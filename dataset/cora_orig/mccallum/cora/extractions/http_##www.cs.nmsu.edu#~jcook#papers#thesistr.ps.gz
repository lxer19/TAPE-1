URL: http://www.cs.nmsu.edu/~jcook/papers/thesistr.ps.gz
Refering-URL: http://www.cs.nmsu.edu/~jcook/papers/
Root-URL: http://www.cs.nmsu.edu
Email: jcook@cs.colorado.edu  
Title: Process Discovery and Validation through Event-Data Analysis  
Author: Jonathan E. Cook 
Note: This work was supported in part by the National Science Foundation under grant CCR-93-02739 and the Air Force Material Command, Rome Laboratory, and the Advanced Research Projects Agency under Contract Number F30602-94-C-0253. The content of the information does not necessarily reflect the position or the policy of the Government and no official endorsement should be inferred.  
Address: Boulder, CO 80309 USA  
Affiliation: Software Engineering Research Laboratory Department of Computer Science University of Colorado  
Abstract: University of Colorado Department of Computer Science Technical Report CU-CS-817-96 November 1996 c fl 1996 Jonathan E. Cook
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> R. Agrawal, T. Imielinski, and A. Swami. </author> <title> Mining association rules between sets of items in large databases. </title> <booktitle> In Proceedings of the ACM-SIGMOD 1993 International Conference on Managment of Data, </booktitle> <pages> pages 207-216. </pages> <publisher> ACM Press, </publisher> <month> May </month> <year> 1993. </year>
Reference-contexts: Other techniques that look at different types of process data will ultimately be useful as 20 Process ModelEvent DataExecuting Process Collection Inference Event 1 Event 2 Event 3 Event 76 Event 77 Event 78 well. For example, the area of data mining (e.g., <ref> [1] </ref>) may have useful techniques for discovering software product relationships. 3.2 Problem Statement and Approach Our goals in this work are to take a trace of the process execution, in the form of an event stream, and deduce (partial) formal models of the behavior of the process.
Reference: [2] <author> A.V. Aho and T.G. Peterson. </author> <title> A minimum distance error-correcting parser for context-free languages. </title> <journal> SIAM Journal on Computing, </journal> <volume> 1(4) </volume> <pages> 305-312, </pages> <month> December </month> <year> 1972. </year>
Reference-contexts: Methods that can compare a model directly with some event stream are also available, but our abstraction away from this is more generalizable. Error-correcting parsers <ref> [2] </ref>, for example, have techniques in which they add error productions to the regular grammar production rules to catch deviations from the expected grammar's syntax. This could be instrumented with measurements, but it is specific to grammar formalisms. <p> Once this symbol is found, the current parse stack is popped until that symbol can be accepted. This method assumes the language has such a set of special symbols, and it makes no attempt at a minimal recovery, only deleting symbols until it can recover. * Aho and Peterson <ref> [2] </ref> show a cubic algorithm for performing global minimum cost error correction in terms of token insertion and deletion. They augment the language grammar with error productions, and modify the parse algorithm to do corrections.
Reference: [3] <author> H. Ahonen, K. Mannila, and E. Nikunen. </author> <title> Grammatical Inference and Applications, </title> <booktitle> volume 862 of Lecture Notes in Artificial Intelligence (subseries of LNCS), </booktitle> <pages> pages 153-167. </pages> <publisher> Springer-Verlag, </publisher> <address> New York, </address> <year> 1994. </year>
Reference-contexts: This machine is then merged according to some rules until a final state machine is output as the learned language. Examples are: * Ahonen et al. <ref> [3] </ref> describe a prefix tree method where states are merged based on previous contexts.
Reference: [4] <author> D. Angluin. </author> <title> Inductive inference of formal languages from positive data. </title> <journal> Information and Control, </journal> <volume> 45 </volume> <pages> 117-135, </pages> <year> 1980. </year>
Reference-contexts: The survey by Angluin and Smith [7] is a broad look at inductive inference learning. Pitt's survey [97] is a very good starting point for understanding the theoretical complexities involved in learning regular grammars. Further theoretical work on language learning is found in <ref> [4, 77, 86, 87, 104] </ref>. Angluin and Smith, in their survey, acknowledge the gap between the theoretical results that have been achieved and the practical application of inference methods.
Reference: [5] <author> D. Angluin. </author> <title> Inference of reversible languages. </title> <journal> Journal of the ACM, </journal> <volume> 29(3) </volume> <pages> 741-765, </pages> <month> July </month> <year> 1982. </year>
Reference-contexts: Examples are: * Ahonen et al. [3] describe a prefix tree method where states are merged based on previous contexts. That is, if two states are entered from the same k-length contexts, then they are merged. 24 * Angluin <ref> [5] </ref> merges states of a prefix tree based on a notion of k-reversibility, which restricts the class of languages her algorithm infers.
Reference: [6] <author> D. Angluin. </author> <title> Learning regular sets from queries and counter-examples. </title> <journal> Information and Computation, </journal> <volume> 75 </volume> <pages> 87-106, </pages> <year> 1987. </year>
Reference-contexts: polynomially approximately predictable, then there is a probabilistic polynomial time algorithm for inverting the RSA encryption function, for factoring Blum integers, and for deciding quadratic residues. 3 Actually, since some of the sequences can be infinite themselves, any theoretical analysis assumes only a presen tation of finite sequences. 23 Angluin <ref> [6] </ref> phrases the learning problem in terms of an oracle. An algorithm can make a fixed set of queries to the oracle, the basic two being "Is this string accepted by the correct grammar?" and "Is this grammar equivalent to the correct grammar?".
Reference: [7] <author> D. Angluin and C.H. Smith. </author> <title> Inductive inference: Theory and methods. </title> <journal> ACM Computing Surveys, </journal> <volume> 15(3) </volume> <pages> 237-269, </pages> <month> September </month> <year> 1983. </year>
Reference-contexts: To develop our initial set of methods, we have cast the process discovery problem in terms of another, previously investigated FSM discovery problem. That problem is the discovery of a grammar for a regular language given example sentences in that language <ref> [7] </ref>. This area of research historically uses the term grammar inference for this problem. If one interprets events as tokens and event streams as sentences in the language of the process, then a natural fit between these areas is seen. <p> ae fl , that is, L is a subset of sentences from fl . 2 A grammar G is a formalism of some class (e.g., regular, context-free) that describes L; L is said to be in that class of languages. 1 This presentation is consistent with others in the field <ref> [7, 97, 105] </ref>. 2 L = fl is not a very interesting language. 22 We define fi L as the infinite set of all pairs hs; li from fl fi f0; 1g, where l = 1 if s 2 L and l = 0 otherwise. <p> With this framework, Angluin gave a polynomial-time algorithm for learning DFAs, and others have extended this to other classes of languages. There is much work describing the computational complexities of various learning paradigms and classes of languages or formulas. The survey by Angluin and Smith <ref> [7] </ref> is a broad look at inductive inference learning. Pitt's survey [97] is a very good starting point for understanding the theoretical complexities involved in learning regular grammars. Further theoretical work on language learning is found in [4, 77, 86, 87, 104].
Reference: [8] <author> A. Apostolico, M.J. Atallah, L.L. Larmore, and S. McFaddin. </author> <title> Efficient parallel algorithms for string editing and related problems. </title> <journal> SIAM Journal on Computing, </journal> <volume> 19(5) </volume> <pages> 968-988, </pages> <year> 1990. </year>
Reference-contexts: Previous work in other areas using string distance measures have also found the need for per-symbol weights <ref> [8, 79] </ref>. 4.4.2 Usability Enhancements The simple and non-linear string distance metrics are naturally decomposable in a hierarchical fashion, to be able to give more information about the measurement than just one number for the whole process model.
Reference: [9] <institution> AT&T Technical Journal, volume 70:2. AT&T Corporation, </institution> <month> March/April </month> <year> 1991. </year>
Reference-contexts: It does not specify what steps a good process contains. The Baldridge award is more focussed on quality control methods. Improvement efforts have been well-received throughout the industry, as evidenced by AT&T <ref> [9] </ref>, IBM [72], and DEC [43] all having a special issue of their technical journals on process improvement.
Reference: [10] <author> G.S. Avrunin, U.A. Buy, J.C. Corbett, L.K. Dillon, and J.C. Wileden. </author> <title> Automated analysis of concurrent systems with the constrained expression toolset. </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> 17(11) </volume> <pages> 1204-1222, </pages> <month> November </month> <year> 1991. </year>
Reference-contexts: Using event data to characterize behavior is widely accepted in other areas of software engineering, such as program visualization [88], concurrent-system analysis <ref> [10] </ref>, and distributed debugging [19, 38]. We feel it is applicable to software process as well. 2.2 Relating Models and Events: Event Sites For our work in this thesis, we focus on behavior formalisms in process modeling languages. <p> If models do not have regularity in the state space, they admit that their techniques will not provide much leverage. Another example that more directly is concerned with matching a behavior to a model is Constrained Expressions <ref> [10, 44] </ref>. This is an elegant method that, given a model, a current simulation state of that model, and a desired event, can answer the question "Can this event be produced in the future?". <p> Cook //----------------------------------------------- #include &lt;stdio.h&gt; #include &lt;Event.h&gt; #include &lt;SocketTokenServer.h&gt; enum Mode -m_event, m_pattern, m_token-; int main (int argc, char **argv) - Mode mode; Event event; Token token; SocketTokenServer *TS; char tokenstr <ref> [10] </ref>; if ((--argc)&lt;4) - fprintf (stderr,"Usage: %s &lt;host&gt; &lt;port&gt; &lt;collection&gt; &lt;mode&gt;"n", argv [0]); return 1; - if (!strcmp (argv [4],"Pattern")) mode = m_pattern; else if (!strcmp (argv [4],"Token")) mode = m_token; else mode = m_event; TS = new SocketTokenServer (argv [1],atoi (argv [2]),argv [3],(int) 'A', 100); while (1) - switch (mode)
Reference: [11] <author> S. Bandinelli, A. Fuggetta, and C. Ghezzi. </author> <title> Software Process Model Evolution in the SPADE Envi-ronement. </title> <journal> IEEE Transactions on Software Engineering, </journal> 19(12) 1128-1144, December 1993. 
Reference-contexts: Paradigms that have been used in formal process modeling include programming [115], graph-based control such as state machines [80] and Petri nets <ref> [11, 12, 40] </ref>, rule-based [14, 96], and planning [68]. Huff [67] gives a good overview of modeling and languages. <p> In response, new methods and tools for supporting various aspects of the software process have been devised. Many of the technologies, including process automation [14, 40, 96, 116], process analysis [61, 63, 80, 103], and process evolution <ref> [11, 76] </ref>, assume the existence of some sort of formal model of a process in order for those technologies to be applied. The need to develop a formal model as a prerequisite to using a new technology is a daunting prospect to the managers of large, on-going projects. <p> Even if one could completely enforce a process, there still remains the issue of managing change in a process, which might lead to a discrepancy between the model and the execution. There has, in fact, been considerable recent work that addresses process evolution <ref> [11, 76] </ref>. Commensurate with the historical approach mentioned above, that work is concerned more with the problem of effecting changes to a process model used for automation, than it is with the problem of uncovering inconsistencies between the model and the execution.
Reference: [12] <author> S. Bandinelli, A. Fuggetta, C. Ghezzi, and L. Lavazza. SPADE: </author> <title> An Environment for Software Process, Analysis, Design, </title> <editor> and Enactment. In A. Finkelstein, J. Kramer, and B. Nuseibeh, editors, </editor> <booktitle> Software Process Modeling and Technology, </booktitle> <pages> pages 223-248. </pages> <publisher> Wiley, </publisher> <year> 1994. </year>
Reference-contexts: Paradigms that have been used in formal process modeling include programming [115], graph-based control such as state machines [80] and Petri nets <ref> [11, 12, 40] </ref>, rule-based [14, 96], and planning [68]. Huff [67] gives a good overview of modeling and languages. <p> Work in this area includes environments based on process programming [116], rule-based systems such as Marvel [14], and other formalism-based environments, such as the Petri net-based SPADE <ref> [12] </ref>. Garg and Jazayeri [53] give a good overview of process-centered environments and their issues. Many environments take the approach of providing the developer with an agenda of tasks to be done, thus giving the control of task execution to the developer. <p> We feel it is applicable to software process as well. 2.2 Relating Models and Events: Event Sites For our work in this thesis, we focus on behavior formalisms in process modeling languages. Examples are the Petri net foundation in Slang <ref> [12] </ref>, state machines, and Statecharts [64]. In viewing a process execution as (part of) an event stream, we are assuming that a process model has some way of producing an event stream if it was "executed". <p> The system is based on watching for events and matching the events and their contexts with the current state of the process. Lacking a process model, their infrastructure could be used to simply collect the event data. Most process enactment systems such as Oz [20] and SPADE <ref> [12] </ref> could easily be instrumented to collect event data. Process integration components, such as ProcessWall [66], are likewise a natural focal point for such instrumentation. <p> Thus, this simple method of providing access through a scripting language allows an extensible collection tools such as Yeast to interface with the Balboa framework. Process-centered environments, such as as Oz [20] and Spade <ref> [12] </ref>, have the ability to invoke and communicate with external tools during the enactment of the process, so they would not have a problem connecting to Balboa.
Reference: [13] <author> S. Bandinelli, C. Ghezzi, and A. Morzenti. </author> <title> A Multi-Paradigm Petri Net Based Approach to Process Description. </title> <booktitle> In Proceedings of the 7th International Software Process Workshop, </booktitle> <pages> pages 41-43, </pages> <month> October </month> <year> 1991. </year>
Reference-contexts: Several formal models suitable for our analyses have been used to describe software processes. These include models based on state machines (e.g., Statemate [65]), Petri nets (e.g., Slang <ref> [13] </ref> and FUNSOFT Nets [63]), and procedural languages (e.g., APPL/A [114]). 4.2.1 Recognition Metric The first metric is a very straightforward one that has just two values, true or false. The value is true if the event streams exactly match and is false otherwise.
Reference: [14] <author> N.S. Barghouti and G.E. Kaiser. </author> <title> Scaling Up Rule-based Development Environments. </title> <booktitle> In Proceedings of the Third European Software Engineering Conference, number 550 in Lecture Notes in Computer Science, </booktitle> <pages> pages 380-395. </pages> <publisher> Springer-Verlag, </publisher> <month> October </month> <year> 1991. </year>
Reference-contexts: Paradigms that have been used in formal process modeling include programming [115], graph-based control such as state machines [80] and Petri nets [11, 12, 40], rule-based <ref> [14, 96] </ref>, and planning [68]. Huff [67] gives a good overview of modeling and languages. <p> This work in some sense is closely related to workflow, since both try to use a model of the work to drive the type and order of activities. Work in this area includes environments based on process programming [116], rule-based systems such as Marvel <ref> [14] </ref>, and other formalism-based environments, such as the Petri net-based SPADE [12]. Garg and Jazayeri [53] give a good overview of process-centered environments and their issues. <p> In response, new methods and tools for supporting various aspects of the software process have been devised. Many of the technologies, including process automation <ref> [14, 40, 96, 116] </ref>, process analysis [61, 63, 80, 103], and process evolution [11, 76], assume the existence of some sort of formal model of a process in order for those technologies to be applied.
Reference: [15] <author> N.S. Barghouti and B. Krishnamurthy. </author> <title> Using event contexts and matching constraints to monitor software processes. </title> <booktitle> In Proceedings of the 17th International Conference on Software Engineering, </booktitle> <pages> pages 83-92. </pages> <publisher> IEEE Computer Society Press, </publisher> <month> April </month> <year> 1995. </year>
Reference-contexts: It also provides functionality for reporting any type of off-computer event as well. * Barghouti and Krishnamurthy <ref> [15] </ref> describe a process enactment system that alternatively could be used to collect event data. The system is based on watching for events and matching the events and their contexts with the current state of the process.
Reference: [16] <author> V.R. Basili and D.M. Weiss. </author> <title> A methodology for collecting valid software engineering data. </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> 10(6) </volume> <pages> 728-737, </pages> <year> 1984. </year> <month> 133 </month>
Reference-contexts: Many efforts use product measurements to infer some process qualities, such as defect production, change data, and other 3 product measurements <ref> [16, 21, 35, 110] </ref>. <p> 2.4 Event Data Collection Techniques for process validation and discovery clearly depend on an ability to collect data about an executing process, but we do not address this topic in this thesis, because a variety of methods for collecting process execution data have already been devised: * Basili and Weiss <ref> [16] </ref> describe a method for manual, forms-based collection of data for use in evaluating and comparing software development methods. <p> They applied Amadeus to automatically building classification trees that describe what properties of a source code module are likely to lead to faults in the module. However, Amadeus itself is a generic data collection and activity automation system. * Basili and Weiss <ref> [16] </ref> describe a methodology for selecting metrics and data collection techniques based on the goals that are desired of the measurement activity. Their work also focuses on using product data, such as code modifications and change classification. <p> This work, so far, has seen the creation of single tools that access process data in an ad hoc manner. Several methods for collecting process data have been proposed and constructed (e.g., <ref> [16, 23, 110, 121] </ref>); however, there has not been a significant effort to propose a coherent framework for tools and methods with which to do analysis from process data. Such a framework would be useful in many ways. <p> If it does not, then the process model may not be useful in describing what is good. The key to answering this question lies in the collection and analysis of reliable data. There have been numerous studies that have analyzed product data to identify patterns of defects (e.g., <ref> [16, 21, 34, 110] </ref>). From these patterns, the process mechanisms behind them are inferred, but this last step of inference is purely speculative and not backed directly by hard data.
Reference: [17] <author> P. Bates. </author> <title> EBBA modelling tool a.k.a. event definition language. </title> <type> Technical Report COINS-87-35, </type> <institution> University of Massachusetts at Amherst, </institution> <year> 1987. </year>
Reference-contexts: process improvement by raising confidence in the correspondence between formal models and executions of processes. 4.1.2 Related Event Work In using event-based data to compare an execution with a formal model, the most closely related work to ours is in the areas of distributed debugging and history checking. * Bates <ref> [17, 18, 19] </ref> uses "event-based behavioral abstraction" to characterize the behavior of programs. He then attempts to match the event data to a model based on regular expressions.
Reference: [18] <author> P. Bates. </author> <title> Shu*e automata: A formal model for behavior recognition in distributed systems. </title> <type> Technical Report COINS-87-27, </type> <institution> University of Massachusetts at Amherst, </institution> <year> 1987. </year>
Reference-contexts: process improvement by raising confidence in the correspondence between formal models and executions of processes. 4.1.2 Related Event Work In using event-based data to compare an execution with a formal model, the most closely related work to ours is in the areas of distributed debugging and history checking. * Bates <ref> [17, 18, 19] </ref> uses "event-based behavioral abstraction" to characterize the behavior of programs. He then attempts to match the event data to a model based on regular expressions.
Reference: [19] <author> P. Bates. </author> <title> Debugging heterogenous systems using event-based models of behavior. </title> <booktitle> In Proceedings of a Workshop on Parallel and Distributed Debugging, </booktitle> <pages> pages 11-22. </pages> <publisher> ACM Press, </publisher> <month> January </month> <year> 1989. </year>
Reference-contexts: Using event data to characterize behavior is widely accepted in other areas of software engineering, such as program visualization [88], concurrent-system analysis [10], and distributed debugging <ref> [19, 38] </ref>. We feel it is applicable to software process as well. 2.2 Relating Models and Events: Event Sites For our work in this thesis, we focus on behavior formalisms in process modeling languages. Examples are the Petri net foundation in Slang [12], state machines, and Statecharts [64]. <p> process improvement by raising confidence in the correspondence between formal models and executions of processes. 4.1.2 Related Event Work In using event-based data to compare an execution with a formal model, the most closely related work to ours is in the areas of distributed debugging and history checking. * Bates <ref> [17, 18, 19] </ref> uses "event-based behavioral abstraction" to characterize the behavior of programs. He then attempts to match the event data to a model based on regular expressions.
Reference: [20] <author> I.S. Ben-Shaul and G.E. Kaiser. </author> <title> A paradigm for decentralized process modeling and it realization in the OZ environment. </title> <booktitle> In Proceedings of the 16th International Conference on Software Engineering, </booktitle> <pages> pages 179-188. </pages> <publisher> IEEE Computer Society Press, </publisher> <month> May </month> <year> 1994. </year>
Reference-contexts: The system is based on watching for events and matching the events and their contexts with the current state of the process. Lacking a process model, their infrastructure could be used to simply collect the event data. Most process enactment systems such as Oz <ref> [20] </ref> and SPADE [12] could easily be instrumented to collect event data. Process integration components, such as ProcessWall [66], are likewise a natural focal point for such instrumentation. <p> Thus, this simple method of providing access through a scripting language allows an extensible collection tools such as Yeast to interface with the Balboa framework. Process-centered environments, such as as Oz <ref> [20] </ref> and Spade [12], have the ability to invoke and communicate with external tools during the enactment of the process, so they would not have a problem connecting to Balboa.
Reference: [21] <author> I. Bhandari, M. Halliday, E. Tarver, D. Brown, J. Chaar, and R. Chillarege. </author> <title> A Case Study of Software Process Improvement During Development. </title> <journal> IEEE Transactions on Software Engineering, </journal> 19(12) 1157-1170, December 1993. 
Reference-contexts: Many efforts use product measurements to infer some process qualities, such as defect production, change data, and other 3 product measurements <ref> [16, 21, 35, 110] </ref>. <p> Below, we summarize some of this work. 1 Numeric sequences, which are really a representation of some function (e.g., a time series of stock value), is a different topic altogether. 56 * Chmura et al. [35] and Bhandari et al. <ref> [21] </ref> try to deduce problems in the process by looking at defect data in the products. Specifically, they statistically analyzed change data and effort data to determine the behavior of the process. <p> If it does not, then the process model may not be useful in describing what is good. The key to answering this question lies in the collection and analysis of reliable data. There have been numerous studies that have analyzed product data to identify patterns of defects (e.g., <ref> [16, 21, 34, 110] </ref>). From these patterns, the process mechanisms behind them are inferred, but this last step of inference is purely speculative and not backed directly by hard data.
Reference: [22] <author> A.W. Biermann and J.A. Feldman. </author> <title> On the Synthesis of Finite State Machines from Samples of Their Behavior. </title> <journal> IEEE Transactions on Computers, </journal> <volume> 21(6) </volume> <pages> 592-597, </pages> <month> June </month> <year> 1972. </year>
Reference-contexts: While 0-reversible languages can be inferred in near-linear time, higher k values cause the algorithm to be cubic in the length of the input. * Bierman and Feldman <ref> [22] </ref> describe a prefix tree method where states are merged based on k-length future behaviors. <p> The Ktail method is a purely algorithmic approach that looks at the future behavior to compute a possible current state. We modified the theoretical description given in <ref> [22] </ref> to make it less dependent on multiple sequences, and extended it to allow for handling noisy data. In addition, we added some post-analysis steps that remove some common overly complex constructs that the basic algorithm tends to leave in a discovered model. Our implementation was from scratch. 3. <p> The plethora of tuning parameters and the lack of guidelines in setting them are a drawback as well. 3.4.2 Ktail The next method is purely algorithmic and based on work by Biermann and Feldman <ref> [22] </ref>. Their original presentation was formulated in terms of sample strings and output values for the FSM at the end of the strings. Our formulation of this algorithm does not make use of the output values and is thus presented as just operating on the sample strings themselves.
Reference: [23] <author> M.G. Bradac, D.E. Perry, and L.G. Votta. </author> <title> Prototyping a process monitoring experiment. </title> <journal> IEEE Transactions on Software Engineering, </journal> <pages> pages 774-784, </pages> <month> October </month> <year> 1994. </year>
Reference-contexts: Some efforts are trying to use measurements to help characterize the process; for example, Wolf and Rosenblum collected event-based data and used it to do some temporal and visualization analysis [121], and Bradac, Votta, and Perry use actual measurements to define the parameters to a queueing model of a process <ref> [23] </ref>. A body of work has looked at experimental measurement of specific process techniques. One example of this is the detailed study of code inspections by Votta, Porter, and Siy [98]. <p> Our use would simply collect the events into an event stream. * Wolf and Rosenblum [121] use a hybrid of manual and automated collection methods to collect event data from a build process. * Bradac, et al. <ref> [23] </ref>, provided the user with a menu-based tool that collects sampled task and state information. <p> centers on using a rule base and goals to derive a generalized execution flow from a specific process history. * Garg et al. [56] employ a manual process history analysis in the context of a meta-process for creating and validating domain-specific process models and software toolkits. * Bradac et al. <ref> [23] </ref> describe the beginnings of a process monitoring experiment in which their goal is to model the process as a queuing network, and use actual data about the time spent by the agents in specific tasks and states to determine the real parameters (i.e., service times and probabilities, and branch path <p> This work, so far, has seen the creation of single tools that access process data in an ad hoc manner. Several methods for collecting process data have been proposed and constructed (e.g., <ref> [16, 23, 110, 121] </ref>); however, there has not been a significant effort to propose a coherent framework for tools and methods with which to do analysis from process data. Such a framework would be useful in many ways. <p> There are many kinds of data we could examine, but we chose to look at event data <ref> [23, 121] </ref> because they neatly characterize the dynamic behavior of the process in terms of the sequencing of its major activities. The event data come from several sources.
Reference: [24] <author> A. Brazma. </author> <title> Algorithmic Learning Theory, </title> <booktitle> volume 872 of Lecture Notes in Artificial Intelligence (subseries of LNCS), </booktitle> <pages> pages 260-271. </pages> <publisher> Springer-Verlag, </publisher> <address> New York, </address> <year> 1994. </year>
Reference-contexts: In this sense, these algorithms make a pass through a sequence, find repetitive patterns, replace all occurrences with the regular expression, then start all over until it is decided that they are done. Two examples are: * The work of Brazma et al. <ref> [25, 24, 26] </ref> is most representative of this work. They have been able to construct fast algorithms that learn restricted regular expressions. The restrictions they place on the inferred regular expressions are that both the or operator and the *-nesting are limited and fixed.
Reference: [25] <author> A. Brazma and K. Cerans. </author> <title> Algorithmic Learning Theory, </title> <booktitle> volume 872 of Lecture Notes in Artificial Intelligence (subseries of LNCS), </booktitle> <pages> pages 76-90. </pages> <publisher> Springer-Verlag, </publisher> <address> New York, </address> <year> 1994. </year>
Reference-contexts: In this sense, these algorithms make a pass through a sequence, find repetitive patterns, replace all occurrences with the regular expression, then start all over until it is decided that they are done. Two examples are: * The work of Brazma et al. <ref> [25, 24, 26] </ref> is most representative of this work. They have been able to construct fast algorithms that learn restricted regular expressions. The restrictions they place on the inferred regular expressions are that both the or operator and the *-nesting are limited and fixed.
Reference: [26] <author> A. Brazma, I. Jonassen, I. Eidhammer, and D. Gilbert. </author> <title> Approaches to the automatic discovery of patterns in biosequences. </title> <type> Technical Report TCU/CS/1995/18, </type> <institution> City University (London), </institution> <month> December </month> <year> 1995. </year>
Reference-contexts: In this sense, these algorithms make a pass through a sequence, find repetitive patterns, replace all occurrences with the regular expression, then start all over until it is decided that they are done. Two examples are: * The work of Brazma et al. <ref> [25, 24, 26] </ref> is most representative of this work. They have been able to construct fast algorithms that learn restricted regular expressions. The restrictions they place on the inferred regular expressions are that both the or operator and the *-nesting are limited and fixed.
Reference: [27] <author> J.R. Burch, E.M. Clarke, K.L. McMillan, D.L. Dill, and L.J. Hwang. </author> <title> Symbolic model checking: 10 20 states and beyond. </title> <journal> Information and Computation, </journal> <volume> 98 </volume> <pages> 141-170, </pages> <year> 1992. </year>
Reference-contexts: Model Checking Another body of research called model checking has created several methods that enable checking very large models (in terms of their state space) for inherent properties, including whether a behavior is allowed by the model. In one example <ref> [27] </ref>, Burch, et.al. describe a model checker based on binary decision diagrams that is able to check models with 10 20 states, where previous work had only handled 10 8 states.
Reference: [28] <author> R.C. Carrasco and J. Oncina. </author> <title> Grammatical Inference and Applications, </title> <booktitle> volume 862 of Lecture Notes in Artificial Intelligence (subseries of LNCS), </booktitle> <pages> pages 139-152. </pages> <publisher> Springer-Verlag, </publisher> <address> New York, </address> <year> 1994. </year>
Reference-contexts: Although their method is not directly described as prefix tree merging, it does fall in this class. * Carrasco and Oncina <ref> [28] </ref> describe a statistical method for learning stochastic regular languages, based on state merging from a prefix tree, where states are merged based on statistical likelihood calculations.
Reference: [29] <author> J. Carrol and D. </author> <title> Long. Theory of Finite Automata. </title> <publisher> Prentice-Hall, </publisher> <address> Englewood Cliffs, New Jersey, </address> <year> 1989. </year>
Reference-contexts: While FSMs may be somewhat deficient for prescribing software processes, they are quite convenient and sufficiently powerful for describing historical patterns of actual behavior. In this thesis we do not present a formal description of finite state machines. A good reference for this background is Carrol and Long <ref> [29] </ref>. We use the term FSM to refer to nondeterministic, transition-labeled state machines.
Reference: [30] <author> F. Casacuberta. </author> <title> Grammatical Inference and Applications, </title> <booktitle> volume 862 of Lecture Notes in Artificial Intelligence (subseries of LNCS), </booktitle> <pages> pages 119-129. </pages> <publisher> Springer-Verlag, </publisher> <address> New York, </address> <year> 1994. </year>
Reference-contexts: * Sanchez and Benedi [106] use a specific inference mechanism (that only finds loop-free automata) to provide a model structure and initial transition probabilities to a forward-backward parameter estimation algorithm. * Chen [33] describes a corpus-based SCFG inference method using bayesian probabilities and compares it to ngram models. * Casacuberta <ref> [30] </ref> provides a transformation algorithm that takes a CFG and translates it to a CFG in Chomsky Normal Form, which is required for parameter estimation using the 26 inside-outside algorithm. 3.3.5 Inference Using Neural Nets The neural network community has also looked at the problem of grammar inference from positive samples.
Reference: [31] <author> A. Castellanos, I. Galiano, and E. Vidal. </author> <title> Grammatical Inference and Applications, </title> <booktitle> volume 862 of Lecture Notes in Artificial Intelligence (subseries of LNCS), </booktitle> <pages> pages 93-105. </pages> <publisher> Springer-Verlag, </publisher> <address> New York, </address> <year> 1994. </year>
Reference-contexts: Running time is maximally n 3 ; however they claim actual times are near-linear. * Castellanos et al. <ref> [31] </ref> apply a method of this class to learning natural language translators. Another class of techniques is directed more towards iteratively building up a regular expression from the sequences, and then translating that into an FSM (if necessary).
Reference: [32] <author> E. Charniak. </author> <title> Statistical Language Learning. </title> <publisher> The MIT Press, </publisher> <address> Cambridge, Massachusetts, </address> <year> 1993. </year>
Reference-contexts: The algorithms mentioned above for parameter estimation on both the HMMs and SCFGs are types of expectation maximization algorithms. All of them are gradient descent techniques that find a local maximum. Hence, initializations are important for them. A good reference on these algorithms is Charniak's book <ref> [32] </ref>. To solve the structure inference problem for statistical models, the standard algorithmic techniques for inferring an FSM or CFG can be applied to finding a structure, and then the well-known algorithms can be used to assign the correct probabilities to the transitions or production rules.
Reference: [33] <author> S.F. Chen. </author> <title> Bayesian grammar induction for language modeling. </title> <type> Technical Report TR-01-95, </type> <institution> Harvard University, Center for Research in Computing Technology, </institution> <month> January </month> <year> 1995. </year>
Reference-contexts: His results show a good improvement over parameter estimation beginning with a fully connected model. * Sanchez and Benedi [106] use a specific inference mechanism (that only finds loop-free automata) to provide a model structure and initial transition probabilities to a forward-backward parameter estimation algorithm. * Chen <ref> [33] </ref> describes a corpus-based SCFG inference method using bayesian probabilities and compares it to ngram models. * Casacuberta [30] provides a transformation algorithm that takes a CFG and translates it to a CFG in Chomsky Normal Form, which is required for parameter estimation using the 26 inside-outside algorithm. 3.3.5 Inference Using
Reference: [34] <author> L.J. Chmura, A.F. Norcio, and T.J. Wicinski. </author> <title> Evaluating software design process by analyzing change data over time. </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> 16(7) </volume> <pages> 729-739, </pages> <month> July </month> <year> 1990. </year> <month> 134 </month>
Reference-contexts: If it does not, then the process model may not be useful in describing what is good. The key to answering this question lies in the collection and analysis of reliable data. There have been numerous studies that have analyzed product data to identify patterns of defects (e.g., <ref> [16, 21, 34, 110] </ref>). From these patterns, the process mechanisms behind them are inferred, but this last step of inference is purely speculative and not backed directly by hard data.
Reference: [35] <author> L.J. Chmura, A.F. Norcio, and T.J. Wicinski. </author> <title> Evaluating Software Design Processes by Analyzing Change Data Over Time. </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> 16(7) </volume> <pages> 729-739, </pages> <month> July </month> <year> 1990. </year>
Reference-contexts: Many efforts use product measurements to infer some process qualities, such as defect production, change data, and other 3 product measurements <ref> [16, 21, 35, 110] </ref>. <p> Below, we summarize some of this work. 1 Numeric sequences, which are really a representation of some function (e.g., a time series of stock value), is a different topic altogether. 56 * Chmura et al. <ref> [35] </ref> and Bhandari et al. [21] try to deduce problems in the process by looking at defect data in the products. Specifically, they statistically analyzed change data and effort data to determine the behavior of the process.
Reference: [36] <author> J.E. Cook and A.L. Wolf. </author> <title> Toward Metrics for Process Validation. </title> <booktitle> In Proceedings of the Third International Conference on the Software Process, </booktitle> <pages> pages 33-44. </pages> <publisher> IEEE Computer Society, </publisher> <month> October </month> <year> 1994. </year>
Reference-contexts: Time-oriented metrics, for example, would be very a useful extension to our validation methods. Real-time systems analysis techniques could be useful here [51, 109]. Methods for measuring the efficiency of a process would be another useful analysis method, which we explored a little in <ref> [36] </ref>. Both of these would help in the optimization of a process that has already been behaviorally validated. * Identifying what formal properties are maintained in a behavior that does not agree 100% with a model. 131 Formal models are often useful because they can detect conflicts and consistency violations.
Reference: [37] <author> J. Corbett and A. Polk. </author> <title> A tool for automatic generation of behaviors for constrained expression analysis. </title> <type> Technical report, </type> <month> February </month> <year> 1993. </year>
Reference-contexts: The ILP system produces a binary answer, and some parameters when it finds a solution. If the answer is "yes", heuristics are used along with the parameters from the ILP system to produce a plausible behavior that leads to the event <ref> [37] </ref>. This behavior constitutes the next sequence of model events. Unfortunately, if the answer is "no", Constrained Expressions do not help in determining a correction to the event stream or model state to continue the analysis of the rest of the event stream.
Reference: [38] <author> J. Cuny, G. Forman, A. Hough, J. Kundu, C. Lin, L. Snyder, and D. Stemple. </author> <title> The adriane debugger: Scalable application of event-based abstraction. </title> <booktitle> In Proceedings of the ACM/ONR Workshop on Parallel and Distributed Debugging, </booktitle> <pages> pages 85-95. </pages> <publisher> ACM Press, </publisher> <month> May </month> <year> 1993. </year>
Reference-contexts: Using event data to characterize behavior is widely accepted in other areas of software engineering, such as program visualization [88], concurrent-system analysis [10], and distributed debugging <ref> [19, 38] </ref>. We feel it is applicable to software process as well. 2.2 Relating Models and Events: Event Sites For our work in this thesis, we focus on behavior formalisms in process modeling languages. Examples are the Petri net foundation in Slang [12], state machines, and Statecharts [64]. <p> He then attempts to match the event data to a model based on regular expressions. However, he only marks the points at which the data and model did not match, not attempting to provide aggregate measures of disparity. * Cuny et al. <ref> [38] </ref> builds on the work of Bates, attempting to deal with large amounts of event data by providing query mechanisms for event relationships.
Reference: [39] <author> S. Das and M.C. Mozer. </author> <title> A Unified Gradient-Descent/Clustering Architecture for Finite State Machine Induction. </title> <booktitle> In Proceedings of the 1993 Conference, number 6 in Advances in Neural Information Processing Systems, </booktitle> <pages> pages 19-26. </pages> <publisher> Morgan Kaufmann, </publisher> <year> 1994. </year>
Reference-contexts: Recent representative work in this field is <ref> [39, 124] </ref>. The various methods all consist of defining a recurrent network architecture, and then analyzing the hidden neuron activity to discover the states and transitions for the resulting grammar. The difference between these methods is how they inspect the hidden neurons to infer state information. <p> The methods can be characterized by the method in which they examine those samples. 1. The RNet method is a purely statistical (neural network) approach that looks at the past behavior to characterize a state. For this method we have extended an implementation by Das <ref> [39] </ref>. Our extensions allow this method to handle more event types (theirs was restricted to two), and enable the easier extraction of the discovered model from the net. 2. The Ktail method is a purely algorithmic approach that looks at the future behavior to compute a possible current state. <p> This recently developed method is due to Das and Mozer <ref> [39] </ref>. We refer to it here as the RNet method. In a standard feed-forward neural network, neurons are split into layers, with all the outputs of the neurons in one layer feeding forward into all the neurons of the next layer (see Figure 3.3a). <p> RNet was adapted from an implementation by Das <ref> [39] </ref>, while the others were implemented from scratch. Additionally, a common user interface, other common code (startup and graph operations), and an interface to the Balboa framework (Chapter 5) are all implemented. Table 3.2 shows the size (LOC) of the various discovery tools.
Reference: [40] <author> W. Deiters and V. Gruhn. </author> <title> Managing Software Processes in the Environment MELMAC. </title> <booktitle> In SIGSOFT '90: Proceedings of the Fourth Symposium on Software Development Environments, </booktitle> <pages> pages 193-205. </pages> <booktitle> ACM SIGSOFT, </booktitle> <month> December </month> <year> 1990. </year>
Reference-contexts: Paradigms that have been used in formal process modeling include programming [115], graph-based control such as state machines [80] and Petri nets <ref> [11, 12, 40] </ref>, rule-based [14, 96], and planning [68]. Huff [67] gives a good overview of modeling and languages. <p> In response, new methods and tools for supporting various aspects of the software process have been devised. Many of the technologies, including process automation <ref> [14, 40, 96, 116] </ref>, process analysis [61, 63, 80, 103], and process evolution [11, 76], assume the existence of some sort of formal model of a process in order for those technologies to be applied.
Reference: [41] <author> W. Edwards Deming. </author> <title> Out of Crisis. </title> <publisher> MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1982. </year>
Reference-contexts: The agreed upon activities found in the process model may be unnecessary structure imposed on the developers. Or worse, they may create a complex set of interactions that in the end serves to completely demotivate people, as shown in Deming's red and white beads experiment <ref> [41] </ref>. Indeed, the current wisdom is that it is possible, for example, to achieve a high CMM maturity level (i.e., use good practices) and still have trouble producing quality products. So the question remains.
Reference: [42] <author> J.L. Devore. </author> <title> Probability and Statistics for Engineering and the Sciences. </title> <address> Brooks/Cole, Pacific Grove, California, </address> <note> 3rd edition, </note> <year> 1991. </year>
Reference-contexts: Thus, one might pick the standard statistical correlation rules of thumb <ref> [42] </ref> and say that any measurement less than 0:2 is a strong correspondence, less than 0:5 is a moderate correspondence, and greater than 0:5 is a weak correspondence. (Actually, these are inversions of the standard statistical rules of thumb, but their effect is the same.) 4.2.3 Non-linear String Distance Metric A <p> Process validation is described in the next section. Most of the analyses were performed using metrics whose values are numeric. For those metrics, we used the Mann-Whitney significance test <ref> [42, Chapter 15] </ref>, which does not assume an underlying distribution of the data but is still nearly as powerful as standard significance tests that do assume a distribution.
Reference: [43] <institution> Digital Technical Journal, volume 5:4. Digital Equipment Corporation, </institution> <month> Fall </month> <year> 1993. </year>
Reference-contexts: It does not specify what steps a good process contains. The Baldridge award is more focussed on quality control methods. Improvement efforts have been well-received throughout the industry, as evidenced by AT&T [9], IBM [72], and DEC <ref> [43] </ref> all having a special issue of their technical journals on process improvement.
Reference: [44] <author> L.K. Dillon, G.S. Avrunin, and J.C. Wileden. </author> <title> Constrained expressions: Toward broad applicability of analysis methods for distributed software systems. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 10(3) </volume> <pages> 374-402, </pages> <month> July </month> <year> 1988. </year>
Reference-contexts: If models do not have regularity in the state space, they admit that their techniques will not provide much leverage. Another example that more directly is concerned with matching a behavior to a model is Constrained Expressions <ref> [10, 44] </ref>. This is an elegant method that, given a model, a current simulation state of that model, and a desired event, can answer the question "Can this event be produced in the future?".
Reference: [45] <author> M.W. Du and S.C. Chang. </author> <title> A model and a fast algorithm for multiple errors spelling correction. </title> <journal> Acta Informatica, </journal> <volume> 29 </volume> <pages> 281-302, </pages> <year> 1992. </year>
Reference: [46] <author> Z. K. F. Eckert and G. J. Nutt. </author> <title> Trace extrapolation for parallel programs on shared memory multiprocessors. </title> <type> Technical Report TR CU-CS-804-96, </type> <institution> Department of Computer Science, University of Colorado, </institution> <month> May </month> <year> 1996. </year>
Reference-contexts: For example, a key event might happen at a specific site, where all previous behavior can then be ignored. This is similar to the concept of trace change points in <ref> [46] </ref>. * Implementing other modeling paradigms, such as Petri nets, for the metric-calculation engine. * Developing techniques for better visualizing the measurements. <p> This idea is similar to the concept of trace change points <ref> [46] </ref>. * Implementing other modeling paradigms, such as Petri nets, for the metric-calculation engine. * Developing techniques for better visualization of the measurements.
Reference: [47] <author> K. El Emam, N. Moukheiber, and N.H. Madhavji. </author> <title> An evaluation of the G/Q/M method. </title> <type> Technical Report MCGILL/SE-94-11, </type> <institution> McGill University, </institution> <year> 1994. </year>
Reference-contexts: This work is more along the lines of a process post-mortem, to analyze by discussion the changes that a process should undergo for the next cycle. 3. Madhavji et al. <ref> [47] </ref> describe a method for eliciting process models from currently executing processes. The method is basically a plan for how a person would enter a development organization, understand their process, and describe the process in a formal way. There is no notion of automation or tool support in any way.
Reference: [48] <author> D. Eppstein. </author> <title> Sequence comparison with mixed convex and concave costs. </title> <journal> Journal of Algorithms, </journal> <volume> 11 </volume> <pages> 85-101, </pages> <year> 1990. </year>
Reference-contexts: For simple operation and symbol weightings, equivalent to our SSD metric, their algorithms operate in O (M N ) time. However, dealing with multi-symbol blocks (or gaps), as our NSD metric requires, complicates matters significantly. In general, for both string to string comparisons <ref> [48] </ref> and string to regular expression comparisons [93], arbitrary cost functions for blocks require at least O (M N max (M; N )), or cubic time.
Reference: [49] <author> M.E. Fagan. </author> <title> Advances in Software Inspections. </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> SE-12(7):744-751, </volume> <month> July </month> <year> 1986. </year>
Reference-contexts: Representative examples include the venerable waterfall model, prototyping, the risk-based spiral model, and the cleanroom process model [99]. Some canonical models involve only specific sub-processes such as analysis, design, and testing. Code inspections and design reviews are perhaps the most studied and formalized of sub-processes <ref> [49] </ref>. Other work in this vein includes SA/SD, Booch Object-Oriented Design, and regression testing [99]. 1.2.2 Execution Execution research is involved in creating process execution environments in which a formal, executable process model drives the activities that take place in the environment.
Reference: [50] <author> M. Felder, D. Mandrioli, and A. Morzenti. </author> <title> Proving Properties of Real-time Systems Through Logical Specifications and Petri Net Models. </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> 20(2) </volume> <pages> 127-141, </pages> <month> February </month> <year> 1994. </year>
Reference-contexts: They assume that there is some problem somewhere in the event stream and that one is trying to locate that problem. * Felder et al. <ref> [50, 51] </ref> describe a method and tool by which one can compare an execution history against a temporal logic specification to decide the correctness of that execution with respect to the model. Our immediate goal is to quantify discrepancies, with correctness being a subsumed issue.
Reference: [51] <author> M. Felder and A. Morzenti. </author> <title> Validating Real-time Systems by History-checking TRIO Specifications. </title> <booktitle> In Proceedings of the 14th International Conference on Software Engineering, </booktitle> <pages> pages 199-211. </pages> <publisher> IEEE Computer Society, </publisher> <month> May </month> <year> 1992. </year> <month> 135 </month>
Reference-contexts: They assume that there is some problem somewhere in the event stream and that one is trying to locate that problem. * Felder et al. <ref> [50, 51] </ref> describe a method and tool by which one can compare an execution history against a temporal logic specification to decide the correctness of that execution with respect to the model. Our immediate goal is to quantify discrepancies, with correctness being a subsumed issue. <p> Time-oriented met--rics, for example, would be very a useful extension to execution stream analysis. Real-time systems analysis techniques could be useful here <ref> [51, 109] </ref>. Methods for measuring the efficiency of a process would be another useful analysis method. Both of these would help in the optimization of a process that has already been behaviorally validated. * Formal models are often useful because they can detect conflicts and consistency violations. <p> Time-oriented metrics, for example, would be very a useful extension to our validation methods. Real-time systems analysis techniques could be useful here <ref> [51, 109] </ref>. Methods for measuring the efficiency of a process would be another useful analysis method, which we explored a little in [36].
Reference: [52] <author> C.N. Fischer and J. Mauney. </author> <title> A simple, fast, and effective LL(1) error repair algorithm. </title> <journal> Acta Infor--matica, </journal> <volume> 29 </volume> <pages> 109-120, </pages> <year> 1992. </year>
Reference-contexts: This method is based on the ideas of minimum distance corrections, but makes the assumption that one never needs to back up in the input stream to find a good correction. * Fischer and Mauney <ref> [52] </ref> describe a method for locally least cost error correction. They are also biased towards insertions, but include deletions. Their method will back up in the current parse stack, and uses a local search with a priority queue to find a locally minimum cost fix.
Reference: [53] <author> P. Garg and M. Jazayeri. </author> <title> Process-centered software engineering environments: A grand tour. In Software Process, </title> <booktitle> Trends in Software, </booktitle> <pages> pages 25-52. </pages> <publisher> Wiley, </publisher> <year> 1996. </year>
Reference-contexts: Work in this area includes environments based on process programming [116], rule-based systems such as Marvel [14], and other formalism-based environments, such as the Petri net-based SPADE [12]. Garg and Jazayeri <ref> [53] </ref> give a good overview of process-centered environments and their issues. Many environments take the approach of providing the developer with an agenda of tasks to be done, thus giving the control of task execution to the developer.
Reference: [54] <author> P.K. Garg and S. Bhansali. </author> <title> Process programming by hindsight. </title> <booktitle> In Proceedings of the 14th International Conference on Software Engineering, </booktitle> <pages> pages 280-293. </pages> <publisher> IEEE Computer Society Press, </publisher> <month> May </month> <year> 1992. </year>
Reference-contexts: Three pieces of work that are related are: 1. Garg and Bhansali <ref> [54] </ref> describe a method using explanation-based learning to discover aspects and fragments of the underlying process model from process history data and rules of operations and their effects. This work centers on using a rule base and goals to derive a generalized execution flow from a specific process history.
Reference: [55] <author> P.K. Garg and S. Bhansali. </author> <title> Process Programming by Hindsight. </title> <booktitle> In Proceedings of the 14th International Conference on Software Engineering, </booktitle> <pages> pages 280-293. </pages> <publisher> IEEE Computer Society, </publisher> <month> May </month> <year> 1992. </year>
Reference-contexts: Though there is no attempt to relate this to real data, "what-if" analyses are powerful tools in their own right. Some recent efforts have begun to look at process data itself, but still not for the purpose of process validation. * Garg and Bhansali <ref> [55] </ref> describe a method that uses explanation-based learning to discover aspects and fragments of the underlying process model from process history data and rules of operations and their effects.
Reference: [56] <author> P.K. Garg, M. Jazayeri, </author> <title> and M.L. Creech. A Meta-Process for Software Reuse, Process Discovery, and Evolution. </title> <booktitle> In Proceedings of the 6th International Workshop on Software Reuse, </booktitle> <month> November </month> <year> 1993. </year>
Reference-contexts: This work centers on using a rule base and goals to derive a generalized execution flow from a specific process history. * Garg et al. <ref> [56] </ref> employ a manual process history analysis in the context of a meta-process for creating and validating domain-specific process models and software toolkits. * Bradac et al. [23] describe the beginnings of a process monitoring experiment in which their goal is to model the process as a queuing network, and use
Reference: [57] <author> P.K. Garg, M. Jazayeri, </author> <title> and M.L. Creech. A meta-process for software reuse, process discovery and evolution. </title> <booktitle> In Proceedings of the 6th International Workshop on Software Reuse, </booktitle> <pages> page [need pages], </pages> <month> November </month> <year> 1993. </year>
Reference-contexts: This work centers on using a rule base and goals to derive a generalized execution flow from a specific process history. By having enough rules, they showed that a complete and correct process fragment could be generated from execution data. 2. Garg et al. <ref> [57] </ref> employ process history analysis, mostly human-centered data validation and analysis, in the context of a meta-process for creating and validating domain specific process and software kits.
Reference: [58] <author> C. Gerety. </author> <title> HP SoftBench: A New Generation of Software Development Tools. </title> <type> Technical Report SESD-89-25, </type> <institution> Hewlett-Packard Software Engineering Systems Division, Fort Collins, Colorado, </institution> <month> November </month> <year> 1989. </year>
Reference-contexts: Process integration components, such as ProcessWall [66], are likewise a natural focal point for such instrumentation. Many software engineering environments, even those not process-based, can supply event data relatively easily; message-based systems, such as Field [100], Softbench <ref> [58] </ref>, and ToolTalk [112], provide a natural framework from which to collect data about system-based activities. Of the existing tools that developers already use, many naturally provide some event data collection as well.
Reference: [59] <author> E.M. Gold. </author> <title> Language identification in the limit. </title> <journal> Information and Control, </journal> <volume> 10 </volume> <pages> 447-474, </pages> <year> 1967. </year>
Reference-contexts: Now, the grammar inference problem can be phrased as, given some presentation of all or part of fi L , can one infer a good grammar G describing L? 3.3.2 Complexity Results Gold's "identification in the limit" was the first framework for analyzing the grammar inference problem <ref> [59] </ref>. Identifying in the limit frames the problem in terms of looking at the complete presentation of fi L .
Reference: [60] <author> E.M. Gold. </author> <title> Complexity of automatic identification from given data. </title> <journal> Information and Control, </journal> <volume> 37 </volume> <pages> 302-320, </pages> <year> 1978. </year>
Reference-contexts: If, after some finite presentation of pairs from fi L , the algorithm guesses G correctly, and does not change its guess as the presentation continues, then the algorithm is said to identify G in the limit. Gold showed <ref> [60] </ref> that finding a DFA with a minimum number of states with a presentation of fi L is NP-hard, and also that it is impossible with just a positive presentation ( L ).
Reference: [61] <author> R.M. Greenwood. </author> <title> Using CSP and System Dynamics as Process Engineering Tools. </title> <booktitle> In Proceedings of the Second European Workshop on Software Process Technology, number 635 in Lecture Notes in Computer Science, </booktitle> <pages> pages 138-145. </pages> <publisher> Springer-Verlag, </publisher> <month> September </month> <year> 1992. </year>
Reference-contexts: In response, new methods and tools for supporting various aspects of the software process have been devised. Many of the technologies, including process automation [14, 40, 96, 116], process analysis <ref> [61, 63, 80, 103] </ref>, and process evolution [11, 76], assume the existence of some sort of formal model of a process in order for those technologies to be applied.
Reference: [62] <author> J. Grudin. </author> <title> Groupware and Cooperative Work: Problems and Prospects. In R.M. </title> <editor> Baeker, editor, </editor> <booktitle> Groupware and Computer-Supported Cooperative Work, </booktitle> <pages> pages 97-105. </pages> <publisher> Morgan Kaufmann, </publisher> <year> 1993. </year>
Reference-contexts: Their hope is that this not only reduces the work that humans do, but also lessens the chance for human error in the process. Research in workflow has shown the need to allow exceptions to the process <ref> [62] </ref>, and it is here that many process execution environments are the weakest, in that they require strict adherence to the process model. <p> That being the case, there is no effective way to enforce the process using this approach nor to guarantee the mutual consistency of a process model and a process execution. In addition, the workflow community has long recognized the need to allow exceptions to the prescribed process <ref> [62] </ref>; so, in reality, there is a strong need to allow for deviations from the model. Even if one could completely enforce a process, there still remains the issue of managing change in a process, which might lead to a discrepancy between the model and the execution.
Reference: [63] <author> V. Gruhn and R. Jegelka. </author> <title> An Evaluation of FUNSOFT Nets. </title> <booktitle> In Proceedings of the Second European Workshop on Software Process Technology, number 635 in Lecture Notes in Computer Science, </booktitle> <pages> pages 196-214. </pages> <publisher> Springer-Verlag, </publisher> <month> September </month> <year> 1992. </year>
Reference-contexts: In response, new methods and tools for supporting various aspects of the software process have been devised. Many of the technologies, including process automation [14, 40, 96, 116], process analysis <ref> [61, 63, 80, 103] </ref>, and process evolution [11, 76], assume the existence of some sort of formal model of a process in order for those technologies to be applied. <p> Several formal models suitable for our analyses have been used to describe software processes. These include models based on state machines (e.g., Statemate [65]), Petri nets (e.g., Slang [13] and FUNSOFT Nets <ref> [63] </ref>), and procedural languages (e.g., APPL/A [114]). 4.2.1 Recognition Metric The first metric is a very straightforward one that has just two values, true or false. The value is true if the event streams exactly match and is false otherwise.
Reference: [64] <author> D. Harel. Statecharts: </author> <title> A Visual Formalism for Complex Systems. </title> <booktitle> Science of Computer Programming, </booktitle> <volume> 8 </volume> <pages> 231-274, </pages> <year> 1987. </year>
Reference-contexts: We feel it is applicable to software process as well. 2.2 Relating Models and Events: Event Sites For our work in this thesis, we focus on behavior formalisms in process modeling languages. Examples are the Petri net foundation in Slang [12], state machines, and Statecharts <ref> [64] </ref>. In viewing a process execution as (part of) an event stream, we are assuming that a process model has some way of producing an event stream if it was "executed".
Reference: [65] <author> D. Harel, H. Lachover, A. Naamad, A. Pnueli, M. Politi, R. Sherman, and A. Shtul-Trauring. STATE-MATE: </author> <title> A Working Environment for the Development of Complex Reactive Systems. </title> <booktitle> In Proceedings of the 10th International Conference on Software Engineering, </booktitle> <pages> pages 396-406. </pages> <publisher> IEEE Computer Society, </publisher> <month> April </month> <year> 1988. </year>
Reference-contexts: Thus, possible paths through the specification|that is, possible behaviors specified by the model|can be represented by event streams produced by a simulation. Several formal models suitable for our analyses have been used to describe software processes. These include models based on state machines (e.g., Statemate <ref> [65] </ref>), Petri nets (e.g., Slang [13] and FUNSOFT Nets [63]), and procedural languages (e.g., APPL/A [114]). 4.2.1 Recognition Metric The first metric is a very straightforward one that has just two values, true or false. The value is true if the event streams exactly match and is false otherwise.
Reference: [66] <author> D. Heimbigner. </author> <title> The ProcessWall: A Process State Server Approach to Process Programming. </title> <booktitle> In SIGSOFT '92: Proceedings of the Fifth Symposium on Software Development Environments, </booktitle> <pages> pages 159-168. </pages> <booktitle> ACM SIGSOFT, </booktitle> <month> December </month> <year> 1992. </year>
Reference-contexts: Lacking a process model, their infrastructure could be used to simply collect the event data. Most process enactment systems such as Oz [20] and SPADE [12] could easily be instrumented to collect event data. Process integration components, such as ProcessWall <ref> [66] </ref>, are likewise a natural focal point for such instrumentation. Many software engineering environments, even those not process-based, can supply event data relatively easily; message-based systems, such as Field [100], Softbench [58], and ToolTalk [112], provide a natural framework from which to collect data about system-based activities.
Reference: [67] <author> K.E. Huff. </author> <title> Software process modelling. In Software Process, </title> <booktitle> Trends in Software, </booktitle> <pages> pages 1-24. </pages> <publisher> Wiley, </publisher> <year> 1996. </year>
Reference-contexts: Paradigms that have been used in formal process modeling include programming [115], graph-based control such as state machines [80] and Petri nets [11, 12, 40], rule-based [14, 96], and planning [68]. Huff <ref> [67] </ref> gives a good overview of modeling and languages.
Reference: [68] <author> K.E. Huff and V.R. Lesser. </author> <title> A plan-based intelligent assistant that supports the software development process. </title> <booktitle> In Proc. 3rd ACM SIGSOFT/SIGPLAN Symposium on Practical Software Development Environments, </booktitle> <pages> pages 97-106. </pages> <publisher> ACM Press, </publisher> <month> February </month> <year> 1989. </year>
Reference-contexts: Paradigms that have been used in formal process modeling include programming [115], graph-based control such as state machines [80] and Petri nets [11, 12, 40], rule-based [14, 96], and planning <ref> [68] </ref>. Huff [67] gives a good overview of modeling and languages.
Reference: [69] <author> W.S. Humphrey. </author> <title> Managing the Software Process. </title> <publisher> Addison-Wesley, </publisher> <address> Reading, Massachusetts, </address> <year> 1989. </year>
Reference-contexts: The most notable of these efforts is the SEI's Capability Maturity Model (CMM) <ref> [69, 94, 95] </ref>, but other work, not necessarily specific to software, includes the ISO 9001 (9000-3) certification [74], and the Baldridge Quality Award. A European initiative centered around augmenting ISO 9001 is the TickIt process [117].
Reference: [70] <author> W.S. Humphrey. </author> <title> A Discipline for Software Engineering. SEI Series in Software Engineering. </title> <publisher> Addison-Wesley, </publisher> <year> 1995. </year>
Reference-contexts: Most recently a new effort has been Humphrey's Personal Software Process (PSP) <ref> [70] </ref>, which is directed at improving the software engineering process at the level of the individual.
Reference: [71] <author> A. Hutchinson. </author> <title> Algorithmic Learning. Graduate Texts in Computer Science. </title> <publisher> Oxford University Press, </publisher> <year> 1994. </year>
Reference-contexts: This approach implies some kind of state-space search method, and implies using a heuristic-driven method to control the state explosion. AI research has provided several search methods that seem applicable; two that we describe here are best first search and A fl search <ref> [71, 101] </ref>. While the standard depth first and breadth first searches of a tree of states are exhaustive in a single dimension, best first search is a heuristic-driven search that determines its search path by following the lowest cost path in the state space.
Reference: [72] <institution> IBM Systems Journal, volume 33:1. IBM Corporation, </institution> <year> 1994. </year>
Reference-contexts: It does not specify what steps a good process contains. The Baldridge award is more focussed on quality control methods. Improvement efforts have been well-received throughout the industry, as evidenced by AT&T [9], IBM <ref> [72] </ref>, and DEC [43] all having a special issue of their technical journals on process improvement.
Reference: [73] <institution> IEEE Software, </institution> <address> volume 11:4. </address> <publisher> IEEE Press, </publisher> <month> July </month> <year> 1994. </year>
Reference-contexts: This work used an experimental setting to determine the actual effects of code inspections on the quality of the product. Another example shows how process exceptions might correlate to defects in the product [119]. A special issue of IEEE Software <ref> [73] </ref> had a theme of measurement-based process improvement; most of the articles dealt with questionnaire-based improvement, with two articles reporting on statistical and experimental improvement.
Reference: [74] <institution> ISO 9000-3 guidelines for the application of ISO 9001 to the development, supply, and maintenance of software. </institution> <type> Technical report, </type> <institution> International Standards Organization, </institution> <year> 1991. </year>
Reference-contexts: The most notable of these efforts is the SEI's Capability Maturity Model (CMM) [69, 94, 95], but other work, not necessarily specific to software, includes the ISO 9001 (9000-3) certification <ref> [74] </ref>, and the Baldridge Quality Award. A European initiative centered around augmenting ISO 9001 is the TickIt process [117]. The CMM rates the goodness of a process by providing a checklist of activities that processes should include, and then provides a path of maturing the process by adding these activities.
Reference: [75] <author> P. Inverardi, B. Krishnamurthy, and D. Yankelevich. Yeast: </author> <title> A Case Study for a Practical use of Formal Methods. </title> <booktitle> In TAPSOFT '93: Proceedings of the 4th International Joint Conference CAAP/FASE, number 668 in Lecture Notes in Computer Science, </booktitle> <pages> pages 105-120. </pages> <publisher> Springer-Verlag, </publisher> <month> April </month> <year> 1993. </year>
Reference-contexts: The group decides 15 to use Balboa to help them with this examination, and appoints a process engineer to coordinate and perform the examination. The first step the process engineer makes is to instrument their site for data collection. The engineer uses Yeast <ref> [75] </ref> to watch a directory structure for file modifications, and output these as events. The engineer wraps the basic commands in shell scripts to first record the command as an event and then execute the command.
Reference: [76] <author> M.L. Jaccheri and R. Conradi. </author> <title> Techniques for Process Model Evolution in EPOS. </title> <journal> IEEE Transactions on Software Engineering, </journal> 19(12) 1145-1156, December 1993. 
Reference-contexts: In response, new methods and tools for supporting various aspects of the software process have been devised. Many of the technologies, including process automation [14, 40, 96, 116], process analysis [61, 63, 80, 103], and process evolution <ref> [11, 76] </ref>, assume the existence of some sort of formal model of a process in order for those technologies to be applied. The need to develop a formal model as a prerequisite to using a new technology is a daunting prospect to the managers of large, on-going projects. <p> Even if one could completely enforce a process, there still remains the issue of managing change in a process, which might lead to a discrepancy between the model and the execution. There has, in fact, been considerable recent work that addresses process evolution <ref> [11, 76] </ref>. Commensurate with the historical approach mentioned above, that work is concerned more with the problem of effecting changes to a process model used for automation, than it is with the problem of uncovering inconsistencies between the model and the execution.
Reference: [77] <author> S. Jain and A. Sharma. </author> <title> Algorithmic Learning Theory, </title> <booktitle> volume 872 of Lecture Notes in Artificial Intelligence (subseries of LNCS), </booktitle> <pages> pages 349-364. </pages> <publisher> Springer-Verlag, </publisher> <address> New York, </address> <year> 1994. </year>
Reference-contexts: The survey by Angluin and Smith [7] is a broad look at inductive inference learning. Pitt's survey [97] is a very good starting point for understanding the theoretical complexities involved in learning regular grammars. Further theoretical work on language learning is found in <ref> [4, 77, 86, 87, 104] </ref>. Angluin and Smith, in their survey, acknowledge the gap between the theoretical results that have been achieved and the practical application of inference methods.
Reference: [78] <author> C.M. Judd, E.R. Smith, </author> <title> and L.H. Kidder. Research Methods in Social Relations. </title> <publisher> Holt, Rinehart and Winston, Inc., </publisher> <address> Fort Worth, sixth edition, </address> <year> 1991. </year>
Reference-contexts: Here we discuss threats to the construct, internal, and external validity of our results. We use the definitions of validity given by Judd, Smith, and Kidder <ref> [78] </ref>. Construct validity is concerned with how well the metrics used in the study faithfully and successfully reflect real-world attributes and values.
Reference: [79] <author> R.L. Kashyap and B.J. Oommen. </author> <title> The noisy substring matching problem. </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> 9(3) </volume> <pages> 365-370, </pages> <year> 1983. </year>
Reference-contexts: Previous work in other areas using string distance measures have also found the need for per-symbol weights <ref> [8, 79] </ref>. 4.4.2 Usability Enhancements The simple and non-linear string distance metrics are naturally decomposable in a hierarchical fashion, to be able to give more information about the measurement than just one number for the whole process model.
Reference: [80] <author> M.I. Kellner. </author> <title> Software Process Modeling Support for Management Planning and Control. </title> <booktitle> In Proceedings of the First International Conference on the Software Process, </booktitle> <pages> pages 8-28. </pages> <publisher> IEEE Computer Society, </publisher> <month> October </month> <year> 1991. </year>
Reference-contexts: Paradigms that have been used in formal process modeling include programming [115], graph-based control such as state machines <ref> [80] </ref> and Petri nets [11, 12, 40], rule-based [14, 96], and planning [68]. Huff [67] gives a good overview of modeling and languages. <p> In response, new methods and tools for supporting various aspects of the software process have been devised. Many of the technologies, including process automation [14, 40, 96, 116], process analysis <ref> [61, 63, 80, 103] </ref>, and process evolution [11, 76], assume the existence of some sort of formal model of a process in order for those technologies to be applied. <p> The example in this section is taken from the ISPW 6/7 process problem [81]. We describe the process using an FSM that is based on Kellner's Statemate solution <ref> [80] </ref>. Our version is shown in Figure 3.8. <p> Their work also focuses on using product data, such as code modifications and change classification. This work has become known as the GQM , or goal-question-metric, paradigm. * Kellner <ref> [80] </ref> shows the usefulness of simulation and "what-if " analyses in forecasting the schedule and outcome of a specific execution of a process. He uses deterministic and stochastic modeling, along with resource constraints, to derive schedule, work effort, and staffing estimations.
Reference: [81] <author> M.I. Kellner, P.H. Feiler, A. Finkelstein, T. Katayama, L.J. Osterweil, M.H. Penedo, and H.D. Rom-bach. </author> <title> Software Process Modeling Example Problem. </title> <booktitle> In Proceedings of the 6th International Software Process Workshop, </booktitle> <pages> pages 19-29, </pages> <month> October </month> <year> 1990. </year>
Reference-contexts: The example in this section is taken from the ISPW 6/7 process problem <ref> [81] </ref>. We describe the process using an FSM that is based on Kellner's Statemate solution [80]. Our version is shown in Figure 3.8. <p> For the moderate cutoff value, we would use 0:5 in place of 0:2. 4.3 Example Use of the Metrics To illustrate the various metrics introduced above, we use the Test Unit task from the ISPW 6/7 process problem <ref> [81] </ref>.
Reference: [82] <author> J.R. Knight and E.W. Myers. </author> <title> Approximate regular expression pattern matching with concave gap penalties. </title> <journal> Algorithmica, </journal> <volume> 14 </volume> <pages> 85-121, </pages> <year> 1995. </year>
Reference-contexts: The problem is, while they leverage system transformations to gain speed and scalability, these transformations make the system inherently uninspectable and their analysis methods only produce binary results. 69 Regular Expression Matching In <ref> [82, 93] </ref>, Myers, Miller, and Knight describe algorithms for approximately matching a string to a regular expression, using the insert, delete, and substitute operations. These methods build on the dynamic programming techniques of the string to string comparison algorithms, and extend this to regular expressions. <p> In general, constructs used in process modeling languages are not reducible to regular expressions. More powerful, yet still restricted, constructs have been looked at. Context free languages, for example, are thought to have high-order polynomial time algorithms for solving approximate matching <ref> [82] </ref>. In general, these super-quadratic to cubic solutions, while providing optimal answers, are impractical unless one requires an optimal solution.
Reference: [83] <author> E. </author> <title> Koutsofios and S.C. North. Drawing Graphs with Dot. </title> <institution> AT&T Bell Laboratories, </institution> <month> October </month> <year> 1993. </year>
Reference-contexts: What is important is that the methods produce an FSM model that reflects the structure inherent in the sample|that is, the A-B-C and B-A-C loops. In this chapter, all of the graphical representations presented have been automatically generated by the dot directed-graph drawing tool <ref> [83] </ref> from the output of the discovery tools. 28 Input Layer Hidden Layer Output Layer (a) (b) In Section 3.5, we demonstrate the methods on a more complex example, exposing the relative performance and possible strengths and weaknesses of each method. 3.4.1 RNet The first method we describe comes from the <p> Table 3.2 shows the size (LOC) of the various discovery tools. The discovery methods output their discovered FSM in a format compatible with the dot graph layout program <ref> [83] </ref>. This leveraging of an existing graph layout tool is central to the successful application of the discovery tools. It provides an end-to-end solution, going from event data directly to a visual display of the discovered process model. Discovery, seen in Figure 3.12, is the process discovery tool of Balboa.
Reference: [84] <author> B. Krishnamurthy and D.S. Rosenblum. Yeast: </author> <title> A General Purpose Event-Action System. </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> 21(10) </volume> <pages> 845-857, </pages> <month> October </month> <year> 1995. </year>
Reference-contexts: This work could help ease the burden of collecting off-computer, manual events. 13 * Krishnamurthy and Rosenblum <ref> [84] </ref> built a system event monitor, Yeast, that can record events that occur on computer, and can react to those events.
Reference: [85] <author> J.B. Kruskal. </author> <title> An Overview of Sequence Comparison. </title> <editor> In D. Sankoff and J.B. Kruskal, editors, </editor> <title> Time Warps, String Edits, and Macromolecules: </title> <booktitle> The Theory and Practice of Sequence Comparison, </booktitle> <pages> pages 1-44. </pages> <publisher> Addison-Wesley, </publisher> <address> Reading, Massachusetts, </address> <year> 1983. </year> <month> 137 </month>
Reference-contexts: This solution paradigm is shown in Figure 4.2. In describing this paradigm, we will put off discussing how to generate the model event stream until Section 4.5. There are several methods for doing a measurement such as this, but one that seems most widely applied is string distance metrics <ref> [85] </ref>. The string distance method counts the number of token (symbols that make up the string) swaps, insertions, and deletions needed to transform one string into the other. By applying various mathematical transformations, this method becomes a family of metrics. <p> These methods have been used in fields as various as DNA/RNA matching ([120]), substring matching ([79, 108]), spelling correction ([45]), syntax error correction ([2, 52, 102]), and even the well-known Unix diff program. A good reference to the general area of sequence comparison is <ref> [85] </ref>. Other methods of doing this measurement do not offer the versatility that the string distance metrics do. <p> We can then apply a well-known method for calculating the distance between strings <ref> [85] </ref> and use distance as the metric of difference between the process model and process execution. String distance metrics have been used profitably as measures of correspondence in a wide variety of other domains, including parsing, DNA/RNA sequencing, and text recognition [107]. <p> Given two strings, one of length n and the other of length m, the minimal total cost of operations can be computed in O (nm) time using a well-known dynamic program <ref> [85] </ref>. In some applications of this method, such as DNA/RNA sequencing or text recognition, token substitution in the string distance metric makes sense. For process validation, however, it is not clear that a substituted event should contribute in any way to the goodness of the correspondence.
Reference: [86] <author> S. Lange, J. Nessel, and R. Wiehagen. </author> <title> Algorithmic Learning Theory, </title> <booktitle> volume 872 of Lecture Notes in Artificial Intelligence (subseries of LNCS), </booktitle> <pages> pages 423-437. </pages> <publisher> Springer-Verlag, </publisher> <address> New York, </address> <year> 1994. </year>
Reference-contexts: The survey by Angluin and Smith [7] is a broad look at inductive inference learning. Pitt's survey [97] is a very good starting point for understanding the theoretical complexities involved in learning regular grammars. Further theoretical work on language learning is found in <ref> [4, 77, 86, 87, 104] </ref>. Angluin and Smith, in their survey, acknowledge the gap between the theoretical results that have been achieved and the practical application of inference methods.
Reference: [87] <author> S. Lange and P. Watson. </author> <title> Algorithmic Learning Theory, </title> <booktitle> volume 872 of Lecture Notes in Artificial Intelligence (subseries of LNCS), </booktitle> <pages> pages 438-452. </pages> <publisher> Springer-Verlag, </publisher> <address> New York, </address> <year> 1994. </year>
Reference-contexts: The survey by Angluin and Smith [7] is a broad look at inductive inference learning. Pitt's survey [97] is a very good starting point for understanding the theoretical complexities involved in learning regular grammars. Further theoretical work on language learning is found in <ref> [4, 77, 86, 87, 104] </ref>. Angluin and Smith, in their survey, acknowledge the gap between the theoretical results that have been achieved and the practical application of inference methods.
Reference: [88] <author> R.J. LeBlanc and A.D. Robbins. </author> <title> Event-Driven Monitoring of Distributed Programs. </title> <booktitle> In Proceedings of the Fifth International Conference on Distributed Computing Systems, </booktitle> <pages> pages 515-522. </pages> <publisher> IEEE Computer Society, </publisher> <month> May </month> <year> 1985. </year>
Reference-contexts: Using event data to characterize behavior is widely accepted in other areas of software engineering, such as program visualization <ref> [88] </ref>, concurrent-system analysis [10], and distributed debugging [19, 38]. We feel it is applicable to software process as well. 2.2 Relating Models and Events: Event Sites For our work in this thesis, we focus on behavior formalisms in process modeling languages.
Reference: [89] <editor> C.M. Lott. </editor> <booktitle> Process and measurement support in SEEs. SIGSOFT Software Engineering Notes, </booktitle> <volume> 18(4) </volume> <pages> 83-93, </pages> <month> October </month> <year> 1993. </year>
Reference-contexts: Sections 5.3, 5.4, and 5.5 describe the data management, data collection, and client tool interfaces to Balboa, respectively. Finally, Section 5.6 concludes with some insights and ideas for future work on Balboa. 85 5.1 Motivation In <ref> [89] </ref>, Lott gives an extensive summary of process support and measurement support in seventeen software engineering environments.
Reference: [90] <author> S.Y. Lu and K.S. Fu. </author> <title> Error-correcting tree automata for syntactic pattern recognition. </title> <journal> IEEE Transactions on Computers, </journal> <volume> C-27:1040-1053, </volume> <month> November </month> <year> 1978. </year>
Reference-contexts: 2 Event 3 Event 76 Event 77 Event 78 Event Data Process Model Generation Event 1 Event 2 Event 3 Event 83 Event 84 Event 85 Event Data Comparison Measurements where symbolic sequence comparison is taking place. 1 String distance methods are also generalizable to tree and graph distance measurements <ref> [90] </ref>. Methods that can compare a model directly with some event stream are also available, but our abstraction away from this is more generalizable.
Reference: [91] <author> L. Miclet. </author> <title> Syntactic and Structural Pattern Recognition: Theory and Applications, </title> <booktitle> volume 7 of Series in Computer Science, chapter 9: Grammatical Inference, </booktitle> <pages> pages 237-290. </pages> <publisher> World Scientific, </publisher> <address> New Jersey, </address> <year> 1990. </year>
Reference-contexts: They have extended this work to handle noisy strings by treating the noise as an edit distance problem, although currently they only allow for a single-token edit, not multi-token gaps. Their work is geared towards analyzing biosequence (DNA/RNA, protein) data. * Miclet <ref> [91, Section 3.3.3] </ref> describes a method he calls the uv k w algorithm, where each pass looks for repetitions of substrings (the v k ), chooses the best candidate, and replaces with a *-expression; then the next pass is started on the reduced string. <p> This method is well-adapted for loops, but has problems with the or operator as well. Miclet's survey <ref> [91] </ref> describes several other techniques for inferring (N)DFAs from positive samples, and is, in general, a good reference for practical inference techniques. 3.3.4 Statistical Model Inference Techniques Statistical models are models of languages that incorporate some notion of the likelihood of a sequence or subsequence occurring in the language. <p> In practice, this approach has been too time-consuming to be practical, and also requires an assumption on the number of states in the model <ref> [91] </ref>. The next level above HMMs is Stochastic Context-Free Grammars (SCFG). An SCFG is just a CFG where each production rule has some associated probability of firing. As with HMMs, inferring an SCFG usually means inferring the probabilities on the given production rules.
Reference: [92] <author> L. Miclet and J. Quinqueton. </author> <title> Syntactic and Structural Pattern Recognition, </title> <booktitle> volume 45 of NATO ASI Series F: Computer and Systems Sciences, </booktitle> <pages> pages 153-171. </pages> <publisher> Springer-Verlag, </publisher> <address> New York, </address> <year> 1988. </year>
Reference-contexts: This method uses the concept of Markov models to find the most probable event sequence productions, and algorithmically converts those probabilities into states and state transitions. Although our method is new, there is previous work that has used similar methods. For example, Miclet and Quinqueton <ref> [92] </ref> use sequence probabilities to create FSM recognizers of protein sequences, and then use the Markov models to predict the center point of new protein sequences.
Reference: [93] <author> E.W. Myers and W. Miller. </author> <title> Approximate matching of regular expressions. </title> <journal> Bulletin of Mathematical Biology, </journal> <volume> 51(1) </volume> <pages> 5-37, </pages> <year> 1989. </year>
Reference-contexts: The problem is, while they leverage system transformations to gain speed and scalability, these transformations make the system inherently uninspectable and their analysis methods only produce binary results. 69 Regular Expression Matching In <ref> [82, 93] </ref>, Myers, Miller, and Knight describe algorithms for approximately matching a string to a regular expression, using the insert, delete, and substitute operations. These methods build on the dynamic programming techniques of the string to string comparison algorithms, and extend this to regular expressions. <p> However, dealing with multi-symbol blocks (or gaps), as our NSD metric requires, complicates matters significantly. In general, for both string to string comparisons [48] and string to regular expression comparisons <ref> [93] </ref>, arbitrary cost functions for blocks require at least O (M N max (M; N )), or cubic time.
Reference: [94] <author> M. Paulk, B. Curtis, M. Chrissis, and C. Weber. </author> <title> Capability maturity model for software, version 1.1. </title> <type> Technical Report CMU/SEI-93-TR-24, </type> <institution> Software Engineering Institute, </institution> <month> February </month> <year> 1993. </year>
Reference-contexts: The most notable of these efforts is the SEI's Capability Maturity Model (CMM) <ref> [69, 94, 95] </ref>, but other work, not necessarily specific to software, includes the ISO 9001 (9000-3) certification [74], and the Baldridge Quality Award. A European initiative centered around augmenting ISO 9001 is the TickIt process [117].
Reference: [95] <author> M. Paulk, B. Curtis, M. Chrissis, and C. Weber. </author> <title> Capability Maturity Model, Version 1.1. </title> <journal> IEEE Software, </journal> <volume> 10(4) </volume> <pages> 18-27, </pages> <month> July </month> <year> 1993. </year>
Reference-contexts: The most notable of these efforts is the SEI's Capability Maturity Model (CMM) <ref> [69, 94, 95] </ref>, but other work, not necessarily specific to software, includes the ISO 9001 (9000-3) certification [74], and the Baldridge Quality Award. A European initiative centered around augmenting ISO 9001 is the TickIt process [117].
Reference: [96] <author> B. Peuschel and W. Schafer. </author> <title> Concepts and Implementation of a Rule-based Process Engine. </title> <booktitle> In Proceedings of the 14th International Conference on Software Engineering, </booktitle> <pages> pages 262-279. </pages> <publisher> IEEE Computer Society, </publisher> <month> May </month> <year> 1992. </year>
Reference-contexts: Paradigms that have been used in formal process modeling include programming [115], graph-based control such as state machines [80] and Petri nets [11, 12, 40], rule-based <ref> [14, 96] </ref>, and planning [68]. Huff [67] gives a good overview of modeling and languages. <p> In response, new methods and tools for supporting various aspects of the software process have been devised. Many of the technologies, including process automation <ref> [14, 40, 96, 116] </ref>, process analysis [61, 63, 80, 103], and process evolution [11, 76], assume the existence of some sort of formal model of a process in order for those technologies to be applied.
Reference: [97] <author> L. Pitt. </author> <title> Analogical and Inductive Inference, </title> <booktitle> volume 397 of Lecture Notes in Artificial Intelligence (subseries of LNCS), </booktitle> <pages> pages 18-44. </pages> <publisher> Springer-Verlag, </publisher> <address> New York, </address> <year> 1989. </year>
Reference-contexts: ae fl , that is, L is a subset of sentences from fl . 2 A grammar G is a formalism of some class (e.g., regular, context-free) that describes L; L is said to be in that class of languages. 1 This presentation is consistent with others in the field <ref> [7, 97, 105] </ref>. 2 L = fl is not a very interesting language. 22 We define fi L as the infinite set of all pairs hs; li from fl fi f0; 1g, where l = 1 if s 2 L and l = 0 otherwise. <p> Classes of boolean formulas and decision trees, and some geometric and algebraic concepts have been shown to be PAC-learnable. Results for grammars, including DFAs, are more negative, but less definite. As Pitt <ref> [97] </ref> reports, If DFAs are polynomially approximately predictable, then there is a probabilistic polynomial time algorithm for inverting the RSA encryption function, for factoring Blum integers, and for deciding quadratic residues. 3 Actually, since some of the sequences can be infinite themselves, any theoretical analysis assumes only a presen tation of <p> There is much work describing the computational complexities of various learning paradigms and classes of languages or formulas. The survey by Angluin and Smith [7] is a broad look at inductive inference learning. Pitt's survey <ref> [97] </ref> is a very good starting point for understanding the theoretical complexities involved in learning regular grammars. Further theoretical work on language learning is found in [4, 77, 86, 87, 104].
Reference: [98] <author> A. Porter, H. Siy, C.A. Toman, and L.G. Votta. </author> <title> An experiment to assess the cost-benefits of code inspections in large scale software development. </title> <booktitle> In Third ACM SIGSOFT Symposium on the Foundations of Software Engineering, </booktitle> <pages> pages 92-103. </pages> <publisher> ACM Press, </publisher> <month> October </month> <year> 1995. </year>
Reference-contexts: A body of work has looked at experimental measurement of specific process techniques. One example of this is the detailed study of code inspections by Votta, Porter, and Siy <ref> [98] </ref>. This work used an experimental setting to determine the actual effects of code inspections on the quality of the product. Another example shows how process exceptions might correlate to defects in the product [119]. <p> probabilities) of the queuing network that can then be analyzed. * Wolf and Rosenblum [121] demonstrate how to collect event-based process data and use basic statistical and visual techniques to find interesting relationships among the data in order to uncover possible areas of process improvement. 57 * Porter et al. <ref> [98] </ref> use a well constructed experimental setting to assess the benefits of code inspections, in terms of group size and meeting gains.
Reference: [99] <author> R.S. Pressman. </author> <title> Software Engineering: A Practitioner's Approach. </title> <publisher> McGraw-Hill, </publisher> <address> Reading, MA, third edition, </address> <year> 1992. </year>
Reference-contexts: Canonical models tend not to be very specific and only attempt to give a framework for more specific activities. Representative examples include the venerable waterfall model, prototyping, the risk-based spiral model, and the cleanroom process model <ref> [99] </ref>. Some canonical models involve only specific sub-processes such as analysis, design, and testing. Code inspections and design reviews are perhaps the most studied and formalized of sub-processes [49]. Other work in this vein includes SA/SD, Booch Object-Oriented Design, and regression testing [99]. 1.2.2 Execution Execution research is involved in creating <p> risk-based spiral model, and the cleanroom process model <ref> [99] </ref>. Some canonical models involve only specific sub-processes such as analysis, design, and testing. Code inspections and design reviews are perhaps the most studied and formalized of sub-processes [49]. Other work in this vein includes SA/SD, Booch Object-Oriented Design, and regression testing [99]. 1.2.2 Execution Execution research is involved in creating process execution environments in which a formal, executable process model drives the activities that take place in the environment. Thus, this area is an application and continuation of the modeling efforts.
Reference: [100] <author> S.P. Reiss. </author> <title> Connecting Tools Using Message Passing in the Field Environment. </title> <journal> IEEE Software, </journal> <pages> pages 57-66, </pages> <month> July </month> <year> 1990. </year>
Reference-contexts: Process integration components, such as ProcessWall [66], are likewise a natural focal point for such instrumentation. Many software engineering environments, even those not process-based, can supply event data relatively easily; message-based systems, such as Field <ref> [100] </ref>, Softbench [58], and ToolTalk [112], provide a natural framework from which to collect data about system-based activities. Of the existing tools that developers already use, many naturally provide some event data collection as well.
Reference: [101] <editor> E. Rich. </editor> <booktitle> Artificial Intelligence. McGraw-Hill Series in Artificial Intelligence. </booktitle> <publisher> McGraw-Hill, </publisher> <year> 1983. </year>
Reference-contexts: This approach implies some kind of state-space search method, and implies using a heuristic-driven method to control the state explosion. AI research has provided several search methods that seem applicable; two that we describe here are best first search and A fl search <ref> [71, 101] </ref>. While the standard depth first and breadth first searches of a tree of states are exhaustive in a single dimension, best first search is a heuristic-driven search that determines its search path by following the lowest cost path in the state space. <p> By pruning, one cannot guarantee a lowest cost goal, but smart pruning, in some domains (such as game playing), has shown that it has negligible effects on the outcome of the search while dramatically reducing search costs <ref> [101] </ref>. Pruning can take many forms, and can use vastly different methods and heuristics. One pruning method is to throw away any newly generated state that has an estimated cost higher than some threshold relative to the current best-looking state.
Reference: [102] <author> J. Rohrich. </author> <title> Methods for the automatic construction of error correcting parsers. </title> <journal> Acta Informatica, </journal> <volume> 13 </volume> <pages> 115-139, </pages> <year> 1980. </year>
Reference-contexts: They augment the language grammar with error productions, and modify the parse algorithm to do corrections. They do not expect that their algorithm to be used, because of its high cost; they only propose it as a baseline for other methods to compare against. * Rohrich <ref> [102] </ref> describes an error correction method biased towards insertion of symbols, arguing that as little of the program text should be skipped (deleted) as possible.
Reference: [103] <author> M. Saeki, T. Kaneko, and M. Sakamoto. </author> <title> A Method for Software Process Modeling and Description Using LOTOS. </title> <booktitle> In Proceedings of the First International Conference on the Software Process, </booktitle> <pages> pages 90-104. </pages> <publisher> IEEE Computer Society, </publisher> <month> October </month> <year> 1991. </year>
Reference-contexts: In response, new methods and tools for supporting various aspects of the software process have been devised. Many of the technologies, including process automation [14, 40, 96, 116], process analysis <ref> [61, 63, 80, 103] </ref>, and process evolution [11, 76], assume the existence of some sort of formal model of a process in order for those technologies to be applied.
Reference: [104] <author> Y. Sakakibara. </author> <title> Efficient learning of context-free grammars from positive structural examples. </title> <journal> Information and Computation, </journal> <volume> 97 </volume> <pages> 23-60, </pages> <year> 1992. </year>
Reference-contexts: The survey by Angluin and Smith [7] is a broad look at inductive inference learning. Pitt's survey [97] is a very good starting point for understanding the theoretical complexities involved in learning regular grammars. Further theoretical work on language learning is found in <ref> [4, 77, 86, 87, 104] </ref>. Angluin and Smith, in their survey, acknowledge the gap between the theoretical results that have been achieved and the practical application of inference methods.
Reference: [105] <author> Y. Sakakibara. </author> <title> Grammatical inference: An old and new paradigm. </title> <type> Technical Report ISIS-RR-95-9E, </type> <institution> Institute for Social Information Science, </institution> <month> September </month> <year> 1995. </year>
Reference-contexts: ae fl , that is, L is a subset of sentences from fl . 2 A grammar G is a formalism of some class (e.g., regular, context-free) that describes L; L is said to be in that class of languages. 1 This presentation is consistent with others in the field <ref> [7, 97, 105] </ref>. 2 L = fl is not a very interesting language. 22 We define fi L as the infinite set of all pairs hs; li from fl fi f0; 1g, where l = 1 if s 2 L and l = 0 otherwise.
Reference: [106] <author> J.A. Sanchez and J.M. Benedi. </author> <title> Grammatical Inference and Applications, </title> <booktitle> volume 862 of Lecture Notes in Artificial Intelligence (subseries of LNCS), </booktitle> <pages> pages 130-138. </pages> <publisher> Springer-Verlag, </publisher> <address> New York, </address> <year> 1994. </year>
Reference-contexts: This has been done in several pieces of work. * Stolke [111] using a prefix tree method of FSM inference to provide an input HMM to a parameter estimation routine. His results show a good improvement over parameter estimation beginning with a fully connected model. * Sanchez and Benedi <ref> [106] </ref> use a specific inference mechanism (that only finds loop-free automata) to provide a model structure and initial transition probabilities to a forward-backward parameter estimation algorithm. * Chen [33] describes a corpus-based SCFG inference method using bayesian probabilities and compares it to ngram models. * Casacuberta [30] provides a transformation algorithm
Reference: [107] <author> D. Sankoff and J.B. Kruskal, </author> <title> editors. Time Warps, String Edits, and Macromolecules: The Theory and Practice of Sequence Comparison. </title> <publisher> Addison-Wesley, </publisher> <address> Reading, Massachusetts, </address> <year> 1983. </year>
Reference-contexts: String distance metrics have been used profitably as measures of correspondence in a wide variety of other domains, including parsing, DNA/RNA sequencing, and text recognition <ref> [107] </ref>. The basic Levenshtein distance between two strings is measured by counting the minimal number of token insertions, deletions, and substitutions needed to transform one string into the other. between their events.
Reference: [108] <author> M. Schneider, H. Lim, and W. Schoaff. </author> <title> The utilization of fuzzy sets in the recognition of imperfect strings. </title> <journal> Fuzzy Sets and Systems, </journal> <volume> 49 </volume> <pages> 331-337, </pages> <year> 1992. </year>
Reference: [109] <author> R.L. Schwartz, P.M. Melliar-Smith, and F.H. Vogt. </author> <title> In Interval Logic for Higher-level Temporal Reasoning. </title> <booktitle> In Proceedings of the Second ACM Symposium on Principles of Distributed Computing, </booktitle> <pages> pages 173-186. </pages> <institution> Association for Computer Machinery, </institution> <month> August </month> <year> 1983. </year>
Reference-contexts: Time-oriented met--rics, for example, would be very a useful extension to execution stream analysis. Real-time systems analysis techniques could be useful here <ref> [51, 109] </ref>. Methods for measuring the efficiency of a process would be another useful analysis method. Both of these would help in the optimization of a process that has already been behaviorally validated. * Formal models are often useful because they can detect conflicts and consistency violations. <p> Time-oriented metrics, for example, would be very a useful extension to our validation methods. Real-time systems analysis techniques could be useful here <ref> [51, 109] </ref>. Methods for measuring the efficiency of a process would be another useful analysis method, which we explored a little in [36].
Reference: [110] <author> R.W. Selby, A.A. Porter, D.C. Schmidt, and J. Berney. </author> <title> Metric-Driven Analysis and Feedback Systems for Enabling Empirically Guided Software Development. </title> <booktitle> In Proceedings of the 13th International Conference on Software Engineering, </booktitle> <pages> pages 288-298. </pages> <publisher> IEEE Computer Society, </publisher> <month> May </month> <year> 1991. </year>
Reference-contexts: Many efforts use product measurements to infer some process qualities, such as defect production, change data, and other 3 product measurements <ref> [16, 21, 35, 110] </ref>. <p> Their methods would be a good place to begin for creating a system for manual event data collection. * Selby <ref> [110] </ref> built a system, Amadeus, for automated collection and analysis of process metrics. It acts as an event monitor, allowing the triggering of actions based on certain events. <p> Specifically, they statistically analyzed change data and effort data to determine the behavior of the process. For example, they saw ripple effects from interface changes, saw high percentages of fix-on-fix changes, and proposed two measures of process progress that required less data than previous measures. * Selby et al. <ref> [110] </ref> take the approach of providing automated support for empirically guided software development. Their system, Amadeus, can automatically collect measurement data (currently focused primarily on product data) that can then be used to guide development efforts. <p> This work, so far, has seen the creation of single tools that access process data in an ad hoc manner. Several methods for collecting process data have been proposed and constructed (e.g., <ref> [16, 23, 110, 121] </ref>); however, there has not been a significant effort to propose a coherent framework for tools and methods with which to do analysis from process data. Such a framework would be useful in many ways. <p> In most systems the type of data and how it was used was fixed; in general, no effort was made to allow extensions to the data collection and analysis methods. The one system that can come closest to claiming the position of a framework is Amadeus <ref> [110] </ref>. It is flexible in the specification of what data to collect and what to do with the data. It is positioned as a supporting unit to a process-guidance system, so it is meant to support other tools. <p> If it does not, then the process model may not be useful in describing what is good. The key to answering this question lies in the collection and analysis of reliable data. There have been numerous studies that have analyzed product data to identify patterns of defects (e.g., <ref> [16, 21, 34, 110] </ref>). From these patterns, the process mechanisms behind them are inferred, but this last step of inference is purely speculative and not backed directly by hard data.
Reference: [111] <author> A. Stolcke. </author> <title> Grammatical Inference and Applications, </title> <booktitle> volume 862 of Lecture Notes in Artificial Intelligence (subseries of LNCS), </booktitle> <pages> pages 106-118. </pages> <publisher> Springer-Verlag, </publisher> <address> New York, </address> <year> 1994. </year>
Reference-contexts: This has been done in several pieces of work. * Stolke <ref> [111] </ref> using a prefix tree method of FSM inference to provide an input HMM to a parameter estimation routine.
Reference: [112] <institution> An Introduction to the ToolTalk Service. Sun Microsystems, Inc., </institution> <year> 1991. </year>
Reference-contexts: Process integration components, such as ProcessWall [66], are likewise a natural focal point for such instrumentation. Many software engineering environments, even those not process-based, can supply event data relatively easily; message-based systems, such as Field [100], Softbench [58], and ToolTalk <ref> [112] </ref>, provide a natural framework from which to collect data about system-based activities. Of the existing tools that developers already use, many naturally provide some event data collection as well.
Reference: [113] <author> S.M. Sutton, Jr. </author> <title> Accommodating Manual Activities in Automated Process Programs. </title> <booktitle> In Proceedings of the 7th International Software Process Workshop, </booktitle> <month> October </month> <year> 1991. </year>
Reference-contexts: This approach, however, suffers from a fundamental flaw. In particular, it assumes that virtually the entire process is executed within the context of the automated environment. In fact, critical aspects of the process occur off the computer and, therefore, not under the watchful eye of the environment <ref> [113, 121, 122] </ref>. That being the case, there is no effective way to enforce the process using this approach nor to guarantee the mutual consistency of a process model and a process execution.
Reference: [114] <author> S.M. Sutton, Jr., D. Heimbigner, and L.J. Osterweil. </author> <title> Language Constructs for Managing Change in Process-Centered Environments. </title> <booktitle> In SIGSOFT '90: Proceedings of the Fourth Symposium on Software Development Environments, </booktitle> <pages> pages 206-217. </pages> <booktitle> ACM SIGSOFT, </booktitle> <month> December </month> <year> 1990. </year>
Reference-contexts: Several formal models suitable for our analyses have been used to describe software processes. These include models based on state machines (e.g., Statemate [65]), Petri nets (e.g., Slang [13] and FUNSOFT Nets [63]), and procedural languages (e.g., APPL/A <ref> [114] </ref>). 4.2.1 Recognition Metric The first metric is a very straightforward one that has just two values, true or false. The value is true if the event streams exactly match and is false otherwise.
Reference: [115] <author> S.M. Sutton, Jr., D. Heimbigner, and L.J. Osterweil. APPL/A: </author> <title> A Language for Software Process Programming. </title> <journal> ACM Transactions on Software Engineering and Methodology, </journal> <volume> 4(3) </volume> <pages> 221-286, </pages> <month> July </month> <year> 1995. </year>
Reference-contexts: We divide software process research into four main areas|modeling, execution, improvement, and measurement|and describe each in turn. 1.2.1 Modeling Modeling research includes the formulation of languages and paradigms for formally describing software processes. Paradigms that have been used in formal process modeling include programming <ref> [115] </ref>, graph-based control such as state machines [80] and Petri nets [11, 12, 40], rule-based [14, 96], and planning [68]. Huff [67] gives a good overview of modeling and languages.
Reference: [116] <author> S.M. Sutton, Jr., H. Ziv, D. Heimbigner, H.E. Yessayan, M. Maybee, , L.J. Osterweil, and X. Song. </author> <title> Programming a Software Requirements-specification Process. </title> <booktitle> In Proceedings of the First International Conference on the Software Process, </booktitle> <pages> pages 68-89. </pages> <publisher> IEEE Computer Society, </publisher> <month> October </month> <year> 1991. </year>
Reference-contexts: This work in some sense is closely related to workflow, since both try to use a model of the work to drive the type and order of activities. Work in this area includes environments based on process programming <ref> [116] </ref>, rule-based systems such as Marvel [14], and other formalism-based environments, such as the Petri net-based SPADE [12]. Garg and Jazayeri [53] give a good overview of process-centered environments and their issues. <p> In response, new methods and tools for supporting various aspects of the software process have been devised. Many of the technologies, including process automation <ref> [14, 40, 96, 116] </ref>, process analysis [61, 63, 80, 103], and process evolution [11, 76], assume the existence of some sort of formal model of a process in order for those technologies to be applied.
Reference: [117] <author> TickIT: </author> <title> A guide to software quality management system construction and certification using EN29001, issue 2.0. </title> <type> Technical report, </type> <institution> UK Department of Trade and Industry, and the British Computer Society, </institution> <address> London, </address> <year> 1992. </year>
Reference-contexts: The most notable of these efforts is the SEI's Capability Maturity Model (CMM) [69, 94, 95], but other work, not necessarily specific to software, includes the ISO 9001 (9000-3) certification [74], and the Baldridge Quality Award. A European initiative centered around augmenting ISO 9001 is the TickIt process <ref> [117] </ref>. The CMM rates the goodness of a process by providing a checklist of activities that processes should include, and then provides a path of maturing the process by adding these activities.
Reference: [118] <author> L.G. Valiant. </author> <title> A theory of the learnable. </title> <journal> Communications of the ACM, </journal> <volume> 27 </volume> <pages> 1134-1142, </pages> <year> 1984. </year>
Reference-contexts: The PAC model for learning concepts from examples was proposed by Valiant <ref> [118] </ref>. PAC stands for "Probably Approximately Correct", which may not sound rigorous but is: probably is defined as within some probability 1 ffi, and approximately is defined as within some 1 * of the correct solution.
Reference: [119] <author> L.G. Votta and M.L. Zajac. </author> <title> Design process improvement case study using process waiver data. </title> <booktitle> In Proceedings of the Fifth European Software Engineering Conference (ESEC'95), </booktitle> <pages> pages 44-58. </pages> <publisher> Springer-Verlag, </publisher> <month> September </month> <year> 1995. </year> <month> 139 </month>
Reference-contexts: This work used an experimental setting to determine the actual effects of code inspections on the quality of the product. Another example shows how process exceptions might correlate to defects in the product <ref> [119] </ref>. A special issue of IEEE Software [73] had a theme of measurement-based process improvement; most of the articles dealt with questionnaire-based improvement, with two articles reporting on statistical and experimental improvement. <p> A similar kind of historical data analysis was successfully employed in a recent study conducted by Votta and Zajac <ref> [119] </ref>. In this chapter we present a study that analyzes a repetitive process to determine if good process does in fact lead to good product.
Reference: [120] <author> M.S. Waterman. </author> <title> General methods of sequence comparison. </title> <journal> Bulletin of Mathematical Biology, </journal> <volume> 46:473--501, </volume> <year> 1984. </year>
Reference: [121] <author> A.L. Wolf and D.S. Rosenblum. </author> <title> A Study in Software Process Data Capture and Analysis. </title> <booktitle> In Proceedings of the Second International Conference on the Software Process, </booktitle> <pages> pages 115-124. </pages> <publisher> IEEE Computer Society, </publisher> <month> February </month> <year> 1993. </year>
Reference-contexts: Some efforts are trying to use measurements to help characterize the process; for example, Wolf and Rosenblum collected event-based data and used it to do some temporal and visualization analysis <ref> [121] </ref>, and Bradac, Votta, and Perry use actual measurements to define the parameters to a queueing model of a process [23]. A body of work has looked at experimental measurement of specific process techniques. <p> This chapter details the definitions of important terms and discusses the framework and type of data that this thesis' analyses are built upon. 2.1 Events Following Wolf and Rosenblum <ref> [121] </ref>, we use an event-based model of process actions, where an event is used to characterize the dynamic behavior of a process in terms of identifiable, instantaneous actions, such as invoking a development tool or deciding upon the next activity to be performed. `Instantaneous' is relative to the time granularity that <p> It acts as an event monitor, allowing the triggering of actions based on certain events. Our use would simply collect the events into an event stream. * Wolf and Rosenblum <ref> [121] </ref> use a hybrid of manual and automated collection methods to collect event data from a build process. * Bradac, et al. [23], provided the user with a menu-based tool that collects sampled task and state information. <p> This approach, however, suffers from a fundamental flaw. In particular, it assumes that virtually the entire process is executed within the context of the automated environment. In fact, critical aspects of the process occur off the computer and, therefore, not under the watchful eye of the environment <ref> [113, 121, 122] </ref>. That being the case, there is no effective way to enforce the process using this approach nor to guarantee the mutual consistency of a process model and a process execution. <p> model the process as a queuing network, and use actual data about the time spent by the agents in specific tasks and states to determine the real parameters (i.e., service times and probabilities, and branch path probabilities) of the queuing network that can then be analyzed. * Wolf and Rosenblum <ref> [121] </ref> demonstrate how to collect event-based process data and use basic statistical and visual techniques to find interesting relationships among the data in order to uncover possible areas of process improvement. 57 * Porter et al. [98] use a well constructed experimental setting to assess the benefits of code inspections, in <p> This work, so far, has seen the creation of single tools that access process data in an ad hoc manner. Several methods for collecting process data have been proposed and constructed (e.g., <ref> [16, 23, 110, 121] </ref>); however, there has not been a significant effort to propose a coherent framework for tools and methods with which to do analysis from process data. Such a framework would be useful in many ways. <p> There are many kinds of data we could examine, but we chose to look at event data <ref> [23, 121] </ref> because they neatly characterize the dynamic behavior of the process in terms of the sequencing of its major activities. The event data come from several sources.
Reference: [122] <editor> A.L. Wolf and D.S. Rosenblum. </editor> <booktitle> Process-centered Environments (Only) Support Environment-centered Processes. In Proceedings of the 8th International Software Process Workshop, </booktitle> <pages> pages 148-149, </pages> <month> March </month> <year> 1993. </year>
Reference-contexts: This approach, however, suffers from a fundamental flaw. In particular, it assumes that virtually the entire process is executed within the context of the automated environment. In fact, critical aspects of the process occur off the computer and, therefore, not under the watchful eye of the environment <ref> [113, 121, 122] </ref>. That being the case, there is no effective way to enforce the process using this approach nor to guarantee the mutual consistency of a process model and a process execution.
Reference: [123] <author> P.H. Worley. </author> <title> A new PICL trace file format. </title> <type> Technical Report ORNL/TM-12125, </type> <institution> Oak Ridge National Laboratory, </institution> <year> 1992. </year>
Reference-contexts: For example, the PICL trace format standardizes the data format for tools that analyze parallel, message-passing systems <ref> [123] </ref>. Thus, they have not needed a general framework. Tools in the software process arena, on the other hand, must deal with event data from multiple, heterogeneous sources, and also with varied formats of the events. In this climate a framework such as Balboa is needed.
Reference: [124] <author> Z. Zeng, R.M. Goodman, and P. Smyth. </author> <title> Learning finite state machines with self-clustering recurrent networks. </title> <journal> Nueral Computation, </journal> <volume> 5 </volume> <pages> 976-990, </pages> <year> 1993. </year> <month> 140 </month>
Reference-contexts: Recent representative work in this field is <ref> [39, 124] </ref>. The various methods all consist of defining a recurrent network architecture, and then analyzing the hidden neuron activity to discover the states and transitions for the resulting grammar. The difference between these methods is how they inspect the hidden neurons to infer state information.
References-found: 124

