URL: http://www.aic.nrl.navy.mil/papers/1994/AIC-94-002.ps
Refering-URL: http://www.aic.nrl.navy.mil/~schultz/perception-action-learning.html
Root-URL: 
Title: EVOLUTIONARY ALGORITHMS IN ROBOTICS  
Author: JOHN GREFENSTETTE 
Note: problems in robotics.  
Address: Washington, DC 20375-5337  
Affiliation: Naval Research Laboratory  
Abstract:  
Abstract-found: 1
Intro-found: 1
Reference: 1. <author> De Jong, K. A. and W. </author> <title> Spears (1993). </title> <booktitle> On the state of evolutionary computation. Proceedings of the Fifth International Conference on Genetic Algorithms, </booktitle> <pages> pp. 618-626, </pages> <address> San Mateo, CA: </address> <publisher> Mor-gan Kaufmann. </publisher>
Reference-contexts: INTRODUCTION The field of robotics offers an endless supply of difficult problems, requiring an equally impressive array of methods for their solution. One class of methods that has shown its utility on a number of relevant problems is called Evolutionary Computation <ref> [1] </ref>. This term applies to computational methods that incorporate principles from biological population genetics to perform search, optimization, and machine learning, and includes a variety of specific formulations with names such as genetic algorithms, evolutionary programming, evolution strategies, and genetic programming.
Reference: 2. <editor> Baeck, T. and H.-P. Schwefel (1993). </editor> <title> An overview of evolutionary algorithms for parameter optimization. </title> <booktitle> Evolutionary Computation 1(1), </booktitle> <pages> 1-23. </pages>
Reference-contexts: What it means to be a "candidate solution" is a function of the application task. Evolutionary algorithms have been designed in which each candidate solution represents a collection of parameter settings <ref> [2] </ref>, an individual rule [3], a collection of rules [4], or a tree-structured computer program [5]. In each case, the algorithm proceeds by evaluating the fitness of the candidate solutions in the population, and then selecting parents from the current population for replication or deletion on the basis of fitness. <p> This cycle repeats, producing generally more fit candidates over time. The details of the individual phases of the evolutionary algorithm vary widely among research groups, but comparative studies are beginning to identify some fundamental properties of the alternative approaches <ref> [2] </ref>. __________________ To appear in the Fifth International Symposium on Robotics and Manufacturing, ISRAM 94. procedure Evolve begin t = 0; initialize population P (t); evaluate solutions in P (t); while an acceptable solution has not been found, do begin t = t + 1; select parents from P (t-1); recombine
Reference: 3. <author> Booker, L. B. </author> <year> (1988). </year> <title> Classifier systems that learn internal world models. </title> <booktitle> Machine Learning 3(3), </booktitle> <pages> 161-192. </pages>
Reference-contexts: What it means to be a "candidate solution" is a function of the application task. Evolutionary algorithms have been designed in which each candidate solution represents a collection of parameter settings [2], an individual rule <ref> [3] </ref>, a collection of rules [4], or a tree-structured computer program [5]. In each case, the algorithm proceeds by evaluating the fitness of the candidate solutions in the population, and then selecting parents from the current population for replication or deletion on the basis of fitness. <p> However, some recent studies have used more heuristic operators that make more directed changes based on the learning agent's experience. For example, some genetic classifier systems use triggered operators such a creating a new rule to cover a novel situation <ref> [3] </ref>. In SAMUEL, we use generalization and specialization operators that are triggered by specific conditions relating the measured utilities of individual rules and the outcome of the task. These may be viewed as Lamarckian forms of evolution [24], and show that artificial evolution need not proceed along purely Darwinian lines.
Reference: 4. <author> Grefenstette, J. J., C. L. Ramsey and A. C. </author> <title> Schultz (1990). Learning sequential decision rules using simulation models and competition. </title> <booktitle> Machine Learning 5(4), </booktitle> <pages> 355-381. </pages>
Reference-contexts: What it means to be a "candidate solution" is a function of the application task. Evolutionary algorithms have been designed in which each candidate solution represents a collection of parameter settings [2], an individual rule [3], a collection of rules <ref> [4] </ref>, or a tree-structured computer program [5]. In each case, the algorithm proceeds by evaluating the fitness of the candidate solutions in the population, and then selecting parents from the current population for replication or deletion on the basis of fitness. <p> ROBOTICS APPLICATIONS Evolutionary methods have found applications that span the range of architectures for intelligent robotics. For example, evolutionary algorithms have been used to learn rule sets for rule-based autonomous agents <ref> [4] </ref>, topologies and weights for neural nets for robotic control [11,12], fuzzy logic control systems [13], programs for LISP-controlled robots [5], and rules for behavior-based robots [14]. <p> The remainder of this paper will address some particular issues applying the genetic knowledge engineering approach to problems in robotics, focusing in part on the SAMUEL evolutionary learning system being developed at NRL <ref> [4] </ref>. USING BACKGROUND KNOWLEDGE It is natural to classify evolutionary algorithms as "weak methods", that is, methods that rely on few assumptions about the space being searched.
Reference: 5. <editor> Koza, J. R. </editor> <booktitle> (1992). Genetic Programming. </booktitle> <address> Cambridge, MA: </address> <publisher> MIT Press. </publisher>
Reference-contexts: What it means to be a "candidate solution" is a function of the application task. Evolutionary algorithms have been designed in which each candidate solution represents a collection of parameter settings [2], an individual rule [3], a collection of rules [4], or a tree-structured computer program <ref> [5] </ref>. In each case, the algorithm proceeds by evaluating the fitness of the candidate solutions in the population, and then selecting parents from the current population for replication or deletion on the basis of fitness. <p> For example, evolutionary algorithms have been used to learn rule sets for rule-based autonomous agents [4], topologies and weights for neural nets for robotic control [11,12], fuzzy logic control systems [13], programs for LISP-controlled robots <ref> [5] </ref>, and rules for behavior-based robots [14]. There are at least two different research paradigms evident in the literature of evolutionary algorithms for robotics, which might be called the artificial life (ALife) paradigm, and the genetic knowledge engineering paradigm. The ALife paradigm is motivated by a number of issues.
Reference: 6. <editor> Forrest, S. (Ed.) </editor> <booktitle> Proceedings of the Fifth International Conference on Genetic Algorithms, </booktitle> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann, </publisher> <year> 1993. </year>
Reference: 7. <editor> R. Maenner and B. Manderick (Eds.), </editor> <booktitle> Proceedings of Parallel Problem Solving from Nature-2. </booktitle> <publisher> North-Holland, </publisher> <year> 1992. </year>
Reference: 8. <editor> Fogel, D. B. and W. Atmar, </editor> <booktitle> Proc. Second Annual Conference on Evolutionary Programming. </booktitle> <address> La Jolla, CA: </address> <publisher> Evolutionary Programming Society. </publisher>
Reference: 9. <editor> D. Whitley (Ed.), </editor> <booktitle> Foundations of Genetic Algorithms-2. </booktitle> <address> San Mateo: </address> <publisher> Morgan Kaufmann, </publisher> <year> 1993. </year>
Reference-contexts: Fortunately, in addition to the extensive experience with evolutionary algorithms of the last twenty-five years [6,7,8], there is a growing body of theory that can guide the user toward successful applications <ref> [9] </ref>. Evolutionary algorithms employ an intelligent random search in the space of candidate solutions. While cost-effective on difficult problems, they are not the method of choice for problems that can be solved easily through analysis, simple heuristic search, or exhaustive enumeration [10].
Reference: 10. <editor> L. Davis (Ed.), </editor> <booktitle> Handbook of Genetic Algorithms. </booktitle> <publisher> Van Nostrand Reinhold, </publisher> <year> 1991. </year>
Reference-contexts: Evolutionary algorithms employ an intelligent random search in the space of candidate solutions. While cost-effective on difficult problems, they are not the method of choice for problems that can be solved easily through analysis, simple heuristic search, or exhaustive enumeration <ref> [10] </ref>. ROBOTICS APPLICATIONS Evolutionary methods have found applications that span the range of architectures for intelligent robotics.
Reference: 11. <author> Whitley, D., S. Dominic, R. Das, C. </author> <title> Anderson (1993). Genetic reinforcement learning for neu-rocontrol problems. </title> <booktitle> Machine Learning 13(2/3), </booktitle> <pages> 259-284. </pages>
Reference: 12. <author> Yamauchi, B. </author> <year> (1994). </year> <title> Dynamic neural networks for mobile robot control. </title> <address> ISRAM 94. </address>
Reference: 13. <author> Karr, C, L. </author> <year> (1991). </year> <title> Design of an adaptive fuzzy logic controller using a genetic algorithm. </title> <booktitle> Proceedings of the Fourth International Conference on Genetic Algorithms, </booktitle> <pages> pp. 450-457, </pages> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: ROBOTICS APPLICATIONS Evolutionary methods have found applications that span the range of architectures for intelligent robotics. For example, evolutionary algorithms have been used to learn rule sets for rule-based autonomous agents [4], topologies and weights for neural nets for robotic control [11,12], fuzzy logic control systems <ref> [13] </ref>, programs for LISP-controlled robots [5], and rules for behavior-based robots [14]. There are at least two different research paradigms evident in the literature of evolutionary algorithms for robotics, which might be called the artificial life (ALife) paradigm, and the genetic knowledge engineering paradigm.
Reference: 14. <author> Dorigo, M. and U. </author> <title> Schnepf (1993). Genetics-based machine learning and behavior-based robotics: a new synthesis. </title> <journal> IEEE Transactions on Systems, Man and Cybernetics, SMC-23, </journal> <volume> 1. </volume>
Reference-contexts: For example, evolutionary algorithms have been used to learn rule sets for rule-based autonomous agents [4], topologies and weights for neural nets for robotic control [11,12], fuzzy logic control systems [13], programs for LISP-controlled robots [5], and rules for behavior-based robots <ref> [14] </ref>. There are at least two different research paradigms evident in the literature of evolutionary algorithms for robotics, which might be called the artificial life (ALife) paradigm, and the genetic knowledge engineering paradigm. The ALife paradigm is motivated by a number of issues.
Reference: 15. <author> P. Todd and S. </author> <title> Wilson (1993). Environment structure and adaptive behavior from the ground up. </title> <booktitle> From Animals to Animats 2: Proc. of the Second International Conference on Simulation of Adaptive Behavior, </booktitle> <pages> pp. 11-20. </pages> <address> Cambridge, MA: </address> <publisher> MIT Press. </publisher>
Reference-contexts: The ALife paradigm is motivated by a number of issues. First, there is interest in the process of evolution itself. Simulations of robotic agents can be used to study the emergence of complex behavior from the interaction of simple components, operating under conditions of competition for resources <ref> [15] </ref>. Second, there are concerns about the utility of the traditional model-based reasoning approach to designing intelligent robots. Models used in artificial intelligence stu-dies often fail to reflect the complexities, noise, and errors that arise in real sensors and actuators operating in the real world.
Reference: 16. <author> Brooks, R. </author> <title> Artificial Life and Real Robots. </title> <publisher> Cambridge: MIT Press. </publisher> <year> 1992. </year>
Reference-contexts: In response to such shortcomings, some researchers argue for the development of adaptive robots that evolve behaviors without using a pre-specified model of the world <ref> [16] </ref>. Finally, there is a growing interest in the possibility of physical robots based on nano-technology, that truly self-replicate and evolve in adaptation to their environment [17].
Reference: 17. <author> Higuchi, T., T. Niwa, T. Tanaka, H. Iba, H. de Garis and T. </author> <month> Furaya </month> <year> (1993). </year> <title> Evolving hardware with genetic learning: a first step towards building a Darwin machine, </title> <booktitle> From Animals to Animats 2: Proc. of the Second International Conference on Simulation of Adaptive Behavior, </booktitle> <pages> pp. 417-424. </pages> <address> Cambridge, MA: </address> <publisher> MIT Press. </publisher>
Reference-contexts: Finally, there is a growing interest in the possibility of physical robots based on nano-technology, that truly self-replicate and evolve in adaptation to their environment <ref> [17] </ref>. The genetic knowledge engineering paradigm treats the knowledge acquisition task for intelligent robots as a cooperative effort between the robot designers and the robot itself [18].
Reference: 18. <author> Grefenstette, J. and C. </author> <title> Ramsey (1993). Combining experience with quantitative models. </title> <booktitle> Workshop on Learning Action Models at AAAI-93 (National Conference on Artificial Intelligence, </booktitle> <year> 1993) </year>
Reference-contexts: The genetic knowledge engineering paradigm treats the knowledge acquisition task for intelligent robots as a cooperative effort between the robot designers and the robot itself <ref> [18] </ref>. Some aspects of the robot's world will be known in great detail to the designer, for example, the size and weight of the robot, the characteristics of its sensors and effectors, and at least some of the physics of the task environment.
Reference: 19. <author> Grefenstette, J. </author> <title> Incorporating problem specific knowledge into genetic algorithms, in Genetic Algorithms and Simulated Annealing, </title> <editor> L. Davis (Ed.), </editor> <publisher> London: Pitman, </publisher> <year> 1987. </year>
Reference-contexts: USING BACKGROUND KNOWLEDGE It is natural to classify evolutionary algorithms as "weak methods", that is, methods that rely on few assumptions about the space being searched. However, the approach affords several opportunities for using background knowledge to augment the basic method <ref> [19] </ref>, producing a more efficient overall knowledge acquisition effort. Choice of Search Space and Representation. The first choice the user of an evolutionary algorithm needs to make is the choice of the search space, along with an appropriate representation. This choice clearly reflects the trade-offs between human and machine effort. <p> Evolutionary methods are less efficient at fine-tuning candidate solutions. Therefore, a natural hybrid is to use an efficient local optimization method to improve the final solutions found by the evolutionary system <ref> [19] </ref>. Another hybrid approach is to augment the population with additional long-term memory, to deal with changing environments. We are exploring an approach called case-based anytime learning [25].
Reference: 20. <author> Grefenstette, J. J. and H. C. </author> <note> Cobb (1994). User's guide for SAMUEL. Version 4.0. NRL Report, </note> <institution> Naval Research Lab, </institution> <address> Washington, DC. </address>
Reference-contexts: In SAMUEL, the user can further limit the search space by defining a set of constraints in the form of rules that specify conditions under which certain actions are either forbidden or required <ref> [20] </ref>. Constraints are intended to limit the robot's actions within physically safe parameters, but still allow freedom to explore a large set of alternative strategies [21]. Initial Population. Evolutionary algorithms often begin with candidate solutions selected at random from the search space.
Reference: 21. <author> Grefenstette, J. </author> <year> (1992). </year> <title> The evolution of strategies for multi-agent environments. </title> <booktitle> Adaptive Behavior 1(1), </booktitle> <pages> 65-90. </pages>
Reference-contexts: Constraints are intended to limit the robot's actions within physically safe parameters, but still allow freedom to explore a large set of alternative strategies <ref> [21] </ref>. Initial Population. Evolutionary algorithms often begin with candidate solutions selected at random from the search space. Often, the approach can be sped up by the use of heuristics to select the starting population.
Reference: 22. <author> Schultz, A. C. and J. J. </author> <title> Grefenstette (1990). Improving tactical plans with genetic algorithms. </title> <booktitle> Proceedings of IEEE Conference on Tools for AI 90 (pp 328-334). </booktitle> <address> Washington, DC: </address> <publisher> IEEE. </publisher>
Reference-contexts: This must be done with care, however, since a lack of sufficient diversity in the initial population is almost guaranteed to produced premature convergence to suboptimal solutions. In SAMUEL, the rule representation was designed to encourage the user to include heuristic strategies in the initial population <ref> [22] </ref>. In fact, for many complex robotic tasks, it is unlikely that the system will be able to evolve solutions without some initial heuristics.
Reference: 23. <author> Ramsey, C. L., A. C. Schultz and J. J. </author> <title> Grefenstette (1990). Simulation-assisted learning by competition: Effects of noise differences between training model and target environment. </title> <booktitle> Proceedings of the Seventh International Conference on Machine Learning pp. </booktitle> <pages> 211-215. </pages> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Evolutionary algorithms may also exploit unexpected features of the simulation model to maximize performance. Of course, this implies that the model should accurately reflect the conditions in the target environment. To the extent that this is not possible, our studies <ref> [23] </ref> have shown that it is still possible to learn from limited-fidelity simulations that err on the side of difficulty (e.g., have more noisy sensors that the real robots). In such cases, the learning time increases, but so does the robustness of the learned rules.
Reference: 24. <author> Grefenstette, J. J. </author> <year> (1991). </year> <title> Lamarckian learning in multi-agent environments. </title> <booktitle> Proceedings of the Fourth International Conference of Genetic Algorithms pp. </booktitle> <pages> 303-310, </pages> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: In SAMUEL, we use generalization and specialization operators that are triggered by specific conditions relating the measured utilities of individual rules and the outcome of the task. These may be viewed as Lamarckian forms of evolution <ref> [24] </ref>, and show that artificial evolution need not proceed along purely Darwinian lines. Hybrid Approaches. Finally, it is often useful to use evolutionary methods in concert with other methods that have complementary strengths.
Reference: 25. <author> Grefenstette, J. J. and C. L. </author> <title> Ramsey (1992). An approach to anytime learning. </title> <booktitle> Proceedings of the Ninth International Conference on Machine Learning pp. </booktitle> <pages> 189-195, </pages> <editor> D. Sleeman and P. Edwards (eds.), </editor> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Therefore, a natural hybrid is to use an efficient local optimization method to improve the final solutions found by the evolutionary system [19]. Another hybrid approach is to augment the population with additional long-term memory, to deal with changing environments. We are exploring an approach called case-based anytime learning <ref> [25] </ref>. In this approach, the robot's evolutionary learning module continuously tests new strategies against a simulation model of the task environment, and dynamically updates the knowledge base used by the real robot on the basis of the results.
Reference: 26. <author> Ramsey, C. L. and J. J. </author> <title> Grefenstette (1993). Case-based initialization of genetic algorithms. </title> <booktitle> Proc. Fifth Int. Conf. on Genetic Algorithms. </booktitle> <pages> pp. 84-91, </pages> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: When the simulation model is modified, the learning process is reinitialized using previously learned strategies from similar cases. Tests using a two-robot cat-and-mouse game as the task environment show that case-based methods provide significant improvement in responding to changes in the task environment <ref> [26] </ref>. FUTURE DIRECTIONS We have presented a brief overview of issues arising in the application of evolutionary algorithms to robotics.
References-found: 26

