URL: ftp://ftp.cs.rochester.edu/pub/papers/systems/96.tr626.Using_peer_support_to_reduce_fault-tolerant_overhead.ps.gz
Refering-URL: http://www.cs.rochester.edu/u/gchunt/
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Email: fgchunt,scottg@cs.rochester.edu  
Title: Using Peer Support to Reduce Fault-Tolerant Overhead in Distributed Shared Memories  
Author: Galen C. Hunt Michael L. Scott 
Keyword: Distributed shared memory, fault tolerance, peer logging.  
Note: Galen Hunt was supported by a research fellowship from Microsoft Corporation. This work was supported in part by NSF Institutional Infrastructure grant no. CDA-8822724, and ONR research grant no.N00014-92-J-1801 (in conjunction with the DARPA Research in Information Science and Technology High Performance Computing, Software Science and Technology Program, ARPA Order no. 8930).  
Address: Rochester, NY 14627-0226  
Affiliation: Department of Computer Science University of Rochester  
Abstract: We present a peer logging system for reducing performance overhead in fault-tolerant distributed shared memory systems. Our system provides fault-tolerant shared memory using individual checkpointing and rollback. Peer logging logs DSM modification messages to remote nodes instead of to local disks. We present results for implementations of our fault-tolerant technique using simulations of both TreadMarks, a software-only DSM, and Cashmere, a DSM using memory mapped hardware. We compare simulations with no fault tolerance to simulations with local disk logging and peer logging. We present results showing that fault-tolerant Treadmarks can be achieved with an average of 17% overhead for peer logging. We also present results showing that while almost any DSM protocol can be made fault tolerant, systems with localized DSM page meta-data have much lower overheads. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> D. Bailey, E. Barszcz, L. Dagum, and H. Simon. </author> <title> NAS Parallel Benchmarking Results. </title> <booktitle> In Proceedings Supercomputing '92, </booktitle> <address> Minneapolis, MN, </address> <month> November </month> <year> 1992. </year>
Reference-contexts: We believe that our parameters accurately reflect optimistic, but achievable numbers. As with any system, your actual mileage may vary. Our test application suite consists of four programs: appbt, em3d, sor and water. Appbt is from the NAS parallel benchmark suite <ref> [1] </ref>. Em3d is from the Split-C [4] project at UC Berkeley. Sor is a local computation kernel. Water is part of the SPLASH suite [21]. Appbt calculates approximate solutions to Navier-Stokes differential equations modeling heat dissipation. Appbt uses linear arrays to represent higher dimensional data structures.
Reference: [2] <author> B. N. Bershad, M. J. Zekauskas, and W. A. Sawdon. </author> <title> The Midway Distributed Shared Memory System. </title> <booktitle> In Proceedings of the IEEE COMPCON '93. </booktitle>
Reference-contexts: The processors communicate by passing messages through a network. The DSM system is responsible for providing consistency between local copies of shared memory. The DSM system converts shared memory modifications to network messages. DSM systems have been implemented both in hardware [15] and in software <ref> [2, 3, 10, 16] </ref>. Software DSM systems are particularly interesting because they can be used on either specialized networks or on commodity networks such as Ethernet [17] and ATM [14]. Using a software DSM, a network of workstations can be turned into a parallel processing environment. <p> The releasing processor returns a message containing a list of exactly the data modified during the intervals that are in the releaser's logical past, but not in the logical past of the acquirer. * Entry consistency <ref> [2] </ref> reduces the number and size of messages (compared to release consistency) by binding data objects to synchronization objects. When a processor acquires a synchronization object, it receives modifications only for the related data objects.
Reference: [3] <author> J. B. Carter. </author> <title> Design of the Munin Distributed Shared Memory System. </title> <journal> In Journal of Parallel and Distributed Computing, </journal> <month> September </month> <year> 1995. </year>
Reference-contexts: The processors communicate by passing messages through a network. The DSM system is responsible for providing consistency between local copies of shared memory. The DSM system converts shared memory modifications to network messages. DSM systems have been implemented both in hardware [15] and in software <ref> [2, 3, 10, 16] </ref>. Software DSM systems are particularly interesting because they can be used on either specialized networks or on commodity networks such as Ethernet [17] and ATM [14]. Using a software DSM, a network of workstations can be turned into a parallel processing environment.
Reference: [4] <author> D. Culler, A. Dusseau, S. Goldstein, A. Krishnamurthy, S. Lumetta, T. von Eicken, and K. Yelick. </author> <title> Parallel Programming in Split-C. </title> <booktitle> In Proceedings Supercomputing '93, </booktitle> <pages> pages 262-273, </pages> <address> Portland, OR, </address> <month> November </month> <year> 1993. </year>
Reference-contexts: We believe that our parameters accurately reflect optimistic, but achievable numbers. As with any system, your actual mileage may vary. Our test application suite consists of four programs: appbt, em3d, sor and water. Appbt is from the NAS parallel benchmark suite [1]. Em3d is from the Split-C <ref> [4] </ref> project at UC Berkeley. Sor is a local computation kernel. Water is part of the SPLASH suite [21]. Appbt calculates approximate solutions to Navier-Stokes differential equations modeling heat dissipation. Appbt uses linear arrays to represent higher dimensional data structures. Each processor is assigned a separate square area.
Reference: [5] <author> M. J. Feeley, J. S. Chase, V. R. Narasayya, and H. M. Levy. </author> <title> Integrating Coherency and Recovery in Distributed Systems. </title> <booktitle> In Proceedings of the First Symposium on Operating Systems Design and Implementation, </booktitle> <address> Monterey, CA, </address> <month> November </month> <year> 1994. </year>
Reference-contexts: For the three networks in Figure 6, fault-tolerant peer logging shows better speedup than disk logging. 7 Related Work Fault-tolerant DSM systems are related to work on transactional virtual memory systems. Transactional VM systems, including Camelot [22], PLinda [8], RVM [20] and LDSM <ref> [5] </ref>, add transaction semantics to persistent virtual memory. A transaction consists of a start operation, a number of modifications to shared memory and either a commit or an abort operation.
Reference: [6] <author> K. Gharachorloo, D. Lenoski, J. Laudon, P. Gibbons, A. Gupta, and J. L. Hennessy. </author> <title> Memory Consistency and Event Ordering in Scalable Shared-Memory Multiprocessors. </title> <booktitle> In Proceedings of the Seventeenth International Symposium on Computer Architecture, </booktitle> <pages> pages 15-26, </pages> <address> Seattle, WA, </address> <month> May </month> <year> 1990. </year>
Reference-contexts: Common consistency models/protocols include: * Sequential consistency [13] requires that a read from any shared object will reflect the most recent write by any processor to that object. Object modifications must be propagated immediately to all sharing processors. * Release consistency <ref> [6] </ref> relaxes consistency guarantees based on the insight that programs use synchronization objects to guard access to data objects. In release consistency, writes need to become visible at remote processors only when a synchronization release become visible.
Reference: [7] <author> R. Gillett. </author> <title> Memory Channel: An Optimized Cluster Interconnect. </title> <journal> IEEE Micro, </journal> <volume> 16(2), </volume> <month> February </month> <year> 1996. </year>
Reference-contexts: Our peer logging simulations use the same network mechanisms as the coherence protocol. In addition to log provider, we also vary the type of network from 10Mb/s Ethernet to 155Mb/s ATM and 50MB/s Memory Channel <ref> [7] </ref>. The Ethernet numbers assume a 51 byte UDP packet overhead. The ATM numbers use a standard packet containing 48 data bytes and 5 bytes of overhead. The Memory Channel hardware is memory mapped and provides write-through of 64-bit values. Remote reads are not supported.
Reference: [8] <author> K. Jeong and D. Shasha. </author> <title> Persistent Linda 2: A Transactional/Checkpointing Approach to Fault Tolerant Linda. </title> <booktitle> In Proceedings of the 13th Symposium on Fault-Tolerant Distributed Systems, </booktitle> <month> October </month> <year> 1994. </year>
Reference-contexts: For the three networks in Figure 6, fault-tolerant peer logging shows better speedup than disk logging. 7 Related Work Fault-tolerant DSM systems are related to work on transactional virtual memory systems. Transactional VM systems, including Camelot [22], PLinda <ref> [8] </ref>, RVM [20] and LDSM [5], add transaction semantics to persistent virtual memory. A transaction consists of a start operation, a number of modifications to shared memory and either a commit or an abort operation.
Reference: [9] <author> P. Keleher, A. L. Cox, and W. Zwaenepoel. </author> <title> Lazy Release Consistency for Software Distributed Shared Memory. </title> <booktitle> In Proceedings of the Nineteenth International Symposium on Computer Architecture, </booktitle> <pages> pages 13-21, </pages> <address> Gold Coast, Australia, </address> <month> May </month> <year> 1992. </year>
Reference-contexts: In release consistency, writes need to become visible at remote processors only when a synchronization release become visible. Release consistency allows optimizations such as collecting all writes and transferring them with the released synchronization object in a single message. * Lazy release consistency <ref> [9] </ref> reduces network bandwidth by lazily copying only objects used by acquiring processors. When a processor acquires a synchronization object it sends the releasing processor a vector time stamp. The time stamp records the intervals when the acquiring processor last received data from each of the other processors.
Reference: [10] <author> P. Keleher, S. Dwarkadas, A. L. Cox, and W. Zwaenepoel. TreadMarks: </author> <title> Distributed Shared Memory on Standard Workstations and Operating Systems. </title> <booktitle> In Proceedings of the USENIX Winter '94 Technical Conference, </booktitle> <pages> pages 115-131, </pages> <address> San Francisco, CA, </address> <month> January </month> <year> 1994. </year>
Reference-contexts: The processors communicate by passing messages through a network. The DSM system is responsible for providing consistency between local copies of shared memory. The DSM system converts shared memory modifications to network messages. DSM systems have been implemented both in hardware [15] and in software <ref> [2, 3, 10, 16] </ref>. Software DSM systems are particularly interesting because they can be used on either specialized networks or on commodity networks such as Ethernet [17] and ATM [14]. Using a software DSM, a network of workstations can be turned into a parallel processing environment. <p> Water uses static scheduling to increase data locality. During any time step, a single process will share data with at most half of the other processes. We simulate 256 molecules for 3 time steps. Our simulations consist of two DSM protocols, TreadMarks and Cashmere. TreadMarks <ref> [10] </ref> represents the state of the art in software-only DSM systems. It uses a multiple-writer, lazy release consistency model to reduce network bandwidth and minimize the impact of network latency. Shared memory accesses are detected using the virtual memory protection mechanisms. Updates collected using twinning and diffing.
Reference: [11] <author> A.-M. Kermarrec, G. Cabillic, A. Gefflaut, C. Morin, and I. Puaut. </author> <title> A Recoverable Distributed Shared Memory Integrating Coherence and Recoverability. </title> <booktitle> In Proceedings of the Twenty-fifth International Symposium on Fault-Tolerant Computing, </booktitle> <pages> pages 289-298, </pages> <address> Los Alamitos, CA, </address> <month> June </month> <year> 1995. </year> <note> INRIA. </note>
Reference-contexts: A log entry can be discarded only when all processes that subsequently requested the object have been checkpointed. If a second node fails before all processes have checkpointed, recovery cannot be guaranteed. An alternative to checkpointing is replication. Kermarrec et al <ref> [11] </ref> describe a fault-tolerant sequential consistency DSM system. Their system takes advantage of the page duplication inherent in DSM systems. Periodically a global checkpoint is created by making sure that duplicates exist for all shared pages. All pages are then marked read only.
Reference: [12] <author> L. I. Kontothanassis and M. L. Scott. </author> <title> High Performance Software Coherence for Current and Future Architectures. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 29(2) </volume> <pages> 179-195, </pages> <month> November </month> <year> 1995. </year> <month> 11 </month>
Reference-contexts: Shared memory accesses are detected using the virtual memory protection mechanisms. Updates collected using twinning and diffing. Page invalidations are forwarded with synchronization objects and diffs are forwarded on read faults. Cashmere <ref> [12] </ref> takes advantage of memory mapped hardware to remove the need for diffs. Modifications to shared memory are written through the network to a master copy of each page. To ease management, a common protocol page directory is maintained in distributed, globally modifiable memory.
Reference: [13] <author> L. Lamport. </author> <title> How to Make a Multiprocessor Computer that Correctly Executes Multiprocess Programs. </title> <journal> IEEE Transactions on Computers, </journal> <volume> C-28(9):241-248, </volume> <month> September </month> <year> 1979. </year>
Reference-contexts: The DSM system maintains consistency using a model, which describes how the program interacts with shared memory, and a protocol, which describes how shared memory on one processor interacts with shared memory on another. Common consistency models/protocols include: * Sequential consistency <ref> [13] </ref> requires that a read from any shared object will reflect the most recent write by any processor to that object.
Reference: [14] <author> J. Lane. </author> <title> ATM Knits Voice, Data on Any Net. </title> <journal> IEEE Spectrum, </journal> <volume> 31(2) </volume> <pages> 42-45, </pages> <month> February </month> <year> 1994. </year>
Reference-contexts: DSM systems have been implemented both in hardware [15] and in software [2, 3, 10, 16]. Software DSM systems are particularly interesting because they can be used on either specialized networks or on commodity networks such as Ethernet [17] and ATM <ref> [14] </ref>. Using a software DSM, a network of workstations can be turned into a parallel processing environment. Programs designed for tightly-coupled, 2 bus-based architectures can be run with little modification. Programmer productivity is increased through the use of a familiar model: shared memory.
Reference: [15] <author> D. Lenoski, J. Laudon, K. Gharachorloo, W.-D. Weber, A. Gupta, J. Hennessy, M. Horowitz, and M. S. Lam. </author> <title> The Stanford Dash Multiprocessor. </title> <journal> Computer, </journal> <volume> 25(3) </volume> <pages> 63-79, </pages> <month> March </month> <year> 1992. </year>
Reference-contexts: The processors communicate by passing messages through a network. The DSM system is responsible for providing consistency between local copies of shared memory. The DSM system converts shared memory modifications to network messages. DSM systems have been implemented both in hardware <ref> [15] </ref> and in software [2, 3, 10, 16]. Software DSM systems are particularly interesting because they can be used on either specialized networks or on commodity networks such as Ethernet [17] and ATM [14]. Using a software DSM, a network of workstations can be turned into a parallel processing environment.
Reference: [16] <author> K. Li and P. Hudak. </author> <title> Memory Coherence in Shared Virtual Memory Systems. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 7(4) </volume> <pages> 321-359, </pages> <month> November </month> <year> 1989. </year> <booktitle> Originally presented at the Fifth ACM Symposium on Principles of Distributed Computing, </booktitle> <month> August </month> <year> 1986. </year>
Reference-contexts: The processors communicate by passing messages through a network. The DSM system is responsible for providing consistency between local copies of shared memory. The DSM system converts shared memory modifications to network messages. DSM systems have been implemented both in hardware [15] and in software <ref> [2, 3, 10, 16] </ref>. Software DSM systems are particularly interesting because they can be used on either specialized networks or on commodity networks such as Ethernet [17] and ATM [14]. Using a software DSM, a network of workstations can be turned into a parallel processing environment.
Reference: [17] <author> R. M. Metcalfe and D. R. Boggs. </author> <title> Ethernet: Distributed Packet Switching for Local Computer Networks. </title> <journal> Communications of the ACM, </journal> <volume> 19(7) </volume> <pages> 395-403, </pages> <month> July </month> <year> 1976. </year>
Reference-contexts: The DSM system converts shared memory modifications to network messages. DSM systems have been implemented both in hardware [15] and in software [2, 3, 10, 16]. Software DSM systems are particularly interesting because they can be used on either specialized networks or on commodity networks such as Ethernet <ref> [17] </ref> and ATM [14]. Using a software DSM, a network of workstations can be turned into a parallel processing environment. Programs designed for tightly-coupled, 2 bus-based architectures can be run with little modification. Programmer productivity is increased through the use of a familiar model: shared memory.
Reference: [18] <author> N. Neves, M. Castro, and P. Guedes. </author> <title> A Checkpoint Protocol for an Entry Consistent Shared Memory System. </title> <booktitle> In Proceedings of the Thirteenth ACM Symposium on Principles of Distributed Computing, </booktitle> <address> Los Angeles, CA, </address> <month> August </month> <year> 1994. </year>
Reference-contexts: Lazy release consistency DSM systems invalidate pages only as result of an acquire on a synchronization object. Their LRC protocol logs the page the first time it is read and the list of invalidated pages at an acquire. Neves et al <ref> [18] </ref> describe fault-tolerant protocol for entry consistency DSM systems using only a volatile log. Before releasing an object, a process copies it into the volatile log.
Reference: [19] <author> G. G. Richard III and M. Singhal. </author> <title> Using logging and Asynchronous Checkpointing to Implement Recoverable Distributed Shared Memory. </title> <booktitle> In Proceedings of the 12th Symposium on Reliable Distributed Systems, </booktitle> <address> Princeton, NJ, </address> <month> October </month> <year> 1993. </year>
Reference-contexts: In a single uniprocessor, to recover from failure, the failed process must be restarted either on the same node, in the case of transient failure, or on another node. In a shared memory system, recovery must also include the DSM. Richard and Singhal <ref> [19] </ref> describe a fault-tolerant checkpointing and logging method for sequential consistency DSM systems. They use a volatile in-memory log and a stable disk log. Before reading a shared page, the processor makes a copy of the page in the volatile log.
Reference: [20] <author> M. Satyanarayanan, H. M. Mashburn, P. Kumar, D. C. Steele, and J. J. Kistler. </author> <title> Lightweight Recoverable Virtual Memory. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 12(1) </volume> <pages> 33-57, </pages> <institution> Carnegie Mellon University, </institution> <month> February </month> <year> 1994. </year>
Reference-contexts: For the three networks in Figure 6, fault-tolerant peer logging shows better speedup than disk logging. 7 Related Work Fault-tolerant DSM systems are related to work on transactional virtual memory systems. Transactional VM systems, including Camelot [22], PLinda [8], RVM <ref> [20] </ref> and LDSM [5], add transaction semantics to persistent virtual memory. A transaction consists of a start operation, a number of modifications to shared memory and either a commit or an abort operation.
Reference: [21] <author> J. P. Singh, W.-D. Weber, and A. Gupta. </author> <title> SPLASH: Stanford Parallel Applications for Shared-Memory. </title> <journal> ACM SIGARCH Computer Architecture News, </journal> <volume> 20(1) </volume> <pages> 5-44, </pages> <month> March </month> <year> 1992. </year>
Reference-contexts: Our test application suite consists of four programs: appbt, em3d, sor and water. Appbt is from the NAS parallel benchmark suite [1]. Em3d is from the Split-C [4] project at UC Berkeley. Sor is a local computation kernel. Water is part of the SPLASH suite <ref> [21] </ref>. Appbt calculates approximate solutions to Navier-Stokes differential equations modeling heat dissipation. Appbt uses linear arrays to represent higher dimensional data structures. Each processor is assigned a separate square area. Communication between processors occurs only on boundaries between the areas. We simulate 5 time steps over 4096 grid points.
Reference: [22] <author> A. Z. Spector, J. J. Bloch, D. S. Daniels, R. P. Draves, J. L. Eppinger, S. G. Menees, and D. S. Thompson. </author> <title> The Camelot Project. </title> <institution> CMU-CS-86-166, Computer Science Department, Carnegie-Mellon University, </institution> <month> November </month> <year> 1986. </year>
Reference-contexts: For the three networks in Figure 6, fault-tolerant peer logging shows better speedup than disk logging. 7 Related Work Fault-tolerant DSM systems are related to work on transactional virtual memory systems. Transactional VM systems, including Camelot <ref> [22] </ref>, PLinda [8], RVM [20] and LDSM [5], add transaction semantics to persistent virtual memory. A transaction consists of a start operation, a number of modifications to shared memory and either a commit or an abort operation.
Reference: [23] <author> G. Suri, B. Janssens, and W. K. Fuchs. </author> <title> Reduced Overhead Logging for Rollback Recovery in Distributed Shared Memory. </title> <booktitle> In Proceedings of the 25th International Symposium on Fault-Tolerant Computing, </booktitle> <month> June </month> <year> 1995. </year>
Reference-contexts: To start recovery, the program loads the lastest checkpoint and begins replay. Whenever the recovering process needs a remote page, it retrieves the page from the stable log. By using logged pages, the process sees exactly the same data that it saw before the failure. Suri et al <ref> [23] </ref> improve on the work of Richard and Singhal. For sequential consistency, they log the contents of the page only on the first read rather than logging a page each time it is read.
Reference: [24] <author> J. E. Veenstra. </author> <title> MINT Tutorial and User Manual. </title> <type> TR 452, </type> <institution> Computer Science Department, University of Rochester, </institution> <month> July </month> <year> 1993. </year>
Reference-contexts: Process recovery completes when all of the log entries have been replayed. 5 Experimental Methodology Our experimental results were measured using execution driven simulations. Our simulator consists of two parts, a MINT <ref> [24, 25] </ref> front end and a memory system back end. MINT runs native parallel applications using the MIPS II instruction set. Memory references are passed to the memory system back end which determines which operations continue and which wait. This information is fed back to the MINT front end.
Reference: [25] <author> J. E. Veenstra and R. J. Fowler. MINT: </author> <title> A Front End for Efficient Simulation of Shared-Memory Multiprocessors. </title> <booktitle> In Proceedings of the Second International Workshop on Modeling, Analysis and Simulation of Computer and Telecommunication Systems (MASCOTS '94), </booktitle> <pages> pages 201-207, </pages> <address> Durham, NC, </address> <month> January - February </month> <year> 1994. </year>
Reference-contexts: Process recovery completes when all of the log entries have been replayed. 5 Experimental Methodology Our experimental results were measured using execution driven simulations. Our simulator consists of two parts, a MINT <ref> [24, 25] </ref> front end and a memory system back end. MINT runs native parallel applications using the MIPS II instruction set. Memory references are passed to the memory system back end which determines which operations continue and which wait. This information is fed back to the MINT front end.
References-found: 25

