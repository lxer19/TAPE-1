URL: http://www.cc.gatech.edu/grads/b/Gary.N.Boone/papers/eff_rl.ps.Z
Refering-URL: http://www.cs.gatech.edu/grads/b/Gary.N.Boone/professional.html
Root-URL: 
Email: gboone@cc.gatech.edu  
Title: Efficient Reinforcement Learning: Model-Based Acrobot Control  
Author: Gary Boone 
Address: Atlanta, GA 30332  
Affiliation: College of Computing Georgia Institute of Technology  
Abstract: Several methods have been proposed in the reinforcement learning literature for learning optimal policies for sequential decision tasks. Q-learning is a model-free algorithm that has recently been applied to the Acrobot, a two-link arm with a single actuator at the elbow that learns to swing its free endpoint above a target height. However, applying Q-learning to a real Acrobot may be impractical due to the large number of required movements of the real robot as the controller learns. This paper explores the planning speed and data efficiency of explicitly learning models, as well as using heuristic knowledge to aid the search for solutions and reduce the amount of data required from the real robot. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Richard S. Sutton, </author> <title> "Generalization in reinforcement learn ing: Successful examples using sparse coarse coding", </title> <booktitle> in Neural Information Processing Systems 8, </booktitle> <address> Cambridge, MA, </address> <year> 1996. </year> <note> MIT Press, To appear. </note>
Reference: [2] <author> Richard S. Sutton, "DYNA, </author> <title> an Integrated Architecture for Learning, Planning and Reacting", </title> <booktitle> in Working Notes of the AAAI Spring Symposium on Integrated Intelligent Architectures, </booktitle> <month> Mar. </month> <year> 1991. </year>
Reference: [3] <author> Mark W. Spong, </author> <title> "The swing up control problem for the acrobot", </title> <journal> IEEE Control Systems Magazine, </journal> <volume> vol. 15, </volume> <pages> pp. 49-55, </pages> <month> Feb. </month> <year> 1995, </year> <note> Reprinted in Neurocomputing: Foundation of Research. </note>
Reference: [4] <author> N. Sadegh and B. Driessen, </author> <title> "Minimum time trajectory learning", </title> <booktitle> in Proceedings of the American Control Conference, </booktitle> <address> Seattle, WA, </address> <month> June </month> <year> 1995. </year>
Reference: [5] <author> Daniel E. Davison and Scott A. Bortoff, </author> <title> "Regulation of the acrobot", </title> <booktitle> in Proceedings of the 1995 IFAC Symposium on Nonlinear Control Systems Design, </booktitle> <pages> pp. 390-395, </pages> <address> Tahoe City, CA, </address> <month> June </month> <year> 1995. </year>
Reference: [6] <author> Michael Athans and Peter Falb, </author> <title> Optimal Control: An In troduction to the Theory and Its Applications, </title> <publisher> McGraw-Hill, </publisher> <address> New York, </address> <year> 1966. </year>
Reference: [7] <author> Christopher J. C. H. Watkins, </author> <title> Learning from Delayed Re wards, </title> <type> PhD thesis, </type> <institution> University of Cambridge, </institution> <year> 1989. </year>
Reference: [8] <author> G. A. Rummery and M. Niranjan, </author> <title> "On-line q-learning us ing connectionist systems", </title> <type> Technical Report CUED/F-INFENG/TR 166, </type> <institution> Cambridge University Engineering Department, </institution> <year> 1994. </year>
Reference: [9] <author> Richard Bellman, </author> <title> Dynamic Programming, </title> <publisher> Princeton Uni versity Press, </publisher> <address> Princeton, NJ, </address> <year> 1957. </year>
Reference: [10] <author> Stuart J. Russell and Peter Norvig, </author> <booktitle> Artificial Intelligence: </booktitle>
References-found: 10

