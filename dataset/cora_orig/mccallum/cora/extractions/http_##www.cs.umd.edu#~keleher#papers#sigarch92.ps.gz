URL: http://www.cs.umd.edu/~keleher/papers/sigarch92.ps.gz
Refering-URL: http://www.cs.umd.edu/~keleher/papers.html
Root-URL: 
Title: Lazy Release Consistency for Software Distributed Shared Memory  
Author: Pete Keleher, Alan L. Cox, and Willy Zwaenepoel 
Date: March 9, 1992  
Affiliation: Department of Computer Science Rice University  
Abstract: Relaxed memory consistency models, such as release consistency, were introduced in order to reduce the impact of remote memory access latency in both software and hardware distributed shared memory (DSM). However, in a software DSM, it is also important to reduce the number of messages and the amount of data exchanged for remote memory access. Lazy release consistency is a new algorithm for implementing release consistency that lazily pulls modifications across the interconnect only when necessary. Trace-driven simulation using the SPLASH benchmarks indicates that lazy release consistency reduces both the number of messages and the amount of data transferred between processors. These reductions are especially significant for programs that exhibit false sharing and make extensive use of locks. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> S. Adve and M. Hill. </author> <title> Weak ordering: A new definition. </title> <booktitle> In Proceedings of the 17th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 2-14, </pages> <month> May </month> <year> 1990. </year>
Reference-contexts: 1 Introduction Over the past few years, researchers in hardware distributed shared memory (DSM) have proposed relaxed memory consistency models to reduce the latency associated with remote memory accesses <ref> [1, 7, 8, 9, 13] </ref>. For instance, in release consistency (RC) [8], writes to shared memory by processor p 1 need to be performed (become visible) at another processor p 2 only when a subsequent release of p 1 performs at p 2 .
Reference: [2] <author> S. V. Adve and M. D. Hill. </author> <title> A unified formalization of four shared-memory models. </title> <type> Technical Report CS-1051, </type> <institution> University of Wisconsin, Madison, </institution> <month> September </month> <year> 1991. </year>
Reference-contexts: At this time, the acquiring processor determines which modifications it needs to see according to the definition of RC. To do so, LRC uses a representation of the happened-before-1 partial order introduced by Adve and Hill <ref> [2] </ref>. The happened-before-1 partial order is a formalization of the "preceding" relation mentioned in Section 1. 4.1 The happened-before-1 Partial Order We summarize here the relevant aspects of the definitions of happened-before-1 [2]. <p> To do so, LRC uses a representation of the happened-before-1 partial order introduced by Adve and Hill <ref> [2] </ref>. The happened-before-1 partial order is a formalization of the "preceding" relation mentioned in Section 1. 4.1 The happened-before-1 Partial Order We summarize here the relevant aspects of the definitions of happened-before-1 [2].
Reference: [3] <author> Mustaque Ahamad, Phillip W. Hutto, and Ranjit John. </author> <title> Implementing and programming causal distributed shared memory. </title> <booktitle> In Proceedings of the 11th International Conference on Distributed Computing Systems, </booktitle> <pages> pages 274-281, </pages> <month> May </month> <year> 1991. </year>
Reference-contexts: Munin's implementation of release consistency merges updates at release time, rather than pipelining them, in order to reduce the number of messages transferred between processors. Munin uses multiple consistency protocols to further reduce the number of messages. Ahamad et al. defined a relaxed memory model called causal memory <ref> [3] </ref>. Causal memory differs from RC because conflicting pairs of ordinary memory accesses establish causal relationships. In contrast, under RC, only special memory accesses establish causal relationships. Entry consistency, defined by Bershad and Zekauskas [4], is another related relaxed memory model.
Reference: [4] <author> B.N. Bershad and M.J. Zekauskas. Midway: </author> <title> Shared memory parallel programming with entry consistency for distributed memory multiprocessors. </title> <type> Technical Report CMU-CS-91-170, </type> <institution> Carnegie-Mellon University, </institution> <month> September </month> <year> 1991. </year>
Reference-contexts: Ahamad et al. defined a relaxed memory model called causal memory [3]. Causal memory differs from RC because conflicting pairs of ordinary memory accesses establish causal relationships. In contrast, under RC, only special memory accesses establish causal relationships. Entry consistency, defined by Bershad and Zekauskas <ref> [4] </ref>, is another related relaxed memory model. EC differs from RC because it requires all shared data to be explicitly associated with some synchronization variable. As a result, when a processor acquires a synchronization variable, an EC implementation only needs to propagate the shared data associated with the synchronization variable.
Reference: [5] <author> J.B. Carter, J.K. Bennett, and W. Zwaenepoel. </author> <title> Implementation and performance of Munin. </title> <booktitle> In Proceedings of the 13th ACM Symposium on Operating Systems Principles, </booktitle> <pages> pages 152-164, </pages> <month> October </month> <year> 1991. </year>
Reference-contexts: Ideally, the number of messages exchanged in a software DSM should equal the number of messages exchanged in a message passing implementation of the same application. Therefore, Munin's write-shared protocol <ref> [5] </ref>, a software implementation of RC, buffers writes until a release, instead of pipelining them as in the DASH implementation. <p> RC implementations can delay the effects of shared memory accesses as long as they meet the constraints of Definition 2.1. 3 Eager Release Consistency We base our eager RC algorithm on Munin's write-shared protocol <ref> [5] </ref>. A processor delays propagating its modifications to shared data until it comes to a release. At that time, it propagates the modifications to all other processors that cache the modified pages. <p> Multiple processors can write to different parts of the same page concurrently, without intervening synchronization. This is in contrast to the exclusive-writer protocol used, for instance, in DASH [8], where a processor must obtain exclusive access to a cache line before it can be modified. Experience with Munin <ref> [5] </ref> indicates that multiple-writer protocols perform well in software DSMs, because they can handle false sharing without generating large amounts of message traffic. Given the large page sizes in software DSMs, false sharing is an important problem. <p> Release consistency was introduced by Gharachorloo et al. [8]. It is a refinement of weak consistency, defined by Dubois and Scheurich [7]. The DASH multiprocessor takes advantage of release consistency by pipelining remote memory accesses [11]. Pipelining reduces the impact of remote memory access latency on the processor. Munin <ref> [5] </ref> was the first software distributed shared memory system to use release consistency. Munin's implementation of release consistency merges updates at release time, rather than pipelining them, in order to reduce the number of messages transferred between processors. Munin uses multiple consistency protocols to further reduce the number of messages.
Reference: [6] <author> H. Davis, S. Goldschmidt, and J. L. Hennessy. </author> <title> Tango: A multiprocessor simulation and tracing system. </title> <type> Technical Report CSL-TR-90-439, </type> <institution> Stan-ford University, </institution> <year> 1990. </year>
Reference-contexts: We then relate the communication behavior to the shared mem ory access patterns of the application programs. 5.1 Methodology A trace was generated from a 32-processor execution of each program using the Tango multiprocessor simulator <ref> [6] </ref>. These traces were then fed into our protocol simulator. We simulated page sizes from 512 to 8192 bytes. We assume infinite caches and reliable FIFO commu nication channels.
Reference: [7] <author> M. Dubois and C. Scheurich. </author> <title> Memory access dependencies in shared-memory multiprocessors. </title> <journal> IEEE Transactions on Computers, </journal> <volume> 16(6) </volume> <pages> 660-673, </pages> <month> June </month> <year> 1990. </year> <month> 9 </month>
Reference-contexts: 1 Introduction Over the past few years, researchers in hardware distributed shared memory (DSM) have proposed relaxed memory consistency models to reduce the latency associated with remote memory accesses <ref> [1, 7, 8, 9, 13] </ref>. For instance, in release consistency (RC) [8], writes to shared memory by processor p 1 need to be performed (become visible) at another processor p 2 only when a subsequent release of p 1 performs at p 2 . <p> In addition, Clouds permits segments to be locked down at a single processor to prevent "ping-ponging". Release consistency was introduced by Gharachorloo et al. [8]. It is a refinement of weak consistency, defined by Dubois and Scheurich <ref> [7] </ref>. The DASH multiprocessor takes advantage of release consistency by pipelining remote memory accesses [11]. Pipelining reduces the impact of remote memory access latency on the processor. Munin [5] was the first software distributed shared memory system to use release consistency.
Reference: [8] <author> K. Gharachorloo, D. Lenoski, J. Laudon, P. Gib--bons, A. Gupta, and J. Hennessy. </author> <title> Memory consistency and event ordering in scalable shared-memory multiprocessors. </title> <booktitle> In Proceedings of the 17th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 15-26, </pages> <address> Seattle, Washing-ton, </address> <month> May </month> <year> 1990. </year>
Reference-contexts: 1 Introduction Over the past few years, researchers in hardware distributed shared memory (DSM) have proposed relaxed memory consistency models to reduce the latency associated with remote memory accesses <ref> [1, 7, 8, 9, 13] </ref>. For instance, in release consistency (RC) [8], writes to shared memory by processor p 1 need to be performed (become visible) at another processor p 2 only when a subsequent release of p 1 performs at p 2 . <p> 1 Introduction Over the past few years, researchers in hardware distributed shared memory (DSM) have proposed relaxed memory consistency models to reduce the latency associated with remote memory accesses [1, 7, 8, 9, 13]. For instance, in release consistency (RC) <ref> [8] </ref>, writes to shared memory by processor p 1 need to be performed (become visible) at another processor p 2 only when a subsequent release of p 1 performs at p 2 . <p> In Section 5, we describe a comparison through simulation of eager RC and LRC. We briefly discuss related work in Section 6, and we draw conclusions and explore avenues for further work in Section 7. 2 Release Consistency Release consistency (RC) <ref> [8] </ref> is a form of relaxed memory consistency that allows the effects of shared memory accesses to be delayed until certain specially labeled accesses occur. RC requires shared memory accesses to be labeled as either ordinary or special . <p> Reads are performed with respect to another processor when a write issued by that processor can no longer affect the value returned by the read. Accesses are performed when they are performed with respect to all processors in the system. Properly labeled programs <ref> [8] </ref> produce the same results on RC memory as they would on sequentially consistent memory [10]. <p> Multiple processors can write to different parts of the same page concurrently, without intervening synchronization. This is in contrast to the exclusive-writer protocol used, for instance, in DASH <ref> [8] </ref>, where a processor must obtain exclusive access to a cache line before it can be modified. Experience with Munin [5] indicates that multiple-writer protocols perform well in software DSMs, because they can handle false sharing without generating large amounts of message traffic. <p> Clouds [15] uses program-based segments rather than pages as the granularity of consistency. In addition, Clouds permits segments to be locked down at a single processor to prevent "ping-ponging". Release consistency was introduced by Gharachorloo et al. <ref> [8] </ref>. It is a refinement of weak consistency, defined by Dubois and Scheurich [7]. The DASH multiprocessor takes advantage of release consistency by pipelining remote memory accesses [11]. Pipelining reduces the impact of remote memory access latency on the processor.
Reference: [9] <author> J.R. Goodman. </author> <title> Cache consistency and sequential consistency. </title> <type> Technical Report Technical report no. 61, </type> <institution> SCI Committee, </institution> <month> March </month> <year> 1989. </year>
Reference-contexts: 1 Introduction Over the past few years, researchers in hardware distributed shared memory (DSM) have proposed relaxed memory consistency models to reduce the latency associated with remote memory accesses <ref> [1, 7, 8, 9, 13] </ref>. For instance, in release consistency (RC) [8], writes to shared memory by processor p 1 need to be performed (become visible) at another processor p 2 only when a subsequent release of p 1 performs at p 2 .
Reference: [10] <author> L. Lamport. </author> <title> How to make a multiprocessor computer that correctly executes multiprocess programs. </title> <journal> IEEE Transactions on Computers, </journal> <volume> C-28(9):690-691, </volume> <month> September </month> <year> 1979. </year>
Reference-contexts: Accesses are performed when they are performed with respect to all processors in the system. Properly labeled programs [8] produce the same results on RC memory as they would on sequentially consistent memory <ref> [10] </ref>. Informally, a program is properly labeled if there are "enough" accesses labeled as acquires or releases, such that, for all legal interleavings of accesses, each pair of conflicting ordinary accesses is separated by a release-acquire chain.
Reference: [11] <author> D. Lenoski, J. Laudon, K. Gharachorloo, A. Gupta, and J. Hennessy. </author> <title> The directory-based cache coherence protocol for the DASH multiprocessor. </title> <booktitle> In Proceedings of the 17th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 148-159, </pages> <month> May </month> <year> 1990. </year>
Reference-contexts: This relaxation of the memory consistency model allows the DASH implementation of RC <ref> [11] </ref> to combat memory latency by pipelining writes to shared memory (see Figure 1). The processor is stalled only when executing a release, at which time it must wait for all its previous writes to perform. 0 This work is supported in part by NSF Grant No. <p> Release consistency was introduced by Gharachorloo et al. [8]. It is a refinement of weak consistency, defined by Dubois and Scheurich [7]. The DASH multiprocessor takes advantage of release consistency by pipelining remote memory accesses <ref> [11] </ref>. Pipelining reduces the impact of remote memory access latency on the processor. Munin [5] was the first software distributed shared memory system to use release consistency.
Reference: [12] <author> K. Li and P. Hudak. </author> <title> Memory coherence in shared virtual memory systems. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 7(4) </volume> <pages> 321-359, </pages> <month> November </month> <year> 1989. </year>
Reference-contexts: Lazy protocols eliminate this communication, because processors that falsely share data are unlikely to be causally related. This observa tion is consistent with the results of our simulations. 8 6 Related Work Ivy <ref> [12] </ref> was the first page-based distributed shared memory system. The shared memory implemented by Ivy is sequentially consistent, and does not allow multiple writers. Clouds [15] uses program-based segments rather than pages as the granularity of consistency.
Reference: [13] <author> R.J. Lipton and J.S. Sandberg. </author> <title> Pram: A scalable shared memory. </title> <type> Technical Report CS-TR-180-88, </type> <institution> Princeton University, </institution> <month> September </month> <year> 1988. </year>
Reference-contexts: 1 Introduction Over the past few years, researchers in hardware distributed shared memory (DSM) have proposed relaxed memory consistency models to reduce the latency associated with remote memory accesses <ref> [1, 7, 8, 9, 13] </ref>. For instance, in release consistency (RC) [8], writes to shared memory by processor p 1 need to be performed (become visible) at another processor p 2 only when a subsequent release of p 1 performs at p 2 .
Reference: [14] <author> F. Mattern. </author> <title> Virtual time and global states of distributed systems. </title> <editor> In Michel Cosnard, Yves Robert, Patrice Quinton, and Michel Raynal, editors, </editor> <booktitle> Parallel & Distributed Algorithms, </booktitle> <pages> pages 215-226. </pages> <publisher> El-sevier Science Publishers, </publisher> <address> Amsterdam, </address> <year> 1989. </year>
Reference-contexts: An interval is said to be performed at a processor if all modifications made during that interval have been performed at that processor. Let V p (i) be the vector timestamp <ref> [14] </ref> for interval i of processor p. The number of elements in the vector V p (i) is equal to the number of processors. The entry for processor p in V p (i) is equal to i.
Reference: [15] <author> U. Ramachandran, M. Ahamad, and Y.A. Kha-lidi. </author> <title> Unifying synchronization and data transfer in maintaining coherence of distributed shared memory. </title> <type> Technical Report GIT-CS-88/23, </type> <institution> Georgia Institute of Technology, </institution> <month> June </month> <year> 1988. </year>
Reference-contexts: This observa tion is consistent with the results of our simulations. 8 6 Related Work Ivy [12] was the first page-based distributed shared memory system. The shared memory implemented by Ivy is sequentially consistent, and does not allow multiple writers. Clouds <ref> [15] </ref> uses program-based segments rather than pages as the granularity of consistency. In addition, Clouds permits segments to be locked down at a single processor to prevent "ping-ponging". Release consistency was introduced by Gharachorloo et al. [8]. It is a refinement of weak consistency, defined by Dubois and Scheurich [7].
Reference: [16] <author> J.P. Singh, W.-D. Weber, and A. Gupta. </author> <title> Splash: Stanford parallel applications for shared-memory. </title> <type> Technical Report CSL-TR-91-469, </type> <institution> Stanford University, </institution> <month> April </month> <year> 1991. </year>
Reference-contexts: The happened-before-1 partial order specifies the order in which the diffs need to be applied. This optimization reduces the amount of data sent. 5 Simulation We present the results of a simulation study based on multiprocessor traces of five shared-memory application programs from the SPLASH suite <ref> [16] </ref>. We measured the number of messages and the amount of data exchanged by each program for an execution using each of four protocols: lazy update (LU), lazy invalidate (LI), eager update (EU), and eager invalidate (EI).
Reference: [17] <author> W.-D. Weber and A. Gupta. </author> <title> Analysis of cache invalidation patterns in multiprocessors. </title> <booktitle> In Proceedings of the 3th Symposium on Architectural Support' for Programming Languages and Operating Systems, </booktitle> <pages> pages 243-256, </pages> <month> April </month> <year> 1989. </year> <month> 10 </month>
Reference-contexts: Work is allocated to processors a wire at a time. Synchronization is accomplished almost entirely through locks that protect access to a central task queue. Data movement in LocusRoute is largely migratory <ref> [17] </ref>: locks dominate the synchronization, and data moves according to lock accesses. As page size increases, false sharing also becomes important. Both of these factors favor lazy protocols. Figures 5 and 6 show LocusRoute's performance.
References-found: 17

