URL: http://c.gp.cs.cmu.edu:5103/afs/cs.cmu.edu/project/scandal/public/papers/icfp96-nesl.ps.gz
Refering-URL: http://c.gp.cs.cmu.edu:5103/afs/cs.cmu.edu/project/scandal/public/papers/icfp96-nesl.html
Root-URL: http://www.cs.cmu.edu
Email: fblelloch,jdgg@cs.cmu.edu  
Title: A Provable Time and Space Efficient Implementation of NESL  
Author: Guy E. Blelloch and John Greiner 
Affiliation: Carnegie Mellon University  
Abstract: In this paper we prove time and space bounds for the implementation of the programming language Nesl on various parallel machine models. Nesl is a sugared typed -calculus with a set of array primitives and an explicit parallel map over arrays. Our results extend previous work on provable implementation bounds for functional languages by considering space and by including arrays. For modeling the cost of Nesl we augment a standard call-by-value operational semantics to return two cost measures: a DAG representing the sequential dependences in the computation, and a measure of the space taken by a sequential implementation. We show that a Nesl program with w work (nodes in the DAG), d depth (levels in the DAG), and s sequential space can be implemented on a p processor butterfly network, hypercube, or CRCW PRAM using O(w=p + d log p) time and O(s + dp log p) reachable space. 1 For programs with sufficient parallelism these bounds are optimal in that they give linear speedup and use space within a constant factor of the sequential space. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> G. Blelloch, G. L. Miller, and D. Talmor. </author> <title> Developing a practical projection-based parallel Delaunay algorithm. </title> <booktitle> In Proceedings ACM Symposium on Computational Geometry, </booktitle> <month> May </month> <year> 1996. </year>
Reference-contexts: Nesl is a strongly typed call-by-value functional language loosely based on ML. It has been implemented on several parallel machines [8], and has been used both for teaching parallel algorithms [9, 7], and implementing various applications <ref> [17, 4, 1] </ref>. The parallelism in the language is based on including a primitive sequence data type, a parallel map operation, and a small set of primitive operations on sequences. <p> Nesl has a polymorphic function Quicksort (S) = if (#S &lt;= 1) then S else let a = S [#S/2]; S2 = fe in S| e == ag; R = fQuicksort (v): v in [S1,S3]g; in R [0] ++ S2 ++ R <ref> [1] </ref>; W ork = O (n log n) (expected) Depth = O (log n) (expected) Space = O (n) (expected) returns the length of a sequence. The expression fe in S| e &lt; ag subselects all elements of S less than a in parallel.
Reference: [2] <author> Guy Blelloch, Phil Gibbons, and Yossi Matias. </author> <title> Provably efficient scheduling for languages with fine-grained parallelism. </title> <booktitle> In ACM Symposium on Parallel Algorithms and Architectures, </booktitle> <month> July </month> <year> 1995. </year>
Reference-contexts: These bounds show that for programs with sufficient parallelism, the parallel execution requires very little extra memory beyond a standard call-by-value sequential execution. These space bounds use recent results on DAG scheduling <ref> [2] </ref> and are non trivial. Although we use these extensions to prove bounds for Nesl, the techniques and results can be applied in a broader context. <p> As mentioned, the idea behind the proof is to show that the P-CEK (q) executes a p-DFT traversal of the DAG g returned by the semantics, then use previous results on the number of nodes scheduled prematurely in a p-DFT <ref> [2] </ref>, and finally use these results to bound the space. By the machine traversing the DAG we mean there is a one-to-one correspondence between substate transitions and nodes in the DAG. <p> This can be implemented with a fetch-and-add. 2 To determine the total running time we use the result that a P-DFT with parameter q on a DAG with w nodes and d levels takes O (w=q + d) steps <ref> [2] </ref>. <p> None of this work, however, has considered the extra reachable space required by a parallel evaluation. There have been a sequence of studies that place space bounds on implementations of parallel languages <ref> [11, 10, 12, 2] </ref>. For a shared memory model, which is required to efficiently simulate the -calculus because of shared pointers, the best results are those by Blelloch, Gibbons, and Matias [2], which are the results we use in this paper. <p> There have been a sequence of studies that place space bounds on implementations of parallel languages [11, 10, 12, 2]. For a shared memory model, which is required to efficiently simulate the -calculus because of shared pointers, the best results are those by Blelloch, Gibbons, and Matias <ref> [2] </ref>, which are the results we use in this paper. Provable time bounds for mapping nested data-parallel languages onto the PRAM were considered by Blelloch [5] and in the definition of Nesl [6], but the time bounds are restricted to a class of program that are called contained. <p> In both cases performance relies heavily on effective use of the cache. Finally, our implementation uses a fetch-and-add operation. Although this is not very practical on many machines, it is likely that the need could be removed using the techniques discussed in <ref> [2] </ref>.
Reference: [3] <author> Guy Blelloch and John Greiner. </author> <title> Parallelism in sequential functional languages. </title> <booktitle> In Proceedings 7th International Conference on Functional Programming Languages and Computer Architecture, </booktitle> <pages> pages 226-237, </pages> <month> June </month> <year> 1995. </year>
Reference-contexts: The motivation is to assure that the costs of a program are well defined and to make guarantees about the performance of the implementation. In previous work we have studied provably time efficient parallel implementations of the -calculus using both call-by-value <ref> [3] </ref> and speculative parallelism [18]. These results accounted for work and depth of a computation using a profiling semantics [29, 30] and then related work and depth to running time on various machine models. This paper applies these ideas to the language Nesl and extends the work in two ways. <p> In particular we translate Nesl into a generic array language which could be used to express other array extensions, and the space bounds we derive can be applied with minor changes to most languages with fork-join style parallelism, including the call-by-value -calculus <ref> [3] </ref>. Cost model. To model time and space in the semantics, we augment a standard call-by-value operational semantics to return two cost measures. The first is a DAG which represents the sequential control dependences in the program. <p> Nesl does not execute the two expressions of a function application e 1 e 2 in parallel, as does the PAL model <ref> [3] </ref>. In the DAGs the ordering among the children of a node is important (it is needed for our space bounds) so our representation of DAGs keeps this information. <p> In this section we define the machine and in the next section we prove bounds on the time and space it requires as a function of the work, depth and space of the Core-Nesl semantics. The P-CEK (q) machine is motivated by the P-ECD machine <ref> [3] </ref>, which was used for proving bounds on the parallel implementation of the call-by-value -calculus (PAL model). However, because of the need to be space efficient and handle arrays, it has three important extensions. <p> The following theorem assumes that program expressions are of constant size and therefore does not include the time for looking up variables in the environment. It would be easy to generalize to account for variable lookup <ref> [3] </ref>. Theorem 4 A step of a P-CEK (p log p) machine can be simulated in O (log p) time on the butterfly, hypercube, or CRCW PRAM machine models. Proof: Each processor takes log p elements from the queue and executes the transition P =) 1 .
Reference: [4] <author> Guy Blelloch and Girija Narlikar. </author> <title> A comparison of two n-body algorithms. </title> <booktitle> In Dimacs implementation challenge workshop, </booktitle> <month> October </month> <year> 1994. </year>
Reference-contexts: Nesl is a strongly typed call-by-value functional language loosely based on ML. It has been implemented on several parallel machines [8], and has been used both for teaching parallel algorithms [9, 7], and implementing various applications <ref> [17, 4, 1] </ref>. The parallelism in the language is based on including a primitive sequence data type, a parallel map operation, and a small set of primitive operations on sequences.
Reference: [5] <author> Guy E. Blelloch. </author> <title> Vector Models for Data-Parallel Computing. </title> <publisher> MIT Press, </publisher> <year> 1990. </year>
Reference-contexts: More detail on our assumptions about the machines are given in Section 6. We note that the implementation discussed in this paper does not correspond to the current implementation of Nesl [8]. The current implementation is based on a technique called flattening nested parallelism <ref> [5] </ref>, which has very good performance characteristics, but can be space inefficient because it generates too much parallelism. <p> Provable time bounds for mapping nested data-parallel languages onto the PRAM were considered by Blelloch <ref> [5] </ref> and in the definition of Nesl [6], but the time bounds are restricted to a class of program that are called contained. Similar results were shown by Suciu and Tannen for a parallel language based on while loops and map recursion [32]. Practical issues.
Reference: [6] <author> Guy E. Blelloch. NESL: </author> <title> A nested data-parallel language (version 3.1). </title> <type> Technical Report CMU-CS-95-170, </type> <institution> Carnegie Mellon University, </institution> <year> 1995. </year>
Reference-contexts: 1 Introduction This paper presents a provably time and space efficient implementation of the parallel programming language Nesl <ref> [6] </ref>. Nesl is a strongly typed call-by-value functional language loosely based on ML. It has been implemented on several parallel machines [8], and has been used both for teaching parallel algorithms [9, 7], and implementing various applications [17, 4, 1]. <p> Figure 1 shows an example of Nesl code for the quicksort algorithm, along with the complexities that can be derived from these rules. The rules are defined somewhat informally in the original language definition <ref> [6] </ref> and are formalized in this paper. To simplify the presentation of this paper we consider a language Core-Nesl instead of the full Nesl. Core-Nesl includes a representative sample of the data types, constants, and primitive functions of Nesl. <p> Provable time bounds for mapping nested data-parallel languages onto the PRAM were considered by Blelloch [5] and in the definition of Nesl <ref> [6] </ref>, but the time bounds are restricted to a class of program that are called contained. Similar results were shown by Suciu and Tannen for a parallel language based on while loops and map recursion [32]. Practical issues.
Reference: [7] <author> Guy E. Blelloch. </author> <title> Programming parallel algorithms. </title> <journal> Communications of the ACM, </journal> <month> March </month> <year> 1996. </year>
Reference-contexts: 1 Introduction This paper presents a provably time and space efficient implementation of the parallel programming language Nesl [6]. Nesl is a strongly typed call-by-value functional language loosely based on ML. It has been implemented on several parallel machines [8], and has been used both for teaching parallel algorithms <ref> [9, 7] </ref>, and implementing various applications [17, 4, 1]. The parallelism in the language is based on including a primitive sequence data type, a parallel map operation, and a small set of primitive operations on sequences.
Reference: [8] <author> Guy E. Blelloch, Siddhartha Chatterjee, Jonathan C. Hardwick, Jay Sipelstein, and Marco Zagha. </author> <title> Implementation of a portable nested data-parallel language. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 21(1) </volume> <pages> 4-14, </pages> <month> April </month> <year> 1994. </year>
Reference-contexts: 1 Introduction This paper presents a provably time and space efficient implementation of the parallel programming language Nesl [6]. Nesl is a strongly typed call-by-value functional language loosely based on ML. It has been implemented on several parallel machines <ref> [8] </ref>, and has been used both for teaching parallel algorithms [9, 7], and implementing various applications [17, 4, 1]. The parallelism in the language is based on including a primitive sequence data type, a parallel map operation, and a small set of primitive operations on sequences. <p> Determining whether garbage collection can be performed within the specified bounds is an interesting area of future research. More detail on our assumptions about the machines are given in Section 6. We note that the implementation discussed in this paper does not correspond to the current implementation of Nesl <ref> [8] </ref>. The current implementation is based on a technique called flattening nested parallelism [5], which has very good performance characteristics, but can be space inefficient because it generates too much parallelism.
Reference: [9] <author> Guy E. Blelloch and Jonathan C. Hardwick. </author> <title> Class notes: Programming parallel algorithms. </title> <type> Technical Report CMU-CS-93-115, </type> <institution> Carnegie Mellon University, </institution> <month> February </month> <year> 1993. </year>
Reference-contexts: 1 Introduction This paper presents a provably time and space efficient implementation of the parallel programming language Nesl [6]. Nesl is a strongly typed call-by-value functional language loosely based on ML. It has been implemented on several parallel machines [8], and has been used both for teaching parallel algorithms <ref> [9, 7] </ref>, and implementing various applications [17, 4, 1]. The parallelism in the language is based on including a primitive sequence data type, a parallel map operation, and a small set of primitive operations on sequences.
Reference: [10] <author> R. D. Blumofe and C. E. Leiserson. </author> <title> Space-efficient scheduling of multithreaded computations. </title> <booktitle> In Proc. 25th ACM Symp. on Theory of Computing, </booktitle> <pages> pages 362-371, </pages> <month> May </month> <year> 1993. </year>
Reference-contexts: None of this work, however, has considered the extra reachable space required by a parallel evaluation. There have been a sequence of studies that place space bounds on implementations of parallel languages <ref> [11, 10, 12, 2] </ref>. For a shared memory model, which is required to efficiently simulate the -calculus because of shared pointers, the best results are those by Blelloch, Gibbons, and Matias [2], which are the results we use in this paper.
Reference: [11] <author> F. Warren Burton. </author> <title> Storage management in virtual tree machines. </title> <journal> IEEE Trans. on Computers, </journal> <volume> 37(3) </volume> <pages> 321-328, </pages> <year> 1988. </year>
Reference-contexts: None of this work, however, has considered the extra reachable space required by a parallel evaluation. There have been a sequence of studies that place space bounds on implementations of parallel languages <ref> [11, 10, 12, 2] </ref>. For a shared memory model, which is required to efficiently simulate the -calculus because of shared pointers, the best results are those by Blelloch, Gibbons, and Matias [2], which are the results we use in this paper.
Reference: [12] <author> F. Warren Burton and David J. Simpson. </author> <title> Space efficient execution of deterministic parallel programs. </title> <type> Manuscript, </type> <month> December </month> <year> 1994. </year>
Reference-contexts: None of this work, however, has considered the extra reachable space required by a parallel evaluation. There have been a sequence of studies that place space bounds on implementations of parallel languages <ref> [11, 10, 12, 2] </ref>. For a shared memory model, which is required to efficiently simulate the -calculus because of shared pointers, the best results are those by Blelloch, Gibbons, and Matias [2], which are the results we use in this paper.
Reference: [13] <author> Matthias Felleisen and Daniel P. Friedman. </author> <title> A calculus for assignments in higher-order languages. </title> <booktitle> In Proceedings 13th ACM Symposium on Principles of Programming Languages, </booktitle> <pages> pages 314-325, </pages> <month> January </month> <year> 1987. </year>
Reference-contexts: Picking an arbitrary set of q states would not be space efficient. The P-CEK (1) machine (q = 1) is a sequential machine and closely corresponds to the CESK machine <ref> [13] </ref>. Section 5 defines the P-CEK (q) machine. Results. Our results are derived, by mapping the Core-Nesl costs first to the array language, then to the P-CEK (q) machine and finally to the target machines. <p> Each state is processed by a transition similar to that of the CESK machine <ref> [13] </ref> (a variant of the SECD machine). The number of states grows as threads fork and shrinks as threads finish.
Reference: [14] <author> Cormac Flanagan and Mattias Felleisen. </author> <title> The semantics of future and its use in program optimization. </title> <booktitle> In Proceedings 22nd ACM Symposium on Principles of Programming Languages, </booktitle> <pages> pages 209-220, </pages> <year> 1995. </year>
Reference-contexts: There are w=q + d steps, and each step takes O (log p) time. 2 7 Related Work and Discussion Several researchers have used cost-augmented semantics for automatic time analysis or definitional purposes <ref> [29, 30, 31, 27, 33, 14] </ref>. Hudak and Anderson [21] used partially ordered multisets (pomsets) to model the dependences in various implementations of the -calculus. Because of the relationship between partially ordered sets and DAGs, these are quite similar in concept to our DAGS.
Reference: [15] <author> Joseph Gil and Yossi Matias. </author> <title> Fast and efficient simulations among CRCW PRAMs. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 23(2) </volume> <pages> 135-148, </pages> <month> November </month> <year> 1994. </year>
Reference-contexts: The stable fetch-and-add operation can be implemented in a butterfly or hypercube network by combining requests as they go through the network [26], and on a PRAM by various other techniques <ref> [24, 15] </ref>. For all machines, if each processor makes up to m fetch-and-add requests, all requests can be processed in O (m + log p) time with high probability (the bounds can be slightly improved on the CRCW PRAM [15]). <p> For all machines, if each processor makes up to m fetch-and-add requests, all requests can be processed in O (m + log p) time with high probability (the bounds can be slightly improved on the CRCW PRAM <ref> [15] </ref>). These bounds assume the butterfly has p log 2 p switches which can do the combining, the hypercube can communicate and combine over all wires simultaneously (multiport version), and that the CRCW PRAM is the priority write version.
Reference: [16] <author> Allan Gottlieb, B. D. Lubachevsky, and Larry Rudolph. </author> <title> Basic techniques for the efficient coordination of very large numbers of cooperating sequential processors. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 5(2), </volume> <month> April </month> <year> 1983. </year>
Reference-contexts: Our simulation uses the fetch-and-add operation <ref> [16] </ref> (or multiprefix [26]). In this operation, each processor has an address and an integer value i. In parallel all processors can atomically fetch the value from the address while incrementing the value by i.
Reference: [17] <author> John Greiner. </author> <title> A comparison of parallel algorithms for connected components. </title> <booktitle> In Proceedings 6th ACM Symposium on Parallel Algorithms and Architectures, </booktitle> <pages> pages 16-25, </pages> <month> June </month> <year> 1994. </year>
Reference-contexts: Nesl is a strongly typed call-by-value functional language loosely based on ML. It has been implemented on several parallel machines [8], and has been used both for teaching parallel algorithms [9, 7], and implementing various applications <ref> [17, 4, 1] </ref>. The parallelism in the language is based on including a primitive sequence data type, a parallel map operation, and a small set of primitive operations on sequences.
Reference: [18] <author> John Greiner and Guy E. Blelloch. </author> <title> A provably time-efficient parallel implementation of full speculation. </title> <booktitle> In Proceedings 23rd ACM Symposium on Principles of Programming Languages, </booktitle> <pages> pages 309-321, </pages> <month> January </month> <year> 1996. </year>
Reference-contexts: The motivation is to assure that the costs of a program are well defined and to make guarantees about the performance of the implementation. In previous work we have studied provably time efficient parallel implementations of the -calculus using both call-by-value [3] and speculative parallelism <ref> [18] </ref>. These results accounted for work and depth of a computation using a profiling semantics [29, 30] and then related work and depth to running time on various machine models. This paper applies these ideas to the language Nesl and extends the work in two ways.
Reference: [19] <author> High Performance Fortran Forum. </author> <title> High Performance Fortran Language Specification, </title> <month> May </month> <year> 1993. </year>
Reference-contexts: Since an expression can itself have parallel calls, this allows for the nesting of parallel calls. Such nested parallelism is crucial for expressing parallel divide-and-conquer or nested parallel loops (most data-parallel languages don't permit nesting <ref> [19, 28] </ref>). For the purpose of analyzing algorithms, the definition of Nesl includes rules for calculating the work and depth of a computation. These rules specify the work and depth for the primitives, and how these costs can be composed across expressions.
Reference: [20] <author> Paul Hudak. </author> <title> A semantic model of reference counting and its abstraction (detailed summary). </title> <booktitle> In Proceedings ACM Conference on LISP and Functional Programming, </booktitle> <pages> pages 351-363, </pages> <month> August </month> <year> 1986. </year>
Reference-contexts: There have been a handful of studies that use semantics to model the reachable space of sequential computations, in the context of both garbage collection (e.g., [25]) and copy avoidance (e.g., <ref> [20] </ref>). None of this work, however, has considered the extra reachable space required by a parallel evaluation. There have been a sequence of studies that place space bounds on implementations of parallel languages [11, 10, 12, 2].
Reference: [21] <author> Paul Hudak and Steve Anderson. </author> <title> Pomset interpretations of parallel functional programs. </title> <booktitle> In Proceedings 3rd International Conference on Functional Programming Languages and Computer Architecture, number 274 in Lecture Notes in Computer Science, </booktitle> <pages> pages 234-256. </pages> <publisher> Springer-Verlag, </publisher> <month> September </month> <year> 1987. </year>
Reference-contexts: Although the operational semantics itself is sequential, the rules for combining DAGs explicitly define what constructs are parallel. This is similar to Hudak and Anderson's <ref> [21] </ref> use of pomsets (partially ordered mul-tisets) to add intensional information on execution order to the denotational semantics of the -calculus. The second cost measure is an accounting of the reachable space used by a sequential implementation of the language. <p> There are w=q + d steps, and each step takes O (log p) time. 2 7 Related Work and Discussion Several researchers have used cost-augmented semantics for automatic time analysis or definitional purposes [29, 30, 31, 27, 33, 14]. Hudak and Anderson <ref> [21] </ref> used partially ordered multisets (pomsets) to model the dependences in various implementations of the -calculus. Because of the relationship between partially ordered sets and DAGs, these are quite similar in concept to our DAGS.
Reference: [22] <author> Joseph JaJa. </author> <title> An Introduction to Parallel Algorithms. </title> <publisher> Addison-Wesley, </publisher> <address> Reading, Mass., </address> <year> 1992. </year>
Reference-contexts: To be useful for analyzing parallel algorithms, Nesl was designed with rules for calculating the work (the total number of operations executed) and depth (the longest chain of sequential dependences) of a computation. These are standard cost measures in the analysis of parallel algorithms <ref> [23, 22] </ref>. In this paper we formalize these rules and give provable implementation bounds for both space and time. 1 The implementation is based on a randomized algorithm for the fetch-and-add operator and will therefore run within the given time with high probability.
Reference: [23] <author> R. M. Karp and V. Ramachandran. </author> <title> Parallel algorithms for shared memory machines. </title> <editor> In J. Van Leeuwen, editor, </editor> <booktitle> Handbook of Theoretical Computer Science | Volume A: Algorithms and Complexity. </booktitle> <publisher> MIT Press, </publisher> <address> Cam-bridge, Mass., </address> <year> 1990. </year>
Reference-contexts: To be useful for analyzing parallel algorithms, Nesl was designed with rules for calculating the work (the total number of operations executed) and depth (the longest chain of sequential dependences) of a computation. These are standard cost measures in the analysis of parallel algorithms <ref> [23, 22] </ref>. In this paper we formalize these rules and give provable implementation bounds for both space and time. 1 The implementation is based on a randomized algorithm for the fetch-and-add operator and will therefore run within the given time with high probability.
Reference: [24] <author> Yossi Matias and Uzi Vishkin. </author> <title> On parallel hashing and integer sorting. </title> <journal> Journal of Algorithms, </journal> <volume> 12(4) </volume> <pages> 573-606, </pages> <year> 1991. </year>
Reference-contexts: The stable fetch-and-add operation can be implemented in a butterfly or hypercube network by combining requests as they go through the network [26], and on a PRAM by various other techniques <ref> [24, 15] </ref>. For all machines, if each processor makes up to m fetch-and-add requests, all requests can be processed in O (m + log p) time with high probability (the bounds can be slightly improved on the CRCW PRAM [15]).
Reference: [25] <author> Greg Morrisett, Matthias Felleisen, and Robert Harper. </author> <title> Abstract models of memory management. </title> <booktitle> In Proceedings of the Symposium on Functional Programming and Computer Architecture, </booktitle> <pages> pages 66-77, </pages> <month> June </month> <year> 1995. </year>
Reference-contexts: None of the above work includes costs that model space or relates the costs of the modeled language to those in machine models. There have been a handful of studies that use semantics to model the reachable space of sequential computations, in the context of both garbage collection (e.g., <ref> [25] </ref>) and copy avoidance (e.g., [20]). None of this work, however, has considered the extra reachable space required by a parallel evaluation. There have been a sequence of studies that place space bounds on implementations of parallel languages [11, 10, 12, 2].
Reference: [26] <author> Abhiram G. Ranade. </author> <title> Fluent Parallel Computation. </title> <type> PhD thesis, </type> <institution> Yale University, </institution> <address> New Haven, CT, </address> <year> 1989. </year>
Reference-contexts: Our simulation uses the fetch-and-add operation [16] (or multiprefix <ref> [26] </ref>). In this operation, each processor has an address and an integer value i. In parallel all processors can atomically fetch the value from the address while incrementing the value by i. <p> The stable fetch-and-add operation can be implemented in a butterfly or hypercube network by combining requests as they go through the network <ref> [26] </ref>, and on a PRAM by various other techniques [24, 15]. For all machines, if each processor makes up to m fetch-and-add requests, all requests can be processed in O (m + log p) time with high probability (the bounds can be slightly improved on the CRCW PRAM [15]).
Reference: [27] <author> Paul Roe. </author> <title> Parallel Programming using Functional Languages. </title> <type> PhD thesis, </type> <institution> Department of Computing Science, University of Glasgow, </institution> <month> February </month> <year> 1991. </year>
Reference-contexts: There are w=q + d steps, and each step takes O (log p) time. 2 7 Related Work and Discussion Several researchers have used cost-augmented semantics for automatic time analysis or definitional purposes <ref> [29, 30, 31, 27, 33, 14] </ref>. Hudak and Anderson [21] used partially ordered multisets (pomsets) to model the dependences in various implementations of the -calculus. Because of the relationship between partially ordered sets and DAGs, these are quite similar in concept to our DAGS.
Reference: [28] <author> J. R. Rose and G. L. Steele Jr. </author> <title> C*: An extended C language for data parallel programming. </title> <booktitle> In Proceedings Second International Conference on Supercomputing, </booktitle> <volume> Vol. 2, </volume> <pages> pages 2-16, </pages> <month> May </month> <year> 1987. </year>
Reference-contexts: Since an expression can itself have parallel calls, this allows for the nesting of parallel calls. Such nested parallelism is crucial for expressing parallel divide-and-conquer or nested parallel loops (most data-parallel languages don't permit nesting <ref> [19, 28] </ref>). For the purpose of analyzing algorithms, the definition of Nesl includes rules for calculating the work and depth of a computation. These rules specify the work and depth for the primitives, and how these costs can be composed across expressions.
Reference: [29] <author> Mads Rosendahl. </author> <title> Automatic complexity analysis. </title> <booktitle> In Proceedings 4th International Conference on Functional Programming Languages and Computer Architecture. </booktitle> <publisher> Springer-Verlag, </publisher> <month> September </month> <year> 1989. </year>
Reference-contexts: In previous work we have studied provably time efficient parallel implementations of the -calculus using both call-by-value [3] and speculative parallelism [18]. These results accounted for work and depth of a computation using a profiling semantics <ref> [29, 30] </ref> and then related work and depth to running time on various machine models. This paper applies these ideas to the language Nesl and extends the work in two ways. <p> There are w=q + d steps, and each step takes O (log p) time. 2 7 Related Work and Discussion Several researchers have used cost-augmented semantics for automatic time analysis or definitional purposes <ref> [29, 30, 31, 27, 33, 14] </ref>. Hudak and Anderson [21] used partially ordered multisets (pomsets) to model the dependences in various implementations of the -calculus. Because of the relationship between partially ordered sets and DAGs, these are quite similar in concept to our DAGS.
Reference: [30] <author> David Sands. </author> <title> Calculi for Time Analysis of Functional Programs. </title> <type> PhD thesis, </type> <institution> University of London, Imperial College, </institution> <month> September </month> <year> 1990. </year>
Reference-contexts: In previous work we have studied provably time efficient parallel implementations of the -calculus using both call-by-value [3] and speculative parallelism [18]. These results accounted for work and depth of a computation using a profiling semantics <ref> [29, 30] </ref> and then related work and depth to running time on various machine models. This paper applies these ideas to the language Nesl and extends the work in two ways. <p> There are w=q + d steps, and each step takes O (log p) time. 2 7 Related Work and Discussion Several researchers have used cost-augmented semantics for automatic time analysis or definitional purposes <ref> [29, 30, 31, 27, 33, 14] </ref>. Hudak and Anderson [21] used partially ordered multisets (pomsets) to model the dependences in various implementations of the -calculus. Because of the relationship between partially ordered sets and DAGs, these are quite similar in concept to our DAGS.
Reference: [31] <author> David B. Skillicorn and W. Cai. </author> <title> A cost calculus for parallel functional programming. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 28(1) </volume> <pages> 65-83, </pages> <month> July </month> <year> 1995. </year>
Reference-contexts: There are w=q + d steps, and each step takes O (log p) time. 2 7 Related Work and Discussion Several researchers have used cost-augmented semantics for automatic time analysis or definitional purposes <ref> [29, 30, 31, 27, 33, 14] </ref>. Hudak and Anderson [21] used partially ordered multisets (pomsets) to model the dependences in various implementations of the -calculus. Because of the relationship between partially ordered sets and DAGs, these are quite similar in concept to our DAGS.
Reference: [32] <author> Dan Suciu and Val Tannen. </author> <title> Efficient compilation of high-level data parallel algorithms. </title> <booktitle> In Proceedings 6th ACM Symposium on Parallel Algorithms and Architectures, </booktitle> <pages> pages 57-66, </pages> <month> June </month> <year> 1994. </year>
Reference-contexts: Similar results were shown by Suciu and Tannen for a parallel language based on while loops and map recursion <ref> [32] </ref>. Practical issues. The design of the intermediate language and machine were optimized to simplify the proofs rather than for practical considerations. Here we briefly discuss some modifications to make the implementation more prac-tical without affecting the asymptotic bounds (although they would complicate the analysis).
Reference: [33] <author> Wolf Zimmermann. </author> <title> Complexity issues in the design of functional languages with explicit parallelism. </title> <booktitle> In Proceedings International Conference on Computer Languages, </booktitle> <pages> pages 34-43, </pages> <year> 1992. </year>
Reference-contexts: There are w=q + d steps, and each step takes O (log p) time. 2 7 Related Work and Discussion Several researchers have used cost-augmented semantics for automatic time analysis or definitional purposes <ref> [29, 30, 31, 27, 33, 14] </ref>. Hudak and Anderson [21] used partially ordered multisets (pomsets) to model the dependences in various implementations of the -calculus. Because of the relationship between partially ordered sets and DAGs, these are quite similar in concept to our DAGS.
References-found: 33

