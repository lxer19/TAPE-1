URL: http://www.cs.rutgers.edu/hpcd/Area_III.3/all_ps_files/jpdc_tao_apo.ps
Refering-URL: http://www.cs.rutgers.edu/hpcd/Area_III.3/all_html_files/papers.html
Root-URL: http://www.cs.rutgers.edu
Email: fgerasoulis, jjiaog@cs.rutgers.edu  tyang@cs.ucsb.edu  
Title: SCHEDULING OF REGULAR AND IRREGULAR COMPUTATIONS: A System and Applications  
Author: Apostolos Gerasoulis, Jia Jiao Tao Yang 
Date: September 16, 1995  
Address: New Brunswick, NJ 08903  Santa Barbara, CA 93106  
Affiliation: Department of Computer Science Rutgers University  Department of Computer Science University of California  
Abstract: We study the performance of task graph scheduling for both regular and irregular parallel computation. We present a scheduling tool named PYRROS, which takes as an input a task graph and a compile time estimation of computation and communication weights. It then produces a schedule and a parallel code based on this schedule. We analyze the effect of compile time errors to run time scheduling performance. For coarse grain task graphs small errors at compile time have a minimal impact in run time performance. We use PYRROS to produce parallel code for Gauss Jordan(GJ) and Gaussian Elimination(GE), sparse matrix computation and Fast Multipole method for irregular n-body simulations. The performance for GJ and GE on dense matrix computation is comparable to optimized hand written codes. For irregular problems such as n-body, scheduling overhead could become significant, but because of the iterative nature of such problems the overhead can be amortized over many iterations. And the overall performance with automatic scheduling is better than optimized manually derived schedules with no overhead.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> F. L. Alvarado, </author> <title> The sparse matrix manipulation system users manual, </title> <institution> University of Wisconsin, </institution> <month> Oct. </month> <year> 1990. </year>
Reference-contexts: Figure 11 shows the speedup of PYRROS for a test data matrix of size 1084 from Alvarado's sparse matrix manipulation system <ref> [1] </ref>. The PYRROS compile predicted performance is close to PYRROS run time performance. PYRROS achieves a maximum speedup of about 7 and the speedup upper bound is less than 11 for this small sized matrix.
Reference: [2] <author> J. Barnes and P. Hut, </author> <title> A hierarchical O(N log N ) force calculation algorithm, </title> <journal> Nature Vol. </journal> <volume> 324 P. 446, </volume> <year> 1986. </year>
Reference-contexts: The direct summation for the above expressions costs O (N 2 ). Two fast sequential algorithms are available: Barnes-Hut <ref> [2] </ref>, or Fast Multipole Method (FMM)[23]. Since FMM has a lower complexity, we consider the 2D FMM for computing F j .
Reference: [3] <author> S. Bokhari, </author> <title> On the mapping problem, </title> <journal> IEEE Trans. Comput. </journal> <volume> C-30 3(1981), </volume> <pages> 207-214. </pages>
Reference-contexts: Currently PYRROS uses a heuristic algorithm based on <ref> [3] </ref>. This algorithm starts from an initial assignment, then performs a series of pairwise interchanges so that F (CC) reduces monotonically.
Reference: [4] <author> D. Callahan, and K. Kennedy, </author> <title> Compiling Programs for Distributed-memory Multi-processors, </title> <journal> Journal of Supercomputing, </journal> <volume> Vol. 2, </volume> <year> 1988, </year> <pages> pp. 151-169. </pages>
Reference: [5] <author> D. Y. Cheng and S. Ranka, </author> <title> An optimization approach for static scheduling of directed-acyclic graphs on distributed memory multiprocessors, </title> <type> Report, </type> <institution> Syracuse Univ, </institution> <year> 1992, </year>
Reference-contexts: Task graph scheduling can effectively balance computational loads and reduce unnecessary communication. Finding optimal scheduling solutions is only possible for a small class of task graphs and in general the problem is NP-hard. Many scheduling heuristic algorithms have been proposed in the literature, e.g. <ref> [5, 9, 15, 20, 28, 41, 44, 34] </ref>. However, only few fl The work presented here was in part supported by ARPA contract DABT-63-93-C-0064 under "Hypercomputing and Design" project, by the Office of Naval research under grant N00014-93-1-0944, by NSF RIA CCR-9409695.
Reference: [6] <author> F. T. Chong, Shamik D. Sharma, Eric A. Brewer and Joel Saltz, </author> <title> Multiprocessor Runtime Support for Fine-Grained Irregular DAGs, </title> <type> Draft, </type> <year> 1994. </year>
Reference-contexts: Using the PYRROS algorithms other researchers have demonstrated performance improvements of up 75% over schedules based on block and wrap mapping for sparse matrix computation with fine grain partitions. But the overhead for fine grain computation is high and it needs to be amortized over many iterations, <ref> [6] </ref>. These results clearly demonstrate that scheduling provides benefits in practice as long as the overhead is kept low. The paper is organized as follows. Section 2 discusses the macro-dataflow task graph model for representing parallel computations and the organization of PYRROS. Section 3 summarizes the PYRROS scheduling algorithms. <p> To achieve higher speedups one needs to use finer grain partitionings and faster communication speed. But then the scheduling cost could increase in relation to computation and needs to be amortized over many iterations. Chong et al. <ref> [6] </ref> compared PYRROS scheduling and the block and cyclic mappings for solving sparse triangular linear systems. <p> We were able to provide such partitions for the LU and GJ examples for dense matrices and the n-body problem without reducing parallelism. Applying scheduling techniques for sparse matrix computation is still a challenging problem. The authors in <ref> [6, 7] </ref> demonstrated dramatic increases in the speedup by exploring finer grain partitions for sparse triangular matrix solvers and then applying scheduling techniques, but the overhead also increases and 28 need to be amortized over many iterations.
Reference: [7] <author> F. T. Chong and R. Schreiber, </author> <title> Parallel sparse triangular solution with partitioned inverses and prescheduled DAGs, </title> <type> Tech Report, </type> <year> 1994. </year> <booktitle> Appeared in Proc. of 1st IPPS Workshop on Solving Irregular Problems, </booktitle> <address> Santa Barbara, </address> <year> 1995. </year>
Reference-contexts: We were able to provide such partitions for the LU and GJ examples for dense matrices and the n-body problem without reducing parallelism. Applying scheduling techniques for sparse matrix computation is still a challenging problem. The authors in <ref> [6, 7] </ref> demonstrated dramatic increases in the speedup by exploring finer grain partitions for sparse triangular matrix solvers and then applying scheduling techniques, but the overhead also increases and 28 need to be amortized over many iterations.
Reference: [8] <author> M. Cosnard and M. Loi, </author> <title> Automatic Task Graph Generation Techniques, </title> <booktitle> Proc. of the Hawaii International Conference on System Sciences, IEEE, </booktitle> <volume> Vol II. </volume> <year> 1995, </year> <pages> pp. 113-122. </pages> <note> A revised version will appear in Parallel Processing Letters, a special issue on partitioning and scheduling. </note>
Reference-contexts: 1 Introduction There are two fundamental problems in automatic program parallelization. First the program partitioning and parallelism detection at the granularity level of the parallel machine, e.g. <ref> [8, 16, 42, 43] </ref>. And second the efficient execution of the detected parallelism. We focus on scheduling a class of parallel computation modeled as directed acyclic task graphs (DAGs). Task graph scheduling can effectively balance computational loads and reduce unnecessary communication. <p> L 1 k and U 1 for i = k + 1 to N k endg for j = k + 1 to N k : f A k;j = L 1 For i = k + 1 to N endg end 4 coarse grain task graphs are described in <ref> [8, 16] </ref>. The weights for the LU DAG are estimated as follows, assuming that ! is the cost for a multiplication and an addition.
Reference: [9] <author> Ph. Chretienne, </author> <title> Task Scheduling over Distributed Memory Machines, </title> <booktitle> Proc. of Inter. Workshop on Parallel and Distributed Algorithms, </booktitle> <publisher> (North Holland, Ed.), </publisher> <year> 1989. </year>
Reference-contexts: Task graph scheduling can effectively balance computational loads and reduce unnecessary communication. Finding optimal scheduling solutions is only possible for a small class of task graphs and in general the problem is NP-hard. Many scheduling heuristic algorithms have been proposed in the literature, e.g. <ref> [5, 9, 15, 20, 28, 41, 44, 34] </ref>. However, only few fl The work presented here was in part supported by ARPA contract DABT-63-93-C-0064 under "Hypercomputing and Design" project, by the Office of Naval research under grant N00014-93-1-0944, by NSF RIA CCR-9409695. <p> In [48], we show that DSC performs well for general DAGs by examining a set of randomly generated DAGs and also produces the optimal solutions for fork, join, coarse grain tree DAGs and a class of fine-grain trees. Chretienne <ref> [9] </ref> shows that the complexity of clustering is NP-complete for scheduling fine-grain tree DAGs and a DAG structure obtained by concatenating a fork and a join together.
Reference: [10] <author> E.G. Coffman and R.L. Graham, </author> <title> Optimal scheduling for two-processors systems, </title> <journal> Acta Informatica, </journal> <volume> 3 (1972), </volume> <pages> 200-213. </pages>
Reference-contexts: PYRROS will read this program and perform lexical and semantic analysis to generate an internal representation of the DAG. 5 3 Scheduling algorithms Scheduling parallel tasks with precedence relations over distributed memory multiprocessors is harder than the classical scheduling problem <ref> [10, 30] </ref>. Several heuristic algorithms have been proposed in the literature, e.g. [15, 31, 28, 41, 44]. These heuristics are based either on a multistage approach or one stage approach to scheduling [28, 41].
Reference: [11] <author> J. W. Cooley and J. W. Tukey, </author> <title> An algorithm for the machine calculation of complex Fourier series, </title> <journal> Mathematics of Computation, </journal> <volume> Vol. 19, No. 90, </volume> <year> 1965, </year> <pages> pp. 297-301. </pages>
Reference: [12] <author> M. Cosnard, M. Marrakchi, Y. Robert, and D. Trystram, </author> <title> Parallel Gaussian Elimination on an MIMD Computer, </title> <journal> Parallel Computing, </journal> <volume> vol. 6, </volume> <pages> pp. 275-296, </pages> <year> 1988. </year>
Reference: [13] <author> J.J. Dongarra, . and D. C. Sorensen, </author> <title> SCHEDULE: Tools for Developing and Analyzing Parallel Fortran Programs, in The Characteristics of Parallel Algorithms, D.B. </title> <editor> Gannon, L.H. Jamieson and R.J. Douglass (Eds), </editor> <publisher> MIT Press, </publisher> <year> 1987, </year> <note> pp363-394. 29 </note>
Reference-contexts: The content of the information herein does not necessarily reflect the position of the Government and official endorsement should not be inferred. 1 automatic scheduling systems have been developed. SCHEDULER by Dongarra and Sorensen <ref> [13] </ref> uses a centralized dynamic scheduling scheme. HYPERTOOL by Wu and Gajski [44] and TASKGRAPHER by El-Rewini and Lewis [15] use compile time scheduling algorithms but do not produce code for parallel machines.
Reference: [14] <author> T.H. Dunigan, </author> <title> Performance of the INTEL iPSC/860 and nCUBE 6400 hypercube, </title> <institution> ORNL/TM-11790, Oak Ridge National Lab., TN, </institution> <year> 1991. </year>
Reference: [15] <author> H. El-Rewini and T.G. Lewis, </author> <title> Scheduling parallel program tasks onto arbitrary target machines, </title> <journal> J. of Parallel and Distributed Computing, </journal> <volume> 9(1990), </volume> <pages> 138-153. </pages>
Reference-contexts: Task graph scheduling can effectively balance computational loads and reduce unnecessary communication. Finding optimal scheduling solutions is only possible for a small class of task graphs and in general the problem is NP-hard. Many scheduling heuristic algorithms have been proposed in the literature, e.g. <ref> [5, 9, 15, 20, 28, 41, 44, 34] </ref>. However, only few fl The work presented here was in part supported by ARPA contract DABT-63-93-C-0064 under "Hypercomputing and Design" project, by the Office of Naval research under grant N00014-93-1-0944, by NSF RIA CCR-9409695. <p> SCHEDULER by Dongarra and Sorensen [13] uses a centralized dynamic scheduling scheme. HYPERTOOL by Wu and Gajski [44] and TASKGRAPHER by El-Rewini and Lewis <ref> [15] </ref> use compile time scheduling algorithms but do not produce code for parallel machines. We present a programming tool called PYRROS which schedules tasks and produces parallel code for distributed memory architectures. PYRROS distinguishing feature is its low complexity algorithms which are competitive to existing higher complexity algorithms. <p> Several heuristic algorithms have been proposed in the literature, e.g. <ref> [15, 31, 28, 41, 44] </ref>. These heuristics are based either on a multistage approach or one stage approach to scheduling [28, 41]. PYRROS uses the following multistage approach where in each stage a heuristic with low complexity has been developed: (1) Assign tasks to a set of u clusters.
Reference: [16] <author> J. Feo, D. Cann, and R. Oldehoeft, </author> <title> A report on the Sisal language project, </title> <journal> J. of Parallel and Distributed Computing, </journal> <volume> 10(1990), </volume> <pages> pp. 349-365. </pages>
Reference-contexts: 1 Introduction There are two fundamental problems in automatic program parallelization. First the program partitioning and parallelism detection at the granularity level of the parallel machine, e.g. <ref> [8, 16, 42, 43] </ref>. And second the efficient execution of the detected parallelism. We focus on scheduling a class of parallel computation modeled as directed acyclic task graphs (DAGs). Task graph scheduling can effectively balance computational loads and reduce unnecessary communication. <p> L 1 k and U 1 for i = k + 1 to N k endg for j = k + 1 to N k : f A k;j = L 1 For i = k + 1 to N endg end 4 coarse grain task graphs are described in <ref> [8, 16] </ref>. The weights for the LU DAG are estimated as follows, assuming that ! is the cost for a multiplication and an addition.
Reference: [17] <author> A. George, , M.T. Heath, and J. Liu, </author> <title> Parallel Cholesky factorization on a shared memory processor, </title> <journal> Lin. Algebra Appl., </journal> <volume> 77(1986), </volume> <pages> pp. 165-187. </pages>
Reference-contexts: Sarkar [41] proposed a cluster merging algorithm with a cost O (pv (v + e)) which is time-consuming for a large graph. PYRROS uses a variation of work profiling method <ref> [17] </ref> for cluster merging. This method is simple and has been shown to work well in practice [18]. The complexity of this algorithm is O (u log u + v), which is 6 less than O (v log v). The algorithm is shown in Figure 6. 1.
Reference: [18] <author> A. Gerasoulis, I. Nelken. </author> <title> Static scheduling for linear algebra DAGs. </title> <booktitle> Proc. of 4th Conf. on Hypercubes, Monterey, </booktitle> <volume> Vol. 1, </volume> <year> 1989, </year> <pages> 671-674. </pages>
Reference-contexts: Sarkar [41] proposed a cluster merging algorithm with a cost O (pv (v + e)) which is time-consuming for a large graph. PYRROS uses a variation of work profiling method [17] for cluster merging. This method is simple and has been shown to work well in practice <ref> [18] </ref>. The complexity of this algorithm is O (u log u + v), which is 6 less than O (v log v). The algorithm is shown in Figure 6. 1. Compute the arithmetic load LM j for each cluster. 2.
Reference: [19] <author> A. Gerasoulis and T. Yang, </author> <title> On the granularity and clustering of directed acyclic task graphs, </title> <journal> IEEE Trans. on Parallel and Distributed Systems., </journal> <volume> Vol 4, No 6, </volume> <month> June </month> <year> 1993 </year> <month> pp. </month> <pages> 686-701. </pages>
Reference-contexts: At run-time, the weights of this graph change, we call the new graph G r . We examine the run-time performance of schedule S used for executing G r : The granularity, a ratio between computation and communication <ref> [19] </ref>, plays an important role in our study. <p> For coarse grain DAGs each task receives or sends a small amount of communication compared to that the computation of its adjacent tasks. For example, the granularity of Figure 1 (a) is 2 and it is a coarse grain graph. In <ref> [19] </ref>, we show that this definition captures the trade-off point between parallelization or sequentialization of parallel tasks to avoid to avoid inter-processor communication. We assume that at run-time, each communication weight c i;j is changed to c r i;j and the computation weight to t r i .
Reference: [20] <author> A. Gerasoulis and T. Yang, </author> <title> A comparison of clustering heuristics for scheduling DAGs on multiprocessors, </title> <journal> J. of Distributed and Parallel Computing, special issue on scheduling and load balancing, </journal> <volume> Vol. 16, No. 4, </volume> <pages> pp. </pages> <month> 276-291 (Dec. </month> <year> 1992). </year>
Reference-contexts: Task graph scheduling can effectively balance computational loads and reduce unnecessary communication. Finding optimal scheduling solutions is only possible for a small class of task graphs and in general the problem is NP-hard. Many scheduling heuristic algorithms have been proposed in the literature, e.g. <ref> [5, 9, 15, 20, 28, 41, 44, 34] </ref>. However, only few fl The work presented here was in part supported by ARPA contract DABT-63-93-C-0064 under "Hypercomputing and Design" project, by the Office of Naval research under grant N00014-93-1-0944, by NSF RIA CCR-9409695. <p> The goal of clustering is to identify "useful" parallelism and eliminate unnecessary communication. Most of the clustering algorithms are based on scheduling an unbounded number of processors of a clique architecture. A comparison is given in <ref> [20] </ref>. PYRROS uses the Dominant Sequence Clustering Algorithm (DSC) [48] which performs a sequence of clustering refinement steps. At each refinement step, DSC tries to zero an edge to reduce the parallel time (PT). The parallel time is determined by the longest path in the scheduled graph.
Reference: [21] <author> A. Gerasoulis and T. Yang, </author> <title> Performance bounds for parallelizing Gaussian-Elimination and Gauss-Jordan on message-passing machines, </title> <journal> Applied Numerical Mathematics Journal, </journal> <pages> 16(1994) 283-297. </pages>
Reference-contexts: communication edges is approximately equal to CP L k=1 k + t k+1 n k=1 Since T 1 = n 3 ! 1 =3 then an upper bound to the speedup is min (N 1; CP L N ! 1 = 0:5N: A further study of performance bounds is in <ref> [21] </ref>. Granularity: Since 1 k N the graph is coarse grain when (N k)r 3 !=2 ff + (N k + 1)r 2 fi while it becomes fine grain after some value of the k.
Reference: [22] <author> M. Girkar and C. Polychronopoulos, </author> <title> Automatic extraction of functional parallelism from ordinary programs, </title> <journal> IEEE Trans. on Parallel and Distributed Systems, </journal> <volume> Vol. 3, No. 2, </volume> <year> 1992, </year> <pages> pp. 166-178. </pages>
Reference: [23] <author> Leslie Greengrad, </author> <title> The Rapid Evaluation of Potential Fields in Particle Systems Ph.D thesis, </title> <institution> Yale University, </institution> <year> 1987. </year>
Reference-contexts: At the root of the tree the entire 2D space particle evaluation has been covered. For more details see Greengard's thesis <ref> [23] </ref>. has most 27 boxes in its interaction list and at most 8 neighbors Since n-body simulation involves the iterative updating of particle positions, we will use PYRROS to schedule force computation involved at each iteration. <p> A non-leaf node in the downward pass-Computation weight O (r). (a) Receives the multipole expansions from the boxes in its interaction list in the upward pass. Then converts these multipole expansions into local expansions, see Greengard thesis <ref> [23] </ref>. (b) Receives the local expansion from its parent, shifts it to its own center, then adds it to the local expansion computed in (a) above to obtain the total local expansion at this box. (c) Sends the local expansion to each of its children. 4.
Reference: [24] <author> A. Gupta, V. Kumar, </author> <title> Optimally scalable parallel sparse cholesky factorization. </title> <booktitle> Proc. of SIAM Conference on Parallel Processing for Scientific Computing. </booktitle> <pages> 442-447. </pages>
Reference-contexts: Thus the performance of PYRROS is competitive to a hand-optimized code for this regular problem. However, for irregular problems such as sparse matrix computation, developing a highly efficient parallel algorithm is difficult. Successful results have been demonstrated in <ref> [24, 37] </ref> for Cholesky factorization. In general, even reprogramming the algorithms proposed by them is not easy for a regular user. Automatic scheduling provides a mechanism to explore parallelism, predict performance and assist code generation.
Reference: [25] <author> J.A. Hoogeveen, S.L. van de Velde, and B. Veltman, </author> <title> Complexity of scheduling multiprocessor tasks with prespecified processor allocations, </title> <publisher> CWI, </publisher> <address> Report BS-R9211 June 1992, Netherlands. </address>
Reference-contexts: In Figure 8 (b) we show one ordering with P T = 7 and in (c) another ordering in which the parallel time increases to P T = 12. Finding a task ordering that minimizes the parallel time is NP-hard <ref> [25] </ref>. We have proposed a modification to the critical path (CP) heuristic for this problem in [47]. This heuristic called RCP costs O (v log v + e) and is described in Figure 7. Other heuristics are given in ??. 1.
Reference: [26] <author> J. J. Hwang, Y. C. Chow, F. D. Anger, and C. Y. Lee, </author> <title> Scheduling precedence graphs in systems with interprocessor communication times, </title> <journal> SIAM J. Comput., </journal> <pages> pp. 244-257, </pages> <year> 1989. </year>
Reference-contexts: more irregular task graphs we expect the scheduling performance to be even much better than the one shown above. 7 A performance comparison between PYRROS and the ETF algo rithm Finally we conduct experiments to compare the performance of the multistage PYRROS algorithms with the one stage ETF scheduling algorithm <ref> [26] </ref>. The ETF has an average complexity O (v 2 ), but the worst-case performance is O (pv 2 ). We first discuss the optimality of ETF and PYRROS for small graphs, and then discuss the performance difference for large graphs.
Reference: [27] <author> N. Karmarkar, </author> <title> A new parallel architecture for sparse matrix computation based on finite project geometries, </title> <booktitle> Proc. of Supercomputing '91, IEEE, </booktitle> <pages> pp. 358-369. </pages>
Reference-contexts: The solution is then derived by an iterative method such as Newton-Raphson which iterates over the same dataflow graph in solving a sparse matrix system. The topology of the iteration matrix remains the same but the data change at each step <ref> [27] </ref>. Given a sparse matrix with fixed nonzero patterns, a symbolic factorization can be performed and a dependence graph for numerical factorization can be derived. The task graph is the same as the one shown in figure 4, with all zero operations tasks being removed.
Reference: [28] <author> S.J. Kim and J.C Browne, </author> <title> A general approach to mapping of parallel computation upon multiprocessor architectures, </title> <booktitle> Proc. of Int'l Conf. on Parallel Processing, </booktitle> <volume> vol 3, </volume> <pages> pp. 1-8, </pages> <year> 1988. </year>
Reference-contexts: Task graph scheduling can effectively balance computational loads and reduce unnecessary communication. Finding optimal scheduling solutions is only possible for a small class of task graphs and in general the problem is NP-hard. Many scheduling heuristic algorithms have been proposed in the literature, e.g. <ref> [5, 9, 15, 20, 28, 41, 44, 34] </ref>. However, only few fl The work presented here was in part supported by ARPA contract DABT-63-93-C-0064 under "Hypercomputing and Design" project, by the Office of Naval research under grant N00014-93-1-0944, by NSF RIA CCR-9409695. <p> Several heuristic algorithms have been proposed in the literature, e.g. <ref> [15, 31, 28, 41, 44] </ref>. These heuristics are based either on a multistage approach or one stage approach to scheduling [28, 41]. PYRROS uses the following multistage approach where in each stage a heuristic with low complexity has been developed: (1) Assign tasks to a set of u clusters. <p> Several heuristic algorithms have been proposed in the literature, e.g. [15, 31, 28, 41, 44]. These heuristics are based either on a multistage approach or one stage approach to scheduling <ref> [28, 41] </ref>. PYRROS uses the following multistage approach where in each stage a heuristic with low complexity has been developed: (1) Assign tasks to a set of u clusters.
Reference: [29] <author> C. Koelbel, and P. Mehrotra, </author> <title> Compiling global name-space parallel loops for distributed execution, </title> <journal> IEEE Trans. on Parallel and Distributed Systems, </journal> <volume> Vol. 2, No. 4 , 1991, </volume> <pages> pp. 440-451. 30 </pages>
Reference: [30] <author> J. K. Lenstra and A.H.G. Rinnooy Kan, </author> <title> Complexity of Scheduling under Precedence Constraints, Operation Research, </title> <month> 26:1 </month> <year> (1978). </year>
Reference-contexts: PYRROS will read this program and perform lexical and semantic analysis to generate an internal representation of the DAG. 5 3 Scheduling algorithms Scheduling parallel tasks with precedence relations over distributed memory multiprocessors is harder than the classical scheduling problem <ref> [10, 30] </ref>. Several heuristic algorithms have been proposed in the literature, e.g. [15, 31, 28, 41, 44]. These heuristics are based either on a multistage approach or one stage approach to scheduling [28, 41].
Reference: [31] <author> C.L. McCreary and D.H. Gill, </author> <title> Automatic determination of grain size for efficient parallel processing, </title> <journal> Communications of ACM, </journal> <volume> vol. 32, </volume> <pages> pp. 1073-1078, </pages> <month> Sept., </month> <year> 1989. </year>
Reference-contexts: Several heuristic algorithms have been proposed in the literature, e.g. <ref> [15, 31, 28, 41, 44] </ref>. These heuristics are based either on a multistage approach or one stage approach to scheduling [28, 41]. PYRROS uses the following multistage approach where in each stage a heuristic with low complexity has been developed: (1) Assign tasks to a set of u clusters.
Reference: [32] <author> J.M. Ortega, </author> <title> Introduction to Parallel and Vector Solution of Linear Systems, </title> <address> New York:Plenum, </address> <year> 1988. </year>
Reference-contexts: The compute-ahead is similar to naive except that the pivot tasks are executed as soon as possible. The best mapping of the data is the cyclic mapping which load balances the processors. Saad [38] uses a similar implementation. We have implemented a hand-made code based on <ref> [32, 38] </ref>. Table 3 lists the performance of PYRROS and hand-made code when n = 1000, r = 10 and N = 100. The parallel times of PYRROS and cyclic 15 #Proc.
Reference: [33] <author> C. Papadimitriou and M. Yannakakis, </author> <title> Towards on an architecture-independent analysis of parallel algorithms, </title> <journal> SIAM J. Comput., </journal> <volume> vol. 19, </volume> <pages> pp. 322-328, </pages> <year> 1990. </year>
Reference: [34] <author> S. Pande, D. </author> <title> Agrawal and Jon Mauney A threshold Scheduling Strategy for Sisal on distributed Memory Machines JPDC, </title> <journal> vol. </journal> <volume> 21, </volume> <pages> pp. 223-236, </pages> <year> 1994. </year>
Reference-contexts: Task graph scheduling can effectively balance computational loads and reduce unnecessary communication. Finding optimal scheduling solutions is only possible for a small class of task graphs and in general the problem is NP-hard. Many scheduling heuristic algorithms have been proposed in the literature, e.g. <ref> [5, 9, 15, 20, 28, 41, 44, 34] </ref>. However, only few fl The work presented here was in part supported by ARPA contract DABT-63-93-C-0064 under "Hypercomputing and Design" project, by the Office of Naval research under grant N00014-93-1-0944, by NSF RIA CCR-9409695.
Reference: [35] <author> C. D. Polychronopoulos, M. Girkar, M. Haghighat,C. Lee, B. Leung, and D. Schouten, </author> <title> The Structure of Parafrase-2: An advanced parallelizing compiler for C and Fortran, in Languages and Compilers for Parallel Computing, </title> <editor> D. Gelernter, A. Nicolau and D. Padua (Eds.), </editor> <year> 1990. </year>
Reference: [36] <author> R. Pozo, </author> <title> Performance modeling of sparse matrix methods for distributed memory architectures, </title> <booktitle> in Lecture Notes in Computer Science, </booktitle> <volume> No. 634, </volume> <booktitle> Parallel Processing: CONPAR 92 - VAPP V, </booktitle> <address> Springer-Varlag, </address> <year> 1992, </year> <pages> pp. 677-688. </pages>
Reference: [37] <author> E. Rothberg and R. Schreiber, </author> <title> Efficient parallel sparse cholesky factorization, </title> <booktitle> Proc. of SIAM Conference on Parallel Processing for Scientific Computing, </booktitle> <year> 1995, </year> <pages> pp. 407-412. </pages>
Reference-contexts: Thus the performance of PYRROS is competitive to a hand-optimized code for this regular problem. However, for irregular problems such as sparse matrix computation, developing a highly efficient parallel algorithm is difficult. Successful results have been demonstrated in <ref> [24, 37] </ref> for Cholesky factorization. In general, even reprogramming the algorithms proposed by them is not easy for a regular user. Automatic scheduling provides a mechanism to explore parallelism, predict performance and assist code generation. <p> The task graph is the same as the one shown in figure 4, with all zero operations tasks being removed. For sparse Cholesky factorization, a task graph may not be the best way to model its parallelism since the DAG does not capture commutativity properties of the updating operations <ref> [37] </ref>. Figure 11 shows the speedup of PYRROS for a test data matrix of size 1084 from Alvarado's sparse matrix manipulation system [1]. The PYRROS compile predicted performance is close to PYRROS run time performance.
Reference: [38] <author> Y. Saad, </author> <title> Gaussian elimination on hypercubes, in Parallel Algorithms and Architectures, </title> <editor> Cosnard, M. et al. Eds., </editor> <publisher> Elsevier Science Publishers, North-Holland, </publisher> <year> 1986. </year>
Reference-contexts: The compute-ahead is similar to naive except that the pivot tasks are executed as soon as possible. The best mapping of the data is the cyclic mapping which load balances the processors. Saad <ref> [38] </ref> uses a similar implementation. We have implemented a hand-made code based on [32, 38]. Table 3 lists the performance of PYRROS and hand-made code when n = 1000, r = 10 and N = 100. The parallel times of PYRROS and cyclic 15 #Proc. <p> The compute-ahead is similar to naive except that the pivot tasks are executed as soon as possible. The best mapping of the data is the cyclic mapping which load balances the processors. Saad [38] uses a similar implementation. We have implemented a hand-made code based on <ref> [32, 38] </ref>. Table 3 lists the performance of PYRROS and hand-made code when n = 1000, r = 10 and N = 100. The parallel times of PYRROS and cyclic 15 #Proc. <p> Scheduling did not improve performance because cyclic mapping is near optimum for this task graph <ref> [38] </ref>. Thus the performance of PYRROS is competitive to a hand-optimized code for this regular problem. However, for irregular problems such as sparse matrix computation, developing a highly efficient parallel algorithm is difficult. Successful results have been demonstrated in [24, 37] for Cholesky factorization.
Reference: [39] <author> P. Sadayappan, F. Ercal and J. </author> <title> Ramanujam Cluster partitioning approaches to mapping parallel programs onto hypercubes, </title> <journal> Parallel Computing, </journal> <volume> 13(1990), </volume> <pages> pp. 1-16. </pages> <editor> Cosnard, M. et al. Eds., </editor> <publisher> Elsevier Science Publishers, North-Holland, </publisher> <year> 1986. </year>
Reference-contexts: Sort the clusters in an increasing order of their loads. 3. Use a load balancing algorithm so that each processor has approximately the same load. 3.3 Physical mapping We now have p virtual processors graph, also known as task interaction graph (TIG) <ref> [39] </ref>, and a p physical processor graph representing the parallel machine. Since physical processors may not be completely connected, we optimize the physical mapping by taking processor distances into account.
Reference: [40] <author> Saltz, J., Crowley, K., Mirchandaney, R. and Berryman,H., </author> <title> Run-time scheduling and execution of loops on message passing machines, </title> <journal> J. of Parallel and Distributed Computing, </journal> <volume> Vol. 8, </volume> <year> 1990, </year> <pages> pp. 303-312. </pages>
Reference: [41] <author> V. Sarkar, </author> <title> Partitioning and Scheduling Parallel Programs for Execution on Multiprocessors, </title> <publisher> The MIT Press, </publisher> <year> 1989. </year>
Reference-contexts: Task graph scheduling can effectively balance computational loads and reduce unnecessary communication. Finding optimal scheduling solutions is only possible for a small class of task graphs and in general the problem is NP-hard. Many scheduling heuristic algorithms have been proposed in the literature, e.g. <ref> [5, 9, 15, 20, 28, 41, 44, 34] </ref>. However, only few fl The work presented here was in part supported by ARPA contract DABT-63-93-C-0064 under "Hypercomputing and Design" project, by the Office of Naval research under grant N00014-93-1-0944, by NSF RIA CCR-9409695. <p> A task is an indivisible unit of computation which may be an assignment statement, a subroutine or even an entire program. We assume that tasks are convex, which means that once a task starts its execution it can run to completion without interrupting for communications, Sarkar <ref> [41] </ref>. In the task computation, a task waits to receive all data in parallel before it starts its execution. As soon as the task completes its execution it sends the output data to all successors in parallel. <p> Several heuristic algorithms have been proposed in the literature, e.g. <ref> [15, 31, 28, 41, 44] </ref>. These heuristics are based either on a multistage approach or one stage approach to scheduling [28, 41]. PYRROS uses the following multistage approach where in each stage a heuristic with low complexity has been developed: (1) Assign tasks to a set of u clusters. <p> Several heuristic algorithms have been proposed in the literature, e.g. [15, 31, 28, 41, 44]. These heuristics are based either on a multistage approach or one stage approach to scheduling <ref> [28, 41] </ref>. PYRROS uses the following multistage approach where in each stage a heuristic with low complexity has been developed: (1) Assign tasks to a set of u clusters. <p> This indicates that DSC has reached a high degree, in terms of parallel time optimality, that a polynomial-time complexity algorithm can attain. 3.2 Merging After the clusters have been determined, they need to be mapped onto the physical architecture. Sarkar <ref> [41] </ref> proposed a cluster merging algorithm with a cost O (pv (v + e)) which is time-consuming for a large graph. PYRROS uses a variation of work profiling method [17] for cluster merging. This method is simple and has been shown to work well in practice [18].
Reference: [42] <author> V. Sarkar, </author> <title> Determining average program execution times and their variance, </title> <booktitle> Proc. of 1989 SIGPLAN, ACM, </booktitle> <pages> pp. 298-312. </pages>
Reference-contexts: 1 Introduction There are two fundamental problems in automatic program parallelization. First the program partitioning and parallelism detection at the granularity level of the parallel machine, e.g. <ref> [8, 16, 42, 43] </ref>. And second the efficient execution of the detected parallelism. We focus on scheduling a class of parallel computation modeled as directed acyclic task graphs (DAGs). Task graph scheduling can effectively balance computational loads and reduce unnecessary communication.
Reference: [43] <author> V. Sarkar and R. Thekkath, </author> <title> A general framework for iteration-reordering loop transformations, </title> <booktitle> ACM SIGPLAN 92 PLDI. </booktitle> <pages> pp 175-187. </pages>
Reference-contexts: 1 Introduction There are two fundamental problems in automatic program parallelization. First the program partitioning and parallelism detection at the granularity level of the parallel machine, e.g. <ref> [8, 16, 42, 43] </ref>. And second the efficient execution of the detected parallelism. We focus on scheduling a class of parallel computation modeled as directed acyclic task graphs (DAGs). Task graph scheduling can effectively balance computational loads and reduce unnecessary communication.
Reference: [44] <author> M. Y. Wu and D. Gajski, Hypertool: </author> <title> A programming aid for message-passing systems, </title> <journal> IEEE Trans. on Parallel and Distributed Systems, </journal> <volume> vol. 1, no. 3, pp.330-343, </volume> <year> 1990. </year> <month> 31 </month>
Reference-contexts: Task graph scheduling can effectively balance computational loads and reduce unnecessary communication. Finding optimal scheduling solutions is only possible for a small class of task graphs and in general the problem is NP-hard. Many scheduling heuristic algorithms have been proposed in the literature, e.g. <ref> [5, 9, 15, 20, 28, 41, 44, 34] </ref>. However, only few fl The work presented here was in part supported by ARPA contract DABT-63-93-C-0064 under "Hypercomputing and Design" project, by the Office of Naval research under grant N00014-93-1-0944, by NSF RIA CCR-9409695. <p> The content of the information herein does not necessarily reflect the position of the Government and official endorsement should not be inferred. 1 automatic scheduling systems have been developed. SCHEDULER by Dongarra and Sorensen [13] uses a centralized dynamic scheduling scheme. HYPERTOOL by Wu and Gajski <ref> [44] </ref> and TASKGRAPHER by El-Rewini and Lewis [15] use compile time scheduling algorithms but do not produce code for parallel machines. We present a programming tool called PYRROS which schedules tasks and produces parallel code for distributed memory architectures. <p> Several heuristic algorithms have been proposed in the literature, e.g. <ref> [15, 31, 28, 41, 44] </ref>. These heuristics are based either on a multistage approach or one stage approach to scheduling [28, 41]. PYRROS uses the following multistage approach where in each stage a heuristic with low complexity has been developed: (1) Assign tasks to a set of u clusters.
Reference: [45] <author> T. Yang and A. Gerasoulis, </author> <title> PYRROS: Static task scheduling and code generation for message-passing multiprocessors, </title> <booktitle> Proc. of 6th ACM Inter. Conf. on Supercomputing, </booktitle> <address> Washington D.C., </address> <month> July, </month> <year> 1992, </year> <pages> pp. 428-437. </pages>
Reference: [46] <author> T. Yang, </author> <title> Scheduling and Code generation for parallel architecture, </title> <type> Ph.D Thesis, </type> <institution> DCS-TR 299, Rutgers University, </institution> <month> May </month> <year> 1993. </year>
Reference-contexts: Also the local memory space for each processor needs to be managed so that tasks access the correct data copies for their execution and the utilization of the limited space of each processor is maximized. The details of code generation techniques for executing a DAG schedule are given in <ref> [46] </ref>. The run-time execution follows the static scheduling results and the run-time weights of tasks could be different from their weights used in static scheduling.
Reference: [47] <author> T. Yang and A. Gerasoulis, </author> <title> List scheduling with and without communication delay, </title> <journal> Parallel Computing, </journal> <volume> Vol 19, </volume> <year> 1993, </year> <pages> pp. 1321-1344. </pages>
Reference-contexts: Finding a task ordering that minimizes the parallel time is NP-hard [25]. We have proposed a modification to the critical path (CP) heuristic for this problem in <ref> [47] </ref>. This heuristic called RCP costs O (v log v + e) and is described in Figure 7. Other heuristics are given in ??. 1. Adjust the communication edges of the DAG based on the processor assignment and physical distance. 2.
Reference: [48] <author> T. Yang and A. Gerasoulis, </author> <title> DSC: Scheduling parallel tasks on an unbounded number of processors, </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> Vol. 5, No. 9, </volume> <pages> 951-967, </pages> <year> 1994. </year>
Reference-contexts: The goal of clustering is to identify "useful" parallelism and eliminate unnecessary communication. Most of the clustering algorithms are based on scheduling an unbounded number of processors of a clique architecture. A comparison is given in [20]. PYRROS uses the Dominant Sequence Clustering Algorithm (DSC) <ref> [48] </ref> which performs a sequence of clustering refinement steps. At each refinement step, DSC tries to zero an edge to reduce the parallel time (PT). The parallel time is determined by the longest path in the scheduled graph. We call this path the Dominant Sequence (DS). <p> We call this path the Dominant Sequence (DS). Identifying DS with low complexity is the major difficulty. We have used an incremental scheme so that the DS can be found in O (log v) for each step. The total complexity of DSC is O ((v +e) log v). In <ref> [48] </ref>, we show that DSC performs well for general DAGs by examining a set of randomly generated DAGs and also produces the optimal solutions for fork, join, coarse grain tree DAGs and a class of fine-grain trees. <p> The run-time performance also depends on the static performance bound. Finding a tight bound B for the PYRROS algorithm is not easy. For a coarse grain DAG, and an unbounded number of processors, the PYRROS algorithm can produce a schedule <ref> [48] </ref> with B = 1 + 1 g (G) : Recently we have developed a low complexity algorithm with a bound B = 2 1=p + 1=g (G) [49] and its actual performance is competitive to the PYRROS algorithm. Thus for coarse grain graphs, the performance bound is small.

References-found: 48

