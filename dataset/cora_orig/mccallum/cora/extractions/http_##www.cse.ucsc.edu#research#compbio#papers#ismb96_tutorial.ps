URL: http://www.cse.ucsc.edu/research/compbio/papers/ismb96_tutorial.ps
Refering-URL: http://www.cse.ucsc.edu/research/compbio/sam.html
Root-URL: http://www.cse.ucsc.edu
Email: leslie@cse.ucsc.edu  rph@cse.ucsc.edu  karplus@cse.ucsc.edu  kimmen@cse.ucsc.edu  
Title: Tutorial Stochastic Modeling Techniques: Understanding and using hidden Markov models  
Author: Leslie Grate Richard Hughey Kevin Karplus Kimmen Sjolander 
Abstract-found: 0
Intro-found: 1
Reference: [1] <author> Stephen F. Altschul. </author> <title> Amino acid substitution matrices from an information theoretic perspective. </title> <journal> JMB, </journal> <volume> 219 </volume> <pages> 555-565, </pages> <year> 1991. </year>
Reference-contexts: Furthermore, as described above, the amount by which the score exceeds the threshold expresses our confidence in the result. This form of test, that a score be larger than some threshold, should be a familiar one, since almost all the recognition models used in computational biology fit this pattern <ref> [1] </ref>. The stochastic model approach tells us what setting the threshold means|we are making a statement of belief about our prior expectations. <p> Substitution matrices Several approaches have been proposed to solve the problem of regularizing, or generalizing amino acid distributions in positions to be able to give probability to amino acids of similar types to those seen. Undoubtedly the most popular of these methods involves the use of substitution matrices <ref> [27, 13, 1, 21] </ref>. These matrices are derived from alignments of homologous proteins, which reveal certain substitutions to be more likely than others among amino acids. For instance, isoleucine and valine are often found in the same position in homologous molecules, but neither is particularly likely to substitute for glutamate. <p> Although modifying the total weight for training sequences is not the means employed to obtain this effect, the various PAM matrices are designed with the same goal in mind. In fact, Altschul's paper gives a table for translating the various PAM matrices into bits saved per column <ref> [1] </ref>. A PAM distance of 120 is a savings of about 1 bit per column and a PAM distance of 240 is a savings of about 0.5 bits per column. <p> The PAM curve is from Altschul <ref> [1] </ref>, and the BLOSUM curve is from the Henikoffs' program (version 5.0 of the Blocks database). 22 matrices. The PAM matrix is put on the x-axis with the same scale as Figure 7 so that one can translate from percent residue difference to PAM distance to entropy visually.
Reference: [2] <author> Stephen F. Altschul, Warren Gish, Webb Miller, Eugene W. Meyers, and David J. Lippman. </author> <title> Basic local alignment search tool. </title> <journal> JMB, </journal> <volume> 215 </volume> <pages> 403-410, </pages> <year> 1990. </year>
Reference-contexts: If we are very certain that something is not true, then find out that we were wrong, a large amount of information is conveyed by the new data. 1 The BLAST program uses a different approach to reach essentially the same technique for more arbitrary scoring systems <ref> [2] </ref>. 7 Technically, we measure information in a sequence (or set of sequences) relative to some model. The information content of a sequence s relative to a model M is log P M (s). <p> One can use the model constructed from the target sequence to search a database for homologs, but this is a rather inefficient use of resources, since this initial model is not really any better for searching than the faster techniques used by BLAST <ref> [2] </ref>. A better technique for finding the close homologs is to use BLAST on the SwissProt or NR database to get a reasonably large set of homologous sequences quickly.
Reference: [3] <author> Timothy L. Bailey and Charles Elkan. </author> <title> The value of prior knowledge in discovering motifs with MEME. </title> <booktitle> In ISMB-95, </booktitle> <pages> pages 21-29, </pages> <address> Cambridge, England, </address> <month> July </month> <year> 1995. </year>
Reference-contexts: These mixtures have been shown in various experiments <ref> [26, 14, 3] </ref> to be more effective at reducing the numbers of false positives and false negatives in database discrimination tests. A theoretical rationale for these results is given in [18, 19], and is included in the Appendix.
Reference: [4] <author> M. Borodovsky and J. McIninch. Genmark: </author> <title> Parallel gene recognition for both DNA strands. </title> <journal> Computers and Chemistry, </journal> <volume> 17(2) </volume> <pages> 123-133, </pages> <year> 1993. </year>
Reference-contexts: Higher-order models have been used for compositional models (for example, for recognizing introns and exons by using 5th-order Markov models on the bases <ref> [4] </ref> and for recognizing repeated regions in DNA [17]). The beauty of simple Markov models is that they provide a very simple computational technique both for using the model and for training the model.
Reference: [5] <author> Philipp Bucher, Kevin Karplus, Nicolas Moeri, and Kay Hoffman. </author> <title> A flexible motif search technique based on generalized profiles. </title> <journal> Computers and Chemistry, </journal> <volume> 20(1) </volume> <pages> 3-24, </pages> <month> January </month> <year> 1996. </year>
Reference-contexts: A good overview of these techniques can be found in <ref> [5] </ref>. A multiple alignment is not really a modeling technique, but a way of presenting a group of related sequences to highlight the parts they have in common. A multiple alignment is easily created from a linear HMM by finding the Viterbi path in the HMM for each sequence.
Reference: [6] <author> R. F. Doolittle. </author> <title> Of URFs and ORFs: A primer on how to analyze derived amino acid sequences. </title> <publisher> University Science Books, </publisher> <address> Mill Valley, California, </address> <year> 1986. </year>
Reference-contexts: Among the first developed were those that were based on string-matching algorithms. These methods were developed based on the observation that when two sequences share at least 25% residue identity, and each is at least 80 residues in length, then the two sequences are homologous <ref> [24, 6] </ref>. These methods attempt to find a way of way of maximizing the number of matches between two sequences. Examples of this group are algorithms that search for the longest common subsequence, or exact matches to motifs of particular lengths.
Reference: [7] <author> R. O. Duda and P. E. Hart. </author> <title> Pattern Classification and Scene Analysis. </title> <publisher> Wiley, </publisher> <address> New York, </address> <year> 1973. </year>
Reference-contexts: Dirichlet mixtures The dependency on sufficient data to estimate the parameters of a HMM prompted us to look for ways to include prior information over amino acid distributions into our model-building process. We used the excellent work by Duda and Hart on estimating mixtures of Gaussian densities <ref> [7] </ref> as a template to estimate mixtures of Dirichlet densities. We chose Dirichlet densities because they had several nice mathematical properties which made them very appealing. Dirichlet densities are densities on probability distributions.
Reference: [8] <author> S. R. Eddy and R. </author> <title> Durbin. RNA sequence analysis using covariance models. </title> <journal> NAR, </journal> <volume> 22 </volume> <pages> 2079-2088, </pages> <year> 1994. </year>
Reference-contexts: One major disadvantage of HMMs is that they cannot directly model long-distance interactions such as base pairing in RNA. For this, more complex models such as stochastic context-free grammars <ref> [23, 8, 11] </ref> are needed. 3.3 Multiple Alignments, Profiles, and HMMs Several modeling techniques that are conceptually quite similar to linear HMMs have been used by biologists, including multiple alignments, profiles, and generalized profiles. A good overview of these techniques can be found in [5].
Reference: [9] <author> Sean Eddy. </author> <title> Multiple alignment using hidden Markov models. </title> <booktitle> In ISMB-95, </booktitle> <pages> pages 114-120, </pages> <address> Cambridge, England, July 1995. </address> <publisher> AAAI/MIT Press. </publisher>
Reference-contexts: Many of the tasks we discuss can also be performed (sometimes better!) with HMMer, about which Sean Eddy will talk in the second half of the tutorial <ref> [10, 9] </ref>. The appendix includes both the SAM and the HMMer documentation. If you have any questions about this material, feel free to direct them to any of the authors.
Reference: [10] <author> S.R. Eddy, G. Mitchison, and R. </author> <title> Durbin. Maximum discrimination hidden Markov models of sequence consensus. </title> <journal> J. Comput. Biol., </journal> <volume> 2 </volume> <pages> 9-23, </pages> <year> 1995. </year>
Reference-contexts: Many of the tasks we discuss can also be performed (sometimes better!) with HMMer, about which Sean Eddy will talk in the second half of the tutorial <ref> [10, 9] </ref>. The appendix includes both the SAM and the HMMer documentation. If you have any questions about this material, feel free to direct them to any of the authors. <p> The SAM suite of HMM programs incorporates the use of one nine-component Dirichlet mixture prior which we have found to be the most effective. HMMer includes both mixture priors and structure-based mixture priors <ref> [10] </ref>. 5.2 Weighting schemes It's undoubtedly not news to anyone attending this tutorial that alignments of protein or DNA sequences often contain a large number of very close homologs or even exact duplicates.
Reference: [11] <author> Leslie Grate. </author> <title> Automatic RNA secondary structure determination with stochastic context-free grammars. </title> <booktitle> In ISMB-95, </booktitle> <pages> pages 136-144, </pages> <address> Menlo Park, CA, July 1995. </address> <publisher> AAAI/MIT Press. </publisher>
Reference-contexts: One major disadvantage of HMMs is that they cannot directly model long-distance interactions such as base pairing in RNA. For this, more complex models such as stochastic context-free grammars <ref> [23, 8, 11] </ref> are needed. 3.3 Multiple Alignments, Profiles, and HMMs Several modeling techniques that are conceptually quite similar to linear HMMs have been used by biologists, including multiple alignments, profiles, and generalized profiles. A good overview of these techniques can be found in [5].
Reference: [12] <author> D. Haussler, A. Krogh, I. S. Mian, and K. Sjolander. </author> <title> Protein modeling using hidden Markov models: Analysis of globins. </title> <type> Technical Report UCSC-CRL-92-23, </type> <institution> University of California at Santa Cruz, Computer and Information Sciences Dept., </institution> <address> Santa Cruz, CA 95064, </address> <year> 1992. </year>
Reference-contexts: The complexity of the model also influences the kind of training data needed. Generally, the more parameters in the model, the more data you need to train it effectively. In early work modeling protein families and domains using HMMs <ref> [12] </ref>, we found this reliance on sufficient data to be pronounced, such that many (upwards of 100) training sequences from the protein family or domain of interest were generally required in order to obtain reasonable results.
Reference: [13] <author> Steven Henikoff and Jorja G. Henikoff. </author> <title> Amino acid substitution matrices from protein blocks. </title> <journal> PNAS, </journal> <volume> 89 </volume> <pages> 10915-10919, </pages> <month> November </month> <year> 1992. </year>
Reference-contexts: Substitution matrices Several approaches have been proposed to solve the problem of regularizing, or generalizing amino acid distributions in positions to be able to give probability to amino acids of similar types to those seen. Undoubtedly the most popular of these methods involves the use of substitution matrices <ref> [27, 13, 1, 21] </ref>. These matrices are derived from alignments of homologous proteins, which reveal certain substitutions to be more likely than others among amino acids. For instance, isoleucine and valine are often found in the same position in homologous molecules, but neither is particularly likely to substitute for glutamate.
Reference: [14] <author> Steven Henikoff and Jorja G. Henikoff. </author> <type> Personal communication, </type> <month> January </month> <year> 1995. </year>
Reference-contexts: These mixtures have been shown in various experiments <ref> [26, 14, 3] </ref> to be more effective at reducing the numbers of false positives and false negatives in database discrimination tests. A theoretical rationale for these results is given in [18, 19], and is included in the Appendix.
Reference: [15] <author> R. Hughey and A. Krogh. SAM: </author> <title> Sequence alignment and modeling software system. </title> <type> Technical Report UCSC-CRL-95-7, </type> <institution> University of California, Santa Cruz, Computer Engineering, UC Santa Cruz, </institution> <address> CA 95064, </address> <year> 1995. </year>
Reference-contexts: The SAM suite program hmmscore has a few different scoring methods the user can choose from (refer to the SAM manual <ref> [15] </ref> for details). Either the Viterbi ("best path") or Baum-Welch ("sum over all paths") (see Section 3) method is used to compute the probability P (sjM ) that a given sequence was generated by a given model. The negative logarithm of this value is termed the NLL cost.
Reference: [16] <author> Richard Hughey and Anders Krogh. </author> <title> Hidden Markov models for sequence analysis: Extension and analysis of the basic method. </title> <journal> CABIOS, </journal> <note> To appear 1996. </note>
Reference-contexts: 1 Introduction This tutorial is organized conceptually to provide, first, a theoretical framework for stochastic modeling, and second, to enable readers to use stochastic models to their advantage. The tutorial includes a wide variety of examples, mostly drawn from the Sequence Alignment and Modeling System (SAM) <ref> [20, 16] </ref>, which will be the focus of the hands-on session in the second half of the tutorial. Many of the tasks we discuss can also be performed (sometimes better!) with HMMer, about which Sean Eddy will talk in the second half of the tutorial [10, 9].
Reference: [17] <author> Kevin Karplus. </author> <title> Using Markov models and hidden Markov models to find repetitive extragenic palindromic sequences in Escherichia coli. </title> <type> Technical Report UCSC-CRL-94-24, </type> <institution> University of California, Santa Cruz, </institution> <month> July </month> <year> 1994. </year>
Reference-contexts: Higher-order models have been used for compositional models (for example, for recognizing introns and exons by using 5th-order Markov models on the bases [4] and for recognizing repeated regions in DNA <ref> [17] </ref>). The beauty of simple Markov models is that they provide a very simple computational technique both for using the model and for training the model.
Reference: [18] <author> Kevin Karplus. </author> <title> Regularizers for estimating distributions of amino acids from small samples. </title> <booktitle> In ISMB-95, </booktitle> <address> Cambridge, England, </address> <month> July </month> <year> 1995. </year>
Reference-contexts: These mixtures have been shown in various experiments [26, 14, 3] to be more effective at reducing the numbers of false positives and false negatives in database discrimination tests. A theoretical rationale for these results is given in <ref> [18, 19] </ref>, and is included in the Appendix. The SAM suite of HMM programs incorporates the use of one nine-component Dirichlet mixture prior which we have found to be the most effective.
Reference: [19] <author> Kevin Karplus. </author> <title> Regularizers for estimating distributions of amino acids from small samples. </title> <type> Technical Report UCSC-CRL-95-11, </type> <institution> University of California, Santa Cruz, </institution> <month> March </month> <year> 1995. </year> <note> URL ftp://ftp.cse.ucsc.edu/pub/tr/ucsc-crl-95-11.ps.Z. </note>
Reference-contexts: These mixtures have been shown in various experiments [26, 14, 3] to be more effective at reducing the numbers of false positives and false negatives in database discrimination tests. A theoretical rationale for these results is given in <ref> [18, 19] </ref>, and is included in the Appendix. The SAM suite of HMM programs incorporates the use of one nine-component Dirichlet mixture prior which we have found to be the most effective.
Reference: [20] <author> A. Krogh, M. Brown, I. S. Mian, K. Sjolander, and D. Haussler. </author> <title> Hidden Markov models in computational biology: Applications to protein modeling. </title> <journal> JMB, </journal> <volume> 235 </volume> <pages> 1501-1531, </pages> <month> February </month> <year> 1994. </year>
Reference-contexts: 1 Introduction This tutorial is organized conceptually to provide, first, a theoretical framework for stochastic modeling, and second, to enable readers to use stochastic models to their advantage. The tutorial includes a wide variety of examples, mostly drawn from the Sequence Alignment and Modeling System (SAM) <ref> [20, 16] </ref>, which will be the focus of the hands-on session in the second half of the tutorial. Many of the tasks we discuss can also be performed (sometimes better!) with HMMer, about which Sean Eddy will talk in the second half of the tutorial [10, 9].
Reference: [21] <author> John Overington, Dan Donnelly, Mark S. Johnson, Andrej Sali, and Tom J. Blundell. </author> <title> Enviroment-specific amino acid substitution tables: Tertiary templates and prediction of protein folds. </title> <journal> Protein Science, </journal> <volume> 1 </volume> <pages> 216-226, </pages> <year> 1992. </year>
Reference-contexts: Substitution matrices Several approaches have been proposed to solve the problem of regularizing, or generalizing amino acid distributions in positions to be able to give probability to amino acids of similar types to those seen. Undoubtedly the most popular of these methods involves the use of substitution matrices <ref> [27, 13, 1, 21] </ref>. These matrices are derived from alignments of homologous proteins, which reveal certain substitutions to be more likely than others among amino acids. For instance, isoleucine and valine are often found in the same position in homologous molecules, but neither is particularly likely to substitute for glutamate.
Reference: [22] <author> Lawrence R. Rabiner. </author> <title> A tutorial on hidden Markov models and selected applications in speech recognition. </title> <journal> Proc. of the IEEE, </journal> <volume> 77(2) </volume> <pages> 257-286, </pages> <month> February </month> <year> 1989. </year>
Reference-contexts: Rabiner's paper, "A Tutorial on Hidden Markov Models and Selected Applications in Speech Recognition" in the appendix <ref> [22] </ref>. 3.2 Linear HMMs for Biological Sequences A general HMM is an arbitrary graph of states and edges, and so can be very difficult for a biologist to interpret. To make HMMs more comprehensible, we usually restrict the structure to a simple linear arrangement.
Reference: [23] <author> Yasubumi Sakakibara, Michael Brown, Richard Hughey, I. Saira Mian, Kimmen Sjolander, Rebecca C. Under-wood, and David Haussler. </author> <title> Stochastic context-free grammars for tRNA modeling. </title> <journal> NAR, </journal> <volume> 22 </volume> <pages> 5112-5120, </pages> <year> 1994. </year> <month> 33 </month>
Reference-contexts: One major disadvantage of HMMs is that they cannot directly model long-distance interactions such as base pairing in RNA. For this, more complex models such as stochastic context-free grammars <ref> [23, 8, 11] </ref> are needed. 3.3 Multiple Alignments, Profiles, and HMMs Several modeling techniques that are conceptually quite similar to linear HMMs have been used by biologists, including multiple alignments, profiles, and generalized profiles. A good overview of these techniques can be found in [5].
Reference: [24] <author> C. Sander and R. Schneider. </author> <title> Database of homology-derived protein structures and the structural meaning of sequence alignment. </title> <journal> Proteins, </journal> <volume> 9(1) </volume> <pages> 56-68, </pages> <year> 1991. </year>
Reference-contexts: Among the first developed were those that were based on string-matching algorithms. These methods were developed based on the observation that when two sequences share at least 25% residue identity, and each is at least 80 residues in length, then the two sequences are homologous <ref> [24, 6] </ref>. These methods attempt to find a way of way of maximizing the number of matches between two sequences. Examples of this group are algorithms that search for the longest common subsequence, or exact matches to motifs of particular lengths. <p> Alignments can take many forms, and the method to refine an alignment has to take that form into account. For instance, at UCSC, most of our work reestimating alignments has been done on HSSP alignments <ref> [24] </ref>. These alignments indicate deleted (or inserted, depending on the perspective) regions by lower-case letters, and put the excised residues at the end of the alignment file.
Reference: [25] <author> Manfred J. Sippl. </author> <title> Calculation of conformational ensembles from potentials of mean force: an approach to the knowledge-based prediction of local structures in globular proteins. </title> <journal> JMB, </journal> <volume> 213 </volume> <pages> 859-883, </pages> <year> 1990. </year>
Reference-contexts: Variants of these methods have been developed that attempt to take into account some of the additional information in the column being regularized. Such methods are called data-dependent pseudocounts (for example, <ref> [25] </ref>). Dirichlet mixtures The dependency on sufficient data to estimate the parameters of a HMM prompted us to look for ways to include prior information over amino acid distributions into our model-building process.
Reference: [26] <author> Roman L. Tatusov, Stephen F. Altschul, and Eugen V. Koonin. </author> <title> Detection of conserved segments in proteins: Iterative scanning of sequence databases with alignment blocks. </title> <journal> PNAS, </journal> <volume> 91 </volume> <pages> 12091-12095, </pages> <month> December </month> <year> 1994. </year>
Reference-contexts: These mixtures have been shown in various experiments <ref> [26, 14, 3] </ref> to be more effective at reducing the numbers of false positives and false negatives in database discrimination tests. A theoretical rationale for these results is given in [18, 19], and is included in the Appendix. <p> This iteration of finding homologs and retraining the model to match all the known homologs can be repeated until no new homologs are found. The iteration is a very effective way to build models <ref> [26] </ref>. Unfortunately, searching a large database (such as NR) can take several hours using hidden Markov models, and so will not be demonstrated here. Searching NRP with the t5_3w7.mod model finds 79 matching domains, most of which are duplicates of ones already found in the Entrez nearest neighbors search.
Reference: [27] <author> Gerhard Vogt, Thure Etzold, and Patrick Argos. </author> <title> An assessment of amino acid exchange matrices in aligning protein sequences: The twilight zone revisited. </title> <journal> JMB, </journal> <volume> 249 </volume> <pages> 816-831, </pages> <year> 1995. </year> <note> 34 9 Appendices 35 </note>
Reference-contexts: Substitution matrices Several approaches have been proposed to solve the problem of regularizing, or generalizing amino acid distributions in positions to be able to give probability to amino acids of similar types to those seen. Undoubtedly the most popular of these methods involves the use of substitution matrices <ref> [27, 13, 1, 21] </ref>. These matrices are derived from alignments of homologous proteins, which reveal certain substitutions to be more likely than others among amino acids. For instance, isoleucine and valine are often found in the same position in homologous molecules, but neither is particularly likely to substitute for glutamate.
References-found: 27

