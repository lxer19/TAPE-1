URL: ftp://ftp.research.microsoft.com/users/palarson/sigmod98.ps
Refering-URL: http://www.research.microsoft.com/~palarson/
Root-URL: http://www.research.microsoft.com
Email: PALarson@microsoft.com  GoetzG@microsoft.com  
Title: Memory Management during Run Generation in External Sorting  
Author: Perke Larson Goetz Graefe 
Keyword: Sorting, merge sort, run formation, memory management, variable length records, replacement selection, last-run optimization  
Affiliation: Microsoft  Microsoft  
Abstract: If replacement selection is used in an external mergesort to generate initial runs, individual records are deleted and inserted in the sort operations workspace. Variable-length records introduce the need for possibly complex memory management and extra copying of records. As a result, few systems employ replacement selection, even though it produces longer runs than commonly used algorithms. We experimentally compared several algorithms and variants for managing this workspace. We found that the simple best fit algorithm achieves memory utilization of 90% or better and run lengths over 1.8 times workspace size, with no extra copying of records and very little other overhead, for widely varying record sizes and for a wide range of memory sizes. Thus, replacement selection is a viable algorithm for commercial database systems, even for variable-length records. Efficient memory management also enables an external sort algorithm that degrades gracefully when its input is only slightly larger than or a small multiple of the available memory size. This is not the case with the usual implementations of external sorting, which incur I/O for the entire input even if it is as little as one record larger than memory. Thus, in some cases, our techniques may reduce I/O volume by a factor 10 compared to traditional database sort algorithms. Moreover, the gradual rather than abrupt growth in I/O volume for increasing input sizes significantly eases design and implementation of intra-query memory management policies. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Vladimir Estivill-Castro, Derick Wood: </author> <title> A Survey of Adaptive Sorting Algorithms. </title> <journal> Computing Surveys 24(4): </journal> <month> 441-476 </month> <year> (1992) </year>
Reference-contexts: Mergesort with run formation by replacement selection has this property but mergesort with run formation by loadsort-store does not, i.e., the amount of work is always the same, even when the input is already sorted. Estivill-Castro and Wood <ref> [1] </ref> provide a survey of adaptive sort algorithms. There is also a large body of research dedicated to dynamic memory allocation. Knuth [5] describes several algorithms, including first fit, next fit and best fit. A recent survey with an extensive bibliography can be found in [10]. <p> This improves the utilization of I/O devices. Secondly, it very effectively exploits presorting, i.e., input sequences that are not random but somewhat correlated to the desired sorted output sequence. In particular, if the input is already sorted, a single run will be generated <ref> [1] </ref>. Sorting is often done as part of grouping with aggregation or for duplicate removal. A technique called early aggregation or early duplicate removal can then be applied, which reduces I/O significantly (see [7] and its references).
Reference: [2] <author> Goetz Graefe, </author> <title> Query Evaluation Techniques for Large Databases. </title> <journal> ACM Computing Surveys 25(2): </journal> <month> 73-170 </month> <year> (1993). </year>
Reference-contexts: There are several advantages to using replacement selection, as outlined in a later section, but the problem of managing variable-length records has misled some researchers, including ourselves <ref> [2] </ref>, to dismiss replacement selection in favor of loadsort-store algorithms. Given the excellent performance of the best-fit algorithm, we reconsider and resolve this argument in favor of replacement selection. Almost needless to say, graceful degradation and replacement selection can be combined, with cumulative benefits. 2.
Reference: [3] <author> Goetz Graefe, Sort-Merge-Join: </author> <title> An Idea whose Time Has(h) Passed? Proc. </title> <booktitle> Data Engineering Conf. </booktitle> <year> 1994: </year> <pages> 406-417. </pages>
Reference: [4] <author> Masaya Nakayama, Masaru Kitsuregawa, </author> <title> Hash-Partitioned Join Method Using Dynamic Destaging Strategy. </title> <booktitle> Proc. VLDB Conf. </booktitle> <year> 1988: </year> <pages> 468-478 </pages>
Reference: [5] <author> D. E. Knuth: </author> <booktitle> The Art of Computer Programming, Vol ume 1 (2nd Ed.), </booktitle> <publisher> Addison-Wesley, </publisher> <year> 1973. </year>
Reference-contexts: Estivill-Castro and Wood [1] provide a survey of adaptive sort algorithms. There is also a large body of research dedicated to dynamic memory allocation. Knuth <ref> [5] </ref> describes several algorithms, including first fit, next fit and best fit. A recent survey with an extensive bibliography can be found in [10].
Reference: [6] <author> D. E. Knuth: </author> <booktitle> The Art of Computer Programming, </booktitle> <volume> Vol ume 3. </volume> <publisher> Addison-Wesley, </publisher> <year> 1973. </year>
Reference-contexts: Almost needless to say, graceful degradation and replacement selection can be combined, with cumulative benefits. 2. Related work Sorting is one of the most extensively studied problems in computing. There is a vast literature on internal sorting, but less on external sorting. Knuth <ref> [6] </ref> provides extensive coverage of the fundamentals of sorting, including excellent descriptions of replacement selection and mergesort. Some sort algorithms are adaptive in the sense that they do less work if the input exhibits some degree of presortedness. <p> Instead, replacement selection uses a priority heap, also called a selection tree, and adds the run number as the first field of the sort key. For details, in particular a very efficient organization of the heap as a tree of losers, see <ref> [6] </ref>. The end result is a simple and very efficient algorithm for run formation that produces runs that are, on average, twice the size of employed memory. This algorithm was invented in the early 1950s and has remained virtually unchanged since 1958. <p> Contrary to an occasionally encountered belief, external sorting using replacement selection does not require more key comparisons than using quicksort, presuming that run generation employs a selection tree and that this selection tree is organized as a tree of losers <ref> [6] </ref>. A tree of losers is a binary tree that is traversed only from leaf to root, never from root to leaf, with one key comparison at each level. <p> So given N initial runs, possibly of variable length, and a maximum merge fan-in F, which merge pattern results in the minimum I/O volume? This has a surprisingly simple solution (see <ref> [6] </ref>, pp. 365-366): first add enough dummy runs of length zero to make the number of runs minus one divisible by F-1 and then repeatedly merge together the F shortest existing runs until only one run remains. <p> The figures show the relative run length, which is the run length divided by the size of the workspace. Replacement selection produces runs that are, on average, twice the size of memory <ref> [6] </ref>, as is well known for fixed-sized records and random input. When records are of variable length, some memory is wasted so the average run length will certainly be less than two times the size of memory. record length distributions and different workspace sizes.
Reference: [7] <author> Perke Larson: </author> <title> Grouping and Duplicate Elimination: Benefits of Early Aggregation, </title> <note> Manuscript submitted for publication, 1997. Available from the author. </note>
Reference-contexts: In particular, if the input is already sorted, a single run will be generated [1]. Sorting is often done as part of grouping with aggregation or for duplicate removal. A technique called early aggregation or early duplicate removal can then be applied, which reduces I/O significantly (see <ref> [7] </ref> and its references). Early aggregation and early duplicate removal achieve much higher I/O reduction if runs are created by replacement selection, which is the third advantage.
Reference: [8] <author> Vinay S. Pai, Peter J. Varman: </author> <title> Prefetching with Multiple Disks for External Mergesort: Simulation and Analysis. </title> <booktitle> Proc. Data Engineering Conf., </booktitle> <year> 1992: </year> <pages> 273-282 </pages>
Reference: [9] <author> Betty Salzberg: </author> <title> Merging Sorted Runs Using Large Main Memory. </title> <journal> Acta Informatica, </journal> <volume> 27(3): </volume> <month> 195-215 </month> <year> (1989) </year>
Reference: [10] <author> Paul R. Wilson, Mark S. Johnstone, Michael Neely, and David Boles: </author> <title> Dynamic Storage Allocation: A Survey and Critical Review. </title> <booktitle> In International Workshop on Memory Management, </booktitle> <address> Kinross, Scotland, UK, </address> <year> 1995. </year> <note> Also http://www.cs.utexas.edu/users/oops/papers.html. </note>
Reference-contexts: Estivill-Castro and Wood [1] provide a survey of adaptive sort algorithms. There is also a large body of research dedicated to dynamic memory allocation. Knuth [5] describes several algorithms, including first fit, next fit and best fit. A recent survey with an extensive bibliography can be found in <ref> [10] </ref>. While the basic ideas are directly applicable to the present study, there are two important differences between managing the workspace for a sort operation and managing the heap for a programming language such as Lisp or Java.
Reference: [11] <author> LuoQuan Zheng, Perke Larson: </author> <title> Speeding up External Mergesort. </title> <journal> IEEE Transactions on Knowledge and Data Engineering, </journal> <volume> 8(2): </volume> <month> 322-332 </month> <year> (1996) </year>
References-found: 11

