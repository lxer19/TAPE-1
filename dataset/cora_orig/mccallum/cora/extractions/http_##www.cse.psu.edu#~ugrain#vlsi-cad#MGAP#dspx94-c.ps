URL: http://www.cse.psu.edu/~ugrain/vlsi-cad/MGAP/dspx94-c.ps
Refering-URL: http://www.cse.psu.edu/~ugrain/publications.html
Root-URL: 
Title: PROGRAMMING MICRO-GRAINED SIGNAL PROCESSORS USING C++  
Author: Chetana Nagendra Raminder Bajwa Paul Keltcher Robert M. Owens Mary Jane Irwin Robert Michael Owens 
Degree: the B.Tech. (1987) degree in Electrical Engineering from the Indian Institute of Technology, Bombay, and the M.Sc. degree  
Date: received  
Note: Raminder S. Bajwa  from Michigan Tech. University in 1989. Currently he is working towards a Ph.D. degree in Computer Science. Paul S. Keltcher received his B.S. (1990) degree in Computer Science from Bucknell University and is currently working towards a Ph.D. degree in  received the M.S. (1975) degree in Computer Science from the Vir-ginia  and the Ph.D. degree (1980) from Penn State. He is presently an associate Professor and has authored over 100 scholarly works. Mary Jane Irwin received the M.S. (1975) and Ph.D. (1977) degrees in Computer Science from The  She is a Professor of Computer Science and Engineering and has authored over 125 scholarly works.  
Address: Park, PA 16802  
Affiliation: Department of Computer Science and Engineering The Pennsylvania State University University  Computer Science.  Polytechnic Institute and State University  University of Illinois, Urbana-Champaign.  
Abstract: The MGAP is a fine-grain, massively parallel processor ideally suited for signal processing applications. It is designed to economically provide almost a teraop of additional computing power to desktop workstations. In this paper we show how we program various signal processing applications on the MGAP using C++. Although there have been parallel extensions to Lisp, Fortran and C, no single high level language allows the user to program the machine at the various levels of granularity that these machines have to offer. The use of the machine at the finer levels of granularity is limited to a selected clique that is familiar with the innards of the architecture. The programming language of the MGAP, on the other hand, is C++ extended to handle parallel data types. Using the same high level language, it is possible to program the MGAP at all levels of granularity, from the bit level to the array level. Thereby the user can write high level code and optimize the application to effectively harness the enormous computing capability of the MGAP. Chetana Nagendra received the B.E. (1990) degree in Computer Science from BMS College of Engineering, Bangalore University, and the M.S. (1992) degree from Penn State in 1992. Currently she is working towards a Ph.D. degree in Computer Science. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <institution> Connection Machine CM-200 Series Technical Summary, 1991. Thinking Machines Corporation. </institution>
Reference-contexts: 1 Introduction Architectures using fine grain processors have been gaining in popularity over the last decade because of their ability to solve a variety of applications such as signal and image processing, fluid dynamics, computational biology, cryptography, graph problems, and so on. The CM-2 <ref> [1] </ref>, CM-5 [9], Splash 1 and 2 [7, 2], DAP [14], CAL [8] and PAM [5] are a few examples of such machines. The MGAP-1 (Micro Grain Array Processor) is also a fine grain, massively parallel processor which was designed at the Penn State University. <p> We have successfully used it to speed up a number of applications including signal processing problems. A lot of research has been done on fine grain architectures themselves. There have been parallel extensions to Lisp, Fortran and C <ref> [1] </ref>, but no high language has been designed to program a fine grain machine at the bit level. For example, in C*, which is the language used to program the Connection Machine, the primitive data type is the integer.
Reference: [2] <author> J. M. Arnold, D. A. Buell, and E. G. Davis. </author> <title> Splash 2. </title> <booktitle> In Proc. 4th Annu. ACM Symp. on Parallel Algorithms and Architectures, </booktitle> <pages> pages 316-324, </pages> <year> 1992. </year>
Reference-contexts: The CM-2 [1], CM-5 [9], Splash 1 and 2 <ref> [7, 2] </ref>, DAP [14], CAL [8] and PAM [5] are a few examples of such machines. The MGAP-1 (Micro Grain Array Processor) is also a fine grain, massively parallel processor which was designed at the Penn State University.
Reference: [3] <author> R. S. Bajwa, R. M. Owens, and M. J. Irwin. </author> <title> A Massively Parallel Micro-Grained, </title> <booktitle> VLSI Architecture. In VLSI Design '93, </booktitle> <pages> pages 250-255, </pages> <month> Jan. </month> <year> 1993. </year> <title> Fig. </title> <booktitle> 7. The Mgap INTerface </booktitle>
Reference-contexts: Finally we give some performance figures and conclude in section 4. 2 MGAP Architecture The MGAP is a simple, yet general and flexible architecture, that enables massive parallelism to bear upon many problems in an economical manner <ref> [10, 3, 4, 13, 12, 6] </ref>. The MGAP-1, our first generation implementation, exhibits a peak peformance of 0.8 teraops at 25 MHz. <p> A hierarchy of parallel data types can be derived from the bit by encapsulating data as well as the operations that act on these data types into new objects. In step 1 of add.cc, the object word is defined. Line (1a), private : local bit val <ref> [3] </ref>; declares that the object word holds an array of 3 bits. The attribute local means that all these bits are local to a single processor. In step 2, line (2a), we define an object shape that consists of 128 fi 128 elements of type word. <p> In order to handle operands of large precision, several processors are grouped together to form a "word processor". By using redundant /* * 1. Define the object word. */ class word - private : local bit val <ref> [3] </ref>; /** 1a **/ public : class word operator+(class word x, y) - /** 1b **/ word z; int i; /** 1c **/ t = 0; z.val [i] = x.val [i] ^ y.val [i] ^ t; - return (z); - /* * 2.
Reference: [4] <author> R. S. Bajwa, R. M. Owens, and M. J. Irwin. </author> <title> Image Processing with the MGAP: A Cost Effective Approach. </title> <booktitle> In Intl. Parallel Processing Symposium, </booktitle> <pages> pages 439-443, </pages> <month> Apr. </month> <year> 1993. </year>
Reference-contexts: Finally we give some performance figures and conclude in section 4. 2 MGAP Architecture The MGAP is a simple, yet general and flexible architecture, that enables massive parallelism to bear upon many problems in an economical manner <ref> [10, 3, 4, 13, 12, 6] </ref>. The MGAP-1, our first generation implementation, exhibits a peak peformance of 0.8 teraops at 25 MHz.
Reference: [5] <author> P. Bertin, D. Roncin, and J. Vuillemin. </author> <title> Introduction to Programmable Active Memories. </title> <booktitle> In Proc. of Inter. Conf. on Systolic Array Processors, </booktitle> <pages> pages 301-309, </pages> <month> May </month> <year> 1989. </year>
Reference-contexts: The CM-2 [1], CM-5 [9], Splash 1 and 2 [7, 2], DAP [14], CAL [8] and PAM <ref> [5] </ref> are a few examples of such machines. The MGAP-1 (Micro Grain Array Processor) is also a fine grain, massively parallel processor which was designed at the Penn State University. It is a single SUN4 coprocessor board which provides almost a teraop of additional computing power.
Reference: [6] <author> M. Borah, C. Nagendra, R. M. Owens, and M. J. Irwin. </author> <title> The MGAP: A High Performance, User Programmable, Multifunctional Architecture for DSP. </title> <booktitle> In Hawaii International Conference on System Sciences, </booktitle> <month> Jan. </month> <year> 1994. </year>
Reference-contexts: Finally we give some performance figures and conclude in section 4. 2 MGAP Architecture The MGAP is a simple, yet general and flexible architecture, that enables massive parallelism to bear upon many problems in an economical manner <ref> [10, 3, 4, 13, 12, 6] </ref>. The MGAP-1, our first generation implementation, exhibits a peak peformance of 0.8 teraops at 25 MHz. <p> We have demonstrated that using the same high level language and the same program, we can operate on data at all levels of granularity from bits to shapes without any loss in performance. Table 1 shows the timings for some signal processing applications when executed on the MGAP <ref> [6] </ref>.
Reference: [7] <author> Gokhale, M, and et. al. </author> <title> Building and using a highly parallel programmable logic array. </title> <journal> Computer, </journal> <volume> 24 </volume> <pages> 81-89, </pages> <month> Jan. </month> <year> 1991. </year>
Reference-contexts: The CM-2 [1], CM-5 [9], Splash 1 and 2 <ref> [7, 2] </ref>, DAP [14], CAL [8] and PAM [5] are a few examples of such machines. The MGAP-1 (Micro Grain Array Processor) is also a fine grain, massively parallel processor which was designed at the Penn State University.
Reference: [8] <author> J. Gray and T. Kean. </author> <title> Configurable Hardware: A New Paradigm for Computation. </title> <booktitle> In Advanced Research in VLSI, Proceedings of the Decennial Caltech Conference on VLSI, </booktitle> <pages> pages 5-16, </pages> <month> March </month> <year> 1989. </year>
Reference-contexts: The CM-2 [1], CM-5 [9], Splash 1 and 2 [7, 2], DAP [14], CAL <ref> [8] </ref> and PAM [5] are a few examples of such machines. The MGAP-1 (Micro Grain Array Processor) is also a fine grain, massively parallel processor which was designed at the Penn State University. It is a single SUN4 coprocessor board which provides almost a teraop of additional computing power.
Reference: [9] <author> W. D. Hillis. </author> <title> The Connection Machine. </title> <publisher> MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1986. </year>
Reference-contexts: 1 Introduction Architectures using fine grain processors have been gaining in popularity over the last decade because of their ability to solve a variety of applications such as signal and image processing, fluid dynamics, computational biology, cryptography, graph problems, and so on. The CM-2 [1], CM-5 <ref> [9] </ref>, Splash 1 and 2 [7, 2], DAP [14], CAL [8] and PAM [5] are a few examples of such machines. The MGAP-1 (Micro Grain Array Processor) is also a fine grain, massively parallel processor which was designed at the Penn State University.
Reference: [10] <author> M. J. Irwin and R. M. Owens. </author> <title> A Micro-Grained VLSI Signal Processor. </title> <booktitle> In ICASSP-92, </booktitle> <pages> pages 641-644, </pages> <month> Mar. </month> <year> 1992. </year>
Reference-contexts: Finally we give some performance figures and conclude in section 4. 2 MGAP Architecture The MGAP is a simple, yet general and flexible architecture, that enables massive parallelism to bear upon many problems in an economical manner <ref> [10, 3, 4, 13, 12, 6] </ref>. The MGAP-1, our first generation implementation, exhibits a peak peformance of 0.8 teraops at 25 MHz.
Reference: [11] <author> C. Nagendra, R.M. Owens, and M.J. Irwin. </author> <title> Digit Systolic Algorithms for Fine-grain Architectures. </title> <booktitle> In Proc. Application Specific Array Processors, </booktitle> <pages> pages 466-477, </pages> <month> Oct </month> <year> 1993. </year>
Reference-contexts: Because of the small local memory the unit of data that Fig. 1. MGAP-1 Block Diagram each processor operates on is limited to a few bits. But by using redundant arithmetic, we have developed algorithms to perform addition in constant time and multiply/divide in linear time <ref> [11] </ref>. 3 Programming the MGAP using C++ We have extended C++ for the MGAP to handle parallel data types and permit exploitation of bit-level granularity. The new features allow programmers to harness the computing power of the MGAP effectively.
Reference: [12] <author> R. M. Owens, M. J. Irwin, C. Nagendra, and R. S. Bajwa. </author> <booktitle> Computer Vision on the MGAP. In Proc. Computer Architectures for Machine Perception, </booktitle> <pages> pages 337-441, </pages> <month> Dec </month> <year> 1993. </year>
Reference-contexts: Finally we give some performance figures and conclude in section 4. 2 MGAP Architecture The MGAP is a simple, yet general and flexible architecture, that enables massive parallelism to bear upon many problems in an economical manner <ref> [10, 3, 4, 13, 12, 6] </ref>. The MGAP-1, our first generation implementation, exhibits a peak peformance of 0.8 teraops at 25 MHz.
Reference: [13] <author> T. P. Kelliher R. M. Owens and M. J. Irwin. </author> <title> Building high performance signal processors cheaply and quickly. </title> <booktitle> In VLSI Signal Processing VI, </booktitle> <month> Oct </month> <year> 1993. </year>
Reference-contexts: Finally we give some performance figures and conclude in section 4. 2 MGAP Architecture The MGAP is a simple, yet general and flexible architecture, that enables massive parallelism to bear upon many problems in an economical manner <ref> [10, 3, 4, 13, 12, 6] </ref>. The MGAP-1, our first generation implementation, exhibits a peak peformance of 0.8 teraops at 25 MHz.
Reference: [14] <author> S. F. Reddaway. </author> <title> DAP A Distributed Array Processor. </title> <booktitle> In Proc. First Annual Symposium on Computer Architecture, </booktitle> <pages> pages 61-65, </pages> <year> 1973. </year>
Reference-contexts: The CM-2 [1], CM-5 [9], Splash 1 and 2 [7, 2], DAP <ref> [14] </ref>, CAL [8] and PAM [5] are a few examples of such machines. The MGAP-1 (Micro Grain Array Processor) is also a fine grain, massively parallel processor which was designed at the Penn State University.
Reference: [15] <author> Bjarne Stroustrup. </author> <title> The C++ Programming Language. </title> <publisher> Addison-Wesley, </publisher> <year> 1987. </year>
Reference-contexts: There is clearly a hierarchy in the organization of data which can be naturally expressed in object-oriented languages. The widespread use and availability of support software for C++ is Fig. 2. Parallel varible A of type shape the main reason for choosing C++ over other object-oriented languages <ref> [15] </ref>. It can be observed that the program add.cc can be compiled using an ordinary C++ compiler. But if it is compiled using our parallelizing *C++ compiler, then all parallel data types are allocated storage on the MGAP-1's micrograin array and all scalar operations are handled by the scalar processor.
Reference: [16] <institution> Thinking Machines Corporation. </institution> <note> Getting Started in C*, </note> <month> February </month> <year> 1991. </year>
Reference-contexts: For example, in C*, which is the language used to program the Connection Machine, the primitive data type is the integer. To operate at finer granularity, one would have to use their low level instruction set, PARIS <ref> [16] </ref>. This is unfortunate since the speed advantage of fine grain systems lies in the exploitation of their fine grain parallelism. Applications for the MGAP can be written using C++ and code targetted for the MGAP can be generated using our parallelizing *C++ compiler. <p> But if it is compiled using our parallelizing *C++ compiler, then all parallel data types are allocated storage on the MGAP-1's micrograin array and all scalar operations are handled by the scalar processor. This is similar to the concept of shapes from C* <ref> [16] </ref>. 3.2 Parallel Operations All the standard operators in C++ are overloaded to handle parallel data. <p> Since we use base-4 operands, a right shift is equivalent to a mulitplication by 4. A left shift, which is a division by 4 can be defined in a similar way. class word - private : snake class digit val <ref> [16] </ref>; - class word sel (class word x, y, bit m) class word shift () - class word z; class word z; int i; z = x; for (i = 0; i &lt; 16; i++) else z.val [i] = z = y; (i &lt; 15) ? val [i+1] : 0; return
References-found: 16

