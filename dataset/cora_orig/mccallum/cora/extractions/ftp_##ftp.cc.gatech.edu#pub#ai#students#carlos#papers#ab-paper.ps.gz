URL: ftp://ftp.cc.gatech.edu/pub/ai/students/carlos/papers/ab-paper.ps.gz
Refering-URL: http://www-anw.cs.umass.edu/~rich/publications.html
Root-URL: 
Email: carlos@cc.gatech.edu  rich@cs.umass.edu  ashwin@cc.gatech.edu  
Phone: Phone: (404) 894-6064 Fax: (404) 894-9846  Phone: (413) 545-0609  Phone: (404) 894-4995  
Title: To appear in the Adaptive Behavior 6:2. Experiments with Reinforcement Learning in Problems with Continuous
Author: Juan C. Santamara Richard S. Sutton Ashwin Ram 
Keyword: Kewords: Reinforcement learning, function approximation, memory-based methods, continuous domains, optimal control, resource preallocation.  
Address: Atlanta, GA 30332-0280  Amherst, MA 01002  Atlanta, GA 30332-0280  
Affiliation: College of Computing Georgia Institute of Technology  Lederle Graduate Research Center University of Massachusetts  College of Computing Georgia Institute of Technology  
Abstract: A key element in the solution of reinforcement learning problems is the value function. The purpose of this function is to measure the long-term utility or value of any given state. The function is important because an agent can use this measure to decide what to do next. A common problem in reinforcement learning when applied to systems having continuous states and action spaces is that the value function must operate with a domain consisting of real-valued variables, which means that it should be able to represent the value of infinitely many state and action pairs. For this reason, function approximators are used to represent the value function when a close-form solution of the optimal policy is not available. In this paper, we extend a previously proposed reinforcement learning algorithm so that it can be used with function approximators that generalize the value of individual experiences across both, state and action spaces. In particular, we discuss the benefits of using sparse coarse-coded function approximators to represent value functions and describe in detail three implementations: CMAC, instance-based, and case-based. Additionally, we discuss how function approximators having different degrees of resolution in different regions of the state and action spaces may influence the performance and learning efficiency of the agent. We propose a simple and modular technique that can be used to implement function approximators with non-uniform degrees of resolution so that it can represent the value function with higher accuracy in important regions of the state and action spaces. We performed extensive experiments in the double integrator and pendulum swing up systems to demonstrate the proposed ideas.
Abstract-found: 1
Intro-found: 1
Reference: <author> Albus, J. S. </author> <year> (1975). </year> <title> A new approach to manipulator control: The cerebellar model articulation controller (cmac). Journal of Dynamic Systems, Measurement, </title> <journal> and Control, </journal> <volume> 97(3) </volume> <pages> 220-227. </pages>
Reference: <author> Atkeson, C. G. </author> <year> (1991). </year> <title> Memory-based learning control. </title> <booktitle> In Proceedings of the 1991 American Control Conference, </booktitle> <volume> volume 3, </volume> <pages> pages 2131-2136, </pages> <address> Boston, MA. </address>
Reference: <author> Barto, A. G., Sutton, R. S., and Anderson, C. W. </author> <year> (1983). </year> <title> Neuronlike elements that can solve difficult learning control problems. </title> <journal> IEEE Transactions on Systems, Man, and Cybernetics, </journal> <volume> 13 </volume> <pages> 835-846. </pages>
Reference: <author> Bellman, R. </author> <year> (1957). </year> <title> Dynamic Programming. </title> <publisher> Princeton University Press, </publisher> <address> Princeton, NJ. </address>
Reference: <author> Bertsekas, D. P. </author> <year> (1995). </year> <title> Dynamic Programming and Optimal Control, volume 1. </title> <publisher> Athena Scientific, </publisher> <address> Belmont, MA. </address>
Reference: <author> Cishosz, P. </author> <year> (1996). </year> <title> Truncating temporal differences: on the efficient implementation of td() for reinforcement learning. </title> <journal> Journal of Artificial Intelligence Research, </journal> <volume> 2 </volume> <pages> 287-318. </pages>
Reference: <author> Kanerva, P. </author> <year> (1993). </year> <title> Sparse distributed memory and related models. </title> <editor> In Hassoun, M. H., editor, </editor> <title> Associative Neural Memories: Theory and Implementation, chapter 3. </title> <publisher> Oxford University Press, </publisher> <address> New York, NY. </address>
Reference: <author> Kibler, D. and Aha, D. W. </author> <year> (1989). </year> <title> Instance-based prediction of real-valued attributes. </title> <journal> Computational Intelligence, </journal> <volume> 5(2) </volume> <pages> 51-57. </pages>
Reference: <author> Lin, L. J. </author> <year> (1992). </year> <title> Self-improving reactive agents based on reinforcement learning. Machine Learning, </title> <publisher> 8(3-4):293-321. </publisher>
Reference: <author> Mahadevan, S. and Connell, J. </author> <year> (1991). </year> <title> Scaling reinforcement learning to robotics by exploiting the subsumption architecture. </title> <booktitle> In Proceedings of the Eight International Workshop on Machine Learning, </booktitle> <volume> volume 1, </volume> <pages> pages 328-332. </pages> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> McCallum, R. A., Tesauro, G., Touretzky, D., and Leen, T. </author> <year> (1995). </year> <title> Instance-based state identification for reinforcement learning. </title> <editor> In Tesauro, G., Touretzky, D., and Leen, T., editors, </editor> <booktitle> Advances in Neural Information Processing Systems 7, </booktitle> <pages> pages 377-384. </pages> <publisher> MIT Press, </publisher> <address> Cambridge, MA. </address>
Reference: <author> Moore, A. W. and Atkeson, C. G. </author> <year> (1995). </year> <title> The parti-game algorithm for variable resolution reinforcement learning in multidimensional state-spaces. </title> <journal> Machine Learning, </journal> <volume> 21(3) </volume> <pages> 199-233. </pages>
Reference: <author> Narendra, K. S. and Annaswamy, A. M. </author> <year> (1989). </year> <title> Stable Adaptive Systems. </title> <publisher> Prentice Hall, </publisher> <address> Englewood Cliffs, NJ. </address>
Reference: <author> Peng, J. </author> <year> (1993). </year> <title> Efficient Dynamic Programming-Based Learning for Control. </title> <type> PhD thesis, </type> <institution> Department of Computer Science, Northeastern University. </institution>
Reference: <author> Peng, J. and Williams, R. J. </author> <year> (1994). </year> <title> Incremental multi-step q-learning. </title> <editor> In Cohen, W. W. and Hirsh, H., editors, </editor> <booktitle> Machine Learning: Proceedings of the Eleventh International Conference, </booktitle> <pages> pages 189-195, </pages> <address> Aberdeen, Scotland. </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Ram, A. and Santamara, J. C. </author> <year> (1997). </year> <title> Continuous case-based reasoning. </title> <journal> Artificial Intelligence, 90(1-2):25-77. </journal>
Reference: <author> Richards, R. J. </author> <year> (1979). </year> <title> An Introduction to Dynamics and Control. </title> <publisher> Longman, </publisher> <address> New York, NY. </address>
Reference: <author> Rummery, G. A. and Niranjan, M. </author> <year> (1994). </year> <title> On-line q-learning using connectionist systems. </title> <type> Technical Report CUED/F-INFEG/TR66, </type> <institution> Cambridge University Department. </institution>
Reference: <author> Shanmugam, K. S. </author> <year> (1979). </year> <title> Digital and Analog Communication Systems. </title> <publisher> John Wiley and Sons, </publisher> <address> New York, NY. </address>
Reference: <author> Singh, S. P. and Sutton, R. S. </author> <year> (1996). </year> <title> Reinforcement learning with replacing eligibility traces. </title> <journal> Machine Learning, </journal> <volume> 22 </volume> <pages> 123-158. </pages>
Reference: <author> Stengel, R. F. </author> <year> (1994). </year> <title> Optimal Control and Estimation. </title> <publisher> Dover Publications, </publisher> <address> Mineola, NY. </address>
Reference: <author> Sutton, R. S. </author> <year> (1988). </year> <title> Learning to predict by the methods of temporal differences. </title> <journal> Machine Learning, </journal> <volume> 3 </volume> <pages> 9-44. </pages>
Reference: <author> Sutton, R. S. </author> <year> (1996). </year> <title> Generalization in reinforcement learning: Successful examples using sparse coarse coding. </title> <editor> In Tesauro, G., Touretzky, D., and Leen, T., editors, </editor> <booktitle> Advances in Neural Information Processing Systems 8, </booktitle> <pages> pages 1038-1044. </pages> <publisher> MIT Press, </publisher> <address> Cambridge, MA. </address>
Reference: <author> Tham, C. L. </author> <year> (1995). </year> <title> Reinforcement learning of multiple tasks using a hierarchical cmac architecture. </title> <booktitle> Robotics and Autonomous Systems, </booktitle> <volume> 15(4) </volume> <pages> 247-274. </pages>
Reference: <author> Tsitsiklis, J. N. and Van Roy, B. </author> <year> (1997). </year> <title> An analysis of temporal-difference learning with function approximation. </title> <journal> IEEE Transactions on Automatic Control, </journal> <volume> 42(5) </volume> <pages> 674-690. </pages>
Reference: <author> Watkins, C. J. C. H. </author> <year> (1989). </year> <title> Learning from Delayed Rewards. </title> <type> PhD thesis, </type> <institution> Univeristy of Cambridge, </institution> <address> England. </address>
References-found: 26

