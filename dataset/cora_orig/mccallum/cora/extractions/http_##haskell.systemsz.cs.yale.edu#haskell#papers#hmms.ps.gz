URL: http://haskell.systemsz.cs.yale.edu/haskell/papers/hmms.ps.gz
Refering-URL: http://haskell.systemsz.cs.yale.edu/haskell/papers/
Root-URL: http://www.cs.yale.edu
Email: dmgob@mitre.org  
Phone: Phone: (703) 883-5450  FAX: (703) 883-6708  
Title: A Software System for Training Phonetic Hidden Markov Models Version 2.0 (*DRAFT*)  
Author: David M. Goblirsch 
Note: Mail Stop: W622 Electronic mail:  (office) or (703) 883-5769 (lab)  
Address: 7525 Colshire Drive McLean, Virginia 22102-3481  
Affiliation: The MITRE Corporation  
Abstract-found: 0
Intro-found: 1
Reference: [1] <author> K.-F. Lee, </author> <title> Automatic Speech Recognition: The Development of the SPHINX System. </title> <publisher> Kluwer Academic Publishers, </publisher> <year> 1989. </year>
Reference-contexts: This report documents those programs written in Haskell. The report assumes familiarity with Haskell and with HMM technology, although Section 1.2 provides some background information about Haskell. Tutorial information on HMMs can be found in <ref> [1] </ref>, [2], or [3]. 1.1 Overview of the Algorithm and Programs To train HMMs, we need a transcribed database of recorded speech utterances. <p> Integers are used in ascending order starting from some positive integer with no skips. (Within the dictionary file, this starting integer will be 1, but under more general circumstances the starting integer could be something different.) 12 according to the conventions listed in the text. [(EH, []), (K, <ref> [1] </ref>), (S, [2]), (G, [1]), (Z, [4]), (IX, [3,5]), (T, [6])] 2. A node's index is always larger than the indices of all the nodes that precede it in the digraph. <p> used in ascending order starting from some positive integer with no skips. (Within the dictionary file, this starting integer will be 1, but under more general circumstances the starting integer could be something different.) 12 according to the conventions listed in the text. [(EH, []), (K, <ref> [1] </ref>), (S, [2]), (G, [1]), (Z, [4]), (IX, [3,5]), (T, [6])] 2. A node's index is always larger than the indices of all the nodes that precede it in the digraph. A node x precedes a node y if there is a directed path starting at x and ending at y [20, p. 198]. <p> Our digraph representation for "EXIT" is shown in Figure 3.3. The Digraph data structure doesn't include information about the initial and terminal nodes, the information needed to completely specify a pronunciation network. To include 13 PrnN 7 <ref> [1] </ref> [7] [(EH, []), (K, [1]), (S, [2]), (G, [1]), (Z, [4]), (IX, [3,5]), (T, [6])] EXIT 7 [1] [7] 2 K [1] 4 G [1] 6 IX [3,5] this information, the pronunciation network is represented using an algebraic datatype. <p> Our digraph representation for "EXIT" is shown in Figure 3.3. The Digraph data structure doesn't include information about the initial and terminal nodes, the information needed to completely specify a pronunciation network. To include 13 PrnN 7 <ref> [1] </ref> [7] [(EH, []), (K, [1]), (S, [2]), (G, [1]), (Z, [4]), (IX, [3,5]), (T, [6])] EXIT 7 [1] [7] 2 K [1] 4 G [1] 6 IX [3,5] this information, the pronunciation network is represented using an algebraic datatype. <p> Our digraph representation for "EXIT" is shown in Figure 3.3. The Digraph data structure doesn't include information about the initial and terminal nodes, the information needed to completely specify a pronunciation network. To include 13 PrnN 7 <ref> [1] </ref> [7] [(EH, []), (K, [1]), (S, [2]), (G, [1]), (Z, [4]), (IX, [3,5]), (T, [6])] EXIT 7 [1] [7] 2 K [1] 4 G [1] 6 IX [3,5] this information, the pronunciation network is represented using an algebraic datatype. <p> The Digraph data structure doesn't include information about the initial and terminal nodes, the information needed to completely specify a pronunciation network. To include 13 PrnN 7 <ref> [1] </ref> [7] [(EH, []), (K, [1]), (S, [2]), (G, [1]), (Z, [4]), (IX, [3,5]), (T, [6])] EXIT 7 [1] [7] 2 K [1] 4 G [1] 6 IX [3,5] this information, the pronunciation network is represented using an algebraic datatype. <p> The Digraph data structure doesn't include information about the initial and terminal nodes, the information needed to completely specify a pronunciation network. To include 13 PrnN 7 <ref> [1] </ref> [7] [(EH, []), (K, [1]), (S, [2]), (G, [1]), (Z, [4]), (IX, [3,5]), (T, [6])] EXIT 7 [1] [7] 2 K [1] 4 G [1] 6 IX [3,5] this information, the pronunciation network is represented using an algebraic datatype. <p> To include 13 PrnN 7 <ref> [1] </ref> [7] [(EH, []), (K, [1]), (S, [2]), (G, [1]), (Z, [4]), (IX, [3,5]), (T, [6])] EXIT 7 [1] [7] 2 K [1] 4 G [1] 6 IX [3,5] this information, the pronunciation network is represented using an algebraic datatype. <p> An example of a dictionary file containing the words "EXIT," "NOW," and "PLEASE" is shown in Figure 3.6. Word representations are separated from each other by blank lines. 14 EXIT 7 <ref> [1] </ref> [7] 2 K [1] 4 G [1] 6 IX [3,5] NOW 1 N [] PLEASE 4 [1] [4] 2 L [1] 4 Z [3] 15 Thus, the user-provided dictionary file is a plain text file that contains the pronunciation networks for all the words in the vocabulary. <p> An example of a dictionary file containing the words "EXIT," "NOW," and "PLEASE" is shown in Figure 3.6. Word representations are separated from each other by blank lines. 14 EXIT 7 <ref> [1] </ref> [7] 2 K [1] 4 G [1] 6 IX [3,5] NOW 1 N [] PLEASE 4 [1] [4] 2 L [1] 4 Z [3] 15 Thus, the user-provided dictionary file is a plain text file that contains the pronunciation networks for all the words in the vocabulary. <p> An example of a dictionary file containing the words "EXIT," "NOW," and "PLEASE" is shown in Figure 3.6. Word representations are separated from each other by blank lines. 14 EXIT 7 <ref> [1] </ref> [7] 2 K [1] 4 G [1] 6 IX [3,5] NOW 1 N [] PLEASE 4 [1] [4] 2 L [1] 4 Z [3] 15 Thus, the user-provided dictionary file is a plain text file that contains the pronunciation networks for all the words in the vocabulary. <p> An example of a dictionary file containing the words "EXIT," "NOW," and "PLEASE" is shown in Figure 3.6. Word representations are separated from each other by blank lines. 14 EXIT 7 <ref> [1] </ref> [7] 2 K [1] 4 G [1] 6 IX [3,5] NOW 1 N [] PLEASE 4 [1] [4] 2 L [1] 4 Z [3] 15 Thus, the user-provided dictionary file is a plain text file that contains the pronunciation networks for all the words in the vocabulary. The program ConvertLinearDic (Chapter 9) can be used to convert a simpler form of dictionary file to this format. <p> Word representations are separated from each other by blank lines. 14 EXIT 7 <ref> [1] </ref> [7] 2 K [1] 4 G [1] 6 IX [3,5] NOW 1 N [] PLEASE 4 [1] [4] 2 L [1] 4 Z [3] 15 Thus, the user-provided dictionary file is a plain text file that contains the pronunciation networks for all the words in the vocabulary. The program ConvertLinearDic (Chapter 9) can be used to convert a simpler form of dictionary file to this format. <p> = readsInt `thenMST` " _ -&gt; -- node index &gt; readsItem `thenMST` " d -&gt; -- node data &gt; readsItem `thenMST` " preds -&gt; -- pred list &gt; returnMST (d, preds) For example, applying readDictionary to the file shown in Figure 3.6 yields the list [ ( "EXIT", PrnN 7 <ref> [1] </ref> [7] [(EH,[]), (K,[1]), (S,[2]), (G,[1]), (Z,[4]), (IX,[3,5]), ( "NOW", PrnN 2 [1] [2] [(N,[]), (AW, [1])] ), ( "PLEASE", PrnN 4 [1] [4] [(P,[]), (L,[1]), (IY,[2]), (Z,[3])] ) 3.4 Building Pronunciation Networks for Complete Utter ances We need to build pronunciation networks for complete utterances, not just single words. <p> " d -&gt; -- node data &gt; readsItem `thenMST` " preds -&gt; -- pred list &gt; returnMST (d, preds) For example, applying readDictionary to the file shown in Figure 3.6 yields the list [ ( "EXIT", PrnN 7 <ref> [1] </ref> [7] [(EH,[]), (K,[1]), (S,[2]), (G,[1]), (Z,[4]), (IX,[3,5]), ( "NOW", PrnN 2 [1] [2] [(N,[]), (AW, [1])] ), ( "PLEASE", PrnN 4 [1] [4] [(P,[]), (L,[1]), (IY,[2]), (Z,[3])] ) 3.4 Building Pronunciation Networks for Complete Utter ances We need to build pronunciation networks for complete utterances, not just single words. This construction is performed as a series of steps. <p> node data &gt; readsItem `thenMST` " preds -&gt; -- pred list &gt; returnMST (d, preds) For example, applying readDictionary to the file shown in Figure 3.6 yields the list [ ( "EXIT", PrnN 7 <ref> [1] </ref> [7] [(EH,[]), (K,[1]), (S,[2]), (G,[1]), (Z,[4]), (IX,[3,5]), ( "NOW", PrnN 2 [1] [2] [(N,[]), (AW, [1])] ), ( "PLEASE", PrnN 4 [1] [4] [(P,[]), (L,[1]), (IY,[2]), (Z,[3])] ) 3.4 Building Pronunciation Networks for Complete Utter ances We need to build pronunciation networks for complete utterances, not just single words. This construction is performed as a series of steps. <p> preds -&gt; -- pred list &gt; returnMST (d, preds) For example, applying readDictionary to the file shown in Figure 3.6 yields the list [ ( "EXIT", PrnN 7 <ref> [1] </ref> [7] [(EH,[]), (K,[1]), (S,[2]), (G,[1]), (Z,[4]), (IX,[3,5]), ( "NOW", PrnN 2 [1] [2] [(N,[]), (AW, [1])] ), ( "PLEASE", PrnN 4 [1] [4] [(P,[]), (L,[1]), (IY,[2]), (Z,[3])] ) 3.4 Building Pronunciation Networks for Complete Utter ances We need to build pronunciation networks for complete utterances, not just single words. This construction is performed as a series of steps. <p> For now this is just silence, represented by the constructor SIL. Later, we might want to enhance the model to include filled pauses, e.g., "ums" and "ahs." &gt; between_word_model = PrnN 1 <ref> [1] </ref> [1] [(SIL,[])] The function interleave in the library module Lists (Chapter refch:Lists) can be used to interleave the between-word model through the list of words. 3.4.4 Reindexing Pronunciation Networks The function reindexPrnNetwork is used to increment the node indices by a fixed amount. <p> For now this is just silence, represented by the constructor SIL. Later, we might want to enhance the model to include filled pauses, e.g., "ums" and "ahs." &gt; between_word_model = PrnN 1 <ref> [1] </ref> [1] [(SIL,[])] The function interleave in the library module Lists (Chapter refch:Lists) can be used to interleave the between-word model through the list of words. 3.4.4 Reindexing Pronunciation Networks The function reindexPrnNetwork is used to increment the node indices by a fixed amount. <p> shows d (" "t" ++ show ps)) Using this pretty-printing function, the results of processing the text "SHOW DEPARTURES FROM ATLANTA FOR AMERICAN," one of the Air Travel Information System (ATIS) sentences from the ATIS2 database distributed by the Linguistic Data Consortium (LDC), is shown in Figure 3.10. 25 40 <ref> [1, 2] </ref> [39, 40] 2 (SH, 1) [1] 4 (SIL, 1) [3] 6 (AX, 1) [5] 8 (AA, 1) [7] 10 (CH, 1) [9] 12 (Z, 2) [11] 14 (F, 1) [12, 13] 16 (AH, 1) [15] 18 (SIL, 1) [17] 20 (T, 1) [19] 22 (AE, 1) [21] 24 (T, <p> Using this pretty-printing function, the results of processing the text "SHOW DEPARTURES FROM ATLANTA FOR AMERICAN," one of the Air Travel Information System (ATIS) sentences from the ATIS2 database distributed by the Linguistic Data Consortium (LDC), is shown in Figure 3.10. 25 40 [1, 2] [39, 40] 2 (SH, 1) <ref> [1] </ref> 4 (SIL, 1) [3] 6 (AX, 1) [5] 8 (AA, 1) [7] 10 (CH, 1) [9] 12 (Z, 2) [11] 14 (F, 1) [12, 13] 16 (AH, 1) [15] 18 (SIL, 1) [17] 20 (T, 1) [19] 22 (AE, 1) [21] 24 (T, 1) [23] 26 (SIL, 1) [25] 28 <p> We are only concerned with the topology in this module because for the subword modeling of utterances we do not need to know the nature of the observations, e.g., whether they are discrete vector-quantizer indices (e.g., <ref> [1] </ref>) or continuous-valued vectors (e.g., [19]). <p> Thus, the HMM of Figure 4.1 is a 3-state HMM. The states correspond to observations. That is, each state is associated with an observation probability model. Such a model might be a probability mass function over the indices of codewords in a vector quantizer (e.g., <ref> [1] </ref>) or a multivariate probability density function (e.g., [19]); the particular form of the observation model is not important for this module. The smaller, filled circle on the left side of the figure is the entry node and is a useful graphical device for identifying the starting states. <p> That is, it takes a dictionary with entries in the format A AX ABLE EY B EL ABLY EY B L IY . . . and produces a dictionary with entries in the format A 1 AX [] ABLE 3 <ref> [1] </ref> [3] 2 B [1] ABLY 4 [1] [4] 2 B [1] 4 IY [3] 56 &gt; module Main where &gt; import Phones &gt; main = getArgs exit $ "args -&gt; &gt; case args of &gt; [linear_dic_file] -&gt; let &gt; outfile = linear_dic_file ++ ".dgs" &gt; in &gt; readFile linear_dic_file exit <p> That is, it takes a dictionary with entries in the format A AX ABLE EY B EL ABLY EY B L IY . . . and produces a dictionary with entries in the format A 1 AX [] ABLE 3 <ref> [1] </ref> [3] 2 B [1] ABLY 4 [1] [4] 2 B [1] 4 IY [3] 56 &gt; module Main where &gt; import Phones &gt; main = getArgs exit $ "args -&gt; &gt; case args of &gt; [linear_dic_file] -&gt; let &gt; outfile = linear_dic_file ++ ".dgs" &gt; in &gt; readFile linear_dic_file exit $ "cs -&gt; &gt; <p> That is, it takes a dictionary with entries in the format A AX ABLE EY B EL ABLY EY B L IY . . . and produces a dictionary with entries in the format A 1 AX [] ABLE 3 <ref> [1] </ref> [3] 2 B [1] ABLY 4 [1] [4] 2 B [1] 4 IY [3] 56 &gt; module Main where &gt; import Phones &gt; main = getArgs exit $ "args -&gt; &gt; case args of &gt; [linear_dic_file] -&gt; let &gt; outfile = linear_dic_file ++ ".dgs" &gt; in &gt; readFile linear_dic_file exit $ "cs -&gt; &gt; writeFile outfile (process <p> is, it takes a dictionary with entries in the format A AX ABLE EY B EL ABLY EY B L IY . . . and produces a dictionary with entries in the format A 1 AX [] ABLE 3 <ref> [1] </ref> [3] 2 B [1] ABLY 4 [1] [4] 2 B [1] 4 IY [3] 56 &gt; module Main where &gt; import Phones &gt; main = getArgs exit $ "args -&gt; &gt; case args of &gt; [linear_dic_file] -&gt; let &gt; outfile = linear_dic_file ++ ".dgs" &gt; in &gt; readFile linear_dic_file exit $ "cs -&gt; &gt; writeFile outfile (process cs) exit $ &gt; <p> lines &gt; build_and_show_dg :: LinearEntry -&gt; String &gt; build_and_show_dg (w,ps) = &gt; let &gt; num_nodes = length ps &gt; term_nodes = [num_nodes] &gt; node_lines = map show_node_line ( &gt; zip3 [1..] ps ([] : [[k] | k &lt;- [1..]]) ) &gt; w ++ ""n" ++ &gt; show num_nodes ++ " <ref> [1] </ref> " ++ show term_nodes ++ ""n" ++ &gt; unlines node_lines ++ ""n" 57 &gt; show_node_line (k, p, ps) = &gt; show k ++ ""t" ++ show p ++ ""t" ++ show ps 58 Chapter 10 "Transcribe" The program "Transcribe" is intended for checking the pronunciation modeling functions on single files.
Reference: [2] <author> L. R. Rabiner, </author> <title> "A tutorial on hidden Markov models and selected applications in speech recognition," </title> <journal> Proc. of the IEEE, </journal> <volume> vol. 77, </volume> <pages> pp. 257-286, </pages> <month> February </month> <year> 1989. </year>
Reference-contexts: This report documents those programs written in Haskell. The report assumes familiarity with Haskell and with HMM technology, although Section 1.2 provides some background information about Haskell. Tutorial information on HMMs can be found in [1], <ref> [2] </ref>, or [3]. 1.1 Overview of the Algorithm and Programs To train HMMs, we need a transcribed database of recorded speech utterances. For each utterance there must originally be two files: a waveform file (the sampled and digitized microphone signal) and a textual transcription file (the words that were spoken). <p> Integers are used in ascending order starting from some positive integer with no skips. (Within the dictionary file, this starting integer will be 1, but under more general circumstances the starting integer could be something different.) 12 according to the conventions listed in the text. [(EH, []), (K, [1]), (S, <ref> [2] </ref>), (G, [1]), (Z, [4]), (IX, [3,5]), (T, [6])] 2. A node's index is always larger than the indices of all the nodes that precede it in the digraph. <p> Our digraph representation for "EXIT" is shown in Figure 3.3. The Digraph data structure doesn't include information about the initial and terminal nodes, the information needed to completely specify a pronunciation network. To include 13 PrnN 7 [1] [7] [(EH, []), (K, [1]), (S, <ref> [2] </ref>), (G, [1]), (Z, [4]), (IX, [3,5]), (T, [6])] EXIT 7 [1] [7] 2 K [1] 4 G [1] 6 IX [3,5] this information, the pronunciation network is represented using an algebraic datatype. <p> d -&gt; -- node data &gt; readsItem `thenMST` " preds -&gt; -- pred list &gt; returnMST (d, preds) For example, applying readDictionary to the file shown in Figure 3.6 yields the list [ ( "EXIT", PrnN 7 [1] [7] [(EH,[]), (K,[1]), (S,<ref> [2] </ref>), (G,[1]), (Z,[4]), (IX,[3,5]), ( "NOW", PrnN 2 [1] [2] [(N,[]), (AW, [1])] ), ( "PLEASE", PrnN 4 [1] [4] [(P,[]), (L,[1]), (IY,[2]), (Z,[3])] ) 3.4 Building Pronunciation Networks for Complete Utter ances We need to build pronunciation networks for complete utterances, not just single words. This construction is performed as a series of steps. <p> shows d (" "t" ++ show ps)) Using this pretty-printing function, the results of processing the text "SHOW DEPARTURES FROM ATLANTA FOR AMERICAN," one of the Air Travel Information System (ATIS) sentences from the ATIS2 database distributed by the Linguistic Data Consortium (LDC), is shown in Figure 3.10. 25 40 <ref> [1, 2] </ref> [39, 40] 2 (SH, 1) [1] 4 (SIL, 1) [3] 6 (AX, 1) [5] 8 (AA, 1) [7] 10 (CH, 1) [9] 12 (Z, 2) [11] 14 (F, 1) [12, 13] 16 (AH, 1) [15] 18 (SIL, 1) [17] 20 (T, 1) [19] 22 (AE, 1) [21] 24 (T,
Reference: [3] <author> L. Rabiner and B.-H. Juang, </author> <title> Fundamentals of Speech Recognition. </title> <publisher> Prentice Hall Signal Processing Series, Prentice Hall, </publisher> <year> 1993. </year>
Reference-contexts: This report documents those programs written in Haskell. The report assumes familiarity with Haskell and with HMM technology, although Section 1.2 provides some background information about Haskell. Tutorial information on HMMs can be found in [1], [2], or <ref> [3] </ref>. 1.1 Overview of the Algorithm and Programs To train HMMs, we need a transcribed database of recorded speech utterances. For each utterance there must originally be two files: a waveform file (the sampled and digitized microphone signal) and a textual transcription file (the words that were spoken). <p> T | K | B | D | G | -- stops &gt; DX | -- flap &gt; -- between-word sounds -- &gt; SIL -- silence &gt; deriving (Eq, Ord, Enum, Ix, Text) The data constructors are the same as a subset of the symbols in the "ARPAbet" phonetic alphabet <ref> [3, Table 2.1, p. 24] </ref>, except for the "SIL" constructor which is represented in the ARPAbet as "h#." As in [18, Table II], [3, Table 2.1], and [19, Table I], the relationships between the phonetic symbols and the sounds they represent are shown by example in Table 2.1. <p> SIL -- silence &gt; deriving (Eq, Ord, Enum, Ix, Text) The data constructors are the same as a subset of the symbols in the "ARPAbet" phonetic alphabet [3, Table 2.1, p. 24], except for the "SIL" constructor which is represented in the ARPAbet as "h#." As in [18, Table II], <ref> [3, Table 2.1] </ref>, and [19, Table I], the relationships between the phonetic symbols and the sounds they represent are shown by example in Table 2.1. The variable phone_bounds is a pair comprised of the first and last phones. <p> Word representations are separated from each other by blank lines. 14 EXIT 7 [1] [7] 2 K [1] 4 G [1] 6 IX [3,5] NOW 1 N [] PLEASE 4 [1] [4] 2 L [1] 4 Z <ref> [3] </ref> 15 Thus, the user-provided dictionary file is a plain text file that contains the pronunciation networks for all the words in the vocabulary. The program ConvertLinearDic (Chapter 9) can be used to convert a simpler form of dictionary file to this format. <p> the results of processing the text "SHOW DEPARTURES FROM ATLANTA FOR AMERICAN," one of the Air Travel Information System (ATIS) sentences from the ATIS2 database distributed by the Linguistic Data Consortium (LDC), is shown in Figure 3.10. 25 40 [1, 2] [39, 40] 2 (SH, 1) [1] 4 (SIL, 1) <ref> [3] </ref> 6 (AX, 1) [5] 8 (AA, 1) [7] 10 (CH, 1) [9] 12 (Z, 2) [11] 14 (F, 1) [12, 13] 16 (AH, 1) [15] 18 (SIL, 1) [17] 20 (T, 1) [19] 22 (AE, 1) [21] 24 (T, 1) [23] 26 (SIL, 1) [25] 28 (AO, 1) [27] 30 <p> &gt; module Viterbi ( HmmDigraphs.., HmmDensities.., align ) where &gt; import Extrema &gt; import Lists &gt; import StateT &gt; import HmmDigraphs &gt; import HmmDensities 6.1 Mathematical Description The Viterbi algorithm for estimating the hidden state sequence given a sequence of observations using logarithmic arithmetic is explained on page 340 of <ref> [3] </ref>. We present a slightly modified form here. The state estimation problem is also sometimes called the alignment problem since we are aligning the observation vectors with a path through the HMM network. We begin by defining some notation. As in [3], the placement of a tilde over a symbol reminds <p> using logarithmic arithmetic is explained on page 340 of <ref> [3] </ref>. We present a slightly modified form here. The state estimation problem is also sometimes called the alignment problem since we are aligning the observation vectors with a path through the HMM network. We begin by defining some notation. As in [3], the placement of a tilde over a symbol reminds us that it is a log value. <p> That is, it takes a dictionary with entries in the format A AX ABLE EY B EL ABLY EY B L IY . . . and produces a dictionary with entries in the format A 1 AX [] ABLE 3 [1] <ref> [3] </ref> 2 B [1] ABLY 4 [1] [4] 2 B [1] 4 IY [3] 56 &gt; module Main where &gt; import Phones &gt; main = getArgs exit $ "args -&gt; &gt; case args of &gt; [linear_dic_file] -&gt; let &gt; outfile = linear_dic_file ++ ".dgs" &gt; in &gt; readFile linear_dic_file exit $ <p> a dictionary with entries in the format A AX ABLE EY B EL ABLY EY B L IY . . . and produces a dictionary with entries in the format A 1 AX [] ABLE 3 [1] <ref> [3] </ref> 2 B [1] ABLY 4 [1] [4] 2 B [1] 4 IY [3] 56 &gt; module Main where &gt; import Phones &gt; main = getArgs exit $ "args -&gt; &gt; case args of &gt; [linear_dic_file] -&gt; let &gt; outfile = linear_dic_file ++ ".dgs" &gt; in &gt; readFile linear_dic_file exit $ "cs -&gt; &gt; writeFile outfile (process cs) exit $ &gt; appendChan stderr (
Reference: [4] <author> B.-H. Juang and L. R. Rabiner, </author> <title> "The segmental k-means algorithm for estimating parameters of hidden Markov models," </title> <journal> IEEE Trans. on Acoustics, Speech, and Signal Processing, </journal> <volume> vol. 38, </volume> <pages> pp. 1639-1641, </pages> <month> September </month> <year> 1990. </year>
Reference-contexts: ascending order starting from some positive integer with no skips. (Within the dictionary file, this starting integer will be 1, but under more general circumstances the starting integer could be something different.) 12 according to the conventions listed in the text. [(EH, []), (K, [1]), (S, [2]), (G, [1]), (Z, <ref> [4] </ref>), (IX, [3,5]), (T, [6])] 2. A node's index is always larger than the indices of all the nodes that precede it in the digraph. A node x precedes a node y if there is a directed path starting at x and ending at y [20, p. 198]. <p> Our digraph representation for "EXIT" is shown in Figure 3.3. The Digraph data structure doesn't include information about the initial and terminal nodes, the information needed to completely specify a pronunciation network. To include 13 PrnN 7 [1] [7] [(EH, []), (K, [1]), (S, [2]), (G, [1]), (Z, <ref> [4] </ref>), (IX, [3,5]), (T, [6])] EXIT 7 [1] [7] 2 K [1] 4 G [1] 6 IX [3,5] this information, the pronunciation network is represented using an algebraic datatype. <p> An example of a dictionary file containing the words "EXIT," "NOW," and "PLEASE" is shown in Figure 3.6. Word representations are separated from each other by blank lines. 14 EXIT 7 [1] [7] 2 K [1] 4 G [1] 6 IX [3,5] NOW 1 N [] PLEASE 4 [1] <ref> [4] </ref> 2 L [1] 4 Z [3] 15 Thus, the user-provided dictionary file is a plain text file that contains the pronunciation networks for all the words in the vocabulary. The program ConvertLinearDic (Chapter 9) can be used to convert a simpler form of dictionary file to this format. <p> -&gt; -- pred list &gt; returnMST (d, preds) For example, applying readDictionary to the file shown in Figure 3.6 yields the list [ ( "EXIT", PrnN 7 [1] [7] [(EH,[]), (K,[1]), (S,[2]), (G,[1]), (Z,<ref> [4] </ref>), (IX,[3,5]), ( "NOW", PrnN 2 [1] [2] [(N,[]), (AW, [1])] ), ( "PLEASE", PrnN 4 [1] [4] [(P,[]), (L,[1]), (IY,[2]), (Z,[3])] ) 3.4 Building Pronunciation Networks for Complete Utter ances We need to build pronunciation networks for complete utterances, not just single words. This construction is performed as a series of steps. <p> That is, it takes a dictionary with entries in the format A AX ABLE EY B EL ABLY EY B L IY . . . and produces a dictionary with entries in the format A 1 AX [] ABLE 3 [1] [3] 2 B [1] ABLY 4 [1] <ref> [4] </ref> 2 B [1] 4 IY [3] 56 &gt; module Main where &gt; import Phones &gt; main = getArgs exit $ "args -&gt; &gt; case args of &gt; [linear_dic_file] -&gt; let &gt; outfile = linear_dic_file ++ ".dgs" &gt; in &gt; readFile linear_dic_file exit $ "cs -&gt; &gt; writeFile outfile (process cs)
Reference: [5] <author> L. R. Rabiner, J. G. Wilpon, and B. H. Juang, </author> <title> "A segmental k-means training procedure for connected word recognition," </title> <journal> AT&T Technical Journal, </journal> <volume> vol. 64, </volume> <pages> pp. 21-40, </pages> <month> May </month> <year> 1986. </year>
Reference-contexts: the text "SHOW DEPARTURES FROM ATLANTA FOR AMERICAN," one of the Air Travel Information System (ATIS) sentences from the ATIS2 database distributed by the Linguistic Data Consortium (LDC), is shown in Figure 3.10. 25 40 [1, 2] [39, 40] 2 (SH, 1) [1] 4 (SIL, 1) [3] 6 (AX, 1) <ref> [5] </ref> 8 (AA, 1) [7] 10 (CH, 1) [9] 12 (Z, 2) [11] 14 (F, 1) [12, 13] 16 (AH, 1) [15] 18 (SIL, 1) [17] 20 (T, 1) [19] 22 (AE, 1) [21] 24 (T, 1) [23] 26 (SIL, 1) [25] 28 (AO, 1) [27] 30 (AXR, 2) [27] 32
Reference: [6] <author> D. M. Goblirsch and T. A. Albina, "Hark: </author> <title> an experimental speech recognition system," </title> <booktitle> in ICSLP 92 Proceedings: 1992 Int. Conf. on Spoken Language Processing, </booktitle> <address> (Banff, Alberta, Canada), </address> <pages> pp. 1507-1510, </pages> <month> Oct. </month> <year> 1992. </year>
Reference-contexts: Third, the sampled waveforms must be processed to produce feature vectors which summarize the information-bearing part of the waveform and suppress irrelevant information. The feature vector file is obtained using a signal processing program written in C that was developed as part of another MITRE ASR system <ref> [6] </ref>; that program is not described in this report. 4 The second stage of the training process consists of repeating the following steps until some convergence criterion is satisfied. <p> some positive integer with no skips. (Within the dictionary file, this starting integer will be 1, but under more general circumstances the starting integer could be something different.) 12 according to the conventions listed in the text. [(EH, []), (K, [1]), (S, [2]), (G, [1]), (Z, [4]), (IX, [3,5]), (T, <ref> [6] </ref>)] 2. A node's index is always larger than the indices of all the nodes that precede it in the digraph. A node x precedes a node y if there is a directed path starting at x and ending at y [20, p. 198]. <p> The Digraph data structure doesn't include information about the initial and terminal nodes, the information needed to completely specify a pronunciation network. To include 13 PrnN 7 [1] [7] [(EH, []), (K, [1]), (S, [2]), (G, [1]), (Z, [4]), (IX, [3,5]), (T, <ref> [6] </ref>)] EXIT 7 [1] [7] 2 K [1] 4 G [1] 6 IX [3,5] this information, the pronunciation network is represented using an algebraic datatype.
Reference: [7] <editor> P. Hudak et al, </editor> <title> "Report on the programming language Haskell, a non-strict, purely functional language, version 1.2," </title> <journal> ACM SIGPLAN Notices, </journal> <volume> vol. 27, </volume> <month> May </month> <year> 1992. </year>
Reference-contexts: It was designed by an international committee of computer scientists to provide a common lazy functional language suitable for teaching, research, and applications. Work began in October 1987 and a formal report was published in the ACM SIGPLAN Notices in May 1992 <ref> [7] </ref>. The language is named for the American mathematician Haskell Brookes Curry (1900-1982), one of the founders of Combinatory Logic. Brief biographies of Dr. Curry can be found in [8] or [9]. A very readable introduction to Combinatory Logic can be found in [10]. <p> Our digraph representation for "EXIT" is shown in Figure 3.3. The Digraph data structure doesn't include information about the initial and terminal nodes, the information needed to completely specify a pronunciation network. To include 13 PrnN 7 [1] <ref> [7] </ref> [(EH, []), (K, [1]), (S, [2]), (G, [1]), (Z, [4]), (IX, [3,5]), (T, [6])] EXIT 7 [1] [7] 2 K [1] 4 G [1] 6 IX [3,5] this information, the pronunciation network is represented using an algebraic datatype. <p> The Digraph data structure doesn't include information about the initial and terminal nodes, the information needed to completely specify a pronunciation network. To include 13 PrnN 7 [1] <ref> [7] </ref> [(EH, []), (K, [1]), (S, [2]), (G, [1]), (Z, [4]), (IX, [3,5]), (T, [6])] EXIT 7 [1] [7] 2 K [1] 4 G [1] 6 IX [3,5] this information, the pronunciation network is represented using an algebraic datatype. <p> An example of a dictionary file containing the words "EXIT," "NOW," and "PLEASE" is shown in Figure 3.6. Word representations are separated from each other by blank lines. 14 EXIT 7 [1] <ref> [7] </ref> 2 K [1] 4 G [1] 6 IX [3,5] NOW 1 N [] PLEASE 4 [1] [4] 2 L [1] 4 Z [3] 15 Thus, the user-provided dictionary file is a plain text file that contains the pronunciation networks for all the words in the vocabulary. <p> readsInt `thenMST` " _ -&gt; -- node index &gt; readsItem `thenMST` " d -&gt; -- node data &gt; readsItem `thenMST` " preds -&gt; -- pred list &gt; returnMST (d, preds) For example, applying readDictionary to the file shown in Figure 3.6 yields the list [ ( "EXIT", PrnN 7 [1] <ref> [7] </ref> [(EH,[]), (K,[1]), (S,[2]), (G,[1]), (Z,[4]), (IX,[3,5]), ( "NOW", PrnN 2 [1] [2] [(N,[]), (AW, [1])] ), ( "PLEASE", PrnN 4 [1] [4] [(P,[]), (L,[1]), (IY,[2]), (Z,[3])] ) 3.4 Building Pronunciation Networks for Complete Utter ances We need to build pronunciation networks for complete utterances, not just single words. <p> FROM ATLANTA FOR AMERICAN," one of the Air Travel Information System (ATIS) sentences from the ATIS2 database distributed by the Linguistic Data Consortium (LDC), is shown in Figure 3.10. 25 40 [1, 2] [39, 40] 2 (SH, 1) [1] 4 (SIL, 1) [3] 6 (AX, 1) [5] 8 (AA, 1) <ref> [7] </ref> 10 (CH, 1) [9] 12 (Z, 2) [11] 14 (F, 1) [12, 13] 16 (AH, 1) [15] 18 (SIL, 1) [17] 20 (T, 1) [19] 22 (AE, 1) [21] 24 (T, 1) [23] 26 (SIL, 1) [25] 28 (AO, 1) [27] 30 (AXR, 2) [27] 32 (AX, 1) [29, 30,
Reference: [8] <editor> J. P. Seldin and J. R. Hindley, eds., To H. B. </editor> <booktitle> Curry: Essays on Combinatory Logic, Lambda Calculus and Formalism. </booktitle> <publisher> Academic Press, </publisher> <year> 1980. </year>
Reference-contexts: Work began in October 1987 and a formal report was published in the ACM SIGPLAN Notices in May 1992 [7]. The language is named for the American mathematician Haskell Brookes Curry (1900-1982), one of the founders of Combinatory Logic. Brief biographies of Dr. Curry can be found in <ref> [8] </ref> or [9]. A very readable introduction to Combinatory Logic can be found in [10]. Although most speech research software has been and continues to be developed in C, we elected to use Haskell for part of our system for a number of reasons.
Reference: [9] <editor> J. P. Seldin, "In memorium: Haskell Brooks Curry," </editor> <booktitle> in Perspectives on the History of Mathematical Logic (T. Drucker, </booktitle> <publisher> ed.), </publisher> <pages> pp. 169-175, </pages> <publisher> Birkhauser, </publisher> <year> 1991. </year>
Reference-contexts: The language is named for the American mathematician Haskell Brookes Curry (1900-1982), one of the founders of Combinatory Logic. Brief biographies of Dr. Curry can be found in [8] or <ref> [9] </ref>. A very readable introduction to Combinatory Logic can be found in [10]. Although most speech research software has been and continues to be developed in C, we elected to use Haskell for part of our system for a number of reasons. <p> one of the Air Travel Information System (ATIS) sentences from the ATIS2 database distributed by the Linguistic Data Consortium (LDC), is shown in Figure 3.10. 25 40 [1, 2] [39, 40] 2 (SH, 1) [1] 4 (SIL, 1) [3] 6 (AX, 1) [5] 8 (AA, 1) [7] 10 (CH, 1) <ref> [9] </ref> 12 (Z, 2) [11] 14 (F, 1) [12, 13] 16 (AH, 1) [15] 18 (SIL, 1) [17] 20 (T, 1) [19] 22 (AE, 1) [21] 24 (T, 1) [23] 26 (SIL, 1) [25] 28 (AO, 1) [27] 30 (AXR, 2) [27] 32 (AX, 1) [29, 30, 31] 34 (EH, 1)
Reference: [10] <author> S. Stenlund, </author> <title> Combinators, -Terms and Proof Theory. </title> <address> D. </address> <publisher> Reidel, </publisher> <year> 1972. </year>
Reference-contexts: The language is named for the American mathematician Haskell Brookes Curry (1900-1982), one of the founders of Combinatory Logic. Brief biographies of Dr. Curry can be found in [8] or [9]. A very readable introduction to Combinatory Logic can be found in <ref> [10] </ref>. Although most speech research software has been and continues to be developed in C, we elected to use Haskell for part of our system for a number of reasons. First, Haskell allows the programmer to work at a higher level of abstraction.
Reference: [11] <author> J. Hughes, </author> <title> "Why functional programming matters," </title> <booktitle> in Research Topics in Functional Programming (D. </booktitle> <editor> A. Turner, </editor> <publisher> ed.), </publisher> <pages> pp. 17-42, </pages> <publisher> Addison-Wesley, </publisher> <year> 1990. </year>
Reference-contexts: For example, it is often possible to develop and modify code using algebraic manipulations. Second, Haskell makes software reuse easier by providing a polymorphic type system and high-order functions. Third, Haskell uses lazy evaluation which enhances modularity <ref> [11] </ref>. Fourth, Haskell is strongly and statically typed so many programming errors are discovered at compile time. Fifth, Haskell supports literate programming, by which we mean the careful integration of source code and documentation. <p> Travel Information System (ATIS) sentences from the ATIS2 database distributed by the Linguistic Data Consortium (LDC), is shown in Figure 3.10. 25 40 [1, 2] [39, 40] 2 (SH, 1) [1] 4 (SIL, 1) [3] 6 (AX, 1) [5] 8 (AA, 1) [7] 10 (CH, 1) [9] 12 (Z, 2) <ref> [11] </ref> 14 (F, 1) [12, 13] 16 (AH, 1) [15] 18 (SIL, 1) [17] 20 (T, 1) [19] 22 (AE, 1) [21] 24 (T, 1) [23] 26 (SIL, 1) [25] 28 (AO, 1) [27] 30 (AXR, 2) [27] 32 (AX, 1) [29, 30, 31] 34 (EH, 1) [33] 36 (IX, 1)
Reference: [12] <author> P. Hudak and J. H. Fasel, </author> <title> "A gentle introduction to Haskell," </title> <journal> ACM SIGPLAN Notices, </journal> <volume> vol. 27, </volume> <month> May </month> <year> 1992. </year>
Reference-contexts: Some of the programs have also been tested with version 0.19 of the Glasgow system, although a few changes need to be made (specifically, some additional import statements are required in some of the modules). References <ref> [12] </ref> and [13] provide tutorial guides to using Haskell. The interactive Yale system has an on-line tutorial. However, those unfamiliar with functional programming will probably need to consult more general textbooks first, e.g., [14-17]. There is an electronic mailing list for discussion about Haskell. <p> sentences from the ATIS2 database distributed by the Linguistic Data Consortium (LDC), is shown in Figure 3.10. 25 40 [1, 2] [39, 40] 2 (SH, 1) [1] 4 (SIL, 1) [3] 6 (AX, 1) [5] 8 (AA, 1) [7] 10 (CH, 1) [9] 12 (Z, 2) [11] 14 (F, 1) <ref> [12, 13] </ref> 16 (AH, 1) [15] 18 (SIL, 1) [17] 20 (T, 1) [19] 22 (AE, 1) [21] 24 (T, 1) [23] 26 (SIL, 1) [25] 28 (AO, 1) [27] 30 (AXR, 2) [27] 32 (AX, 1) [29, 30, 31] 34 (EH, 1) [33] 36 (IX, 1) [35] 38 (IX, 1)
Reference: [13] <author> A. J. T. Davie, </author> <title> An Introduction to Functional Programming Systems using Haskell. Cambridge Texts in Computer Science, </title> <publisher> Cambridge Univeristy Press, </publisher> <year> 1992. </year> <month> 93 </month>
Reference-contexts: Some of the programs have also been tested with version 0.19 of the Glasgow system, although a few changes need to be made (specifically, some additional import statements are required in some of the modules). References [12] and <ref> [13] </ref> provide tutorial guides to using Haskell. The interactive Yale system has an on-line tutorial. However, those unfamiliar with functional programming will probably need to consult more general textbooks first, e.g., [14-17]. There is an electronic mailing list for discussion about Haskell. To join, send a request to haskell-request@cs.yale.edu. <p> sentences from the ATIS2 database distributed by the Linguistic Data Consortium (LDC), is shown in Figure 3.10. 25 40 [1, 2] [39, 40] 2 (SH, 1) [1] 4 (SIL, 1) [3] 6 (AX, 1) [5] 8 (AA, 1) [7] 10 (CH, 1) [9] 12 (Z, 2) [11] 14 (F, 1) <ref> [12, 13] </ref> 16 (AH, 1) [15] 18 (SIL, 1) [17] 20 (T, 1) [19] 22 (AE, 1) [21] 24 (T, 1) [23] 26 (SIL, 1) [25] 28 (AO, 1) [27] 30 (AXR, 2) [27] 32 (AX, 1) [29, 30, 31] 34 (EH, 1) [33] 36 (IX, 1) [35] 38 (IX, 1)
Reference: [14] <author> R. Bird and P. Wadler, </author> <title> Introduction to Functional Programming. </title> <publisher> Prentice Hall inter-national series in computer science, Prentice Hall, </publisher> <year> 1988. </year>
Reference-contexts: For training hidden Markov models on a large set of training utterances with a large vocabulary, it is necessary to provide efficient retrieval of word pronunciation models. This section describes a Haskell implementation of balanced binary search trees as described in <ref> [14] </ref> that is sufficient for our needs (i.e., we don't implement all of the functions normally associated with the abstract type). 16.1 The Balanced Binary Search Tree Datatype While the data type and functions are basically those of Bird & Wadler [14, Chapter 9], we have extended their search tree structure <p> implementation of balanced binary search trees as described in [14] that is sufficient for our needs (i.e., we don't implement all of the functions normally associated with the abstract type). 16.1 The Balanced Binary Search Tree Datatype While the data type and functions are basically those of Bird & Wadler <ref> [14, Chapter 9] </ref>, we have extended their search tree structure by tagging each node in the tree with two values instead of one: a key and a definition. <p> If the key is already in the tree, an error is signaled and evaluation is halted. The definition of bbstInsert follows the definition of insert of <ref> [14, p. 255] </ref> except for the way we handle duplicate keys. &gt; bbstInsert :: (Ord a) =&gt; BalBinSTree a b -&gt; (a,b) -&gt; &gt; BalBinSTree a b &gt; bbstInsert Nil (x,d) = Node x d Nil Nil &gt; bbstInsert (Node y e l r) (x,d) &gt; | x &lt; y = <p> The definition of this function follows the definition of insert of <ref> [14, p. 255] </ref>. 77 &gt; bbstInsertQuiet :: (Ord a) =&gt; BalBinSTree a b -&gt; (a,b) -&gt; &gt; BalBinSTree a b &gt; bbstInsertQuiet Nil (x,d) = Node x d Nil Nil &gt; bbstInsertQuiet t@(Node y e l r) a@(x,d) &gt; | x &lt; y = rebalance (Node y e (bbstInsertQuiet l a) <p> This is the function rebal of <ref> [14, p. 255] </ref>. &gt; rebalance :: BalBinSTree a b -&gt; BalBinSTree a b &gt; rebalance t = case slope t of &gt; 2 -&gt; shift_right t &gt; -2 -&gt; shift_left t &gt; _ -&gt; t The function slope is the function slope of [14, p. 253]. &gt; slope :: BalBinSTree a <p> This is the function rebal of [14, p. 255]. &gt; rebalance :: BalBinSTree a b -&gt; BalBinSTree a b &gt; rebalance t = case slope t of &gt; 2 -&gt; shift_right t &gt; -2 -&gt; shift_left t &gt; _ -&gt; t The function slope is the function slope of <ref> [14, p. 253] </ref>. &gt; slope :: BalBinSTree a b -&gt; Int &gt; slope Nil = 0 &gt; slope (Node _ _ l r) = bbstDepth l - bbstDepth r The function bbstDepth computes the depth of a binary search tree; it is the function depth of [14, p. 235]. &gt; bbstDepth <p> the function slope of [14, p. 253]. &gt; slope :: BalBinSTree a b -&gt; Int &gt; slope Nil = 0 &gt; slope (Node _ _ l r) = bbstDepth l - bbstDepth r The function bbstDepth computes the depth of a binary search tree; it is the function depth of <ref> [14, p. 235] </ref>. &gt; bbstDepth :: BalBinSTree a b -&gt; Int &gt; bbstDepth Nil = 0 &gt; bbstDepth (Node _ _ l r) = 1 + (bbstDepth l `max` bbstDepth r) The functions shift_right and shift_left are used to rebalance a tree. <p> These are the functions shiftr and shiftl of <ref> [14, p. 255] </ref>. &gt; shift_right (Node x d l r) &gt; | slope l == -1 = rotate_right ( &gt; Node x d (rotate_left l) r) &gt; &gt; | otherwise = rotate_right ( &gt; Node x d l r) 78 &gt; shift_left (Node x d l r) &gt; | slope r <p> These are the functions rotr and rotl of <ref> [14, p. 255] </ref>. &gt; rotate_right (Node x d (Node y e t1 t2) t3) = &gt; Node y e t1 (Node x d t2 t3) &gt; rotate_left (Node x d t1 (Node y e t2 t3)) = &gt; Node y e (Node x d t1 t2) t3 bbstLookUp The function bbstLookUp <p> This is the function member of <ref> [14, p. 246] </ref>. &gt; bbstMember :: (Ord a) =&gt; BalBinSTree a b -&gt; a -&gt; Bool &gt; bbstMember Nil _ = False 79 &gt; bbstMember (Node k d l r) x &gt; | x &lt; k = bbstMember l x &gt; | x == k = True &gt; | x &gt; <p> It is basically the function labels of <ref> [14, p. 247] </ref>. &gt; bbstFlatten :: BalBinSTree a b -&gt; [(a,b)] &gt; bbstFlatten Nil = [] &gt; bbstFlatten (Node k v l r) = bbstFlatten l ++ [(k,v)] ++ bbstFlatten r 80 Chapter 17 General List Processing This module provides a number of useful general purpose functions for processing lists that
Reference: [15] <author> A. J. Field and P. G. Harrison, </author> <title> Functional Programming. </title> <booktitle> International Computer Science Series, </booktitle> <publisher> Addison-Wesley, </publisher> <year> 1988. </year>
Reference-contexts: distributed by the Linguistic Data Consortium (LDC), is shown in Figure 3.10. 25 40 [1, 2] [39, 40] 2 (SH, 1) [1] 4 (SIL, 1) [3] 6 (AX, 1) [5] 8 (AA, 1) [7] 10 (CH, 1) [9] 12 (Z, 2) [11] 14 (F, 1) [12, 13] 16 (AH, 1) <ref> [15] </ref> 18 (SIL, 1) [17] 20 (T, 1) [19] 22 (AE, 1) [21] 24 (T, 1) [23] 26 (SIL, 1) [25] 28 (AO, 1) [27] 30 (AXR, 2) [27] 32 (AX, 1) [29, 30, 31] 34 (EH, 1) [33] 36 (IX, 1) [35] 38 (IX, 1) [37] 40 (SIL, 0) [39]
Reference: [16] <author> B. J. MacLennan, </author> <title> Functional Programming: Practice and Theory. </title> <publisher> Addison-Wesley, </publisher> <year> 1990. </year>
Reference: [17] <author> C. Reade, </author> <title> Elements of Functional Programming. </title> <booktitle> International Computer Science Series, </booktitle> <publisher> Addison-Wesley, </publisher> <year> 1989. </year>
Reference-contexts: Data Consortium (LDC), is shown in Figure 3.10. 25 40 [1, 2] [39, 40] 2 (SH, 1) [1] 4 (SIL, 1) [3] 6 (AX, 1) [5] 8 (AA, 1) [7] 10 (CH, 1) [9] 12 (Z, 2) [11] 14 (F, 1) [12, 13] 16 (AH, 1) [15] 18 (SIL, 1) <ref> [17] </ref> 20 (T, 1) [19] 22 (AE, 1) [21] 24 (T, 1) [23] 26 (SIL, 1) [25] 28 (AO, 1) [27] 30 (AXR, 2) [27] 32 (AX, 1) [29, 30, 31] 34 (EH, 1) [33] 36 (IX, 1) [35] 38 (IX, 1) [37] 40 (SIL, 0) [39] function showPrnNetwork. 26 Chapter
Reference: [18] <author> K.-F. Lee, </author> <title> "Context-dependent phonetic hidden Markov models for speaker-independent continuous speech recognition," </title> <journal> IEEE Trans. on Acoustics, Speech, and Signal Processing, </journal> <volume> vol. 38, </volume> <pages> pp. 599-609, </pages> <month> April </month> <year> 1990. </year>
Reference-contexts: sounds -- &gt; SIL -- silence &gt; deriving (Eq, Ord, Enum, Ix, Text) The data constructors are the same as a subset of the symbols in the "ARPAbet" phonetic alphabet [3, Table 2.1, p. 24], except for the "SIL" constructor which is represented in the ARPAbet as "h#." As in <ref> [18, Table II] </ref>, [3, Table 2.1], and [19, Table I], the relationships between the phonetic symbols and the sounds they represent are shown by example in Table 2.1. The variable phone_bounds is a pair comprised of the first and last phones.
Reference: [19] <author> C. H. Lee, L. R. Rabiner, R. Pieraccini, and J. G. Wilpon, </author> <title> "Acoustic modeling for large vocabulary speech recognition," </title> <booktitle> Computer Speech and Language, </booktitle> <volume> vol. 4, </volume> <pages> pp. 127-165, </pages> <month> April </month> <year> 1990. </year>
Reference-contexts: deriving (Eq, Ord, Enum, Ix, Text) The data constructors are the same as a subset of the symbols in the "ARPAbet" phonetic alphabet [3, Table 2.1, p. 24], except for the "SIL" constructor which is represented in the ARPAbet as "h#." As in [18, Table II], [3, Table 2.1], and <ref> [19, Table I] </ref>, the relationships between the phonetic symbols and the sounds they represent are shown by example in Table 2.1. The variable phone_bounds is a pair comprised of the first and last phones. <p> However, for better recognition accuracy, we may want to expand the symbol set to allow finer distinctions, e.g., context-dependent phones <ref> [19, Section 2.6.3] </ref>. <p> shown in Figure 3.10. 25 40 [1, 2] [39, 40] 2 (SH, 1) [1] 4 (SIL, 1) [3] 6 (AX, 1) [5] 8 (AA, 1) [7] 10 (CH, 1) [9] 12 (Z, 2) [11] 14 (F, 1) [12, 13] 16 (AH, 1) [15] 18 (SIL, 1) [17] 20 (T, 1) <ref> [19] </ref> 22 (AE, 1) [21] 24 (T, 1) [23] 26 (SIL, 1) [25] 28 (AO, 1) [27] 30 (AXR, 2) [27] 32 (AX, 1) [29, 30, 31] 34 (EH, 1) [33] 36 (IX, 1) [35] 38 (IX, 1) [37] 40 (SIL, 0) [39] function showPrnNetwork. 26 Chapter 4 Hidden Markov Models: <p> We are only concerned with the topology in this module because for the subword modeling of utterances we do not need to know the nature of the observations, e.g., whether they are discrete vector-quantizer indices (e.g., [1]) or continuous-valued vectors (e.g., <ref> [19] </ref>). <p> The states correspond to observations. That is, each state is associated with an observation probability model. Such a model might be a probability mass function over the indices of codewords in a vector quantizer (e.g., [1]) or a multivariate probability density function (e.g., <ref> [19] </ref>); the particular form of the observation model is not important for this module. The smaller, filled circle on the left side of the figure is the entry node and is a useful graphical device for identifying the starting states.
Reference: [20] <author> F. Harary, </author> <title> Graph Theory. </title> <publisher> Addison-Wesley, </publisher> <year> 1969. </year>
Reference-contexts: in Part III. &gt; import BalBinSTrees &gt; import Lists &gt; import MaybeStateT &gt; import PlainTextIO &gt; import StateT The module Phones was defined in Chapter 2. &gt; import Phones 11 3.1 Mathematical Representation The pronunciation model for a word is a pronunciation network, consisting of (1) an acyclic directed graph <ref> [20] </ref> called the pronunciation digraph, (2) a subset of nodes designated as the initial nodes, and (3) a subset of nodes designated as the terminal nodes. <p> A node's index is always larger than the indices of all the nodes that precede it in the digraph. A node x precedes a node y if there is a directed path starting at x and ending at y <ref> [20, p. 198] </ref>. Thus, the acyclic digraph is topologically sorted. Under these conventions, the highest indexed node is always a terminal node, but not necessarily the only one (in general, words may have more than one terminal node).
Reference: [21] <author> B. S. Everitt and D. J. </author> <title> Hand, Finite Mixture Distributions. </title> <publisher> Chapman and Hall, </publisher> <year> 1981F. </year>
Reference-contexts: 25 40 [1, 2] [39, 40] 2 (SH, 1) [1] 4 (SIL, 1) [3] 6 (AX, 1) [5] 8 (AA, 1) [7] 10 (CH, 1) [9] 12 (Z, 2) [11] 14 (F, 1) [12, 13] 16 (AH, 1) [15] 18 (SIL, 1) [17] 20 (T, 1) [19] 22 (AE, 1) <ref> [21] </ref> 24 (T, 1) [23] 26 (SIL, 1) [25] 28 (AO, 1) [27] 30 (AXR, 2) [27] 32 (AX, 1) [29, 30, 31] 34 (EH, 1) [33] 36 (IX, 1) [35] 38 (IX, 1) [37] 40 (SIL, 0) [39] function showPrnNetwork. 26 Chapter 4 Hidden Markov Models: The Transition Structure This <p> The c i 's are the mixture coefficients or the component probabilities, and the f i 's are the component densities. An excellent introduction to the theory of mixture distributions is provided in <ref> [21] </ref>. 5.3 Gaussian Mixture Densities A Gaussian mixture density is a mixture density for which each component density is Gaussian.
Reference: [22] <author> P. Wadler, </author> <title> "The essence of functional programming," </title> <booktitle> in 19th Annual Symposium on Principles of Programming Languages, </booktitle> <address> (Santa Fe, NM), </address> <month> January </month> <year> 1992. </year>
Reference: [23] <author> P. Wadler, </author> <title> "Monads for functional programming," in Program Design Calculi (M. </title> <editor> Broy, ed.), </editor> <publisher> Spring Verlag, </publisher> <year> 1994. </year>
Reference-contexts: [39, 40] 2 (SH, 1) [1] 4 (SIL, 1) [3] 6 (AX, 1) [5] 8 (AA, 1) [7] 10 (CH, 1) [9] 12 (Z, 2) [11] 14 (F, 1) [12, 13] 16 (AH, 1) [15] 18 (SIL, 1) [17] 20 (T, 1) [19] 22 (AE, 1) [21] 24 (T, 1) <ref> [23] </ref> 26 (SIL, 1) [25] 28 (AO, 1) [27] 30 (AXR, 2) [27] 32 (AX, 1) [29, 30, 31] 34 (EH, 1) [33] 36 (IX, 1) [35] 38 (IX, 1) [37] 40 (SIL, 0) [39] function showPrnNetwork. 26 Chapter 4 Hidden Markov Models: The Transition Structure This module defines data structures
Reference: [24] <author> D. J. King and P. Wadler, </author> <title> "Combining monads," in Functional Programming, </title> <editor> Glasgow 1992 (J. Launchbury and P. M. Sansom, eds.), </editor> <publisher> Springer-Verlag, </publisher> <year> 1993. </year>
Reference-contexts: This approach to combining the State Transformer monad with another monad was suggested in <ref> [24] </ref>. The "Maybe" type is implemented in the hbc library module Maybe.
Reference: [25] <author> M. P. Jones, </author> <title> "Release notes for gofer 2.28." Included as part of the standard Gofer distribution, </title> <month> February </month> <year> 1993. </year>
Reference-contexts: 1) [1] 4 (SIL, 1) [3] 6 (AX, 1) [5] 8 (AA, 1) [7] 10 (CH, 1) [9] 12 (Z, 2) [11] 14 (F, 1) [12, 13] 16 (AH, 1) [15] 18 (SIL, 1) [17] 20 (T, 1) [19] 22 (AE, 1) [21] 24 (T, 1) [23] 26 (SIL, 1) <ref> [25] </ref> 28 (AO, 1) [27] 30 (AXR, 2) [27] 32 (AX, 1) [29, 30, 31] 34 (EH, 1) [33] 36 (IX, 1) [35] 38 (IX, 1) [37] 40 (SIL, 0) [39] function showPrnNetwork. 26 Chapter 4 Hidden Markov Models: The Transition Structure This module defines data structures for representing the topology <p> Some of the definitions in this module are taken from the Glasgow distribution (ghc) library file src/ghc-0.19/ghc/lib/glaExts/PreludeST.lhs and the files demos/Cse/stateMonad.gs and demos/Ccexamples/ccexamples.gs that are part of the Gofer distribution <ref> [25, 26] </ref>, and are included here by permission of the authors.
Reference: [26] <author> M. P. Jones, </author> <title> "An introduction to gofer 2.20." </title> <note> Available by anonymous ftp from nebula.cs.yale.edu in the directory pub/haskell/gofer as part of the standard Gofer distribution, </note> <month> September </month> <year> 1991. </year>
Reference-contexts: Some of the definitions in this module are taken from the Glasgow distribution (ghc) library file src/ghc-0.19/ghc/lib/glaExts/PreludeST.lhs and the files demos/Cse/stateMonad.gs and demos/Ccexamples/ccexamples.gs that are part of the Gofer distribution <ref> [25, 26] </ref>, and are included here by permission of the authors.

References-found: 26

