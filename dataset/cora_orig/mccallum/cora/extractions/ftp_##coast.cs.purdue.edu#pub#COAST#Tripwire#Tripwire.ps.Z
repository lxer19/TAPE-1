URL: ftp://coast.cs.purdue.edu/pub/COAST/Tripwire/Tripwire.ps.Z
Refering-URL: http://www.cs.purdue.edu/coast/archive/data/categ2.html
Root-URL: http://www.cs.purdue.edu
Title: The Design and Implementation of Tripwire: A File System Integrity Checker  
Author: Gene H. Kim and Eugene H. Spafford 
Date: February 23, 1995  
Address: West Lafayette, IN 47907-1398  
Affiliation: COAST Laboratory Department of Computer Sciences Purdue University  
Abstract: At the heart of most computer systems is a file system. The file system contains user data, executable programs, configuration and authorization information, and (usually) the base executable version of the operating system itself. The ability to monitor file systems for unauthorized or unexpected changes gives system administrators valuable data for protecting and maintaining their systems. However, in environments of many networked heterogeneous platforms with different policies and software, the task of monitoring changes becomes quite daunting. Tripwire is tool that aids UNIX system administrators and users in monitoring a designated set of files and directories for any changes. Used with system files on a regular (e.g., daily) basis, Tripwire can notify system administrators of corrupted or altered files, so corrective actions may be taken in a timely manner. Tripwire may also be used on user or group files or databases to signal changes. This paper describes the design and implementation of the Tripwire tool. It uses interchangeable signature (usually, message digest) routines to identify changes in files, and is highly configurable. Tripwire is no-cost software, available on the Internet, and is currently in use on thousands of machines around the world.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Maurice J. Bach. </author> <title> The Design of the UNIX Operating System. </title> <publisher> Prentice-Hall, </publisher> <address> Englewood Cliffs, NJ, </address> <year> 1986. </year>
Reference-contexts: Typically, these changes would not concern system administrators. However, changes to certain files, such as system binaries, might elicit a different reaction. Similarly, changes to certain file attributes (stored in the file's inode structure <ref> [1] </ref>) occur frequently and are usually benign. A tool reporting every changed file potentially forces security administrators to interpret large amounts of data.
Reference: [2] <author> Vesselin Bontchev. </author> <title> Possible virus attacks against integrity programs and how to prevent them. </title> <type> Technical report, </type> <institution> Virus Test Center, University of Hamburg, </institution> <year> 1993. </year>
Reference-contexts: It was written in response to repeated break-in activity on the Internet, and the difficulty experienced by affected administrators in finding all of the backdoors left by the intruders. The foundations of integrity checking programs are surveyed in <ref> [2] </ref>. In simplest terms, a database is created with some unique identifier for each file to be monitored. By recreating that identifier (which could be a copy of the entire file contents) and comparing it against the saved version, it is possible to determine if a file has been altered.
Reference: [3] <author> J. Compbell. </author> <title> C Programmer's Guide to Serial Communications. </title> <editor> Howard W. </editor> <publisher> Sams & Co., </publisher> <year> 1987. </year>
Reference-contexts: The use of CRC signatures are poorly suited for integrity checking. Originally intended for hardware-based error-detection, CRC functions were designed to detect multiple bit errors in a data stream (e.g., <ref> [3] </ref>).
Reference: [4] <author> David A. Curry. </author> <title> UNIX System Security: A Guide for Users and System Administrators. </title> <publisher> Addison-Wesley, </publisher> <address> Reading, MA, </address> <year> 1992. </year>
Reference-contexts: As such, the security administrator needs to closely monitor the integrity of the file system contents. The near-ubiquitous UNIX system is an example of a file system where such monitoring is useful. Flaws and weaknesses in typical UNIX systems are well-documented (e.g., <ref> [8, 25, 19, 4, 9] </ref>). UNIX file systems are susceptible to threats in the guise of unauthorized users, intruders, viruses, worms, and logic bombs as well as failures and bugs. As such, UNIX system administrators are faced with prospects of subtle, difficult-to-detect damage to files, malicious and accidental. <p> It should signal if the file changes but be sufficiently large as to make a chance collision unlikely. Signature functions and methods are discussed in <ref> [24, 18, 9, 17, 4, 7, 16, 23] </ref>. Although various candidate signature functions have been studied over the past few years, we were unaware of any tool in general use that used these methods under UNIX. <p> The lack of integrity monitoring in COPS was addressed after its release by the addition of the crc check program. It is a checklisting program, similar to the shell scripts described in <ref> [9, 4] </ref>. It is based on a simple CRC checksum of the files being monitored. Numerous problems prevent it from being a comprehensive integrity checking solution as we have outlined in the previous sections. Most obvious is the lack of extensibility and flexibility in crc check.
Reference: [5] <editor> Edward DeHart, editor. </editor> <booktitle> Proceedings of the Security IV Conference, </booktitle> <address> Berkeley, CA, 1993. </address> <publisher> USENIX Association. </publisher>
Reference: [6] <institution> Data encryption standard. National Bureau of Standards FIPS, </institution> <year> 1977. </year>
Reference-contexts: Each file examined had its signatures generated using (in order) the 16-bit SunOS sum command, two standard CRC algorithms, the final 64 bits from a DES-CBC <ref> [6] </ref> encoded version of the file, and the 128-bit values taken from standard message digest functions.
Reference: [7] <author> Paul Fahn. </author> <title> Answers to frequently asked questions about today's cryptography. </title> <type> Technical Report Version 1.0 draft 1e, </type> <institution> RSA Laboratories, </institution> <year> 1992. </year>
Reference-contexts: It should signal if the file changes but be sufficiently large as to make a chance collision unlikely. Signature functions and methods are discussed in <ref> [24, 18, 9, 17, 4, 7, 16, 23] </ref>. Although various candidate signature functions have been studied over the past few years, we were unaware of any tool in general use that used these methods under UNIX. <p> For any desired signature, an intruder could reverse the signature function and generate an arbitrary file that also yields that signature (cf. [16]). For these reasons, message-digest algorithms (also known as one-way hash functions, fingerprinting routines, or manipulation detection codes) as described in <ref> [7, 17, 16, 23] </ref> become valuable as integrity checking tools. Message-digests are usually large, often at least 128 bits, and computationally infeasible to reverse. 2.4.3 Empirical results Table 1 shows signature collision frequencies for 254,686 files.
Reference: [8] <author> Daniel Farmer and Eugene H. Spafford. </author> <title> The COPS security checker system. </title> <booktitle> In Proceedings of the Summer Conference, </booktitle> <pages> pages 165-190, </pages> <address> Berkely, CA, 1990. </address> <publisher> Usenix Association. </publisher>
Reference-contexts: As such, the security administrator needs to closely monitor the integrity of the file system contents. The near-ubiquitous UNIX system is an example of a file system where such monitoring is useful. Flaws and weaknesses in typical UNIX systems are well-documented (e.g., <ref> [8, 25, 19, 4, 9] </ref>). UNIX file systems are susceptible to threats in the guise of unauthorized users, intruders, viruses, worms, and logic bombs as well as failures and bugs. As such, UNIX system administrators are faced with prospects of subtle, difficult-to-detect damage to files, malicious and accidental. <p> was also one of the principles at the heart of the COPS tool,<ref> [8] </ref> and one which we believe contributed greatly to its wide-spread acceptance and use. 3 Existing Tools Most available UNIX security tools fall into two categories: static audit tools and integrity checkers. Among the most prominent are COPS [8], TAMU [22], crc check [8], Hobgoblin [15], and ATP [28]. 5 A few commercial security tools also exist, but they are comparable to the user-community tools mentioned here. <p> principles at the heart of the COPS tool,<ref> [8] </ref> and one which we believe contributed greatly to its wide-spread acceptance and use. 3 Existing Tools Most available UNIX security tools fall into two categories: static audit tools and integrity checkers. Among the most prominent are COPS [8], TAMU [22], crc check [8], Hobgoblin [15], and ATP [28]. 5 A few commercial security tools also exist, but they are comparable to the user-community tools mentioned here.
Reference: [9] <author> Simson Garfinkel and Gene Spafford. </author> <title> Practical Unix Security. </title> <publisher> O'Reilly & Associates, Inc., </publisher> <address> Sebastopol, CA, </address> <year> 1991. </year>
Reference-contexts: As such, the security administrator needs to closely monitor the integrity of the file system contents. The near-ubiquitous UNIX system is an example of a file system where such monitoring is useful. Flaws and weaknesses in typical UNIX systems are well-documented (e.g., <ref> [8, 25, 19, 4, 9] </ref>). UNIX file systems are susceptible to threats in the guise of unauthorized users, intruders, viruses, worms, and logic bombs as well as failures and bugs. As such, UNIX system administrators are faced with prospects of subtle, difficult-to-detect damage to files, malicious and accidental. <p> Furthermore, by comparing entries in the database, it is possible to determine if files have been added or deleted from the system. As described in <ref> [9] </ref>, a checklist is one form of this database for a UNIX system. The file contents themselves are not usually saved as this would require too much disk space. <p> Instead, a checklist would contain a set of values generated from the original file usually including the length, time of last modification, and owner. The checklist is periodically regenerated and compared against the saved copies, with discrepancies noted. However, as noted in <ref> [9] </ref>, changes may be made to the contents of UNIX files without any of these values changing from the stored values; in particular, a user gaining access to the root account may modify the raw disk to alter the saved data without it showing in the checklist. <p> It should signal if the file changes but be sufficiently large as to make a chance collision unlikely. Signature functions and methods are discussed in <ref> [24, 18, 9, 17, 4, 7, 16, 23] </ref>. Although various candidate signature functions have been studied over the past few years, we were unaware of any tool in general use that used these methods under UNIX. <p> The lack of integrity monitoring in COPS was addressed after its release by the addition of the crc check program. It is a checklisting program, similar to the shell scripts described in <ref> [9, 4] </ref>. It is based on a simple CRC checksum of the files being monitored. Numerous problems prevent it from being a comprehensive integrity checking solution as we have outlined in the previous sections. Most obvious is the lack of extensibility and flexibility in crc check.
Reference: [10] <author> Chuck Gilmore. </author> <title> README file for PROVECRC.EXE. README file with program, </title> <year> 1991. </year>
Reference-contexts: Originally intended for hardware-based error-detection, CRC functions were designed to detect multiple bit errors in a data stream (e.g., [3]). Reversing the CRC function to yield a desired signature is a well-understood process, and tools to assist a potential intruder are widely available <ref> [10] </ref>. 3.2 TAMU TAMU is a set of security utilities being distributed by Texas A&M University.[22] Included in the package is a static audit tool, a signature database to check system binaries against known signatures of patch files, and a sophisticated network traffic analyzer that aids system administrators in assessing outside
Reference: [11] <author> Brian W. Kernighan and Dennis M. Ritchie. </author> <title> The M4 Macro Processor. </title> <institution> AT&T Bell Laboratories, </institution> <year> 1977. </year>
Reference-contexts: This serves both to check the consistency of the distribution, and to ensure that all features of the Tripwire program are working as expected. 4.1.2 Scalability Tripwire includes an M4-like preprocessing language <ref> [11] </ref> to help system administrators maximize reuse of configuration files. By including directives such as @@include, @@ifdef, @@ifhost, and @@define, system administrators can write a core configuration file describing portions of the file system shared by many machines.
Reference: [12] <author> Brian W. Kernighan and Dennis M. Ritchie. </author> <title> The C Programming Language. </title> <publisher> Prentice-Hall, </publisher> <address> Englewood Cliffs, NJ, </address> <year> 1978. </year>
Reference: [13] <author> Gene H. Kim and Eugene H. Spafford. </author> <title> Experiences with tripwire: Using integrity checkers for intrusion detection. </title> <booktitle> In Systems Administration, Networking and Security Conference III. Usenix, </booktitle> <month> April </month> <year> 1994. </year>
Reference: [14] <author> Gene H. Kim and Eugene H. Spafford. </author> <title> Writing, supporting, and evaluating tripwire: A publically available security tool. </title> <booktitle> In Proceedings of the Usenix Applications Development Symposium, </booktitle> <address> Berkeley, CA, 1994. </address> <publisher> Usenix. </publisher>
Reference-contexts: Several bugs have been identified, and four updates were released in 1993. In December 1993, the formal release of Tripwire was made. Some of the more involved software engineering aspects of Tripwire construction and maintenance are discussed in <ref> [14] </ref>. This section describes the structure of Tripwire. A high level model of Tripwire operation is shown in and a database of previously-generated signatures putatively matching the configuration.
Reference: [15] <author> Scott Leadly, Kenneth Rich, and Mark Sirota. Hobgoblin: </author> <title> A File and Directory Auditor. </title> <institution> University Computing Center, University of Rochester, </institution> <year> 1991. </year> <month> 17 </month>
Reference-contexts: Among the most prominent are COPS [8], TAMU [22], crc check [8], Hobgoblin <ref> [15] </ref>, and ATP [28]. 5 A few commercial security tools also exist, but they are comparable to the user-community tools mentioned here.
Reference: [16] <author> Ralph C. Merkle. </author> <title> A fast software one-way hash function. </title> <journal> Journal of Cryptology, </journal> <volume> 3(1) </volume> <pages> 43-58, </pages> <year> 1990. </year>
Reference-contexts: It should signal if the file changes but be sufficiently large as to make a chance collision unlikely. Signature functions and methods are discussed in <ref> [24, 18, 9, 17, 4, 7, 16, 23] </ref>. Although various candidate signature functions have been studied over the past few years, we were unaware of any tool in general use that used these methods under UNIX. <p> By understanding how a function generates a signature, one could reverse-engineer the function. For any desired signature, an intruder could reverse the signature function and generate an arbitrary file that also yields that signature (cf. <ref> [16] </ref>). For these reasons, message-digest algorithms (also known as one-way hash functions, fingerprinting routines, or manipulation detection codes) as described in [7, 17, 16, 23] become valuable as integrity checking tools. <p> For any desired signature, an intruder could reverse the signature function and generate an arbitrary file that also yields that signature (cf. [16]). For these reasons, message-digest algorithms (also known as one-way hash functions, fingerprinting routines, or manipulation detection codes) as described in <ref> [7, 17, 16, 23] </ref> become valuable as integrity checking tools. Message-digests are usually large, often at least 128 bits, and computationally infeasible to reverse. 2.4.3 Empirical results Table 1 shows signature collision frequencies for 254,686 files. <p> We also generated empirical support of the difficulty of spoofing 128-bit signatures. An attempt was made to find a duplicate Snefru <ref> [16] </ref> signature for the /bin/login program using 130 Sun workstations. 4 Over a time of several weeks, 17 million signatures were generated and compared with ten thousand stored signatures, the maximum number of signatures that fit in memory without forcing virtual memory page faults on each search iteration. <p> The following routines are included in the latest Tripwire distribution: MD5 [21] (the RSA Data Security, Inc. MD5 Message-Digest Algorithm), MD4 [20] (the RSA Data Security, Inc. MD4 Message-Digest Algorithm), MD2 (the RSA Data Security, Inc. MD2 Message-Digest Algorithm), 6 4-pass Snefru <ref> [16] </ref> (the Xerox Secure Hash Function), 7 128-bit HAVAL [29], and SHA (the NIST Secure Hash Algorithm). Tripwire also includes POSIX 1003.2 compliant CRC-32 and CCITT compliant CRC-16 signatures. Each signature may be included in the selection-mask by including its index.
Reference: [17] <author> W. T. Polk and L. E. Bassham. </author> <title> A guide to the selection of anti-virus tools and techniques. </title> <institution> National Institute of Standards and Technology report, </institution> <month> December </month> <year> 1992. </year>
Reference-contexts: It should signal if the file changes but be sufficiently large as to make a chance collision unlikely. Signature functions and methods are discussed in <ref> [24, 18, 9, 17, 4, 7, 16, 23] </ref>. Although various candidate signature functions have been studied over the past few years, we were unaware of any tool in general use that used these methods under UNIX. <p> For any desired signature, an intruder could reverse the signature function and generate an arbitrary file that also yields that signature (cf. [16]). For these reasons, message-digest algorithms (also known as one-way hash functions, fingerprinting routines, or manipulation detection codes) as described in <ref> [7, 17, 16, 23] </ref> become valuable as integrity checking tools. Message-digests are usually large, often at least 128 bits, and computationally infeasible to reverse. 2.4.3 Empirical results Table 1 shows signature collision frequencies for 254,686 files.
Reference: [18] <author> Yisrael Radai. </author> <title> Checksumming techniques for anti-viral proposed. </title> <editor> In Edward Wilding, editor, </editor> <booktitle> Virus Bulletin Conference Proceedings. </booktitle> <publisher> Virus Bulletin, Ltd., </publisher> <month> September </month> <year> 1991. </year>
Reference-contexts: It should signal if the file changes but be sufficiently large as to make a chance collision unlikely. Signature functions and methods are discussed in <ref> [24, 18, 9, 17, 4, 7, 16, 23] </ref>. Although various candidate signature functions have been studied over the past few years, we were unaware of any tool in general use that used these methods under UNIX.
Reference: [19] <author> Robert B. Reinhardt. </author> <title> An architectural overview of UNIX network security. </title> <type> Technical report, </type> <institution> ARINC Research Corportation, </institution> <month> February </month> <year> 1993. </year>
Reference-contexts: As such, the security administrator needs to closely monitor the integrity of the file system contents. The near-ubiquitous UNIX system is an example of a file system where such monitoring is useful. Flaws and weaknesses in typical UNIX systems are well-documented (e.g., <ref> [8, 25, 19, 4, 9] </ref>). UNIX file systems are susceptible to threats in the guise of unauthorized users, intruders, viruses, worms, and logic bombs as well as failures and bugs. As such, UNIX system administrators are faced with prospects of subtle, difficult-to-detect damage to files, malicious and accidental.
Reference: [20] <author> R. L. Rivest. </author> <title> The md4 message digest algorithm. </title> <booktitle> Advances in Cryptology Crypto '90, </booktitle> <pages> pages 303-311, </pages> <year> 1991. </year>
Reference-contexts: The following routines are included in the latest Tripwire distribution: MD5 [21] (the RSA Data Security, Inc. MD5 Message-Digest Algorithm), MD4 <ref> [20] </ref> (the RSA Data Security, Inc. MD4 Message-Digest Algorithm), MD2 (the RSA Data Security, Inc. MD2 Message-Digest Algorithm), 6 4-pass Snefru [16] (the Xerox Secure Hash Function), 7 128-bit HAVAL [29], and SHA (the NIST Secure Hash Algorithm). Tripwire also includes POSIX 1003.2 compliant CRC-32 and CCITT compliant CRC-16 signatures.
Reference: [21] <author> R. L. Rivest. </author> <title> RFC 1321: The md5 message-digest algorithm. </title> <type> Technical report, </type> <institution> Internet Activities Board, </institution> <month> April </month> <year> 1992. </year>
Reference-contexts: The following routines are included in the latest Tripwire distribution: MD5 <ref> [21] </ref> (the RSA Data Security, Inc. MD5 Message-Digest Algorithm), MD4 [20] (the RSA Data Security, Inc. MD4 Message-Digest Algorithm), MD2 (the RSA Data Security, Inc. MD2 Message-Digest Algorithm), 6 4-pass Snefru [16] (the Xerox Secure Hash Function), 7 128-bit HAVAL [29], and SHA (the NIST Secure Hash Algorithm).
Reference: [22] <author> David R. Safford, Douglas Lee Schales, and David K. Hess. </author> <title> The TAMU security package: An ongoing response to internet intruders in an academic environment. </title> <booktitle> In DeHart [5], </booktitle> <pages> pages 91-118. </pages>
Reference-contexts: Among the most prominent are COPS [8], TAMU <ref> [22] </ref>, crc check [8], Hobgoblin [15], and ATP [28]. 5 A few commercial security tools also exist, but they are comparable to the user-community tools mentioned here.
Reference: [23] <author> Bruce Schneier. </author> <title> Applied Cryptography. </title> <publisher> John Wiley & Sons, Inc, </publisher> <year> 1993. </year>
Reference-contexts: It should signal if the file changes but be sufficiently large as to make a chance collision unlikely. Signature functions and methods are discussed in <ref> [24, 18, 9, 17, 4, 7, 16, 23] </ref>. Although various candidate signature functions have been studied over the past few years, we were unaware of any tool in general use that used these methods under UNIX. <p> For any desired signature, an intruder could reverse the signature function and generate an arbitrary file that also yields that signature (cf. [16]). For these reasons, message-digest algorithms (also known as one-way hash functions, fingerprinting routines, or manipulation detection codes) as described in <ref> [7, 17, 16, 23] </ref> become valuable as integrity checking tools. Message-digests are usually large, often at least 128 bits, and computationally infeasible to reverse. 2.4.3 Empirical results Table 1 shows signature collision frequencies for 254,686 files.
Reference: [24] <author> Gustavus J. Simmons. </author> <booktitle> Contemporary Cryptology: The Science of Information Integrity. </booktitle> <publisher> IEEE Press, </publisher> <year> 1992. </year>
Reference-contexts: It should signal if the file changes but be sufficiently large as to make a chance collision unlikely. Signature functions and methods are discussed in <ref> [24, 18, 9, 17, 4, 7, 16, 23] </ref>. Although various candidate signature functions have been studied over the past few years, we were unaware of any tool in general use that used these methods under UNIX.
Reference: [25] <author> Cliff Stoll. </author> <title> The Cuckoo's Egg. </title> <publisher> Simon & Schuster, Inc., </publisher> <address> New York, </address> <year> 1990. </year>
Reference-contexts: As such, the security administrator needs to closely monitor the integrity of the file system contents. The near-ubiquitous UNIX system is an example of a file system where such monitoring is useful. Flaws and weaknesses in typical UNIX systems are well-documented (e.g., <ref> [8, 25, 19, 4, 9] </ref>). UNIX file systems are susceptible to threats in the guise of unauthorized users, intruders, viruses, worms, and logic bombs as well as failures and bugs. As such, UNIX system administrators are faced with prospects of subtle, difficult-to-detect damage to files, malicious and accidental.
Reference: [26] <author> Sun Microsystems, Inc. </author> <title> System and Network Administration, </title> <booktitle> 1990. Part number 800-3805-10. </booktitle>
Reference-contexts: One method of accomplishing this involves storing the databases on a write-protected disk or on a secure server where logins can be strictly controlled. The database could also be made available via a read-only remote file system (e.g., read-only NFS <ref> [26] </ref>). Installing an updated database is problematic because intruders might replace the database (or selected entries) with one of their own choosing during the update. Therefore, to best ensure the security of the database, the Tripwire documentation suggests that the machine be operated in single-user mode to install the database.
Reference: [27] <author> Steve Talbott. </author> <title> Managing Projects with make. </title> <publisher> O'Reilly & Associates, Inc., </publisher> <year> 1991. </year>
Reference-contexts: After unpacking the Tripwire distribution, the administrator edits the top level Makefile <ref> [27] </ref> to specify system-specific tools (e.g., compiler, compiler flags, etc.). Next, the user would choose a conf-machine.h header file that describes special options for the machine to be monitored.
Reference: [28] <author> David Vincenzetti and Massimo Cotrozzi. </author> <title> ATP anti tampering program. </title> <booktitle> In DeHart [5], </booktitle> <pages> pages 79-90. </pages>
Reference-contexts: Among the most prominent are COPS [8], TAMU [22], crc check [8], Hobgoblin [15], and ATP <ref> [28] </ref>. 5 A few commercial security tools also exist, but they are comparable to the user-community tools mentioned here.
Reference: [29] <author> Y. Zhang, J. Pieprzyk, and J. Seberry. </author> <title> HAVAL one-way hashing algorithm with variable length of output. </title> <booktitle> In Advances in Cryptology AUSCRYPT 92 Proceedings. </booktitle> <publisher> Springer-Verlag, </publisher> <year> 1992. </year> <note> As cited in Schneier. 18 </note>
Reference-contexts: MD5 Message-Digest Algorithm), MD4 [20] (the RSA Data Security, Inc. MD4 Message-Digest Algorithm), MD2 (the RSA Data Security, Inc. MD2 Message-Digest Algorithm), 6 4-pass Snefru [16] (the Xerox Secure Hash Function), 7 128-bit HAVAL <ref> [29] </ref>, and SHA (the NIST Secure Hash Algorithm). Tripwire also includes POSIX 1003.2 compliant CRC-32 and CCITT compliant CRC-16 signatures. Each signature may be included in the selection-mask by including its index.
References-found: 29

