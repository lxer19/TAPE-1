URL: http://rgai00.inf.u-szeged.hu/rgai/techrep/115.ps.gz
Refering-URL: http://forum.swarthmore.edu/~jay/learn-game/indexes/other-papers.html
Root-URL: 
Title: Multi-criteria reinforcement learning  
Author: Zoltan Gabor, Zsolt Kalmar, and Csaba Szepesvari 
Date: January 28, 1997  
Address: Aradi vrt tere 1,Szeged, 6720  
Affiliation: Research Group on Artificial Intelligence "Jozsef Attila" University, Szeged  
Abstract: We consider multi-criteria sequential decision making problems, where the criteria are ordered according to their importance. Structural properties of these problems are touched and reinforcement learning algorithms, which learn asymptotically optimal decisions, are derived. Computer experiments confirm the theoretical results and provide further insight in the learning processes.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> A.G. Barto, S.J. Bradtke, and S.P. Singh. </author> <title> Real-time learning and control using asynchronous dynamic programming. </title> <type> Technical report 91-57, </type> <institution> Computer Science Department, University of Massachusetts, </institution> <year> 1991. </year>
Reference-contexts: if Player I acts according to an optimal policy corresponding to this ordering then he/she will win the game in the fastest way if he can and he/she will try to mark time, otherwise. 4 Learning optimal policies The generalization of well-known RL algorithms such as Adaptive Real-Time Dynamic Programming <ref> [1] </ref>, Q-learning [23], TD () [15] and many others are easy to generalize to vector-valued MDPs. When T is a contraction the convergence proofs can be repeated using the general asynchronous contraction-mapping theorem of [12] (see also [20]).
Reference: [2] <author> T.A. Brown and R.E. Strauch. </author> <title> Dynamic programming on multiplicative lattices. </title> <journal> J. Math. Anal. and App., </journal> <volume> 12 </volume> <pages> 364-370, </pages> <year> 1965. </year>
Reference-contexts: Another approach is to search for policies which are Pareto maximal. The return of such a policy can not be majorized for each state and vector component. Pareto-optimality has been studied by many researchers (see e.g. <ref> [2, 5, 10, 6, 7] </ref> and the references therein). In some cases it is possible to give a complete ordering over the goals: for instance, in the above example one may assign higher priority to the goal "quench your thirst" than to the goal "avoid risky places".
Reference: [3] <author> K.J. Chung and M.J. Sobel. Discounted MDPs: </author> <title> Distribution functions and exponential utility maximization. </title> <journal> SIAM J. Control and Optimization, </journal> <volume> 25(1) </volume> <pages> 49-62, </pages> <year> 1987. </year>
Reference-contexts: First, one may introduce preferences among the goals and determine the utility of the different situations and/or actions according to the well established utility-theory initiated by von Neumann [22, 11]. Even sequential decision problems were considered and cases were determined for which a recursive (dynamic programming based) solution exist <ref> [3] </ref>. Another approach is to treat the goals separately, i.e. the rewards become vectors, each component representing the value of a decision from the point of view of a certain goal.
Reference: [4] <author> E.V. Denardo. </author> <title> Contraction mappings in the theory underlying dynamic programming. </title> <journal> SIAM Rev., </journal> <volume> 9 </volume> <pages> 165-177, </pages> <year> 1967. </year>
Reference-contexts: In the context of Markovian decision problems it is the theory of ordinal and/or 2 lexicographic dynamic programming that deals with multi-criteria sequential decision problems with an ordering over the goals. Ordinal dynamic programming was analyzed by Mitten [13] and Sobel [14]. Denardo <ref> [4] </ref> commented on lexicographic maximization. In this article we present an experiment with exactly this latter example.
Reference: [5] <author> E.A. Feinberg. </author> <title> Controlled Markov decision process with arbitrary numerical criteria. </title> <journal> Theory of Probability and Applications, </journal> <volume> 27 </volume> <pages> 486-503, </pages> <year> 1982. </year>
Reference-contexts: Another approach is to search for policies which are Pareto maximal. The return of such a policy can not be majorized for each state and vector component. Pareto-optimality has been studied by many researchers (see e.g. <ref> [2, 5, 10, 6, 7] </ref> and the references therein). In some cases it is possible to give a complete ordering over the goals: for instance, in the above example one may assign higher priority to the goal "quench your thirst" than to the goal "avoid risky places".
Reference: [6] <author> E.A. Feinberg and A. Schwartz. </author> <title> Markov decision models with weighted discounted rewards. </title> <journal> Mathematics of Operations Research, </journal> <volume> 19 </volume> <pages> 152-168, </pages> <year> 1994. </year>
Reference-contexts: How to determine the meaning of best if evaluations are vector valued? One approach is to use a weighing of the relative merits of the goals in which case the real-valued value of a state will the weighted sum of the components of the state's evaluation-vector <ref> [6] </ref>. However, this leaves open the question of how to determine the weighing of goals and also the structure of optimal policies can be quite complicated in this case [6]. Another approach is to search for policies which are Pareto maximal. <p> goals in which case the real-valued value of a state will the weighted sum of the components of the state's evaluation-vector <ref> [6] </ref>. However, this leaves open the question of how to determine the weighing of goals and also the structure of optimal policies can be quite complicated in this case [6]. Another approach is to search for policies which are Pareto maximal. The return of such a policy can not be majorized for each state and vector component. Pareto-optimality has been studied by many researchers (see e.g. [2, 5, 10, 6, 7] and the references therein). <p> Another approach is to search for policies which are Pareto maximal. The return of such a policy can not be majorized for each state and vector component. Pareto-optimality has been studied by many researchers (see e.g. <ref> [2, 5, 10, 6, 7] </ref> and the references therein). In some cases it is possible to give a complete ordering over the goals: for instance, in the above example one may assign higher priority to the goal "quench your thirst" than to the goal "avoid risky places".
Reference: [7] <author> E.A. Feinberg and A. Schwartz. </author> <title> Constrained Markov decision models with weighted discounted rewards. </title> <journal> Mathematics of Operations Research, </journal> <volume> 20(2) </volume> <pages> 302-320, </pages> <year> 1995. </year>
Reference-contexts: Another approach is to search for policies which are Pareto maximal. The return of such a policy can not be majorized for each state and vector component. Pareto-optimality has been studied by many researchers (see e.g. <ref> [2, 5, 10, 6, 7] </ref> and the references therein). In some cases it is possible to give a complete ordering over the goals: for instance, in the above example one may assign higher priority to the goal "quench your thirst" than to the goal "avoid risky places".
Reference: [8] <author> M. </author> <title> Heger. Consideration of risk in reinforcement learning. </title> <booktitle> Revised submission to the 11th International Machine Learning Conference ML-94, </booktitle> <year> 1994. </year>
Reference-contexts: The analogous of Q-learning for MDPs with the maximin criterion, proposed by Heger <ref> [8, 9] </ref>, is the Q-hat algorithm defined as Q t+1 (x t ; a t ) = min Q t (x t ; a t ); R t (x t ; a t ; y t ) + fl max Q t (y t ; b) : This algorithm will converge
Reference: [9] <author> M. </author> <title> Heger. The loss from imperfect value functions in expectation-based and minimax-based tasks. </title> <journal> Machine Learning, </journal> <volume> 22 </volume> <pages> 197-225, </pages> <year> 1996. </year>
Reference-contexts: The analogous of Q-learning for MDPs with the maximin criterion, proposed by Heger <ref> [8, 9] </ref>, is the Q-hat algorithm defined as Q t+1 (x t ; a t ) = min Q t (x t ; a t ); R t (x t ; a t ; y t ) + fl max Q t (y t ; b) : This algorithm will converge
Reference: [10] <author> M.I. Henig. </author> <title> Vector-valued dynamic programming. </title> <journal> SIAM J. Control and Optimization, </journal> <volume> 21(3) </volume> <pages> 490-499, </pages> <year> 1983. </year>
Reference-contexts: Another approach is to search for policies which are Pareto maximal. The return of such a policy can not be majorized for each state and vector component. Pareto-optimality has been studied by many researchers (see e.g. <ref> [2, 5, 10, 6, 7] </ref> and the references therein). In some cases it is possible to give a complete ordering over the goals: for instance, in the above example one may assign higher priority to the goal "quench your thirst" than to the goal "avoid risky places".
Reference: [11] <author> R. A. Howard. </author> <title> Information value theory. </title> <journal> IEEE Transactions on Systems Science and Cybernetics, </journal> <volume> SSC-2(1):22-26, </volume> <month> August </month> <year> 1966. </year>
Reference-contexts: First, one may introduce preferences among the goals and determine the utility of the different situations and/or actions according to the well established utility-theory initiated by von Neumann <ref> [22, 11] </ref>. Even sequential decision problems were considered and cases were determined for which a recursive (dynamic programming based) solution exist [3].
Reference: [12] <author> M.L. Littman and Cs. Szepesvari. </author> <title> A Generalized Reinforcement Learning Model: Convergence and applications. </title> <booktitle> In Int. Conf. on Machine Learning, </booktitle> <pages> pages 310-318, </pages> <year> 1996. </year>
Reference-contexts: When T is a contraction the convergence proofs can be repeated using the general asynchronous contraction-mapping theorem of <ref> [12] </ref> (see also [20]). Note that the proofs cannot be reduced to componentwise convergence since all the components of the evaluation-vectors take part in the action selection. Consider for example the case of Q-learning [23].
Reference: [13] <author> L.G. Mitten. </author> <title> Composition principles for synthesis of optimum multi-stage processes. </title> <journal> Operations Research, </journal> <volume> 12 </volume> <pages> 610-619, </pages> <year> 1964. </year>
Reference-contexts: In the context of Markovian decision problems it is the theory of ordinal and/or 2 lexicographic dynamic programming that deals with multi-criteria sequential decision problems with an ordering over the goals. Ordinal dynamic programming was analyzed by Mitten <ref> [13] </ref> and Sobel [14]. Denardo [4] commented on lexicographic maximization. In this article we present an experiment with exactly this latter example.
Reference: [14] <author> M.J. Sobel. </author> <title> Ordinal dynamic programming. </title> <journal> Management Science, </journal> <volume> 21 </volume> <pages> 967-975, </pages> <year> 1975. </year> <month> 12 </month>
Reference-contexts: In the context of Markovian decision problems it is the theory of ordinal and/or 2 lexicographic dynamic programming that deals with multi-criteria sequential decision problems with an ordering over the goals. Ordinal dynamic programming was analyzed by Mitten [13] and Sobel <ref> [14] </ref>. Denardo [4] commented on lexicographic maximization. In this article we present an experiment with exactly this latter example.
Reference: [15] <author> R.S. Sutton. </author> <title> Learning to predict by the method of temporal differences. </title> <journal> Machine Learning, </journal> <volume> 3(1) </volume> <pages> 9-44, </pages> <year> 1988. </year>
Reference-contexts: to an optimal policy corresponding to this ordering then he/she will win the game in the fastest way if he can and he/she will try to mark time, otherwise. 4 Learning optimal policies The generalization of well-known RL algorithms such as Adaptive Real-Time Dynamic Programming [1], Q-learning [23], TD () <ref> [15] </ref> and many others are easy to generalize to vector-valued MDPs. When T is a contraction the convergence proofs can be repeated using the general asynchronous contraction-mapping theorem of [12] (see also [20]).
Reference: [16] <author> Cs. Szepesvari. </author> <title> Abstract dynamic programming under monotonicity assumptions: Non-Markovian policies, policy iteration and the optimality equation. </title> <type> Technical Report 96-103, </type> <institution> Research Group on Artificial Intelligence, </institution> <address> JATE-MTA, Szeged 6720, Aradi vrt tere 1., HUN-GARY, </address> <month> August </month> <year> 1996. </year> <note> e-mail: szepes@math.u-szeged.hu. </note>
Reference-contexts: In the simulations we also address the exploration-exploitation dilemma. 2 Ordinal dynamic programming We will give a summary of results for ordinal vector-valued sequential decision problems in the framework of Abstract Dynamic Programming (ADPs) <ref> [16] </ref>. In this way several optimality criteria (such as the expected discounted cost or the total worst-case discounted cost criterion) can be dealt with simultaneously. <p> This relation holds for the discounted total reward criteria when Q is defined by Eq. 1. The evaluation of other than stationary policies can also be defined but we will not consider these here. The interested reader is referred to the report <ref> [16] </ref>. The optimal value function defined as v fl (x) = Sup v (x) (2) gives the total reward that can be achieved for a given state by choosing the policy which is best suited for that state. <p> Trivially, T n v converges to v fl and one can prove that Howard's policy iteration routine will find an optimal policy within a finite number of steps <ref> [16] </ref>. 3 Multi-criteria dynamic games In the computer experiments we will present results for the game tic-tac-toe so in this section we briefly review the theory of dynamic games. It is known that deterministic, strictly alternating games can be given in the form of maximin problems. <p> The proof is left to the reader as an exercise, for hints see <ref> [16] </ref>. 11
Reference: [17] <author> Cs. Szepesvari. </author> <title> Certainty equivalence policies are self-optimizing under minimax optimality. </title> <type> Technical Report 96-101, </type> <institution> Research Group on Artificial Intelligence, </institution> <address> JATE-MTA, Szeged 6720, Aradi vrt tere 1., HUNGARY, </address> <month> August </month> <year> 1996. </year> <note> e-mail: szepes@math.u-szeged.hu, http://www.inf.u-szeged.hu/ rgai. </note>
Reference-contexts: The first opponent is the optimal one, while the last one is the opponent that chooses the actions totally randomly. For comparison 2 If the estimates of R do not get exact within finite time then Theorem 2.1 of [19] (see also <ref> [17] </ref>) can be used to show the convergence of the asynchronous dynamic programming algorithm. 8 0 0.25 0.5 0.75 1 ARTDP 0.73 0.74 0.74 0.76 0.74 3.55 4.2 4.18 4.18 4.19 MC-ARTDP 0.85 1 0.96 1 1 3.59 3.28 3.29 3.28 3.28 Table 1: Results of the exhaustive testing.
Reference: [18] <author> Cs. Szepesvari. </author> <title> Some basic facts concerning minimax sequential decision problems. </title> <type> Technical Report 96-100, </type> <institution> Research Group on Artificial Intelligence, </institution> <address> JATE-MTA, Szeged 6720, Aradi vrt tere 1., HUNGARY, </address> <month> August </month> <year> 1996. </year> <note> e-mail: szepes@math.u-szeged.hu, http://www.inf.u-szeged.hu/ rgai. </note>
Reference-contexts: to surmount this problem one has to update the second components and the rest by means of other methods. (Or one may want to use 7 another method to update all the components.) As a first choice we considered the adaptive real--time dynamic programming algorithm for minimax problems, discussed in <ref> [18] </ref>. This algorithm builds an estimate of the transition sets T (x; a) = f y 2 X j P (x; a; y) &gt; 0 g and another estimate of the rewards R (x; a; y).
Reference: [19] <author> Cs. Szepesvari. </author> <title> Learning and exploitation do not conflict under minimax optimality. </title> <editor> In M.van Someren and G. Widmer, editors, </editor> <booktitle> Machine Learning: ECML'97 (9th European Conf. on Machine Learning, Proceedings), volume 1224 of Lecture Notes in Artificial Intelligence, </booktitle> <pages> pages 242-249. </pages> <publisher> Springer, </publisher> <address> Berlin, </address> <year> 1997. </year>
Reference-contexts: If every state-action pair is tried infinitely often then it is possible to prove that both Q-hat learning and adaptive real-time dynamic programming will find the optimal actions within a finite (but in general random) time <ref> [19] </ref>. This comes from the finiteness of the problem. So the equation A 1 (Q t ; x) = A 1 (Q fl ; x) is guaranteed to hold for large enough t. Also for large enough t the estimates of the transitions T will be correct. <p> An unusual property of maximin problems is that the condition that all actions should be tried in every state infinitely often can be substantially relaxed while retaining convergence to optimal behaviour. Namely, it is proven in <ref> [19] </ref> that if actions are chosen greedily then ARTDP will converge in a way that the chosen actions become optimal for large enough t. <p> Unfortunately, the inequality A i (Q t ; x) 6= A i (Q fl ; x) (i 1) may invalidate the `optimistic estimates condition' as it was discussed above meaning that the nice result of <ref> [19] </ref> can not be generalized to ordinal dynamic programming. 5 Computer simulations The purpose of the computer simulations was two-fold: to demonstrate that the theory works in practice, and to provide some intuition for the rate of convergence (at the moment there exist no theoretical result for the rate of convergence <p> The first opponent is the optimal one, while the last one is the opponent that chooses the actions totally randomly. For comparison 2 If the estimates of R do not get exact within finite time then Theorem 2.1 of <ref> [19] </ref> (see also [17]) can be used to show the convergence of the asynchronous dynamic programming algorithm. 8 0 0.25 0.5 0.75 1 ARTDP 0.73 0.74 0.74 0.76 0.74 3.55 4.2 4.18 4.18 4.19 MC-ARTDP 0.85 1 0.96 1 1 3.59 3.28 3.29 3.28 3.28 Table 1: Results of the exhaustive
Reference: [20] <author> Cs. </author> <title> Szepesvari and M.L. Littman. A unified analysis of value-function-based reinforcement-learning algorithms. </title> <booktitle> Neural Computation, </booktitle> <year> 1997. </year> <note> submitted. </note>
Reference-contexts: When T is a contraction the convergence proofs can be repeated using the general asynchronous contraction-mapping theorem of [12] (see also <ref> [20] </ref>). Note that the proofs cannot be reduced to componentwise convergence since all the components of the evaluation-vectors take part in the action selection. Consider for example the case of Q-learning [23].
Reference: [21] <author> S.B. Thrun. </author> <title> The role of exploration in learning control. </title> <publisher> Van Nostrand Rheinhold, </publisher> <address> Florence KY, </address> <year> 1992. </year>
Reference-contexts: Results are 3 The *-greedy exploration strategy chooses the best-looking (greedy) action with probability 1 * and chooses an action uniformly randomly from the rest with probability * <ref> [21] </ref>. 9 shown in Figure 1. opponents of different strengths The l.h.s. subfigure shows the percent of plays won or drew. The largest the convergence speed to 1 is the smallest is the cost of exploration.
Reference: [22] <author> J. von Neumann and O. Morgenstern. </author> <title> Theory of Games and Economic Behavior. </title> <publisher> Princeton University Press, </publisher> <address> Princeton, NJ, </address> <year> 1947. </year>
Reference-contexts: First, one may introduce preferences among the goals and determine the utility of the different situations and/or actions according to the well established utility-theory initiated by von Neumann <ref> [22, 11] </ref>. Even sequential decision problems were considered and cases were determined for which a recursive (dynamic programming based) solution exist [3].
Reference: [23] <author> C.J.C.H. Watkins. </author> <title> Learning from Delayed Rewards. </title> <type> PhD thesis, </type> <institution> King's College, </institution> <address> Cambridge, </address> <year> 1990. </year>
Reference-contexts: I acts according to an optimal policy corresponding to this ordering then he/she will win the game in the fastest way if he can and he/she will try to mark time, otherwise. 4 Learning optimal policies The generalization of well-known RL algorithms such as Adaptive Real-Time Dynamic Programming [1], Q-learning <ref> [23] </ref>, TD () [15] and many others are easy to generalize to vector-valued MDPs. When T is a contraction the convergence proofs can be repeated using the general asynchronous contraction-mapping theorem of [12] (see also [20]). <p> Note that the proofs cannot be reduced to componentwise convergence since all the components of the evaluation-vectors take part in the action selection. Consider for example the case of Q-learning <ref> [23] </ref>. Q-learning is a reinforcement learning algorithm for learning the optimal Q-function, defined by Q fl = Qv fl .
References-found: 23

