URL: ftp://ftp-pubs.lcs.mit.edu/pub/lcs-pubs/tr.outbox/MIT-LCS-TR-704.ps.gz
Refering-URL: ftp://ftp-pubs.lcs.mit.edu/pub/lcs-pubs/listings/tr700.html
Root-URL: 
Title: Learning Algorithms with Applications to Robot Navigation and Protein Folding  
Author: by Mona Singh Ronald L. Rivest Bonnie A. Berger 
Degree: Submitted to the Department of Electrical Engineering and Computer Science in partial fulfillment of the requirements for the degree of Doctor of Philosophy at the  Signature of Author  Certified by  Professor of Computer Science Thesis Supervisor Certified by  Assistant Professor of Mathematics Thesis Supervisor Accepted by Frederic R. Morgenthaler Chairman, Departmental Committee on Graduate Students  
Note: c Massachusetts Institute of Technology  
Date: (1989)  (1989)  September 1995  1995  September, 1995  
Affiliation: S.M., Computer Science Harvard University  A.B., Computer Science Harvard University  MASSACHUSETTS INSTITUTE OF TECHNOLOGY  Department of Electrical Engineering and Computer Science  
Abstract-found: 0
Intro-found: 1
Reference: [1] <author> Dana Angluin. </author> <title> On the complexity of minimum inference of regular sets. </title> <journal> Information and Computation, </journal> <volume> 39 </volume> <pages> 337-350, </pages> <year> 1987. </year>
Reference-contexts: The robot gets information about the automaton by actively experimenting in the environment and by observing input-output behavior. Rivest and Schapire show that a robot with a teacher can with high probability learn such an environment. They use homing sequences to improve Angluin's algorithm <ref> [1] </ref> to learn without using a "reset" mechanism. Ron and Rubinfeld [71] further extend this result by giving an efficient algorithm that with high probability learns finite automata with small cover time, without requiring a teacher. <p> Then for each sequence in the set of test sequences, we do the following. First, we score each sequence and then convert its sequence score to a likelihood. Next, we draw a number uniformly at random from the interval <ref> [0; 1] </ref>. If the number drawn is less than or equal to the likelihood of the sequence, then the sequence is added to the new database.
Reference: [2] <author> Dana Angluin. </author> <title> Computational learning theory: Survey and selected bibliography. </title> <booktitle> In Proceedings of the Twenty-Fourth Annual ACM Symposium on Theory of Computing, </booktitle> <pages> pages 351-369, </pages> <month> May </month> <year> 1992. </year>
Reference-contexts: For a symmetric function f and integer i, we let f (i) denote the value of f when exactly i of its inputs are 1. We study learning in the distribution-free or Probably Approximately Correct (PAC) learning model <ref> [74, 2] </ref>. In the PAC learning model, we assume that the learning algorithm has available an oracle EXAMPLES (c) that when queried, produces a labeled example h~v; c (~v)i according to a fixed but unknown probability distribution D.
Reference: [3] <author> T. K. Attwood and J. B. C. Findlay. </author> <title> Design of a discriminating fingerprint for G-protein-coupled receptors. </title> <journal> Protein Engineering, </journal> <volume> 6 </volume> <pages> 167-176, </pages> <year> 1993. </year>
Reference-contexts: Although a few papers have dealt with iterative algorithms <ref> [73, 3, 46, 36] </ref>, they do not use randomness and weighting for updating of parameters. In our experience, we find that these components of the algorithm are critical to achieving good performance. <p> Other types of iterative approaches have been applied to sequence alignment and protein structure prediction by researchers <ref> [73, 3, 46, 36] </ref>. Algorithmically, our approach differs from these approaches in two major ways. The first is our use of randomness to incorporate sequences into our database, and the second is our use of weighting to update the database (see section 4.3).
Reference: [4] <author> Baruch Awerbuch, Bonnie Berger, Lenore Cowen, and David Peleg. </author> <title> Near-linear cost constructions of neighborhood covers in sequential and distributed environments and their applications. </title> <booktitle> In Proceedings of the Thirty-Fourth Annual Foundations of Computer Science, </booktitle> <pages> pages 638-647, </pages> <month> November </month> <year> 1993. </year>
Reference: [5] <author> Baruch Awerbuch, Margrit Betke, Ronald L. Rivest, and Mona Singh. </author> <title> Piecemeal graph exploration by a mobile robot. </title> <booktitle> In Proceedings of the Eighth Conference on Computational Learning Theory, </booktitle> <address> Santa Cruz, CA, </address> <month> July </month> <year> 1995. </year>
Reference: [6] <author> Baruch Awerbuch and Robert G. </author> <title> Gallager. </title> <booktitle> Distributed BFS algorithms. The 26th Symposium on Foundations of Computer Science, </booktitle> <pages> pages 250-256, </pages> <month> October </month> <year> 1985. </year> <note> 113 114 Bibliography </note>
Reference-contexts: The teleport-free BFS algorithms we present never visit a vertex more than twice as far from s as the nearest unvisited vertex is from s. Our techniques for piecemeal learning of arbitrary undirected graphs are inspired by the work of Awerbuch and Gallager <ref> [6, 7] </ref>. We observe that our learning model bears some similarity to the asynchronous distributed model. This similarity is surprising and has not been explored in the past. <p> Awerbuch and Gallager <ref> [6, 7] </ref> give a distributed BFS algorithm which partitions the network 72 Piecemeal learning of unknown environments in strips, where each strip is a group of L consecutive layers. (Here L is a parameter to be chosen.) All vertices in strip i 1 are expanded before any vertices in strip i <p> The worst case is while exploring the second strip. 3.7 Piecemeal learning of undirected graphs 75 3.7.2 Iterative strip algorithm We now describe Iterative-Strip, an algorithm similar to the Strip-Explore algorithm. It is an efficiently interruptible algorithm for undirected graphs inspired by Awerbuch and Gallager's <ref> [6] </ref> distributed iterative BFS algorithm.
Reference: [7] <author> Baruch Awerbuch and Robert G. Gallager. </author> <title> A new distributed algorithm to find breadth first search trees. </title> <journal> IEEE Transactions on Information Theory, </journal> <volume> IT-33(3):315-322, </volume> <year> 1987. </year>
Reference-contexts: The teleport-free BFS algorithms we present never visit a vertex more than twice as far from s as the nearest unvisited vertex is from s. Our techniques for piecemeal learning of arbitrary undirected graphs are inspired by the work of Awerbuch and Gallager <ref> [6, 7] </ref>. We observe that our learning model bears some similarity to the asynchronous distributed model. This similarity is surprising and has not been explored in the past. <p> Awerbuch and Gallager <ref> [6, 7] </ref> give a distributed BFS algorithm which partitions the network 72 Piecemeal learning of unknown environments in strips, where each strip is a group of L consecutive layers. (Here L is a parameter to be chosen.) All vertices in strip i 1 are expanded before any vertices in strip i
Reference: [8] <author> Ricardo A. Baeza-Yates, Joseph C. Culberson, and Gregory J. E. Rawlins. </author> <title> Searching in the plane. </title> <journal> Information and Computation, </journal> <volume> 106(2) </volume> <pages> 234-252, </pages> <month> October </month> <year> 1993. </year>
Reference-contexts: In some cases, the robot knows exactly where there the goal location is, and in others it is assumed that the robot will recognize the goal location. Baeza-Yates, Culberson and Rawlins <ref> [8] </ref> study the cow path problem. The robot must search for an object in an unknown location on 2 or more rays (the endpoints of the rays are at some fixed start position). They give an optimal deterministic strategy for this problem.
Reference: [9] <author> P. Baldi. </author> <title> Hidden Markov models in molecular biology. </title> <type> Technical report, </type> <institution> JPL California Institute of Technology, </institution> <year> 1993. </year>
Reference-contexts: Various machine learning techniques have been applied to the protein structure prediction problem. The two main approaches are neural nets (e.g., [47, 67, 59]) and hidden Markov models (e.g., <ref> [53, 9] </ref>). Both of these approaches require adequate data on the target motif, since there is a "training session" on sequences that are known to contain the target motif.
Reference: [10] <author> E. Bar-Eli, P. Berman, A. Fiat, and P. Yan. </author> <title> On-line navigation in a room. </title> <booktitle> In Symposium on Discrete Algorithms, </booktitle> <pages> pages 237-249. </pages> <publisher> SIAM, </publisher> <year> 1992. </year>
Reference-contexts: The room contains axis parallel obstacles, but the obstacles do not touch the sides of the wall. Bar-Eli, Berman, Fiat, and Yan <ref> [10] </ref> show that any algorithm for this problem has competitive ratio (log n), and give an algorithm attaining this bound. Blum and Chalasani [21] consider the point to point problem in an unknown environment when the robot makes repeated trips between two points. <p> A 1 fi 1 face might correspond to a standard city-block; larger faces might correspond to obstacles (parks or shopping malls). Figure 3.3 gives an example. City-block graphs are also studied by Papadimitriou and Yanakakis [62], Blum, Raghavan, and Schieber [22], and Bar-Eli, Berman, Fiat and Yan <ref> [10] </ref>. An m fi n city-block graph with no obstacles has exactly mn vertices (at points (i; j) for 1 i m, 1 j n) and 2mn (m + n) edges (between points at distance 1 from each other).
Reference: [11] <author> Michael A. Bender and Donna K. </author> <title> Slonim. The power of team exploration: two robots can learn unlabeled directed graphs. </title> <booktitle> In Proceedings of the Thirty-Fifth Annual Symposium on Foundations of Computer Science, </booktitle> <pages> pages 75-85, </pages> <month> November </month> <year> 1994. </year>
Reference-contexts: They give a learning algorithm with a constant competitive ratio when the graph is Eulerian or when the deficiency of the graph is 1. For general graphs, they give a competitive ratio that is exponential in the deficiency of the graph. Bender and Slonim <ref> [11] </ref> look at the more complicated case of directed graphs with indistinguishable vertices. They show that a single robot with a constant number of pebbles cannot learn such environments without knowing the size of the graph.
Reference: [12] <author> B. Berger. </author> <title> Algorithms for protein structural motif recognition. </title> <journal> Journal of Computational Biology, </journal> <volume> 2 </volume> <pages> 125-138, </pages> <year> 1995. </year>
Reference-contexts: For example, in the case of the coiled coil motif, most known instances are 2-stranded coiled coils (i.e, coiled coils consisting of 2 ff-helices). As a result, known prediction algorithms work well for predicting 2-stranded coiled coils <ref> [14, 13, 12, 42, 58, 63] </ref>, but do not work as well for the related 3-stranded coiled coil motif (i.e., coiled coils consisting of 3 ff-helices), due to the lack of known 3-stranded coiled coil sequences. <p> The selected sequences are then used to update the parameters of the algorithm; these updates affect the performance of the algorithm in the next iteration. * In each iteration, the algorithm scores all the sequences based on its current estimates of the parameters and the theoretical framework developed in <ref> [12] </ref>. * In each iteration, the algorithm uses randomness to select which sequences are presumed to fold into the target motif. * The selected sequences are used in the beginning of the next iteration to update the parameters of the algorithm in a Bayesian-like weighting scheme. <p> Coiled coils show a characteristic heptad repeat with hydrophobic residues found in positions a and d, and this repeat makes coiled coils particularly amenable to recognition by computational techniques. Computational methods have been quite successful for predicting coiled coils <ref> [63, 58, 42, 12, 13, 14] </ref>. These techniques can be described, broadly, as follows: 1. Collect a database of known coiled coils and available amino acid subsequences. 2. Determine whether the unknown sequence shares enough distinguishing features with the known coiled coils to be considered a coiled coil. <p> If the product of the relative frequencies for each residue in some window is greater than some threshold, it concludes that the residue is part of a coiled coil. Recently researchers have put this problem within a probabilistic framework <ref> [12, 13, 14] </ref>, and have given linear-time algorithms for predicting coiled coils by approximating dependencies between positions in the coiled coil using pairwise frequencies. This method for prediction uses estimates of probabilities for singles and pair positions. <p> Given good estimates for the probabilities for the singles and pair positions for the motif, and reasonable assumptions about dependencies in the motif, the PairCoil scoring method which we use as a subroutine is mathematically justified <ref> [12] </ref>. 4.3.2 Computing likelihoods Once we have a sequence score, we assess it by converting it into a likelihood that the sequence contains the target motif.
Reference: [13] <author> B. Berger and D. Wilson. </author> <title> Improved algorithms for protein motif recognition. </title> <booktitle> In Symposium on Discrete Algorithms, </booktitle> <pages> pages 58-67. </pages> <publisher> SIAM, </publisher> <month> January </month> <year> 1995. </year>
Reference-contexts: For example, in the case of the coiled coil motif, most known instances are 2-stranded coiled coils (i.e, coiled coils consisting of 2 ff-helices). As a result, known prediction algorithms work well for predicting 2-stranded coiled coils <ref> [14, 13, 12, 42, 58, 63] </ref>, but do not work as well for the related 3-stranded coiled coil motif (i.e., coiled coils consisting of 3 ff-helices), due to the lack of known 3-stranded coiled coil sequences. <p> Coiled coils show a characteristic heptad repeat with hydrophobic residues found in positions a and d, and this repeat makes coiled coils particularly amenable to recognition by computational techniques. Computational methods have been quite successful for predicting coiled coils <ref> [63, 58, 42, 12, 13, 14] </ref>. These techniques can be described, broadly, as follows: 1. Collect a database of known coiled coils and available amino acid subsequences. 2. Determine whether the unknown sequence shares enough distinguishing features with the known coiled coils to be considered a coiled coil. <p> If the product of the relative frequencies for each residue in some window is greater than some threshold, it concludes that the residue is part of a coiled coil. Recently researchers have put this problem within a probabilistic framework <ref> [12, 13, 14] </ref>, and have given linear-time algorithms for predicting coiled coils by approximating dependencies between positions in the coiled coil using pairwise frequencies. This method for prediction uses estimates of probabilities for singles and pair positions.
Reference: [14] <author> B. Berger, D. B. Wilson, E. Wolf, T. Tonchev, M. Milla, and P. S. Kim. </author> <title> Predicting coiled coils using pairwise residue correlations. </title> <booktitle> Proceedings of the National Academy of Sciences, </booktitle> <volume> 92 </volume> <pages> 8259-8263, </pages> <year> 1995. </year>
Reference-contexts: For example, in the case of the coiled coil motif, most known instances are 2-stranded coiled coils (i.e, coiled coils consisting of 2 ff-helices). As a result, known prediction algorithms work well for predicting 2-stranded coiled coils <ref> [14, 13, 12, 42, 58, 63] </ref>, but do not work as well for the related 3-stranded coiled coil motif (i.e., coiled coils consisting of 3 ff-helices), due to the lack of known 3-stranded coiled coil sequences. <p> Recently it has been proposed that the 3-stranded coiled coil motif acts as the cell fusion mechanism for many viruses, and algorithms for predicting these structures could aid in the study of how viruses invade cells. Computational methods <ref> [14, 58] </ref> have already identified such coiled coil regions in influenza virus hemagluttinin and Moloney murine leukemia virus envelope protein; both of these predictions have been corroborated in the laboratory [30, 40]. <p> Coiled coils show a characteristic heptad repeat with hydrophobic residues found in positions a and d, and this repeat makes coiled coils particularly amenable to recognition by computational techniques. Computational methods have been quite successful for predicting coiled coils <ref> [63, 58, 42, 12, 13, 14] </ref>. These techniques can be described, broadly, as follows: 1. Collect a database of known coiled coils and available amino acid subsequences. 2. Determine whether the unknown sequence shares enough distinguishing features with the known coiled coils to be considered a coiled coil. <p> If the product of the relative frequencies for each residue in some window is greater than some threshold, it concludes that the residue is part of a coiled coil. Recently researchers have put this problem within a probabilistic framework <ref> [12, 13, 14] </ref>, and have given linear-time algorithms for predicting coiled coils by approximating dependencies between positions in the coiled coil using pairwise frequencies. This method for prediction uses estimates of probabilities for singles and pair positions. <p> We now describe each of the components of the algorithm in more detail, using coiled coils as an example, although the algorithm can be applied to other protein motifs. 4.3 The algorithm 97 4.3.1 Scoring In our implementation, we use the PairCoil program described by Berger et al. <ref> [14] </ref> as our scoring procedure, although any good prediction algorithm with a low false positive rate can be used for scoring. This scoring method uses correlation methods that incorporate pairwise dependencies between amino acids at multiple distances. <p> In each iteration of the algorithm, we compute a function that takes a residue score and computes the likelihood that the residue is part of the target motif. We compute this likelihood function in a manner described in <ref> [14] </ref>. In particular, every sequence in a large sequence database is scored. (Ideally, this large sequence database is the PIR. <p> describe the databases we use to test the program, and then we follow by describing the program's performance. 4.4.1 The databases and test sequences Our original database of 2-stranded coiled coils consists of 58; 217 amino acid residues which were gathered from sequences of myosin, tropomyosin, and intermediate filament proteins <ref> [14] </ref>. We also have separate databases containing sequences from each of these protein subclasses individually. A synthetic peptide of tropomyosin is the only solved structure among these. We test LearnCoil on the 3-stranded coiled coils by starting the algorithm with the base database of all 2-stranded coiled coils. <p> The set of iteration test sequences for testing performance on 3-stranded coiled coils consists of the following 5516 sequences: 286 known non-coiled coils from the non-redundant version of the PDB created in <ref> [14] </ref> (the PDB is the database of solved protein structures); 2% of the sequences in OWL (a large non-redundant composite database, where no two sequences in the database are exactly the same and no two sequences show only "trivial" differences [20]), with any obvious members of the PDB removed (2815 total); <p> the three dimensional structure for most of the protein sequences in our iteration test sets (except for the sequences from the PDB and portions of the sequences making up the 2-and 3-stranded coiled coil data sets). 4.4.2 Learning 3-stranded coiled coils Our techniques improve non-learning based approaches, such as PairCoil <ref> [14] </ref>, which often fails to identify 3-stranded coiled coil regions. 104 Learning-based algorithms for protein motif recognition Base Set Evaluation Performance Performance Set without LearnCoil with LearnCoil % of seqs # of false % of seqs # of false positive seqs positive seqs 2-str CCs 46 3-str CCs 67% 0=286 93% <p> For example, we are able to learn coiled coils in intermediate filaments from a database of coiled coils in either myosins or tropomyosins. Our techniques improve non-learning based approaches, such as the PairCoil program <ref> [14] </ref>, which fail to identify conjectured coiled coil residue positions.
Reference: [15] <author> J. Berger. </author> <title> Statistical Decision Theory and Bayesian Analysis. </title> <publisher> Springer-Verlag, </publisher> <address> New York, </address> <year> 1985. </year>
Reference-contexts: We now describe a theoretical framework for updating probabilities in this manner in each iteration of our algorithm. The approach we give is motivated by a Bayesian viewpoint <ref> [45, 15] </ref>. In particular, we think of the probabilities we are trying to estimate as the parameters of a Multinomial distribution, and we use the Dirichlet density to model the prior information we have about these probabilities. <p> Thus a Bayesian estimate for the probabilities p 1 ; p 2 ; : : :; p 20 can be found by looking at the posterior distribution. The Dirichlet distribution is conjugate for the Multinomial, and the posterior distribution is the Dirichlet distribution D (~ff + ~y) <ref> [15, 45] </ref>. That is, the new parameter of the distribution is the vector sum of the original parameters and the observed data.
Reference: [16] <author> Margrit Betke. </author> <title> Algorithms for exploring an unknown graph. </title> <type> Master's thesis, </type> <institution> MIT Department of Electrical Engineering and Computer Science, </institution> <month> February </month> <year> 1992. </year> <note> (Published as MIT Laboratory for Computer Science Technical Report MIT/LCS/TR-536, </note> <month> March, </month> <year> 1992). </year>
Reference-contexts: They give an algorithm for learning finite automata, assuming that the robot has access to a distinguishing sequence. Freund et al. [43] give algorithms for learning "typical" deterministic finite automata from random walks. Deng and Papadimitriou [35] and Betke <ref> [16] </ref> model the robot's environment as a directed graph, with distinct and recognizable vertices and edges. They give a learning algorithm with a constant competitive ratio when the graph is Eulerian or when the deficiency of the graph is 1.
Reference: [17] <author> Margrit Betke, Ronald L. Rivest, and Mona Singh. </author> <title> Piecemeal learning of an unknown environment. </title> <booktitle> In Proceedings of the Sixth Conference on Computational Learning Theory, </booktitle> <pages> pages 277-286, </pages> <address> Santa Cruz, CA, </address> <month> July </month> <year> 1993. </year> <note> (Published as MIT AI-Memo 1474 and CBCL-Memo 93.). Bibliography 115 </note>
Reference: [18] <author> Margrit Betke, Ronald L. Rivest, and Mona Singh. </author> <title> Piecemeal learning of an unknown environment. </title> <journal> Machine Learning, </journal> 18(2/3):231-254, March 1995. 
Reference: [19] <author> S. Blacklow, M. Lu, and P. S. Kim. </author> <title> A trimeric subdomain of the Simian Immunodeficiency Virus envelope glycoprotein. Biochemistry, </title> <address> 34:14955, </address> <year> 1995. </year>
Reference-contexts: Introduction 17 human rotavirus, human T-cell lymphotropic virus, Human Immunodeficiency Virus (HIV) and Simian Immunodeficiency Virus (SIV). Independently, recent laboratory work has predicted the existence of a coiled-coil-like structure in HIV and SIV <ref> [19, 56] </ref>, and our algorithm is able to predict the regions of this structure to within a few residues. We hope that biologists will direct their laboratory efforts towards testing other new candidate sequences which we identify. Organization of thesis The thesis is organized in three self-contained chapters. <p> Independent experimental investigations have also predicted these coiled-coil-like regions in HIV and SIV <ref> [19, 56] </ref>. 92 Learning-based algorithms for protein motif recognition 4.2 Further background The coiled coil motif is found in fibrous proteins, DNA binding proteins, and in tRNA-synthetase proteins. <p> For example, recent biological work has identified a coiled-coil-like structure which is believed to consist of a parallel, trimeric coiled coil encircled by three helices packed in an antiparallel formation; this structure is thought to be in the envelope glycoproteins of both HIV and SIV (Simian Immunodeficiency Virus) <ref> [19, 56] </ref>. Our program seems to be able to accurately predict this new coiled-coil-like structure. For example, it identifies two coiled-coil-like regions in the envelope protein of SIV. <p> For example, it identifies two coiled-coil-like regions in the envelope protein of SIV. Independently, the biological investigation of SIV by Blacklow et al. predicts that these are the two regions that are part of the coiled-coil-like structure <ref> [19] </ref>. One of these regions (comprising the outer three helices) is predicted by the NewCoil program and is given a 26% likelihood by the PairCoil program. The other region (comprising the trimeric coiled coil) is only predicted by 4.4 Results 109 our LearnCoil program.
Reference: [20] <author> A. J. Bleasby and J.C. </author> <title> Wooton. </title> <journal> Protein Engineering, </journal> <volume> 4, </volume> <year> 1990. </year>
Reference-contexts: coils from the non-redundant version of the PDB created in [14] (the PDB is the database of solved protein structures); 2% of the sequences in OWL (a large non-redundant composite database, where no two sequences in the database are exactly the same and no two sequences show only "trivial" differences <ref> [20] </ref>), with any obvious members of the PDB removed (2815 total); sequences in OWL whose names contain the strings actinin, alpha spectrin, dystrophin, tail fiber, laminin, fibrinogen, env, spike, glycoprotein, bacteriophage T4 wac, bacteriophage K3 fibritin, heat shock transcription, or macrophage scavenger receptor, as well as the 3-stranded coiled coil mutant
Reference: [21] <author> Avrim Blum and Prasad Chalasani. </author> <title> An on-line algorithm for improving performance in navigation. </title> <booktitle> In Proceedings of the Thirty-Fourth Annual Symposium on Foundations of Computer Science, </booktitle> <pages> pages 2-11, </pages> <month> November </month> <year> 1993. </year>
Reference-contexts: The room contains axis parallel obstacles, but the obstacles do not touch the sides of the wall. Bar-Eli, Berman, Fiat, and Yan [10] show that any algorithm for this problem has competitive ratio (log n), and give an algorithm attaining this bound. Blum and Chalasani <ref> [21] </ref> consider the point to point problem in an unknown environment when the robot makes repeated trips between two points. The goal of the robot is to find better paths in each trip.
Reference: [22] <author> Avrim Blum, Prabhakar Raghavan, and Baruch Schieber. </author> <title> Navigating in unfamiliar geometric terrain. </title> <booktitle> In Proceedings of Twenty-Third ACM Symposium on Theory of Computing, </booktitle> <pages> pages 494-504. </pages> <publisher> ACM, </publisher> <year> 1991. </year>
Reference-contexts: For the case of square obstacles, they give a 1 3 26 1:7 competitive algorithm, and show that any strategy must have competitive ratio greater than 3 2 . Blum, Raghavan, and Schieber <ref> [22] </ref> also study the problem of point to point navigation in 3.3 Formal model 39 an unknown two-dimensional geometric environment with convex obstacles. For the case of axis parallel rectangular obstacles, they give an algorithm with competitive ratio O ( p n), matching the lower bound of Papadimitriou and Yanakakis. <p> A 1 fi 1 face might correspond to a standard city-block; larger faces might correspond to obstacles (parks or shopping malls). Figure 3.3 gives an example. City-block graphs are also studied by Papadimitriou and Yanakakis [62], Blum, Raghavan, and Schieber <ref> [22] </ref>, and Bar-Eli, Berman, Fiat and Yan [10]. An m fi n city-block graph with no obstacles has exactly mn vertices (at points (i; j) for 1 i m, 1 j n) and 2mn (m + n) edges (between points at distance 1 from each other).
Reference: [23] <author> Avrim Blum and Mona Singh. </author> <title> Learning functions of k terms. </title> <booktitle> In Proceedings of the Third Annual Workshop on Computational Learning Theory, </booktitle> <pages> pages 144-153. </pages> <publisher> Morgan Kaufmann, </publisher> <year> 1990. </year>
Reference: [24] <author> Manuel Blum and Dexter Kozen. </author> <title> On the power of the compass (or, why mazes are easier to search than graphs. </title> <booktitle> In Proceedings of the 19th Annual Symposium on Foundations of Computer Science. IEEE, </booktitle> <year> 1978. </year>
Reference-contexts: Kleinberg [52] improves this by giving an algorithm with competitive ratio q p 8 2:61. For rectilinear streets, the algorithm achieves a competitive ratio of p There are many other related papers in the literature, particularly in the area of robotics (e.g., [57]) and maze searching (e.g., <ref> [25, 24] </ref>). Rao, Kareti, Shi, and Iyengar [68] give a survey of work on robot navigation in unknown terrains. 3.3 Formal model We model the robot's environment as a finite connected undirected graph G = (V; E) with distinguished start vertex s. Vertices represent accessible locations.
Reference: [25] <author> Manuel Blum and W. Sakoda. </author> <title> On the capability of finite automata in 2 and 3 dimensional space. </title> <booktitle> In Proceedings of the 18th Annual Symposium on Foundations of Computer Science. IEEE, </booktitle> <year> 1977. </year>
Reference-contexts: Kleinberg [52] improves this by giving an algorithm with competitive ratio q p 8 2:61. For rectilinear streets, the algorithm achieves a competitive ratio of p There are many other related papers in the literature, particularly in the area of robotics (e.g., [57]) and maze searching (e.g., <ref> [25, 24] </ref>). Rao, Kareti, Shi, and Iyengar [68] give a survey of work on robot navigation in unknown terrains. 3.3 Formal model We model the robot's environment as a finite connected undirected graph G = (V; E) with distinguished start vertex s. Vertices represent accessible locations.
Reference: [26] <author> Anselm Blumer, Andrzej Ehrenfeucht, David Haussler, and Manfred K. Warmuth. </author> <title> Occam's razor. </title> <journal> Information Processing Letters, </journal> <volume> 24 </volume> <pages> 377-380, </pages> <month> April </month> <year> 1987. </year>
Reference-contexts: The error of a hypothesis h is the probability that h (~v) 6= c (~v) when ~v is chosen from the distribution D. For the purposes of our positive results, it will be enough to consider the following sufficient condition for learnability <ref> [26] </ref>. An algorithm A is an "Occam algorithm" for C if on any sample (collection of labeled examples) of size m consistent with some c 2 C, algorithm A produces a consistent hypothesis of size at most jcj fi m ff for constants ff &lt; 1; fi 1.
Reference: [27] <author> J. U Bowie, R. Luthy, and D. Eisenberg. </author> <title> A method to identify protein sequences that fold into a known three-dimensional structure. </title> <journal> Science, </journal> <volume> 253 </volume> <pages> 164-170, </pages> <year> 1991. </year>
Reference: [28] <author> C. Branden and J. Tooze. </author> <title> Introduction to Protein Structure. </title> <publisher> Garland Publishing, Inc., </publisher> <address> New York and London, </address> <year> 1991. </year> <note> 116 Bibliography </note>
Reference: [29] <author> M. Brown, R. Hughey, A. Krogh, I. S. Mian, K. Sjolander, and D. Haussler. </author> <title> Using Dirichlet mixture priors to derive hidden Markov models for protein families. </title> <booktitle> In International Conference on Intelligent Systems and Molecular Biology, </booktitle> <pages> pages 47-55, </pages> <year> 1993. </year>
Reference-contexts: In our test domain of coiled coils, we found that this method of updating probabilities missed more sequences that contain coiled coils than did our method for updating probabilities. Using Dirichlet mixture densities as priors to estimate amino acid probabilities has been studied by Brown et al. <ref> [29] </ref>. Their approach uses as a prior the maximum likelihood estimate of a mixture Dirichlet density, based on data previously obtained from multiple alignments of various sets of sequences.
Reference: [30] <author> P. A. Bullough, F. M. Hughson, J. J. Skehel, and D. C. Wiley. </author> <title> Structure of influenza hemagglutinin at the pH of membrane fusion. </title> <journal> Nature, </journal> <volume> 371 </volume> <pages> 37-43, </pages> <year> 1994. </year>
Reference-contexts: Computational methods [14, 58] have already identified such coiled coil regions in influenza virus hemagluttinin and Moloney murine leukemia virus envelope protein; both of these predictions have been corroborated in the laboratory <ref> [30, 40] </ref>. Coiled coils are a particular type of ff-helix, consisting of two or more ff-helices wrapped around each other with a slight left-handed superhelical twist. Coiled coils have a cyclic repeat of seven positions, a, b, c, d, e, f , and g (see Figure 1).
Reference: [31] <author> C. Carr and P. S. Kim. </author> <title> A spring-loaded mechanism for the conformational change of influenza hemagglutinin. </title> <journal> Cell, </journal> <volume> 73 </volume> <pages> 823-832, </pages> <month> May </month> <year> 1993. </year>
Reference: [32] <author> Thomas H. Cormen, Charles E. Leiserson, and Ronald L. Rivest. </author> <title> Introduction to Algorithms. </title> <publisher> MIT Press/McGraw-Hill, </publisher> <year> 1990. </year>
Reference-contexts: approaches to piecemeal learning A simple approach to piecemeal learning of arbitrary undirected graphs is to use an ordinary search algorithm|breadth-first search (BFS) or depth-first search (DFS)|and just interrupt the search as needed to return to visit s. (Detailed descriptions of BFS and DFS can be found in algorithms textbooks <ref> [32] </ref>.) Once the robot has returned to s, it goes back to the vertex at which search was interrupted and resumes exploration. We now illustrate the problems each of these approaches has for efficient piecemeal learning.
Reference: [33] <author> Thomas Dean, Dana Angluin, Kenneth Basye, Sean Engelson, Leslie Kaelbling, Evangelos Kokkevis, and Oded Maron. </author> <title> Inferring finite automata with stochastic output functions and an application to map learning. </title> <booktitle> In Proceedings of the Tenth National Conference on Artificial Intelligence, </booktitle> <pages> pages 208-214, </pages> <year> 1992. </year>
Reference-contexts: They use homing sequences to improve Angluin's algorithm [1] to learn without using a "reset" mechanism. Ron and Rubinfeld [71] further extend this result by giving an efficient algorithm that with high probability learns finite automata with small cover time, without requiring a teacher. Dean et al. <ref> [33] </ref> study the problem of learning finite automaton when the output at each state has some probability of being incorrect. They give an algorithm for learning finite automata, assuming that the robot has access to a distinguishing sequence.
Reference: [34] <author> Xiaotie Deng, Tiko Kameda, and Christos H. Papadimitriou. </author> <title> How to learn an unknown environment. </title> <booktitle> In Proceedings of the 32nd Symposium on Foundations of Computer Science, </booktitle> <pages> pages 298-303. </pages> <publisher> IEEE, </publisher> <year> 1991. </year>
Reference-contexts: Dudek et al. [38] study the easier problem of learning undirected graphs with indistinguishable vertices, and give an algorithm for a robot with one or markers to learn such an environment. Deng, Kameda, and Papadimitriou <ref> [34] </ref> model environments such as "rooms" as polygons 38 Piecemeal learning of unknown environments with polygonal obstacles. They assume the robot has vision, and must learn a map of the room.
Reference: [35] <author> Xiaotie Deng and Christos H. Papadimitriou. </author> <title> Exploring an unknown graph. </title> <booktitle> In Proceedings of the 31st Symposium on Foundations of Computer Science, </booktitle> <volume> volume I, </volume> <pages> pages 355-361, </pages> <year> 1990. </year>
Reference-contexts: They give an algorithm for learning finite automata, assuming that the robot has access to a distinguishing sequence. Freund et al. [43] give algorithms for learning "typical" deterministic finite automata from random walks. Deng and Papadimitriou <ref> [35] </ref> and Betke [16] model the robot's environment as a directed graph, with distinct and recognizable vertices and edges. They give a learning algorithm with a constant competitive ratio when the graph is Eulerian or when the deficiency of the graph is 1.
Reference: [36] <author> I. Dodd and J. B. Egan. </author> <title> Improved detection of helix-turn-helix DNA-binding motifs in protein sequences. </title> <journal> Nucleic Acid Research, </journal> <volume> 18 </volume> <pages> 5019-5026, </pages> <year> 1990. </year>
Reference-contexts: Although a few papers have dealt with iterative algorithms <ref> [73, 3, 46, 36] </ref>, they do not use randomness and weighting for updating of parameters. In our experience, we find that these components of the algorithm are critical to achieving good performance. <p> Other types of iterative approaches have been applied to sequence alignment and protein structure prediction by researchers <ref> [73, 3, 46, 36] </ref>. Algorithmically, our approach differs from these approaches in two major ways. The first is our use of randomness to incorporate sequences into our database, and the second is our use of weighting to update the database (see section 4.3).
Reference: [37] <author> R. Duda and P. Hart. </author> <title> Pattern Classification and Scene Analysis. </title> <publisher> John Wiley and Sons, Inc., </publisher> <address> New York, </address> <year> 1973. </year>
Reference: [38] <author> Gregory Dudek, Michael Jenkin, Evangelos Milios, and David Wilkes. </author> <title> Using multiple markers in graph exploration. </title> <booktitle> In SPIE Vol. 1195 Mobile Robots IV, </booktitle> <pages> pages 77-87, </pages> <year> 1989. </year>
Reference-contexts: They show that a single robot with a constant number of pebbles cannot learn such environments without knowing the size of the graph. On the other hand, they give a probabilistic algorithm for two cooperating robots to learn such an environment. Dudek et al. <ref> [38] </ref> study the easier problem of learning undirected graphs with indistinguishable vertices, and give an algorithm for a robot with one or markers to learn such an environment. Deng, Kameda, and Papadimitriou [34] model environments such as "rooms" as polygons 38 Piecemeal learning of unknown environments with polygonal obstacles.
Reference: [39] <author> Jack Edmonds and Ellis L. Johnson. </author> <title> Matching, Euler tours and the Chinese Postman. </title> <journal> Mathematical Programming, </journal> <volume> 5 </volume> <pages> 88-124, </pages> <year> 1973. </year> <note> Bibliography 117 </note>
Reference-contexts: Note that since the graph is given, the problem does not actually have a learning or exploration component. However, for simplicity we continue using "learning" and "exploration." The off-line piecemeal learning problem is similar to the well-known Chinese Postman Problem <ref> [39] </ref>, but where the postman must return to the post-office every so often. (We could call the off-line problem the Weak Postman Problem, for postmen who cannot carry much mail.) The same problem arises when many postmen must cover the same city with their routes. <p> The Chinese Postman Problem can be solved by a polynomial time algorithm if the graph is either undirected or directed <ref> [39] </ref>. The Chinese Postman problem for a mixed graph that has undirected and directed edges was shown to be NP-complete by Papadimitriou [61]. We do not know an optimal off-line algorithm for the Weak Postman Problem; this may be an NP-hard problem.
Reference: [40] <author> D. Fass and P. S. Kim. </author> <title> The envelope protein of moloney murine leukemia virus contains a three-stranded coiled coil: Structural similarity between retrovirus and influenza membrane-fusion proteins. </title> <note> Submitted for publication, </note> <year> 1995. </year>
Reference-contexts: Computational methods [14, 58] have already identified such coiled coil regions in influenza virus hemagluttinin and Moloney murine leukemia virus envelope protein; both of these predictions have been corroborated in the laboratory <ref> [30, 40] </ref>. Coiled coils are a particular type of ff-helix, consisting of two or more ff-helices wrapped around each other with a slight left-handed superhelical twist. Coiled coils have a cyclic repeat of seven positions, a, b, c, d, e, f , and g (see Figure 1).
Reference: [41] <author> Paul Fischer and Hans Ulrich Simon. </author> <title> On learning ring-sum-expansions. </title> <journal> SIAM J. Computing, </journal> <volume> 21(1) </volume> <pages> 181-192, </pages> <month> February </month> <year> 1992. </year>
Reference-contexts: Suppose, however, that we wish to learn in the same manner another class of concepts C k;f (that is, other than k-term DNF) for which learning C k;f by C k;f is NP-hard. Our results and related results by Fischer and Simon <ref> [41] </ref> show that exclusive-or (XOR) is one such function.
Reference: [42] <author> V. Fischetti, G. Landau, J. Schmidt, and P. Sellers. </author> <title> Identifying periodic occurrences of a template with applications to protein structure. </title> <journal> Information Processing Letters, </journal> <volume> 45(1) </volume> <pages> 11-18, </pages> <year> 1993. </year>
Reference-contexts: For example, in the case of the coiled coil motif, most known instances are 2-stranded coiled coils (i.e, coiled coils consisting of 2 ff-helices). As a result, known prediction algorithms work well for predicting 2-stranded coiled coils <ref> [14, 13, 12, 42, 58, 63] </ref>, but do not work as well for the related 3-stranded coiled coil motif (i.e., coiled coils consisting of 3 ff-helices), due to the lack of known 3-stranded coiled coil sequences. <p> Coiled coils show a characteristic heptad repeat with hydrophobic residues found in positions a and d, and this repeat makes coiled coils particularly amenable to recognition by computational techniques. Computational methods have been quite successful for predicting coiled coils <ref> [63, 58, 42, 12, 13, 14] </ref>. These techniques can be described, broadly, as follows: 1. Collect a database of known coiled coils and available amino acid subsequences. 2. Determine whether the unknown sequence shares enough distinguishing features with the known coiled coils to be considered a coiled coil.
Reference: [43] <author> Yoav Freund, Michael Kearns, Dana Ron, Ronitt Rubinfeld, Robert Schapire, and Linda Sellie. </author> <title> Efficient learning of typical finite automata from random walks. </title> <booktitle> In Proceedings of the Twenty-Fifth Annual ACM Symposium on Theory of Computing, </booktitle> <pages> pages 315-324, </pages> <month> May </month> <year> 1993. </year>
Reference-contexts: Dean et al. [33] study the problem of learning finite automaton when the output at each state has some probability of being incorrect. They give an algorithm for learning finite automata, assuming that the robot has access to a distinguishing sequence. Freund et al. <ref> [43] </ref> give algorithms for learning "typical" deterministic finite automata from random walks. Deng and Papadimitriou [35] and Betke [16] model the robot's environment as a directed graph, with distinct and recognizable vertices and edges.
Reference: [44] <author> Sally Goldman. </author> <title> Learning Binary Relations, Total Orders, and Read-Once Formulas. </title> <type> PhD thesis, </type> <institution> MIT Dept. of Electrical Engineering and Computer Science, </institution> <month> September </month> <year> 1990. </year> <institution> (MIT Laboratory for Computer Science Technical Report MIT/LCS/TR-483, </institution> <month> July </month> <year> 1990). </year>
Reference: [45] <author> Irving John Good. </author> <title> The Estimation of Probabilities. </title> <publisher> The MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1965. </year>
Reference-contexts: We now describe a theoretical framework for updating probabilities in this manner in each iteration of our algorithm. The approach we give is motivated by a Bayesian viewpoint <ref> [45, 15] </ref>. In particular, we think of the probabilities we are trying to estimate as the parameters of a Multinomial distribution, and we use the Dirichlet density to model the prior information we have about these probabilities. <p> In fact, the approach we give is not completely Bayesian, as we will use the seen data to pick the parameters of the prior distribution; this is sometimes called a Bayes/Non-Bayes compromise <ref> [45] </ref>. We will use frequency counts from our databases to estimate singles and pair probabilities. For simplicity, we focus on the case of updating singles probabilities; updating pair probabilities is analogous. Initially, we have a database of sequences which fold into a particular base motif. <p> Thus a Bayesian estimate for the probabilities p 1 ; p 2 ; : : :; p 20 can be found by looking at the posterior distribution. The Dirichlet distribution is conjugate for the Multinomial, and the posterior distribution is the Dirichlet distribution D (~ff + ~y) <ref> [15, 45] </ref>. That is, the new parameter of the distribution is the vector sum of the original parameters and the observed data.
Reference: [46] <author> M. Gribskov. </author> <title> Translational initiation factors IF-1 and eIF-2ff share an RNA-binding motif with prokaryotic ribosomal protein S1 and polynucleotide phosphorylase. </title> <journal> Gene, </journal> <volume> 119 </volume> <pages> 107-111, </pages> <year> 1992. </year>
Reference-contexts: Although a few papers have dealt with iterative algorithms <ref> [73, 3, 46, 36] </ref>, they do not use randomness and weighting for updating of parameters. In our experience, we find that these components of the algorithm are critical to achieving good performance. <p> Other types of iterative approaches have been applied to sequence alignment and protein structure prediction by researchers <ref> [73, 3, 46, 36] </ref>. Algorithmically, our approach differs from these approaches in two major ways. The first is our use of randomness to incorporate sequences into our database, and the second is our use of weighting to update the database (see section 4.3).
Reference: [47] <author> L. Holley and M. Karplus. </author> <title> Protein secondary structure prediction with a neural network. </title> <booktitle> Proceedings of the National Academy of Sciences, </booktitle> <volume> 86 </volume> <pages> 152-156, </pages> <year> 1989. </year>
Reference-contexts: Various machine learning techniques have been applied to the protein structure prediction problem. The two main approaches are neural nets (e.g., <ref> [47, 67, 59] </ref>) and hidden Markov models (e.g., [53, 9]). Both of these approaches require adequate data on the target motif, since there is a "training session" on sequences that are known to contain the target motif.
Reference: [48] <author> Ming-Yang Kao, Yuan Ma, Michael Sipser, and Yiqun Yin. </author> <title> Optimal constructions of hybrid algorithms. </title> <booktitle> In Symposium on Discrete Algorithms. </booktitle> <publisher> SIAM, </publisher> <year> 1994. </year>
Reference-contexts: Kao, Reif and Tate [49] give a randomized algorithm for this problem that has better expected performance than any deterministic algorithm. Kao, Ma, Sipser and Yin <ref> [48] </ref> give an optimal deterministic search strategy for the case of multiple robots. Papadimitriou and Yanakakis [62] consider the problem of a robot with vision moving around in a plane filled with obstacles.
Reference: [49] <author> Ming-Yang Kao, John Reif, and Stephen Tate. </author> <title> Searching in an unknown environment: An optimal randomized algorithm for the cow-path problem. </title> <booktitle> In Symposium on Discrete Algorithms. </booktitle> <publisher> SIAM, </publisher> <year> 1993. </year> <note> 118 Bibliography </note>
Reference-contexts: For the case of 2 rays, they use a doubling strategy and get a competitive ratio of 9; they extend this technique for m rays and get a competitive ratio of 1 + 2 (m m =(m 1) m1 ). Kao, Reif and Tate <ref> [49] </ref> give a randomized algorithm for this problem that has better expected performance than any deterministic algorithm. Kao, Ma, Sipser and Yin [48] give an optimal deterministic search strategy for the case of multiple robots.
Reference: [50] <author> Michael Kearns, Ming Li, Leonard Pitt, and Leslie Valiant. </author> <title> On the learnability of boolean formulae. </title> <booktitle> In Proceedings of the Nineteenth Annual ACM Symposium on Theory of Computing, </booktitle> <pages> pages 285-295, </pages> <address> New York, New York, </address> <month> May </month> <year> 1987. </year>
Reference: [51] <author> Rolf Klein. </author> <title> Walking an unknown street with bounded detour. Computational geometry: </title> <journal> theory and applications, </journal> <volume> 1(6) </volume> <pages> 325-351, </pages> <month> June </month> <year> 1992. </year> <note> Also published in The 32nd Symposium on Foundations of Computer Science, </note> <year> 1991. </year>
Reference-contexts: The goal of the robot is to find better paths in each trip. In environments with axis parallel obstacles, they give an algorithm with the property that at the i-th trip, the robot's path is O ( p n=i) times the shortest path length. Klein <ref> [51] </ref> considers the problem of a polygon with distinguished start and goal vertices. The robot's goal is to walk inside the polygon from the start location to the goal location. The goal location is recognized as soon as the robot sees it.
Reference: [52] <author> Jon Kleinberg. </author> <title> On-line algorithms for robot navigation and server problems. </title> <type> Master's thesis, </type> <institution> MIT Department of Electrical Engineering and Computer Science, </institution> <month> May </month> <year> 1994. </year> <note> (Published as MIT Laboratory for Computer Science Technical Report MIT/LCS/TR-641). </note>
Reference-contexts: For the simplified case of a rectilinear room with no obstacles, they show a 2 p 2 competitive algorithm for learning the room. Kleinberg <ref> [52] </ref> improves this to a 5 4 2 competitive algorithm. For a rectilinear room with at most k obstacles, Deng et al. give an algorithm with O (k) competitive ratio. <p> The goal location is recognized as soon as the robot sees it. For a special type of polygon known as a street, Klein gives an algorithm with a 1 + 3 2 5:71 competitive ratio. Kleinberg <ref> [52] </ref> improves this by giving an algorithm with competitive ratio q p 8 2:61. For rectilinear streets, the algorithm achieves a competitive ratio of p There are many other related papers in the literature, particularly in the area of robotics (e.g., [57]) and maze searching (e.g., [25, 24]).
Reference: [53] <author> A. Krogh, M. Brown, S. Mian, K. Sjolander, and D. Haussler. </author> <title> Hidden Markov models in computational biology: Applications to protein modeling. </title> <type> Technical Report UCSC-CRL-93-32, </type> <institution> University of California at Santa Cruz, </institution> <year> 1993. </year>
Reference-contexts: Various machine learning techniques have been applied to the protein structure prediction problem. The two main approaches are neural nets (e.g., [47, 67, 59]) and hidden Markov models (e.g., <ref> [53, 9] </ref>). Both of these approaches require adequate data on the target motif, since there is a "training session" on sequences that are known to contain the target motif.
Reference: [54] <author> Nick Littlestone. </author> <title> Learning quickly when irrelevant attributes abound: A new linear-threshold algorithm. </title> <journal> Machine Learning, </journal> <volume> 2 </volume> <pages> 285-318, </pages> <year> 1988. </year>
Reference-contexts: The learner is judged by the total number of mistakes it makes in such a sequence. Using the halving algorithm <ref> [54] </ref>, k-decision lists can be learned in the mistake-bound model with O (n k ) mistakes. Thus have the following theorem: Theorem 2 All functions on k terms can be learned in the mistake-bound model with O (n k ) mistakes, using a representation of k-decision lists. <p> In fact, we can learn k-term functions in an "attribute-efficient" sense, where the number of mistakes is polynomial in the number of relevant variables (variables that appear in some term T i ) and is only logarithmic in the number of irrelevant variables. This uses a result of Littlestone <ref> [54] </ref> as follows. An alternation in a decision list is a pair of adjacent rules such that the boolean classification values for the rules differ. <p> If only r variables are relevant to the k-term function, then the number of rules m is at most r k . Therefore, the maximum weight in the threshold function is r k 2 . Littlestone <ref> [54] </ref> gives an algorithm that can be used to learn such a function, where the number of mistakes is at most O ((mr k 2 ) 2 log (n k )) = O (kr 2k+2k 2 log n).
Reference: [55] <author> D. Liu, J. Bienkowska, C. Petosa, R. J. Collier, H. Fu, and R. Liddington. </author> <title> Crystal structure of the zeta isoform of the 14-3-3 protein. </title> <journal> Nature, </journal> <volume> 376, </volume> <month> July 13, </month> <year> 1995. </year>
Reference-contexts: Our predictions were made independently of these results. Recently, the crystal structure of two 14-3-3 proteins have been solved <ref> [55, 75] </ref>. The paper of Liu et al. studies the zeta transform of the 14-3-3 structure in E. coli, and they report a 2-stranded antiparallel coiled coil structure. On the other hand, the paper of Xiao et al. studies the human T-cell t dimer, and they report helical bundles.
Reference: [56] <author> M. Lu, S. Blacklow, and P. S. Kim. </author> <title> A trimeric structural domain of the HIV-1 transmem-brane glycoprotein. </title> <journal> Nature Structural Biology, </journal> <volume> 2(12), </volume> <month> December </month> <year> 1995. </year>
Reference-contexts: Introduction 17 human rotavirus, human T-cell lymphotropic virus, Human Immunodeficiency Virus (HIV) and Simian Immunodeficiency Virus (SIV). Independently, recent laboratory work has predicted the existence of a coiled-coil-like structure in HIV and SIV <ref> [19, 56] </ref>, and our algorithm is able to predict the regions of this structure to within a few residues. We hope that biologists will direct their laboratory efforts towards testing other new candidate sequences which we identify. Organization of thesis The thesis is organized in three self-contained chapters. <p> Independent experimental investigations have also predicted these coiled-coil-like regions in HIV and SIV <ref> [19, 56] </ref>. 92 Learning-based algorithms for protein motif recognition 4.2 Further background The coiled coil motif is found in fibrous proteins, DNA binding proteins, and in tRNA-synthetase proteins. <p> For example, recent biological work has identified a coiled-coil-like structure which is believed to consist of a parallel, trimeric coiled coil encircled by three helices packed in an antiparallel formation; this structure is thought to be in the envelope glycoproteins of both HIV and SIV (Simian Immunodeficiency Virus) <ref> [19, 56] </ref>. Our program seems to be able to accurately predict this new coiled-coil-like structure. For example, it identifies two coiled-coil-like regions in the envelope protein of SIV.
Reference: [57] <author> Vladimir Lumelsky and Alexander Stepanov. </author> <title> Path-planning strategies for a point mobile automaton moving amidst unknown obstacles of arbitrary shape. </title> <journal> Algorithmica, </journal> <volume> 2 </volume> <pages> 403-430, </pages> <year> 1987. </year>
Reference-contexts: Kleinberg [52] improves this by giving an algorithm with competitive ratio q p 8 2:61. For rectilinear streets, the algorithm achieves a competitive ratio of p There are many other related papers in the literature, particularly in the area of robotics (e.g., <ref> [57] </ref>) and maze searching (e.g., [25, 24]). Rao, Kareti, Shi, and Iyengar [68] give a survey of work on robot navigation in unknown terrains. 3.3 Formal model We model the robot's environment as a finite connected undirected graph G = (V; E) with distinguished start vertex s.
Reference: [58] <author> A. Lupas, M. van Dyke, and J. </author> <title> Stock. Predicting coiled coils from protein sequences. </title> <journal> Science, </journal> <volume> 252 </volume> <pages> 1162-1164, </pages> <year> 1991. </year>
Reference-contexts: For example, in the case of the coiled coil motif, most known instances are 2-stranded coiled coils (i.e, coiled coils consisting of 2 ff-helices). As a result, known prediction algorithms work well for predicting 2-stranded coiled coils <ref> [14, 13, 12, 42, 58, 63] </ref>, but do not work as well for the related 3-stranded coiled coil motif (i.e., coiled coils consisting of 3 ff-helices), due to the lack of known 3-stranded coiled coil sequences. <p> Recently it has been proposed that the 3-stranded coiled coil motif acts as the cell fusion mechanism for many viruses, and algorithms for predicting these structures could aid in the study of how viruses invade cells. Computational methods <ref> [14, 58] </ref> have already identified such coiled coil regions in influenza virus hemagluttinin and Moloney murine leukemia virus envelope protein; both of these predictions have been corroborated in the laboratory [30, 40]. <p> Coiled coils show a characteristic heptad repeat with hydrophobic residues found in positions a and d, and this repeat makes coiled coils particularly amenable to recognition by computational techniques. Computational methods have been quite successful for predicting coiled coils <ref> [63, 58, 42, 12, 13, 14] </ref>. These techniques can be described, broadly, as follows: 1. Collect a database of known coiled coils and available amino acid subsequences. 2. Determine whether the unknown sequence shares enough distinguishing features with the known coiled coils to be considered a coiled coil. <p> These techniques can be described, broadly, as follows: 1. Collect a database of known coiled coils and available amino acid subsequences. 2. Determine whether the unknown sequence shares enough distinguishing features with the known coiled coils to be considered a coiled coil. Standard approaches <ref> [63, 58] </ref> look at the frequencies of each amino acid residue in each of the seven repeated positions. Overall this singles method does pretty well. <p> Standard approaches [63, 58] look at the frequencies of each amino acid residue in each of the seven repeated positions. Overall this singles method does pretty well. When the NewCoil program of Lupas et al. <ref> [58] </ref> is tested on the PDB (the database of all solved protein structures), it finds all sequences which contain coiled coils. On the other hand, 2/3 of the sequences it predicts to contain coiled coils do not. That is, the false positive rate for the standard method is quite high. <p> Intuitively, this table entry represents the "propensity" that Leucine is in position a in a coiled coil. The singles method approach <ref> [58] </ref> actually looks at 28long windows, since stable coiled coils are believed to be at least 28 residues long. Thus for each residue, it looks at each possible position (a through g), and at all 28-long windows that contain it. <p> In contrast, PairCoil obtained a performance of 83:3%, with four false positive predictions. For all three above experiments, LearnCoil improved performance of PairCoil in identifying coiled coil residues, while also improving its false positive rate. We also tested LearnCoil with the NewCoils program <ref> [58] </ref> used as the underlying scoring algorithm. For subclasses of 2-stranded coiled coils, we found that LearnCoil enhanced the performance of NewCoils as well.
Reference: [59] <author> R. Maclin and J. Shavlik. </author> <title> Refining algorithms with knowledge-based neural networks: improving the Chou-Fasman algorithm for protein folding. </title> <editor> In Stephen Hanson, George Drastal, and Ronald Rivest, editors, </editor> <booktitle> Computational Learning Theory and Natural Learning Systems, </booktitle> <pages> pages 249-286. </pages> <publisher> The MIT Press, </publisher> <year> 1994. </year> <note> Bibliography 119 </note>
Reference-contexts: Various machine learning techniques have been applied to the protein structure prediction problem. The two main approaches are neural nets (e.g., <ref> [47, 67, 59] </ref>) and hidden Markov models (e.g., [53, 9]). Both of these approaches require adequate data on the target motif, since there is a "training session" on sequences that are known to contain the target motif.
Reference: [60] <author> S. Muggleton, R. D. King, and M. Sternberg. </author> <title> Protein secondary structure prediction using logic-based machine learning. </title> <journal> Protein Engineering, </journal> <volume> 5(7) </volume> <pages> 647-657, </pages> <year> 1992. </year>
Reference-contexts: Other learning approaches which have been applied to protein structure prediction include rule-based methods (e.g., <ref> [60] </ref>). 4.3 The algorithm We first describe the general framework for our algorithm. Namely, we are initially given a set of parameters that help characterize our base concept, and a set of test examples.
Reference: [61] <author> Christos H. Papadimitriou. </author> <title> On the complexity of edge traversing. </title> <journal> J. Assoc. Comp. Mach., </journal> <volume> 23 </volume> <pages> 544-554, </pages> <year> 1976. </year>
Reference-contexts: The Chinese Postman Problem can be solved by a polynomial time algorithm if the graph is either undirected or directed [39]. The Chinese Postman problem for a mixed graph that has undirected and directed edges was shown to be NP-complete by Papadimitriou <ref> [61] </ref>. We do not know an optimal off-line algorithm for the Weak Postman Problem; this may be an NP-hard problem. We now give an approximation algorithm for the off-line piecemeal learning problem using a simple "interrupted-DFS" approach.
Reference: [62] <author> Christos H. Papadimitriou and Mihalis Yanakakis. </author> <title> Shortest paths without a map. </title> <journal> Theoretical Computer Science, </journal> <volume> 84 </volume> <pages> 127-150, </pages> <year> 1991. </year>
Reference-contexts: Kao, Reif and Tate [49] give a randomized algorithm for this problem that has better expected performance than any deterministic algorithm. Kao, Ma, Sipser and Yin [48] give an optimal deterministic search strategy for the case of multiple robots. Papadimitriou and Yanakakis <ref> [62] </ref> consider the problem of a robot with vision moving around in a plane filled with obstacles. The robot does not know its environment, but knows its exact absolute location at all times, as well as its start position and its goal position. <p> A 1 fi 1 face might correspond to a standard city-block; larger faces might correspond to obstacles (parks or shopping malls). Figure 3.3 gives an example. City-block graphs are also studied by Papadimitriou and Yanakakis <ref> [62] </ref>, Blum, Raghavan, and Schieber [22], and Bar-Eli, Berman, Fiat and Yan [10].
Reference: [63] <author> D. A. D. Parry. </author> <title> Coiled coils in alpha-helix-containing proteins: analysis of residue types within the heptad repeat and the use of these data in the prediction of coiled-coils in other proteins. </title> <journal> Bioscience Reports, </journal> <volume> 2 </volume> <pages> 1017-1024, </pages> <year> 1982. </year>
Reference-contexts: For example, in the case of the coiled coil motif, most known instances are 2-stranded coiled coils (i.e, coiled coils consisting of 2 ff-helices). As a result, known prediction algorithms work well for predicting 2-stranded coiled coils <ref> [14, 13, 12, 42, 58, 63] </ref>, but do not work as well for the related 3-stranded coiled coil motif (i.e., coiled coils consisting of 3 ff-helices), due to the lack of known 3-stranded coiled coil sequences. <p> Coiled coils show a characteristic heptad repeat with hydrophobic residues found in positions a and d, and this repeat makes coiled coils particularly amenable to recognition by computational techniques. Computational methods have been quite successful for predicting coiled coils <ref> [63, 58, 42, 12, 13, 14] </ref>. These techniques can be described, broadly, as follows: 1. Collect a database of known coiled coils and available amino acid subsequences. 2. Determine whether the unknown sequence shares enough distinguishing features with the known coiled coils to be considered a coiled coil. <p> These techniques can be described, broadly, as follows: 1. Collect a database of known coiled coils and available amino acid subsequences. 2. Determine whether the unknown sequence shares enough distinguishing features with the known coiled coils to be considered a coiled coil. Standard approaches <ref> [63, 58] </ref> look at the frequencies of each amino acid residue in each of the seven repeated positions. Overall this singles method does pretty well.
Reference: [64] <author> Leonard Pitt and Leslie G. Valiant. </author> <title> Computational limitations on learning from examples. </title> <type> Technical report, </type> <institution> Harvard University Aiken Computation Laboratory, </institution> <month> July </month> <year> 1986. </year>
Reference-contexts: hypotheses may be any polynomial-time algorithm <ref> [64] </ref>[50][66]. Several examples are known of concept classes that are hard to learn when hypotheses are restricted to belong to the same class as the target concept but easy to learn when they may belong to a larger class. In particular, Pitt and Valiant [64] showed that learning the class of k-term DNF formulas (that is, functions that can be represented by a disjunction of k monomials) is NP-hard if the learner is required to produce a k-term DNF formula, but is easy if the learner may use a representation of k-CNF formulas. <p> We show the following: Theorem 4 For any symmetric function f on k inputs except for f 2 f^; :^; T; F g, learning the class C k;f by C k;f is NP-hard. This theorem extends the work of Pitt and Valiant <ref> [64] </ref>, which shows that learning the class of k-term DNF formulas is NP-hard if the learner is required to produce a k-term DNF formula. Before giving the proof of Theorem 4, we first provide some intuition. <p> Proof: First note that if k = 2 then the only functions f with f 62 f^; :^; T; F g are the functions f_; :_; ; :g. The proof of <ref> [64] </ref> for 2-term DNF can be applied directly for these cases; so, we assume that k 3. Without loss of generality, we assume that f (k 1) = 0; that is, f outputs 0 when exactly k 1 of its inputs are 1.
Reference: [65] <author> Leonard Pitt and Leslie G. Valiant. </author> <title> Computational limitations on learning from examples. </title> <journal> Journal of the ACM, </journal> <volume> 35(4) </volume> <pages> 965-984, </pages> <year> 1988. </year>
Reference: [66] <author> Leonard Pitt and Manfred K. Warmuth. </author> <title> Reductions among prediction problems: On the difficulty of predicting automata (extended abstract). </title> <booktitle> In 3rd IEEE Conference on Structure in Commplexity Theory, </booktitle> <pages> pages 60-69, </pages> <address> Washington, DC, </address> <month> June </month> <year> 1988. </year>
Reference: [67] <author> N. Qian and T. Sejnowski. </author> <title> Predicting the secondary structure of globular proteins using neural network models. </title> <journal> Journal of Molecular Biology, </journal> <volume> 202 </volume> <pages> 865-884, </pages> <year> 1988. </year>
Reference-contexts: Various machine learning techniques have been applied to the protein structure prediction problem. The two main approaches are neural nets (e.g., <ref> [47, 67, 59] </ref>) and hidden Markov models (e.g., [53, 9]). Both of these approaches require adequate data on the target motif, since there is a "training session" on sequences that are known to contain the target motif.
Reference: [68] <author> Nagewara S. V. Rao, Srikumar Kareti, Weimin Shi, and S. Sitharama Iyengar. </author> <title> Robot navigation in unknown terrains: Introductory survey of non-heuristic algorithms. </title> <type> Technical Report ORNL/TM-12410, </type> <institution> Oak Ridge National Laboratory, </institution> <month> July </month> <year> 1993. </year>
Reference-contexts: For rectilinear streets, the algorithm achieves a competitive ratio of p There are many other related papers in the literature, particularly in the area of robotics (e.g., [57]) and maze searching (e.g., [25, 24]). Rao, Kareti, Shi, and Iyengar <ref> [68] </ref> give a survey of work on robot navigation in unknown terrains. 3.3 Formal model We model the robot's environment as a finite connected undirected graph G = (V; E) with distinguished start vertex s. Vertices represent accessible locations.
Reference: [69] <author> Ronald Rivest. </author> <booktitle> Machine learning lecture notes, </booktitle> <year> 1994. </year>
Reference: [70] <author> Ronald L. Rivest and Robert E. Schapire. </author> <title> Inference of finite automata using homing sequences. </title> <journal> Information and Computation, </journal> <volume> 103(2) </volume> <pages> 299-347, </pages> <month> April </month> <year> 1993. </year> <note> 120 Bibliography </note>
Reference-contexts: Rivest and Schapire <ref> [70] </ref> study environments that can be modeled by a strongly connected deterministic finite automata. The robot gets information about the automaton by actively experimenting in the environment and by observing input-output behavior. Rivest and Schapire show that a robot with a teacher can with high probability learn such an environment.
Reference: [71] <author> Dana Ron and Ronitt Rubinfeld. </author> <title> Exactly learning automata with small cover time. </title> <booktitle> In Proceedings of the 1995 Conference on Computational Learning Theory, </booktitle> <address> Santa Cruz, CA, </address> <month> July </month> <year> 1995. </year>
Reference-contexts: Rivest and Schapire show that a robot with a teacher can with high probability learn such an environment. They use homing sequences to improve Angluin's algorithm [1] to learn without using a "reset" mechanism. Ron and Rubinfeld <ref> [71] </ref> further extend this result by giving an efficient algorithm that with high probability learns finite automata with small cover time, without requiring a teacher. Dean et al. [33] study the problem of learning finite automaton when the output at each state has some probability of being incorrect.
Reference: [72] <author> Robert H. Sloan. </author> <title> Computational Learning Theory: New Models and Algorithms. </title> <type> PhD thesis, </type> <institution> MIT EECS Department, </institution> <month> May </month> <year> 1989. </year> <note> (Published as MIT/LCS/TR-448.). </note>
Reference: [73] <author> R. Tatusov, S. Altschul, and E. Koonin. </author> <title> Detection of conserved segments in proteins: Iterative scanning of sequence databases with alignment blocks. </title> <booktitle> Proceedings of the National Academy of Science, </booktitle> <volume> 91 </volume> <pages> 12091-12095, </pages> <month> December </month> <year> 1994. </year>
Reference-contexts: Although a few papers have dealt with iterative algorithms <ref> [73, 3, 46, 36] </ref>, they do not use randomness and weighting for updating of parameters. In our experience, we find that these components of the algorithm are critical to achieving good performance. <p> Other types of iterative approaches have been applied to sequence alignment and protein structure prediction by researchers <ref> [73, 3, 46, 36] </ref>. Algorithmically, our approach differs from these approaches in two major ways. The first is our use of randomness to incorporate sequences into our database, and the second is our use of weighting to update the database (see section 4.3).
Reference: [74] <author> Leslie G. Valiant. </author> <title> A theory of the learnable. </title> <journal> Communications of the ACM, </journal> <volume> 27(11) </volume> <pages> 1134-1142, </pages> <month> November </month> <year> 1984. </year>
Reference-contexts: In Chapter 4, we study the problem of learning protein motifs. Finally, in Chapter 5, we finish with some concluding remarks. C h a p t e r 2 Learning functions on k terms 2.1 Introduction Since its introduction, Valiant's distribution-free or PAC learning framework <ref> [74] </ref> has been a well-studied model of concept learning. In this framework, the object of a learning algorithm is to approximately infer an unknown target concept that belongs to some known concept class. The learner is given examples chosen randomly according to a fixed but unknown distribution. <p> For a symmetric function f and integer i, we let f (i) denote the value of f when exactly i of its inputs are 1. We study learning in the distribution-free or Probably Approximately Correct (PAC) learning model <ref> [74, 2] </ref>. In the PAC learning model, we assume that the learning algorithm has available an oracle EXAMPLES (c) that when queried, produces a labeled example h~v; c (~v)i according to a fixed but unknown probability distribution D.
Reference: [75] <author> B. Xiao, S. Smerdon, D. Jones, G. Dodson, Y. Soneji, A. Aitken, and S. Gamblin. </author> <title> Structure of a 14-3-3 protein and implications for coordination of multiple signalling pathways. </title> <journal> Nature, </journal> <volume> 376, </volume> <month> July 13, </month> <year> 1995. </year>
Reference-contexts: Our predictions were made independently of these results. Recently, the crystal structure of two 14-3-3 proteins have been solved <ref> [55, 75] </ref>. The paper of Liu et al. studies the zeta transform of the 14-3-3 structure in E. coli, and they report a 2-stranded antiparallel coiled coil structure. On the other hand, the paper of Xiao et al. studies the human T-cell t dimer, and they report helical bundles.
References-found: 75

