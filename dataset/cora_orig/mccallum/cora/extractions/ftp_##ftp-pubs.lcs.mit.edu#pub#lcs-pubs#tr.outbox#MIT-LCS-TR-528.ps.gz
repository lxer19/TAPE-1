URL: ftp://ftp-pubs.lcs.mit.edu/pub/lcs-pubs/tr.outbox/MIT-LCS-TR-528.ps.gz
Refering-URL: ftp://ftp-pubs.lcs.mit.edu/pub/lcs-pubs/listings/tr500.html
Root-URL: 
Title: MIT/LCS/TR-528 FILE SYSTEMS WITH MULTIPLE FILE IMPLEMENTATIONS  
Author: Raymie Stata 
Date: February 1992  
Abstract-found: 0
Intro-found: 1
Reference: [Anderson90] <author> David P. Anderson, Shin-Yuan Tzou, Robert Wahbe, Ramesh Govindan, and Martin Andrews. </author> <title> Support for continuous media in the DASH system. </title> <booktitle> Proceedings of 10th International Conference on Distributed Computing Systems (Paris), </booktitle> <pages> pages 54-61. </pages> <publisher> IEEE, </publisher> <month> May </month> <year> 1990. </year>
Reference-contexts: Obviously, this would greatly lower the utilization of the hardware. (The DASH system contains many ideas for implementing hard bounds in a server; however, the DASH system does not do static pre-allocation, and thus can not guarantee that the required resources will be available on demand <ref> [Anderson90] </ref>.) 4.4 File life cycles The service level a user needs from a file is seldom static; it changes over time.
Reference: [Carson90] <author> Scott D. Carson. </author> <title> Experimental performance evaluation of the Berkeley file system. </title> <institution> Technical report UMIACS-TR-90-5 and CS-TR-2387. Institute for Advanced Computer Studies and Department of Computer Science, University of Mary-land, </institution> <month> January </month> <year> 1990. </year>
Reference-contexts: As Figure 3-6 shows, randomizing placement within a disk results in high seek distances; some other intra-disk placement is needed. Seek distance can be reduced using an intra-disk placement algorithm called the "organ-pipe cylinder optimization" introduced by Grossman and Harvey [Grossman73] and recently pursued by Carson and Vongsathorn <ref> [Carson90] </ref> and Ruemmler [Ruemmler90]. In this algorithm, cylinders (or tracks or blocks) are sorted according to access frequency. 37 The most heavily used block is placed in the center of the disk. The second and third most heavily used blocks are placed on either side of that. <p> In this experiment, a reorganization was done once per day based on the statistics from the previous day. The resulting seek distance is slightly better than Unix placement. These results are consistent with other, more detailed experiments on organ-pipe optimization <ref> [Carson90, Vongsathorn90, Ruemmler90] </ref>. 3.4 Inter-disk placement constraints The previous section suggests a block placement algorithm that randomly selects spindles for blocks. However, fault-tolerance mechanisms often require that some blocks have 38 failure modes independent from other blocks.
Reference: [Cate90] <author> Vince Cate. </author> <title> Two levels of filesystem hierarchy on one disk. </title> <type> Technical report CMU-CS-90-129. </type> <institution> Carnegie-Mellon University, </institution> <address> Pittsburgh, PA, </address> <month> May </month> <year> 1990. </year>
Reference-contexts: Another way multiple file implementations will help close the I/O gap is by providing a mechanism to focus the increased processing power of future servers. For example, a file implementation could compress the file before sending it to disk <ref> [Cate90] </ref> or derive a file from a set of parameters. 2.3 The Virtual File System File systems with multiple file implementations are already being built and sold.
Reference: [Chervenak90] <author> Ann L. Chervenak. </author> <title> Performance measurements of the first RAID prototype. </title> <type> UCB/CSD 90/574. </type> <institution> University of California at Berkeley, </institution> <month> May </month> <year> 1990. </year>
Reference-contexts: No attempt was made to model interference among disks such as contention for I/O bandwidth at a shared junction. Although real-life experience has proven that such interference can be a bottleneck <ref> [Chervenak90] </ref>, we assumed that these bottlenecks would be removed by hardware designers. The individual disk simulators included a request queue, an elevator scheduling algorithm (taken from the HP-UX file system), and a model of the disk hardware.
Reference: [Finlayson87] <author> Ross S. Finlayson and David R. Cheriton. </author> <title> Log files: an extended file service exploiting write-once storage. </title> <booktitle> Proceedings of 11th ACM Symposium on Operating Systems Principles (Austin, Texas). Published as Operating Systems Review, </booktitle> <volume> 21(5) </volume> <pages> 139-48, </pages> <month> November </month> <year> 1987. </year>
Reference-contexts: Since many of the new media types are both denser and slower than magnetic disk, one result of the new media will be deeper storage hierarchies. Another result will be a number of specialized file implementations, such as log-structured file systems for optical WORM <ref> [Finlayson87] </ref>. 1.1.3 The increasing use of device parallelism A third trend faced by file system designers is the increasing use of storage device parallelism. N -dimensional disk arrays are a configuration of disks with N independent I/O channels used to increase bandwidth.
Reference: [Gelb89] <author> J. P. Gelb. </author> <title> System managed storage. </title> <journal> IBM Systems Journal, </journal> <volume> 28(1) </volume> <pages> 77-103, </pages> <year> 1989. </year>
Reference-contexts: When a new file implementation is added, existing files must be checked manually and, where appropriate, moved to the new implementation. Existing programs must be reconfigured before being able to use the new implementation. Gelb examines in detail the problems of exposing file implementations to the user <ref> [Gelb89] </ref>; note that these problems also apply to VFS, since it too exposes implementations to users. Another alternative approach would be to completely isolate the user from the assignment of files to implementations. <p> Through parameters, the user provides the extra information needed by the system to make assignments, yet the parameters do not expose low-level mechanism to the user. This approach was pioneered in IBM's system managed data product <ref> [Gelb89] </ref>. <p> While templates serve a useful role, one might wonder if the basic parameters do. After all, the system administrator could map templates directly to file implementations. This is the approach taken by IBM's system managed data product <ref> [Gelb89] </ref>. This alternative has two problems. First, it defeats the purpose of parameters. Directly mapping templates to implementations would still suffer the problems associated with exposing low-level mechanism.
Reference: [Gray90] <author> Jim Gray. </author> <title> A census of Tandem system availability between 1985 and 1990. </title> <type> Technical Report 90.1. </type> <institution> Tandem Computers Incorporated, </institution> <month> September </month> <year> 1990. </year>
Reference-contexts: The more devices a file is striped across, the higher the probability that one of them will fail [Schulze89]. Although the failure rates of secondary storage are decreasing dramatically <ref> [Gray90] </ref>, relying on the reliability of individual devices is not a cost-effective solution to the fault-tolerance problem of disk arrays; rather, redundancy is needed [Patterson88]. 7 1.1.4 The increasing demand for capacity The final trend we will consider is the growing size of file servers.
Reference: [Grossman73] <author> David D. Grossman and Harvey F. Silverman. </author> <title> Placement of records on a secondary storage device to minimize access time. </title> <journal> JACM, </journal> <volume> 20(3) </volume> <pages> 429-38, </pages> <month> July </month> <year> 1973. </year>
Reference-contexts: As Figure 3-6 shows, randomizing placement within a disk results in high seek distances; some other intra-disk placement is needed. Seek distance can be reduced using an intra-disk placement algorithm called the "organ-pipe cylinder optimization" introduced by Grossman and Harvey <ref> [Grossman73] </ref> and recently pursued by Carson and Vongsathorn [Carson90] and Ruemmler [Ruemmler90]. In this algorithm, cylinders (or tracks or blocks) are sorted according to access frequency. 37 The most heavily used block is placed in the center of the disk.
Reference: [Henderson89] <author> Robert L. Henderson and Alan Poston. MSS-II and RASH: </author> <title> a mainframe Unix based mass storage system with a rapid access storage hierarchy file management system. </title> <booktitle> USENIX Winter 1989 Conference (San Diego, </booktitle> <address> California, </address> <month> January </month> <year> 1990), </year> <pages> pages 65-84. </pages> <publisher> USENIX, </publisher> <month> January </month> <year> 1989. </year>
Reference-contexts: These implementations spread, or "stripe," the data of a single file across multiple devices (the number of devices is called the stripe width). Files can be striped at the bit level [Tucker88, Ng89], at the byte level [Kim85], or at the block level <ref> [Livny87, Patterson88, Henderson89] </ref>. File striping can potentially multiply throughput by the stripe width [Patterson88]. File striping can also decrease queuing time by spreading workload across disks [Livny87]. File striping has proven successful in practice for very large files that are accessed sequentially [Henderson89]. <p> File striping can potentially multiply throughput by the stripe width [Patterson88]. File striping can also decrease queuing time by spreading workload across disks [Livny87]. File striping has proven successful in practice for very large files that are accessed sequentially <ref> [Henderson89] </ref>. However, it is not clear how well striping works for small files [Ousterhout89]. <p> File system designers can improve parallelism through placement as well. The designers of the RASH system increased performance for large, sequentially accessed files by placing the blocks of files on different disks <ref> [Henderson89] </ref>. However, placement optimization is one of many system-wide goals that make up 23 a complicated design equation. Foremost among other considerations is software com-plexity: as a rule, the more aggressive the optimization, the more complex the software required.
Reference: [Karels86] <author> Michael J. Karels and Marshall Kirk McKusick. </author> <title> Toward a compatible filesystem interface. </title> <booktitle> European UNIX Systems User Group Autumn'86 (Manchester, </booktitle> <address> Eng-land, </address> <month> 2224 September </month> <year> 1986), </year> <pages> pages 481-96. </pages> <address> EUUG Secretariat, </address> <publisher> Owles Hall, </publisher> <address> Buntingford, Herts SG9 9PL, </address> <month> September </month> <year> 1986. </year>
Reference-contexts: At DEC, an experimental Unix system was built that can access media from MS-DOS, VMS, and BSD 4.3 [Koehler87]. Unix file systems with multiple implementations are based on Sun Microsystem's VFS. (Variations on VFS have been proposed and built, e.g., see [Rodriguez86] and <ref> [Karels86] </ref>.) Although VFS was introduced to allow remote file access, its design is general. It provides a general mechanism for multiple implementations; remote access is achieved by suppling an implementation that implements file operations by remote procedure calls to a file server.
Reference: [Kim85] <author> M. Y. Kim. </author> <title> Parallel operation of magnetic disk storage devices: synchronized disk interleaving. </title> <booktitle> Proceedings of 4th International Workshop on Database Machines (Grand Bahama Island), </booktitle> <pages> pages 300-30. </pages> <publisher> Springer-Verlag, </publisher> <address> New York, </address> <month> March </month> <year> 1985. </year>
Reference-contexts: These implementations spread, or "stripe," the data of a single file across multiple devices (the number of devices is called the stripe width). Files can be striped at the bit level [Tucker88, Ng89], at the byte level <ref> [Kim85] </ref>, or at the block level [Livny87, Patterson88, Henderson89]. File striping can potentially multiply throughput by the stripe width [Patterson88]. File striping can also decrease queuing time by spreading workload across disks [Livny87]. File striping has proven successful in practice for very large files that are accessed sequentially [Henderson89].
Reference: [Koehler87] <author> Matt Koehler. </author> <title> GFS revisited or how I lived with four different local file systems. </title> <booktitle> Proceedings of of the Summer 1987 USENIX Conference (Phoenix, </booktitle> <month> June </month> <year> 1987), </year> <pages> pages 291-305. </pages> <publisher> USENIX Association, </publisher> <address> Berkeley, </address> <month> June </month> <year> 1987. </year>
Reference-contexts: At DEC, an experimental Unix system was built that can access media from MS-DOS, VMS, and BSD 4.3 <ref> [Koehler87] </ref>. Unix file systems with multiple implementations are based on Sun Microsystem's VFS. (Variations on VFS have been proposed and built, e.g., see [Rodriguez86] and [Karels86].) Although VFS was introduced to allow remote file access, its design is general.
Reference: [Livny87] <author> Miron Livny, Setrag Khoshafian, and Haran Boral. </author> <title> Multi-disk management algorithms. </title> <booktitle> Proceedings of SIGMETRICS. </booktitle> <volume> '87, </volume> <pages> pages 69-77, </pages> <year> 1987. </year>
Reference-contexts: N -dimensional disk arrays are a configuration of disks with N independent I/O channels used to increase bandwidth. Recently, researchers in the workstation world have been looking at disk arrays to both increase bandwidth and decrease latency in hopes of closing the I/O gap <ref> [Patterson88, Livny87] </ref>. Although disk arrays have proven successful in bettering performance, they have also increased demand for better fault-tolerance. Increasing the number of disks on which a file is stored in order to increase potential parallelism also increases the probability that the file will fail. <p> These implementations spread, or "stripe," the data of a single file across multiple devices (the number of devices is called the stripe width). Files can be striped at the bit level [Tucker88, Ng89], at the byte level [Kim85], or at the block level <ref> [Livny87, Patterson88, Henderson89] </ref>. File striping can potentially multiply throughput by the stripe width [Patterson88]. File striping can also decrease queuing time by spreading workload across disks [Livny87]. File striping has proven successful in practice for very large files that are accessed sequentially [Henderson89]. <p> Files can be striped at the bit level [Tucker88, Ng89], at the byte level [Kim85], or at the block level [Livny87, Patterson88, Henderson89]. File striping can potentially multiply throughput by the stripe width [Patterson88]. File striping can also decrease queuing time by spreading workload across disks <ref> [Livny87] </ref>. File striping has proven successful in practice for very large files that are accessed sequentially [Henderson89]. However, it is not clear how well striping works for small files [Ousterhout89]. <p> However, simulation studies indicate that under most conditions declustering data across multiple disks performs better than clustering the data on a single disk <ref> [Livny87] </ref>. Given our assumption that future file servers will have multiple devices available, log-structured file implementations will most likely be striped.) 13 2.1.4 Delayed-write file implementations The delayed-write file implementation trades-off fault-tolerance for performance [Ohta90]. Unix 1 uses a write-back cache to speed file accesses.
Reference: [McKusick84] <author> Marshall K. McKusick, William N. Joy, Samuel J. Le*er, and Robert S. Fabry. </author> <title> A fast file system for UNIX. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 2(3) </volume> <pages> 181-97, </pages> <month> August </month> <year> 1984. </year>
Reference-contexts: For example, the designers at Berkeley reduced seek time of BSD Unix by ensuring that the data in a given file is clustered closely on the disk <ref> [McKusick84] </ref>. File system designers can improve parallelism through placement as well. The designers of the RASH system increased performance for large, sequentially accessed files by placing the blocks of files on different disks [Henderson89]. <p> The BSD placement algorithm uses knowledge of the on-disk representations of files and directories in order to reduce seek distance and rotational latency. This algorithm has been effective at increasing performance over that of previous versions of the Unix file system <ref> [McKusick84] </ref>, but it has problems in a file system with multiple file implementations. Pushing block placement into file implementations requires that the effort of developing and tuning a placement algorithm is duplicated for each implementation. <p> As we shall see, giving these algorithms such an advantage does not alter the end results. Third, the "Unix" placement is the placement given to blocks by HP-UX, which uses the Berkeley 4.3 file system <ref> [McKusick84] </ref>. 3.2.2 Results Table 3.1 shows the relative disk access frequency according to the different placement algorithms. The table shows that all three proposed placement methods (random, packed, and colored) are effective at fixing the disk load balancing problem. Although packed seems to be best, the difference is small. <p> Unix, on the other hand, carefully places blocks within disks in order to minimize seek distance <ref> [McKusick84] </ref>. As Figure 3-6 shows, randomizing placement within a disk results in high seek distances; some other intra-disk placement is needed.
Reference: [Ng89] <author> Spencer Ng. </author> <title> Some design issues of disk arrays. </title> <booktitle> Proceedings of COMPCON Spring '89, </booktitle> <pages> pages 137-42. </pages> <publisher> IEEE, </publisher> <year> 1989. </year> <month> 54 </month>
Reference-contexts: These implementations spread, or "stripe," the data of a single file across multiple devices (the number of devices is called the stripe width). Files can be striped at the bit level <ref> [Tucker88, Ng89] </ref>, at the byte level [Kim85], or at the block level [Livny87, Patterson88, Henderson89]. File striping can potentially multiply throughput by the stripe width [Patterson88]. File striping can also decrease queuing time by spreading workload across disks [Livny87].
Reference: [Ohta90] <author> Masataka Ohta and Hiroshi Tezuka. </author> <title> A fast /tmp file system by delay mount option. </title> <booktitle> 1990 Summer USENIX Technical Conference (Anaheim, </booktitle> <address> California, </address> <month> June </month> <year> 1990), </year> <pages> pages 145-50. </pages> <publisher> USENIX, </publisher> <month> June </month> <year> 1990. </year>
Reference-contexts: Given our assumption that future file servers will have multiple devices available, log-structured file implementations will most likely be striped.) 13 2.1.4 Delayed-write file implementations The delayed-write file implementation trades-off fault-tolerance for performance <ref> [Ohta90] </ref>. Unix 1 uses a write-back cache to speed file accesses. To minimize data loss in the case of a failure, Unix makes two exceptions to the write-back discipline. First, it periodically flushes all dirty buffers, usually every half-minute.
Reference: [Ousterhout85] <author> John K. Ousterhout, Herve Da Costa, David Harrison, John A. Kunze, Mike Kupfer, and James G. Thompson. </author> <title> A trace-driven analysis of the UNIX 4.2 BSD file system. </title> <booktitle> Proceedings of 10th ACM Symposium on Operating Systems Principles (Orcas Island, Washington). Published as Operating Systems Review, </booktitle> <volume> 19(5) </volume> <pages> 15-24, </pages> <month> December </month> <year> 1985. </year>
Reference-contexts: In the office and engineering environments, files tend to be small, often less than 8 kilobytes, which limits value of striping <ref> [Satyanarayanan81, Ousterhout85] </ref>. The LFS organizes many small files into one long, sequentially written log which in turn can be striped to great advantage.
Reference: [Ousterhout89] <author> John Ousterhout and Fred Douglis. </author> <title> Beating the I/O bottleneck: a case for log-structured file systems. </title> <journal> Operating Systems Review, </journal> <volume> 23(1) </volume> <pages> 11-27, </pages> <month> January </month> <year> 1989. </year>
Reference-contexts: CPU performance, network bandwidth, and primary and secondary storage density are increasing exponentially, but the access times and transfer rates of secondary storage are failing to keep pace <ref> [Ousterhout89, Wilkes90] </ref>. In fact, the access time to newer media such as magneto-optical disk is longer than preceding technology. This "I/O gap" is aggravated by other factors. As computer networks become increasingly common, file servers must handle the workloads of increasing numbers of systems. <p> Second, as the density of primary storage increases, designers can use larger file caches, both at the server and the client. Large file caches have already proven effective, and researchers are calling for even larger caches in the future <ref> [Ousterhout89] </ref>. 6 1.1.2 The proliferation of storage media Another trend faced by file system designers is the proliferation of secondary storage media types. Ten years ago, most files were stored on two media: magnetic disk and tape. <p> File striping can also decrease queuing time by spreading workload across disks [Livny87]. File striping has proven successful in practice for very large files that are accessed sequentially [Henderson89]. However, it is not clear how well striping works for small files <ref> [Ousterhout89] </ref>. As mentioned in the introduction, striping a file devices greatly decreases the mean time to failure (MTTF) for a file since the MTTF of N disks failing is 1=N that of a single disk (assuming independent failures). <p> the MTTF of a single disk divided by N (the number of disks in a stripe), this configuration is reliable enough for all but the largest stripes or most unreliable disks. 2.1.3 Log-structured file implementations The log-structured file system (LFS) is another implementation that takes advantage of storage device parallelism <ref> [Ousterhout89, Rosenblum90] </ref>. In the office and engineering environments, files tend to be small, often less than 8 kilobytes, which limits value of striping [Satyanarayanan81, Ousterhout85]. The LFS organizes many small files into one long, sequentially written log which in turn can be striped to great advantage.
Reference: [Parish90] <author> Tom Parish. </author> <title> Volume holographic storage devices, or, storing data in crystals with light. </title> <type> Technical report ACT-BOB-296-90. </type> <institution> Microelectronics and Computer Technology Corporation, </institution> <month> September </month> <year> 1990. </year>
Reference-contexts: Instead, file servers will need a number of complementary file implementations working together to provide better tuned service. (Although, if the I/O gap is closed by storage technology such as holographic storage <ref> [Parish90] </ref>, going back to "one-size-fits-all" file systems might be possible.) 1 Unix is a trademark of AT&T, Inc. 14 An example will make this argument more concrete. Parity striping requires less redundant storage than disk mirroring but requires an extra disk access for small writes [Patterson88].
Reference: [Patterson88] <author> David A. Patterson, Garth Gibson, and Randy H. Katz. </author> <title> A case for redundant arrays of inexpensive disks (RAID). </title> <booktitle> Proceedings of SIGMOD. </booktitle> <address> (Chicago, Illinois), </address> <month> 1-3 June </month> <year> 1988. </year>
Reference-contexts: N -dimensional disk arrays are a configuration of disks with N independent I/O channels used to increase bandwidth. Recently, researchers in the workstation world have been looking at disk arrays to both increase bandwidth and decrease latency in hopes of closing the I/O gap <ref> [Patterson88, Livny87] </ref>. Although disk arrays have proven successful in bettering performance, they have also increased demand for better fault-tolerance. Increasing the number of disks on which a file is stored in order to increase potential parallelism also increases the probability that the file will fail. <p> Although the failure rates of secondary storage are decreasing dramatically [Gray90], relying on the reliability of individual devices is not a cost-effective solution to the fault-tolerance problem of disk arrays; rather, redundancy is needed <ref> [Patterson88] </ref>. 7 1.1.4 The increasing demand for capacity The final trend we will consider is the growing size of file servers. If past history is any indication, then as CPU performance continues to grow exponentially, demand for secondary storage capacity will grow with it. <p> These implementations spread, or "stripe," the data of a single file across multiple devices (the number of devices is called the stripe width). Files can be striped at the bit level [Tucker88, Ng89], at the byte level [Kim85], or at the block level <ref> [Livny87, Patterson88, Henderson89] </ref>. File striping can potentially multiply throughput by the stripe width [Patterson88]. File striping can also decrease queuing time by spreading workload across disks [Livny87]. File striping has proven successful in practice for very large files that are accessed sequentially [Henderson89]. <p> Files can be striped at the bit level [Tucker88, Ng89], at the byte level [Kim85], or at the block level [Livny87, Patterson88, Henderson89]. File striping can potentially multiply throughput by the stripe width <ref> [Patterson88] </ref>. File striping can also decrease queuing time by spreading workload across disks [Livny87]. File striping has proven successful in practice for very large files that are accessed sequentially [Henderson89]. However, it is not clear how well striping works for small files [Ousterhout89]. <p> As a result, file striping is usually joined with 12 redundancy for fault-tolerance. Patterson et. al. survey a range of redundancy schemes for file striping <ref> [Patterson88] </ref>. The most popular of these options is a single parity stripe: given a stripe width of N disks, each stripe consists of N 1 data blocks and one parity block. <p> Parity striping requires less redundant storage than disk mirroring but requires an extra disk access for small writes <ref> [Patterson88] </ref>. A file system with multiple implementations could take advantage of the strengths of both by using mirroring when small writes are expected and parity striping when large writes (or a heavy read bias) are expected.
Reference: [Rodriguez86] <author> R. Rodriguez, M. Koehler, and R. Hyde. </author> <title> The generic file system. </title> <booktitle> 1986 Summer USENIX Technical Conference (Atlanta, </booktitle> <address> GA, </address> <month> June </month> <year> 1986), </year> <pages> pages 260-9. </pages> <publisher> USENIX, </publisher> <month> June </month> <year> 1986. </year>
Reference-contexts: At DEC, an experimental Unix system was built that can access media from MS-DOS, VMS, and BSD 4.3 [Koehler87]. Unix file systems with multiple implementations are based on Sun Microsystem's VFS. (Variations on VFS have been proposed and built, e.g., see <ref> [Rodriguez86] </ref> and [Karels86].) Although VFS was introduced to allow remote file access, its design is general. It provides a general mechanism for multiple implementations; remote access is achieved by suppling an implementation that implements file operations by remote procedure calls to a file server.
Reference: [Rosenblum90] <author> Mendel Rosenblum. </author> <title> The LFS file system. Sprite group, </title> <institution> Computer Science Div., Department of Electrical Engineering and Computer Science, University of California at Berkeley, </institution> <year> 1990. </year> <note> Slides for a presentation. </note>
Reference-contexts: the MTTF of a single disk divided by N (the number of disks in a stripe), this configuration is reliable enough for all but the largest stripes or most unreliable disks. 2.1.3 Log-structured file implementations The log-structured file system (LFS) is another implementation that takes advantage of storage device parallelism <ref> [Ousterhout89, Rosenblum90] </ref>. In the office and engineering environments, files tend to be small, often less than 8 kilobytes, which limits value of striping [Satyanarayanan81, Ousterhout85]. The LFS organizes many small files into one long, sequentially written log which in turn can be striped to great advantage.
Reference: [Ruemmler90] <author> Chris Ruemmler. </author> <title> Shu*eboard | methods for adaptive data reorganization. </title> <type> Technical Report HPL-CSP-90-41. </type> <institution> Concurrent Systems Project, Hewlett-Packard Laboratories, </institution> <month> 24 August </month> <year> 1990. </year> <month> 55 </month>
Reference-contexts: Seek distance can be reduced using an intra-disk placement algorithm called the "organ-pipe cylinder optimization" introduced by Grossman and Harvey [Grossman73] and recently pursued by Carson and Vongsathorn [Carson90] and Ruemmler <ref> [Ruemmler90] </ref>. In this algorithm, cylinders (or tracks or blocks) are sorted according to access frequency. 37 The most heavily used block is placed in the center of the disk. The second and third most heavily used blocks are placed on either side of that. <p> In this experiment, a reorganization was done once per day based on the statistics from the previous day. The resulting seek distance is slightly better than Unix placement. These results are consistent with other, more detailed experiments on organ-pipe optimization <ref> [Carson90, Vongsathorn90, Ruemmler90] </ref>. 3.4 Inter-disk placement constraints The previous section suggests a block placement algorithm that randomly selects spindles for blocks. However, fault-tolerance mechanisms often require that some blocks have 38 failure modes independent from other blocks.
Reference: [Satyanarayanan81] <author> M. Satyanarayanan. </author> <title> A study of file sizes and functional lifetimes. </title> <booktitle> Proceedings of 8th ACM Symposium on Operating Systems Principles (Asilomar, Ca). Published as Operating Systems Review, </booktitle> <volume> 15(5) </volume> <pages> 96-108, </pages> <month> December </month> <year> 1981. </year>
Reference-contexts: In the office and engineering environments, files tend to be small, often less than 8 kilobytes, which limits value of striping <ref> [Satyanarayanan81, Ousterhout85] </ref>. The LFS organizes many small files into one long, sequentially written log which in turn can be striped to great advantage.
Reference: [Schulze89] <author> Martin Schulze, Garth Gibson, Randy Katz, and David Patterson. </author> <title> How reliable is a RAID? Spring COMPCON'89 (San Francisco), </title> <address> pages 118-23. </address> <publisher> IEEE, </publisher> <month> March </month> <year> 1989. </year>
Reference-contexts: Increasing the number of disks on which a file is stored in order to increase potential parallelism also increases the probability that the file will fail. The more devices a file is striped across, the higher the probability that one of them will fail <ref> [Schulze89] </ref>.
Reference: [Sidebotham86] <author> Bob Sidebotham. </author> <title> VOLUMES the Andrew file system data structuring primitive. </title> <booktitle> European UNIX Systems User Group Autumn'86 (Manchester, </booktitle> <address> England, </address> <month> 2224 September </month> <year> 1986), </year> <pages> pages 473-80. </pages> <address> EUUG Secretariat, </address> <publisher> Owles Hall, </publisher> <address> Buntingford, Herts SG9 9PL, </address> <month> September </month> <year> 1986. </year>
Reference-contexts: As illustrated in Table 2.1, this is difficult to do (among other things, a division into volumes is hard to change once it is made). A more flexible volume system such as the one in AFS may mitigate this problem <ref> [Sidebotham86] </ref>. However, over the long run, as file servers contain more and more data, the sheer size of the problem will make manual disk balancing impractical.
Reference: [Tucker88] <author> Lewis W. Tucker and George G. Robertson. </author> <title> Architecture and applications of the Connection Machine. </title> <journal> Computer, </journal> <volume> 21(8) </volume> <pages> 26-38, </pages> <month> August </month> <year> 1988. </year>
Reference-contexts: These implementations spread, or "stripe," the data of a single file across multiple devices (the number of devices is called the stripe width). Files can be striped at the bit level <ref> [Tucker88, Ng89] </ref>, at the byte level [Kim85], or at the block level [Livny87, Patterson88, Henderson89]. File striping can potentially multiply throughput by the stripe width [Patterson88]. File striping can also decrease queuing time by spreading workload across disks [Livny87].
Reference: [Vongsathorn90] <author> Paul Vongsathorn and Scott D. Carson. </author> <title> A system for adaptive disk rearrangement. </title> <journal> Software|Practice and Experience, </journal> <volume> 20(3) </volume> <pages> 225-42, </pages> <month> March </month> <year> 1990. </year>
Reference-contexts: In this experiment, a reorganization was done once per day based on the statistics from the previous day. The resulting seek distance is slightly better than Unix placement. These results are consistent with other, more detailed experiments on organ-pipe optimization <ref> [Carson90, Vongsathorn90, Ruemmler90] </ref>. 3.4 Inter-disk placement constraints The previous section suggests a block placement algorithm that randomly selects spindles for blocks. However, fault-tolerance mechanisms often require that some blocks have 38 failure modes independent from other blocks.
Reference: [Wilkes89a] <author> John Wilkes. </author> <title> DataMesh | scope and objectives: a commentary. </title> <type> Technical Report HPL-DSD-89-44. </type> <institution> Distributed Systems Department, Hewlett-Packard Laboratories, </institution> <month> 18 July </month> <year> 1989. </year>
Reference-contexts: They will overcome the I/O gap by using caching, device parallelism, and sophisticated software optimizations. They will perform automatically most administrative functions, in particular failure recovery. An example of such a file server is the DataMesh, a large, high-performance file server being developed at Hewlett-Packard Laboratories <ref> [Wilkes89a, Wilkes89b] </ref>. The DataMesh is a large file server (.1-1.5 terabytes) built from a tightly coupled network of DataMesh nodes. Each DataMesh node has a secondary storage device (e.g., a hard disk) and a 20 MIPS processor.
Reference: [Wilkes89b] <author> John Wilkes. </author> <title> DataMesh TM | scope and objectives. </title> <type> Technical Report HPL-DSD-89-37rev1. </type> <institution> Distributed Systems Department, Hewlett-Packard Laboratories, </institution> <month> 19 July </month> <year> 1989. </year>
Reference-contexts: They will overcome the I/O gap by using caching, device parallelism, and sophisticated software optimizations. They will perform automatically most administrative functions, in particular failure recovery. An example of such a file server is the DataMesh, a large, high-performance file server being developed at Hewlett-Packard Laboratories <ref> [Wilkes89a, Wilkes89b] </ref>. The DataMesh is a large file server (.1-1.5 terabytes) built from a tightly coupled network of DataMesh nodes. Each DataMesh node has a secondary storage device (e.g., a hard disk) and a 20 MIPS processor.
Reference: [Wilkes90] <author> John Wilkes. </author> <title> DataMesh | project definition document. </title> <type> Technical Report HPL-CSP-90-1. </type> <institution> Concurrent Systems Project, Hewlett-Packard Laboratories, </institution> <month> 18 January </month> <year> 1990. </year>
Reference-contexts: CPU performance, network bandwidth, and primary and secondary storage density are increasing exponentially, but the access times and transfer rates of secondary storage are failing to keep pace <ref> [Ousterhout89, Wilkes90] </ref>. In fact, the access time to newer media such as magneto-optical disk is longer than preceding technology. This "I/O gap" is aggravated by other factors. As computer networks become increasingly common, file servers must handle the workloads of increasing numbers of systems.
Reference: [Wilkes91] <author> John Wilkes and Raymie Stata. </author> <title> Specifying data availability in multi-device file systems. </title> <booktitle> Position paper for 4th ACM-SIGOPS European Workshop (Bologna, </booktitle> <month> 56 3-5 September </month> <year> 1990). </year> <title> Published as Operating Systems Review, </title> <booktitle> 25(1) </booktitle> <pages> 56-9, </pages> <month> January </month> <year> 1991. </year>
Reference-contexts: Users must be able to express their needs to the system. Our approach combines these two alternatives. As in the first alternative, the user must set per-file parameters to control the assignment of files to implementations <ref> [Wilkes91] </ref>. However, unlike before, these parameters are abstract, they hide low-level mechanism 20 from the user. As in the second alternative, the system takes an active role in assign-ing files to implementations, using measurements like before and also using the user's parameter settings.
Reference: [Wong83] <author> C. K. Wong. </author> <title> Algorithmic studies in mass storage systems. </title> <publisher> Computer Science Press, 11 Taft Court, </publisher> <address> Rockville, MD 20850, </address> <year> 1983. </year> <month> 57 </month>
Reference-contexts: The fourth and fifth most heavily used blocks are placed on either side of those, and so on. The resulting arrangement is called the "organ-pipe" arrangement because the graph of access frequency versus cylinder number resembles a set of organ pipes <ref> [Wong83] </ref>. with randomized inter-disk placement. In this experiment, a reorganization was done once per day based on the statistics from the previous day. The resulting seek distance is slightly better than Unix placement.
References-found: 33

