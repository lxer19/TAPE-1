URL: http://robotics.stanford.edu/users/sahami/papers-dir/dl98-sonia.ps
Refering-URL: http://robotics.stanford.edu/users/sahami/papers.html
Root-URL: 
Email: baldonado@parc.xerox.com fsahami, yusufalig@cs.stanford.edu  
Title: SONIA: A Service for Organizing Networked Information Autonomously  
Author: Mehran Sahami Salim Yusufali Michelle Q. W. 
Keyword: Clustering, Classification, Feature Selection, Distributed Information  
Address: Gates Building 1A  3333 Coyote Hill Road Stanford University Palo Alto, CA 94304 Stanford, CA 94305-9010  
Affiliation: Baldonado  Xerox Palo Alto Research Center Computer Science Department  
Abstract: The recent explosion of on-line information in Digital Libraries and on the World Wide Web has given rise to a number of query-based search engines and manually constructed topical hierarchies. However, these tools are quickly becoming inadequate as query results grow incomprehensibly large and manual classification in topic hierarchies creates an immense bottleneck. We address these problems with a system for topical information space navigation that combines the query-based and taxonomic systems. We employ machine learning techniques to create dynamic document categorizations based on the full-text of articles that are retrieved in response to users' queries. Our system, named SONIA (Service for Organizing Networked Information Autonomously), has been implemented as part of the Stanford Digital Libraries Testbed. It employs a combination of technologies that takes the results of queries to networked information sources and, in real-time, automatically retrieve, parse and organize these documents into coherent categories for presentation to the user. Moreover, the system can then save such document organizations in user profiles which can then be used to help classify future query results by the same user. SONIA uses a multi-tier approach to extracting relevant terms from documents as well as statistical clustering methods to determine potential topics within a document collection. It also makes use of Bayesian classification techniques to classify new documents within an existing categorization scheme. In this way, it allows users to navigate the results of a query at a more topical level rather than having to examine each document text separately. 
Abstract-found: 1
Intro-found: 1
Reference: 1. <author> James Allan, Anton V. Leouski, and Russell C. Swan. </author> <title> Interactive cluster visualization for information retrieval. </title> <type> Technical Report IR-116, </type> <address> U. Mass, Amherst, </address> <note> Center for Intelligent Information Retrieval, </note> <year> 1997. </year>
Reference-contexts: Other researchers have focused on the use of visualization methods for conveying similarity between documents to the user. Such systems rely on the Cluster Hypothesis which states that closely associated documents tend to be relevant to the same requests [22]. Accordingly, the system of Allan, Leouski, and Swan <ref> [1] </ref> conveys document similarity to the user via spatial layout. Here, relevant documents are often located near each other spatially. Thus, when a user locates a relevant document, it is more likely that they will find other relevant documents by examining the local neighborhood.
Reference: 2. <author> Robert B. Allen, Pascal Obry, and Michael Littman. </author> <title> An interface for navigating clustered document sets returned by queries. </title> <booktitle> In Proceedings of ACM SIGOIS, </booktitle> <pages> pages 166-171, </pages> <year> 1993. </year>
Reference-contexts: Moreover, document clustering can also be useful for navigating query results [10], and specialized user interfaces have been developed for such document clustering systems. For example, Allen, Obry and Littman <ref> [2] </ref> have developed an interface that allows users to navigate through the dendogram of documents generated by a hierarchical agglomerative clustering algorithm. In this way, users can potentially locate subsets of particularly relevant documents.
Reference: 3. <author> Michelle Baldonado, Chen-Chuan K. Chang, Luis Gra-vano, and Andreas Paepcke. </author> <title> The Stanford digital library metadata architecture. </title> <journal> International Journal of Digital Libraries, </journal> <volume> 1(2), </volume> <year> 1997. </year>
Reference-contexts: Subsequently, we give a detailed description of the components that comprise SONIA. InfoBus Architecture The focus of the Stanford Digital Libraries project is on providing interoperability among heterogeneous, distributed information sources, services and interfaces. To this end, the InfoBus architecture <ref> [3] </ref> shown in Figure 1 has been developed. In brief, the InfoBus is comprised of network proxies that encapsulate the protocols used by disparate interfaces, information sources, and information services.
Reference: 4. <author> Michelle Q. Wang Baldonado and Terry Winograd. SenseMaker: </author> <title> An information-exploration interface supporting the contextual evolution of a user's interests. </title> <booktitle> In Proceedings of CHI, </booktitle> <year> 1997. </year>
Reference-contexts: In this way, new query results can be integrated into a topical partitioning derived from previous query results. This allows the user to build up a large collection of results spanning multiple related queries within the same organizational scheme. Currently, SONIA is accessed through the Java-based Sense-Maker interface <ref> [4] </ref>, which allows users to simultaneously query multiple heterogeneous information sources including popular Web search engines, proprietary information databases (e.g., DIALOG) and many others.
Reference: 5. <author> T. M. Cover and J. A. Thomas. </author> <title> Elements of Information Theory. </title> <publisher> Wiley, </publisher> <year> 1991. </year>
Reference-contexts: In the case where the user chooses not to categorize documents according to an existing profile, but instead wishes that a new categorization be created, we consider a different form of feature selection based on an entropy criterion <ref> [5] </ref>. Here we hone in on terms with high distributional variability among documents, making them likely to identify subtopics within a varied collection. For each term t i we compute the probability of its occurrence in a randomly chosen doc ument from our collection.
Reference: 6. <author> D. R. Cutting, D. R. Karger, J. O. Pederson, and J. W. Tukey. Scatter/Gather: </author> <title> a cluster-based approach to browsing large document collections. </title> <booktitle> In Proceedings of ACM/SIGIR, </booktitle> <pages> pages 318-329, </pages> <year> 1992. </year>
Reference-contexts: In this way, users can explicitly specify their information needs as queries while also having the ability to browse the results of their queries at a topical, rather than document, level. Related work in this area, most notably the Scatter/Gather ap-proach <ref> [6] </ref>, has shown that document clustering is an effective way for allowing users to quickly hone in on the documents relevant to them. Moreover, document clustering can also be useful for navigating query results [10], and specialized user interfaces have been developed for such document clustering systems. <p> For comparison, we consider the Cosine similarity measure used in conjunction with square root word frequency dampening (as in the Scatter/Gather system <ref> [6] </ref>) and standard TFIDF weighting [20] used widely in information retrieval systems. Realizing that methods for evaluating clustering algorithms are not without controversy, we use the following strategy, being aware of its limitations. <p> In this way, the user can explore the information space at a variety of granularity levels and thereby quickly focus on just those few documents that are truly relevant to their information need. Note that this interaction model is very related to that of the Scatter/Gather system <ref> [6] </ref>. More significant, however, is the fact that the user can save multiple profiles during their interactions with the system and thus maintain classification schemes at several different levels of granularity.
Reference: 7. <author> Moises Goldszmidt and Mehran Sahami. </author> <title> A probabilistic approach to full-text document clustering. </title> <type> Technical Report ITAD-433-MS-98-044, </type> <institution> SRI International, </institution> <year> 1998. </year>
Reference-contexts: As with the classification module, any reasonable clustering method can be used at this stage. We have recently conducted comparisons with a number of different clustering algorithms including hierarchical agglomerative clustering [18] and iterative clustering methods, such as K-Means [15], using different measures of similarity between documents <ref> [7] </ref>. Currently, we have chosen to use a two-step approach to clustering. First, group-average hierarchical agglomerative clustering is used to form an initial set of clusters which is then further optimized with an iterative method. <p> To compute the probabilities in Eq. 2, namely P (Y i = wjd i ), we use a novel normalized geometric mean (NGM) smoothing estimate. A justification of this estimate is beyond the scope of this paper (we refer the interested reader to <ref> [7] </ref> for further details), but we have found it to work quite well in practice and present a brief overview of these results shortly. The hierarchical agglomerative clustering method proceeds by initially placing each document in a separate cluster. <p> A brief review of the results of these experiments is presented in Table 2. For a more comprehensive comparison of these and other related similarity measures for clustering, we refer the reader to <ref> [7] </ref>. Here, we can clearly see that the measure of probabilistic 1 An updated version of this data set, Reuters-21578, is now available from David Lewis (http://www.research.att.com/lewis).
Reference: 8. <author> I. J. </author> <title> Good. The Estimation of Probabilities: An Essay on Modern Bayesian Methods. </title> <publisher> MIT Press, </publisher> <year> 1965. </year>
Reference-contexts: From this data, a classifier is built that can then be used to classify incoming documents. While SONIA provides full generality to use any classification algorithm, we have chosen to focus on techniques based on Bayesian networks. Currently, we use the Naive Bayesian classification algorithm <ref> [8] </ref>. This algorithm attempts to pre-dict for each document, d, the category, c j , for which it has maximal probability.
Reference: 9. <institution> Stanford Digital Libraries Group. The Stanford digital libraries project. Comm. of the ACM, </institution> <month> April </month> <year> 1995. </year>
Reference-contexts: Furthermore, since our system exists as part of a general architecture within the Stanford Digital Libraries Testbed <ref> [9] </ref>, it has the ability to simultaneously retrieve information from a number of heterogeneous sources, thereby making our system maximally flexible. SONIA was also designed with efficiency in mind, thereby facilitating real-time user interactivity even when accessing diverse, distributed document collections.
Reference: 10. <author> Marti A. Hearst, David R. Karger, and Jan O. Pederson. </author> <title> Scatter/Gather as a tool for the navigation of retrieval results. </title> <booktitle> In Proceedings of AAAI Fall Symposium on Knowledge Navigation, </booktitle> <year> 1995. </year>
Reference-contexts: Related work in this area, most notably the Scatter/Gather ap-proach [6], has shown that document clustering is an effective way for allowing users to quickly hone in on the documents relevant to them. Moreover, document clustering can also be useful for navigating query results <ref> [10] </ref>, and specialized user interfaces have been developed for such document clustering systems. For example, Allen, Obry and Littman [2] have developed an interface that allows users to navigate through the dendogram of documents generated by a hierarchical agglomerative clustering algorithm.
Reference: 11. <author> Marti A. Hearst and Jan O. Pederson. </author> <title> Reexamining the cluster hypothesis: </title> <booktitle> Scatter/Gather on retrieval results. In Proceedings of ACM/SIGIR, </booktitle> <year> 1996. </year>
Reference-contexts: Further support for the Cluster Hypothesis comes from the empirical observation that clustering tends to concentrate documents particularly relevant to a query in just one or two groupings <ref> [11] </ref>. Moreover, this work has shown that users are generally successful at locating a higher proportion of relevant documents by simply identifying the appropriate high-level groupings. Another example of the use of clustering to aid in information access includes the WEBSOM system [21].
Reference: 12. <author> T. Kohonen. </author> <title> Self-Organizing Maps. </title> <publisher> Springer, </publisher> <year> 1995. </year>
Reference-contexts: Moreover, this work has shown that users are generally successful at locating a higher proportion of relevant documents by simply identifying the appropriate high-level groupings. Another example of the use of clustering to aid in information access includes the WEBSOM system [21]. WEBSOM uses a Self-Organizing Map (SOM) <ref> [12] </ref> to group together related words into a word category map. This map is in turn used to automatically organize documents according to the words that they contain.
Reference: 13. <author> Daphne Koller and Mehran Sahami. </author> <title> Toward optimal feature selection. </title> <booktitle> In Proceedings of Machine Learning, </booktitle> <pages> pages 284-292, </pages> <year> 1996. </year>
Reference-contexts: To this end, we employ a form of information theoretic feature selection that is effective for a number of similar classification problems, including text categorization <ref> [13] </ref>. Moreover, we have also found in previous work [14] that very few terms are needed for accurate document classification. We corroborate these early findings with further experimental results presented later in this paper, showing that 50 features are quite sufficient for accurate classification.
Reference: 14. <author> Daphne Koller and Mehran Sahami. </author> <title> Hierarchically classifying documents using very few words. </title> <booktitle> In Proceedings of Machine Learning, </booktitle> <pages> pages 170-178, </pages> <year> 1997. </year>
Reference-contexts: To this end, we employ a form of information theoretic feature selection that is effective for a number of similar classification problems, including text categorization [13]. Moreover, we have also found in previous work <ref> [14] </ref> that very few terms are needed for accurate document classification. We corroborate these early findings with further experimental results presented later in this paper, showing that 50 features are quite sufficient for accurate classification. <p> We provide several examples of the performance of this method in controlled experiments subsequently. Nevertheless, to relax the restrictive independence assumption we have recently implemented more expressive Bayesian classification schemes [19] in SO-NIA, and found them to yield even better results for document classification <ref> [14] </ref>. Once the documents are classified into groups, this grouping information is passed through the InfoBus to the SenseMaker interface. These documents are then displayed according to the categories defined in the user's profile.
Reference: 15. <author> P. R. Krishnaiah and L. N. Kanal. </author> <title> Classification, Pattern Recognition, and Reduction in Dimensionality. </title> <publisher> Ams-terdam: North Holland, </publisher> <year> 1982. </year>
Reference-contexts: As with the classification module, any reasonable clustering method can be used at this stage. We have recently conducted comparisons with a number of different clustering algorithms including hierarchical agglomerative clustering [18] and iterative clustering methods, such as K-Means <ref> [15] </ref>, using different measures of similarity between documents [7]. Currently, we have chosen to use a two-step approach to clustering. First, group-average hierarchical agglomerative clustering is used to form an initial set of clusters which is then further optimized with an iterative method.
Reference: 16. <author> David D. Lewis and M. Ringuette. </author> <title> Comparison of two learning algorithms for text categorization. </title> <booktitle> In Proceedings of SDAIR, </booktitle> <year> 1994. </year>
Reference-contexts: This corresponds to assuming that the appearance of each term is independent of every other term given the value of the category variable C. While this assumption may seem unrealistic for text, the Naive Bayesian classifier has shown very good empirical results in text domains <ref> [16] </ref>. We provide several examples of the performance of this method in controlled experiments subsequently. Nevertheless, to relax the restrictive independence assumption we have recently implemented more expressive Bayesian classification schemes [19] in SO-NIA, and found them to yield even better results for document classification [14].
Reference: 17. <author> M. F. Porter. </author> <title> An algorithm for suffix stripping. </title> <booktitle> Program, </booktitle> <volume> 14(3) </volume> <pages> 130-137, </pages> <year> 1980. </year>
Reference-contexts: The retrieved document texts are then parsed into a series of alphanumeric terms (i.e., words). Optionally, these terms may be stemmed to their root as SONIA's parser includes a standard word stemming scheme <ref> [17] </ref>. Each term then forms a dimension in a high-dimensional vector-space in which the documents can now be represented as points. That is, the vector representing a document contains in the dimension for each term, the count of how many times that term appeared in the document.
Reference: 18. <author> E. Rasmussen. </author> <title> Clustering algorithms. </title> <editor> In William B. Frakes and Ricardo Baeza-Yates, editors, </editor> <booktitle> Information Retrieval: Data Structures and Algorithms, </booktitle> <pages> pages 419-442. </pages> <publisher> Prentice Hall, </publisher> <year> 1992. </year>
Reference-contexts: As with the classification module, any reasonable clustering method can be used at this stage. We have recently conducted comparisons with a number of different clustering algorithms including hierarchical agglomerative clustering <ref> [18] </ref> and iterative clustering methods, such as K-Means [15], using different measures of similarity between documents [7]. Currently, we have chosen to use a two-step approach to clustering.
Reference: 19. <author> Mehran Sahami. </author> <title> Learning limited dependence Bayesian classiers. </title> <booktitle> In Proceedings of KDD, </booktitle> <pages> pages 335-338, </pages> <year> 1996. </year>
Reference-contexts: We provide several examples of the performance of this method in controlled experiments subsequently. Nevertheless, to relax the restrictive independence assumption we have recently implemented more expressive Bayesian classification schemes <ref> [19] </ref> in SO-NIA, and found them to yield even better results for document classification [14]. Once the documents are classified into groups, this grouping information is passed through the InfoBus to the SenseMaker interface. These documents are then displayed according to the categories defined in the user's profile.
Reference: 20. <author> Gerard Salton and Chris Buckley. </author> <title> Term weighting approaches in automatic text retrieval. </title> <type> Technical Report 87-881, </type> <institution> Cornell Computer Science Dept., </institution> <year> 1987. </year>
Reference-contexts: Since we now have the term counts for each document, SONIA is capable of transforming the vector representation of documents to different weighting schemes, such as TFIDF weights <ref> [20] </ref> or a simple Boolean representation, indicating only term appearance or non-appearance in documents. Such different representations are easily generated when needed by different modules within SONIA. <p> For comparison, we consider the Cosine similarity measure used in conjunction with square root word frequency dampening (as in the Scatter/Gather system [6]) and standard TFIDF weighting <ref> [20] </ref> used widely in information retrieval systems. Realizing that methods for evaluating clustering algorithms are not without controversy, we use the following strategy, being aware of its limitations.
Reference: 21. <author> Honkela Timo, Kaski Samuel, Lagus Krista, and Koho-nen Teuvo. </author> <title> WEBSOM - self-organizing maps of document collections. </title> <booktitle> In Proceedings of WSOM'97 Workshop on Self-Organizing Maps, </booktitle> <pages> pages 310-315, </pages> <year> 1997. </year>
Reference-contexts: Moreover, this work has shown that users are generally successful at locating a higher proportion of relevant documents by simply identifying the appropriate high-level groupings. Another example of the use of clustering to aid in information access includes the WEBSOM system <ref> [21] </ref>. WEBSOM uses a Self-Organizing Map (SOM) [12] to group together related words into a word category map. This map is in turn used to automatically organize documents according to the words that they contain.
Reference: 22. <editor> C. J. van Rijsbergen. </editor> <booktitle> Information Retrieval. </booktitle> <address> Butter-worths, </address> <year> 1979. </year>
Reference-contexts: Other researchers have focused on the use of visualization methods for conveying similarity between documents to the user. Such systems rely on the Cluster Hypothesis which states that closely associated documents tend to be relevant to the same requests <ref> [22] </ref>. Accordingly, the system of Allan, Leouski, and Swan [1] conveys document similarity to the user via spatial layout. Here, relevant documents are often located near each other spatially. <p> These stop words are determined using a standard English stop word list of 570 words as well as a hand-crafted list of approximately 100 Web stop words (such as html and url). In the second-tier of feature selection, a Zipf's Law analysis <ref> [22] </ref> of term occurrence over the collection is used. This essentially eliminates terms that appear fewer than 3 or greater than 1000 times in the entire collection as not having adequate resolving power to differentiate sub-collections of documents.
Reference: 23. <author> Sholom M. Weiss and Casimir A. </author> <title> Kulikowski. Computer Systems that Learn. </title> <publisher> Morgan Kaufmann, </publisher> <year> 1991. </year>
Reference-contexts: Since we seek both to measure the performance of Naive Bayes on an absolute scale, as well as the relative effects of feature selection, we run Naive Bayes several times on each data set, using a different number of features in each case. We employ 10-fold cross-validation <ref> [23] </ref> for evaluation and report both the average classification accuracy and standard deviation over these 10 runs for each entry in Table 3. We also provide the overall average over all five data sets for each feature selection regime.
References-found: 23

