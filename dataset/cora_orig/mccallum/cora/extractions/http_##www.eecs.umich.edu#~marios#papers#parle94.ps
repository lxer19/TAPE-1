URL: http://www.eecs.umich.edu/~marios/papers/parle94.ps
Refering-URL: http://www.eecs.umich.edu/~marios/pubs.html
Root-URL: http://www.cs.umich.edu
Title: Memory Assignment for Multiprocessor Caches through Grey Coloring  
Author: Anant Agarwal John V. Guttag Christoforos N. Hadjicostis Marios C. Papaefthymiou 
Address: Cambridge, MA 02139 New Haven, CT 06520  
Affiliation: 1 Massachusetts Institute of Technology 2 Yale University  
Abstract: The achieved performance of multiprocessors is heavily dependent on the performance of their caches. Cache performance is severely degraded when data tiles used by a program conflict in the caches. This paper explores techniques for improving multiprocessor performance by improving cache utilization. Specifically, we investigate the problem of statically assigning data tiles to memory in a way that minimizes the impact of collisions in multiprocessor caches. We define the problem precisely and present an efficient procedure for finding solutions to it. The procedure incorporates a new technique, grey coloring, that reduces the maximum number of conflicts in any cache in the system by distributing cache misses evenly among processors.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> A. Agarwal, D. Chaiken, G. D'Souza, K. Johnson, D. Kranz, J. Kubiatowicz, K. Kurihara, B. Lim, G. Maa, D. Nussbaum, M. Parkin, and D. Yeung. </author> <title> The MIT Alewife Machine: A Large-Scale Distributed-Memory Multiprocessor. </title> <booktitle> In Workshop on Scalable Shared Memory Multiprocessors. </booktitle> <publisher> Kluwer Academic Publishers, </publisher> <year> 1991. </year> <note> Also appears as MIT/LCS Memo TM-454, </note> <year> 1991. </year>
Reference-contexts: In bus-based machines, the memory is implemented as a single module accessed over the bus, while in most large-scale machines the memory is physically distributed among all the processing nodes. Virtually all machines that support the shared-memory programming abstraction provide local caches at each processor <ref> [1, 15, 16, 18, 19] </ref>. Caches automatically replicate memory locations close to the processor and avoid expensive network traversals for most memory accesses. The existence of caches does not change the memory abstraction. <p> This phenomenon is illustrated by the graph of Figure 1 (a). This graph gives the activity profile of a blocked matrix multiply program running on a simulator of the Alewife machine <ref> [1] </ref>, using 64 processors. After a start-up transient (not shown in the graph), the number of operations going on in parallel becomes high. Subsequently, this number decreases drastically and remains at low levels for most of the rest of the computation.
Reference: [2] <author> A. Agarwal, D. Kranz, and V. Natarajan. </author> <title> Automatic Partitioning of Parallel Loops and Data Arrays for Distributed Shared Memory Multiprocessors. </title> <type> Technical Report MIT/LCS TM-481, </type> <institution> Massachusetts Institute of Technology, </institution> <month> December </month> <year> 1992. </year> <note> A short version also appears in ICPP 1993. </note>
Reference-contexts: This problem is non-trivial and has been the focus of much recent attention <ref> [2, 3, 6, 14, 17] </ref>. Our work applies to systems that perform data partitioning either at compile time or program creation time and to systems where we know which data tiles are accessed by which processors.
Reference: [3] <author> J. M. Anderson and M. S. Lam. </author> <title> Global Optimizations for Parallelism and Locality on Scalable Parallel Machines. </title> <booktitle> In Proceedings of SIGPLAN '93, Conference on Programming Languages Design and Implementation, </booktitle> <month> June </month> <year> 1993. </year>
Reference-contexts: This problem is non-trivial and has been the focus of much recent attention <ref> [2, 3, 6, 14, 17] </ref>. Our work applies to systems that perform data partitioning either at compile time or program creation time and to systems where we know which data tiles are accessed by which processors.
Reference: [4] <author> R. Barua, A. Agarwal, D. Kranz, and V. Natarajan. </author> <title> Addressing Partitioned Arrays in Distributed Shared Memory. </title> <note> In preparation, </note> <month> August </month> <year> 1993. </year>
Reference: [5] <author> A. Blum. </author> <title> Some tools for approximate 3-coloring. </title> <booktitle> Proc. of the 31st Annual Symposium on Foundations of Computer Science, </booktitle> <pages> pages 554-562, </pages> <month> October </month> <year> 1990. </year>
Reference-contexts: Theorem 1 The zero-conflict memory assignment problem is computationally equivalent to the graph k-colorability problem. Proof. Straightforward reduction. Graph k-colorability is among the most difficult intractable problems <ref> [5, 11, 20] </ref>. Thus, Theorem 1 implies that it is very unlikely to find a provably good algorithm for the memory assignment problem. 4 Overview of our method for memory assignment In this section we present an overview of our two-phase approach to dealing with the memory assignment problem.
Reference: [6] <author> D. Callahan and K. Kennedy. </author> <title> Compiling Programs for Distributed-Memory Multiprocessors. </title> <journal> Journal of Supercomputing, </journal> <pages> 2(151-169), </pages> <month> October </month> <year> 1988. </year>
Reference-contexts: This problem is non-trivial and has been the focus of much recent attention <ref> [2, 3, 6, 14, 17] </ref>. Our work applies to systems that perform data partitioning either at compile time or program creation time and to systems where we know which data tiles are accessed by which processors.
Reference: [7] <author> D. Chaiken, C. Fields, K. Kurihara, and A. Agarwal. </author> <title> Directory-Based Cache-Coherence in Large-Scale Multiprocessors. </title> <journal> IEEE Computer, </journal> <volume> 23(6) </volume> <pages> 41-58, </pages> <month> June </month> <year> 1990. </year>
Reference-contexts: The existence of caches does not change the memory abstraction. The hardware is responsible for maintaining the illusion that all reads and writes take place directly to main memory using some cache coherency protocol (see [10] for a description and evaluation of several bus-based protocols, and <ref> [7] </ref> for an evaluation of interconnection-network based protocols.) This paper focuses on direct-mapped caches. (We note, however, that this work applies equally to set-associative caches.) In a direct-mapped cache, each line in main memory corresponds to a unique entry in the cache memory.
Reference: [8] <author> G. J. Chaitin. </author> <title> Register allocation and spilling via graph coloring. </title> <booktitle> In Proc. of the ACM Sigplan '82 Symposium on Compiler Construction, </booktitle> <pages> pages 22-31, </pages> <month> June </month> <year> 1982. </year>
Reference-contexts: The cost of cache misses is proportional to the number of lines l in each data tile. edge for each pair of data tiles that must be stored in the same cache. We color the conflict graph G by applying a minor variation of the graph-coloring scheme in <ref> [8, 9] </ref> that is used in the context of register allocation for sequential processors.
Reference: [9] <author> F. C. Chow and J. L. Hennessy. </author> <title> The prioriry-based coloring approach to register allocation. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 12(4), </volume> <month> October </month> <year> 1990. </year>
Reference-contexts: The cost of cache misses is proportional to the number of lines l in each data tile. edge for each pair of data tiles that must be stored in the same cache. We color the conflict graph G by applying a minor variation of the graph-coloring scheme in <ref> [8, 9] </ref> that is used in the context of register allocation for sequential processors.
Reference: [10] <author> S. J. Eggers and R. H. Katz. </author> <title> Evaluating the performance of four snooping cache coherency protocols. </title> <booktitle> In Proceedings of the 16th International Symposium on Computer Architecture, </booktitle> <address> New York, </address> <month> June </month> <year> 1989. </year> <note> IEEE. </note>
Reference-contexts: The existence of caches does not change the memory abstraction. The hardware is responsible for maintaining the illusion that all reads and writes take place directly to main memory using some cache coherency protocol (see <ref> [10] </ref> for a description and evaluation of several bus-based protocols, and [7] for an evaluation of interconnection-network based protocols.) This paper focuses on direct-mapped caches. (We note, however, that this work applies equally to set-associative caches.) In a direct-mapped cache, each line in main memory corresponds to a unique entry in
Reference: [11] <author> M. R. Garey and D. S. Johnson. </author> <title> Computers and Intractability. </title> <editor> W. H. </editor> <publisher> Freeman and Co., </publisher> <address> San Francisco, </address> <year> 1979. </year>
Reference-contexts: Theorem 1 The zero-conflict memory assignment problem is computationally equivalent to the graph k-colorability problem. Proof. Straightforward reduction. Graph k-colorability is among the most difficult intractable problems <ref> [5, 11, 20] </ref>. Thus, Theorem 1 implies that it is very unlikely to find a provably good algorithm for the memory assignment problem. 4 Overview of our method for memory assignment In this section we present an overview of our two-phase approach to dealing with the memory assignment problem.
Reference: [12] <author> C. N. Hadjicostis. </author> <title> Heuristics for solving the memory assignment problem in multiprocessor caches. </title> <type> Bachelor's thesis, </type> <institution> Massachusetts Institute of Technology, </institution> <month> May </month> <year> 1993. </year>
Reference-contexts: It combines in a single structure the marionettes for all spilled tiles, starting at the slots computed in Place. The shifting procedure is similar to that for a single spilled tile described in the beginning of Section 6. A more detailed description of this scheme can be found in <ref> [12] </ref>. 7 Experiments This section describes an empirical evaluation of our proposed technique for the memory assignment problem. The purpose of our experiments was twofold. First, we wanted to investigate how often graph coloring can lead to a conflict-free assignment of the tiles in the main memory. <p> When grey coloring was employed in the cases where spilled tiles existed, the critical number of conflicts was approximately half of that achieved with the random scheme. A more detailed discussion of these experiments can be found in <ref> [12] </ref>. 7.1 Jacobi Relaxation In this set of experiments, we assumed that the processors access data tiles with access patterns similar to those found in Jacobi relaxation: each tile b is accessed by a specific number j of consecutive processors. <p> If a processor's cache was already full, we simply ignored the special tile for this specific processor, but we allowed the rest of them to access it in a regular way. More details about the generation of the Jacobian test inputs can be found in <ref> [12] </ref>. We generated the following kinds of input instances. For j = 3; 5; 7, we created two groups of inputs, J j (6) and J j (8), which we employed to experiment with caches that have 6 and 8 slots, respectively. <p> The terms MCC, MinAll, Min-Max, and Rnd refer to four schemes that we employed in the Place part of grey-coloring. Due to space limitations, we do not describe these schemes in this extended abstract. A detailed description, however, can be found in <ref> [12] </ref>.
Reference: [13] <author> D. O. Tanguay Jr. </author> <title> Compile-Time Loop Splitting for Distributed Memory Multiprocessors. </title> <type> Technical Report TM-490, </type> <institution> MIT, </institution> <month> May </month> <year> 1993. </year>
Reference: [14] <author> C. Koelbel, P. Mehrotra, and J. Van Rosendale. </author> <title> Supporting Shared Data Structures on Distributed Memory Architectures. </title> <booktitle> In Proceedings Principles and Practice of Parallel Programming II, ACM, </booktitle> <month> March </month> <year> 1990. </year> <note> ACM. </note>
Reference-contexts: This problem is non-trivial and has been the focus of much recent attention <ref> [2, 3, 6, 14, 17] </ref>. Our work applies to systems that perform data partitioning either at compile time or program creation time and to systems where we know which data tiles are accessed by which processors.
Reference: [15] <author> D. Lenoski, J. Laudon, K. Gharachorloo, A. Gupta, J. Hennessy, M. Horowitz, and M. Lam. </author> <title> Design of the Stanford DASH Multiprocessor. </title> <institution> Computer Systems Laboratory TR 89-403, Stanford University, </institution> <month> December </month> <year> 1989. </year>
Reference-contexts: In bus-based machines, the memory is implemented as a single module accessed over the bus, while in most large-scale machines the memory is physically distributed among all the processing nodes. Virtually all machines that support the shared-memory programming abstraction provide local caches at each processor <ref> [1, 15, 16, 18, 19] </ref>. Caches automatically replicate memory locations close to the processor and avoid expensive network traversals for most memory accesses. The existence of caches does not change the memory abstraction.
Reference: [16] <institution> Encore Multimax. Encore, Marlboro, Massachusetts. </institution>
Reference-contexts: In bus-based machines, the memory is implemented as a single module accessed over the bus, while in most large-scale machines the memory is physically distributed among all the processing nodes. Virtually all machines that support the shared-memory programming abstraction provide local caches at each processor <ref> [1, 15, 16, 18, 19] </ref>. Caches automatically replicate memory locations close to the processor and avoid expensive network traversals for most memory accesses. The existence of caches does not change the memory abstraction.
Reference: [17] <author> J. Ramanujam and P. Sadayappan. </author> <title> Compile-Time Techniques for Data Distribution in Distributed Memory Machines. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 2(4) </volume> <pages> 472-482, </pages> <month> October </month> <year> 1991. </year>
Reference-contexts: This problem is non-trivial and has been the focus of much recent attention <ref> [2, 3, 6, 14, 17] </ref>. Our work applies to systems that perform data partitioning either at compile time or program creation time and to systems where we know which data tiles are accessed by which processors.
Reference: [18] <author> J. B. Rothnie. </author> <title> Architecture of the KSR1 Computer System, </title> <address> March 1992. </address> <publisher> MIT LCS Seminar, </publisher> <address> Cambridge, MA. </address>
Reference-contexts: In bus-based machines, the memory is implemented as a single module accessed over the bus, while in most large-scale machines the memory is physically distributed among all the processing nodes. Virtually all machines that support the shared-memory programming abstraction provide local caches at each processor <ref> [1, 15, 16, 18, 19] </ref>. Caches automatically replicate memory locations close to the processor and avoid expensive network traversals for most memory accesses. The existence of caches does not change the memory abstraction.
Reference: [19] <institution> Sequent Symmetry. Sequent, Portland, Oregon. </institution>
Reference-contexts: In bus-based machines, the memory is implemented as a single module accessed over the bus, while in most large-scale machines the memory is physically distributed among all the processing nodes. Virtually all machines that support the shared-memory programming abstraction provide local caches at each processor <ref> [1, 15, 16, 18, 19] </ref>. Caches automatically replicate memory locations close to the processor and avoid expensive network traversals for most memory accesses. The existence of caches does not change the memory abstraction.
Reference: [20] <author> A. Wigderson. </author> <title> Improving the performance guarantee for approximate graph coloring. </title> <journal> JACM, </journal> <volume> 30(4) </volume> <pages> 729-735, </pages> <year> 1983. </year>
Reference-contexts: Theorem 1 The zero-conflict memory assignment problem is computationally equivalent to the graph k-colorability problem. Proof. Straightforward reduction. Graph k-colorability is among the most difficult intractable problems <ref> [5, 11, 20] </ref>. Thus, Theorem 1 implies that it is very unlikely to find a provably good algorithm for the memory assignment problem. 4 Overview of our method for memory assignment In this section we present an overview of our two-phase approach to dealing with the memory assignment problem.
References-found: 20

