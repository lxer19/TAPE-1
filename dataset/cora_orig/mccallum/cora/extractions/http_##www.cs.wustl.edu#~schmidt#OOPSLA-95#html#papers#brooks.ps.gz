URL: http://www.cs.wustl.edu/~schmidt/OOPSLA-95/html/papers/brooks.ps.gz
Refering-URL: http://www.cs.wustl.edu/~schmidt/OOPSLA-95/html/papers.html
Root-URL: 
Email: phil_brooks@mentorg.com  
Title: overall implementation. Some efficiencies can be gained back through the use of C++ type-safe wrappers
Author: by Phil Brooks 
Address: 8005 Boeckman Road  97070  
Affiliation: Mentor Graphics Corporation  Wilsonville, Oregon  
Date: January 15, 1996 1  
Note: Buffered Collection and Buffered Iterator Patterns  1.0 Introduction small part of the  like those provided by CORBA are not provided by these wrappers. These patterns are motivated by the following forces:  
Abstract: This paper describes implementation of a pair of patterns that improve performance of large collections and their iter-ators in distributed object systems through a combination of buffering and specialized inter-process communication techniques. The Buffered Collection and Buffered Iterator patterns provide a transparent mechanism for efficient transport of primitive data elements between collections residing in one address space and their clients residing in another. Performance trade-offs between the use of distributed object systems such as CORBA based distributed object services and lower level networking interfaces such as sockets and TLI are often very apparent in network bandwidth-intensive applications. [1] provides discussions regarding performance trade-offs related to the use of various transport mechanisms over ethernet and ATM networks and suggests that for certain applications, the use of lower level interfaces may be required for performance reasons. This trade-off, however, can result in a much more expensive development task if performance critical portions of the network interface make up a relatively The patterns described in this paper provide a mechanism that allows use of standard buffering techniques to improve performance of certain distributed objects. The buffered collection and buffered itera-tor patterns allow efficient handling of collections of small primitive elements while supporting conventional distributed object semantics through remote proxy objects[3][4]. Specifically, the patterns apply to remote proxies for collections that are composed of very large numbers of small primitive elements in collection form. 
Abstract-found: 1
Intro-found: 0
Reference: [1] <author> Douglas C. Schmidt, Tim Harrison, Ehab Al-Shaer, </author> <title> Object-Oriented Components for High-Speed Network Programming, </title> <booktitle> Proceedings of the USENIX Conference on Object-Oriented Technologies, </booktitle> <address> Monterey, CA, </address> <month> June </month> <year> 1995. </year>
Reference-contexts: 1.0 Introduction Performance trade-offs between the use of distributed object systems such as CORBA based distributed object services and lower level networking interfaces such as sockets and TLI are often very apparent in network bandwidth-intensive applications. <ref> [1] </ref> provides discussions regarding performance trade-offs related to the use of various transport mechanisms over ethernet and ATM networks and suggests that for certain applications, the use of lower level interfaces may be required for performance reasons. <p> This mechanism removes much of the RPC marshalling and unmarshalling overhead associated with the RPC Buffer/RBuffer. Performance gains related to the use of this type of transport mechanism are dis cussed in detail in <ref> [1] </ref>. In addition, data compression techniques can be applied to the buffered data to further reduce network transmission overhead. For example, ICrules uses a number of compression techniques including representing orthogonal rectangles (usually four x,y coordinate pairs) with two x,y coordinate pairs representing the opposite corners of the rectangle.
Reference: [2] <author> Douglas C. Schmidt, </author> <title> The ADAPTIVE Communication Environment: Object-Oriented Network Programming Components for Developing Client/ Server Applications, </title> <booktitle> Proceedings of the 12th Annual Sun Users Group Conference, </booktitle> <address> SUG, San Francisco, CA, </address> <pages> pp. 214-225, </pages> <month> June, </month> <year> 1994. </year>
Reference-contexts: Some efficiencies can be gained back through the use of C++ type-safe wrappers for the traditional C style network programming interfaces such as the ACE components <ref> [2] </ref>, but distributed object semantics like those provided by CORBA are not provided by these wrappers. The patterns described in this paper provide a mechanism that allows use of standard buffering techniques to improve performance of certain distributed objects. <p> The two in the example are: RPC Buffer/RBuffer - uses ONC RPC as the underlying data transport. This provides a very exible transport mech anism useful under general circum stances. Shared_Malloc Buffer/RBuffer - uses ACE Shared_Malloc <ref> [2] </ref> as the underly ing data transport. This provides a trans port mechanism that is optimized for same-host execution. Other transport mechanisms could be encapsulated here as well. Two additional transport mechanisms that might be useful are: File Buffer/RBuffer - uses NFS mounted files as the underlying data transport. <p> This can provide an especially efficient mechanism for broadcasting data to multiple remote address spaces since the single file can be read by mul tiple remote processes. Sock SAP Buffer/RBuffer - uses ACE socket wrappers <ref> [2] </ref> to provide an efficient mechanism for transfer where hosts are known use consistent data representations. This mechanism removes much of the RPC marshalling and unmarshalling overhead associated with the RPC Buffer/RBuffer. <p> These techniques include high-speed networks or lower latency data exchange mechanisms (such as shared memory through a facility like ACE Shared_Malloc <ref> [2] </ref>, sockets, or disk file sharing). Encapsulating the optimizations behind a consistent abstract interface simplifies implementation by allowing one consistent general purpose mechanism (such as RPC) to handle the bulk of the implementation. <p> Two buffer implementations are illustrated, the first uses RPC and the second uses Shared_Malloc_MM from the ACE class library <ref> [2] </ref>. For simplicity and brevitys sake, most of the error handling code and C++ const correctness has been omitted. The List class is a shortened version of the List class described in [3].
Reference: [3] <author> Erich Gamma, Richard Helm, Ralph Johnson, and John Vlissides, </author> <title> Design Patterns: Elements of Reusable Object-Oriented Software, </title> <publisher> Addison-Wesley, </publisher> <address> Reading, MA, </address> <year> 1994 </year>
Reference-contexts: This paper is organized in the canonical design pattern format described in <ref> [3] </ref>. Section 4 motivates these patterns by showing how they are used in Mentor Graphics Distributed Design Rule Checking system ICrules. Sections 5-8 describe the patterns in detail. Section 9 describes implementation in which a single address space application is converted to a multi process application. <p> The pat tern is not applicable when complex caching is required. It is also not suitable for robust iterators or iterators that can cope with insertion and removal wont interfere with tra versal <ref> [3] </ref>. 6.0 Structure and Participants The participants in each of these patterns act in client and server pairs. The server side of the pair is distinguished by the R in its name (for Remote). 6.1 Buffered Collection FIGURE 3. <p> Real Collection - Reuse a non-distrib uted Collection. Buffered Collection and Buffered Iterator Patterns January 15, 1996 6 6.2 Buffered Iterator FIGURE 4. Buffered Iterator Buffered Iterator proxy - This proxy is derived from an Iterator as described in <ref> [3] </ref>. Collection Proxy - The Buffered Iterator proxy knows about a Real Collection through a Collection Proxy that need not be a Buffered Collection Proxy. Output Buffer/RBuffer - Buffered Itera tor proxies use the Output Buffer inter face to pull data from the actual collection. <p> These patterns have minimal cache consistency checks and are therefore limited to fairly simple usage. Many of the issues described in relation to the buffered collection and iterator apply to a more complex distributed data implementations. In addition, the more complex forms require something like an observer pattern <ref> [3] </ref> to guarantee cache consistency. Examples of services offering more complex distributed data management include the DCE distributed file system [10] from OSF, and Linda [11] distributed shared memory system. <p> Linda provides a virtual shared memory system which also guarantees the consistency of multiple cached copies of data. More complex data distribution mechanisms like these are required for the following situations: - Robust Iterator pattern <ref> [3] </ref> - Gamma et al describe a pattern called the Robust Iterator in which a collection can be altered while an iterator is running and the iterator will behave in an expected manner (i.e. it should not return items that have been previously deleted from the collection). <p> For simplicity and brevitys sake, most of the error handling code and C++ const correctness has been omitted. The List class is a shortened version of the List class described in <ref> [3] </ref>. <p> for brevitys sake. template&lt;class Item&gt; class List - friend class ListIterator&lt;Item&gt;; public: List (); ~List (); unsigned int Count (); bool Prepend ( Item & d ); bool Append ( Item & d ); bool RemoveFirst ( Item & d ); -; The ListIterator presents the Iterator interface described in <ref> [3] </ref>: template &lt;class Item&gt; class Iterator - public: virtual void First () = 0; virtual void Next () = 0; virtual bool IsDone () = 0; virtual Item CurrentItem () = 0; protected: Buffered Collection and Buffered Iterator Patterns January 15, 1996 11 Iterator (); -; The BufferedListProxy class serves as <p> Lazy Evaluation of the buffered data - some systems might benefit from collections which transmit and marshall/ unmarshall buffered data on demand rather than as it is added to the collec tion. 13.0 See Also Remote Proxy pattern <ref> [3] </ref>, [4] Iterator pattern [3] 14.0 Acknowledgments The following individuals contributed to the development and refinement of this pattern: Douglas Schmidt - schmidt@cs.wustl.edu Rich Strobel - rich_strobel@mentorg.com Robert Todd - robert_todd@mentorg.com George. Moberly - georgem@homer.atria.com Participants in the OOPSLA 1995 workshop on Concurrent, Parallel and Distributed systems. <p> Lazy Evaluation of the buffered data - some systems might benefit from collections which transmit and marshall/ unmarshall buffered data on demand rather than as it is added to the collec tion. 13.0 See Also Remote Proxy pattern <ref> [3] </ref>, [4] Iterator pattern [3] 14.0 Acknowledgments The following individuals contributed to the development and refinement of this pattern: Douglas Schmidt - schmidt@cs.wustl.edu Rich Strobel - rich_strobel@mentorg.com Robert Todd - robert_todd@mentorg.com George. Moberly - georgem@homer.atria.com Participants in the OOPSLA 1995 workshop on Concurrent, Parallel and Distributed systems.
Reference: [4] <author> A. Birrell, G. Nelson, S. Owicki, E. Wobber, </author> <title> Network Objects, </title> <institution> Digital Systems Research Center, </institution> <month> Feb </month> <year> 1994 </year>
Reference-contexts: Lazy Evaluation of the buffered data - some systems might benefit from collections which transmit and marshall/ unmarshall buffered data on demand rather than as it is added to the collec tion. 13.0 See Also Remote Proxy pattern [3], <ref> [4] </ref> Iterator pattern [3] 14.0 Acknowledgments The following individuals contributed to the development and refinement of this pattern: Douglas Schmidt - schmidt@cs.wustl.edu Rich Strobel - rich_strobel@mentorg.com Robert Todd - robert_todd@mentorg.com George. Moberly - georgem@homer.atria.com Participants in the OOPSLA 1995 workshop on Concurrent, Parallel and Distributed systems.
Reference: [5] <author> John Bloomer, </author> <title> Power Programming with RPC, </title> <publisher> OReilly & Associates, </publisher> <year> 1992. </year> <title> Buffered Collection and Buffered Iterator Patterns January 15, </title> <year> 1996 </year> <month> 14 </month>
Reference-contexts: Control and Synchronization - Control issues also need to be dealt with for each buffer implementation: ONC RPC systems provide their own control and synchronization mechanism that provides a simple mechanism in which the client blocks until the server returns. More elaborate message style control mechanisms can be constructed <ref> [5] </ref> to enable parallel process ing in the client and the server. mmap based - shared memory does not, by itself, provide any mechanism for synchronization, so some other mechanism, such as semaphores, must be selected. <p> - else - // writeData is a simple helper that // copies into the shared memory buffer // via memcpy and updates the cursize_. writeData ( &P.vertex_info.vertex_info_len, sizeof (P.vertex_info.vertex_info_len) ); writeData ( &P.vertex_info.vertex_info_val, P.vertex_info.vertex_info_len * sizeof ( vertex ) ); - 11.0 Examples Distributed ICrules Batch RPC on ONC RPC <ref> [5] </ref> DCE Pipes [8] Corba Event Channels 12.0 Variants Variations or enhancements to this pattern might include: Use of cache consistency algorithms to allow for a more complex collection/ iterator interactions like the Robust Iter ator. Support for multi-client manipulation of a server side of the collection.
Reference: [6] <author> G. Hamilton, M. L. Powell, J. G. Mitchell, Subcontract: </author> <title> A Flexible Base for Distributed Programming. </title> <booktitle> Proc. 14th ACM Symposium on Operating Systems Principles, </booktitle> <month> December </month> <year> 1993. </year>
Reference: [7] <author> E.V. Krishnamurthy, </author> <booktitle> Parallel processing: principles and practice, </booktitle> <publisher> Addison-Wesley, </publisher> <year> 1989. </year>
Reference-contexts: New Edge Collections are created as output. of a typical DRC run. Since any two operation vectors can proceed in parallel without interaction if their respective input data layers are available, the divide and conquer <ref> [7] </ref> strategy to tackling parallel problems is ideal for the DRC algorithms used in ICverify. Tasks can be partitioned and scheduled by a master process and executed by a number of slave processes that are distributed through a network.
Reference: [8] <author> J. Shirley, W. Hu, D. </author> <title> Magid Guide to writing DCE Applications, </title> <publisher> OReilly & Associates, </publisher> <year> 1992. </year>
Reference-contexts: // writeData is a simple helper that // copies into the shared memory buffer // via memcpy and updates the cursize_. writeData ( &P.vertex_info.vertex_info_len, sizeof (P.vertex_info.vertex_info_len) ); writeData ( &P.vertex_info.vertex_info_val, P.vertex_info.vertex_info_len * sizeof ( vertex ) ); - 11.0 Examples Distributed ICrules Batch RPC on ONC RPC [5] DCE Pipes <ref> [8] </ref> Corba Event Channels 12.0 Variants Variations or enhancements to this pattern might include: Use of cache consistency algorithms to allow for a more complex collection/ iterator interactions like the Robust Iter ator. Support for multi-client manipulation of a server side of the collection.
Reference: [9] <author> W.R. </author> <title> Stevens Advanced Programming in the UNIX Environment, </title> <publisher> Addison-Wesley, </publisher> <year> 1992. </year>
Reference-contexts: If the proxies are constructed using RPC, a simple RPC based control mechanism is easy to implement. file based record locking <ref> [9] </ref> can pro vide an efficient and simple mechanism for file based control, although NFS locking capabilities are not well known for their robustness. socket based - Mechanisms here can be similar to those used by RPC systems themselves.
Reference: [10] <institution> Open Software Foundation, </institution> <note> DCE Filesystem white paper, http://www.ofs.org/comm/lit/OSF-O-WP8-0990-2.ps </note>
Reference-contexts: In addition, the more complex forms require something like an observer pattern [3] to guarantee cache consistency. Examples of services offering more complex distributed data management include the DCE distributed file system <ref> [10] </ref> from OSF, and Linda [11] distributed shared memory system. OSFs DCE implements a token passing scheme to guarantee consistency between multiple cached copies of pages from distributed files that are a part of its distributed file system.
Reference: [11] <author> N. Carriero, D. Gelernter, </author> <title> Linda in Context, </title> <journal> Communications of the ACM Vol32 April 1989. </journal>
Reference-contexts: In addition, the more complex forms require something like an observer pattern [3] to guarantee cache consistency. Examples of services offering more complex distributed data management include the DCE distributed file system [10] from OSF, and Linda <ref> [11] </ref> distributed shared memory system. OSFs DCE implements a token passing scheme to guarantee consistency between multiple cached copies of pages from distributed files that are a part of its distributed file system.
References-found: 11

