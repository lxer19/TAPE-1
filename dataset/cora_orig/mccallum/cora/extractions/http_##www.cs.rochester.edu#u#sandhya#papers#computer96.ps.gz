URL: http://www.cs.rochester.edu/u/sandhya/papers/computer96.ps.gz
Refering-URL: http://www.cs.rochester.edu/u/sandhya/papers/
Root-URL: 
Email: treadmarks@ece.rice.edu  
Title: TreadMarks: Shared Memory Computing on Networks of Workstations  
Author: Cristiana Amza, Alan L. Cox, Sandhya Dwarkadas, Pete Keleher, Honghui Lu, Ramakrishnan Rajamony, Weimin Yu and Willy Zwaenepoel 
Web: http://www.cs.rice.edu/~willy/TreadMarks/overview.html  
Affiliation: Rice University  
Abstract: TreadMarks supports parallel computing on networks of workstations by providing the application with a shared memory abstraction. Shared memory facilitates the transition from sequential to parallel programs. After identifying possible sources of parallelism in the code, most of the data structures can be retained without change, and only synchronization needs to be added to achieve a correct shared memory parallel program. Additional transformations may be necessary to optimize performance, but this can be done in an incremental fashion. We discuss the techniques used in TreadMarks to provide efficient shared memory, and our experience with two large applications, mixed integer programming and genetic linkage analysis. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> S. Ahuja, N. Carreiro, and D. Gelernter. </author> <title> Linda and friends. </title> <journal> IEEE Computer, </journal> <volume> 19(8) </volume> <pages> 26-34, </pages> <month> August </month> <year> 1986. </year>
Reference-contexts: The programmability and performance differences between these two approaches are not yet well understood. Structured DSM Systems (Linda). Rather than providing the programmer with a shared memory space organized as a linear array of bytes, structured DSM systems offer a shared space of objects or tuples <ref> [1] </ref>, which are accessed by properly synchronized methods. Besides the advantages 19 from a programming perspective, this approach allows the compiler to infer certain optimizations that can be used to reduce the amount of communication.
Reference: [2] <author> B.N. Bershad, M.J. Zekauskas, </author> <title> and W.A. </title> <booktitle> Sawdon. The Midway distributed shared memory system. In Proceedings of the '93 CompCon Conference, </booktitle> <pages> pages 528-537, </pages> <month> February </month> <year> 1993. </year>
Reference-contexts: It was also shown that an invalidate protocol works better than an update protocol, because of the large amount of data resulting from the update protocol. Entry Consistency (Midway). Entry consistency is another relaxed memory model <ref> [2] </ref>. As in release consistency, consistency actions are taken in conjunction with synchronization operations. Unlike release consistency, however, entry consistency requires that each shared data object be associated with a synchronization object. When a synchronization object is acquired, only the modified data associated with that synchronization object is made consistent.
Reference: [3] <author> J.B. Carter, J.K. Bennett, and W. Zwaenepoel. </author> <title> Techniques for reducing consistency-related information in distributed shared memory systems. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 13(3) </volume> <pages> 205-243, </pages> <month> August </month> <year> 1995. </year>
Reference-contexts: TreadMarks uses the lazy release consistency algorithm [5] to implement release consistency. Roughly speaking, lazy release consistency enforces consistency at the time of an acquire, in contrast to the earlier implementation of release consistency in Munin <ref> [3] </ref>, sometimes referred to as eager release consistency, which enforced consistency at the time of a release. Figure 6 illustrates the intuitive argument behind lazy release consistency. Assume that x is replicated at all processors. <p> In contrast, in release consistency, messages are sent for every synchronization operation. Although the net effect is somewhat application dependent, release consistent DSMs in general send fewer messages than sequentially consistent DSMs and therefore perform better. A recent paper by Carter et al. <ref> [3] </ref> contains a comparison of seven application programs run either with eager release consistency (Munin) or with sequential consistency. Compared to a sequentially consistent DSM, Munin achieves performance improvements ranging from a few to several hundred percent, depending on the application. Lazy vs. Eager Release Consistency (Munin). <p> Compared to a sequentially consistent DSM, Munin achieves performance improvements ranging from a few to several hundred percent, depending on the application. Lazy vs. Eager Release Consistency (Munin). Lazy release consistency causes fewer messages to be sent than eager release consistency, as implemented in Munin <ref> [3] </ref>. At the time of a lock release, Munin sends messages to all processors who cache data modified by the releasing processor. In contrast, in lazy release consistency, consistency messages only travel between the last releaser and the new acquirer.
Reference: [4] <author> S.K. Gupta, A.A. Schaffer, A.L. Cox, S. Dwarkadas, and W. Zwaenepoel. </author> <title> Integrating par-allelization strategies for linkage analysis. </title> <journal> Computers and Biomedical Research, </journal> <volume> 28 </volume> <pages> 116-139, </pages> <month> June </month> <year> 1995. </year>
Reference-contexts: From these probabilities an approximate location of the gene on the chromosome can be computed. ILINK is a parallelized version of a widely used genetic linkage analysis program, which is part of the FASTLINK package <ref> [4] </ref>. ILINK takes as input a family tree, called a pedigree, augmented with some genetic information about the members of the family. It computes a maximum-likelihood estimate of , the recombination probability. At the top level, ILINK consists of a loop that optimizes .
Reference: [5] <author> P. Keleher. </author> <title> Distributed Shared Memory Using Lazy Release Consistency. </title> <type> PhD thesis, </type> <institution> Rice University, </institution> <month> December </month> <year> 1994. </year> <note> Appeared as Rice Technical Report RICE COMP-TR-240 and available by anonymous ftp from cs.rice.edu under public/TreadMarks/papers. </note>
Reference-contexts: The naive use of virtual memory protection hardware may lead to poor performance because of discrepancies between the page size of the machine and the granularity of sharing in the application. The system discussed in this paper, TreadMarks <ref> [5] </ref>, provides shared memory as a linear array of bytes. The memory model is a relaxed memory model, namely release consistency. <p> This allows an implementation of release consistency considerable latitude in deciding when and how exactly a shared memory update gets propagated. TreadMarks uses the lazy release consistency algorithm <ref> [5] </ref> to implement release consistency. Roughly speaking, lazy release consistency enforces consistency at the time of an acquire, in contrast to the earlier implementation of release consistency in Munin [3], sometimes referred to as eager release consistency, which enforced consistency at the time of a release. <p> The processes acquire and release the lock l, and read and write the variable x. protocols used in TreadMarks is beyond the scope of this paper. We refer the reader to Keleher's thesis <ref> [5] </ref> for more detail. We will compare the performance of various implementations of release consistency with each other and with sequential consistency in Section 9. 5 Multiple-Writer Protocols Most hardware cache and DSM systems such as IVY use single-writer protocols. <p> In this section, we briefly describe how communication and memory management are implemented by TreadMarks. For a more detailed discussion of the implementation, we refer the reader to Keleher's Ph.D. thesis <ref> [5] </ref>. By default, TreadMarks implements intermachine communication using UDP/IP through the Berkeley sockets interface. Since UDP/IP does not guarantee reliable delivery, TreadMarks uses light-weight, operation-specific, user-level protocols to insure message arrival. Every message sent by TreadMarks is either a request message or a response message. <p> The worst case occurs when every other word in the page is changed. In that case, making a diff takes 686 microseconds. 8 Applications A number of applications have been implemented using TreadMarks, and the performance of some benchmarks has been reported earlier <ref> [5] </ref>. Here we describe our experience with two large applications that were recently implemented using TreadMarks. These applications, mixed integer programming and genetic linkage analysis, were parallelized, starting from an existing efficient sequential code, by the authors of the sequential code with some help from the authors of this paper. <p> We share with this approach the programming model, but our implementation avoids expensive cache controller hardware. On the other hand, a hardware implementation can efficiently support applications with finer-grain parallelism. We have some limited experience with comparing the performance of hardware and software shared memory <ref> [5] </ref>. In particular, we compared the performance of four applications, including a slightly older version of ILINK, on an 8-processor SGI 4D/380 hardware shared memory multiprocessor and on TreadMarks running on our 8-processor ATM network of 18 DECStation-5000/240s. <p> In particular, Keleher has compared the performance of ten applications under lazy and eager release consistency, and found that for all but one (3-D Fast Fourier Transform) the lazy implementation performed better <ref> [5] </ref>. It was also shown that an invalidate protocol works better than an update protocol, because of the large amount of data resulting from the update protocol. Entry Consistency (Midway). Entry consistency is another relaxed memory model [2].
Reference: [6] <author> C. Koelbel, D. Loveman, R. Schreiber, G. Steele, Jr., and M. Zosel. </author> <title> The High Performance Fortran Handbook. </title> <publisher> The MIT Press, </publisher> <year> 1994. </year>
Reference-contexts: Implicit Parallelism (HPF). TreadMarks and PVM are both explicitly parallel programming methods: the programmer has to divide the computation among different threads and use either synchronization or message passing to control the interactions among the concurrent threads. With implicit parallelism, as in HPF <ref> [6] </ref>, the user writes a single-threaded program, which is then par-allelized by the compiler. In particular, HPF contains data distribution primitives, which may be used by the compiler to drive the parallelization process. This approach is suitable for data-parallel programs, such as Jacobi.
Reference: [7] <author> L. Lamport. </author> <title> How to make a multiprocessor computer that correctly executes multiprocess programs. </title> <journal> IEEE Transactions on Computers, </journal> <volume> C-28(9):690-691, </volume> <month> September </month> <year> 1979. </year> <month> 20 </month>
Reference-contexts: Unfortunately, the notion of "the last value written" is not well defined in a distributed system. A more precise notion is sequential consistency, whereby the memory appears to all processes as if they were executing on a single multiprogrammed processor <ref> [7] </ref>. With sequential consistency, the notion of "the last value written" is precisely defined. <p> The consistency model defines how the programmer can expect the memory system to behave. The first DSM system, IVY [9], implemented sequential consistency <ref> [7] </ref>. In this memory model, processes observe shared memory as if they were executing on a multiprogrammed uniprocessor (with a single memory).
Reference: [8] <author> D. Lenoski, J. Laudon, K. Gharachorloo, A. Gupta, and J. Hennessy. </author> <title> The directory-based cache coherence protocol for the DASH multiprocessor. </title> <booktitle> In Proceedings of the 17th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 148-159, </pages> <month> May </month> <year> 1990. </year>
Reference-contexts: An alternative approach to shared memory is to implement it in hardware, using a snooping bus protocol for a small number of processors or using a directory-based protocol for larger number of processors (e.g., <ref> [8] </ref>). We share with this approach the programming model, but our implementation avoids expensive cache controller hardware. On the other hand, a hardware implementation can efficiently support applications with finer-grain parallelism. We have some limited experience with comparing the performance of hardware and software shared memory [5].
Reference: [9] <author> K. Li and P. Hudak. </author> <title> Memory coherence in shared virtual memory systems. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 7(4) </volume> <pages> 321-359, </pages> <month> November </month> <year> 1989. </year>
Reference-contexts: In this paper, we discuss our experience with parallel computing on networks of workstations using the TreadMarks distributed shared memory (DSM) system. DSM allows processes to assume a globally shared virtual memory even though they execute on nodes that do not physically share memory <ref> [9] </ref>. Figure 1 illustrates a DSM system consisting of N networked workstations, each with its own memory, connected by a network. <p> The consistency model defines how the programmer can expect the memory system to behave. The first DSM system, IVY <ref> [9] </ref>, implemented sequential consistency [7]. In this memory model, processes observe shared memory as if they were executing on a multiprogrammed uniprocessor (with a single memory).
Reference: [10] <author> H. Lu. </author> <title> Message passing versus distributed shared memory on networks of workstations. </title> <type> Master's thesis, </type> <institution> Rice University, </institution> <month> April </month> <year> 1995. </year> <note> Appeared as Rice Technical Report RICE COMP-TR-250 and available by anonymous ftp from cs.rice.edu under public/TreadMarks/papers. </note>
Reference-contexts: For a more detailed comparison in programmability and performance between TreadMarks and PVM we refer the reader to Lu's M.S. thesis, which includes a comparison for nine different applications <ref> [10] </ref>. Implicit Parallelism (HPF). TreadMarks and PVM are both explicitly parallel programming methods: the programmer has to divide the computation among different threads and use either synchronization or message passing to control the interactions among the concurrent threads.
Reference: [11] <author> M. Stumm and S. Zhou. </author> <title> Algorithms implementing distributed shared memory. </title> <journal> IEEE Computer, </journal> <volume> 24(5) </volume> <pages> 54-64, </pages> <month> May </month> <year> 1990. </year>
Reference-contexts: Most DSM systems choose to replicate data, because this approach gives the best performance for a wide range of application parameters of interest <ref> [11] </ref>. With replicated data, the provision of memory consistency is at the heart of a DSM system: the DSM software must control replication in a manner that provides the abstraction of a single shared memory. The consistency model defines how the programmer can expect the memory system to behave.
Reference: [12] <author> V. Sunderam. </author> <title> PVM: A framework for parallel distributed computing. </title> <journal> Concurrency:Practice and Experience, </journal> <volume> 2(4) </volume> <pages> 315-339, </pages> <month> December </month> <year> 1990. </year> <month> 21 </month>
Reference-contexts: Currently, message passing is the prevailing programming paradigm for distributed memory systems. Parallel Virtual Machine (PVM) <ref> [12] </ref> is a popular software message passing package. It allows a heterogeneous network of computers to appear as a single concurrent computational engine. TreadMarks is currently restricted to a homogeneous set of nodes.
References-found: 12

