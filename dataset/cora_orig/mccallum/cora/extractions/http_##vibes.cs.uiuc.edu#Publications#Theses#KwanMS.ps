URL: http://vibes.cs.uiuc.edu/Publications/Theses/KwanMS.ps
Refering-URL: http://vibes.cs.uiuc.edu/Publications/Theses/theses.htm
Root-URL: http://www.cs.uiuc.edu
Title: PERFORMANCE EVALUATION OF THE THINKING MACHINES CM-5  
Author: BY THOMAS TAI YUNG KWAN 
Degree: 1987 THESIS Submitted in partial fulfillment of the requirements for the degree of Master of Science in Computer Science in the Graduate College of the  
Address: 1994 Urbana, Illinois  
Affiliation: B.S.E.E., University of Washington,  University of Illinois at Urbana-Champaign,  
Abstract-found: 0
Intro-found: 1
Reference: [1] <author> Almasi, G. S., and Gottlieb, A. </author> <title> Highly Parallel Computing. </title> <address> Benjamin/Cummings, </address> <year> 1989. </year>
Reference-contexts: Thus, the theoretical r 1 for a CM-5 computation node is 4 fi 2 fi 16, or 128 megaflops. Because, in practice, vectors have finite lengths, the measured r 1 will always be lower than the theoretical r 1 . As derived in <ref> [1] </ref>, n 1=2 = t where T o is the time to start and fill the vector pipeline and t is 1/r 1 , the theoretical average time needed to process one vector element. <p> Figure 8.3 illustrates the data layout on the disks. Array portions from each computation node are striped across the disks in units of 16 bytes, the size of the payload of a data network packet. In other words, the first data packet transfers data elements A <ref> [1] </ref> and A [2] from node zero to disk zero, the second data packet transfers data elements A [3] and A [4] from node zero to disk one, and so on.
Reference: [2] <author> Batcher, K. E. </author> <title> Design of a Massively Parallel Processor. </title> <journal> IEEE Transactions on Computers C-29, </journal> <month> 9 (Sept. </month> <year> 1980), </year> <pages> 836-842. </pages>
Reference-contexts: Figure 8.3 illustrates the data layout on the disks. Array portions from each computation node are striped across the disks in units of 16 bytes, the size of the payload of a data network packet. In other words, the first data packet transfers data elements A [1] and A <ref> [2] </ref> from node zero to disk zero, the second data packet transfers data elements A [3] and A [4] from node zero to disk one, and so on. <p> A hardware solution mandates either a fast network interface and sufficient interconnection network bandwidth for rapid reordering of data from its canonical storage format to the user data distribution, or a dedicated data turning network (e.g., as found in the Goodyear/NASA MPP <ref> [2] </ref>). Alternatively, rather than enforcing a fixed set of storage formats, exporting control of secondary storage data distributions to the application developers would allow them to manage input/output data distributions just as they do in high-performance Fortran [18]. This would obviate the need for system specific data reordering algorithms.
Reference: [3] <author> Best, M. </author> <type> Personal Communication. </type>
Reference-contexts: In other words, the first data packet transfers data elements A [1] and A [2] from node zero to disk zero, the second data packet transfers data elements A <ref> [3] </ref> and A [4] from node zero to disk one, and so on. File reads are the exact opposite of file writes; the array is first read from the disks into the New Mexico Order canonical layout illustrated in Figure 8.2. <p> The difference in the communication pattern required to convert the array from the user distribution to the New Mexico Order canonical layout accounts for the approximately 3 megabytes/second, 15 percent difference between the one-dimensional and two-dimensional cases <ref> [3] </ref>. 8.2.2 File Reads of the reading application increases from thirty-two to sixty-four nodes, the input data rate increases by about 20 megabytes/second. Thereafter, the data rate increases by roughly 10 megabytes/second as the partition size doubles. <p> input/output benchmarks and, where relevant, compare the results to those of x8.2. 8.3.1 Synchronous Broadcast although the file is conceptually read by one node, in reality, for a partition with n nodes reading B bytes, each node reads B=n bytes and then use the control network to broadcast the data <ref> [3] </ref>. In Figure 8.10, the maximum data rate is less than 1 megabyte/second for modest size arrays and quickly approaches an asymptote.
Reference: [4] <author> Best, M. L., Greenberg, A., Stanfill, C., and Tucker, L. W. </author> <title> CMMD I/O: A Parallel Unix I/O. </title> <booktitle> In Proceedings of the 1993 International Parallel Processing Symposium (1993), </booktitle> <pages> pp. 489-495. </pages>
Reference-contexts: In other words, the first data packet transfers data elements A [1] and A [2] from node zero to disk zero, the second data packet transfers data elements A [3] and A <ref> [4] </ref> from node zero to disk one, and so on. File reads are the exact opposite of file writes; the array is first read from the disks into the New Mexico Order canonical layout illustrated in Figure 8.2. <p> This is about 85 percent of the peak disk transfer rate. 8.3 C/CMMD Benchmarks Whereas the CM Fortran input/output interfaces allow data parallel programs to access data on the SDA, the CMMD input/output interfaces <ref> [30, 4] </ref> enable message passing programs to do the same. In this section, we present a detailed analysis of the CMMD input/output results. Like the CM Fortran utility library, the CMMD message passing library supports both hardware-dependent and hardware-independent data storage formats.
Reference: [5] <author> Bordawekar, R., Choudhary, A., and del Rosario, J. M. </author> <title> An Experimental Performance Evaluation of Touchstone Delta Concurrent File System. </title> <booktitle> In Proceedings of the International Conference on Supercomputing (July 1993), </booktitle> <pages> pp. 367-377. </pages>
Reference-contexts: This was not possible on the CM-2 unless users wrote files in the serial order format. On the CM-2, this limited the input/output performance to about 4.3 megabytes/second [11]. 4 The data for the Delta are from <ref> [5] </ref>. 67 However, to achieve the highest possible input/output performance on the CM-5, users must still use a configuration-dependent file format. 68 Chapter 9 Conclusions In chapters 4-7, we have evaluated the CM-5's computation and communication performance and the interaction of the two.
Reference: [6] <author> Bordawekar, R., del Rosario, J. M., and Choudhary, A. </author> <title> Design and Evaluation of Primitives for Parallel I/O. </title> <booktitle> In Proceedings of Supercomputing 1993 (Nov 1993), </booktitle> <pages> pp. 452-461. </pages>
Reference-contexts: Intel is redressing this differential with the Parallel File System (PFS) on the Intel Paragon XP/S [19]. PFS supports many of the same features as SFS, including disk striping, parity for fault tolerance, and distributed file caching. Recently, Bordawekar et al <ref> [6] </ref> proposed a two-phase access strategy to improve input/output performance on the Delta and other parallel file systems.
Reference: [7] <author> Bozkus, Z., Ranka, S., and Fox, G. </author> <title> Benchmarking the CM-5 Multicomputer. </title> <booktitle> In Proceedings of 4 th Symposium on the Frontiers of Massively Parallel Computation (Oct 1992), </booktitle> <pages> pp. 100-107. </pages>
Reference-contexts: However, not much is known about the achievable performance, the software overhead involved, and the interactions among the new features. Previous performance measurements by Bozkus et al <ref> [7] </ref>, Ponnusamy et al [26, 27] and Lin et al [22] exposed the performance limitations of an early version of the CM-5 communication library. Since then, the new communication library has been re-implemented using active messages [34] as the base layer.
Reference: [8] <author> Bradley, D. K. </author> <title> First and Second Generation Hypercube Performance. </title> <type> Master's thesis, </type> <institution> University of Illinois at Urbana-Champaign, Department of Computer Science, </institution> <month> Sept </month> <year> 1988. </year> <month> 72 </month>
Reference-contexts: The cooperative message passing benchmark and spatial communication locality benchmark were derived from work by Grunwald and Bradley <ref> [13, 8] </ref>. The original purpose of these two benchmarks was to measure the communication performance of distributed memory parallel systems, specifically the Intel iPSC family of hypercubes, for a variety of regular and irregular communication patterns.
Reference: [9] <author> Brewer, E. A., and Kuszmaul, B. C. </author> <title> How to Get Good Performance from the CM-5 Data Network. </title> <booktitle> In Proceedings of the 1994 International Parallel Processing Symposium (April 1994). </booktitle>
Reference-contexts: The effective data rate is limited not by the link bandwidth, but by the speed the SPARC processor can inject data into the data network [22]. The cost of receiving sixteen bytes of data (the payload of a data network packet) is about 60 cycles <ref> [9] </ref>. Because the SPARC processor has a 32 megahertz clock, the receiving bandwidth is at most 16fi32 60 = 8.5 megabytes/second. The second test, send-reply, is identical to simple send except the destination node sends a message back to the source node. <p> Here, the normalized bandwidth reflects the use of two communication links, one in each direction, rather than just one link, as is the case for the simple send and send-reply benchmarks. The overhead to send and receive a network packet is about 90 cycles <ref> [9] </ref>. Hence, the achievable bandwidth is limited to 16fi32 90 = 5.7 megabytes/second. Thus, for two active communication links, the upper bound is 5:7 fi 2, or 11.4 megabytes/second.
Reference: [10] <author> Dally, W. J. </author> <title> Performance Analysis of k-ary n-cube Interconnection Networks. </title> <journal> IEEE Transactions on Computers C-39, </journal> <month> 6 (June </month> <year> 1990), </year> <pages> 775-785. </pages>
Reference-contexts: Unlike the previous generation of MPP systems (e.g., the Intel's iPSC/860) which used hypercube topologies, the current generation favors low-dimensional interconnection networks (e.g., as in the Intel Paragon XP/S and the Cray T3D). This is directly influenced by the work of Dally <ref> [10] </ref>. Dally showed that under the assumption of constant wire bisection, low-dimensional networks with wide channels provide low latency, less contention, and higher hot-spot throughput than higher-dimensional networks with narrow channels.
Reference: [11] <author> Fineberg, S. A. </author> <title> Implementing the NHT-1 Application I/O Benchmark. </title> <booktitle> In Workshop on I/O in Parallel Computer Systems (April 1993). </booktitle>
Reference-contexts: This was not possible on the CM-2 unless users wrote files in the serial order format. On the CM-2, this limited the input/output performance to about 4.3 megabytes/second <ref> [11] </ref>. 4 The data for the Delta are from [5]. 67 However, to achieve the highest possible input/output performance on the CM-5, users must still use a configuration-dependent file format. 68 Chapter 9 Conclusions In chapters 4-7, we have evaluated the CM-5's computation and communication performance and the interaction of the
Reference: [12] <author> Fox, G., Johnson, M., Lyzenga, G., Otto, S., Salmon, J., and Walker, D. </author> <title> Solving Problems on Concurrent Processors, vol. I. </title> <publisher> Prentice-Hall, </publisher> <address> Englewood Cliffs, N.J., </address> <year> 1989. </year>
Reference-contexts: Because this is a test, no convergence checking occurred; the code executes for a fixed number of iterations. Although there are many potential definitions of system balance, the commonly accepted definition is that a system is balanced if it is neither computation nor communication limited <ref> [15, 12] </ref>.
Reference: [13] <author> Grunwald, D. C., and Reed, D. A. </author> <title> Benchmarking Hypercube Hardware and Software. In Hpercube Multiprocessors (1987), </title> <editor> M. T. Heath, Ed., </editor> <booktitle> Society for Industrial and Applied Mathematics, </booktitle> <pages> pp. 169-177. </pages>
Reference-contexts: The cooperative message passing benchmark and spatial communication locality benchmark were derived from work by Grunwald and Bradley <ref> [13, 8] </ref>. The original purpose of these two benchmarks was to measure the communication performance of distributed memory parallel systems, specifically the Intel iPSC family of hypercubes, for a variety of regular and irregular communication patterns. <p> To provide a rough performance comparison between the combine subnetwork and the data network, we used the n-way broadcast benchmark derived from <ref> [13] </ref>.
Reference: [14] <author> Heidelberger, P., and Lavenberg, S. S. </author> <title> Computer Performance Evaluation Methodology. </title> <journal> IEEE Transactions on Computers C-33, </journal> <month> 12 (Dec </month> <year> 1984), </year> <pages> 1195-1220. </pages>
Reference-contexts: There are three major approaches to performance evaluation of computer systems: analytic performance modeling, simulation, and performance measurement <ref> [14] </ref>. Below, we briefly describe the advantages and disadvantages of each approach. Analytic performance modeling makes use of mathematical techniques to estimate the performance of computer systems.
Reference: [15] <author> Helin, J. </author> <title> Performance Analysis of the CM-2, a Massively Parallel SIMD Computer. </title> <booktitle> In Proceedings of the International Conference on Supercomputing (July 1992). </booktitle>
Reference-contexts: Thus, although the peak vector processing capability of the CM-5 computation node is approaching that of the traditional vector processor, the measured performance of the CM-5 computation node still trails that of the Cray Y-MP processor. 1 The data for the Cray Y-MP are from <ref> [15] </ref>. 27 Chapter 5 Data Network Benchmarks To explore the characteristics of the CM-5 fat tree network, we identified five sets of benchmarks to measure specific aspects of data network performance: cooperative message passing, virtual channels, active messages, circular shifts, and spatial communication locality. <p> Because this is a test, no convergence checking occurred; the code executes for a fixed number of iterations. Although there are many potential definitions of system balance, the commonly accepted definition is that a system is balanced if it is neither computation nor communication limited <ref> [15, 12] </ref>.
Reference: [16] <author> Hillis, W. D., and Boghosian, B. M. </author> <title> Parallel Scientific Computation. </title> <booktitle> Science 261 (Aug 1993), </booktitle> <pages> 856-863. </pages>
Reference-contexts: Thus, the performance of our Laplace code, about 13 percent of the theoretical peak rate, is not only within the reasonable range achievable by real applications but actually quite close to the group average. 2 The NCSA data are from <ref> [16] </ref>. The average gigaflops for a code is estimated by hand-counting the total number of floating point operations and dividing that number by the execution time for that code. 49 Chapter 8 Input/Output Benchmarks For many MPP systems, input/output is now a major performance bottleneck.
Reference: [17] <author> Hockney, W., and Jesshope, C. R. </author> <title> Parallel Computers. </title> <publisher> Adam Hilger Ltd, </publisher> <year> 1981. </year>
Reference-contexts: Thus, local maxima occur when the vector size is a multiple of the node's effective vector register length. asymptotic processing rate r 1 and the vector length n 1=2 needed to achieve one half of r 1 <ref> [17] </ref>. Theoretically, r 1 = cR where c is the number of operations that can be chained together and R the processor clock rate. For example, the Cray Y-MP processor has a 6 nanosecond cycle time (i.e., a 166.7 megahertz clock) and can chain two floating point operations together.
Reference: [18] <author> HPFF. </author> <title> High-Performance Fortran Language Specfication, version 1.0. </title> <type> Tech. rep., </type> <institution> High Performance Fortran Forum, </institution> <month> May </month> <year> 1993. </year>
Reference-contexts: Alternatively, rather than enforcing a fixed set of storage formats, exporting control of secondary storage data distributions to the application developers would allow them to manage input/output data distributions just as they do in high-performance Fortran <ref> [18] </ref>. This would obviate the need for system specific data reordering algorithms. Similarly, user control of file 70 caching and prefetching algorithms would allow application developers to further reduce the input/output time by exploiting the input/output access patterns of their codes.
Reference: [19] <institution> Intel Supercomputer Systems Division. </institution> <note> Paragon User's Guide, </note> <month> Oct </month> <year> 1993. </year>
Reference-contexts: However, the data rates for the read and write operations of the CM-5 are nearly any order of magnitude faster than the Delta. Intel is redressing this differential with the Parallel File System (PFS) on the Intel Paragon XP/S <ref> [19] </ref>. PFS supports many of the same features as SFS, including disk striping, parity for fault tolerance, and distributed file caching. Recently, Bordawekar et al [6] proposed a two-phase access strategy to improve input/output performance on the Delta and other parallel file systems.
Reference: [20] <author> Leiserson, C. E. Fat-trees: </author> <title> Universal Networks for Hardware-Efficient Supercomputing. </title> <journal> IEEE Transactions on Computers C-34, </journal> <month> 10 (Oct </month> <year> 1985), </year> <pages> 892-901. 73 </pages>
Reference-contexts: The diagnostic network supports system diagnostics and is not directly accessible by users. Hence, it will not be considered here. The data network, which is responsible for application data motion, connects the nodes in a fat tree topology <ref> [21, 20] </ref>. A fat tree is a routing network based on a complete binary tree. Leaves, edges, and internal nodes of the tree correspond to processors, channels, and routers, respectively. Unlike an ordinary binary tree, the channel capacities of a fat-tree increases as we go up the tree.
Reference: [21] <author> Leiserson, C. E., Abuhamdeh, Z. S., Douglas, D. C., Feynman, C. R., Gan--mukhi, M. N., Hill, J. V., Hillis, W. D., Kuszmaul, B. C., St. Pierre, M. A., Wells, D. S., Wong, M. C., Yang, S., and Zak, R. </author> <title> The Network Architecture of the Connection Machine CM-5. </title> <booktitle> In Proceedings of Parallel Algorithms and Architectures Symposium (May 1992), </booktitle> <pages> pp. 272-285. </pages>
Reference-contexts: Then we describe the test configurations used in our benchmarks. 2.1 Architecture The CM-5 is a MIMD, distributed memory system composed of interconnected computation nodes, input/output nodes, and control processors. The nodes are interconnected by a fat tree data network and a binary tree control network <ref> [21] </ref>. Below, we describe each of these architectural components in detail. 2.1.1 Interconnection Network The CM-5 interconnection network consists of three separate networks: a data network, a control network, and a diagnostic network. The diagnostic network supports system diagnostics and is not directly accessible by users. <p> The diagnostic network supports system diagnostics and is not directly accessible by users. Hence, it will not be considered here. The data network, which is responsible for application data motion, connects the nodes in a fat tree topology <ref> [21, 20] </ref>. A fat tree is a routing network based on a complete binary tree. Leaves, edges, and internal nodes of the tree correspond to processors, channels, and routers, respectively. Unlike an ordinary binary tree, the channel capacities of a fat-tree increases as we go up the tree.
Reference: [22] <author> Lin, M. J., Tsang, R., Du, D. H. C., Klietz, A. E., and Saroff, S. </author> <title> Performance Evaluation of the CM-5 Interconnection Network. </title> <booktitle> In Proceedings of Spring COMPCON 93 (Feb 1993), </booktitle> <pages> pp. 189-198. </pages>
Reference-contexts: However, not much is known about the achievable performance, the software overhead involved, and the interactions among the new features. Previous performance measurements by Bozkus et al [7], Ponnusamy et al [26, 27] and Lin et al <ref> [22] </ref> exposed the performance limitations of an early version of the CM-5 communication library. Since then, the new communication library has been re-implemented using active messages [34] as the base layer. <p> However, Table 5.3 shows that, for simple send, the observed data rate is only 8.3 megabytes/second. The effective data rate is limited not by the link bandwidth, but by the speed the SPARC processor can inject data into the data network <ref> [22] </ref>. The cost of receiving sixteen bytes of data (the payload of a data network packet) is about 60 cycles [9]. Because the SPARC processor has a 32 megahertz clock, the receiving bandwidth is at most 16fi32 60 = 8.5 megabytes/second. <p> Because the communication bandwidth of the data network is modest, optimizing communication remains important on the CM-5. As described in chapter 5, the communication bandwidth is limited by the SPARC processor; specifically, the limitation is the memory management unit cache-line load and store time <ref> [22] </ref>. Small improvements in communication bandwidth are possible by upgrading the 32 megahertz SPARC to a processor with a faster clock rate (e.g., a 50 megahertz TI Viking SPARC processor). However, in general, to fully exploit the network bandwidth, it may be necessary to introduce direct memory access (DMA).
Reference: [23] <author> LoVerso, S. J., Isman, M., Nanopoulos, A., Nesheim, W., Milne, E. D., and Wheeler, R. sfs: </author> <title> A Parallel File System for the CM-5. </title> <booktitle> In Proceedings of the 1993 Summer Usenix Conference (1993), </booktitle> <pages> pp. 291-305. </pages>
Reference-contexts: Since then, the new communication library has been re-implemented using active messages [34] as the base layer. Thus, the first goal of this thesis is to re-evaluate the CM-5's computation and communication performance and the interaction of the two. Preliminary CM-5 measurements by LoVerso et al <ref> [23] </ref> explored the input/output data rate achievable via operating system calls. However, the CM-5 input/output library includes a rich set of file organization and access methods; these methods are those of most interest to application developers. <p> The raw data transfer rate of each SCSI channel is 10 megabytes/second; the raw read and write data rates of a disk are, respectively, 2 megabytes/second and 1.8 megabytes/second <ref> [23] </ref>. The tape storage and CM5-HIPPI input/output nodes will not be considered here. The CM-5's Scalable Disk Array (SDA) is the union of the small disk arrays on the CM-5 input/output nodes. <p> Because file data is transferred to and from the SDA using the data network, this sixteen byte 13 increment was chosen to match the size of the payload of a data network packet. The Scalable File System (SFS) <ref> [31, 23] </ref> logically sits atop the SDA and arbitrates input/output requests from applications. <p> For small transfer sizes (i.e., small arrays), the software overhead to retrieve file block information from the file system when the operation is initiated and then to update the file state at the end of the input/output operation dominates the access cost <ref> [23] </ref>. The data rates do increase with the partition size. With a larger partition, the data in the array is distributed across more nodes, and more network bandwidth is available for data transmission from the computation nodes to the disk storage nodes. <p> Thinking Machines is currently testing a software patch to be included in the CMOST 7.3 final release. 3 Although the hardware configurations used by <ref> [23] </ref> differ from those in the our study, they provide enough data to bound the expected data rates for the NCSA CM-5 configuration. 66 Parameter TMC CM-5 Intel Delta Number of processing nodes 512 512 Number of disk I/O nodes 15 32 Number of disks (per file partition) 57 64 Number <p> In summary, possible improvements to the CM-5 SFS include lowering the latency required for input/output of smaller arrays by moving part of the file system code into the operating system kernel of the computation nodes <ref> [23] </ref>, and avoiding the use of the control network when handling large data requests for the CMMD synchronous read operation. More generally, we believe that SFS and other parallel file systems should allow users greater control over data placement and caching policies.
Reference: [24] <author> Padua, D. A., and Wolfe, M. J. </author> <title> Advanced Compiler Optimization for Supercomputers. </title> <journal> Communications of the ACM 29, </journal> <month> 12 (Dec. </month> <year> 1986), </year> <pages> 1184-1201. </pages>
Reference-contexts: Although compiler technology for traditional vector processors has steadily improved over the past fifteen years, and we now have a better understanding of how to use various transformations (e.g., loop 4 fusion or loop interchange) to parallelize loops <ref> [24] </ref>, effectively managing the data distribution and data locality on physically distributed memory systems is still an open problem. In addition to compiler issues, many operating system resource management problems that have been solved for uniprocessor systems must now be reworked in the context of MPP systems.
Reference: [25] <author> Patterson, D. A., Gibson, G., and Katz, R. H. </author> <title> A Case for Redundant Arrays of Inexpensive Disks (RAID). </title> <booktitle> In ACM SIGMOD Conference (June 1988), </booktitle> <pages> pp. 109-116. </pages>
Reference-contexts: Thus, both disk input/output and networking input/output are now major performance bottlenecks. Scientific codes executing on large MPP configurations can generate tens of gigabytes of data during a single execution. It is now necessary to exploit inexpensive, high capacity disks by constructing disk arrays <ref> [25] </ref> to increase the disk input/output bandwidth. However, exploiting disk arrays alone is not sufficient to resolve the input/output bottleneck. In a multi-programming environment, there will be many users simultaneously accessing the disk file system.
Reference: [26] <author> Ponnusamy, R., Choudhary, A., and Fox, G. </author> <title> Communication Overhead on CM5: An Experimental Performance Evaluation. </title> <booktitle> In Proceedings of 4 th Symposium on the Frontiers of Massively Parallel Computation (Oct 1992), </booktitle> <pages> pp. 108-115. </pages>
Reference-contexts: However, not much is known about the achievable performance, the software overhead involved, and the interactions among the new features. Previous performance measurements by Bozkus et al [7], Ponnusamy et al <ref> [26, 27] </ref> and Lin et al [22] exposed the performance limitations of an early version of the CM-5 communication library. Since then, the new communication library has been re-implemented using active messages [34] as the base layer.
Reference: [27] <author> Ponnusamy, R., Thakur, R., Choudhary, A., and Fox, G. </author> <title> Scheduling Regular and Irregular Communication Patterns on the CM-5. </title> <booktitle> In Proceedings of Supercomputing 1992 (Nov 1992), </booktitle> <pages> pp. 394-402. </pages>
Reference-contexts: However, not much is known about the achievable performance, the software overhead involved, and the interactions among the new features. Previous performance measurements by Bozkus et al [7], Ponnusamy et al <ref> [26, 27] </ref> and Lin et al [22] exposed the performance limitations of an early version of the CM-5 communication library. Since then, the new communication library has been re-implemented using active messages [34] as the base layer.
Reference: [28] <author> Seitz, C. L., et al. </author> <title> The Hypercube Communications Chip. Display file 5182:df:85, </title> <institution> California Institute of Technology, Department of Computer Science, </institution> <month> March </month> <year> 1985. </year>
Reference-contexts: These two interfaces use disjoint links of the data network. Thus, for a leaf node, the unidirectional bandwidth (either in or out) is 40/2, or 20 megabytes/second. Messages are transmitted in a manner similar to wormhole routing <ref> [28] </ref>. In wormhole routing, a packet is divided into a number of flits (flow control digits) for data transmission. As the header flit moves through the network, the remaining flits follow in a pipeline fashion.
Reference: [29] <institution> Thinking Machines Corporation. </institution> <address> CM-5: </address> <booktitle> Programming the Connection Machine System, </booktitle> <month> Dec </month> <year> 1992. </year>
Reference-contexts: Both parallel memory and serial memory use the same memory banks, though. The vector units have a set of internal scalar registers, vector registers, and arithmetic-logic units <ref> [29] </ref>. The arithmetic-logic units have a clock rate of 32 megahertz. However, because two vector units share an arithmetic-logic unit, the clock rate of each vector unit is 16 megahertz, same as the clock rate of the memory module.
Reference: [30] <institution> Thinking Machines Corporation. </institution> <note> CMMD Reference Manual, Version 3.0 Beta, </note> <month> Dec </month> <year> 1992. </year>
Reference-contexts: The generic format allows a code to re-read a file without the partition size restriction. Finally, the serial order format allows other codes and workstations to access the data. 2.2.2 CMMD Communication Library The CMMD communication library <ref> [30] </ref> provides a set of message passing routines for programs written using the message passing programming model. <p> Active message primitives are part of the CMMD active message layer (CMAML), a protocol-less transport layer, upon which the higher level CMMD functions are built <ref> [30] </ref>. There are three tests in this set of active message benchmarks. The first test, active request, measures the latency to send a request active message using the CMAML request primitive; this primitive transmits data using the left data network interface. <p> As 34 shown in Table 5.5, the latency for active messages is an order of magnitude lower than virtual channels. Because active messages are never buffered on arrival <ref> [30] </ref>, the overhead required to do buffering is eliminated. <p> This is about 85 percent of the peak disk transfer rate. 8.3 C/CMMD Benchmarks Whereas the CM Fortran input/output interfaces allow data parallel programs to access data on the SDA, the CMMD input/output interfaces <ref> [30, 4] </ref> enable message passing programs to do the same. In this section, we present a detailed analysis of the CMMD input/output results. Like the CM Fortran utility library, the CMMD message passing library supports both hardware-dependent and hardware-independent data storage formats.
Reference: [31] <institution> Thinking Machines Corporation. </institution> <note> Connection Machine CM-5 Technical Summary, Revised Edition, </note> <month> Nov </month> <year> 1992. </year>
Reference-contexts: Because file data is transferred to and from the SDA using the data network, this sixteen byte 13 increment was chosen to match the size of the payload of a data network packet. The Scalable File System (SFS) <ref> [31, 23] </ref> logically sits atop the SDA and arbitrates input/output requests from applications.
Reference: [32] <institution> Thinking Machines Corporation. </institution> <note> CM-5 I/O System Programming Guide, Version 7.2, </note> <month> Sept </month> <year> 1993. </year>
Reference-contexts: In this section, we describe the special libraries used in our benchmarks: the CM Fortran utility library and the CMMD communication library. 2.2.1 CM Fortran Utility Library The CM Fortran utility library <ref> [33, 32] </ref> supports parallel input/output for data parallel programs. It provides users with the capability to read and write arrays that are distributed across the CM-5 node memories.
Reference: [33] <author> Thinking Machines Corporation. </author> <title> CM Fortran Utility Library Reference Manual, Version 2.0 Beta, </title> <month> Jan </month> <year> 1993. </year> <note> [34] von Eicken, </note> <author> T., Culler, D., Goldstein, S., and Schauser, K. </author> <title> Active Messages: a Mechanism for Integrated Communication and Computation. </title> <booktitle> In Proceedings of the 19 th International Symposium on Computer Architecture (May 1992), </booktitle> <pages> pp. 256-266. 75 </pages>
Reference-contexts: In this section, we describe the special libraries used in our benchmarks: the CM Fortran utility library and the CMMD communication library. 2.2.1 CM Fortran Utility Library The CM Fortran utility library <ref> [33, 32] </ref> supports parallel input/output for data parallel programs. It provides users with the capability to read and write arrays that are distributed across the CM-5 node memories.
References-found: 33

