URL: http://www.lsi.upc.es/~talavera/papers/iberamia98-apisaac.ps.gz
Refering-URL: http://www-lsi.upc.es/~talavera/papers.html
Root-URL: 
Email: talavera@lsi.upc.es  
Title: Exploring efficient attribute prediction in hierarchical clustering  
Author: Luis Talavera 
Address: Campus Nord, Modul C6, Jordi Girona 1-3 08034 Barcelona, Catalonia, Spain  
Affiliation: Departament de Llenguatges i Sistemes Informatics Universitat Politecnica de Catalunya  
Abstract: This work explores the feasibility of constructing hierarchical clusterings minimizing the expected cost of exploiting these clusterings for a prediction task. Particularly, we focus on gaining efficiency by means of reducing the number of features used to describe each node in the hierarchy. To explore a number of different hierarchical clusterings we use the Isaac clustering system, which can select subsets of features at each node of the hierarchy and also, allows to bias the clustering process in order to create different type of clusterings. Experiments show evidence that it is possible to construct hierarchies with a significant improvement in efficiency without degrading predictive accuracy. Results also show that in order to achieve higher improvements, some accuracy must be sacrificed.
Abstract-found: 1
Intro-found: 1
Reference: 1. <author> M. Devaney and A. Ram. </author> <title> Efficient feature selection in conceptual clustering. </title> <booktitle> In Machine Learning: Proceedings of the Fourteenth International Conference, </booktitle> <address> Nashville, TN, </address> <year> 1997. </year>
Reference-contexts: In addition, most of the work has focused on incremental clustering and, particularly, on Fisher's COBWEB. Gennari [4] investigated the benefits of incorporating an attentional mechanism into COBWEB from a viewpoint of efficiency gaining, similarly to this work. Recently, Devaney and Ram <ref> [1] </ref> propose an attribute-incremental concept creator which has the capability of efficiently deal with the feature selection problem. However, these works evaluate clusterings by measuring the accuracy of predicting the class labels. As regards to previous work, there are two relevant aspects of our work.
Reference: 2. <author> U. M. Fayyad, G. Piatetsky-Shapiro, and P. Smyth. </author> <title> From data mining to knowledge discovery: An overview. </title> <editor> In U. M. Fayyad, G. Piatetsky-Shapiro, P. Smyth, and R. Uthurusamy, editors, </editor> <booktitle> Advances in Knowledge Discovery and Data Mining, </booktitle> <pages> pages 1-34. </pages> <publisher> AAAI Press, </publisher> <address> Cambridge, MA, </address> <year> 1996. </year>
Reference-contexts: 1 Introduction The selection of relevant features is a central problem in inductive learning. Learning algorithms tend to degrade in performance when applied to domains with many irrelevant features [5, 7]. The problem is especially severe when dealing with large databases such as those used in data mining tasks <ref> [2] </ref>. Learning in these environments often requires systems to be able of scaling to large numbers of irrelevant features, since usually there are no experts to select a set of useful features.
Reference: 3. <author> D. H. Fisher. </author> <title> Knowledge acquisition via incremental conceptual clustering. </title> <journal> Machine Learning, </journal> (2):139-172, 1987. 
Reference-contexts: This sort of representation provides greater flexibility than the logical ones, allowing graded concept descriptions to be created. Probabilistic representations are present in a number of conceptual clustering systems, COBWEB <ref> [3] </ref> being the most representative example. Isaac allows the user to specify the structure of the cluster hierarchies via the N G parameter. A set of N G values may be given to the system that creates a level for each given value. <p> Compared efficiency between using all features and subsets. instead of selecting a global subset of features. And secondly, we evaluate the results with regard to prediction accuracy over all attributes, which appears to be a more adequate and widely accepted performance measure for hierarchical clusterings than class label prediction <ref> [3] </ref>. 6 Conclusions and Future Work The presented framework is one of the few existing approaches to cope with the problem of feature selection in hierarchical clustering.
Reference: 4. <author> J. H. Gennari. </author> <title> Focused concept formation. </title> <booktitle> In Proceedings of the Fifth International Workshop on Machine Learning, </booktitle> <pages> pages 379-382. </pages> <publisher> Morgan Kauffmann, </publisher> <year> 1989. </year>
Reference-contexts: In addition, most of the work has focused on incremental clustering and, particularly, on Fisher's COBWEB. Gennari <ref> [4] </ref> investigated the benefits of incorporating an attentional mechanism into COBWEB from a viewpoint of efficiency gaining, similarly to this work. Recently, Devaney and Ram [1] propose an attribute-incremental concept creator which has the capability of efficiently deal with the feature selection problem.
Reference: 5. <author> G. H. John, R. Kohavi, and K. Pfleger. </author> <title> Irrelevant features and the subset selection problem. </title> <booktitle> In Proceedings of the Eleventh International Conference on Machine Learning, </booktitle> <pages> pages 121-129. </pages> <publisher> Morgan Kauffmann, </publisher> <address> San Mateo, CA, </address> <year> 1994. </year>
Reference-contexts: 1 Introduction The selection of relevant features is a central problem in inductive learning. Learning algorithms tend to degrade in performance when applied to domains with many irrelevant features <ref> [5, 7] </ref>. The problem is especially severe when dealing with large databases such as those used in data mining tasks [2]. <p> A search strategy must also be determined in order to apply the method. We chose a best-first search to assure a reasonable approximation to the optimal solution. Some implementations of the wrapper method have employed this strategy with good results <ref> [5, 6] </ref>. The best--first strategy repeatedly expands the highest performing state but does not conclude as soon as it cannot find a better state by expanding the current state.
Reference: 6. <author> R. Kohavi and G. H. John. </author> <title> Automatic parameter selection by minimizing estimated error. </title> <booktitle> In Machine Learning: Proceedings of the Twelfth International Conference, </booktitle> <address> San Francisco, 1995. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: An heuristic parameter search procedure appears to be a reasonable solution to this problem. For this purpose, we will introduce an automatic parameter setting version of Isaac which will be called APIsaac, that uses the well-known wrapper method <ref> [6] </ref>. The wrapper method takes into account the properties of the training data set and their interaction with the system biases when searching for parameter values. In this approach the inductive learning system itself is used as part of the evaluation function. <p> A search strategy must also be determined in order to apply the method. We chose a best-first search to assure a reasonable approximation to the optimal solution. Some implementations of the wrapper method have employed this strategy with good results <ref> [5, 6] </ref>. The best--first strategy repeatedly expands the highest performing state but does not conclude as soon as it cannot find a better state by expanding the current state.
Reference: 7. <author> P. Langley. </author> <title> Selection of relevant features in machine learning. </title> <booktitle> In Proceeding of the AAAI Fall Symposium on Relevance, </booktitle> <address> New Orleans, LA, 1994. </address> <publisher> AAAI Press. </publisher>
Reference-contexts: 1 Introduction The selection of relevant features is a central problem in inductive learning. Learning algorithms tend to degrade in performance when applied to domains with many irrelevant features <ref> [5, 7] </ref>. The problem is especially severe when dealing with large databases such as those used in data mining tasks [2].
Reference: 8. <author> R. Lopez de Mantaras. </author> <title> A distance based attribute selection measure for decision tree induction. </title> <journal> Machine Learning, </journal> (6):81-92, 1991. 
Reference-contexts: It incorporates an attentional mechanism that computes the relevance of each attribute using a relevance measure formerly used in attribute selection in decision trees, called the distance measure <ref> [8] </ref>, and the current set of clusters. The attributes which do not score high enough are declared irrelevant and are discarded. Isaac assumes that in more general levels the set of attributes necessary to discriminate among the clusters created is smaller than in more specific levels.
Reference: 9. <author> E. E. Smith and D. L. Medin. </author> <title> Categories and concepts. </title> <publisher> Harvard University Press, </publisher> <address> Cambridge,MA, </address> <year> 1981. </year>
Reference-contexts: Finally, we compare our research with previous work and conclude with plans for some future work. 2 ISAAC: A Hierarchical Clustering System Isaac [10] is a hierarchical clustering system that organizes clusters in probabilistic concept hierarchies. A probabilistic concept <ref> [9] </ref> is represented as a summary description that lists, for each attribute value, the conditional probability that this value occurs in an instance of a cluster. This sort of representation provides greater flexibility than the logical ones, allowing graded concept descriptions to be created.
Reference: 10. <author> L. Talavera and U. Cortes. Generalizacion y atencion selectiva para la formacion de conceptos. </author> <booktitle> In V Congreso Iberoamericano de Inteligencia Artificial, IBERAMIA96, </booktitle> <pages> pages 320-330, </pages> <address> Cholula, Puebla, Mexico, 1996. </address> <publisher> Limusa, Mexico. </publisher>
Reference-contexts: The effect of removing a feature may be beneficial for predicting some features, but also may degrade performance in some other cases, causing average prediction accuracy to decrease. In this work, we investigate this problem using the Isaac clustering system <ref> [10] </ref>. Isaac fits well into this task since it incorporates an attentional mechanism which selects subsets of features at each node of the hierarchy. <p> In the experiments, we use this extension to search for accurate clusterings under different conditions to test whether we may preserve accuracy while gaining in efficiency. Finally, we compare our research with previous work and conclude with plans for some future work. 2 ISAAC: A Hierarchical Clustering System Isaac <ref> [10] </ref> is a hierarchical clustering system that organizes clusters in probabilistic concept hierarchies. A probabilistic concept [9] is represented as a summary description that lists, for each attribute value, the conditional probability that this value occurs in an instance of a cluster. <p> The Refinement stage follows an agglomerative strategy. Starting with the partition created in the Preprocessing stage, clusters are continuously merged until achieving a new level which reflects the generality given by the current N G value (more details on the algorithm can be found in <ref> [10] </ref>). As opposed to traditional clustering algorithms, intermediate mergings are not stored, and the final number of levels corresponds to the number of N G values provided by the user.
References-found: 10

