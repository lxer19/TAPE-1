URL: http://www.cs.monash.edu.au/~jono/TechReports/evid4.ps
Refering-URL: http://www.cs.monash.edu.au/~jono/
Root-URL: 
Email: jono@cs.monash.edu.au Catherine.Forbes@BusEco.monash.edu.au  
Title: Bayesian Approaches to Segmenting a Simple Time Series Keywords: Segmentation, Minimum Message Length, MML, Bayes
Author: Jonathan J. Oliver, Catherine S. Forbes, 
Date: January 11, 1998  
Address: Clayton, 3168 Australia Clayton, 3168 Australia  
Affiliation: Dept. of Computer Science Dept. Econometrics Monash University, Monash University,  
Abstract: The segmentation problem arises in many applications in data mining, A.I. and statistics. In this paper, we consider segmenting simple time series. We develop two Bayesian approaches for segmenting a time series, namely the Bayes Factor approach, and the Minimum Message Length (MML) approach. We perform simulations comparing these Bayesian approaches, and then perform a comparison with other classical approaches, namely AIC, MDL and BIC. We conclude that the MML criterion is the preferred criterion. We then apply the segmentation method to financial time series data. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> H. Akaike. </author> <title> Information theory and an extension of the maximum likelihood principle. In B.N. </title> <editor> Petrov and F. Csaki, editors, </editor> <booktitle> Proceedings of the 2nd International Symposium on Information Theory, </booktitle> <pages> pages 267-281, </pages> <year> 1973. </year>
Reference-contexts: A variety of criteria have been used for determining the number of segments in data, including: * Tong [24, Section 7.2.7] used AIC <ref> [1] </ref> for the segmentation of a TAR model; Liang [11] used AIC for image segmentation. * Koop and Potter [9] use Bayes factors to compare the hypothesis of a linear model (one segment) with a single alternative of either a threshold autoregressive (TAR) model with a prespecified number of segments or
Reference: [2] <author> J. Albert and S. Chib. </author> <title> Bayesian Inference via Gibbs sampling of autoregressive time series subject to Markov mean and variance shifts. </title> <journal> Journal of Business and Economic Statistics, </journal> <volume> 11, </volume> <pages> pages 1-15, </pages> <year> 1993. </year>
Reference-contexts: to compare the hypothesis of a linear model (one segment) with a single alternative of either a threshold autoregressive (TAR) model with a prespecified number of segments or a Markov Trend (or a so-called 'Hamilton') model where the number of segments is estimated using the method of Albert and Chib <ref> [2] </ref>. * Li [10] used MDL [19] (equivalent to BIC [21]) for image segmentation; Dom [7], Pfahringer [17] and Quinlan [18] refined the criterion within the context of the segmentation of binary strings. * Baxter and Oliver [3] used MML [25, 27] (a Bayesian method for point estimation) for the segmentation <p> The 1 prior distribution is normalised to be in the range <ref> [0:01; 2] </ref>.
Reference: [3] <author> R.A. Baxter and J.J. Oliver. </author> <title> The kindest cut: minimum message length segmentation. </title> <editor> In S. Arikawa and A. Sharma, editors, </editor> <booktitle> Lecture Notes in Artificial Intelligence 1160, Algorithmic Learning Theory, ALT-96, </booktitle> <pages> pages 83-90. </pages> <publisher> Springer-Verlag, </publisher> <address> Berlin, </address> <year> 1996. </year>
Reference-contexts: the number of segments is estimated using the method of Albert and Chib [2]. * Li [10] used MDL [19] (equivalent to BIC [21]) for image segmentation; Dom [7], Pfahringer [17] and Quinlan [18] refined the criterion within the context of the segmentation of binary strings. * Baxter and Oliver <ref> [3] </ref> used MML [25, 27] (a Bayesian method for point estimation) for the segmentation of line segments with Gaussian noise. In this paper we concentrate on the Bayesian approaches to segmentation (Bayes factors and MML). We develop the Bayesian approaches for segmentation, and highlight differences between them.
Reference: [4] <author> J.O. Berger. </author> <title> Statistical Decision Theory and Bayesian analysis. </title> <publisher> Springer-Verlag, </publisher> <year> 1993. </year>
Reference-contexts: this prior distribution is scale invariant (i.e., when using Bayesian methods such as Bayes factors or MML we obtain equivalent results independent of whether we measure our time series in dollars or cents). 3.1.4 Prior #3 We assume that each j is a priori distributed according to a Gamma distribution <ref> [4, p. 560] </ref> j ~ (ff; fi). Again, we assume that we have rough knowledge about the average difference between data points and the average variability in this difference. A convenient way to make this prior scale invariant is to set fi = y .
Reference: [5] <author> P. Cheeseman. </author> <type> Personal communication, </type> <year> 1993. </year> <month> 18 </month>
Reference-contexts: the message length is approximately M essLen (y&v&) = log P rob (y&v&) + d + C (11) 3.5 Why is the MML Different from the Bayes Factor Approach? A number of authors who advocate the Bayes factors (or Evidence) approach have suggested that MDL is equivalent to Bayes factors <ref> [5, 12] </ref>, and that MML is an approximation to Bayes factors [13]: 1 By coding theory, we can encode an event of probability P in a message of length log P nits. A nit is the unit of message length when we take logarithms to the base e.
Reference: [6] <author> J.H. Conway and N.J.A Sloane. </author> <title> Sphere Packings, Lattices and Groups. </title> <publisher> Springer-Verlag, </publisher> <address> London, </address> <year> 1988. </year>
Reference-contexts: point is 0 is of size: V ol ( 0 ) = d d q where d is the dimension of , det (F ( 0 )) is the determinant of the Fisher Information matrix, and d is the d dimensional optimal quantizing lattice constant (given in Conway and Sloane <ref> [6] </ref>). The volumes (V ol ( 0 )) will vary around the parameter space. For example, that part of the parameter space with low j will have a smaller regions than that part with high j (see Section 4.2 of [14]).
Reference: [7] <author> B. Dom. </author> <title> MDL estimation with Small Sample Sizes including an application to the problem of segmenting binary strings using bernoulli models. </title> <type> Technical Report RJ 9997 (89085) 12/15/95, </type> <institution> IBM Research Division, Almaden Research Center, 650 Harry Rd, </institution> <address> San Jose, CA, 95120-6099, </address> <year> 1995. </year>
Reference-contexts: either a threshold autoregressive (TAR) model with a prespecified number of segments or a Markov Trend (or a so-called 'Hamilton') model where the number of segments is estimated using the method of Albert and Chib [2]. * Li [10] used MDL [19] (equivalent to BIC [21]) for image segmentation; Dom <ref> [7] </ref>, Pfahringer [17] and Quinlan [18] refined the criterion within the context of the segmentation of binary strings. * Baxter and Oliver [3] used MML [25, 27] (a Bayesian method for point estimation) for the segmentation of line segments with Gaussian noise.
Reference: [8] <author> R.E. Kass and A.E. Raftery. </author> <title> Bayes Factors. </title> <journal> Journal of the American Statistical Association, </journal> <volume> 90(430) </volume> <pages> 773-795, </pages> <year> 1995. </year>
Reference-contexts: renormalises the 1 prior to be in the range [0:1; 20]. 3.2 The Bayes Factor or Evidence Approach Due primarily to the recent development of fast algorithms for approximating high dimensional integrals, interest has been renewed in Bayes factors for use in Bayesian model selection procedures; see Kass and Raftery <ref> [8] </ref>. In particular, Koop and Potter [9] have used Bayes factors to compare linear and nonlinear models in time series.
Reference: [9] <author> G. Koop and S.M. Potter. </author> <title> Bayes Factors and nonlinearity: Evidence from economic time series. </title> <note> UCLA Working Paper, August 1995, submitted to Journal of Econometrics. </note>
Reference-contexts: A variety of criteria have been used for determining the number of segments in data, including: * Tong [24, Section 7.2.7] used AIC [1] for the segmentation of a TAR model; Liang [11] used AIC for image segmentation. * Koop and Potter <ref> [9] </ref> use Bayes factors to compare the hypothesis of a linear model (one segment) with a single alternative of either a threshold autoregressive (TAR) model with a prespecified number of segments or a Markov Trend (or a so-called 'Hamilton') model where the number of segments is estimated using the method of <p> In particular, Koop and Potter <ref> [9] </ref> have used Bayes factors to compare linear and nonlinear models in time series.
Reference: [10] <author> Mengxiang Li. </author> <title> Minimum description length based 2-D shape description. </title> <booktitle> In IEEE 4th Int. Conf. on Computer Vision, </booktitle> <pages> pages 512-517, </pages> <month> May </month> <year> 1992. </year>
Reference-contexts: hypothesis of a linear model (one segment) with a single alternative of either a threshold autoregressive (TAR) model with a prespecified number of segments or a Markov Trend (or a so-called 'Hamilton') model where the number of segments is estimated using the method of Albert and Chib [2]. * Li <ref> [10] </ref> used MDL [19] (equivalent to BIC [21]) for image segmentation; Dom [7], Pfahringer [17] and Quinlan [18] refined the criterion within the context of the segmentation of binary strings. * Baxter and Oliver [3] used MML [25, 27] (a Bayesian method for point estimation) for the segmentation of line segments <p> of the Bayes Factor inference procedure may include the mode of the posterior (MAP), the mean of the posterior, etc. 10 4 Simulations We use the following criteria: * AIC, using log f (yj) + d + C [11]. * BIC, using log f (yj) + d+C 2 log n <ref> [10] </ref>. * MDL, using log f (yj) + d 2 log n + log n ! * BF, the Bayes Factor approach described in Section 3.2-3.3, with Prior #1 (with hyper parameters ; q; ff and fi) given in Relations (3) and (3). * MML1, using Equation (11) of this paper,
Reference: [11] <author> Z. Liang, R.J. Jaszczak, and R.E. Coleman. </author> <title> Parameter estimation of finite mixtures using the EM algorithm and information criteria with applications to medical image processing. </title> <journal> IEEE Trans. on Nuclear Science, </journal> <volume> 39(4) </volume> <pages> 1126-1133, </pages> <year> 1992. </year>
Reference-contexts: A variety of criteria have been used for determining the number of segments in data, including: * Tong [24, Section 7.2.7] used AIC [1] for the segmentation of a TAR model; Liang <ref> [11] </ref> used AIC for image segmentation. * Koop and Potter [9] use Bayes factors to compare the hypothesis of a linear model (one segment) with a single alternative of either a threshold autoregressive (TAR) model with a prespecified number of segments or a Markov Trend (or a so-called 'Hamilton') model where <p> 4 is `Level 1 Inference'. 4 The estimators used in Step 4 of the Bayes Factor inference procedure may include the mode of the posterior (MAP), the mean of the posterior, etc. 10 4 Simulations We use the following criteria: * AIC, using log f (yj) + d + C <ref> [11] </ref>. * BIC, using log f (yj) + d+C 2 log n [10]. * MDL, using log f (yj) + d 2 log n + log n ! * BF, the Bayes Factor approach described in Section 3.2-3.3, with Prior #1 (with hyper parameters ; q; ff and fi) given in
Reference: [12] <author> David J.C. MacKay. </author> <title> Bayesian Modeling and Neural Networks. </title> <type> PhD thesis, </type> <institution> Dept. of Computation and Neural Systems, CalTech, </institution> <year> 1992. </year>
Reference-contexts: the message length is approximately M essLen (y&v&) = log P rob (y&v&) + d + C (11) 3.5 Why is the MML Different from the Bayes Factor Approach? A number of authors who advocate the Bayes factors (or Evidence) approach have suggested that MDL is equivalent to Bayes factors <ref> [5, 12] </ref>, and that MML is an approximation to Bayes factors [13]: 1 By coding theory, we can encode an event of probability P in a message of length log P nits. A nit is the unit of message length when we take logarithms to the base e.
Reference: [13] <author> David J.C. MacKay. </author> <type> Personal communication, </type> <year> 1997. </year>
Reference-contexts: rob (y&v&) + d + C (11) 3.5 Why is the MML Different from the Bayes Factor Approach? A number of authors who advocate the Bayes factors (or Evidence) approach have suggested that MDL is equivalent to Bayes factors [5, 12], and that MML is an approximation to Bayes factors <ref> [13] </ref>: 1 By coding theory, we can encode an event of probability P in a message of length log P nits. A nit is the unit of message length when we take logarithms to the base e.
Reference: [14] <author> J.J. Oliver and R.A. Baxter. </author> <title> MML and Bayesianism: Similarities and differences. </title> <type> Technical report TR 206, </type> <institution> Dept. of Computer Science, Monash University, Clayton, </institution> <address> Victoria 3168, Australia, </address> <year> 1994. </year> <note> Available on the WWW from http://www.cs.monash.edu.au/ ~ jono. </note>
Reference-contexts: The volumes (V ol ( 0 )) will vary around the parameter space. For example, that part of the parameter space with low j will have a smaller regions than that part with high j (see Section 4.2 of <ref> [14] </ref>). We approximate the prior probability of region R (with representative point 0 R ) as: R R ) h ( 0 8 where h () is the assumed known prior density on .
Reference: [15] <author> J.J. Oliver, Baxter R.A., and Wallace C.S. </author> <title> Unsupervised Learning using MML. </title> <booktitle> In Machine Learning: Proceedings of the Thirteenth International Conference, </booktitle> <pages> pages 364-372, </pages> <year> 1996. </year>
Reference-contexts: j ; 2 j ) (see Zellner [28]) j j 2 ~ N (; 2 2 where ff; fi; and q are hyper-parameters for the prior distribution. 3.1.3 Prior #2 (An Improper Prior) We considered using improper prior distributions analogous to the distributions used in the context of mixture modelling <ref> [25, 15] </ref>.
Reference: [16] <author> J.J. Oliver, Baxter R.A., and Wallace C.S. </author> <title> Minimum message length segmentation. </title> <note> In submitted to Pacific-Asia Conference on Knowledge Discovery and Data Mining (PAKDD-98), 1997. Available on the WWW from http://www.cs.monash.edu.au/~jono. </note>
Reference-contexts: Cutpoint-like parameters v do not satisfy the regularity conditions required, and the Fisher Information matrix is not defined for this type of parameter. Oliver, Baxter and Wallace <ref> [16] </ref> derive expressions for the width of a region for cutpoint-like parameters: W idth (v) = 4 0 + RSS 0 RSS 1 +n 0 D 2 + RSS 1 RSS 0 +n 1 D 2 where n 0 and n 1 are the number of points in the segment either <p> Oliver, Baxter and Wallace <ref> [16] </ref> show that on average this introduces an additional length of C nits.
Reference: [17] <author> B. Pfahringer. </author> <title> Compression-based discretization of continuous attributes. </title> <booktitle> In Machine Learning: Proceedings of the Twelfth International Workshop, </booktitle> <pages> pages 456-463, </pages> <year> 1995. </year>
Reference-contexts: threshold autoregressive (TAR) model with a prespecified number of segments or a Markov Trend (or a so-called 'Hamilton') model where the number of segments is estimated using the method of Albert and Chib [2]. * Li [10] used MDL [19] (equivalent to BIC [21]) for image segmentation; Dom [7], Pfahringer <ref> [17] </ref> and Quinlan [18] refined the criterion within the context of the segmentation of binary strings. * Baxter and Oliver [3] used MML [25, 27] (a Bayesian method for point estimation) for the segmentation of line segments with Gaussian noise.
Reference: [18] <author> J.R. Quinlan. </author> <title> Improved use of continuous attributes in C4.5. </title> <journal> Journal of Artificial Intelligence, </journal> <volume> 4 </volume> <pages> 77-90, </pages> <year> 1996. </year>
Reference-contexts: model with a prespecified number of segments or a Markov Trend (or a so-called 'Hamilton') model where the number of segments is estimated using the method of Albert and Chib [2]. * Li [10] used MDL [19] (equivalent to BIC [21]) for image segmentation; Dom [7], Pfahringer [17] and Quinlan <ref> [18] </ref> refined the criterion within the context of the segmentation of binary strings. * Baxter and Oliver [3] used MML [25, 27] (a Bayesian method for point estimation) for the segmentation of line segments with Gaussian noise.
Reference: [19] <author> J. Rissanen. </author> <title> Modeling by shortest data description. </title> <journal> Automatica, </journal> <volume> 14 </volume> <pages> 465-471, </pages> <year> 1978. </year>
Reference-contexts: linear model (one segment) with a single alternative of either a threshold autoregressive (TAR) model with a prespecified number of segments or a Markov Trend (or a so-called 'Hamilton') model where the number of segments is estimated using the method of Albert and Chib [2]. * Li [10] used MDL <ref> [19] </ref> (equivalent to BIC [21]) for image segmentation; Dom [7], Pfahringer [17] and Quinlan [18] refined the criterion within the context of the segmentation of binary strings. * Baxter and Oliver [3] used MML [25, 27] (a Bayesian method for point estimation) for the segmentation of line segments with Gaussian noise.
Reference: [20] <author> J. Rissanen. </author> <title> Stochastic Complexity in Statistical Inquiry. </title> <publisher> World Scientific, </publisher> <address> Singapore, </address> <year> 1989. </year>
Reference-contexts: The 1 prior distribution is normalised to be in the range [0:01; 2]. To see the scale invariance of Prior #2 and Prior #3, Figure 2 (b) gives the prior distributions with y = 10 and renormalises the 1 prior to be in the range <ref> [0:1; 20] </ref>. 3.2 The Bayes Factor or Evidence Approach Due primarily to the recent development of fast algorithms for approximating high dimensional integrals, interest has been renewed in Bayes factors for use in Bayesian model selection procedures; see Kass and Raftery [8]. <p> H M : there are C = M cutpoints. 2 The MML inference procedure uses two part messages in Step 2. Rissanen <ref> [20] </ref> advocates one part messages (which are shorter).
Reference: [21] <author> G. Schwarz. </author> <title> Estimating dimension of a model. </title> <journal> Ann. Stat., </journal> <volume> 6 </volume> <pages> 461-464, </pages> <year> 1978. </year>
Reference-contexts: with a single alternative of either a threshold autoregressive (TAR) model with a prespecified number of segments or a Markov Trend (or a so-called 'Hamilton') model where the number of segments is estimated using the method of Albert and Chib [2]. * Li [10] used MDL [19] (equivalent to BIC <ref> [21] </ref>) for image segmentation; Dom [7], Pfahringer [17] and Quinlan [18] refined the criterion within the context of the segmentation of binary strings. * Baxter and Oliver [3] used MML [25, 27] (a Bayesian method for point estimation) for the segmentation of line segments with Gaussian noise.
Reference: [22] <author> S.L. Sclove. </author> <title> On segmentation of time series. </title> <editor> In S. Karlin, T. Amemiya, and L. Goodman, editors, </editor> <title> Studies in econometrics, time series, </title> <journal> and multivariate statistics, </journal> <pages> pages 311-330. </pages> <publisher> Academic Press, </publisher> <year> 1983. </year>
Reference-contexts: Figure 1 gives an example of a time series with two segments. Such models may be useful for segmenting data from (for example) (i) economic time series, (ii) electrocardiogram measurements and (iii) eye movement measurements from a sleeping person <ref> [22] </ref>. 1 Maximum likelihood estimation is a frequently used technique for fitting the segment parameters (in our simplified case j and 2 j ). <p> This approach appears difficult to apply to the C &gt; 1 case. 6 Applications 6.1 The US GNP 1947 - 1966 We segmented the quarterly gross national product (GNP) for the United States from 1947 16 - 1966 <ref> [22] </ref>. Figure 5 7 shows the preferred MML2 segmentation for this data.
Reference: [23] <author> C.W. Therrien. </author> <title> Decision, estimation, and classification : an introduction to pattern recognition and related topics. </title> <publisher> Wiley, </publisher> <address> New York, </address> <year> 1989. </year>
Reference-contexts: In addition, Table 1 gives the average Kullback-Liebler distance (Mean KL) between the predicted distribution, and the underlying distribution 6 . 6 The Kullback-Liebler distance (given for example in <ref> [23, Chp. 9] </ref>) between a true distribution N ( t ; 2 t ) and a fitted distribution N ( f ; 2 f ) is f 2 1 f t + ( t f ) 2 ): 4.2 Simulation #2 4.2.1 The Search Method It is impractical to consider every
Reference: [24] <author> H. Tong. </author> <title> Non-linear time series : a dynamical system approach. </title> <publisher> Clarendon Press, Oxford, </publisher> <year> 1990. </year>
Reference-contexts: This model is a simplification of the TAR models proposed by Tong <ref> [24] </ref>. Figure 1 gives an example of a time series with two segments. <p> However, maximum likelihood is an inappropriate method for selecting the number of segments, since it will result in a model with homogeneous regions containing only one datum each. A variety of criteria have been used for determining the number of segments in data, including: * Tong <ref> [24, Section 7.2.7] </ref> used AIC [1] for the segmentation of a TAR model; Liang [11] used AIC for image segmentation. * Koop and Potter [9] use Bayes factors to compare the hypothesis of a linear model (one segment) with a single alternative of either a threshold autoregressive (TAR) model with a
Reference: [25] <author> C.S. Wallace and D.M. Boulton. </author> <title> An information measure for classification. </title> <journal> Computer Journal, </journal> <volume> 11 </volume> <pages> 185-194, </pages> <year> 1968. </year>
Reference-contexts: segments is estimated using the method of Albert and Chib [2]. * Li [10] used MDL [19] (equivalent to BIC [21]) for image segmentation; Dom [7], Pfahringer [17] and Quinlan [18] refined the criterion within the context of the segmentation of binary strings. * Baxter and Oliver [3] used MML <ref> [25, 27] </ref> (a Bayesian method for point estimation) for the segmentation of line segments with Gaussian noise. In this paper we concentrate on the Bayesian approaches to segmentation (Bayes factors and MML). We develop the Bayesian approaches for segmentation, and highlight differences between them. <p> j ; 2 j ) (see Zellner [28]) j j 2 ~ N (; 2 2 where ff; fi; and q are hyper-parameters for the prior distribution. 3.1.3 Prior #2 (An Improper Prior) We considered using improper prior distributions analogous to the distributions used in the context of mixture modelling <ref> [25, 15] </ref>. <p> into Equation (7) gives us: m C (y 1 ; : : : ; y N ) = v2V C m (y 1 ; : : : ; y N j v) (8) which is the quantity we wish to maximise. 3.4 The Minimum Message Length Approach Wallace et al <ref> [25, 26, 27] </ref> advocate the use of Minimum Message Length (MML) for Bayesian point estimation. The MML approach can be interpreted as partitioning the parameter space into regions, where the models within a region are considered similar.
Reference: [26] <author> C.S. Wallace and D.M. Boulton. </author> <title> An invariant Bayes method for point estimation. </title> <journal> Classification Society Bulletin, </journal> <volume> 3(3) </volume> <pages> 11-34, </pages> <year> 1975. </year>
Reference-contexts: into Equation (7) gives us: m C (y 1 ; : : : ; y N ) = v2V C m (y 1 ; : : : ; y N j v) (8) which is the quantity we wish to maximise. 3.4 The Minimum Message Length Approach Wallace et al <ref> [25, 26, 27] </ref> advocate the use of Minimum Message Length (MML) for Bayesian point estimation. The MML approach can be interpreted as partitioning the parameter space into regions, where the models within a region are considered similar.
Reference: [27] <author> C.S. Wallace and P.R. Freeman. </author> <title> Estimation and inference by compact coding. </title> <journal> Journal of the Royal Statistical Society (Series B), </journal> <volume> 49 </volume> <pages> 240-252, </pages> <year> 1987. </year>
Reference-contexts: segments is estimated using the method of Albert and Chib [2]. * Li [10] used MDL [19] (equivalent to BIC [21]) for image segmentation; Dom [7], Pfahringer [17] and Quinlan [18] refined the criterion within the context of the segmentation of binary strings. * Baxter and Oliver [3] used MML <ref> [25, 27] </ref> (a Bayesian method for point estimation) for the segmentation of line segments with Gaussian noise. In this paper we concentrate on the Bayesian approaches to segmentation (Bayes factors and MML). We develop the Bayesian approaches for segmentation, and highlight differences between them. <p> into Equation (7) gives us: m C (y 1 ; : : : ; y N ) = v2V C m (y 1 ; : : : ; y N j v) (8) which is the quantity we wish to maximise. 3.4 The Minimum Message Length Approach Wallace et al <ref> [25, 26, 27] </ref> advocate the use of Minimum Message Length (MML) for Bayesian point estimation. The MML approach can be interpreted as partitioning the parameter space into regions, where the models within a region are considered similar. <p> 0 (R 1A ) = (v = 6:5; 0 = 0:5; 0 = 0:1; 1 = 0:5; 1 = 0:3) 0 (R 1B ) = (v = 10; 0 = 0:5; 0 = 0:1; 1 = 0:5; 1 = 0:3) 3.4.1 The Wallace and Freeman MML Approach Wallace and Freeman <ref> [27] </ref> used the following method to approximate the message length of y encoded using (a vector of continuous parameters).
Reference: [28] <author> A. </author> <title> Zellner. An introduction to Bayesian inference in economics. </title> <publisher> Wiley, </publisher> <address> New York, </address> <year> 1971. </year> <month> 20 </month>
Reference-contexts: t+1 y t ) = N 1 and the standard deviation of the differences y = t=1 (y t+1 y t y ) 2 3.1.2 Prior #1 (A Mathematically Convenient Prior) We assume the frequently employed mathematically convenient prior specification for each ( j ; 2 j ) (see Zellner <ref> [28] </ref>) j j 2 ~ N (; 2 2 where ff; fi; and q are hyper-parameters for the prior distribution. 3.1.3 Prior #2 (An Improper Prior) We considered using improper prior distributions analogous to the distributions used in the context of mixture modelling [25, 15].
References-found: 28

