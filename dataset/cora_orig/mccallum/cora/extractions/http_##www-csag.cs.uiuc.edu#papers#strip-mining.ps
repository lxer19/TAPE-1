URL: http://www-csag.cs.uiuc.edu/papers/strip-mining.ps
Refering-URL: http://www-csag.cs.uiuc.edu/papers/index.html
Root-URL: http://www.cs.uiuc.edu
Email: fzhang,achieng@cs.uiuc.edu  
Title: Dynamic Pointer Alignment: Tiling and Communication Optimizations for Parallel Pointer-based Computations  
Author: Xingbin Zhang Andrew A. Chien 
Web: http://www-csag.cs.uiuc.edu/  
Affiliation: Department of Computer Science University of Illinois at Urbana-Champaign  Department of Computer Science University of Illinois at Urbana-Champaign and Hewlett Packard Laboratories  
Abstract: Loop tiling and communication optimizations, such as message pipelining and aggregation, can achieve optimized and robust memory performance by proactively managing storage and data movement. In this paper, we generalize these techniques to pointer-based data structures (PBDSs). Our approach, dynamic pointer alignment (DPA), has two components. The compiler decomposes a program into non-blocking threads that operate on specific pointers and labels thread creation sites with their corresponding pointers. At runtime, an explicit mapping from pointers to dependent threads is updated at thread creation and is used to dynamically schedule both threads and communication, such that threads using the same objects execute together, communication overlaps with local work, and messages are aggregated. We have implemented DPA to optimize remote reads to global PBDSs on parallel machines. Our empirical results on the force computation phases of two applications that use sophisticated PBDSs, Barnes-Hut and FMM, show that DPA achieves good absolute performance and speedups by enabling tiling and communication optimizations on the CRAY T3D. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Jennifer M. Anderson and Monica S. Lam. </author> <title> Global optimizations for parallelism and locality on scalable parallel machines. </title> <booktitle> In Proceedings of the ACM SIGPLAN Conference on Programming Language Design and Implementation, </booktitle> <year> 1993. </year>
Reference-contexts: Because bandwidth and latency hierarchies can span several orders of magnitude, both data reuse optimizations to minimize bandwidth usage and latency tolerance optimizations are required. Loop tiling <ref> [1, 4] </ref> and communication optimizations [20], such as message pipelining (i.e. prefetching) and aggregation, can achieve optimized and robust memory performance by proactively managing storage and data movement. Tiling improves data reuse using a combination of strip-mining and iteration reordering to group iterations that access the same data. <p> for Barnes-Hut is competitive with other studies both on MPPs [3, 14] and on shared-memory architectures [34], and the speedup of FMM, 54-fold on 64 nodes, also compares favorably with other implementations [29, 35]. 6 Discussion and Related Work The goal of DPA is to generalize loop- and array-oriented tiling <ref> [1, 4, 24, 32] </ref> and communication optimizations [20] to pointer-based computations. Although developed independently, our use of non-blocking threads labeled by pointers is similar to the recent cache optimization study by Philbin, et al. [21].
Reference: [2] <author> Thomas Ball and Susan Horwitz. </author> <title> Constructing control flow from control dependence. </title> <type> Technical Report 1091, </type> <institution> University of WisconsinMadison, </institution> <year> 1992. </year>
Reference-contexts: Within the last control construct, the algorithm includes, transitively, all dependent program nodes into the thread, eliminating local dependence. This final transitive expansion guarantees linearizability <ref> [2] </ref> required by the function promotion. Applying the basic algorithm on the example in Section 3.4 results in the thread structure shown in Figure 7 (a).
Reference: [3] <author> Martin C. Carlisle and Anne Rogers. </author> <title> Software caching and computation migration in olden. </title> <booktitle> In Proceedings of Fifth Symposium on Principles and Practice of Parallel Programming, </booktitle> <year> 1995. </year>
Reference-contexts: Compared with prior locality optimizations for general PBDSs, such as caching <ref> [3, 14] </ref>, prefetching [23], and multithreading [10], the simultaneous computation and communication scheduling using an explicit runtime mapping is a unique feature provided by DPA. Thread scheduling enables tiling, and communication scheduling enables both message pipelining and aggregation. <p> We use Illinois Fast Messages (FM) [22, 26] for our message passing implementation. We selected these applications because they rely on sophisticated PBDSs for efficiency, and previous studies <ref> [3, 14, 29, 34, 35, 37] </ref> show that locality optimizations are crucial to achieve good performance. Our codes are in ICC++ and are adapted from the SPLASH-2 version. <p> Both configurations of DPA achieve better speedups than the caching scheme, which we believe is due to minimized hashing and better cache performance because of access hoisting. Although configurations differ, our speedup of over 42 on 64 nodes for Barnes-Hut is competitive with other studies both on MPPs <ref> [3, 14] </ref> and on shared-memory architectures [34], and the speedup of FMM, 54-fold on 64 nodes, also compares favorably with other implementations [29, 35]. 6 Discussion and Related Work The goal of DPA is to generalize loop- and array-oriented tiling [1, 4, 24, 32] and communication optimizations [20] to pointer-based computations. <p> We are not aware of any other approach that enables automatic message aggregation for such computations. In the context of dynamic locality optimizations, DPA requires inherent program concurrency, similar to multithreading [10], but unlike caching <ref> [3, 14] </ref> and prefetching [23]. DPA is a specialized form of multithreading with explicit data renaming to integrate data reuse. In addition, DPA uses additional memory to hold outstanding thread states, while caching uses memory for data.
Reference: [4] <author> Steve Carr, Kathryn S. McKinley, and Chau-Wen Tseng. </author> <title> Compiler optimizations for improving data locality. </title> <booktitle> In Proceedings of the Sixth Symposium on Architectural Support for Programming Languages and Operating Systems (ASPLOS-VI), </booktitle> <year> 1994. </year>
Reference-contexts: Because bandwidth and latency hierarchies can span several orders of magnitude, both data reuse optimizations to minimize bandwidth usage and latency tolerance optimizations are required. Loop tiling <ref> [1, 4] </ref> and communication optimizations [20], such as message pipelining (i.e. prefetching) and aggregation, can achieve optimized and robust memory performance by proactively managing storage and data movement. Tiling improves data reuse using a combination of strip-mining and iteration reordering to group iterations that access the same data. <p> for Barnes-Hut is competitive with other studies both on MPPs [3, 14] and on shared-memory architectures [34], and the speedup of FMM, 54-fold on 64 nodes, also compares favorably with other implementations [29, 35]. 6 Discussion and Related Work The goal of DPA is to generalize loop- and array-oriented tiling <ref> [1, 4, 24, 32] </ref> and communication optimizations [20] to pointer-based computations. Although developed independently, our use of non-blocking threads labeled by pointers is similar to the recent cache optimization study by Philbin, et al. [21].
Reference: [5] <author> K. Mani Chandy and Carl Kesselman. </author> <title> Compositional C++: Compositional parallel programming. </title> <booktitle> In Proceed 9 ings of the Fifth Workshop on Compilers and Languages for Parallel Computing, </booktitle> <address> New Haven, Connecticut, </address> <year> 1992. </year> <note> YALEU/DCS/RR-915, Springer-Verlag Lecture Notes in Computer Science, </note> <year> 1993. </year>
Reference-contexts: We have found that coarse-grained aliasing and block-level concurrency information are often sufficient to enable optimizations. This result is encouraging because it is practical to obtain coarse-grained aliasing information, and block-level concurrency annotations are both simple to add to programs and already supported by various programming languages <ref> [5, 8, 16] </ref>. We have implemented DPA in the Illinois Concert system 1 [7] in the context of optimizing concurrent object-oriented programs relying on global, distributed PBDSs on parallel machines. <p> More precise aliasing information can enable optimizations of more general access patterns, such as reductions. 3 3.2.3 Dependence Information We assume the following form of block-level concurrency information, obtainable from either language constructs <ref> [5, 8, 16] </ref> or by analyses [19, 30] in restricted cases. conc - conc for (i=0;i&lt;5;i++) object.method_1 (); objects [i].method (); object.method_2 (); - The conc specification asserts the absence of indirect dependency among loop iterations or statements within a block and enables interleaved execution. 3.3 Project Context We implemented DPA
Reference: [6] <author> A. A. Chien, U. S. Reddy, J. Plevyak, and J. Dolby. </author> <title> ICC++ a C++ dialect for high performance parallel computing. </title> <booktitle> In Proceedings of the 2nd International Symposium on Object Technologies for Advanced Software. </booktitle> <publisher> Springer-Verlag, LNCS 742, </publisher> <year> 1996. </year>
Reference-contexts: The Concert compiler also supports a suite of optimizations to reduce overhead due to object orientation. In particular, our example codes benefit from inline allocation of objects [13] to enlarge object granularity that amortizes object access overhead and simplifies communication of object state. Our source language is ICC++ <ref> [6, 8] </ref>, which extends C++ with object-level sequential consistency and block-level concurrency constructs shown in x3.2.3. 3.4 Example We illustrate our approach using a pair of examples.
Reference: [7] <author> Andrew Chien, Julian Dolby, Bishwaroop Ganguly, Vijay Karamcheti, and Xingbin Zhang. </author> <title> Supporting high level programming with high performance: The Illinois Concert system. </title> <booktitle> In Proceedings of the Second International Workshop on High-level Parallel Programming Models and Supportive Environments, </booktitle> <month> April </month> <year> 1997. </year>
Reference-contexts: This result is encouraging because it is practical to obtain coarse-grained aliasing information, and block-level concurrency annotations are both simple to add to programs and already supported by various programming languages [5, 8, 16]. We have implemented DPA in the Illinois Concert system 1 <ref> [7] </ref> in the context of optimizing concurrent object-oriented programs relying on global, distributed PBDSs on parallel machines. We have experimentally evaluated our technique on the CRAY T3D, on the force computation phases of Barnes-Hut and FMM, two hierarchical N-body programs from the SPLASH-2 suite [38] that rely on non-trivial PBDSs. <p> [19, 30] in restricted cases. conc - conc for (i=0;i&lt;5;i++) object.method_1 (); objects [i].method (); object.method_2 (); - The conc specification asserts the absence of indirect dependency among loop iterations or statements within a block and enables interleaved execution. 3.3 Project Context We implemented DPA in the Illinois Concert compiler <ref> [7] </ref>, which uses program dependence graph (PDG) [15] in gated SSA [12] form as its intermediate representation. The Concert compiler supports an adaptive, context-sensitive interprocedural analysis framework [27, 28] that we use to implement our analysis and code transformations.
Reference: [8] <author> Andrew Chien and Uday Reddy. </author> <title> ICC++ language definition. Concurrent Systems Architecture Group Memo, </title> <note> Also available from http://www-csag.cs.uiuc.edu/, February 1995. </note>
Reference-contexts: We have found that coarse-grained aliasing and block-level concurrency information are often sufficient to enable optimizations. This result is encouraging because it is practical to obtain coarse-grained aliasing information, and block-level concurrency annotations are both simple to add to programs and already supported by various programming languages <ref> [5, 8, 16] </ref>. We have implemented DPA in the Illinois Concert system 1 [7] in the context of optimizing concurrent object-oriented programs relying on global, distributed PBDSs on parallel machines. <p> More precise aliasing information can enable optimizations of more general access patterns, such as reductions. 3 3.2.3 Dependence Information We assume the following form of block-level concurrency information, obtainable from either language constructs <ref> [5, 8, 16] </ref> or by analyses [19, 30] in restricted cases. conc - conc for (i=0;i&lt;5;i++) object.method_1 (); objects [i].method (); object.method_2 (); - The conc specification asserts the absence of indirect dependency among loop iterations or statements within a block and enables interleaved execution. 3.3 Project Context We implemented DPA <p> The Concert compiler also supports a suite of optimizations to reduce overhead due to object orientation. In particular, our example codes benefit from inline allocation of objects [13] to enlarge object granularity that amortizes object access overhead and simplifies communication of object state. Our source language is ICC++ <ref> [6, 8] </ref>, which extends C++ with object-level sequential consistency and block-level concurrency constructs shown in x3.2.3. 3.4 Example We illustrate our approach using a pair of examples.
Reference: [9] <author> Cray Research, Inc. </author> <title> Cray T3D System Architecture Overview, </title> <month> March </month> <year> 1993. </year>
Reference-contexts: D and M are runtime data structures described in Section 3.2.2. T represents a thread, and TOUCH blocks until the given request completes. 5 Experimental Results We empirically evaluated DPA using two applications from the SPLASH-2 suite [38], Barnes-Hut and FMM, on the Cray T3D <ref> [9] </ref>, a distributed memory machine with support for one-sided remote put/get memory operations. We use Illinois Fast Messages (FM) [22, 26] for our message passing implementation.
Reference: [10] <author> David Culler, Anurag Sah, Klaus Erik Schauser, Thorsten von Eicken, and John Wawrzynek. </author> <title> Fine-grain parallelism with minimal hardware support: A compiler-controlled threaded abstract machine. </title> <booktitle> In Proceedings of the Fourth International Conference on Architectural Support for Programming Languages an Operating Systems, </booktitle> <pages> pages 16475, </pages> <year> 1991. </year>
Reference-contexts: Compared with prior locality optimizations for general PBDSs, such as caching [3, 14], prefetching [23], and multithreading <ref> [10] </ref>, the simultaneous computation and communication scheduling using an explicit runtime mapping is a unique feature provided by DPA. Thread scheduling enables tiling, and communication scheduling enables both message pipelining and aggregation. <p> We are not aware of any other approach that enables automatic message aggregation for such computations. In the context of dynamic locality optimizations, DPA requires inherent program concurrency, similar to multithreading <ref> [10] </ref>, but unlike caching [3, 14] and prefetching [23]. DPA is a specialized form of multithreading with explicit data renaming to integrate data reuse. In addition, DPA uses additional memory to hold outstanding thread states, while caching uses memory for data. <p> Our partitioning algorithm is similar to the dependence sets partitioning of [33, 36] for compiling for TAM <ref> [10] </ref>. Our use of aliasing to hoist data accesses enables larger threads.
Reference: [11] <author> David E. Culler. </author> <title> Managing Parallelism and Resources in Scientific Dataflow Programs. </title> <type> PhD thesis, </type> <institution> Massachusetts Institute of Technology, Laboratory of Computer Science, </institution> <address> Cam-bridge, MA, </address> <month> June </month> <year> 1989. </year>
Reference-contexts: The compiler chooses a scheduling template, such as the ones in Figure 14, for optimization. To limit excess concurrency due to storage constraints, we statically strip-mine all top-level concurrent loops with a given strip size, similar to k-bounded loops <ref> [11] </ref>.
Reference: [12] <author> R. Cytron, J. Ferrante, B. Rosen, M. Wegman, and F. Zadeck. </author> <title> An efficient method of computing static single assignment form and the control dependence graph. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 13(4):451490, </volume> <month> October </month> <year> 1991. </year>
Reference-contexts: (); objects [i].method (); object.method_2 (); - The conc specification asserts the absence of indirect dependency among loop iterations or statements within a block and enables interleaved execution. 3.3 Project Context We implemented DPA in the Illinois Concert compiler [7], which uses program dependence graph (PDG) [15] in gated SSA <ref> [12] </ref> form as its intermediate representation. The Concert compiler supports an adaptive, context-sensitive interprocedural analysis framework [27, 28] that we use to implement our analysis and code transformations. The Concert compiler also supports a suite of optimizations to reduce overhead due to object orientation.
Reference: [13] <author> Julian Dolby. </author> <title> Automatic inline allocation of objects. </title> <booktitle> In Proceedings of the 1997 ACM SIGPLAN Conference on Programming Language Design and Implementation, </booktitle> <month> June </month> <year> 1997. </year>
Reference-contexts: The Concert compiler supports an adaptive, context-sensitive interprocedural analysis framework [27, 28] that we use to implement our analysis and code transformations. The Concert compiler also supports a suite of optimizations to reduce overhead due to object orientation. In particular, our example codes benefit from inline allocation of objects <ref> [13] </ref> to enlarge object granularity that amortizes object access overhead and simplifies communication of object state. Our source language is ICC++ [6, 8], which extends C++ with object-level sequential consistency and block-level concurrency constructs shown in x3.2.3. 3.4 Example We illustrate our approach using a pair of examples.
Reference: [14] <author> B. Falsafi, A. Lebeck, S. Reinhardt, I. Schoinas, M. Hill, J. Larus, A. Rogers, and D. Wood. </author> <title> Application-specific protocols for user-level shared memory. </title> <booktitle> In Proceedings of Supercomputing '94, </booktitle> <address> Washington, D.C., </address> <year> 1994. </year>
Reference-contexts: Compared with prior locality optimizations for general PBDSs, such as caching <ref> [3, 14] </ref>, prefetching [23], and multithreading [10], the simultaneous computation and communication scheduling using an explicit runtime mapping is a unique feature provided by DPA. Thread scheduling enables tiling, and communication scheduling enables both message pipelining and aggregation. <p> We use Illinois Fast Messages (FM) [22, 26] for our message passing implementation. We selected these applications because they rely on sophisticated PBDSs for efficiency, and previous studies <ref> [3, 14, 29, 34, 35, 37] </ref> show that locality optimizations are crucial to achieve good performance. Our codes are in ICC++ and are adapted from the SPLASH-2 version. <p> Both configurations of DPA achieve better speedups than the caching scheme, which we believe is due to minimized hashing and better cache performance because of access hoisting. Although configurations differ, our speedup of over 42 on 64 nodes for Barnes-Hut is competitive with other studies both on MPPs <ref> [3, 14] </ref> and on shared-memory architectures [34], and the speedup of FMM, 54-fold on 64 nodes, also compares favorably with other implementations [29, 35]. 6 Discussion and Related Work The goal of DPA is to generalize loop- and array-oriented tiling [1, 4, 24, 32] and communication optimizations [20] to pointer-based computations. <p> We are not aware of any other approach that enables automatic message aggregation for such computations. In the context of dynamic locality optimizations, DPA requires inherent program concurrency, similar to multithreading [10], but unlike caching <ref> [3, 14] </ref> and prefetching [23]. DPA is a specialized form of multithreading with explicit data renaming to integrate data reuse. In addition, DPA uses additional memory to hold outstanding thread states, while caching uses memory for data.
Reference: [15] <author> Jeanne Ferrante, Karl J. Ottenstein, and Joe D. Warren. </author> <title> The program dependence graph and its use in optimization. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 9(3):31949, </volume> <month> July </month> <year> 1987. </year>
Reference-contexts: conc for (i=0;i&lt;5;i++) object.method_1 (); objects [i].method (); object.method_2 (); - The conc specification asserts the absence of indirect dependency among loop iterations or statements within a block and enables interleaved execution. 3.3 Project Context We implemented DPA in the Illinois Concert compiler [7], which uses program dependence graph (PDG) <ref> [15] </ref> in gated SSA [12] form as its intermediate representation. The Concert compiler supports an adaptive, context-sensitive interprocedural analysis framework [27, 28] that we use to implement our analysis and code transformations. The Concert compiler also supports a suite of optimizations to reduce overhead due to object orientation.
Reference: [16] <author> High Performance Fortran Forum. </author> <title> High performance Fortran language specification version 1.0. </title> <type> Technical Report CRPC-TR92225, </type> <institution> Rice University, </institution> <month> January </month> <year> 1993. </year>
Reference-contexts: We have found that coarse-grained aliasing and block-level concurrency information are often sufficient to enable optimizations. This result is encouraging because it is practical to obtain coarse-grained aliasing information, and block-level concurrency annotations are both simple to add to programs and already supported by various programming languages <ref> [5, 8, 16] </ref>. We have implemented DPA in the Illinois Concert system 1 [7] in the context of optimizing concurrent object-oriented programs relying on global, distributed PBDSs on parallel machines. <p> More precise aliasing information can enable optimizations of more general access patterns, such as reductions. 3 3.2.3 Dependence Information We assume the following form of block-level concurrency information, obtainable from either language constructs <ref> [5, 8, 16] </ref> or by analyses [19, 30] in restricted cases. conc - conc for (i=0;i&lt;5;i++) object.method_1 (); objects [i].method (); object.method_2 (); - The conc specification asserts the absence of indirect dependency among loop iterations or statements within a block and enables interleaved execution. 3.3 Project Context We implemented DPA
Reference: [17] <author> Rakesh Ghiya and Laurie J. Hendren. </author> <title> Connection analysis: A practical interprocedural heap analysis for C. </title> <booktitle> In Proceedings of the Workshop for Languages and Compilers for Parallel Computing, </booktitle> <year> 1995. </year>
Reference-contexts: It relies on aliasing and concurrency information, 2 outlined 2 Both general alias and dependence analyses for PBDSs are challenging research problems (e.g. <ref> [17, 18, 19, 30] </ref>) and are out the scope of this study. below, to enlarge thread granularity and enable reordering. <p> Our partitioning algorithm is similar to the dependence sets partitioning of [33, 36] for compiling for TAM [10]. Our use of aliasing to hoist data accesses enables larger threads. DPA is also an example optimization enabled by various aliasing and par-allelization analyses such as, connection analysis <ref> [17] </ref> and commu-tativity analysis [30], which can help to enable DPA on sequential 8 BARNES-HUT PROCESSORS Version 1 2 4 8 16 32 64 DPA (50) 118.02 61.23 33.05 17.15 8.59 4.48 2.63 Caching 115.15 65.77 38.02 20.21 10.46 5.41 2.90 FMM PROCESSORS Version 2 4 8 16 32 64 DPA
Reference: [18] <author> Rakesh Ghiya and Laurie J. Hendren. </author> <title> Is it a tree, a DAG, or a cyclic graph? A shape analysis for heap-directed pointers in C. </title> <booktitle> In Proceedings of POPL, </booktitle> <year> 1996. </year>
Reference-contexts: However, loop tiling and communication optimizations do not directly apply to PBDSs due to two inherent compile-time limitations. First, precise aliasing information for general PBDSs is an open problem (e.g. <ref> [18, 19] </ref>), and thus the compile-time data access pattern information required for static optimizations is unavailable. Second, computations on PBDSs are often expressed as recursive functions or while loops with data-dependent termination conditions, causing unknown control flow and thus inhibiting static iteration reordering. <p> It relies on aliasing and concurrency information, 2 outlined 2 Both general alias and dependence analyses for PBDSs are challenging research problems (e.g. <ref> [17, 18, 19, 30] </ref>) and are out the scope of this study. below, to enlarge thread granularity and enable reordering.
Reference: [19] <author> L. Hendren and A. Nicolau. </author> <title> Parallelizing programs with recursive data structures. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 1(1):3547, </volume> <month> January </month> <year> 1990. </year>
Reference-contexts: However, loop tiling and communication optimizations do not directly apply to PBDSs due to two inherent compile-time limitations. First, precise aliasing information for general PBDSs is an open problem (e.g. <ref> [18, 19] </ref>), and thus the compile-time data access pattern information required for static optimizations is unavailable. Second, computations on PBDSs are often expressed as recursive functions or while loops with data-dependent termination conditions, causing unknown control flow and thus inhibiting static iteration reordering. <p> It relies on aliasing and concurrency information, 2 outlined 2 Both general alias and dependence analyses for PBDSs are challenging research problems (e.g. <ref> [17, 18, 19, 30] </ref>) and are out the scope of this study. below, to enlarge thread granularity and enable reordering. <p> More precise aliasing information can enable optimizations of more general access patterns, such as reductions. 3 3.2.3 Dependence Information We assume the following form of block-level concurrency information, obtainable from either language constructs [5, 8, 16] or by analyses <ref> [19, 30] </ref> in restricted cases. conc - conc for (i=0;i&lt;5;i++) object.method_1 (); objects [i].method (); object.method_2 (); - The conc specification asserts the absence of indirect dependency among loop iterations or statements within a block and enables interleaved execution. 3.3 Project Context We implemented DPA in the Illinois Concert compiler [7],
Reference: [20] <author> Seema Hiranandani, Ken Kennedy, and Chau-Wen Tseng. </author> <title> Compiler optimizations for FORTRAN D on MIMD distributed-memory machines. </title> <journal> Communications of the ACM, </journal> <month> August </month> <year> 1992. </year>
Reference-contexts: Because bandwidth and latency hierarchies can span several orders of magnitude, both data reuse optimizations to minimize bandwidth usage and latency tolerance optimizations are required. Loop tiling [1, 4] and communication optimizations <ref> [20] </ref>, such as message pipelining (i.e. prefetching) and aggregation, can achieve optimized and robust memory performance by proactively managing storage and data movement. Tiling improves data reuse using a combination of strip-mining and iteration reordering to group iterations that access the same data. <p> both on MPPs [3, 14] and on shared-memory architectures [34], and the speedup of FMM, 54-fold on 64 nodes, also compares favorably with other implementations [29, 35]. 6 Discussion and Related Work The goal of DPA is to generalize loop- and array-oriented tiling [1, 4, 24, 32] and communication optimizations <ref> [20] </ref> to pointer-based computations. Although developed independently, our use of non-blocking threads labeled by pointers is similar to the recent cache optimization study by Philbin, et al. [21]. In [21], the programmer manually extracts threads and can supply multiple scalar hints as thread labels.
Reference: [21] <author> James Philbin, et. al. </author> <title> Thread scheduling for cache locality. </title> <booktitle> In Proceedings of the Seventh Symposium on Architectural Support for Programming Languages and Operating Systems (ASPLOS-VII), </booktitle> <year> 1996. </year>
Reference-contexts: The figure shows total computation time in terms of idle time, communication overhead, and local computation, with the speedup shown on top of each bar. The Base bars of both codes use DPA for 7 Although reordering computation may also improve sequential performance (see Section 6 and <ref> [21] </ref>), because of the small L1 cache and the lack of a L2 cache, we believe this is not the case on a single T3D node. 7 STATIC MAX. NO. OF MAX. <p> Although developed independently, our use of non-blocking threads labeled by pointers is similar to the recent cache optimization study by Philbin, et al. <ref> [21] </ref>. In [21], the programmer manually extracts threads and can supply multiple scalar hints as thread labels. DPA is an automatic approach relying on the compiler for thread extraction and integrates communication optimizations, not addressed in [21], to tolerate latency and reduce communication overhead. <p> Although developed independently, our use of non-blocking threads labeled by pointers is similar to the recent cache optimization study by Philbin, et al. <ref> [21] </ref>. In [21], the programmer manually extracts threads and can supply multiple scalar hints as thread labels. DPA is an automatic approach relying on the compiler for thread extraction and integrates communication optimizations, not addressed in [21], to tolerate latency and reduce communication overhead. <p> by pointers is similar to the recent cache optimization study by Philbin, et al. <ref> [21] </ref>. In [21], the programmer manually extracts threads and can supply multiple scalar hints as thread labels. DPA is an automatic approach relying on the compiler for thread extraction and integrates communication optimizations, not addressed in [21], to tolerate latency and reduce communication overhead. DPA performs explicit renaming and is also applicable to cache optimizations, which we are currently investigating. The execution strategy of DPA is also similar to the inspector/executor model [31].
Reference: [22] <author> Vijay Karamcheti and Andrew A. Chien. </author> <title> FM fast messaging on the Cray T3D. </title> <note> Available from http://www-csag.cs.uiuc.edu/ papers/ t3d-fm-manual.ps, </note> <month> February </month> <year> 1995. </year>
Reference-contexts: We use Illinois Fast Messages (FM) <ref> [22, 26] </ref> for our message passing implementation. We selected these applications because they rely on sophisticated PBDSs for efficiency, and previous studies [3, 14, 29, 34, 35, 37] show that locality optimizations are crucial to achieve good performance. Our codes are in ICC++ and are adapted from the SPLASH-2 version.
Reference: [23] <author> Chi-Keung Luk and Todd. C. Mowry. </author> <title> Compiler-based prefetching for recursive data structures. </title> <booktitle> In Proceedings of the Seventh International Conference on Architectural Support for Programming Languages an Operating Systems, </booktitle> <pages> pages 6273, </pages> <year> 1996. </year>
Reference-contexts: Compared with prior locality optimizations for general PBDSs, such as caching [3, 14], prefetching <ref> [23] </ref>, and multithreading [10], the simultaneous computation and communication scheduling using an explicit runtime mapping is a unique feature provided by DPA. Thread scheduling enables tiling, and communication scheduling enables both message pipelining and aggregation. <p> We are not aware of any other approach that enables automatic message aggregation for such computations. In the context of dynamic locality optimizations, DPA requires inherent program concurrency, similar to multithreading [10], but unlike caching [3, 14] and prefetching <ref> [23] </ref>. DPA is a specialized form of multithreading with explicit data renaming to integrate data reuse. In addition, DPA uses additional memory to hold outstanding thread states, while caching uses memory for data.
Reference: [24] <author> Kathryn S. McKinley. </author> <title> Evaluating automatic parallelization for efficient execution on shared-memory multiprocessors. </title> <booktitle> In Proceedings of the International Conference on Supercomputing, </booktitle> <year> 1994. </year>
Reference-contexts: for Barnes-Hut is competitive with other studies both on MPPs [3, 14] and on shared-memory architectures [34], and the speedup of FMM, 54-fold on 64 nodes, also compares favorably with other implementations [29, 35]. 6 Discussion and Related Work The goal of DPA is to generalize loop- and array-oriented tiling <ref> [1, 4, 24, 32] </ref> and communication optimizations [20] to pointer-based computations. Although developed independently, our use of non-blocking threads labeled by pointers is similar to the recent cache optimization study by Philbin, et al. [21].
Reference: [25] <author> D. Padua and M. Wolfe. </author> <title> Advanced compiler optimizations for supercomputers. </title> <journal> Communications of the ACM, </journal> <volume> 29(12), </volume> <year> 1986. </year>
Reference-contexts: In addition, DPA uses additional memory to hold outstanding thread states, while caching uses memory for data. Because DPA reorders both computation and data accesses, it is complementary to both caching and prefetching for explicitly managing fast storage. In the context of other compilation techniques, loop transformations <ref> [25] </ref>, such as loop splitting, can decouple dependence to improve the effectiveness of DPA, while DPA also applies to loop optimizations when precise static access pattern or iteration space is unavailable. Our partitioning algorithm is similar to the dependence sets partitioning of [33, 36] for compiling for TAM [10].
Reference: [26] <author> Scott Pakin, Vijay Karamcheti, and Andrew A. Chien. </author> <title> Fast Messages (FM): Efficient, portable communication for workstation clusters and massively-parallel processors. </title> <journal> IEEE Concurrency, </journal> <note> to appear in 1997. Available from http://www-csag.cs.uiuc.edu/papers/ fm-pdt.ps. </note>
Reference-contexts: We use Illinois Fast Messages (FM) <ref> [22, 26] </ref> for our message passing implementation. We selected these applications because they rely on sophisticated PBDSs for efficiency, and previous studies [3, 14, 29, 34, 35, 37] show that locality optimizations are crucial to achieve good performance. Our codes are in ICC++ and are adapted from the SPLASH-2 version.
Reference: [27] <author> John Plevyak and Andrew A. Chien. </author> <title> Precise concrete type inference of object-oriented programs. </title> <booktitle> In Proceedings of OOP-SLA'94, Object-Oriented Programming Systems, Languages and Architectures, </booktitle> <pages> pages 324340, </pages> <year> 1994. </year>
Reference-contexts: The Concert compiler supports an adaptive, context-sensitive interprocedural analysis framework <ref> [27, 28] </ref> that we use to implement our analysis and code transformations. The Concert compiler also supports a suite of optimizations to reduce overhead due to object orientation.
Reference: [28] <author> John Plevyak and Andrew A. Chien. </author> <title> Type directed cloning for object-oriented programs. </title> <booktitle> In Proceedings of the Workshop for Languages and Compilers for Parallel Computing, </booktitle> <pages> pages 566580, </pages> <year> 1995. </year>
Reference-contexts: The Concert compiler supports an adaptive, context-sensitive interprocedural analysis framework <ref> [27, 28] </ref> that we use to implement our analysis and code transformations. The Concert compiler also supports a suite of optimizations to reduce overhead due to object orientation.
Reference: [29] <author> William T. Rankin and John A. Board Jr. </author> <title> A portable distributed implementation of the parallel multiple tree algorithm. </title> <type> Technical Report 95-002, </type> <institution> Duke University, Department of Electrical Engineering, </institution> <year> 1995. </year>
Reference-contexts: We use Illinois Fast Messages (FM) [22, 26] for our message passing implementation. We selected these applications because they rely on sophisticated PBDSs for efficiency, and previous studies <ref> [3, 14, 29, 34, 35, 37] </ref> show that locality optimizations are crucial to achieve good performance. Our codes are in ICC++ and are adapted from the SPLASH-2 version. <p> Although configurations differ, our speedup of over 42 on 64 nodes for Barnes-Hut is competitive with other studies both on MPPs [3, 14] and on shared-memory architectures [34], and the speedup of FMM, 54-fold on 64 nodes, also compares favorably with other implementations <ref> [29, 35] </ref>. 6 Discussion and Related Work The goal of DPA is to generalize loop- and array-oriented tiling [1, 4, 24, 32] and communication optimizations [20] to pointer-based computations.
Reference: [30] <author> Martin Rinard and Pedro C. Diniz. </author> <title> Commutativity analysis: A new analysis framework for parallelizing compilers. </title> <booktitle> In Proceedings of the 1996 ACM SIGPLAN Conference on Programming Language Design and Implementation, </booktitle> <pages> pages 5467, </pages> <year> 1996. </year>
Reference-contexts: It relies on aliasing and concurrency information, 2 outlined 2 Both general alias and dependence analyses for PBDSs are challenging research problems (e.g. <ref> [17, 18, 19, 30] </ref>) and are out the scope of this study. below, to enlarge thread granularity and enable reordering. <p> More precise aliasing information can enable optimizations of more general access patterns, such as reductions. 3 3.2.3 Dependence Information We assume the following form of block-level concurrency information, obtainable from either language constructs [5, 8, 16] or by analyses <ref> [19, 30] </ref> in restricted cases. conc - conc for (i=0;i&lt;5;i++) object.method_1 (); objects [i].method (); object.method_2 (); - The conc specification asserts the absence of indirect dependency among loop iterations or statements within a block and enables interleaved execution. 3.3 Project Context We implemented DPA in the Illinois Concert compiler [7], <p> Our use of aliasing to hoist data accesses enables larger threads. DPA is also an example optimization enabled by various aliasing and par-allelization analyses such as, connection analysis [17] and commu-tativity analysis <ref> [30] </ref>, which can help to enable DPA on sequential 8 BARNES-HUT PROCESSORS Version 1 2 4 8 16 32 64 DPA (50) 118.02 61.23 33.05 17.15 8.59 4.48 2.63 Caching 115.15 65.77 38.02 20.21 10.46 5.41 2.90 FMM PROCESSORS Version 2 4 8 16 32 64 DPA (50) 7.39 3.80 1.91
Reference: [31] <author> J. Saltz, K. Crowley, R. Mirchandaney, and H. Berryman. </author> <title> Run-time scheduling and execution of loops on message passing machines. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 8:303312, </volume> <year> 1990. </year> <month> 10 </month>
Reference-contexts: Thread scheduling enables tiling, and communication scheduling enables both message pipelining and aggregation. Our approach is similar in spirit to the inspector/executor model <ref> [31] </ref> for discovering the access pattern at runtime and using it for dynamic optimizations. However, because DPA uses threads for dynamic reordering, it allows incremental runtime mapping construction without separate inspector phases. <p> DPA performs explicit renaming and is also applicable to cache optimizations, which we are currently investigating. The execution strategy of DPA is also similar to the inspector/executor model <ref> [31] </ref>. However DPA constructs dynamic access pattern incrementally, enabling optimization for recursive 8 This is despite the manual optimization of poll placement. particles, 29 terms) using DPA (strip size 300) on 16 nodes, with speedups shown on top of each bar.
Reference: [32] <author> Vivek Sarkar and Radhika Thekkath. </author> <title> A general framework for iteration-reordering loop transformations. </title> <booktitle> In Proceedings of the ACM SIGPLAN Conference on Programming Language Design and Implementation, </booktitle> <year> 1992. </year>
Reference-contexts: for Barnes-Hut is competitive with other studies both on MPPs [3, 14] and on shared-memory architectures [34], and the speedup of FMM, 54-fold on 64 nodes, also compares favorably with other implementations [29, 35]. 6 Discussion and Related Work The goal of DPA is to generalize loop- and array-oriented tiling <ref> [1, 4, 24, 32] </ref> and communication optimizations [20] to pointer-based computations. Although developed independently, our use of non-blocking threads labeled by pointers is similar to the recent cache optimization study by Philbin, et al. [21].
Reference: [33] <author> Klaus Erik Schauser, David E. Culler, and Thorsten von Eicken. </author> <title> Compiler-controlled multithreading for lenient parallel languages. </title> <booktitle> In Proceedings of the Conference on Functional Programming and Computer Architecture, </booktitle> <year> 1991. </year>
Reference-contexts: Our partitioning algorithm is similar to the dependence sets partitioning of <ref> [33, 36] </ref> for compiling for TAM [10]. Our use of aliasing to hoist data accesses enables larger threads.
Reference: [34] <author> Jaswinder Pal Singh. </author> <title> Parallel Hierarchical N-Body Methods and Their Implications For Multiprocessors. </title> <type> PhD thesis, </type> <institution> Stanford University Department of Computer Science, </institution> <address> Stan-ford, CA, </address> <month> February </month> <year> 1993. </year>
Reference-contexts: We use Illinois Fast Messages (FM) [22, 26] for our message passing implementation. We selected these applications because they rely on sophisticated PBDSs for efficiency, and previous studies <ref> [3, 14, 29, 34, 35, 37] </ref> show that locality optimizations are crucial to achieve good performance. Our codes are in ICC++ and are adapted from the SPLASH-2 version. <p> Although configurations differ, our speedup of over 42 on 64 nodes for Barnes-Hut is competitive with other studies both on MPPs [3, 14] and on shared-memory architectures <ref> [34] </ref>, and the speedup of FMM, 54-fold on 64 nodes, also compares favorably with other implementations [29, 35]. 6 Discussion and Related Work The goal of DPA is to generalize loop- and array-oriented tiling [1, 4, 24, 32] and communication optimizations [20] to pointer-based computations.
Reference: [35] <author> Jaswinder Pal Singh, Chris Holt, John L. Hennessy, and Anoop Gupta. </author> <title> A parallel adaptive fast multipole method. </title> <booktitle> In Proceedings of Supercomputing Conference, </booktitle> <pages> pages 5465, </pages> <year> 1993. </year>
Reference-contexts: We use Illinois Fast Messages (FM) [22, 26] for our message passing implementation. We selected these applications because they rely on sophisticated PBDSs for efficiency, and previous studies <ref> [3, 14, 29, 34, 35, 37] </ref> show that locality optimizations are crucial to achieve good performance. Our codes are in ICC++ and are adapted from the SPLASH-2 version. <p> Although configurations differ, our speedup of over 42 on 64 nodes for Barnes-Hut is competitive with other studies both on MPPs [3, 14] and on shared-memory architectures [34], and the speedup of FMM, 54-fold on 64 nodes, also compares favorably with other implementations <ref> [29, 35] </ref>. 6 Discussion and Related Work The goal of DPA is to generalize loop- and array-oriented tiling [1, 4, 24, 32] and communication optimizations [20] to pointer-based computations.
Reference: [36] <author> Kenneth R. Traub, David E. Culler, and Klause E. Schauser. </author> <title> Global analysis for partitioning non-strict programs into sequential threads. </title> <booktitle> In Proc. 1992 ACM Conference on Lisp and Functional Programming, </booktitle> <month> June </month> <year> 1992. </year>
Reference-contexts: Our partitioning algorithm is similar to the dependence sets partitioning of <ref> [33, 36] </ref> for compiling for TAM [10]. Our use of aliasing to hoist data accesses enables larger threads.
Reference: [37] <author> M. Warren and J. Salmon. </author> <title> A parallel hashed oct-tree N-body algorithm. </title> <booktitle> In Proceedings of Supercomputing Conference, </booktitle> <pages> pages 1221, </pages> <year> 1993. </year>
Reference-contexts: We use Illinois Fast Messages (FM) [22, 26] for our message passing implementation. We selected these applications because they rely on sophisticated PBDSs for efficiency, and previous studies <ref> [3, 14, 29, 34, 35, 37] </ref> show that locality optimizations are crucial to achieve good performance. Our codes are in ICC++ and are adapted from the SPLASH-2 version. <p> The sequential version execution time for Barnes-Hut is 97.84 secs (16,384 particles, 4 steps), for FMM is 14.46 secs (32,768-particle, 29 terms, 1 step). languages, such as C or C++. Finally, DPA is motivated by manual communication optimizations for Barnes-Hut in <ref> [37] </ref>. Two limitations of our current implementation require source level modification of our codes. Because we have yet to integrate DPA with loop parallelization, we manually split a loop in Barnes-Hut to decouple dependence.
Reference: [38] <author> Steven Cameron Woo, Moriyoshi Ohara, Evan Torrie, Jaswinder Pal Singh, and Anoop Gupta. </author> <title> The SPLASH-2 programs: Characterization and methodological considerations. </title> <booktitle> In Proceedings of the International Symposium on Computer Architecture, </booktitle> <pages> pages 2436, </pages> <year> 1995. </year> <month> 11 </month>
Reference-contexts: We have experimentally evaluated our technique on the CRAY T3D, on the force computation phases of Barnes-Hut and FMM, two hierarchical N-body programs from the SPLASH-2 suite <ref> [38] </ref> that rely on non-trivial PBDSs. <p> D and M are runtime data structures described in Section 3.2.2. T represents a thread, and TOUCH blocks until the given request completes. 5 Experimental Results We empirically evaluated DPA using two applications from the SPLASH-2 suite <ref> [38] </ref>, Barnes-Hut and FMM, on the Cray T3D [9], a distributed memory machine with support for one-sided remote put/get memory operations. We use Illinois Fast Messages (FM) [22, 26] for our message passing implementation.
References-found: 38

