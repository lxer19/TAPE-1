URL: ftp://ftp.cs.columbia.edu/reports/reports-1995/cucs-037-95.ps.gz
Refering-URL: http://www.cs.columbia.edu/~library/1995.html
Root-URL: http://www.cs.columbia.edu
Title: Estimating an Eigenvector by the Power Method with a Random Start  
Author: Gianna M. Del Corso 
Keyword: Key words. eigenvectors, power method, random start, randomized error.  
Pubnum: CUCS-037-95  
Abstract: This paper addresses the problem of approximating an eigenvector belonging to the largest eigenvalue of a symmetric positive definite matrix by the power method. We assume that the starting vector is randomly chosen with uniform distribution over the unit sphere. This paper provides lower and upper as well as asymptotic bounds on the randomized error in the L p sense, p 2 [1; +1]. We prove that it is impossible to achieve bounds that are independent of the ratio between the two largest eigenvalues. This should be contrasted to the problem of approximating the largest eigenvalue for which Kuczynski and Wozniakowski in 1992 proved that it is possible to bound the randomized error at the k-th step with a quantity that depends only on k and on the size of the matrix. We prove that the rate of convergence depends on the ratio of the two largest eigenvalues, on their multiplicities, and on the particular norm. The rate of convergence is at most linear in the ratio of the two largest eigenvalues. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> I. S. Gradshteyn and I. M. Ryzhik. </author> <title> Table of Integrals, Series, and Products. </title> <publisher> Academic Press, </publisher> <year> 1994. </year>
Reference-contexts: we consider the acute angle ff k = ff k (b) between the vector computed by the power method at the k-th step and the eigenspace corresponding to the largest eigenvalue, and we study the expectation of sin (ff k (b)) over b in the L p sense, p 2 <ref> [1; +1] </ref>. We first ask whether it is possible to get bounds on the randomized error that do not depend on the distribution of the eigenvalues. We prove, see Section 3, that for every k and p there are matrices for which the randomized error is very close to 1. <p> The speed of convergence is however poor for large p. In the paper we will denote by c i the measure of the unit ball over IR i . We have c i = (i=2 + 1) see <ref> [1] </ref> for the definition of the gamma function (x). We will also use the following relation between the beta and gamma functions B (i; j) = 2 0 (i) (j) : (11) We will denote by F (a; b; c; x) the hypergeometric function, see [1] for the definition and the <p> = (i=2 + 1) see <ref> [1] </ref> for the definition of the gamma function (x). We will also use the following relation between the beta and gamma functions B (i; j) = 2 0 (i) (j) : (11) We will denote by F (a; b; c; x) the hypergeometric function, see [1] for the definition and the properties of this function. 4 3 Worst Case Matrices In [3], Kuczynski and Wozniakowski considered the power method for approximating the largest eigenvalue 1 . <p> We apply twice formula [4.642] of <ref> [1] </ref> to reduce the last integral to the two dimensional integral and we get [e ran Z 1 Z 1 t n+pr1 b r1 (1 b 2 ) (n+pr)=2 Since b 2 + a 2 t 2 (1 b 2 ) b 2 , we have [e ran Z 1 t <p> We have [e ran a p Z 0 n X b 2 1 p=2 Z 1 i=1 b 2 i=p+1 b 2 p=2 db: Let jjbjj 2 = P n i . From formula [4.642] of <ref> [1] </ref>, we get [e ran c n B np jjbjj p Z p 1jjbjj 2 0 (t 2 + a 2 jjbjj 2 ) p=2 dt db: (16) We have two cases, p = r = 1 and p = r 2. <p> 1, (16) becomes [e ran 2 Z jjbjj Z p 1jjbjj 2 0 (t 2 + a 2 jjbjj 2 ) 1=2 dt db = a c n B n1 jjbjj ln p 1 jjbjj 2 + p 1 (1 a 2 )jjbjj 2 a jjbjj ! Using [4.642] of <ref> [1] </ref>, and observing that p 1 jjbjj 2 p 1 (1 a 2 )jjbjj 2 , we get [e ran Z 1 b n1 ln 2 1 (1 a 2 )b 2 ! fl ln 1 fl + a n 2 ; (17) where fl = (n 1)2c n1 =c n <p> Z p 1jjbjj 2 0 t p + p=2 t p2 a 2 jjbjj 2 dt db = a p p c p Z jjbjj p Z p 1jjbjj 2 0 t 2 + p=2 a 2 jjbjj 2 dt db: Solving the last integral, and using again [4.642] of <ref> [1] </ref> to reduce the first integral to a one-dimensional integral, we obtain [e ran Z 1 b n1 1 ln 1 (1 p=2a 2 )b 2 ! 8 = a p fl Z 1 b n1 ln 1 2 0 p a 2 b 2 db 2n pa 2 + a <p> Then we have Z d 2 y r=21 Z +1 y r=21 2 p r due to formula [3.194] of <ref> [1] </ref>. We apply [4.642] of [1] to reduce the integral over B nr to a one dimensional integral, and we get [e ran c n 0 2 p r ((n r)=2) (p=2) This concludes the proof. 2 Note that, when p = r, the bound is composed of two terms. <p> Then we have Z d 2 y r=21 Z +1 y r=21 2 p r due to formula [3.194] of <ref> [1] </ref>. We apply [4.642] of [1] to reduce the integral over B nr to a one dimensional integral, and we get [e ran c n 0 2 p r ((n r)=2) (p=2) This concludes the proof. 2 Note that, when p = r, the bound is composed of two terms. <p> Writing the last integral as an integral over the ball B nr and the r dimensional ball of radius q = q P n i , and applying [4:642] of <ref> [1] </ref>, we get [e ran rc r Z Z q t r1 a 2 b 2 t 2 + a 2 b 2 ! p=2 Let us denote a 2 b 2 r+1 by ff, and consider the integral f (ff) = 0 t 2 + ff dt: (22) We have <p> Let us start with p &lt; 2. Notice that (y + 1) p=2 y p=2 1. Then from (24) we get Z q g (t) dt 2 0 (y + 1) p=2 dy q rp F p ; 2 r p + 1; ff ; due to formula <ref> [3.194, 1] </ref> of [1] (see also [1] for the definition and the properties of the hypergeometric function F (a; b; c; x)). <p> Notice that (y + 1) p=2 y p=2 1. Then from (24) we get Z q g (t) dt 2 0 (y + 1) p=2 dy q rp F p ; 2 r p + 1; ff ; due to formula [3.194, 1] of <ref> [1] </ref> (see also [1] for the definition and the properties of the hypergeometric function F (a; b; c; x)). <p> Notice that (y + 1) p=2 y p=2 1. Then from (24) we get Z q g (t) dt 2 0 (y + 1) p=2 dy q rp F p ; 2 r p + 1; ff ; due to formula [3.194, 1] of <ref> [1] </ref> (see also [1] for the definition and the properties of the hypergeometric function F (a; b; c; x)). <p> + 1; ff : Hence, (21) becomes [e ran rc r Z a p b r+1 @ 1 i=r+1 i A db rc r Z a p b r+1 @ 1 i=r+1 i A F p ; 2 r p + 1; P n i r+1 db: Using [4.642] of <ref> [1] </ref>, we get [e ran (1=2) (r=2) a p (r + 1) ((r + 1)=2) Z 1 t p (1 t 2 ) (rp)=2 F p ; 2 r p + 1; a 2 t 2 dt: (25) After setting y = (1 t 2 )=(a 2 t 2 ), we <p> rewrite the integral in (25) as Z 1 t p (1 t 2 ) (rp)=2 F p ; 2 r p + 1; a 2 t 2 dt = a p1 Z 1 y (rp)=2 2 r p ; 2 &gt;From the last equation and using formula [7.512, 10] of <ref> [1] </ref>, we have Z 1 t p (1 t 2 ) (rp)=2 F p ; 2 r p + 1; a 2 t 2 dt = a p1 ((r p)=2 + 1) (p + 1=2) ((r + 1)=2) F p + 2 r + 1 ; 2 1 Applying transformation formula <p> p (1 t 2 ) (rp)=2 F p ; 2 r p + 1; a 2 t 2 dt = a p1 ((r p)=2 + 1) (p + 1=2) ((r + 1)=2) F p + 2 r + 1 ; 2 1 Applying transformation formula to the hypergeometric function, see <ref> [9.131, 1] </ref> of [1], we have F p + 2 r + 1 ; 2 1 2 r p ; 2 Substituting it into (26) and then into (25), we get [e ran (1=2) (r=2) where fl = 2 (r=2) (1=2) ((r + p + 1)=2) 2 r p ; 2 <p> 2 ) (rp)=2 F p ; 2 r p + 1; a 2 t 2 dt = a p1 ((r p)=2 + 1) (p + 1=2) ((r + 1)=2) F p + 2 r + 1 ; 2 1 Applying transformation formula to the hypergeometric function, see [9.131, 1] of <ref> [1] </ref>, we have F p + 2 r + 1 ; 2 1 2 r p ; 2 Substituting it into (26) and then into (25), we get [e ran (1=2) (r=2) where fl = 2 (r=2) (1=2) ((r + p + 1)=2) 2 r p ; 2 12 This concludes <p> Since ~ p=21 (y + 1) p=21 , we obtain the bound Z q g (t) dt 4 0 y + 1 = q rp p F 1; 2 r p + 1; ff (27) which follows from formula <ref> [3.194, 1] </ref> of [1]. <p> Since ~ p=21 (y + 1) p=21 , we obtain the bound Z q g (t) dt 4 0 y + 1 = q rp p F 1; 2 r p + 1; ff (27) which follows from formula [3.194, 1] of <ref> [1] </ref>. <p> Z a p b r+1 @ 1 i=r+1 i A db rpc r Z a p b r+1 @ 1 i=r+1 i A F 1; 2 r p + 1; P n i r+1 db: (28) Solving the integral in (28) as before, and applying the transformation formula [9.131] of <ref> [1] </ref> to the hypergeometric function, we have [e ran (1=2) (r=2) where fl = 2 (r + 1) (r=2) (1=2) 2 r p ; 2 This concludes the proof for p &lt; r. Let p = r. <p> Since ff = a 2 b 2 2 , using [4.642] of <ref> [1] </ref>, we have [e ran 2 Z Z 1 t 1 jjbjj 2 1=2 a 2 t 2 dt db (n 2)c n2 B n 1; 2 0 1=2a 2 t 2 dt (n 2)c n2 B n 1; 2 1 ln 2 (n 2)c n2 B n 1; 2 ln <p> Then from the definition of ff and using [4.642] of <ref> [1] </ref>, we have [e ran c n B np1 0 t p (1 jjbjj 2 ) (p+1)=2 ln p=2 a 2 t 2 dt db c n p 1 B np1 0 t p (1 jjbjj 2 ) (p+1)=2 (1 t 2 ) 1 (1 a 2 )t 2 ) Using <p> Hence, from (32) and using formula [3.197, 3] of <ref> [1] </ref> to solve the integral in (31), we have [e ran p + 1 pa 2 (p + 3) p + 1 ; 2 p + 1 where fl 0 = flB ( (n p 1)=2; (p + 1)=2 + 1). <p> 2 0 (y + 1) p=2 dy: It can be rewritten as f (ff) = 2 0 (y + 1) p=2 dy q 2 =ff (y + 1) p=2 dy : (34) The first integral of the right hand side of (34) can be solved using formula [3.194, 3] of <ref> [1] </ref> and is equal to B (r=2; (p r)=2). The second integral of (34) can be solved using formula [3.194, 2] of [1] and is equal to q 2 p r 2 p r ; 2 ff Hence, (34) becomes f (ff) = 2 2 p r ff p=2 2 p <p> q 2 =ff (y + 1) p=2 dy : (34) The first integral of the right hand side of (34) can be solved using formula [3.194, 3] of <ref> [1] </ref> and is equal to B (r=2; (p r)=2). The second integral of (34) can be solved using formula [3.194, 2] of [1] and is equal to q 2 p r 2 p r ; 2 ff Hence, (34) becomes f (ff) = 2 2 p r ff p=2 2 p r ; 2 ff By substituting (35) into (21), and from the definition of ff and q we have [e ran 2c <p> setting z = t 2 =(1 t 2 ), we obtain Z 1 t p 2 p r ; 2 1 t 2 dt = 1 Z 1 z (p1)=2 2 p r ; 2 We notice that z (p1)=2 z (pr)=2 Using this inequality and formula [7.51, 10] of <ref> [1] </ref>, (37) can be bounded as follows 1 Z 1 z (p1)=2 2 p r ; 2 2a pr2 0 (a 2 + z) 2 F p ; 2 p r + 1; z dz 1 ((p r)=2 + 1) (r=2 + 1) F r + 1; 1; 2 Substituting (38) <p> balls B r and B s , we have [e ran 1 + o (1) c n B r B s jjtjj p (1 jjbjj 2 ) (n+pr)=2 (1 jjtjj 2 ) (nrs)=2 [jjbjj 2 + a 2 jjtjj 2 (1 jjbjj 2 )] p=2 dt db: Using [4.642] of <ref> [1] </ref> for both integrals, we get [e ran 1 + o (1) = a p fl 0 0 [b 2 + a 2 t 2 (1 b 2 )] p=2 dt db Z 1 t p+s1 (1 t 2 ) (nrs)=2 0 [b 2 + a 2 t 2 (1 b <p> 0 r n+pr (z + 1) 2 Z 1 0 r n+pr (z + 1) 2 Z 1 0 r (z + 1) 2 Since lim Z 1=(at) z r=21 (1 at) (n+pr)=2 a!0 0 (z + 1) p=2 dz; passing to the limit and then using [3.194, 3] of <ref> [1] </ref>, we get = lim Z 1=(a 2 t 2 ) z r=21 Z +1 z r=21 2 p r Hence, we also have lim Z 1=(a 2 t 2 ) z r=21 (1 a 2 t 2 z) (n+pr)=2 2 p r From (43), we get lim [e ran x <p> We have analyzed the L p norm of sin (ff k ()), for p 2 <ref> [1; +1] </ref>. We have shown that, if the starting vector b is chosen according to the uniform distribution over the unit sphere, the rate of convergence depends on the ratio between the two largest eigenvalues.
Reference: [2] <author> E. Kostlan. </author> <title> Statistical Complexity of Dominant Eigenvector Calculation. </title> <journal> J. of Complexity, </journal> <volume> 7 </volume> <pages> 371-379, </pages> <year> 1991. </year>
Reference-contexts: Shub shows, however, that even for n = 2 there are matrices for which this problem is very hard. In our paper we apply the power method to the matrix A and we are only interested in approximating a largest eigenvector. Wright [8] and Kostlan <ref> [2] </ref> analyzed the problem of approximating a largest eigenvector by the power method in a different setting. They considered the average case setting over a class of matrices, whereas we consider the randomized setting. <p> The second integral of (34) can be solved using formula <ref> [3.194, 2] </ref> of [1] and is equal to q 2 p r 2 p r ; 2 ff Hence, (34) becomes f (ff) = 2 2 p r ff p=2 2 p r ; 2 ff By substituting (35) into (21), and from the definition of ff and q we have
Reference: [3] <author> J. Kuczynski and H. Wozniakowski. </author> <title> Estimating the Largest Eigenvalue by the Power an Lanczos Algorithms with a Random Start. </title> <journal> SIAM J. Matrix Anal. Appl., </journal> <volume> 13 </volume> <pages> 1094-1122, </pages> <year> 1992. </year>
Reference-contexts: Address: Dipartimento di Matematica, Universita di Milano and IMC-CNR, Via Santa Maria 46, 56100 Pisa, Italy. E-mail: delcorso@iei.pi.cnr.it 1 been considered in <ref> [3] </ref>, where sharp upper bounds on the randomized relative error at each step are given. An important feature of these bounds is that they are independent of the distribution of the eigenvalues. The approach of our paper is similar to that of [3]. <p> E-mail: delcorso@iei.pi.cnr.it 1 been considered in <ref> [3] </ref>, where sharp upper bounds on the randomized relative error at each step are given. An important feature of these bounds is that they are independent of the distribution of the eigenvalues. The approach of our paper is similar to that of [3]. We analyze the convergence of the power method for approximating a largest eigenvector when the starting vector b is randomly chosen with uniform distribution over the unit sphere of the n dimensional space. <p> Then the L p norm of the function sin (ff k ()), defined as in (3), is given by jj sin (ff k ())jj p = S n jsin (ff k (b))j p (db) 1=p &gt;From Remark 7.2 of <ref> [3] </ref>, we have Z jsin (ff k (b))j p (db) = 1 Z jsin (ff k (b))j p db; (6) where c n is the Lebesgue's measure of the unit ball B n = fb : jjbjj 1g, see (10) for the definition of c n . <p> also use the following relation between the beta and gamma functions B (i; j) = 2 0 (i) (j) : (11) We will denote by F (a; b; c; x) the hypergeometric function, see [1] for the definition and the properties of this function. 4 3 Worst Case Matrices In <ref> [3] </ref>, Kuczynski and Wozniakowski considered the power method for approximating the largest eigenvalue 1 . They proved that the randomized error after k steps is bounded by a quantity that goes to zero as ln (n)=k independently on the distribution of the eigenvalues. <p> Hence, from (32) and using formula <ref> [3.197, 3] </ref> of [1] to solve the integral in (31), we have [e ran p + 1 pa 2 (p + 3) p + 1 ; 2 p + 1 where fl 0 = flB ( (n p 1)=2; (p + 1)=2 + 1). <p> f (ff) = 2 0 (y + 1) p=2 dy: It can be rewritten as f (ff) = 2 0 (y + 1) p=2 dy q 2 =ff (y + 1) p=2 dy : (34) The first integral of the right hand side of (34) can be solved using formula <ref> [3.194, 3] </ref> of [1] and is equal to B (r=2; (p r)=2). <p> 1) Z 1 0 r n+pr (z + 1) 2 Z 1 0 r n+pr (z + 1) 2 Z 1 0 r (z + 1) 2 Since lim Z 1=(at) z r=21 (1 at) (n+pr)=2 a!0 0 (z + 1) p=2 dz; passing to the limit and then using <ref> [3.194, 3] </ref> of [1], we get = lim Z 1=(a 2 t 2 ) z r=21 Z +1 z r=21 2 p r Hence, we also have lim Z 1=(a 2 t 2 ) z r=21 (1 a 2 t 2 z) (n+pr)=2 2 p r From (43), we get lim <p> Moreover, the uniform distribution on the unit sphere of the vectors b implies the same distribution of vectors Q T b. So, without loss of generality, we can restrict ourselves only to consider diagonal matrices, see also <ref> [3] </ref> and [4]. Vectors uniformly distributed over the unit sphere can be generated as described in [3] and [4]. The tests were performed on a Sun SPARCsystem 10 using double precision. To compute the values of the hypergeometric and the gamma functions we used the program Mathematica. <p> So, without loss of generality, we can restrict ourselves only to consider diagonal matrices, see also <ref> [3] </ref> and [4]. Vectors uniformly distributed over the unit sphere can be generated as described in [3] and [4]. The tests were performed on a Sun SPARCsystem 10 using double precision. To compute the values of the hypergeometric and the gamma functions we used the program Mathematica. We tested many different matrices of size 100 with the distributions of the eigenvalues chosen as in [4]. <p> We stress that our results hold for a class of norms and that they show how the specific norm affects the speed of convergence. Our bounds depend on the distribution of the eigenvalues, and we have proven that this is unavoidable. Comparing with results of <ref> [3] </ref>, we conclude that approximating a largest eigenvector by the power method is more difficult than approximating the largest eigenvalue in the randomized setting. 8 Acknowledgments I wish to thank Henryk Wozniakowski for the guidance and for the valuable help provided during all the stages of this work.
Reference: [4] <author> J. Kuczynski and H. Wozniakowski. </author> <title> Probabilistic Bounds on the Extremal Eigenvalues and Condition Number by the Lanczos Algorithm. </title> <journal> SIAM J. Matrix Anal. Appl., </journal> <volume> 15 </volume> <pages> 672-691, </pages> <year> 1994. </year> <month> 25 </month>
Reference-contexts: Moreover, the uniform distribution on the unit sphere of the vectors b implies the same distribution of vectors Q T b. So, without loss of generality, we can restrict ourselves only to consider diagonal matrices, see also [3] and <ref> [4] </ref>. Vectors uniformly distributed over the unit sphere can be generated as described in [3] and [4]. The tests were performed on a Sun SPARCsystem 10 using double precision. To compute the values of the hypergeometric and the gamma functions we used the program Mathematica. <p> So, without loss of generality, we can restrict ourselves only to consider diagonal matrices, see also [3] and <ref> [4] </ref>. Vectors uniformly distributed over the unit sphere can be generated as described in [3] and [4]. The tests were performed on a Sun SPARCsystem 10 using double precision. To compute the values of the hypergeometric and the gamma functions we used the program Mathematica. We tested many different matrices of size 100 with the distributions of the eigenvalues chosen as in [4]. <p> described in [3] and <ref> [4] </ref>. The tests were performed on a Sun SPARCsystem 10 using double precision. To compute the values of the hypergeometric and the gamma functions we used the program Mathematica. We tested many different matrices of size 100 with the distributions of the eigenvalues chosen as in [4].
Reference: [5] <author> B. N. Parlett. </author> <title> The Symmetric Eigenvalue Problem. </title> <publisher> Prentice-Hall, </publisher> <address> Englewood Cliffs, </address> <year> 1980. </year>
Reference-contexts: We will denote by Z the eigenspace corresponding to 1 . We recall that the power method is defined as follows, see e.g. <ref> [5] </ref>. Let u 0 = b be any nonzero starting vector. Then, for every k = 1; 2; : : :, we construct the following sequences of vectors ( u k = y k =jjy k jj; where jj jj is the Euclidean vector norm. <p> Otherwise, u k converges to a vector of Z and the angle ff k goes to zero as k goes to infinity. The analysis of the power method for a fixed starting vector b may be found in many books, see for example <ref> [5] </ref> and [7], where in particular one finds that, if the method converges, the rate convergence is r+1 = 1 . As already mentioned, we study the randomized error of sin (ff k ()) in the L p sense.
Reference: [6] <author> M. Shub. </author> <title> The Geometry and Topology of Dynamical Systems and Algorithms for Numerical Problems. </title> <booktitle> In Proc. of the 1983 Beijing Symposium on Differential Geometry and Differential Equations, </booktitle> <editor> Ed. Liao Shantao. </editor> <publisher> Science Press, </publisher> <address> Beijing, China, </address> <year> 1986. </year>
Reference-contexts: For p = +1, the power method has the randomized error equal to one for all k. We briefly comment on related work on approximate computation of eigenvectors. The idea of using random starting vectors for the power method can be found in the paper of Shub <ref> [6] </ref>. Shub applies the power method to the matrix e A , and approximates an eigenvector of A which is not necessarily a largest eigenvector. Although for this problem the power method is globally convergent, the random start is used to improve efficiency.
Reference: [7] <author> J. H. Wilkinson. </author> <title> The Algebraic Eigenvalue Problem. </title> <publisher> Oxford University Press, </publisher> <address> London, </address> <year> 1965. </year>
Reference-contexts: Otherwise, u k converges to a vector of Z and the angle ff k goes to zero as k goes to infinity. The analysis of the power method for a fixed starting vector b may be found in many books, see for example [5] and <ref> [7] </ref>, where in particular one finds that, if the method converges, the rate convergence is r+1 = 1 . As already mentioned, we study the randomized error of sin (ff k ()) in the L p sense.
Reference: [8] <author> P. E. Wright. </author> <title> Statistical Complexity of the Power Method for Markov Chains. </title> <journal> J. of Complexity, </journal> <volume> 5 </volume> <pages> 119-143, </pages> <year> 1989. </year>
Reference-contexts: Shub shows, however, that even for n = 2 there are matrices for which this problem is very hard. In our paper we apply the power method to the matrix A and we are only interested in approximating a largest eigenvector. Wright <ref> [8] </ref> and Kostlan [2] analyzed the problem of approximating a largest eigenvector by the power method in a different setting. They considered the average case setting over a class of matrices, whereas we consider the randomized setting.
References-found: 8

