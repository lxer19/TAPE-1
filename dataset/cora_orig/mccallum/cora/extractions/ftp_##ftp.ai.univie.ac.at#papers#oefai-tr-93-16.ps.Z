URL: ftp://ftp.ai.univie.ac.at/papers/oefai-tr-93-16.ps.Z
Refering-URL: http://www.ai.univie.ac.at/cgi-bin/tr-online/?number+93-16
Root-URL: 
Email: juffi@ai.univie.ac.at  
Title: Avoiding noise fitting in a Foil-like learning algorithm  
Author: Johannes Furnkranz 
Address: Schottengasse 3 A-1010 Vienna Austria  
Affiliation: Austrian Research Institute for Artificial Intelligence  
Abstract: The research reported in this paper describes Fossil, an ILP system that uses a search heuristic based on statistical correlation. This algorithm implements a new method for learning useful concepts in the presence of noise. In contrast to Foil's stopping criterion which allows theories to grow in complexity as the size of the training sets increase, we propose a new stopping criterion that is independent of the number of training examples. Instead, Fossil's stopping criterion depends on a search heuristic that estimates the utility of literals on a uniform scale.
Abstract-found: 1
Intro-found: 1
Reference: [Angluin and Laird, 1988] <author> D. Angluin and P. Laird. </author> <title> Learning from noisy examples. </title> <journal> Machine Learning, </journal> <volume> 2(4) </volume> <pages> 343-370, </pages> <year> 1988. </year>
Reference-contexts: Typing constraints were used to speed up the search and recursion was not allowed for efficiency reasons. Class noise in the training instances was generated according to the Classification Noise Process described in <ref> [Angluin and Laird, 1988] </ref>. In this model a noise level of means that the sign of each example is reversed with a probability of .
Reference: [Bosch, 1982] <author> Karl Bosch. </author> <title> Elementare Einf-uhrung in die angewandte Statistik. </title> <publisher> Friedr. Vieweg & Sohn, </publisher> <address> Braunschweig/M-unchen, 2nd edition, </address> <year> 1982. </year>
Reference-contexts: The correlation coefficient of two random variables X and Y is defined as corr (X; Y ) = E ((X X )(Y Y )) = X fi Y where and are expected value and standard deviation, respectively, of the random variables X and Y , and (see e.g. <ref> [Bosch, 1982] </ref>). This correlation coefficient measures the degree of dependency of two series of points on a scale from 1 (negative correlation) to +1 (positive correlation).
Reference: [Buntine and Niblett, 1992] <author> Wray Buntine and Tim Niblett. </author> <title> A further comparison of splitting rules for decision-tree induction. </title> <journal> Machine Learning, </journal> <volume> 8 </volume> <pages> 75-85, </pages> <year> 1992. </year>
Reference-contexts: c 0 (which in general will have a different size) and the process continues as described in [Quinlan, 1990]. 3 Important features of the correlation coefficient heuristic The information gain heuristic used in ID3 [Quinlan, 1983] and Foil has been extensively compared to other search heuristics in decision tree generation <ref> [Mingers, 1989, Buntine and Niblett, 1992] </ref> and Inductive Logic Programming [Lavrac et al., 1992]. The general consensus seems to be that it is hard to improve on this heuristic in terms of predictive accuracy in learning from noise-free data.
Reference: [Dzeroski and Bratko, 1992a] <author> Saso Dzeroski and Ivan Bratko. </author> <title> Handling noise in Induc tive Logic Programming. </title> <booktitle> In Proceedings of the International Workshop on Inductive Logic Programming, </booktitle> <address> Tokyo, Japan, </address> <year> 1992. </year>
Reference-contexts: In addition, mFoil and Foil both have to do additional calculations to determine when to stop learning | mFoil computes a statistical significance test <ref> [Dzeroski and Bratko, 1992a] </ref>, while Foil uses a heuristic based on the compression of the theory [Quinlan, 1990]. Fossil's simple cutoff method reduces the amount of additional computation to a mere comparison.
Reference: [Dzeroski and Bratko, 1992b] <author> Saso Dzeroski and Ivan Bratko. </author> <title> Using the m-estimate in Inductive Logic Programming. In Logical Approaches to Machine Learning, </title> <booktitle> Workshop Notes of the 10th European Conference on AI, </booktitle> <address> Vienna, Austria, </address> <year> 1992. </year>
Reference-contexts: Thus a noise level of in our experiments is roughly equivalent to a noise level of 2 in the results reported in <ref> [Lavrac and Dzeroski, 1992, Dzeroski and Bratko, 1992b] </ref>. Noise was added incrementally, i.e. instances which had a reversed sign at a noise level 1 also had a reversed sign at a noise level 2 &gt; 1 . <p> While both Foil and Fossil successively improve their predictive accuracy with increasing training set sizes, only Fossil converges towards a useful theory. 7 Related Work A comparison of the above findings to the relevant results reported for mFoil <ref> [Dzeroski and Bratko, 1992b] </ref> and LINUS [Lavrac and Dzeroski, 1992] would be interesting, but the results cited for the performance of Foil differ in all these papers. Considering the different noise model we are using, our results for Foil are significantly better than in both other papers.
Reference: [Dzeroski and Lavrac, 1991] <author> Saso Dzeroski and Nada Lavrac. </author> <title> Learning relations from noisy examples: An empirical comparison of LINUS and FOIL. </title> <booktitle> In Proceedings of the 8th International Workshop on Machine Learning, </booktitle> <pages> pages 399-402, </pages> <address> Evanston, Illinois, </address> <year> 1991. </year>
Reference: [Furnkranz, 1993] <author> Johannes Furnkranz. </author> <title> A numerical analysis of the KRK domain. </title> <note> Working Note, 1993. Available upon request. </note>
Reference-contexts: literal A "== C 5 This theory correctly classifies all but 4060 of the 262,144 possible domain examples (98.45%). 2940 positions (1.12%)with WK and WR on the same squares and 1120 positions (0.43%) where the WK is between WR and BK on the same row or file are erroneously classified <ref> [Furnkranz, 1993] </ref>. (Remember that we have defined adjacent to mean adjacent or equal). illegal (A,B,C,D,E,F) :- C = E. illegal (A,B,C,D,E,F) :- D = F. illegal (A,B,C,D,E,F) :- adjacent (A,E), adjacent (B,F). had been added to the first clause, which gives a 97.98% correct theory [Furnkranz, 1993]. <p> or file are erroneously classified <ref> [Furnkranz, 1993] </ref>. (Remember that we have defined adjacent to mean adjacent or equal). illegal (A,B,C,D,E,F) :- C = E. illegal (A,B,C,D,E,F) :- D = F. illegal (A,B,C,D,E,F) :- adjacent (A,E), adjacent (B,F). had been added to the first clause, which gives a 97.98% correct theory [Furnkranz, 1993]. Efficiency: Foil grows an increasing number of clauses with an increasing number of literals. Also, several of the literals chosen to fit the noise introduce new variables, which leads to an explosion of the size of the tuple set.
Reference: [Lavrac and Dzeroski, 1992] <author> Nada Lavrac and Saso Dzeroski. </author> <title> Inductive learning of relations from noisy examples. </title> <editor> In Stephen Muggleton, editor, </editor> <booktitle> Inductive Logic Programming, </booktitle> <pages> pages 495-516. </pages> <publisher> Academic Press Ltd., </publisher> <address> London, </address> <year> 1992. </year>
Reference-contexts: Thus a noise level of in our experiments is roughly equivalent to a noise level of 2 in the results reported in <ref> [Lavrac and Dzeroski, 1992, Dzeroski and Bratko, 1992b] </ref>. Noise was added incrementally, i.e. instances which had a reversed sign at a noise level 1 also had a reversed sign at a noise level 2 &gt; 1 . <p> While both Foil and Fossil successively improve their predictive accuracy with increasing training set sizes, only Fossil converges towards a useful theory. 7 Related Work A comparison of the above findings to the relevant results reported for mFoil [Dzeroski and Bratko, 1992b] and LINUS <ref> [Lavrac and Dzeroski, 1992] </ref> would be interesting, but the results cited for the performance of Foil differ in all these papers. Considering the different noise model we are using, our results for Foil are significantly better than in both other papers.
Reference: [Lavrac et al., 1992] <author> Nada Lavrac, Bojan Cestnik, and Saso Dzeroski. </author> <title> Search heuris tics in empirical inductive logic programming. In Logical Approaches to Machine Learning, </title> <booktitle> Workshop Notes of the 10th European Conference on AI, </booktitle> <address> Vienna, Austria, </address> <year> 1992. </year>
Reference-contexts: In the following description of its adaptation as a search heuristic for the Inductive Logic Programming algorithm Foil, we will follow the notational conventions used in <ref> [Lavrac et al., 1992] </ref>. Suppose Fossil has learned a partial clause c. Let the set of tuples T c of size n (c), containing n (c) positive and n (c) negative instances, be the current training set. <p> and the process continues as described in [Quinlan, 1990]. 3 Important features of the correlation coefficient heuristic The information gain heuristic used in ID3 [Quinlan, 1983] and Foil has been extensively compared to other search heuristics in decision tree generation [Mingers, 1989, Buntine and Niblett, 1992] and Inductive Logic Programming <ref> [Lavrac et al., 1992] </ref>. The general consensus seems to be that it is hard to improve on this heuristic in terms of predictive accuracy in learning from noise-free data. <p> We want to emphasize that this type of stopping criterion is not limited to Fossil's correlation coefficient heuristic, but may yield similar results with all search heuristics that assign values on a uniform scale, as e.g. the expected accuracy measure <ref> [Lavrac et al., 1992] </ref>. The first series of experiments aimed at determining an appropriate value for this parameter for further experimentation. 10 training sets of 100 instances each were used at three different noise levels (5%, 10% and 20%). 6 different settings for the cutoff parameter C were used.
Reference: [Mingers, 1989] <author> John Mingers. </author> <title> An empirical comparison of selection measures for decision-tree induction. </title> <journal> Machine Learning, </journal> <volume> 3 </volume> <pages> 319-342, </pages> <year> 1989. </year>
Reference-contexts: c 0 (which in general will have a different size) and the process continues as described in [Quinlan, 1990]. 3 Important features of the correlation coefficient heuristic The information gain heuristic used in ID3 [Quinlan, 1983] and Foil has been extensively compared to other search heuristics in decision tree generation <ref> [Mingers, 1989, Buntine and Niblett, 1992] </ref> and Inductive Logic Programming [Lavrac et al., 1992]. The general consensus seems to be that it is hard to improve on this heuristic in terms of predictive accuracy in learning from noise-free data.
Reference: [Muggleton et al., 1989] <author> Stephen Muggleton, Michael Bain, Jean Hayes-Michie, and Donald Michie. </author> <title> An experimental comparison of human and machine learning formalisms. </title> <booktitle> In Proceedings of the 6th International Workshop on Machine Learning, </booktitle> <pages> pages 113-118, </pages> <year> 1989. </year>
Reference-contexts: This paper reports experiments that confirm the last hypothesis. 4 Experimental setup For the experiments in this paper we have used the domain of recognizing illegal chess positions in the KRK ending <ref> [Muggleton et al., 1989] </ref>, which has become a running example in ILP research. The goal is to learn the concept of an illegal white-to-move position with only white king, white rook and black king being on the board.
Reference: [Quinlan, 1983] <author> J. Ross Quinlan. </author> <title> Learning efficient classification procedures and their application to chess end games. </title> <editor> In Ryszard S. Michalski, Jaime G. Carbonell, and Tom M. Mitchell, editors, </editor> <booktitle> Machine Learning. An Artificial Intelligence Approach, </booktitle> <pages> pages 463-482. </pages> <publisher> Tioga Publishing Co., </publisher> <year> 1983. </year>
Reference-contexts: The set T c is then extended to a new a set of tuples T c 0 (which in general will have a different size) and the process continues as described in [Quinlan, 1990]. 3 Important features of the correlation coefficient heuristic The information gain heuristic used in ID3 <ref> [Quinlan, 1983] </ref> and Foil has been extensively compared to other search heuristics in decision tree generation [Mingers, 1989, Buntine and Niblett, 1992] and Inductive Logic Programming [Lavrac et al., 1992].
Reference: [Quinlan, 1990] <author> John Ross Quinlan. </author> <title> Learning logical definitions from relations. </title> <journal> Ma chine Learning, </journal> <volume> 5 </volume> <pages> 239-266, </pages> <year> 1990. </year>
Reference-contexts: 1 Introduction In this paper we introduce an Inductive Logic Programming algorithm closely related to Foil <ref> [Quinlan, 1990] </ref>. Fossil uses a search heuristic based on statistical correlation. Advantages of this new heuristic are that there is no seperate calculation for negated literals and that the quality of literals is assessed on a uniform scale. <p> The set T c is then extended to a new a set of tuples T c 0 (which in general will have a different size) and the process continues as described in <ref> [Quinlan, 1990] </ref>. 3 Important features of the correlation coefficient heuristic The information gain heuristic used in ID3 [Quinlan, 1983] and Foil has been extensively compared to other search heuristics in decision tree generation [Mingers, 1989, Buntine and Niblett, 1992] and Inductive Logic Programming [Lavrac et al., 1992]. <p> The experiments reported in this paper ignored this problem by treating undefined cases as having correlation 0. Defining the heuristic value of determinate literals as 1 would put all determinate into the clause body. Irrelevant literals could be removed later in a post-processing phase <ref> [Quinlan, 1990] </ref>. <p> What seems to be responsible for the drastic increase in the complexity of the learned clauses is that Foil's stopping criterion <ref> [Quinlan, 1990] </ref> is dependent on the size of the training set. In the KRK domain it performs very well on sample sizes of 100 training examples. The more this number increases, the more bits are allowed for the theory to explain the data. <p> In addition, mFoil and Foil both have to do additional calculations to determine when to stop learning | mFoil computes a statistical significance test [Dzeroski and Bratko, 1992a], while Foil uses a heuristic based on the compression of the theory <ref> [Quinlan, 1990] </ref>. Fossil's simple cutoff method reduces the amount of additional computation to a mere comparison.
Reference: [Quinlan, 1991] <author> John Ross Quinlan. </author> <title> Determinate literals in inductive logic program ming. </title> <booktitle> In Proceedings of the 8th International Workshop on Machine Learning, </booktitle> <pages> pages 442-446, </pages> <year> 1991. </year>
Reference-contexts: Defining the heuristic value of determinate literals as 1 would put all determinate into the clause body. Irrelevant literals could be removed later in a post-processing phase [Quinlan, 1990]. Values between 0 and 1 result in the behavior proposed in <ref> [Quinlan, 1991] </ref>: Until a literal with a correlation above this pre-set value is found, determinate literals will be added to the clause body. * The value of Foil's evaluation function is dependent on the size of the tuple set.
Reference: [Srinivasan et al., 1992] <author> A. Srinivasan, S. H. Muggleton, and M. E. Bain. </author> <title> Distinguish ing noise from exceptions in non-monotonic learning. </title> <booktitle> In Proceedings of the International Workshop on Inductive Logic Programming, </booktitle> <address> Tokyo, Japan, </address> <year> 1992. </year>
Reference-contexts: Fossil's simple cutoff method reduces the amount of additional computation to a mere comparison. We currently work on repeating the test series described above on mFoil and on a version of Foil that uses a similar cutoff stopping criterion. <ref> [Srinivasan et al., 1992] </ref> report a series of similar experiments using CW-GOLEM. Here the results are directly comparable to our findings, as the same noise model has been used in both experiments.
References-found: 15

