URL: http://www.cs.indiana.edu/database/Ode/doc/att-db-92-2.ps.Z
Refering-URL: http://www.cs.indiana.edu/database/Ode/doc/
Root-URL: http://www.cs.indiana.edu
Email: biliris@research.att.com  
Title: The EOS Large Object Manager  
Author: Alexandros Biliris 
Keyword: database storage organization, object-oriented databases, computer-aided de sign, multimedia.  
Date: December 1992  
Address: Murray Hill, NJ 07974  
Affiliation: AT&T Bell Laboratories  
Abstract: Large objects are used in many unconventional database applications; pictures, digitized video and sound recordings are a few examples of large objects that need to be stored in a database. This paper discribes the large object manager of the EOS object store. We present the storage structures and algorithms used for the efficient manipulation of general-purpose large unstructured objects. The large object is stored in a sequence of variable-size segments, each of which consists of a large number of physically contiguous disk blocks. Disk space management is based on the binary buddy system. The scheme supports operations that replace, insert, delete bytes at arbitrary positions within the object, and append bytes at the end of the object. We analyze the performance of our scheme and compare it with the corresponding ones employed in two other experimental database storage systems. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Agrawal, R. and N.H. Gehani, </author> <title> "Ode (Object Database and Environment): The Language and the Data Model," </title> <booktitle> Proc. ACM-SIGMOD 1989 Int'l Conf. Management of Data, </booktitle> <address> Portland, Oregon, </address> <month> May </month> <year> 1989, </year> <pages> pp. 36-45. </pages>
Reference-contexts: The number of bytes stored in the subtree rooted at p [i] is c [i] c [i 1]. (Ordering of pairs starts with 0 and by convention c <ref> [1] </ref> = 0.) Thus, if the i-th pair of N is the rightmost pair, c [i] gives the total number of bytes stored below N , and with N being the root, this value provides the total object size. <p> Suppose we want to read 320 bytes starting from byte 1470 of the object shown in Figure 4.c. To locate byte B = 1470, we find that c <ref> [1] </ref> = 1820 is the smallest count of the root that is greater than 1470. Now we have to locate byte B = 1470 1020 = 450 in the child node pointed by p [1]. We repeat the same process in the child; i.e., we find that c [1] = 710 <p> To locate byte B = 1470, we find that c <ref> [1] </ref> = 1820 is the smallest count of the root that is greater than 1470. Now we have to locate byte B = 1470 1020 = 450 in the child node pointed by p [1]. We repeat the same process in the child; i.e., we find that c [1] = 710 is the smallest count greater than 450, and thus, we set S = p [1], and B = 450 c [0] = 450 280 = 170. <p> that c <ref> [1] </ref> = 1820 is the smallest count of the root that is greater than 1470. Now we have to locate byte B = 1470 1020 = 450 in the child node pointed by p [1]. We repeat the same process in the child; i.e., we find that c [1] = 710 is the smallest count greater than 450, and thus, we set S = p [1], and B = 450 c [0] = 450 280 = 170. Byte 170 is in page S + b170=100c = S + 1, at byte 70 within that page. <p> Now we have to locate byte B = 1470 1020 = 450 in the child node pointed by p <ref> [1] </ref>. We repeat the same process in the child; i.e., we find that c [1] = 710 is the smallest count greater than 450, and thus, we set S = p [1], and B = 450 c [0] = 450 280 = 170. Byte 170 is in page S + b170=100c = S + 1, at byte 70 within that page. We read pages S + 1 through S + 4 to retrieve the first (c [1] c [0]) (100 + 70) <p> we set S = p <ref> [1] </ref>, and B = 450 c [0] = 450 280 = 170. Byte 170 is in page S + b170=100c = S + 1, at byte 70 within that page. We read pages S + 1 through S + 4 to retrieve the first (c [1] c [0]) (100 + 70) = 260 of the desired bytes. Then, the (logically) next segment needs to be retrieved for the remaining 60 bytes. <p> Also, EOS does not impose any limit on the object size as Starburst does. The facilities of the EOS object store described in this paper have been implemented and they are fully operational. The EOS object store is being used as the storage engine of Ode prototype <ref> [1] </ref>, an object-oriented database system based on C++ [20]. Applications that use the large object management facilities of EOS include AI, CAD and multimedia. Acknowledgments I would like to thank Amir Samad who wrote the simulation driver and run some initial experiments.
Reference: [2] <editor> Astrahan M.M., et al, </editor> <title> "System R: Relational Approach to Database Management", </title> <journal> ACM Transactions on Database Systems, </journal> <volume> Vol. 1, No. 2, </volume> <month> June </month> <year> 1976, </year> <pages> pp. 97-137. </pages>
Reference-contexts: The algorithms that we present in this paper as well as those that appear in the literature can be used for both. 3 Review of Large Ob ject Management Techniques A number of solutions have been proposed to manage large objects <ref> [2, 12, 6, 3, 15] </ref>. They are either block-based or segment-based. Algorithms of the first kind store the large object in a number of single blocks [2, 12, 6]. <p> They are either block-based or segment-based. Algorithms of the first kind store the large object in a number of single blocks <ref> [2, 12, 6] </ref>. One the contrary, segment-based algorithms attempt to store the object in segments of physically adjacent blocks [3, 15]. 3.1 Block-based Algorithms The first mechanism for long fields of sizes up to 32 Kilobytes was implemented in the context of System R [2]. <p> One the contrary, segment-based algorithms attempt to store the object in segments of physically adjacent blocks [3, 15]. 3.1 Block-based Algorithms The first mechanism for long fields of sizes up to 32 Kilobytes was implemented in the context of System R <ref> [2] </ref>. The field was stored in a linear linked list of small data segments, each 255 bytes in length, with the long field descriptor pointing to the head of the list. There was no support for byte range reads or updates.
Reference: [3] <author> Carey, M. J., DeWitt, D. J., Richardson, J. E., Shekita, E. J., </author> <title> "Object and File Management in the EXODUS Extensible Database System," </title> <booktitle> Proc. of the 12th International Conference on Very Large Data Bases, </booktitle> <address> Kyoto, Japan, </address> <month> August </month> <year> 1986, </year> <pages> pp. 91-100. 39 </pages>
Reference-contexts: We analyze by means of simulation the performance of the proposed 1 Experimental Object Store. Also, the goddess of dawn in Greek mythology. 1 technique and we comparate it with the ones of Exodus <ref> [3] </ref> and Starburst [15]. Given this background we can now state the principle performance objectives of the large object manager of the EOS object store as follows: * Object creation (and deletion). <p> Thus, the segments that comprise the large object may have sizes that vary 2 drastically. A B-tree-like structure is built to index byte positions within the object. The data structure is identical to the one proposed in Exodus <ref> [3] </ref>. However, because the leaf nodes of the tree are variable-size segments, the EOS algorithms for insert, delete and append are significantly different than the corresponding algorithms of Exodus. Regarding our performance study, the work closer to ours is the one reported in [3]. <p> identical to the one proposed in Exodus <ref> [3] </ref>. However, because the leaf nodes of the tree are variable-size segments, the EOS algorithms for insert, delete and append are significantly different than the corresponding algorithms of Exodus. Regarding our performance study, the work closer to ours is the one reported in [3]. Our work differs from this effort in several ways. First, we employ a much more detailed model of reading and writing large data segments, the importance of which will be clear from our results. Second, we present results for object creation and sequential read time. Third, the work in [3] <p> <ref> [3] </ref>. Our work differs from this effort in several ways. First, we employ a much more detailed model of reading and writing large data segments, the importance of which will be clear from our results. Second, we present results for object creation and sequential read time. Third, the work in [3] analyzes the design decisions and tradeoffs of their own algorithm only while our study compares the design of three such algorithms. We know of no other work on the evaluation of large object management techniques and the relative performance of these algorithms is an open question. <p> The algorithms that we present in this paper as well as those that appear in the literature can be used for both. 3 Review of Large Ob ject Management Techniques A number of solutions have been proposed to manage large objects <ref> [2, 12, 6, 3, 15] </ref>. They are either block-based or segment-based. Algorithms of the first kind store the large object in a number of single blocks [2, 12, 6]. <p> They are either block-based or segment-based. Algorithms of the first kind store the large object in a number of single blocks [2, 12, 6]. One the contrary, segment-based algorithms attempt to store the object in segments of physically adjacent blocks <ref> [3, 15] </ref>. 3.1 Block-based Algorithms The first mechanism for long fields of sizes up to 32 Kilobytes was implemented in the context of System R [2]. <p> be slow because virtually every disk page fetch will most likely result in a disk seek. 3.2 Segment-based Algorithms The EXODUS Storage Manager (ESM), at the University of Wisconsin-Madison, is an database storage system with support for large objects of unlimited size on which all byte operations can be applied, <ref> [3, 4] </ref>. ESM stores large objects on data segments. The data segments are indexed by a B-tree-like structure. This is a dynamic structure in that it can gracefully support byte inserts and deletes. Exodus handles large objects of unlimited size. <p> There are no holes in each segment in that all of its pages must get filled up except the last one which may be partially full. These segments are then pointed to by a "positional" tree structure, whose internal nodes are identical to the ones proposed in <ref> [3] </ref>. <p> the reorganization is local to a single node and it is performed at the time this node is going to overflow. 5.6 Concurrency Control and Recovery Concurrency can be handled either by locking the root of the large object or, for finer granularity, the byte range affected by each operation <ref> [3] </ref>. In addition to locking the large object per se, there is the concurrency control problem associated with freeing a segment in that an update on the allocation status of a segment may propagate to its buddies. A comprehensive solution to this problem is provided in [15]. <p> The cost associated with such allocation (and possible swapping) is not considered in our simulation. Regarding ESM, there are two insert algorithms that appear in <ref> [3] </ref>, termed basic and improved. In the basic algorithm, when an overflow occurs during an insertion in a leaf block L, the bytes of L and the new bytes being inserted in L are evenly distributed in new leaves. <p> The improved algorithm attempts to redistribute the new bytes, the bytes of L and the bytes of one of L's neighbor, if in doing so the creation of a new leaf block is avoided. As it is shown in <ref> [3] </ref>, the improved algorithm leads to significant gains in storage utilization with minimal additional insert cost. <p> For example, if after the allocation (and subsequent update) of a 16-page leaf only the first 10 pages contain useful data, only these first 10 pages will be written to disk. In contrast, the preliminary results reported in <ref> [3] </ref> for the ESM prototype assumed leaf segments rather than single pages to be the unit of both read and write operations. 7 Performance Evaluation In this section we present the results of the performance evaluation of the three techniques. <p> First, in section 7.1, we present the values of the system and simulation parameters we have assumed throughout these experiments. In order to make direct comparison with the results reported in <ref> [3] </ref>, we have chosen these parameters to be the same with the ones used in [3]. Next, in sections 7.2 and 7.3, we present the cost results of creating a large object and then performing a sequential scan on the entire object, respectively. <p> First, in section 7.1, we present the values of the system and simulation parameters we have assumed throughout these experiments. In order to make direct comparison with the results reported in <ref> [3] </ref>, we have chosen these parameters to be the same with the ones used in [3]. Next, in sections 7.2 and 7.3, we present the cost results of creating a large object and then performing a sequential scan on the entire object, respectively. <p> The solution proposed in this paper satisfies all principle objectives set in the introductory section. The key characteristic of our scheme that made the above possible is the use of variable-size segments as opposed to 38 fixed-size segments used in <ref> [3] </ref> or segments of fixed pattern of growth used in [15]. Efficient manipulation of variable-length segments is facilitated by a disk space allocation mechanism based on the buddy system. We have examined the performance of the EOS large object manager as it is compared with the ones of Exodus [3] and <p> in <ref> [3] </ref> or segments of fixed pattern of growth used in [15]. Efficient manipulation of variable-length segments is facilitated by a disk space allocation mechanism based on the buddy system. We have examined the performance of the EOS large object manager as it is compared with the ones of Exodus [3] and Starburst [15]. To analyze the algorithms we measured object creation time, sequential scan time, storage utilization in the presence of updates, and the I/O cost of random reads, inserts, and deletes.
Reference: [4] <author> Carey, M. J., DeWitt, D. J., Richardson, J. E., Shekita, E. J., </author> <title> "Storage Management for Objects in EXODUS". In Object-Oriented Concepts, Databases, and Applications, </title> <editor> W.Kim and F.Lochovsky, eds., </editor> <publisher> Addison-Wesley, </publisher> <year> 1989. </year>
Reference-contexts: be slow because virtually every disk page fetch will most likely result in a disk seek. 3.2 Segment-based Algorithms The EXODUS Storage Manager (ESM), at the University of Wisconsin-Madison, is an database storage system with support for large objects of unlimited size on which all byte operations can be applied, <ref> [3, 4] </ref>. ESM stores large objects on data segments. The data segments are indexed by a B-tree-like structure. This is a dynamic structure in that it can gracefully support byte inserts and deletes. Exodus handles large objects of unlimited size.
Reference: [5] <institution> Communications of the ACM, </institution> <note> Special Issue on Digital Multimedia Systems, Vol. 34, No. 4, </note> <month> April </month> <year> 1991. </year>
Reference-contexts: Multimedia presentation are perhaps the most demanding applications as far as the efficient storage and retrieval of large objects is concerned. They require displaying images, showing movies, or playing digital sound recordings in real time <ref> [5] </ref>. Efficient manipulation of large objects is also important in any object-oriented and extended relational database management system, regardless of the application in which they are being used, in supporting general-purpose advanced data modeling constructs such as long arrays and long lists (aka "insertable" arrays).
Reference: [6] <author> Chou, H-T., D.J DeWitt, R.H. Katz, and A.C. Klug, </author> <title> "Design and Implementation of the Wisconsin Storage Systems," Software Practice and Experience, </title> <publisher> John Wiley & Sons, Vol. </publisher> <address> 15(10), </address> <month> October </month> <year> 1985, </year> <pages> pp. 943-962. </pages>
Reference-contexts: For example, O 2 a commercial object-oriented database system [7] uses the large object manager of the Wisconsin Storage System <ref> [6] </ref> to store large lists of data elements. The management of such objects imposes several requirements on the database storage system. First, ideally, objects of virtually unlimited size (within the bounds of the physical storage available) have to be supported. <p> The algorithms that we present in this paper as well as those that appear in the literature can be used for both. 3 Review of Large Ob ject Management Techniques A number of solutions have been proposed to manage large objects <ref> [2, 12, 6, 3, 15] </ref>. They are either block-based or segment-based. Algorithms of the first kind store the large object in a number of single blocks [2, 12, 6]. <p> They are either block-based or segment-based. Algorithms of the first kind store the large object in a number of single blocks <ref> [2, 12, 6] </ref>. One the contrary, segment-based algorithms attempt to store the object in segments of physically adjacent blocks [3, 15]. 3.1 Block-based Algorithms The first mechanism for long fields of sizes up to 32 Kilobytes was implemented in the context of System R [2]. <p> The maximum long field was limited to approximately 2 Gigabytes. The Wisconsin Storage System (WiSS) stores large objects in data segments called slices, <ref> [6] </ref>. A 1-level directory, containing the address and size of each slice, is stored as small record and thus, it may grow approximately to the size of a page. Each slice can be at most one page in length.
Reference: [7] <editor> Deux, O., et al, </editor> <title> "The Story of O 2 ," IEEE, </title> <journal> Trans. on Knowledge and Data Engineering, Special Issue on Database Prototype Systems, </journal> <volume> Vol. 2(1), </volume> <month> March </month> <year> 1990, </year> <pages> pp. 91-108. </pages>
Reference-contexts: For example, O 2 a commercial object-oriented database system <ref> [7] </ref> uses the large object manager of the Wisconsin Storage System [6] to store large lists of data elements. The management of such objects imposes several requirements on the database storage system.
Reference: [8] <author> DeWitt, D.J., D. Maier, P. Futtersack, and F. Velez, </author> <title> "A Study of Three Alternative Workstation-Server Architectures for Object-Oriented Database Systems," </title> <booktitle> Proc. 16th Int. Conference on Very Large Data Bases, </booktitle> <address> Brisbane, Australia, </address> <month> August </month> <year> 1990, </year> <pages> pp. 107-121. </pages>
Reference: [9] <author> Effelsberg W., Haerder T., </author> <title> "Principles of Database Buffer Management", </title> <journal> ACM Transactions on Database Systems, </journal> <volume> Vol. 9, No 4, </volume> <year> 1984, </year> <month> pp.560-595. </month>
Reference: [10] <author> Gray, J. N., and A. Rauter, </author> <title> Transaction Proccessing: Concepts and Techniques. </title> <publisher> Morgan Kayfmann, </publisher> <address> San Mateo, California, </address> <year> 1993. </year>
Reference-contexts: As in hierarchical locking, segments that are descendants of a locked segment are also locked, and thus they remain unallocated until the holding transaction releases the locks. For recovery, one or a combination of logging or shadowing may be used <ref> [10] </ref>. With logging, updates are performed in place after the old and new values of the updated item have been recorded to the log. <p> leaf segments, the log record of all updates must contain the operation that caused the update as well as its parameters, and the log sequence number of the update must be placed in the root page of the object to ensure that the update can be undone or redone idempotently <ref> [10] </ref>. 22 6 Prototyping the Large Ob ject Managers We want to compare the performance of the large object algorithms employeed in EOS, ESM and Starburst. <p> Thus, when few bytes need to be read from a segment, only those pages that contain the desired bytes are read, not the entire segment. Regarding updates, all three algorithms examined in this study assume some kind of shadowing for recovery <ref> [10] </ref>. Shadowing is a recovery technique in which a page is never overwritten; instead, a write is performed by allocating and writing a new page and leaving the old one intact until it is no longer needed for recovery.
Reference: [11] <author> Haas, L., et al, </author> <title> "Starburst Mid-Flight: As the Dust Clears," </title> <journal> IEEE, Trans. on Knowledge and Data Engineering, Special Issue on Database Prototype Systems, </journal> <volume> Vol. 2(1), </volume> <month> March </month> <year> 1990, </year> <pages> pp. 143-160. </pages>
Reference-contexts: Large pages waste too much space at the end of partially full pages (but offer good search time), and small pages offer good storage utilization (but require doing many I/O's for reads). Starburst is an extensible database system being developed at IBM's Almaden Research Center <ref> [11] </ref>. The Starburst long field manager, presented in [15], uses extent based allocation with extents being organized into a binary buddy system [13, 14].
Reference: [12] <author> Haskin, R. L., and Lorie, R. A., </author> <title> "On Extending the Relational Database System," </title> <booktitle> Proc. ACM-SIGMOD Int. Conf. on Management of Data, </booktitle> <year> 1982, </year> <month> pp.207-212. </month>
Reference-contexts: The algorithms that we present in this paper as well as those that appear in the literature can be used for both. 3 Review of Large Ob ject Management Techniques A number of solutions have been proposed to manage large objects <ref> [2, 12, 6, 3, 15] </ref>. They are either block-based or segment-based. Algorithms of the first kind store the large object in a number of single blocks [2, 12, 6]. <p> They are either block-based or segment-based. Algorithms of the first kind store the large object in a number of single blocks <ref> [2, 12, 6] </ref>. One the contrary, segment-based algorithms attempt to store the object in segments of physically adjacent blocks [3, 15]. 3.1 Block-based Algorithms The first mechanism for long fields of sizes up to 32 Kilobytes was implemented in the context of System R [2]. <p> The field was stored in a linear linked list of small data segments, each 255 bytes in length, with the long field descriptor pointing to the head of the list. There was no support for byte range reads or updates. This approach was later extended in <ref> [12] </ref>; long fields were stored as a sequence of 4K-byte pages with support for partial reads and updates. 4 The long field descriptor is an array of entries each containing the address and length of each page. The maximum long field was limited to approximately 2 Gigabytes.
Reference: [13] <author> Knuth D. E., </author> <booktitle> The Art of Computer Programming, </booktitle> <address> Addisson-Wesley, </address> <year> 1973. </year>
Reference-contexts: Ideally, storage utilization must be very close to 100%. A key component of EOS that makes the above performance goals realizable is its disk space allocation manager. It is based on the binary buddy system <ref> [13] </ref>. Strangely enough, although this scheme has been around for a long time and has been successfully used in file systems such as the Dartmouth Time Sharing System [14], it has been largely ignored by most designers of database storage systems (with Starburst [15] being an exception). <p> Starburst is an extensible database system being developed at IBM's Almaden Research Center [11]. The Starburst long field manager, presented in [15], uses extent based allocation with extents being organized into a binary buddy system <ref> [13, 14] </ref>. When the eventual size of a long field is not known in advance, successive segments allocated for storage double in size until the maximum segment size is reached; then, a sequence of maximum size segments is used until the entire long field is stored.
Reference: [14] <author> Kotch, </author> <title> P.D., "Disk File Allocation Based on The Buddy System," </title> <journal> ACM Trans. on Computer Systems, </journal> <volume> Vol. 5, No 4, </volume> <month> November </month> <year> 1987. </year>
Reference-contexts: It is based on the binary buddy system [13]. Strangely enough, although this scheme has been around for a long time and has been successfully used in file systems such as the Dartmouth Time Sharing System <ref> [14] </ref>, it has been largely ignored by most designers of database storage systems (with Starburst [15] being an exception). The buddy system performs fast allocation and deallocation of disk segments that differ in sizes by several orders of magnitude with minimal I/O and CPU cost. <p> Starburst is an extensible database system being developed at IBM's Almaden Research Center [11]. The Starburst long field manager, presented in [15], uses extent based allocation with extents being organized into a binary buddy system <ref> [13, 14] </ref>. When the eventual size of a long field is not known in advance, successive segments allocated for storage double in size until the maximum segment size is reached; then, a sequence of maximum size segments is used until the entire long field is stored.
Reference: [15] <author> Lehman, T.J., and B.G. Lindsay, </author> <title> "The Starburst Long Field Manager," </title> <booktitle> Proc. 15-th Int. Conference on Very Large Data Bases, </booktitle> <address> Amsterdam, The Netherlands, </address> <month> August </month> <year> 1989, </year> <pages> pp. 375-383. </pages>
Reference-contexts: We analyze by means of simulation the performance of the proposed 1 Experimental Object Store. Also, the goddess of dawn in Greek mythology. 1 technique and we comparate it with the ones of Exodus [3] and Starburst <ref> [15] </ref>. Given this background we can now state the principle performance objectives of the large object manager of the EOS object store as follows: * Object creation (and deletion). <p> Strangely enough, although this scheme has been around for a long time and has been successfully used in file systems such as the Dartmouth Time Sharing System [14], it has been largely ignored by most designers of database storage systems (with Starburst <ref> [15] </ref> being an exception). The buddy system performs fast allocation and deallocation of disk segments that differ in sizes by several orders of magnitude with minimal I/O and CPU cost. <p> The algorithms that we present in this paper as well as those that appear in the literature can be used for both. 3 Review of Large Ob ject Management Techniques A number of solutions have been proposed to manage large objects <ref> [2, 12, 6, 3, 15] </ref>. They are either block-based or segment-based. Algorithms of the first kind store the large object in a number of single blocks [2, 12, 6]. <p> They are either block-based or segment-based. Algorithms of the first kind store the large object in a number of single blocks [2, 12, 6]. One the contrary, segment-based algorithms attempt to store the object in segments of physically adjacent blocks <ref> [3, 15] </ref>. 3.1 Block-based Algorithms The first mechanism for long fields of sizes up to 32 Kilobytes was implemented in the context of System R [2]. <p> Starburst is an extensible database system being developed at IBM's Almaden Research Center [11]. The Starburst long field manager, presented in <ref> [15] </ref>, uses extent based allocation with extents being organized into a binary buddy system [13, 14]. <p> When the eventual size of the object is not known in advance, we follow the growth scheme used in <ref> [15] </ref>; successive segments allocated for storage double in size until the maximum segment size is reached. Then, a sequence of maximum size segments is used until the entire object is stored. <p> In addition to locking the large object per se, there is the concurrency control problem associated with freeing a segment in that an update on the allocation status of a segment may propagate to its buddies. A comprehensive solution to this problem is provided in <ref> [15] </ref>. When a segment is freed, a (release) lock is placed on the segment and an intention (release) lock is placed on all of the segment's ancestors. <p> The key characteristic of our scheme that made the above possible is the use of variable-size segments as opposed to 38 fixed-size segments used in [3] or segments of fixed pattern of growth used in <ref> [15] </ref>. Efficient manipulation of variable-length segments is facilitated by a disk space allocation mechanism based on the buddy system. We have examined the performance of the EOS large object manager as it is compared with the ones of Exodus [3] and Starburst [15]. <p> segments of fixed pattern of growth used in <ref> [15] </ref>. Efficient manipulation of variable-length segments is facilitated by a disk space allocation mechanism based on the buddy system. We have examined the performance of the EOS large object manager as it is compared with the ones of Exodus [3] and Starburst [15]. To analyze the algorithms we measured object creation time, sequential scan time, storage utilization in the presence of updates, and the I/O cost of random reads, inserts, and deletes.
Reference: [16] <author> Lohman, G.M, B. Lindsay, H. Pirahesh, </author> <title> and K.B. Schiefer, "Extensions to Starburst: Objects, Types, Functions, and Rules," </title> <journal> CACM, </journal> <volume> Vol. 34 (10), </volume> <month> October </month> <year> 1991, </year> <pages> pp. 94-109. </pages>
Reference-contexts: The maximum field size that can be supported depends on the possible segment sizes. The current implementation handles objects up to 1.5 gigabytes long <ref> [16] </ref>. This scheme can efficiently support (sequential and random) reads, appends, and byte range replace, but it cannot gracefully handle insertion (deletion) of bytes in (from) the middle of the object.
Reference: [17] <author> Mohan, C., </author> <title> "Commit LSN: A Novel and Simple Method for Reducing Locking and Latching in Transaction Processing Systems," </title> <booktitle> Proc. 16th Int. Conference on Very Large Data Bases, </booktitle> <address> Brisbane, Australia, </address> <month> August </month> <year> 1990, </year> <pages> pp. 406-418. 40 </pages>
Reference-contexts: Finally, it is worth mentioning that the superdirectory does not have to be transaction protected (otherwise, it would quickly become a hot spot). It is enough to hold a short duration lock (also called latch <ref> [17] </ref>) on the superdirectory during a read or update and release it right after this operation completes; i.e., the lock does not have to be held until the end of the transaction.
Reference: [18] <author> Seltzer, M., and M. Stonebraker, </author> <title> "Read Optimized File Sytem Designs: A Performance Evaluation," </title> <booktitle> Proc. 7th IEEE Int. Conference on Data Engineering, </booktitle> <address> Kobe, Japan, </address> <month> April </month> <year> 1991, </year> <pages> pp. 602-611. </pages>
Reference-contexts: Previous work on the performance of the buddy system confirms the above, but it also suggests that this allocation policy is prone to severe internal fragmentation <ref> [18] </ref>. Our design does not suffer from this problem because the unused portion of an allocated segment is always less than a page. When a large object is created it is stored in a sequence of large variable-size segments, each consisting of physically contiguous disk pages.
Reference: [19] <author> Stonebraker, M. et. al., </author> <title> "Managing Persistent Objects in a Multi-Level Store," </title> <institution> Electronics Research Laboratory, University of California at Berkeley, TR M91/16, </institution> <month> March </month> <year> 1991. </year>
Reference-contexts: Some applications may prefer the second view of objects because it is easier to treat the long fields within the same object in different ways; e.g., by applying different compression techniques in storing the picture and the voice attribute <ref> [19] </ref>. 3 The object representation may also depend on the data model that imposes its own constraints on the object layout.
Reference: [20] <author> Stroustrup, B., </author> <title> The C++ Programming Language Addison-Wesley, </title> <address> Andover, Mas-sachussets, </address> <year> 1986. </year> <month> 41 </month>
Reference-contexts: For instance, in an object-oriented database system based on C++ <ref> [20] </ref> a user has explicit control on wether an object contains another one or points to it; the class person would be defined as in one of the following two alternatives. class person - char name [30]; char picture [1M]; char voice [2M]; -; class person - char name [30]; char* <p> The facilities of the EOS object store described in this paper have been implemented and they are fully operational. The EOS object store is being used as the storage engine of Ode prototype [1], an object-oriented database system based on C++ <ref> [20] </ref>. Applications that use the large object management facilities of EOS include AI, CAD and multimedia. Acknowledgments I would like to thank Amir Samad who wrote the simulation driver and run some initial experiments.
References-found: 20

