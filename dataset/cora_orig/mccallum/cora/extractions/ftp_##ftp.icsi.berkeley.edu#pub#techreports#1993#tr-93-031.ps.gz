URL: ftp://ftp.icsi.berkeley.edu/pub/techreports/1993/tr-93-031.ps.gz
Refering-URL: http://www.icsi.berkeley.edu/techreports/1993.html
Root-URL: http://www.icsi.berkeley.edu
Title: On Some Stability Properties of the LRAAM Model  
Phone: 1-510-642-4274 FAX 1-510-643-7684  
Author: Alessandro Sperduti 
Date: July 1993  
Address: I 1947 Center Street Suite 600 Berkeley, California 94704  
Affiliation: INTERNATIONAL COMPUTER SCIENCE INSTITUTE  
Pubnum: TR-93-031  
Abstract: In this report we discuss some mathematical properties of the LRAAM model. The LRAAM model is an extension of the RAAM model by Pollack. It allows one to obtain distributed reduced representations of labeled graphs. In particular, we give sufficient conditions on the asymptotical stability of the decoding process along a cycle of the encoded structure. Data encoded in an LRAAM can also be accessed by content by transforming the LRAAM in an analog Hopfield network with hidden units and asymmetric connection matrix (CA network.) Different access procedures can be defined according to the access key. Each access procedure corresponds to a particular constrained version of the CA network. We give sufficient conditions under which the property of asymptotical stability of a fixed point in one particular constrained version of the CA network can be extended to related fixed points of different constrained versions of the CA network. An example of encoding of a labeled graph on which the theoretical results are applied is given as well. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> A. Atiya and Y. S. Abu-Mostafa. </author> <title> An analog feedback associative memory. </title> <journal> IEEE Transaction on Neural Networks, </journal> <volume> 4 </volume> <pages> 117-126, </pages> <year> 1993. </year>
Reference-contexts: In recent years, several results related to the analog Hopfield model have been published, both without considering hidden units [3, 8, 2, 14] and with hidden units <ref> [9, 7, 1] </ref>. In this report, we discuss conditions for the stability of equilibria arising in the LRAAM model. <p> Actually, this theorem is a discrete time adaptation of the Theorem 2 presented in <ref> [1] </ref> by Atiya and Abu-Mostafa, where the pointer transformations can be assimilated to different hidden layers of an appropriate network. Note that if at least one pointer, say ~ d (x) , belonging to a cycle has saturated components, then the cycle is guaranteed to be stable. <p> Several issues remain to be explored. First of all, the definition of a training procedure able to guarantee the asymptotical stability of the equilibria for every constrained version of the CA network and the asymptotical stability of the decoding process. Unfortunately, the procedure proposed by Atiya and Abu-Mostafa <ref> [1] </ref>, in order to guarantee at least the asymptotical stability of fixed points for the unconstrained CA network, cannot be applied since it requires the saturation of the hidden units.
Reference: [2] <author> J. A. Farrell and A. N. Michel. </author> <title> A synthesis procedure for Hopfield's continuous-time associative memory. </title> <journal> IEEE Transaction on Circuits and Systems, </journal> <volume> 37 </volume> <pages> 877-884, </pages> <year> 1990. </year>
Reference-contexts: In recent years, several results related to the analog Hopfield model have been published, both without considering hidden units <ref> [3, 8, 2, 14] </ref> and with hidden units [9, 7, 1]. In this report, we discuss conditions for the stability of equilibria arising in the LRAAM model.
Reference: [3] <author> A. Guez, V. Protopopsecu, and J. Barhen. </author> <title> On the stability, storage capacity, and design of nonlinear continuous neural networks. </title> <journal> IEEE Transactions on Systems, Man, and Cybernetics, </journal> <volume> 18 </volume> <pages> 80-87, </pages> <year> 1988. </year>
Reference-contexts: In recent years, several results related to the analog Hopfield model have been published, both without considering hidden units <ref> [3, 8, 2, 14] </ref> and with hidden units [9, 7, 1]. In this report, we discuss conditions for the stability of equilibria arising in the LRAAM model.
Reference: [4] <author> G. E. Hinton. </author> <title> Mapping part-whole hierarchies into connectionist networks. </title> <journal> Artificial Intelligence, </journal> <volume> 46 </volume> <pages> 47-75, </pages> <year> 1990. </year>
Reference-contexts: 1 Introduction The concept of distributed reduced representations was introduced by Hinton <ref> [4] </ref> in order to allow a neural network to represent compositional structure (see also [12, 15, 17]). Concrete examples of distributed reduced representations are given by the Recursive Auto-Associative Memory (RAAM) by Pollack [11, 12] and by the Holographic Reduced Representations of Plate [10].
Reference: [5] <author> J. J. </author> <title> Hopfield. Neurons with graded response have collective computational properties like those of two-state neurons. </title> <booktitle> In Proc. </booktitle> <institution> Natl. Acad. Sci., </institution> <month> 81 </month> <pages> 3088-3092, </pages> <year> 1984. </year>
Reference-contexts: Another advantage of the LRAAM model is that it allows one to retrieve information both by using the reduced representations and by content. The last capability is obtained by exploiting the structure of the LRAAM network which can be easily transformed in an analog Hopfield network <ref> [5] </ref> with hidden units. Because of the structure of each pattern in the training set, several constrained versions of the modified LRAAM network can be used, according to the access key, in order to improve the retrieval by content.
Reference: [6] <author> J. D. Keeler. </author> <title> Basins of attraction of neural network models. </title> <booktitle> In AIP Conference Proceedings, </booktitle> <volume> 151 </volume> <pages> 259-265, </pages> <year> 1986. </year>
Reference-contexts: The next step will be the attempt to perform an analysis of a basin of attraction by the technique used by Michel and Farrell in [8]. Moreover, in order to improve the shape of the domain of attraction, an adaptation of the unlearning method exploited by Keeler <ref> [6] </ref> for binary Hopfield networks will be tested. Spurious equilibria are not frequent in the unconstrained CA network. This can be a consequence of the fact that the fixed points are not completely defined by the user, since the actual value for the pointers is generated by the network.
Reference: [7] <author> B. Kosko. </author> <title> Neural Networks and Fuzzy Systems. </title> <publisher> Prentice Hall, </publisher> <year> 1992. </year>
Reference-contexts: In recent years, several results related to the analog Hopfield model have been published, both without considering hidden units [3, 8, 2, 14] and with hidden units <ref> [9, 7, 1] </ref>. In this report, we discuss conditions for the stability of equilibria arising in the LRAAM model.
Reference: [8] <author> A. N. Michel, J. A. Farrell, and W. Porod. </author> <title> Qualitative analysis of neural networks. </title> <journal> IEEE Transaction on Circuits and Systems, </journal> <volume> 36 </volume> <pages> 229-243, </pages> <year> 1989. </year> <month> 17 </month>
Reference-contexts: In recent years, several results related to the analog Hopfield model have been published, both without considering hidden units <ref> [3, 8, 2, 14] </ref> and with hidden units [9, 7, 1]. In this report, we discuss conditions for the stability of equilibria arising in the LRAAM model. <p> Another research issue is the definition of the shape of the basin of attraction of the equilibria of the CA network. The next step will be the attempt to perform an analysis of a basin of attraction by the technique used by Michel and Farrell in <ref> [8] </ref>. Moreover, in order to improve the shape of the domain of attraction, an adaptation of the unlearning method exploited by Keeler [6] for binary Hopfield networks will be tested. Spurious equilibria are not frequent in the unconstrained CA network.
Reference: [9] <author> F. J. Pineda. </author> <title> Dynamics and architecture for neural computation. </title> <journal> Journal of Complexity, </journal> <volume> 4 </volume> <pages> 216-245, </pages> <year> 1988. </year>
Reference-contexts: In recent years, several results related to the analog Hopfield model have been published, both without considering hidden units [3, 8, 2, 14] and with hidden units <ref> [9, 7, 1] </ref>. In this report, we discuss conditions for the stability of equilibria arising in the LRAAM model.
Reference: [10] <author> T. </author> <title> Plate. Holographic reduced representations. </title> <type> Technical Report CRG-TR-91-1, </type> <institution> Department of Computer Science, University of Toronto, </institution> <year> 1991. </year>
Reference-contexts: Concrete examples of distributed reduced representations are given by the Recursive Auto-Associative Memory (RAAM) by Pollack [11, 12] and by the Holographic Reduced Representations of Plate <ref> [10] </ref>. In particular, the RAAM model is able to generate reduced representations of lists and fixed-valence trees with information stored in the leaves. The Labeling RAAM model (LRAAM) [16] has extended the RAAM model, allowing the synthesis of distributed reduced representations for fixed-valence labeled graphs.
Reference: [11] <author> J. B. Pollack. </author> <title> Implications of Recursive Distributed Representations, </title> <booktitle> pages 527-536. Advances in Neural Information Processing Systems I. </booktitle> <address> San Mateo: </address> <publisher> Morgan Kaufmann, </publisher> <year> 1989. </year>
Reference-contexts: 1 Introduction The concept of distributed reduced representations was introduced by Hinton [4] in order to allow a neural network to represent compositional structure (see also [12, 15, 17]). Concrete examples of distributed reduced representations are given by the Recursive Auto-Associative Memory (RAAM) by Pollack <ref> [11, 12] </ref> and by the Holographic Reduced Representations of Plate [10]. In particular, the RAAM model is able to generate reduced representations of lists and fixed-valence trees with information stored in the leaves.
Reference: [12] <author> J. B. Pollack. </author> <title> Recursive distributed representations. </title> <journal> Artificial Intelligence, </journal> <volume> 46(1-2):77-106, </volume> <year> 1990. </year>
Reference-contexts: 1 Introduction The concept of distributed reduced representations was introduced by Hinton [4] in order to allow a neural network to represent compositional structure (see also <ref> [12, 15, 17] </ref>). Concrete examples of distributed reduced representations are given by the Recursive Auto-Associative Memory (RAAM) by Pollack [11, 12] and by the Holographic Reduced Representations of Plate [10]. <p> 1 Introduction The concept of distributed reduced representations was introduced by Hinton [4] in order to allow a neural network to represent compositional structure (see also [12, 15, 17]). Concrete examples of distributed reduced representations are given by the Recursive Auto-Associative Memory (RAAM) by Pollack <ref> [11, 12] </ref> and by the Holographic Reduced Representations of Plate [10]. In particular, the RAAM model is able to generate reduced representations of lists and fixed-valence trees with information stored in the leaves.
Reference: [13] <author> D. E. Rumelhart and J. L. McClelland. </author> <title> Parallel Distributed Processing: Explorations in the Microstructure of Cognition. </title> <publisher> MIT Press, </publisher> <year> 1986. </year>
Reference-contexts: Conclusions are drawn in Section 7. 2 The LRAAM model The Labeling RAAM (LRAAM) is an extension of the RAAM model which allows one to encode labeled structures. The general structure of the network for an LRAAM is shown in Figure 1. The network is trained by backpropagation <ref> [13] </ref> to learn the identity function.
Reference: [14] <author> F. M. A. Salam, Y. Wang, and M-R. Choi. </author> <title> On the analysis of dynamic feedback neural nets. </title> <journal> IEEE Transaction on Circuits and Systems, </journal> <volume> 38 </volume> <pages> 196-201, </pages> <year> 1991. </year>
Reference-contexts: In recent years, several results related to the analog Hopfield model have been published, both without considering hidden units <ref> [3, 8, 2, 14] </ref> and with hidden units [9, 7, 1]. In this report, we discuss conditions for the stability of equilibria arising in the LRAAM model.
Reference: [15] <author> P. Smolensky. </author> <title> Tensor product variable binding and the representation of symbolic structures in connectionist systems. </title> <journal> Artificial Intelligence, </journal> <volume> 46 </volume> <pages> 159-216, </pages> <year> 1990. </year>
Reference-contexts: 1 Introduction The concept of distributed reduced representations was introduced by Hinton [4] in order to allow a neural network to represent compositional structure (see also <ref> [12, 15, 17] </ref>). Concrete examples of distributed reduced representations are given by the Recursive Auto-Associative Memory (RAAM) by Pollack [11, 12] and by the Holographic Reduced Representations of Plate [10].
Reference: [16] <author> A. Sperduti. </author> <title> Labeling RAAM. </title> <type> Technical Report 93-029, </type> <institution> International Computer Science Institute, </institution> <year> 1993. </year>
Reference-contexts: In particular, the RAAM model is able to generate reduced representations of lists and fixed-valence trees with information stored in the leaves. The Labeling RAAM model (LRAAM) <ref> [16] </ref> has extended the RAAM model, allowing the synthesis of distributed reduced representations for fixed-valence labeled graphs. Another advantage of the LRAAM model is that it allows one to retrieve information both by using the reduced representations and by content. <p> A discussion about this kind of distribution for the gains of the units can be found in <ref> [16] </ref>. The initial weights for the PA network were initialized with real numbers uniformly distributed in the interval [-0.6,0.6]. <p> the fact that, under the hypotheses of perfect learning, linear output units, and representation of a cycle of length k in the same pointer field p, the eigenvalues of the matrix (D (p) ) k , corresponding 16 to the pointers of the cycle, must be equal to 1 (see <ref> [16] </ref>).
Reference: [17] <author> D. S. Touretzky. Boltzcons: </author> <title> Dynamic symbol structures in a connectionist network. </title> <journal> Artificial Intellicence, </journal> <volume> 46 </volume> <pages> 5-46, </pages> <year> 1990. </year>
Reference-contexts: 1 Introduction The concept of distributed reduced representations was introduced by Hinton [4] in order to allow a neural network to represent compositional structure (see also <ref> [12, 15, 17] </ref>). Concrete examples of distributed reduced representations are given by the Recursive Auto-Associative Memory (RAAM) by Pollack [11, 12] and by the Holographic Reduced Representations of Plate [10].
Reference: [18] <author> M. Zak. </author> <title> Terminal attractors in neural networks. </title> <booktitle> Neural Networks, </booktitle> <volume> 2 </volume> <pages> 259-274, </pages> <year> 1989. </year> <month> 18 </month>
Reference-contexts: In fact, in our example, a network with 3 hidden units was able to store 7 asymptotically stable fixed points. In any case, spurious attractors can be eliminated by introducing terminal attractors <ref> [18] </ref>. In conclusion, complex structures are encoded by an LRAAM in such a way that both distributed reduced representations and access by content can be exploited.
References-found: 18

