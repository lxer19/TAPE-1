URL: http://www.cs.jhu.edu/~cypher/pubs/ccl.ps
Refering-URL: http://www.cs.jhu.edu/~cypher/pubs/pubs.html
Root-URL: http://www.cs.jhu.edu
Title: CCL: A Portable and Tunable Collective Communication Library for Scalable Parallel Computers  
Author: Vasanth Bala Jehoshua Bruck Robert Cypher Pablo Elustondo Alex Ho Ching-Tien Ho Shlomo Kipnis Marc Snir 
Keyword: Index terms: collective communication algorithms, collective communication semantics, message-passing parallel systems, portable library, process group, tunable algorithms.  
Address: Waltham, MA 02154  San Jose, CA 95120 Yorktown Heights, NY 10598 IBM Argentina -IBM Israel  Beunos Aires, Argentina Haifa, Israel 31905  
Affiliation: Kendall Research Corp. IBM Research Division IBM Research Division  Almaden Research Center T.J. Watson Research Center  Science and Technology  
Abstract: A collective communication library for parallel computers includes frequently used operations such as broadcast, reduce, scatter, gather, concatenate, synchronize, and shift. Such a library provides users with a convenient programming interface, efficient communication operations, and the advantage of portability. A library of this nature, the Collective Communication Library (CCL), intended for the line of scalable parallel computer products by IBM, has been designed. CCL is part of the parallel application programming interface of the recently announced IBM 9076 Scalable POWERparallel System 1 (SP1). In this paper, we examine several issues related to the functionality, correctness, and performance of a portable collective communication library while focusing on three novel aspects in the design and implementation of CCL: (i) the introduction of process groups, (ii) the definition of semantics that ensures correctness, and (iii) the design of new and tunable algorithms based on a realistic point-to-point communication model. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> A. Aggarwal and S. Kipnis, </author> <title> "Message-time tradeoff for combining data in message-passing systems", </title> <institution> IBM Research Report, RC-18349, </institution> <month> September </month> <year> 1992. </year>
Reference-contexts: In fact, hybrid algorithms for these operations exist. For instance, a hybrid algorithm for the combine operation requires C 1 = dlog ne + k communication steps and communicates C 3 = m n 2 k (log n + 2 k+1 k 2) units of data (see <ref> [1] </ref>). The scatter and gather operations resemble the bcast and reduce operations in their functionality.
Reference: [2] <author> Y. Amir, D. Dolev, S. Kramer, D. Malki, "Transis: </author> <title> a communication sub-system for high availability", </title> <booktitle> Proceedings of the 22nd Annual International Symposium on Fault-Tolerant Computing, </booktitle> <month> July </month> <year> 1992, </year> <pages> pp. 76-84. </pages>
Reference-contexts: Section 2 examines issues related to process groups in CCL. The notion of creating and using process groups has been known in the distributed computing community, such as in the V system [17], ISIS [9] and Transis <ref> [2] </ref>. In the parallel applications environment, the support for creating and using process groups for collective communication routines has been very limited. For instance, most libraries on hypercubes only support broadcasts within subcubes.
Reference: [3] <author> V. Bala and S. Kipnis, </author> <title> "Efficient Collective Communication over Dynamically Determined Sets of Processors in Massively Parallel Computers", </title> <institution> IBM Research Report, RC-17771, </institution> <month> March </month> <year> 1992. </year> <title> Presented at the First CRPC Workshop on Standards for Message Passing in a Distributed Memory Environment, </title> <address> April 29-30, 1992, in Williamsburg, Virginia. </address>
Reference-contexts: Although Express also supports collective communication within an arbitrary group of processes, there is no support in creating process groups by partitioning an existing group into subgroups based on a local value supplied by each process. Such notion and its use in parallel programming was first suggested in <ref> [3, 4] </ref>, and our work influenced the introduction of this notion in an earlier MPI proposal [22] and the recent MPIF proposal [21]. <p> This section describes the concept of Process Groups in CCL, which is based on a similar concept that was presented in <ref> [3] </ref>. (In the IBM EUI documentation [28], Process Groups are called Task Groups.) 3 2.1 Defining Process Groups A Process Group is an ordered set of processes that has a system-wide unique name. It can be operated upon as a single object.
Reference: [4] <author> V. Bala and S. Kipnis, </author> <title> "Process Groups: a mechanism for the coordination of and communication among processes in the Venus collective communication library", </title> <booktitle> Proceedings of the 7th International Parallel Processing Symposium, IEEE, </booktitle> <month> April </month> <year> 1993. </year>
Reference-contexts: Although Express also supports collective communication within an arbitrary group of processes, there is no support in creating process groups by partitioning an existing group into subgroups based on a local value supplied by each process. Such notion and its use in parallel programming was first suggested in <ref> [3, 4] </ref>, and our work influenced the introduction of this notion in an earlier MPI proposal [22] and the recent MPIF proposal [21]. <p> These trends are evident in systems such as Thinking Machines' CM-5 [36], Intel's Paragon [38], NCUBE's nCUBE/2 [34], MIT's J-Machine [20], IBM's Vulcan [10, 39], and the recently announced IBM's Scalable POWERparallel System 1 (SP1), and in environments such as Express [37], PARMACS [31], PICL [29], Zipcode [35] and Venus <ref> [4] </ref>. These systems and environments generally ignore the specific structure and topology of the communication network and assume a fully-connected collection of processes, in which each process can communicate directly with any other process by sending and receiving messages.
Reference: [5] <author> A. Bar-Noy and S. Kipnis, </author> <title> "Designing broadcasting algorithms in the postal model for message-passing systems", to appear in Mathematical Systems Theory. </title> <booktitle> Also appeared in Proceedings of the 4th Annual ACM Symposium on Parallel Algorithms and Architectures, </booktitle> <month> June </month> <year> 1992, </year> <pages> pp. 11-22. </pages>
Reference-contexts: It should be noted that there are more detailed communication models, such as the Postal model <ref> [5] </ref> and the LogP model [18], which further take into account that a receiving process generally completes its receive operation later than the corresponding sending process finishes its send operation. However, designing efficient algorithms for these models seems to be more complicated.
Reference: [6] <author> A. Bar-Noy and S. Kipnis, </author> <title> "Broadcasting multiple messages in simultaneous send/receive systems", </title> <note> to appear in Discrete Applied Mathematics. Also appeared in Proceedings of the 5th IEEE Symposium on Parallel and Distributed Processing, De-cember 1993. </note>
Reference-contexts: For arbitrary values of n, an algorithm based on the generalized Fibonacci trees was given in [13] with C 2 m + log n + 3 log log n + 15. More recently, an algorithm based on edge-disjoint spanning trees of cascaded decreasing-size hypercubes was given <ref> [6] </ref> with nearly optimal C 2 = m + dlog ne. The reduce operation is usually implemented in a similar manner to the bcast operation by reversing the flow of the messages. As another example, consider the combine and prefix operations. <p> As another example, when n is a power of two, the bcast algorithm described in [32] is substantially simpler, in terms of local data structures and control, than the algorithm for arbitrary 16 values of n described in <ref> [6] </ref>. 5 Conclusions We have described the main issues that we have encountered in designing and implementing a Collective Communication Library (CCL) for the recently announced IBM Scalable POWERparallel System 1, (SP1).
Reference: [7] <author> A. Bar-Noy, S. Kipnis, and B. Schieber, </author> <title> "An optimal algorithm for computing census functions in message-passing systems", </title> <journal> Parallel Processing Letters, </journal> <volume> Vol. 3, No. 1, </volume> <month> March </month> <year> 1993. </year>
Reference-contexts: Examples of other such operations are combine, index, and sync. An algorithm for the combine operation with T = dlog ne (t s + mt c ), which is optimal with respect to C 1 (and also to C 2 when m = 1), appears in <ref> [7] </ref>. 4.4 Specialized Algorithms For certain CCL operations, the best-known algorithms for the case when n is a power of 2 may perform better than algorithms for the general case. Since, in many situations, n is a power of 2, it is worthwhile to implement this case separately.
Reference: [8] <author> A. Beguelin, J. Dongarra, A. Geist, R. Manchek, and V. Sunderam, </author> <title> "A user's guide to PVM Parallel Virtual Machine", </title> <type> ORNL Technical Report, </type> <institution> ORNL/TM-11826, </institution> <month> May </month> <year> 1992. </year>
Reference-contexts: Over the past few years, a large number of programming environments and communication libraries for parallel computers have been developed, including PVM <ref> [8] </ref>, Linda [16], PICL [29], PARMACS [31], Zipcode [35], Express [37], the nCUBE/2 library [34], the CM-5 library [36], and the iPSC/860 library. <p> Also, the CCL can be easily implemented on top of message-passing libraries such as IBM EUI [28], IBM VIPER Operating System [25, 30], Parasoft Express [37], and PVM <ref> [8] </ref>. The CCL is a software layer that can sit on top of point-to-point communication primitives. A crucial step in the design of CCL was to define a simple and realistic model for the underlying point-to-point communication.
Reference: [9] <author> K. Birman, R. Cooper, T.A. Joseph, K. Marzullo, M. Makpangou, K. Kane, F. Schmuck and M. Wood, </author> <title> "The ISIS system manual", </title> <institution> Department of Computer Science, Cornell University, Spet. </institution> <year> 1990. </year>
Reference-contexts: Section 2 examines issues related to process groups in CCL. The notion of creating and using process groups has been known in the distributed computing community, such as in the V system [17], ISIS <ref> [9] </ref> and Transis [2]. In the parallel applications environment, the support for creating and using process groups for collective communication routines has been very limited. For instance, most libraries on hypercubes only support broadcasts within subcubes.
Reference: [10] <author> J. Bruck, R. Cypher, L. Gravano, A. Ho, C. T. Ho, S. Kipnis, S. Konstantinidou, M. Snir and E. Upfal, </author> <title> "Survey of routing issues for the Vulcan parallel computer", </title> <institution> IBM Research Report, RJ-8839, </institution> <month> June </month> <year> 1992. </year>
Reference-contexts: Such a fully-connected model addresses emerging trends in many modern distributed-memory parallel computers and message-passing communication environments. These trends are evident in systems such as Thinking Machines' CM-5 [36], Intel's Paragon [38], NCUBE's nCUBE/2 [34], MIT's J-Machine [20], IBM's Vulcan <ref> [10, 39] </ref>, and the recently announced IBM's Scalable POWERparallel System 1 (SP1), and in environments such as Express [37], PARMACS [31], PICL [29], Zipcode [35] and Venus [4].
Reference: [11] <author> J. Bruck, R. Cypher, P. Elustondo, A. Ho, and C.T. Ho, </author> <title> "A proposal for common group structures in a collective communication library", </title> <institution> IBM Research Report, RJ-9421, </institution> <month> March </month> <year> 1993. </year> <month> 19 </month>
Reference-contexts: counter of the process with rank 0, since each process has all the information needed to calculate the PGID of the new Process Group to which it belongs. 2.4 Common Group Structure Routines Recently, a set of Common Group Structure (CGS) routines has been proposed as an extension to CCL <ref> [11] </ref>. The CGS routines make use of process group routines for defining 7 process groups that arise in algorithms with grid and hypercube structures. Once these grid-structured and hypercube-structured groups have been defined, the standard CCL routines can be used within the structured groups to perform collective communication operations.
Reference: [12] <author> J. Bruck, R. Cypher, and C.T. Ho, </author> <title> "Efficient fault-tolerant mesh and hypercubes architectures", </title> <booktitle> Proceedings of the 1992 International Symposium on Fault-Tolerant Computing, </booktitle> <pages> pp. 162-169. </pages>
Reference-contexts: Each node i is connected to nodes of the form ((i s) mod n) for all s 2 S (see [24]). Circulant graphs are an important class of networks which can be used as fault-tolerant networks for many other networks <ref> [12, 23] </ref>. We use a circulant graph with power-of-two offsets S = f1; 2; 4; ; 2 k1 g, where k = dlog ne, as a structure for the concat algorithm [14] (see Figure 1). The algorithm consists of k steps.
Reference: [13] <author> J. Bruck, R. Cypher, and C.T. Ho, </author> <title> "Multiple message broadcasting with generalized Fibonacci trees", </title> <booktitle> Proceedings of the 4th IEEE Symposium on Parallel and Distributed Processing, </booktitle> <month> December </month> <year> 1992, </year> <pages> pp. 424-431. </pages>
Reference-contexts: This algorithm requires C 2 = m + log n 1, which is optimal, and takes T = ( p p log nt s ) 2 time. For arbitrary values of n, an algorithm based on the generalized Fibonacci trees was given in <ref> [13] </ref> with C 2 m + log n + 3 log log n + 15. More recently, an algorithm based on edge-disjoint spanning trees of cascaded decreasing-size hypercubes was given [6] with nearly optimal C 2 = m + dlog ne.
Reference: [14] <author> J. Bruck, C.T. Ho, and S. Kipnis, </author> <title> "Concatenating data optimally in message-passing systems", </title> <institution> IBM Research Report, RJ-9191, </institution> <month> January </month> <year> 1993. </year>
Reference-contexts: 3 (m log n)=2 units of data, or, alternatively, they can be implemented with C 1 = n 1 communication steps and C 3 = m (n 1)=n units of data. 15 4.3 An Optimal Algorithm for Concatenation In this subsection, we outline an algorithm for the concat operation. (In <ref> [14] </ref> we showed that this algorithm is optimal with respect to measures C 1 , C 2 and C 3 .) In the concat (all-to-all broadcast) operation among n processes, each process has a fixed-size message (also called a data block) that it needs to broadcast to the other n 1 <p> We use a circulant graph with power-of-two offsets S = f1; 2; 4; ; 2 k1 g, where k = dlog ne, as a structure for the concat algorithm <ref> [14] </ref> (see Figure 1). The algorithm consists of k steps. In step 0, each process i sends its data block to process (i 1) mod n, receives a data block from process (i + 1) mod n, and appends the received data block to its current data.
Reference: [15] <author> J. Bruck, R. Cypher, C.T. Ho, and S. Kipnis, </author> <title> "Efficient algorithms for the index operation in message-passing systems", </title> <institution> IBM Research Report, RJ-9300, </institution> <month> April </month> <year> 1993. </year>
Reference-contexts: Hence, it is possible to implement a parameterized algorithm which can be tuned according to the start-up time t s , per-byte transfer time t c , the message size m, and possibly the number of "parallel ports" that can support concurrent sends and receives effectively (see <ref> [15] </ref>). As a second example, consider the broadcasting problem. The algorithm for the bcast routine is straightforward when the size of the data is m = 1, that is, when the source of the broadcast has one item to broadcast.
Reference: [16] <author> N. Carriero and D. Gelernter, </author> <title> "Linda in context", </title> <journal> Communication of the ACM, </journal> <volume> Vol. 32, No.4, </volume> <month> April </month> <year> 1989, </year> <pages> pp. 444-458. </pages>
Reference-contexts: Over the past few years, a large number of programming environments and communication libraries for parallel computers have been developed, including PVM [8], Linda <ref> [16] </ref>, PICL [29], PARMACS [31], Zipcode [35], Express [37], the nCUBE/2 library [34], the CM-5 library [36], and the iPSC/860 library.
Reference: [17] <author> D.R. Cheriton and W. Zwaenpoel, </author> <title> "Distributed Process Groups in the V Kernel", </title> <journal> ACM Trans. Comput. Syst., </journal> <volume> 2(3) </volume> <pages> 77-107, </pages> <month> May </month> <year> 1985. </year>
Reference-contexts: Section 2 examines issues related to process groups in CCL. The notion of creating and using process groups has been known in the distributed computing community, such as in the V system <ref> [17] </ref>, ISIS [9] and Transis [2]. In the parallel applications environment, the support for creating and using process groups for collective communication routines has been very limited. For instance, most libraries on hypercubes only support broadcasts within subcubes.
Reference: [18] <author> D. Culler, R. Karp, D. Patterson, A. Sahay, K.E. Schauser, E. Santos, R. Subramo-nian, and T. von Eicken, </author> <title> "LogP: towards a realistic model of parallel computation", </title> <booktitle> Proceedings of the 4th SIGPLAN Symposium on Principles and Practices of Parallel Programming, ACM, </booktitle> <month> May </month> <year> 1993. </year>
Reference-contexts: It should be noted that there are more detailed communication models, such as the Postal model [5] and the LogP model <ref> [18] </ref>, which further take into account that a receiving process generally completes its receive operation later than the corresponding sending process finishes its send operation. However, designing efficient algorithms for these models seems to be more complicated. Another important issue is the uniformity of the implementation.
Reference: [19] <author> R. Cypher and E. Leu, </author> <title> "Message-Passing Semantics and Portable Parallel Programs", </title> <institution> IBM Research Report, RJ-9654, </institution> <month> January </month> <year> 1994. </year>
Reference-contexts: It has been proven that a deterministic program will always behave identically, regardless of the system on which it is implemented <ref> [19] </ref>. The correctness of the CCL implementation follows from the fact that each CCL routine is deterministic. As a result, each CCL routine is guaranteed to operate correctly when it is run in isolation.
Reference: [20] <author> W. J. Dally, A. Chien, S. Fiske, W. Horwat, J. Keen, M. Larivee, R. Lethin, P. Nuth, S. Wills, P. Carrick, and G. </author> <title> Fyler "The J-Machine: a fine-grain concurrent computer", </title> <booktitle> Information Processing 89, </booktitle> <publisher> Elsevier Science Publishers, IFIP, </publisher> <year> 1989, </year> <pages> pp. 1147-1153. </pages>
Reference-contexts: Such a fully-connected model addresses emerging trends in many modern distributed-memory parallel computers and message-passing communication environments. These trends are evident in systems such as Thinking Machines' CM-5 [36], Intel's Paragon [38], NCUBE's nCUBE/2 [34], MIT's J-Machine <ref> [20] </ref>, IBM's Vulcan [10, 39], and the recently announced IBM's Scalable POWERparallel System 1 (SP1), and in environments such as Express [37], PARMACS [31], PICL [29], Zipcode [35] and Venus [4].
Reference: [21] <institution> Document for a Standard Message-Passing Interface, Message Passing Interface Forum, University of Tennessee, </institution> <type> Technical Report No. </type> <institution> CS-93-214, </institution> <month> November, </month> <year> 1993. </year>
Reference-contexts: Such notion and its use in parallel programming was first suggested in [3, 4], and our work influenced the introduction of this notion in an earlier MPI proposal [22] and the recent MPIF proposal <ref> [21] </ref>.
Reference: [22] <author> J. Dongarra, R. Hempel, A. Hey, and D. Walker, </author> <title> "A proposal for a user-level, message-passing interface in a distributed memory environment", </title> <type> ORNL Technical Report, </type> <institution> ORNL/TM-12231, </institution> <month> October </month> <year> 1992. </year>
Reference-contexts: Such notion and its use in parallel programming was first suggested in [3, 4], and our work influenced the introduction of this notion in an earlier MPI proposal <ref> [22] </ref> and the recent MPIF proposal [21].
Reference: [23] <author> S. Dutt and J. P. Hayes, </author> <title> "Designing fault-tolerant systems using automorphisms", </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> Vol. 12, </volume> <year> 1991, </year> <pages> pp. 249-268. </pages>
Reference-contexts: Each node i is connected to nodes of the form ((i s) mod n) for all s 2 S (see [24]). Circulant graphs are an important class of networks which can be used as fault-tolerant networks for many other networks <ref> [12, 23] </ref>. We use a circulant graph with power-of-two offsets S = f1; 2; 4; ; 2 k1 g, where k = dlog ne, as a structure for the concat algorithm [14] (see Figure 1). The algorithm consists of k steps.
Reference: [24] <author> B. Elspas and J. Turner, </author> <title> "Graphs with circulant adjacency matrices", </title> <journal> Journal of Combinatorial Theory, </journal> <volume> No. 9, </volume> <year> 1970, </year> <pages> pp. 297-307. </pages>
Reference-contexts: The nodes of the graph are labeled from 0 through n 1. Each node i is connected to nodes of the form ((i s) mod n) for all s 2 S (see <ref> [24] </ref>). Circulant graphs are an important class of networks which can be used as fault-tolerant networks for many other networks [12, 23].
Reference: [25] <author> B. G. Fitch and M. E. Giampapa, </author> <title> "The Vulcan operation environment: a brief overview and status report", </title> <booktitle> Proceedings of the 5th Workshop on Use of Parallel Processors in Meteorology, ECMWF, </booktitle> <month> November </month> <year> 1992. </year> <month> 20 </month>
Reference-contexts: Also, the CCL can be easily implemented on top of message-passing libraries such as IBM EUI [28], IBM VIPER Operating System <ref> [25, 30] </ref>, Parasoft Express [37], and PVM [8]. The CCL is a software layer that can sit on top of point-to-point communication primitives. A crucial step in the design of CCL was to define a simple and realistic model for the underlying point-to-point communication.
Reference: [26] <author> G. Fox and W. Furmanski, </author> <title> "Optimal communication algorithms for regular decompo-sitions on the hypercube", </title> <booktitle> Proceedings of the 3rd Conference on Hypercube Concurrent Computers and Applications, ACM, </booktitle> <year> 1988, </year> <pages> pp. 648-713. </pages>
Reference-contexts: Figure 2 presents an example of this algorithm. In general, all the homogeneous operations (operations for which there is no notion of a distinct source and/or destination, <ref> [26] </ref>) in CCL, can be implemented with the minimal number of start-ups (dlog ne communication steps) using the same circulant graph structure. Examples of other such operations are combine, index, and sync.
Reference: [27] <author> G. Fox, M. Johnson, G. Lyzenga, S. Otto, J. Salmon, and D. Walker, </author> <title> Solving Problems on Concurrent Processors, Volume I: General Techniques and Regular Problems, </title> <publisher> Prentice-Hall, </publisher> <address> Englewood Cliffs, New Jersey, </address> <year> 1988. </year>
Reference-contexts: In particular, collective communication is extensively used in many scientific applications for which the interleaving of stages of local computations with stages of global communication is possible (see <ref> [27] </ref>). This paper discusses issues related to the design and implementation of a portable and tunable Collective Communication Library (CCL). This library is intended for distributed-memory parallel computers where explicit communication among processes is achieved via message-passing.
Reference: [28] <author> D. Frye, R. Bryant, C.T. Ho, P. de Jong, R. Lawrence, and M. Snir, </author> <title> "An external user interface for scalable parallel systems: FORTRAN interface", </title> <type> Technical Report, </type> <institution> IBM Highly Parallel Supercomputing Systems Laboratory, </institution> <month> November </month> <year> 1992. </year>
Reference-contexts: Also, the CCL can be easily implemented on top of message-passing libraries such as IBM EUI <ref> [28] </ref>, IBM VIPER Operating System [25, 30], Parasoft Express [37], and PVM [8]. The CCL is a software layer that can sit on top of point-to-point communication primitives. A crucial step in the design of CCL was to define a simple and realistic model for the underlying point-to-point communication. <p> This section describes the concept of Process Groups in CCL, which is based on a similar concept that was presented in [3]. (In the IBM EUI documentation <ref> [28] </ref>, Process Groups are called Task Groups.) 3 2.1 Defining Process Groups A Process Group is an ordered set of processes that has a system-wide unique name. It can be operated upon as a single object. <p> Each of these novel aspects suggests interesting avenues for further learning and research. Acknowledgements We thank the following people: Dan Frye for his valuable comments and help in revising the CCL semantics as part of the spec in <ref> [28] </ref>, Magda Konstantinidou for her contributions to the initial stage of the CCL design, Eric Leu for his comments on a draft of this paper, many internal and external reviewers of [28] for their valuable comments to the CCL semantics, and Dragutin Petkovic for his constant support and help. 17 Appendix: <p> Dan Frye for his valuable comments and help in revising the CCL semantics as part of the spec in <ref> [28] </ref>, Magda Konstantinidou for her contributions to the initial stage of the CCL design, Eric Leu for his comments on a draft of this paper, many internal and external reviewers of [28] for their valuable comments to the CCL semantics, and Dragutin Petkovic for his constant support and help. 17 Appendix: List of CCL routines CCL consists of two parts, the collective communication routines (CC part) and the process group routines (PG part which is called TG for task group in the
Reference: [29] <author> G. A. Geist, M. T. Heath, B. W. Peyton, and P. H. Worley, </author> <title> "A user's guide to PICL: a Portable Instrumented Communication Library", </title> <type> ORNL Technical Report, </type> <institution> ORNL/TM-11616, </institution> <month> October </month> <year> 1990. </year>
Reference-contexts: Over the past few years, a large number of programming environments and communication libraries for parallel computers have been developed, including PVM [8], Linda [16], PICL <ref> [29] </ref>, PARMACS [31], Zipcode [35], Express [37], the nCUBE/2 library [34], the CM-5 library [36], and the iPSC/860 library. The design and implementation of the CCL adopts some of the popular communication concepts that already exist in many of these libraries, and, in addition, it provides several novel aspects. <p> These trends are evident in systems such as Thinking Machines' CM-5 [36], Intel's Paragon [38], NCUBE's nCUBE/2 [34], MIT's J-Machine [20], IBM's Vulcan [10, 39], and the recently announced IBM's Scalable POWERparallel System 1 (SP1), and in environments such as Express [37], PARMACS [31], PICL <ref> [29] </ref>, Zipcode [35] and Venus [4]. These systems and environments generally ignore the specific structure and topology of the communication network and assume a fully-connected collection of processes, in which each process can communicate directly with any other process by sending and receiving messages.
Reference: [30] <author> M. E. Giampapa, B. G. Fitch, G. R. Irwin, and D. G. Shea, </author> <title> "Vulcan operating environment: programmer's reference", Internal Memorandum, </title> <institution> IBM T.J. Watson Research Center, </institution> <month> March </month> <year> 1991. </year>
Reference-contexts: Also, the CCL can be easily implemented on top of message-passing libraries such as IBM EUI [28], IBM VIPER Operating System <ref> [25, 30] </ref>, Parasoft Express [37], and PVM [8]. The CCL is a software layer that can sit on top of point-to-point communication primitives. A crucial step in the design of CCL was to define a simple and realistic model for the underlying point-to-point communication.
Reference: [31] <author> R. Hempel, </author> <title> "The ANL/GMD macros (PARMACS) in FORTRAN for portable parallel programming using the message passing programming model, user's guide and reference manual", </title> <type> Technical Memorandum, </type> <institution> Gesellschaft fur Mathematik und Daten-verabeitung mbH, West Germany. </institution>
Reference-contexts: Over the past few years, a large number of programming environments and communication libraries for parallel computers have been developed, including PVM [8], Linda [16], PICL [29], PARMACS <ref> [31] </ref>, Zipcode [35], Express [37], the nCUBE/2 library [34], the CM-5 library [36], and the iPSC/860 library. The design and implementation of the CCL adopts some of the popular communication concepts that already exist in many of these libraries, and, in addition, it provides several novel aspects. <p> These trends are evident in systems such as Thinking Machines' CM-5 [36], Intel's Paragon [38], NCUBE's nCUBE/2 [34], MIT's J-Machine [20], IBM's Vulcan [10, 39], and the recently announced IBM's Scalable POWERparallel System 1 (SP1), and in environments such as Express [37], PARMACS <ref> [31] </ref>, PICL [29], Zipcode [35] and Venus [4]. These systems and environments generally ignore the specific structure and topology of the communication network and assume a fully-connected collection of processes, in which each process can communicate directly with any other process by sending and receiving messages.
Reference: [32] <author> C.T. Ho, </author> <title> "Optimal broadcasting on SIMD hypercubes without indirect addressing capability", </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> Vol. 13, No. 2, </volume> <month> October </month> <year> 1991, </year> <pages> pp. 246-255. </pages>
Reference-contexts: The common divide-and-conquer algorithm based on a binomial tree takes dlog ne (t s + mt c ) time, which may be far from optimal. When n is a power of two, an algorithm based on log n edge-disjoint spanning trees on a (log n)-cube is given in <ref> [33, 32] </ref>. This algorithm requires C 2 = m + log n 1, which is optimal, and takes T = ( p p log nt s ) 2 time. <p> For instance, the concat algorithm for n that is a power of two can use a well-known hypercube recursive exchange algorithm which eliminates shifting local arrays at the end of the operation. As another example, when n is a power of two, the bcast algorithm described in <ref> [32] </ref> is substantially simpler, in terms of local data structures and control, than the algorithm for arbitrary 16 values of n described in [6]. 5 Conclusions We have described the main issues that we have encountered in designing and implementing a Collective Communication Library (CCL) for the recently announced IBM Scalable
Reference: [33] <author> S. L. Johnsson and C.T. Ho, </author> <title> "Spanning graphs for optimum broadcasting and personalized communication in hypercubes", </title> <journal> IEEE Transactions on Computers, </journal> <volume> Vol. C-38, No. 9, </volume> <month> September </month> <year> 1989, </year> <pages> pp 1249-1268. </pages>
Reference-contexts: The common divide-and-conquer algorithm based on a binomial tree takes dlog ne (t s + mt c ) time, which may be far from optimal. When n is a power of two, an algorithm based on log n edge-disjoint spanning trees on a (log n)-cube is given in <ref> [33, 32] </ref>. This algorithm requires C 2 = m + log n 1, which is optimal, and takes T = ( p p log nt s ) 2 time.
Reference: [34] <author> J. F. </author> <title> Palmer "The NCUBE family of parallel supercomputers", </title> <booktitle> Proceedings of the International Conference on Computer Design, IEEE, </booktitle> <year> 1986. </year>
Reference-contexts: Over the past few years, a large number of programming environments and communication libraries for parallel computers have been developed, including PVM [8], Linda [16], PICL [29], PARMACS [31], Zipcode [35], Express [37], the nCUBE/2 library <ref> [34] </ref>, the CM-5 library [36], and the iPSC/860 library. The design and implementation of the CCL adopts some of the popular communication concepts that already exist in many of these libraries, and, in addition, it provides several novel aspects. <p> Such a fully-connected model addresses emerging trends in many modern distributed-memory parallel computers and message-passing communication environments. These trends are evident in systems such as Thinking Machines' CM-5 [36], Intel's Paragon [38], NCUBE's nCUBE/2 <ref> [34] </ref>, MIT's J-Machine [20], IBM's Vulcan [10, 39], and the recently announced IBM's Scalable POWERparallel System 1 (SP1), and in environments such as Express [37], PARMACS [31], PICL [29], Zipcode [35] and Venus [4].
Reference: [35] <author> A. Skjellum and A. P. Leung, </author> <title> "Zipcode: a portable multicomputer communication library atop the Reactive Kernel", </title> <booktitle> Proceedings of the 5th Distributed Memory Computing Conference, IEEE, </booktitle> <month> April </month> <year> 1990, </year> <pages> pp. 328-337. </pages>
Reference-contexts: Over the past few years, a large number of programming environments and communication libraries for parallel computers have been developed, including PVM [8], Linda [16], PICL [29], PARMACS [31], Zipcode <ref> [35] </ref>, Express [37], the nCUBE/2 library [34], the CM-5 library [36], and the iPSC/860 library. The design and implementation of the CCL adopts some of the popular communication concepts that already exist in many of these libraries, and, in addition, it provides several novel aspects. <p> This can be achieved either with an include/exclude mechanism for wildcard tag ranges, as provided by Express [37] or with an additional context mechanism, as provided by Zipcode <ref> [35] </ref>. 3.2 Implementation Correctness There are many issues related to a correct implementation of CCL. <p> These trends are evident in systems such as Thinking Machines' CM-5 [36], Intel's Paragon [38], NCUBE's nCUBE/2 [34], MIT's J-Machine [20], IBM's Vulcan [10, 39], and the recently announced IBM's Scalable POWERparallel System 1 (SP1), and in environments such as Express [37], PARMACS [31], PICL [29], Zipcode <ref> [35] </ref> and Venus [4]. These systems and environments generally ignore the specific structure and topology of the communication network and assume a fully-connected collection of processes, in which each process can communicate directly with any other process by sending and receiving messages.
Reference: [36] <institution> Connection Machine CM-5 Technical Summary, Thinking Machines Corporation, </institution> <year> 1991. </year>
Reference-contexts: Over the past few years, a large number of programming environments and communication libraries for parallel computers have been developed, including PVM [8], Linda [16], PICL [29], PARMACS [31], Zipcode [35], Express [37], the nCUBE/2 library [34], the CM-5 library <ref> [36] </ref>, and the iPSC/860 library. The design and implementation of the CCL adopts some of the popular communication concepts that already exist in many of these libraries, and, in addition, it provides several novel aspects. <p> Such a fully-connected model addresses emerging trends in many modern distributed-memory parallel computers and message-passing communication environments. These trends are evident in systems such as Thinking Machines' CM-5 <ref> [36] </ref>, Intel's Paragon [38], NCUBE's nCUBE/2 [34], MIT's J-Machine [20], IBM's Vulcan [10, 39], and the recently announced IBM's Scalable POWERparallel System 1 (SP1), and in environments such as Express [37], PARMACS [31], PICL [29], Zipcode [35] and Venus [4].
Reference: [37] <institution> Express 3.0 Introductory Guide, Parasoft Corporation, </institution> <year> 1990. </year>
Reference-contexts: Over the past few years, a large number of programming environments and communication libraries for parallel computers have been developed, including PVM [8], Linda [16], PICL [29], PARMACS [31], Zipcode [35], Express <ref> [37] </ref>, the nCUBE/2 library [34], the CM-5 library [36], and the iPSC/860 library. The design and implementation of the CCL adopts some of the popular communication concepts that already exist in many of these libraries, and, in addition, it provides several novel aspects. <p> Also, the CCL can be easily implemented on top of message-passing libraries such as IBM EUI [28], IBM VIPER Operating System [25, 30], Parasoft Express <ref> [37] </ref>, and PVM [8]. The CCL is a software layer that can sit on top of point-to-point communication primitives. A crucial step in the design of CCL was to define a simple and realistic model for the underlying point-to-point communication. <p> If wildcard tag values are allowed in receive operations, one also needs to make sure that user receive operations cannot match tag values used by CCL. This can be achieved either with an include/exclude mechanism for wildcard tag ranges, as provided by Express <ref> [37] </ref> or with an additional context mechanism, as provided by Zipcode [35]. 3.2 Implementation Correctness There are many issues related to a correct implementation of CCL. <p> These trends are evident in systems such as Thinking Machines' CM-5 [36], Intel's Paragon [38], NCUBE's nCUBE/2 [34], MIT's J-Machine [20], IBM's Vulcan [10, 39], and the recently announced IBM's Scalable POWERparallel System 1 (SP1), and in environments such as Express <ref> [37] </ref>, PARMACS [31], PICL [29], Zipcode [35] and Venus [4]. These systems and environments generally ignore the specific structure and topology of the communication network and assume a fully-connected collection of processes, in which each process can communicate directly with any other process by sending and receiving messages.
Reference: [38] <institution> Paragon XP/S Overview, Intel Corporation, </institution> <year> 1991. </year> <title> [39] "Vulcan system summary", Internal Memorandum, </title> <institution> IBM T.J. Watson Research Center. </institution> <type> 21 22 23 </type>
Reference-contexts: Such a fully-connected model addresses emerging trends in many modern distributed-memory parallel computers and message-passing communication environments. These trends are evident in systems such as Thinking Machines' CM-5 [36], Intel's Paragon <ref> [38] </ref>, NCUBE's nCUBE/2 [34], MIT's J-Machine [20], IBM's Vulcan [10, 39], and the recently announced IBM's Scalable POWERparallel System 1 (SP1), and in environments such as Express [37], PARMACS [31], PICL [29], Zipcode [35] and Venus [4].
References-found: 38

