URL: ftp://psyche.mit.edu/pub/lksaul/exploit.ps.Z
Refering-URL: http://www.cs.cmu.edu/Web/Groups/NIPS/NIPS95/Papers.html
Root-URL: 
Email: flksaul,jordang@psyche.mit.edu  
Title: Exploiting Tractable Substructures in Intractable Networks  
Author: Lawrence K. Saul and Michael I. Jordan 
Note: To appear in Advances of Neural Information Processing Systems: Proceedings of the 1995 Conference.  
Address: 79 Amherst Street, E10-243 Cambridge, MA 02139  
Affiliation: Center for Biological and Computational Learning Massachusetts Institute of Technology  
Abstract: We develop a refined mean field approximation for inference and learning in probabilistic neural networks. Our mean field theory, unlike most, does not assume that the units behave as independent degrees of freedom; instead, it exploits in a principled way the existence of large substructures that are computationally tractable. To illustrate the advantages of this framework, we show how to incorporate weak higher order interactions into a first-order hidden Markov model, treating the corrections (but not the first order structure) within mean field theory.
Abstract-found: 1
Intro-found: 1
Reference: <author> A. Dempster, N. Laird, and D. Rubin. </author> <title> (1977) Maximum likelihood from incomplete data via the EM algorithm. </title> <journal> J. Roy. Stat. Soc. B39:1-38. </journal>
Reference-contexts: for QfSg, then chooses parameters to minimize the Kullback-Liebler (KL) divergence: KL (QjjP ) = X QfSg ln QfSg P fSg : (2) Why are mean field approximations valuable for learning? Suppose that P fSg represents the posterior distribution over hidden variables, as in the E-step of an EM algorithm <ref> (Dempster, Laird, & Rubin, 1977) </ref>. Then we obtain a mean field approximation to this E-step by replacing the statistics of P fSg (which may be quite difficult to compute) with those of QfSg (which may be much simpler).
Reference: <author> B. H. Juang and L. R. Rabiner. </author> <title> (1991) Hidden Markov models for speech recognition, </title> <type> Technometrics 33: </type> <pages> 251-272. </pages>
Reference-contexts: For simplicity we focus on networks with binary units; the extension to discrete-valued (Potts) units is straightforward. We apply these ideas to hidden Markov modeling <ref> (Rabiner & Juang, 1991) </ref>. The first order probabilistic structure of hidden Markov models (HMMs) leads to networks with chained architectures for which efficient, exact algorithms are available.
Reference: <author> S. Luttrell. </author> <title> (1989) The Gibbs machine applied to hidden Markov model problems. </title> <journal> Royal Signals and Radar Establishment: </journal> <note> SP Research Note 99. </note>
Reference: <author> G. Parisi. </author> <title> (1988) Statistical field theory. </title> <publisher> Addison-Wesley: </publisher> <address> Redwood City, CA. </address>
Reference-contexts: This is possible within the more sophisticated mean field theory described here. 2 MEAN FIELD THEORY We briefly review the basic methodology of mean field theory for networks of binary (1) stochastic units <ref> (Parisi, 1988) </ref>.
Reference: <author> C. Peterson and J. R. Anderson. </author> <title> (1987) A mean field theory learning algorithm for neural networks. </title> <booktitle> Complex Systems 1 </booktitle> <pages> 995-1019. </pages>
Reference-contexts: In networks with sparse connectivity (e.g. trees and chains), there exist efficient algorithms for the exact probabilistic calculations that support inference and learning. In general, however, these calculations are intractable, and approximations are required. Mean field theory provides a framework for approximation in probabilistic neural networks <ref> (Peterson & Anderson, 1987) </ref>. Most applications of mean field theory, however, have made a rather drastic probabilistic assumption|namely, that the units in the network behave as independent degrees of freedom. In this paper we show how to go beyond this assumption.
Reference: <author> L. Saul and M. Jordan. </author> <title> (1994) Learning in Boltzmann trees. </title> <journal> Neural Comp. </journal> <volume> 6: </volume> <pages> 1174-1184. </pages>
Reference: <author> L. Saul and M. Jordan. </author> <title> (1995) Boltzmann chains and hidden Markov models. </title>
Reference: <editor> In G. Tesauro, D. Touretzky, and T. Leen, eds. </editor> <booktitle> Advances in Neural Information Processing Systems 7. </booktitle> <publisher> MIT Press: </publisher> <address> Cambridge, MA. </address>
Reference: <author> P. Stolorz. </author> <title> (1994) Recursive approaches to the statistical physics of lattice proteins. </title>
Reference-contexts: The first order probabilistic structure of hidden Markov models (HMMs) leads to networks with chained architectures for which efficient, exact algorithms are available. More elaborate networks are obtained by introducing couplings between multiple HMMs (Williams & Hinton, 1990) and/or long-range couplings within a single HMM <ref> (Stolorz, 1994) </ref>. Both sorts of extensions have interesting applications; in speech, for example, multiple HMMs can provide a distributed representation of the articulatory state, while long-range couplings can model the effects of coarticulation. In general, however, such extensions lead to networks for which exact probabilistic calculations are not feasible.
Reference: <editor> In L. Hunter, ed. </editor> <booktitle> Proc. 27th Hawaii Intl. Conf. on System Sciences V: </booktitle> <pages> 316-325. </pages>
Reference: <author> C. Williams and G. E. Hinton. </author> <title> (1990) Mean field networks that learn to discriminate temporally distorted strings. </title> <booktitle> Proc. Connectionist Models Summer School: </booktitle> <pages> 18-22. </pages>
Reference-contexts: We apply these ideas to hidden Markov modeling (Rabiner & Juang, 1991). The first order probabilistic structure of hidden Markov models (HMMs) leads to networks with chained architectures for which efficient, exact algorithms are available. More elaborate networks are obtained by introducing couplings between multiple HMMs <ref> (Williams & Hinton, 1990) </ref> and/or long-range couplings within a single HMM (Stolorz, 1994). Both sorts of extensions have interesting applications; in speech, for example, multiple HMMs can provide a distributed representation of the articulatory state, while long-range couplings can model the effects of coarticulation.
References-found: 11

