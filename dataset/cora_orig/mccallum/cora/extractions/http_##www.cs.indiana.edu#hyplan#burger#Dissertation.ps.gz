URL: http://www.cs.indiana.edu/hyplan/burger/Dissertation.ps.gz
Refering-URL: http://www.cs.indiana.edu/proglang/dynamo/
Root-URL: http://www.cs.indiana.edu
Title: EFFICIENT COMPILATION AND PROFILE-DRIVEN DYNAMIC RECOMPILATION IN SCHEME  
Author: Robert G. Burger 
Degree: Submitted to the faculty of the University Graduate School in partial fulfillment of the requirements for the degree Doctor of Philosophy in the  
Date: March 1997  
Affiliation: Department of Computer Science, Indiana University  
Abstract-found: 0
Intro-found: 1
Reference: [1] <author> Kenneth R. Anderson. </author> <title> Courage in profiles. </title> <booktitle> The Fourth International Lisp Users and Vendors Conference, Performing LISP Tutorial, </booktitle> <month> July </month> <year> 1994. </year>
Reference-contexts: DYNAMIC RECOMPILATION 62 in Section 3.4. Section 4.7 describes related work. 4.1 Background Profiling tools have certainly been valuable for programmers by helping them identify execution bottlenecks and inefficiencies. Anderson <ref> [1] </ref> demonstrates the value of profiling with two 2000-line machine learning programs. Both are floating-point intensive, and the second is also structure manipulation intensive.
Reference: [2] <author> Andrew W. Appel and Zhong Shao. </author> <title> Callee-save registers in continuation-passing style. </title> <journal> Lisp and Symbolic Computation, </journal> <volume> 5(3) </volume> <pages> 191-221, </pages> <year> 1992. </year>
Reference-contexts: introduction of temporaries but differs in that any cycle causes a complete spill of all arguments into temporary stack locations. Although Kranz [29] describes a register shu*ing algorithm similar to ours, details regarding the selection of the node used to break cycles are not given. Shao and Appel <ref> [39, 2] </ref> have developed a closure conversion algorithm that exploits control and data flow information to obtain extensive closure sharing. This sharing enhances the benefit they obtain from allocating closures in registers.
Reference: [3] <author> J. Michael Ashley and R. Kent Dybvig. </author> <title> An efficient implementation of multiple return values in Scheme. </title> <booktitle> In Proceedings of the ACM SIGPLAN '94 Conference on Programming Language Design and Implementation, </booktitle> <pages> pages 140-149, </pages> <month> June </month> <year> 1994. </year>
Reference-contexts: Representing nontail procedure calls in the graph is complicated by our implementation of multiple return values and run-time error checking. As shown in Figure 13, four words of data are placed in the instruction stream immediately before the single-value return point from each nontail call <ref> [28, 3, 8] </ref>. The first three words are used to support efficient garbage collection and continuation invocation. The last word, the multiple-value return address, is used when a procedure returns more than one value or zero values. <p> The two x's in the figure indicate unused bits. In order to support multiple return values, first-class continuations, and garbage collection efficiently, four words of data are placed in the instruction stream immediately before the single-value return point from each non-tail call <ref> [28, 3, 8] </ref>. The first is a live mask, a bit vector describing which frame locations contain live data, used by the garbage collector.
Reference: [4] <author> Thomas Ball and James R. Larus. </author> <title> Optimally profiling and tracing programs. </title> <booktitle> In Proceedings of the 19th Annual ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages, </booktitle> <pages> pages 59-70, </pages> <month> January </month> <year> 1992. </year>
Reference-contexts: Chapter 3 Edge-Count Profiling This chapter describes a low-overhead edge-count profiling strategy that supports first-class continuations and reinstrumentation of active procedures. It is based on one described by Ball and Larus <ref> [4] </ref> that minimizes the total number of profile counter increments done at run time. It extends Ball and Larus's strategy with support for first-class continuations and reinstrumentation of active procedures. <p> Second, it is used to give feedback to the programmer via graphical annotation of the original source code. Section 3.1 reviews Ball and Larus's optimal edge-count placement algorithm <ref> [4] </ref>. Section 3.2 presents modifications to support first-class continuations and reinstru-mentation of active procedures. Section 3.3 describes our implementation. Section 3.4 presents our linear static edge-count estimator. Section 3.5 explains how the profile data is correlated with the original source. Section 3.6 reports the run-time and 34 CHAPTER 3. <p> If B's only incoming edge is e (and B is not the exit block), the increment code is placed in B. Otherwise, the increment code is placed in a new block C that is spliced into the control-flow graph. described by Ball and Larus <ref> [4] </ref>. Instead, the maximal spanning tree algorithm generates the list used to propagate the weights quickly and easily. <p> First, we could modify the implementation of continuation invocation to update the weights of exit edges directly. Ball and Larus use this technique for exit-only continuations by incrementing the weights of the exit edges associated with each CHAPTER 3. EDGE-COUNT PROFILING 45 activation that would exit prematurely <ref> [4] </ref>. This approach would support fully general continuations if it would also decrement the weights of the exit edges associated with each activation that would be reinstated. <p> Our static estimator significantly improves initial counter placement by simulating profile data. It correctly predicts the behavior of common run-time tests such as heap and stack overflow, identifies and predicts internal loops, and assumes all other tests have a 50% chance of succeeding. Unlike Ball and Larus's estimator <ref> [4] </ref>, it is linear and does not require the control-flow graph to be reducible. The key idea that enables the estimator to be linear is that a static weighting of the control-flow graph need not satisfy the conservation of flow property. <p> Finally, compression of the resulting symbolic information would reduce object file size considerably. 3.7 Related Work Profiling can be divided into two main areas: count-based and time-based. Ball and Larus's optimal edge-count profiling strategy <ref> [4] </ref> is the main work in count-based profiling. ATOM [42] is a system used to specify code-instrumenting tools such as profilers and can be used to implement edge-count strategies. It operates on object files and provides functions that make it easy to access procedures, basic blocks, and instructions.
Reference: [5] <author> Thomas Ball and James R. Larus. </author> <title> Branch prediction for free. </title> <booktitle> In Proceedings of the ACM SIGPLAN '93 Conference on Programming Language Design and Implementation, </booktitle> <pages> pages 300-313, </pages> <month> June </month> <year> 1993. </year>
Reference-contexts: Profile data can assist the compiler is these areas. Basic blocks can be reordered to reduce the number of mispredicted branches and instruction cache misses [35, 36]. Although there are various heuristic algorithms that estimate branch behavior statically, research has demonstrated that profile-based prediction is significantly more effective <ref> [5, 47, 34] </ref>. In current programming environments, profile-based prediction requires a tedious compile-profile-recompile cycle. In our system, the instrumentation for profiling and the subsequent recompilation are done at run time. CHAPTER 4. DYNAMIC RECOMPILATION 63 4.2 Overview Dynamic recompilation proceeds in three phases. <p> CHAPTER 4. DYNAMIC RECOMPILATION 80 4.7 Related Work In the absence of profiling information, various heuristic algorithms can be used for statically estimating branch behavior. Ball and Larus <ref> [5] </ref> describe a simple heuristic that can significantly reduce the number of mispredicted branches. They found, however, that on average their static prediction generates code that executes twice as many mispredicted branches as the code generated by profile-based prediction.
Reference: [6] <author> Anders Bondorf. </author> <note> Similix Manual, System Version 5.0. </note> <institution> DIKU, University of Copenhagen, Denmark, </institution> <year> 1993. </year>
Reference-contexts: EDGE-COUNT PROFILING 56 Benchmark Lines Description compiler 35,913 Chez Scheme 5.0g recompiling itself softscheme 10,073 Andrew Wright's soft typer [50] checking his pattern matcher ddd 9,578 Digital Design Derivation System 1.0 [7] deriving hard ware for a Scheme machine [9] similix 7,305 self-application of the Similix 5.0 partial evaluator <ref> [6] </ref> nucleic 3,475 3-D structure determination of a nucleic acid slatex 2,343 SLaTeX 2.2 typesetting its own manual interpret 1,069 Marc Feeley's Scheme interpreter evaluating takl maze 730 Hexagonal maze maker by Olin Shivers earley 655 Earley's parser by Marc Feeley peval 639 Feeley's simple Scheme partial evaluator boyer 572 Logic
Reference: [7] <author> Bhaskar Bose. </author> <title> DDD|A transformation system for Digital Design Derivation. </title> <type> Technical Report 331, </type> <institution> Indiana University, Computer Science Department, </institution> <month> May </month> <year> 1991. </year>
Reference-contexts: On average, removing the support decreases run time by 20% for CHAPTER 3. EDGE-COUNT PROFILING 56 Benchmark Lines Description compiler 35,913 Chez Scheme 5.0g recompiling itself softscheme 10,073 Andrew Wright's soft typer [50] checking his pattern matcher ddd 9,578 Digital Design Derivation System 1.0 <ref> [7] </ref> deriving hard ware for a Scheme machine [9] similix 7,305 self-application of the Similix 5.0 partial evaluator [6] nucleic 3,475 3-D structure determination of a nucleic acid slatex 2,343 SLaTeX 2.2 typesetting its own manual interpret 1,069 Marc Feeley's Scheme interpreter evaluating takl maze 730 Hexagonal maze maker by Olin
Reference: [8] <author> Carl Bruggeman, Oscar Waddell, and R. Kent Dybvig. </author> <title> Representing control in the presence of one-shot continuations. </title> <booktitle> In Proceedings of the ACM SIGPLAN '96 Conference on Programming Language Design and Implementation, </booktitle> <pages> pages 99-107, </pages> <month> May </month> <year> 1996. </year> <note> 86 BIBLIOGRAPHY 87 </note>
Reference-contexts: This approach is simple to implement, requiring no change to the implementation of continuations. Because the notion of maximal is weakened, however, the profiling overhead increases. Our system's segmented stack implementation supports constant-time continuation operations <ref> [28, 8] </ref>. Implementing the first approach would destroy this property. Moreover, if any procedure is profiled, the system must traverse all activations to find the profiled ones. <p> Representing nontail procedure calls in the graph is complicated by our implementation of multiple return values and run-time error checking. As shown in Figure 13, four words of data are placed in the instruction stream immediately before the single-value return point from each nontail call <ref> [28, 3, 8] </ref>. The first three words are used to support efficient garbage collection and continuation invocation. The last word, the multiple-value return address, is used when a procedure returns more than one value or zero values. <p> The two x's in the figure indicate unused bits. In order to support multiple return values, first-class continuations, and garbage collection efficiently, four words of data are placed in the instruction stream immediately before the single-value return point from each non-tail call <ref> [28, 3, 8] </ref>. The first is a live mask, a bit vector describing which frame locations contain live data, used by the garbage collector. <p> DYNAMIC RECOMPILATION 73 by a bit in the type field of the code object associated with the entry field. The free variable slots are used to store information about the continuation object such as the size and address of its stack segment (see <ref> [28, 8] </ref> for a more detailed description). Since the only pointer to a continuation object's stack segment is in the continuation object, the stack segment is stored in the data space and copied when its continuation object is copied.
Reference: [9] <author> Robert G. Burger. </author> <title> The Scheme Machine. </title> <type> Technical Report 413, </type> <institution> Indiana University, Computer Science Department, </institution> <month> August </month> <year> 1994. </year>
Reference-contexts: EDGE-COUNT PROFILING 56 Benchmark Lines Description compiler 35,913 Chez Scheme 5.0g recompiling itself softscheme 10,073 Andrew Wright's soft typer [50] checking his pattern matcher ddd 9,578 Digital Design Derivation System 1.0 [7] deriving hard ware for a Scheme machine <ref> [9] </ref> similix 7,305 self-application of the Similix 5.0 partial evaluator [6] nucleic 3,475 3-D structure determination of a nucleic acid slatex 2,343 SLaTeX 2.2 typesetting its own manual interpret 1,069 Marc Feeley's Scheme interpreter evaluating takl maze 730 Hexagonal maze maker by Olin Shivers earley 655 Earley's parser by Marc Feeley
Reference: [10] <author> Robert G. Burger and R. Kent Dybvig. </author> <title> Printing floating-point numbers quickly and accurately. </title> <booktitle> In Proceedings of the ACM SIGPLAN '96 Conference on Programming Language Design and Implementation, </booktitle> <pages> pages 108-116, </pages> <month> May </month> <year> 1996. </year>
Reference: [11] <author> Robert G. Burger, Oscar Waddell, and R. Kent Dybvig. </author> <title> Register allocation using lazy saves, eager restores, and greedy shu*ing. </title> <booktitle> In Proceedings of the ACM SIG-PLAN '95 Conference on Programming Language Design and Implementation, </booktitle> <pages> pages 130-138, </pages> <month> June </month> <year> 1995. </year>
Reference: [12] <institution> Cadence Research Systems, Bloomington, </institution> <note> Indiana. Chez Scheme Version 5 System Manual, Revision 2.5, </note> <month> October </month> <year> 1994. </year>
Reference-contexts: They manipulate data using register, register displacement, index, immediate, and special relocation addressing modes. The new pass collects symbolic assembly code and splits it into basic blocks. Machine code is then generated from the blocks. Our system implements procedures of variable arity using case-lambda <ref> [12] </ref>, a variant of lambda* [22]. Consequently, there may be multiple entry points to a given procedure. When the compiler determines that a given call site refers to a particular case of some procedure, it generates a direct call to the appropriate entry point.
Reference: [13] <author> G. J. Chaitin, M. A. Auslander, A. K. Cocke, M. E. Hopkins, and P. W. Mark-stein. </author> <title> Register allocation via coloring. </title> <journal> Computer Languages, </journal> <volume> 6 </volume> <pages> 47-57, </pages> <year> 1981. </year>
Reference: [14] <author> Pohua P. Chang, Scott A. Mahlke, William Y. Chen, and Wen-mei W. Hwu. </author> <title> Using profile information to assist classic compiler code optimizations. </title> <journal> Software Practice and Experience, </journal> <volume> 21(12) </volume> <pages> 1301-1321, </pages> <month> December </month> <year> 1991. </year>
Reference-contexts: Since determining the optimal ordering is NP-complete, they use a greedy "closest is best" strategy. Other optimizations can also make effective use of profiling information. Chang, Mahlke, and Hwu <ref> [14] </ref> use profile data to optimize the most frequently executed portions of a procedure. They build "superblocks," which are chains of basic blocks like those generated by our reordering algorithm, and they duplicate blocks to reduce the number of join points in an attempt to expose more opportunities for optimization.
Reference: [15] <author> William Y. Chen, Scott A. Mahlke, Nancy J. Warter, Sadun Anik, and Wen-mei W. Hwu. </author> <title> Profile-assisted instruction scheduling. </title> <journal> International Journal of Parallel Programming, </journal> <volume> 22(2) </volume> <pages> 151-181, </pages> <month> April </month> <year> 1994. </year>
Reference-contexts: The duplication of blocks is controlled by profile information so that code growth can be bounded by a small constant. Classic optimizations such as constant folding, copy propagation, common subexpression elimination, and dead code elimination are CHAPTER 4. DYNAMIC RECOMPILATION 81 performed at the superblock level. Chen et al. <ref> [15] </ref> use profile data to guide instruction scheduling by removing constraints that arise from infrequently executed portions of a procedure. McFarling [33] uses profile data and specific information about the target architecture's instruction cache to guide procedure inlining.
Reference: [16] <author> C. J. </author> <title> Cheney. A nonrecursive list compacting algorithm. </title> <journal> Communications of the ACM, </journal> <volume> 13(11) </volume> <pages> 677-678, </pages> <month> November </month> <year> 1970. </year>
Reference-contexts: When collection finishes, all old space segments are marked free. The collection algorithm uses a form of breadth-first search to traverse all reachable objects in old space <ref> [16] </ref>. The queue of objects for each space is maintained in new space by associating an allocation pointer (the back of the queue) and a sweep pointer (the front of the queue) with each space in generation t. The search begins by relocating the root set of pointers.
Reference: [17] <author> F. Chow. </author> <title> Minimizing register usage penalty at procedure calls. </title> <booktitle> In Proceedings of the ACM SIGPLAN '88 Conference on Programming Language Design and Implementation, </booktitle> <pages> pages 85-94, </pages> <month> June </month> <year> 1988. </year>
Reference-contexts: The baseline for comparison 82 CHAPTER 5. CONCLUSIONS 83 is efficient code generated by an optimizing compiler that already makes extensive use of global registers and local register allocation. This is within range of improvements reported for interprocedural register allocation <ref> [44, 17] </ref>. Although the compiler now spends around 7% of its time on register allocation, the compiler actually runs faster since it is self-compiled and benefits from its own register allocation strategy.
Reference: [18] <author> Fred C. Chow and John L. Hennessy. </author> <title> The priority-based coloring approach to register allocation. </title> <journal> Transactions on Programming Languages and Systems, </journal> <volume> 12(4) </volume> <pages> 501-536, </pages> <month> October </month> <year> 1990. </year>
Reference: [19] <editor> William Clinger and Jonathan Rees (editors). </editor> <title> Revised 4 report on the algorithmic language Scheme. LISP Pointers, </title> <journal> IV(3):1-55, </journal> <note> July-September 1991. BIBLIOGRAPHY 88 </note>
Reference: [20] <author> William D. Clinger and Lars Thomas Hansen. </author> <title> Lambda, the ultimate label, or a simple optimizing compiler for Scheme. </title> <booktitle> In Proceedings of the 1994 ACM Conference on LISP and Functional Programming, </booktitle> <pages> pages 128-139, </pages> <year> 1994. </year>
Reference-contexts: Other researchers have investigated the use of lambda lifting to increase the number of arguments available for placement in registers <ref> [39, 20] </ref>. While lambda lifting can easily result in net performance decreases, it is worth investigating whether lambda lifting with an appropriate set of heuristics such as those described in [39] can indeed increase the effectiveness of our register allocator without significantly increasing compile time. <p> For example, profile data can be used to measure how many times a procedure is called versus how many times it is created, and this ratio could be used to guide lambda lifting <ref> [20] </ref>. Combined with an estimate of the cost of generating code for the procedure, this ratio could also help determine when run-time code generation [30, 31] would be profitable. Our system could be extended to recompile (specialize) a procedure based on the run-time values of its free variables.
Reference: [21] <author> R. Kent Dybvig, David Eby, and Carl Bruggeman. </author> <title> Don't stop the BIBOP: Flexible and efficient storage management for dynamically typed languages. </title> <type> Technical Report 400, </type> <institution> Indiana University, Computer Science Department, </institution> <month> March </month> <year> 1994. </year>
Reference-contexts: Our system uses a hybrid of these three methods, which are described in detail in <ref> [21] </ref>. Every Scheme object is represented by a 32-bit tagged integer. For a heap-allocated object, the tagged integer is a typed pointer. The primary type tag is stored in the lower three bits of the pointer. <p> A resizable segment table associates with each segment its generation and space. Consequently, the segments comprising a given generation and space need not be contiguous. See <ref> [21] </ref> for a more detailed description of our segmented storage model. The garbage collector takes three inputs: the root set of pointers, the oldest generation to be collected (o), and the target generation (t). All objects in generations zero to l will be collected into generation t. <p> Since our collector is generational, we must address the problem of potential cross-generational pointers from obsolete to new code objects. Our segmented heap model allows us to allocate new objects in older generations when necessary <ref> [21] </ref>; thus, we always allocate a new code object in the same generation as the corresponding obsolete code object.
Reference: [22] <author> R. Kent Dybvig and Robert Hieb. </author> <title> A new approach to procedures with variable arity. </title> <journal> Lisp and Symbolic Computation, </journal> <volume> 3(3) </volume> <pages> 229-244, </pages> <year> 1990. </year>
Reference-contexts: They manipulate data using register, register displacement, index, immediate, and special relocation addressing modes. The new pass collects symbolic assembly code and splits it into basic blocks. Machine code is then generated from the blocks. Our system implements procedures of variable arity using case-lambda [12], a variant of lambda* <ref> [22] </ref>. Consequently, there may be multiple entry points to a given procedure. When the compiler determines that a given call site refers to a particular case of some procedure, it generates a direct call to the appropriate entry point.
Reference: [23] <author> R. Kent Dybvig and Robert Hieb. </author> <title> An optimistic strategy for argument register saving in Scheme. </title> <institution> Indiana University Computer Science Department, </institution> <month> April </month> <year> 1991. </year>
Reference: [24] <author> R. Kent Dybvig, Robert Hieb, and Carl Bruggeman. </author> <title> Syntactic abstraction in Scheme. </title> <journal> Lisp and Symbolic Computation, </journal> <volume> 5(4) </volume> <pages> 295-326, </pages> <year> 1993. </year>
Reference-contexts: By propagating these source records through all the intermediate passes, the compiler can associate each source record with an appropriate basic block. In our system, the reader annotates its output with source records that are preserved through CHAPTER 3. EDGE-COUNT PROFILING 53 macro expansion by the syntax-case macro expander <ref> [24] </ref>. Each intermediate representation contains a source field that the transformation passes maintain. The last transformation pass generates a special "source" pseudo-instruction that contains the source record. This instruction is stored in the appropriate basic block, but it does not generate any machine code.
Reference: [25] <author> Robert R. Fenichel and Jerome C. Yochelson. </author> <title> A LISP garbage-collector for virtual-memory computer systems. </title> <journal> Communications of the ACM, </journal> <volume> 12(11) </volume> <pages> 611-612, </pages> <month> November </month> <year> 1969. </year>
Reference-contexts: It would be possible to relax this restriction, but in practice we have found no reason to do so. 4.4 Garbage Collection In order to describe how our generational [32], stop-and-copy <ref> [25] </ref> garbage collector transparently replaces obsolete code objects with recompiled ones, we first summarize how it manages objects without the infrastructure for dynamic recompilation.
Reference: [26] <author> Richard P. Gabriel. </author> <title> Performance and Evaluation of LISP Systems. </title> <publisher> MIT Press series in computer systems. MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1985. </year>
Reference: [27] <author> R. J. Hall. </author> <title> Call Path Profiling. </title> <booktitle> In International Conference on Software Engineering 14, </booktitle> <pages> pages 296-306, </pages> <month> May </month> <year> 1992. </year>
Reference-contexts: It operates on object files and provides functions that make it easy to access procedures, basic blocks, and instructions. The current framework, however, does not allow for dynamically created procedures; all procedures must be in object files at link time. CHAPTER 3. EDGE-COUNT PROFILING 60 Hall <ref> [27] </ref> describes a time-based profiling technique for functional languages that addresses the reuse of functions problem. Rather than focusing on procedures, the technique focuses on procedure calls in their full lexical context. The system uses renaming strategies to create separate instances of procedures for different lexical calling contexts.
Reference: [28] <author> Robert Hieb, R. Kent Dybvig, and Carl Bruggeman. </author> <title> Representing control in the presence of first-class continuations. </title> <booktitle> In Proceedings of the ACM SIGPLAN '90 Conference on Programming Language Design and Implementation, </booktitle> <pages> pages 66-77, </pages> <month> June </month> <year> 1990. </year>
Reference-contexts: This approach is simple to implement, requiring no change to the implementation of continuations. Because the notion of maximal is weakened, however, the profiling overhead increases. Our system's segmented stack implementation supports constant-time continuation operations <ref> [28, 8] </ref>. Implementing the first approach would destroy this property. Moreover, if any procedure is profiled, the system must traverse all activations to find the profiled ones. <p> Representing nontail procedure calls in the graph is complicated by our implementation of multiple return values and run-time error checking. As shown in Figure 13, four words of data are placed in the instruction stream immediately before the single-value return point from each nontail call <ref> [28, 3, 8] </ref>. The first three words are used to support efficient garbage collection and continuation invocation. The last word, the multiple-value return address, is used when a procedure returns more than one value or zero values. <p> The two x's in the figure indicate unused bits. In order to support multiple return values, first-class continuations, and garbage collection efficiently, four words of data are placed in the instruction stream immediately before the single-value return point from each non-tail call <ref> [28, 3, 8] </ref>. The first is a live mask, a bit vector describing which frame locations contain live data, used by the garbage collector. <p> DYNAMIC RECOMPILATION 73 by a bit in the type field of the code object associated with the entry field. The free variable slots are used to store information about the continuation object such as the size and address of its stack segment (see <ref> [28, 8] </ref> for a more detailed description). Since the only pointer to a continuation object's stack segment is in the continuation object, the stack segment is stored in the data space and copied when its continuation object is copied.
Reference: [29] <author> David Kranz. </author> <title> Orbit: an optimizing compiler for Scheme. </title> <type> Technical Report 632, </type> <institution> Yale University, Computer Science Department, </institution> <year> 1988. </year>
Reference-contexts: introduction of temporaries but differs in that any cycle causes a complete spill of all arguments into temporary stack locations. Although Kranz <ref> [29] </ref> describes a register shu*ing algorithm similar to ours, details regarding the selection of the node used to break cycles are not given. Shao and Appel [39, 2] have developed a closure conversion algorithm that exploits control and data flow information to obtain extensive closure sharing.
Reference: [30] <author> Mark Leone and Peter Lee. </author> <title> Lightweight run-time code generation. </title> <booktitle> In Proceedings of the 1994 ACM SIGPLAN Workshop on Partial Evaluation and Semantics-Based Program Manipulation, </booktitle> <pages> pages 97-106, </pages> <year> 1994. </year>
Reference-contexts: Combined with an estimate of the cost of generating code for the procedure, this ratio could also help determine when run-time code generation <ref> [30, 31] </ref> would be profitable. Our system could be extended to recompile (specialize) a procedure based on the run-time values of its free variables.
Reference: [31] <author> Mark Leone and Peter Lee. </author> <title> Optimizing ML with run-time code generation. </title> <booktitle> In Proceedings of the ACM SIGPLAN '96 Conference on Programming Language Design and Implementation, </booktitle> <pages> pages 137-148, </pages> <month> May </month> <year> 1996. </year> <note> BIBLIOGRAPHY 89 </note>
Reference-contexts: Combined with an estimate of the cost of generating code for the procedure, this ratio could also help determine when run-time code generation <ref> [30, 31] </ref> would be profitable. Our system could be extended to recompile (specialize) a procedure based on the run-time values of its free variables.
Reference: [32] <author> Henry Lieberman and Carl Hewitt. </author> <title> A real-time garbage collector based on the lifetimes of objects. </title> <journal> Communications of the ACM, </journal> <volume> 26(6) </volume> <pages> 419-429, </pages> <month> June </month> <year> 1983. </year>
Reference-contexts: It would be possible to relax this restriction, but in practice we have found no reason to do so. 4.4 Garbage Collection In order to describe how our generational <ref> [32] </ref>, stop-and-copy [25] garbage collector transparently replaces obsolete code objects with recompiled ones, we first summarize how it manages objects without the infrastructure for dynamic recompilation.
Reference: [33] <author> Scott McFarling. </author> <title> Procedure merging with instruction caches. </title> <booktitle> In Proceedings of the ACM SIGPLAN '91 Conference on Programming Language Design and Implementation, </booktitle> <pages> pages 71-79, </pages> <month> June </month> <year> 1991. </year>
Reference-contexts: DYNAMIC RECOMPILATION 81 performed at the superblock level. Chen et al. [15] use profile data to guide instruction scheduling by removing constraints that arise from infrequently executed portions of a procedure. McFarling <ref> [33] </ref> uses profile data and specific information about the target architecture's instruction cache to guide procedure inlining. Chapter 5 Conclusions This dissertation describes a fast linear intraprocedural register allocation strategy and an efficient infrastructure for profile-driven dynamic recompilation in Scheme.
Reference: [34] <author> Jason R. C. Patterson. </author> <title> Accurate static branch prediction by value range propagation. </title> <booktitle> In Proceedings of the ACM SIGPLAN '95 Conference on Programming Language Design and Implementation, </booktitle> <pages> pages 67-78, </pages> <month> June </month> <year> 1995. </year>
Reference-contexts: Profile data can assist the compiler is these areas. Basic blocks can be reordered to reduce the number of mispredicted branches and instruction cache misses [35, 36]. Although there are various heuristic algorithms that estimate branch behavior statically, research has demonstrated that profile-based prediction is significantly more effective <ref> [5, 47, 34] </ref>. In current programming environments, profile-based prediction requires a tedious compile-profile-recompile cycle. In our system, the instrumentation for profiling and the subsequent recompilation are done at run time. CHAPTER 4. DYNAMIC RECOMPILATION 63 4.2 Overview Dynamic recompilation proceeds in three phases. <p> They found, however, that on average their static prediction generates code that executes twice as many mispredicted branches as the code generated by profile-based prediction. Wall [47] also found that profile-based prediction is significantly better than any of his heuristic-based predictions. Patterson <ref> [34] </ref> uses value range propagation to predict branch behavior. Even though his technique does better than Ball and Larus's, profile-based prediction is still substantially better. Profiling information can be used to increase the locality of executing code at both intra- and inter-procedural levels.
Reference: [35] <author> Karl Pettis and Robert C. Hansen. </author> <title> Profile guided code positioning. </title> <booktitle> In Proceedings of the ACM SIGPLAN '90 Conference on Programming Language Design and Implementation, </booktitle> <pages> pages 16-27, </pages> <month> June </month> <year> 1990. </year>
Reference-contexts: Section 4.4 describes how the garbage collector transparently replaces code objects with recompiled ones. Section 4.5 illustrates dynamic recompilation using a variant of Pettis and Hansen's basic block reordering algorithm to reduce the number of mispredicted branches and instruction cache misses <ref> [35] </ref>. Section 4.6 reports the cost of recompilation and the effectiveness of dynamic block reordering versus static block ordering based on the edge-count estimator described 61 CHAPTER 4. DYNAMIC RECOMPILATION 62 in Section 3.4. <p> Profile data can assist the compiler is these areas. Basic blocks can be reordered to reduce the number of mispredicted branches and instruction cache misses <ref> [35, 36] </ref>. Although there are various heuristic algorithms that estimate branch behavior statically, research has demonstrated that profile-based prediction is significantly more effective [5, 47, 34]. In current programming environments, profile-based prediction requires a tedious compile-profile-recompile cycle. <p> remembered set and promote the new code object to the older generation during collection. 4.5 Block Reordering Example To demonstrate the feasibility and utility of dynamic recompilation, we use a variant of Pettis and Hansen's basic block reordering algorithm to reduce the number of mispredicted branches and instruction cache misses <ref> [35] </ref>. We also use edge-count profile data to decrease profiling overhead by re-running the maximal spanning tree algorithm to improve counter placement, as described in Chapter 3. The block reordering algorithm proceeds in two steps. <p> Even though his technique does better than Ball and Larus's, profile-based prediction is still substantially better. Profiling information can be used to increase the locality of executing code at both intra- and inter-procedural levels. Section 4.5 summarizes Pettis and Hansen's <ref> [35] </ref> intraprocedural block reordering algorithm. Samples [36] explores a similar intrapro-cedural algorithm that reduces instruction cache miss rates by up to 50%. Pettis and Hansen [35] also describe an interprocedural algorithm that places procedures in memory such that those that execute close together in time will also be close together in <p> Profiling information can be used to increase the locality of executing code at both intra- and inter-procedural levels. Section 4.5 summarizes Pettis and Hansen's <ref> [35] </ref> intraprocedural block reordering algorithm. Samples [36] explores a similar intrapro-cedural algorithm that reduces instruction cache miss rates by up to 50%. Pettis and Hansen [35] also describe an interprocedural algorithm that places procedures in memory such that those that execute close together in time will also be close together in memory. Since determining the optimal ordering is NP-complete, they use a greedy "closest is best" strategy.
Reference: [36] <author> A. </author> <title> Dain Samples. Profile-driven compilation. </title> <type> Technical Report 627, </type> <institution> University of California, Berkeley, </institution> <year> 1991. </year>
Reference-contexts: Profile data can assist the compiler is these areas. Basic blocks can be reordered to reduce the number of mispredicted branches and instruction cache misses <ref> [35, 36] </ref>. Although there are various heuristic algorithms that estimate branch behavior statically, research has demonstrated that profile-based prediction is significantly more effective [5, 47, 34]. In current programming environments, profile-based prediction requires a tedious compile-profile-recompile cycle. <p> Even though his technique does better than Ball and Larus's, profile-based prediction is still substantially better. Profiling information can be used to increase the locality of executing code at both intra- and inter-procedural levels. Section 4.5 summarizes Pettis and Hansen's [35] intraprocedural block reordering algorithm. Samples <ref> [36] </ref> explores a similar intrapro-cedural algorithm that reduces instruction cache miss rates by up to 50%. Pettis and Hansen [35] also describe an interprocedural algorithm that places procedures in memory such that those that execute close together in time will also be close together in memory.
Reference: [37] <author> Patrick M. Sansom and Simon L. Peyton Jones. </author> <title> Time and space profiling for non-strict higher-order functional languages. </title> <booktitle> In Proceedings of the 22nd Annual ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages, </booktitle> <pages> pages 355-366, </pages> <month> January </month> <year> 1995. </year>
Reference-contexts: It assumes the entire program text is available to the compiler at once. Because the system defines the lexical context of an anonymous procedure as the context of the closest enclosing named procedure, it does not handle anonymous and higher-order procedures very well. Sansom and Peyton Jones <ref> [37] </ref> describe a time-based profiling technique for lazy functional languages.
Reference: [38] <author> Robert Sedgewick. </author> <title> Algorithms, chapter 31. </title> <publisher> Addison-Wesley Publishing Company, </publisher> <address> second edition, </address> <year> 1988. </year>
Reference-contexts: Alternatively, we could add edges from the exit block to all the entry blocks. For efficiency, our compiler uses the priority-first search algorithm for finding a maximal spanning tree <ref> [38] </ref>. Its worst-case behavior is O ((E + B) log B), where B is the number of blocks and E is the number of edges. Since each block has no more than two outgoing edges, E is O (B).
Reference: [39] <author> Zhong Shao and Andrew W. Appel. </author> <title> Space-efficient closure representations. </title> <booktitle> In Proceedings of the 1994 ACM Conference on LISP and Functional Programming, </booktitle> <pages> pages 150-161, </pages> <year> 1994. </year>
Reference-contexts: introduction of temporaries but differs in that any cycle causes a complete spill of all arguments into temporary stack locations. Although Kranz [29] describes a register shu*ing algorithm similar to ours, details regarding the selection of the node used to break cycles are not given. Shao and Appel <ref> [39, 2] </ref> have developed a closure conversion algorithm that exploits control and data flow information to obtain extensive closure sharing. This sharing enhances the benefit they obtain from allocating closures in registers. <p> Other researchers have investigated the use of lambda lifting to increase the number of arguments available for placement in registers <ref> [39, 20] </ref>. While lambda lifting can easily result in net performance decreases, it is worth investigating whether lambda lifting with an appropriate set of heuristics such as those described in [39] can indeed increase the effectiveness of our register allocator without significantly increasing compile time. <p> While lambda lifting can easily result in net performance decreases, it is worth investigating whether lambda lifting with an appropriate set of heuristics such as those described in <ref> [39] </ref> can indeed increase the effectiveness of our register allocator without significantly increasing compile time. The infrastructure for profile-driven dynamic recompilation enables completely transparent recompilation of procedures, even while they are executing.
Reference: [40] <author> Richard L. </author> <title> Sites, editor. Alpha Architecture Reference Manual. </title> <publisher> Digital Press, </publisher> <year> 1992. </year>
Reference-contexts: As before, the measurements were taken on a DEC Alpha 3000/600 running Digital UNIX V4.0A. The Alpha architecture encourages hardware and compiler implementors to predict backward conditional branches taken and forward conditional branches not taken <ref> [40] </ref>. Current Alpha implementations use this static model as a starting point for dynamic branch prediction. We computed the mispredicted branch percentage using the static model as a metric for determining the effectiveness of the block reordering algorithm. Table 9 gives the results.
Reference: [41] <author> Patrick G. Sobalvarro. </author> <title> A lifetime-based garbage collector for LISP systems on general-purpose computers. B.S. </title> <type> thesis, </type> <institution> Massachusetts Institute of Technology, Electrical Engineering and Computer Science Department, Cambridge, Massachusetts, </institution> <year> 1988. </year>
Reference-contexts: Pointers from objects in generations older than o to objects in younger generations are relocated by sweeping the appropriate "dirty" sections of the older generations. The dirty sections are identified using a form of card marking <ref> [41, 49] </ref>. The overhead of maintaining the marks is reduced by recognizing that only mutable objects can contain pointers to younger generations. Consequently, dirty marks are not maintained for the pure, code, data, and continuation spaces. Each generation/space pair requires a separate allocation pointer.
Reference: [42] <author> Amitabh Srivastava and Alan Eustace. </author> <title> ATOM: A system for building customized program analysis tools. </title> <type> Technical Report 94/2, </type> <institution> Digital Equipment Corporations's Western Research Laboratory, </institution> <month> March </month> <year> 1994. </year> <note> BIBLIOGRAPHY 90 </note>
Reference-contexts: Finally, compression of the resulting symbolic information would reduce object file size considerably. 3.7 Related Work Profiling can be divided into two main areas: count-based and time-based. Ball and Larus's optimal edge-count profiling strategy [4] is the main work in count-based profiling. ATOM <ref> [42] </ref> is a system used to specify code-instrumenting tools such as profilers and can be used to implement edge-count strategies. It operates on object files and provides functions that make it easy to access procedures, basic blocks, and instructions.
Reference: [43] <author> P. A. Steenkiste. </author> <title> Tags and run-time type checking. </title> <editor> In Peter Lee, editor, </editor> <booktitle> Advanced Language Implementation, chapter 1, </booktitle> <pages> pages 3-24. </pages> <publisher> The MIT Press, </publisher> <year> 1991. </year>
Reference-contexts: By retaining original procedures as long as they have live unassociated return addresses, the collector can support these optimizations| even on running procedures. 4.3 Ob ject Representations Every Scheme object consists of a type tag and a value. There are three main methods for representing the type tag <ref> [43] </ref>: associating the type tag with the object (typed objects), associating the type tag with all pointers to the object (typed pointers), and associating the tag type with the segment in which the objects reside (big bag of pages).
Reference: [44] <author> P. A. Steenkiste and J. L. Hennessy. </author> <title> A simple interprocedural register allocation algorithm and its effectiveness for Lisp. </title> <journal> Transactions on Programming Languages and Systems, </journal> <volume> 11(1) </volume> <pages> 1-32, </pages> <month> January </month> <year> 1989. </year>
Reference-contexts: The baseline for comparison 82 CHAPTER 5. CONCLUSIONS 83 is efficient code generated by an optimizing compiler that already makes extensive use of global registers and local register allocation. This is within range of improvements reported for interprocedural register allocation <ref> [44, 17] </ref>. Although the compiler now spends around 7% of its time on register allocation, the compiler actually runs faster since it is self-compiled and benefits from its own register allocation strategy.
Reference: [45] <author> M. Esen Tuna, Steven D. Johnson, and Robert G. Burger. </author> <title> Continuations in hardware-software codesign. </title> <booktitle> In Proceedings of the IEEE International Conference on Computer Design, </booktitle> <pages> pages 264-269, </pages> <month> October </month> <year> 1994. </year>
Reference: [46] <author> Oscar Waddell. </author> <title> SWL reference manual. </title> <institution> Indiana University and Cadence Research Systems, </institution> <month> August </month> <year> 1996. </year>
Reference-contexts: Second, we build an association list of source expressions and block counts from CHAPTER 3. EDGE-COUNT PROFILING 54 frequencies the source records stored in each basic block. Third, we use this list and graphical tools from the Scheme Widget Library <ref> [46] </ref> to pop up a window with the source code colored according to the counts. The programmer can also click on an expression, and the system finds and displays the corresponding count. Figure 12 gives an example.
Reference: [47] <author> David W. Wall. </author> <title> Predicting program behavior using real or estimated profiles. </title> <booktitle> In Proceedings of the ACM SIGPLAN '91 Conference on Programming Language Design and Implementation, </booktitle> <pages> pages 59-70, </pages> <month> June </month> <year> 1991. </year>
Reference-contexts: Profile data can assist the compiler is these areas. Basic blocks can be reordered to reduce the number of mispredicted branches and instruction cache misses [35, 36]. Although there are various heuristic algorithms that estimate branch behavior statically, research has demonstrated that profile-based prediction is significantly more effective <ref> [5, 47, 34] </ref>. In current programming environments, profile-based prediction requires a tedious compile-profile-recompile cycle. In our system, the instrumentation for profiling and the subsequent recompilation are done at run time. CHAPTER 4. DYNAMIC RECOMPILATION 63 4.2 Overview Dynamic recompilation proceeds in three phases. <p> Ball and Larus [5] describe a simple heuristic that can significantly reduce the number of mispredicted branches. They found, however, that on average their static prediction generates code that executes twice as many mispredicted branches as the code generated by profile-based prediction. Wall <ref> [47] </ref> also found that profile-based prediction is significantly better than any of his heuristic-based predictions. Patterson [34] uses value range propagation to predict branch behavior. Even though his technique does better than Ball and Larus's, profile-based prediction is still substantially better.
Reference: [48] <author> Paul R. Wilson. </author> <title> Uniprocessor garbage collection techniques. </title> <editor> In Yves Bekkers and Jacques Cohen, editors, </editor> <booktitle> Proceedings of the International Workshop on Memory Management, </booktitle> <pages> pages 1-42. </pages> <publisher> Springer Verlag, </publisher> <month> September </month> <year> 1992. </year>
Reference-contexts: We then explain the minor modifications necessary to support the infrastructure. (See Wilson <ref> [48] </ref> for an excellent survey of garbage collection techniques.) Our storage management system subdivides objects logically by generation and space. Objects of the same generation have survived the same number of collections. Objects of the same space are swept the same way by the collector.
Reference: [49] <author> Paul R. Wilson and Thoman G. Moher. </author> <title> Design of the opportunistic garbage collector. </title> <booktitle> In ACM OOPSLA '89 Conference Proceedings, </booktitle> <pages> pages 23-35, </pages> <month> October </month> <year> 1989. </year>
Reference-contexts: Pointers from objects in generations older than o to objects in younger generations are relocated by sweeping the appropriate "dirty" sections of the older generations. The dirty sections are identified using a form of card marking <ref> [41, 49] </ref>. The overhead of maintaining the marks is reduced by recognizing that only mutable objects can contain pointers to younger generations. Consequently, dirty marks are not maintained for the pure, code, data, and continuation spaces. Each generation/space pair requires a separate allocation pointer.

References-found: 49

