URL: http://www.cs.cmu.edu/afs/cs.cmu.edu/Web/People/thrun/papers/mitchell.EBNN_newell_book.ps.gz
Refering-URL: http://www.cs.cmu.edu/afs/cs.cmu.edu/Web/People/thrun/papers/mitchell.EBNN_newell_book.html
Root-URL: http://www.cs.cmu.edu
Email: E-mail: mitchell@cs.cmu.edu  E-mail: thrun@uran.cs.bonn.edu  
Phone: 1  
Title: Learning Analytically and Inductively  
Author: Tom M. Mitchell Sebastian B. Thrun 
Address: Pittsburgh, PA 15213, U.S.A.  Romerstr. 164, 53117 Bonn, Germany  
Affiliation: School of Computer Science Carnegie Mellon University  University of Bonn Institut fur Informatik III  Learning  
Abstract: Learning is a fundamental component of intelligence, and a key consideration in designing cognitive architectures such as Soar [ Laird et al., 1986 ] . This chapter considers the question of what constitutes an appropriate general-purpose learning mechanism. We are interested in mechanisms that might explain and reproduce the rich variety of learning capabilities of humans, ranging from learning perceptual-motor skills such as how to ride a bicycle, to learning highly cognitive tasks such as how to play chess. Research on learning in fields such as cognitive science, artificial intelligence, neurobiology, and statistics has led to the identification of two distinct classes of learning methods: inductive and analytic. Inductive methods, such as neural network Backpropagation, learn general laws by finding statistical correlations and regularities among a large set of training examples. In contrast, analytical methods, such as Explanation-Based Learning, acquire general laws from many fewer training examples. They rely instead on prior knowledge to analyze individual training examples in detail, then use this analysis to distinguish relevant example features from the irrelevant. The question considered in this chapter is how to best combine inductive and analytical learning in an architecture that seeks to cover the range of learning exhibited by intelligent systems such as humans. We present a specific learning mechanism, Explanation Based Neural Network learning (EBNN), that blends these two types of learning, and present experimental results demonstrating its ability to learn control strategies for a mobile robot using 
Abstract-found: 1
Intro-found: 1
Reference: [ Ahn and Brewer, 1993 ] <author> Woo-Kyoung Ahn and William F. Brewer. </author> <title> Psychological studies of explanation-based learning. </title> <editor> In Gerald DeJong, editor, </editor> <title> Investigating Explanation-Based Learning. </title> <publisher> Kluwer Academic Publishers, </publisher> <address> Boston/Dordrecht/London, </address> <year> 1993. </year>
Reference-contexts: While our purpose here is to examine computer learning algorithms, it is interesting to note that research on human learning provides support for the conjecture that humans learn through such explanations (see, for example, [ Chi and Bassok, 1989 ] , [ Qin et al., 1992 ] , <ref> [ Ahn and Brewer, 1993 ] </ref> ). Analytical learning methods have been used successfully in a number of applications - notably for learning rules to control search.
Reference: [ Barto et al., 1991 ] <author> Andy G. Barto, Steven J. Bradtke, and Satinder P. Singh. </author> <title> Real-time learning and control using asynchronous dynamic programming. </title> <type> Technical Report COINS 91-57, </type> <institution> Department of Computer Science, University of Massachusetts, </institution> <address> MA, </address> <month> August </month> <year> 1991. </year>
Reference-contexts: In essence, Q-Learning constructs utility functions Q (s; a) that map sensations s and actions a to task-specific utility values. Q-values will be positive for final successes and negative for final failures. In between, utilities are calculated recursively using an asynchronous dynamic programming technique <ref> [ Barto et al., 1991 ] </ref> . More specifically, the utility Q (s t ; a t ) at time t is estimated through a mixture of the utilities of subsequent observation-action pairs, up to the final utility at the end of the episode.
Reference: [ Chi and Bassok, 1989 ] <author> Michelene T.H. Chi and Miriam Bassok. </author> <title> Learning from examples via self-explanations. </title> <editor> In Lauren B. Resnick, editor, </editor> <title> Knowing, learning, and instruction : essays in honor of Robert Glaser. </title> <editor> L. </editor> <publisher> Erlbaum Associates, </publisher> <address> Hillsdale, N.J., </address> <year> 1989. </year> <month> 22 </month>
Reference-contexts: Explanation-based learning generalizes through explaining and analyzing training instances in terms of prior knowledge. While our purpose here is to examine computer learning algorithms, it is interesting to note that research on human learning provides support for the conjecture that humans learn through such explanations (see, for example, <ref> [ Chi and Bassok, 1989 ] </ref> , [ Qin et al., 1992 ] , [ Ahn and Brewer, 1993 ] ). Analytical learning methods have been used successfully in a number of applications - notably for learning rules to control search.
Reference: [ DeJong and Mooney, 1986 ] <author> Gerald DeJong and Raymond Mooney. </author> <title> Explanation-based learning: An alternative view. </title> <journal> Machine Learning, </journal> <volume> 1(2) </volume> <pages> 145-176, </pages> <year> 1986. </year>
Reference-contexts: The most common analytical learning method is explanation-based learning <ref> [ DeJong and Mooney, 1986 ] </ref> , [ Mitchell et al., 1986 ] . To illustrate, consider the problem of learning to play chess. More specifically, consider the subtask of learning to recognize chess positions in which one's queen will be lost within the next few moves.
Reference: [ Doorenbos, 1993 ] <author> Robert E. Doorenbos. </author> <title> Matching 100,000 learned rules. </title> <booktitle> In Proceeding of the Eleventh National Conference on Artificial Intelligence AAAI-93, </booktitle> <pages> pages 290-296, </pages> <address> Menlo Park, CA, 1993. </address> <publisher> AAAI, AAAI Press/The MIT Press. </publisher>
Reference-contexts: Soar's analytical chunking mechanism has been shown to learn successfully to speed up problem solving across a broad range of domains. For example, <ref> [ Doorenbos, 1993 ] </ref> presents results in which over 100,000 productions are learned from such explanations within one particular domain. 2.2 Inductive Learning Whereas analytical learning can produce appropriate generalizations by analyzing single training examples, it requires strong prior knowledge about its domain in order to construct appropriate explanations.
Reference: [ Fu, 1989 ] <author> Li-Min Fu. </author> <title> Integration of neural heuristics into knowledge-based inference. </title> <journal> Connection Science, </journal> <volume> 1(3) </volume> <pages> 325-339, </pages> <year> 1989. </year>
Reference-contexts: For example, [ Shavlik and Towell, 1989 ] describes a method called KBANN for using prior symbolic knowledge to initialize the structure and weights of a neural network, which is then inductively refined using the Backpropagation method. A similar method has been reported by <ref> [ Fu, 1989 ] </ref> . [ Pazzani et al., 1991 ] describes a combined inductive/analytical method called FOCL for learning sets of horn clauses, demonstrating its ability to operate robustly given errors in the initial domain knowledge. [ Ourston and Mooney, 1994 ] describes a method called EITHER for refining domain
Reference: [ Gullapalli, 1992 ] <author> Vijaykumar Gullapalli. </author> <title> Reinforcement Learning and its Application to Control. </title> <type> PhD thesis, </type> <institution> Department of Computer and Information Science, University of Massachusetts, </institution> <year> 1992. </year>
Reference-contexts: In our implementation, the Q function was represented by a collection of artificial neural networks which mapped sensations s to utility values, one for each action a. Recently, Q-Learning and other related dynamic programming algorithms have been applied successfully to game playing domains [ Tesauro, 1992 ] and robotics <ref> [ Gullapalli, 1992 ] </ref> . In these previous approaches, the update of the target networks was purely inductive. EBNN, applied to Q-Learning, extends inductive learning by an analytical component.
Reference: [ Khatib, 1986 ] <author> Oussama Khatib. </author> <title> Real-time obstacle avoidance for robot manipulator and mobile robots. </title> <journal> The International Journal of Robotics Research, </journal> <volume> 5(1) </volume> <pages> 90-98, </pages> <year> 1986. </year>
Reference-contexts: In order to avoid collisions, the robot employed a pre-coded obstacle avoidance routine based on potential field navigation <ref> [ Khatib, 1986 ] </ref> . Whenever the projected path of the robot was blocked by an obstacle, the robot decelerated and, if necessary, changed its motion direction (regardless of the commanded action). Xavier was operated continuously in real-time.
Reference: [ Laird and Rosembloom, 1990 ] <author> John E. Laird and Paul S. Rosembloom. </author> <title> Integrating execution, planning, and learning in soar for external environments. </title> <booktitle> In Proceeding of the Eigth National Conference on Artificial Intelligence AAAI-90, </booktitle> <pages> pages 1022-1029, </pages> <address> Menlo Park, CA, 1990. </address> <publisher> AAAI, AAAI Press/The MIT Press. </publisher>
Reference-contexts: Even the communication delay caused by heavy network traffic would have to be explainable, since it matters crucially for the outcome of the robot's actions. While past research has illustrated the feasibility of hand-coding models for idealized and usually noise-free learning scenarios <ref> [ Laird and Rosembloom, 1990 ] </ref> , [ Mitchell, 1990 ] , in a domain as complex and as stochastic as Xavier's world this will be an unreasonably difficult undertaking.
Reference: [ Laird et al., 1986 ] <author> John Laird, Paul Rosenbloom, and Allen Newell. </author> <title> Chunking in SOAR: The anatomy of a general learning mechanism. </title> <journal> Machine Learning, </journal> <volume> 1(1) </volume> <pages> 11-46, </pages> <year> 1986. </year>
Reference-contexts: 1 Learning Learning is a fundamental component of intelligence, and a key consideration in designing cognitive architectures such as Soar <ref> [ Laird et al., 1986 ] </ref> . This chapter considers the question of what constitutes an appropriate general-purpose learning mechanism. <p> It has been demonstrated to learn search control rules comparable to hand-coded rules in a variety of task domains [ Minton et al., 1989 ] . The chunking mechanism in Soar <ref> [ Laird et al., 1986 ] </ref> also provides an example of analytical learning, as explained in [ Rosenbloom and Laird, 1986 ] . In Soar, problem solving corresponds to search in problem spaces (a problem space is defined by problem states and operators). <p> Chunking is a variant of explanation-based learning, as discussed in [ Rosenbloom and Laird, 1986 ] . For the current discussion, the key points regarding learning in Soar are: * Chunking is the only architectural learning mechanism in Soar. It has been claimed <ref> [ Laird et al., 1986 ] </ref> that chunking is a sufficient architectural learning mechanism. * The chunking mechanism preserves the correctness of Soar's prior knowledge. More precisely, each production rule created by chunking follows deductively from previous rules in the system.
Reference: [ Lin, 1992 ] <author> Long-Ji Lin. </author> <title> Self-supervised Learning by Reinforcement and Artificial Neural Networks. </title> <type> PhD thesis, </type> <institution> Carnegie Mellon University, School of Computer Science, </institution> <address> Pittsburgh, PA, </address> <year> 1992. </year>
Reference-contexts: When training the Q networks, we explicitly memorized all training data, and used a recursive replay technique similar to experience replay described in <ref> [ Lin, 1992 ] </ref> . The update parameter fl was set to 0:9, and was set to 0:7. In all cases Xavier learned to navigate to a static target location in less than 19 episodes, each of which was between 2 and 11 actions in length.
Reference: [ Masuoka, 1993 ] <author> Ryusuke Masuoka. </author> <title> Noise robustness of EBNN learning. </title> <booktitle> In Proceedings of the International Joint Conference on Neural Networks, </booktitle> <month> October </month> <year> 1993. </year>
Reference-contexts: Gradient descent is employed to iteratively minimize E. Notice that in our implementation we used a modified version of the Tangent-Prog algorithm [ Simard et al., 1992 ] to refine the weights and biases of the target network <ref> [ Masuoka, 1993 ] </ref> . What is a reasonable method for weighting the contributions of the inductive versus analytical components of learning; that is, for selecting a value for ff? Because the domain theory might be incorrect, the analytically extracted slopes for a particular training example might be misleading.
Reference: [ Miller and Laird, 1991 ] <author> Craig S. Miller and John E. Laird. </author> <title> A constraint-motivated lexical acquisition model. </title> <booktitle> In Proceedings of the Thirteenth Annual Meeting of the Cognitive science Society, </booktitle> <pages> pages 827-831, </pages> <address> Hillsdale, NJ, 1991. </address> <publisher> Erlbaum. </publisher>
Reference-contexts: training examples, and can result in behavior not produced by the initial knowledge, we will consider this an inductive effect. * Explicit inductive learning methods have also been implemented on top of the Soar architecture by providing problem spaces that perform induction (e.g., [ Rosenbloom and Aasman, 1990 ] , <ref> [ Miller and Laird, 1991 ] </ref> ). The detailed implementation of this inductive mechanism relies on the above effect of screening incorrect rules by learning new rules that better fit the observed data.
Reference: [ Minton et al., 1989 ] <author> Steve Minton, Jaime Carbonnel, Craig A. Knoblock, Dan R. Kuokka, Oren Etzioni, and Yolanda Gil. </author> <title> Explanation-based learning: A problem solving perspective. </title> <journal> Artificial Intelligence, </journal> <volume> 40 </volume> <pages> 63-118, </pages> <year> 1989. </year>
Reference-contexts: Analytical learning methods have been used successfully in a number of applications - notably for learning rules to control search. For example, Prodigy <ref> [ Minton et al., 1989 ] </ref> is a domain-independent framework for means-ends planning that uses explanation-based learning to acquire search control knowledge. Prodigy learns general rules that characterize concepts such as situations in which pursuing subgoal ?x will lead to backtracking. <p> Given 3 a specific problem solving domain defined by a set of states, operators, and goals, Prodigy learns control rules that significantly reduce backtracking when solving problems in this domain. It has been demonstrated to learn search control rules comparable to hand-coded rules in a variety of task domains <ref> [ Minton et al., 1989 ] </ref> . The chunking mechanism in Soar [ Laird et al., 1986 ] also provides an example of analytical learning, as explained in [ Rosenbloom and Laird, 1986 ] .
Reference: [ Mitchell and Thrun, 1993 ] <author> Tom M. Mitchell and Sebastian B. Thrun. </author> <title> Explanation-based neural network learning for robot control. </title> <editor> In S. J. Hanson, J. Cowan, and C. L. Giles, editors, </editor> <booktitle> Advances in Neural Information Processing Systems 5, </booktitle> <pages> pages 287-294, </pages> <address> San Mateo, CA, 1993. </address> <publisher> Morgan Kaufmann. </publisher> <pages> 23 </pages>
Reference-contexts: This weighting scheme attempts to give accurate slopes a large weight in training, 9 while ignoring inaccurate slopes. This heuristic weighting scheme, called LOB* <ref> [ Mitchell and Thrun, 1993 ] </ref> , is based on the heuristic assumption that the accuracy of the explanation's slopes are correlated to the accuracy of the explanation's predictions. This completes the description of the EBNN learning mechanism. To summarize, EBNN refines the target network using a combined inductive-analytical mechanism.
Reference: [ Mitchell et al., 1986 ] <author> Tom M. Mitchell, Rich Keller, and Smadar Kedar-Cabelli. </author> <title> Explanation-based generalization: A unifying view. </title> <journal> Machine Learning, </journal> <volume> 1(1) </volume> <pages> 47-80, </pages> <year> 1986. </year>
Reference-contexts: The most common analytical learning method is explanation-based learning [ DeJong and Mooney, 1986 ] , <ref> [ Mitchell et al., 1986 ] </ref> . To illustrate, consider the problem of learning to play chess. More specifically, consider the subtask of learning to recognize chess positions in which one's queen will be lost within the next few moves.
Reference: [ Mitchell, 1990 ] <author> Tom M. Mitchell. </author> <title> Becoming increasingly reactive. </title> <booktitle> In Proceedings of 1990 AAAI Conference, </booktitle> <address> Menlo Park, CA, </address> <month> August </month> <year> 1990. </year> <booktitle> AAAI, </booktitle> <publisher> AAAI Press / The MIT Press. </publisher>
Reference-contexts: Even the communication delay caused by heavy network traffic would have to be explainable, since it matters crucially for the outcome of the robot's actions. While past research has illustrated the feasibility of hand-coding models for idealized and usually noise-free learning scenarios [ Laird and Rosembloom, 1990 ] , <ref> [ Mitchell, 1990 ] </ref> , in a domain as complex and as stochastic as Xavier's world this will be an unreasonably difficult undertaking.
Reference: [ O'Sullivan, 1994 ] <author> Joseph O'Sullivan. </author> <title> Xavier manual. </title> <institution> Carnegie Mellon University, Learning Robot Lab Internal Document contact josullvn@cs.cmu.edu, </institution> <month> January </month> <year> 1994. </year>
Reference-contexts: Here we report an application of EBNN to learning mobile robot navigation using Q-Learning [ Watkins, 1989 ] . Xavier <ref> [ O'Sullivan, 1994 ] </ref> , the robot at hand, is shown in Fig. 5. Xavier is equipped with a ring of 24 sonar sensors, a laser range finder, and a color camera mounted on a pan/tilt head. Sonar sensors return approximate echo distances along with noise.
Reference: [ Ourston and Mooney, 1994 ] <author> Dirk Ourston and Raymond Mooney. </author> <title> Theory refinement combining analytical and empirical methods. </title> <journal> Artificial Intelligence, </journal> <volume> 66 </volume> <pages> 311-344, </pages> <year> 1994. </year>
Reference-contexts: A similar method has been reported by [ Fu, 1989 ] . [ Pazzani et al., 1991 ] describes a combined inductive/analytical method called FOCL for learning sets of horn clauses, demonstrating its ability to operate robustly given errors in the initial domain knowledge. <ref> [ Ourston and Mooney, 1994 ] </ref> describes a method called EITHER for refining domain theories in the light of additional empirical data. While research in this area is very active, the question of how to best blend inductive and analytical learning is still open.
Reference: [ Pazzani et al., 1991 ] <author> Michael J. Pazzani, Clifford A. Brunk, and Glenn Silverstein. </author> <title> A knowledge-intensive aproach to learning relational concepts. </title> <booktitle> In Proceedings of the Eigth International Workshop on Machine Learning, </booktitle> <pages> pages 432-436, </pages> <address> Evanston, IL, </address> <month> June </month> <year> 1991. </year>
Reference-contexts: For example, [ Shavlik and Towell, 1989 ] describes a method called KBANN for using prior symbolic knowledge to initialize the structure and weights of a neural network, which is then inductively refined using the Backpropagation method. A similar method has been reported by [ Fu, 1989 ] . <ref> [ Pazzani et al., 1991 ] </ref> describes a combined inductive/analytical method called FOCL for learning sets of horn clauses, demonstrating its ability to operate robustly given errors in the initial domain knowledge. [ Ourston and Mooney, 1994 ] describes a method called EITHER for refining domain theories in the light of
Reference: [ Pomerleau, 1989 ] <author> D. A. Pomerleau. ALVINN: </author> <title> an autonomous land vehicle in a neural network. </title> <type> Technical Report CMU-CS-89-107, </type> <institution> Computer Science Dept. Carnegie Mellon University, </institution> <address> Pittsburgh PA, </address> <year> 1989. </year>
Reference: [ Qin et al., 1992 ] <author> Yulin Qin, Tom M. Mitchell, , and Simon Herbert. </author> <title> Using EBG to simulate human learning from examples and learning by doing. </title> <booktitle> In Proceedings of the Florida AI Research Symposium, </booktitle> <pages> pages 235-239, </pages> <month> April </month> <year> 1992. </year>
Reference-contexts: While our purpose here is to examine computer learning algorithms, it is interesting to note that research on human learning provides support for the conjecture that humans learn through such explanations (see, for example, [ Chi and Bassok, 1989 ] , <ref> [ Qin et al., 1992 ] </ref> , [ Ahn and Brewer, 1993 ] ). Analytical learning methods have been used successfully in a number of applications - notably for learning rules to control search.
Reference: [ Rosenbloom and Aasman, 1990 ] <author> Paul S. Rosenbloom and Jans Aasman. </author> <title> Knowledge level and inductive uses of chunking (ebl). </title> <booktitle> In Proceedings of the Eighth National Conference on Artificial Intelligence, </booktitle> <pages> pages 821-827, </pages> <address> Boston, 1990. </address> <publisher> AAAI, MIT Press. </publisher>
Reference-contexts: Because this effect depends on the observed training examples, and can result in behavior not produced by the initial knowledge, we will consider this an inductive effect. * Explicit inductive learning methods have also been implemented on top of the Soar architecture by providing problem spaces that perform induction (e.g., <ref> [ Rosenbloom and Aasman, 1990 ] </ref> , [ Miller and Laird, 1991 ] ). The detailed implementation of this inductive mechanism relies on the above effect of screening incorrect rules by learning new rules that better fit the observed data.
Reference: [ Rosenbloom and Laird, 1986 ] <author> Paul S. Rosenbloom and John E. Laird. </author> <title> Mapping explanation-based generalization onto soar. </title> <type> Technical Report 1111, </type> <institution> Stanford University, Dept. of Computer Science, Stanford, </institution> <address> CA, </address> <year> 1986. </year>
Reference-contexts: It has been demonstrated to learn search control rules comparable to hand-coded rules in a variety of task domains [ Minton et al., 1989 ] . The chunking mechanism in Soar [ Laird et al., 1986 ] also provides an example of analytical learning, as explained in <ref> [ Rosenbloom and Laird, 1986 ] </ref> . In Soar, problem solving corresponds to search in problem spaces (a problem space is defined by problem states and operators). <p> The first subsection below briefly summarizes current approaches, and the following subsection considers the possibility of incorporating EBNN within Soar. 4.1 Current Learning Mechanisms in Soar Soar has a single, analytical learning mechanism embedded in the architecture: chunking. Chunking is a variant of explanation-based learning, as discussed in <ref> [ Rosenbloom and Laird, 1986 ] </ref> . For the current discussion, the key points regarding learning in Soar are: * Chunking is the only architectural learning mechanism in Soar.
Reference: [ Rosenbloom et al., 1987 ] <author> Paul S. Rosenbloom, John Laird, and Allen Newell. </author> <title> Knowledge level learning in soar. Technical Report AIP-8 (Artificial Intelligence and Psychology Project), </title> <institution> Carnegie Mellon University, </institution> <address> Pittsburgh, PA 15213, </address> <year> 1987. </year>
Reference-contexts: In this case, the ill effects of one incorrect rule may be screened by a second (contradictory) rule that applies to some of the same situations (e.g., see <ref> [ Rosenbloom et al., 1987 ] </ref> ). Whether this beneficial screening will occur depends on the form of the initial knowledge, and on the sequence in which training examples are presented.
Reference: [ Rosenbloom, 1983 ] <author> Paul Rosenbloom. </author> <title> The Chunking of Goal Hierarchies: A model of Practice and Stimmuls-Response compatibility. </title> <type> PhD thesis, </type> <institution> Carnegie Mellon University, School of Computer Science, </institution> <address> Pittsburgh, PA, </address> <year> 1983. </year> <note> Technical Report CMU-CS-83-148. </note>
Reference-contexts: This will lead to less abrupt changes to system knowledge than in Chunking-Soar, where a complete rule is formulated based solely on the analysis of a single example. Others have noted <ref> [ Rosenbloom, 1983 ] </ref> that learning in Chunking-Soar may be too abrupt to be a correct model of human learning. * EBNN-Soar may avoid the average growth effect encountered by Chunking-Soar [ Tambe et al., 1992 ] , in which the large number of learned chunks can lead to significant slowdown
Reference: [ Rumelhart et al., 1986 ] <author> David E. Rumelhart, Geoffrey E. Hinton, and Ronald J. Williams. </author> <title> Learning internal representations by error propagation. </title> <editor> In D. E. Rumelhart and J. L. McClelland, editors, </editor> <booktitle> Parallel Distributed Processing. </booktitle> <volume> Vol. I + II. </volume> <publisher> MIT Press, </publisher> <year> 1986. </year> <month> 24 </month>
Reference-contexts: Notice this task involves learning control knowledge, much like the control knowledge learned by Prodigy and Soar. In this domain, however, a complete and correct model of the effects of different steering actions is not known a priori. Therefore an inductive learning method, neural network Backpropagation <ref> [ Rumelhart et al., 1986 ] </ref> , is used to learn a mapping from the camera image to the appropriate steering direction. Thousands of training examples are collected 4 by recording camera images and steering commands while a human drives the vehicle for approximately 10 minutes. <p> Neural networks are used to draw inferences about the domain, just as rules are used to draw inferences in symbolic representations. By using neural network representation, pure inductive learning algorithms such as the Backpropagation algorithm <ref> [ Rumelhart et al., 1986 ] </ref> become applicable. In addition, EBNN includes an analytic learning component, based on explaining and analyzing training instances in terms of other, previously learned networks. In what follows we describe the EBNN learning mechanism.
Reference: [ Shavlik and Towell, 1989 ] <author> Jude W. Shavlik and G.G. Towell. </author> <title> An approach to combining explanation-based and neural learning algorithms. </title> <journal> Connection Science, </journal> <volume> 1(3) </volume> <pages> 231-253, </pages> <year> 1989. </year>
Reference-contexts: In this case, inductive learning can sort through the large number of potential features by finding regularities among thousands of training examples. Methods for combining inductive and analytical learning have been the subject of considerable recent research. For example, <ref> [ Shavlik and Towell, 1989 ] </ref> describes a method called KBANN for using prior symbolic knowledge to initialize the structure and weights of a neural network, which is then inductively refined using the Backpropagation method.
Reference: [ Simard et al., 1992 ] <author> Patrice Simard, Bernard Victorri, Yann LeCun, and John Denker. </author> <title> Tangent prop a formalism for specifying selected invariances in an adaptive network. </title> <editor> In J. E. Moody, S. J. Hanson, and R. P. Lippmann, editors, </editor> <booktitle> Advances in Neural Information Processing Systems 4, </booktitle> <pages> pages 895-903, </pages> <address> San Mateo, CA, 1992. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Gradient descent is employed to iteratively minimize E. Notice that in our implementation we used a modified version of the Tangent-Prog algorithm <ref> [ Simard et al., 1992 ] </ref> to refine the weights and biases of the target network [ Masuoka, 1993 ] .
Reference: [ Sutton, 1988 ] <author> Richard S. Sutton. </author> <title> Learning to predict by the methods of temporal differences. </title> <journal> Machine Learning, </journal> <volume> 3, </volume> <year> 1988. </year>
Reference: [ Tambe et al., 1992 ] <author> Milind Tambe, Robert Doorenbos, and Allen Newell. </author> <title> The match cost of adding a new rule : a clash of views. </title> <type> Technical Report CMU-CS-92-158, </type> <institution> Carnegie Mellon University, </institution> <address> Pittsburgh, PA 15213, </address> <year> 1992. </year>
Reference-contexts: Whereas the capabilities of Soar's analytical learning mechanism have been convincingly demonstrated in a variety of domains (e.g., <ref> [ Tambe et al., 1992 ] </ref> ), its inductive learning capabilities have yet to be demonstrated on the same scale. <p> Others have noted [ Rosenbloom, 1983 ] that learning in Chunking-Soar may be too abrupt to be a correct model of human learning. * EBNN-Soar may avoid the average growth effect encountered by Chunking-Soar <ref> [ Tambe et al., 1992 ] </ref> , in which the large number of learned chunks can lead to significant slowdown in processing.
Reference: [ Tesauro, 1992 ] <author> Gerald J. Tesauro. </author> <title> Practical issues in temporal difference learning. </title> <editor> In J. E. Moody, S. J. Hanson, and R. P. Lippmann, editors, </editor> <booktitle> Advances in Neural Information Processing Systems 4, </booktitle> <pages> pages 259-266, </pages> <address> San Mateo, CA, 1992. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: In our implementation, the Q function was represented by a collection of artificial neural networks which mapped sensations s to utility values, one for each action a. Recently, Q-Learning and other related dynamic programming algorithms have been applied successfully to game playing domains <ref> [ Tesauro, 1992 ] </ref> and robotics [ Gullapalli, 1992 ] . In these previous approaches, the update of the target networks was purely inductive. EBNN, applied to Q-Learning, extends inductive learning by an analytical component.
Reference: [ Thrun and Mitchell, 1993 ] <author> Sebastian B. Thrun and Tom M. Mitchell. </author> <title> Integrating inductive neural network learning and explanation-based learning. </title> <booktitle> In Proceedings of IJCAI-93, </booktitle> <address> Chamberry, France, </address> <month> July </month> <year> 1993. </year> <title> IJCAI, </title> <publisher> Inc. </publisher>
Reference-contexts: The dashed lines indicate average performance. In this experiment, the agent used well-trained predictive action models as its domain theory. the fact that misleading slopes are identified and their influence weakened (cf. Eq. (2)). In other experiments reported elsewhere <ref> [ Thrun and Mitchell, 1993 ] </ref> it was demonstrated that EBNN will fail to learn control if the domain theory is poor and ff is kept fixed. These results also indicate that in cases where the domain theory is poor a pure analytical learner would be hopelessly lost.
Reference: [ Towell and Shavlik, 1989 ] <author> Geoffrey G. Towell and Jude W. Shavlik. </author> <title> Combining explanation-based learning and neural networks: an algorithm and empirical results. </title> <type> Technical Report 859, </type> <institution> University of Wisconsin-Madison, Computer Science, </institution> <year> 1989. </year>
Reference-contexts: Based on these we discuss the role of inductive and analytical learning in EBNN. 3.1 Introduction to EBNN To understand the EBNN learning mechanism, consider the example given in Fig. 2, adapted from [ Winston et al., 1983 ] , <ref> [ Towell and Shavlik, 1989 ] </ref> . Suppose we are facing the problem of learning to classify cups.
Reference: [ Watkins, 1989 ] <author> Chris J. C. H. Watkins. </author> <title> Learning from Delayed Rewards. </title> <type> PhD thesis, </type> <institution> King's College, </institution> <address> Cambridge, England, </address> <year> 1989. </year>
Reference-contexts: Here we report an application of EBNN to learning mobile robot navigation using Q-Learning <ref> [ Watkins, 1989 ] </ref> . Xavier [ O'Sullivan, 1994 ] , the robot at hand, is shown in Fig. 5. Xavier is equipped with a ring of 24 sonar sensors, a laser range finder, and a color camera mounted on a pan/tilt head. <p> In order to learn to generate actions from delayed reward, we applied EBNN in the context of Q-Learning <ref> [ Watkins, 1989 ] </ref> . In essence, Q-Learning constructs utility functions Q (s; a) that map sensations s and actions a to task-specific utility values. Q-values will be positive for final successes and negative for final failures.
Reference: [ Winston et al., 1983 ] <author> P.H. Winston, T.O. Binford, B. Katz, and M. Lowry. </author> <title> Learning physical descriptions from functional definitions, examples, and precedents. </title> <booktitle> In Proceedings of the National Conference on Artificial Intelligence, </booktitle> <pages> pages 433-439, </pages> <address> Washington D.C., 1983. </address> <publisher> Morgan Kaufmann. </publisher> <pages> 25 </pages>
Reference-contexts: We also present some results obtained in the domain of mobile robot navigation. Based on these we discuss the role of inductive and analytical learning in EBNN. 3.1 Introduction to EBNN To understand the EBNN learning mechanism, consider the example given in Fig. 2, adapted from <ref> [ Winston et al., 1983 ] </ref> , [ Towell and Shavlik, 1989 ] . Suppose we are facing the problem of learning to classify cups.
References-found: 36

