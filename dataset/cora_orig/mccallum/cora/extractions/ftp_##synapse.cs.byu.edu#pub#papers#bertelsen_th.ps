URL: ftp://synapse.cs.byu.edu/pub/papers/bertelsen_th.ps
Refering-URL: ftp://synapse.cs.byu.edu/pub/papers/details.html
Root-URL: 
Title: Automatic Feature Extraction in Machine Learning  
Author: Rick Bertelsen 
Degree: A Thesis Presented to the  In Partial Fulfillment of the Requirements for the Degree Master of Science  
Date: August 1994  
Affiliation: Department of Computer Science Brigham Young University  
Abstract: This thesis presents a machine learning model capable of extracting discrete classes out of continuous valued input features. This is done using a neurally inspired novel competitive classifier (CC) which feeds the discrete classifications forward to a supervised machine learning model. The supervised learning model uses the discrete classifications and perhaps other information available to solve a problem. The supervised learner then generates feedback to guide the CC into potentially more useful classifications of the continuous valued input features. Two supervised learning models are combined with the CC creating ASOCS-AFE and ID3-AFE. Both models are simulated and the results are analyzed. Based on these results, several areas of future research are proposed. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Barker, J. C., </author> <title> Eclectic Learning, </title> <type> PhD Dissertation, </type> <institution> Brigham Young University, Provo, Utah, </institution> <year> 1994. </year>
Reference-contexts: Single layer means that the CC has two layers of nodes connected by a single layer of adjustable weights. A node is a neurally inspired unit capable of accepting inputs and generating an output. A weight is a real value on the interval of <ref> [0, 1] </ref>. Figure 4.3 gives a diagram of a FC. Figure 4.4 shows a CC made up of three FCs and thus capable of coding three continuous features. 4 . 2 . 1 Input Nodes The input layer of a FC consists of an input node. <p> The node represents one continuous valued feature. The output of an input node is a real valued number on the interval <ref> [0, 1] </ref>. All continuous valued inputs are normalized. The need for normalizing inputs is discussed later. Each input node is connected to one or more output nodes by an adjustable weight. A weight is also referred to as a prototype. <p> The input m i is normalized and squared to force the denominator of equation 4.1 to be on the interval <ref> [0, 1] </ref>. <p> Various actions by the SL-AFE depend upon which trial instance is used to produce a certain output. Specific details of the algorithm are discussed in sections 5.2.1 through 5.2.4. 37 SL-AFE learning algorithm <ref> [1] </ref> coarse-coding (); // if necessary for given SL [2] SL_pre_feedback_learning (); // if necessary for given SL [3] for i:= 0 to max_iterations - [4] for j:= 0 to number_training_instances - [5] trial_instance [0] = build_trial_instance ( training_instance [j], 0); [6] for k:= 0 to number_continuous_features - [7] output:= generate_output <p> [26] break; - // end if trial_instance [k+1]=Null - // end else output [k]training_instance_output - // end for loop (k) - // end for loop (j) - // end for loop (i) [27] SL_post_feedback_learning (); // if necessary for given SL end SL-AFE learning algorithm The coarse coding in line <ref> [1] </ref> of the continuous features may be necessary for any learning that the SL performs prior to generating feedback. In section 5.4, ID3-AFE uses coarse coding to give the ID3 algorithm codings to which the gain function is applied. <p> The Test Correct column shows the number correctly classified of the test set, and the Accuracy column gives the same result as the percentage of the test set correctly classified. The results shown in figure 5.20 are slightly lower than other learning models <ref> [1] </ref>. The results for the iris data set are within 1% of other popular learning models. The results for the hepatitis data set are about 10% lower than the best of other learning models, but better than some of the worst. <p> Examination of the initial and final prototypes for the continuous valued features showed that some input features had initial prototypes near the boundary of the possible inputs. Because the inputs are normalized and prototypes are <ref> [0, 1] </ref>, the initial values were near 0 or 1. All continuous valued features had initial prototypes far apart. Only 2 of the 6 continuous valued features created a new coding. The other 4 continuous valued features did not create a new coding.
Reference: [2] <author> Bertelsen, R., and Martinez T. R., </author> <title> Extending ID3 Through Discretization of Contnuous Inputs, </title> <booktitle> Proceedings of FLAIRS-94, </booktitle> <year> 1994, </year> <pages> 122-125. </pages>
Reference-contexts: Various actions by the SL-AFE depend upon which trial instance is used to produce a certain output. Specific details of the algorithm are discussed in sections 5.2.1 through 5.2.4. 37 SL-AFE learning algorithm [1] coarse-coding (); // if necessary for given SL <ref> [2] </ref> SL_pre_feedback_learning (); // if necessary for given SL [3] for i:= 0 to max_iterations - [4] for j:= 0 to number_training_instances - [5] trial_instance [0] = build_trial_instance ( training_instance [j], 0); [6] for k:= 0 to number_continuous_features - [7] output:= generate_output (trial_instance [k]); [8] if (output = training_instance_output) - [9]
Reference: [3] <author> Carpenter, G.A., and Grossberg, S., </author> <title> A massively parallel architecture for a self-organizing neural pattern recognition machine. Computer Vision, Graphics, </title> <booktitle> and Image Processing, 1987, </booktitle> <volume> 37, </volume> <pages> 54-115. </pages>
Reference-contexts: Specific details of the algorithm are discussed in sections 5.2.1 through 5.2.4. 37 SL-AFE learning algorithm [1] coarse-coding (); // if necessary for given SL [2] SL_pre_feedback_learning (); // if necessary for given SL <ref> [3] </ref> for i:= 0 to max_iterations - [4] for j:= 0 to number_training_instances - [5] trial_instance [0] = build_trial_instance ( training_instance [j], 0); [6] for k:= 0 to number_continuous_features - [7] output:= generate_output (trial_instance [k]); [8] if (output = training_instance_output) - [9] if (k = 0) - [10] reinforce_codings (); [11]
Reference: [4] <author> Clark, Peter, and Tim Niblett, </author> <title> The CN2 Induction Algorithm, </title> <journal> Machine Learning, </journal> <volume> vol. 3, </volume> <pages> pp. 261-283, </pages> <year> 1989. </year>
Reference-contexts: Specific details of the algorithm are discussed in sections 5.2.1 through 5.2.4. 37 SL-AFE learning algorithm [1] coarse-coding (); // if necessary for given SL [2] SL_pre_feedback_learning (); // if necessary for given SL [3] for i:= 0 to max_iterations - <ref> [4] </ref> for j:= 0 to number_training_instances - [5] trial_instance [0] = build_trial_instance ( training_instance [j], 0); [6] for k:= 0 to number_continuous_features - [7] output:= generate_output (trial_instance [k]); [8] if (output = training_instance_output) - [9] if (k = 0) - [10] reinforce_codings (); [11] SL_learning_correct_response (trial_instance [0]); - // end if
Reference: [5] <author> Grossberg, S., </author> <title> A neural model of attention, reinforcement, and discrimination learning. </title> <journal> International Review of Neurobiology, 1975, </journal> <volume> 18, </volume> <pages> 263-327. </pages>
Reference-contexts: Specific details of the algorithm are discussed in sections 5.2.1 through 5.2.4. 37 SL-AFE learning algorithm [1] coarse-coding (); // if necessary for given SL [2] SL_pre_feedback_learning (); // if necessary for given SL [3] for i:= 0 to max_iterations - [4] for j:= 0 to number_training_instances - <ref> [5] </ref> trial_instance [0] = build_trial_instance ( training_instance [j], 0); [6] for k:= 0 to number_continuous_features - [7] output:= generate_output (trial_instance [k]); [8] if (output = training_instance_output) - [9] if (k = 0) - [10] reinforce_codings (); [11] SL_learning_correct_response (trial_instance [0]); - // end if k=0 [12] else // k0, output correct <p> Applying the gain function and constructing an initial ID3 tree is an example of necessary learning prior to generating useful feedback. Through the use of iteration, the SL-AFE can train on each instance in the training set a number of times. The first trial instance is constructed on line <ref> [5] </ref> by the algorithm build_trial_instance (figure 5.4). 38 build_trial_instance (trial_instance, trial) if (trial = 0) - Copy the discrete features from instance to trial instance. Call the CC to code the continuous features and forward codings and confidence information. <p> Suppose the stream forwarded by the CC is. Temperature:hot, mild, 0.83; Humidity:high, low, 0.55-The SL constructs the first trial instance by line <ref> [5] </ref> of the SL-AFE training algorithm as Temperature:hot Humidity:high Windy:False Outlook:Cloudy (Trial 1) Using this instance, the SL generates an output and compares this output to the output of the training instance.
Reference: [6] <author> Grossberg, S., </author> <title> Competitive learning: from interaction activation to adaptive resonance. </title> <journal> Cognitive Science, 1987, </journal> <volume> 11, </volume> <pages> 23-63. </pages>
Reference-contexts: in sections 5.2.1 through 5.2.4. 37 SL-AFE learning algorithm [1] coarse-coding (); // if necessary for given SL [2] SL_pre_feedback_learning (); // if necessary for given SL [3] for i:= 0 to max_iterations - [4] for j:= 0 to number_training_instances - [5] trial_instance [0] = build_trial_instance ( training_instance [j], 0); <ref> [6] </ref> for k:= 0 to number_continuous_features - [7] output:= generate_output (trial_instance [k]); [8] if (output = training_instance_output) - [9] if (k = 0) - [10] reinforce_codings (); [11] SL_learning_correct_response (trial_instance [0]); - // end if k=0 [12] else // k0, output correct [13] adjust_boundaries (); [14] SL_learning_correct_response (trial_instance [k]); - //
Reference: [7] <author> Jarvis, R. A and Patrick, Edward A., </author> <title> Clustering using a similarity measure based on shared nearest neighbors, Nearest Neighbor Norms: NN Pattern Classification Techniques, </title> <editor> Ed. Belur Dasarathy, </editor> <publisher> IEEE Computer Society Press, Los Alamitos, </publisher> <address> California, </address> <year> 1991, </year> <pages> pp. 388-97. </pages>
Reference-contexts: learning algorithm [1] coarse-coding (); // if necessary for given SL [2] SL_pre_feedback_learning (); // if necessary for given SL [3] for i:= 0 to max_iterations - [4] for j:= 0 to number_training_instances - [5] trial_instance [0] = build_trial_instance ( training_instance [j], 0); [6] for k:= 0 to number_continuous_features - <ref> [7] </ref> output:= generate_output (trial_instance [k]); [8] if (output = training_instance_output) - [9] if (k = 0) - [10] reinforce_codings (); [11] SL_learning_correct_response (trial_instance [0]); - // end if k=0 [12] else // k0, output correct [13] adjust_boundaries (); [14] SL_learning_correct_response (trial_instance [k]); - // end else k0 [15] break; // goto
Reference: [8] <author> Martinez, T. R., </author> <title> Adaptive Self-Organizing Logic Networks, </title> <type> PhD Dissertation, </type> <institution> University of California Los Angeles, </institution> <address> Los Angeles, California, </address> <year> 1986. </year>
Reference-contexts: // if necessary for given SL [2] SL_pre_feedback_learning (); // if necessary for given SL [3] for i:= 0 to max_iterations - [4] for j:= 0 to number_training_instances - [5] trial_instance [0] = build_trial_instance ( training_instance [j], 0); [6] for k:= 0 to number_continuous_features - [7] output:= generate_output (trial_instance [k]); <ref> [8] </ref> if (output = training_instance_output) - [9] if (k = 0) - [10] reinforce_codings (); [11] SL_learning_correct_response (trial_instance [0]); - // end if k=0 [12] else // k0, output correct [13] adjust_boundaries (); [14] SL_learning_correct_response (trial_instance [k]); - // end else k0 [15] break; // goto next training instance - //
Reference: [9] <author> Martinez, T. R. and Vidal, J. J., </author> <title> Adaptive Parallel Logic Networks, </title> <journal> Journal of Parallel and Distributed Computing, v. </journal> <volume> 5, </volume> <year> 1988, </year> <pages> pp 26-58. </pages>
Reference-contexts: [2] SL_pre_feedback_learning (); // if necessary for given SL [3] for i:= 0 to max_iterations - [4] for j:= 0 to number_training_instances - [5] trial_instance [0] = build_trial_instance ( training_instance [j], 0); [6] for k:= 0 to number_continuous_features - [7] output:= generate_output (trial_instance [k]); [8] if (output = training_instance_output) - <ref> [9] </ref> if (k = 0) - [10] reinforce_codings (); [11] SL_learning_correct_response (trial_instance [0]); - // end if k=0 [12] else // k0, output correct [13] adjust_boundaries (); [14] SL_learning_correct_response (trial_instance [k]); - // end else k0 [15] break; // goto next training instance - // end if output=training_instance_output [16] else // <p> The test on line <ref> [9] </ref> of the SL-AFE learning algorithm determines if the first trial instance produced the correct output. Reinforcing the codings which produced the correct output makes those codings more likely to occur when a similar training instance is processed. The algorithm to reinforce codings is shown in figure 5.5.
Reference: [10] <author> Martinez, T. R., and Campbell, D. M., </author> <title> A Self-Adjusting Dynamic Logic Module Journal of Parallel and Distributed Computing, </title> <editor> v. </editor> <volume> 11, no. 4, </volume> <year> 1991, </year> <pages> pp. 303-13. </pages>
Reference-contexts: for given SL [3] for i:= 0 to max_iterations - [4] for j:= 0 to number_training_instances - [5] trial_instance [0] = build_trial_instance ( training_instance [j], 0); [6] for k:= 0 to number_continuous_features - [7] output:= generate_output (trial_instance [k]); [8] if (output = training_instance_output) - [9] if (k = 0) - <ref> [10] </ref> reinforce_codings (); [11] SL_learning_correct_response (trial_instance [0]); - // end if k=0 [12] else // k0, output correct [13] adjust_boundaries (); [14] SL_learning_correct_response (trial_instance [k]); - // end else k0 [15] break; // goto next training instance - // end if output=training_instance_output [16] else // outputtraining_instance_output [17] SL_learning_incorrect_response (trial_instance [k]); [18]
Reference: [11] <author> Martinez, Tony R., ASOCS: </author> <title> Towards Bridging Neural Network and Artificial Intelligence Learning, </title> <booktitle> Presented at 2nd Government Neural Network Workshop, </booktitle> <year> 1991. </year>
Reference-contexts: [3] for i:= 0 to max_iterations - [4] for j:= 0 to number_training_instances - [5] trial_instance [0] = build_trial_instance ( training_instance [j], 0); [6] for k:= 0 to number_continuous_features - [7] output:= generate_output (trial_instance [k]); [8] if (output = training_instance_output) - [9] if (k = 0) - [10] reinforce_codings (); <ref> [11] </ref> SL_learning_correct_response (trial_instance [0]); - // end if k=0 [12] else // k0, output correct [13] adjust_boundaries (); [14] SL_learning_correct_response (trial_instance [k]); - // end else k0 [15] break; // goto next training instance - // end if output=training_instance_output [16] else // outputtraining_instance_output [17] SL_learning_incorrect_response (trial_instance [k]); [18] trial_instance [k+1]:= build_trial_instance
Reference: [12] <author> Martinez, T. R., Hughes, B., and Campbell, D. M., </author> <title> Priority ASOCS, </title> <note> To appear in Journal of Artificial Neural Networks, </note> <year> 1994. </year>
Reference-contexts: OI is replaced with two instances as shown below. A B' -&gt; Z (OI1) A B C -&gt; Z' (NI) For large instances, DVA can become expensive in terms of the number of instances created. Priority ASOCS (PASOCS) <ref> [12] </ref> attempts to eliminate DVA by the use of priorities. In training, when the NI is found to contradict OI, NI is simply added to IS with a greater priority than the contradicting instance. <p> PASOCS is extended in section 5.3 to support continuous input features. A much more detailed description is given in <ref> [12] </ref>. Figure 1 shows a functional diagram of a PASOCS system. PASOCS Outputs Instances Inputs input to the system during the training phase. A PASOCS is shown in figure 2.2. The adaption unit is used in training and is connected to the PASOCS logic network by a communication path. <p> j:= 0 to number_training_instances - [5] trial_instance [0] = build_trial_instance ( training_instance [j], 0); [6] for k:= 0 to number_continuous_features - [7] output:= generate_output (trial_instance [k]); [8] if (output = training_instance_output) - [9] if (k = 0) - [10] reinforce_codings (); [11] SL_learning_correct_response (trial_instance [0]); - // end if k=0 <ref> [12] </ref> else // k0, output correct [13] adjust_boundaries (); [14] SL_learning_correct_response (trial_instance [k]); - // end else k0 [15] break; // goto next training instance - // end if output=training_instance_output [16] else // outputtraining_instance_output [17] SL_learning_incorrect_response (trial_instance [k]); [18] trial_instance [k+1]:= build_trial_instance ( trial_instance [k], k+1); [19] if (trial_instance [k+1] = <p> This is accomplished by generating feedback for the CC to adjust the coding boundary between the first and second possible codings of that feature. The else of line <ref> [12] </ref> in the algorithm SL-AFE of figure 5.3 uses the variable k to detect if the trial instance producing the correct output is not the first trial instance.
Reference: [13] <author> Murphy, P. M. and Aha D. W., </author> <title> University California, Irvine Repository of machine learning databases. </title> <address> Irvine, CA: </address> <institution> University of California, Department of Information and Computer Science, </institution> <year> 1992. </year>
Reference-contexts: trial_instance [0] = build_trial_instance ( training_instance [j], 0); [6] for k:= 0 to number_continuous_features - [7] output:= generate_output (trial_instance [k]); [8] if (output = training_instance_output) - [9] if (k = 0) - [10] reinforce_codings (); [11] SL_learning_correct_response (trial_instance [0]); - // end if k=0 [12] else // k0, output correct <ref> [13] </ref> adjust_boundaries (); [14] SL_learning_correct_response (trial_instance [k]); - // end else k0 [15] break; // goto next training instance - // end if output=training_instance_output [16] else // outputtraining_instance_output [17] SL_learning_incorrect_response (trial_instance [k]); [18] trial_instance [k+1]:= build_trial_instance ( trial_instance [k], k+1); [19] if (trial_instance [k+1] = Null) - [20] if (trial_instance [0] <p> Simulations have been run to determine two things. First, what combinations of confidence methods and split methods give the best results and worst results. Second, using the best combinations found above, how does ASOCS-AFE do compared to other inductive learning models. The data used for the simulations comes from <ref> [13] </ref>. Results are shown here for only three data sets due to the number of simulation runs for each data set. 5.3.6.1 Combinations of Methods and Their Performance Table 5.1 shows results of the iris data set for all possible confidence methods and two of the four possible split methods.
Reference: [14] <author> Quinlan J. R., </author> <title> Induction of Decision Trees, </title> <journal> Machine Learning, </journal> <volume> 1 </volume> <pages> 81-106, </pages> <publisher> Kluwer Academic Publishers. </publisher> <address> Boston, </address> <year> 1985 </year>
Reference-contexts: This in turn introduces a form of generalization in the real number realm, and automatic feature extraction (AFE). The methodology is illustrated using two different supervised learning systems (referred to as SL), ASOCS [8][9][10][11] and ID3 <ref> [14] </ref>. A supervised learner is a learning model which uses a known output in its learning. ASOCS (Adaptive Self-Organizing Concurrent Systems) is overviewed in chapter 2 of this work. <p> ID3 is a machine learning system which builds decision trees inductively <ref> [14] </ref>. The resulting trees then are capable of classifying based upon decisions determined by the input features. Because of the utility of the these trees, ID3 is demonstrated as a SL for the learning methodology presented in chapter 5. <p> This chapter begins with an example decision tree based on a training set. Then the algorithm for building the tree used in the example is presented. The chapter concludes with an analysis of ID3. 3 . 1 Introductory example The training set in figure 3.1 is taken from <ref> [14] </ref>. The decision is whether to play golf based on current weather conditions. The input features are temperature, humidity, wind, and outlook. Unlike the similar data set given in chapter 1, this data set has no continuous values. <p> build_trial_instance ( training_instance [j], 0); [6] for k:= 0 to number_continuous_features - [7] output:= generate_output (trial_instance [k]); [8] if (output = training_instance_output) - [9] if (k = 0) - [10] reinforce_codings (); [11] SL_learning_correct_response (trial_instance [0]); - // end if k=0 [12] else // k0, output correct [13] adjust_boundaries (); <ref> [14] </ref> SL_learning_correct_response (trial_instance [k]); - // end else k0 [15] break; // goto next training instance - // end if output=training_instance_output [16] else // outputtraining_instance_output [17] SL_learning_incorrect_response (trial_instance [k]); [18] trial_instance [k+1]:= build_trial_instance ( trial_instance [k], k+1); [19] if (trial_instance [k+1] = Null) - [20] if (trial_instance [0] is learnable by
Reference: [15] <author> Quinlan, J. R., C4.5: </author> <title> Programs for Machine Learning, </title> <address> San Mateo,CA: </address> <publisher> Morgan Kaufman, </publisher> <year> 1993. </year> <month> 59 </month>
Reference-contexts: Unlike the similar data set given in chapter 1, this data set has no continuous values. ID3 supports only discrete features, therefore temperature and humidity have previously been coded by some preprocessing. There are variations to ID3 which support continuous values <ref> [15] </ref>. This work shows another alternative. In 9 cases the decision is positive; in 5 cases the decision is negative (14 cases total). <p> Thus, the training window can slowly grow to encompass the entire training set. Therefore, it may be simpler to begin with the entire training set thus creating the tree only once. Current versions of ID3 and variations to ID3 do not use a training window. <ref> [15] </ref> The time complexity of execution is bounded by O (m), where m is the number of input features. Each input feature may occur at most once down each path of the tree. <p> to number_continuous_features - [7] output:= generate_output (trial_instance [k]); [8] if (output = training_instance_output) - [9] if (k = 0) - [10] reinforce_codings (); [11] SL_learning_correct_response (trial_instance [0]); - // end if k=0 [12] else // k0, output correct [13] adjust_boundaries (); [14] SL_learning_correct_response (trial_instance [k]); - // end else k0 <ref> [15] </ref> break; // goto next training instance - // end if output=training_instance_output [16] else // outputtraining_instance_output [17] SL_learning_incorrect_response (trial_instance [k]); [18] trial_instance [k+1]:= build_trial_instance ( trial_instance [k], k+1); [19] if (trial_instance [k+1] = Null) - [20] if (trial_instance [0] is learnable by SL based on discrete features) [21] SL_learning (trial_instance [0]);
Reference: [16] <author> Rumelhart, D.E., and Zipser D., </author> <title> Competitive learning. </title> <journal> Cognitive Science, 1985, </journal> <volume> 9, </volume> <pages> 75-112. </pages>
Reference-contexts: After the structure is presented, the details of how these functions are carried out are given. 4 . 2 The Structure of the CC The CC is a collection of feature coders (FC). Each FC is a single layer neural network model derived from the competitive learning model <ref> [16] </ref>. Single layer means that the CC has two layers of nodes connected by a single layer of adjustable weights. A node is a neurally inspired unit capable of accepting inputs and generating an output. A weight is a real value on the interval of [0, 1]. <p> training_instance_output) - [9] if (k = 0) - [10] reinforce_codings (); [11] SL_learning_correct_response (trial_instance [0]); - // end if k=0 [12] else // k0, output correct [13] adjust_boundaries (); [14] SL_learning_correct_response (trial_instance [k]); - // end else k0 [15] break; // goto next training instance - // end if output=training_instance_output <ref> [16] </ref> else // outputtraining_instance_output [17] SL_learning_incorrect_response (trial_instance [k]); [18] trial_instance [k+1]:= build_trial_instance ( trial_instance [k], k+1); [19] if (trial_instance [k+1] = Null) - [20] if (trial_instance [0] is learnable by SL based on discrete features) [21] SL_learning (trial_instance [0]); [22] else - [23] split_codings (); [24] new_instance:= build_new_instance (); [25] SL_learning_new_instance
Reference: [17] <author> Wilson, D. Randall, and Tony R. Martinez, </author> <title> The Importance of Using Multiple Styles of Generalization, </title> <booktitle> Proceedings of the First New Zealand International Conference on Artificial Neural Networks and Expert Systems (ANNES) , pp. </booktitle> <pages> 54-57, </pages> <month> November </month> <year> 1993. </year>
Reference-contexts: (k = 0) - [10] reinforce_codings (); [11] SL_learning_correct_response (trial_instance [0]); - // end if k=0 [12] else // k0, output correct [13] adjust_boundaries (); [14] SL_learning_correct_response (trial_instance [k]); - // end else k0 [15] break; // goto next training instance - // end if output=training_instance_output [16] else // outputtraining_instance_output <ref> [17] </ref> SL_learning_incorrect_response (trial_instance [k]); [18] trial_instance [k+1]:= build_trial_instance ( trial_instance [k], k+1); [19] if (trial_instance [k+1] = Null) - [20] if (trial_instance [0] is learnable by SL based on discrete features) [21] SL_learning (trial_instance [0]); [22] else - [23] split_codings (); [24] new_instance:= build_new_instance (); [25] SL_learning_new_instance (new_instance); - // end
Reference: [18] <author> Ventura, Dan, and Tony R. Martinez, </author> <title> BRACE: A Paradigm For the Discretization of Continuously Valued Data, </title> <booktitle> to appear in Proceedings of the Seventh Annual Florida AI Research Symposium(FLAIRS), </booktitle> <year> 1994. </year>
Reference-contexts: [10] reinforce_codings (); [11] SL_learning_correct_response (trial_instance [0]); - // end if k=0 [12] else // k0, output correct [13] adjust_boundaries (); [14] SL_learning_correct_response (trial_instance [k]); - // end else k0 [15] break; // goto next training instance - // end if output=training_instance_output [16] else // outputtraining_instance_output [17] SL_learning_incorrect_response (trial_instance [k]); <ref> [18] </ref> trial_instance [k+1]:= build_trial_instance ( trial_instance [k], k+1); [19] if (trial_instance [k+1] = Null) - [20] if (trial_instance [0] is learnable by SL based on discrete features) [21] SL_learning (trial_instance [0]); [22] else - [23] split_codings (); [24] new_instance:= build_new_instance (); [25] SL_learning_new_instance (new_instance); - // end else split a coding <p> If the SL generated output were P (not equal to training instance output), then the SL would construct a second trial instance as in line <ref> [18] </ref> of the SL-AFE algorithm. The second trial instance is constucted by considering an alternate coding for a feature. The feature selected for an alternate coding is Humidity because its confidence is lower than the confidence of Temperature.
References-found: 18

