URL: ftp://softlib.rice.edu/pub/CRPC-TRs/reports/CRPC-TR93306-S.ps.gz
Refering-URL: http://www.cs.rice.edu:80/~roth/papers.html
Root-URL: 
Title: Context Optimization for SIMD Execution  
Author: Ken Kennedy Gerald Roth 
Address: P.O. Box 1892 Houston, TX 77251-1892  
Affiliation: Rice University  
Note: Center for Research on Parallel Computation  
Date: April, 1993  Revised: October, 1993; March, 1994.  
Pubnum: CRPC-TR93306-S  
Abstract-found: 0
Intro-found: 1
Reference: [1] <author> W. Abu-Sufah. </author> <title> Improving the Performance of Virtual Memory Computers. </title> <type> PhD thesis, </type> <institution> Dept. of Computer Science, University of Illinois at Urbana-Champaign, </institution> <year> 1979. </year>
Reference-contexts: Unfortunately, loop fusion is not always safe. A data dependence between two adjacent loops is called fusion-preventing if after fusion the direction of the dependence is reversed <ref> [1, 23] </ref>. The existence of such a dependence means that fusion is not safe. In our case however, no such fusion-preventing dependences can exist between adjacent subgrid loops.
Reference: [2] <author> E. Albert, K. Knobe, J. Lukas, and G. Steele, Jr. </author> <title> Compiling Fortran 8x array features for the Connection Machine computer system. </title> <booktitle> In Proceedings of the ACM SIGPLAN Symposium on Parallel Programming: Experience with Applications, Languages, and Systems (PPEALS), </booktitle> <address> New Haven, CT, </address> <month> July </month> <year> 1988. </year> <month> 21 </month>
Reference-contexts: The order that the steps are given reflects our compiler's structure, but this does not imply that all SIMD compilers are structured similarly. For a comparison, Albert et al. give an overview of compiling for the Connection Machine in Paris mode <ref> [2] </ref>, and Sabot describes compiling for the Connection Machine in Slicewise mode [20]. 2.3.1 Array Distribution To exploit parallelism, the Fortran 90D SIMD compiler distributes the data arrays across the PE array so that each PE has some of the data to process. <p> The new split version reduced the execution time of the hand-optimized MPL version by 12%, compared to the 13% reduction of the original split version. 20 5 Related Work Work at Compass by Albert et al. describes the generation and optimization of context setting code <ref> [2] </ref>. They avoid redundant context computations when adjacent statements operate under the same context. They also perform classical optimizations on the context expressions, such as common subexpression elimination. They mention the possibility of reordering computations to minimize context changes, but they do not discuss such transformations.
Reference: [3] <author> F. Allen and J. Cocke. </author> <title> A catalogue of optimizing transformations. </title> <editor> In J. Rustin, editor, </editor> <booktitle> Design and Optimization of Compilers. </booktitle> <publisher> Prentice-Hall, </publisher> <year> 1972. </year>
Reference-contexts: Subgrid looping is very closely related to sectioning used for allocating vector registers [6]. In this case, the PE array can be thought of as a multidimensional vector register. Just as in vector register allocation, loop fusion <ref> [3] </ref> can be a powerful optimization. Loop fusion merges multiple loops covering the same iteration space into a single loop.
Reference: [4] <author> J. R. Allen. </author> <title> Dependence Analysis for Subscripted Variables and Its Application to Program Transformations. </title> <type> PhD thesis, </type> <institution> Dept. of Computer Science, Rice University, </institution> <month> April </month> <year> 1983. </year>
Reference-contexts: Since each PE is in fact a serial processor, the array expressions must be scalarized, i.e., translated into serial code <ref> [4, 25] </ref>. The serial code operates on the data local to a PE. If an array is 5 distributed such that several elements are allocated per PE, then the serial code is placed in a loop (or loop nest as required) that iterates over the subgrid.
Reference: [5] <author> J. R. Allen and K. Kennedy. </author> <title> Automatic translation of Fortran programs to vector form. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 9(4) </volume> <pages> 491-542, </pages> <month> October </month> <year> 1987. </year>
Reference-contexts: Whereas they were concerned with partitioning parallel and serial loops, we are partitioning Fortran 90 statements. The algorithm works on the data dependence graph (ddg) [17] which must be acyclic. Since we are working with a basic block of statements, our dependence graph will contain only loop-independent dependences <ref> [5] </ref> and thus meets that criteria. Besides the ddg, the algorithm takes two other arguments: the set of congruence classes contained in the ddg, and a priority ordering of the congruence classes. We create congruence classes for scalar statements, communication statements and each set of congruent array statements.
Reference: [6] <author> J. R. Allen and K. Kennedy. </author> <title> Vector register allocation. </title> <journal> IEEE Transactions on Computers, </journal> <volume> 41(10) </volume> <pages> 1290-1317, </pages> <month> October </month> <year> 1992. </year>
Reference-contexts: The broadcast instructions will result in each PE adding 1.0 to X 0 (I), where the value of I is also broadcast from the FE. Subgrid looping is very closely related to sectioning used for allocating vector registers <ref> [6] </ref>. In this case, the PE array can be thought of as a multidimensional vector register. Just as in vector register allocation, loop fusion [3] can be a powerful optimization. Loop fusion merges multiple loops covering the same iteration space into a single loop.
Reference: [7] <author> U. Banerjee. </author> <title> Speedup of ordinary programs. </title> <type> PhD thesis, </type> <institution> Dept. of Computer Science, University of Illinois at Urbana-Champaign, </institution> <month> October </month> <year> 1979. </year> <note> Report No. 79-989. </note>
Reference-contexts: To take advantage of this invariance, we will modify the subgrid loop by performing loop splitting, also called index set splitting <ref> [7, 25] </ref>. By splitting the iteration space into disjoint sets, each requiring a single context, we can safely hoist the context setting code out of the resulting loops. We call this optimization context splitting.
Reference: [8] <author> D. Callahan and K. Kennedy. </author> <title> Compiling programs for distributed-memory multiprocessors. </title> <journal> Journal of Supercomputing, </journal> <volume> 2 </volume> <pages> 151-169, </pages> <month> October </month> <year> 1988. </year>
Reference-contexts: Our philosophy is to use the "owner computes" rule, where every processor only performs computations that update data it owns <ref> [8, 26] </ref>.
Reference: [9] <author> S. Chatterjee, J. Gilbert, R. Schreiber, and S. Teng. </author> <title> Optimal evaluation of array expressions on massively parallel machines. </title> <type> Technical Report CSL-92-11, </type> <institution> Xerox Corporation, </institution> <month> December </month> <year> 1992. </year>
Reference-contexts: This rule could be replaced by a data optimization phase which may determine an alternate distribution for the computation and associated intermediate results in an attempt to reduce the amount of communication <ref> [9, 16] </ref>. 2.3.3 Communication Generation Once data and computation distributions are finalized, the compiler must insert any necessary communication operations to move data so that all operands of an expression reside on the PE which will perform the computation.
Reference: [10] <author> M. Chen and J. Cowie. </author> <title> Prototyping Fortran-90 compilers for massively parallel machines. </title> <booktitle> In Proceedings of the SIGPLAN '92 Conference on Program Language Design and Implementation, </booktitle> <address> San Francisco, CA, </address> <month> June </month> <year> 1992. </year>
Reference-contexts: Furthermore, due to the limited dependence analysis performed by the compiler, only compiler-generated scalar code will be moved. It was this work that motivated us to investigate the context partitioning problem. Chen and Cowie also recognize the need to fuse parallel loops in their Fortran 90 compiler <ref> [10] </ref>. However, they only fuse adjacent loops and perform no code motion to increase the chances of fusion. 6 Summary We have developed a double-edged sword to combat the cost of context switching in codes for SIMD machines.
Reference: [11] <author> K. Droegemeier, M. Xue, P. Reid, J. Bradley, and R. Lindsay. </author> <title> Development of the CAPS advanced regional prediction system (ARPS): An adaptive, massively parallel, multi-scale prediction model. </title> <booktitle> In Proceedings of the 9th Conference on Numerical Weather Prediction, </booktitle> <publisher> American Meteorological Society, </publisher> <month> October </month> <year> 1991. </year>
Reference-contexts: This is an item of current research. 4 Results To verify the effectiveness of these optimizations, we performed them by hand on a section of code taken from a Fortran 90 version of the ARPS weather prediction code <ref> [11] </ref>. The section of code initializes 16 two-dimensional arrays. We chose this section of code since context partitioning would not benefit additionally from data reuse nor would it be penalized 18 for generating excessive register pressure.
Reference: [12] <author> G. Fox, S. Hiranandani, K. Kennedy, C. Koelbel, U. Kremer, C. Tseng, and M. Wu. </author> <title> Fortran D language specification. </title> <type> Technical Report TR90-141, </type> <institution> Dept. of Computer Science, Rice University, </institution> <month> December </month> <year> 1990. </year>
Reference-contexts: Each dimension of the decomposition is distributed in a BLOCK, CYCLIC, or BLOCK CYCLIC manner. Both irregular and dynamic data decomposition are supported. In this paper, however, we will only be concerned with regular data distributions. The complete language is described in detail elsewhere <ref> [12] </ref>. 2.3 SIMD Compilation This section describes our overall compilation strategy. It describes the steps necessary in translating a Fortran 90D program for execution on a SIMD architecture.
Reference: [13] <author> M. Gerndt. </author> <title> Work distribution in parallel programs for distributed memory multiprocessors. </title> <booktitle> In Proceedings of the 1991 ACM International Conference on Supercomputing, </booktitle> <address> Cologne, Germany, </address> <month> June </month> <year> 1991. </year>
Reference-contexts: By reducing the loop bounds, the compiler can often avoid iterations for which the PE has no work and thus does not need to introduce any guard statements into the subgrid loop body in those cases <ref> [13, 22] </ref>. We will now discuss the details of context splitting. To simplify the discussion, we will first discuss one-dimensional CYCLIC, BLOCK, and BLOCK CYCLIC distributions, and then show how to combine one-dimensional splitting to handle multidimensional cases.
Reference: [14] <author> S. Hiranandani, K. Kennedy, and C. Tseng. </author> <title> Compiler support for machine-independent parallel programming in Fortran D. </title> <editor> In J. Saltz and P. Mehrotra, editors, </editor> <title> Languages, Compilers, and Run-Time Environments for Distributed Memory Machines. </title> <publisher> North-Holland, </publisher> <address> Amsterdam, The Netherlands, </address> <year> 1992. </year>
Reference-contexts: The PE array itself will be considered has having a rank equal to the number of distributed dimensions of the distributed array. To simplify our discussion, we will limit the number of distributed dimensions to two. The compiler uses a distribution function <ref> [14] </ref> to calculate the mapping of an array element to a subgrid location within a PE. Given an array A, the distribution function A (~-) maps an array index ~- into a pair consisting of a PE index ~ iproc and a subgrid index ~|.
Reference: [15] <author> K. Kennedy and K. S. M c Kinley. </author> <title> Typed fusion with applications to parallel and sequential code generation. </title> <type> Technical Report TR93-208, </type> <institution> Dept. of Computer Science, Rice University, </institution> <month> August </month> <year> 1993. </year>
Reference-contexts: We call this optimization context partitioning. The partitioning could group scalar statements and communication statements together; however, we prefer to separate them so that the communication operations can be further optimized by subsequent phases. To accomplish context partitioning, we use an algorithm proposed by Kennedy and M c Kinley <ref> [15] </ref>. Whereas they were concerned with partitioning parallel and serial loops, we are partitioning Fortran 90 statements. The algorithm works on the data dependence graph (ddg) [17] which must be acyclic.
Reference: [16] <author> K. Knobe, J. Lukas, and G. Steele, Jr. </author> <title> Data optimization: Allocation of arrays to reduce communication on SIMD machines. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 8(2) </volume> <pages> 102-118, </pages> <month> February </month> <year> 1990. </year>
Reference-contexts: This rule could be replaced by a data optimization phase which may determine an alternate distribution for the computation and associated intermediate results in an attempt to reduce the amount of communication <ref> [9, 16] </ref>. 2.3.3 Communication Generation Once data and computation distributions are finalized, the compiler must insert any necessary communication operations to move data so that all operands of an expression reside on the PE which will perform the computation.
Reference: [17] <author> D. Kuck, R. Kuhn, D. Padua, B. Leasure, and M. J. Wolfe. </author> <title> Dependence graphs and compiler optimizations. </title> <booktitle> In Conference Record of the Eighth Annual ACM Symposium 22 on the Principles of Programming Languages, </booktitle> <address> Williamsburg, VA, </address> <month> January </month> <year> 1981. </year>
Reference-contexts: To accomplish context partitioning, we use an algorithm proposed by Kennedy and M c Kinley [15]. Whereas they were concerned with partitioning parallel and serial loops, we are partitioning Fortran 90 statements. The algorithm works on the data dependence graph (ddg) <ref> [17] </ref> which must be acyclic. Since we are working with a basic block of statements, our dependence graph will contain only loop-independent dependences [5] and thus meets that criteria. <p> This produces a set of imperfectly nested DO-loops. We then use loop distribution <ref> [17, 19] </ref> to produce a set of perfectly nested DO-loops, each of which operates under a single context. The context for each loop nest is the intersection of the contexts produced for each dimension. Let's consider again the array Y2 as declared and distributed in Figures 5 and 6.
Reference: [18] <author> J. Li and M. Chen. </author> <title> Compiling communication-efficient programs for massively parallel machines. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 2(3) </volume> <pages> 361-376, </pages> <month> July </month> <year> 1991. </year>
Reference-contexts: The exact details of how the required communication operations are determined and generated are beyond the scope of this paper and are not necessary for understanding the optimizations discussed in later sections. Interested readers are referred to the work by Li and Chen <ref> [18] </ref>. After the communication operations have been inserted, all computational expressions reference data that are strictly local to the associated PEs. For example, the statement: X (2:255) = X (1:254) + X (2:255) + X (3:256) onto a 16 PE machine.
Reference: [19] <author> Y. Muraoka. </author> <title> Parallelism Exposure and Exploitation in Programs. </title> <type> PhD thesis, </type> <institution> Dept. of Computer Science, University of Illinois at Urbana-Champaign, </institution> <month> February </month> <year> 1971. </year> <note> Report No. 71-424. </note>
Reference-contexts: This produces a set of imperfectly nested DO-loops. We then use loop distribution <ref> [17, 19] </ref> to produce a set of perfectly nested DO-loops, each of which operates under a single context. The context for each loop nest is the intersection of the contexts produced for each dimension. Let's consider again the array Y2 as declared and distributed in Figures 5 and 6.
Reference: [20] <author> G. Sabot. </author> <title> Optimized CM Fortran compiler for the Connection Machine computer. </title> <booktitle> In Proceedings of the 25th Annual Hawaii International Conference on System Sciences, </booktitle> <address> Kauai, HI, </address> <month> January </month> <year> 1992. </year>
Reference-contexts: For a comparison, Albert et al. give an overview of compiling for the Connection Machine in Paris mode [2], and Sabot describes compiling for the Connection Machine in Slicewise mode <ref> [20] </ref>. 2.3.1 Array Distribution To exploit parallelism, the Fortran 90D SIMD compiler distributes the data arrays across the PE array so that each PE has some of the data to process. The manner in which arrays are distributed is very important for maximizing parallelism while minimizing expensive communication operations. <p> In a later paper describing the internals of the compiler, he describes how it attempts to perform code motion so that subgrid loops may become adjacent and thus fused <ref> [20] </ref>. However, the code motion performed is limited to only moving scalar code from between subgrid loops, not in moving the loops themselves. Furthermore, due to the limited dependence analysis performed by the compiler, only compiler-generated scalar code will be moved.
Reference: [21] <author> G. Sabot, (with D. Gingold, and J. Marantz). </author> <title> CM Fortran optimization notes: Slicewise model. </title> <type> Technical Report TMC-184, </type> <institution> Thinking Machines Corporation, </institution> <month> March </month> <year> 1991. </year>
Reference-contexts: However, unless an effort is made to make congruent array state-ments adjacent, many small subgrid loops may still be generated. Sabot has recognized this problem, and recommends that users of the CM Fortran compiler rearrange program statements, when possible, to avoid the inefficiencies of such subgrid loops <ref> [21] </ref>. In order to alleviate this problem automatically, our compiler has an optimization phase that reorders the statements within a basic block. The reordering attempts to create separate partitions of scalar statements, communication statements, and congruent array statements. We call this optimization context partitioning. <p> While giving some optimization hints for the slicewise CM Fortran compiler, Sabot describes the need for code motion to increase the size of elemental code blocks (blocks of code for which a single subgrid loop can be generated) <ref> [21] </ref>. He goes on to state that the compiler does not perform this code motion on user code, and thus it is up to the programmer to make them as large as possible.
Reference: [22] <author> C. Tseng. </author> <title> An Optimizing Fortran D Compiler for MIMD Distributed-Memory Machines. </title> <type> PhD thesis, </type> <institution> Dept. of Computer Science, Rice University, </institution> <month> January </month> <year> 1993. </year>
Reference-contexts: By reducing the loop bounds, the compiler can often avoid iterations for which the PE has no work and thus does not need to introduce any guard statements into the subgrid loop body in those cases <ref> [13, 22] </ref>. We will now discuss the details of context splitting. To simplify the discussion, we will first discuss one-dimensional CYCLIC, BLOCK, and BLOCK CYCLIC distributions, and then show how to combine one-dimensional splitting to handle multidimensional cases.
Reference: [23] <author> J. Warren. </author> <title> A hierachical basis for reordering transformations. </title> <booktitle> In Conference Record of the Eleventh Annual ACM Symposium on the Principles of Programming Languages, </booktitle> <address> Salt Lake City, UT, </address> <month> January </month> <year> 1984. </year>
Reference-contexts: Unfortunately, loop fusion is not always safe. A data dependence between two adjacent loops is called fusion-preventing if after fusion the direction of the dependence is reversed <ref> [1, 23] </ref>. The existence of such a dependence means that fusion is not safe. In our case however, no such fusion-preventing dependences can exist between adjacent subgrid loops.
Reference: [24] <author> M. Weiss. </author> <title> Strip mining on SIMD architectures. </title> <booktitle> In Proceedings of the 1991 ACM International Conference on Supercomputing, </booktitle> <address> Cologne, Germany, </address> <month> June </month> <year> 1991. </year>
Reference-contexts: This is known as the subgrid loop. For a detailed description of the issues involved in generating correct subgrid loops for SIMD architectures, see the paper by Weiss <ref> [24] </ref>.
Reference: [25] <author> M. J. Wolfe. </author> <title> Optimizing Supercompilers for Supercomputers. </title> <publisher> The MIT Press, </publisher> <address> Cam-bridge, MA, </address> <year> 1989. </year>
Reference-contexts: Since each PE is in fact a serial processor, the array expressions must be scalarized, i.e., translated into serial code <ref> [4, 25] </ref>. The serial code operates on the data local to a PE. If an array is 5 distributed such that several elements are allocated per PE, then the serial code is placed in a loop (or loop nest as required) that iterates over the subgrid. <p> To take advantage of this invariance, we will modify the subgrid loop by performing loop splitting, also called index set splitting <ref> [7, 25] </ref>. By splitting the iteration space into disjoint sets, each requiring a single context, we can safely hoist the context setting code out of the resulting loops. We call this optimization context splitting.
Reference: [26] <author> H. Zima, H.-J. Bast, and M. Gerndt. </author> <title> SUPERB: A tool for semi-automatic MIMD/SIMD parallelization. </title> <journal> Parallel Computing, </journal> <volume> 6 </volume> <pages> 1-18, </pages> <year> 1988. </year> <month> 23 </month>
Reference-contexts: Our philosophy is to use the "owner computes" rule, where every processor only performs computations that update data it owns <ref> [8, 26] </ref>.
References-found: 26

