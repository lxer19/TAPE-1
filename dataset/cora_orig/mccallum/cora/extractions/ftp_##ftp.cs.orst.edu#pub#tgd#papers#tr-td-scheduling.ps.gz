URL: ftp://ftp.cs.orst.edu/pub/tgd/papers/tr-td-scheduling.ps.gz
Refering-URL: http://www.cs.orst.edu/~tgd/cv/pubs.html
Root-URL: 
Email: zhangw@redwood.rt.cs.boeing.com  tgd@cs.orst.edu  
Title: Solving Combinatorial Optimization Tasks by Reinforcement Learning: A General Methodology Applied to Resource-Constrained Scheduling  
Author: Wei Zhang Thomas G. Dietterich 
Address: P.O. Box 3707, MS 7L-66, Seattle, WA 98124-2207 USA  Corvallis, OR 97331-3202 USA  
Affiliation: Boeing Research Technology,  Department of Computer Science, Oregon State University,  
Note: Journal of Artificial Intelligence Research 1 (2000) 1-1 Submitted 6/91; published 9/91  
Abstract: This paper introduces a methodology for solving combinatorial optimization problems through the application of reinforcement learning methods. The approach can be applied in cases where several similar instances of a combinatorial optimization problem must be solved. The key idea is to analyze a set of "training" problem instances and learn a search control policy for solving new problem instances. The search control policy has the twin goals of finding high-quality solutions and finding them quickly. Results of applying this methodology to a NASA scheduling problem show that the learned search control policy is much more effective than the best known non-learning search procedure|a method based on simulated annealing.
Abstract-found: 1
Intro-found: 1
Reference: <author> Barto, A. G., Bradtke, S. J., & Singh, S. P. </author> <year> (1995). </year> <title> Learning to act using real-time dynamic programming. </title> <journal> Artificial Intelligence, </journal> <volume> 72 (1), </volume> <pages> 81-138. </pages>
Reference-contexts: According to this hypothesis, T D () is not propagating information backward along solution paths, and the theory of reinforcement learning as a form of online dynamic programming <ref> (Barto et al., 1995) </ref> is irrelevant to the success of TD scheduling. * Hypothesis 2: Feature Smoothing. The second hypothesis is that the network is smoothing the input features to remove local minima, but the network is not learning to predict the final RDF.
Reference: <author> Boyan, J. A., & Moore, A. W. </author> <year> (1996). </year> <title> Learning evaluation functions for large acyclic domains. </title> <editor> In Saitta, L. (Ed.), </editor> <booktitle> Machine Learning: Proceedings of the Thirteenth International Conference, </booktitle> <pages> pp. 63-70. </pages> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Cleveland, G. A., & Smith, S. F. </author> <year> (1989). </year> <title> Using genetic algorithms to schedule flow shop release. </title> <booktitle> In Proceedings of the the Third International Conference on Genetic Algorithms, </booktitle> <pages> pp. 160-169. </pages> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Related Work Many different methods in operations research (OR) and artificial intelligence (AI) have been applied to scheduling problems including constraint-based search (Fox, 1983; Craw-ford & Baker, 1994), simulated annealing (Zweben et al., 1994), tabu search (Hooker & Natraj, 1995), and genetic programming <ref> (Cleveland & Smith, 1989) </ref>. Some of this work has resulted in systems for solving large-scale real-world problems.
Reference: <author> Crawford, J. M., & Baker, A. B. </author> <year> (1994). </year> <title> Experimental results on the application of sat-isfiability algorithms to scheduling problems. </title> <booktitle> In Proceedings of the Twelfth National Conference on Artificial Intelligence. 36 Reinforcement Learning for Scheduling Crites, </booktitle> <editor> R. H., & Barto, A. G. </editor> <year> (1996). </year> <title> Improving elevator performance using reinforcement learning. </title> <booktitle> In Advances in Neural Information Processing Systems 8 Cambridge, </booktitle> <address> MA. </address> <publisher> MIT Press. </publisher>
Reference: <author> Deale, M., Yvanovich, M., Schnitzius, D., Kautz, D., </author> & <title> et al (1994). The space shuttle ground processing scheduling system. In Zweben, </title> <editor> M., & Fox, M. S. (Eds.), </editor> <title> Intelligent Scheduling, </title> <journal> chap. </journal> <volume> 15, </volume> <pages> pp. 423-449. </pages> <publisher> Morgan Kaufmann, </publisher> <address> San Francisco, CA. </address>
Reference: <author> Fox, M. S. </author> <year> (1983). </year> <title> Constraint-Directed Search: A Case Study of Job-Shop Scheduling. </title> <type> Ph.D. thesis, </type> <institution> Carnegie Mellon University. </institution>
Reference-contexts: Some of this work has resulted in systems for solving large-scale real-world problems. For example, the work on the ISIS <ref> (Fox, 1983) </ref>, OPIS (Smith, 1994), and Micro-Boss (Sadeh, 1991) systems showed that large-scale manufacturing scheduling problems can be efficiently solved by using combinations of several methods (including constraint propagation and opportunistic search).
Reference: <author> Garey, M. R., Johnson, D. S., & Smith, R. </author> <year> (1976). </year> <title> The complexity of flowshop and jobshop scheduling. </title> <journal> Mathematics of Operations Research, </journal> <volume> 1 (2), </volume> <pages> 117-129. </pages>
Reference: <author> Garey, M. R., & Johnson, D. S. </author> <year> (1979). </year> <title> Computers and intractability: A guide to the theory of NP-completeness. </title> <editor> W. H. </editor> <publisher> Freeman, </publisher> <address> New York. </address>
Reference: <author> Hooker, J. N., & Natraj, N. R. </author> <year> (1995). </year> <title> Solving a general routing and scheduling problem by chain decompostion and tabu search. </title> <journal> Transportation Science, </journal> <volume> 29 (1), </volume> <pages> 30-44. </pages>
Reference-contexts: We now briefly summarize related work on resource-constrained scheduling and reinforcement learning. 5. Related Work Many different methods in operations research (OR) and artificial intelligence (AI) have been applied to scheduling problems including constraint-based search (Fox, 1983; Craw-ford & Baker, 1994), simulated annealing (Zweben et al., 1994), tabu search <ref> (Hooker & Natraj, 1995) </ref>, and genetic programming (Cleveland & Smith, 1989). Some of this work has resulted in systems for solving large-scale real-world problems.
Reference: <author> Kaelbling, L. P., Littman, M. L., & Moore, A. W. </author> <year> (1996). </year> <title> Reinforcement learning: A survey. </title> <journal> Journal of AI Research, </journal> <volume> 4, </volume> <pages> 237-285. </pages>
Reference: <author> Koenig, S., & Simmons, R. G. </author> <year> (1996). </year> <title> The effect of representation and knowledge on goal-directed exploration with reinforcement-learning algorithms. </title> <journal> Machine Learning, </journal> <volume> 22, </volume> <pages> 227-250. </pages>
Reference-contexts: Domains that contain many cycles or that are unbounded can cause randomized exploration methods to take exponential or even infinite expected time to reach a terminal state <ref> (Koenig & Simmons, 32 Reinforcement Learning for Scheduling 1996) </ref>. Our scheduling problem space is closed and nearly acyclic: most repairs do not undo previous repairs.
Reference: <author> Lin, L. </author> <year> (1992). </year> <title> Self-improving reactive agents based on reinforcement learning, planning and teaching. </title> <journal> Machine Learning, </journal> <volume> 8, </volume> <pages> 293-321. </pages>
Reference: <author> Lin, L. </author> <year> (1993). </year> <title> Reinforcement Learning for Robots using Neural Networks. </title> <type> Ph.D. thesis, </type> <institution> Carnegie Mellon University. </institution>
Reference: <author> Lin, S., & Kernighan, B. W. </author> <year> (1971). </year> <title> An effective heuristic algorithm for the TSP. </title> <journal> Operations Research, </journal> <volume> 21, </volume> <pages> 498-516. </pages>
Reference-contexts: For particular combinatorial optimization problems, such as the TSP, many years of research have produced several good general-purpose heuristics, such as the Lin-Kernighan heuristic <ref> (Lin & Kernighan, 1971) </ref>. General purpose heuristics have also been discovered for combinatorial search problems such as boolean satisfiability (GSAT, Selman, Levesque, & Mitchell, 1992). For specific application problems, domain-specific heuristics can usually be discovered.
Reference: <author> Pomerleau, D. A. </author> <year> (1991). </year> <title> Efficient training of artificial neural networks for autonomous navigation. </title> <journal> Neural Computation, </journal> <volume> 3 (1), </volume> <pages> 88-97. </pages>
Reference-contexts: The hidden units were fully connected to the input features, and the output units were fully connected to the hidden units. The standard sigmoid activation function (x) = 1 + e x was used. The output units encode the predicted RDF using the technique of overlapping Gaussian ranges <ref> (Pomerleau, 1991) </ref> as follows. Each output unit represents one assigned RDF value, v i (i = 1; : : : ; 8).
Reference: <author> Sadeh, N. </author> <year> (1991). </year> <title> Look-ahead Techniques for Micro-opportunistic Job Shop Scheduling. </title> <type> Ph.D. thesis, </type> <institution> Carnegie Mellon University. </institution>
Reference-contexts: Some of this work has resulted in systems for solving large-scale real-world problems. For example, the work on the ISIS (Fox, 1983), OPIS (Smith, 1994), and Micro-Boss <ref> (Sadeh, 1991) </ref> systems showed that large-scale manufacturing scheduling problems can be efficiently solved by using combinations of several methods (including constraint propagation and opportunistic search). Several companies, including i2 and Red Pepper Software, are now applying combinations of these technologies to solve large scale scheduling and supply-chain management problems.
Reference: <author> Selman, B., Levesque, H., & Mitchell, D. </author> <year> (1992). </year> <title> A new method for solving hard satisfiability problems. </title> <booktitle> In Proceedings of AAAI-92, </booktitle> <pages> pp. 440-446. </pages> <publisher> AAAI/MIT Press. </publisher>
Reference-contexts: For particular combinatorial optimization problems, such as the TSP, many years of research have produced several good general-purpose heuristics, such as the Lin-Kernighan heuristic (Lin & Kernighan, 1971). General purpose heuristics have also been discovered for combinatorial search problems such as boolean satisfiability <ref> (GSAT, Selman, Levesque, & Mitchell, 1992) </ref>. For specific application problems, domain-specific heuristics can usually be discovered. For example, Zweben, Daun, and Deale (1994) describe a very good heuristic approach to resource-constrained scheduling problems.
Reference: <author> Singh, S., & Bertsekas, D. </author> <year> (1997). </year> <title> Reinforcement learning for dynamic channel allocation in cellular telephone systems. </title> <booktitle> In Advances in Neural Information Processing Systems, </booktitle> <volume> Vol. 10, </volume> <pages> pp. 974-980. </pages> <publisher> MIT Press, </publisher> <address> Cambridge, </address> <note> MA. </note> <author> 37 Zhang & Dietterich Smith, S. F. </author> <year> (1994). </year> <title> OPIS: A metholology and architecture for reactive scheduling. In Zweben, </title> <editor> M., & Fox, M. S. (Eds.), </editor> <title> Intelligent Scheduling, </title> <journal> chap. </journal> <volume> 2, </volume> <pages> pp. 29-66. </pages> <publisher> Morgan Kaufmann, </publisher> <address> San Francisco, CA. </address>
Reference: <author> Sutton, R. S. </author> <year> (1988). </year> <title> Learning to predict by the method of temporal differences. </title> <journal> Machine Learning, </journal> <volume> 3, </volume> <pages> 9-44. </pages>
Reference-contexts: Given a fixed policy, the value function can be computed by solving the system of linear equations defined by Equation (1). This can be accomplished online via the T D () algorithm for temporal difference learning <ref> (Sutton, 1988) </ref>. In T D (), the value function is represented by some form of trainable function approximator, such as a neural network. We will use the notation f (s; W ) to denote the value computed by this function approximator in state s.
Reference: <author> Tesauro, G. </author> <year> (1992). </year> <title> Practical issues in temporal difference learning. </title> <journal> Machine Learning, </journal> <volume> 8, </volume> <pages> 257-277. </pages>
Reference-contexts: This approach of applying T D () for control was very successful in the domain of backgammon, where Tesauro's TD-gammon has become the world's best backgammon program <ref> (Tesauro, 1992, 1995) </ref>. Because of the random dice rolls in backgammon, Tesauro did not need to include any random exploration|TD-gammon always chooses the best operator based on a one-step lookahead search. <p> However, none of these methods is based on a learning approach. We believe that the methods introduced in this paper will provide improvements in the speed and performance of scheduling methods on large-scale industrial problems. Other important large applications of reinforcement learning include Tesauro's TD gam-mon <ref> (Tesauro, 1992) </ref>, Crites and Barto's work on elevator control (Crites & Barto, 1996), and the work of Bertsekas and Singh (1997) on cellular telephone channel allocation. All of these studies showed that reinforcement learning algorithms combined with neural network function approximators can learn control policies in large search spaces. 6.
Reference: <author> Tesauro, G. </author> <year> (1995). </year> <title> Temporal difference learning and TD-Gammon. </title> <journal> Communications of the ACM, </journal> <volume> 28 (3), </volume> <pages> 58-68. </pages>
Reference: <author> Watkins, C. J. C. H., & Dayan, P. </author> <year> (1992). </year> <title> Q-learning. </title> <journal> Machine Learning, </journal> <volume> 8 (3/4), </volume> <pages> 279-292. </pages>
Reference-contexts: When formulating a problem space for T D (), it is therefore important to keep the branching factor as small as possible. There is an alternative approach that can work efficiently with much larger branching factors: Q learning <ref> (Watkins & Dayan, 1992) </ref>. In the Q learning method, the function approximator learns a value function Q (s; u), that is a function of the state s and the operator u.
Reference: <author> Zhang, W. </author> <year> (1996). </year> <title> Reinforcement Learning for Job-Shop Scheduling. </title> <type> Ph.D. thesis, </type> <institution> Oregon State University. </institution>
Reference-contexts: Each call to this procedure may perform up to 20 steps of gradient descent. We perform the updates in reverse order, because preliminary experiments showed that this produced substantially better results <ref> (Zhang, 1996) </ref>. We suspect that performing updates in reverse order would improve the performance of other T D () applications. Since the goal of temporal difference learning is to propagate information about later states backwards along the state trajectory, it makes sense to update the neural network in this order. <p> The second property important for efficient execution is that the features should be very efficient to compute. We performed a study in which a time-delay neural network was used to learn good features for resource-constrained scheduling <ref> (Zhang & Dietterich, 1996) </ref> 34 Reinforcement Learning for Scheduling using an extension of the T D () method. We found that these learned features resulted in slightly better schedules than the features described in Study 1 in this paper.
Reference: <author> Zhang, W., & Dietterich, T. </author> <year> (1995). </year> <title> A reinforcement learning approach to job-shop scheduling. </title> <booktitle> In IJCAI-95, </booktitle> <pages> pp. 1114-1120. </pages>
Reference-contexts: Of course, if IR is permitted to run long enough, it will eventually generate all possible operator sequences and hence find the optimal solution, but this is not of practical value. The results in this study have been replicated using synthetic scheduling problems <ref> (Zhang & Dietterich, 1995) </ref>. That study also showed that the learned TD scheduler worked very well on problem instances that were larger than the ones on which it had been trained.
Reference: <author> Zhang, W., & Dietterich, T. </author> <year> (1996). </year> <title> High-performance job-shop scheduling with a time-delay TD() network. </title> <booktitle> In Advances in Neural Information Processing Systems 8 Cam-bridge, </booktitle> <address> MA. </address> <publisher> MIT Press. </publisher>
Reference-contexts: Each call to this procedure may perform up to 20 steps of gradient descent. We perform the updates in reverse order, because preliminary experiments showed that this produced substantially better results <ref> (Zhang, 1996) </ref>. We suspect that performing updates in reverse order would improve the performance of other T D () applications. Since the goal of temporal difference learning is to propagate information about later states backwards along the state trajectory, it makes sense to update the neural network in this order. <p> The second property important for efficient execution is that the features should be very efficient to compute. We performed a study in which a time-delay neural network was used to learn good features for resource-constrained scheduling <ref> (Zhang & Dietterich, 1996) </ref> 34 Reinforcement Learning for Scheduling using an extension of the T D () method. We found that these learned features resulted in slightly better schedules than the features described in Study 1 in this paper.
Reference: <author> Zweben, M., Daun, B., & Deale, M. </author> <year> (1994). </year> <title> Scheduling and rescheduling with iterative repair. In Zweben, </title> <editor> M., & Fox, M. S. (Eds.), </editor> <title> Intelligent Scheduling, </title> <journal> chap. </journal> <volume> 8, </volume> <pages> pp. 241-255. </pages> <publisher> Morgan Kaufmann, </publisher> <address> San Francisco, CA. </address> <month> 38 </month>
Reference-contexts: We now briefly summarize related work on resource-constrained scheduling and reinforcement learning. 5. Related Work Many different methods in operations research (OR) and artificial intelligence (AI) have been applied to scheduling problems including constraint-based search (Fox, 1983; Craw-ford & Baker, 1994), simulated annealing <ref> (Zweben et al., 1994) </ref>, tabu search (Hooker & Natraj, 1995), and genetic programming (Cleveland & Smith, 1989). Some of this work has resulted in systems for solving large-scale real-world problems.
References-found: 26

