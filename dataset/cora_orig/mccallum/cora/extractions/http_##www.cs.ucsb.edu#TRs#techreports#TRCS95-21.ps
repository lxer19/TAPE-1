URL: http://www.cs.ucsb.edu/TRs/techreports/TRCS95-21.ps
Refering-URL: http://www.cs.ucsb.edu/TRs/
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Email: fcfu,tyangg@cs.ucsb.edu  
Title: Run-time Techniques for Exploiting Irregular Task Parallelism on Distributed Memory Architectures  
Author: Cong Fu and Tao Yang 
Address: Santa Barbara, CA 93106  
Affiliation: Department of Computer Science University of California  
Abstract: Automatic scheduling for directed acyclic graphs (DAG) and its applications for coarse-grained irregular problems such as large n-body simulation have been studied in the literature. However solving irregular problems with mixed granularities such as sparse matrix factorization is challenging since it requires efficient run-time support for executing the DAG schedule. In this paper, we investigate run-time compilation and supporting techniques for executing general asynchronous DAG schedules on distributed memory machines. Our solution tightly integrates the run-time scheme with a fast communication mechanism to eliminate unnecessary overhead in message buffering and copying, and takes advantage of task dependence properties to ensure the correctness of execution. We demonstrate the applications of this scheme in sparse Cholesky and LU factorizations for which actual speedups have been hard to obtain in the literature. Our experiments on Meiko CS-2 show that the automatically scheduled code achieves scalable performance for these problems and the run-time overhead is small compared to the total execution time.
Abstract-found: 1
Intro-found: 1
Reference: [AG89] <author> Cleve Ashcraft and Roger Grimes. </author> <title> The Influence of Relaxed Supernode Partitions on the Multifrontal Method . ACM Transactions on Mathematical Software, </title> <booktitle> 15(4) </booktitle> <pages> 291-309, </pages> <year> 1989. </year>
Reference-contexts: In order to improve the performance further, our future work is to consider techniques of increasing average supernode sizes, e.g. supernode amalgamation <ref> [AG89, Rot92] </ref>.
Reference: [AISS95] <author> A. Alexandrov, M. Ionescu, K. E. Schauser, and C. Scheiman. LogGP: </author> <title> Incorporating Long Messages into the LogP model One step closer towards a realistic model for parallel computation. </title> <booktitle> In 7th Annual Symposium on Parallel Algorithms and Architectures, </booktitle> <pages> pages 95-105, </pages> <month> July </month> <year> 1995. </year>
Reference-contexts: The model of RMA is defined as follows. When a program in a processor needs to write a data 1 It should be noted that an extension for long active messages in Meiko CS-2 has been developed in <ref> [AISS95] </ref>, but their current implementation only allows one active message outstanding at one time due to hardware restriction and is not quite suitable for the asynchronous execution of tasks in our current work. 8 item to the remote processor, the user only needs to provide a remote service descriptor which specifies
Reference: [BCL + 95] <author> Eric A. Brewer, Frederic T. Chong, Lok T. Liu, Shamik D. Sharma, and John Kubia-towicz. </author> <title> Remote queues: Exposing message queues for optimization and atomicity. </title> <booktitle> In 7th Annual ACM Symposium on Parallel Algorithms and Architectures, </booktitle> <pages> pages 42-53, </pages> <year> 1995. </year>
Reference-contexts: And in the situation of task parallelism with mixed granularities, it is not easy to decide where to insert the polling code <ref> [BCL + 95] </ref>. Additionally, active messages usually support short messages 1 . For a general task computation with mixed granularities, efficient support for delivering both short and long messages is required.
Reference: [BF93] <author> Richard L. Burden and J. Douglas Faires. </author> <title> Numerical Analysis. </title> <publisher> PWS Publishing Company, </publisher> <address> fifth edition, </address> <year> 1993. </year>
Reference-contexts: The boundary conditions usually make the first row of matrices strictly diagonal dominant. For such matrices, it can be shown [Cai95] that stable solutions can be obtained by LU (Gaussian Elimination) without pivoting. The proof in [Cai95] is based on a modification to Theorem 6.19 in <ref> [BF93] </ref>.
Reference: [Cai95] <author> Wei Cai. </author> <type> Personal communication, </type> <year> 1995. </year>
Reference-contexts: The boundary conditions usually make the first row of matrices strictly diagonal dominant. For such matrices, it can be shown <ref> [Cai95] </ref> that stable solutions can be obtained by LU (Gaussian Elimination) without pivoting. The proof in [Cai95] is based on a modification to Theorem 6.19 in [BF93]. <p> The boundary conditions usually make the first row of matrices strictly diagonal dominant. For such matrices, it can be shown <ref> [Cai95] </ref> that stable solutions can be obtained by LU (Gaussian Elimination) without pivoting. The proof in [Cai95] is based on a modification to Theorem 6.19 in [BF93].
Reference: [CF87] <author> R. Cytron and J. Ferrante. </author> <title> What's in a name? The Value of Renaming for Parallelism Detection and Storage Allocation. </title> <booktitle> In Proc. of International Conf. on Parallel Processing, </booktitle> <pages> pages 19-27, </pages> <month> February </month> <year> 1987. </year>
Reference-contexts: In functional languages, the "single assignment" principle has been used in data flow computation and functional languages [Sar89, PAM95], i.e., each data item can be written at most once, then there is neither output nor anti dependence. The renaming techniques <ref> [CF87] </ref> can eliminate the output and anti dependence in transforming a DDG to a "single assignment" graph. The advantage of such principle is that dependence enforcement in task execution can be simplified in certain degree. The disadvantage is that memory usage and data copying overhead increase accordingly.
Reference: [CR92] <author> Y-C. Chung and S. Ranka. </author> <title> Applications and Performance Analysis of a Compile-time Optimization Approach for List Scheduling Algorithms on Distributed Memory Multiprocessor. </title> <booktitle> In Proc. of Supercomputing '92, </booktitle> <year> 1992. </year> <month> 25 </month>
Reference-contexts: Therefore, we have processor assignment (A) = (B) = (D) = 0, (C) = (E) = 1, and execution orderings O 0 = fA; B; Dg, O 1 = fC; Eg. Algorithms for static scheduling of DAGs have been extensively studied in the literatures, e.g. <ref> [CR92, PP95, Sar89, YG92] </ref>. Dynamic scheduling which is adaptive to run-time change of dependence structures and weights is more attractive [Pol88], but it is still difficult to balance its benefits and the run-time control overhead in executing tasks with mixed grains on message-passing ma 3 chines.
Reference: [CRY94] <author> S. Chakrabarti, A. Ranade, and K. Yelick. </author> <title> Randomized Load Balancing for Tree--structured Computation. </title> <booktitle> In Proc. of Scalable High-Performance Computing Conference, </booktitle> <pages> pages 666-673, </pages> <month> February </month> <year> 1994. </year>
Reference-contexts: The automatic parallelization of such problems on distributed memory machines is extremely difficult and presents a great challenge. Automatic scheduling techniques are useful in exploiting irregular parallelism in unstructured computations. A lot of work has addressed these problems, for example, randomized load balancing for tree-structured tasks <ref> [CRY94] </ref>. For iterative irregular problems in which communication and computation phases alternate, the CHAOS/PARTI system [DUSH94] has used a run-time compilation approach (inspector/executor) to exploit irregular parallelism.
Reference: [CSBS95] <author> F. T. Chong, S. D. Sharma, E. A. Brewer, and J. Saltz. </author> <title> Multiprocessor Runtime Support for Fine-Grained Irregular DAGs. </title> <editor> In Rajiv K. Kalia and Priya Vashishta, editors, </editor> <title> Toward Teraflop Computing and New Grand Challenge Applications., </title> <address> New York, 1995. </address> <publisher> Nova Science Publishers. </publisher>
Reference-contexts: However the run-time overhead for task execution is still large and there is still a lot of room for obtaining better absolute performance. Recently the work by <ref> [CSBS95] </ref> demonstrates that using both effective DAG scheduling and low-overhead communication mechanisms, scalable performance can be obtained on fine-grained DAGs for solving sparse triangular systems. We have developed an efficient run-time support system called RAPID for executing general DAG computations with mixed granularities. <p> An active message usually 7 contains a function handler and a few short data arguments, and the function at the destination processor can be invoked as soon as this message arrives at the destination. Active messages have been used in <ref> [CSBS95] </ref> for executing fine-grained triangular solving DAGs. We find that it is not easy to integrate active messages with a general task graph execution scheme because careful network polling is required for efficiently implementing active messages as demonstrated in [CSBS95]. <p> Active messages have been used in <ref> [CSBS95] </ref> for executing fine-grained triangular solving DAGs. We find that it is not easy to integrate active messages with a general task graph execution scheme because careful network polling is required for efficiently implementing active messages as demonstrated in [CSBS95]. And in the situation of task parallelism with mixed granularities, it is not easy to decide where to insert the polling code [BCL + 95]. Additionally, active messages usually support short messages 1 . <p> Additionally, active messages usually support short messages 1 . For a general task computation with mixed granularities, efficient support for delivering both short and long messages is required. The implementation in <ref> [CSBS95] </ref> takes advantage of commutativity in triangular solving and as soon as one data item arrives at the destination, an arithmetic operation can be performed. Presence counter is used to check if the computation for a task is completed.
Reference: [DGL86] <author> I. S. Duff, R. G. Grimes, and J. G. Lewis. </author> <title> User's guide for the harwell-boeing sparse matrix collection. </title> <type> Technical report, </type> <year> 1986. </year>
Reference-contexts: Note that LINPACK performance is 8:05 MFLOPS. We have tested a set of benchmark matrices from Harwell-Boeing collection <ref> [DGL86] </ref>. BCSSTK15 arises from structural engineering analysis for Module of an Offshore Platform, BCSSTK16 is for Corp. of Engineers Dam, BCSSTK17 is for Elevated Pressure Vessel, and BCSSTK24 is for Calgary Olympic Saddledome Arena. Task graphs derived from these matrices are very big and unstructured. <p> For a sparse matrix, we also use supernode partitioning to divide the matrix and construct the sparse task graph from the partitioned matrix. It can be shown that a LU task graph is dependence-complete. We have tested two benchmark matrices from Boeing-Harwell collection: BCSSTK15 and BC-SSTK24 <ref> [DGL86] </ref>. Both matrices have symmetric structures. The purpose of using these matrices is to compare with the performance of Cholesky factorization and analyze the impact of algorithm characteristics on the code performance.
Reference: [DUSH94] <author> Raja Das, Mustafa Uysal, Joel Saltz, and Yuan-Shin Hwang. </author> <title> Communication Optimizations for Irregular Scientific Computations on Distributed Memory Architectures . Journal of Parallel and Distributed Computing, </title> <booktitle> 22(3) </booktitle> <pages> 462-479, </pages> <month> September </month> <year> 1994. </year>
Reference-contexts: Automatic scheduling techniques are useful in exploiting irregular parallelism in unstructured computations. A lot of work has addressed these problems, for example, randomized load balancing for tree-structured tasks [CRY94]. For iterative irregular problems in which communication and computation phases alternate, the CHAOS/PARTI system <ref> [DUSH94] </ref> has used a run-time compilation approach (inspector/executor) to exploit irregular parallelism. The key idea 1 of this approach is to identify data accessing patterns at run-time and optimize data distribution and communication for the rest of computation.
Reference: [GJY95] <author> A. Gerasoulis, J. Jiao, and T. Yang. </author> <title> Scheduling of Structured and Unstructured Computation . In Dominique Sotteau D. </title> <editor> Frank Hsu, Arnold Rosenberg, editor, </editor> <title> Interconnections Networks and Mappings and Scheduling Parallel Computation. </title> <journal> American Math. Society, </journal> <year> 1995. </year>
Reference-contexts: In a more complicated iterative application, the computation phase may involve task parallelism with irregular dependence structures. For example, in the adaptive n-body simulation using the fast multipole method, parallelism at each time step can be modeled as a directed acyclic task graph (DAG) <ref> [GJY95] </ref>. The DAG schedule can be re-used for a number of iterations before re-scheduling since particles movement is slow. Then good speedups are obtained on nCUBE-2 for large n-body simulation problems after applying PYRROS DAG scheduling techniques [YG92]. <p> Static scheduling can adapt to certain degree of run-time weight variation <ref> [GJY95, YFGS95] </ref> and its run-time control mechanism is relatively simple. <p> These two issues make timing among tasks different from what is expected at the static time and tend to increase the processor idle time. The performance becomes even more sensitive when task granularities are small <ref> [GJY95, YFGS95] </ref>.
Reference: [GN89] <author> G. A. Geist and E. Ng. </author> <title> Task Scheduling for Parallel Sparse Cholesky Factorization . International Journal of Parallel Programming, </title> <booktitle> 18(4) </booktitle> <pages> 291-314, </pages> <year> 1989. </year>
Reference-contexts: Rothberg and Schreiber [Rot92, RS94] show that the supernode-based approach can deliver good performance in both shared and distributed memory machines. Specialized scheduling techniques have been studied for sparse Cholesky factorization in <ref> [GN89, Liu86] </ref>. We will examine the performance of applying general task scheduling and executing techniques to this problem. For a dense matrix, the BLAS-3 level Cholesky factorization algorithm 3 is shown in Figure 9.
Reference: [HNP91] <author> Michael Heath, Esmond Ng, and Barry Peyton. </author> <title> Parallel Algorithms for Sparse Linear Systems . SIAM Review, </title> <booktitle> 33(3) </booktitle> <pages> 420-460, </pages> <month> September </month> <year> 1991. </year>
Reference-contexts: For instance, Figure 10 is a 21 fi 21 sparse matrix with fill-ins. After that we use supernode partitioning <ref> [HNP91] </ref> so that every basic task operation involves only dense matrix or vector operations which can be implemented using BLAS and LAPACK routines. We also partition supernodes into smaller blocks. And for those supernodes smaller than the block size used, they will remain unchanged. <p> From this small example it can be seen that exploiting operation commutativity does expose more parallelism. exposing commutative operations. However while the task graph derived from sequential semantics (such as the one in the left of 4 This is called fan-in parallelism in the literature <ref> [HNP91] </ref>. Fan-out parallelism in Cholesky factorization is naturally included in the task graph model as submatrix multicasting edges. 16 of Figure 12) is not dependence-complete since all updating operations in Figure 11 (b) modify the same data item which violates Assumptions DA1 and DA3.
Reference: [Kar91] <author> N. Karmarkar. </author> <title> A New Parallel Architecture for Sparse Matrix Computation Based on Finite Project Geometries . In Proc. </title> <booktitle> of Supercomputing '91, </booktitle> <pages> pages 358-369, </pages> <month> February </month> <year> 1991. </year>
Reference-contexts: In solving nonlinear equations using iterative methods, sparse matrix factorization usually dominates the computation time at each iteration step and the topology of the iteration matrix usually remains the same but the numerical values change at each step <ref> [Kar91] </ref>. Efficient parallelization of sparse factorization requires certain compilation cost, but the optimized solution can be used for many iterations. The sparse matrix factorizations can be modeled as DAG [Sch93].
Reference: [Liu86] <author> Joseph W. H. Liu. </author> <title> Computational Models and Task Scheduling for Parallel Sparse Cholesky Factorization . Parallel Computing, </title> <booktitle> 18 </booktitle> <pages> 327-342, </pages> <year> 1986. </year>
Reference-contexts: Rothberg and Schreiber [Rot92, RS94] show that the supernode-based approach can deliver good performance in both shared and distributed memory machines. Specialized scheduling techniques have been studied for sparse Cholesky factorization in <ref> [GN89, Liu86] </ref>. We will examine the performance of applying general task scheduling and executing techniques to this problem. For a dense matrix, the BLAS-3 level Cholesky factorization algorithm 3 is shown in Figure 9.
Reference: [PAM95] <author> S. Pande, D. P. Agrawal, and J. Mauney. </author> <title> A Scalable Scheduling Scheme for Functional Parallelism on Distributed Memory Multiprocessor Systems . IEEE Transactions on Parallel and Distributed Systems, </title> <booktitle> 6(4) </booktitle> <pages> 388-399, </pages> <month> April </month> <year> 1995. </year>
Reference-contexts: With the presence of anti and output dependence, synchronization in run-time support becomes more complicated in order to ensure the correctness of execution. In functional languages, the "single assignment" principle has been used in data flow computation and functional languages <ref> [Sar89, PAM95] </ref>, i.e., each data item can be written at most once, then there is neither output nor anti dependence. The renaming techniques [CF87] can eliminate the output and anti dependence in transforming a DDG to a "single assignment" graph.
Reference: [Pol88] <author> C. D. </author> <title> Polychronopoulos. </title> <publisher> Parallel Programming and Compilers . Kluwer Academic Publishers, </publisher> <year> 1988. </year>
Reference-contexts: Algorithms for static scheduling of DAGs have been extensively studied in the literatures, e.g. [CR92, PP95, Sar89, YG92]. Dynamic scheduling which is adaptive to run-time change of dependence structures and weights is more attractive <ref> [Pol88] </ref>, but it is still difficult to balance its benefits and the run-time control overhead in executing tasks with mixed grains on message-passing ma 3 chines. Static scheduling can adapt to certain degree of run-time weight variation [GJY95, YFGS95] and its run-time control mechanism is relatively simple.
Reference: [PP95] <author> Santosh Pande and Kleanthis Psarris. </author> <title> Scheduling Acylic Task Graphs on Distributed Memory Parallel Architectures, volume 22 of Parallel Processing of Discrete Optimization (Edited by P. </title> <editor> Pardalos, M. Resende and K. G. </editor> <booktitle> Ramakrishnan), </booktitle> <pages> pages 289-304. </pages> <publisher> AMS, </publisher> <year> 1995. </year> <title> DIMACS Series. </title> <type> 26 </type>
Reference-contexts: Therefore, we have processor assignment (A) = (B) = (D) = 0, (C) = (E) = 1, and execution orderings O 0 = fA; B; Dg, O 1 = fC; Eg. Algorithms for static scheduling of DAGs have been extensively studied in the literatures, e.g. <ref> [CR92, PP95, Sar89, YG92] </ref>. Dynamic scheduling which is adaptive to run-time change of dependence structures and weights is more attractive [Pol88], but it is still difficult to balance its benefits and the run-time control overhead in executing tasks with mixed grains on message-passing ma 3 chines.
Reference: [PT83] <author> Roger Peyret and Thomas D. Taylor. </author> <title> Computational Methods for Fluid Flow . Springer--Verlag, </title> <year> 1983. </year>
Reference-contexts: RS/6000 Model 320. 19 Matrix P=2 P=4 P=8 P=16 P=32 BCSSTK15 4% 4% 10% 9% 15% BCSSTK16 6% 8% 4% 3% 7% BCSSTK17 8% 15% 4% -9% 3% BCSSTK24 6% 1% 4% 1% 4% Table 4: Execution time improvements after exploiting commutativity in sparse Cholesky factorization. problems in fluid dynamics <ref> [PT83] </ref> are nonsymmetric and diagonal dominant. The boundary conditions usually make the first row of matrices strictly diagonal dominant. For such matrices, it can be shown [Cai95] that stable solutions can be obtained by LU (Gaussian Elimination) without pivoting.
Reference: [Rot92] <author> Edward Rothberg. </author> <title> Exploiting the Memory Hierarchy in Sequential and Parallel Sparse Cholesky Factorization . PhD thesis, </title> <institution> Dept. of Computer Science, Stanford University, </institution> <month> December </month> <year> 1992. </year>
Reference-contexts: We demonstrate the applications of our techniques for sparse matrix factorizations. It has been very difficult to obtain actual speedups by hand-made code for these problems, not to mention automatically parallelized code, especially on distributed memory machines. Recently impressive performance is obtained in factorizing sparse Cholesky factorization <ref> [Rot92, RS94] </ref>. Our experiments will show that our approach delivers comparable performance. As far as we know, this is the best performance achieved by the automatically scheduled code. The paper is organized as follows. Section 2 describes the task computation model. <p> Rothberg and Schreiber <ref> [Rot92, RS94] </ref> show that the supernode-based approach can deliver good performance in both shared and distributed memory machines. Specialized scheduling techniques have been studied for sparse Cholesky factorization in [GN89, Liu86]. We will examine the performance of applying general task scheduling and executing techniques to this problem. <p> We are further investigating this issue. 6.2 Sparse LU factorization without pivoting LU factorization without pivoting can be used for positive definite or strictly diagonal dominant matrices. Many of algebraic systems arising from the discretization of diffusion and convection 5 In comparison, Rothberg <ref> [Rot92] </ref> reported that the sequential performance of his code achieved 77% of LINPACK for BCSSTK15 and 71% in average for other matrices on IBM RS/6000 Model 320. 19 Matrix P=2 P=4 P=8 P=16 P=32 BCSSTK15 4% 4% 10% 9% 15% BCSSTK16 6% 8% 4% 3% 7% BCSSTK17 8% 15% 4% -9% <p> In order to improve the performance further, our future work is to consider techniques of increasing average supernode sizes, e.g. supernode amalgamation <ref> [AG89, Rot92] </ref>.
Reference: [RS94] <author> E. Rothberg and R. Schreiber. </author> <title> Improved Load Distribution in Parallel Sparse Cholesky Factorization. </title> <booktitle> In Proc. of Supercomputing'94, </booktitle> <pages> pages 783-792, </pages> <month> November </month> <year> 1994. </year>
Reference-contexts: We demonstrate the applications of our techniques for sparse matrix factorizations. It has been very difficult to obtain actual speedups by hand-made code for these problems, not to mention automatically parallelized code, especially on distributed memory machines. Recently impressive performance is obtained in factorizing sparse Cholesky factorization <ref> [Rot92, RS94] </ref>. Our experiments will show that our approach delivers comparable performance. As far as we know, this is the best performance achieved by the automatically scheduled code. The paper is organized as follows. Section 2 describes the task computation model. <p> Rothberg and Schreiber <ref> [Rot92, RS94] </ref> show that the supernode-based approach can deliver good performance in both shared and distributed memory machines. Specialized scheduling techniques have been studied for sparse Cholesky factorization in [GN89, Liu86]. We will examine the performance of applying general task scheduling and executing techniques to this problem. <p> In average, we have obtained speedup 7 on 16 processors and 11:3 on 32 processors. We will discuss overhead of run-time execution in Section 6.3. The speedup on 32 processors is 11:33 for BCSSTK15 and processor utilization efficiency is 35:4%. In <ref> [RS94] </ref>, efficiency 25% has been reported for BCSSTK15 on 64 processors (Intel Paragon) after improving the load balancing strategy. Thus our scheduled code has achieved comparable performance. To the best of our knowledge, this is the highest performance obtained for Cholesky factorization by automatically scheduled code.
Reference: [Sar89] <author> V. Sarkar. </author> <title> Partitioning and Scheduling Parallel Programs for Execution on Multiprocessors . MIT Press, </title> <year> 1989. </year>
Reference-contexts: Most of previous work in DAG scheduling has focused on the algorithmic research for mapping tasks onto multi-processors based on the macro task computation model <ref> [Sar89, WG90] </ref>. But few researches are conducted on efficient run-time support for executing task schedules. <p> Therefore, we have processor assignment (A) = (B) = (D) = 0, (C) = (E) = 1, and execution orderings O 0 = fA; B; Dg, O 1 = fC; Eg. Algorithms for static scheduling of DAGs have been extensively studied in the literatures, e.g. <ref> [CR92, PP95, Sar89, YG92] </ref>. Dynamic scheduling which is adaptive to run-time change of dependence structures and weights is more attractive [Pol88], but it is still difficult to balance its benefits and the run-time control overhead in executing tasks with mixed grains on message-passing ma 3 chines. <p> With the presence of anti and output dependence, synchronization in run-time support becomes more complicated in order to ensure the correctness of execution. In functional languages, the "single assignment" principle has been used in data flow computation and functional languages <ref> [Sar89, PAM95] </ref>, i.e., each data item can be written at most once, then there is neither output nor anti dependence. The renaming techniques [CF87] can eliminate the output and anti dependence in transforming a DDG to a "single assignment" graph.
Reference: [Sch93] <author> Robert Schreiber. </author> <title> Scalability of Sparse Direct Solvers, volume 56 of Graph Theory and Sparse Matrix Computation (Edited by Alan George and John R. </title> <editor> Gilbert and Joseph W.H. </editor> <booktitle> Liu), </booktitle> <pages> pages 191-209. </pages> <publisher> Springer-Verlag, </publisher> <address> New York, </address> <year> 1993. </year>
Reference-contexts: Efficient parallelization of sparse factorization requires certain compilation cost, but the optimized solution can be used for many iterations. The sparse matrix factorizations can be modeled as DAG <ref> [Sch93] </ref>. However compared to large n-body simulations, efficient execution of DAG schedules for sparse matrix problems is more challenging because partitioned graphs contain both coarse and fine grained tasks while in the DAGs arising from large n-body simulations, most of tasks are coarse-grained.
Reference: [SS95] <author> Klaus E. Schauser and Chris J. Scheiman. </author> <title> Experience with active messages on the meiko cs-2. </title> <booktitle> In International Parallel Processing Symposium, </booktitle> <pages> pages 140-149, </pages> <year> 1995. </year>
Reference-contexts: RMA can be implemented in modern multi-processor architectures such as Cray-T3D and Meiko CS-2 <ref> [SS95] </ref>. With RMA, a processor can write to memory of any other processor if the remote address is given. RMA allows passing data directly from one source location to the destination location, without any copying, packing/unpacking and buffering. <p> We have implemented our system on Meiko-CS2 which provides Direct Memory Access (DMA) as the major way to access non-local memory. Each node of Meiko-CS2 is also equipped with a DMA co-processor for handling communication. It takes the main processor 9 microseconds <ref> [SS95] </ref> to dispatch the remote memory access descriptor to the co-processor. The co-processor afterwards will be responsible for sending data without interrupting the main processor so that the opportunity of overlapping communication and computation is maximized. <p> The cost to dispatch a RMA request is 9 microseconds. The communication peak bandwidth of Meiko-CS2 is 40MBytes/Sec. The effective peak bandwidth has been reported as 39MB/Sec and but this is obtained through a ping-pong test <ref> [SS95] </ref> and message caching improves the performance implicitly. In practice, a message is sent only once. We show the effective bandwidth of a single-message sending or direct memory access in Figure 8 when the sizes of messages vary from 1K to 10K.
Reference: [vECGS92] <author> T. von Eicken, D. E. Culler, S. C. Goldstein, and K. E. Schauser. </author> <title> Active Messages: a Mechanism for Integrated Communication and Computation. </title> <booktitle> In Proc. of the 19th Int'l Symposium on Computer Architecture, </booktitle> <address> Gold Coast, Australia, </address> <month> May </month> <year> 1992. </year>
Reference-contexts: Active messages provide an efficient integrated service for passing data and control on distributed memory machines <ref> [vECGS92] </ref>. An active message usually 7 contains a function handler and a few short data arguments, and the function at the destination processor can be invoked as soon as this message arrives at the destination. Active messages have been used in [CSBS95] for executing fine-grained triangular solving DAGs.
Reference: [VNS92] <author> Sesh Venugopal, Vijay Naik, and Joel Saltz. </author> <title> Performance of Distributed Sparse Cholesky Factorization with Pre-scheduling. </title> <booktitle> In Proc. of Supercomputing'92, </booktitle> <pages> pages 52-61, </pages> <address> Minneapolis, </address> <month> November </month> <year> 1992. </year>
Reference-contexts: In <ref> [VNS92] </ref>, static task and communication scheduling is used for sparse Cholesky, and they found pre-scheduling improves the performance of distributed factorization by 30% to 40%. However the run-time overhead for task execution is still large and there is still a lot of room for obtaining better absolute performance.
Reference: [WB87] <author> M. Wolfe and U. Banerjee. </author> <title> Data Dependence and Its Application to Parallel Processing. </title> <journal> International Journal of Parallel Programming, </journal> <volume> 16(2) </volume> <pages> 137-178, </pages> <year> 1987. </year>
Reference-contexts: Classical data dependence graphs (DDG) for modeling the interaction between computation statements have three types of dependencies: true, anti and output <ref> [WB87] </ref>. Task graphs differ from DDG in that they only have true data dependence edges. In a DDG, many of anti or output dependence edges could be redundant if they are subsumed by other true data dependence edges.
Reference: [WG90] <author> M. Y. Wu and D. Gajski. Hypertool: </author> <title> A Programming Aid for Message-passing Systems . IEEE Transactions on Parallel and Distributed Systems, </title> <booktitle> 1(3) </booktitle> <pages> 330-343, </pages> <year> 1990. </year>
Reference-contexts: Most of previous work in DAG scheduling has focused on the algorithmic research for mapping tasks onto multi-processors based on the macro task computation model <ref> [Sar89, WG90] </ref>. But few researches are conducted on efficient run-time support for executing task schedules.
Reference: [Yan93] <author> Tao Yang. </author> <title> Scheduling and Code Generation for Parallel Architectures . PhD thesis, </title> <institution> Dept. of Computer Science, Rutgers University, </institution> <address> New Brunswick, </address> <month> May </month> <year> 1993. </year>
Reference-contexts: This will become clearer as we discuss in the subsequent sections. More discussion on the above properties and their relationship can be found in <ref> [Yan93] </ref>. Most task graphs derived from real scientific applications satisfy these properties without renaming. For example, sparse LU graphs discussed in Section 6.2 are dependence-complete.
Reference: [YFGS95] <author> Tao Yang, Cong Fu, Apostolos Gerasoulis, and Vivek Sarkar. </author> <title> Mapping Iterative Task Graphs on Distributed-memory Machines . In International Conf. </title> <booktitle> on Parallel Processing, </booktitle> <pages> pages 151-158, </pages> <month> August </month> <year> 1995. </year>
Reference-contexts: Static scheduling can adapt to certain degree of run-time weight variation <ref> [GJY95, YFGS95] </ref> and its run-time control mechanism is relatively simple. <p> These two issues make timing among tasks different from what is expected at the static time and tend to increase the processor idle time. The performance becomes even more sensitive when task granularities are small <ref> [GJY95, YFGS95] </ref>.
Reference: [YG92] <author> T. Yang and A. Gerasoulis. </author> <title> PYRROS: Static Task Scheduling and Code Generation for Message-Passing Multiprocessors . In Proc. </title> <booktitle> of 6th ACM Inter. Confer. on Supercomputing, </booktitle> <pages> pages 428-437, </pages> <year> 1992. </year> <month> 27 </month>
Reference-contexts: The DAG schedule can be re-used for a number of iterations before re-scheduling since particles movement is slow. Then good speedups are obtained on nCUBE-2 for large n-body simulation problems after applying PYRROS DAG scheduling techniques <ref> [YG92] </ref>. Such results demonstrate that graph scheduling can effectively exploit irregular task parallelism to balance loads and overlap computation with communication. <p> Most of previous work in DAG scheduling has focused on the algorithmic research for mapping tasks onto multi-processors based on the macro task computation model [Sar89, WG90]. But few researches are conducted on efficient run-time support for executing task schedules. The early work in the PYRROS system <ref> [YG92] </ref> provides a complete framework for general task computation; however, its run-time support system uses the NX/2 level communication interface and the overhead of message buffer managing and data copying is significant, which prevents PYRROS from obtaining good performance in executing sparse matrix computations with mixed granularities. <p> Therefore, we have processor assignment (A) = (B) = (D) = 0, (C) = (E) = 1, and execution orderings O 0 = fA; B; Dg, O 1 = fC; Eg. Algorithms for static scheduling of DAGs have been extensively studied in the literatures, e.g. <ref> [CR92, PP95, Sar89, YG92] </ref>. Dynamic scheduling which is adaptive to run-time change of dependence structures and weights is more attractive [Pol88], but it is still difficult to balance its benefits and the run-time control overhead in executing tasks with mixed grains on message-passing ma 3 chines. <p> Optimizations have to be performed to eliminate redundant messages. Correspondingly, in the destination only one receiving is needed to pull out data from the network. Subsequent receiving operations for this data item should be re-directed to read from the local memory instead. PYRROS <ref> [YG92] </ref> uses the buffered message-passing mechanism and embodies aggregate communication as much as possible.
References-found: 32

