URL: http://www-cse.ucsd.edu/users/gary/pubs/padgett_cogsci96.ps
Refering-URL: http://www.cse.ucsd.edu/users/gary/
Root-URL: 
Email: fcpadgett,garyg@cs.ucsd.edu  radolphs@blue.weeg.uiowa.edu  
Title: Categorical Perception in Facial Emotion Classification  
Author: Curtis Padgett and Garrison W. Cottrell Ralph Adolphs 
Address: La Jolla, CA 92093  Iowa, Iowa City  
Affiliation: Computer Science Engineering University of California, San Diego  Department of Neurology University of  
Abstract: We present an automated emotion recognition system that is capable of identifying six basic emotions (happy, surprise, sad, angry, fear, disgust) in novel face images. An ensemble of simple feed-forward neural networks are used to rate each of the images. The outputs of these networks are then combined to generate a score for each emotion. The networks were trained on a database of face images that human subjects consistently rated as portraying a single emotion. Such a system achieves 86% generalization on novel face images (individuals the networks were not trained on) drawn from the same database. The neural network model exhibits categorical perception between some emotion pairs. A linear sequence of morph images is created between two expressions of an individual's face and this sequence is analyzed by the model. Sharp transitions in the output response vector occur in a single step in the sequence for some emotion pairs and not for others. We plan to us the model's response to limit and direct testing in determining if human subjects exhibit categorical perception in morph image sequences. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> M. Bartlett, P. Viola, T. Sejnowski, J. Larsen, J. Hager, and P. Ekman. </author> <title> Classifying facial action. </title> <booktitle> In Advances in Neural Information Processing Systems 8, </booktitle> <address> Cambridge, MA, 1996. </address> <publisher> MIT Press. </publisher>
Reference-contexts: These results are comparable with emotion recognition rates obtained by other automated vision systems which require a neutral to emotion temporal sequences for training and evaluation <ref> [11, 15, 1] </ref>. Face Data In working with emotions in face images, care must be taken to insure that the particular emotion being portrayed is correct. Feigned emotions by untrained individuals exhibit significant differences with the prototypical face expression [7].
Reference: [2] <author> J. Beale and F. Keil. </author> <title> Categorical effects in the perception of faces. </title> <journal> Cognition, </journal> <volume> 57 </volume> <pages> 217-239, </pages> <year> 1992. </year>
Reference-contexts: Although the evidence for certain "low level" categorical perception is strong (e.g. phonemes, colors), much less is known about how we categorize more complex stimuli. Two recent studies suggest that some of the information signaled by faces, notably their emotional expression [8] and their unique identity <ref> [2] </ref>, is perceived categorically. The present study concentrates on the perception of emotional facial expressions. We are aware of only a single study that has provided evidence for categorical perception of emotion in facial expressions [8].
Reference: [3] <author> D. Beymer. </author> <title> Face recognition under varying pose. </title> <type> Technical Report AI Memo No. 1461, </type> <institution> MIT Artificial Intelligence Lab, </institution> <year> 1993. </year>
Reference-contexts: Illumination variances were minimized by individually stretching each of the images to encompass the full grey scale range. Similar techniques have been employed in previous work on faces <ref> [3, 4, 14, 13] </ref>. Figure 3 shows examples of some of the normalized face images used in the study. Classifier design and training The models used to conduct this study consist of ensembles of feed-forward, fully connected neural networks each containing a single hidden layer with 10 nodes.
Reference: [4] <author> R. Brunelli and T. Poggio. </author> <title> Face recognition: Feature versus templates. </title> <journal> IEEE Trans. Patt. Anal. Machine In-tell., </journal> <volume> 15(10), </volume> <month> October </month> <year> 1993. </year>
Reference-contexts: Illumination variances were minimized by individually stretching each of the images to encompass the full grey scale range. Similar techniques have been employed in previous work on faces <ref> [3, 4, 14, 13] </ref>. Figure 3 shows examples of some of the normalized face images used in the study. Classifier design and training The models used to conduct this study consist of ensembles of feed-forward, fully connected neural networks each containing a single hidden layer with 10 nodes.
Reference: [5] <author> Garrison W. Cottrell and Janet Metcalfe. Empath: </author> <title> Face, gender and emotion recognition using holons. </title> <editor> In R.P. Lippman, J. Moody, and D.S. Touretzky, editors, </editor> <booktitle> Advances in Neural Information Processing Systems 3, </booktitle> <pages> pages 564-571, </pages> <address> San Mateo, 1991. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: In previous work, Cottrell and Metcalfe had undergraduates feign emotions. While their network performed well on identity and gender classification, it never did well on emotion. Cottrell and Metcalfe speculated that their results were due to poor portrayal of the emotions by their subjects <ref> [5] </ref>. patches drawn randomly over the face database. To reduce this possibility, we make use of a validated facial emotion database (Pictures of Facial Affect) assembled by Ekman and Friesen [6].
Reference: [6] <author> P. Ekman and W. Friesen. </author> <title> Pictures of facial affect, </title> <year> 1976. </year>
Reference-contexts: Cottrell and Metcalfe speculated that their results were due to poor portrayal of the emotions by their subjects [5]. patches drawn randomly over the face database. To reduce this possibility, we make use of a validated facial emotion database (Pictures of Facial Affect) assembled by Ekman and Friesen <ref> [6] </ref>. Each of the face images in this set exhibits a substantial agreement between the labeled emotion and the observed response of human subjects. The actors used in this database were trained to reliably produce emotions using FACS [7] and their images were presented to undergraduates for testing.
Reference: [7] <author> P. Ekman and W. Friesen. </author> <title> Facial Action Coding System. </title> <publisher> Consulting Psychologists, </publisher> <address> Palo Alto, CA, </address> <year> 1977. </year>
Reference-contexts: Face Data In working with emotions in face images, care must be taken to insure that the particular emotion being portrayed is correct. Feigned emotions by untrained individuals exhibit significant differences with the prototypical face expression <ref> [7] </ref>. These differences often result in disagreement between the observed emotion and the expression the actor is attempting to feign. In previous work, Cottrell and Metcalfe had undergraduates feign emotions. While their network performed well on identity and gender classification, it never did well on emotion. <p> Each of the face images in this set exhibits a substantial agreement between the labeled emotion and the observed response of human subjects. The actors used in this database were trained to reliably produce emotions using FACS <ref> [7] </ref> and their images were presented to undergraduates for testing. The agreement between the emotion the actor was required to express and the students' observations was at least 70% on all the images incorporated into the database.
Reference: [8] <author> N. Etcoff and J. Magee. </author> <title> Categorical perception of facial expressions. </title> <journal> Cognition, </journal> <volume> 44 </volume> <pages> 227-240, </pages> <year> 1992. </year>
Reference-contexts: Although the evidence for certain "low level" categorical perception is strong (e.g. phonemes, colors), much less is known about how we categorize more complex stimuli. Two recent studies suggest that some of the information signaled by faces, notably their emotional expression <ref> [8] </ref> and their unique identity [2], is perceived categorically. The present study concentrates on the perception of emotional facial expressions. We are aware of only a single study that has provided evidence for categorical perception of emotion in facial expressions [8]. <p> of the information signaled by faces, notably their emotional expression <ref> [8] </ref> and their unique identity [2], is perceived categorically. The present study concentrates on the perception of emotional facial expressions. We are aware of only a single study that has provided evidence for categorical perception of emotion in facial expressions [8]. That study used line drawings of faces, and morphs of those line drawings, as the stimuli. Transitions between certain emotional expressions appeared to be perceived categorically, while other transitions did not show such an effect.
Reference: [9] <author> Stevan R. Harnad. </author> <title> Categorical perception: the groundwork of cognition. </title> <publisher> Cambridge University Press, </publisher> <address> Cam-bridge, NY, </address> <year> 1987. </year>
Reference-contexts: That is, prior to the transition, the model classifies all the images in the sequence as examples of the first category and all the subsequent images as examples of the second emotion. Such transitions are known as categorical perception and are known to occur in many perceptual tasks <ref> [9] </ref>. The model's predictions can then be compared with a similar set of tasks performed on human subjects. From this interaction we hope to discern the functional organization of the visual emotion recognition system. <p> The average generalization rate achieved by the classifiers is 86% (0:2). Morph Sequence A large body of literature in cognitive psychology has demonstrated that certain stimuli, such as phonemes, are perceived categorically by human subjects <ref> [10, 9] </ref>.
Reference: [10] <author> A. Liberman, K. Harris, H. Hoffman, and B. Griffith. </author> <title> The discrimination of speech sounds within and across phoneme boundaries. </title> <journal> Journal of Experimental Psychology, </journal> <volume> 54 </volume> <pages> 358-368, </pages> <year> 1957. </year>
Reference-contexts: The average generalization rate achieved by the classifiers is 86% (0:2). Morph Sequence A large body of literature in cognitive psychology has demonstrated that certain stimuli, such as phonemes, are perceived categorically by human subjects <ref> [10, 9] </ref>.
Reference: [11] <author> K. Mase. </author> <title> Recognition of facial expression from optical flow. </title> <journal> IEICE Transactions, </journal> <volume> 74(10) </volume> <pages> 3474-3483, </pages> <year> 1991. </year>
Reference-contexts: These results are comparable with emotion recognition rates obtained by other automated vision systems which require a neutral to emotion temporal sequences for training and evaluation <ref> [11, 15, 1] </ref>. Face Data In working with emotions in face images, care must be taken to insure that the particular emotion being portrayed is correct. Feigned emotions by untrained individuals exhibit significant differences with the prototypical face expression [7].
Reference: [12] <author> C. Padgett and G. Cottrell. </author> <title> Identifying emotion in static face images. </title> <booktitle> In Proceedings of the 2nd Joint Symposium on Neural Computation, </booktitle> <volume> volume 5, </volume> <pages> pages 91-101, </pages> <address> La Jolla, CA, </address> <year> 1995. </year> <institution> University of California, </institution> <address> San Diego. </address>
Reference-contexts: In previous work we have shown that the generalization obtained with this representation is superior to those obtained using the eigen-face/feature strategy <ref> [12] </ref>. The expected generalization rate on novel individuals presented to the network making use of the random block representation is 86% while humans do nearly 92% on the same database. <p> We set the threshold at 0.5 standard deviations above the average response, which is the maximum value that maintains the 86% generalization rate of the network (in the original work, a correct response was taken as the maximum Z score <ref> [12] </ref>). For Type 1 transitions, both emotions elicit high responses over a large portion of the sequence indicating similarity between the two emotions in the transition sequence. A Type 2 transition is when both emotion responses are below threshold at the crossover point.
Reference: [13] <author> A. Pentland, B. Moghaddam, and T. Starner. </author> <title> View-based and modular eigenspaces for face recognition. </title> <booktitle> In IEEE Conference on Computer Vision & Pattern Recognition, </booktitle> <year> 1994. </year>
Reference-contexts: This technique is similar to the eigen-face/feature recognition work of Turk and Pentland [14] and Pentland et. al. <ref> [13] </ref> where projections of faces or features are used to reduce the dimensionality of the image. However, where their work uses fixed locations on the face to generate the eigen-space (result blocks in the feature regions (only one eye region is shown, both are used). <p> Illumination variances were minimized by individually stretching each of the images to encompass the full grey scale range. Similar techniques have been employed in previous work on faces <ref> [3, 4, 14, 13] </ref>. Figure 3 shows examples of some of the normalized face images used in the study. Classifier design and training The models used to conduct this study consist of ensembles of feed-forward, fully connected neural networks each containing a single hidden layer with 10 nodes.
Reference: [14] <author> Matthew Turk and Alexander Pentland. </author> <title> Eigenfaces for recognition. </title> <journal> The Journal of Cognitive Neuroscience, </journal> <volume> 3 </volume> <pages> 71-86, </pages> <year> 1991. </year>
Reference-contexts: This technique is similar to the eigen-face/feature recognition work of Turk and Pentland <ref> [14] </ref> and Pentland et. al. [13] where projections of faces or features are used to reduce the dimensionality of the image. However, where their work uses fixed locations on the face to generate the eigen-space (result blocks in the feature regions (only one eye region is shown, both are used). <p> Illumination variances were minimized by individually stretching each of the images to encompass the full grey scale range. Similar techniques have been employed in previous work on faces <ref> [3, 4, 14, 13] </ref>. Figure 3 shows examples of some of the normalized face images used in the study. Classifier design and training The models used to conduct this study consist of ensembles of feed-forward, fully connected neural networks each containing a single hidden layer with 10 nodes.
Reference: [15] <author> Yacoob and Davis. </author> <title> Recognizing human facial expression. </title> <type> Technical Report CAR-TR-706, </type> <institution> University of Maryland Center for Automation Research, </institution> <year> 1994. </year>
Reference-contexts: These results are comparable with emotion recognition rates obtained by other automated vision systems which require a neutral to emotion temporal sequences for training and evaluation <ref> [11, 15, 1] </ref>. Face Data In working with emotions in face images, care must be taken to insure that the particular emotion being portrayed is correct. Feigned emotions by untrained individuals exhibit significant differences with the prototypical face expression [7].
References-found: 15

