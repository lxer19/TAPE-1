URL: http://www.cs.tamu.edu/faculty/bhuyan/papers/TR98-002.ps
Refering-URL: http://www.cs.tamu.edu/people/ravi/
Root-URL: http://www.cs.tamu.edu
Email: E-mail: fravi, hwang, bhuyang@cs.tamu.edu  
Title: Effect of CC-NUMA Memory Management on the Performance of Interconnection Networks  
Author: R. Iyer, H. Wang and L. Bhuyan 
Keyword: memory management, switch design, wormhole routing, virtual channels, execution-based simulation, scientific applications, shared-memory multiprocessor.  
Note: This research has been supported by NSF grant MIP 9622740.  
Address: College Station, TX 77843-3112, USA.  
Affiliation: Department of Computer Science Texas A&M University  
Abstract: CC-NUMA architectures have become extremely popular by providing fast and transparent access to data with multiple levels of caches and local and remote memories. However, the bottleneck remains in the remote memory access that has latencies several magnitudes higher than the cache access. Designing effective data allocation policies that provide local memory data access and limit the need to access remote memories remains a challenge. We study three different static memory management policies, namely buddy, round-robin and first-touch, and analyze their impact on data locality and application memory access patterns. Interconnection network performance depends heavily on the memory access patterns of the workload. Using these realistic memory management policies, we reevaluate the performance of a multistage interconnection network (MIN). Limited earlier work in the area of CC-NUMA memory management has assumed constant network delays and thus ignored the impact of switch design on performance. We use accurate switch design models in conjunction with the changing memory access patterns caused by the memory management policies and the application data access behavior. Sensitivity studies are conducted over a wide range of important parameters to determine the impact of memory management and switch architecture design. Our results show improvements as high as 90% in stall time due to better memory management policies and improved switch designs. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> R. P. Larowe and C. S. Ellis, </author> <title> "Experimental Comparisons of Memory Management Policies for NUMA Multiprocessors," </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> vol. 9, no. 4, </volume> <pages> pp. 319-363, </pages> <month> Nov. </month> <year> 1991. </year>
Reference-contexts: While considering related work in this area, we find two categories. The first category has been the system level performance evaluation of memory management policies <ref> [1, 2, 3] </ref>. Most of these studies focus on distributed shared memory systems without hardware cache coherence. Recently the effect of different policies was studied for CC-NUMA systems in [4]. This work concentrated on the OS/hardware support required for dynamic page migration and replication policies.
Reference: [2] <author> C. Scheurich and M. Dubois, </author> <title> "Dynamic Page Migration in Multiprocessors with Distributed Global Memory," </title> <journal> IEEE Transactions on Computers, </journal> <volume> vol. 38, no. 8, </volume> <month> Aug. </month> <year> 1989. </year>
Reference-contexts: While considering related work in this area, we find two categories. The first category has been the system level performance evaluation of memory management policies <ref> [1, 2, 3] </ref>. Most of these studies focus on distributed shared memory systems without hardware cache coherence. Recently the effect of different policies was studied for CC-NUMA systems in [4]. This work concentrated on the OS/hardware support required for dynamic page migration and replication policies.
Reference: [3] <author> J. Ramanathan and L. M. Ni, </author> <title> "Critical Factors in NUMA memory management," </title> <booktitle> In Proc of the 11th Conference on Distributed Computing Systems, </booktitle> <pages> pp. 500-507, </pages> <address> Arlington , Texas, </address> <month> May </month> <year> 1991, </year> <journal> IEEE Computer Society. </journal>
Reference-contexts: While considering related work in this area, we find two categories. The first category has been the system level performance evaluation of memory management policies <ref> [1, 2, 3] </ref>. Most of these studies focus on distributed shared memory systems without hardware cache coherence. Recently the effect of different policies was studied for CC-NUMA systems in [4]. This work concentrated on the OS/hardware support required for dynamic page migration and replication policies.
Reference: [4] <author> B. Vergese, et al., </author> <title> "Operating System Support for improving data locality on CC-NUMA Computer Servers," </title> <booktitle> Proc. ASPLOS-VII, </booktitle> <pages> pp. 279-289, </pages> <month> Oct. </month> <year> 1996. </year> <month> 27 </month>
Reference-contexts: The first category has been the system level performance evaluation of memory management policies [1, 2, 3]. Most of these studies focus on distributed shared memory systems without hardware cache coherence. Recently the effect of different policies was studied for CC-NUMA systems in <ref> [4] </ref>. This work concentrated on the OS/hardware support required for dynamic page migration and replication policies. While this work presents significant results in CC-NUMA memory management, it did not model the delay/interference at the interconnection network level. <p> The improvement in stall time performance is as high as 90% for some applications. The paper makes the following contributions. * We consider buddy, round-robin and first-touch <ref> [4] </ref> page allocation and study their effect on memory access patterns, data sharing patterns and the network latencies for several scientific applications. * We consider two crossbar switch design alternatives for the multistage interconnection network in cache-coherent shared memory systems and evaluate its performance.
Reference: [5] <author> L.N. Bhuyan, Q. Yang and D.P. Agrawal, </author> <title> "Performance of Multiprocessor Interconnection Net--works," </title> <journal> Computer, </journal> <volume> vol. 22, no. 2, pp.25-37, </volume> <month> Feb. </month> <year> 1989. </year>
Reference-contexts: The second related work category is the design and performance evaluation of interconnection networks. Performance evaluation of interconnection networks (IN's) has been an active area of research for a long time <ref> [5, 6, 7, 8] </ref>. These studies are conducted with a synthetic workload that is more suitable for a message passing or networking environment. Workload in a CC-NUMA environment is characterized by unsymmetric bulky messages due to cache coherence, synchronization and memory management.
Reference: [6] <author> H. Yoon, K.Y.Lee, and M.T .Liu, </author> <title> "Performance Analysis of Multibuffered Packet-Switching Networks in Multiprocessor Systems," </title> <journal> IEEE Transactions on Computers, </journal> <volume> vol. 39, no. 3, </volume> <pages> pp. 319-327, </pages> <month> Mar. </month> <year> 1990. </year>
Reference-contexts: The second related work category is the design and performance evaluation of interconnection networks. Performance evaluation of interconnection networks (IN's) has been an active area of research for a long time <ref> [5, 6, 7, 8] </ref>. These studies are conducted with a synthetic workload that is more suitable for a message passing or networking environment. Workload in a CC-NUMA environment is characterized by unsymmetric bulky messages due to cache coherence, synchronization and memory management.
Reference: [7] <author> W. J. Dally, </author> <title> "Virtual-Channel Flow Control," </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> vol. 3, no. 2, </volume> <pages> pp. 194-205, </pages> <month> March </month> <year> 1992. </year>
Reference-contexts: The second related work category is the design and performance evaluation of interconnection networks. Performance evaluation of interconnection networks (IN's) has been an active area of research for a long time <ref> [5, 6, 7, 8] </ref>. These studies are conducted with a synthetic workload that is more suitable for a message passing or networking environment. Workload in a CC-NUMA environment is characterized by unsymmetric bulky messages due to cache coherence, synchronization and memory management.
Reference: [8] <author> L. N. Bhuyan, et al., </author> <title> "Performance of Multistage Bus Networks for a Distributed Shared Memory Multiprocessor," </title> <journal> IEEE Transactions on Prallel and Distributed Systems, </journal> <volume> vol. 8, no. 1, </volume> <pages> pp. 82-95, </pages> <month> Jan. </month> <year> 1997. </year>
Reference-contexts: The second related work category is the design and performance evaluation of interconnection networks. Performance evaluation of interconnection networks (IN's) has been an active area of research for a long time <ref> [5, 6, 7, 8] </ref>. These studies are conducted with a synthetic workload that is more suitable for a message passing or networking environment. Workload in a CC-NUMA environment is characterized by unsymmetric bulky messages due to cache coherence, synchronization and memory management.
Reference: [9] <author> A. Kumar and L.N. Bhuyan, </author> <title> "Evaluating virtual channels for cache coherent shared memory multiprocessors," </title> <booktitle> ACM International Conference on Supercomuting, </booktitle> <address> Philadelphia, </address> <month> May </month> <year> 1996. </year>
Reference-contexts: The aim of this paper is to evaluate the performance of an IN with realistic application data accesses and memory management policies governing the placement of the data blocks. Execution-based evaluations for mesh interconnection networks have been reported recently <ref> [9, 10] </ref> to test the effectiveness of virtual channels and adaptive routing. While these studies provide useful data, they do not explore different memory management techniques as we do. <p> Contrary to current assumptions of constant network delays, we show that the impact of switch architectures is significant. In a recent study [10], the performance benefits of virtual channels were shown to be negligible contradicting earlier work in <ref> [9] </ref> where a buddy allocation was used. However, their work [10] was based on a hand-optimized memory management policy, which underestimates the frequency of remote memory accesses in realistic CC-NUMA systems.
Reference: [10] <author> A. Vaidya, A. Sivasubramaniam and C. Das, </author> <title> "Performance Benefits of Virtual Channels and Adaptive Routing: An Application-Driven Study," </title> <booktitle> Proc. 11th International Conference on Supercomputing, </booktitle> <address> Vienna, </address> <month> July </month> <year> 1997. </year>
Reference-contexts: The aim of this paper is to evaluate the performance of an IN with realistic application data accesses and memory management policies governing the placement of the data blocks. Execution-based evaluations for mesh interconnection networks have been reported recently <ref> [9, 10] </ref> to test the effectiveness of virtual channels and adaptive routing. While these studies provide useful data, they do not explore different memory management techniques as we do. <p> Advanced switch architectures together with the first-touch policy gives the best performance for all the applications. Contrary to current assumptions of constant network delays, we show that the impact of switch architectures is significant. In a recent study <ref> [10] </ref>, the performance benefits of virtual channels were shown to be negligible contradicting earlier work in [9] where a buddy allocation was used. However, their work [10] was based on a hand-optimized memory management policy, which underestimates the frequency of remote memory accesses in realistic CC-NUMA systems. <p> Contrary to current assumptions of constant network delays, we show that the impact of switch architectures is significant. In a recent study <ref> [10] </ref>, the performance benefits of virtual channels were shown to be negligible contradicting earlier work in [9] where a buddy allocation was used. However, their work [10] was based on a hand-optimized memory management policy, which underestimates the frequency of remote memory accesses in realistic CC-NUMA systems.
Reference: [11] <author> D. Chaiken, et al., </author> <title> "Directory-Based Cache Coherence in Large Scale Multiprocessors," </title> <journal> Computer, </journal> <volume> vol. 23, no. 6, </volume> <pages> pp. 49-58, </pages> <month> June </month> <year> 1990. </year>
Reference-contexts: Execution-based evaluations for mesh interconnection networks have been reported recently [9, 10] to test the effectiveness of virtual channels and adaptive routing. While these studies provide useful data, they do not explore different memory management techniques as we do. Also, several cache coherence studies <ref> [11, 12] </ref> do not model the IN switches explicitly, thus ignoring possible inter 1 ference among various messages. Use of highly sophisticated crossbar switches [13, 14] are becoming increasingly common in commercial multiprocessing systems.
Reference: [12] <author> D.J. Lilja, </author> <title> "Cache Coherence in Large Scale Shared Memory Multiprocessors," </title> <journal> ACM Computing Surveys, </journal> <volume> vol. 25, no.3, </volume> <pages> pp. 303-338, </pages> <month> Sept. </month> <year> 1993. </year>
Reference-contexts: Execution-based evaluations for mesh interconnection networks have been reported recently [9, 10] to test the effectiveness of virtual channels and adaptive routing. While these studies provide useful data, they do not explore different memory management techniques as we do. Also, several cache coherence studies <ref> [11, 12] </ref> do not model the IN switches explicitly, thus ignoring possible inter 1 ference among various messages. Use of highly sophisticated crossbar switches [13, 14] are becoming increasingly common in commercial multiprocessing systems.
Reference: [13] <author> M. Galles, </author> <title> "Scalable Pipelined Interconnect for Distributed Endpoint Routing: The SGI SPIDER Chip," </title> <booktitle> Proc. Symp. High Performance Interconnects (Hot Interconnects 4), </booktitle> <month> Aug. </month> <year> 1996. </year>
Reference-contexts: While these studies provide useful data, they do not explore different memory management techniques as we do. Also, several cache coherence studies [11, 12] do not model the IN switches explicitly, thus ignoring possible inter 1 ference among various messages. Use of highly sophisticated crossbar switches <ref> [13, 14] </ref> are becoming increasingly common in commercial multiprocessing systems. We consider a multistage interconnection network (MIN) in this paper, similar to the one employed in Butterfly and Cedar multiprocessors [15]. <p> The switch parameters are explained in detail in section 3.1. A flit size of 16 bits was considered which is same as the width of the link. The switch latency or delay is considered to be 1, 2 or 4 processor cycles. The SGI Spider and Cavallino <ref> [13, 14] </ref> both have 4 cycles of latency. Additionally, we consider 1 or 2 cycle delays for comparison since our aim is to design high-speed networks. The interface delay is 20 cycles similar to DASH [16]. 3.1 Network/Switch Architecture The schematic of the network is shown in Fig. 3b. <p> In virtual cut-through, the message is buffered when it is blocked. We use these switching techniques because they give better performance than packet switching and because they are used in the design of current multiprocessor switches (SPIDER <ref> [13] </ref>, CAVALLINO 6 [14]). There are two types of messages that are transmitted over the MIN. One is a control message that consists of read, write and invalidation signals that are short and 4 flits long. <p> The input synchronization takes one to two cycles and it takes one more cycle for transmission. As a result, the switch delay is about 4 cycles, which is similar to the time taken in SGI Spider and Intel Cavallino switches <ref> [13, 14] </ref>. Virtual channels (VC's)[7] are used in wormhole networks to avoid deadlocks and to improve link utilization and network throughput. Whenever there is a blocking of a message, another message can be routed through other VCs.
Reference: [14] <author> J. Carbonaro and F. Verhoorn, "Cavallino: </author> <title> The Teraflops Router and NIC," </title> <booktitle> Proc. Symp. High Performance Interconnects (Hot Interconnects 4), </booktitle> <month> Aug. </month> <year> 1996. </year>
Reference-contexts: While these studies provide useful data, they do not explore different memory management techniques as we do. Also, several cache coherence studies [11, 12] do not model the IN switches explicitly, thus ignoring possible inter 1 ference among various messages. Use of highly sophisticated crossbar switches <ref> [13, 14] </ref> are becoming increasingly common in commercial multiprocessing systems. We consider a multistage interconnection network (MIN) in this paper, similar to the one employed in Butterfly and Cedar multiprocessors [15]. <p> The switch parameters are explained in detail in section 3.1. A flit size of 16 bits was considered which is same as the width of the link. The switch latency or delay is considered to be 1, 2 or 4 processor cycles. The SGI Spider and Cavallino <ref> [13, 14] </ref> both have 4 cycles of latency. Additionally, we consider 1 or 2 cycle delays for comparison since our aim is to design high-speed networks. The interface delay is 20 cycles similar to DASH [16]. 3.1 Network/Switch Architecture The schematic of the network is shown in Fig. 3b. <p> In virtual cut-through, the message is buffered when it is blocked. We use these switching techniques because they give better performance than packet switching and because they are used in the design of current multiprocessor switches (SPIDER [13], CAVALLINO 6 <ref> [14] </ref>). There are two types of messages that are transmitted over the MIN. One is a control message that consists of read, write and invalidation signals that are short and 4 flits long. <p> The input synchronization takes one to two cycles and it takes one more cycle for transmission. As a result, the switch delay is about 4 cycles, which is similar to the time taken in SGI Spider and Intel Cavallino switches <ref> [13, 14] </ref>. Virtual channels (VC's)[7] are used in wormhole networks to avoid deadlocks and to improve link utilization and network throughput. Whenever there is a blocking of a message, another message can be routed through other VCs. <p> Multiple input channels can request for the same output channel in this phase. This conflict is resolved in the second step. A similar arbitration policy is used in the Intel Cavallino switch <ref> [14] </ref>. 3.2 Cache Coherence And Synchronization We implemented the full-map directory-based cache coherence protocol [19] with some modifications for evaluation in this paper. In this scheme, each shared memory block is assigned to a node, called home node, which maintains the directory entries for that block.
Reference: [15] <author> J. Torrellas and Z. Zheng, </author> <title> "The Performance of the Cedar Multistage Switching Network," </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> vol. 8, no. 4, </volume> <pages> pp. 321-336, </pages> <month> April </month> <year> 1994. </year>
Reference-contexts: Use of highly sophisticated crossbar switches [13, 14] are becoming increasingly common in commercial multiprocessing systems. We consider a multistage interconnection network (MIN) in this paper, similar to the one employed in Butterfly and Cedar multiprocessors <ref> [15] </ref>. Unlike Cedar, we employ a NUMA organization with one network (like Butterfly) that is used for both forward and backward (reply) messages. To evaluate the different memory management techniques in conjunction with different switch architectures, we have significantly modified our CC-NUMA simulator based on Proteus [17]. <p> The immediate result that can be observed in the average message latency figures is the large amount of delay in the backward stage 0 for buddy and round-robin policies. The same situation was observed in Cedar network <ref> [15] </ref> where there was a large delay at the input of the backward network. In our case, the same network is used both for forward and backward requests and we use wormhole or cut-through switching instead of packet switching in Cedar.
Reference: [16] <author> D. Lenoski, et. al., </author> <title> "The DASH Prototype: Logic Overhead and Performance," </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> vol. 4, no. 1, </volume> <pages> pp. 41-61, </pages> <month> Jan </month> <year> 1993. </year>
Reference-contexts: The distribution of the pages is extremely uniform thus smoothening out the traffic generated in the network and in most cases making sure that simultaneous access to different data from different processors are interleaved among the different nodes. This scheme is used in DASH <ref> [16] </ref>. However, the scheme does not place data intelligently enough to reduce the number of remote accesses. Since the distribution is regular in most scientific applications, many processors tend to access multiple pages on the same node in a given time interval. <p> The SGI Spider and Cavallino [13, 14] both have 4 cycles of latency. Additionally, we consider 1 or 2 cycle delays for comparison since our aim is to design high-speed networks. The interface delay is 20 cycles similar to DASH <ref> [16] </ref>. 3.1 Network/Switch Architecture The schematic of the network is shown in Fig. 3b. It is a multistage interconnection network (MIN) employing 2x2 switches. In general, an NxN MIN consists of log 2 N stages of 2x2 switches with N/2 such switches per stage.
Reference: [17] <author> E. A. Brewer, C. N. Dellarocas, A. Colbrook, and W. E. Weihl, "Proteus: </author> <title> A High-Performance Parallel-Architecture Simulator," </title> <type> Technical Report MIT/LCS/TR-516, </type> <institution> Massachusetts Institute of Technology, </institution> <address> Cambridge, MA, </address> <month> September </month> <year> 1991. </year>
Reference-contexts: Unlike Cedar, we employ a NUMA organization with one network (like Butterfly) that is used for both forward and backward (reply) messages. To evaluate the different memory management techniques in conjunction with different switch architectures, we have significantly modified our CC-NUMA simulator based on Proteus <ref> [17] </ref>. For this evaluation, we have added virtual memory support to the simulator which enables us to study page-based memory management techniques such as round-robin and first-touch. <p> However, in applications where one processor touches a page first that is later used more often by another processor, there will be frequent remote memory accesses. 3 Simulator Design Our simulator is based on Proteus <ref> [17] </ref> that implemented MIN using an analytical model. We have modified the simulator extensively to exactly model the MIN with wormhole routing. We have also incorporated different switch architectures with virtual channels and multi-flit buffers.
Reference: [18] <author> Y. Tamir and H. Chi, </author> <title> "Symmetric Crossbar Arbiters for VLSI Communication Switches," </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> vol. 4, no. 1, </volume> <pages> pp. 13-27, </pages> <month> Jan </month> <year> 1993. </year>
Reference-contexts: The amount of buffer needed per input and output for wormhole routing is only one flit that is absolutely essential for transmission over the links. Once the flit arrives, the arbitration starts and the connection is made. Such a simple crossbar arbiter takes one cycle arbitration time <ref> [18] </ref>, however, the grant signals are modified to ensure continuity of transmission due to wormhole routing. The input synchronization takes one to two cycles and it takes one more cycle for transmission.
Reference: [19] <author> L. M. Censier and P. Feautrier, </author> <title> "A New Solution to Coherence Problems in Multicache Systems," </title> <journal> IEEE Transactions on Computers, </journal> <volume> vol. C-27, no. 12, </volume> <pages> pp. 1112-1118, </pages> <month> December </month> <year> 1978. </year>
Reference-contexts: In addition to the memory management techniques, we have also incorporated detailed switch models and wormhole routing with virtual channels in the network to accurately interpret the effect on network latency and application stall time. We have modified the directory based cache coherence protocol <ref> [19] </ref> in the simulator to correctly account for the possible out-of-order messages due to the presence of virtual channels in the IN. We present the results of two switch architectures (simple wormhole (SWH) and buffered virtual channel (BVC)) in this paper. <p> Multiple input channels can request for the same output channel in this phase. This conflict is resolved in the second step. A similar arbitration policy is used in the Intel Cavallino switch [14]. 3.2 Cache Coherence And Synchronization We implemented the full-map directory-based cache coherence protocol <ref> [19] </ref> with some modifications for evaluation in this paper. In this scheme, each shared memory block is assigned to a node, called home node, which maintains the directory entries for that block. Each entry in the directory is a bit-vector of same length as the number of nodes.
Reference: [20] <author> J. P. Singh, W.-D. Weber, and A. Gupta, </author> <title> "SPLASH: Stanford Parallel Applications for Shared-Memory," </title> <journal> ACM SIGARCH Computer Architecture News, </journal> <volume> vol. 20, no. 1, </volume> <pages> pp. 5-44, </pages> <month> March </month> <year> 1992. </year> <month> 28 </month>
Reference-contexts: The overhead due to contention on synchronization variables is significant for some applications, such as MP3D <ref> [20] </ref>. The network latency also plays an important role on the synchronization overhead. In this paper, we do not discuss the synchronization overhead associated with any of the simulations. <p> In rest of the log 2 P stages, every data point is shared by two processors. During these stages, instead of using two separate input and output array, we interleave these arrays to avoid large number of conflict misses. MP3D <ref> [20] </ref> is a three-dimensional particle simulator used in rarefied fluid flow simulation. We used 25000 molecules with the default geometry provided with SPLASH [20] which uses a 14 fi 24 fi 4 (2646-cell) space containing a single flat sheet placed at an angle to the free stream. <p> During these stages, instead of using two separate input and output array, we interleave these arrays to avoid large number of conflict misses. MP3D <ref> [20] </ref> is a three-dimensional particle simulator used in rarefied fluid flow simulation. We used 25000 molecules with the default geometry provided with SPLASH [20] which uses a 14 fi 24 fi 4 (2646-cell) space containing a single flat sheet placed at an angle to the free stream. The simulation was done for 5 time steps.
References-found: 20

