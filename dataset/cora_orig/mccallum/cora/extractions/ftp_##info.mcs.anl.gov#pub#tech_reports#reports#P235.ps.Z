URL: ftp://info.mcs.anl.gov/pub/tech_reports/reports/P235.ps.Z
Refering-URL: http://www.mcs.anl.gov/publications/abstracts/abstracts91.htm
Root-URL: http://www.mcs.anl.gov
Email: bischof@mcs.anl.gov  
Title: Issues in Parallel Automatic Differentiation  Automatic Differentiation of Algorithms,  
Author: Christian H. Bischof A. Griewank and G. Corliss, 
Note: Argonne Preprint MCS-P235-0491 Published in  Eds., SIAM, Philadelphia, pp. 100-113, 1991.  
Address: Argonne, IL 60439-4801  
Affiliation: Mathematics and Computer Science Division Argonne National Laboratory  
Abstract: This paper shows how first-order derivatives can be computed in parallel by considering the computational graph that underlies the evaluation of the target function. The graph can be generated efficiently from the ADOL-C computational trace and can be used to automatically deduce the structure of the Jacobian matrix and compute the Jacobian using the reverse mode of automatic differentiation. By employing well-known graph-coloring techniques, one can dramatically decrease the number of reverse passes required. The resulting implementation performs well on the Sequent Symmetry and BBN Butterfly TC2000 shared-memory multiprocessors. Lastly, we look at the problems that must be tackled to make automatic differentiation a commonplace computing tool, and to allow for efficient implementations on high-performance computers. In our view, the key lies in finding better ways to incorporate user and/or compile-time information about the behavior of the program into the automatic differentiation approach. 1. Introduction. In this paper, we are mainly concerned with the evaluation of first-order 
Abstract-found: 1
Intro-found: 1
Reference: <institution> 13 </institution>
Reference-contexts: This view of computation is a natural one, as it captures common subexpressions that can be exploited in the course of the computation of F . This point is demonstrated convincingly in <ref> [13] </ref>. This paper as well as [1] also gives an intuitive explanation of the forward and reverse mode of automatic differentiation within this framework.
Reference: [1] <author> Christian Bischof, Andreas Griewank, and David Juedes. </author> <title> Exploiting parallelism in automatic differentiation. </title> <editor> In Elias Houstis and Yoichi Muraoka, editors, </editor> <booktitle> Proceedings of the 1991 International Conference on Supercomputing, </booktitle> <pages> pages 146-143, </pages> <address> Baltimore, Md., 1991. </address> <publisher> ACM Press. </publisher>
Reference-contexts: This view of computation is a natural one, as it captures common subexpressions that can be exploited in the course of the computation of F . This point is demonstrated convincingly in [13]. This paper as well as <ref> [1] </ref> also gives an intuitive explanation of the forward and reverse mode of automatic differentiation within this framework. <p> If we wish, for example, to compute the gradient rF 3 (x)j x=xo , we initialize the adjoint values in the root corresponding to F 3 to 1, and to zero in all other roots. See <ref> [1] </ref> for a detailed example. With each graph node, we associate a counter which counts how many parents of a given node still have to be computed.
Reference: [2] <author> Christian H. Bischof and James Hu. </author> <title> Creating and optimizing a computational graph for algorithmic decomposition. </title> <institution> ANL/MCS-TM-148, Argonne National Laboratory, Mathematics and Computer Sciences Division, </institution> <year> 1991. </year>
Reference-contexts: An example is shown in Figure 5. Upon encountering a dead root, we check recursively whether children of this node have become dead roots themselves. The implementation of those optimizations is nontrivial, since all these optimizations are performed on the fly; for details the reader is referred to <ref> [2] </ref>. On the other hand, the effect of those optimizations can be quite noticeable. We generated computational graphs for the ADOL-C tapes of the following three application codes: Shallow: This code solves the shallow-water equation to simulate the development of the atmosphere in a rectangular region [21].
Reference: [3] <author> Bruce Christianson. </author> <title> Automatic Hessians by reverse accumulation. </title> <type> Technical Report No. 228, </type> <institution> The Hatfield Polytechnic, Hatfield, U.K., </institution> <year> 1990. </year>
Reference-contexts: To decrease the size of the graph and increase computational granularity, we perform several optimizations on the fly as we construct the computational graph from the ADOL-C tape. In doing so, we incorporate some suggestions made in <ref> [3] </ref>. First, we eliminate assignments nodes. Recall that 3 x Fig. 2. Graph Representation of x+=y t = a+b; y = t/b; a b + b a * / = Fig. 3. <p> A survey of currently available packages is given in [16]. This property makes chain-rule based automatic differentiation a natural candidate for packages that require derivative values. In addition, our work as well as that of Christianson <ref> [3] </ref> and Dixon [7] has shown that there is scope for exploiting parallelism in the automatic computation of derivatives, again in a fashion that is transparent to the user.
Reference: [4] <author> T. F. Coleman and J. J. </author> <title> More. Estimation of sparse Jacobian matrices and graph coloring problems. </title> <journal> SIAM Journal on Numerical Analysis, </journal> <volume> 20 </volume> <pages> 187-209, </pages> <year> 1983. </year>
Reference-contexts: This structure has been exploited in finite-difference approximations of the Jacobian, and graph coloring algorithms have been used to identify maximal sets of component functions that depend on mutually disjoint sets of input values <ref> [5, 4, 9, 18, 22] </ref>. In our implementation we use the sequential coloring algorithm by Coleman, Garbow, and More [5], but we also mention that recent research has shown that this step can be efficiently implemented in parallel as well [15].
Reference: [5] <author> Thomas F. Coleman, Burton S. Garbow, and Jorge J. </author> <title> More. Software for estimating sparse Jacobian matrices. </title> <journal> ACM Transactions on Mathematical Software, </journal> <volume> 10(3) </volume> <pages> 329-345, </pages> <year> 1984. </year>
Reference-contexts: This structure has been exploited in finite-difference approximations of the Jacobian, and graph coloring algorithms have been used to identify maximal sets of component functions that depend on mutually disjoint sets of input values <ref> [5, 4, 9, 18, 22] </ref>. In our implementation we use the sequential coloring algorithm by Coleman, Garbow, and More [5], but we also mention that recent research has shown that this step can be efficiently implemented in parallel as well [15]. <p> In our implementation we use the sequential coloring algorithm by Coleman, Garbow, and More <ref> [5] </ref>, but we also mention that recent research has shown that this step can be efficiently implemented in parallel as well [15].
Reference: [6] <author> John Dennis and Robert Schnabel. </author> <title> Numerical Methods for Unconstrained Optimization and Nonlin&lt;ear Equations. </title> <publisher> Prentice-Hall, </publisher> <address> Englewood Cliffs, New Jersey, </address> <year> 1983. </year>
Reference-contexts: This is one of the important fringe benefits of automatic differentiation. While current software for solving nonlinear least squares problems (see, for example, <ref> [6, 19, 20] </ref>) requires the user to specify the sparsity structure of the Jacobian | a tedious and error-prone process unless the structure of the Jacobian is very regular | the dependency analysis that is performed as a by-product of automatic differentiation automatically takes care of that.
Reference: [7] <author> L. C. W. Dixon. </author> <title> Automatic differentiation and parallel processing in optimization. </title> <type> Technical Report No. 176, </type> <institution> The Hatfield Polytechnic, Hatfield, U.K., </institution> <year> 1987. </year>
Reference-contexts: A survey of currently available packages is given in [16]. This property makes chain-rule based automatic differentiation a natural candidate for packages that require derivative values. In addition, our work as well as that of Christianson [3] and Dixon <ref> [7] </ref> has shown that there is scope for exploiting parallelism in the automatic computation of derivatives, again in a fashion that is transparent to the user. <p> This is what we have done in the implementation we reported on in the preceding sections. * Alternatively, we can exploit parallelism through parallelizing the computations associated with each graph node. This has been done by Dixon <ref> [7] </ref>, where the gradient computation associated with each node in the forward mode is done elementwise in parallel (see also [8]). Even though by no means trivial, the latter alternative is comparatively straightforward, since it is clear which parts of the problem can be done in parallel.
Reference: [8] <author> Herbert Fischer. </author> <title> Automatic differentiation: Parallel computation of function, gradient and Hessian matrix. </title> <journal> Parallel Computing, </journal> <volume> 13 </volume> <pages> 101-110, </pages> <year> 1990. </year>
Reference-contexts: This has been done by Dixon [7], where the gradient computation associated with each node in the forward mode is done elementwise in parallel (see also <ref> [8] </ref>). Even though by no means trivial, the latter alternative is comparatively straightforward, since it is clear which parts of the problem can be done in parallel. On the other hand, extracting parallelism in the evaluation of the computational graph is more difficult.
Reference: [9] <author> D. Goldfarb and P.L. Toint. </author> <title> Optimal estimation of Jacobian and Hessian matrices that arise in finite difference calculations. </title> <journal> Mathematics of Computation, </journal> <volume> 43 </volume> <pages> 69-88, </pages> <year> 1984. </year>
Reference-contexts: This structure has been exploited in finite-difference approximations of the Jacobian, and graph coloring algorithms have been used to identify maximal sets of component functions that depend on mutually disjoint sets of input values <ref> [5, 4, 9, 18, 22] </ref>. In our implementation we use the sequential coloring algorithm by Coleman, Garbow, and More [5], but we also mention that recent research has shown that this step can be efficiently implemented in parallel as well [15].
Reference: [10] <author> Andreas Griewank. </author> <title> On automatic differentiation. </title> <booktitle> In Mathematical Programming: Recent Developments and Applications, </booktitle> <pages> pages 83-108, </pages> <address> Amsterdam, 1989. </address> <publisher> Kluwer Academic Publishers. </publisher>
Reference-contexts: Speedy Execution: Theoretically, the evaluation of the gradient with the reverse mode of automatic differentiation should require no more than five times the effort of evaluating the underlying function itself <ref> [10] </ref>, and observed running times should be close to that factor. Moderate Storage Requirements: The storage requirements for performing the forward as well as the reverse mode should be a modest multiple of the storage required for evaluating the functions.
Reference: [11] <author> Andreas Griewank. </author> <title> Achieving logarithmic growth of temporal and spatial complexity in reverse automatic differentiation. </title> <type> Preprint MCS-P228-0491, </type> <institution> Argonne National Laboratory, Mathematics and Computer Sciences Division, </institution> <year> 1991. </year>
Reference-contexts: Of course, we could now further reduce storage by applying this same technique in a recursive fashion to f and g themselves. Such an approach has been used in hand-coded implementations of the reverse mode. Griewank <ref> [11] </ref> recently analyzed such a scheme, showing that if one accepts an increase in the number of operations by a fixed factor k, the storage required for tracing is limited essentially by the k-th root of the original run-time T .
Reference: [12] <author> Andreas Griewank. </author> <title> Automatic Evaluation of First- and Higher-Derivative Vectors, </title> <booktitle> volume 97, </booktitle> <pages> pages 135-148. </pages> <publisher> Birkhauser Verlag, </publisher> <address> Basel, Switzerland, </address> <year> 1991. </year>
Reference-contexts: 1. Introduction. In this paper, we are mainly concerned with the evaluation of first-order derivatives on parallel machines. These techniques can easily be generalized to higher derivatives <ref> [12] </ref>.
Reference: [13] <author> Andreas Griewank. </author> <title> The chain rule revisited in scientific computing. </title> <type> Preprint MCS-P227-0491, </type> <institution> Argonne National Laboratory, Mathematics and Computer Sciences Division, </institution> <year> 1991. </year>
Reference-contexts: This view of computation is a natural one, as it captures common subexpressions that can be exploited in the course of the computation of F . This point is demonstrated convincingly in <ref> [13] </ref>. This paper as well as [1] also gives an intuitive explanation of the forward and reverse mode of automatic differentiation within this framework.
Reference: [14] <author> Andreas Griewank, David Juedes, and Jay Srinivasan. ADOL-C, </author> <title> a package for the automatic differentiation of algorithms written in C/C++. </title> <type> Preprint MCS-P180-1190, </type> <institution> Argonne National Laboratory, Mathematics and Computer Sciences Division, </institution> <year> 1990. </year>
Reference-contexts: The information needed to generate a computational graph corresponding to the computation of F (x)j x=x o can easily be generated through operator overloading. The computational trace that is generated by packages like ADOL-C <ref> [14] </ref> corresponds to one particular topological ordering of G. In the next section we describe our approach for representing the computational graph generated from an ADOL-C tape, as well as the optimizations to reduce size of the graph and increase the computational granularity of the graph nodes. <p> As will be seen, the key lies in finding better ways to incorporate user and/or compile-time information about the behavior of the program into the automatic differentiation approach. 2. An Efficient Computational Graph Representation. We used the computational trace produced by the ADOL-C package <ref> [14] </ref> to generate a computational graph representing the computation of F (x)j x=x o .
Reference: [15] <author> Mark T. Jones and Paul E. Plassmann. </author> <title> A parallel graph coloring heuristic. </title> <type> Preprint ANL/MCS-P246-0691, </type> <institution> Argonne National Laboratory, Mathematics and Computer Sciences Division, </institution> <year> 1991. </year>
Reference-contexts: In our implementation we use the sequential coloring algorithm by Coleman, Garbow, and More [5], but we also mention that recent research has shown that this step can be efficiently implemented in parallel as well <ref> [15] </ref>. For example, in the cavity problem we require 21 "colors", that is, we can compute the gradients for all 961 component functions in only 21 reverse passes through the computational graph.
Reference: [16] <author> David Juedes. </author> <title> A taxonomy of automatic differentiation tools. </title> <editor> In Andreas Griewank and George Corliss, editors, </editor> <booktitle> Proceedings of the Workshop on Automatic Differentiation of Algorithms: Theory, Implementation, and Application, </booktitle> <address> Philadelphia, </address> <year> 1991. </year> <note> SIAM. to appear. </note>
Reference-contexts: Given just the code for the function, we can compute derivatives of any order (as well as the structure of derivative matrices) exactly (in contrast to finite differences) in a fashion that is transparent to the user. A survey of currently available packages is given in <ref> [16] </ref>. This property makes chain-rule based automatic differentiation a natural candidate for packages that require derivative values. <p> Moderate Storage Requirements: The storage requirements for performing the forward as well as the reverse mode should be a modest multiple of the storage required for evaluating the functions. Unfortunately, as useful as they are in many respects, all packages surveyed in <ref> [16] </ref> are suboptimal with respect to some of these criteria. Fortunately, the method is not at fault.
Reference: [17] <author> David Juedes and Andreas Griewank. </author> <title> Implementing automatic differentiation efficiently. </title> <institution> ANL/MCS-TM-140, Argonne National Laboratory, Mathematics and Computer Sciences Division, </institution> <year> 1990. </year>
Reference-contexts: The evaluation is then the summation of the adjoint values (*). Here g p is the elementary function associated with node p. In this simple form, the global queue will clearly become a bottleneck in the computation. Griewank and Juedes <ref> [17] </ref> avoided this problem by employing a hierarchical queue structure.
Reference: [18] <author> Jorge J. </author> <title> More. On the performance of algorithms for large-scale bound constrained problems. </title> <booktitle> In Large-Scale Numerical Optimization, </booktitle> <pages> pages 31-45, </pages> <address> Philadelphia, </address> <year> 1990. </year> <note> SIAM. </note>
Reference-contexts: This structure has been exploited in finite-difference approximations of the Jacobian, and graph coloring algorithms have been used to identify maximal sets of component functions that depend on mutually disjoint sets of input values <ref> [5, 4, 9, 18, 22] </ref>. In our implementation we use the sequential coloring algorithm by Coleman, Garbow, and More [5], but we also mention that recent research has shown that this step can be efficiently implemented in parallel as well [15].
Reference: [19] <author> Jorge J. More, Burton S. Garbow, and Kenneth E. Hillstrom. </author> <title> Implementation guide for MINPACK-1. </title> <type> Technical Report ANL-80-68, </type> <institution> Argonne National Laboratory, Mathematics and Computer Sciences Division, </institution> <year> 1980. </year>
Reference-contexts: This is one of the important fringe benefits of automatic differentiation. While current software for solving nonlinear least squares problems (see, for example, <ref> [6, 19, 20] </ref>) requires the user to specify the sparsity structure of the Jacobian | a tedious and error-prone process unless the structure of the Jacobian is very regular | the dependency analysis that is performed as a by-product of automatic differentiation automatically takes care of that.
Reference: [20] <author> Jorge J. More, Burton S. Garbow, and Kenneth E. Hillstrom. </author> <title> User guide for MINPACK-1. </title> <type> Technical Report ANL-80-74, </type> <institution> Argonne National Laboratory, Mathematics and Computer Sciences Division, </institution> <year> 1980. </year>
Reference-contexts: This is one of the important fringe benefits of automatic differentiation. While current software for solving nonlinear least squares problems (see, for example, <ref> [6, 19, 20] </ref>) requires the user to specify the sparsity structure of the Jacobian | a tedious and error-prone process unless the structure of the Jacobian is very regular | the dependency analysis that is performed as a by-product of automatic differentiation automatically takes care of that.
Reference: [21] <author> I. M. Navon and U. Muller. </author> <title> FESW | a finite-element Fortran IV program for solving the shallow water equations. </title> <booktitle> Advances in Engineering Software, </booktitle> <volume> 1 </volume> <pages> 77-84, </pages> <year> 1979. </year>
Reference-contexts: On the other hand, the effect of those optimizations can be quite noticeable. We generated computational graphs for the ADOL-C tapes of the following three application codes: Shallow: This code solves the shallow-water equation to simulate the development of the atmosphere in a rectangular region <ref> [21] </ref>. We had 243 independent variables, corresponding to an initial state defined by a 9 fi 9 grid with 3 variables at each node. Starting from this initial state, we integrated over 31 time steps.
Reference: [22] <author> Paul E. Plassmann. </author> <title> Sparse Jacobian estimation and factorization on a multiprocessor. </title> <editor> In T. F. Coleman and Y. Li, editors, </editor> <booktitle> Large-Scale Optimization, </booktitle> <pages> pages 152-179, </pages> <address> Philadelphia, </address> <year> 1990. </year> <note> SIAM. </note>
Reference-contexts: This structure has been exploited in finite-difference approximations of the Jacobian, and graph coloring algorithms have been used to identify maximal sets of component functions that depend on mutually disjoint sets of input values <ref> [5, 4, 9, 18, 22] </ref>. In our implementation we use the sequential coloring algorithm by Coleman, Garbow, and More [5], but we also mention that recent research has shown that this step can be efficiently implemented in parallel as well [15].
Reference: [23] <author> K. H. Winters and K. A. Cliffe. </author> <title> A finite element study of driven laminar flow in a square cavity. </title> <type> Technical Report AERE - R 9444, </type> <institution> AERE Harwell, Theoretical Physics Division, </institution> <year> 1979. </year> <month> 14 </month>
Reference-contexts: There is only one dependent variable, corresponding to the sum of squares between the measured and computationally predicted values. Bratu: Bratu is a partial differential equation model of the exothermic reaction in a section of a cylindrical combustion chamber <ref> [23] </ref>. The code assumes radial symmetry and converts the problem to a two-dimensional grid with mixed boundary conditions. These results were obtained with a 40 fi 80 grid of the chamber section, yielding 3,200 independent variables and 3,200 dependent variables.
References-found: 24

