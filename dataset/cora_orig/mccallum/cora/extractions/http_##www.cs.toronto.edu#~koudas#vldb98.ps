URL: http://www.cs.toronto.edu/~koudas/vldb98.ps
Refering-URL: http://www.cs.toronto.edu/~koudas/publications.htm
Root-URL: http://www.cs.toronto.edu
Email: jag@research.att.com  poosala@research.bell-labs.com  koudas@cs.toronto.edu  kcs@cs.toronto.edu  muthu@research.bell-labs.com  suel@research.bell-labs.com  
Title: Optimal Histograms with Quality Guarantees  
Author: H. V. Jagadish Viswanath Poosala Nick Koudas Ken Sevcik S. Muthukrishnan Torsten Suel 
Address: Toronto  Toronto  
Affiliation: AT&T Labs  Bell Labs  University of  University of  Bell Labs  Bell Labs  
Abstract: Histograms are commonly used to capture attribute value distribution statistics for query optimizers. More recently, histograms have also been considered as a way to produce quick approximate answers to decision support queries. This widespread interest in histograms motivates the problem of computing histograms that are good under a given error metric. In particular, we are interested in an efficient algorithm for choosing the bucket boundaries in a way that either minimizes the estimation error for a given amount of space (number of buckets) or, conversely, minimizes the space needed for a given upper bound on the error. Under the assumption that finding optimal bucket boundaries is computationally inefficient, previous research has focused on heuristics with no provable bounds on the quality of the solutions. In this paper, we present algorithms for computing optimal bucket boundaries in time proportional to the square of the number of distinct data values, for a broad class of optimality metrics. This class includes the V-Optimality constraint, which has been shown to result in the most accurate histograms for several selectivity estimation problems. Through experiments, we show that optimal histograms can achieve substantially lower estimation errors than histograms produced by popular heuristics. We also present new heuristics with provably good space-accuracy tradeoffs that are significantly faster than the optimal algorithm. Finally, we present an enhancement to traditional histograms that allows us to provide quality guarantees on individual selectivity estimates. In our experiments, these quality guarantees were highly effective in isolating outliers in selectivity estimates. Permission to copy without fee all or part of this material is granted provided that the copies are not made or distributed for direct commercial advantage, the VLDB copyright notice and the title of the publication and its date appear, and notice is given that copying is by permission of the Very Large Data Base Endowment. To copy otherwise, or to republish, requires a fee and/or special permission from the Endowment. Proceedings of the 24th VLDB Conference New York, USA, 1998 
Abstract-found: 1
Intro-found: 1
Reference: [CdB72] <author> S. D. Conte and Carl de Boor. </author> <title> Elementary Numerical Analysis: An algorithmic approach. </title> <publisher> Mc-Graw Hill Publishing Company, </publisher> <year> 1972. </year>
Reference-contexts: In numerical analysis, the problem has been studied in the context of approximating a given function in a piecewise fashion by a class of simple functions such as polynomials of some fixed degree <ref> [CdB72] </ref>. However, not much attention has been given to the number of parameters or amount of space required for the representation.
Reference: [dB97] <author> Carl de Boor. </author> <type> Personal communication. </type> <year> 1997. </year>
Reference-contexts: Finding an optimal set of "breakpoints" for a piecewise polynomial (or even linear) approximation is believed to be hard due to the continuous domain and the non-linearity of the problem space <ref> [dB97] </ref>. In statistics, the problem has been posed in connection with non-parametric density estimation as that of constructing a histogram of a given data distribution. But again the effort has focused on minimizing the error without taking space constraints into account [GES85].
Reference: [GES85] <author> T. Gasser, J. Engel, and B. Seifert. </author> <title> Non parametric density estimation. </title> <journal> Ann. Stat., </journal> <month> Septem-ber </month> <year> 1985. </year>
Reference-contexts: In statistics, the problem has been posed in connection with non-parametric density estimation as that of constructing a histogram of a given data distribution. But again the effort has focused on minimizing the error without taking space constraints into account <ref> [GES85] </ref>. In the database community, the problem has been studied in the field of query optimization and more specifically in the context of selectivity estimation for relational operators. Several techniques have been proposed [MCS88], including histograms [Koo80, SC84, Ioa93, IP95], sampling [OR86, LNS90, HS92], and parametric techniques.

Reference: [Ioa93] <author> Y. Ioannidis. </author> <title> Universality of serial histograms. </title> <booktitle> Proc. of the 19th Int. Conf. on Very Large Databases, </booktitle> <pages> pages 256-267, </pages> <month> December </month> <year> 1993. </year>
Reference-contexts: In the database community, the problem has been studied in the field of query optimization and more specifically in the context of selectivity estimation for relational operators. Several techniques have been proposed [MCS88], including histograms <ref> [Koo80, SC84, Ioa93, IP95] </ref>, sampling [OR86, LNS90, HS92], and parametric techniques.
Reference: [IP95] <author> Y. Ioannidis and V. Poosala. </author> <title> Balancing histogram optimality and practicality for query result size estimation. </title> <booktitle> Proc. of ACM SIGMOD, </booktitle> <pages> pages 233-244, </pages> <month> May </month> <year> 1995. </year>
Reference-contexts: This metric was chosen because it plays an important role in selectivity estimation it is identical to the V-Optimality constraint which has been shown to minimize the average selectivity estimation error for equality-join and selection queries <ref> [IP95] </ref>. In Section 8, we briefly discuss how our algorithms can be used to generate optimal histograms for other error metrics such as the metric arising in the context of selectivity estimation for range queries, join queries, and metrics that incorporate knowledge about the query workload. <p> In the database community, the problem has been studied in the field of query optimization and more specifically in the context of selectivity estimation for relational operators. Several techniques have been proposed [MCS88], including histograms <ref> [Koo80, SC84, Ioa93, IP95] </ref>, sampling [OR86, LNS90, HS92], and parametric techniques. <p> Several types of histograms have been proposed and evaluated experimentally in terms of their accuracy, including EquiWidth and EquiHeight [Koo80, SC84], MaxDiff, Compressed, End-Biased (EBV), and V-Optimal histograms <ref> [IP95, PIHS96] </ref>. A formal taxonomy of histograms was proposed in [PIHS96]. The V-Optimal histograms have been shown to minimize the average error for several selectivity estimation problems [IP95], but no efficient algorithms for constructing them have been proposed. <p> A formal taxonomy of histograms was proposed in [PIHS96]. The V-Optimal histograms have been shown to minimize the average error for several selectivity estimation problems <ref> [IP95] </ref>, but no efficient algorithms for constructing them have been proposed. <p> The space-bounded histogram with SSE as error metric is known in the literature as the V - Optimal histogram <ref> [IP95] </ref>. <p> It has been argued in <ref> [IP95] </ref> that the error in the estimate of the query result size is minimized when the variance of the counts of frequencies in each bucket is minimized. This turns out to be the same as our SSE metric. <p> More discussion and experimental results are available elsewhere [JKS98]. Experimental Results. The algorithms compared are DP, EquiWidth, EquiDepth and End-Biased (EBV). For all the graphs in this subsection dealing with accuracy of estimation, the standard deviation of the prediction error is used as previously suggested <ref> [IP95] </ref>. Figures 11 (a) and 11 (b) present the accuracy of the algorithms for D2 with increasing number of buckets (Figure 11 (a)) and, with increasing number of cells (Figure 11 (b)).
Reference: [JKS98] <author> H. V. Jagadish, Nick Koudas, and K. C. Sevcik. </author> <title> Choosing Bucket Boundaries for a Histogram. </title> <institution> University of Toronto Technical Report, TR-375, </institution> <month> May </month> <year> 1998. </year>
Reference-contexts: Finally, Section 9 offers some concluding remarks. Due to space constraints, many of the proofs and more general forms of our results, as well as some of the experimental results, could not be included in this paper; the details can be found in <ref> [JKS98, MPS98] </ref>. 2 Related Work The problem of approximating a given data distribution has received considerable attention in several scientific communities. <p> Of course, the accuracy of any operation performed using the histogram depends on the accuracy of the approximation, which is determined by two factors, the partitioning technique employed for grouping 1 More general assumptions are possible, and are discussed in <ref> [JKS98] </ref>. To simplify the presentation, we will assume that X takes only integer values. values into buckets and the approximation technique employed within each bucket. Several techniques for the approximation within a bucket have been studied in the literature. <p> We begin by specifying the data sets used. Due to space constraints we only give a sample of the most interesting results; more can be found in <ref> [JKS98, MPS98] </ref>. 5.1 Experimental Testbed We describe experiments using the following two real data sets, extracted from census statistics. * D1: A density function on the third attribute of the SGI adult data set. 3 This data set has 732 unique values (N = 732). * D2: The hourly wages of <p> All one has to do is to factor in the weights during the preprocessing phase when the SSE (i; j) (or any other metric) function values are calculated. The rest of the algorithm remains unchanged (details are available elsewhere <ref> [JKS98] </ref>). None of the other techniques accommodate this and, in fact, no one has addressed this issue in the existing literature. It is not surprising that no single heuristic can possibly generate histograms even remotely similar to both Figures 9 (b) and (c), for example. <p> Following [PIHS96] an error metric can be constructed which minimizes the variance of the "area" inside buckets. Details are available elsewhere <ref> [JKS98] </ref>. Experimental Results. We compared the estimation accuracy of DP, EquiWidth, EquiDepth, and MaxDiff. The starting point of each query range is uniformly distributed over the attribute domain, and the ending point is uniformly distributed between the starting point and the end of the attribute domain. <p> This turns out to be the same as our SSE metric. We next present sample experimental results investigating the accuracy of join result size estimation of various types of histograms. More discussion and experimental results are available elsewhere <ref> [JKS98] </ref>. Experimental Results. The algorithms compared are DP, EquiWidth, EquiDepth and End-Biased (EBV). For all the graphs in this subsection dealing with accuracy of estimation, the standard deviation of the prediction error is used as previously suggested [IP95].
Reference: [Koo80] <author> R. P. Kooi. </author> <title> The optimization of queries in relational databases. </title> <type> PhD thesis, </type> <institution> Case Western Re-server University, </institution> <month> Sept </month> <year> 1980. </year>
Reference-contexts: In the database community, the problem has been studied in the field of query optimization and more specifically in the context of selectivity estimation for relational operators. Several techniques have been proposed [MCS88], including histograms <ref> [Koo80, SC84, Ioa93, IP95] </ref>, sampling [OR86, LNS90, HS92], and parametric techniques. <p> Several types of histograms have been proposed and evaluated experimentally in terms of their accuracy, including EquiWidth and EquiHeight <ref> [Koo80, SC84] </ref>, MaxDiff, Compressed, End-Biased (EBV), and V-Optimal histograms [IP95, PIHS96]. A formal taxonomy of histograms was proposed in [PIHS96]. The V-Optimal histograms have been shown to minimize the average error for several selectivity estimation problems [IP95], but no efficient algorithms for constructing them have been proposed. <p> Several techniques for the approximation within a bucket have been studied in the literature. The frequencies in a bucket are most commonly approximated by their average. The value domain is approximated either by a continuous distribution in the bucket range <ref> [Koo80] </ref> or by uniformly placing m values in the bucket range, where m is the total number of distinct values of V grouped into that bucket [PIHS96]. The latter approach has been experimentally shown to be more accurate for several estimation problems [PIHS96]. <p> those B pairs of adjacent values that differ the most in their frequencies [PIHS96]. * EquiDepth a heuristic that partitions the distribution such that the sum of the frequencies in each bucket is approximately equal [SC84]. * EquiWidth a trivial heuristic that partitions the distribution into buckets of equal width <ref> [Koo80] </ref>. In the next two subsections we present the running times and accuracies of the various techniques. 5.2 Running Times We first compare the running times of the three new algorithms based on dynamic programming (NAIVE DP, DP, and CHUNK).
Reference: [LNS90] <author> R. J. Lipton, J. F. Naughton, and D. A. Schnei-der. </author> <title> Practical selectivity estimation through adaptive sampling. </title> <booktitle> Proc. of ACM SIGMOD, </booktitle> <pages> pages 1-11, </pages> <month> May </month> <year> 1990. </year>
Reference-contexts: In the database community, the problem has been studied in the field of query optimization and more specifically in the context of selectivity estimation for relational operators. Several techniques have been proposed [MCS88], including histograms [Koo80, SC84, Ioa93, IP95], sampling <ref> [OR86, LNS90, HS92] </ref>, and parametric techniques. Histograms are the most commonly used form of statistics in practice (e.g., they are used in DB2, Oracle, and Microsoft SQL Server) because they incur almost no run-time overhead and are effective even with a very small amount of storage space.
Reference: [MCS88] <author> M. V. Mannino, P. Chu, and T. Sager. </author> <title> Statistical profile estimation in database systems. </title> <journal> ACM Computing Surveys, </journal> <volume> 20(3) </volume> <pages> 192-221, </pages> <month> Sept </month> <year> 1988. </year>
Reference-contexts: But again the effort has focused on minimizing the error without taking space constraints into account [GES85]. In the database community, the problem has been studied in the field of query optimization and more specifically in the context of selectivity estimation for relational operators. Several techniques have been proposed <ref> [MCS88] </ref>, including histograms [Koo80, SC84, Ioa93, IP95], sampling [OR86, LNS90, HS92], and parametric techniques.
Reference: [MPS98] <author> S. Muthukrishnan, V. Poosala, and T. Suel. </author> <title> Optimal Histograms with Quality Guarantees. </title> <type> Bell Labs Technical Report, </type> <month> May </month> <year> 1998. </year>
Reference-contexts: Finally, Section 9 offers some concluding remarks. Due to space constraints, many of the proofs and more general forms of our results, as well as some of the experimental results, could not be included in this paper; the details can be found in <ref> [JKS98, MPS98] </ref>. 2 Related Work The problem of approximating a given data distribution has received considerable attention in several scientific communities. <p> We begin by specifying the data sets used. Due to space constraints we only give a sample of the most interesting results; more can be found in <ref> [JKS98, MPS98] </ref>. 5.1 Experimental Testbed We describe experiments using the following two real data sets, extracted from census statistics. * D1: A density function on the third attribute of the SGI adult data set. 3 This data set has 732 unique values (N = 732). * D2: The hourly wages of <p> Lemma 4 The error-bounded histogram problem with a maximum SSE in any bucket of at most * can be solved in O (minfB fl log N; N g) time after O (N ) time preprocessing, where B fl is the optimal solution. Proof. The details are in <ref> [MPS98] </ref>. We now describe our approximation algorithm, which, at the high level, is somewhat non-intuitive.
Reference: [OR86] <author> F. Olken and D. Rotem. </author> <title> Simple random sampling from relational databases. </title> <booktitle> Proc. of the 12th Int. Conf. on Very Large Databases, </booktitle> <month> August </month> <year> 1986. </year>
Reference-contexts: Furthermore, we address a known limitation of histograms: Current histogramming techniques do not provide any quality guarantees for individual estimates. This is unlike, say, random-sampling techniques, which usually provide probabilistic error bounds on their estimates <ref> [OR86] </ref>. This problem has not been significant until recently because histograms have mostly been employed within optimizers, where there is no need to report the errors. <p> In the database community, the problem has been studied in the field of query optimization and more specifically in the context of selectivity estimation for relational operators. Several techniques have been proposed [MCS88], including histograms [Koo80, SC84, Ioa93, IP95], sampling <ref> [OR86, LNS90, HS92] </ref>, and parametric techniques. Histograms are the most commonly used form of statistics in practice (e.g., they are used in DB2, Oracle, and Microsoft SQL Server) because they incur almost no run-time overhead and are effective even with a very small amount of storage space.
Reference: [PI97] <author> V. Poosala and Y. Ioannidis. </author> <title> Selectivity estimation without the attribute value independence assumption. </title> <booktitle> Proc. of the 23rd Int. Conf. on Very Large Databases, </booktitle> <month> August </month> <year> 1997. </year>
Reference-contexts: We compared these new algorithms with the following known techniques: * MHIST a greedy heuristic that repeatedly selects and splits the bucket with the highest SSE. This is the one-dimensional variant of the multi dimensional MHIST algorithm proposed in <ref> [PI97] </ref>. * MaxDiff a heuristic that places the bucket boundaries between those B pairs of adjacent values that differ the most in their frequencies [PIHS96]. * EquiDepth a heuristic that partitions the distribution such that the sum of the frequencies in each bucket is approximately equal [SC84]. * EquiWidth a trivial
Reference: [PIHS96] <author> V. Poosala, Y. Ioannidis, P. Haas, and E. Shekita. </author> <title> Improved histograms for selectivity estimation of range predicates. </title> <booktitle> Proc. of ACM SIGMOD, </booktitle> <pages> pages 294-305, </pages> <month> June </month> <year> 1996. </year>
Reference-contexts: Several types of histograms have been proposed and evaluated experimentally in terms of their accuracy, including EquiWidth and EquiHeight [Koo80, SC84], MaxDiff, Compressed, End-Biased (EBV), and V-Optimal histograms <ref> [IP95, PIHS96] </ref>. A formal taxonomy of histograms was proposed in [PIHS96]. The V-Optimal histograms have been shown to minimize the average error for several selectivity estimation problems [IP95], but no efficient algorithms for constructing them have been proposed. <p> Several types of histograms have been proposed and evaluated experimentally in terms of their accuracy, including EquiWidth and EquiHeight [Koo80, SC84], MaxDiff, Compressed, End-Biased (EBV), and V-Optimal histograms [IP95, PIHS96]. A formal taxonomy of histograms was proposed in <ref> [PIHS96] </ref>. The V-Optimal histograms have been shown to minimize the average error for several selectivity estimation problems [IP95], but no efficient algorithms for constructing them have been proposed. <p> For each v 2 V , the frequency f (v) is the number of tuples t 2 R with t:X = v. We assume that the elements of V have been sorted according to some sort parameter (following <ref> [PIHS96] </ref>), most commonly according to the numeric values of the v i , i.e., V = fv i j 1 i N g where i &lt; j iff v i &lt; v j . <p> The value domain is approximated either by a continuous distribution in the bucket range [Koo80] or by uniformly placing m values in the bucket range, where m is the total number of distinct values of V grouped into that bucket <ref> [PIHS96] </ref>. The latter approach has been experimentally shown to be more accurate for several estimation problems [PIHS96]. The main focus of this paper, however, is on the partitioning task. <p> by a continuous distribution in the bucket range [Koo80] or by uniformly placing m values in the bucket range, where m is the total number of distinct values of V grouped into that bucket <ref> [PIHS96] </ref>. The latter approach has been experimentally shown to be more accurate for several estimation problems [PIHS96]. The main focus of this paper, however, is on the partitioning task. We are interested in computing a histogram of F , i.e., a summary vector H of length B &lt;< N that approximates F . <p> This is the one-dimensional variant of the multi dimensional MHIST algorithm proposed in [PI97]. * MaxDiff a heuristic that places the bucket boundaries between those B pairs of adjacent values that differ the most in their frequencies <ref> [PIHS96] </ref>. * EquiDepth a heuristic that partitions the distribution such that the sum of the frequencies in each bucket is approximately equal [SC84]. * EquiWidth a trivial heuristic that partitions the distribution into buckets of equal width [Koo80]. <p> Following <ref> [PIHS96] </ref> an error metric can be constructed which minimizes the variance of the "area" inside buckets. Details are available elsewhere [JKS98]. Experimental Results. We compared the estimation accuracy of DP, EquiWidth, EquiDepth, and MaxDiff.
Reference: [SC84] <author> G. P. Shapiro and C. Connell. </author> <title> Accurate estimation of the number of tuples satisfying a condition. </title> <booktitle> Proc. of ACM SIGMOD, </booktitle> <pages> pages 256-276, </pages> <year> 1984. </year>
Reference-contexts: In the database community, the problem has been studied in the field of query optimization and more specifically in the context of selectivity estimation for relational operators. Several techniques have been proposed [MCS88], including histograms <ref> [Koo80, SC84, Ioa93, IP95] </ref>, sampling [OR86, LNS90, HS92], and parametric techniques. <p> Several types of histograms have been proposed and evaluated experimentally in terms of their accuracy, including EquiWidth and EquiHeight <ref> [Koo80, SC84] </ref>, MaxDiff, Compressed, End-Biased (EBV), and V-Optimal histograms [IP95, PIHS96]. A formal taxonomy of histograms was proposed in [PIHS96]. The V-Optimal histograms have been shown to minimize the average error for several selectivity estimation problems [IP95], but no efficient algorithms for constructing them have been proposed. <p> MHIST algorithm proposed in [PI97]. * MaxDiff a heuristic that places the bucket boundaries between those B pairs of adjacent values that differ the most in their frequencies [PIHS96]. * EquiDepth a heuristic that partitions the distribution such that the sum of the frequencies in each bucket is approximately equal <ref> [SC84] </ref>. * EquiWidth a trivial heuristic that partitions the distribution into buckets of equal width [Koo80].
Reference: [Zip49] <author> G. K. Zipf. </author> <title> Human behaviour and the principle of least effort. </title> <publisher> Addison-Wesley, </publisher> <address> Reading, MA, </address> <year> 1949. </year>
Reference-contexts: In addition, in the comparison of the running times, we also generated data according to a randomly permuted Zipf distribution <ref> [Zip49] </ref>. The frequency vectors of the two real data sets are plotted in Figures 1 and 2.
References-found: 15

