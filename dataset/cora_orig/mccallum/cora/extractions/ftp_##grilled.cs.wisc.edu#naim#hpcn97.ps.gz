URL: ftp://grilled.cs.wisc.edu/naim/hpcn97.ps.gz
Refering-URL: http://www.cs.wisc.edu/~naim/publications.html
Root-URL: 
Email: E-mail: naim@cs.wisc.edu  E-Mail: ajgh@ecs.soton.ac.uk  
Phone: 2  
Title: Visualization of Do-Loop Performance  
Author: Oscar Nam and Anthony J.G. Hey 
Address: 53706, USA.  Southampton S017 1BJ, UK.  
Affiliation: 1 Computer Sciences Department, University of Wisconsin, Madison, Wisconsin  Department of Electronics and Computer Science, University of Southampton,  
Abstract: Performance visualization is the use of graphical display techniques for the analysis of performance data in order to improve understanding of complex performance phenomena. Performance visualization systems for parallel programs have been helpful in the past and they are commonly used in order to improve parallel program performance. However, despite the advances that have been made in visualizing scientific data, techniques for visualizing performance of parallel programs remain ad hoc and performance visualization becomes more difficult as the parallel system becomes more complex. The use of scientific visualization tools (e.g. AVS, Application Visualization System) to display performance data is becoming a very powerful alternative to support performance analysis of parallel programs. One advantage of this approach is that no tool development is required and that every feature of the data visualization tool can be used for further data analysis. In this paper the Do-Loop-Surface (DLS) display, an abstract view of the performance of a particular do-loop in a program implemented using AVS, is presented as an example on how a data visualization tool can be used to define new abstract representations of performance, helping the user to analyze complex data potentially generated by a large number of processors.
Abstract-found: 1
Intro-found: 1
Reference: 1. <institution> Advanced Visual Systems Inc. </institution> <note> AVS User's Guide, Release 4, </note> <month> May </month> <year> 1992. </year>
Reference-contexts: Unfortunately, for these tools to be truly useful in the domain of large scale parallel machines they must include abstract high level views [20]. This situation motivates the use of AVS (Application Visualization System) <ref> [1] </ref>, a scientific data visualization tool, in order to support performance analysis of parallel programs by using visual abstract representations of the performance of the program (Do-Loop-Surface displays [14, 15]). <p> Other systems like Paradyn [13], do not use scientific visualization tools directly, but gives us the possibility to provide data to commercial data visualization packages such as AVS. 3 AVS: A DATA VISUALIZATION TOOL AVS, Application Visualization System <ref> [1] </ref>, is a data visualization environment designed to analyze scientific and engineering data in areas like fluid dynamics, computer-aided engineering, and molecular modeling. AVS users can construct their own visualization applications, by combining software components into executable flow networks.
Reference: 2. <author> Bangalore, P. </author> <title> Private electronic mail communication. Subject: MPI version of LINPACK, </title> <month> March </month> <year> 1995. </year>
Reference-contexts: The figure illustrates a clear pattern of blocks repeated every 10 iterations and processors. This is due to the fact that the topology used by the algorithm is a grid of 10x10 processes. This is exactly the way the algorithm is designed to work <ref> [2] </ref>. If you assume that the processors are arranged as a 10x10 grid (10 rows and 10 columns), and the grid is composed of rows and columns, then the factorization of the block is performed by processor columns in a cyclic manner.
Reference: 3. <author> Ken Brodlie. </author> <title> Scientific Visualization past, present and future. </title> <journal> Nuclear Instruments and Methods in Physics Research A, </journal> <volume> 354 </volume> <pages> 104-111, </pages> <year> 1995. </year>
Reference-contexts: If you do inform, then you will impress" <ref> [3] </ref>. The DLS display and its implementation by using AVS have shown to be useful to represent and analyze the performance of parallel programs such as the LINPACK Benchmark. In terms of scalability, we are providing results up to 100 processors.
Reference: 4. <author> Alva Couch. </author> <title> Categories and Context in Scalable Execution Visualization. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 18 </volume> <pages> 195-204, </pages> <year> 1993. </year>
Reference-contexts: The examples presented by Rover [19], have shown how visualization is "more than a pretty picture". The work done by Couch in <ref> [4] </ref> deals with the problem of scalability. In that paper, global context is described by scalable execution views that do not change in format, size, meaning, or clarity as processors are added to an execution. <p> Categorical views are particularly useful when there is an inverse mapping for an arbitrary view region to the subset of processors whose behaviour was described in the region. The execution visualization tool Seeplex implements this form of category management to provide scalable execution views <ref> [4] </ref>.
Reference: 5. <author> Cwik, T. and van de Geijn, R. and Patterson, J. </author> <title> Application of Massively Parallell Computation to Integral Equation Models of Electromagnetic Scattering (Invited Paper). </title> <journal> J. Opt. Soc. Am. A, </journal> <volume> 11(4), </volume> <month> April </month> <year> 1994. </year>
Reference-contexts: The experiment consists of the LINPACK benchmark and we analyze it using DLS displays. This experiment was performed on an IBM SP2 with 128 processors at Argonne National Laboratory. 5.1 The LINPACK Benchmark The LINPACK benchmark <ref> [5, 8] </ref> is a well known numerical code that is often used to benchmark computers for floating-point performance. This program solves the linear system Ax = b, where A is a dense matrix.
Reference: 6. <author> Dongarra, Jack and van de Geijn, Robert and Walker, David. </author> <title> LAPACK Working Note 43: A look at Scalable Dense Linear Algebra Libraries. </title> <type> Technical report, </type> <institution> University of Tennessee, Oak Ridge National Laboratory, and University of Texas, </institution> <month> May </month> <year> 1992. </year>
Reference-contexts: The matrix A is distributed amongst the processors using the square block scattered decomposition described in <ref> [6] </ref>, which 3 A visualization makes some details of the view manifest, while obscuring others. A view defines what information is presented; a visualization describes how the infor mation is displayed [11]. is a practical and general purpose way of decomposing dense linear algebra com-putations. <p> In problems, such as LU factorization, in which rows and/or columns become inactive as the algorithm progresses, this decomposition provides a good load balance (as it is shown in <ref> [6] </ref>). This algorithm was run on an IBM SP2 at ANL. This machine consists of 128 nodes and two compile servers. Each node is essentially an RS/6000 model 370. This model has a 62.5 MHz clock, a 32Kb date cache, and a 32Kb instruction cache.
Reference: 7. <author> Gropp, William and Lusk, Ewing. </author> <title> Users Guide for the ANL IBM SPx. </title> <type> MCS, </type> <institution> Argonne National Laboratory, </institution> <month> January </month> <year> 1995. </year> <note> Draft. </note>
Reference-contexts: This machine consists of 128 nodes and two compile servers. Each node is essentially an RS/6000 model 370. This model has a 62.5 MHz clock, a 32Kb date cache, and a 32Kb instruction cache. The main features of this system can be found in <ref> [7] </ref>. (subroutine PDLUBR) for 100 processors (with N=6400 and NB=64, where N is the size of matrix and NB the block size used for wrapping). The figure illustrates a clear pattern of blocks repeated every 10 iterations and processors.
Reference: 8. <author> Gropp, William and Lusk, Ewing and Skjellum, Anthony. </author> <title> USING MPI: Portable Parallel Programming with the Message-Passing Interface. Scientific and ENgineering Computation Series, </title> <publisher> MIT Press, </publisher> <year> 1994. </year>
Reference-contexts: PARMACS and PVM). However, for MPI it was easier. The MPI version of Argonne National Laboratory (ANL) comes with a separate environment called MPE (Multi-Processing Environment), which is a software provided by ANL to enhance message-passing programming with MPI, including graphics and profiling capabilities <ref> [8] </ref>. MPE has a set of profiling routines that are used to create logfiles of events that occur during the execution of a parallel program. <p> The experiment consists of the LINPACK benchmark and we analyze it using DLS displays. This experiment was performed on an IBM SP2 with 128 processors at Argonne National Laboratory. 5.1 The LINPACK Benchmark The LINPACK benchmark <ref> [5, 8] </ref> is a well known numerical code that is often used to benchmark computers for floating-point performance. This program solves the linear system Ax = b, where A is a dense matrix.
Reference: 9. <author> M. Heath and J. Etheridge. </author> <title> Visualizing the performance of parallel programs. </title> <journal> IEEE Software, </journal> <pages> pages 29-39, </pages> <month> September </month> <year> 1991. </year>
Reference-contexts: Performance visualization systems for parallel programs have been helpful in the past and they are commonly used in order to improve parallel program performance <ref> [21, 9, 17] </ref>. However, despite the advances that have been made in visualizing scientific data, techniques for visualizing performance of parallel programs remain ad hoc and performance visualization becomes more difficult as the parallel system becomes more complex. <p> However, despite the advances that have been made in visualizing scientific data, techniques for visualizing performance of parallel programs remain ad hoc and performance visualization becomes more difficult as the parallel system becomes more complex. Popular views like space-time/Feynman diagram (ParaGraph <ref> [9, 10] </ref>) or event timelines (Express Etool [16]) often provide some insight about program be-haviour. Unfortunately, for these tools to be truly useful in the domain of large scale parallel machines they must include abstract high level views [20]. <p> To create a visualization program is not a trivial task at all and this effort may be incorporated to a more productive goal, i.e. the performance analysis process. Many of the current performance visualization tools (e.g. PABLO [17], ParaGraph <ref> [9] </ref>) provide several set of views that are useful to understand and improve performance. However, when the number of processors increases up to a large number (e.g. hundreds), most of the visualizations provided by these tools become useless.
Reference: 10. <author> Michael Heath. </author> <title> Recent Developments and Case Studies in Performance Visualization using ParaGraph. </title> <booktitle> In Workshop on Performance Measurement and Visualization of Parallel Systems, </booktitle> <address> Moravany, Czecho-Slovakia, </address> <month> October </month> <year> 1992. </year>
Reference-contexts: However, despite the advances that have been made in visualizing scientific data, techniques for visualizing performance of parallel programs remain ad hoc and performance visualization becomes more difficult as the parallel system becomes more complex. Popular views like space-time/Feynman diagram (ParaGraph <ref> [9, 10] </ref>) or event timelines (Express Etool [16]) often provide some insight about program be-haviour. Unfortunately, for these tools to be truly useful in the domain of large scale parallel machines they must include abstract high level views [20].
Reference: 11. <author> T. LeBlanc, J. Mellor-Crummey, and R. Fowler. </author> <title> Analyzing parallel program executions using multiple views. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 9(2) </volume> <pages> 203-217, </pages> <year> 1990. </year>
Reference-contexts: The matrix A is distributed amongst the processors using the square block scattered decomposition described in [6], which 3 A visualization makes some details of the view manifest, while obscuring others. A view defines what information is presented; a visualization describes how the infor mation is displayed <ref> [11] </ref>. is a practical and general purpose way of decomposing dense linear algebra com-putations. In problems, such as LU factorization, in which rows and/or columns become inactive as the algorithm progresses, this decomposition provides a good load balance (as it is shown in [6]).
Reference: 12. <author> Allen Malony and Gregory Wilson. </author> <title> Future directions in parallel performance environments. </title> <booktitle> In Workshop on Performance Measurement and Visualization of Parallel Systems, </booktitle> <address> Moravany, Czecho-Slovakia, </address> <month> October </month> <year> 1992. </year>
Reference-contexts: 1 INTRODUCTION Performance visualization is the use of graphical display techniques to present an analysis of performance data for an improved understanding of complex performance phenomena <ref> [12] </ref>. Performance visualization systems for parallel programs have been helpful in the past and they are commonly used in order to improve parallel program performance [21, 9, 17].
Reference: 13. <author> Barton Miller, M. Callaghan, J. Cargille, J. Hollingsworth, B. Irvin, K. Karavanic, K. Kunchithapadam, and T. Newhall. </author> <title> The Paradyn Parallel Performance Measurement Tools. </title> <journal> IEEE Computer, </journal> <volume> 28(11) </volume> <pages> 37-46, </pages> <month> November </month> <year> 1995. </year>
Reference-contexts: Categorical views are particularly useful when there is an inverse mapping for an arbitrary view region to the subset of processors whose behaviour was described in the region. The execution visualization tool Seeplex implements this form of category management to provide scalable execution views [4]. Other systems like Paradyn <ref> [13] </ref>, do not use scientific visualization tools directly, but gives us the possibility to provide data to commercial data visualization packages such as AVS. 3 AVS: A DATA VISUALIZATION TOOL AVS, Application Visualization System [1], is a data visualization environment designed to analyze scientific and engineering data in areas like fluid <p> For a very large number of processors, a dynamic notion of performance instrumentation and measurement can be used <ref> [13] </ref>. In this way, the amount of information produced can be dramatically reduced. DLS is not tide to any particular programming model. At present, a DLS displays the information of postmortem events, but they could also be real time events.
Reference: 14. <author> Oscar Nam and Tony Hey. Do-Loop-Surface: </author> <title> An Abstract Performance Data Visualization. </title> <booktitle> Lecture Notes in Computer Science, </booktitle> <volume> 797 </volume> <pages> 367-372, </pages> <month> April </month> <year> 1994. </year>
Reference-contexts: This situation motivates the use of AVS (Application Visualization System) [1], a scientific data visualization tool, in order to support performance analysis of parallel programs by using visual abstract representations of the performance of the program (Do-Loop-Surface displays <ref> [14, 15] </ref>). One advantage of this approach is that no tool development is required and that every feature of the data visualization tool can be used for further data analysis. <p> AVS provides a very large set of modules that analyze data in several ways. Also, custom designed modules can be written for specific data transformations. 4 DO-LOOP-SURFACE A Do-Loop-Surface or DLS <ref> [14, 15] </ref>, is an abstract view of the performance of a particular do-loop in a program. This surface is created by measuring the elapsed time per iteration for every processor and representing this data as a 3D display.
Reference: 15. <author> Oscar Nam, Tony Hey, and Ed Zaluska. Do-Loop-Surface: </author> <title> An Abstract Representation of Parallel Program Performance. </title> <journal> Concurrency-Practice and Experience, </journal> <volume> 8(3) </volume> <pages> 205-234, </pages> <year> 1996. </year>
Reference-contexts: This situation motivates the use of AVS (Application Visualization System) [1], a scientific data visualization tool, in order to support performance analysis of parallel programs by using visual abstract representations of the performance of the program (Do-Loop-Surface displays <ref> [14, 15] </ref>). One advantage of this approach is that no tool development is required and that every feature of the data visualization tool can be used for further data analysis. <p> AVS provides a very large set of modules that analyze data in several ways. Also, custom designed modules can be written for specific data transformations. 4 DO-LOOP-SURFACE A Do-Loop-Surface or DLS <ref> [14, 15] </ref>, is an abstract view of the performance of a particular do-loop in a program. This surface is created by measuring the elapsed time per iteration for every processor and representing this data as a 3D display.
Reference: 16. <institution> Parasoft Corporation. ParaSoft Express. </institution> <note> User's Guide, </note> <year> 1990. </year>
Reference-contexts: However, despite the advances that have been made in visualizing scientific data, techniques for visualizing performance of parallel programs remain ad hoc and performance visualization becomes more difficult as the parallel system becomes more complex. Popular views like space-time/Feynman diagram (ParaGraph [9, 10]) or event timelines (Express Etool <ref> [16] </ref>) often provide some insight about program be-haviour. Unfortunately, for these tools to be truly useful in the domain of large scale parallel machines they must include abstract high level views [20].
Reference: 17. <author> Daniel Reed, Ruth Aydt, Tara Madhyastha, Roger Noe, Keith Shields, and Bradley Schwartz. </author> <title> The PABLO performance analysis environment. </title> <institution> Department of Computer Science, University of Illinois at Urbana-Champaign, </institution> <year> 1992. </year>
Reference-contexts: Performance visualization systems for parallel programs have been helpful in the past and they are commonly used in order to improve parallel program performance <ref> [21, 9, 17] </ref>. However, despite the advances that have been made in visualizing scientific data, techniques for visualizing performance of parallel programs remain ad hoc and performance visualization becomes more difficult as the parallel system becomes more complex. <p> To create a visualization program is not a trivial task at all and this effort may be incorporated to a more productive goal, i.e. the performance analysis process. Many of the current performance visualization tools (e.g. PABLO <ref> [17] </ref>, ParaGraph [9]) provide several set of views that are useful to understand and improve performance. However, when the number of processors increases up to a large number (e.g. hundreds), most of the visualizations provided by these tools become useless.
Reference: 18. <author> Diane Rover and Abdul Waheed. </author> <title> Multiple Domain Analysis Methods. </title> <booktitle> In Proc. of the 3rd ACM/ONR Workshop on Parallel and Distributed Debugging, </booktitle> <pages> pages 53-63, </pages> <month> May </month> <year> 1993. </year>
Reference-contexts: For an overall view of performance, the combination AVS-DLS provides a better alternative. 6 CONCLUSIONS Conventional processing, analysis, and display methods for current performance visualization tools typically do not adequately support the evaluation of program performance on parallel systems having more than a few tens of processors <ref> [18] </ref>. Tools like ParaGraph, do not exploit the analysis capability of the representation. That is the strength of tools such as AVS. The general nature of these tools makes it very convenient to try various image processing functions on the data, after transforming the performance data to images [23].
Reference: 19. <author> Diane Rover and Charles Wright. </author> <title> Visualizing the Performance of SPMD and Data-Parallel Programs. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 18 </volume> <pages> 129-146, </pages> <year> 1993. </year>
Reference-contexts: A case study is also discussed as well as a brief summary about related work in this field. 2 RELATED WORK In this section, several papers related to the use of scientific data visualization for performance analysis are briefly discussed. Rover and Wright in <ref> [19] </ref> present VISTA, a framework to capture, process, and display performance information. In general, the views specific to VISTA are based on applying the techniques of scientific data visualization and multidimensional graphics to program performance visualization. <p> VISTA is applicable to a large class of scalable programs and machines, specifically SPMD and data-parallel programs executing on distributed memory computer systems, and it is presented via a hierarchy of views. The examples presented by Rover <ref> [19] </ref>, have shown how visualization is "more than a pretty picture". The work done by Couch in [4] deals with the problem of scalability.
Reference: 20. <author> Sekhar Sarukkai, Doug Kimelman, and Larry Rudolph. </author> <title> A methodology for visualizing performance of loosely synchronous programs. </title> <booktitle> In Scalable High Performance Computing Conference, SHPCC-92, </booktitle> <pages> pages 424-432. </pages> <publisher> IEEE Computer Society, </publisher> <month> April </month> <year> 1992. </year>
Reference-contexts: Popular views like space-time/Feynman diagram (ParaGraph [9, 10]) or event timelines (Express Etool [16]) often provide some insight about program be-haviour. Unfortunately, for these tools to be truly useful in the domain of large scale parallel machines they must include abstract high level views <ref> [20] </ref>. This situation motivates the use of AVS (Application Visualization System) [1], a scientific data visualization tool, in order to support performance analysis of parallel programs by using visual abstract representations of the performance of the program (Do-Loop-Surface displays [14, 15]).
Reference: 21. <author> Margaret Simmons and Rebecca Koskela. </author> <title> Performance Instrumentation and Visualization. </title> <publisher> ACM Press, Frontier Series, </publisher> <year> 1990. </year>
Reference-contexts: Performance visualization systems for parallel programs have been helpful in the past and they are commonly used in order to improve parallel program performance <ref> [21, 9, 17] </ref>. However, despite the advances that have been made in visualizing scientific data, techniques for visualizing performance of parallel programs remain ad hoc and performance visualization becomes more difficult as the parallel system becomes more complex.
Reference: 22. <author> Abdul Waheed, Bernd Kronmuller, Roomi Sinha, and Diane Rover. </author> <title> A Toolkit for Advanced Perofrmance Analysis. </title> <booktitle> In International Workshop on Modeling, Analysis, and Simulation of Computers and Telecommunication Systems (MASCOTS'94), </booktitle> <address> Durham NC, </address> <month> January </month> <year> 1994. </year>
Reference-contexts: The main contrast between scientific visualization and performance visualization is that the former renders the natural, physical structure of the system under study, and the latter contends with a hierarchy of structures, including higher-dimensional, logical structure, of the system under study <ref> [22] </ref>. The Do-Loop-Surface (DLS) representation of the performance of a particular do-loop in a program, has been presented in this paper as an alternative to provide an abstract and scalable representation of performance that is implemented using a Scientific Data Visualization Tool (AVS).
Reference: 23. <author> Abdul Waheed and Diane Rover. </author> <title> Performance Visualization of Parallel Programs. In Visualization '93, </title> <address> San Jose, California, </address> <month> October </month> <year> 1993. </year> <title> This article was processed using the L A T E X macro package with LLNCS style </title>
Reference-contexts: Tools like ParaGraph, do not exploit the analysis capability of the representation. That is the strength of tools such as AVS. The general nature of these tools makes it very convenient to try various image processing functions on the data, after transforming the performance data to images <ref> [23] </ref>. Fig. 2. DLS display for LINPACK (IBM SP2, 100 processors). Notice the cyclic patterns every 10 iterations/processors. It is also important to say that in this single picture we are displaying the information of 100 iterations for 100 processors. <p> Disadvantages: * Fixed data description format that differs from the record structure of raw event data. * Specialized programming conventions that make it difficult to write cus tomized code for user-specific functions. * Limited options for rapidly developing general graphical displays <ref> [23] </ref>. * It is difficult to relate visual data to source code. * Visualization is useful, but it is important to give a warning about the misuse of visualization. "The purpose of visualization if to inform, not to impress. If you do inform, then you will impress" [3].
References-found: 23

