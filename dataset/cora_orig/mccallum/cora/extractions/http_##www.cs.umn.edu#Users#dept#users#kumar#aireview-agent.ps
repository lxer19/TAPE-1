URL: http://www.cs.umn.edu/Users/dept/users/kumar/aireview-agent.ps
Refering-URL: http://www.cs.umn.edu/Users/dept/users/kumar/
Root-URL: http://www.cs.umn.edu
Title: Document Categorization and Query Generation on the World Wide Web Using WebACE  
Author: Daniel Boley, Maria Gini, Robert Gross, Eui-Hong (Sam) Han, Kyle Hastings, George Karypis, Vipin Kumar, Bamshad Mobasher, and Jerome Moore 
Note: Authors are listed alphabetically.  
Affiliation: Department of Computer Science and Engineering, University of Minnesota  
Abstract: We present WebACE, an agent for exploring and categorizing documents on the World Wide Web based on a user profile. The heart of the agent is an unsupervised categorization of a set of documents, combined with a process for generating new queries that is used to search for new related documents and for filtering the resulting documents to extract the ones most closely related to the starting set. The document categories are not given a priori. We present the overall architecture and describe two novel algorithms which provide significant improvement over Hierarchical Agglomeration Clustering and AutoClass algorithms and form the basis for the query generation and search component of the agent. We report on the results of our experiments comparing these new algorithms with more traditional clustering algorithms and we show that our algorithms are fast and scalable. 
Abstract-found: 1
Intro-found: 1
Reference: [Ack97] <author> M. Ackerman et al. </author> <title> Learning probabilistic user profiles. </title> <journal> AI Magazine, </journal> <volume> 18(2) </volume> <pages> 47-56, </pages> <year> 1997. </year>
Reference-contexts: A few recent examples of such agents include WebWatcher [AFJM95], Syskill & Webert, and others. For example, Syskill & Webert <ref> [Ack97] </ref> utilizes a user profile and learns to rate Web pages of interest using a Bayesian classifier. Balabanovic [BSY95] uses a single well-defined profile to find similar web documents. Candidate web pages are located using best-first search. <p> The number of times a user visits a document and the total amount of time a user spends viewing a document are just a few methods for determining user interest <ref> [Ack97, AFJM95, BSY95] </ref>. Once WebACE has recorded a sufficient number of interesting documents, each document is reduced to a document vector and the document vectors are passed to the the clustering modules.
Reference: [AFJM95] <author> R. Armstrong, D. Freitag, T. Joachims, and T. Mitchell. WebWatcher: </author> <title> A learning apprentice for the world wide web. </title> <booktitle> In Proc. AAAI Spring Symposium on Information Gathering from Heterogeneous, Distributed Environments. </booktitle> <publisher> AAAI Press, </publisher> <year> 1995. </year>
Reference-contexts: Personalized Web Agents Another category of Web agents includes those that obtain or learn user preferences and discover Web information sources that correspond to these preferences, and possibly those of other individuals with similar interests (using collaborative filtering). A few recent examples of such agents include WebWatcher <ref> [AFJM95] </ref>, Syskill & Webert, and others. For example, Syskill & Webert [Ack97] utilizes a user profile and learns to rate Web pages of interest using a Bayesian classifier. Balabanovic [BSY95] uses a single well-defined profile to find similar web documents. Candidate web pages are located using best-first search. <p> The number of times a user visits a document and the total amount of time a user spends viewing a document are just a few methods for determining user interest <ref> [Ack97, AFJM95, BSY95] </ref>. Once WebACE has recorded a sufficient number of interesting documents, each document is reduced to a document vector and the document vectors are passed to the the clustering modules.
Reference: [AMS + 96] <author> R. Agrawal, H. Mannila, R. Srikant, H. Toivonen, and A.I. Verkamo. </author> <title> Fast discovery of association rules. In U.M. </title> <editor> Fayyad, G. Piatetsky-Shapiro, P. Smith, and R. Uthurusamy, editors, </editor> <booktitle> Advances in Knowledge Discovery and Data Mining, </booktitle> <pages> pages 307-328. </pages> <publisher> AAAI/MIT Press, </publisher> <year> 1996. </year>
Reference-contexts: The ARHP method first finds set of items that occur frequently together in transactions 7 using association rule discovery methods <ref> [AMS + 96] </ref>. These frequent item sets are then used to group items into hypergraph edges, and a hypergraph partitioning algorithm [KAKS97] is used to find the item clusters. The similarity among items is captured implicitly by the frequent item sets. <p> In this case, hyperedges represent the frequent item sets found by the association rule discovery algorithm. Association rules capture the relationships among items that are present in a transac 8 tion <ref> [AMS + 96] </ref>. Let T be the set of transactions where each transaction is a subset of the item-set I, and C be a subset of I. <p> Scalability of ARHP The problem of finding association rules that meet a minimum support criterion has been shown to be linearly scalable with respect to the number of transactions <ref> [AMS + 96] </ref>. It has also been shown in [AMS + 96] that association rule algorithms are scalable with respect to the number of items assuming the average size of transactions is fixed. <p> Scalability of ARHP The problem of finding association rules that meet a minimum support criterion has been shown to be linearly scalable with respect to the number of transactions <ref> [AMS + 96] </ref>. It has also been shown in [AMS + 96] that association rule algorithms are scalable with respect to the number of items assuming the average size of transactions is fixed.
Reference: [And54] <author> T.W. Anderson. </author> <title> On estimation of parameters in latent structure analysis. </title> <journal> Psychometrika, </journal> <volume> 19 </volume> <pages> 1-10, </pages> <year> 1954. </year>
Reference-contexts: Then the traditional clustering algorithms can be applied to this transformed data space. Principal Component Analysis (PCA) [Jac91], Multidimensional Scaling (MDS) [JD88] and Kohonen Self-Organizing Feature Maps (SOFM) [Koh88] are some of the commonly used techniques for dimensionality reduction. In addition, Latent Semantic Indexing (LSI) <ref> [And54, DDF + 90, Ber92, BDO95] </ref> is a method frequently used in the information retrieval domain that employs a dimensionality reduction technique similar to PCA.
Reference: [BDO95] <author> M. W. Berry, S. T. Dumais, and Gavin W. O'Brien. </author> <title> Using linear algebra for intelligent information retrieval. </title> <journal> SIAM Review, </journal> <volume> 37 </volume> <pages> 573-595, </pages> <year> 1995. </year>
Reference-contexts: Then the traditional clustering algorithms can be applied to this transformed data space. Principal Component Analysis (PCA) [Jac91], Multidimensional Scaling (MDS) [JD88] and Kohonen Self-Organizing Feature Maps (SOFM) [Koh88] are some of the commonly used techniques for dimensionality reduction. In addition, Latent Semantic Indexing (LSI) <ref> [And54, DDF + 90, Ber92, BDO95] </ref> is a method frequently used in the information retrieval domain that employs a dimensionality reduction technique similar to PCA. <p> Each document is represented by a column of word counts and all the columns are collected into a term frequency matrix M , in a manner similar to Latent Semantic Indexing (LSI) <ref> [BDO95] </ref>. Specifically, the i; j-th entry, M ij , is the number of occurrences of word w i in document d j . <p> The process stops when all the scatter values for the individual clusters fall below the scatter value of centroid vectors collected together. This process differs from that in <ref> [BDO95] </ref> in that (a) we first scale the columns to have unit length to make the results independent of the document length, (b) we translate the collection of document columns so that their mean lie at the origin, (c) we compute only the single leading singular value with its associated left <p> In LSI as described in <ref> [BDO95] </ref>, the SVD is applied once to the original untranslated matrix of word counts, and the first k singular values and associated vectors are retrieved, for some choice of k. <p> The objective of these methods is to extract the most salient features of the document data set to be used as the dimensions in the clustering algorithm. Another approach to dimentionality reduction is that of Latent Semantic Indexing (LSI) <ref> [BDO95] </ref>, where the SVD [Ber92] is applied once to the original untranslated matrix of word counts, and the first k singular values and associated vectors are retrieved, for some choice of k.
Reference: [Ber76] <author> C. Berge. </author> <title> Graphs and Hypergraphs. </title> <publisher> American Elsevier, </publisher> <year> 1976. </year>
Reference-contexts: These frequent item sets are mapped into hyperedges in a hypergraph. A typical document-feature dataset, represented as a transactional database, is depicted in Figure 2. A hypergraph <ref> [Ber76] </ref> H = (V; E) consists of a set of vertices (V ) and a set of hyper-edges (E). A hypergraph is an extension of a graph in the sense that each hyperedge can connect more than two vertices.
Reference: [Ber92] <author> M. W. Berry. </author> <title> Large-scale sparse singular value computations. </title> <journal> International Journal of Supercomputer Applications, </journal> <volume> 6(1) </volume> <pages> 13-49, </pages> <year> 1992. </year>
Reference-contexts: Then the traditional clustering algorithms can be applied to this transformed data space. Principal Component Analysis (PCA) [Jac91], Multidimensional Scaling (MDS) [JD88] and Kohonen Self-Organizing Feature Maps (SOFM) [Koh88] are some of the commonly used techniques for dimensionality reduction. In addition, Latent Semantic Indexing (LSI) <ref> [And54, DDF + 90, Ber92, BDO95] </ref> is a method frequently used in the information retrieval domain that employs a dimensionality reduction technique similar to PCA. <p> The objective of these methods is to extract the most salient features of the document data set to be used as the dimensions in the clustering algorithm. Another approach to dimentionality reduction is that of Latent Semantic Indexing (LSI) [BDO95], where the SVD <ref> [Ber92] </ref> is applied once to the original untranslated matrix of word counts, and the first k singular values and associated vectors are retrieved, for some choice of k. This removes much of the noise present in the data, and also yields a representation of the documents of reduced dimensionality. <p> However, at least k, and often at least 2k, iterations are needed in the LSI case <ref> [Ber92] </ref>. On the other hand, obtaining the single leading eigenvector can be accomplished in relatively few iterations, typically around 15 to 20, and never more than 25 in our application.
Reference: [Bol97] <author> D.L. Boley. </author> <title> Principal Direction Divisive Partitioning. </title> <type> Technical Report TR-97-056, </type> <institution> Department of Computer Science, University of Minnesota, Minneapolis, </institution> <year> 1997. </year>
Reference-contexts: This feature is particularly useful for clustering large document sets which are returned by standard search engines using keyword queries. 11 4.2 Principal Direction Divisive Partitioning The method of Principal Direction Divisive Partitioning (PDDP) <ref> [Bol97] </ref> is based on the computation of the leading principal direction (also known as principal component) for a collection of documents and then cutting the collection of documents along a hyperplane resulting in two separate clusters. The algorithm is then repeated on each separate cluster. <p> In our experiments, only up to 3% of the entries were nonzero, and the PDDP algorithm depends on this sparsity for its performance. Hence the TFIDF scaling substantially raises the cost of the PDDP algorithm while not yielding any improvement of the cluster quality <ref> [Bol97] </ref>. At each stage of the algorithm a cluster is split as follows. <p> In our algorithm, it is obtained by computing the leading left singular vector of ( c M ce) using a "Lanczos"-type interative method <ref> [Bol97] </ref>.
Reference: [BSY95] <author> Marko Balabanovic, Yoav Shoham, and Yeogirl Yun. </author> <title> An adaptive agent for automated Web browsing. Journal of Visual Communication and Image Representation, </title> <type> 6(4), </type> <year> 1995. </year>
Reference-contexts: A few recent examples of such agents include WebWatcher [AFJM95], Syskill & Webert, and others. For example, Syskill & Webert [Ack97] utilizes a user profile and learns to rate Web pages of interest using a Bayesian classifier. Balabanovic <ref> [BSY95] </ref> uses a single well-defined profile to find similar web documents. Candidate web pages are located using best-first search. The system needs to keep a large dictionary and is limited to a single user. WebACE incorporates aspects from all three categories. <p> The number of times a user visits a document and the total amount of time a user spends viewing a document are just a few methods for determining user interest <ref> [Ack97, AFJM95, BSY95] </ref>. Once WebACE has recorded a sufficient number of interesting documents, each document is reduced to a document vector and the document vectors are passed to the the clustering modules.
Reference: [CS96] <author> P. Cheeseman and J. Stutz. </author> <title> Baysian classification (autoclass): Theory and results. In U.M. </title> <editor> Fayyad, G. Piatetsky-Shapiro, P. Smith, and R. Uthurusamy, editors, </editor> <booktitle> Advances in Knowledge Discovery and Data Mining, </booktitle> <pages> pages 153-180. </pages> <publisher> AAAI/MIT Press, </publisher> <year> 1996. </year>
Reference-contexts: Hence the clustering based on these mean values does not always produce very good clusters. Similarly, probabilistic methods such as Bayesian classification used in AutoClass <ref> [CS96] </ref>, do not perform well when the size of the feature space is much larger than the size of the sample set. This type of data distribution seems to be characteristic of document categorization applications on the Web, such as categorizing a bookmark file. <p> For our evaluation, we used two sets of sample documents retrieved from the Web to compare these algorithms to two well-known methods: Bayesian classification as used by AutoClass <ref> [CS96] </ref> and hierarchical agglomeration clustering (HAC) based on the use of a distance function [DH73]. AutoClass is based on the probabilistic mixture modeling [TSM85], and given a data set it finds maximum parameter values for a specific probability distribution functions of the clusters.
Reference: [DDF + 90] <author> S. Deerwester, S.T. Dumais, G.W. Furnas, </author> <title> T.K. Landauer, and R Harshman. Indexing by latent semantic analysis. </title> <journal> J. Amer. Soc. Inform. Sci., </journal> <volume> 41:41, </volume> <year> 1990. </year>
Reference-contexts: Then the traditional clustering algorithms can be applied to this transformed data space. Principal Component Analysis (PCA) [Jac91], Multidimensional Scaling (MDS) [JD88] and Kohonen Self-Organizing Feature Maps (SOFM) [Koh88] are some of the commonly used techniques for dimensionality reduction. In addition, Latent Semantic Indexing (LSI) <ref> [And54, DDF + 90, Ber92, BDO95] </ref> is a method frequently used in the information retrieval domain that employs a dimensionality reduction technique similar to PCA.
Reference: [DEW96] <author> R. B. Doorenbos, O. Etzioni, and D. S. Weld. </author> <title> A scalable comparison shopping agent for the World Wide Web. </title> <type> Technical Report 96-01-03, </type> <institution> University of Washington, Dept. of Computer Science and Engineering, </institution> <year> 1996. </year>
Reference-contexts: For example, agents such as FAQ-Finder [HBML95], Information Manifold [KLSS95], and OCCAM [KW96] rely either on pre-specified and domain specific information about particular types 2 of documents, or on hard coded models of the information sources to retrieve and interpret documents. Other agents, such as ShopBot <ref> [DEW96] </ref> and ILA [PE95], attempt to interact with and learn the structure of unfamiliar information sources. ShopBot retrieves product information from a variety of vendor sites using only general information about the product domain.
Reference: [DH73] <author> Richard O. Duda and Peter E. Hart. </author> <title> Pattern Classification and scene analysis. </title> <publisher> John Wiley & Sons, </publisher> <year> 1973. </year>
Reference-contexts: For our evaluation, we used two sets of sample documents retrieved from the Web to compare these algorithms to two well-known methods: Bayesian classification as used by AutoClass [CS96] and hierarchical agglomeration clustering (HAC) based on the use of a distance function <ref> [DH73] </ref>. AutoClass is based on the probabilistic mixture modeling [TSM85], and given a data set it finds maximum parameter values for a specific probability distribution functions of the clusters. The clustering results provide the full description of each cluster in terms of probability distribution of each attributes. <p> However, in our experiments, this resulted in imbalance in the sizes of the clusters, including some clusters with only 1 document. Another option is to use any appropriate measure of cohesion. For simplicity of computation, the experiments shown in this paper have been conducted using a modified scatter value <ref> [DH73] </ref> defined below. Each document is represented by a column of word counts and all the columns are collected into a term frequency matrix M , in a manner similar to Latent Semantic Indexing (LSI) [BDO95].
Reference: [FBY92] <author> W. B. Frakes and R. Baeza-Yates. </author> <title> Information Retrieval Data Structures and Algorithms. </title> <publisher> Prentice Hall, </publisher> <address> Englewood Cliffs, NJ, </address> <year> 1992. </year>
Reference-contexts: ILA, on the other hand, learns models of various information sources and translates these into its own internal concept hierarchy. Information Filtering/Categorization A number of Web agents use various information retrieval techniques <ref> [FBY92] </ref> and characteristics of open hypertext Web documents to automatically retrieve, filter, and categorize. For example, HyPursuit [WVS + 96] uses semantic information embedded in link structures as well as document content to create cluster hierarchies of hypertext documents, and structure an information space. <p> The lifespan of these request threads is short, i.e. the duration of one HTTP request, Conversely, the browser listener thread persists for the duration of the application. 4 Clustering Methods Existing approaches to document clustering are generally based on either probabilistic methods, or distance and similarity measures (see <ref> [FBY92] </ref>). Distance-based methods such as k-means analysis, hierarchical clustering [JD88] and nearest-neighbor clustering [LF78] use a selected set of words (features) appearing in different documents as the dimensions. Each such feature vector, representing a document, can be viewed as a point in this multi-dimensional space.
Reference: [Fra92] <author> W. B. Frakes. </author> <title> Stemming algorithms. </title> <editor> In W. B. Frakes and R. Baeza-Yates, editors, </editor> <booktitle> Information Retrieval Data Structures and Algorithms, </booktitle> <pages> pages 131-160. </pages> <publisher> Prentice Hall, </publisher> <year> 1992. </year>
Reference-contexts: This ensures a stable data sample since some pages are fairly dynamic in content. The word lists from all documents were filtered with a stop-list and "stemmed" using Porter's suffix-stripping algorithm [Por80] as implemented by <ref> [Fra92] </ref>. We derived 10 experiments (according to the method used for feature selection) and clustered the documents using the four algorithms described earlier. The objective of feature selection was to reduce the dimensionality of the clustering problem while retain the important features of the documents.
Reference: [GV96] <author> G. H. Golub and C. F. Van Loan. </author> <title> Matrix Computations. </title> <publisher> Johns Hopkins Univ. Press, 3rd edition, </publisher> <year> 1996. </year>
Reference-contexts: This is consistent with the results shown in Fig. 6. The PDDP algorithm is based on an efficient method for computing the principal direction. The method for computing the principal direction is itself based on the Lanczos method <ref> [GV96] </ref> in which the major cost arises from matrix-vector products involving the term frequency matrix. The total cost is the cost of each matrix-vector product times the total number of products. The number of products has never exceeded 20 in any of the experiments we have tried.
Reference: [HBML95] <author> K. Hammond, R. Burke, C. Martin, and S. Lytinen. FAQ-Finder: </author> <title> A case-based approach to knowledge navigation. </title> <booktitle> In Working Notes of the AAAI Spring Symposium: Information Gathering from Heterogeneous, Distributed Environments. </booktitle> <publisher> AAAI Press, </publisher> <year> 1995. </year> <month> 30 </month>
Reference-contexts: For example, agents such as FAQ-Finder <ref> [HBML95] </ref>, Information Manifold [KLSS95], and OCCAM [KW96] rely either on pre-specified and domain specific information about particular types 2 of documents, or on hard coded models of the information sources to retrieve and interpret documents.
Reference: [HKKM97a] <author> E.H. Han, G. Karypis, V. Kumar, and B. Mobasher. </author> <title> Clustering based on association rule hypergraphs (position paper). </title> <booktitle> In Workshop on Research Issues on Data Mining and Knowledge Discovery, </booktitle> <pages> pages 9-13, </pages> <address> Tucson, Arizona, </address> <year> 1997. </year>
Reference-contexts: In HAC, the features in each document vector is usually weighted using the TFIDF scaling [SM83], which is an increasing function of the feature's text frequency and its inverse document frequency in the document space. 4.1 Association Rule Hypergraph Partitioning Algorithm The ARHP method <ref> [HKKM97a, HKKM97b, HKKM98] </ref> is used for clustering related items in transaction-based databases, such as supermarket bar code data, using association rules and hypergraph partitioning.
Reference: [HKKM97b] <author> E.H. Han, G. Karypis, V. Kumar, and B. Mobasher. </author> <title> Clustering in a high-dimensional space using hypergraph models. </title> <type> Technical Report TR-97-063, </type> <institution> Department of Computer Science, University of Minnesota, Minneapolis, </institution> <year> 1997. </year>
Reference-contexts: In HAC, the features in each document vector is usually weighted using the TFIDF scaling [SM83], which is an increasing function of the feature's text frequency and its inverse document frequency in the document space. 4.1 Association Rule Hypergraph Partitioning Algorithm The ARHP method <ref> [HKKM97a, HKKM97b, HKKM98] </ref> is used for clustering related items in transaction-based databases, such as supermarket bar code data, using association rules and hypergraph partitioning.
Reference: [HKKM98] <author> E.H. Han, G. Karypis, V. Kumar, and B. Mobasher. </author> <title> Hypergraph based clustering in high-dimensional data sets: A summary of results. </title> <journal> Bulletin of the Technical Committee on Data Engineering, </journal> <volume> 21(1), </volume> <year> 1998. </year>
Reference-contexts: In HAC, the features in each document vector is usually weighted using the TFIDF scaling [SM83], which is an increasing function of the feature's text frequency and its inverse document frequency in the document space. 4.1 Association Rule Hypergraph Partitioning Algorithm The ARHP method <ref> [HKKM97a, HKKM97b, HKKM98] </ref> is used for clustering related items in transaction-based databases, such as supermarket bar code data, using association rules and hypergraph partitioning.
Reference: [Jac91] <author> J. E. Jackson. </author> <title> A User's Guide To Principal Components. </title> <publisher> John Wiley & Sons, </publisher> <year> 1991. </year>
Reference-contexts: Other, more general methods, have also been proposed for dimensionality reduction which attempt to transform the data space into a smaller space in which relationship among data items is preserved. Then the traditional clustering algorithms can be applied to this transformed data space. Principal Component Analysis (PCA) <ref> [Jac91] </ref>, Multidimensional Scaling (MDS) [JD88] and Kohonen Self-Organizing Feature Maps (SOFM) [Koh88] are some of the commonly used techniques for dimensionality reduction.
Reference: [JD88] <author> A.K. Jain and R. C. Dubes. </author> <title> Algorithms for Clustering Data. </title> <publisher> Prentice Hall, </publisher> <year> 1988. </year>
Reference-contexts: Distance-based methods such as k-means analysis, hierarchical clustering <ref> [JD88] </ref> and nearest-neighbor clustering [LF78] use a selected set of words (features) appearing in different documents as the dimensions. Each such feature vector, representing a document, can be viewed as a point in this multi-dimensional space. <p> Then the traditional clustering algorithms can be applied to this transformed data space. Principal Component Analysis (PCA) [Jac91], Multidimensional Scaling (MDS) <ref> [JD88] </ref> and Kohonen Self-Organizing Feature Maps (SOFM) [Koh88] are some of the commonly used techniques for dimensionality reduction. In addition, Latent Semantic Indexing (LSI) [And54, DDF + 90, Ber92, BDO95] is a method frequently used in the information retrieval domain that employs a dimensionality reduction technique similar to PCA.
Reference: [KAKS97] <author> G. Karypis, R. Aggarwal, V. Kumar, and S. Shekhar. </author> <title> Multilevel hypergraph partitioning: Application in VLSI domain. </title> <booktitle> In Proceedings ACM/IEEE Design Automation Conference, </booktitle> <year> 1997. </year>
Reference-contexts: The ARHP method first finds set of items that occur frequently together in transactions 7 using association rule discovery methods [AMS + 96]. These frequent item sets are then used to group items into hypergraph edges, and a hypergraph partitioning algorithm <ref> [KAKS97] </ref> is used to find the item clusters. The similarity among items is captured implicitly by the frequent item sets. In the document retrieval domain, it is also possible to view a set of documents in a transactional form. <p> Note that by minimizing the hyperedge-cut we essentially minimize the relations that are violated by splitting the items into two groups. Now each of these two parts can be further bisected recursively, until each partition is highly connected. For this task we use HMETIS <ref> [KAKS97] </ref>, a multi-level hypergraph partitioning algorithm which can partition very large hypergraphs (of size &gt; 100K nodes) in minutes on personal computers. 10 Once, the overall hypergraph has been partitioned into k parts, we eliminate bad clusters using the following cluster fitness criterion.
Reference: [KLSS95] <author> T. Kirk, A. Y. Levy, Y. Sagiv, and D. Srivastava. </author> <title> The information manifold. </title> <booktitle> In Working Notes of the AAAI Spring Symposium: Information Gathering from Heterogeneous, Distributed Environments. </booktitle> <publisher> AAAI Press, </publisher> <year> 1995. </year>
Reference-contexts: For example, agents such as FAQ-Finder [HBML95], Information Manifold <ref> [KLSS95] </ref>, and OCCAM [KW96] rely either on pre-specified and domain specific information about particular types 2 of documents, or on hard coded models of the information sources to retrieve and interpret documents.
Reference: [Koh88] <author> T. Kohonen. </author> <title> Self-Organization and Associated Memory. </title> <publisher> Springer-Verlag, </publisher> <year> 1988. </year>
Reference-contexts: Then the traditional clustering algorithms can be applied to this transformed data space. Principal Component Analysis (PCA) [Jac91], Multidimensional Scaling (MDS) [JD88] and Kohonen Self-Organizing Feature Maps (SOFM) <ref> [Koh88] </ref> are some of the commonly used techniques for dimensionality reduction. In addition, Latent Semantic Indexing (LSI) [And54, DDF + 90, Ber92, BDO95] is a method frequently used in the information retrieval domain that employs a dimensionality reduction technique similar to PCA.
Reference: [KW96] <author> C. Kwok and D. Weld. </author> <title> Planning to gather information. </title> <booktitle> In Proc. 14th National Conference on AI, </booktitle> <year> 1996. </year>
Reference-contexts: For example, agents such as FAQ-Finder [HBML95], Information Manifold [KLSS95], and OCCAM <ref> [KW96] </ref> rely either on pre-specified and domain specific information about particular types 2 of documents, or on hard coded models of the information sources to retrieve and interpret documents. Other agents, such as ShopBot [DEW96] and ILA [PE95], attempt to interact with and learn the structure of unfamiliar information sources.
Reference: [LF78] <author> S.Y. Lu and K.S. Fu. </author> <title> A sentence-to-sentence clustering procedure for pattern analysis. </title> <journal> IEEE Transactions on Systems, Man and Cybernetics, </journal> <volume> 8 </volume> <pages> 381-389, </pages> <year> 1978. </year>
Reference-contexts: Distance-based methods such as k-means analysis, hierarchical clustering [JD88] and nearest-neighbor clustering <ref> [LF78] </ref> use a selected set of words (features) appearing in different documents as the dimensions. Each such feature vector, representing a document, can be viewed as a point in this multi-dimensional space. There are a number of problems with clustering in a multi-dimensional space using traditional distance- or probability-based methods.
Reference: [LS97] <author> H. Vernon Leighton and J. Srivastava. </author> <title> Precision among WWW search services (search engines): Alta Vista, </title> <address> Excite, Hotbot, Infoseek, Lycos. http://www.winona.msus.edu/is-f/library-f/webind2/webind2.htm, 1997. </address>
Reference-contexts: A recent study provides a comprehensive and statistically thorough comparative evaluation of the most popular search tools <ref> [LS97] </ref>. In recent years these factors have prompted researchers to develop more intelligent tools for information retrieval, such as intelligent Web agents.
Reference: [MHB + 97] <author> J. Moore, E. Han, D. Boley, M. Gini, R. Gross, K. Hastings, G. Karypis, V. Kumar, and B. Mobasher. </author> <title> Web page categorization and feature selection using association rule and principal component clustering. </title> <booktitle> In 7th Workshop on Information Technologies and Systems, </booktitle> <month> Dec </month> <year> 1997. </year>
Reference-contexts: It should be noted that the conclusions drawn in the above discussion have been confirmed by another experiment using a totally independent set of documents <ref> [MHB + 97] </ref>. Dimensionality Reduction using LSI/SVD As noted above, feature selection methods, such as those used in our experiments, are often used in order to reduce the dimentionality of clustering problem in information retrieval domains. <p> For the categorization component, our experiments have shown that the ARHP algorithm and the PDDP algorithm are capable of extracting higher quality clusters while operating much faster compared to more classical algorithms such as HAC or AutoClass. This is consistent with our previous results <ref> [MHB + 97] </ref>. The ARHP algorithm is also capable of filtering out documents by setting a support threshold. Our experiments show that the PDDP and ARHP algorithms are fast and scale with the number of words in the documents.
Reference: [MS96] <author> Y. S. Maarek and I.Z. Ben Shaul. </author> <title> Automatically organizing bookmarks per content. </title> <booktitle> In Proc. of 5th International World Wide Web Conference, </booktitle> <year> 1996. </year>
Reference-contexts: For example, HyPursuit [WVS + 96] uses semantic information embedded in link structures as well as document content to create cluster hierarchies of hypertext documents, and structure an information space. BO (Bookmark Organizer) <ref> [MS96] </ref> combines hierarchical clustering techniques and user interaction to organize a collection of Web documents based on conceptual information.
Reference: [PE95] <author> M. Perkowitz and O. Etzioni. </author> <title> Category translation: learning to understand information on the internet. </title> <booktitle> In Proc. 15th International Joint Conference on AI, </booktitle> <pages> pages 930-936, </pages> <address> Montral, Canada, </address> <year> 1995. </year>
Reference-contexts: For example, agents such as FAQ-Finder [HBML95], Information Manifold [KLSS95], and OCCAM [KW96] rely either on pre-specified and domain specific information about particular types 2 of documents, or on hard coded models of the information sources to retrieve and interpret documents. Other agents, such as ShopBot [DEW96] and ILA <ref> [PE95] </ref>, attempt to interact with and learn the structure of unfamiliar information sources. ShopBot retrieves product information from a variety of vendor sites using only general information about the product domain.
Reference: [Por80] <author> M. F. Porter. </author> <title> An algorithm for suffix stripping. </title> <booktitle> Program, </booktitle> <volume> 14(3) </volume> <pages> 130-137, </pages> <year> 1980. </year>
Reference-contexts: The labeling facilitates an entropy calculation and subsequent references to any page were directed to the archive. This ensures a stable data sample since some pages are fairly dynamic in content. The word lists from all documents were filtered with a stop-list and "stemmed" using Porter's suffix-stripping algorithm <ref> [Por80] </ref> as implemented by [Fra92]. We derived 10 experiments (according to the method used for feature selection) and clustered the documents using the four algorithms described earlier. The objective of feature selection was to reduce the dimensionality of the clustering problem while retain the important features of the documents.
Reference: [SM83] <author> Gerard Salton and Michael J. McGill. </author> <title> Introduction to Modern Information Retrieval. </title> <publisher> McGraw-Hill, </publisher> <year> 1983. </year>
Reference-contexts: Some words are more frequent in a document than other words. Simple frequency of the occurrence of words is not adequate, as some documents are larger than others. Furthermore, some words may occur frequently across documents. Techniques 5 such as TFIDF <ref> [SM83] </ref> have been proposed precisely to deal with some of these problems. Secondly, the number of all the words in all the documents can be very large. Distance-based schemes generally require the calculation of the mean of document clusters. <p> The HAC method starts with trivial clusters, each containing one document and iteratively combines smaller clusters that are sufficiently "close" based on a distance metric. In HAC, the features in each document vector is usually weighted using the TFIDF scaling <ref> [SM83] </ref>, which is an increasing function of the feature's text frequency and its inverse document frequency in the document space. 4.1 Association Rule Hypergraph Partitioning Algorithm The ARHP method [HKKM97a, HKKM97b, HKKM98] is used for clustering related items in transaction-based databases, such as supermarket bar code data, using association rules and <p> To make the results independent of document length, each column is scaled to have unit length in the usual Euclidean norm: c M ij = M ij = q P ij , so that P c M 2 alternative scaling is the TFIDF scaling <ref> [SM83] </ref>, but this scaling fills in all the zero entries in M . In our experiments, only up to 3% of the entries were nonzero, and the PDDP algorithm depends on this sparsity for its performance.
Reference: [TSM85] <author> D.M. Titterington, A.F.M. Smith, and U.E. Makov. </author> <title> Statistical Analysis of Finite Mixture Distributions. </title> <publisher> John Wiley & Sons, </publisher> <year> 1985. </year>
Reference-contexts: AutoClass is based on the probabilistic mixture modeling <ref> [TSM85] </ref>, and given a data set it finds maximum parameter values for a specific probability distribution functions of the clusters. The clustering results provide the full description of each cluster in terms of probability distribution of each attributes.
Reference: [WP97] <author> Marilyn R. Wulfekuhler and William F. Punch. </author> <title> Finding salient features for personal Web page categories. </title> <booktitle> In Proc. of 6th International World Wide Web Conference, </booktitle> <month> April </month> <year> 1997. </year>
Reference-contexts: BO (Bookmark Organizer) [MS96] combines hierarchical clustering techniques and user interaction to organize a collection of Web documents based on conceptual information. Pattern recognition methods and word clustering using the Hartigan's K-means partitional clustering algorithm are used in <ref> [WP97] </ref> to discover salient HTML document features (words) that can be used in finding similar HTML documents on the Web.
Reference: [WVS + 96] <author> Ron Weiss, Bienvenido Velez, Mark A. Sheldon, Chanathip Nemprempre, Peter Szilagyi, Andrzej Duda, and David K. Gifford. Hypursuit: </author> <title> A hierarchical network search engine that exploits content-link hypertext clustering. </title> <booktitle> In Seventh ACM Conference on Hypertext, </booktitle> <month> March </month> <year> 1996. </year> <month> 32 </month>
Reference-contexts: ILA, on the other hand, learns models of various information sources and translates these into its own internal concept hierarchy. Information Filtering/Categorization A number of Web agents use various information retrieval techniques [FBY92] and characteristics of open hypertext Web documents to automatically retrieve, filter, and categorize. For example, HyPursuit <ref> [WVS + 96] </ref> uses semantic information embedded in link structures as well as document content to create cluster hierarchies of hypertext documents, and structure an information space. BO (Bookmark Organizer) [MS96] combines hierarchical clustering techniques and user interaction to organize a collection of Web documents based on conceptual information.
References-found: 36

