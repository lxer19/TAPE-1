URL: ftp://ftp.cs.umd.edu/pub/papers/papers/2847/2847.ps.Z
Refering-URL: http://www.cs.umd.edu/TRs/TR.html
Root-URL: 
Title: Gaussian Elimination, Perturbation Theory and Markov Chains  
Author: G. W. Stewart 
Affiliation: University of Maryland College Park Institute for Advanced Computer Studies TR-92-23 Department of Computer Science  
Pubnum: TR-2847  
Abstract: The purpose of this paper is to describe the special problems that emerge when Gaussian elimination is used to determinin the steady state vector of a Markov chain. fl This report is available by anonymous ftp from thales.cs.umd.edu in the directory pub/reports. It will appear in the Proceedings of the IMA Workshop on Linear Algebra, Markov Chains, and Queuing Models. y Department of Computer Science and Institute for Advanced Computer Studies, University of Maryland, College Park, MD 20742. This work was supported in part by the Air Force Office of Scientific Research under Contract AFOSR-87-0188 and was done while the author was a visiting faculty member at the Institute for Mathematics and Its Applications, The University of Minnesota, Minneapolis, MN 55455. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> P.-J. Courtois. </author> <title> Decomposability. </title> <publisher> Academic Press, </publisher> <address> New York, </address> <year> 1977. </year> <title> 4 This assumes that the exterior product form of Gaussian elimination is used. If inner-product forms of the kind associated with the names of Crout and Doolittle are used, the algorithm is a little more complicated, though the amount of extra work stays the same. 5 Some of the examples appearing in the literature purporting to show that Gaussian elimination fails are of the same nature: the damage is done in the initial rounding of problem. 12 Gaussian Elimination and Markov Chains </title>
Reference-contexts: Such chains were introduced by Simon and Ando [11], and have been studied extensively since (e.g., see <ref> [1, 13] </ref>). Since the E ij are small, each of the matrices P ii has an eigenvalue near one. Consequently the entire matrix, in addition to an eigenvalue of one, has k 1 eigenvalues near one, where k is the number of blocks.
Reference: [2] <author> I. S. Duff, A. M. Erisman, and J. K. Reid. </author> <title> Direct Methods for Sparse Matrices. </title> <publisher> Clarendon Press, Oxford, </publisher> <year> 1986. </year>
Reference-contexts: The second point is that symmetric pivoting, in which two rows and the same two columns are interchanged, does preserve the structure of Q and can be used with complete freedom. This fact is important in applications involving sparse matrices, in which pivoting is necessary to avoid fill-in <ref> [2] </ref>. Turning now to the perturbation theory of Markov chains, the basic theory goes back almost a quarter of a century and has been presented in a variety of ways [3, 7, 9, 10]. Here we give it in a form that will be useful in the sequel.
Reference: [3] <author> R. E. Funderlic. </author> <title> Sensitivity of the stationary distribution vector for an ergodic Markov chain. </title> <journal> Linear Algebra and Its Applications, </journal> <volume> 76 </volume> <pages> 1-17, </pages> <year> 1986. </year>
Reference-contexts: This fact is important in applications involving sparse matrices, in which pivoting is necessary to avoid fill-in [2]. Turning now to the perturbation theory of Markov chains, the basic theory goes back almost a quarter of a century and has been presented in a variety of ways <ref> [3, 7, 9, 10] </ref>. Here we give it in a form that will be useful in the sequel. For proofs see [14].
Reference: [4] <author> R. E. Funderlic and J. B. Mankin. </author> <title> Solution of homogeneous systems of linear equations arising from compartmental models. </title> <journal> SIAM Journal on Scientific and Statistical Computing, </journal> <volume> 2 </volume> <pages> 375-109, </pages> <year> 1981. </year>
Reference-contexts: Since the growth factor fl in (2.2) is one, the algorithm is stable. This stability was first pointed out by Funderlic and Mankin <ref> [4] </ref>. Two further points. not only is there no need to pivot in the algorithm, but the usual form of partial pivoting is in some sense harmful.
Reference: [5] <author> G. H. Golub and C. F. Van Loan. </author> <title> Matrix Computations. </title> <publisher> Johns Hopkins University Press, </publisher> <address> Baltimore, Maryland, 2nd edition, </address> <year> 1989. </year>
Reference-contexts: Throughout out this paper we will assume that the reader is familiar with Gaussian elimination and its relation to the LU decomposition. Treatments of this material can be found in most introductory books on numerical linear algebra (e.g., <ref> [5, 12, 17] </ref>). The symbol k k denote a family of consistent matrix norms; i.e., one for which kABk kAkkBk, whenever the product AB is defined. 2.
Reference: [6] <author> W. K. Grassmann, M. I. Taksar, and D. P. Heyman. </author> <title> Regenerative analysis and steady state distributions. </title> <journal> Operations Research, </journal> <volume> 33 </volume> <pages> 1107-1116, </pages> <year> 1985. </year>
Reference-contexts: Paradoxically, although the algorithm fails, the problem is well determined in the sense that all the information necessary to compute a solution is available at the outset of the computations. A closer study of this paradox motivates a variant of Gaussian elimination proposed by Grassmann, Taksar, and Heyman <ref> [6] </ref>. Throughout out this paper we will assume that the reader is familiar with Gaussian elimination and its relation to the LU decomposition. Treatments of this material can be found in most introductory books on numerical linear algebra (e.g., [5, 12, 17]). <p> 22 are known to be zero, instead of using Gaussian elimination to compute its diagonal elements, we use the alternative formula u jj = i=k+1 u ij ; j = k + 1; : : : ; n: (5:2) This, in essence, is the algorithm of Grassmann, Taksar, and Heyman <ref> [6] </ref> mentioned in the introduction. There are three points to be made about it. In the first place, it is easy to implement and not very expensive.
Reference: [7] <author> M. Haviv and L. van der Heyden. </author> <title> Perturbation bounds for the stationary probabilities of a finite Markov chain. </title> <booktitle> Advances in Applied Probability, </booktitle> <volume> 16 </volume> <pages> 804-818, </pages> <year> 1984. </year>
Reference-contexts: This fact is important in applications involving sparse matrices, in which pivoting is necessary to avoid fill-in [2]. Turning now to the perturbation theory of Markov chains, the basic theory goes back almost a quarter of a century and has been presented in a variety of ways <ref> [3, 7, 9, 10] </ref>. Here we give it in a form that will be useful in the sequel. For proofs see [14].
Reference: [8] <author> H. Hotelling. </author> <title> Some new methods in matrix calculations. </title> <journal> Annals of Mathematical Statistics, </journal> <volume> 14 </volume> <pages> 1-34, </pages> <year> 1943. </year>
Reference-contexts: In an early analysis, the statistician Hotelling <ref> [8] </ref> arrived at the conclusion that the errors in Gaussian elimination could grow exponentially with the size of the matrix, which would have precluded its use for matrices of even modest size.
Reference: [9] <author> C. D. Meyer. </author> <title> The condition of a Markov chain and perturbations bounds for the limiting probabilities. </title> <journal> SIAM Journal on Algebraic and Discrete Methods, </journal> <volume> 1 </volume> <pages> 273-283, </pages> <year> 1980. </year>
Reference-contexts: This fact is important in applications involving sparse matrices, in which pivoting is necessary to avoid fill-in [2]. Turning now to the perturbation theory of Markov chains, the basic theory goes back almost a quarter of a century and has been presented in a variety of ways <ref> [3, 7, 9, 10] </ref>. Here we give it in a form that will be useful in the sequel. For proofs see [14].
Reference: [10] <author> P. J. Schweitzer. </author> <title> Perturbation theory and finite Markov chains. </title> <journal> Journal of Applied Probability, </journal> <volume> 5 </volume> <pages> 401-413, </pages> <year> 1968. </year>
Reference-contexts: This fact is important in applications involving sparse matrices, in which pivoting is necessary to avoid fill-in [2]. Turning now to the perturbation theory of Markov chains, the basic theory goes back almost a quarter of a century and has been presented in a variety of ways <ref> [3, 7, 9, 10] </ref>. Here we give it in a form that will be useful in the sequel. For proofs see [14].
Reference: [11] <author> H. A. Simon and A. Ando. </author> <title> Aggregation of variables in dynamic systems. </title> <journal> Econometrica, </journal> <volume> 29 </volume> <pages> 111-138, </pages> <year> 1961. </year>
Reference-contexts: For the case of three blocks, such a chain has the form P = B P 11 E 12 E 13 E 31 E 32 P 33 C where the matrices E ij are small. Such chains were introduced by Simon and Ando <ref> [11] </ref>, and have been studied extensively since (e.g., see [1, 13]). Since the E ij are small, each of the matrices P ii has an eigenvalue near one.
Reference: [12] <author> G. W. Stewart. </author> <title> Introduction to Matrix Computations. </title> <publisher> Academic Press, </publisher> <address> New York, </address> <year> 1973. </year>
Reference-contexts: Throughout out this paper we will assume that the reader is familiar with Gaussian elimination and its relation to the LU decomposition. Treatments of this material can be found in most introductory books on numerical linear algebra (e.g., <ref> [5, 12, 17] </ref>). The symbol k k denote a family of consistent matrix norms; i.e., one for which kABk kAkkBk, whenever the product AB is defined. 2.
Reference: [13] <author> G. W. Stewart. </author> <title> On the structure of nearly uncoupled Markov chains. </title> <editor> In G. Iazeolla, P. J. Courtois, and A. Hordijk, editors, </editor> <booktitle> Mathematical Computer Performance and Reliability, </booktitle> <pages> pages 287-302, </pages> <publisher> Elsevier, </publisher> <address> New York, </address> <year> 1984. </year> <title> Gaussian Elimination and Markov Chains 13 </title>
Reference-contexts: Such chains were introduced by Simon and Ando [11], and have been studied extensively since (e.g., see <ref> [1, 13] </ref>). Since the E ij are small, each of the matrices P ii has an eigenvalue near one. Consequently the entire matrix, in addition to an eigenvalue of one, has k 1 eigenvalues near one, where k is the number of blocks.
Reference: [14] <author> G. W. Stewart. </author> <title> Perturbation theory for nearly uncoupled Markov chains. </title> <editor> In W. J. Stewart, editor, </editor> <booktitle> Numerical Methods for Markov Chains, </booktitle> <pages> pages 105-120, </pages> <publisher> North Holland, </publisher> <address> Amsterdam, </address> <year> 1990. </year>
Reference-contexts: Here we give it in a form that will be useful in the sequel. For proofs see <ref> [14] </ref>. <p> Since the E ij are small compared to the P ii , small relative perturbations in P can be large compared to the E ij and harm the solution. All this agrees with the perturbation theory for ncd Markov chains. It can be shown <ref> [14] </ref> that under suitable regularity conditions the matrix P has a spectral decomposition of the form P = 1y T + X s B s Y T f ; (4:2) where 0 B y T s f C 1 The matrix B s is a perturbation of the identity, while the
Reference: [15] <author> G. W. Stewart and G. Zhang. </author> <title> On a direct method for the solution of nearly uncoupled Markov chains. </title> <journal> Numerische Mathematik, </journal> <volume> 59 </volume> <pages> 1-11, </pages> <year> 1991. </year>
Reference-contexts: Unfortunately, this line of reasoning, when formalized, is essentially the analysis of Hotelling mentioned in the introduction, and it leads to the same pessimistic conclusions. With G. Zhang, I have given a rounding-error analysis of a closely related algorithm <ref> [15] </ref>; however, the original algorithm remains unanalyzed. Nontheless, I believe that the GTH algorithm is stable and should be used routinely in the direct solution of Markov chains.
Reference: [16] <author> J. von Neumann and H. H. Goldstine. </author> <title> Numerical inverting of matrices of high order. </title> <journal> Bulletin of the American Mathematical Society, </journal> <volume> 53 </volume> <pages> 1021-1099, </pages> <year> 1947. </year>
Reference-contexts: In an early analysis, the statistician Hotelling [8] arrived at the conclusion that the errors in Gaussian elimination could grow exponentially with the size of the matrix, which would have precluded its use for matrices of even modest size. A subsequent analysis by von Neumann and Goldstine <ref> [16] </ref> showed that the algorithm would solve positive definite systems accurately, provided they were what we now call well conditioned. Finally, in 1961 Wilkinson [18] provided a comprehensive error analysis of Gaussian elimination for general linear systems.
Reference: [17] <author> D. S. Watkins. </author> <title> Fundamentals of Matrix Computations. </title> <publisher> John Wiley & Sons, </publisher> <address> New York, </address> <year> 1991. </year>
Reference-contexts: Throughout out this paper we will assume that the reader is familiar with Gaussian elimination and its relation to the LU decomposition. Treatments of this material can be found in most introductory books on numerical linear algebra (e.g., <ref> [5, 12, 17] </ref>). The symbol k k denote a family of consistent matrix norms; i.e., one for which kABk kAkkBk, whenever the product AB is defined. 2.
Reference: [18] <author> J. H. Wilkinson. </author> <title> Error analysis of direct methods of matrix inversion. </title> <journal> Journal of the ACM, </journal> <volume> 8 </volume> <pages> 281-330, </pages> <year> 1961. </year>
Reference-contexts: A subsequent analysis by von Neumann and Goldstine [16] showed that the algorithm would solve positive definite systems accurately, provided they were what we now call well conditioned. Finally, in 1961 Wilkinson <ref> [18] </ref> provided a comprehensive error analysis of Gaussian elimination for general linear systems. A key component of Wilkinson's treatment is the projection of the errors back onto the original problem, a procedure known as backward rounding-error analysis.
References-found: 18

