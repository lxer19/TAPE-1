URL: file://ftp.cs.utexas.edu/pub/neural-nets/papers/miikkulainen.nlp-handbook.ps.Z
Refering-URL: http://www.cs.utexas.edu/users/nn/pages/publications/abstracts.html
Root-URL: http://www.cs.utexas.edu
Email: risto@cs.utexas.edu  
Title: Text and Discourse Understanding: The DISCERN System  
Author: Risto Miikkulainen 
Address: Austin, TX 78712  
Affiliation: Department of Computer Sciences The University of Texas at Austin  
Abstract-found: 0
Intro-found: 1
Reference: [1] <author> S. Alvarado, M. G. Dyer, and M. Flowers. </author> <title> Argument representation for editorial text. </title> <journal> Knowledge-Based Systems, </journal> <volume> 3 </volume> <pages> 87-107, </pages> <year> 1990. </year>
Reference-contexts: For example, in story understanding, symbolic systems have been developed that analyze realistic stories in depth, based on higher-level knowledge structures such as goals, plans, themes, affects, beliefs, argument structures, plots, and morals (e.g. <ref> [1; 7; 23; 26] </ref>).
Reference: [2] <author> A. D. Baddeley. </author> <title> The Psychology of Memory. </title> <publisher> Basic Books, </publisher> <address> New York, </address> <year> 1976. </year>
Reference-contexts: If the memory capacity is exceeded, older traces will be selectively replaced by newer ones. Traces that are unique, that is, located in a sparse area of the map, are not affected, no matter how old they are. Similar 9 effects are common in human long-term memory <ref> [2; 22] </ref>. 8 DISCERN High-Level Behavior DISCERN is more than just a collection of individual cognitive models. Interesting behavior results from the interaction of the components in a complete story-processing system. Let us follow DISCERN as it processes the story about John's visit to MaMaison (figure 6).
Reference: [3] <author> G. H. Bower, J. B. Black, and T. J. Turner. </author> <title> Scripts in memory for text. </title> <journal> Cognitive Psychology, </journal> <volume> 11 </volume> <pages> 177-220, </pages> <year> 1979. </year>
Reference-contexts: They are assumed immediately and automatically upon reading the story and have become part of the memory of the story. In a similar fashion, human readers often confuse what was mentioned in the story with what was only inferred <ref> [3; 10; 11] </ref>. A number of issues can be identified from the above examples. <p> If there is not enough information to fill a role, the most likely filler is selected and maintained throughout the paraphrase generation. Such behavior automatically results from the modular architecture of DISCERN and is consistent with experimental observations on how people remember stories of familiar event sequences <ref> [3; 10; 11] </ref>. In general, given the information in the question, DISCERN recalls the story that best matches it in the memory.
Reference: [4] <author> A. Caramazza. </author> <title> Some aspects of language processing revealed through the analysis of acquired aphasia: The lexical system. </title> <journal> Annual Review of Neuroscience, </journal> <volume> 11 </volume> <pages> 395-421, </pages> <year> 1988. </year>
Reference-contexts: Both maps and the associative connections between them are organized simultaneously, based on examples of co-occurring symbols and meanings. The lexicon architecture facilitates interesting behavior. Localized damage to the semantic map results in category-specific lexical deficits similar to human aphasia <ref> [4; 15] </ref>. For example, the system selectively loses access to restaurant names, or animate words, when that part of the map is damaged. Dyslexic performance errors can also be modeled.
Reference: [5] <author> M. Coltheart, K. Patterson, and J. C. Marshall, </author> <title> editors. Deep Dyslexia. </title> <publisher> Routledge and Kegan Paul, </publisher> <address> London; New York, </address> <note> second edition, </note> <year> 1988. </year>
Reference-contexts: Dyslexic performance errors can also be modeled. If the performance is degraded, for example, by adding noise to the connections, parsing and generation errors that occur are quite similar to those observed in human deep dyslexia <ref> [5] </ref>. For example, the system may confuse Leone's with MaMaison, or LINE with LIKE, because they are nearby in the map and share similar associative connections. 5 are modified according to the backpropagation error signal, and replace the old representations in the lexicon.
Reference: [6] <author> R. E. Cullingford. </author> <title> Script Application: Computer Understanding of Newspaper Stories. </title> <type> PhD thesis, </type> <institution> Department of Computer Science, Yale University, </institution> <address> New Haven, CT, </address> <year> 1978. </year> <type> Technical Report 116. </type>
Reference-contexts: Dale, H. Moisl and H. Somers (editors), A Handbook of Natural Language Processing: Techniques and Applications for the Processing of Language as Text. New York: Marcel Dekker. 2 The Script Processing Task Scripts <ref> [6; 8; 26] </ref> are schemas of often-encountered, stereotypic event sequences, such as visiting a restaurant, traveling by airplane, and shopping at a supermarket. Each script divides further into tracks, or established minor variations. A script can be represented as a causal chain of events with a number of open roles.
Reference: [7] <author> M. G. Dyer. </author> <title> In-Depth Understanding: A Computer Model of Integrated Processing for Narrative Comprehension. </title> <publisher> MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1983. </year>
Reference-contexts: For example, in story understanding, symbolic systems have been developed that analyze realistic stories in depth, based on higher-level knowledge structures such as goals, plans, themes, affects, beliefs, argument structures, plots, and morals (e.g. <ref> [1; 7; 23; 26] </ref>).
Reference: [8] <author> M. G. Dyer, R. E. Cullingford, and S. Alvarado. </author> <title> Scripts. </title> <editor> In S. C. Shapiro, editor, </editor> <booktitle> Encyclopedia of Artificial Intelligence, </booktitle> <pages> pages 980-994. </pages> <publisher> Wiley, </publisher> <address> New York, </address> <year> 1987. </year>
Reference-contexts: Dale, H. Moisl and H. Somers (editors), A Handbook of Natural Language Processing: Techniques and Applications for the Processing of Language as Text. New York: Marcel Dekker. 2 The Script Processing Task Scripts <ref> [6; 8; 26] </ref> are schemas of often-encountered, stereotypic event sequences, such as visiting a restaurant, traveling by airplane, and shopping at a supermarket. Each script divides further into tracks, or established minor variations. A script can be represented as a causal chain of events with a number of open roles.
Reference: [9] <author> J. L. Elman. </author> <title> Finding structure in time. </title> <journal> Cognitive Science, </journal> <volume> 14 </volume> <pages> 179-211, </pages> <year> 1990. </year>
Reference-contexts: Each module performs a specific subtask, such as parsing a sentence or generating an answer to a question. All these networks have the same basic architecture: they are three-layer, simple-recurrent backpropagation networks <ref> [9] </ref>, with the extension called FGREP that allows them to develop distributed representations for their input/output words. The FGREP mechanism (Forming Global Representations with Extended backPropagation, [21]) is based on a basic three-layer backward error propagation network, with the I/O representation patterns stored in an external lexicon (figure 3).
Reference: [10] <author> A. C. Graesser, S. E. Gordon, and J. D. Sawyer. </author> <title> Recognition memory for typical and atypical actions in scripted activities: Tests for the script pointer+tag hypothesis. </title> <journal> Journal of Verbal Learning and Verbal Behavior, </journal> <volume> 18 </volume> <pages> 319-332, </pages> <year> 1979. </year>
Reference-contexts: They are assumed immediately and automatically upon reading the story and have become part of the memory of the story. In a similar fashion, human readers often confuse what was mentioned in the story with what was only inferred <ref> [3; 10; 11] </ref>. A number of issues can be identified from the above examples. <p> If there is not enough information to fill a role, the most likely filler is selected and maintained throughout the paraphrase generation. Such behavior automatically results from the modular architecture of DISCERN and is consistent with experimental observations on how people remember stories of familiar event sequences <ref> [3; 10; 11] </ref>. In general, given the information in the question, DISCERN recalls the story that best matches it in the memory.
Reference: [11] <author> A. C. Graesser, S. B. Woll, D. J. Kowalski, and D. A. Smith. </author> <title> Memory for typical and atypical actions in scripted activities. Journal of Experimental Psychology: </title> <booktitle> Human Learning and Memory, </booktitle> <volume> 6 </volume> <pages> 503-515, </pages> <year> 1980. </year>
Reference-contexts: They are assumed immediately and automatically upon reading the story and have become part of the memory of the story. In a similar fashion, human readers often confuse what was mentioned in the story with what was only inferred <ref> [3; 10; 11] </ref>. A number of issues can be identified from the above examples. <p> If there is not enough information to fill a role, the most likely filler is selected and maintained throughout the paraphrase generation. Such behavior automatically results from the modular architecture of DISCERN and is consistent with experimental observations on how people remember stories of familiar event sequences <ref> [3; 10; 11] </ref>. In general, given the information in the question, DISCERN recalls the story that best matches it in the memory.
Reference: [12] <author> T. Kohonen. </author> <title> The self-organizing map. </title> <booktitle> Proceedings of the IEEE, </booktitle> <volume> 78 </volume> <pages> 1464-1480, </pages> <year> 1990. </year>
Reference-contexts: The semantic representations stand for distinct meanings and are developed automatically by the system while it is learning the processing task. The lexicon stores the lexical and semantic representations and translates between them (figure 2; [20]). It is implemented as two feature maps <ref> [12; 13] </ref>, one lexical and the other semantic. Words whose lexical forms are similar, such as LINE and LIKE, are represented by nearby units in the lexical map.
Reference: [13] <author> T. Kohonen. </author> <title> Self-Organizing Maps. </title> <publisher> Springer, </publisher> <address> Berlin; Heidelberg; New York, </address> <year> 1995. </year>
Reference-contexts: The semantic representations stand for distinct meanings and are developed automatically by the system while it is learning the processing task. The lexicon stores the lexical and semantic representations and translates between them (figure 2; [20]). It is implemented as two feature maps <ref> [12; 13] </ref>, one lexical and the other semantic. Words whose lexical forms are similar, such as LINE and LIKE, are represented by nearby units in the lexical map.
Reference: [14] <author> J. L. Kolodner. </author> <title> Retrieval and Organizational Strategies in Conceptual Memory: A Computer Model. </title> <publisher> Erlbaum, </publisher> <address> Hillsdale, NJ, </address> <year> 1984. </year>
Reference-contexts: representing them accurately. (4) Because the representation is based on salient differences in the data, the classification is very robust, and usually correct even if the input is noisy or incomplete. (5) Because the memory is based on classifying the similarities and storing the differences, retrieval becomes a reconstructive process <ref> [14; 28] </ref> similar to human memory. The trace feature map exhibits interesting memory effects that result from interactions between traces. Later traces capture units from earlier ones, making later traces more likely to be retrieved (figure 5). The extent of the traces determines memory capacity.
Reference: [15] <author> R. A. McCarthy and E. K. Warrington. </author> <title> Cognitive Neuropsychology: A Clinical Introduction. </title> <publisher> Academic Press, </publisher> <address> New York, </address> <year> 1990. </year>
Reference-contexts: Both maps and the associative connections between them are organized simultaneously, based on examples of co-occurring symbols and meanings. The lexicon architecture facilitates interesting behavior. Localized damage to the semantic map results in category-specific lexical deficits similar to human aphasia <ref> [4; 15] </ref>. For example, the system selectively loses access to restaurant names, or animate words, when that part of the map is damaged. Dyslexic performance errors can also be modeled.
Reference: [16] <author> R. Miikkulainen. </author> <title> Script recognition with hierarchical feature maps. </title> <journal> Connection Science, </journal> <volume> 2 </volume> <pages> 83-101, </pages> <year> 1990. </year>
Reference-contexts: This allows us to approximate a large vocabulary with only a small number of semantically different representations (which are expensive to develop) at our disposal. 7 Episodic Memory The episodic memory in DISCERN <ref> [16; 17] </ref> consists of a hierarchical pyramid of feature maps organized according to the taxonomy of script-based stories (figure 4). The highest level of the hierarchy is a single feature map that lays out the different script classes.
Reference: [17] <author> R. Miikkulainen. </author> <title> Trace feature map: A model of episodic associative memory. </title> <journal> Biological Cybernetics, </journal> <volume> 67 </volume> <pages> 273-282, </pages> <year> 1992. </year>
Reference-contexts: This allows us to approximate a large vocabulary with only a small number of semantically different representations (which are expensive to develop) at our disposal. 7 Episodic Memory The episodic memory in DISCERN <ref> [16; 17] </ref> consists of a hierarchical pyramid of feature maps organized according to the taxonomy of script-based stories (figure 4). The highest level of the hierarchy is a single feature map that lays out the different script classes.
Reference: [18] <author> R. Miikkulainen. </author> <title> Subsymbolic Natural Language Processing: An Integrated Model of Scripts, Lexicon, and Memory. </title> <publisher> MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1993. </year> <month> 15 </month>
Reference-contexts: This is where DISCERN (DIstributed SCript processing and Episodic memoRy Network <ref> [18] </ref>), a subsymbolic neural network model of script-based story understanding, fits in. DISCERN is purely a subsymbolic model, but at the high level it consists of modules and information structures similar to those of symbolic systems, such as scripts, lexicon, and episodic memory. <p> In designing subsymbolic models that would do that, we are faced with two major challenges <ref> [18] </ref>: (1) how to implement connectionist control of high-level processing strategies (making it possible to model processes more sophisticated than a series of reflex responses), and (2) how to represent and learn abstractions (making it possible to process information at a higher level than correlations in the raw input data).
Reference: [19] <author> R. Miikkulainen. </author> <title> Script-based inference and memory retrieval in subsymbolic story processing. </title> <journal> Applied Intelligence, </journal> <volume> 5 </volume> <pages> 137-163, </pages> <year> 1995. </year>
Reference-contexts: DISCERN also demonstrates strong script-based inferencing <ref> [19] </ref>. Even when the input story is incomplete, consisting of only a few main events, DISCERN can usually form an accurate internal representation of it.
Reference: [20] <author> R. Miikkulainen. </author> <title> Dyslexic and category-specific impairments in a self-organizing feature map model of the lexicon. Brain and Language, </title> <publisher> in press. </publisher>
Reference-contexts: The semantic representations stand for distinct meanings and are developed automatically by the system while it is learning the processing task. The lexicon stores the lexical and semantic representations and translates between them (figure 2; <ref> [20] </ref>). It is implemented as two feature maps [12; 13], one lexical and the other semantic. Words whose lexical forms are similar, such as LINE and LIKE, are represented by nearby units in the lexical map.
Reference: [21] <author> R. Miikkulainen and M. G. Dyer. </author> <title> Natural language processing with modular neural networks and distributed lexicon. </title> <journal> Cognitive Science, </journal> <volume> 15 </volume> <pages> 343-399, </pages> <year> 1991. </year>
Reference-contexts: All these networks have the same basic architecture: they are three-layer, simple-recurrent backpropagation networks [9], with the extension called FGREP that allows them to develop distributed representations for their input/output words. The FGREP mechanism (Forming Global Representations with Extended backPropagation, <ref> [21] </ref>) is based on a basic three-layer backward error propagation network, with the I/O representation patterns stored in an external lexicon (figure 3). The input and output layers of the network are divided into assemblies (i.e. groups of units).
Reference: [22] <author> L. Postman. </author> <title> Transfer, interference and forgetting. </title> <editor> In J. W. Kling and L. A. Riggs, editors, </editor> <booktitle> Woodworth and Schlosberg's Experimental Psychology, </booktitle> <pages> pages 1019-1132. </pages> <publisher> Holt, Rinehart and Winston, </publisher> <address> New York, third edition, </address> <year> 1971. </year>
Reference-contexts: If the memory capacity is exceeded, older traces will be selectively replaced by newer ones. Traces that are unique, that is, located in a sparse area of the map, are not affected, no matter how old they are. Similar 9 effects are common in human long-term memory <ref> [2; 22] </ref>. 8 DISCERN High-Level Behavior DISCERN is more than just a collection of individual cognitive models. Interesting behavior results from the interaction of the components in a complete story-processing system. Let us follow DISCERN as it processes the story about John's visit to MaMaison (figure 6).
Reference: [23] <author> J. F. Reeves. </author> <title> Computational Morality: A Process Model of Belief Conflict and Resolution for Story Understanding. </title> <type> PhD thesis, </type> <institution> Computer Science Department, University of California, </institution> <address> Los Angeles, </address> <year> 1991. </year> <note> Technical Report UCLA-AI-91-05. </note>
Reference-contexts: For example, in story understanding, symbolic systems have been developed that analyze realistic stories in depth, based on higher-level knowledge structures such as goals, plans, themes, affects, beliefs, argument structures, plots, and morals (e.g. <ref> [1; 7; 23; 26] </ref>).
Reference: [24] <author> D. E. Rumelhart, G. E. Hinton, and R. J. Williams. </author> <title> Learning internal representations by error propagation. </title> <editor> In D. E. Rumelhart and J. L. McClelland, editors, </editor> <booktitle> Parallel Distributed Processing: Explorations in the Microstructure of Cognition, Volume 1: Foundations, </booktitle> <pages> pages 318-362. </pages> <publisher> MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1986. </year>
Reference-contexts: In a sequential output network, the input is stationary, but the teaching pattern changes at each step. The network learns to produce a sequential interpretation of its input. The network learns the processing task by adapting the connection weights according to the standard on-line backpropagation procedure <ref> [24] </ref>. The error signal is propagated to the input layer, and the current input representations are modified as if they were an extra layer of weights. The modified representation vectors are put back in the lexicon, replacing the old representations.
Reference: [25] <author> David E. Rumelhart and James L. McClelland. </author> <title> On learning past tenses of English verbs. </title> <editor> In D. E. Rumelhart and J. L. McClelland, editors, </editor> <booktitle> Parallel Distributed Processing: Explorations in the Microstructure of Cognition, Volume 2: Psychological and Biological Models, </booktitle> <pages> pages 216-271. </pages> <publisher> MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1986. </year>
Reference-contexts: They produce the statistically most likely answer given the input conditions in a process that is opaque to the external observer. This is well suited to the modeling of isolated low-level tasks, such as learning past tense forms of verbs or word pronunciation <ref> [25; 27] </ref>. Given the success of such models, a possible approach to higher-level cognitive modeling would be to construct the system from several submodules that work together to produce the higher-level behavior.
Reference: [26] <author> R. C. Schank and R. P. Abelson. </author> <title> Scripts, Plans, Goals, and Understanding: An Inquiry into Human Knowledge Structures. </title> <publisher> Erlbaum, </publisher> <address> Hillsdale, NJ, </address> <year> 1977. </year>
Reference-contexts: Dale, H. Moisl and H. Somers (editors), A Handbook of Natural Language Processing: Techniques and Applications for the Processing of Language as Text. New York: Marcel Dekker. 2 The Script Processing Task Scripts <ref> [6; 8; 26] </ref> are schemas of often-encountered, stereotypic event sequences, such as visiting a restaurant, traveling by airplane, and shopping at a supermarket. Each script divides further into tracks, or established minor variations. A script can be represented as a causal chain of events with a number of open roles. <p> For example, in story understanding, symbolic systems have been developed that analyze realistic stories in depth, based on higher-level knowledge structures such as goals, plans, themes, affects, beliefs, argument structures, plots, and morals (e.g. <ref> [1; 7; 23; 26] </ref>).
Reference: [27] <author> T. J. Sejnowski and C. R. Rosenberg. </author> <title> Parallel networks that learn to pronounce English text. </title> <journal> Complex Systems, </journal> <volume> 1 </volume> <pages> 145-168, </pages> <year> 1987. </year>
Reference-contexts: They produce the statistically most likely answer given the input conditions in a process that is opaque to the external observer. This is well suited to the modeling of isolated low-level tasks, such as learning past tense forms of verbs or word pronunciation <ref> [25; 27] </ref>. Given the success of such models, a possible approach to higher-level cognitive modeling would be to construct the system from several submodules that work together to produce the higher-level behavior.
Reference: [28] <author> M. D. Williams and J. D. Hollan. </author> <title> The process of retrieval from very long-term memory. </title> <journal> Cognitive Science, </journal> <volume> 5 </volume> <pages> 87-119, </pages> <year> 1981. </year> <month> 16 </month>
Reference-contexts: representing them accurately. (4) Because the representation is based on salient differences in the data, the classification is very robust, and usually correct even if the input is noisy or incomplete. (5) Because the memory is based on classifying the similarities and storing the differences, retrieval becomes a reconstructive process <ref> [14; 28] </ref> similar to human memory. The trace feature map exhibits interesting memory effects that result from interactions between traces. Later traces capture units from earlier ones, making later traces more likely to be retrieved (figure 5). The extent of the traces determines memory capacity.
References-found: 28

