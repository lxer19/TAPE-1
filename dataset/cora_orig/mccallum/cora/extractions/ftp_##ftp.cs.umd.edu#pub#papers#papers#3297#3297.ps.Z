URL: ftp://ftp.cs.umd.edu/pub/papers/papers/3297/3297.ps.Z
Refering-URL: http://www.cs.umd.edu/TRs/TR.html
Root-URL: 
Email: wak@cs.umd.edu, (301)-405-2726 pugh@cs.umd.edu, (301)-405-2705  
Title: Finding Legal Reordering Transformations using Mappings  
Author: Wayne Kelly William Pugh 
Date: April 29, 1994  
Address: College Park, MD 20742  
Affiliation: Dept. of Computer Science Univ. of Maryland,  
Abstract: Traditionally, optimizing compilers attempt to improve the performance of programs by applying source to source transformations, such as loop interchange, loop skewing and loop distribution. Each of these transformations has its own special legality checks and transformation rules which make it hard to analyze or predict the effects of compositions of these transformations. To overcome these problems we have developed a framework for unifying iteration reordering transformations. The framework is based on the idea that all reordering transformation can be represented as a mapping from the original iteration space to a new iteration space. The framework is designed to provide a uniform way to represent and reason about transformations. An optimizing compiler would use our framework by finding a mapping that both corresponds to a legal transformation and produces efficient code. We present the mapping selection problem as a search problem by decomposing it into a sequence of smaller choices. We then characterize the set of all legal mappings by defining an implicit search tree. 
Abstract-found: 1
Intro-found: 1
Reference: [ACK87] <author> R. Allen, D. Callahan, and K. Kennedy. </author> <title> Automatic decomposition of scientific programs for parallel execution. </title> <booktitle> In Conference Record of the Fourteenth ACM Symposium on Principles of Programming Languages, </booktitle> <pages> pages 63-76, </pages> <month> January </month> <year> 1987. </year>
Reference-contexts: Representing traditional transformations In [KP93b] we demonstrated how mappings can be used to represent all transformations than can be obtained by applying any sequence of the following traditional transformations: * loop interchange * loop reversal * loop skewing * statement reordering * loop distribution * loop fusion * loop alignment <ref> [ACK87] </ref> * loop interleaving [ST92] * loop blocking 1 (or tiling) [AK87] * index set splitting 1 [Ban79] * loop coalescing 1 [Pol88] * loop scaling 1 [LP92] 2.5 Examples particular mappings. 3 Tuple Relations and Sets Most of the previous work on program transformations uses data dependence directions and distances
Reference: [AI91] <author> Corinne Ancourt and Franccois Irigoin. </author> <title> Scanning polyhedra with DO loops. </title> <booktitle> In Proc. of the 3rd ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming, </booktitle> <pages> pages 39-50, </pages> <month> April </month> <year> 1991. </year>
Reference-contexts: Quasi-affine expressions <ref> [AI91] </ref> are affine functions plus integer division and remainder when dividing by a constant. Quasi-affine expressions allow us to create mappings corresponding to blocking (or strip-mining) transformations.
Reference: [AK87] <author> J. R. Allen and K. Kennedy. </author> <title> Automatic translation of Fortran programs to vector form. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 9(4) </volume> <pages> 491-542, </pages> <month> October </month> <year> 1987. </year>
Reference-contexts: used to represent all transformations than can be obtained by applying any sequence of the following traditional transformations: * loop interchange * loop reversal * loop skewing * statement reordering * loop distribution * loop fusion * loop alignment [ACK87] * loop interleaving [ST92] * loop blocking 1 (or tiling) <ref> [AK87] </ref> * index set splitting 1 [Ban79] * loop coalescing 1 [Pol88] * loop scaling 1 [LP92] 2.5 Examples particular mappings. 3 Tuple Relations and Sets Most of the previous work on program transformations uses data dependence directions and distances to summarize dependences between array references.
Reference: [AKPW83] <author> J.R. Allen, K. Kennedy, C. Porterfield, and J. Warren. </author> <title> Conversion of control dependence to data dependence. </title> <booktitle> In Conf. Rec. Tenth ACM Symp. on Principles of Programming Languages, </booktitle> <pages> pages 177-189, </pages> <month> January </month> <year> 1983. </year>
Reference-contexts: q) is a conjunction containing a minimal subset of the constraints in p such that ((gist p given q) ^ q) = (p ^ q)), otherwise it is False. 3.2 Control dependence If conditionals exist in a program, then we require that they be converted to guarded statements via if-conversion <ref> [AKPW83] </ref>. Alternatively, structured if statements can be handled by treating them as atomic statements. We also require that all loop bounds be affine functions of surrounding loop variables and symbolic constants.
Reference: [B + 89] <author> M. Berry et al. </author> <title> The PERFECT Club benchmarks: Effective performance evaluation of supercomputers. </title> <journal> International Journal of Supercomputing Applications, </journal> <volume> 3(3) </volume> <pages> 5-40, </pages> <month> March </month> <year> 1989. </year>
Reference-contexts: We can therefore ignore control dependences, as those that do exist are implicitly contained in our description of the iteration space. 1 Our current implementation cannot handle all cases of these transformations. 3 Code adapted from CHOSOL in the Perfect club (SD) Code adapted from OLDA in Perfect club (TI) <ref> [B + 89] </ref> Original code do 3 i=2,n do 2 j=1,i-1 3 b (i) = b (i) - sum (i) Original code do 2 p = 1, n do 2 i = 1, orb 1 xrsiq (i,q)=xrsiq (i,q) + ... S1 2 xrsiq (i,p)=xrsiq (i,p) + ...
Reference: [Ban79] <author> U. Banerjee. </author> <title> Speedup of Ordinary Programs. </title> <type> PhD thesis, </type> <institution> Dept. of Computer Science, U. of Illinois at Urbana-Champaign, </institution> <month> October </month> <year> 1979. </year>
Reference-contexts: can be obtained by applying any sequence of the following traditional transformations: * loop interchange * loop reversal * loop skewing * statement reordering * loop distribution * loop fusion * loop alignment [ACK87] * loop interleaving [ST92] * loop blocking 1 (or tiling) [AK87] * index set splitting 1 <ref> [Ban79] </ref> * loop coalescing 1 [Pol88] * loop scaling 1 [LP92] 2.5 Examples particular mappings. 3 Tuple Relations and Sets Most of the previous work on program transformations uses data dependence directions and distances to summarize dependences between array references. For our purposes, these abstractions are too crude.
Reference: [Ban90] <author> U. Banerjee. </author> <title> Unimodular transformations of double loops. </title> <booktitle> In Proc. of the 3rd Workshop on Programming Languages and Compilers for Parallel Computing, </booktitle> <pages> pages 192-219, </pages> <address> Irvine, CA, </address> <month> August </month> <year> 1990. </year>
Reference-contexts: Each of these transformations has its own special legality checks and transformation rules. This makes it difficult to find sequences of transformations that obtain some desired goal. To overcome these problems, many researchers have proposed frameworks that unify some of these reordering transformations <ref> [Ban90, WL91, LMQ91, LP92, Ram92, Fea92a, Fea92b, ST92, DR92] </ref>. We have developed a framework that handles more reordering transformations than most existing transformation frameworks. Our framework is based on the idea that a transformation can be represented as a mapping from the original iteration space to a new iteration space. <p> In finding this set of satisfying values, we could consider optimality criteria such as locality or lack of loop carried dependences. 8 Related Work The framework of Unimodular transformations <ref> [Ban90, WL91, ST92, KKB92] </ref> has the same goal as our work, in that it attempts to provide a unified framework for describing loop transformations.
Reference: [DR92] <author> Alain Darte and Yves Robert. </author> <title> Scheduling uniform loop nests. </title> <booktitle> In Proceedings of ISMN International Conference on Parallel and Distributed Computer Systems, </booktitle> <month> October </month> <year> 1992. </year> <note> Also available as Technical Report 92-10, </note> <institution> Laboratoire de l'Informatique du Parallelisme, Ecolo Normal Superieure de Lyon, Instiut IMAG. </institution>
Reference-contexts: Each of these transformations has its own special legality checks and transformation rules. This makes it difficult to find sequences of transformations that obtain some desired goal. To overcome these problems, many researchers have proposed frameworks that unify some of these reordering transformations <ref> [Ban90, WL91, LMQ91, LP92, Ram92, Fea92a, Fea92b, ST92, DR92] </ref>. We have developed a framework that handles more reordering transformations than most existing transformation frameworks. Our framework is based on the idea that a transformation can be represented as a mapping from the original iteration space to a new iteration space.
Reference: [Fea92a] <author> Paul Feautrier. </author> <title> Some efficient solutions to the affine scheduling problem, Part I, One-dimensional time. </title> <journal> Int. J. of Parallel Programming, </journal> <volume> 21(5), </volume> <month> Oct </month> <year> 1992. </year>
Reference-contexts: Each of these transformations has its own special legality checks and transformation rules. This makes it difficult to find sequences of transformations that obtain some desired goal. To overcome these problems, many researchers have proposed frameworks that unify some of these reordering transformations <ref> [Ban90, WL91, LMQ91, LP92, Ram92, Fea92a, Fea92b, ST92, DR92] </ref>. We have developed a framework that handles more reordering transformations than most existing transformation frameworks. Our framework is based on the idea that a transformation can be represented as a mapping from the original iteration space to a new iteration space. <p> There are various techniques we can use to generate F . The most complete, but also most expensive method is to apply the affine form of Farkas Lemma <ref> [Sch86, Fea92a] </ref>: Lemma 6.1 (Farkas) Let the system Ax b of affine inequalities have at least one solution. An affine form is non-negative for each x satisfying Ax b if and only if (x) 0 is a nonnegative affine combination of the inequalities in the system Ax b. <p> This allows the resulting programs to have steps in their loops, which can be useful for optimizing locality. Our mappings are not required to be unimodular and can therefore also generate steps. Paul Feautrier <ref> [Fea92a, Fea92b] </ref> has independently developed a framework which is very similar to our own.
Reference: [Fea92b] <author> Paul Feautrier. </author> <title> Some efficient solutions to the affine scheduling problem, Part II, Multidimensional time. </title> <journal> Int. J. of Parallel Programming, </journal> <volume> 21(6), </volume> <month> Dec </month> <year> 1992. </year>
Reference-contexts: Each of these transformations has its own special legality checks and transformation rules. This makes it difficult to find sequences of transformations that obtain some desired goal. To overcome these problems, many researchers have proposed frameworks that unify some of these reordering transformations <ref> [Ban90, WL91, LMQ91, LP92, Ram92, Fea92a, Fea92b, ST92, DR92] </ref>. We have developed a framework that handles more reordering transformations than most existing transformation frameworks. Our framework is based on the idea that a transformation can be represented as a mapping from the original iteration space to a new iteration space. <p> This allows the resulting programs to have steps in their loops, which can be useful for optimizing locality. Our mappings are not required to be unimodular and can therefore also generate steps. Paul Feautrier <ref> [Fea92a, Fea92b] </ref> has independently developed a framework which is very similar to our own.
Reference: [KKB92] <author> K. G. Kumar, D. Kulkarni, and A. Basu. </author> <title> Deriving good transformations for mapping nested loops on hieracical parallel machines in polynomial time. </title> <booktitle> In Proc. of the 1992 International Conference on Supercomputing, </booktitle> <pages> pages 82-92, </pages> <month> July </month> <year> 1992. </year>
Reference-contexts: In finding this set of satisfying values, we could consider optimality criteria such as locality or lack of loop carried dependences. 8 Related Work The framework of Unimodular transformations <ref> [Ban90, WL91, ST92, KKB92] </ref> has the same goal as our work, in that it attempts to provide a unified framework for describing loop transformations.
Reference: [KP93a] <author> Wayne Kelly and William Pugh. </author> <title> Determining schedules based on performance estimation. </title> <type> Technical Report CS-TR-3108, </type> <institution> Dept. of Computer Science, University of Maryland, College Park, </institution> <month> July </month> <year> 1993. </year> <note> to appear in Parallel Processing Letters (1994). </note>
Reference-contexts: The first task corresponds to defining the search tree and the second task corresponds to selecting a path within that tree. This paper will deal almost entirely with the first task, i.e. defining the search tree. Interested readers are referred to our previous work <ref> [KP93b, KP93a] </ref> for more information on the second and third tasks. In Section 2, we explain how mappings can be used to represent reordering transformations. In Section 3, we describe tuple relations and how they are used to represent mappings and data dependences. <p> This would allow us to use admissible search algorithms such as the A fl algorithm [Nil80]. In a previous paper <ref> [KP93a] </ref>, we described an approach along these lines. Depending on the search algorithm used, parts of the search tree may actually be constructed, or may only exist implicitly. <p> To satisfy this goal, we select mappings in a completely different way. We start by considering some statement, and generate the constraints on choices for legal variable parts for that statement. We then choose one of these legal variable parts by methods described in <ref> [KP93a] </ref>. We incorporate that information into the constraints for the variable parts of the other statements. We continue this selection process until we have selected a variable part for each statement. We then move on to making decisions about constant parts in a similar way. <p> We have described two basic techniques: * Techniques that infer constraints on the legal mappings for individual statements or the relationship between the mappings for different statements. Using these methods, we can easily produce legal mappings. Other methods, such as those described in <ref> [KP93a] </ref>, are required to determine which of the possible legal mappings are desirable. * Techniques that given mappings for some statements can infer constraints on the legal mappings for the other statements.
Reference: [KP93b] <author> Wayne Kelly and William Pugh. </author> <title> A framework for unifying reordering transformations. </title> <type> Technical Report CS-TR-3193, </type> <institution> Dept. of Computer Science, University of Maryland, College Park, </institution> <month> April </month> <year> 1993. </year>
Reference-contexts: The first task corresponds to defining the search tree and the second task corresponds to selecting a path within that tree. This paper will deal almost entirely with the first task, i.e. defining the search tree. Interested readers are referred to our previous work <ref> [KP93b, KP93a] </ref> for more information on the second and third tasks. In Section 2, we explain how mappings can be used to represent reordering transformations. In Section 3, we describe tuple relations and how they are used to represent mappings and data dependences. <p> The algorithm we use to generate the transformed code is relatively complicated and is not described in this paper. A description of the algorithm can be found in <ref> [KP93b] </ref>. 2.4 Representing traditional transformations In [KP93b] we demonstrated how mappings can be used to represent all transformations than can be obtained by applying any sequence of the following traditional transformations: * loop interchange * loop reversal * loop skewing * statement reordering * loop distribution * loop fusion * loop <p> The algorithm we use to generate the transformed code is relatively complicated and is not described in this paper. A description of the algorithm can be found in <ref> [KP93b] </ref>. 2.4 Representing traditional transformations In [KP93b] we demonstrated how mappings can be used to represent all transformations than can be obtained by applying any sequence of the following traditional transformations: * loop interchange * loop reversal * loop skewing * statement reordering * loop distribution * loop fusion * loop alignment [ACK87] * loop interleaving [ST92] <p> In previous work <ref> [KP93b] </ref> we used generate and test techniques; that is, for a given variable part v we described a test that determined whether or not v 2 V p .
Reference: [LMQ91] <author> Herve Leverge, Christophe Mauras, and Patrice Quinton. </author> <title> A language-orientied approach to the design of systolic chips. </title> <journal> Journal of VLSI Signal Processing, </journal> <pages> pages 173-182, </pages> <month> Mar </month> <year> 1991. </year> <month> 14 </month>
Reference-contexts: Each of these transformations has its own special legality checks and transformation rules. This makes it difficult to find sequences of transformations that obtain some desired goal. To overcome these problems, many researchers have proposed frameworks that unify some of these reordering transformations <ref> [Ban90, WL91, LMQ91, LP92, Ram92, Fea92a, Fea92b, ST92, DR92] </ref>. We have developed a framework that handles more reordering transformations than most existing transformation frameworks. Our framework is based on the idea that a transformation can be represented as a mapping from the original iteration space to a new iteration space.
Reference: [LP92] <author> Wei Li and Keshav Pingali. </author> <title> A singular loop transformation framework based on non-singular matrices. </title> <booktitle> In 5th Workshop on Languages and Compilers for Parallel Computing, </booktitle> <pages> pages 249-260, </pages> <institution> Yale University, </institution> <month> August </month> <year> 1992. </year>
Reference-contexts: Each of these transformations has its own special legality checks and transformation rules. This makes it difficult to find sequences of transformations that obtain some desired goal. To overcome these problems, many researchers have proposed frameworks that unify some of these reordering transformations <ref> [Ban90, WL91, LMQ91, LP92, Ram92, Fea92a, Fea92b, ST92, DR92] </ref>. We have developed a framework that handles more reordering transformations than most existing transformation frameworks. Our framework is based on the idea that a transformation can be represented as a mapping from the original iteration space to a new iteration space. <p> traditional transformations: * loop interchange * loop reversal * loop skewing * statement reordering * loop distribution * loop fusion * loop alignment [ACK87] * loop interleaving [ST92] * loop blocking 1 (or tiling) [AK87] * index set splitting 1 [Ban79] * loop coalescing 1 [Pol88] * loop scaling 1 <ref> [LP92] </ref> 2.5 Examples particular mappings. 3 Tuple Relations and Sets Most of the previous work on program transformations uses data dependence directions and distances to summarize dependences between array references. For our purposes, these abstractions are too crude. We describe dependences exactly using integer tuple relations. <p> Most existing frameworks use dependence direction/distance vectors as a dependence abstraction rather than the dependence relations that we use. This is adequate for unimodular frameworks, but is not adequate to test the legality of the sorts of transformations that we can represent. Unimodular transformations are generalized in 12 <ref> [LP92, Ram92] </ref> to include mappings that are invertible but not unimodular. This allows the resulting programs to have steps in their loops, which can be useful for optimizing locality. Our mappings are not required to be unimodular and can therefore also generate steps.
Reference: [Lu91] <author> Lee-Chung Lu. </author> <title> A unified framework for systematic loop transformations. </title> <booktitle> In Proc. of the 3rd ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming, </booktitle> <pages> pages 28-38, </pages> <month> April </month> <year> 1991. </year>
Reference: [Nil80] <author> Nils J. Nilsson. </author> <booktitle> Principles of Artificial Intelligence. </booktitle> <publisher> Morgan Kaufmann Publishers, </publisher> <year> 1980. </year>
Reference-contexts: include: * Using a heuristic to choose which branch to follow at each fork. * An interactive tool where a human user can select some or all of the branches to follow at each fork. * Defining a function from the nodes to the integers which satisfies the monotone property <ref> [Nil80] </ref>. This would allow us to use admissible search algorithms such as the A fl algorithm [Nil80]. In a previous paper [KP93a], we described an approach along these lines. Depending on the search algorithm used, parts of the search tree may actually be constructed, or may only exist implicitly. <p> interactive tool where a human user can select some or all of the branches to follow at each fork. * Defining a function from the nodes to the integers which satisfies the monotone property <ref> [Nil80] </ref>. This would allow us to use admissible search algorithms such as the A fl algorithm [Nil80]. In a previous paper [KP93a], we described an approach along these lines. Depending on the search algorithm used, parts of the search tree may actually be constructed, or may only exist implicitly.
Reference: [Pol88] <author> C. Polychronopoulos. </author> <title> Parallel Programming and Compilers. </title> <publisher> Kluwer Academic Publishers, </publisher> <year> 1988. </year>
Reference-contexts: any sequence of the following traditional transformations: * loop interchange * loop reversal * loop skewing * statement reordering * loop distribution * loop fusion * loop alignment [ACK87] * loop interleaving [ST92] * loop blocking 1 (or tiling) [AK87] * index set splitting 1 [Ban79] * loop coalescing 1 <ref> [Pol88] </ref> * loop scaling 1 [LP92] 2.5 Examples particular mappings. 3 Tuple Relations and Sets Most of the previous work on program transformations uses data dependence directions and distances to summarize dependences between array references. For our purposes, these abstractions are too crude.
Reference: [Pug91] <author> William Pugh. </author> <title> Uniform techniques for loop optimization. </title> <booktitle> In 1991 International Conference on Supercomputing, </booktitle> <pages> pages 341-352, </pages> <address> Cologne, Germany, </address> <month> June </month> <year> 1991. </year>
Reference-contexts: We introduce new variables corresponding to each of the input positions and output positions. Relationships between these variables and those corresponding to symbolic constants are represented as a disjunction of convex regions. See <ref> [Pug91] </ref> for a more thorough description. The gist operation We make use of the gist operation, originally developed in [PW92]. Intuitively, (gist p given q) is defined as the new information contained in p, given that we already know q. <p> This suggests that we need to compute the transitive closure of the dependence relations (as suggested in <ref> [Pug91] </ref>). As it turns out, this isn't exactly what we want or need. Firstly, exact computation of the transitive closure of an affine integer tuple relation is undecidable (see Section 6.2). We can, however, often compute closure of relations exactly, or bound them from above or below. <p> We can't add affinity dependences between statements because the statements will most likely have different mapping components. This does not mean however, that dependences between different statements are not important. To explain how these inter-statement dependences are used we need to briefly review <ref> [Pug91] </ref> how to compute the transitive closure of an entire dependence graph. Consider the simple case where there are only two statements of 7 interest, s p and s q . <p> This process of computing affine transitive dependences can be extended to the multiple statement case in the obvious way <ref> [Pug91] </ref>. The affine transitive self-dependences introduced, may produce new self dependence distances, so it might be possible to infer more information by iterating the above process. We don't know of any upper bound on the number of times this process would need to be repeated before reaching a fixed point.
Reference: [Pug92] <author> William Pugh. </author> <title> The Omega test: a fast and practical integer programming algorithm for dependenceanalysis. </title> <journal> Communications of the ACM, </journal> <volume> 8 </volume> <pages> 102-114, </pages> <month> August </month> <year> 1992. </year>
Reference-contexts: These free variables correspond to symbolic constants or parameters in the source program. We use Sym to represent the set of all symbolic constants. Tuple relations and sets are represented using the Omega test <ref> [Pug92, PW92, PW93] </ref> which is a package for manipulating affine constraints over integer variables. We introduce new variables corresponding to each of the input positions and output positions. Relationships between these variables and those corresponding to symbolic constants are represented as a disjunction of convex regions.
Reference: [PW92] <author> William Pugh and David Wonnacott. </author> <title> Going beyond integer programming with the Omega test to eliminate false data dependences. </title> <type> Technical Report CS-TR-3191, </type> <institution> Dept. of Computer Science, University of Maryland, College Park, </institution> <month> December </month> <year> 1992. </year> <note> An earlier version of this paper appeared at the SIGPLAN PLDI'92 conference. </note>
Reference-contexts: These free variables correspond to symbolic constants or parameters in the source program. We use Sym to represent the set of all symbolic constants. Tuple relations and sets are represented using the Omega test <ref> [Pug92, PW92, PW93] </ref> which is a package for manipulating affine constraints over integer variables. We introduce new variables corresponding to each of the input positions and output positions. Relationships between these variables and those corresponding to symbolic constants are represented as a disjunction of convex regions. <p> Relationships between these variables and those corresponding to symbolic constants are represented as a disjunction of convex regions. See [Pug91] for a more thorough description. The gist operation We make use of the gist operation, originally developed in <ref> [PW92] </ref>. Intuitively, (gist p given q) is defined as the new information contained in p, given that we already know q.
Reference: [PW93] <author> William Pugh and David Wonnacott. </author> <title> An evaluation of exact methods for analysis of value-based array data dependences. </title> <booktitle> In Sixth Annual Workshop on Programming Languages and Compilers for Parallel Computing, </booktitle> <address> Portland, OR, </address> <month> August </month> <year> 1993. </year>
Reference-contexts: These free variables correspond to symbolic constants or parameters in the source program. We use Sym to represent the set of all symbolic constants. Tuple relations and sets are represented using the Omega test <ref> [Pug92, PW92, PW93] </ref> which is a package for manipulating affine constraints over integer variables. We introduce new variables corresponding to each of the input positions and output positions. Relationships between these variables and those corresponding to symbolic constants are represented as a disjunction of convex regions.
Reference: [Ram92] <author> J. Ramanujam. </author> <title> Non-unimodular transformations of nested loops. </title> <booktitle> In Supercomputing `92, </booktitle> <pages> pages 214-223, </pages> <month> November </month> <year> 1992. </year>
Reference-contexts: Each of these transformations has its own special legality checks and transformation rules. This makes it difficult to find sequences of transformations that obtain some desired goal. To overcome these problems, many researchers have proposed frameworks that unify some of these reordering transformations <ref> [Ban90, WL91, LMQ91, LP92, Ram92, Fea92a, Fea92b, ST92, DR92] </ref>. We have developed a framework that handles more reordering transformations than most existing transformation frameworks. Our framework is based on the idea that a transformation can be represented as a mapping from the original iteration space to a new iteration space. <p> Most existing frameworks use dependence direction/distance vectors as a dependence abstraction rather than the dependence relations that we use. This is adequate for unimodular frameworks, but is not adequate to test the legality of the sorts of transformations that we can represent. Unimodular transformations are generalized in 12 <ref> [LP92, Ram92] </ref> to include mappings that are invertible but not unimodular. This allows the resulting programs to have steps in their loops, which can be useful for optimizing locality. Our mappings are not required to be unimodular and can therefore also generate steps.
Reference: [Sch86] <author> A. Schrijver. </author> <title> Theory of Linear and Integer Programming. </title> <publisher> John Wiley and Sons, </publisher> <address> Chichester, Great Britain, </address> <year> 1986. </year>
Reference-contexts: There are various techniques we can use to generate F . The most complete, but also most expensive method is to apply the affine form of Farkas Lemma <ref> [Sch86, Fea92a] </ref>: Lemma 6.1 (Farkas) Let the system Ax b of affine inequalities have at least one solution. An affine form is non-negative for each x satisfying Ax b if and only if (x) 0 is a nonnegative affine combination of the inequalities in the system Ax b.
Reference: [ST92] <author> Vivek Sarkar and Radhika Thekkath. </author> <title> A general framework for iteration-reordering loop transformations. </title> <booktitle> In ACM SIG-PLAN'92 Conference on Programming Language Design and Implementation, </booktitle> <pages> pages 175-187, </pages> <address> San Francisco, California, </address> <month> Jun </month> <year> 1992. </year>
Reference-contexts: Each of these transformations has its own special legality checks and transformation rules. This makes it difficult to find sequences of transformations that obtain some desired goal. To overcome these problems, many researchers have proposed frameworks that unify some of these reordering transformations <ref> [Ban90, WL91, LMQ91, LP92, Ram92, Fea92a, Fea92b, ST92, DR92] </ref>. We have developed a framework that handles more reordering transformations than most existing transformation frameworks. Our framework is based on the idea that a transformation can be represented as a mapping from the original iteration space to a new iteration space. <p> [KP93b] we demonstrated how mappings can be used to represent all transformations than can be obtained by applying any sequence of the following traditional transformations: * loop interchange * loop reversal * loop skewing * statement reordering * loop distribution * loop fusion * loop alignment [ACK87] * loop interleaving <ref> [ST92] </ref> * loop blocking 1 (or tiling) [AK87] * index set splitting 1 [Ban79] * loop coalescing 1 [Pol88] * loop scaling 1 [LP92] 2.5 Examples particular mappings. 3 Tuple Relations and Sets Most of the previous work on program transformations uses data dependence directions and distances to summarize dependences between <p> In finding this set of satisfying values, we could consider optimality criteria such as locality or lack of loop carried dependences. 8 Related Work The framework of Unimodular transformations <ref> [Ban90, WL91, ST92, KKB92] </ref> has the same goal as our work, in that it attempts to provide a unified framework for describing loop transformations.
Reference: [WL91] <author> Michael E. Wolf and Monica S. Lam. </author> <title> A data locality optimizing algorithm. </title> <booktitle> In ACM SIGPLAN'91 Conference on Programming Language Design and Implementation, </booktitle> <year> 1991. </year>
Reference-contexts: Each of these transformations has its own special legality checks and transformation rules. This makes it difficult to find sequences of transformations that obtain some desired goal. To overcome these problems, many researchers have proposed frameworks that unify some of these reordering transformations <ref> [Ban90, WL91, LMQ91, LP92, Ram92, Fea92a, Fea92b, ST92, DR92] </ref>. We have developed a framework that handles more reordering transformations than most existing transformation frameworks. Our framework is based on the idea that a transformation can be represented as a mapping from the original iteration space to a new iteration space. <p> In finding this set of satisfying values, we could consider optimality criteria such as locality or lack of loop carried dependences. 8 Related Work The framework of Unimodular transformations <ref> [Ban90, WL91, ST92, KKB92] </ref> has the same goal as our work, in that it attempts to provide a unified framework for describing loop transformations.
Reference: [Wol89a] <author> M. J. Wolfe. </author> <title> Optimizing Supercompilers for Supercomputers. </title> <publisher> The MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1989. </year>
Reference-contexts: 1 Introduction Traditionally, optimizing compilers attempt to parallelize programs and improve their performance, by applying source to source transformations, such as loop interchange, loop skewing and loop distribution <ref> [Wol89a] </ref>. Each of these transformations has its own special legality checks and transformation rules. This makes it difficult to find sequences of transformations that obtain some desired goal.
Reference: [Wol89b] <author> Michael Wolfe. </author> <title> More iteration space tiling. </title> <booktitle> In Proc. Supercomputing 89, </booktitle> <pages> pages 655-664, </pages> <month> November </month> <year> 1989. </year>
References-found: 28

