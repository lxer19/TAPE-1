URL: ftp://info.mcs.anl.gov/pub/tech_reports/reports/P420.ps.Z
Refering-URL: http://www.mcs.anl.gov/publications/abstracts/abstracts94.htm
Root-URL: http://www.mcs.anl.gov
Title: Computational Differentiation and Multidisciplinary Design  on Inverse Problems and Optimal Design in Industry, J.  
Author: Christian Bischof and Andreas Griewank McLaughlin and H. Engl, 
Address: Stuttgart, Germany  
Note: Preprint MCS-P420-0294 to appear in Proceedings of the Symposium  Eds, Teubner Verlag,  
Abstract: Multidisciplinary Design Optimization (MDO) by means of formal sensitivity analysis requires that each single-discipline analysis code supply not only the output functions for the (usually constrained) optimization process and other discipline analysis inputs, but also the derivatives of all of these output functions with respect to its input variables. Computational differentiation techniques and automatic aifferentiation tools enable MDO by providing accurate and efficient derivatives of computer programs with little human effort. We discuss the principles behind automatic differentiation and give a brief overview of automatic differentiation tools and how they can be employed judiciously, for example, for sparse Jacobians and to exploit parallelism. We show how, and under what circumstances, automatic differentiation applied to iterative solvers delivers the mathematically desired derivatives. We then show how derivatives that can now be feasibly obtained by computational differentiation techniques can lead to improved solution schemes for nonlinear coupled systems and multidisciplinary design optimization. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Brett Averick, Richard G. Carter, and Jorge J. </author> <title> More. The MINPACK-2 test problem collection (preliminary version). </title> <type> Technical Report ANL/MCS-TM-150, </type> <institution> Mathematics and Computer Science Division, Argonne National Laboratory, </institution> <year> 1991. </year>
Reference-contexts: This effect is most noticeable in the computation of gradients of so-called partially separable functions [6, 8], a rather common class of functions first described by Griewank and Toint [27]. For example, on the 2-D Ginsburg-Landau Superconductivity problem <ref> [1] </ref>, the sparse gradient computation on a 400 fi 400 grid requires only 7 percent of the floating-point operations required for the dense version. The dependency information collected and utilized in the sparse propagation of derivative objects also determines the sparsity pattern of the Jacobian, Hessian, and higher-derivative tensors.
Reference: [2] <author> Brett Averick, Jorge More, Christian Bischof, Alan Carle, and Andreas Griewank. </author> <title> Computing large sparse Jacobian matrices using automatic differentiation. </title> <type> Technical Report MCS-P348-0193, </type> <institution> Mathematics and Computer Science Division, Argonne National Laboratory, </institution> <year> 1993. </year>
Reference-contexts: This is due to the superscalar architecture of this chip and, we suspect, to efficient microcode implementations of multiplications by zero. A comprehensive study of this methodology applied to a collection of optimization model problems can be found in <ref> [2] </ref>. The compressed Jacobian does not tell the whole story, though. When we keep track of how many operations really have to be performed, avoiding additions and multiplications with zeros, we observe the behavior shown in Table 2.
Reference: [3] <author> Christian Bischof, Alan Carle, George Corliss, Andreas Griewank, and Paul Hovland. ADIFOR: </author> <title> Generating derivative codes from Fortran programs. </title> <journal> Scientific Programming, </journal> <volume> 1(1) </volume> <pages> 11-29, </pages> <year> 1992. </year>
Reference-contexts: The interpretation overhead of this trace and its potentially very large size can be a serious computational bottleneck [39]. Recently, a "source transformation" approach to automatic differentiation has been explored in the ADIFOR <ref> [3] </ref> and ODYSSEE [37] projects. Both tools transform Fortran code, applying the rules of automatic differentiation and generating new Fortran code that, when executed, computes derivatives without the overhead associated with "tape interpretation" schemes. ODYSEE generates reverse mode; ADIFOR uses a hybrid forward/reverse mode strategy. <p> Both tools transform Fortran code, applying the rules of automatic differentiation and generating new Fortran code that, when executed, computes derivatives without the overhead associated with "tape interpretation" schemes. ODYSEE generates reverse mode; ADIFOR uses a hybrid forward/reverse mode strategy. ADIFOR (Automatic Differentiation in Fortran) <ref> [3, 7] </ref> provides automatic differentiation for programs written in Fortran 77.
Reference: [4] <author> Christian Bischof, George Corliss, Larry Green, Andreas Griewank, Kara Haigler, and Perry Newman. </author> <title> Automatic differentiation of advanced CFD codes for multidisciplinary design. </title> <journal> Journal on Computing Systems in Engineering, </journal> <volume> 3(6) </volume> <pages> 625-638, </pages> <year> 1992. </year> <month> 17 </month>
Reference-contexts: The code employs grid sequencing, multigrid, and local time stepping to accelerate convergence and efficiently obtain steady-state high Reynolds number turbulent flow solutions. Experiences with ADIFOR on TLNS3D are described in <ref> [4, 18] </ref>. 11 dF/dx * S dF/dx * S dF/dx * S # Processors Average Time Std. <p> More important, Gilbert has shown recently that in the case of 12 contractive fixed-point iterations, the corresponding derivative values also converge to their correct values [17]. We have confirmed the validity of this result on a transsonic fluid dynamics code <ref> [4] </ref>, where the previously used semi-analytic approximation to the derivative of the lift coefficient with respect to the Mach number turned out to be off by 50%. We have extended Gilbert's result to a wider class of iterative schemes, including Broyden's method, and other quasi-Newton schemes.
Reference: [5] <author> Christian Bischof, Larry Green, Kitty Haigler, and Tim Knauff. </author> <title> Parallel calculation of sensitivity derivatives for aircraft design using automatic differentiation. Extended Abstract, </title> <booktitle> submitted to the 5th AIAA/NASA/USAF/ISSMO Symposium on Multidisciplinary Analysis and Optimization, </booktitle> <year> 1994. </year>
Reference-contexts: Employing 15 processors, the same set of sensitivities can now be obtained in just about one hour, a dramatic increase in turnaround time. Details are reported in <ref> [5] </ref>. 4 Differentiation of Iterative Processes Until recently it was not clear under what conditions automatic differentiation could be expected to yield useful derivative values if the evaluation program is not just a straight-line code but contains branches dependent on argument values.
Reference: [6] <author> Christian Bischof and Paul Hovland. </author> <title> Using ADIFOR to compute dense and sparse Jacobians. </title> <note> ADIFOR Working Note #2, </note> <institution> MCS-TM-158, Mathematics and Computer Science Division, Argonne National Laboratory, </institution> <year> 1991. </year>
Reference-contexts: Note that none of the common subexpres-sions x (i) fl x (j) is recomputed in the reverse mode section. ADIFOR-generated code can be used in various ways <ref> [6] </ref>: Instead of simply producing code to compute the Jacobian J , ADIFOR produces code to compute J flS, where the "seed matrix" S is initialized by the user. <p> This effect is most noticeable in the computation of gradients of so-called partially separable functions <ref> [6, 8] </ref>, a rather common class of functions first described by Griewank and Toint [27]. For example, on the 2-D Ginsburg-Landau Superconductivity problem [1], the sparse gradient computation on a 400 fi 400 grid requires only 7 percent of the floating-point operations required for the dense version.
Reference: [7] <author> Christian H. Bischof, Alan Carle, George Corliss, Andreas Griewank, and Paul Hov-land. </author> <title> Getting started with ADIFOR. </title> <note> ADIFOR Working Note #9, </note> <institution> ANL-MCS-TM-164, Mathematics and Computer Science Division, Argonne National Laboratory, </institution> <year> 1992. </year>
Reference-contexts: Both tools transform Fortran code, applying the rules of automatic differentiation and generating new Fortran code that, when executed, computes derivatives without the overhead associated with "tape interpretation" schemes. ODYSEE generates reverse mode; ADIFOR uses a hybrid forward/reverse mode strategy. ADIFOR (Automatic Differentiation in Fortran) <ref> [3, 7] </ref> provides automatic differentiation for programs written in Fortran 77.
Reference: [8] <author> Christian H. Bischof and Moe El-Khadiri. </author> <title> Extending compile-time reverse mode and exploiting partial separability in ADIFOR. </title> <note> ADIFOR Working Note #7, </note> <institution> ANL-MCS-TM-163, Mathematics and Computer Science Division, Argonne National Laboratory, </institution> <year> 1992. </year>
Reference-contexts: This effect is most noticeable in the computation of gradients of so-called partially separable functions <ref> [6, 8] </ref>, a rather common class of functions first described by Griewank and Toint [27]. For example, on the 2-D Ginsburg-Landau Superconductivity problem [1], the sparse gradient computation on a 400 fi 400 grid requires only 7 percent of the floating-point operations required for the dense version.
Reference: [9] <author> Stephen L. Campbell, Edward Moore, and Yangchun Zhong. </author> <title> Utilization of automatic differentiation in control algorithms. </title> <note> To appear in IEEE Trans. Automatic Control, </note> <year> 1993. </year>
Reference-contexts: Consequently, the direct representation and differentiation of the dependent variables in terms of the independent variables are generally very resource demanding or impossible, while they pose no difficulties for automatic differentiation (see, for example, <ref> [9, 28] </ref>). In this section, we give an overview of the classical approaches to automatic differentiation, as well as some of the more recent research questions relating to the computational complexity for computing derivatives.
Reference: [10] <author> T. F. Coleman, B. S. Garbow, and J. J. </author> <title> More. Software for estimating sparse Jacobian matrices. </title> <journal> ACM Trans. Math. Software, </journal> <volume> 10:329 - 345, </volume> <year> 1984. </year>
Reference-contexts: Parallelism In this section we show how we can exploit high-level program structures and parallelism to speed derivative computations. 3.1 Computing Large, Sparse Jacobians In the approximation of large, sparse Jacobians bu divided differences, one usually exploits the sparsity structure of these matrices by simultaneously perturbing several "unrelated" input parameters <ref> [11, 10] </ref>. This structure can also be exploited by a suitable choice of the "seed matrix." The idea is best understood with an example.
Reference: [11] <author> T. F. Coleman and J. J. </author> <title> More. Estimation of sparse Jacobian matrices and graph coloring problems. </title> <journal> SIAM Journal on Numerical Analysis, </journal> <volume> 20:187 - 209, </volume> <year> 1984. </year>
Reference-contexts: Parallelism In this section we show how we can exploit high-level program structures and parallelism to speed derivative computations. 3.1 Computing Large, Sparse Jacobians In the approximation of large, sparse Jacobians bu divided differences, one usually exploits the sparsity structure of these matrices by simultaneously perturbing several "unrelated" input parameters <ref> [11, 10] </ref>. This structure can also be exploited by a suitable choice of the "seed matrix." The idea is best understood with an example.
Reference: [12] <author> George Corliss and Andreas Griewank. </author> <title> Operator overloading as an enabling technology for automatic differentiation. </title> <type> Technical Report MCS-P358-0493, </type> <institution> Mathematics and Computer Science Division, Argonne National Laboratory, </institution> <year> 1993. </year>
Reference-contexts: ADOL-C employs the operator overloading capabilities of the C++ language to transparently augment the original code with computations for derivative of arbitrary order. Operator overloading also allows for a flexible and easily customizable system. These issues are further explored in <ref> [12] </ref>. GRESS, PADRE-2, and ADOL-C implement both the forward and reverse mode. The reverse mode requires that one saves or recomputes all intermediate values that nonlinearly impact the final result, and to this end these tools generate, a trace of the computation.
Reference: [13] <author> Lawrence C. W. Dixon. </author> <title> Use of automatic differentiation for calculating Hessians and Newton steps. </title> <editor> In Andreas Griewank and George F. Corliss, editors, </editor> <title> Automatic Differentiation of Algorithms: Theory, </title> <booktitle> Implementation, and Application, </booktitle> <pages> pages 114 - 125. </pages> <publisher> SIAM, </publisher> <address> Philadelphia, Penn., </address> <year> 1991. </year>
Reference-contexts: Since in most applications Jacobians are used only as a means of solving nonlinear systems iteratively, it may be preferable to directly attack the larger problem of computing Newton-steps or their approximations [20], <ref> [13] </ref>. Like derivatives, many other numerically useful pieces of information about a composite function can be derived from the properties and mutual relations of its elementary constituents. For example, one may determine error bounds, Lipschitz constants, trust region radii, and even parallel evaluation schedules.
Reference: [14] <author> Hans-C. Fischer. </author> <title> Differentiation arithmetic and applications in Pascal-XSC. </title> <booktitle> Poster presented at SIAM Workshop on Automatic Differentiation of Algorithms, </booktitle> <address> Breckenridge, Colo., </address> <month> January </month> <year> 1991. </year>
Reference: [15] <author> Ian Foster, Robert Olson, and Steven Tuecke. </author> <title> Programming in Fortran M. </title> <type> Technical Report ANL-93/26, Rev. 1, </type> <institution> Mathematics and Computer Science Division, Argonne National Laboratory, </institution> <month> October </month> <year> 1993. </year>
Reference-contexts: It can also easily be mapped on any collection of compute nodes, be it a MIMD parallel computer or a heterogeneous workstation network. We implemented this approach with the Fortran M system <ref> [16, 15] </ref>, and the resulting "parallel wrapper" can easily be adapted to other codes. The TLNS3D code [40] is a high-fidelity aerodynamic computer code that solves the time-dependent, 3-D, thin-layer Navier-Stokes equations with a finite-volume formulation.
Reference: [16] <author> Ian T. Foster and K. Mani Chandy. </author> <title> Fortran M: A language for modular parallel programming. </title> <type> Technical Report MCS-P327-0992, </type> <institution> Mathematics and Computer Science Division, Argonne National Laboratory, </institution> <year> 1992. </year>
Reference-contexts: It can also easily be mapped on any collection of compute nodes, be it a MIMD parallel computer or a heterogeneous workstation network. We implemented this approach with the Fortran M system <ref> [16, 15] </ref>, and the resulting "parallel wrapper" can easily be adapted to other codes. The TLNS3D code [40] is a high-fidelity aerodynamic computer code that solves the time-dependent, 3-D, thin-layer Navier-Stokes equations with a finite-volume formulation.
Reference: [17] <author> Jean-Charles Gilbert. </author> <title> Automatic differentiation and iterative processes. </title> <journal> Optimization Methods and Software, </journal> <volume> 1(1) </volume> <pages> 13-22, </pages> <year> 1992. </year> <month> 18 </month>
Reference-contexts: More important, Gilbert has shown recently that in the case of 12 contractive fixed-point iterations, the corresponding derivative values also converge to their correct values <ref> [17] </ref>. We have confirmed the validity of this result on a transsonic fluid dynamics code [4], where the previously used semi-analytic approximation to the derivative of the lift coefficient with respect to the Mach number turned out to be off by 50%.
Reference: [18] <author> Lawrence Green, Perry Newman, and Kara Haigler. </author> <title> Sensitivity derivatives for advanced CFD algorithm and viscous modeling parameters via automatic differentiation. </title> <booktitle> In Proceedings of the 11th AIAA Computational Fluid Dynamics Conference, </booktitle> <institution> AIAA Paper 93-3321. American Institute of Aeronautics and Astronautics, </institution> <year> 1993. </year>
Reference-contexts: The code employs grid sequencing, multigrid, and local time stepping to accelerate convergence and efficiently obtain steady-state high Reynolds number turbulent flow solutions. Experiences with ADIFOR on TLNS3D are described in <ref> [4, 18] </ref>. 11 dF/dx * S dF/dx * S dF/dx * S # Processors Average Time Std.
Reference: [19] <author> Andreas Griewank. </author> <title> On automatic differentiation. </title> <booktitle> In Mathematical Programming: Recent Developments and Applications, </booktitle> <pages> pages 83-108, </pages> <address> Amsterdam, 1989. </address> <publisher> Kluwer Academic Publishers. </publisher>
Reference-contexts: Hence the reverse mode has a lower operations count on this example. Unfortunately, apart from the computation of gradients, for which the reverse mode is optimal <ref> [19] </ref>, there is no reliable rule to determine a priori whether the forward or reverse mode is better, and there are in fact even more variations on the chain rule.
Reference: [20] <author> Andreas Griewank. </author> <title> Direct calculation of Newton steps without accumulating Jacobians. </title> <editor> In T. F. Coleman and Y. Li, editors, </editor> <booktitle> Large-Scale Numerical Optimization, </booktitle> <pages> pages 115-137, </pages> <address> Philadelphia, Pa., </address> <year> 1990. </year> <note> SIAM. </note>
Reference-contexts: Since in most applications Jacobians are used only as a means of solving nonlinear systems iteratively, it may be preferable to directly attack the larger problem of computing Newton-steps or their approximations <ref> [20] </ref>, [13]. Like derivatives, many other numerically useful pieces of information about a composite function can be derived from the properties and mutual relations of its elementary constituents. For example, one may determine error bounds, Lipschitz constants, trust region radii, and even parallel evaluation schedules.
Reference: [21] <author> Andreas Griewank. </author> <title> Achieving logarithmic growth of temporal and spatial complexity in reverse automatic differentiation. </title> <journal> Optimization Methods & Software, </journal> <volume> 1(1) </volume> <pages> 35-54, </pages> <year> 1992. </year>
Reference-contexts: We also note that although it seems as if the reverse mode requires storage that is proportional to the runtime of the undifferentiated code, Griewank <ref> [21] </ref> has shown that much more reasonable compromises between temporal and spatial complexity can be achieved by a recursive checkpointing approach. 2.2 Variations of Derivative Accumulation In the "explicit" expressions on the right margins of the informal program listed above we have deliberately used the intermediate values rather than their expansion,
Reference: [22] <author> Andreas Griewank. </author> <title> Some bounds on the complexity of gradients, Jacobians, and Hes-sians. </title> <type> Technical Report MCS-P355-0393, </type> <institution> Mathematics and Computer Science Division, Argonne National Laboratory, </institution> <year> 1993. </year>
Reference-contexts: While this example is merely of academic interest, there are certain classes of discretized PDEs for which the Markowitz heuristic is a good one [26]. In general, however, these various accumulation strategies may exhibit vastly differing computational complexity. These complexity issues are further explored in <ref> [22] </ref>. The computational graphs from which the corresponding Jacobians can be obtained by successively eliminating all intermediate vertices contains one node for each arithmetic operation or intrinsic function call.
Reference: [23] <author> Andreas Griewank, Christian Bischof, George Corliss, Alan Carle, and Karen Williamson. </author> <title> Derivative convergence of iterative equation solvers. </title> <type> Technical Report MCS-P333-1192, </type> <institution> Mathematics and Computer Science Division, Argonne National Laboratory, </institution> <year> 1992. </year>
Reference-contexts: A typical situation is depicted in Fig. 9 for a parameter estimation problem with three parameters x and a 40-dimensional state vector v. Details are reported in <ref> [23] </ref>.
Reference: [24] <author> Andreas Griewank and George Corliss. </author> <title> Automatic Differentiation of Algorithms. </title> <publisher> SIAM, </publisher> <address> Philadelphia, Penn., </address> <year> 1991. </year>
Reference-contexts: Historically, this is due both to a lack of general-purpose tools and a lack of scientific communication and cooperation. It was not until 1991 that the first meeting focusing on Computational Differentiation was held <ref> [24] </ref>. Since then, much has changed as automatic differentiation tools have acquired a level of maturity that enables them to deliver the promises of the theory.
Reference: [25] <author> Andreas Griewank, David Juedes, and Jay Srinivasan. ADOL-C, </author> <title> a package for the automatic differentiation of algorithms written in C/C++. </title> <type> Technical Report MCS-P180-1190, </type> <institution> Mathematics and Computer Science Division, Argonne National Laboratory, </institution> <year> 1990. </year>
Reference-contexts: Unfortunately, no convincing alternative terminology has yet emerged. 2.3 Automatic Differentiation Tools There have been various implementations of automatic differentiation, and an extensive survey can be found in [31]. In particular, we mention GRESS [30] and PADRE-2 [34] for Fortran Programs, and ADOL-C <ref> [25] </ref> for C and C++ programs. ADOL-C employs the operator overloading capabilities of the C++ language to transparently augment the original code with computations for derivative of arbitrary order. Operator overloading also allows for a flexible and easily customizable system. These issues are further explored in [12].
Reference: [26] <author> Andreas Griewank and Shawn Reese. </author> <title> On the calculation of Jacobian matrices by the Markowitz rule. </title> <editor> In Andreas Griewank and George F. Corliss, editors, </editor> <title> Automatic Differentiation of Algorithms: Theory, </title> <booktitle> Implementation, and Application, </booktitle> <pages> pages 126 - 135. </pages> <publisher> SIAM, </publisher> <address> Philadelphia, Penn., </address> <year> 1991. </year>
Reference-contexts: It has been shown that automatic differentiation is backward stable in the sense that the derivative values obtained correspond to the exact results for slightly perturbed independent variables and intermediates <ref> [26] </ref>. As an example let us consider the following simple program with four independent variables x 1 ; x 2 ; x 3 ; x 4 and three dependent variables y 1 ; y 2 ; y 3 , where the c i and d i are distinct constants. <p> While this example is merely of academic interest, there are certain classes of discretized PDEs for which the Markowitz heuristic is a good one <ref> [26] </ref>. In general, however, these various accumulation strategies may exhibit vastly differing computational complexity. These complexity issues are further explored in [22]. The computational graphs from which the corresponding Jacobians can be obtained by successively eliminating all intermediate vertices contains one node for each arithmetic operation or intrinsic function call.
Reference: [27] <author> Andreas Griewank and Philippe L. Toint. </author> <title> Partitioned variable metric updates for large structured optimization problems. </title> <journal> Numerische Mathematik, </journal> <volume> 39 </volume> <pages> 119-137, </pages> <year> 1982. </year>
Reference-contexts: This effect is most noticeable in the computation of gradients of so-called partially separable functions [6, 8], a rather common class of functions first described by Griewank and Toint <ref> [27] </ref>. For example, on the 2-D Ginsburg-Landau Superconductivity problem [1], the sparse gradient computation on a 400 fi 400 grid requires only 7 percent of the floating-point operations required for the dense version.
Reference: [28] <author> Uli Hauermann. </author> <title> Automatische Differentiation zur Rekursiven Bestimmung von Par-tiellen Ableitungen. </title> <institution> STUD-102, Institut B fur Mechanik, Universitat Stuttgart, </institution> <year> 1993. </year>
Reference-contexts: Consequently, the direct representation and differentiation of the dependent variables in terms of the independent variables are generally very resource demanding or impossible, while they pose no difficulties for automatic differentiation (see, for example, <ref> [9, 28] </ref>). In this section, we give an overview of the classical approaches to automatic differentiation, as well as some of the more recent research questions relating to the computational complexity for computing derivatives.
Reference: [29] <author> Kieran Herley. </author> <title> On the NP-completeness of optimum accumulation by vertex elimination. </title> <type> Unpublished manuscript, </type> <year> 1993. </year>
Reference-contexts: It is therefore not surprising that the Jacobian accumulation problem can be shown to be NP-complete as welll <ref> [29] </ref>. Just as in the sparse matrix case, the development of near-optimal heuristics for accumulating Jacobians by successively eliminating intermediates provides a rich field for future research.
Reference: [30] <author> Jim E. Horwedel. GRESS: </author> <title> A preprocessor for sensitivity studies on Fortran programs. </title> <editor> In Andreas Griewank and George F. Corliss, editors, </editor> <title> Automatic Differentiation of Algorithms: Theory, </title> <booktitle> Implementation, and Application, </booktitle> <pages> pages 243 - 250. </pages> <publisher> SIAM, </publisher> <address> Philadelphia, Penn., </address> <year> 1991. </year>
Reference-contexts: Unfortunately, no convincing alternative terminology has yet emerged. 2.3 Automatic Differentiation Tools There have been various implementations of automatic differentiation, and an extensive survey can be found in [31]. In particular, we mention GRESS <ref> [30] </ref> and PADRE-2 [34] for Fortran Programs, and ADOL-C [25] for C and C++ programs. ADOL-C employs the operator overloading capabilities of the C++ language to transparently augment the original code with computations for derivative of arbitrary order. Operator overloading also allows for a flexible and easily customizable system.
Reference: [31] <author> David Juedes. </author> <title> A taxonomy of automatic differentiation tools. </title> <editor> In Andreas Griewank and George Corliss, editors, </editor> <booktitle> Proceedings of the Workshop on Automatic Differentiation of Algorithms: Theory, Implementation, and Application, </booktitle> <address> Philadelphia, </address> <year> 1991. </year> <note> SIAM. 19 </note>
Reference-contexts: Unfortunately, no convincing alternative terminology has yet emerged. 2.3 Automatic Differentiation Tools There have been various implementations of automatic differentiation, and an extensive survey can be found in <ref> [31] </ref>. In particular, we mention GRESS [30] and PADRE-2 [34] for Fortran Programs, and ADOL-C [25] for C and C++ programs. ADOL-C employs the operator overloading capabilities of the C++ language to transparently augment the original code with computations for derivative of arbitrary order.
Reference: [32] <author> G. Kedem. </author> <title> Automatic differentiation of computer programs. </title> <journal> ACM Trans. Math. Soft--ware, </journal> <volume> 6(2):150 - 165, </volume> <month> June </month> <year> 1980. </year>
Reference: [33] <author> V. M. Korivi, A. C. Taylor, P. A. Newman, G. W. Hou, and H. E. Jones. </author> <title> An incremental strategy for calculating consistent discrete CFD sensitivity derivatives. </title> <type> NASA Technical Memorandum 104207, </type> <institution> NASA Langley Research Center, </institution> <month> February </month> <year> 1992. </year>
Reference-contexts: The resulting simplified derivative recurrence ~v 0 k P k [F v (v k ; x) ~v 0 is in general cheaper to perform and corresponds to the so-called incremental iterative schemes proposed in the context of CFD sensitivity analysis <ref> [35, 33] </ref>.
Reference: [34] <author> Koichi Kubota. PADRE2, </author> <title> a FORTRAN precompiler yielding error estimates and second derivatives. </title> <editor> In Andreas Griewank and George F. Corliss, editors, </editor> <title> Automatic Differentiation of Algorithms: Theory, </title> <booktitle> Implementation, and Application, </booktitle> <pages> pages 251 - 262. </pages> <publisher> SIAM, </publisher> <address> Philadelphia, Penn., </address> <year> 1991. </year>
Reference-contexts: Unfortunately, no convincing alternative terminology has yet emerged. 2.3 Automatic Differentiation Tools There have been various implementations of automatic differentiation, and an extensive survey can be found in [31]. In particular, we mention GRESS [30] and PADRE-2 <ref> [34] </ref> for Fortran Programs, and ADOL-C [25] for C and C++ programs. ADOL-C employs the operator overloading capabilities of the C++ language to transparently augment the original code with computations for derivative of arbitrary order. Operator overloading also allows for a flexible and easily customizable system.
Reference: [35] <author> P. A. Newman, G. J.-W. Hou, H. E. Jones, A. C. Taylor, and V. M. Korivi. </author> <title> Observations on computational methodologies for use in large-scale, gradient-based, multidisciplinary design incorporating advanced CFD codes. </title> <type> NASA Technical Memorandum 104206, </type> <institution> NASA Langley Research Center, </institution> <year> 1992. </year>
Reference-contexts: The resulting simplified derivative recurrence ~v 0 k P k [F v (v k ; x) ~v 0 is in general cheaper to perform and corresponds to the so-called incremental iterative schemes proposed in the context of CFD sensitivity analysis <ref> [35, 33] </ref>.
Reference: [36] <author> G. M. Ostrovskii, Ju. M. Wolin, and W. W. Borisov. </author> <title> Uber die Berechnung von Ableitun-gen. </title> <journal> Wissenschaftliche Zeitschrift der Technischen Hochschule fur Chemie, Leuna-Merseburg, </journal> <volume> 13(4):382 - 384, </volume> <year> 1971. </year>
Reference-contexts: The basic forward, or bottom-up, mode of automatic differentiation (AD) was apparently first proposed by R. E. Wengert [41]. The mathematically more interesting reverse, or top-down, mode was first published by G. M. Ostrowskii <ref> [36] </ref>. This observation gives rise to automatic differentiation (AD), which can compute derivatives of a function defined by a computer code in a black-box fashion, without any knowledge of the application beyond what are considered "dependent" and "independent" variables with respect to differentiation.
Reference: [37] <author> Nicole Rostaing, Stephane Dalmas, and Andre Galligo. </author> <title> Automatic differentiation in Odyssee. </title> <address> Tellus, 45a(5):558-568, </address> <month> October </month> <year> 1993. </year>
Reference-contexts: The interpretation overhead of this trace and its potentially very large size can be a serious computational bottleneck [39]. Recently, a "source transformation" approach to automatic differentiation has been explored in the ADIFOR [3] and ODYSSEE <ref> [37] </ref> projects. Both tools transform Fortran code, applying the rules of automatic differentiation and generating new Fortran code that, when executed, computes derivatives without the overhead associated with "tape interpretation" schemes. ODYSEE generates reverse mode; ADIFOR uses a hybrid forward/reverse mode strategy.
Reference: [38] <author> G. R. Shubin, A. B. Stephens, H. M. Glaz, A. B. Wardlaw, and L. B. Hackerman. </author> <title> Steady shock tracking, Newton's method, and the supersonic blunt body problem. </title> <journal> SIAM J. on Sci. and Stat. Computing, </journal> <volume> 3(2) </volume> <pages> 127-144, </pages> <month> June </month> <year> 1982. </year>
Reference-contexts: We can exploit this fact by setting S = B @ 1 0 0 1 C A : For a more realistic example, the 190 fi 190 Jacobian of the blunt body shock-tracking problem described in <ref> [38] </ref> has only 2582 nonzero entries; its structure is shown in Figure 7a. Because of its sparsity structure, it can be condensed into the "compressed Jacobian" shown in Figure 7b, which has only 28 columns.
Reference: [39] <author> Edgar Soulie. </author> <title> User's experience with Fortran compilers for least squares problems. </title> <editor> In Andreas Griewank and George F. Corliss, editors, </editor> <title> Automatic Differentiation of Algorithms: Theory, </title> <booktitle> Implementation, and Application, </booktitle> <pages> pages 297-306. </pages> <publisher> SIAM, </publisher> <address> Philadelphia, Penn., </address> <year> 1991. </year>
Reference-contexts: The reverse mode requires that one saves or recomputes all intermediate values that nonlinearly impact the final result, and to this end these tools generate, a trace of the computation. The interpretation overhead of this trace and its potentially very large size can be a serious computational bottleneck <ref> [39] </ref>. Recently, a "source transformation" approach to automatic differentiation has been explored in the ADIFOR [3] and ODYSSEE [37] projects. Both tools transform Fortran code, applying the rules of automatic differentiation and generating new Fortran code that, when executed, computes derivatives without the overhead associated with "tape interpretation" schemes.
Reference: [40] <author> V. N. Vatsa and B. W. Wedan. </author> <title> Development of a multigrid code for 3-D Navier-Stokes equations and its application to a grid-refinement study. </title> <journal> Computers & Fluids, </journal> <volume> 18(4) </volume> <pages> 391-403, </pages> <year> 1990. </year>
Reference-contexts: It can also easily be mapped on any collection of compute nodes, be it a MIMD parallel computer or a heterogeneous workstation network. We implemented this approach with the Fortran M system [16, 15], and the resulting "parallel wrapper" can easily be adapted to other codes. The TLNS3D code <ref> [40] </ref> is a high-fidelity aerodynamic computer code that solves the time-dependent, 3-D, thin-layer Navier-Stokes equations with a finite-volume formulation. The code employs grid sequencing, multigrid, and local time stepping to accelerate convergence and efficiently obtain steady-state high Reynolds number turbulent flow solutions.
Reference: [41] <author> R. E. Wengert. </author> <title> A simple automatic derivative evaluation program. </title> <journal> Comm. ACM, </journal> <volume> 7(8) </volume> <pages> 463-464, </pages> <year> 1964. </year>
Reference-contexts: Almost as soon as the first programmable systems became available, scientists observed that the mechanical application of the chain rule to obtain derivatives can be automatically and reliably performed by computers. The basic forward, or bottom-up, mode of automatic differentiation (AD) was apparently first proposed by R. E. Wengert <ref> [41] </ref>. The mathematically more interesting reverse, or top-down, mode was first published by G. M. Ostrowskii [36].
References-found: 41

