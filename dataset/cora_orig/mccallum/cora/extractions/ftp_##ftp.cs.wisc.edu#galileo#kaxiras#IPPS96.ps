URL: ftp://ftp.cs.wisc.edu/galileo/kaxiras/IPPS96.ps
Refering-URL: http://www.cs.wisc.edu/~kaxiras/kaxiras.html
Root-URL: 
Email: kaxiras@cs.wisc.edu  
Title: Kiloprocessor Extensions to SCI  
Author: Stefanos Kaxiras 
Address: 1210 West Dayton Street, Madison, Wisconsin 53706 USA  
Affiliation: Computer Sciences Department, University of Wisconsin-Madison  
Abstract: This work was supported in part by NSF Grants CCR-9207971 and CCR-9509589, funding from the Apple Computer Advanced Technology Group, and donations from Thinking Machines Corporation. Our T.M.C. CM-5 was purchased through NSF Grant CDA-9024618, with matching funding from the University of Wisconsin Graduate School. Editor, IEEE P1596.2 proposed Standard. The views expressed in this paper are the authors and do not imply endorsement from IEEE. Abstract To expand the Scalable Coher ent Interfaces (SCI) capabilities so it can be used to efficiently handle sharing in systems of hundreds or even thousands of processors, the SCI working group is developing the Kilopr ocessor Extensions to SCI. In this paper we describe the pr oposed GLOW and STEM kilopr ocessor extensions to SCI. These two sets of extensions provide SCI with scalable reads and scalable writes to widely-shar ed data. This kind of datum represents one of the main obstacles to scalability for many cache coherence protocols. The GLOW extensions are intended for systems with complex networks of inter connected SCI rings, (e.g., lar ge networks of workstations). GLOW extensions are based on building k-ary sharing trees that map well to the underlying topology. In contrast, STEM is intended for systems where GLOW is not applicable (e.g., topologies based on centralized switches). STEM defines algorithms to build and maintain binary sharing trees. We show that latencies of GLOW reads and writes gr ow only logarithmically with the number of nodes sharing, in contrast to SCI wher e latencies grow linearly, therefore validating GLOW as a good solution to efficient wide sharing of data. Previous work showed the same for STEM. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <institution> IEEE Standard for Scalable Coherent Interface (SCI) 1596 1992, </institution> <note> IEEE 1993. </note>
Reference-contexts: The ANSI/IEEE Standard 1596 Scalable Coherent Interface (SCI) <ref> [1] </ref> defines a cache coherence protocol based on distributed sharing lists. T o improve the performance for widely-shared data in SCI, the GLOW [4] and STEM [2] kiloprocessor extensions are developed. Both are designed to handle accesses to widely-shared data and provide good scalability to lar ge numbers of processors.
Reference: [2] <author> Ross E. Johnson, </author> <title> Extending the Scalable Coherent Interface for Lar ge-Scale Shared-Memory Multiprocessors, </title> <type> PhD Thesis, </type> <institution> University of Wisconsin-Madison, </institution> <year> 1993. </year>
Reference-contexts: The ANSI/IEEE Standard 1596 Scalable Coherent Interface (SCI) [1] defines a cache coherence protocol based on distributed sharing lists. T o improve the performance for widely-shared data in SCI, the GLOW [4] and STEM <ref> [2] </ref> kiloprocessor extensions are developed. Both are designed to handle accesses to widely-shared data and provide good scalability to lar ge numbers of processors. To enable scalability of programs, both scalable reads and scalable writes for widely-shared data are essential. <p> When the agent finds itself childless and tail in its list, it will invalidate and roll out from its upstream neighbor, freeing that to invalidate itself. 4 STEM kiloprocessor extensions to SCI The STEM extensions to SCI, originally developed by John-son <ref> [2] </ref>, provide a logarithmic-time algorithm to build, maintain and invalidate a binary sharing tree (in contrast to GLOW s k-ary trees) without regard to the topology of the interconnection network. STEM employs combining in the interconnect to provide scalable reads. <p> To generate balanced trees, nodes three and two should be merged if nodes one and zero are being mer ged. Techniques for selecting the nodes to be mer ged are discussed by James [14] and by Johnson <ref> [2] </ref>. Following the mer ging, data are distributed to the nodes. Data are propagated by cacheto-cache writes, as illustrated in figure 9 D. Some of these transactions can take place concurrently with the sharing-tree creation. 4.2 STEM rollout Rollout in STEM occurs for the same reasons as for SCI caches. <p> We have microbenchmarked reads and writes in various simulated systems. The STEM extensions have not yet been implemented in full detail. However , Johnson simulated the behavior of STEM and in his thesis <ref> [2] </ref> he shows that indeed its performance for both reads and writes is O (logN) where N is the number of nodes sharing.
Reference: [3] <author> Gregory F. Pfister and V. Alan Norton, </author> <title> Hot Spot Conten tion and Combining in Multistage Interconnection Net works. </title> <booktitle> Proc. of the 1985 International Confer ence on Parallel Processing, </booktitle> <pages> pp. 790797, </pages> <month> August </month> <year> 2023 1985. </year>
Reference-contexts: Instead, when the agent is invoked, it generates its own request and sends it toward the remote memory directory . The memory directory sees only the requests of the first level of agents. Such behavior has two effects: First, it eliminates memory hot spots <ref> [3] </ref>. Second, because the requests are satisfied locally , messages travel only short distances, thus decreasing the load on the network. <p> Combining depends heavily on traffic patterns, and in fact will rarely occur except in the presence of substantial congestion <ref> [3] </ref>. Combining only returns list-pointer information (and not data, which are returned later). This is much simpler than other approaches [5], which leave residual state within the interconnect for modifying the responses when they return. STEM defines one additional pointer for the SCI caches.
Reference: [4] <author> Stefanos Kaxiras and James R. Goodman, </author> <title> Implementation and Performance of the GLOW Kiloprocessor Extensions to SCI on the Wisconsin Wind Tunnel. </title> <booktitle> Proc. of the 2nd International Workshop on SCI-Based High-Performance Low Cost Computing, </booktitle> <month> March </month> <year> 1995. </year>
Reference-contexts: The ANSI/IEEE Standard 1596 Scalable Coherent Interface (SCI) [1] defines a cache coherence protocol based on distributed sharing lists. T o improve the performance for widely-shared data in SCI, the GLOW <ref> [4] </ref> and STEM [2] kiloprocessor extensions are developed. Both are designed to handle accesses to widely-shared data and provide good scalability to lar ge numbers of processors. To enable scalability of programs, both scalable reads and scalable writes for widely-shared data are essential.
Reference: [5] <author> A. Gottlieb, R. Grishman, C.P . Kruskal, K.P. McAuliffe, L. Rudolph, M. Snir, </author> <title> The NYU Ultracomputer Designing a MIMD Shared-Memory Parallel Computer . IEEE Trans. </title> <journal> on Computers, </journal> <volume> Vol. C-32, no 2, </volume> <pages> pp. 175189, </pages> <month> Feb. </month> <year> 1983. </year>
Reference-contexts: Both are designed to handle accesses to widely-shared data and provide good scalability to lar ge numbers of processors. To enable scalability of programs, both scalable reads and scalable writes for widely-shared data are essential. Request combining, originally proposed for the NYU Ultracomputer <ref> [5] </ref>, is the main vehicle for achieving scalable reads. The ef fect of request combining is achieved ef ficiently in GLOW because of the nature of the protocol itself, which caches certain information in the network. <p> Combining depends heavily on traffic patterns, and in fact will rarely occur except in the presence of substantial congestion [3]. Combining only returns list-pointer information (and not data, which are returned later). This is much simpler than other approaches <ref> [5] </ref>, which leave residual state within the interconnect for modifying the responses when they return. STEM defines one additional pointer for the SCI caches. The representation (triangle) of a STEM cache (devised by James [14]) is shown in figure 8.
Reference: [6] <author> Alain Kgi, Nagi Aboulenein, Douglas C. Bur ger, James R. Goodman, </author> <title> Techniques for Reducing Overheads of Shared-Memory Multiprocessing. </title> <booktitle> International Conference on SuperComputing, </booktitle> <month> July </month> <year> 1995. </year>
Reference-contexts: The simulations were performed with the assumption of unit-latency messages. 5.1 GLOW in the Wisconsin Wind Tunnel (WWT) We implemented GLOW on the WWT [7] and, in particular , as extensions to the WWT SCI simulator <ref> [6] </ref>. WWT is a parallel discrete event simulator that runs on a Thinking Machines CM-5. It executes application programs on the simulated machine with a very small slowdown compared to other simulation methods. The simulated machine was based on a k-ary n-cube topology constructed with SCI rings. <p> The simulated machine was based on a k-ary n-cube topology constructed with SCI rings. W e simulated message latency but we assumed contention only in the endpoints (the rest of the network was contention-free). The simulation parameters were the same as those used in a previous study <ref> [6] </ref>. 5.2 Micro-benchmarks We measured the average read and write latency of nodes repeatedly accessing shared variables for both SCI and GLOW. We simulated systems ranging from 32 to 256 nodes in two, three and four dimensional k-ary n-cubes.
Reference: [7] <author> Steven K. Reinhardt, Mark D. Hill, James R. Larus, Alvin R. Lebeck, James C. Lewis, and David A. Wood, </author> <title> The Wis consin Wind Tunnel: Virtual Prototyping of Parallel Computers. </title> <booktitle> Proc. of the 1993 ACM SIGMETRICS Conference on Measurements and Modeling of Computer Systems , pp. </booktitle> <volume> 4860, </volume> <month> May </month> <year> 1993. </year>
Reference-contexts: The simulations were performed with the assumption of unit-latency messages. 5.1 GLOW in the Wisconsin Wind Tunnel (WWT) We implemented GLOW on the WWT <ref> [7] </ref> and, in particular , as extensions to the WWT SCI simulator [6]. WWT is a parallel discrete event simulator that runs on a Thinking Machines CM-5. It executes application programs on the simulated machine with a very small slowdown compared to other simulation methods.
Reference: [8] <author> Yeong-Chang Maa, Dhiraj K. Pradhan, Dominique Thie baut, </author> <title> Two Economical Directory Schemes for Lar ge-Scale Cache-Coherent Multiprocessors. </title> <journal> Computer Architecture News, </journal> <volume> Vol. 19, No. 5, </volume> <pages> pp. 1018, </pages> <month> September </month> <year> 1991. </year>
Reference-contexts: The request combining that STEM uses does not require information to be stored in the network. For scalable writes, the traditional approach is to devise sharing tree protocols. Examples include the Scalable T ree Protocol (STP) [9], the T ree Directory (TD), and the Hierarchical Full-Map Directory (HFMD) <ref> [8] </ref>. GLOW is based on k-ary sharing trees, while STEM defines binary sharing trees. GLOW and STEM are optimized for different types of target system. Each has its own advantages that may be better realized in a specific class of SCI environments.
Reference: [9] <author> Hfikan Nilsson, Per Stenstrm, </author> <title> The Scalable T ree Protocol a Cache Coherence Approach for Lar ge-Scale Multiprocessors. </title> <booktitle> 4th IEEE Symposium on Parallel and Distributed Processing, </booktitle> <pages> pp. 498506, </pages> <year> 1992. </year>
Reference-contexts: The request combining that STEM uses does not require information to be stored in the network. For scalable writes, the traditional approach is to devise sharing tree protocols. Examples include the Scalable T ree Protocol (STP) <ref> [9] </ref>, the T ree Directory (TD), and the Hierarchical Full-Map Directory (HFMD) [8]. GLOW is based on k-ary sharing trees, while STEM defines binary sharing trees. GLOW and STEM are optimized for different types of target system.
Reference: [10] <author> Daniel Lenoski et al., </author> <title> The Stanford DASH Multiproces sor. </title> <journal> IEEE Computer, </journal> <volume> Vol. 25 No. 3, </volume> <pages> pp. 63-79, </pages> <month> March </month> <year> 1992. </year>
Reference-contexts: T opologies can be built either by using multiple bridges, each connecting a small number of rings, or centralized switches, each connecting many rings. SCI also defines a distributed directory-based cache-coherence protocol. In contrast to most other directory-based protocols ( e.g., DASH <ref> [10] </ref>), which keep all the directory information in memory , SCI distributes the directory information among the sharing nodes in a doubly linked sharing list. The sharing list is stored with the cache lines throughout the system.
Reference: [11] <author> Steven L. Scott, James R. Goodman, Mary K. Vernon, </author> <title> Performance of the SCI Ring. </title> <booktitle> Proc. of the 19th Annual International Symposium on Computer Architecture, </booktitle> <pages> pp. 403 414, </pages> <month> May </month> <year> 1992. </year>
Reference-contexts: It defines both a network interface and a cache-coherence protocol. The network interface section of SCI defines a 1Gbyte/s ring interconnect, and the transactions that can be generated over it. A performance analysis by Scott, Ver-non, and Goodman <ref> [11] </ref> showed that an SCI ring can accommodate small numbers of high-performance nodes, in the range of four to eight. T o build lar ger systems, topologies constructed out of smaller rings must be used ( e.g., k-ary n-cubes, multistage topologies) [13].
Reference: [12] <author> David Chaiken, John Kubiatowicz, Anant Agarwal, </author> <title> Limit LESS Directories: A Scalable Cache Coherence Scheme. </title> <booktitle> Proc. of the 4th International Conference on Architectural Support for Programming Languages and Operating Sys tems, </booktitle> <pages> pp. 224234, </pages> <month> April </month> <year> 1991. </year>
Reference-contexts: 1 Introduction For most cache coherence protocols, widely-shared data represent one of the major obstacles to scalability to large numbers of processors. This type of data does not appear in abundance in application programs <ref> [12] </ref>. However , accessing these data is expensive and, worse yet, it becomes progressively more expensive as the size of the parallel computer increases.
Reference: [13] <author> Ross E. Johnson, James R. Goodman, </author> <title> Interconnect Topologies with Point-to-Point Rings. </title> <booktitle> Proc. of the International Conference on Parallel Processing, </booktitle> <month> August </month> <year> 1992. </year>
Reference-contexts: A performance analysis by Scott, Ver-non, and Goodman [11] showed that an SCI ring can accommodate small numbers of high-performance nodes, in the range of four to eight. T o build lar ger systems, topologies constructed out of smaller rings must be used ( e.g., k-ary n-cubes, multistage topologies) <ref> [13] </ref>. T opologies can be built either by using multiple bridges, each connecting a small number of rings, or centralized switches, each connecting many rings. SCI also defines a distributed directory-based cache-coherence protocol.

References-found: 13

