URL: http://www-wavelet.eecs.berkeley.edu/~wychan/MSreport.ps.Z
Refering-URL: http://www-wavelet.eecs.berkeley.edu/~wychan/
Root-URL: http://www.cs.berkeley.edu
Title: Lossy Compression of Individual Signals based on One Pass Codebook Adaptation  
Author: by Christopher Chan 
Degree: Submitted in partial fulfillment of the requirements for the degree of Master of Science, Plan II in Electrical Engineering and Computer Sciences in the GRADUATE DIVISION of the UNIVERSITY of CALIFORNIA at BERKELEY Committee in charge: Professor  Advisor Professor Avideh Zakhor  
Date: 1995  
Affiliation: Research Project  Martin Vetterli, Research  
Abstract-found: 0
Intro-found: 1
Reference: [1] <author> T. C. Bell, J. G. Cleary, and I. H. Witten, </author> <title> Text Compression. </title> <address> Englewood Cliffs, NJ: </address> <publisher> Prentice-Hall, </publisher> <year> 1990. </year>
Reference-contexts: Entropy codes such as Huffman code, Shannon-Fano-Elias code, or arithmetic code can be used for this purpose. When estimating the distribution from fn (k) (k) M g, the zero frequency prob lem <ref> [1] </ref> arises: if n (k) j = 0 for some j, should we infer that the j-th cell is of zero probability, and assign an infinite length index code to it (or simply delete it)? Or should we assume that the set of samples is not of sufficient size? In Section
Reference: [2] <author> R. F. Chang, W. T. Chen, and J. S. Wang, </author> <title> "Image sequence coding using adaptive nonuniform tree-structured vector quantization.," </title> <journal> Journal of Visual Comm. and Image Representation, </journal> <volume> vol. 2, </volume> <pages> pp. 166-176, </pages> <month> June </month> <year> 1991. </year>
Reference-contexts: Hence, a one pass adaptive tree-structured VQ algorithm would be an interesting and worthwhile entity to pursue. The structure of the codebook, however, poses new complications to the problem. Some previous works of this flavor are <ref> [2, 3] </ref>, which reorganize the structure of the codebook subtree selected from a large pre-designed fixed complete tree-structured codebook at an update interval. These works do not involve sending of side information. Another direction of future work is a vector quantization scheme with variable vector dimensions.
Reference: [3] <author> R. F. Chang, W. T. Chen, and J. S. Wang, </author> <title> "Image sequence coding using adaptive tree-structured vector quantisation with multipath searching," </title> <booktitle> IEE Proc. </booktitle> <volume> I, vol. 139, </volume> <pages> pp. 9-14, </pages> <month> February </month> <year> 1992. </year>
Reference-contexts: Hence, a one pass adaptive tree-structured VQ algorithm would be an interesting and worthwhile entity to pursue. The structure of the codebook, however, poses new complications to the problem. Some previous works of this flavor are <ref> [2, 3] </ref>, which reorganize the structure of the codebook subtree selected from a large pre-designed fixed complete tree-structured codebook at an update interval. These works do not involve sending of side information. Another direction of future work is a vector quantization scheme with variable vector dimensions.
Reference: [4] <author> P. A. Chou, T. Lookabaugh, and R. M. Gray, </author> <title> "Entropy-constrained vector quantization," </title> <journal> IEEE Trans. Acoust., Speech and Sig. Proc., </journal> <volume> vol. 37, </volume> <pages> pp. 31-42, </pages> <month> Jan-uary </month> <year> 1989. </year>
Reference-contexts: For a variable rate vector quantizer, codes for the codevector indices are not of the same length, hence the strategy mentioned above that only minimizes distortion may not be optimal in the rate-distortion sense. Using entropy-constrained vector quantization (ECVQ) <ref> [4] </ref>, vector quantizers can be designed which have minimum distortion subject to an entropy constraint. It is based on minimizing the Lagrangian cost function J = R + D. The two step iterative codebook improvement algorithm for fixed rate VQ becomes a three step process: 1. <p> Send side information for codevector movements. 4. Also at intervals of L, update the index codes s j for the codevectors. 12 2.4 Codebook Adaptation In <ref> [4] </ref>, Chou et al. introduced the entropy-constrained vector quantization (ECVQ) algorithm to design vector quantizers that minimize average distortion subject to an entropy constraint. Refer back to Section 1.3 for the three steps involved in each iteration of the algorithm. The algorithm iterates these three steps until convergence is reached. <p> In that case, the equality in (2.15) no longer holds. In our experiments, however, (2.15) is still being used. In Section 3, we will show that the exact formulation of these decision equations is relatively unimportant. As before, a codevector is updated if (2.5) holds. 2.4.4 Deleting Codevectors In <ref> [4] </ref>, codevectors whose cells are unpopulated after several iterations are effectively deleted from the codebook, since the entropy coder will assign an infinite length code to its index.
Reference: [5] <author> T. M. Cover and J. A. Thomas, </author> <title> Elements of Information Theory. </title> <address> New York: </address> <publisher> Wiley, </publisher> <year> 1991. </year>
Reference: [6] <author> I. Csiszar and J. Korner, </author> <title> Information Theory: Coding Theorems for Discrete Memoryless Systems. </title> <address> New York: </address> <publisher> Academic Press, </publisher> <year> 1981. </year>
Reference: [7] <author> M. Effros, P. A. Chou, and R. M. Gray, </author> <title> "One-pass adaptive universal vector quantization," </title> <booktitle> in Proceedings of ICASSP'94, </booktitle> <volume> vol. 5, </volume> <pages> pp. 625-628, </pages> <year> 1994. </year>
Reference-contexts: Algorithms taking this approach are called two pass algorithms. In [10], a VQ codebook is modified by splitting codevectors with the largest partial distortion in encoding a source signal, and running the LBG algorithm on the same signal to determine the final locations of the new codevectors. In <ref> [7] </ref>, the encoder finds 3 the optimal codebook for the part of the source signal seen thus far at increasing intervals, and refines its initial codebook by transmitting codebook changes with increasing accuracy.
Reference: [8] <author> S. Geman and D. Geman, </author> <title> "Stochastic relaxation, gibbs distribution, and the bayesian restoration of images," </title> <journal> IEEE Trans Pattern Anal. and Mach. Int., </journal> <volume> vol. 11(6), </volume> <pages> pp. 689-691, </pages> <year> 1984. </year> <month> 39 </month>
Reference-contexts: These iterative algorithms, although guaranteed to converge, do not always converge to the optimal quantizer. More likely than not, they will go to some local minima. Hence, additional techniques have been introduced to evade local minima, such as simulated annealing <ref> [8, 21] </ref>, but they add to the computational complexity significantly. Another problem in designing the quantizer using these iterative algorithms is 2 that the distribution of the source X is unknown.
Reference: [9] <author> A. Gersho and R. M. Gray, </author> <title> Vector Quantization and Signal Compression. </title> <address> Nor-wood, MA: </address> <publisher> Kluwer, </publisher> <year> 1992. </year>
Reference-contexts: These codes, however, assumes a priori the knowledge of the source distribution. And even with this assumption, the design of the code is a non-trivial process. For instance, in the vector quantization (VQ) framework, some necessary optimality conditions have been proved <ref> [9] </ref>. If a vector quantizer Q fl is optimal over any vector quantizer Q with a particular vector dimension `, then it must satisfy these conditions. Two well known optimality conditions are the nearest neighbor condition and the centroid condition. <p> Two well known optimality conditions are the nearest neighbor condition and the centroid condition. Based on these conditions, various versions of an iterative method for designing quantizers have been introduced. They are called variably the generalized Lloyd algorithm <ref> [9] </ref>, k-means algorithm [17], or the Linde-Buzo-Gray (LBG) algorithm [14]. These iterative algorithms, although guaranteed to converge, do not always converge to the optimal quantizer. More likely than not, they will go to some local minima. <p> Hence, c i = E [XjX 2 C i ]: Based on these two optimality conditions, iterative codebook improvement algorithms have been developed <ref> [9, 17, 14] </ref>. Starting from an arbitrary initial quantizer, they alternately optimize the encoder and decoder using the nearest neighbor and centroid conditions respectively. <p> This is more attractive than two-pass adaptive algorithms, which require more computation per source vector due to their iterative nature. 22 With the concern for lower encoding complexity, tree-structured VQ (TSVQ) <ref> [9] </ref> provides an effective way to reduce the search complexity to O (log M=`) per source symbol. A one pass adaptive version of TSVQ would therefore be highly desirable.
Reference: [10] <author> A. Gersho and M. Yano, </author> <title> "Adaptive vector quantization by progressive codevector replacement," </title> <booktitle> in Proceedings of ICASSP'85, </booktitle> <pages> pp. 133-136, </pages> <year> 1985. </year>
Reference-contexts: The first approach involves iterations on the source signal, requiring the encoder to look at the source signal more than once in determining the adaptation of the code. Algorithms taking this approach are called two pass algorithms. In <ref> [10] </ref>, a VQ codebook is modified by splitting codevectors with the largest partial distortion in encoding a source signal, and running the LBG algorithm on the same signal to determine the final locations of the new codevectors. <p> With these assumptions, R = r j 0 r j fl + ^n new D = d ` (x i ; c j fl ) d ` (x i ; c new ) (2.7) 2.4.2 Splitting Codevectors In <ref> [10] </ref>, it is proposed that the codevector with the highest partial distortion should split into two codevectors.
Reference: [11] <author> J. C. Kieffer, </author> <title> "A unified approach to weak universal source coding," </title> <journal> IEEE Trans. Inform. Theory, </journal> <volume> vol. IT-24, </volume> <pages> pp. 674-682, </pages> <month> November </month> <year> 1978. </year>
Reference-contexts: Asymptotically, the code should achieve the optimal performance of the corresponding stationary ergodic source. Previous works in this area fall into two categories: 1) existence proof and code construction, and 2) practical, computationally efficient algorithms. In works by Ziv [25], Neuhoff et al. [18], MacKenthun and Pursley [16], Kieffer <ref> [11] </ref>, Linder et al. [15], Yu and Speed [22] and others, the existence of universal lossy source codes is established under various assumptions about the class of sources, the distortion measure, and the type of convergence of the rates to the rate-distortion bound.
Reference: [12] <author> H. Koga and S. Arimoto, </author> <title> "Asymptotic properties of algorithms of data compression with fidelity criterion based on string matching," </title> <booktitle> in 1994 IEEE Int. Symp. Inform. Theory, </booktitle> <address> p. 264, </address> <year> 1994. </year>
Reference-contexts: Along these lines, Steinberg and Gutman proposed an algorithm that achieves a rate of R (D=2) given an average distortion D &gt; 0 for a large class of sources and distortion measures [20]. Koga and Arimoto <ref> [12] </ref> further proved that the algorithm achieves the rate-distortion bound asymptotically for certain sources and fidelity criteria. These works establish the theoretical foundation and motivation for a lossy version of LZ coding. In this work, a lossy coding algorithm that has the flavor of LZ coding is developed.
Reference: [13] <author> M. Lightstone and S. K. Mitra, </author> <title> "Adaptive vector quantization for image coding in an entropy-constrained framework," </title> <booktitle> in Proc. ICIP-94, </booktitle> <volume> vol. 1, </volume> <pages> pp. 618-622, </pages> <year> 1994. </year>
Reference-contexts: The tradeoff between spending bits on transmitting updates of the codebook and specifying codevector index is addressed in [23], which also involves finding the optimal codebook periodically. Lightstone and Mitra <ref> [13] </ref> then addressed the rate-distortion tradeoff by using the entropy-constrained framework. One common characteristic of these algorithms is that some kind of iterative techniques, such as the LBG algorithm, is involved at certain steps of the algorithms. <p> There are several alternatives to encoding (quantizing) the offsets: 1. Scalar quantization of each component with several scalar quantizers of different resolutions. The quantized version with the best rate-distortion tradeoff is selected, and a selector code that specifies that resolution is transmitted. 2. Vector quantization of the offset. <ref> [13] </ref> has given a more detailed treatment of quantization strategies for this purpose.
Reference: [14] <author> Y. Linde, A. Buzo, and R. M. Gray, </author> <title> "An algorithm for vector quantizer design," </title> <journal> IEEE Trans. Comm., </journal> <volume> vol. COM-28, </volume> <pages> pp. 84-95, </pages> <month> January </month> <year> 1980. </year>
Reference-contexts: Two well known optimality conditions are the nearest neighbor condition and the centroid condition. Based on these conditions, various versions of an iterative method for designing quantizers have been introduced. They are called variably the generalized Lloyd algorithm [9], k-means algorithm [17], or the Linde-Buzo-Gray (LBG) algorithm <ref> [14] </ref>. These iterative algorithms, although guaranteed to converge, do not always converge to the optimal quantizer. More likely than not, they will go to some local minima. <p> Hence, c i = E [XjX 2 C i ]: Based on these two optimality conditions, iterative codebook improvement algorithms have been developed <ref> [9, 17, 14] </ref>. Starting from an arbitrary initial quantizer, they alternately optimize the encoder and decoder using the nearest neighbor and centroid conditions respectively.
Reference: [15] <author> T. Linder, G. Lugosi, and K. Zeger, </author> <title> "Rates of convergence in the source coding theorem, in empirical quantizer design, and in universal lossy source coding," </title> <journal> IEEE Trans. Inform. Theory, </journal> <volume> vol. IT-40, </volume> <pages> pp. 1728-1740, </pages> <month> November </month> <year> 1994. </year>
Reference-contexts: Previous works in this area fall into two categories: 1) existence proof and code construction, and 2) practical, computationally efficient algorithms. In works by Ziv [25], Neuhoff et al. [18], MacKenthun and Pursley [16], Kieffer [11], Linder et al. <ref> [15] </ref>, Yu and Speed [22] and others, the existence of universal lossy source codes is established under various assumptions about the class of sources, the distortion measure, and the type of convergence of the rates to the rate-distortion bound.
Reference: [16] <author> K. M. MacKenthun and M. B. Pursley, </author> <title> "Variable-rate universal block source coding subject to a fidelity constraint," </title> <journal> IEEE Trans. Comm., </journal> <volume> vol. IT-24, </volume> <pages> pp. 340-360, </pages> <month> May </month> <year> 1978. </year>
Reference-contexts: Asymptotically, the code should achieve the optimal performance of the corresponding stationary ergodic source. Previous works in this area fall into two categories: 1) existence proof and code construction, and 2) practical, computationally efficient algorithms. In works by Ziv [25], Neuhoff et al. [18], MacKenthun and Pursley <ref> [16] </ref>, Kieffer [11], Linder et al. [15], Yu and Speed [22] and others, the existence of universal lossy source codes is established under various assumptions about the class of sources, the distortion measure, and the type of convergence of the rates to the rate-distortion bound.
Reference: [17] <author> J. MacQueen, </author> <title> "Some methods for classification and analysis of multivariate observations," </title> <booktitle> in Proc. of the Fifth Berkeley Symposium on Math. Stat. and Prob., </booktitle> <volume> vol. 1, </volume> <pages> pp. 281-296, </pages> <year> 1967. </year>
Reference-contexts: Two well known optimality conditions are the nearest neighbor condition and the centroid condition. Based on these conditions, various versions of an iterative method for designing quantizers have been introduced. They are called variably the generalized Lloyd algorithm [9], k-means algorithm <ref> [17] </ref>, or the Linde-Buzo-Gray (LBG) algorithm [14]. These iterative algorithms, although guaranteed to converge, do not always converge to the optimal quantizer. More likely than not, they will go to some local minima. <p> Hence, c i = E [XjX 2 C i ]: Based on these two optimality conditions, iterative codebook improvement algorithms have been developed <ref> [9, 17, 14] </ref>. Starting from an arbitrary initial quantizer, they alternately optimize the encoder and decoder using the nearest neighbor and centroid conditions respectively.
Reference: [18] <author> D. L. Neuhoff, R. M. Gray, and L. D. Davisson, </author> <title> "Fixed rate universal block source coding with a fidelity criterion," </title> <journal> IEEE Trans. Inform. Theory, </journal> <volume> vol. IT-21, </volume> <pages> pp. 511-523, </pages> <month> September </month> <year> 1975. </year> <month> 40 </month>
Reference-contexts: Asymptotically, the code should achieve the optimal performance of the corresponding stationary ergodic source. Previous works in this area fall into two categories: 1) existence proof and code construction, and 2) practical, computationally efficient algorithms. In works by Ziv [25], Neuhoff et al. <ref> [18] </ref>, MacKenthun and Pursley [16], Kieffer [11], Linder et al. [15], Yu and Speed [22] and others, the existence of universal lossy source codes is established under various assumptions about the class of sources, the distortion measure, and the type of convergence of the rates to the rate-distortion bound.
Reference: [19] <author> A. Ortega and M. Vetterli, </author> <title> "Adaptive quantization without side information," </title> <booktitle> in Proc. ICIP-94, </booktitle> <volume> vol. 3, </volume> <pages> pp. 856-860, </pages> <year> 1994. </year>
Reference-contexts: Steinberg and Gutman [20] proposed an algorithm based on string matching with distortion, which achieves R (D=2) for a large class of stationary sources and distortion measures. Zhang and Wei [24] introduced the "gold-washing" method, which sequentially updates the set of codevectors. In <ref> [19] </ref>, the source distribution function is estimated based on the occurrence counts of the scalar quantizer bins, and the quantizer adapts without requiring side information. Our work adopts the one pass approach. Advantages of one pass algorithms include simplicity, computational efficiency, and low encoding delay.
Reference: [20] <author> Y. Steinberg and M. Gutman, </author> <title> "An algorithm for source coding subject to a fidelity criterion, based on string matching," </title> <journal> IEEE Trans. Inform. Theory, </journal> <volume> vol. IT-39, </volume> <pages> pp. 877-886, </pages> <month> May </month> <year> 1993. </year>
Reference-contexts: The second approach, described as one pass, adapts the code as encoding proceeds without using iterative techniques. The source signal is hence parsed through just once. Steinberg and Gutman <ref> [20] </ref> proposed an algorithm based on string matching with distortion, which achieves R (D=2) for a large class of stationary sources and distortion measures. Zhang and Wei [24] introduced the "gold-washing" method, which sequentially updates the set of codevectors. <p> Along these lines, Steinberg and Gutman proposed an algorithm that achieves a rate of R (D=2) given an average distortion D &gt; 0 for a large class of sources and distortion measures <ref> [20] </ref>. Koga and Arimoto [12] further proved that the algorithm achieves the rate-distortion bound asymptotically for certain sources and fidelity criteria. These works establish the theoretical foundation and motivation for a lossy version of LZ coding.
Reference: [21] <author> J. Vaisey and A. Gersho, </author> <title> "Simulated annealing and codebook design," </title> <booktitle> in Proceedings of ICASSP'88, </booktitle> <pages> pp. 1176-1179, </pages> <year> 1988. </year>
Reference-contexts: These iterative algorithms, although guaranteed to converge, do not always converge to the optimal quantizer. More likely than not, they will go to some local minima. Hence, additional techniques have been introduced to evade local minima, such as simulated annealing <ref> [8, 21] </ref>, but they add to the computational complexity significantly. Another problem in designing the quantizer using these iterative algorithms is 2 that the distribution of the source X is unknown.
Reference: [22] <author> B. Yu and T. P. </author> <title> Speed, "A rate of convergence result for a universal d-semifaithful code," </title> <journal> IEEE Trans. Inform. Theory, </journal> <volume> vol. IT-39, </volume> <pages> pp. 813-821, </pages> <month> May </month> <year> 1993. </year>
Reference-contexts: Previous works in this area fall into two categories: 1) existence proof and code construction, and 2) practical, computationally efficient algorithms. In works by Ziv [25], Neuhoff et al. [18], MacKenthun and Pursley [16], Kieffer [11], Linder et al. [15], Yu and Speed <ref> [22] </ref> and others, the existence of universal lossy source codes is established under various assumptions about the class of sources, the distortion measure, and the type of convergence of the rates to the rate-distortion bound.
Reference: [23] <author> K. Zeger, A. Bist, and T. Linder, </author> <title> "Universal source coding with codebook transmission," </title> <journal> IEEE Trans. Comm., </journal> <volume> vol. COM-42, </volume> <pages> pp. 336-346, </pages> <month> Feb/Mar/Apr </month> <year> 1994. </year>
Reference-contexts: The tradeoff between spending bits on transmitting updates of the codebook and specifying codevector index is addressed in <ref> [23] </ref>, which also involves finding the optimal codebook periodically. Lightstone and Mitra [13] then addressed the rate-distortion tradeoff by using the entropy-constrained framework. One common characteristic of these algorithms is that some kind of iterative techniques, such as the LBG algorithm, is involved at certain steps of the algorithms.
Reference: [24] <author> Z. Zhang and V. K. Wei, </author> <title> "An on-line universal lossy data compression algorithm by continuous codebook refinement," </title> <booktitle> in 1994 IEEE Int. Symp. Inform. Theory, </booktitle> <address> p. 262, </address> <year> 1994. </year>
Reference-contexts: The source signal is hence parsed through just once. Steinberg and Gutman [20] proposed an algorithm based on string matching with distortion, which achieves R (D=2) for a large class of stationary sources and distortion measures. Zhang and Wei <ref> [24] </ref> introduced the "gold-washing" method, which sequentially updates the set of codevectors. In [19], the source distribution function is estimated based on the occurrence counts of the scalar quantizer bins, and the quantizer adapts without requiring side information. Our work adopts the one pass approach.
Reference: [25] <author> J. Ziv, </author> <title> "Coding of sources with unknown statistics- part ii: Distortion relative to a fidelity criterion," </title> <journal> IEEE Trans. Inform. Theory, </journal> <volume> vol. IT-18, </volume> <pages> pp. 389-394, </pages> <month> May </month> <year> 1972. </year>
Reference-contexts: Asymptotically, the code should achieve the optimal performance of the corresponding stationary ergodic source. Previous works in this area fall into two categories: 1) existence proof and code construction, and 2) practical, computationally efficient algorithms. In works by Ziv <ref> [25] </ref>, Neuhoff et al. [18], MacKenthun and Pursley [16], Kieffer [11], Linder et al. [15], Yu and Speed [22] and others, the existence of universal lossy source codes is established under various assumptions about the class of sources, the distortion measure, and the type of convergence of the rates to the
Reference: [26] <author> J. Ziv and A. Lempel, </author> <title> "A universal algorithm for sequential data compression," </title> <journal> IEEE Trans. Inform. Theory, </journal> <volume> vol. IT-23, </volume> <pages> pp. 337-343, </pages> <month> May </month> <year> 1977. </year>
Reference-contexts: Our work adopts the one pass approach. Advantages of one pass algorithms include simplicity, computational efficiency, and low encoding delay. In lossless source coding, a family of universal coding algorithms which exhibits these advantages is Lempel-Ziv coding <ref> [26, 27] </ref>.
Reference: [27] <author> J. Ziv and A. Lempel, </author> <title> "Compression of individual sequences by variable rate coding," </title> <journal> IEEE Trans. Inform. Theory, </journal> <volume> vol. IT-24, </volume> <pages> pp. 530-536, </pages> <month> September </month> <year> 1978. </year>
Reference-contexts: Our work adopts the one pass approach. Advantages of one pass algorithms include simplicity, computational efficiency, and low encoding delay. In lossless source coding, a family of universal coding algorithms which exhibits these advantages is Lempel-Ziv coding <ref> [26, 27] </ref>.
References-found: 27

