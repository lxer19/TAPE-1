URL: ftp://ftp.di.unito.it/pub/articles/robotics/lrobot95.ps.Z
Refering-URL: http://www.di.unito.it/WWW/MLgroup/attiliobib.html
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Email: e-mail: blanzier@psych.unito.it  email: attilio@di.unito.it  
Title: GROWING RADIAL BASIS FUNCTION NETWORKS  
Author: E. BLANZIERI P. KATENKAMP flfl and A. GIORDANA flflfl 
Keyword: Key Words. Machine Learning, Robotics, Neural Nets  
Address: Torino, Via Lagrange 3, 10100 Torino, Italy.  Torino, C.so Svizzera 185, 10149 Torino, Italy.  
Affiliation: Centro di Scienza Cognitiva, Universita di  flfl Institute for Real Time Systems and Robotics, University of Karlsruhe, Germany. flflfl Dipartimento di Informatica, Universita di  
Abstract: This paper presents and evaluates two algorithms for incrementally constructing Radial Basis Function Networks, a class of neural networks which looks more suitable for adtaptive control applications than the more popular backpropagation networks. The first algorithm has been derived by a previous method developed by Fritzke, while the second one has been inspired by the CART algorithm developed by Breiman for generation regression trees. Both algorithms proved to work well on a number of tests and exhibit comparable performances. An evaluation on the standard case study of the Mackey-Glass temporal series is reported. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Baroglio, C., A. Giordana, M. Kaiser, M. Nuttin and R. </author> <month> Pi-ola </month> <year> (1996). </year> <title> Learning controllers for industrial robots. </title> <booktitle> Machine Learning. </booktitle>
Reference-contexts: Then, they can be used for automatically constructing the layout of a F-RBFN <ref> (Baroglio et al., 1996) </ref>. In the present case we will only consider a specific algorithm named CART (Breiman et al., 1984) which can work both for classification and for regression.
Reference: <author> Berenji, H.R. </author> <year> (1992). </year> <title> Fuzzy logic controllers. In: An Introduction to Fuzzy Logic Applications in Intelligent Systems (R.R. </title> <editor> Yager and L.A. Zadeh, Eds.). </editor> <publisher> Kluver Academic Publishers. </publisher>
Reference-contexts: growing F-RBFNs in some sense can be seen as incremental versions of this algorithms. 2.1 Architecture The specific network architecture we will investigate in this paper is a hybrid between the Factorized Radial Basis Function Networks (F-RBFNs), mentioned in (Poggio and Girosi, 1990) and the fuzzy/neural networks introduced by Berenji <ref> (Berenji, 1992) </ref> for implementing fuzzy controllers capable of learning from a reinforcement signal. It is similar to the architecture proposed by Tresp et al. (Tresp et al., 1993). Figure 1 describes the basic network topology. first layer hidden units have a unidimensional Gaussian activation function. <p> The consequence is under-generalization. This problem can be avoided by introducing a normalization term in the output activation function: ^ Y = j w j r j j r j This function is frequently used for fuzzy controller architectures <ref> (Berenji, 1992) </ref> and, in this case, one obtains a network biased toward overgeneralization in a similar way as it happen for the multi-layer perceptron. Depending on the application, under-generalization or overgeneralization can be preferable.
Reference: <author> Bonissone, </author> <title> P.P. and K.H. Chiang (1993). Fuzzy logic controllers: from development to deployment. </title> <booktitle> In: IEEE International Conference on Neural Networks. </booktitle> <volume> Vol. </volume> <pages> 2. </pages> <address> San Francisco, CA. </address>
Reference-contexts: Special cases of LRFNs are the Radial Basis Functions (Poggio and Girosi, 1990), the statistical neural networks (Specht, 1988; Specht, 1990), the fuzzy controllers <ref> (Bonissone and Chiang, 1993) </ref> and other more rough architectures such as the C- MAC (Lin, 1991) and the Boxes (Michie and Chambers, 1968).
Reference: <author> Breiman, L., J.H. Friedman, R.A. </author> <note> Ohlsen and C.J. </note>
Reference: <author> Stone (1984). </author> <title> Classification And Regression Trees. </title> <publisher> Wadsworth & Brooks. </publisher> <address> Pacific Grove, CA. </address>
Reference: <author> Crowder, R.S. </author> <year> (1990). </year> <title> Predicting the mackey-glass time series with cascade-correlation learning. </title> <booktitle> In: Proceedings of the 1990 Connectionist Models Summer School (G. </booktitle> <editor> Hinton D. Touretzky and T.Sejnovsky, Eds.). </editor> <publisher> Carnegie Mellon University. </publisher> <pages> pp. 117-123. </pages>
Reference: <author> Fritzke, B. </author> <year> (1994). </year> <title> Growing cell structure a self-organizing networks for unsupervised and supervised learning. </title> <booktitle> Neural Networks 7(9), </booktitle> <pages> 1441-1460. </pages>
Reference-contexts: to adopt the strategy proposed by Tresp et al. (Tresp et al., 1993) in order to preserve the knowledge inserted in the network. 3 DYNAMIC COMPETITIVE LEARNING The first algorithm for dynamically growing F-RBFNs, we present in this section, is rather an extension of the algorithm GCS introduced by Fritzke <ref> (Fritzke, 1994) </ref> than a complete new method. It uses a combination between the gradient descent algorithm and GCS improved with respect to several points. The main algorithm includes two more specific algorithms: one for adapting the network parameters and another one for adding new neurons (rules). <p> But like by Fritzke <ref> (Fritzke, 1994) </ref> suggested for on-line approaches there are some small differences to the Kohonen approach. In the Kohonen's model the strength of the adaptation is decreasing according to a cooling schedule. Moreover, also the topological neighborhood involved in the training process decreases over time.
Reference: <author> Fritzke, B. </author> <year> (1995). </year> <title> A growing neural gas network learns topologies. </title> <booktitle> G.Tesauro, D.S.Touretzky, and T.K.Leen in Advances in Neural Information Processing Systems,MIT Press. </booktitle>
Reference-contexts: An elegant way to learn such a structure is competitive Hebbian learning. The method is basically the one proposed for the 'Neural Gas' algorithm by Martinetz (Martinetz, 1993) (Martinetz and Schulten, 1991) and extended for incremental use from Fritzke <ref> (Fritzke, 1995) </ref>. The algorithm we will use is the following one: Algorithm MakeTopology. 1. Determine the two nearest rules for the input signal X 2. If an edge exists between this two rules set the age of this edge to zero.
Reference: <author> Jang, J.S.R. </author> <year> (1993). </year> <title> ANFIS: Adaptive-Network-Based Fuzzy Inference System. </title> <journal> IEEE Transactions on Systems, Men and Cybernetics SMC-23(3), </journal> <pages> 665-687. </pages>
Reference-contexts: In table 2, the comparison is made using the gradient descent off-line, both for the network generated by DCL and DRT and the ones generated by CART and k-means. Finally, Table 3 reports a comparison with some of the best results reported in the literature, such as ANFIS <ref> (Jang, 1993) </ref>, a fuzzy neural network designed by Jang and a F-RBFN generated using a symbolic learner called SMART+ which allows to account for a domain theory.
Reference: <author> Jones, R.D., Y.C. Lee, C.W. Barnes, G.W. Flake, K. Lee and P.S. </author> <title> Lewis (1990). Function approximation and time series prediction with neural networks. </title> <booktitle> In: Proceedings of IEEE International Joint Conference on Neural Networks. </booktitle> <pages> pp. </pages> <month> I-649-665. </month>
Reference: <author> Kohonen, T. </author> <year> (1982). </year> <title> Self-organized formation of topological correct feature maps. </title> <journal> Biaological Cybernetics pp. </journal> <pages> 59-69. </pages>
Reference-contexts: Remove all edges which have a age higher than MAXAGE 3.2 Center Clustering Algorithm The positioning of the multidimensional gaussian centers is done using an algorithm similar to the self-organizing feature maps proposed by Kohonen <ref> (Kohonen, 1982) </ref>(Martinetz and Schulten, 1994): * Determine the best matching rule for the cur rent input signal. * Increase the matching of the best rule and its topological neighbors. But like by Fritzke (Fritzke, 1994) suggested for on-line approaches there are some small differences to the Kohonen approach.
Reference: <author> Lin, L.J. </author> <year> (1991). </year> <title> Programming robots using reinforcement learning and teaching. </title> <booktitle> In: 9th International Conference on Artificial Intelligence, </booktitle> <address> AAA-91. </address>
Reference-contexts: Special cases of LRFNs are the Radial Basis Functions (Poggio and Girosi, 1990), the statistical neural networks (Specht, 1988; Specht, 1990), the fuzzy controllers (Bonissone and Chiang, 1993) and other more rough architectures such as the C- MAC <ref> (Lin, 1991) </ref> and the Boxes (Michie and Chambers, 1968). However, for all kind of LRFNs two major properties hold: (a) the symbolic interpretability and (b) the locality property, i.e. the fact that every learning action only affects a restricted area in the function domain.
Reference: <author> Martinetz, T.M. </author> <year> (1993). </year> <title> Competitive hebbian learning rule forms perfectly topology preseving maps. </title> <booktitle> International Conference on Artificial Neural Networks pp. </booktitle> <pages> 427-434. </pages>
Reference-contexts: However, in presence of a high-dimensional data distribution finding a topological structure closely reflecting the data topology is not a trivial problem. An elegant way to learn such a structure is competitive Hebbian learning. The method is basically the one proposed for the 'Neural Gas' algorithm by Martinetz <ref> (Martinetz, 1993) </ref> (Martinetz and Schulten, 1991) and extended for incremental use from Fritzke (Fritzke, 1995). The algorithm we will use is the following one: Algorithm MakeTopology. 1. Determine the two nearest rules for the input signal X 2.
Reference: <author> Martinetz, </author> <title> T.M. and K.J. Schulten (1991). A neural-gas network learns topologies. </title> <editor> In T. Kohonen,K. Mk-isara, O. Simula and J. Kangas, editors, </editor> <booktitle> Artificial Neural Networks pp. </booktitle> <pages> 397-402. </pages>
Reference-contexts: An elegant way to learn such a structure is competitive Hebbian learning. The method is basically the one proposed for the 'Neural Gas' algorithm by Martinetz (Martinetz, 1993) <ref> (Martinetz and Schulten, 1991) </ref> and extended for incremental use from Fritzke (Fritzke, 1995). The algorithm we will use is the following one: Algorithm MakeTopology. 1. Determine the two nearest rules for the input signal X 2.
Reference: <author> Martinetz, </author> <title> T.M. and K.J. Schulten (1994). Topology representing networks. Neural Networks. </title>
Reference: <editor> Michie, D. and R.A. </editor> <title> Chambers (1968). Boxes: an experiment in adaptive control. </title> <booktitle> In: Machine Intelligence 2. </booktitle> <pages> pp. 137-152. </pages> <publisher> Ellis Horwood. </publisher>
Reference-contexts: Special cases of LRFNs are the Radial Basis Functions (Poggio and Girosi, 1990), the statistical neural networks (Specht, 1988; Specht, 1990), the fuzzy controllers (Bonissone and Chiang, 1993) and other more rough architectures such as the C- MAC (Lin, 1991) and the Boxes <ref> (Michie and Chambers, 1968) </ref>. However, for all kind of LRFNs two major properties hold: (a) the symbolic interpretability and (b) the locality property, i.e. the fact that every learning action only affects a restricted area in the function domain.
Reference: <author> Millan, J.R. </author> <year> (1994). </year> <title> Learning efficient reactive behavioral sequences from basic reflexes in a goal-directed autonomous robot.. </title> <booktitle> In: Proceedings of the third International Conference on Simulation of Adaptive Behavior. </booktitle>
Reference-contexts: Nevertheless, this property is little exploited by the current learning algorithms which, with few exceptions <ref> (Millan, 1994) </ref>, learn the network in a single step from a learning set. In fact, one step learning is of little use when we move to real application in robotic domains where the target function can be discovered only incrementally by operating the robot itself.
Reference: <author> Moody, J. and C. </author> <month> Darken </month> <year> (1989). </year> <title> Fast learning in networks of locally tuned units. </title> <booktitle> Neural Computations 1(2), </booktitle> <pages> 281-294. </pages>
Reference: <author> Poggio, T. and F. </author> <title> Girosi (1990). Networks for approximation and learning. </title> <booktitle> Proceedings of the IEEE 78(9), </booktitle> <pages> 1481-1497. </pages>
Reference-contexts: On the one hand, LRFNs can be seen as a natural extension of the look up tables in order to represent continuous functions, on the other hand they can be seen as development into a Green's function series <ref> (Poggio and Girosi, 1990) </ref>. A LRFNs can be represented as a three layer architecture where every hidden unit is associated to a local activation function which contributes to the definition of the output value only in a closed region of the domain of the target function. <p> Special cases of LRFNs are the Radial Basis Functions <ref> (Poggio and Girosi, 1990) </ref>, the statistical neural networks (Specht, 1988; Specht, 1990), the fuzzy controllers (Bonissone and Chiang, 1993) and other more rough architectures such as the C- MAC (Lin, 1991) and the Boxes (Michie and Chambers, 1968). <p> The paper is organised as in the following. Section 2 describes the specific architecture we adopted for LRFN which corresponds to the Factorisable Radial Basis Function Networks (F-RBFNs) introduced by Poggio and Girosi <ref> (Poggio and Girosi, 1990) </ref>. Section 3 and Section 4 describe two alter-native algorithms for growing F-RBFNs. Then, Section 5 reports an evaluation of the methods on the classical test case of the Mackey-Glass chaotic time series. <p> The algorithms for growing F-RBFNs in some sense can be seen as incremental versions of this algorithms. 2.1 Architecture The specific network architecture we will investigate in this paper is a hybrid between the Factorized Radial Basis Function Networks (F-RBFNs), mentioned in <ref> (Poggio and Girosi, 1990) </ref> and the fuzzy/neural networks introduced by Berenji (Berenji, 1992) for implementing fuzzy controllers capable of learning from a reinforcement signal. It is similar to the architecture proposed by Tresp et al. (Tresp et al., 1993). <p> Depending on the application, under-generalization or overgeneralization can be preferable. The results reported in this paper refer to networks based on function (4). 2.2 Statistical Clustering The usual methods for learning from examples Radial Basis Function networks <ref> (Poggio and Girosi, 1990) </ref> are based on a two step learning procedure. First a statistical clustering algorithm, such as k-Means is used to determine the centers and the amplitude of the activation functions. <p> First a statistical clustering algorithm, such as k-Means is used to determine the centers and the amplitude of the activation functions. Then, the weights on the links to the output neuron are determined by computing the coefficients of the pseudo-inverse matrix <ref> (Poggio and Girosi, 1990) </ref>. The method is usually faster than gradient descent. On the other hand, this last is sometime preferable, because it is more simple to implement and because it is suitable for on-line learning.
Reference: <author> Sanger, T.D. </author> <year> (1991). </year> <title> A tree-structured adaptive network for function approximate in high-dimensional spaces. </title> <journal> IEEE Transactions on Neural Networks 2(2), </journal> <pages> 285-293. </pages>
Reference: <author> Specht, D.F. </author> <year> (1988). </year> <title> Probabilistic neural networks for classification mapping, or associative memory. </title> <booktitle> In: IEEE International Conference on Neural Networks. </booktitle> <volume> Vol. 1. </volume> <pages> pp. 525-532. </pages>
Reference: <author> Specht, D.F. </author> <year> (1990). </year> <title> Probabilistic neural networks. </title> <booktitle> Neural Networks 3, </booktitle> <pages> 109-118. </pages>
Reference: <author> Tresp, V., J. Hollatz and S. </author> <title> Ahmad (1993). Network structuring and training using rule-based knowledge. </title> <booktitle> In: Advances in Neural Information Processing Systems 5 (NIPS-5). </booktitle>
Reference-contexts: It is similar to the architecture proposed by Tresp et al. <ref> (Tresp et al., 1993) </ref>. Figure 1 describes the basic network topology. first layer hidden units have a unidimensional Gaussian activation function. The second layer hidden units compose the input values using arithmetic product. An average sum unit performs the weighted sum of the activation values received from the product units. <p> In fact it is easy to control the overlap by means of direct monitoring and freezing of the centers. Alternatively, it is possible to adopt the strategy proposed by Tresp et al. <ref> (Tresp et al., 1993) </ref> in order to preserve the knowledge inserted in the network. 3 DYNAMIC COMPETITIVE LEARNING The first algorithm for dynamically growing F-RBFNs, we present in this section, is rather an extension of the algorithm GCS introduced by Fritzke (Fritzke, 1994) than a complete new method.
References-found: 23

