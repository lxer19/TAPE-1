URL: http://www.robotics.stanford.edu/~koller/papers/uai91.ps
Refering-URL: http://www.robotics.stanford.edu/~koller/papers/uai91.html
Root-URL: http://www.robotics.stanford.edu
Email: grove@cs.stanford.edu  daphne@theory.stanford.edu  
Title: Probability Estimation in face of Irrelevant Information  
Author: Adam J. Grove Daphne Koller 
Address: Stanford, CA 94305  Stanford, CA 94305  
Affiliation: Department of Computer Science Stanford University  Department of Computer Science Stanford University  
Abstract: In this paper, we consider one aspect of the problem of applying decision theory to the design of agents that learn how to make decisions under uncertainty. This aspect concerns how an agent can estimate probabilities for the possible states of the world, given that it only makes limited observations before committing to a decision. We show that the naive application of statistical tools can be improved upon if the agent can determine which of his observations are truly relevant to the estimation problem at hand. We give a framework in which such determinations can be made, and define an estimation procedure to use them. Our framework also suggests several extensions, which show how additional knowledge can be used to improve the estimation procedure still further.
Abstract-found: 1
Intro-found: 1
Reference: [ Bacchus, 1988 ] <author> F. Bacchus. </author> <title> Representing and reasoning with probabilistic knowledge. </title> <type> PhD thesis, </type> <institution> University of Alberta, </institution> <year> 1988. </year> <note> Also issued as Waterloo University Technical Report CS-88-31. </note>
Reference-contexts: Our method for estimating probabilities, which is based on a procedure that attempts to discover irrelevant attributes, is described in Section 3. Some extensions are outlined in Section 4. Our technique combines concepts reminiscent of probabilistic reference hierarchies (see, for example, <ref> [ Bacchus, 1988 ] </ref> ) with statistical tools.
Reference: [ Bundy, 1984 ] <author> A. Bundy. </author> <title> Incidence calculus: a mechanism for probabilistic reasoning. </title> <type> Technical Report 216, </type> <institution> University of Edinburgh Dept. of Artificial Intelligence, </institution> <year> 1984. </year>
Reference-contexts: Some works [ Simon and Kadane, 1975 ] simply assume that the probabilities are known in advance. Others (e.g. <ref> [ Bundy, 1984; Lee and Mahajan, 1988 ] </ref> ) suggest the concept of sampling, but do not discuss which class to sample. Ren-dell [ Rendell, 1983 ] deals with the concept of sampling on different classes, but in the very limited context of search trees.
Reference: [ Cox, 1961 ] <author> R. T. Cox. </author> <title> The algebra of probable inference. </title> <publisher> Baltimore: The Johns Hopkins Press, </publisher> <year> 1961. </year>
Reference-contexts: For further discussion, see, for example, <ref> [ Cox, 1961; Jaynes, 1968; Savage, 1954 ] </ref> . In our model, a decision-making situation evolves as follows. At the time the agent learns that he is to make a decision, the actual state of the world is regarded as being randomly chosen according to (W; ).
Reference: [ Etzioni, 1989a ] <author> O. Etzioni. </author> <title> Hypothesis filtering: a practical approach to reliable learning. </title> <booktitle> In Proceedings of the Fifth International Conference on Machine Learning, </booktitle> <year> 1989. </year>
Reference-contexts: Others (e.g. [ Bundy, 1984; Lee and Mahajan, 1988 ] ) suggest the concept of sampling, but do not discuss which class to sample. Ren-dell [ Rendell, 1983 ] deals with the concept of sampling on different classes, but in the very limited context of search trees. Etzioni's work <ref> [ Etzioni, 1989a ] </ref> on estimating utilities is based on machine learning techniques, which attempt to discover classes over which the utility is homogeneous.
Reference: [ Etzioni, 1989b ] <author> O. Etzioni. </author> <title> Tractable decision analytic control: an expanded version. </title> <type> Technical Report CMU-CS-89-119, </type> <institution> Carnegie Mellon University, </institution> <year> 1989. </year>
Reference: [ Fung and Crawford, 1990 ] <author> R. M. Fung and S. L. Crawford. </author> <title> Constructor: a system for the induction of probabilistic models. </title> <booktitle> In Proceedings of the National Conference on Artificial Intelligence (AAAI-90), </booktitle> <pages> pages 762-769, </pages> <year> 1990. </year>
Reference-contexts: Typically, it is impossible to find an attribute language precise enough to define such classes. A different approach to finding the right class for estimating probabilities is to treat the problem of inferring the independence structure as a separate task. For example, <ref> [ Fung and Crawford, 1990 ] </ref> use techniques similar to ours|classical statistics, and in particular, the 2 test|in a system which infers qualitative structure from data, modeling this structure as a probabilistic network. Once constructed, the network can be used for several purposes, such as estimation. <p> Therefore, larger amounts of data will be needed. Our approach is also better in situations where new data is being constantly accumulated, because the new information could cause us to change our decision as to the relevance of certain attributes. <ref> [ Fung and Crawford, 1990 ] </ref> also show how to find the smallest possible set of relevant attributes. This procedure is computationally expensive and relies on strong assumptions about the relationships among the attributes. These assumptions also prevent the procedure in [ Fung and Crawford, 1990 ] from being efficiently used <p> our decision as to the relevance of certain attributes. <ref> [ Fung and Crawford, 1990 ] </ref> also show how to find the smallest possible set of relevant attributes. This procedure is computationally expensive and relies on strong assumptions about the relationships among the attributes. These assumptions also prevent the procedure in [ Fung and Crawford, 1990 ] from being efficiently used in our framework, because each decision situation will need to be investigated separately. This eliminates the computational advantage of computing the entire independence structure simultaneously. Our approach eliminates the attributes one by one, in an arbitrary order.
Reference: [ Goodman, 1955 ] <author> N. Goodman. </author> <title> Fact, fiction, and forecast, chapter iii. </title> <publisher> Harvard University Press, </publisher> <year> 1955. </year>
Reference-contexts: In particular, if some restrictions on the possible relationships between attributes are given to the agent (for example, as a reference hierarchy), this can be used in our process. By avoiding the hypothesis 7 This is reminiscent of the well-known "grue/bleen" paradox ( <ref> [ Goodman, 1955 ] </ref> ), which concerns the attribute vocabulary appropriate for inductive inference. test, we gain computational efficiency and elimi- nate the chance of error. * Suppose we know that if some attribute is relevant, then it must affect probabilities in a particular way.
Reference: [ Haddawy and Hanks, 1990 ] <author> P. Haddawy and S. Hanks. </author> <title> Issues in decision-theoretic planning: symbolic goals and numeric utilities. </title> <booktitle> In Proceedings of the 1990 DARPA Workshop on Innovative Approaches to Planning, Scheduling, and Control, </booktitle> <year> 1990. </year>
Reference-contexts: This leads to more accurate estimates. While some research in AI has adopted this separation of probabilities and utilities (notably <ref> [ Haddawy and Hanks, 1990; Simon and Kadane, 1975 ] </ref> ), the problem of estimating the probabilities in the face of too much initial information has not, to our knowledge, been attacked directly. Some works [ Simon and Kadane, 1975 ] simply assume that the probabilities are known in advance.
Reference: [ Horvitz et al., 1988 ] <author> E. J. Horvitz, J. S. Breese, and M. </author> <title> Henrion. </title> <journal> Decision theory in expert systems and artificial intelligence. International Journal of Approximate Reasoning, </journal> <volume> 2 </volume> <pages> 247-302, </pages> <year> 1988. </year>
Reference: [ Horvitz, 1988 ] <author> E. J. Horvitz. </author> <title> Reasoning under varying and uncertain resource constraints. </title> <booktitle> In Proceedings of the National Conference on Artificial Intelligence (AAAI-88), </booktitle> <pages> pages 111-116, </pages> <year> 1988. </year>
Reference: [ Jaynes, 1968 ] <author> E. T. Jaynes. </author> <title> Prior probabilities. </title> <journal> IEEE Transactions on Systems Science and Cybernetics, </journal> <volume> 4 </volume> <pages> 227-241, </pages> <year> 1968. </year>
Reference-contexts: For further discussion, see, for example, <ref> [ Cox, 1961; Jaynes, 1968; Savage, 1954 ] </ref> . In our model, a decision-making situation evolves as follows. At the time the agent learns that he is to make a decision, the actual state of the world is regarded as being randomly chosen according to (W; ). <p> While not guaranteed to find the minimal set of relevant attributes, this technique is much faster and requires no assumptions. We conclude this section by comparing our methods to the Bayesian approach. It should be noted that, if prior probabilities are available, Bayesian updating (see <ref> [ Jaynes, 1968 ] </ref> ) can, in a sense, replace the 2 test described in Section 3. We have chosen not to assume the existence of prior probabilities, and therefore use a technique from classical statistics.
Reference: [ Larsen and Marx, 1981 ] <author> R. J. Larsen and M. L. Marx. </author> <title> An introduction to mathematical statistics and its applications. </title> <publisher> Prentice-Hall, </publisher> <year> 1981. </year>
Reference-contexts: of these two possibilities to use on the basis of a hypothesis test (the hypothesis being that p i = p j , for all i; j.) One relatively simple hypothesis test we can use for this is the 2 test, which is discussed in most statistics texts (such as, <ref> [ Larsen and Marx, 1981; Sachs, 1982 ] </ref> ).
Reference: [ Lee and Mahajan, 1988 ] <author> K. F. Lee and S. Mahajan. </author> <title> A pattern classification approach to evaluation function learning. </title> <journal> Artificial Intelligence, </journal> <volume> 36, </volume> <year> 1988. </year>
Reference-contexts: Some works [ Simon and Kadane, 1975 ] simply assume that the probabilities are known in advance. Others (e.g. <ref> [ Bundy, 1984; Lee and Mahajan, 1988 ] </ref> ) suggest the concept of sampling, but do not discuss which class to sample. Ren-dell [ Rendell, 1983 ] deals with the concept of sampling on different classes, but in the very limited context of search trees.
Reference: [ Pearl, 1986 ] <author> J. Pearl. </author> <title> On evidential reasoning in a hierarchy of hypotheses. </title> <journal> Artificial Intelligence, </journal> <volume> 28, </volume> <year> 1986. </year>
Reference-contexts: We have chosen not to assume the existence of prior probabilities, and therefore use a technique from classical statistics. A work similar in outlook to ours, which deals with a different problem using Bayesian techniques is Pearl's work about hierarchies of hypotheses <ref> [ Pearl, 1986 ] </ref> . 6 CONCLUSION We have investigated the problem of estimating the conditional probability of a state given some initial information I, based on a database of observations. This problem is straightforward when there is plenty of data.
Reference: [ Rendell, 1983 ] <author> L. Rendell. </author> <title> A new basis for state-space learning systems and a successful implementation. </title> <journal> Artificial Intelligence, </journal> <volume> 20, </volume> <year> 1983. </year>
Reference-contexts: Some works [ Simon and Kadane, 1975 ] simply assume that the probabilities are known in advance. Others (e.g. [ Bundy, 1984; Lee and Mahajan, 1988 ] ) suggest the concept of sampling, but do not discuss which class to sample. Ren-dell <ref> [ Rendell, 1983 ] </ref> deals with the concept of sampling on different classes, but in the very limited context of search trees. Etzioni's work [ Etzioni, 1989a ] on estimating utilities is based on machine learning techniques, which attempt to discover classes over which the utility is homogeneous.
Reference: [ Russel and Wefald, 1988 ] <author> S. Russel and E. Wefald. </author> <title> Decision-theoretic control of reasoning: general theory and an application to game playing. </title> <type> Technical Report UCB/CSD 88/435, </type> <institution> University of California at Berkeley, </institution> <year> 1988. </year>
Reference: [ Sachs, 1982 ] <author> L. Sachs. </author> <title> Applied statistics. </title> <publisher> Springer-Verlag, </publisher> <year> 1982. </year>
Reference-contexts: of these two possibilities to use on the basis of a hypothesis test (the hypothesis being that p i = p j , for all i; j.) One relatively simple hypothesis test we can use for this is the 2 test, which is discussed in most statistics texts (such as, <ref> [ Larsen and Marx, 1981; Sachs, 1982 ] </ref> ).
Reference: [ Savage, 1954 ] <author> L. J. Savage. </author> <title> Foundations of statistics. </title> <publisher> John Wiley & Sons, </publisher> <year> 1954. </year>
Reference-contexts: The model which stands at the heart of classical decision theory, and on which our work is based, is the decision matrix (see <ref> [ Savage, 1954 ] </ref> for definitions, and Section 5 for some discussion of related work in AI). As explained in Section 2, applying this technique requires the estimation of the probabilities of the various states of the world that the agent considers possible. <p> These two components of the matrix will be described later in this section. The elements of the matrix are the agent's outcomes for each action/state pair. It is a well-known result of decision theory (see <ref> [ Savage, 1954 ] </ref> ) that in many cases, the agent's preference ordering on outcomes can be expressed by numerical utilities. In the particular context of intelligent agents, the utility will often be expressed in terms of certain parameters, such as time, fuel consumption, or money. <p> For further discussion, see, for example, <ref> [ Cox, 1961; Jaynes, 1968; Savage, 1954 ] </ref> . In our model, a decision-making situation evolves as follows. At the time the agent learns that he is to make a decision, the actual state of the world is regarded as being randomly chosen according to (W; ).
Reference: [ Simon and Kadane, 1975 ] <author> H. A. Simon and J. B. Kadane. </author> <title> Optimal problem solving search: all-or-none solutions. </title> <journal> Artificial Intelligence, </journal> <volume> 6, </volume> <year> 1975. </year>
Reference-contexts: Using these probabilities, the agent can estimate the expected utility for each alternative action, and choose the one that maximizes that quantity. Unlike many previous works (e.g. <ref> [ Simon and Kadane, 1975 ] </ref> ), we do not assume that these probabilities are made available by an external source. More realistically, we assume that the agent uses its own experience as the major source of information. The agent will thus learn from experience, by gradually refining its estimates. <p> This leads to more accurate estimates. While some research in AI has adopted this separation of probabilities and utilities (notably <ref> [ Haddawy and Hanks, 1990; Simon and Kadane, 1975 ] </ref> ), the problem of estimating the probabilities in the face of too much initial information has not, to our knowledge, been attacked directly. Some works [ Simon and Kadane, 1975 ] simply assume that the probabilities are known in advance. <p> While some research in AI has adopted this separation of probabilities and utilities (notably [ Haddawy and Hanks, 1990; Simon and Kadane, 1975 ] ), the problem of estimating the probabilities in the face of too much initial information has not, to our knowledge, been attacked directly. Some works <ref> [ Simon and Kadane, 1975 ] </ref> simply assume that the probabilities are known in advance. Others (e.g. [ Bundy, 1984; Lee and Mahajan, 1988 ] ) suggest the concept of sampling, but do not discuss which class to sample.
Reference: [ Spiegelhalter, 1986 ] <author> D. J. Spiegelhalter. </author> <title> Probabilistic reasoning in predictive expert systems. </title> <editor> In L. N. Kanal and J. F. Lemmer, editors, </editor> <booktitle> Proceedings of the Second Workshop on Uncertainty in Artificial Intelligence, </booktitle> <pages> pages 47-68. </pages> <address> Amsterdam, </address> <publisher> North Holland, </publisher> <year> 1986. </year>
Reference: [ Wellman, 1990 ] <author> M. P. Wellman. </author> <title> Formulation of tradeoffs in planning under uncertainty. </title> <publisher> Pitman, </publisher> <address> London, </address> <year> 1990. </year>
References-found: 21

