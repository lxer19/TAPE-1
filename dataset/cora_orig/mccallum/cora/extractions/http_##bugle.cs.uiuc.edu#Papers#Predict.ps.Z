URL: http://bugle.cs.uiuc.edu/Papers/Predict.ps.Z
Refering-URL: http://bugle.cs.uiuc.edu/Projects/HPF/publications.html
Root-URL: http://www.cs.uiuc.edu
Email: Email: fmendes,jcwang,reedg@cs.uiuc.edu  
Title: Automatic Performance Prediction and Scalability Analysis for Data Parallel Programs  
Author: Celso L. Mendes Jhy-Chun Wang Daniel A. Reed 
Address: Urbana, Illinois 61801  
Affiliation: Department of Computer Science University of Illinois  
Abstract: With data parallel languages like High Performance Fortran (HPF), an application developer's mental model of program execution can differ markedly from the code that actually executes on a particular parallel system. In such situations, understanding the reasons for poor program performance requires a time consuming cycle of performance experimentation and code tuning where one seeks to understand the relation between data parallel source and the performance of compiler-synthesized SPMD code. Because performance is the primary motivation for parallel computing, it is critical to understand the potential scalability of a code as a function of the number of processors (P ) and problem size (N ). It is equally important that this understanding be related to the high-level data parallel source code, not just to the SPMD code. In this paper, we propose a performance scalability model for data parallel code fragments based on symbolic expressions of N and P . In this approach, the data parallel compiler synthesizes instrumented SPMD code and records sufficient compile-time data to relate the dynamic performance of the SPMD code to the original data parallel source. By correlating this data, we can derive coefficients for the original data parallel program's scalability model. We present preliminary results of our model with two examples written in Fortran D, a precursor to HPF. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Adve, V. S., Mellor-Crummey, J., Andersdon, M., Kennedy, K., Wang, J.-C., and Reed, D. A. </author> <title> An Integrated Compilation and Performance Analysis Environment for Data Parallel Programs, </title> <month> March </month> <year> 1995. </year> <note> Submitted for publication. </note>
Reference-contexts: Thus, we predict the performance of non-instrumented codes using performance data from the instrumented versions. Our current model is based on an integrated compilation and performance analysis system jointly developed by the Rice Fortran D group and the University of Illinois Pablo group <ref> [1] </ref>. Compile-time data on program transformations and the relation of generated SPMD code to high-level data parallel source allows us to link dynamic performance data with the corresponding high-level program. Using this linkage, we then build parametrized execution models for code fragments in the original Fortran D source code. <p> Our current model is based on an integrated compilation and performance analysis system jointly developed by the Rice Fortran D group and the University of Illinois Pablo group <ref> [1] </ref>. <p> A detailed description of the SDDF record contents is beyond the scope of this paper; see <ref> [1] </ref> for details. <p> The performance prediction model interacts with data correlation toolkit and symbolic expression manipulator to anaylze performance data and provide performance prediction functionalities. 1 The mapping strategy is discussed in depth in <ref> [1] </ref>. 5 3.2 Prediction Details As an example, suppose that the Fortran D compiler generates the instrumented SPMD code fragment shown in Figure 2.
Reference: [2] <author> Aydt, R. A. </author> <title> The Pablo Self-Defining Data Format. </title> <institution> Department of Computer Science, University of Illinois at Urbana-Champaign, </institution> <month> Dec. </month> <year> 1994. </year>
Reference-contexts: This file, comprising static information about the program, is written in the Pablo SDDF format <ref> [2] </ref>, and contains one record for each program event.
Reference: [3] <author> Carlson, B. M., Wagner, T. D., Dowdy, L. W., and Worley, P. H. </author> <title> Speedup Properties of Phases in the Execution Profile of Distributed Parallel Programs. </title> <type> Tech. Rep. </type> <institution> ORNL/TM-11900, Oak Ridge National Laboratory, </institution> <month> August </month> <year> 1992. </year>
Reference-contexts: As we continue to build our scalability software, we are working with the Rice Fortran D group to include such capabilities. 5 Related Work Several groups have developed techniques for predicting the scalability of parallel systems and parallel programs. Carlson et al <ref> [3] </ref> studied the performance of parallel systems using execution profiles, which specify the number of busy processors as a function of time. Using such profiles, the authors identified phases of homogeneous utilization, and characterized program scalability based on the scalability of individual phases.
Reference: [4] <author> Crovella, M. E., and LeBlanc, T. L. </author> <title> Parallel Performance Prediction Using Lost Cycles Analysis. </title> <booktitle> In Proceedings of Supercomputing'94 (Washington, </booktitle> <month> November </month> <year> 1994), </year> <pages> pp. 600-609. </pages>
Reference-contexts: The selection of appropriate terms to include in a model, however, still required human intervention, and the regression procedure needed performance data from numerous executions with different data set sizes and numbers of processors. Crovella and LeBlanc <ref> [4] </ref> predicted the performance of parallel programs based on lost cycles analysis. This technique measures overhead categories and fits the measurements to functions that relate overhead to the number of processors and problem size.
Reference: [5] <author> Fahringer, T., and Zima, H. P. </author> <title> A static parameter based performance prediction tool for parallel programs. </title> <booktitle> In Proceedings of the 7th International Conference on Supercomputing (Tokyo, </booktitle> <address> Japan, </address> <month> July </month> <year> 1993), </year> <pages> pp. 207-219. </pages>
Reference-contexts: Using abstractions HPF source code constructs and benchmark data on the performance of those constructs, they assign costs to application constructs and estimate execution time. This method requires extremely detailed knowledge of the underlying system, and its target is guidance for efficient data distributions. Fahringer and Zima <ref> [5] </ref> designed a performance prediction tool named PPPT (Parameter based Performance Prediction Tool), which analyzes a set of parameters that characterize the behavior of a parallel program, including work distribution, amount of communication and data locality.
Reference: [6] <author> Grama, A. Y., Gupta, A., and Kumar, V. Isoefficiency: </author> <title> Measuring the Scalability of Parallel Algorithms and Architectures. </title> <booktitle> IEEE Parallel & Distributed Technology 1, </booktitle> <month> 3 (August </month> <year> 1993), </year> <pages> 12-21. </pages>
Reference-contexts: In addition, identifying phase transitions can be computationally expensive for long-running codes. Grama, Gupta and Kumar <ref> [6] </ref> studied the performance of parallel systems using isoefficiency analysis. For a given algorithm/system combination, they defined the corresponding isoefficiency function as the required growth in the input data set size to maintain the same efficiency, as the number of processors grows.
Reference: [7] <author> Hiranandani, S., Kennedy, K., and Tseng, C.-W. </author> <title> Compiling Fortran D for MIMD Distributed-Memory Machines. </title> <journal> Communications of the ACM 35, </journal> <month> 8 (August </month> <year> 1992), </year> <pages> 66-80. </pages>
Reference-contexts: Of these, high performance variability and the large programming effort required to write explicitly parallel message passing code are among the most significant. Data parallel languages, like HPF [8] and its precursor, Fortran D <ref> [7] </ref>, have been proposed as mechanisms to lessen this parallel programming burden. By allowing the programmer to construct a parallel application at a semantic higher level, without recourse to low-level message passing code, HPF is an effective specification language for regular, data parallel algorithms.
Reference: [8] <author> Loveman, D. B. </author> <title> High Performance Fortran. </title> <booktitle> IEEE Parallel & Distributed Technology 1, </booktitle> <month> 1 (February </month> <year> 1993), </year> <pages> 25-42. </pages>
Reference-contexts: 1 Introduction Despite the performance potential of distributed memory parallel systems, several factors have limited their widespread adoption. Of these, high performance variability and the large programming effort required to write explicitly parallel message passing code are among the most significant. Data parallel languages, like HPF <ref> [8] </ref> and its precursor, Fortran D [7], have been proposed as mechanisms to lessen this parallel programming burden. By allowing the programmer to construct a parallel application at a semantic higher level, without recourse to low-level message passing code, HPF is an effective specification language for regular, data parallel algorithms.
Reference: [9] <author> Mehra, P., Gower, M., and Bass, M. A. </author> <title> Automated modeling of message-passing programs. </title> <booktitle> In Proceedings of the ACM/IEEE Workshop on Modeling, Analysis and Simulation of Computer and Telecommunications Systems - MASCOTS'94 (Durham, </booktitle> <month> January </month> <year> 1994), </year> <pages> pp. 187-192. </pages>
Reference-contexts: Mehra, Schulback and Yan [10] conducted performance prediction by modeling message-passing programs at varying levels of syntactic detail, using statistical regression techniques to infer the parameters in the model. This modeling process was completely manual. As a next step, Mehra et al <ref> [9] </ref> presented a system for automated modeling, using a grammar-driven approach to describe tracefiles and estimation of numerical parameters in the models by statistical regression.
Reference: [10] <author> Mehra, P., Schulbach, C. H., and Yan, J. C. </author> <title> A comparison of two model-based performance-prediction techniques for message-passing parallel programs. </title> <booktitle> In Proceedings of the ACM Conference on Measurement & Modeling of Computer Systems - SIGMETRICS'94 (Nashville, </booktitle> <month> May </month> <year> 1994), </year> <pages> pp. 181-190. </pages>
Reference-contexts: In contrast to our work, Sarukkai's technique does not permit a direct comparison among the costs of individual program sections and assumes the program is already in SPMD form. Mehra, Schulback and Yan <ref> [10] </ref> conducted performance prediction by modeling message-passing programs at varying levels of syntactic detail, using statistical regression techniques to infer the parameters in the model. This modeling process was completely manual.
Reference: [11] <author> Mendes, C. L. </author> <title> Performance Prediction and Communication Optimization for Multicomput-ers. </title> <type> PhD thesis, </type> <institution> Department of Computer Science, University of Illinois at Urbana-Champaign, </institution> <year> 1995. </year> <note> In preparation. </note>
Reference-contexts: 2048 0.636 0.325 96 32 1024 0.253 0.087 191 32 4096 1.225 0.656 87 Table 4: Intel iPSC/860 Pablo Instrumentation Overhead (SOR Program) should now be clear, we omit most details of the derivation, highlighting only those places where the derivation differs from that shown for the SOR code; see <ref> [11] </ref> for details. The Fortran D code for this problem is a template. It does not include a back substitution phase, and it algorithmically initializes the matrix prior to invoking the elimination code. <p> to the time to receive a message on the iPSC/860, as indicated in x3.2, we obtain the following scalability model for Gaussian elimination T GE = K init N 2 + [c + 2d (log 2 P )] (N 1) + (2a + b) (N 1) + (2) 3 See <ref> [11] </ref> for details. 14 Constant Value (Seconds) K init 2:044 fi 10 6 K reduce 3:566 fi 10 7 K loop1 4:902 fi 10 7 K loop2 2:715 fi 10 7 K buf 1:111 fi 10 6 Table 5: Gaussian Elimination Scalability Constants (Intel iPSC/860) (K loop1 + K loop2 +
Reference: [12] <author> Noe, R. J. </author> <title> Pablo Instrumentation Environment Reference Manual. </title> <institution> Department of Computer Science, University of Illinois at Urbana-Champaign, </institution> <month> Dec. </month> <year> 1994. </year>
Reference-contexts: To reduce the cost of dynamic instrumentation and the total volume of captured data, we exploit the Pablo instrumentation library's support for real-time data reduction <ref> [12] </ref>. Rather than recording the generated events for post-mortem analysis, we compute performance metrics as the events are generated. For this example, this summarization would generate only the estimates L innerloop and S csend , the total time spent in the inner loop and message send function, respectively.
Reference: [13] <author> Parashar, M., Hariri, S., Haupt, T., and Fox, G. C. </author> <title> Interpreting the Performance of HPF/Fortran 90D. </title> <booktitle> In Proceedings of Supercomputing'94 (Washington, </booktitle> <month> November </month> <year> 1994), </year> <pages> pp. 743-752. </pages>
Reference-contexts: This technique measures overhead categories and fits the measurements to functions that relate overhead to the number of processors and problem size. Though general, the technique presumes the overhead categories are known and exploits no compile-time knowledge of program behavior. Parashar et al <ref> [13] </ref> presented a framework to predict the performance of HPF/Fortran 90D 16 C Fortran D Code C distribute a (:,CYCLIC) DO j = 1, N x = mod (3125 * init, 65536) a (i, j) = (x - 32768.0d0) / 32384.0d0 END DO C SPMD Equivalent DO j = 1, N
Reference: [14] <author> Riddle, A. </author> <title> Methematical power tools. </title> <journal> IEEE Spectrum 31, </journal> <month> 11 (November </month> <year> 1994), </year> <pages> 35-47. </pages>
Reference-contexts: In the system, Pablo SDDF serves as a flexible medium of data interchange between the compiler and Pablo. Since static performance data is presented as symbolic expressions of P and N , the system employs a symbolic expression manipulator (e.g., Maple or Mathematica <ref> [14] </ref>) to derive such expressions.
Reference: [15] <author> Sarukkai, S. R. </author> <title> Scalability Analysis Tools for SPMD Message-Passing Parallel Programs. </title> <booktitle> In Proceedings of the ACM/IEEE Workshop on Modeling, Analysis and Simulation of Computer and Telecommunications Systems - MASCOTS'94 (Durham, </booktitle> <month> January </month> <year> 1994), </year> <pages> pp. 180-186. 19 </pages>
Reference-contexts: The lower the complexity of the isoefficiency function, the easier it is to achieve scaled speedups with the given algorithm/system pair. Deriving the isoefficiency function requires detailed, extrinsic knowledge of the underlying algorithm and system and would be difficult to automate. Sarukkai <ref> [15] </ref> analyzed the scalability of parallel programs using both static program information and dynamic execution traces. His study is closest in spirit to our approach, as it also automatically builds scalability models for computation and communication sections of an SPMD program.
References-found: 15

