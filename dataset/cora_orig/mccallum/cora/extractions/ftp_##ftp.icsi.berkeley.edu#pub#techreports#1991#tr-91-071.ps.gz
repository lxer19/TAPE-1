URL: ftp://ftp.icsi.berkeley.edu/pub/techreports/1991/tr-91-071.ps.gz
Refering-URL: http://www.icsi.berkeley.edu/techreports/1991.html
Root-URL: http://www.icsi.berkeley.edu
Title: GDNN: A Gender-Dependent Neural Network for Continuous Speech Recognition  
Author: Yochai Konig Nelson Morgan Claudia Chandra 
Address: Berkeley, Berkeley, CA 94720  
Affiliation: EECS Department, University of California at  
Date: December 1991  
Pubnum: TR-91-071  
Abstract: Conventional speaker-independent speech recognition systems do not consider speaker-dependent parameters in the probability estimation of phonemes. These recognition systems are instead tuned to the ensemble statistics over many speakers. Most parametric representations of speech, however, are highly speaker dependent, and probability distributions suitable for a certain speaker may not perform as well for other speakers. It would be desirable to incorporate constraints on analysis that rely on the same speaker producing all the frames in an utterance. Our experiments take a first step towards this speaker consistency modeling by using a classification network to help generate gender-dependent phonetic probabilities for a statistical recognition system. Our results show a good classification rate for the gender classification net. Simple use of such a model to augment an existing larger network that estimates phonetic probabilities does not help speech recognition performance. However, when the new net is properly integrated in an HMM recognizer, it provides significant improvement in word accuracy. fl International Computer Science Institute, 1947 Center Street, Berkeley, CA 94704
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Bourlard, H., Morgan, .N., </author> " <title> Merging Multilayer Perceptrons and Hidden Markov Models: Some Experiments in Continuous Speech Recognition", Neural Networks: </title> <booktitle> Advances and Applications, </booktitle> <publisher> Elsevier Science Publishers B.V., North Holland, </publisher> <year> 1991. </year>
Reference-contexts: Bourlard and Wellekens [2] showed the relationship between minimizing a squared-error cost function and estimating Bayesian probabilities for the multiclass case (see Richard and Lippmann [6] for a good overview of theory and results for that problem). In particular, experiments by Bourlard and Morgan <ref> [1] </ref> have shown that the MLP outputs (when divided by the prior classification probabilities) can be used as estimates of the emission probability of HMM's. <p> The latter network (which uses 9 frames of 26 mel cepstral and delta mel cepstral coefficients) is then trained and used for HMM likelihood estimation, that is to generate (after division by priors) output probabilities for hypothesized HMMs in the recognition process (see <ref> [1] </ref> for a more detailed description of this method) . This straightforward approach would generate the same probabilities as in the gender-independent system, that is P (phonejdata) .
Reference: [2] <author> Bourlard, H., Wellekens, C. J., </author> <title> "Links Between Markov Models and Multilayer Perceptrons" , IEEE Trans. </title> <journal> on Pattern Analysis and Machine Intelligence, </journal> <volume> Vol 12, No. 12, </volume> <month> December </month> <year> 1990. </year>
Reference-contexts: 1 Introduction Reports over the last few years have described improvements in continuous speech recognition using a Multilayer Perceptron (MLP) to estimate output probabilities for Hidden Markov Models (HMMs). Bourlard and Wellekens <ref> [2] </ref> showed the relationship between minimizing a squared-error cost function and estimating Bayesian probabilities for the multiclass case (see Richard and Lippmann [6] for a good overview of theory and results for that problem).
Reference: [3] <author> Huang, X. D., Lee, K. F., and Waibel, A., </author> <title> "Connectionist Speaker Normalization and Its Applications To Speech Recognition ", Neural Networks for Signal Processing, </title> <booktitle> Proc. of the 1991 IEEE Workshop, </booktitle> <address> Princeton, New Jersey, </address> <month> October </month> <year> 1991. </year>
Reference-contexts: K.F. Lee [4] experimented with two algorithms for speaker adaptation. However, his results are not better than speaker-independent recognition. In a more recent work Huang et al <ref> [3] </ref> presented a codeword-dependent network (CDNN) for speaker normalization. The network is used as a nonlinear mapping function to transform speech data between two speakers. The goal is to minimize speaker variations with a limited amount of training sentences.
Reference: [4] <author> Lee, K. F., </author> <title> Automatic Speech Recognition: The Development of SPHINX System, </title> <publisher> Kluwer Academic Publishers, </publisher> <year> 1989. </year>
Reference-contexts: In the second stage, the HMM corresponding to the cluster is used to identify the speaker's sentence. For the cluster identification stage, the speaker has to provide a training sentence with its correct identity, in order to find the correct cluster. K.F. Lee <ref> [4] </ref> experimented with two algorithms for speaker adaptation. However, his results are not better than speaker-independent recognition. In a more recent work Huang et al [3] presented a codeword-dependent network (CDNN) for speaker normalization. The network is used as a nonlinear mapping function to transform speech data between two speakers.
Reference: [5] <author> Murveit, H., Weintraub, M., and Cohen, M., </author> <title> Training Set Issues in SRI's DECIPHER Speech Recognition System, </title> <booktitle> Proc. Speech and Natural Language Workshop, </booktitle> <month> June </month> <year> 1990, </year> <month> pp.337-340 </month>
Reference-contexts: This suggests that speaker-dependent feature estimation should be done over a large window of the input waveform, since these features are not local properties of a small input window. As has been observed for some mainstream HMM systems <ref> [5] </ref>, given enough training data, separate phonetic models for male and female speakers can be used to improve performance. Our first attack on consistency, then, is to train an MLP to estimate the probability of gender.
Reference: [6] <author> Richard, M. D., Lippmann R. P., </author> " <title> Neural Network Classifiers Estimate Bayesian a posteriori Probalilities" , Neural Computation, </title> <booktitle> 3, </booktitle> <pages> 461-483, </pages> <year> 1991. </year> <month> 6 </month>
Reference-contexts: Bourlard and Wellekens [2] showed the relationship between minimizing a squared-error cost function and estimating Bayesian probabilities for the multiclass case (see Richard and Lippmann <ref> [6] </ref> for a good overview of theory and results for that problem). In particular, experiments by Bourlard and Morgan [1] have shown that the MLP outputs (when divided by the prior classification probabilities) can be used as estimates of the emission probability of HMM's.
References-found: 6

