URL: http://www.sls.lcs.mit.edu/sree/thesis.ps
Refering-URL: http://www.sls.lcs.mit.edu/sree/publications.html
Root-URL: 
Title: A Segment-Based Speaker Verification System Using SUMMIT  
Author: by Sridevi Vedula Sarma Victor W. Zue Arthur C. Smith 
Degree: 1994 Submitted to the Department of Electrical Engineering and Computer Science in partial fulfillment of the requirements for the degree of Master of Science in Electrical Engineering and Computer Science at the  All rights reserved. Author Department of Electrical Engineering and Computer Science  Certified by  Senior Research Scientist Thesis Supervisor Accepted by  Chairman, Departmental Committee on Graduate Students  
Date: April 1997  April 20, 1997  
Affiliation: B.S., Cornell University,  MASSACHUSETTS INSTITUTE OF TECHNOLOGY  c Massachusetts Institute of Technology 1997.  
Abstract-found: 0
Intro-found: 0
Reference: [1] <author> B. Atal. </author> <title> Automatic speaker recognition based on pitch contours. </title> <journal> JASA, </journal> <volume> 52 </volume> <pages> 1687-1697, </pages> <year> 1972. </year>
Reference-contexts: Fundamental frequency (F0) also carries speaker-specific information, because F0 is dependent on accents, different phonological forms, behavior and other individualistic factors <ref> [41, 1] </ref>. To compute MFCCs, the speech signal was processed through a number of steps. First, the digitized utterances were initially passed through a pre-emphasis filter, which enhances higher frequency components of the speech samples, and attenuates lower frequency components. <p> These features attempt to measure psychophysical perceptions of intonation, stress, and rhythm, which are presumably characteristics humans use to differentiate between speakers [6]. Prosodic features have also proven to be robust in noisy environments <ref> [41, 17, 1] </ref>. Therefore, these features show great potential for the speaker verification task. To estimate F0, we used the ESPS tracker, in particular the FORMANT function [7]. For each frame of sampled data, FORMANT estimates speech formant trajectories, fundamental frequency, and other related information.
Reference: [2] <author> Y. Bennani. </author> <title> Speaker identification through modular connectionist architecture: evaluation on the timit database. </title> <booktitle> In Proceedings of the 1992 International Conference on Spoken Language Processing, </booktitle> <pages> pages 607-610, </pages> <year> 1992. </year>
Reference-contexts: As a result, better performance may result over a system which tests completely different orthography than the training data. 3.5.2 Neural Network Approach Another competitive system that uses the TIMIT corpus is a neural network-based speaker identification system built by Younes Bennani <ref> [2] </ref>. The system computes 16 frame-based cepstral coefficients derived from linear prediction coefficients (LPCs).
Reference: [3] <author> Y. Bennani and P. Gallinari. </author> <title> On the use of tddn-extracted features information in talker identification. </title> <booktitle> In Proceedings of the 1991 International Conference on Acoustics, Speech, and Signal Processing, </booktitle> <pages> pages 385-388. </pages> <publisher> IEEE, </publisher> <year> 1991. </year>
Reference-contexts: In a speaker verification system, each speaker is typically represented by a unique neural network. When a test utterance is applied, a verification decision is based on the score for the speaker's models. Some examples of systems that use neural networks to represent and classify speakers are <ref> [41, 3, 18, 28, 36] </ref>. 1.3 Discussion Thirty years ago, researchers manually computed segment-based acoustic features, and modeled the speech signal with templates consisting of acoustic centroids. Presently, systems automatically compute frame-based acoustic features, and use statistical 15 models to represent the speech signals, such as HMMs and neural networks. <p> The ff k 's are chosen to minimize the residual error. SV systems that use LPCs for acoustic features are <ref> [18, 42, 3] </ref>. For a tutorial on LPC analysis, refer to [22]. B.1 Estimation of Fundamental Frequency There are many methods to approximate F0, such as cepstral analysis and LPC analysis. We describe the approximation of F0 using linear prediction analysis below.
Reference: [4] <author> J. Campbell. </author> <title> Testing with he yoho cd-rom voice verification corpus. </title> <booktitle> In Proceedings of the 1995 International Conference on Acoustics, Speech, and Signal Processing, </booktitle> <pages> pages 341-344. </pages> <publisher> IEEE, </publisher> <year> 1995. </year>
Reference-contexts: In addition, TIMIT consists of sentences, which create a more natural environment for users than, for example, passwords or digit combinations. YOHO, a corpus specifically designed for speaker verification, contains large amounts of data per speaker and a large number of speakers. However, the corpus consists solely of digits <ref> [4] </ref>. Finally, NTIMIT, a corpus obtained by transmitting TIMIT over a telephone network, is also publicly available [16].
Reference: [5] <author> T. Cormen, C. Leiserson, and R. Rivest. </author> <title> Introduction to Algorithms. </title> <publisher> The MIT Press, </publisher> <year> 1990. </year>
Reference-contexts: To find a (sub)-optimal subset of the 17 features, we conducted a greedy search, because an exhaustive search is computationally prohibitive. A greedy search may not always produce an optimal solution. However, it significantly prunes large search spaces without much loss in optimality <ref> [5] </ref>. At every decision point in a greedy algorithm, the best choice, based on some optimality criterion, is selected. Our search criterion is the speaker verification performance of each proposed feature set. Performance is measured in terms of a distance metric describe in detail in section 3.2. <p> However, the system implements the Viterbi algorithm to find the forced alignment without scoring all possible segmentation paths. The Viterbi algorithm is based on dynamic programming methods, and prunes the search without any loss in optimality. Details of the Viterbi algorithm can be found in <ref> [33, 5] </ref>. Chapter 3 presents the detailed results of our feature search, followed by analysis.
Reference: [6] <author> G. Doddington. </author> <title> Speaker recognition-identifying people by their voices. </title> <booktitle> Proceedings of the IEEE, </booktitle> <volume> 73(11) </volume> <pages> 1651-1663, </pages> <month> November </month> <year> 1985. </year>
Reference-contexts: Of course, many details are left out of the block diagram, such as the type of text the system prompts, the features the system extracts, and 11 the speaker models and classifiers the system implements. For detailed tutorials on speaker verification, refer to <ref> [27, 6] </ref>. 1.2 Previous Research Research in speaker verification has been active for many years. In this section, we describe general approaches to speaker verification research in the last 3 decades, and illustrate these methods with a few specific examples. <p> The templates represented speech frames of words with feature centroids. Just as before, speakers were classified with distances computed between test feature vectors and centroids. One of the earliest automated speaker verification systems was implemented in the early 1980's at Texas Instruments (TI) corporate headquarters in Dallas, Texas <ref> [6] </ref>. 13 The system automatically computed features from 6 frames for each word, regardless of the word's duration. Specifically, each frame used the output of a 14 channel filter bank, uniformly spaced between 300 and 3000Hz, as a 14x1 spectral amplitude feature vector. <p> More details are given in Appendix A. 2.3.2 Prosodic Features In addition to MFCCs, we decided to explore three prosodic features: fundamental frequency (F0), energy and duration. These features attempt to measure psychophysical perceptions of intonation, stress, and rhythm, which are presumably characteristics humans use to differentiate between speakers <ref> [6] </ref>. Prosodic features have also proven to be robust in noisy environments [41, 17, 1]. Therefore, these features show great potential for the speaker verification task. To estimate F0, we used the ESPS tracker, in particular the FORMANT function [7].
Reference: [7] <author> G. Doddington and B. Secrest. </author> <title> An integrated pitch tracking algorithm for speech systems. </title> <booktitle> In Proceedings of the 1983 International Conference on Acoustics, Speech, and Signal Processing, </booktitle> <pages> pages 1352-1355, </pages> <year> 1983. </year> <month> 75 </month>
Reference-contexts: Prosodic features have also proven to be robust in noisy environments [41, 17, 1]. Therefore, these features show great potential for the speaker verification task. To estimate F0, we used the ESPS tracker, in particular the FORMANT function <ref> [7] </ref>. For each frame of sampled data, FORMANT estimates speech formant trajectories, fundamental frequency, and other related information. The ESPS formant 22 tracker implements the linear prediction analysis method, described in Appendix B, to estimate F0.
Reference: [8] <author> K. Farrel and R. Mammone. </author> <title> Speaker identification using neural networks. </title> <booktitle> In Proceedings of the 1994 International Conference on Acoustics, Speech, and Signal Processing, </booktitle> <pages> pages 165-168. </pages> <publisher> IEEE, </publisher> <year> 1994. </year>
Reference-contexts: The details of the signal processing described above is summarized in the block diagram below. More details on computing MFCCs can be found in [26]. Some SV systems that compute MFCCs are <ref> [30, 21, 8] </ref>. Figure A-2: Block Diagram for Computing MFCCs 68 Appendix B Linear Prediction Analysis The principles of linear prediction involve modeling the vocal tract system with an all-pole system function. The processing of a speech signal is shown in Figure B-1.
Reference: [9] <author> D. Gaganelis. </author> <title> A novel approach to speaker verification. </title> <booktitle> In Proceedings of the 1991 International Conference on Acoustics, Speech, and Signal Processing, </booktitle> <pages> pages 373-376. </pages> <publisher> IEEE, </publisher> <year> 1991. </year>
Reference-contexts: At the time, the system achieved 99.1% acceptance rate of valid users, and 0.7% acceptance rate of impostors. Similar speaker verification systems that use template matching classification techniques are described in <ref> [15, 9] </ref>. As mentioned above, these pioneering systems typically modeled words with templates for each speaker. Templates do not capture variations in the acoustic feature space, because each frame is represented by a fixed acoustic centroid. Consequently, the templates are not robust models of speech.
Reference: [10] <author> J. Garofolo, L. Lamel, W. Fisher, J. Fiscus, D. Pallett, and N. Dahlgren. </author> <title> Darpa timit acoustic-phonetic continuous speech corpus cd-rom. </title> <institution> In National Institute of Standards and Technology, </institution> <year> 1990. </year>
Reference-contexts: Finally, details are given on how speaker models were developed, and how speakers were classified. 18 2.2 Corpus Many researchers in speaker verification use a variety of existing corpora, while others collect their own data. We chose to use the TIMIT corpus for a variety of reasons <ref> [10] </ref>. First, TIMIT is publicly available and widely used. Therefore, it facilitates a direct comparison of our work with that of others. Second, TIMIT contains data for many speakers, and provides time-aligned phonetic transcriptions. Thus, TIMIT allows us to easily develop phonetic models for each speaker.
Reference: [11] <author> J. Glass. </author> <title> Finding acoustic regularities in speech: applications to phonetic recognition. </title> <type> PhD thesis, </type> <institution> Massachusetts Institute of Technology, </institution> <year> 1988. </year>
Reference-contexts: The multi-level description of the segmentation is illustrated in Figure 2-2 for the utterance "Delta three fifteen". The segmentation algorithm is described in more detail in <ref> [11] </ref>. and the segmentation network proposed is illustrated below the spectrogram. Finally, the phonetic labels of the utterance are given underneath the segmentation network. 24 2.5 Measurement Search Each of the segments proposed by the segmentation algorithm is described by a set of acoustic features.
Reference: [12] <author> J. Glass, J. Chang, and M. McCandless. </author> <title> A probabilistic framework for feature-based speech recognition. </title> <booktitle> In Proceedings of the 1996 International Conference on Spoken Language Processing, </booktitle> <year> 1996. </year>
Reference-contexts: An analysis of the networks proposed using this algorithm shows that on a development set, there are an average of 2.4 landmarks proposed for every transcription landmark, and 7 segments hypothesized for every transcription segment <ref> [12] </ref>. The multi-level description of the segmentation is illustrated in Figure 2-2 for the utterance "Delta three fifteen". The segmentation algorithm is described in more detail in [11]. and the segmentation network proposed is illustrated below the spectrogram.
Reference: [13] <author> D. Goddeau, E. Brill, J. Glass, C. Pao, M. Phillips, J. Polifroni, S. Seneff, and V. Zue. </author> <title> Galaxy: A human-language interface to on-line travel information. </title> <booktitle> In Proceedings of the 1994 International Conference on Spoken Language Processing, </booktitle> <year> 1994. </year>
Reference-contexts: Details on the MAP technique can be found in [39, 23]. 4.2.6 Incorporating into GALAXY Finally, we plan to incorporate our speaker verification system into the GALAXY conversational system <ref> [13] </ref>. GALAXY is a system currently under development in our group that enables information access using spoken dialogue.
Reference: [14] <author> U. Goldstein. </author> <title> An Investigation of Vowel Formant Tracks for Purposes of Speaker Identification. </title> <type> PhD thesis, </type> <institution> Massachusetts Institute of Technology, </institution> <year> 1975. </year>
Reference-contexts: However, Wolf extracted the features from manually segmented speech data. Consequently, he could not build an automated speaker verification system that derived the benefits of his knowledge-based approach. Other studies that also used knowledge-based approaches to speaker verification are described in <ref> [37, 14] </ref>. In the 1980s, researchers abandoned the notion of using segment-based measurements for speaker verification, because algorithms to automatically segment speech remained inadequate. Instead, investigators began using measurements that are easily computed automatically, such as features extracted from speech frames. Frame-based features may not necessarily distinguish speakers well.
Reference: [15] <author> S. Hangai and K. Miyauchi. </author> <title> Speaker identification based on multipulse excitation and lpc vocal-tract model. </title> <booktitle> In Proceedings of the 1990 International Conference on Spoken Language Processing, </booktitle> <pages> pages 1269-1272, </pages> <year> 1990. </year>
Reference-contexts: At the time, the system achieved 99.1% acceptance rate of valid users, and 0.7% acceptance rate of impostors. Similar speaker verification systems that use template matching classification techniques are described in <ref> [15, 9] </ref>. As mentioned above, these pioneering systems typically modeled words with templates for each speaker. Templates do not capture variations in the acoustic feature space, because each frame is represented by a fixed acoustic centroid. Consequently, the templates are not robust models of speech.
Reference: [16] <author> C. Jankowski, A. Kalyanswamy, S. Basson, and J. Spitz. </author> <title> N-timit:a phonetically balanced, continuous speech, telephone bandwidth speech database. </title> <booktitle> In ICAASP, </booktitle> <pages> pages 109-112, </pages> <year> 1990. </year> <month> 76 </month>
Reference-contexts: YOHO, a corpus specifically designed for speaker verification, contains large amounts of data per speaker and a large number of speakers. However, the corpus consists solely of digits [4]. Finally, NTIMIT, a corpus obtained by transmitting TIMIT over a telephone network, is also publicly available <ref> [16] </ref>.
Reference: [17] <author> C. Jankowski, T. Quatieri, and D. Reynolds. </author> <title> Measuring fine structure in speech: Application to speaker identification. </title> <booktitle> In Proceedings of the 1995 International Conference on Acoustics, Speech, and Signal Processing, </booktitle> <pages> pages 325-328. </pages> <publisher> IEEE, </publisher> <year> 1995. </year>
Reference-contexts: As a result, we hope to achieve competitive speaker verification performance with minimal computation. We do not investigate robustness issues specifically. However, we explore acoustic features that have been proven to be robust in the past, such as fundamental frequency and energy <ref> [41, 17] </ref>. To achieve our goal, we modified SUMMIT, a state-of-the-art speech recognition system developed at MIT [43], for speaker verification. We chose SUMMIT for the following reasons. <p> These features attempt to measure psychophysical perceptions of intonation, stress, and rhythm, which are presumably characteristics humans use to differentiate between speakers [6]. Prosodic features have also proven to be robust in noisy environments <ref> [41, 17, 1] </ref>. Therefore, these features show great potential for the speaker verification task. To estimate F0, we used the ESPS tracker, in particular the FORMANT function [7]. For each frame of sampled data, FORMANT estimates speech formant trajectories, fundamental frequency, and other related information. <p> We realized that such pruning will result in a search that is not greedy in the strictest sense of the word. Past observations have shown that MFCCs and prosodic features are useful for speaker verification <ref> [19, 41, 17] </ref>. In our search, we found that two of the three prosodic measurements investigated performed well. Specifically, energy ranked first in the set of 17 features, and F0 ranked fourth. However, duration ranked last in the first stage of the feature search.
Reference: [18] <author> I. Jou, S. Lee, and M. Lin. </author> <title> A neural network based speaker verification system. </title> <booktitle> In Proceedings of the 1990 International Conference on Spoken Language Processing, </booktitle> <pages> pages 1273-1276, </pages> <year> 1990. </year>
Reference-contexts: In a speaker verification system, each speaker is typically represented by a unique neural network. When a test utterance is applied, a verification decision is based on the score for the speaker's models. Some examples of systems that use neural networks to represent and classify speakers are <ref> [41, 3, 18, 28, 36] </ref>. 1.3 Discussion Thirty years ago, researchers manually computed segment-based acoustic features, and modeled the speech signal with templates consisting of acoustic centroids. Presently, systems automatically compute frame-based acoustic features, and use statistical 15 models to represent the speech signals, such as HMMs and neural networks. <p> The ff k 's are chosen to minimize the residual error. SV systems that use LPCs for acoustic features are <ref> [18, 42, 3] </ref>. For a tutorial on LPC analysis, refer to [22]. B.1 Estimation of Fundamental Frequency There are many methods to approximate F0, such as cepstral analysis and LPC analysis. We describe the approximation of F0 using linear prediction analysis below.
Reference: [19] <author> L. Lamel. </author> <title> A phone-based approach to non-linguistic speech feature identification. </title> <booktitle> Computer Speech and Language, </booktitle> <volume> 9 </volume> <pages> 87-103, </pages> <year> 1995. </year>
Reference-contexts: We realized that such pruning will result in a search that is not greedy in the strictest sense of the word. Past observations have shown that MFCCs and prosodic features are useful for speaker verification <ref> [19, 41, 17] </ref>. In our search, we found that two of the three prosodic measurements investigated performed well. Specifically, energy ranked first in the set of 17 features, and F0 ranked fourth. However, duration ranked last in the first stage of the feature search. <p> In order to make somewhat meaningful comparisons, we compare our system with two other systems, described below, that also use the TIMIT corpus. 52 Cohorts 53 3.5.1 HMM Approach A state-of-the-art HMM speech recognition system, built by Lamel and Gauvain <ref> [19] </ref>, was recently modified for speaker recognition. The system extracts frame-based acoustic features, which include 15 MFCCs, first derivatives of the MFCCs, energy, and the first and second derivative of energy. During training, 8 utterances (2 SA, 3 SX and 3 SI) were used to build speaker models.
Reference: [20] <author> R. Lippman. </author> <title> An introduction to computing with neural nets. </title> <journal> IEEE ASSP Magazine, </journal> <year> 1987. </year>
Reference-contexts: Recently, investigators have applied other statistical methods, such as neural networks, to speaker verification. Neural networks have also been successful in other tasks, such as speech and handwriting recognition. They are statistical pattern classifiers that utilize a dense interconnection of simple computational elements, or nodes <ref> [20] </ref>. The layers of nodes operate in parallel, with the set of node outputs in a given layer providing the inputs to each of the nodes in a subsequent layer. In a speaker verification system, each speaker is typically represented by a unique neural network.
Reference: [21] <author> M. Lund, C. Lee, and C. Lee. </author> <title> A distributed decision approach to speaker verification. </title> <booktitle> In Proceedings of the 1994 International Conference on Acoustics, Speech, and Signal Processing, </booktitle> <pages> pages 141-144. </pages> <publisher> IEEE, </publisher> <year> 1994. </year>
Reference-contexts: The details of the signal processing described above is summarized in the block diagram below. More details on computing MFCCs can be found in [26]. Some SV systems that compute MFCCs are <ref> [30, 21, 8] </ref>. Figure A-2: Block Diagram for Computing MFCCs 68 Appendix B Linear Prediction Analysis The principles of linear prediction involve modeling the vocal tract system with an all-pole system function. The processing of a speech signal is shown in Figure B-1.
Reference: [22] <author> J. Makhoul. </author> <title> Linear prediction: A tutorial review. </title> <booktitle> Proceedings of the IEEE, </booktitle> <volume> 63(4), </volume> <month> April </month> <year> 1975. </year>
Reference-contexts: The ff k 's are chosen to minimize the residual error. SV systems that use LPCs for acoustic features are [18, 42, 3]. For a tutorial on LPC analysis, refer to <ref> [22] </ref>. B.1 Estimation of Fundamental Frequency There are many methods to approximate F0, such as cepstral analysis and LPC analysis. We describe the approximation of F0 using linear prediction analysis below. Refer to [29] for the method of approximating F0 from cepstral coefficients.
Reference: [23] <author> J. Makhoul, R. Schwartz, and G. Zavaliagkos. </author> <title> Adaptation algorithms for bbn's phonetically tied mixture system. </title> <booktitle> In ARPA SCSTW, </booktitle> <pages> pages 82-87, </pages> <year> 1995. </year>
Reference-contexts: MAP provides a way to incorporate prior information into the estimation process, by assuming an a-priori distribution of the parameters that are being estimated. Details on the MAP technique can be found in <ref> [39, 23] </ref>. 4.2.6 Incorporating into GALAXY Finally, we plan to incorporate our speaker verification system into the GALAXY conversational system [13]. GALAXY is a system currently under development in our group that enables information access using spoken dialogue.
Reference: [24] <author> T. Matsui and S. Furui. </author> <title> A text independent speaker recognition method robust against utterance variations. </title> <booktitle> In Proceedings of the 1991 International Conference on Acoustics, Speech, and Signal Processing, </booktitle> <pages> pages 377-380. </pages> <publisher> IEEE, </publisher> <year> 1991. </year>
Reference-contexts: During testing, the purported speaker's cumulative likelihood score was used to make a verification decision. Furui and Mat-sui reached a performance of 98.1% speaker verification rate, using continuous HMMs. Other studies that are based on HMMs include <ref> [24, 35, 34] </ref>. Recently, investigators have applied other statistical methods, such as neural networks, to speaker verification. Neural networks have also been successful in other tasks, such as speech and handwriting recognition. They are statistical pattern classifiers that utilize a dense interconnection of simple computational elements, or nodes [20].
Reference: [25] <author> T. Matsui and S. Furui. </author> <title> Comparison of text-independent speaker recognition methods using vq distortion and discrete/continuous hmms. </title> <journal> IEEE Transactions on Signal Processing, </journal> <volume> 2(157), </volume> <year> 1992. </year>
Reference-contexts: To verify the speaker, the test sentence is scored by the HMM. The score represents the probability of an observation sequence, given a test sequence and a speaker HMM. Furui and Matsui investigated various HMM systems for speaker verification. In one study <ref> [25] </ref>, they built a word-independent speaker verification system and compared discrete HMM to continuous HMM speaker models. The speaker verification system computed frame-based cepstral features, and the corpus consisted of 23 male and 13 female speakers, recorded during three sessions over a period of 6 months. <p> Presently, systems automatically compute frame-based acoustic features, and use statistical 15 models to represent the speech signals, such as HMMs and neural networks. As Matsui and Furui showed in one of their studies <ref> [25] </ref>, most statistical methods give improved performance over template methods. In addition, frame-based measurements are easy to compute and are successful in speaker verification.
Reference: [26] <author> H. Meng. </author> <title> The use of distinctive features for automatic speech recognition, </title> <booktitle> 1991. </booktitle> <pages> 77 </pages>
Reference-contexts: The details of the signal processing described above is summarized in the block diagram below. More details on computing MFCCs can be found in <ref> [26] </ref>. Some SV systems that compute MFCCs are [30, 21, 8]. Figure A-2: Block Diagram for Computing MFCCs 68 Appendix B Linear Prediction Analysis The principles of linear prediction involve modeling the vocal tract system with an all-pole system function.
Reference: [27] <author> J. Naik. </author> <title> Speaker verification:a tutorial. </title> <journal> IEEE Communications Magazine, </journal> <pages> pages 42-47, </pages> <month> January </month> <year> 1990. </year>
Reference-contexts: Of course, many details are left out of the block diagram, such as the type of text the system prompts, the features the system extracts, and 11 the speaker models and classifiers the system implements. For detailed tutorials on speaker verification, refer to <ref> [27, 6] </ref>. 1.2 Previous Research Research in speaker verification has been active for many years. In this section, we describe general approaches to speaker verification research in the last 3 decades, and illustrate these methods with a few specific examples. <p> The system transitions from one state to another at discrete intervals of time, and each state produces a probabilistic output <ref> [27] </ref>. In a speaker verification system, each speaker is typically represented by an HMM, which 14 may capture statistics of any component of speech such as a sub-phone, phone, sub--word, word etc. To verify the speaker, the test sentence is scored by the HMM.
Reference: [28] <author> J. Oglesby and J. Mason. </author> <title> Radial basis function networks for speaker recognition. </title> <booktitle> In Proceedings of the 1991 International Conference on Acoustics, Speech, and Signal Processing, </booktitle> <pages> pages 393-396. </pages> <publisher> IEEE, </publisher> <year> 1991. </year>
Reference-contexts: In a speaker verification system, each speaker is typically represented by a unique neural network. When a test utterance is applied, a verification decision is based on the score for the speaker's models. Some examples of systems that use neural networks to represent and classify speakers are <ref> [41, 3, 18, 28, 36] </ref>. 1.3 Discussion Thirty years ago, researchers manually computed segment-based acoustic features, and modeled the speech signal with templates consisting of acoustic centroids. Presently, systems automatically compute frame-based acoustic features, and use statistical 15 models to represent the speech signals, such as HMMs and neural networks.
Reference: [29] <author> A. Oppenheim and R. Schafer. </author> <title> Discrete-Time Signal Processing. </title> <publisher> PTR Prentice Hall, Inc., </publisher> <year> 1989. </year>
Reference-contexts: MFCCs are cepstral features obtained from a system that approximates the frequency response of the human ear. Presumably, MFCCs have been successful in speaker verification because they capture inter-speaker differences. It can be shown via cepstral analysis of speech <ref> [29] </ref> that MFCCs carry vocal 21 tract information (i.e., formant frequency locations), as well as fundamental frequency information. The vocal tract system function is dependent on the shape and size of the vocal tract, which is unique to a speaker and the sound that is being produced. <p> For a tutorial on LPC analysis, refer to [22]. B.1 Estimation of Fundamental Frequency There are many methods to approximate F0, such as cepstral analysis and LPC analysis. We describe the approximation of F0 using linear prediction analysis below. Refer to <ref> [29] </ref> for the method of approximating F0 from cepstral coefficients. In order to estimate the fundamental frequency using LPC analysis, the autocorrelation of the error function is computed.
Reference: [30] <author> E. Parris and M. Carey. </author> <title> Discriminative phonemes for speaker identification. </title> <booktitle> In Proceedings of the 1994 International Conference on Spoken Language Processing, </booktitle> <pages> pages 1843-1846, </pages> <year> 1994. </year>
Reference-contexts: Therefore, we increased the number of tokens per model by collapsing phones into broad classes. For the speaker verification task, the broad classes should capture speaker-specific cues. Since past observations have shown that speaker trends are easily captured in the broad manner classes <ref> [30, 40] </ref>, we chose to collapse the 61 TIMIT-labeled phones into 6 broad manner classes. <p> The details of the signal processing described above is summarized in the block diagram below. More details on computing MFCCs can be found in [26]. Some SV systems that compute MFCCs are <ref> [30, 21, 8] </ref>. Figure A-2: Block Diagram for Computing MFCCs 68 Appendix B Linear Prediction Analysis The principles of linear prediction involve modeling the vocal tract system with an all-pole system function. The processing of a speech signal is shown in Figure B-1.
Reference: [31] <author> M. Phillips and V. Zue. </author> <title> Automatic discovery of acoustic measurements for phonetic classification. </title> <booktitle> In Proceedings of the 1992 International Conference on Spoken Language Processing, </booktitle> <pages> pages 795-798, </pages> <year> 1992. </year>
Reference-contexts: However, performance degrades significantly with the same feature set, when testing in a noisy domain. In the future, we plan to use a program called SAILS to help us extract optimal and robust features. SAILS <ref> [31] </ref> was originally used to extract optimal acoustic attributes that signify phonetic contrasts for speech recognition. It allows the user to vary parameters such as frequency range and time interval for measuring any set of features for selected speakers' phonemes, and their left and right phonetic contexts.
Reference: [32] <author> L. Rabiner. </author> <title> A tutorial on hidden markov models and selected applications in speech recognition. </title> <booktitle> Proceedings of the IEEE, </booktitle> <volume> 77(2) </volume> <pages> 257-286, </pages> <month> February </month> <year> 1989. </year>
Reference-contexts: As a result, researchers began applying the technology to speaker verification. Specifically, speaker verification research focused on investigating hidden Markov models (HMMs), because HMMs were becoming very successful in speech recognition <ref> [32] </ref>. Many investigators simply modified existing speech recognition systems for speaker verification, in hopes of achieving high performance. HMMs are developed from frame-based features; therefore, investigators neglected to further explore segment-based features.
Reference: [33] <author> L. Rabiner and B.H. Juang. </author> <title> Fundamentals of Speech Recognition. </title> <publisher> PTR Prentice Hall, Inc., </publisher> <year> 1993. </year>
Reference-contexts: However, the system implements the Viterbi algorithm to find the forced alignment without scoring all possible segmentation paths. The Viterbi algorithm is based on dynamic programming methods, and prunes the search without any loss in optimality. Details of the Viterbi algorithm can be found in <ref> [33, 5] </ref>. Chapter 3 presents the detailed results of our feature search, followed by analysis.
Reference: [34] <author> Rosenburg, DeLong, Lee, Juang, and Soong. </author> <title> The use of cohort normalized scores for speaker verification. </title> <booktitle> In Proceedings of the 1992 International Conference on Spoken Language Processing, </booktitle> <pages> pages 599-602, </pages> <year> 1992. </year>
Reference-contexts: During testing, the purported speaker's cumulative likelihood score was used to make a verification decision. Furui and Mat-sui reached a performance of 98.1% speaker verification rate, using continuous HMMs. Other studies that are based on HMMs include <ref> [24, 35, 34] </ref>. Recently, investigators have applied other statistical methods, such as neural networks, to speaker verification. Neural networks have also been successful in other tasks, such as speech and handwriting recognition. They are statistical pattern classifiers that utilize a dense interconnection of simple computational elements, or nodes [20].
Reference: [35] <author> A. Rosenburg, C. Lee, and S. Gokcen. </author> <title> Connected word talker verification using whole word hidden markov models. </title> <booktitle> In Proceedings of the 1991 International Conference on Acoustics, Speech, and Signal Processing, </booktitle> <pages> pages 381-384. </pages> <publisher> IEEE, </publisher> <year> 1991. </year>
Reference-contexts: During testing, the purported speaker's cumulative likelihood score was used to make a verification decision. Furui and Mat-sui reached a performance of 98.1% speaker verification rate, using continuous HMMs. Other studies that are based on HMMs include <ref> [24, 35, 34] </ref>. Recently, investigators have applied other statistical methods, such as neural networks, to speaker verification. Neural networks have also been successful in other tasks, such as speech and handwriting recognition. They are statistical pattern classifiers that utilize a dense interconnection of simple computational elements, or nodes [20].
Reference: [36] <author> L. Rudasi and S. Zahorian. </author> <title> Text-independent talker identification with neural networks. </title> <booktitle> In Proceedings of the 1991 International Conference on Acoustics, Speech, and Signal Processing, </booktitle> <pages> pages 389-392. </pages> <publisher> IEEE, </publisher> <year> 1991. </year> <month> 78 </month>
Reference-contexts: In a speaker verification system, each speaker is typically represented by a unique neural network. When a test utterance is applied, a verification decision is based on the score for the speaker's models. Some examples of systems that use neural networks to represent and classify speakers are <ref> [41, 3, 18, 28, 36] </ref>. 1.3 Discussion Thirty years ago, researchers manually computed segment-based acoustic features, and modeled the speech signal with templates consisting of acoustic centroids. Presently, systems automatically compute frame-based acoustic features, and use statistical 15 models to represent the speech signals, such as HMMs and neural networks.
Reference: [37] <author> M. Sambur. </author> <title> Speaker Recognition and Verification using Linear Prediction Anal--ysis. </title> <type> PhD thesis, </type> <institution> Massachusetts Institute of Technology, </institution> <year> 1972. </year>
Reference-contexts: However, Wolf extracted the features from manually segmented speech data. Consequently, he could not build an automated speaker verification system that derived the benefits of his knowledge-based approach. Other studies that also used knowledge-based approaches to speaker verification are described in <ref> [37, 14] </ref>. In the 1980s, researchers abandoned the notion of using segment-based measurements for speaker verification, because algorithms to automatically segment speech remained inadequate. Instead, investigators began using measurements that are easily computed automatically, such as features extracted from speech frames. Frame-based features may not necessarily distinguish speakers well.
Reference: [38] <author> Y. Tohkura. </author> <title> A weighted cepstral distance measure for speech recognition. </title> <booktitle> Computer Speech and Language, </booktitle> <address> 35:1414, </address> <year> 1987. </year>
Reference-contexts: Specifically, the speaker models consist of diagonal Gaussian probability density functions (pdfs). We chose to represent the acoustic space with Gaussian distributions because features of speech data, such as cepstral coefficients, fit these bell-shaped curves well <ref> [38] </ref>. Diagonal distributions were implemented because they have few parameters to train (diagonal covariance matrices), and thus do not require much training data to accurately estimate the parameters. However, features that are correlated are not modeled well with diagonal covariance matrices.
Reference: [39] <author> H. </author> <title> Van Trees. Detection, Estimation, and Modulation Theory (Part I). </title> <publisher> John Wiley and Sons, Inc., </publisher> <year> 1968. </year>
Reference-contexts: Detailed results of the normalization are given in section 3.4. For each feature set, we found S nearest neighbors (cohorts) for each speaker using the Mahalanobis distance metric <ref> [39] </ref>. Specifically, 1 and 2 , 2 1 and 2 d-dimensional mean vectors and dxd-dimensional covariance matrices for two speaker models, respectively. <p> MAP provides a way to incorporate prior information into the estimation process, by assuming an a-priori distribution of the parameters that are being estimated. Details on the MAP technique can be found in <ref> [39, 23] </ref>. 4.2.6 Incorporating into GALAXY Finally, we plan to incorporate our speaker verification system into the GALAXY conversational system [13]. GALAXY is a system currently under development in our group that enables information access using spoken dialogue.
Reference: [40] <author> J. Wolf. </author> <title> Acoustic Measurements for Speaker Recognition. </title> <type> PhD thesis, </type> <institution> Mas-sachusetts Institute of Technology, </institution> <year> 1969. </year>
Reference-contexts: Consequently, investigators resorted to manually segmenting speech data and estimating features to conduct their studies, which constrained the amount of data observed, and the statistical validity of their results. 12 One example of research done in this era is the doctoral thesis of Wolf <ref> [40] </ref>. Wolf found specific segmental measurements that discriminated well among speakers. He investigated 17 different features such as, fundamental frequency (F0), glottal source spectral slopes, duration, and features characterizing vowel and nasal spectra. During training, 21 male speakers repeated 6 short sentences 10 times. <p> Therefore, we increased the number of tokens per model by collapsing phones into broad classes. For the speaker verification task, the broad classes should capture speaker-specific cues. Since past observations have shown that speaker trends are easily captured in the broad manner classes <ref> [30, 40] </ref>, we chose to collapse the 61 TIMIT-labeled phones into 6 broad manner classes.
Reference: [41] <author> B. Yegnanarayana, S. Wagh, and S. Rajendra. </author> <title> A speaker verification system using prosodic features. </title> <booktitle> In Proceedings of the 1994 International Conference on Spoken Language Processing, </booktitle> <pages> pages 1867-1870, </pages> <year> 1994. </year>
Reference-contexts: In a speaker verification system, each speaker is typically represented by a unique neural network. When a test utterance is applied, a verification decision is based on the score for the speaker's models. Some examples of systems that use neural networks to represent and classify speakers are <ref> [41, 3, 18, 28, 36] </ref>. 1.3 Discussion Thirty years ago, researchers manually computed segment-based acoustic features, and modeled the speech signal with templates consisting of acoustic centroids. Presently, systems automatically compute frame-based acoustic features, and use statistical 15 models to represent the speech signals, such as HMMs and neural networks. <p> As a result, we hope to achieve competitive speaker verification performance with minimal computation. We do not investigate robustness issues specifically. However, we explore acoustic features that have been proven to be robust in the past, such as fundamental frequency and energy <ref> [41, 17] </ref>. To achieve our goal, we modified SUMMIT, a state-of-the-art speech recognition system developed at MIT [43], for speaker verification. We chose SUMMIT for the following reasons. <p> Fundamental frequency (F0) also carries speaker-specific information, because F0 is dependent on accents, different phonological forms, behavior and other individualistic factors <ref> [41, 1] </ref>. To compute MFCCs, the speech signal was processed through a number of steps. First, the digitized utterances were initially passed through a pre-emphasis filter, which enhances higher frequency components of the speech samples, and attenuates lower frequency components. <p> These features attempt to measure psychophysical perceptions of intonation, stress, and rhythm, which are presumably characteristics humans use to differentiate between speakers [6]. Prosodic features have also proven to be robust in noisy environments <ref> [41, 17, 1] </ref>. Therefore, these features show great potential for the speaker verification task. To estimate F0, we used the ESPS tracker, in particular the FORMANT function [7]. For each frame of sampled data, FORMANT estimates speech formant trajectories, fundamental frequency, and other related information. <p> We realized that such pruning will result in a search that is not greedy in the strictest sense of the word. Past observations have shown that MFCCs and prosodic features are useful for speaker verification <ref> [19, 41, 17] </ref>. In our search, we found that two of the three prosodic measurements investigated performed well. Specifically, energy ranked first in the set of 17 features, and F0 ranked fourth. However, duration ranked last in the first stage of the feature search. <p> In the past, duration has been proven to be robust and speaker-specific <ref> [41] </ref>. However, the classes we selected did not reflect different duration characteristics. As a result, the variances of duration were large for all speakers models, and the performance scores using duration ranked last in the scores for the 1-dimensional stage for both searches conducted.
Reference: [42] <author> H. Yin and T. Zhou. </author> <title> Speaker recognition using static and dynamic cepstral feature by a learning neural network. </title> <booktitle> In Proceedings of the 1990 International Conference on Spoken Language Processing, </booktitle> <pages> pages 1277-1280, </pages> <year> 1990. </year>
Reference-contexts: The ff k 's are chosen to minimize the residual error. SV systems that use LPCs for acoustic features are <ref> [18, 42, 3] </ref>. For a tutorial on LPC analysis, refer to [22]. B.1 Estimation of Fundamental Frequency There are many methods to approximate F0, such as cepstral analysis and LPC analysis. We describe the approximation of F0 using linear prediction analysis below.
Reference: [43] <author> V. Zue, M. Phillips, and S. Seneff. </author> <title> The mit summit speech recognition system: A progress report. </title> <booktitle> In Proceedings DARPA Speech and Natural Language Workshop, </booktitle> <pages> pages 179-189, </pages> <year> 1989. </year> <month> 79 </month>
Reference-contexts: We do not investigate robustness issues specifically. However, we explore acoustic features that have been proven to be robust in the past, such as fundamental frequency and energy [41, 17]. To achieve our goal, we modified SUMMIT, a state-of-the-art speech recognition system developed at MIT <ref> [43] </ref>, for speaker verification. We chose SUMMIT for the following reasons. First, SUMMIT treats the speech signal as a concatenation of segments, which allows us to capitalize on the speaker-discriminating abilities of such units.
References-found: 43

