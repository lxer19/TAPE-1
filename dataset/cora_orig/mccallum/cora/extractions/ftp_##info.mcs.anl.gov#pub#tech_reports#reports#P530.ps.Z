URL: ftp://info.mcs.anl.gov/pub/tech_reports/reports/P530.ps.Z
Refering-URL: http://www.mcs.anl.gov/publications/abstracts/abstracts95.htm
Root-URL: http://www.mcs.anl.gov
Email: thakur@mcs.anl.gov choudhar@cat.syr.edu  
Title: An Extended Two-Phase Method for Accessing Sections of Out-of-Core Arrays  
Author: Rajeev Thakur Alok Choudhary 
Address: Argonne, IL 60439 Syracuse, NY 13244  
Affiliation: Math. and Computer Science Div. Dept. of Elect. and Comp. Eng. Argonne National Laboratory Syracuse University  
Note: To appear in Scientific Programming  
Abstract: A number of applications on parallel computers deal with very large data sets that cannot fit in main memory. In such applications, data must be stored in files on disks and fetched into memory during program execution. Parallel programs with large out-of-core arrays stored in files must read/write smaller sections of the arrays from/to files. In this paper, we describe a method for accessing sections of out-of-core arrays efficiently. Our method, the extended two-phase method, uses collective I/O: Processors cooperate to combine several I/O requests into fewer larger granularity requests, reorder requests so that the file is accessed in proper sequence, and eliminate simultaneous I/O requests for the same data. In addition, the I/O workload is divided among processors dynamically, depending on the access requests. We present performance results obtained from two real out-of-core parallel applications|matrix multiplication and a Laplace's equation solver|and several synthetic access patterns, all on the Intel Touchstone Delta. These results indicate that the extended two-phase method significantly outperformed a direct (non-collective) method for accessing out-of-core array sections. fl This work was supported in part by the Scalable I/O Initiative, a multiagency project funded by the Advanced Research Projects Agency (contract number DABT63-94-C-0049), the Department of Energy, the National Aeronautics and Space Administration, and the National Science Foundation; by a National Science Foundation Young Investigator Award (CCR-9357840); and by a grant from Intel Scalable Systems Division. This work was performed in part using the Intel Touchstone Delta System operated by Caltech on behalf of the Concurrent Supercomputing Consortium. Access to this facility was provided by the Center for Research on Parallel Computation. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <institution> Applications Working Group of the Scalable I/O Initiative. </institution> <note> Preliminary Survey of I/O Intensive Applications. Scalable I/O Initiative Working Paper Number 1. On the World-Wide Web at http://www.ccsf.caltech.edu/SIO/SIO apps.ps, </note> <year> 1994. </year>
Reference-contexts: The program must perform I/O to access data from disks. Examples of such applications are Hartree-Fock calculations in chemistry, very large Fast Fourier Transforms to detect faint radio pulsars, seismic data processing, weather and climate modeling, 3D turbulence simulations, scattering and radiation problems in computational electromagnetics, and several others <ref> [1] </ref>. Multidimensional arrays are widely used as data structures in scientific programs. Scientific applications with large out-of-core data sets may therefore have one or more out-of-core multidimensional arrays stored in files.
Reference: [2] <author> R. Bordawekar, A. Choudhary, and J. del Rosario. </author> <title> An Experimental Performance Evaluation of Touchstone Delta Concurrent File System. </title> <booktitle> In Proceedings of the 7th ACM International Conference on Supercomputing, </booktitle> <pages> pages 367-376, </pages> <month> July </month> <year> 1993. </year>
Reference-contexts: Each I/O node is connected to two disks, resulting in a total of 64 disks. Intel's Concurrent File System (CFS) provides parallel access to files. By default, CFS stripes files across all 64 disks in 4-Kbyte blocks. See <ref> [2] </ref> for a detailed discussion of the performance of CFS. We studied the performance of the extended two-phase method versus the direct method extensively for several synthetic access patterns as well as for two real out-of-core parallel applications| matrix multiplication and a Laplace's equation solver.
Reference: [3] <author> R. Bordawekar, A. Choudhary, and R. Thakur. </author> <title> Data Access Reorganizations in Compiling Out-of-Core Data Parallel Programs on Distributed Memory Machines. </title> <type> Technical Report SCCS-622, </type> <institution> NPAC, Syracuse University, </institution> <month> September </month> <year> 1994. </year> <note> On the World-Wide Web at ftp://erc.cat.syr.edu/ece/choudhary/PASSION/access reorg.ps.Z. </note>
Reference-contexts: We draw overall conclusions in Section 6. 2 Two Out-of-Core Parallel Applications Here we describe the I/O access patterns of two out-of-core parallel applications|matrix multiplication and a Laplace's equation solver. 2.1 Out-of-Core Matrix Multiplication We consider an out-of-core GAXPY algorithm for matrix multiplication, described in <ref> [3] </ref>. Let A, B, and C be n fi n matrices such that C = A fi B.
Reference: [4] <author> R. Bordawekar, J. del Rosario, and A. Choudhary. </author> <title> Design and Evaluation of Primitives for Parallel I/O. </title> <booktitle> In Proceedings of Supercomputing '93, </booktitle> <pages> pages 452-461, </pages> <month> November </month> <year> 1993. </year>
Reference-contexts: The extended two-phase method specifies a procedure for performing collective I/O to access out-of-core array sections. Other examples of collective I/O are disk-directed I/O [11] and server-directed collective I/O [12]. 3 Extended Two-Phase Method The two-phase method, proposed in <ref> [7, 4] </ref>, is a collective I/O technique for reading an entire in-core array from a file into a distributed array in main memory, and conversely, for writing a distributed 4 in-core array to a file. I/O is done in two phases. <p> In the second phase, data is redistributed among processors to the desired distribution. Since I/O cost is orders of magnitude more than communication cost, the cost incurred by the second phase is negligible. This two-phase approach is found to perform well for all array distributions <ref> [7, 4] </ref>. We have extended the basic two-phase method to access sections of out-of-core arrays.
Reference: [5] <author> P. Corbett, D. Feitelson, J. Prost, and S. Baylor. </author> <title> Parallel Access to Files in the Vesta File System. </title> <booktitle> In Proceedings of Supercomputing '93, </booktitle> <pages> pages 472-481, </pages> <month> November </month> <year> 1993. </year>
Reference-contexts: We call this the direct method. The Vesta and PIOFS file systems on the IBM SP <ref> [5, 9] </ref> and the nCUBE file system [6] do provide support for the user to specify a logical view of the data to be read and use a single call to read data. Each processor's request, however, is serviced independently, and the file systems do not perform collective I/O.
Reference: [6] <author> E. DeBenedictis and J. del Rosario. </author> <title> nCUBE Parallel I/O Software. </title> <booktitle> In Proceedings of 11 th International Phoenix Conference on Computers and Communications, </booktitle> <pages> pages 117-124, </pages> <month> April </month> <year> 1992. </year>
Reference-contexts: We call this the direct method. The Vesta and PIOFS file systems on the IBM SP [5, 9] and the nCUBE file system <ref> [6] </ref> do provide support for the user to specify a logical view of the data to be read and use a single call to read data. Each processor's request, however, is serviced independently, and the file systems do not perform collective I/O.
Reference: [7] <author> J. del Rosario, R. Bordawekar, and A. Choudhary. </author> <title> Improved Parallel I/O via a Two-Phase Runtime Access Strategy. </title> <booktitle> In Proceedings of the Workshop on I/O in Parallel Computer Systems at IPPS '93, </booktitle> <pages> pages 56-70, </pages> <month> April </month> <year> 1993. </year>
Reference-contexts: The extended two-phase method specifies a procedure for performing collective I/O to access out-of-core array sections. Other examples of collective I/O are disk-directed I/O [11] and server-directed collective I/O [12]. 3 Extended Two-Phase Method The two-phase method, proposed in <ref> [7, 4] </ref>, is a collective I/O technique for reading an entire in-core array from a file into a distributed array in main memory, and conversely, for writing a distributed 4 in-core array to a file. I/O is done in two phases. <p> In the second phase, data is redistributed among processors to the desired distribution. Since I/O cost is orders of magnitude more than communication cost, the cost incurred by the second phase is negligible. This two-phase approach is found to perform well for all array distributions <ref> [7, 4] </ref>. We have extended the basic two-phase method to access sections of out-of-core arrays.
Reference: [8] <author> J. del Rosario and A. Choudhary. </author> <title> High Performance I/O for Parallel Computers: Problems and Prospects. </title> <booktitle> IEEE Computer, </booktitle> <pages> pages 59-68, </pages> <month> March </month> <year> 1994. </year>
Reference-contexts: The data required by many of these applications must be stored in files on disks, as it is too large to fit in main memory <ref> [8] </ref>. The program must perform I/O to access data from disks.
Reference: [9] <author> IBM Corp. </author> <title> IBM AIX Parallel I/O File System: Installation, Administration, and Use. Document Number SH34-6065-01, </title> <month> August </month> <year> 1995. </year> <month> 25 </month>
Reference-contexts: We call this the direct method. The Vesta and PIOFS file systems on the IBM SP <ref> [5, 9] </ref> and the nCUBE file system [6] do provide support for the user to specify a logical view of the data to be read and use a single call to read data. Each processor's request, however, is serviced independently, and the file systems do not perform collective I/O.
Reference: [10] <author> K. Klimkowski and R. van de Geijn. </author> <title> Anatomy of an Out-of-Core Dense Linear Solver. </title> <booktitle> In Proceedings of the 1995 International Conference on Parallel Processing, </booktitle> <pages> pages III-29|III-33, </pages> <month> August </month> <year> 1995. </year>
Reference-contexts: The algorithm is repeated for further iterations until it converges. 2.3 Accessing Out-of-Core Array Sections In the above applications, processors access two-dimensional sub-blocks of out-of-core arrays. This type of access pattern also occurs in other applications, such as out-of-core LU solvers <ref> [10] </ref>. Since 3 0 1 0 1 0 1 arrays are usually stored in a file in either column-major order (as in Fortran) or row-major order (as in C), the data required by each processor is not located contiguously in the file.
Reference: [11] <author> D. Kotz. </author> <title> Disk-directed I/O for MIMD Multiprocessors. </title> <booktitle> In Proceedings of the 1994 Symposium on Operating Systems Design and Implementation, </booktitle> <pages> pages 61-74, </pages> <month> November </month> <year> 1994. </year> <note> Updated as Technical Report PCS-TR94-226, </note> <institution> Dept. of Computer Science, Dartmouth College. </institution>
Reference-contexts: Processors can cooperate among themselves to perform I/O in large chunks and in the proper order, a process known as collective I/O. The extended two-phase method specifies a procedure for performing collective I/O to access out-of-core array sections. Other examples of collective I/O are disk-directed I/O <ref> [11] </ref> and server-directed collective I/O [12]. 3 Extended Two-Phase Method The two-phase method, proposed in [7, 4], is a collective I/O technique for reading an entire in-core array from a file into a distributed array in main memory, and conversely, for writing a distributed 4 in-core array to a file.
Reference: [12] <author> K. Seamons, Y. Chen, P. Jones, J. Jozwiak, and M. Winslett. </author> <title> Server-Directed Collective I/O in Panda. </title> <booktitle> In Proceedings of Supercomputing '95, </booktitle> <month> December </month> <year> 1995. </year>
Reference-contexts: The extended two-phase method specifies a procedure for performing collective I/O to access out-of-core array sections. Other examples of collective I/O are disk-directed I/O [11] and server-directed collective I/O <ref> [12] </ref>. 3 Extended Two-Phase Method The two-phase method, proposed in [7, 4], is a collective I/O technique for reading an entire in-core array from a file into a distributed array in main memory, and conversely, for writing a distributed 4 in-core array to a file.
Reference: [13] <author> R. Thakur. </author> <title> Runtime Support for In-Core and Out-of-Core Data-Parallel Programs. </title> <type> PhD thesis, </type> <institution> Dept. of Electrical and Computer Engineering, Syracuse University, </institution> <month> May </month> <year> 1995. </year>
Reference-contexts: This section may also contain some data that is not required by any processor. If the processor attempts to read only the useful data, it may result in a number of small strided accesses. To avoid this, the processor uses an optimization we proposed previously, called data sieving <ref> [14, 13] </ref>. The processor reads a column (for column-major order) of the section at a time in a single operation into a temporary buffer. This may include some unwanted data. <p> The FDAT is analyzed in the same way as in the read algorithm. Each processor calculates the minimum and maximum of all indices in its FDAT, which determines the smallest section containing all the data to be written to the file domain. The processor uses data sieving <ref> [14, 13] </ref> to write the useful data in this section. Note that, since there may be "holes" between the useful data to be written, an extra read operation is required before writing. This extra read is not required if the useful data is located contiguously in the file.
Reference: [14] <author> R. Thakur, R. Bordawekar, A. Choudhary, R. Ponnusamy, and T. Singh. </author> <title> PASSION Runtime Library for Parallel I/O. </title> <booktitle> In Proceedings of the Scalable Parallel Libraries Conference, </booktitle> <pages> pages 119-128, </pages> <month> October </month> <year> 1994. </year>
Reference-contexts: This section may also contain some data that is not required by any processor. If the processor attempts to read only the useful data, it may result in a number of small strided accesses. To avoid this, the processor uses an optimization we proposed previously, called data sieving <ref> [14, 13] </ref>. The processor reads a column (for column-major order) of the section at a time in a single operation into a temporary buffer. This may include some unwanted data. <p> The FDAT is analyzed in the same way as in the read algorithm. Each processor calculates the minimum and maximum of all indices in its FDAT, which determines the smallest section containing all the data to be written to the file domain. The processor uses data sieving <ref> [14, 13] </ref> to write the useful data in this section. Note that, since there may be "holes" between the useful data to be written, an extra read operation is required before writing. This extra read is not required if the useful data is located contiguously in the file.
Reference: [15] <author> R. Thakur, A. Choudhary, R. Bordawekar, S. More, and S. Kuditipudi. </author> <title> Passion: Optimized I/O for Parallel Applications. </title> <booktitle> IEEE Computer, </booktitle> <month> June </month> <year> 1996. </year>
Reference-contexts: The best way to use the extended two-phase method is to implement it as a library routine that can be called from an application program. We have implemented it in the PASSION runtime library <ref> [15] </ref>, which is available on the World-Wide Web at http://www.cat.syr.edu/passion.html.
Reference: [16] <author> The MPI-IO Committee. </author> <title> MPI-IO: A Parallel File I/O Interface for MPI, </title> <note> Version 0.5. On the World-Wide Web at http://lovelace.nas.nasa.gov/MPI-IO/mpi-io-report.0.5.ps, April 1996. 26 </note>
Reference-contexts: The I/O workload can be divided among the processors in this subset. The extended two-phase method is not specific to any particular machine, file system, or architecture; it can be easily implemented by using any file-system interface, or by using portable interfaces, such as MPI-IO <ref> [16] </ref>, resulting in portable implementations. It can also be easily modified and tuned for any particular system|by defining file domains appropriately and possibly using 24 a different algorithm for interprocessor communication.
References-found: 16

