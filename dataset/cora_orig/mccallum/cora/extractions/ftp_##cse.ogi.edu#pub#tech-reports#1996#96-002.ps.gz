URL: ftp://cse.ogi.edu/pub/tech-reports/1996/96-002.ps.gz
Refering-URL: ftp://cse.ogi.edu/pub/tech-reports/README.html
Root-URL: http://www.cse.ogi.edu
Title: integration with a summation over these points. This is repeated 500 times, picking new random
Author: Marcel Dekker, Inc. Friedman, J. H. Stuetzle, W. Geman, S., Bienenstock, E. Doursat, R. Girosi, F., Jones, M. Poggio, T. Hastie, T. J. Tibshirani, R. J. Chapman and Hall. Moody, J. E. Yarvin, N. J. E. Moody, S. J. Hanson R. P. Lippmann, Moody, J. Rognvaldsson, T. Poggio, T. Girosi, F. Powell, M. J. Mason M. Cox, Wahba, G. 
Keyword: Acknowledgements  
Address: 76(376), 817-823.  
Note: 5 Discussion  References Eubank, R. L. (1988),  (1981), `Projection pursuit regression', J. Amer. Stat. Assoc.  eds, `Advances in Neural Information Processing Systems 4', Morgan Kaufmann Publishers, San Mateo, CA, pp. 1048-55.  eds, `Algorithms for Approximation', Clarendon Press, Oxford.  
Abstract: The correlation is very high for both the global and local forms, although the global form is slightly better. To verify that this finding is not spurious, we repeat our Monte Carlo simulations for many different network architectures with varying D and N , using the same method for sampling weights. These results (presented in Moody & Rognvaldsson (1996)) show that the same conclusions hold as the number of hidden or input units increase or decrease. As anticipated, the regularizers are better estimates of S(W; m) when the number of inputs grows or when the order m is increased. Our regularizers R(W; m) are the first general class of m th -order smoothing regularizers to be proposed for projective basis function networks. The forms of the regularizers differ fundamentally from quadratic weight decay, in that they distinguish the roles of the input weights v j and output weights u j , and capture the interactions between them. Our regularizers apply to PBFs with large classes of transfer functions g[], including sigmoids. Our approach differs from that developed for smoothing splines and smoothing radial basis functions, in that we derive smoothing regularizers for given classes of units g[; x], rather than derive the forms of the units g[] by requiring them to be Greens functions of the smoothing operator S(). Our approach thus has the advantage that it can be applied to the types of networks most often used in practice. In a longer paper (Moody & R ognvaldsson 1996), we present the application of our approach to radial basis function networks and present extensive simulation results for both PBFs and RBFs. Both authors thank Steve Rehfuss and Dr. Lizhong Wu for stimulating input. John Moody thanks Volker Tresp for a provocative discussion at a 1991 Neural Networks Workshop sponsored by the Deutsche Informatik Akademie. We gratefully acknowledge support for this work from ARPA and ONR (grant N00014-92-J-4062), NSF (grant CDA-9503968), the Swedish Institute, and the Swedish Research Council for Engineering Sciences (contract TFR-282-95-847). 
Abstract-found: 1
Intro-found: 1
Reference: <author> Eubank, R. L. </author> <year> (1988), </year> <title> Spline Smoothing and Nonparametric Regression, </title> <publisher> Marcel Dekker, Inc. </publisher>
Reference: <author> Friedman, J. H. & Stuetzle, W. </author> <year> (1981), </year> <title> `Projection pursuit regression', </title> <journal> J. Amer. Stat. Assoc. </journal> <volume> 76(376), </volume> <pages> 817-823. </pages>
Reference: <author> Geman, S., Bienenstock, E. & Doursat, R. </author> <year> (1992), </year> <title> `Neural networks and the bias/variance dilemma', </title> <booktitle> Neural Computation 4(1), </booktitle> <pages> 1-58. </pages>
Reference: <author> Girosi, F., Jones, M. & Poggio, T. </author> <year> (1995), </year> <title> `Regularization theory and neural networks architectures', </title> <booktitle> Neural Computation 7, </booktitle> <pages> 219-269. </pages>
Reference: <author> Hastie, T. J. & Tibshirani, R. J. </author> <year> (1990), </year> <title> Generalized Additive Models, </title> <booktitle> Vol. 43 of Monographs on Statistics and Applied Probability, </booktitle> <publisher> Chapman and Hall. </publisher>
Reference: <author> Moody, J. E. & Yarvin, N. </author> <year> (1992), </year> <title> Networks with learned unit response functions, </title> <editor> in J. E. Moody, S. J. Hanson & R. P. Lippmann, eds, </editor> <booktitle> `Advances in Neural Information Processing Systems 4', </booktitle> <publisher> Morgan Kaufmann Publishers, </publisher> <address> San Mateo, CA, </address> <pages> pp. 1048-55. </pages>
Reference: <author> Moody, J. & Rognvaldsson, T. </author> <year> (1996), </year> <title> Smoothing regularizers for projective and radial basis function networks, </title> <note> Manuscript in preparation. </note>
Reference: <author> Poggio, T. & Girosi, F. </author> <year> (1990), </year> <title> `Networks for approximation and learning', </title> <booktitle> IEEE Proceedings 78(9). </booktitle>
Reference: <author> Powell, M. </author> <year> (1987), </year> <title> Radial basis functions for multivariable interpolation: a review., </title> <editor> in J. Mason & M. Cox, eds, </editor> <title> `Algorithms for Approximation', </title> <publisher> Clarendon Press, Oxford. </publisher>

References-found: 9

