URL: ftp://ftp.sics.se/pub/SICS-reports/Reports/SICS-R--91-17--SE.ps.Z
Refering-URL: http://www.sics.se/libindex.html
Root-URL: 
Title: A Performance Study of the DDM -a Cache-Only Memory Architecture  
Author: Erik Hagersten, Par Andersson, Anders Landin and Seif Haridi 
Date: November 1991.  
Pubnum: SICS Research Report R91:17  
Abstract: Large-scale multiprocessors suffer from long latencies for remote accesses. Caching is by far the most popular technique for hiding such delays. Caching not only hides the delay, but also decreases the network load. Cache-Only Memory Architectures (COMA), have no physically shared memory. Instead, all the memory resources are invested in caches, resulting in caches of the largest possible size. A datum has no home, and is moved by a protocol between the caches, according to its usage. It might exist in multiple caches. Even though no shared memory exists, the architecture still provides the shared memory view to a programmer. Simulation results from large programs running on 64 processors indicate that the COMA adapts well to existing programs for shared memory. They also show that an application with a poor locality can benefit by adopting to the COMA principle of no home for data, resulting in a reduced execution time of a factor three. In a COMA, a large majority of the misses are invalidation misses, or share misses caused by write-once/read-many behavior, or a producer-consumer relation, i.e. would benefit from write broadcast. A new protocol is proposed that behaves like a write-invalidate protocol by default for all data. A reader can detect its need for a write-broadcast behavior for a datum, which it enables by sending a subscribe request for the datum to the writer.
Abstract-found: 1
Intro-found: 1
Reference: [BD91] <author> L. Barroso and M. Dubois. </author> <title> Cache Coherence on a Slotted Ring. </title> <booktitle> In Proceedings of the International Conference on Parallel Processing, </booktitle> <pages> pages 230-237, </pages> <year> 1991. </year>
Reference-contexts: Other structures might be possible for improving the bandwidth, like a ring-based bus time-slotted into different address domains <ref> [BD91] </ref>. 4 SIMULATION TECHNIQUE Inspired by the Tango simulator at Stanford [DGH91], we have developed an efficient execution-driven simulation method that models the parallel applications as if they were running on a real physical implementation of the architecture.
Reference: [CKA91] <author> D. Chaiken, J. Kubiatowicz, and A. Agarwal. </author> <title> LimitLESS Directories: A Scalable Cache Coherence Scheme. </title> <booktitle> In Proceedings of the 4th Annual ASPLOS, </booktitle> <year> 1991. </year>
Reference-contexts: Even though no shared memory exists, the processors, and subsequently 2 the programmer, are still provided with a coherent shared-memory view of the system. The COMA is reminiscent of a non-uniform memory architecture (NUMA), like the DASH [LLG + 90] and Alewife <ref> [CKA91] </ref>, in that all the shared memory is physically divided between the processors.
Reference: [DGH91] <author> H. Davis, S. Goldschmidt, and J.L. Hennessy. </author> <title> Tango: a Multiprocessor Simulation and Tracing System. </title> <type> Tech. Report No CSL-TR-90-439, </type> <institution> Stanford University, </institution> <year> 1991. </year>
Reference-contexts: Other structures might be possible for improving the bandwidth, like a ring-based bus time-slotted into different address domains [BD91]. 4 SIMULATION TECHNIQUE Inspired by the Tango simulator at Stanford <ref> [DGH91] </ref>, we have developed an efficient execution-driven simulation method that models the parallel applications as if they were running on a real physical implementation of the architecture.
Reference: [EK89] <author> S.J. Eggers and R.H. Katz. </author> <title> Evaluating the Performance of Four Snooping Cache Coherency Protocols. </title> <booktitle> In Proceedings of the 16th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 2-15, </pages> <year> 1989. </year> <title> 4 And thereby relaxing the access order model to processor consistency. </title> <type> 15 </type>
Reference-contexts: Often, cache coherence is maintained by a cache-coherence protocol implemented in hardware. The strategy used is either write-invalidate, where all other copies but the one updated are destroyed, or, write-update, where the other copies instead are updated. Both approaches have their advantages <ref> [EK89] </ref>. A COMA coherence protocol can adapt the techniques used in other coherence protocols, but must be extended to search for and retrieve a datum on a read miss. The protocol must also make sure that the last copy of a datum is not lost upon replacement [HHW90]. <p> This is especially true for large caches. Write broadcast protocols have been studied that turn the broadcasting off if nobody uses the broadcaster values (competitive snooping [KMRS86]). But the communication overhead is still too large <ref> [EK89] </ref>. Instead we propose a protocol that is a write-invalidate protocol by default. A read miss to a datum in state invalidated 3 tells us that the datum has been here but was invalidated by a writer.
Reference: [G + 83] <author> A. Gottlieb et al. </author> <title> THE NYU Ultracomputer Designing a MIMD Shared-Memory Parallel Machine. </title> <journal> IEEE Transactions on Computers, </journal> <pages> pages 175-189, </pages> <month> Feb. </month> <year> 1983. </year>
Reference-contexts: The reply is returned in 1 A selection mechanism makes sure that only one AM receives the request 4 at most 2L 1 bus transactions. The transient states will also detect and handle combining effects, like combining reads, shown to be useful for large multiprocessors <ref> [P + 85, G + 83] </ref>. Write-invalidate is implemented in a general network by the writing node sending out an erase request and waiting for write acknowledges to be received from each individual AM with a copy of the datum.
Reference: [GHG + 91] <author> A. Gupta, J. Hennessy, K. Gharachorloo, T. Mowry, and W.-D. Weber. </author> <title> Comparative Evaluation of Latency Reducing and Tolerating Techniques. </title> <booktitle> In Proceedings of the 18th Annual International Symposium on Computer Architecture, </booktitle> <year> 1991. </year>
Reference-contexts: Such behavior has a negative impact on the goal of making multiprocessors that scale well with the number of processors. Several techniques have been proposed to reduce and/or hide latency due to remote ac cesses <ref> [GHG + 91] </ref>. Prefetching techniques hide the latency in a network by stimulating an access (e.g. retrieval of a datum) before it is requested by the processor.
Reference: [HHW90] <author> E. Hagersten, S. Haridi, and D.H.D. Warren. </author> <title> The Cache-Coherence Protocol of the Data Diffusion Machine. </title> <editor> In M. Dubois and S. Thakkar, editors, </editor> <title> Cache and Interconnect Architectures in Multiprocessors. </title> <publisher> Kluwer Academic Publisher, Norwell, </publisher> <address> Mass, </address> <year> 1990. </year>
Reference-contexts: A COMA coherence protocol can adapt the techniques used in other coherence protocols, but must be extended to search for and retrieve a datum on a read miss. The protocol must also make sure that the last copy of a datum is not lost upon replacement <ref> [HHW90] </ref>. The address space of a COMA corresponds to the physical addresses of conventional architectures. There is however nothing very physical about them nor are they addresses, since they do not tell where a datum resides. <p> Between each level in the hierarchy are set-associative state memories, directories, storing state information for all data in their subsystem, but not their values. In this performance study we have used a hierarchical write-invalidate cache-coherence protocol which uses the directories to make the coherence traffic as local as possible <ref> [HHW90] </ref>. The state of a datum in the directory indicates if the datum resides in the subsystem of the directory, and if so, whether other copies might exist outside the subsystem (stable states: exclusive, shared and invalid). <p> Many different protocols for the DDM have been designed [HLH91, LHH91]. In this study we use the simplest protocol, supporting sequential consistency <ref> [HHW90] </ref>. For the configuration 1 fi 1 and 4 fi 1, the DDM network has not been simulated. Instead a 100% hit rate in the AM is assumed. The hit-rates for instructions in the processor caches and the AMs are close to 100% for all configurations and applications.
Reference: [HLH] <author> E. Hagersten, A. Landin, and S. Haridi. </author> <title> Moving the Shared Memory Closer to the Processor - DDM. </title> <institution> Swedish Institute of Computer Science, </institution> <note> Report 1990. To appear in IEEE Computer. </note>
Reference-contexts: Finally we end by suggesting a new mechanism that would remove most of the misses for a COMA running the studied applications. 2 CACHE-ONLY MEMORY ARCHITECTURES A Cache-Only Memory Architecture, (COMA), is characterized by its lack of any physically shared memory <ref> [HLH] </ref>. Instead processors host a large (set-) associative memory. The task of such a memory is twofold.
Reference: [HLH91] <author> E. Hagersten, A. Landin, and S. Haridi. </author> <title> Multiprocessor Consistency and Synchronization Through Transient Cache States. </title> <editor> In M. Dubois and S. Thakkar, editors, </editor> <title> Scalable Shared-Memory Multiprocessors. </title> <publisher> Kluwer Academic Publisher, </publisher> <address> Nor-well, Mass, </address> <year> 1991. </year>
Reference-contexts: Many different protocols for the DDM have been designed <ref> [HLH91, LHH91] </ref>. In this study we use the simplest protocol, supporting sequential consistency [HHW90]. For the configuration 1 fi 1 and 4 fi 1, the DDM network has not been simulated. Instead a 100% hit rate in the AM is assumed.
Reference: [HS89] <author> M. Hill and A.J. Smith. </author> <title> Evaluating Associativity in CPU Caches. </title> <journal> IEEE Transactions on Computers, </journal> <volume> 38(12) </volume> <pages> 1612-1630, </pages> <month> December </month> <year> 1989. </year>
Reference-contexts: The accesses are divided into different categories. 6 REMOVING SOME OF THE MISSES Hill and Smith classify cache-misses into three categories, capacity misses (the cache is not large enough), conflict misses (there is not enough associativity), and compulsory misses (the datum is being touched for the first time) <ref> [HS89] </ref>. They also note that compulsory misses are the major source of misses for uni processors with large caches. We refer to the sum of these three categories as "uni-misses". When running statically scheduled programs that run in steps, like WATER and MP3D, compulsory misses disappears after the first step.
Reference: [KMRS86] <author> A.R. Karlin, M.S. Manasse, L. Rudolph, and D.D. Sleator. </author> <title> Competitive Snoopy Caching. </title> <booktitle> In Proceedings of the 27th Annual International Symposium on Foundation of Computer Science, </booktitle> <year> 1986. </year>
Reference-contexts: However, write broadcast may also create unnecessary traffic for many applications that shades the positive effects of the better hit rate. This is especially true for large caches. Write broadcast protocols have been studied that turn the broadcasting off if nobody uses the broadcaster values (competitive snooping <ref> [KMRS86] </ref>). But the communication overhead is still too large [EK89]. Instead we propose a protocol that is a write-invalidate protocol by default. A read miss to a datum in state invalidated 3 tells us that the datum has been here but was invalidated by a writer.
Reference: [Lar90] <author> J. Larus. </author> <title> Abstract Execution: A Technique for Efficient Tracing Programs. </title> <type> Tech Report, </type> <institution> Computer Science Department, University of Wisconsin at Madison, </institution> <year> 1990. </year>
Reference-contexts: The parallel applications are developed in, or ported to, C to run on a SUN SPARC station as multiple processes sharing memory under UNIX. A modified gcc compiler, Abstract Execution (AE) <ref> [Lar90] </ref>, is used to produce processes that not only execute the programs, but also produce a stream of information when doing so. The level of detail in the information 6 stream is selectable, and has been the full address trace of instructions and data for this study. <p> We are grateful to the people at Stan-ford, who made the SPLASH benchmarks available to us [SWG91], and to James Larus for providing us with the AE tracing tool <ref> [Lar90] </ref>. Mats Grindal modified the simulator and gathered the statistics for classifying the misses.
Reference: [LHH91] <author> A. Landin, E. Hagersten, and S. Haridi. </author> <title> Race-free Interconnection Networks and Multiprocessor Consistency. </title> <booktitle> In Proceedings of the 18th Annual International Symposium on Computer Architecture, </booktitle> <year> 1991. </year>
Reference-contexts: In a hierarchical network the topmost node of the subsystem in which all the copies of the item reside may send the acknowledge. The acknowledge might be received by the writing AM even before all other AMs have received the erase request. Still, sequential consistency is provided <ref> [LHH91] </ref>. This not only reduces the traffic in the network, but also shortens the waiting time for the acknowledge. Although most memory accesses tend to be localized in the machine, the higher level in the hierarchy may nevertheless demand a higher bandwidth than the lower systems, which creates a bottleneck. <p> Many different protocols for the DDM have been designed <ref> [HLH91, LHH91] </ref>. In this study we use the simplest protocol, supporting sequential consistency [HHW90]. For the configuration 1 fi 1 and 4 fi 1, the DDM network has not been simulated. Instead a 100% hit rate in the AM is assumed.
Reference: [LLG + 90] <author> D. Lenoski, J. Laundo, K. Gharachorloo, A. Gupta, and J. Hennessy. </author> <title> The Directory-Based Cache Coherence Protocol for the DASH Multiprocessor. </title> <booktitle> In Proceedings of the 17th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 148-159, </pages> <year> 1990. </year>
Reference-contexts: Even though no shared memory exists, the processors, and subsequently 2 the programmer, are still provided with a coherent shared-memory view of the system. The COMA is reminiscent of a non-uniform memory architecture (NUMA), like the DASH <ref> [LLG + 90] </ref> and Alewife [CKA91], in that all the shared memory is physically divided between the processors.
Reference: [LRW91] <author> M.S. Lamm, E.E. Rothberg, and E. Wolf. </author> <title> The Cache Performance and Optimizations of Blocked Algorithms. </title> <booktitle> In Proceedings of the 4th Annual ASPLOS, </booktitle> <year> 1991. </year>
Reference-contexts: The programs studied are the MP3D, a "wind tunnel" simulator; Cholesky, a factorization program for sparse matrices; and Water, a simulator of water molecules. We also present numbers from a matrix multiplication application that uses blocking <ref> [LRW91] </ref>. The MP3D is also run in a modified version, which explores the data diffusion capability of our system. Our results show a good performance for the studied applications. <p> Note also the large hit rate in the data cache for the bcsstk14, which is an example of how important it is to simulate real-sized problems. Matrix is a program multiplying two 500-by-500 matrices using a blocked algorithm <ref> [LRW91] </ref>. The blocking algorithm is interesting, since it makes an effective use of caches. Once a portion of a matrix (a block) has been read to a cache, it is being used many times before it is replaced, and a new block read into the cache.
Reference: [P + 85] <author> G.F. Pfister et al. </author> <title> The IBM Research Parallel Processor Prototype (RP3). </title> <booktitle> In Proceedings of the 1985 International Conference on Parallel Processing, </booktitle> <address> Chigago, </address> <year> 1985. </year>
Reference-contexts: The reply is returned in 1 A selection mechanism makes sure that only one AM receives the request 4 at most 2L 1 bus transactions. The transient states will also detect and handle combining effects, like combining reads, shown to be useful for large multiprocessors <ref> [P + 85, G + 83] </ref>. Write-invalidate is implemented in a general network by the writing node sending out an erase request and waiting for write acknowledges to be received from each individual AM with a copy of the datum.
Reference: [SL88] <author> R.T. Short and H.M. Levy. </author> <title> A Simulation Study of Two-Level Caches. </title> <booktitle> In Pro--ceedings of the 15th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 81-88, </pages> <year> 1988. </year>
Reference-contexts: Taking some of the load off the network also results in less contention and, subsequently, shorter access time. In this study we focus on exploring the effects of large second-level caches <ref> [SL88] </ref>. One specialized form of caching, data diffusion, is also studied. In data diffusion, a datum has no home and might be moved to live in any or many of the large associative memories local to the processors.
Reference: [SWG91] <author> J.S. Sing, W.-D. Weber, and A Gupta. </author> <title> SPLASH: Stanford Parallel Applications for Shared Memory. </title> <institution> Stanford University, </institution> <type> Report, </type> <month> April </month> <year> 1991. </year>
Reference-contexts: The architecture simulated is the Data Diffusion Machine prototype, that is currently being implemented at SICS. The simulator has been parameterized by data from the ongoing hardware implementation. We study three unchanged applications from the Stanford SPLASH selection of engineering programs <ref> [SWG91] </ref>. The programs studied are the MP3D, a "wind tunnel" simulator; Cholesky, a factorization program for sparse matrices; and Water, a simulator of water molecules. We also present numbers from a matrix multiplication application that uses blocking [LRW91]. <p> The number of simulated processors has a small effect on the slow down if the application simulated has an ideal speedup, which allows for large machines running large applications to be studied. 5 RESULTS In this study, three programs from Stanford Parallel Applications for Shared Memory <ref> [SWG91] </ref> (SPLASH), and one matrix multiplication program are used. The SPLASH programs represent applications used in an engineering computing environment. The applications used are written in C and use the synchronization primitives provided by the Argonne National Laboratory (ANL) macro package. <p> In the graphs the linear (Speedup = #processors) are shown. For the SPLASH applications, the unit delay speedups from a simulation where all memory accesses take one cycle, reported by Sing et. al. <ref> [SWG91] </ref>, are also shown. 0 20 40 60 0 10 20 30 40 50 60 70 80 Speedup Processors WATER UNIT DELAY LINEAR delay is reported for 288 molecules, and does not include cold-start effects. <p> The simulation is performed in discrete time steps, where each molecule is moved according to its velocity and possible collision with other molecules, the flying object, and the boundaries. This "move-phase" accounts for 93% of the execution time profiled on a single DECstation 3100 <ref> [SWG91] </ref>. Between each move-phase, some administrative phases are performed, like moving or removing particles from the entrance of the wind-tunnel, and calculating collision probabilities for each space cell. We simulate 75.000 particles and 14x24x7 space cells, resulting in a total work space of about 4 Mbytes. <p> MP3D is normally run for many simulation steps. To avoid the cold-start effect in our measurements, we present the steady-state behavior of the last four simulation steps. MP3D-DIFF is a modified version of MP3D with some 30 lines added. The modification is along the lines suggested in <ref> [SWG91] </ref> by Cheriton et. al. Instead of statically assigning a portion of the particles to each processor, MP3D-DIFF assigns one portion of the space to each processor. During the move phase, a processor handles all the particles that are currently in its space. <p> We are grateful to the people at Stan-ford, who made the SPLASH benchmarks available to us <ref> [SWG91] </ref>, and to James Larus for providing us with the AE tracing tool [Lar90]. Mats Grindal modified the simulator and gathered the statistics for classifying the misses.
Reference: [WH88] <author> D. H. D. Warren and S. Haridi. </author> <title> Data Diffusion Machine-multiprocessor. </title> <booktitle> In International Conference on Fifth Generation Computer Systems 1988. </booktitle> <publisher> ICOT, </publisher> <year> 1988. </year> <month> 17 </month>
Reference-contexts: How much smaller depends on what degree of sharing should be guaranteed by the system, i.e. how many additional copies of data should be allowed. This can be changed dynamically by the operating system. 3 Data Diffusion Machine, a Hierarchical COMA The Data Diffusion Machine (DDM) <ref> [WH88] </ref> is a COMA architecture with a hierarchical network topology and set-associative AMs at its tips storing data-value, address-tag and state for all data. Between each level in the hierarchy are set-associative state memories, directories, storing state information for all data in their subsystem, but not their values.
References-found: 19

