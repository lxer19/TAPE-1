URL: ftp://ftp.cs.utexas.edu/pub/mooney/papers/dolphin-chill-proposal-93.ps.Z
Refering-URL: http://www.cs.utexas.edu/users/ml/abstracts.html
Root-URL: 
Email: zelle@cs.utexas.edu  
Phone: (512) 471-9589  
Title: Learning Search-Control Heuristics for Logic Programs: Applications to Speedup Learning and Language Acquisition  
Author: John M. Zelle 
Date: March 5, 1993  
Address: Austin, TX 78712  
Affiliation: Department of Computer Sciences University of Texas  
Abstract: This paper presents a general framework, learning search-control heuristics for logic programs, which can be used to improve both the efficiency and accuracy of knowledge-based systems expressed as definite-clause logic programs. The approach combines techniques of explanation-based learning and recent advances in inductive logic programming to learn clause-selection heuristics that guide program execution. Two specific applications of this framework are detailed: dynamic optimization of Prolog programs (improving efficiency) and natural language acquisition (improving accuracy). In the area of program optimization, a prototype system, Dolphin is able to transform some intractable specifications into polynomial-time algorithms, and outperforms competing approaches in several benchmark speedup domains. A prototype language acquisition system, Chill is also described. It is capable of automatically acquiring semantic grammars, which uniformly incorprate syntactic and semantic constraints to parse sentences into case-role representations. Initial experiments show that this approach is able to construct accurate parsers which generalize well to novel sentences and significantly outperform previous approaches to learning case-role mapping based on connectionist techniques. Planned extensions of the general framework and the specific applications as well as plans for further evaluation are also discussed.
Abstract-found: 1
Intro-found: 1
Reference: <author> Allen, J. F. </author> <year> (1987). </year> <title> Natural Language Understanding. </title> <address> Menlo Park, CA: Benjamin/Cummings. </address>
Reference: <author> Anderson, J. R. </author> <year> (1977). </year> <title> Induction of augmented transition networks. </title> <journal> Cognitive Science, </journal> <volume> 1 </volume> <pages> 125-157. </pages>
Reference: <author> Anderson, J. R. </author> <year> (1983). </year> <title> The Architecture of Cognition. </title> <address> Cambridge, MA: </address> <publisher> Harvard University Press. </publisher>
Reference: <author> Berwick, B. </author> <year> (1985). </year> <title> The Acquisition of Syntactic Knowledge. </title> <address> Cambridge, MA: </address> <publisher> MIT Press. </publisher>
Reference-contexts: Making use of these negative examples allows the lifting of many "hard-wired" constraints based on specific theories of grammar (e.g., the X-bar assumptions of <ref> (Berwick, 1985) </ref>). Further benefit can be gained from the use of modern induction techniques, particularly those incorporating constructive induction. Appropriate word-classes can be automatically acquired rather than being given to the system a priori. <p> Langley and Anderson (Langley, 1982; Anderson, 1983) have independently posited acquisition mechanisms based on learning search control in production systems. These systems were cognitively motivated and addressed the task of language generation rather than the case-role analysis task examined here. Berwick's LPARSIFAL <ref> (Berwick, 1985) </ref> acquired parsing rules for a type of shift-reduce parser. His system was linguistically motivated and incorporated many constraints specific to the theory of language assumed. In contrast, Chill relies on implicit negative examples and first-order induction techniques to avoid commitment to any specific model of grammar.
Reference: <author> Berwick, R. C. and Pilato, S. </author> <year> (1987). </year> <title> Learning syntax by automata induction. </title> <journal> Machine Learning, </journal> <volume> 2(1) </volume> <pages> 9-38. </pages>
Reference: <author> Black, E., Lafferty, J., and Roukaos, S. </author> <year> (1992). </year> <title> Development and evaluation of a broad-coverage probabilistic grammar of English-language computer manuals. </title> <booktitle> In Proceedings of the 30th Annual Meeting of the Association for Computational Linguistics, </booktitle> <pages> pages 185-192. </pages> <address> Newark, Delaware. </address>
Reference: <author> Bratko, I. </author> <year> (1990). </year> <title> Prolog Programming for Artificial Intelligence. </title> <publisher> Reading:MA: Addison Wesley. </publisher>
Reference-contexts: The N-queens problem is adapted from a Prolog program given in <ref> (Bratko, 1990) </ref>. The problem is to find a placement of N queens on an NxN chessboard such that no queen is attacking another. The program implements a generate and test strategy where a configuration is represented by a permutation of the list, [1..N].
Reference: <author> Braudaway, W. and Tong, C. </author> <year> (1989). </year> <title> Automatic synthesis of constrained generators. </title> <booktitle> In Proceedings of the Eleventh International Joint conference on Artificial intelligence. </booktitle> <address> Detroit, MI. </address>
Reference: <author> Brown, J. S. and Burton, R. R. </author> <year> (1975). </year> <title> Multiple representations of knowledge for tutorial reasoning. </title> <editor> In Bobrow, D. and Collins, A., editors, </editor> <booktitle> Representation and Understanding. </booktitle> <address> New York: </address> <publisher> Academic Press. </publisher>
Reference: <author> Bruynooghe, M., Schreye, D. D., and Krekels, B. </author> <year> (1989). </year> <title> Compiling control. </title> <journal> Journal of Logic Programming, </journal> <volume> 6 </volume> <pages> 135-243. </pages>
Reference: <author> Chase, M., Zweban, M., Piazza, R., Burger, J., Maglio, P., and Hirsh, H. </author> <year> (1989). </year> <title> Approximating learned search control knowledge. </title> <booktitle> In Proceedings of the Sixth International Workshop on Machine Learning, </booktitle> <pages> pages 40-42. </pages> <address> Ithaca, NY. </address> <note> 33 Chien, </note> <author> S. </author> <year> (1989). </year> <title> Using and refining simplifications: Explanation-based learning of plans in in-tractable domains. </title> <booktitle> In Proceedings of the Eleventh International Joint conference on Artificial intelligence. </booktitle> <address> Detroit, MI. </address>
Reference-contexts: The first use of approximations in learning control-rules was probably MetaLEX (Keller, 1987) which used a simple technique for removing conditions. Most other recent investigations have not focussed on learning control-rules (Ellman, 1988; Tadepalli, 1989; Chien, 1989) or have not employed induction <ref> (Chase et al., 1989) </ref>. Yoo and Fisher (Yoo and Fisher, 1991) combine induction and explanation to improve performance in a problem-solving framework. They enhance the utility of EBL-macros by clustering them in a Cobweb-style classification tree maintaining explanations at various levels of detail.
Reference: <author> Cohen, W. W. </author> <year> (1990). </year> <title> Learning approximate control rules of high utility. </title> <booktitle> In Proceedings of the Seventh International Conference on Machine Learning, </booktitle> <pages> pages 268-276. </pages> <address> Austin, TX. </address>
Reference-contexts: A deterministic program is one that computes a (partial) function; that is, it finds at most a single solution for each unique instantiation of its input arguments. A nondeterministic program computes a relation, admitting multiple solutions. Search-control in a logic program can be viewed as a clause selection problem <ref> (Cohen, 1990) </ref>. Clause selection is the process of deciding which of several applicable program clauses should be used to reduce a particular subgoal during the course of a proof. If program clauses are always applied appropriately, the program executes deterministically (without backtracking) and produces only correct solutions. <p> point, all of the positive examples are covered, and we have found exactly the definition of useful insert 1 which was presented earlier. 3.3 Preliminary Experimental Results The Dolphin system has been evaluated on five problem domains: naivesort, N-queens, and three "standard" EBL problems LEX, RW, and BW borrowed from <ref> (Cohen, 1990) </ref>. The N-queens problem is adapted from a Prolog program given in (Bratko, 1990). The problem is to find a placement of N queens on an NxN chessboard such that no queen is attacking another. <p> This makes the program similar to naivesort, which also permutes a list and tests. LEX is a simplified symbolic integration solver using state-space search with iterative deepening. The actual Prolog code for the solver is the same as that used in <ref> (Cohen, 1990) </ref>. RW and BW are planning domains utilizing means-ends analysis planners, which were automatically generated from operator definitions. RW is based on the STRIPS robot world of (Fikes et al., 1972). BW is generated from the blocks world operators of (Nilsson, 1980). <p> Unfortunately, the optimized program may not be complete. There may be problems that the original program can solve, but whose solutions have been pruned from 12 the search space of the optimized version. In order to guarantee the completeness of the final program, the strategy used by <ref> (Cohen, 1990) </ref> of retaining the original clauses is adopted. In testing, the problem is first attempted using the optimized program. If this fails, the original program is then used to find a solution to the problem. <p> The LEX training and testing problems are from (Keller, 1987). In the planning domains, problems were generated by taking a random walk of bounded length in the state space of the planner in a manner identical to <ref> (Cohen, 1990) </ref>. The results of the experiments on these problems are graphically summarized in Figure 7 and Figure 8. In all domains tested, Dolphin was able to significantly improve the performance of the initial program. <p> They enhance the utility of EBL-macros by clustering them in a Cobweb-style classification tree maintaining explanations at various levels of detail. By contrast, Dolphin uses supervised learning methods to acquire explicit search-control rules. The most closely related work is AxA-EBL <ref> (Cohen, 1990) </ref> mentioned above. AxA-EBL "explains" correct uses of a clause by compiling out a generalized macro for the subgoal to which the clause was applied. A pool of candidate control rules is formed by considering all k-bounded approximations of these macros.
Reference: <author> DeJong, G. F. and Mooney, R. J. </author> <year> (1986). </year> <title> Explanation-based learning: An alternative view. </title> <journal> Machine Learning, </journal> <volume> 1(2) </volume> <pages> 145-176. </pages>
Reference: <author> Dietterich, T. </author> <year> (1986). </year> <title> Learning at the knowledge level. </title> <journal> Machine Learning, </journal> <volume> 1 </volume> <pages> 287-316. </pages>
Reference-contexts: The naivesort example above illustrates how the learning of clause selection heuristics effects a form of test 8 incorporation <ref> (Dietterich, 1986) </ref>, which can sometimes dramatically enhance the efficiency of an algorithm. Our specific learning problem then is: given a Prolog program and a set of training problems, produce a new program which incorporates clause selection heuristics to solve the training problems more efficiently.
Reference: <author> Ellman, T. </author> <year> (1988). </year> <title> Approximate theory formation: An explanation-based approach. </title> <booktitle> In Proceedings of the Seventh National Conference on Artificial Intelligence, </booktitle> <pages> pages 564-569. </pages> <address> St. Paul, MN. </address>
Reference: <author> Fikes, R. E., Hart, P. E., and Nilsson, N. J. </author> <year> (1972). </year> <title> Learning and executing generalized robot plans. </title> <journal> Artificial Intelligence, </journal> <volume> 3(4) </volume> <pages> 251-288. </pages>
Reference-contexts: The actual Prolog code for the solver is the same as that used in (Cohen, 1990). RW and BW are planning domains utilizing means-ends analysis planners, which were automatically generated from operator definitions. RW is based on the STRIPS robot world of <ref> (Fikes et al., 1972) </ref>. BW is generated from the blocks world operators of (Nilsson, 1980). Control rules were learned for a single recursive predicate having seven arguments and averaging about 5 conjuncts per clause. This predicate consisted of 18 and 12 clauses in RW and BW respectively.
Reference: <author> Fillmore, C. J. </author> <year> (1968). </year> <title> The case for case. In Bach, </title> <editor> E. and Harms, R. T., editors, </editor> <booktitle> Universals in Linguistic Theory. </booktitle> <address> New York: </address> <publisher> Holt, Reinhart and Winston. </publisher>
Reference-contexts: Traditional case theory <ref> (Fillmore, 1968) </ref> decomposes a sentence into a proposition represented by the main verb and various arguments such as agent, patient, and instrument, represented by noun phrases. The basic mapping problem is to decide which sentence constituents fill which roles.
Reference: <author> Flann, N. S. and Dietterich, T. G. </author> <year> (1989). </year> <title> A study of explanation-based methods for inductive learning. </title> <journal> Machine Learning, </journal> <volume> 4(2) </volume> <pages> 187-226. </pages>
Reference-contexts: However, there is no clear separation between these goals and methods. Inductive learning can be used to improve efficiency (Mitchell et al., 1983) and analytical methods such as EBL can sometimes be used to learn new domain knowledge that improves accuracy <ref> (Flann and Dietterich, 1989) </ref>. One framework that cleanly unifies these goals and techniques is that of learning search-control rules for a problem solver. The notion of intelligence as controlled search pervades AI. Learning rules to control search may improve both the efficiency and accuracy of a problem solver.
Reference: <author> Gazdar, G. and Mellish, C. </author> <year> (1989). </year> <title> Natural Language Processing in Prolog. </title> <address> New York: </address> <publisher> Adison-Wesley Publishing Company. </publisher>
Reference: <author> Gil, Y. </author> <year> (1991). </year> <title> A specification of process planning for PRODIGY. </title> <type> Technical Report CMU-CS-91-179, </type> <institution> Pittsburgh, PA: School of Computer Science, Carnegie Mellon University. </institution>
Reference-contexts: Most of the existing benchmarks in speedup learning are "toy problems," such as blocks-world planning. However, realistic problem domains are beginning to be explored. The Prodigy group at Carnegie Mellon University has recently assembled domain theories and problem sets for process planning <ref> (Gil, 1991) </ref> and logistics transportation (Veloso, 1992).
Reference: <author> Keller, R. </author> <year> (1987). </year> <title> The Role of Explicit Contextual Knowledge in Learning Concepts to Improve Performance. </title> <type> PhD thesis, </type> <institution> New Brunswick, N: Rutgers University. </institution> <note> Also appears as tech. report ML-TR-7. </note>
Reference-contexts: The data for the N-queens domain consisted of the nine problems corresponding to the 4-queens through 12-queens problems. The four largest problems were used as the test set, and training was done on successively larger subsets of the smaller problems. The LEX training and testing problems are from <ref> (Keller, 1987) </ref>. In the planning domains, problems were generated by taking a random walk of bounded length in the state space of the planner in a manner identical to (Cohen, 1990). The results of the experiments on these problems are graphically summarized in Figure 7 and Figure 8. <p> Dolphin on the other hand, uses induction to select the most useful pieces of EBL generalizations. Also, Dolphin takes advantage of recent progress in relational learning, namely, Foil. The first use of approximations in learning control-rules was probably MetaLEX <ref> (Keller, 1987) </ref> which used a simple technique for removing conditions. Most other recent investigations have not focussed on learning control-rules (Ellman, 1988; Tadepalli, 1989; Chien, 1989) or have not employed induction (Chase et al., 1989).
Reference: <author> Kijsirikul, B., Numao, M., and Shimura, M. </author> <year> (1992). </year> <title> Discrimination-based constructive induction of logic programs. </title> <booktitle> In Proceedings of the Tenth National Conference on Artificial Intelligence, </booktitle> <pages> pages 44-49. </pages> <address> San Jose, CA. </address>
Reference-contexts: of which is a completed noun phrase whose head is animate, then attach this phrase as the agent of the top of stack." The Chill induction algorithm may be viewed as a composite of proven techniques from other systems, notably Golem (Muggleton and Feng, 1992), Foil (Quinlan, 1990), and Champ <ref> (Kijsirikul et al., 1992) </ref>. The intuition is that we want to find a small (hence general) definition which discriminates between the positive and negative examples. <p> This process is analogous to determining a small set of arguments for newly constructed predicates and can be efficiently performed using greedy search (see Section 4.2.2 and <ref> (Kijsirikul et al., 1992) </ref>). In the state-space search example, the current goal and state should provide sufficient 3 A set of examples is inconsistent if it contains the same example description labeled as both positive and negative. 28 information to unambiguously choose an operator.
Reference: <author> Laird, J., Rosenbloom, P., and Newell, A. </author> <year> (1986). </year> <title> Chunking in soar: The anatomy of a general learning mechanism. </title> <journal> Machine Learning, </journal> <volume> 1(1). </volume>
Reference: <author> Langley, P. </author> <year> (1982). </year> <title> Language acquisition through error recovery. </title> <journal> Cognition and Brain Theory, </journal> <volume> 5. </volume>
Reference: <author> Langley, P. </author> <year> (1985). </year> <title> Learning to search: From weak methods to domain specific heuristics. </title> <journal> Cognitive Science, </journal> <volume> 9(2) </volume> <pages> 217-260. </pages>
Reference: <author> Liu, R.-L. and Soo, V.-W. </author> <year> (1992). </year> <title> Augmenting and efficiently utilizing domain theory in explanation-based natural language acquisition. </title> <booktitle> In Proceedings of the Ninth International Conference on Machine Learning, </booktitle> <pages> pages 475-479. </pages> <address> Aberdeen, Scotland. </address>
Reference: <author> Lloyd, J. W., </author> <title> editor (1984). </title> <booktitle> Foundations of Logic Programming. </booktitle> <address> Berlin: </address> <publisher> Springer-Verlag. </publisher>
Reference-contexts: This proposal embraces such an approach and offers a general framework for learning search-control heuristics in logic programs. The motivation for this research is three-fold. First, logic programming provides a very well-understood representational and computational platform upon which to build. Logic programming has a firm theoretical foundation <ref> (Lloyd, 1984) </ref> and a practical instantiation in the programming language, Prolog. The value of the former is well-known. The value of the latter should not be underestimated. Efficient commercial Prolog implementations are readily avaiable, and the language is considered a viable tool for the production of real-world systems.
Reference: <author> Marcus, M. </author> <year> (1980). </year> <title> A Theory of Syntactic Recognition for Natural Language. </title> <address> Cambridge, MA: </address> <publisher> MIT Press. 34 McClelland, </publisher> <editor> J. L. and Kawamoto, A. H. </editor> <year> (1986). </year> <title> Mechnisms of sentence processing: Assigning roles to constituents of sentences. </title> <editor> In Rumelhart, D. E. and McClelland, J. L., editors, </editor> <booktitle> Parallel Distributed Processing, </booktitle> <volume> Vol. II, </volume> <pages> pages 318-362. </pages> <address> Cambridge, MA: </address> <publisher> MIT Press. </publisher>
Reference-contexts: Shift-reduce parsing has proven to be an efficient mechanism for natural language analysis (Tomita, 1986), and deterministic variants have been argued to accurately model certain aspects of human language processing <ref> (Marcus, 1980) </ref>. A reasonable beginning, then, is to posit an underlying shift-reduce parsing mechanism capable of mapping input sentences onto appropriate representations and to learn control rules that guide this mapping. In this respect our approach is similar to others, notably (Berwick, 1985; Simmons and Yu, 1992).
Reference: <author> Michalski, R. S. </author> <year> (1983). </year> <title> A theory and methodology of inductive learning. </title> <editor> In Michalski, R. S., Carbonell, J. G., and Mitchell, T. M., editors, </editor> <booktitle> Machine Learning: An Artificial Intelligence Approach, </booktitle> <pages> pages 83-134. </pages> <publisher> Tioga. </publisher>
Reference: <author> Miikkulainen, R. and Dyer, M. G. </author> <year> (1991). </year> <title> Natural language processing with modular PDP networks and distributed lexicon. </title> <journal> Cognitive Science, </journal> <volume> 15 </volume> <pages> 343-399. </pages>
Reference-contexts: In the first experiment, Chill was tried on the baseline task reported in <ref> (Miikkulainen and Dyer, 1991) </ref> using the 1475 sentence/case-structure examples from the M & K corpus. The sample actually comprises 1390 unique sentences, some of which allow multiple analyses. <p> The system also exhibits the desirable property that it tends to produce very few inaccurate parses. The right half of Figure 13 shows the number of spurious parses per sentence as a function of training set size. This initial experiment, following the example in <ref> (Miikkulainen and Dyer, 1991) </ref>, did not use distinct tokens for different senses of ambiguous words. However, one of the original motivations for connectionist approaches was the ability to handle lexical ambiguity (McClelland and Kawamoto, 1986; St. John and McClelland, 1990). <p> The closest comparison on these experiments can be made with the results in <ref> (Miikkulainen and Dyer, 1991) </ref> where an accuracy of 95% was achieved at the "word level" training with 1439 of the 1475 pairs from the M & K corpus. Since the output contains five slots, assuming independence of errors gives an estimate of 0:95 5 or only 78% completely correct parses.
Reference: <author> Minton, S. </author> <year> (1988). </year> <title> Quantitative results concerning the utility of explanation-based learning. </title> <booktitle> In Proceedings of the Seventh National Conference on Artificial Intelligence, </booktitle> <pages> pages 564-569. </pages> <address> St. </address>
Reference-contexts: In applications where a strong domain theory is available to explain operator success, EBL techniques may be used to generalize the explanation and extract sufficient conditions for future applications of the operator. The learning of search-control knowledge has been previously investigated primarily in the context of STRIPS-like planners <ref> (Minton, 1988) </ref> and forward-chaining production systems (Langley, 1985; Laird et al., 1986). Recently, Cohen (1990) has argued some advantages of extending the search-control learning framework into the domain of logic programming. This proposal embraces such an approach and offers a general framework for learning search-control heuristics in logic programs. <p> Standard EBL methods can be applied to learn clause selection heuristics for Prolog programs; however, they tend to produce control rules that are accurate but highly complex. The complexity of the rules makes them costly to use and can often degrade overall performance rather than improving it <ref> (Minton, 1988) </ref>. Cohen (1990) handles this problem by combining EBL with induction to learn a small set of "approximate" control rules with reduced match cost. His method, AxA-EBL, was shown to out-perform standard EBL control rules across a number of problem solving domains.
Reference: <editor> Paul, </editor> <address> MN. </address>
Reference: <author> Mitchell, T., Utgoff, T., and Banerji, R. </author> <year> (1983). </year> <title> Learning problem solving heuristics by experimentation. </title> <editor> In Michalski, R., Mitchell, T., and Carbonell, J., editors, </editor> <booktitle> Machine Learning: An Artificial Intelligence Approach. </booktitle> <address> Palo Alto, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Research in concept learning has tended to focus on techniques of inductive, or similarity-based learning, (SBL) while speedup is often achieved through explanation-based learning (EBL). However, there is no clear separation between these goals and methods. Inductive learning can be used to improve efficiency <ref> (Mitchell et al., 1983) </ref> and analytical methods such as EBL can sometimes be used to learn new domain knowledge that improves accuracy (Flann and Dietterich, 1989). One framework that cleanly unifies these goals and techniques is that of learning search-control rules for a problem solver.
Reference: <author> Mitchell, T. M., Keller, R. M., and Kedar-Cabelli, S. T. </author> <year> (1986). </year> <title> Explanation-based generalization: A unifying view. </title> <journal> Machine Learning, </journal> <volume> 1(1) </volume> <pages> 47-80. </pages>
Reference: <author> Muggleton, S. and Buntine, W. </author> <year> (1988). </year> <title> Machine invention of first-order predicates by inverting resolution. </title> <booktitle> In Proceedings of the Fifth International Conference on Machine Learning, </booktitle> <pages> pages 339-352. </pages> <address> Ann Arbor, MI. </address>
Reference-contexts: The intuition is that we want to find a small (hence general) definition which discriminates between the positive and negative examples. We start with a most specific definition (the set of positive examples) and introduce generalizations which make the definition more compact (as measured by a Cigol-like size metric <ref> (Muggleton and Buntine, 1988) </ref>). The search for more general definitions is carried out in a hill-climbing fashion. At each step, a number of possible generalizations are considered; the one producing the greatest compaction of the theory is implemented, and the process repeats. The basic algorithm is outlined in Figure 11.
Reference: <author> Muggleton, S. and Feng, C. </author> <year> (1992). </year> <title> Efficient induction of logic programs. </title> <editor> In Muggleton, S., editor, </editor> <booktitle> Inductive Logic Programming, </booktitle> <pages> pages 281-297. </pages> <address> New York: </address> <publisher> Academic Press. </publisher>
Reference-contexts: stating: "If the stack contains two items, the second of which is a completed noun phrase whose head is animate, then attach this phrase as the agent of the top of stack." The Chill induction algorithm may be viewed as a composite of proven techniques from other systems, notably Golem <ref> (Muggleton and Feng, 1992) </ref>, Foil (Quinlan, 1990), and Champ (Kijsirikul et al., 1992). The intuition is that we want to find a small (hence general) definition which discriminates between the positive and negative examples.
Reference: <author> Muggleton, S. H., </author> <title> editor (1992). </title> <booktitle> Inductive Logic Programming. </booktitle> <address> New York, NY: </address> <publisher> Academic Press. </publisher>
Reference-contexts: stating: "If the stack contains two items, the second of which is a completed noun phrase whose head is animate, then attach this phrase as the agent of the top of stack." The Chill induction algorithm may be viewed as a composite of proven techniques from other systems, notably Golem <ref> (Muggleton and Feng, 1992) </ref>, Foil (Quinlan, 1990), and Champ (Kijsirikul et al., 1992). The intuition is that we want to find a small (hence general) definition which discriminates between the positive and negative examples.
Reference: <author> Ng, H. T. </author> <year> (1988). </year> <title> A computerized prototype natural language tour guide. </title> <type> Technical Report AI88-75, </type> <institution> Austin, TX: Artificial Intelligence Laboratory, University of Texas. </institution>
Reference-contexts: A fourth experiment was designed to test Chill on a more realistic task. A portion of a semantic grammar was "lifted" from an extant prototype natural language database designed to support queries concerning tourist information <ref> (Ng, 1988) </ref>. The portion of the grammar used recognized over 150,000 distinct sentences. A simple case grammar, which produced labellings deemed useful for the database query task, was devised to generate a sample of sentence/case-structure analyses.
Reference: <author> Nilsson, N. </author> <year> (1980). </year> <booktitle> Principles of Artificial Intelligence. </booktitle> <address> Palo Alto, CA: </address> <publisher> Tioga. </publisher>
Reference-contexts: RW and BW are planning domains utilizing means-ends analysis planners, which were automatically generated from operator definitions. RW is based on the STRIPS robot world of (Fikes et al., 1972). BW is generated from the blocks world operators of <ref> (Nilsson, 1980) </ref>. Control rules were learned for a single recursive predicate having seven arguments and averaging about 5 conjuncts per clause. This predicate consisted of 18 and 12 clauses in RW and BW respectively.
Reference: <author> Pereira, F. and Shieber, S. </author> <year> (1987). </year> <title> Prolog and Natural Language Analysis. Stanford, CA: Center for the Study of Language and Information. </title>
Reference: <author> Plotkin, G. D. </author> <year> (1970). </year> <title> A note on inductive generalization. </title> <editor> In Meltzer, B. and Michie, D., editors, </editor> <booktitle> Machine Intelligence (Vol. 5). </booktitle> <address> New York: </address> <publisher> Elsevier North-Holland. </publisher>
Reference-contexts: The Find Generalization algorithm is shown in Figure 12. The first step is to construct the least general generalization (LGG) <ref> (Plotkin, 1970) </ref> of the input clauses. If the LGG does not cover any negative examples, no further refinement is necessary. If the clause is too general, an attempt is made to refine it using a Foil-like mechanism which adds literals derivable either from background or previously-invented predicates.
Reference: <author> Prieditis, A. and Mostow, J. </author> <year> (1987). </year> <title> Prolearn: Towards a prolog interpreter that learns. </title> <booktitle> In Proceedings of the Sixth National Conference on Artificial Intelligence. </booktitle> <address> Seattle, WA. </address>
Reference: <author> Quinlan, J. </author> <year> (1990). </year> <title> Learning logical definitions from relations. </title> <journal> Machine Learning, </journal> <volume> 5(3) </volume> <pages> 239-266. </pages>
Reference-contexts: This proposal shows how speedup may be achieved by learning search-control rules that determine which clauses in a Prolog program to use to solve particular subgoals. A combination of explanation-based generalization (EBG) (Mitchell et al., 1986; DeJong and Mooney, 1986) and ILP (Foil <ref> (Quinlan, 1990) </ref>) techniques are used to learn these control rules. An initial version of this approach has already been implemented and tested and is capable of automatically converting an O (n!) generate-and-test sorting program into an O (n 2 ) insertion sort based on experience gained from a single example. <p> The Dolphin, (Dynamic Optimization of Logic Programs through Heuristics INduction) system can be viewed as an extension of the AxA-EBL approach. Dolphin improves on AxA-EBL in two significant ways. First, it employs a more powerful induction algorithm, namely Quinlan's Foil <ref> (Quinlan, 1990) </ref>. <p> Dolphin adopts a Foil-like <ref> (Quinlan, 1990) </ref> framework to perform this induction. positivesto-cover = -positive examples. While positives-to-cover is not empty Find a clause, C, that covers some examples in positives-to-cover but covers no negative examples. Add C to the developing definition. Remove examples covered by C from positives-to-cover. <p> two items, the second of which is a completed noun phrase whose head is animate, then attach this phrase as the agent of the top of stack." The Chill induction algorithm may be viewed as a composite of proven techniques from other systems, notably Golem (Muggleton and Feng, 1992), Foil <ref> (Quinlan, 1990) </ref>, and Champ (Kijsirikul et al., 1992). The intuition is that we want to find a small (hence general) definition which discriminates between the positive and negative examples.
Reference: <author> Samuelsson, C. and Rayner, M. </author> <year> (1991). </year> <title> Quantitative evaluation of explanation-based learning as an optimization tool for a large-scale natural language system. </title> <booktitle> In Proceedings of the Twelfth International Joint Conference on Artificial intelligence, </booktitle> <pages> pages 609-615. </pages> <address> Sydney, Australia. </address>
Reference-contexts: In the area of database queries, potential data sets include constructing a corpus from the tourist domain used in previous experiments or, more ambitiously, using an existing corpus such as the ATIS sample which was extracted using "Wizard of Oz" techniques <ref> (Samuelsson and Rayner, 1991) </ref>. An existing system has produced semantic analyses of subsets of this corpus and we could attempt to automatically build such a system from samples of its input and output.
Reference: <author> Shavlik, J. </author> <year> (1990). </year> <title> Generalizing number in explanation based learning. </title> <journal> Machine Learning, </journal> <volume> 5 </volume> <pages> 39-70. </pages> <note> 35 Shavlik, </note> <author> J. W. and Dietterich, T. G., </author> <title> editors (1990). </title> <booktitle> Readings in Machine Learning. </booktitle> <address> San Mateo,CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: 1 Introduction A general goal of machine learning is automating the construction of efficient knowledge-based systems. Research has typically concentrated on either acquiring basic domain knowledge (concept learning) or improving the efficiency of a problem solver (speedup learning) <ref> (Shavlik and Dietterich, 1990) </ref>. A second dichotomy has grown up around the techniques 1 generally considered appropriate to these tasks. Research in concept learning has tended to focus on techniques of inductive, or similarity-based learning, (SBL) while speedup is often achieved through explanation-based learning (EBL). <p> EBL-macro suffers from the recursive, unbounded nature of the problems. This strategy is forced to acquire generalized proofs of all permutations of various sized lists with little hope of achieving significant coverage on novel examples. Techniques such as "generalizing to N" <ref> (Shavlik, 1990) </ref> would not help significantly since there is no regular recursive structure in the correct explanations; each ordering of the input list produces a different sequence of applications of the first and second clauses of insert. AxA-EBL fails to learn any control rules for these programs at all.
Reference: <author> Simmons, R. F. and Yu, Y. </author> <year> (1992). </year> <title> The acquisition and use of context dependent grammars for english. </title> <journal> Computational Linguistics, </journal> <volume> 18(4) </volume> <pages> 391-418. </pages>
Reference-contexts: The mechanism for inducing control rules employs constructive induction to invent word and phrase categories which support the parsing process. The next two subsections detail these two tasks. 4.2.1 Creating an Overly General Parser Our system adopts a simple shift-reduce framework for case-role mapping <ref> (Simmons and Yu, 1992) </ref>. The process is best illustrated by way of example. Consider the sentence: "The man ate the pasta." Parsing begins with an empty stack and an input buffer containing the entire sentence. <p> Chill also differs in that it produces semantic analyses and can uniformly learn syntactic and semantic constraints for sentence interpretation. More recently, an exemplar-based acquisition system for the style of case grammar used 27 in Chill is described in <ref> (Simmons and Yu, 1992) </ref>. Their system depends on an analyst to provide word classifications appropriate to the parsing task and requires detailed interaction to guide the parsing of training examples. <p> A few corpora of parsed sentences (tree banks) have been assembled to support this work (Simmons and Yu, 1992; Black et al., 1992). Some of the data from <ref> (Simmons and Yu, 1992) </ref> will be used to evaluate Chill on a larger corpus. It is anticipated that this will present difficulties as the text is drawn from a sample of newswire stories and may not contain sufficient regularity upon which Chill can construct a parser.
Reference: <author> St. John, M. F. and McClelland, J. L. </author> <year> (1990). </year> <title> Learning and applying contextual constraints in sentence comprehension. </title> <journal> Artificial Intelligence, </journal> <volume> 46 </volume> <pages> 217-257. </pages>
Reference-contexts: This compares favorably with backpropagation training times usually measured in hours or days. As a further comparison to connectionist approaches, Chill was used to duplicate a generalization experiment reported in <ref> (St. John and McClelland, 1990) </ref>. This experiment used an artificial corpus created from ten reversible actions and ten people, yielding sentences such as: "John saw Mary." These sentences could appear in either active or passive voice resulting in a total of 2000 such sentences.
Reference: <author> Tadepalli, P. </author> <year> (1989). </year> <title> Lazy explanation-based learning: A solution to the intractable theory problem. </title> <booktitle> In Proceedings of the Eleventh International Joint conference on Artificial intelligence. </booktitle> <address> Detroit, MI. </address>
Reference: <author> Tadepalli, P., </author> <title> editor (1992). </title> <booktitle> Proceedings of the ML92 Workshop on Knowledge Compilation and Speedup Learning. </booktitle> <address> Aberdeen, Scotland. </address>
Reference-contexts: Unfortunately, in existing languages such as Prolog, abstract, declarative program specifications often execute too slowly to be generally useful. One possible remedy is to employ speedup learning, which involves dynamically improving the efficiency of a system as it gains experience solving problems in a domain <ref> (Tadepalli, 1992) </ref>, to enhance the efficiency of programs based on experience gained from solving sample problems. In this way, declarative specifications might be dynamically transformed into efficient procedural programs.
Reference: <author> Tomita, M. </author> <year> (1986). </year> <title> Efficient Parsing for Natural Language. </title> <address> Boston: </address> <publisher> Kluwer Academic Publishers. </publisher>
Reference-contexts: Rather than focusing on the structure of language per se, the alternative approach taken here is to consider the implications of acquiring of an efficient, deterministic parser. Shift-reduce parsing has proven to be an efficient mechanism for natural language analysis <ref> (Tomita, 1986) </ref>, and deterministic variants have been argued to accurately model certain aspects of human language processing (Marcus, 1980). A reasonable beginning, then, is to posit an underlying shift-reduce parsing mechanism capable of mapping input sentences onto appropriate representations and to learn control rules that guide this mapping.
Reference: <author> VanLehn, K. and Ball, W. </author> <year> (1987). </year> <title> A version space approach to learning context-free grammars. </title> <journal> Machine Learning, </journal> <volume> 2(1) </volume> <pages> 39-74. </pages>
Reference: <author> Veloso, M. </author> <year> (1992). </year> <title> Learning by analogical reasoning in general problem solving. </title> <type> Technical Report CMU-CS-92-174, </type> <institution> Pittsburgh, PA: School of Computer Science, Carnegie Mellon University. </institution>
Reference-contexts: Most of the existing benchmarks in speedup learning are "toy problems," such as blocks-world planning. However, realistic problem domains are beginning to be explored. The Prodigy group at Carnegie Mellon University has recently assembled domain theories and problem sets for process planning (Gil, 1991) and logistics transportation <ref> (Veloso, 1992) </ref>.
Reference: <author> Wirth, R. </author> <year> (1989). </year> <title> Completing logic programs by inverse resolution. </title> <booktitle> In Proceedings of the European Working Session on Learning, </booktitle> <pages> pages 239-250. </pages> <address> Montpelier, France: </address> <publisher> Pitman. </publisher>
Reference: <author> Wolff, J. G. </author> <year> (1982). </year> <title> Language acquisition, data compression, and generalization. </title> <journal> Language and Communication, </journal> <volume> 2 </volume> <pages> 57-89. </pages>
Reference: <author> Yoo, J. and Fisher, D. </author> <year> (1991). </year> <title> Concept formation over problem-solving experience. </title> <editor> In Fisher, D., Pazzani, M., and Langley, P., editors, </editor> <title> Concept Formation: Knowledge and Experience in Unspervised Learning. </title> <address> Palo Alto, CA: </address> <publisher> Morgan Kaufmann. </publisher> <pages> 36 </pages>
Reference-contexts: The first use of approximations in learning control-rules was probably MetaLEX (Keller, 1987) which used a simple technique for removing conditions. Most other recent investigations have not focussed on learning control-rules (Ellman, 1988; Tadepalli, 1989; Chien, 1989) or have not employed induction (Chase et al., 1989). Yoo and Fisher <ref> (Yoo and Fisher, 1991) </ref> combine induction and explanation to improve performance in a problem-solving framework. They enhance the utility of EBL-macros by clustering them in a Cobweb-style classification tree maintaining explanations at various levels of detail. By contrast, Dolphin uses supervised learning methods to acquire explicit search-control rules.
References-found: 55

