URL: ftp://ftp.ai.mit.edu/pub/users/oded/papers/NIPS97.ps.Z
Refering-URL: http://www.ai.mit.edu/people/oded/multiple-instance.html
Root-URL: 
Email: oded@ai.mit.edu  tlp@ai.mit.edu  
Title: A Framework for Multiple-Instance Learning  
Author: Oded Maron Tom as Lozano-P erez 
Address: Cambridge, MA 02139  Cambridge, MA 02139  
Affiliation: NE43-755  NE43-836a  
Abstract: Multiple-instance learning is a variation on supervised learning, where the task is to learn a concept given positive and negative bags of instances. Each bag may contain many instances, but a bag is labeled positive even if only one of the instances in it falls within the concept. A bag is labeled negative only if all the instances in it are negative. We describe a new general framework, called Diverse Density, for solving multiple-instance learning problems. We apply this framework to learn a simple description of a person from a series of images (bags) containing that person, to a stock selection problem, and to the drug activity prediction problem.
Abstract-found: 1
Intro-found: 1
Reference: [ Auer, 1997 ] <author> P. Auer. </author> <title> On Learning from Multi-Instance Examples: Empirical Evaluation of a theoretical Approach. </title> <type> NeuroCOLT Technical Report Series, </type> <institution> NC-TR-97-025, </institution> <month> March </month> <year> 1997. </year>
Reference-contexts: This work was followed by [ Long and Tan, 1996 ] , where a high-degree polynomial PAC bound was given for the number of examples needed to learn in the multiple-instance learning model. <ref> [ Auer, 1997 ] </ref> gives a more efficient algorithm, and [ Blum and Kalai, 1998 ] shows that learning from multiple-instance examples is reducible to PAC-learning with two sided noise and to the Statistical Query model. <p> respect to location, maximizing with respect to scaling of feature weights may still lead to local maxima. 3 Applications of Diverse Density By way of benchmarking, we tested the Diverse Density approach on the musk data sets from [ Dietterich et al., 1997 ] , which were also used in <ref> [ Auer, 1997 ] </ref> . We also have begun investigating two new applications of multiple-instance learning. We describe preliminary results on all of these below. The musk data sets contain feature vectors describing the surfaces of a variety of low-energy shapes from approximately 100 molecules. <p> The table below lists the average accuracy of twenty runs, compared with the performance of the two principal algorithms reported in [ Dietterich et al., 1997 ] (iterated-discrim APR and GFS elim-kde APR), as well as the MULTINST algorithm from <ref> [ Auer, 1997 ] </ref> . We note that the performances reported for iterated-discrim APR involves choosing parameters to maximize test set performance and so probably represents an upper bound for accuracy on this data set. The MULTINST algorithm assumes that all instances from all bags are generated independently.
Reference: [ Blum and Kalai, 1998 ] <author> A. Blum and A. Kalai. </author> <title> A Note on Learning from Multiple-Instance Examples. </title> <note> To appear in Machine Learning, </note> <year> 1998. </year>
Reference-contexts: This work was followed by [ Long and Tan, 1996 ] , where a high-degree polynomial PAC bound was given for the number of examples needed to learn in the multiple-instance learning model. [ Auer, 1997 ] gives a more efficient algorithm, and <ref> [ Blum and Kalai, 1998 ] </ref> shows that learning from multiple-instance examples is reducible to PAC-learning with two sided noise and to the Statistical Query model. Unfortunately, the last three papers make the restrictive assumption that all instances from all bags are generated independently.
Reference: [ Dietterich et al., 1997 ] <author> T. G. Dietterich, R. H. Lathrop, and T. Lozano-P erez. </author> <title> Solving the Multiple-Instance Problem with Axis-Parallel Rectangles. </title> <journal> Artificial Intelligence Journal, </journal> <volume> 89, </volume> <year> 1997. </year>
Reference-contexts: However, a negative example means that none of the shapes that the molecule can achieve was the right key. The multiple-instance learning model was only recently formalized by <ref> [ Dietterich et al., 1997 ] </ref> . They assume a hypothesis class of axis-parallel rectangles, and develop algorithms for dealing with the drug activity prediction problem described above. <p> By maximizing Diverse Density we can find the point of intersection (the desired concept), and also the set of feature weights that lead to the best intersection. We show results of applying this algorithm to a difficult synthetic training set as well as the musk data set from <ref> [ Dietterich et al., 1997 ] </ref> . We then use Diverse Density in two novel applications: one is to learn a simple description of a person from a series of images that are labeled positive if the person is somewhere in the image and negative otherwise. <p> While this heuristic is sensible for maximizing with respect to location, maximizing with respect to scaling of feature weights may still lead to local maxima. 3 Applications of Diverse Density By way of benchmarking, we tested the Diverse Density approach on the musk data sets from <ref> [ Dietterich et al., 1997 ] </ref> , which were also used in [ Auer, 1997 ] . We also have begun investigating two new applications of multiple-instance learning. We describe preliminary results on all of these below. <p> The table below lists the average accuracy of twenty runs, compared with the performance of the two principal algorithms reported in <ref> [ Dietterich et al., 1997 ] </ref> (iterated-discrim APR and GFS elim-kde APR), as well as the MULTINST algorithm from [ Auer, 1997 ] .
Reference: [ Long and Tan, 1996 ] <author> P. M. Long and L. Tan. </author> <title> PAC-learning axis alligned rectangles with respect to product distributions from multiple-instance examples. </title> <booktitle> In Proceedings of the 1996 Conference on Computational Learning Theory, </booktitle> <year> 1996. </year>
Reference-contexts: The multiple-instance learning model was only recently formalized by [ Dietterich et al., 1997 ] . They assume a hypothesis class of axis-parallel rectangles, and develop algorithms for dealing with the drug activity prediction problem described above. This work was followed by <ref> [ Long and Tan, 1996 ] </ref> , where a high-degree polynomial PAC bound was given for the number of examples needed to learn in the multiple-instance learning model. [ Auer, 1997 ] gives a more efficient algorithm, and [ Blum and Kalai, 1998 ] shows that learning from multiple-instance examples is
Reference: [ Maron and LakshmiRatan, 1998 ] <author> O. Maron and A. LakshmiRatan. </author> <title> Multiple-Instance Learning for Natural Scene Classification. </title> <note> In Submitted to CVPR-98, </note> <year> 1998. </year>
Reference-contexts: The three dominant colors (one for each subsection) are used to represent the image. Figure 4 shows a training set where every bag included two people, yet the algorithm learned a description of the person who appears in all the images. This technique is expanded in <ref> [ Maron and LakshmiRatan, 1998 ] </ref> to learn descriptions of natural images and use the learned concept to retrieve similar images from a large image database. Another new application uses Diverse Density in the stock selection problem.
References-found: 5

