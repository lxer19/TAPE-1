URL: http://www.cse.ogi.edu/~mak/PS/rp.ps
Refering-URL: http://www.cse.ogi.edu/CSLU/personnel/bios/mak.html
Root-URL: http://www.cse.ogi.edu
Email: mak@cse.ogi.edu  
Title: A Distance Measure of Speech Phones and Its Application to Phonetic Context Clustering for Spoken
Author: Brian Mak 
Date: April 23, 1996  
Address: 20000 N.W.Walker Road, Portland, OR 97291-1000  
Affiliation: Center  Oregon Graduate Institute of Science and Technology  
Abstract: It is generally acknowledged that the biggest obstacle to improving speech recognition is our incomplete quantitative understanding of the sources of variability in speech. One way of quantifying the variability is by measuring the distance among acoustic manifestations of the variants. In this paper, we study the feasibility of using the Bhattacharyya distance as a distance measure of speech. Its original formulation is generalized to measure the distance between two Gaussian mixtures. The distance measure is then used to compute the distances among the 17 most common (context-independent) phones 1 in the OGI TS story database. Only 1-mixture Gaussian models are currently investigated. To assess fitness of the measure, we compare the separability result with (1) our acoustic and linguistic knowledge; (2) the recognition result from a simple Bayesian classifier (based on the same theory from which the Bhattacharyya distance is derived); and, (3) the confusion matrix from a context-independent phone recognizer. Very promising results are obtained. The Bhattacharyya distance is then used as the distance measure for phonetic context clustering. Two clustering algorithms are tried, namely, the modified k-means algorithm, and the standard agglomerative algorithm. The agglomerative clustering algorithm, which exploits only the similarity between phones, generates better results. A context-dependent phone recognizer is being built to evaluate the effectiveness of the generalized triphones obtained. 1 They should more appropriately be called phone-like units, but are called phones in this paper for brevity. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> S. F. Boll. </author> <title> "Suppression of Acoustic Noise in Speech Using Spectral Subtraction". </title> <journal> IEEE Transactions on Acoustics, Speech and Signal Processing, </journal> <volume> 27(2) </volume> <pages> 113-120, </pages> <month> April </month> <year> 1979. </year>
Reference-contexts: Lombard effect VQ [17], DTW [17], multiple templates, HMM [18] gender gender specific recognizer cross-speaker dialect, articulatory habit multiple pronounciations age, vocal tract length, voice quality (breathy or creaky) VQ, DTW, HMM, more training data from a diverse population acoustic environment recording channel characteristics, background noise, room reverberation spectral normalization <ref> [1, 14] </ref>, RASTA [6], stochastic channel matching [22] word environment context, coarticulation, prosody, phonological and phonetic recoding context-dependent modelling [10] Once the sources of variability are identified, we would like to represent the variability quantitatively.
Reference: [2] <author> L. Deng, V. Gupta, M. Lennig, P. Kenny, and P. Mermelstein. </author> <title> "Acoustic Recognition component of an 86,000-word Speech Recognizer". </title> <booktitle> Proceeding IEEE ICASSP, </booktitle> <pages> pages 741-744, </pages> <year> 1990. </year>
Reference-contexts: For example, L. Deng et al. <ref> [2] </ref> and L. Jiang [7] both defined contexts based on broad phonetic categories. Deng classified the articulatory effects on vowels and consonants each to 5 types and derived 25 generalized contexts for each phone.
Reference: [3] <author> W. R. Dillon and M. Goldstein. </author> <title> Multivariate Analysis, Methods and Applications. </title> <publisher> John Wiley & Sons, </publisher> <year> 1984. </year>
Reference-contexts: As both are standard clustering algorithms, they are only sketched in Appendix C and Appendix D for quick reference and for showing the particular options in the algorithms we choose for this application. Refer to <ref> [3, 23] </ref> for details. For the agglomerative hierarchical clustering algorithm, we choose the complete linkage method to compute inter-cluster distance. The method guarantees that all context-dependent phones in a cluster must be similar to each other, and tends to produce more spherical clusters.
Reference: [4] <author> P. D'Orta, M. Ferretti, and S. Scarci. </author> <title> "Phoneme Classification for Real Time Speech Recognition of Italian". </title> <booktitle> Proceeding IEEE ICASSP, </booktitle> <pages> pages 81-84, </pages> <year> 1987. </year>
Reference-contexts: The distance measure may be applied to both discrete and continuous HMMs and has been proven reliable by Juang and Rabiner. Thereafter several other similar distance measures have been derived. D'Orta et al. <ref> [4] </ref> defined a discriminability measure among a set of HMMs from the concept of mutual information, such that the measure increases with the mutual information. Lee [10] measured the distance between two HMMs as the amount of information (entropy) lost if the two models are merged. <p> D'Orta et al. <ref> [4] </ref> tried two methods to merge HMMs: (1) a standard agglomerative hierarchical clustering algorithm with a divergence-based HMM distance measure; and (2) an ordered search through a tree of possible merges with a distance defined on mutual information.
Reference: [5] <author> K. Fukunaga. </author> <title> Introduction to Statistical Pattern Recognition. </title> <publisher> Academic Press, Inc., </publisher> <address> 2nd edition, </address> <year> 1990. </year>
Reference-contexts: In this paper, we study the feasibility of using the Bhattacharyya distance as a distance measure of speech. The Bhattacharyya distance is a well-known distance measure between two Gaussian distributions <ref> [5] </ref>. We further generalize its definition to measure the distance between two distributions which are Gaussian mixtures. <p> to be basically equivalent to the information-theoretic measures mentioned above, it has the advantage that it is not limited to HMMs. 4 2.2 Bhattacharyya Distance as A Separability Measure Bhattacharyya distance is a separability measure between two distributions and is covered in many texts on statistical pattern recognition (for example, <ref> [5] </ref>). We will first briefly present its formulation so as to assist the reader in understanding our derivation of its extension to N -mixture Gaussians. Let us begin by considering a two-class classification problem.
Reference: [6] <author> H. Hermansky, N. Morgan, A. Bayya, and P. Kohn. </author> <title> "RASTA-PLP Speech Analysis Technique". </title> <booktitle> Proceeding IEEE ICASSP, </booktitle> <year> 1992. </year>
Reference-contexts: [17], DTW [17], multiple templates, HMM [18] gender gender specific recognizer cross-speaker dialect, articulatory habit multiple pronounciations age, vocal tract length, voice quality (breathy or creaky) VQ, DTW, HMM, more training data from a diverse population acoustic environment recording channel characteristics, background noise, room reverberation spectral normalization [1, 14], RASTA <ref> [6] </ref>, stochastic channel matching [22] word environment context, coarticulation, prosody, phonological and phonetic recoding context-dependent modelling [10] Once the sources of variability are identified, we would like to represent the variability quantitatively. One way to do this is through measuring the separability among acoustic manifestations of the variants.
Reference: [7] <author> L. Jiang. </author> <title> "Neural Network Based Context-Dependent Modeling for Speech Recognition". </title> <note> Research Proficiency Examination paper, </note> <institution> Oregon Graduate Institute of Science and Technology, </institution> <year> 1994. </year>
Reference-contexts: For example, L. Deng et al. [2] and L. Jiang <ref> [7] </ref> both defined contexts based on broad phonetic categories. Deng classified the articulatory effects on vowels and consonants each to 5 types and derived 25 generalized contexts for each phone.
Reference: [8] <author> B. H. Juang and L. R. Rabiner. </author> <title> "A Probabilistic Distance Measure for Hidden Markov Models". </title> <journal> AT&T Technical Journal, </journal> <volume> 64(2) </volume> <pages> 391-408, </pages> <month> Feb </month> <year> 1985. </year>
Reference-contexts: The measure requires a great deal of computation and its application to continuous HMMs is probably intractable <ref> [8] </ref>. Juang and Rabiner [8] first proposed an asymmetric distance measure between two HMMs based on information theory. <p> The measure requires a great deal of computation and its application to continuous HMMs is probably intractable <ref> [8] </ref>. Juang and Rabiner [8] first proposed an asymmetric distance measure between two HMMs based on information theory.
Reference: [9] <author> B. H. Juang, L. R. Rabiner, and J. G. Wilpon. </author> <title> "On the Use of Bandpass Liftering in Speech Recognition". </title> <journal> IEEE ASSP Magazine, </journal> <volume> 35(7) </volume> <pages> 947-954, </pages> <month> July </month> <year> 1987. </year>
Reference-contexts: Thus MFCC seems to be better to tell the speaker differences. One possible reason is 14 that the MFCC, being computed using the filter-bank approach which is known to be sensitive to the speaker's voice excitation <ref> [9] </ref>, retains some speaker information and can better tell the differences between inter-speaker phones. 4 An Application: Phonetic Context Clustering As speech recognition aims at larger and larger vocabularies, it is clear that we cannot create an acoustic model for each word separately.
Reference: [10] <author> K. F. Lee. </author> <title> "Context-Dependent Phonetic Hidden Markov Models for Speaker-Independent Continuous Speech Recognition". </title> <journal> IEEE Transactions on Acoustics, Speech and Signal Processing, </journal> <volume> 38(4) </volume> <pages> 599-609, </pages> <month> April </month> <year> 1990. </year>
Reference-contexts: pronounciations age, vocal tract length, voice quality (breathy or creaky) VQ, DTW, HMM, more training data from a diverse population acoustic environment recording channel characteristics, background noise, room reverberation spectral normalization [1, 14], RASTA [6], stochastic channel matching [22] word environment context, coarticulation, prosody, phonological and phonetic recoding context-dependent modelling <ref> [10] </ref> Once the sources of variability are identified, we would like to represent the variability quantitatively. One way to do this is through measuring the separability among acoustic manifestations of the variants. Studying variability of speech through quantitative measurement of the separability of the variants has other advantages. <p> Thereafter several other similar distance measures have been derived. D'Orta et al. [4] defined a discriminability measure among a set of HMMs from the concept of mutual information, such that the measure increases with the mutual information. Lee <ref> [10] </ref> measured the distance between two HMMs as the amount of information (entropy) lost if the two models are merged. <p> The second method was HMM threshold clustering | for each HMM model, split off those training data whose likelihood scores fall below a threshold and create a new model for them. The threshold is generally a matter of guesswork. Lee <ref> [10] </ref> proposed to merge two HMMs that result in minimal information (entropy) loss using a modified agglomerative clustering algorithm.
Reference: [11] <author> K. F. Lee, S. Hayamizu, H. W. Hon, C. Huang, J. Swartz, and R. Weide. </author> <title> "Allophone Clustering For Continuous Speech Recognition". </title> <booktitle> Proceeding IEEE ICASSP, </booktitle> <pages> pages 749-752, </pages> <year> 1990. </year> <month> 21 </month>
Reference-contexts: Lee [10] proposed to merge two HMMs that result in minimal information (entropy) loss using a modified agglomerative clustering algorithm. Noticing that the method may not cover all possible triphones if some of them are absent from the training data, Lee et al. <ref> [11] </ref> later suggested another tree-based allophone 5 clustering method. All allophones are placed in the root of the decision tree. Each node of the tree is associated with a binary question, which is selected from a set derived by an expert linguist.
Reference: [12] <author> S. E. Levinson, L. R. Rabiner, and M. M. Sondhi. </author> <title> "An Introduction to The Application of The Theory of Probabilistic Functions of A Markov Process to Automatic Speech Recognition". </title> <journal> The Bell System Technical Journal, </journal> <volume> 62(4) </volume> <pages> 1035-74, </pages> <month> April </month> <year> 1983. </year>
Reference-contexts: This was done in [20] to derive word equivalence classes. The measure is limited to classes with small numbers of samples because DTW is computationally very expensive. 2.1.2 Probabilistic Acoustic Models Levinson et al. <ref> [12] </ref> derived a distance measure on discrete HMMs as the Euclidean distance on the state observation probability matrices. The measure requires a great deal of computation and its application to continuous HMMs is probably intractable [8].
Reference: [13] <author> A. Ljolje. </author> <title> "High Accuracy Phone Recognition Using Context Clustering and Quasi-Triphonic Models". </title> <booktitle> Computer Speech and Language, </booktitle> <volume> 8 </volume> <pages> 129-151, </pages> <year> 1994. </year>
Reference-contexts: Deng classified the articulatory effects on vowels and consonants each to 5 types and derived 25 generalized contexts for each phone. Jiang derived 6 categories based on the manner of articulation: vowels, stops, liquids and glides, fricatives and affricatives, nasals, and silence. 15 A. Ljolje <ref> [13] </ref> used more detailed contextual effects to derive a set of 19 left context classes and 18 right context classes. The advantage of this approach is that it is simple, and the derived clusters are independent of the training data and the particular spectral analysis.
Reference: [14] <author> P. J. Moreno and R. M. Stern. </author> <title> "Sources of Degradation of Speech Recognition in the Telephone Network". </title> <booktitle> Proceeding IEEE ICASSP, </booktitle> <pages> pages 109-113, </pages> <year> 1994. </year>
Reference-contexts: Lombard effect VQ [17], DTW [17], multiple templates, HMM [18] gender gender specific recognizer cross-speaker dialect, articulatory habit multiple pronounciations age, vocal tract length, voice quality (breathy or creaky) VQ, DTW, HMM, more training data from a diverse population acoustic environment recording channel characteristics, background noise, room reverberation spectral normalization <ref> [1, 14] </ref>, RASTA [6], stochastic channel matching [22] word environment context, coarticulation, prosody, phonological and phonetic recoding context-dependent modelling [10] Once the sources of variability are identified, we would like to represent the variability quantitatively.
Reference: [15] <author> Y.K. Muthusamy, R.A. Cole, </author> <title> and B.T. Oshika. "he OGI Multi-Language Telephone Speech Corpus". </title> <booktitle> Proceeding The International Conference on Spoken Language, </booktitle> <pages> pages 895-898, </pages> <year> 1992. </year>
Reference-contexts: In Section 3, some simple experiments are done to assess its fitness. 7 3 Evaluation of the Bhattacharyya Distance Measure To evaluate the Bhattacharyya distance as a distance measure of speech, the speaker-independent Bhattacharyya distance matrix among the 17 most common context-independent (CI) phones in the OGI TS <ref> [15] </ref> story database are computed over various groups of speakers.
Reference: [16] <editor> J. S. Perkell and D. H. Klatt, editors. </editor> <booktitle> Invariance and Variability in Speech Processes. </booktitle> <publisher> Lawrence Erlbaum Associates, Inc., </publisher> <year> 1986. </year>
Reference-contexts: Despite all these successes, it is generally agreed that a poor understanding of speech variability is one of the major hindrances to further progress <ref> [16] </ref>. Table 1 summarizes some sources of speech variability [16] and techniques developed to cope with them. <p> Despite all these successes, it is generally agreed that a poor understanding of speech variability is one of the major hindrances to further progress <ref> [16] </ref>. Table 1 summarizes some sources of speech variability [16] and techniques developed to cope with them.
Reference: [17] <author> L. Rabiner and B. H. Juang. </author> <title> Fundamentals of Speech Recognition. </title> <publisher> Prentice Hall, </publisher> <year> 1993. </year>
Reference-contexts: Table 1 summarizes some sources of speech variability [16] and techniques developed to cope with them. Table 1: Sources of speech variability and some techniques to cope with them category sources of varibility proposed solutions within-speaker statistical variations in articulation, speaking rate, Lombard effect VQ <ref> [17] </ref>, DTW [17], multiple templates, HMM [18] gender gender specific recognizer cross-speaker dialect, articulatory habit multiple pronounciations age, vocal tract length, voice quality (breathy or creaky) VQ, DTW, HMM, more training data from a diverse population acoustic environment recording channel characteristics, background noise, room reverberation spectral normalization [1, 14], RASTA [6], <p> Table 1 summarizes some sources of speech variability [16] and techniques developed to cope with them. Table 1: Sources of speech variability and some techniques to cope with them category sources of varibility proposed solutions within-speaker statistical variations in articulation, speaking rate, Lombard effect VQ <ref> [17] </ref>, DTW [17], multiple templates, HMM [18] gender gender specific recognizer cross-speaker dialect, articulatory habit multiple pronounciations age, vocal tract length, voice quality (breathy or creaky) VQ, DTW, HMM, more training data from a diverse population acoustic environment recording channel characteristics, background noise, room reverberation spectral normalization [1, 14], RASTA [6], stochastic channel <p> models before presenting our proposed Bhattacharyya distance measure. 3 2.1 Reviews of Some Distance Measures 2.1.1 Non-probabilistic Acoustic Models The most commonly used distance measure between two utterances in speech recognition is the sum of the spectral distances between the two after they are time-aligned by dynamic time warping (DTW) <ref> [17] </ref>. Thus the distance between two utterance classes may be computed by averaging distances between any pair of utterances, one from each class. This was done in [20] to derive word equivalence classes.
Reference: [18] <author> L. R. Rabiner and B. H. Juang. </author> <title> "An Introduction to Hidden Markov Models". </title> <journal> IEEE ASSP Magazine, </journal> <pages> pages 4-16, </pages> <month> Jan </month> <year> 1986. </year>
Reference-contexts: Table 1: Sources of speech variability and some techniques to cope with them category sources of varibility proposed solutions within-speaker statistical variations in articulation, speaking rate, Lombard effect VQ [17], DTW [17], multiple templates, HMM <ref> [18] </ref> gender gender specific recognizer cross-speaker dialect, articulatory habit multiple pronounciations age, vocal tract length, voice quality (breathy or creaky) VQ, DTW, HMM, more training data from a diverse population acoustic environment recording channel characteristics, background noise, room reverberation spectral normalization [1, 14], RASTA [6], stochastic channel matching [22] word environment <p> As most successful ASR systems to date use Hidden Markov modelling technology <ref> [18] </ref>, it is no surprise that most distance measures developed so far are limited to measuring the distance between two Hidden Markov models (HMM). In this paper, we study the feasibility of using the Bhattacharyya distance as a distance measure of speech.
Reference: [19] <author> L. R. Rabiner, C. H. Lee, B. H. Juang, and J. G. Wilpon. </author> <title> "HMM Clustering for Connected Word Recognition". </title> <booktitle> Proceeding IEEE ICASSP, </booktitle> <pages> pages 405-408, </pages> <year> 1989. </year>
Reference-contexts: D'Orta et al. [4] tried two methods to merge HMMs: (1) a standard agglomerative hierarchical clustering algorithm with a divergence-based HMM distance measure; and (2) an ordered search through a tree of possible merges with a distance defined on mutual information. Juang and Rabiner <ref> [19] </ref> also tried two methods based on continuous binary splitting. The first was HMM likelihood clustering | a model splits into 2 child models which have the same parameters as the parent model except for the spectral means of each mixture.
Reference: [20] <author> L. R. Rabiner and J. G. Wilpon. </author> <title> "A Two-Pass Pattern-Recognition Approach to Isolated Word Recognition". </title> <journal> The Bell System Technical Journal, </journal> <volume> 60(5) </volume> <pages> 739-766, </pages> <month> May-June </month> <year> 1981. </year>
Reference-contexts: Thus the distance between two utterance classes may be computed by averaging distances between any pair of utterances, one from each class. This was done in <ref> [20] </ref> to derive word equivalence classes. The measure is limited to classes with small numbers of samples because DTW is computationally very expensive. 2.1.2 Probabilistic Acoustic Models Levinson et al. [12] derived a distance measure on discrete HMMs as the Euclidean distance on the state observation probability matrices.
Reference: [21] <author> R. Rosenfeld, X. Huang, and M. Furst. </author> <title> "Exploiting Correlations Among Competing Models With Application to Large Vocabulary Speech Recognition". </title> <booktitle> Proceeding IEEE ICASSP, </booktitle> <pages> pages 5-8, </pages> <year> 1992. </year>
Reference-contexts: Lee [10] measured the distance between two HMMs as the amount of information (entropy) lost if the two models are merged. Recently Rosenfeld et al. <ref> [21] </ref> treated the match score between an acoustic signal and an acoustic model as a random variable and derived a distance measure based on the means of its conditional probability distributions on different models.
Reference: [22] <author> A. Sankar and C. H. Lee. </author> <title> "A Maximum-Likelihood Approach to Stochastic Matching for Robust Speech Recognition". </title> <note> To be published. </note>
Reference-contexts: templates, HMM [18] gender gender specific recognizer cross-speaker dialect, articulatory habit multiple pronounciations age, vocal tract length, voice quality (breathy or creaky) VQ, DTW, HMM, more training data from a diverse population acoustic environment recording channel characteristics, background noise, room reverberation spectral normalization [1, 14], RASTA [6], stochastic channel matching <ref> [22] </ref> word environment context, coarticulation, prosody, phonological and phonetic recoding context-dependent modelling [10] Once the sources of variability are identified, we would like to represent the variability quantitatively. One way to do this is through measuring the separability among acoustic manifestations of the variants.
Reference: [23] <author> J. G. Wilpon and L. R. Rabiner. </author> <title> "A Modified K-Means Clustering Algorithm for Use in Isolated Word Recognition". </title> <journal> IEEE Transactions on Acoustics, Speech and Signal Processing, </journal> <volume> 55 </volume> <pages> 587-594, </pages> <month> Jan </month> <year> 1985. </year>
Reference-contexts: As both are standard clustering algorithms, they are only sketched in Appendix C and Appendix D for quick reference and for showing the particular options in the algorithms we choose for this application. Refer to <ref> [3, 23] </ref> for details. For the agglomerative hierarchical clustering algorithm, we choose the complete linkage method to compute inter-cluster distance. The method guarantees that all context-dependent phones in a cluster must be similar to each other, and tends to produce more spherical clusters.
Reference: [24] <author> P. C. Woodland, C. J. Leggetter, J. J. Odell, V. Valtchev, and S. J. Young. </author> <title> "The 1994 HTK Large Vocabulary Speech Recognition System". </title> <booktitle> Proceeding IEEE ICASSP, </booktitle> <pages> pages 73-76, </pages> <year> 1995. </year> <month> 22 </month>
Reference-contexts: Powerful statistical modelling techniques together with better peripheral auditory models and, of course, good engineering work have made it possible to recognize read speech from a vocabulary as large as 60,000 words independent of the speaker with an error rate of less than 10% <ref> [24] </ref>. Despite all these successes, it is generally agreed that a poor understanding of speech variability is one of the major hindrances to further progress [16]. Table 1 summarizes some sources of speech variability [16] and techniques developed to cope with them.
References-found: 24

