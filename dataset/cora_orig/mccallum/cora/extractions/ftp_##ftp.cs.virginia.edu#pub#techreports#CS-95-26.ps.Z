URL: ftp://ftp.cs.virginia.edu/pub/techreports/CS-95-26.ps.Z
Refering-URL: ftp://ftp.cs.virginia.edu/pub/techreports/README.html
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Email: -jwd,sj3e-@virginia.edu  
Title: An Aggressive Approach to Loop Unrolling  
Author: JACK W. DAVIDSON and SANJAY JINTURKAR S. A. 
Note: U.  
Address: Thornton Hall  Charlottesville, VA 22903  
Affiliation: Department of Computer Science,  University of Virginia,  
Abstract: A well-known code transformation for improving the execution performance of a program is loop unrolling. The most obvious benefit of unrolling a loop is that the transformed loop usually, but not always, requires fewer instruction executions than the original loop. The reduction in instruction executions comes from two sources: the number of branch instructions executed is reduced, and the index variable is modified fewer times. In addition, for architectures with features designed to exploit instruction-level parallelism, loop unrolling can expose greater levels of instruction-level parallelism. Loop unrolling is an effective code transformation often improving the execution performance of programs that spend much of their execution time in loops by ten to thirty percent. Possibly because of the effectiveness of a simple application of loop unrolling, it has not been studied as extensively as other code improvements such as register allocation or common subexpression elimination. The result is that many compilers employ simplistic loop unrolling algorithms that miss many opportunities for improving the performance. This paper describes how aggressive loop unrolling is done in a retargetable optimizing compiler. Using a set of 32 benchmark programs, the effectiveness of this more aggressive approach to loop unrolling is evaluated. The results show that aggressive loop unrolling can yield performance improvements of 20 to 30 percent over the simple, naive approaches employed by many production compilers. 
Abstract-found: 1
Intro-found: 1
Reference: [Aho86] <author> Aho, A., Sethi, R., and Ullman, J. D., </author> <booktitle> Compilers Principles, Techniques and Tools, </booktitle> <address> Addi-son-Wesley, Reading, MA, </address> <year> 1986. </year>
Reference: [Alex93] <author> Alexander, M. J., Bailey, M. W., Childers, B. R., Davidson, J. W., and Jinturkar, S., </author> <title> Memory Bandwidth Optimizations for Wide-Bus Machines, </title> <booktitle> Proceedings of the 25th Hawaii International Conference on System Sciences , Mauii, HA, </booktitle> <month> January </month> <year> 1993, </year> <pages> pp. 466-475. </pages>
Reference: [Baco94] <author> Bacon, D. F., Graham, S. L., and Sharp, O. J., </author> <title> Compiler Transformations for High-Performance Computing, </title> <journal> ACM Computing Surveys , 26(4), </journal> <month> Dec. </month> <year> 1994, </year> <pages> pp. 345-420. </pages>
Reference-contexts: The primary effect is a reduction in the total number of instructions executed by the CPU when the loop is executed. In addition, loop unrolling, in conjunction with other code optimizations, can increase instruction-level parallelism and improve memory hierarchy locality <ref> [Baco94, Davi94, Mahl92] </ref>. As an introduction to loop unrolling, consider the C code fragment in Figure 1a. The code computes the dot product of two vectors. When the code is executed, the body of the loop will be executed 13 times. <p> For instance, if the loop body of a rolled loop is replicated n times to get an unrolled loop, then this amount has been described as unroll depth [Dong79] of ( n + 1 ), unroll factor <ref> [Lo95, Baco94] </ref> of (n + 1), unroll amount [Freu94] of (n + 1), or unroll factor [Scho92] of n . Some researchers do not use any term at all [Wall92, Mahl92]. In this paper, we use the term unroll factor in the manner used by Schofield.
Reference: [Beni88] <author> Benitez, M. E. and Davidson, J. W., </author> <title> A Portable Global Optimizer and Linker , Proceedings of SIGPLAN 88 Conference on Programming Language Design and Implementation , Atlanta, </title> <address> GA, </address> <month> June </month> <year> 1988, </year> <pages> pp. 329-338. </pages>
Reference-contexts: Section 7 discusses the interactions between loop unrolling and other common optimizations. Section 8 gives the performance increase achieved by applying loop unrolling. All the results presented in this paper are based on implementation of the algorithms in an existing retargetable back end called vpo <ref> [Beni88, Beni94] </ref>. Using the C front end and vpo , the effectiveness of the algorithms is evaluated on computer systems based on the MIPS R2000 and Motorola 68020 processors [Kan89, Moto84]. 2 TERMINOLOGY This section defines the frequently used terms in this paper.
Reference: [Beni94] <author> Benitez, M. E. and Davidson, J. W., </author> <title> The Advantages of MachineDependent Global Optimizations, </title> <booktitle> Proceedings of the Conference on Programming Languages and System Architecture , Springer Verlag Lecture Notes in Computer Science, </booktitle> <address> Zurich, Switzerland, </address> <month> March </month> <year> 1994, </year> <pages> pp. 105-124. </pages>
Reference-contexts: Section 7 discusses the interactions between loop unrolling and other common optimizations. Section 8 gives the performance increase achieved by applying loop unrolling. All the results presented in this paper are based on implementation of the algorithms in an existing retargetable back end called vpo <ref> [Beni88, Beni94] </ref>. Using the C front end and vpo , the effectiveness of the algorithms is evaluated on computer systems based on the MIPS R2000 and Motorola 68020 processors [Kan89, Moto84]. 2 TERMINOLOGY This section defines the frequently used terms in this paper. <p> If automatic unrolling is applied at the intermediate-code level, then a sophisticated system to perform loop analysis is required to identify anything beyond counting loops containing more than one basic block. Introducing such a system at this level is a wasteful duplication of effort, because recent research <ref> [Beni94] </ref> has shown that loop optimizations are more beneficial if they are done in the compiler back end. If unrolling is applied at the back-end level after other loop optimizations have been applied, then the shortcomings in the above two approaches are eliminated. <p> Function inlining increases the spatial locality and decreases the number of function calls. This subject has been investigated by previous researchers [Davi88, Hwu89, McFa91, Davi93, Chen93]. Inlining increases the size of a basic block, which facilitates better application of traditional optimizations like dead code elimination, constant propagation <ref> [Beni94, Davi88] </ref>, and instruction scheduling. <p> Unrolling this loop will be cheaper. Also, some dead code may be eliminated <ref> [Beni94] </ref>. Thus, unrolling after inlining is easier and likely to yield better code. 7.4 Register Allocation Loop unrolling also interacts with register allocation. If loop unrolling is done before register allocation, the register allocator sees a larger loop body.
Reference: [Cald94] <author> Calder, B. and Grunwald, D., </author> <title> Reducing Branch Costs via Branch Alignment, </title> <booktitle> Proceedings of Sixth International conference on Architectural Support for Programming Languages and Operating Systems , San Jose, </booktitle> <address> CA, </address> <month> Oct. </month> <year> 1994, </year> <pages> pp. 242-251. </pages>
Reference-contexts: Therefore, such loops will benefit disproportionately from unrolling on this architecture. On the Alpha AXP systems [DEC92], branch prediction is static and done by the compiler. All backward branches are predicted taken, and all the forward branches are predicted not taken. Calder <ref> [Cald94] </ref> indicates that even a correctly predicted taken branch will incur a two cycle penalty. Since loop branches are predicted taken, unrolling the loop n times can eliminate as much as a 2n cycle penalty.
Reference: [Chen93] <author> Chen, W. Y., Chung, P. P., Conte, T. M. and Hwu., W. W., </author> <title> The Effect of Code Expanding Optimizations on Instruction Cache Design, </title> <journal> IEEE Transactions on Computers , 42(9), </journal> <month> Sept. </month> <year> 1993, </year> <pages> pp. 1045-1057. </pages>
Reference-contexts: Function inlining increases the spatial locality and decreases the number of function calls. This subject has been investigated by previous researchers <ref> [Davi88, Hwu89, McFa91, Davi93, Chen93] </ref>. Inlining increases the size of a basic block, which facilitates better application of traditional optimizations like dead code elimination, constant propagation [Beni94, Davi88], and instruction scheduling. <p> Page 21 of 36 Last Modified 10/11/95 Chen <ref> [Chen93] </ref> suggests that function inlining decreases the miss ratio due to increased sequentiality of instructions for caches in the range 1-4K, while it increases the miss ratio for caches in the range 8-32K due to increase in the size of the working set. <p> In Figure 13d, the call site is in a loop and the callee also contains a loop. Chen <ref> [Chen93] </ref> has shown that inlining does not have an adverse effect on the performance of large instruction caches. Taking this into consideration, the function can be inlined without any adverse effects.
Reference: [Davi88] <author> Davidson, J. W., and Holler, A. M., </author> <title> A Study of a C Function Inliner, </title> <journal> Software-Practice & Experience, </journal> <volume> 18(8), </volume> <month> Aug. </month> <year> 1988, </year> <pages> pp. 775-790. </pages>
Reference-contexts: The Figure indicates that as the size of the unrolled loop exceeds 16384 instructions, the performance degrades. This degradation in performance will be even more pronounced if unrolling is applied in conjunction with other code replicating optimizations like function inlining <ref> [Davi88, Davi93] </ref>. To make sure that the unrolled loop does not overflow the instruction cache, it is necessary for the compiler to determine the size of the unrolled loop code in terms of machine-language instructions. <p> Function inlining increases the spatial locality and decreases the number of function calls. This subject has been investigated by previous researchers <ref> [Davi88, Hwu89, McFa91, Davi93, Chen93] </ref>. Inlining increases the size of a basic block, which facilitates better application of traditional optimizations like dead code elimination, constant propagation [Beni94, Davi88], and instruction scheduling. <p> Function inlining increases the spatial locality and decreases the number of function calls. This subject has been investigated by previous researchers [Davi88, Hwu89, McFa91, Davi93, Chen93]. Inlining increases the size of a basic block, which facilitates better application of traditional optimizations like dead code elimination, constant propagation <ref> [Beni94, Davi88] </ref>, and instruction scheduling.
Reference: [Davi90] <author> Davidson, J. W. and Whalley, D. B., </author> <title> Ease: An Environment for Architecture Study and Experimentation, </title> <booktitle> Proceedings of the 1990 ACM Sigmetrics Conference on Measurement and Modelling of Computer Systems , Boulder, </booktitle> <publisher> CO, </publisher> <month> May </month> <year> 1990, </year> <pages> pp. 259-260. </pages>
Reference-contexts: The unit of measurement is cycles. Performance of benchmarks gcc , espresso and xlisp were measured using ease <ref> [Davi90] </ref>, a tool to evaluate architectures. One of the measures provided by ease is dynamic instruction counts. It works at the assembly language level, and therefore, does not count no-ops .
Reference: [Davi93] <author> Davidson, J. W., and Holler, A. M., </author> <title> Subprogram Inlining: A Study of its effect on program execution time, </title> <journal> IEEE Transactions on Software Engineering , 18(2), </journal> <month> Feb. </month> <year> 1992, </year> <pages> pp. 89-101. </pages>
Reference-contexts: The Figure indicates that as the size of the unrolled loop exceeds 16384 instructions, the performance degrades. This degradation in performance will be even more pronounced if unrolling is applied in conjunction with other code replicating optimizations like function inlining <ref> [Davi88, Davi93] </ref>. To make sure that the unrolled loop does not overflow the instruction cache, it is necessary for the compiler to determine the size of the unrolled loop code in terms of machine-language instructions. <p> Function inlining increases the spatial locality and decreases the number of function calls. This subject has been investigated by previous researchers <ref> [Davi88, Hwu89, McFa91, Davi93, Chen93] </ref>. Inlining increases the size of a basic block, which facilitates better application of traditional optimizations like dead code elimination, constant propagation [Beni94, Davi88], and instruction scheduling.
Reference: [Davi94] <author> Davidson, J. W. and Jinturkar, S., </author> <title> Memory Access Coalescing: A Technique for Eliminating Redundant Memory Accesses, </title> <booktitle> Proceedings of SIGPLAN 94 Conference on Programming Language Design and Implementation , Orlando, </booktitle> <address> FL, </address> <month> June </month> <year> 1994, </year> <pages> pp 186-195. </pages>
Reference-contexts: The primary effect is a reduction in the total number of instructions executed by the CPU when the loop is executed. In addition, loop unrolling, in conjunction with other code optimizations, can increase instruction-level parallelism and improve memory hierarchy locality <ref> [Baco94, Davi94, Mahl92] </ref>. As an introduction to loop unrolling, consider the C code fragment in Figure 1a. The code computes the dot product of two vectors. When the code is executed, the body of the loop will be executed 13 times. <p> The performance of the benchmark Cache suffers if the leftover iterations are inserted as prologue to the unrolled loop. Furthermore, inserting the code to execute the leftover iterations as prologue code loop skews the alignment of memory references. The benefits from memory-access coalescing <ref> [Davi94] </ref> are dependent on the alignment of the memory references. So it is important that this pattern not be skewed. Our experience is that the starting address of array references which are passed as parameters are usually aligned at a proper boundary.
Reference: [Davi95] <author> Davidson, J. W. and Jinturkar, S., </author> <title> Improving Instruction-level Parallelism by Loop Unrolling and Dynamic Memory Disambiguation, </title> <booktitle> Submitted to 28th International Conference on Microarchitecture </booktitle> . 
Reference-contexts: Since the loop has already been optimized, the structure of the loop code is fixed due to which register renaming needs to be applied only once. This approach, in conjunction with dynamic-memory disambiguation, improves instruction-level parallelism on modern superscalar and VLIW processors <ref> [Davi95] </ref>. 7.6 Architecture dependence Loop unrolling is an architecture-dependent optimization. As mentioned earlier, in isolation, the major gain in performance from loop unrolling is from the reduction in loop overhead which includes the execution of a conditional branch. <p> Loop unrolling will result in larger performance increase on the DECstation (a RISC architecture) if register renaming is applied along with it, since then the instruction pipeline will be better utilized <ref> [Davi95] </ref>. In general, the benefits from loop unrolling, to a large extent, are contingent on the cost of branch instructions to other instructions inside the loop body. From the data presented in the above sections, it is clear loop unrolling can be a very effective code improvement.
Reference: [Digi92] <institution> Alpha Architecture Handbook , Digital Equipment Corporation, </institution> <address> Boston, MA, </address> <year> 1992. </year>
Reference-contexts: The techniques used by C compilers on five RISC platforms were examined. The platforms were the MIPS R2000, the MIPS R4000 [Kane89], the DEC Alpha <ref> [Digi92] </ref>, the IBM RS6000 [IBM90] and the SUN SPARC [Sun87]. Five native compilers and the GNU C [Stal89] compiler on each platform were evaluated. The compiler versions used are documented in the appendix. This is the most common option. These iterations can also be executed as a straight line code.
Reference: [Dong79] <author> Dongarra, J.J. and Hinds, A. R., </author> <title> Unrolling Loops in Fortran , Software-Practice and Experience , 9(3), </title> <month> Mar. </month> <year> 1979, </year> <pages> pp. 219-226. </pages>
Reference-contexts: When implementing loop unrolling in a production compiler, three obvious questions are: how should loop unrolling be done, when should it be done, and what kind of code bodies should it be applied to? Although loop unrolling is a well-known code improvement technique <ref> [Dong79, Weiss87, Henn90] </ref>, there has not been a thorough study of this important technique provides satisfactory answers to these questions. Most previous studies are restricted to unrolling loops which have a single basic block and whose iteration counts are easily determinable at compile time. <p> For instance, if the loop body of a rolled loop is replicated n times to get an unrolled loop, then this amount has been described as unroll depth <ref> [Dong79] </ref> of ( n + 1 ), unroll factor [Lo95, Baco94] of (n + 1), unroll amount [Freu94] of (n + 1), or unroll factor [Scho92] of n . Some researchers do not use any term at all [Wall92, Mahl92]. <p> Candidates for unrolling : All innermost counting loops are candidates for unrolling. 3 PREVIOUS WORK Many researchers discuss loop unrolling as a way of decreasing loop overhead. Dongarra suggests manual replication of the code body for loops written in FORTRAN <ref> [Dong79] </ref>. Array subscripts and loop increments are adjusted to reflect that the loop has been unrolled. Weiss discusses loop unrolling from the perspective of automatic scheduling by the compiler [Weis87]. His study considers only Livermore loops [McMa72]. <p> If unrolling is applied at the back-end level after other loop optimizations have been applied, then the shortcomings in the above two approaches are eliminated. Loop unrolling can have an adverse impact on the performance of the unrolled loop if the latter overflows the instruction cache <ref> [Dong79, Weis87] </ref>. The performance degradation depends on the size, organization, and replacement policy of the cache. For instance, the MIPS R2000 has a 64K direct-mapped write-through instruction cache. This cache can accommodate a maximum of 16K (16384) instructions . If Each instruction on MIPS R2000 requires four bytes.
Reference: [Fish84] <author> Fisher, J. A., Ellis, J. R., Ruttenberg, J. C. and Nicolau, A., </author> <title> Parallel Processing: A Smart Compiler and a Dumb Machine, </title> <booktitle> Proceedings of the SIGPLAN 84 Symposium on Compiler Construction , Montreal, </booktitle> <address> Canada, </address> <month> June </month> <year> 1984, </year> <pages> pp. 37-47. </pages>
Reference: [Freu94] <author> Freudenberger, S. M., Gross, T. R. and Lowney, P. G., </author> <title> Avoidance and Suppression of Compensation Code in a Trace Scheduling Compiler, </title> <journal> ACM Transactions on Programming Languages and Systems , 16(4), </journal> <month> July </month> <year> 1994, </year> <pages> pp. 1156-1214. </pages> <note> Page 35 of 36 Last Modified 10/11/95 </note>
Reference-contexts: For instance, if the loop body of a rolled loop is replicated n times to get an unrolled loop, then this amount has been described as unroll depth [Dong79] of ( n + 1 ), unroll factor [Lo95, Baco94] of (n + 1), unroll amount <ref> [Freu94] </ref> of (n + 1), or unroll factor [Scho92] of n . Some researchers do not use any term at all [Wall92, Mahl92]. In this paper, we use the term unroll factor in the manner used by Schofield. <p> Mahlke discusses optimizations which can increase instruction-level parallelism for supercomputers [Mahl92]. Loop unrolling is one of them. By analyzing loops with known bounds, they show that if register renaming is applied after loop unrolling, the execution time of the loop decreases. In tracescheduling and global compaction methodology <ref> [Fish83, Freu94] </ref>, loop unrolling is a key feature. Freudenberger discusses the effect of loop unrolling on SPEC benchmarks and the way in which it facilitates global scheduling and insertion of the compensation code [Freu94]. <p> In tracescheduling and global compaction methodology [Fish83, Freu94], loop unrolling is a key feature. Freudenberger discusses the effect of loop unrolling on SPEC benchmarks and the way in which it facilitates global scheduling and insertion of the compensation code <ref> [Freu94] </ref>. In addition to reviewing the results reported in the literature, it is worthwhile to take a look at what is done in practice by examining the extent to which existing modern compilers apply loop unrolling automatically. The techniques used by C compilers on five RISC platforms were examined. <p> If unrolling is applied to either the source code or the intermediate code, then this information is either not available or is inaccurate. Some researchers use a conversion factor to estimate the number of machine-language instructions produced for a source-code line. For instance, Freudenberger <ref> [Freu94] </ref> unrolls a C loop four times and a Fortran loop eight times, thus implying that a source-code line in C will yield more assembly code line than a source-code line in Fortran.
Reference: [Henn90] <author> Hennessy, J. L. and Patterson, D. A., </author> <title> Computer Architecture: </title> <publisher> A Quantitative Approach , Morgan Kaufmann Publishers, Inc, </publisher> <address> San Mateo, CA, </address> <year> 1990. </year>
Reference-contexts: When implementing loop unrolling in a production compiler, three obvious questions are: how should loop unrolling be done, when should it be done, and what kind of code bodies should it be applied to? Although loop unrolling is a well-known code improvement technique <ref> [Dong79, Weiss87, Henn90] </ref>, there has not been a thorough study of this important technique provides satisfactory answers to these questions. Most previous studies are restricted to unrolling loops which have a single basic block and whose iteration counts are easily determinable at compile time. <p> With this approach, only one additional register is required to maintain the iteration count of the unrolled loop. The remaining registers are available for use in applying other code improvements. 7.5 Instruction scheduling Instruction scheduling attempts to reorder instructions so that the pipeline performance is improved <ref> [Henn90] </ref>. Instruction scheduling can improve the performance of an unrolled loop [Lo95]. However, if register renaming is applied after register allocation, the ability of an instruction scheduler to reorder instructions in an unrolled loop is limited by artificial dependencies created by naive reuse of registers by loop unrolling.
Reference: [Hwu89] <author> Hwu, W. W., and Chang, P. H., </author> <title> Inline function expansion for compiling C programs, </title> <booktitle> Proceedings of SIGPLAN 89 Conference on Programming Language Design and Implementation , Portland, </booktitle> <address> OR, </address> <month> June </month> <year> 1989, </year> <pages> pp. 146-157. </pages>
Reference-contexts: Function inlining increases the spatial locality and decreases the number of function calls. This subject has been investigated by previous researchers <ref> [Davi88, Hwu89, McFa91, Davi93, Chen93] </ref>. Inlining increases the size of a basic block, which facilitates better application of traditional optimizations like dead code elimination, constant propagation [Beni94, Davi88], and instruction scheduling.
Reference: [IBM90] <institution> IBM RISC System/6000 Technology, Austin, TX, </institution> <year> 1990. </year>
Reference-contexts: The techniques used by C compilers on five RISC platforms were examined. The platforms were the MIPS R2000, the MIPS R4000 [Kane89], the DEC Alpha [Digi92], the IBM RS6000 <ref> [IBM90] </ref> and the SUN SPARC [Sun87]. Five native compilers and the GNU C [Stal89] compiler on each platform were evaluated. The compiler versions used are documented in the appendix. This is the most common option. These iterations can also be executed as a straight line code. <p> If such a loop is unrolled n t imes, then as many as 2n instructions per iteration of the unrolled loop are eliminated. Benefits from unrolling will be different on the RS6000 <ref> [IBM90] </ref>. This architecture uses branch prediction. A correctly predicted branch does not incur any penalty, while a mispredicted branch can incur up to a three cycle penalty. All the conditional branches on this architecture are predicted not taken. This means that most of the loop branches will be mispredicted.
Reference: [Kane89] <author> Kane, G., </author> <title> MIPS RISC Architecture, </title> <publisher> Prentice-Hall, </publisher> <address> Englewood Cliffs, NJ, </address> <year> 1992. </year>
Reference-contexts: The techniques used by C compilers on five RISC platforms were examined. The platforms were the MIPS R2000, the MIPS R4000 <ref> [Kane89] </ref>, the DEC Alpha [Digi92], the IBM RS6000 [IBM90] and the SUN SPARC [Sun87]. Five native compilers and the GNU C [Stal89] compiler on each platform were evaluated. The compiler versions used are documented in the appendix. This is the most common option. <p> Table 5 shows the percent increase in the size of the executable code. 8.2 Performance increase 8.2.1 Reduction in cycle count Measurements of execution cycles of all benchmarks except xlisp , espresso and gcc were taken using pixie , an architecture evaluation tool for MIPS processors <ref> [Kane89] </ref>. The unit of measurement is cycles. Performance of benchmarks gcc , espresso and xlisp were measured using ease [Davi90], a tool to evaluate architectures. One of the measures provided by ease is dynamic instruction counts. It works at the assembly language level, and therefore, does not count no-ops .
Reference: [Lo95] <author> Lo, L. J., and Eggers, S., J., </author> <title> Improving Balanced Scheduling with Compiler Optimizations that increase Instruction-level Parallelism, </title> <booktitle> Proceedings of SIGPLAN 89 Conference on Programming Language Design and Implementation , La Jolla, </booktitle> <address> CA, </address> <month> June </month> <year> 1995, </year> <pages> pp. 151-162. </pages>
Reference-contexts: For instance, if the loop body of a rolled loop is replicated n times to get an unrolled loop, then this amount has been described as unroll depth [Dong79] of ( n + 1 ), unroll factor <ref> [Lo95, Baco94] </ref> of (n + 1), unroll amount [Freu94] of (n + 1), or unroll factor [Scho92] of n . Some researchers do not use any term at all [Wall92, Mahl92]. In this paper, we use the term unroll factor in the manner used by Schofield. <p> The remaining registers are available for use in applying other code improvements. 7.5 Instruction scheduling Instruction scheduling attempts to reorder instructions so that the pipeline performance is improved [Henn90]. Instruction scheduling can improve the performance of an unrolled loop <ref> [Lo95] </ref>. However, if register renaming is applied after register allocation, the ability of an instruction scheduler to reorder instructions in an unrolled loop is limited by artificial dependencies created by naive reuse of registers by loop unrolling.
Reference: [Moto84] <institution> MC68020 32-Bit Microprocessor Users Manual , Prentice-Hall, </institution> <address> Englewood Cliffs, N.J. </address>
Reference-contexts: Using the C front end and vpo , the effectiveness of the algorithms is evaluated on computer systems based on the MIPS R2000 and Motorola 68020 processors <ref> [Kan89, Moto84] </ref>. 2 TERMINOLOGY This section defines the frequently used terms in this paper. Iteration count : The number of times a loop is executed. Loop code : The instructions comprising the loop. <p> Consequently, we felt it important to gather both types of measurements. We gathered our measurements on a R2000-based DECstation Model 5000"125 and Motorola 68020 based Sun-3/200 <ref> [Moto84] </ref>. These two architectures were chosen because they represent two ends of the computer architecture spectrum: the DECstation is a RISC architecture while the Sun-3 is a CISC architecture.
Reference: [Mahl92] <author> Mahlke, S. A., Chen, W. Y., Gyllenhaal, J. C. and Hwu, W. W., </author> <title> Compiler Code Transformations for Superscalar-Based High-Performance Systems, </title> <booktitle> Proceedings of Supercomputing 92, </booktitle> <address> Portland, OR, </address> <month> Nov. </month> <year> 1992, </year> <pages> pp. 808-817. </pages>
Reference-contexts: The primary effect is a reduction in the total number of instructions executed by the CPU when the loop is executed. In addition, loop unrolling, in conjunction with other code optimizations, can increase instruction-level parallelism and improve memory hierarchy locality <ref> [Baco94, Davi94, Mahl92] </ref>. As an introduction to loop unrolling, consider the C code fragment in Figure 1a. The code computes the dot product of two vectors. When the code is executed, the body of the loop will be executed 13 times. <p> Some researchers do not use any term at all <ref> [Wall92, Mahl92] </ref>. In this paper, we use the term unroll factor in the manner used by Schofield. Thus, if the loop body of a rolled loop is replicated n times to get an unrolled loop, then the unroll factor is n. <p> His study considers only Livermore loops [McMa72]. This study also discusses the effect of loop unrolling on instruction buffer size and register pressure within the loop. Mahlke discusses optimizations which can increase instruction-level parallelism for supercomputers <ref> [Mahl92] </ref>. Loop unrolling is one of them. By analyzing loops with known bounds, they show that if register renaming is applied after loop unrolling, the execution time of the loop decreases. In tracescheduling and global compaction methodology [Fish83, Freu94], loop unrolling is a key feature. <p> However, if register renaming is applied after register allocation, the ability of an instruction scheduler to reorder instructions in an unrolled loop is limited by artificial dependencies created by naive reuse of registers by loop unrolling. These artificial dependencies can be eliminated by applying register renaming <ref> [Mahl92] </ref> in software while the instructions are being scheduled at the very end of the compilation process. Since the loop has already been optimized, the structure of the loop code is fixed due to which register renaming needs to be applied only once.
Reference: [McMa72] <author> McMohan, F. H, </author> <title> The Livermore Fortran Kernels: A Computer Test of the Numerical Performance Range, </title> <institution> Lawrence Livermore National Laboratory, Livermore, </institution> <address> CA, </address> <year> 1986. </year>
Reference-contexts: Dongarra suggests manual replication of the code body for loops written in FORTRAN [Dong79]. Array subscripts and loop increments are adjusted to reflect that the loop has been unrolled. Weiss discusses loop unrolling from the perspective of automatic scheduling by the compiler [Weis87]. His study considers only Livermore loops <ref> [McMa72] </ref>. This study also discusses the effect of loop unrolling on instruction buffer size and register pressure within the loop. Mahlke discusses optimizations which can increase instruction-level parallelism for supercomputers [Mahl92]. Loop unrolling is one of them.
Reference: [McFa91] <author> McFarling, </author> <title> S, Procedure Merging with Instruction Caches, </title> <booktitle> Proceedings of SIGPLAN 91 Conference on Programming Language Design and Implementation , Ontario, </booktitle> <address> Canada, </address> <month> June </month> <year> 1991, </year> <pages> pp. 71-79. </pages>
Reference-contexts: Function inlining increases the spatial locality and decreases the number of function calls. This subject has been investigated by previous researchers <ref> [Davi88, Hwu89, McFa91, Davi93, Chen93] </ref>. Inlining increases the size of a basic block, which facilitates better application of traditional optimizations like dead code elimination, constant propagation [Beni94, Davi88], and instruction scheduling.
Reference: [Nico87] <author> Nicolau, A., </author> <title> Loop Quantization or Unwinding Done Right , First International Conference on SuperComputing , Athens, </title> <address> Greece, </address> <month> June </month> <year> 1987, </year> <pages> pp. 294-306. </pages>
Reference: [Scho89] <author> Schofield, </author> <title> C.F., Optimizing Fortran Programs , Ellis Horwood Limited, </title> <address> Chichester, UK, </address> <year> 1989. </year>
Reference: [Stal89] <author> Stallman, R. M., </author> <title> Using and Porting GNU CC , Free Software Foundation, </title> <address> Cambridge, MA, </address> <year> 1989. </year>
Reference-contexts: The techniques used by C compilers on five RISC platforms were examined. The platforms were the MIPS R2000, the MIPS R4000 [Kane89], the DEC Alpha [Digi92], the IBM RS6000 [IBM90] and the SUN SPARC [Sun87]. Five native compilers and the GNU C <ref> [Stal89] </ref> compiler on each platform were evaluated. The compiler versions used are documented in the appendix. This is the most common option. These iterations can also be executed as a straight line code.
Reference: [Sun87] <institution> The SPARC Architecture Manual , Version 7, Sun Microsystems Corporation, Mountain View, </institution> <address> CA, </address> <year> 1987. </year>
Reference-contexts: The techniques used by C compilers on five RISC platforms were examined. The platforms were the MIPS R2000, the MIPS R4000 [Kane89], the DEC Alpha [Digi92], the IBM RS6000 [IBM90] and the SUN SPARC <ref> [Sun87] </ref>. Five native compilers and the GNU C [Stal89] compiler on each platform were evaluated. The compiler versions used are documented in the appendix. This is the most common option. These iterations can also be executed as a straight line code.
Reference: [Wall93] <author> Wall, D. W., </author> <title> Limits of Instruction-Level Parallelism, </title> <note> WRL Research Report , 93/6, </note> <institution> Digital Equipment Corporation, </institution> <address> Palo Alto, CA, </address> <year> 1993. </year>

References-found: 30

