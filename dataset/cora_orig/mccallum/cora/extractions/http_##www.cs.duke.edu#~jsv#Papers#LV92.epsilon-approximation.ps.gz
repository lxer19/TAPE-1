URL: http://www.cs.duke.edu/~jsv/Papers/LV92.epsilon-approximation.ps.gz
Refering-URL: http://www.cs.duke.edu/~jsv/Papers/catalog/node75.html
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Title: *-Approximations with Minimum Packing Constraint Violation (extended abstract)  
Author: Jyh-Han Lin and Jeffrey Scott Vitter 
Note: Support was provided in part by an National Science Foundation Presidential Young Investigator Award CCR-9047466 with matching funds from IBM, by NSF research grant CCR-9007851, by Army Research Office grant DAAL03-91-G-0035, and by the Office of Naval Research and the Defense Advanced Research Projects Agency under contract N00014-91-J-4052, ARPA order 8225. The authors can be reached by electronic mail at jhl@cs.brown.edu and jsv@cs.brown.edu, respectively.  
Address: Providence, R. I. 02912-1910  
Affiliation: Department of Computer Science Brown University  
Abstract: We present efficient new randomized and deterministic methods for transforming optimal solutions for a type of relaxed integer linear program into provably good solutions for the corresponding N P-hard discrete optimization problem. Without any constraint violation, the *-approximation problem for many problems of this type is itself N P-hard. Our methods provide polynomial-time *-approximations while attempting to minimize the packing constraint violation. Our methods lead to the first known approximation algorithms with provable performance guarantees for the s-median problem, the tree pruning problem, and the generalized assignment problem. These important problems have numerous applications to data compression, vector quantization, memory-based learning, computer graphics, image processing, clustering, regression, network location, scheduling, and communication. We provide evidence via reductions that our approximation algorithms are nearly optimal in terms of the packing constraint violation. We also discuss some recent applications of our techniques to scheduling problems. 
Abstract-found: 1
Intro-found: 1
Reference: [ACC] <author> S. Ahn, A. Cooper, G. Cornuejols, and A. Frieze, </author> <title> "Probabilistic Analysis of a Relaxation for the K-Median Problem," </title> <note> Mathematics of Operations Research 13 (February 1988), 1-31. </note>
Reference-contexts: Clearly, the optimal solution for the fractional s-median problem is a lower bound on the solutions of the s-median problem. The following lemma is useful for the rest of this section: Lemma 1 <ref> [ACC] </ref> Given a solution by = (by 1 ; . . . ; by n ) for the fractional s-median problem, we can determine the optimal fractional values for x ij . <p> Without any probabilistic assumptions <ref> [ACC, FiH, Pap] </ref>, no approximation algorithms are known. Even if the cost matrix is symmetric, by similar reasoning as in [KaH, SaG], it is easy to show the following: Lemma 2 The *-approximation problem for the s-median problem is N P-hard even if the cost matrix is symmetric.
Reference: [BFO] <author> L. Breiman, J. H. Friedman, R. A. Olshen, and C. J. Stone, </author> <title> Classification Trees and Regression Trees, </title> <publisher> Wadsworth, </publisher> <address> Belmont, CA, </address> <year> 1984. </year>
Reference-contexts: The central problem for tree-structured vector quantization is how to find a particular tree subject to some cost constraint (such as a bound on the average path length) that minimizes the average distortion. The most popular approach for this problem, introduced by Breiman, Freidman, Olshen, and Stone <ref> [BFO] </ref> in the context of classification and regression trees, is to first build a large initial tree-structured vector quantizer and then prune back the tree to satisfy the cost requirement. <p> Besides tree-structured vector quantization, tree pruning algorithms have many applications such as memory-based learning, regression trees, decision trees, and computer graphics. For more applications of the tree pruning problem, we refer the readers to <ref> [BFO, CLG] </ref>. In this section we present a provably good approximate tree pruning algorithm. We also introduce the notion of probability search trees, which have the potential of outperforming the optimal pruned tree when the cost of trees is the average path length. <p> Chou, Lookabaugh, and Gray [CLG] proposed a tree pruning heuristic based on the BFOS algorithm <ref> [BFO] </ref> in which a given initial tree is pruned back according to certain optimization criterion. Their heuristic traces the lower convex hull of the distortion-cost function and the final pruned subtrees are optimal for their costs.
Reference: [CLG] <author> P. A. Chou, T. Lookabaugh, and R. M. Gray, </author> <title> "Optimal Pruning with Applications to Tree-Structured Source Coding and Modeling," </title> <journal> IEEE Transactions on Information Theory (1989), </journal> <pages> 299-315. </pages>
Reference-contexts: Besides tree-structured vector quantization, tree pruning algorithms have many applications such as memory-based learning, regression trees, decision trees, and computer graphics. For more applications of the tree pruning problem, we refer the readers to <ref> [BFO, CLG] </ref>. In this section we present a provably good approximate tree pruning algorithm. We also introduce the notion of probability search trees, which have the potential of outperforming the optimal pruned tree when the cost of trees is the average path length. <p> We also introduce the notion of probability search trees, which have the potential of outperforming the optimal pruned tree when the cost of trees is the average path length. Experimental results on lossy image compression are given in [LiVb]. 3.1 Definitions We use the notation of <ref> [CLG] </ref>: A tree T is a finite set of nodes t 0 ; t 1 ; . . . ; t n , with a unique root node t 0 . The set of leaves of a tree T is denoted by e T . <p> Definition 2 Given a tree T and a bound C on the cost, the tree pruning problem is to find a pruned sub-tree S of T such that C (S) C and D (S) is minimized. Chou, Lookabaugh, and Gray <ref> [CLG] </ref> proposed a tree pruning heuristic based on the BFOS algorithm [BFO] in which a given initial tree is pruned back according to certain optimization criterion. Their heuristic traces the lower convex hull of the distortion-cost function and the final pruned subtrees are optimal for their costs.
Reference: [Chv] <author> V. Chvatal, </author> <title> "A Greedy Heuristic for the Set-Covering Problem," </title> <booktitle> Mathematics of Operations Research 4 (1979), </booktitle> <pages> 233-235. </pages>
Reference-contexts: The rest of the proof of Theorem 1 follows from Theorem 3. 2.3 Deterministic Greedy Rounding In this section, we prove Theorem 2 by showing that the well-known greedy set cover heuristic <ref> [Chv, Joh, Lov] </ref> can be adapted to solve the filtered program for the s-median problem given in Section 2.1. The algorithm is outlined as follows: 1. Solve the linear program relaxation of the s-median problem by linear programming techniques; denote the fractional solution by by; bx. 2. <p> Vertex i is in S j if and only if c ij (1+*) b C i (or equivalently if and only if j 2 V i ). 4. Apply the greedy set cover algorithm <ref> [Chv, Joh, Lov] </ref>: At each iteration, we choose the set S j which covers the most uncovered vertices. We repeat this process until all vertices are covered. Let U be the set of vertices that the greedy algorithm selects. <p> The linear program relaxation of the simplified filtered program is an instance of the fractional set cover problem <ref> [Chv, Lov] </ref> with the collection of sets fS j g j2V defined as above. We now show that the size of the optimal fractional cover is less than (1 + 1=*)s by converting by to a fractional cover ey of size less than (1 + 1=*)s. <p> By Lemma 3, we have P j2V i ey j 1. Hence, ey is a valid fractional cover and its size is X ey j = j2V minfby j = b Y ; 1g &lt; (1 + 1=*) j2V (1 + 1=*)s: By the results in <ref> [Chv, Lov] </ref>, the number of sets (medians) selected by the greedy algorithm is less than (1 + 1=*)s (ln n + 1).
Reference: [FeG] <author> T. Feder and D. Greene, </author> <title> "Optimal Algorithms for Approximate Clustering," </title> <booktitle> in Proceedings of the 20th Annual Symposium on the Theory of Computing, </booktitle> <year> 1988, </year> <pages> 434-444. </pages>
Reference-contexts: For approximation algorithms for the s-center problem, we refer the readers to <ref> [FeG, Gon, HoSb] </ref> ger linear program of minimizing X X c ij x ij (12) subject to X x ij = 1; i 2 V; j2V x ij y j ; i; j 2 V; where y j = 1 if and only if vertex j is chosen as a median,
Reference: [FiH] <author> M. L. Fisher and D. S. Hochbaum, </author> <title> "Probabilistic Analysis of the Planar K-Median Problem," </title> <booktitle> Mathematics of Operations Research 5 (Febru-ary 1980), </booktitle> <pages> 27-34. </pages>
Reference-contexts: Without any probabilistic assumptions <ref> [ACC, FiH, Pap] </ref>, no approximation algorithms are known. Even if the cost matrix is symmetric, by similar reasoning as in [KaH, SaG], it is easy to show the following: Lemma 2 The *-approximation problem for the s-median problem is N P-hard even if the cost matrix is symmetric.
Reference: [GaJ] <author> M. R. Garey and D. S. Johnson, </author> <title> Computers and intractability: A Guide to the Theory of N P-completeness, </title> <editor> W. H. </editor> <publisher> Freeman and Co., </publisher> <address> San Francisco, CA, </address> <year> 1979. </year>
Reference-contexts: Three important problems we consider of the above type are the s-median problem, the tree pruning problem, and the generalized assignment problem. These problems arise directly in data compression, vector quantization, memory-based learning, computer graphics, image processing, clustering, regression, network location, scheduling, and communication. The *-approximation problem <ref> [GaJ, SaG] </ref> for a N P-hard minimization problem is to approximate the optimal solution within a relative factor of * &gt; 0. The *- approximation problem for many problems of the above type is N P-hard [SaG]. <p> Then we set bx ij = by j for j = j 1 (i); . . . ; j p1 (i), bx ij p (i) = 1 `=1 by j ` (i) , and bx ij = 0 otherwise. 2 The s-median problem is N P-hard, even in Euclidean space <ref> [GaJ, KaH, MeS, Pap] </ref>. Without any probabilistic assumptions [ACC, FiH, Pap], no approximation algorithms are known.
Reference: [Gon] <author> T. F. Gonzalez, </author> <title> "Clustering to Minimize the Maximum Intercluster Distance," </title> <booktitle> Theoretical Computer Science 38 (1985), </booktitle> <pages> 293-306. </pages>
Reference-contexts: For approximation algorithms for the s-center problem, we refer the readers to <ref> [FeG, Gon, HoSb] </ref> ger linear program of minimizing X X c ij x ij (12) subject to X x ij = 1; i 2 V; j2V x ij y j ; i; j 2 V; where y j = 1 if and only if vertex j is chosen as a median,
Reference: [GLS] <author> M. Grotschel, L. Lovasz, and A. Schrijver, </author> <title> Geometric Algorithms and Combinatorial Optimization, </title> <publisher> Springer-Verlag, </publisher> <address> Berlin, Heidelberg, New York, </address> <year> 1988. </year>
Reference-contexts: If r ij = 1 for all i 2 I and j 2 J , then the well-known Hungarian method [Kuh] can be adapted for its solution <ref> [GLS] </ref>. On the other hand, Sahni and Gonza-lez [SaG] show that the *-approximation problem is N P-hard for the generalized assignment problem. For simplicity, we assume for the rest of the section that the generalized assignment problem is feasible.
Reference: [Hoc] <author> D. S. Hochbaum, </author> <title> "Heuristics for the Fixed-Cost Median Problem," </title> <booktitle> Mathematical Programming 22 (1982), </booktitle> <pages> 148-162. </pages>
Reference-contexts: To the best of our knowledge, the only approximation algorithm for a median problem is due to Hochbaum <ref> [Hoc] </ref> for the fixed-cost median problem, where the objective is to minimize the sum P i2V min j2U fc ij g; where U is a median set and f j is a fixed cost for selecting vertex j, and there is no restriction on the number of medians chosen. <p> Theorem 7 matches the logarithmic performance bound of <ref> [Hoc] </ref> up to a constant factor. However, when P j2V fl f j is dominated by P formance bound is better than that of [Hoc]. 3 Approximate Tree Pruning A tree-structured vector quantizer partitions a signal space into a hierarchy of regions, each of which is represented by a representative vector. <p> Theorem 7 matches the logarithmic performance bound of <ref> [Hoc] </ref> up to a constant factor. However, when P j2V fl f j is dominated by P formance bound is better than that of [Hoc]. 3 Approximate Tree Pruning A tree-structured vector quantizer partitions a signal space into a hierarchy of regions, each of which is represented by a representative vector.
Reference: [HoSa] <author> D. S. Hochbaum and D. B. Shmoys, </author> <title> "Using Dual Approximation Algorithms for Scheduling Problems: Practical and Theoretical Results," </title> <journal> Journal of the Association for Computing Machinery 34 (1987), </journal> <pages> 144-162. </pages>
Reference-contexts: For simplicity, we assume for the rest of the section that the generalized assignment problem is feasible. We remark that we can use the notion of relaxed decision procedures <ref> [HoSa, LST] </ref> for checking feasibility approximately. The filtering procedure for the generalized assignment problem is similar to that of the s-median problem.
Reference: [HoSb] <author> D. S. Hochbaum and D. B. Shmoys, </author> <title> "A Unified Approach to Approximation Algorithms for Bottleneck Problems," </title> <journal> Journal of the Association for Computing Machinery 33 (July 1986), </journal> <pages> 533-550. </pages>
Reference-contexts: For approximation algorithms for the s-center problem, we refer the readers to <ref> [FeG, Gon, HoSb] </ref> ger linear program of minimizing X X c ij x ij (12) subject to X x ij = 1; i 2 V; j2V x ij y j ; i; j 2 V; where y j = 1 if and only if vertex j is chosen as a median,
Reference: [Joh] <author> D. S. Johnson, </author> <title> "Approximation Algorithms for Combinatorial Problems," </title> <journal> Journal of Computer and System Sciences 9 (1974), </journal> <pages> 256-278. </pages>
Reference-contexts: The rest of the proof of Theorem 1 follows from Theorem 3. 2.3 Deterministic Greedy Rounding In this section, we prove Theorem 2 by showing that the well-known greedy set cover heuristic <ref> [Chv, Joh, Lov] </ref> can be adapted to solve the filtered program for the s-median problem given in Section 2.1. The algorithm is outlined as follows: 1. Solve the linear program relaxation of the s-median problem by linear programming techniques; denote the fractional solution by by; bx. 2. <p> Vertex i is in S j if and only if c ij (1+*) b C i (or equivalently if and only if j 2 V i ). 4. Apply the greedy set cover algorithm <ref> [Chv, Joh, Lov] </ref>: At each iteration, we choose the set S j which covers the most uncovered vertices. We repeat this process until all vertices are covered. Let U be the set of vertices that the greedy algorithm selects.
Reference: [KaH] <author> O. Kariv and S. L. Hakimi, </author> <title> "An Algorithmic Approach to Network Location Problems. II: </title> <journal> The p-Medians," SIAM Journal on Applied Mathematics (1979), </journal> <pages> 539-560. </pages>
Reference-contexts: Then we set bx ij = by j for j = j 1 (i); . . . ; j p1 (i), bx ij p (i) = 1 `=1 by j ` (i) , and bx ij = 0 otherwise. 2 The s-median problem is N P-hard, even in Euclidean space <ref> [GaJ, KaH, MeS, Pap] </ref>. Without any probabilistic assumptions [ACC, FiH, Pap], no approximation algorithms are known. <p> Without any probabilistic assumptions [ACC, FiH, Pap], no approximation algorithms are known. Even if the cost matrix is symmetric, by similar reasoning as in <ref> [KaH, SaG] </ref>, it is easy to show the following: Lemma 2 The *-approximation problem for the s-median problem is N P-hard even if the cost matrix is symmetric.
Reference: [Kar] <author> N. Karmarkar, </author> <title> "A New Polynomial-Time Algorithm for Linear Programming," </title> <booktitle> Combina-torica 4 (1984), </booktitle> <pages> 373-395. </pages>
Reference-contexts: We now give an outline of the techniques. Let be a 0-1 program of the given type and let L be its linear program relaxation. The basic algorithm consists of the following three phases: Phase 1 Solve L by linear programming techniques <ref> [Kar, Kha] </ref>; denote the fractional solution by bx.
Reference: [Kha] <author> L. G. Khachiyan, </author> <title> "A Polynomial Algorithm in Linear Programming," </title> <journal> Soviet Math. </journal> <volume> Doklady 20 (1979), </volume> <pages> 191-194. </pages>
Reference-contexts: We now give an outline of the techniques. Let be a 0-1 program of the given type and let L be its linear program relaxation. The basic algorithm consists of the following three phases: Phase 1 Solve L by linear programming techniques <ref> [Kar, Kha] </ref>; denote the fractional solution by bx.
Reference: [Kuh] <author> H. W. Kuhn, </author> <title> "The Hungarian Method for the Assignment Problem," </title> <journal> Naval Research Logistics Quarterly 2 (1955), </journal> <pages> 83-97. </pages>
Reference-contexts: If r ij = 1 for all i 2 I and j 2 J , then the well-known Hungarian method <ref> [Kuh] </ref> can be adapted for its solution [GLS]. On the other hand, Sahni and Gonza-lez [SaG] show that the *-approximation problem is N P-hard for the generalized assignment problem. For simplicity, we assume for the rest of the section that the generalized assignment problem is feasible.
Reference: [LaL] <author> E. L. Lawler and J. Labetoulle, </author> <title> "On Preemptive Scheduling of Unrelated Parallel Processors by Linear Programming," </title> <journal> Journal of ACM 25 (October 1978), </journal> <pages> 612-61. </pages>
Reference-contexts: Then we have L fl &lt; 4 b L: Proof Sketch: Lawler and Labetoulle <ref> [LaL] </ref> showed the optimal makespan with preemption is the optimal solution to the following linear program of minimizing L subject to X x ij = 1; i 2 I; i2I X p ij x ij L; i 2 I; Given the optimal fractional solution bx and the optimal makespan with preemption
Reference: [LST] <author> J. K. Lenstra, D. B. Shmoys, and E. Tar-dos, </author> <title> "Approximation Algorithms for Scheduling Unrelated Parallel Machines," </title> <booktitle> Mathematical Programming A 46 (1990), </booktitle> <pages> 259-271. </pages>
Reference-contexts: For simplicity, we assume for the rest of the section that the generalized assignment problem is feasible. We remark that we can use the notion of relaxed decision procedures <ref> [HoSa, LST] </ref> for checking feasibility approximately. The filtering procedure for the generalized assignment problem is similar to that of the s-median problem. <p> By Corollary 2 in <ref> [LST] </ref>, we have proven the hardness result. 2 4.2 Scheduling of Unrelated Parallel Machines One basic question in the scheduling of unrelated parallel machines is the relationship between the optimal makespan with preemption and the optimal makespan without preemption. Surprisingly, no nontrivial bound was known for this problem. <p> It is clear that ex is a feasible solution to the following set of linear constraints: X x ij = 1; i 2 I; j2J x ij 2 f0; 1g; i 2 I; j 2 J; By the Rounding Theorem of <ref> [LST] </ref>, we can find a non-preemptive schedule of length less than 4 b L in polynomial time. 2 Partially based on our techniques, Shmoys and Tardos [ShT] developed an O (log 2 K)-approximation algorithm for non-preemptive scheduling of unrelated parallel machines with precedence constraints on the jobs. 5 Conclusions Our results
Reference: [LSC] <author> J. Lin, J. A. Storer, and M. Cohn, </author> <title> "On the Complexity of Optimal Tree Pruning for Source Coding," </title> <booktitle> in Proceedings of the Data Compression Conference, </booktitle> <editor> J. A. Storer and J. H. Reif, eds., </editor> <address> Snowbird, Utah, </address> <month> April </month> <year> 1991, </year> <pages> 63-72. </pages>
Reference-contexts: However, if there is no point (pruned subtree) on the lower convex hull at the desired cost, it requires timesharing between two neighboring points (pruned sub-trees). Lin, Storer, and Cohn <ref> [LSC] </ref> show that the tree pruning problem is N P-hard in general. 2 Our main result is the following theorem: Theorem 8 Given any * &gt; 0, there exist an approximate tree pruning algorithm that outputs a pruned sub-tree S satisfying C (S) &lt; (1 + 1=*)C (14) D (S) (1
Reference: [LiVa] <author> J.-H. Lin and J. S. Vitter, </author> <title> "A Theory for Memory-Based Learning," </title> <type> Technical Report, </type> <institution> Dept. of Computer Science, Brown University, </institution> <year> 1992. </year>
Reference-contexts: It is open whether our performance guarantees for the s-median problem can be tightened for the special case of Euclidean space. Practical use of our algorithms for vector quantization, clustering, and memory-based learning can be found in <ref> [LiVa, LiVb] </ref>. Acknowledgements. The application in Section 4.2 was communicated to us by Shmoys and Tardos [ShT]. We thank them for their permission to include the result here.
Reference: [LiVb] <author> J.-H. Lin and J. S. Vitter, </author> <title> "Nearly Optimal Vector Quantization via Linear Programming," </title> <booktitle> in Proceedings of the IEEE Data Compression Conference, </booktitle> <address> Snowbird, Utah, </address> <month> March </month> <year> 1992. </year>
Reference-contexts: We also introduce the notion of probability search trees, which have the potential of outperforming the optimal pruned tree when the cost of trees is the average path length. Experimental results on lossy image compression are given in <ref> [LiVb] </ref>. 3.1 Definitions We use the notation of [CLG]: A tree T is a finite set of nodes t 0 ; t 1 ; . . . ; t n , with a unique root node t 0 . <p> It is open whether our performance guarantees for the s-median problem can be tightened for the special case of Euclidean space. Practical use of our algorithms for vector quantization, clustering, and memory-based learning can be found in <ref> [LiVa, LiVb] </ref>. Acknowledgements. The application in Section 4.2 was communicated to us by Shmoys and Tardos [ShT]. We thank them for their permission to include the result here.
Reference: [Lov] <author> L. Lovasz, </author> <title> "On the Ratio of Optimal Integral and Fractional Covers," </title> <booktitle> Discrete Mathematics 13 (1975), </booktitle> <pages> 383-390. </pages>
Reference-contexts: The rest of the proof of Theorem 1 follows from Theorem 3. 2.3 Deterministic Greedy Rounding In this section, we prove Theorem 2 by showing that the well-known greedy set cover heuristic <ref> [Chv, Joh, Lov] </ref> can be adapted to solve the filtered program for the s-median problem given in Section 2.1. The algorithm is outlined as follows: 1. Solve the linear program relaxation of the s-median problem by linear programming techniques; denote the fractional solution by by; bx. 2. <p> Vertex i is in S j if and only if c ij (1+*) b C i (or equivalently if and only if j 2 V i ). 4. Apply the greedy set cover algorithm <ref> [Chv, Joh, Lov] </ref>: At each iteration, we choose the set S j which covers the most uncovered vertices. We repeat this process until all vertices are covered. Let U be the set of vertices that the greedy algorithm selects. <p> The linear program relaxation of the simplified filtered program is an instance of the fractional set cover problem <ref> [Chv, Lov] </ref> with the collection of sets fS j g j2V defined as above. We now show that the size of the optimal fractional cover is less than (1 + 1=*)s by converting by to a fractional cover ey of size less than (1 + 1=*)s. <p> By Lemma 3, we have P j2V i ey j 1. Hence, ey is a valid fractional cover and its size is X ey j = j2V minfby j = b Y ; 1g &lt; (1 + 1=*) j2V (1 + 1=*)s: By the results in <ref> [Chv, Lov] </ref>, the number of sets (medians) selected by the greedy algorithm is less than (1 + 1=*)s (ln n + 1).
Reference: [MeS] <author> N. Megiddo and K. J. Supowit, </author> <title> "On the Complexity of Some Common Geometric Location Problems," </title> <journal> SIAM Journal on Computing 13 (1984), </journal> <pages> 182-196. </pages>
Reference-contexts: Then we set bx ij = by j for j = j 1 (i); . . . ; j p1 (i), bx ij p (i) = 1 `=1 by j ` (i) , and bx ij = 0 otherwise. 2 The s-median problem is N P-hard, even in Euclidean space <ref> [GaJ, KaH, MeS, Pap] </ref>. Without any probabilistic assumptions [ACC, FiH, Pap], no approximation algorithms are known.
Reference: [Pap] <author> C. H. Papadimitriou, </author> <title> "Worst-case and Probabilistic Analysis of a Geometric Location Problem," </title> <journal> SIAM Journal on Computing 10 (1981), </journal> <pages> 542-557. </pages>
Reference-contexts: Then we set bx ij = by j for j = j 1 (i); . . . ; j p1 (i), bx ij p (i) = 1 `=1 by j ` (i) , and bx ij = 0 otherwise. 2 The s-median problem is N P-hard, even in Euclidean space <ref> [GaJ, KaH, MeS, Pap] </ref>. Without any probabilistic assumptions [ACC, FiH, Pap], no approximation algorithms are known. <p> Without any probabilistic assumptions <ref> [ACC, FiH, Pap] </ref>, no approximation algorithms are known. Even if the cost matrix is symmetric, by similar reasoning as in [KaH, SaG], it is easy to show the following: Lemma 2 The *-approximation problem for the s-median problem is N P-hard even if the cost matrix is symmetric.
Reference: [Rag] <author> P. Raghavan, </author> <title> "Probabilistic Construction of Deterministic Algorithms: Approximating Packing Integer Programs," </title> <journal> Journal of Computer and System Science 37 (1988), </journal> <pages> 130-143. </pages>
Reference-contexts: That is, without any constraint violation, the *-approximation problem is as hard as finding the optimal solution. In this paper, we develop the first known algorithms for finding *-approximate solutions with minimum packing constraint violation. Raghavan and Thompson <ref> [Rag, RaT] </ref> introduce techniques for approximating a 0-1 integer program by first solving its linear program relaxation and rounding the resulting solution. They successfully apply their rounding techniques to several discrete optimization problems 1 such as routing problems in VLSI, undirected multi--commodity flow problems, and the k-matching problem on hypergraphs. <p> Our algorithm *-approximates the optimal number of medians while bounding the total distance by a factor of 2 (1 + 1=*). The transformation technique used is fundamentally different from those of randomized and deterministic rounding <ref> [Rag, RaT] </ref> and the methods used in this paper in the following way: Previous techniques never set variables with zero values in the fractional solution to 1. Our new algorithm, on the other hand, may set 0-valued variables to 1.
Reference: [RaT] <author> P. Raghavan and C. D. Thompson, </author> <title> "Randomized Rounding: A Technique for Provably Good Algorithms and Algorithmic Proofs," </title> <booktitle> Combinatorics 7 (1987), </booktitle> <pages> 365-374. </pages>
Reference-contexts: That is, without any constraint violation, the *-approximation problem is as hard as finding the optimal solution. In this paper, we develop the first known algorithms for finding *-approximate solutions with minimum packing constraint violation. Raghavan and Thompson <ref> [Rag, RaT] </ref> introduce techniques for approximating a 0-1 integer program by first solving its linear program relaxation and rounding the resulting solution. They successfully apply their rounding techniques to several discrete optimization problems 1 such as routing problems in VLSI, undirected multi--commodity flow problems, and the k-matching problem on hypergraphs. <p> Our algorithm *-approximates the optimal number of medians while bounding the total distance by a factor of 2 (1 + 1=*). The transformation technique used is fundamentally different from those of randomized and deterministic rounding <ref> [Rag, RaT] </ref> and the methods used in this paper in the following way: Previous techniques never set variables with zero values in the fractional solution to 1. Our new algorithm, on the other hand, may set 0-valued variables to 1.
Reference: [SaG] <author> S. Sahni and T. F. Gonzalez, </author> <title> "P-Complete Approximation Problems," </title> <journal> Journal of the Association for Computing Machinery 23 (July, </journal> <year> 1976), </year> <pages> 555-565. </pages>
Reference-contexts: Three important problems we consider of the above type are the s-median problem, the tree pruning problem, and the generalized assignment problem. These problems arise directly in data compression, vector quantization, memory-based learning, computer graphics, image processing, clustering, regression, network location, scheduling, and communication. The *-approximation problem <ref> [GaJ, SaG] </ref> for a N P-hard minimization problem is to approximate the optimal solution within a relative factor of * &gt; 0. The *- approximation problem for many problems of the above type is N P-hard [SaG]. <p> The *-approximation problem [GaJ, SaG] for a N P-hard minimization problem is to approximate the optimal solution within a relative factor of * &gt; 0. The *- approximation problem for many problems of the above type is N P-hard <ref> [SaG] </ref>. That is, without any constraint violation, the *-approximation problem is as hard as finding the optimal solution. In this paper, we develop the first known algorithms for finding *-approximate solutions with minimum packing constraint violation. <p> Without any probabilistic assumptions [ACC, FiH, Pap], no approximation algorithms are known. Even if the cost matrix is symmetric, by similar reasoning as in <ref> [KaH, SaG] </ref>, it is easy to show the following: Lemma 2 The *-approximation problem for the s-median problem is N P-hard even if the cost matrix is symmetric. <p> If r ij = 1 for all i 2 I and j 2 J , then the well-known Hungarian method [Kuh] can be adapted for its solution [GLS]. On the other hand, Sahni and Gonza-lez <ref> [SaG] </ref> show that the *-approximation problem is N P-hard for the generalized assignment problem. For simplicity, we assume for the rest of the section that the generalized assignment problem is feasible. We remark that we can use the notion of relaxed decision procedures [HoSa, LST] for checking feasibility approximately.
Reference: [ShT] <author> D. B. Shmoys and E. Tardos, </author> <type> Personal Communication, </type> <month> January 23, </month> <year> 1992. </year>
Reference-contexts: problem that, given any * &gt; 0, outputs an assignment x such that X i2I j2J X X c ij x fl where x fl is an optimal assignment, and X r ij x ij &lt; (2 + 1=*)b j Recently, since the acceptance of this paper, Shmoys and Tardos <ref> [ShT] </ref> have developed an approximation algorithm for the generalized assignment problem that outputs an assignment x such that P P i2I j2J c ij x fl ij ; where x fl is an optimal assignment, and i2I r ij x ij 2b j for all j 2 J: We now relate <p> x ij = 1; i 2 I; j2J x ij 2 f0; 1g; i 2 I; j 2 J; By the Rounding Theorem of [LST], we can find a non-preemptive schedule of length less than 4 b L in polynomial time. 2 Partially based on our techniques, Shmoys and Tardos <ref> [ShT] </ref> developed an O (log 2 K)-approximation algorithm for non-preemptive scheduling of unrelated parallel machines with precedence constraints on the jobs. 5 Conclusions Our results deal with a class of frequently encountered 0-1 optimization problems whose *-approximation problems are N P-hard. <p> Practical use of our algorithms for vector quantization, clustering, and memory-based learning can be found in [LiVa, LiVb]. Acknowledgements. The application in Section 4.2 was communicated to us by Shmoys and Tardos <ref> [ShT] </ref>. We thank them for their permission to include the result here.
References-found: 29

