URL: http://www.cs.msu.edu/~guentche/AAAI98S.ps
Refering-URL: http://www.cs.msu.edu/~guentche/project.html
Root-URL: http://www.cs.msu.edu
Email: guentche@cps.msu.edu weng@cps.msu.edu  
Title: Learning-Based Three Dimensional Sound Localization Using a Compact Non-Coplanar Array of Microphones  
Author: Kamen Y. Guentchev and John J. Weng 
Address: 3115 Engineering Building  East Lansing, MI 48824  
Affiliation: Department of Computer Science  Michigan State University  
Abstract: One of the various human sensory capabilities is to identify the direction of perceived sounds. The goal of this work is to study sound source localization in three dimensions using some of the most important cues the human uses. Having robotics as a major application, the approach involves a compact sensor structure that can be placed on a mobile platform. The objective is to estimate the relative sound source position in three dimensional space without imposing excessive restrictions on its spatio-temporal characteristics and the environment structure. Two types of features are considered, interaural time and level differences. Their relative effectiveness for localization is studied, as well as a practical way of using these complementary parameters. A two-stage procedure was used. In the training stage, sound samples are produced from points with known coordinates and then are stored. In the recognition stage, unknown sounds are processed by the trained system to estimate the 3D location of the sound source. Results from the experiments showed under 3 ffi in average angular error and less than 20% in average radial distance error. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Albert, A. </author> <year> 1966. </year> <title> Solid Analytic Geometry. Phoenix books: </title> <publisher> University of Chicago Press. </publisher>
Reference-contexts: Depending on the sign of the difference, one of the sheets contains the source location. Then from equation (2), for the matching ILD, it is less obvious but it can be shown the locus is a sphere (Guentchev 1997) <ref> (Albert 1966) </ref> (Bell 1918) (Sommerville 1929). The intersection of these surfaces, defined by a number of detector couples, will determine the solution.
Reference: <author> Bell, R. </author> <year> 1918. </year> <title> Coordinate Geometry of three dimensions. </title> <publisher> London: Macmillan. </publisher>
Reference-contexts: Depending on the sign of the difference, one of the sheets contains the source location. Then from equation (2), for the matching ILD, it is less obvious but it can be shown the locus is a sphere (Guentchev 1997) (Albert 1966) <ref> (Bell 1918) </ref> (Sommerville 1929). The intersection of these surfaces, defined by a number of detector couples, will determine the solution. It is clear that, apart from some special cases, with three couples the intersection is two points located symmetrically on both sides of the plane passing through the three detectors.
Reference: <author> Blauert, J. </author> <year> 1969. </year> <title> Sound localization in the median plane. </title> <booktitle> Acustica 22 </booktitle> <pages> 205-213. </pages>
Reference-contexts: It is known that two of the most important cues used by humans are the interaural differences: in time and level (ITD, ILD) (MacCabe 1994) (Wight-man 1992) (Yost 1987). Other cues relate to the spectral variations caused by diffractions at the head and pinnae <ref> (Blauert 1969) </ref>. For sounds with longer duration, cognitive processes start playing an important role, including dynamic head adjustments, high-level reasoning, etc. (Yost 1987).
Reference: <author> Brandstein, M.S.; Adcock, J., and Silverman, H. </author> <year> 1995. </year> <title> A practical time-delay estimator for localizing speech sources with a microphone array. </title> <booktitle> Computer, Speech and Language 9 </booktitle> <pages> 153-169. </pages>
Reference-contexts: For example, a number of microphone subarrays have been placed on the walls with a goal to pick up the location of a speaker in a room (Brandstein 1997a) (Brandstein & Silverman 1997) <ref> (Brandstein & Silverman 1995) </ref> (Rabinkin 1996). In other studies a human model has been followed to some degree resulting in constraints in applicability and limited accuracy (Martin 1995).
Reference: <author> Brandstein, M.S.; Adcock, J., and Silverman, H. </author> <year> 1997. </year> <title> A closed-form location estimator for use with room environment microphone arrays. </title> <journal> IEEE Transactions on Speech and Audio Processing 5(1) </journal> <pages> 45-50. </pages>
Reference-contexts: For example, a number of microphone subarrays have been placed on the walls with a goal to pick up the location of a speaker in a room (Brandstein 1997a) <ref> (Brandstein & Silverman 1997) </ref> (Brandstein & Silverman 1995) (Rabinkin 1996). In other studies a human model has been followed to some degree resulting in constraints in applicability and limited accuracy (Martin 1995). <p> Most works determine the ITD and then use either an iterative search algorithm to minimize a certain objective function (Hobbs 1992) (Martin 1995) (Rabinkin 1996), or an approximation model for which a closed-form solution can be derived (Brand-stein 1997a) <ref> (Brandstein & Silverman 1997) </ref>. The former is relatively slow and thus, it may not reach real time speed. The latter introduces model errors and cannot use more feature types for better accuracy. <p> Theoretical problem and sensor structure Required by versatile applications such as the dimensional hearing of a mobile robot, we cannot use room-oriented solutions <ref> (Brandstein & Silverman 1997) </ref> (Ra-binkin 1996), which typically use a large intersensor distance, with all the sensors fixed in the room. In our case the sound source will necessarily be located outside of the sensor structure.
Reference: <author> Brandstein, M.S.; Silverman, H. </author> <year> 1997a. </year> <title> A practical methodology for speech source localization with microphone arrays. </title> <booktitle> Computer, Speech and Language 11(2) </booktitle> <pages> 91-126. </pages>
Reference-contexts: For example, a number of microphone subarrays have been placed on the walls with a goal to pick up the location of a speaker in a room <ref> (Brandstein 1997a) </ref> (Brandstein & Silverman 1997) (Brandstein & Silverman 1995) (Rabinkin 1996). In other studies a human model has been followed to some degree resulting in constraints in applicability and limited accuracy (Martin 1995). <p> Numerous studies report the ITD as the main cue in human dimensional hearing (Wightman 1992). The clear geometrical representation of the problem makes it the favorite feature to be used when approaching such a task by a machine setup (Brandstein 1997b) <ref> (Brandstein 1997a) </ref> (Brand-stein & Silverman 1995) (Bub & Weibel 1995) (Chan & Plant 1978) (Ianiello 1982) (Knapp 1976) (Rabinkin 1996).
Reference: <author> Brandstein, M. </author> <year> 1997b. </year> <title> A pitch based approach to time-delay estimation of reverberant speech. </title> <booktitle> In Proc. 1997 Workshop on Applications of Signal Processing to Audio and Acoustics, </booktitle> <address> New Paltz, New York, Octo-ber 19-22, </address> <year> 1997. </year>
Reference-contexts: Numerous studies report the ITD as the main cue in human dimensional hearing (Wightman 1992). The clear geometrical representation of the problem makes it the favorite feature to be used when approaching such a task by a machine setup <ref> (Brandstein 1997b) </ref> (Brandstein 1997a) (Brand-stein & Silverman 1995) (Bub & Weibel 1995) (Chan & Plant 1978) (Ianiello 1982) (Knapp 1976) (Rabinkin 1996).
Reference: <author> Bub, U.; Hunke, M., and Weibel, A. </author> <year> 1995. </year> <title> Knowing who to listen to in speech recognition: visually guided beamforming. </title> <booktitle> In Proceedings of the 1995 ICASSP, </booktitle> <address> Detroit, MI. </address>
Reference-contexts: A significant amount of work has been devoted to devices with a limited functionality (e.g. constrained to localization in a single half-plane while still using large sensor structures) <ref> (Bub & Weibel 1995) </ref> (Rabinkin 1996) or the help of a non-acoustical modality has been used (e.g. vision)(Bub & Weibel 1995). <p> Numerous studies report the ITD as the main cue in human dimensional hearing (Wightman 1992). The clear geometrical representation of the problem makes it the favorite feature to be used when approaching such a task by a machine setup (Brandstein 1997b) (Brandstein 1997a) (Brand-stein & Silverman 1995) <ref> (Bub & Weibel 1995) </ref> (Chan & Plant 1978) (Ianiello 1982) (Knapp 1976) (Rabinkin 1996). <p> The next phase involves the use of a cross-correlation procedure to determine the shift between the sampled signals at each of the sensor couples. This gives a direct measure for the ITD <ref> (Bub & Weibel 1995) </ref>. We find the peak of the cross-correlation function varying across a range of possible time-delays (3).
Reference: <author> Capel, V. </author> <year> 1978. </year> <title> Microphones in action. </title> <address> Hertfordshire, England: </address> <publisher> Fountain Press, Argus Books Ltd. </publisher>
Reference-contexts: It is known that the location of the source can be established by detecting the front of the propagating wave and computing the center of the sphere <ref> (Capel 1978) </ref> (Carr 1966). Unfortunately acoustical waves are not clearly distinguishable objects and such a task is not trivial in real environments even if real-life sources could be approximated by points (MacCabe 1994).
Reference: <author> Carr, H. </author> <year> 1966. </year> <title> An introduction to space perception. </title> <address> New York: Hafner. </address>
Reference-contexts: It is known that the location of the source can be established by detecting the front of the propagating wave and computing the center of the sphere (Capel 1978) <ref> (Carr 1966) </ref>. Unfortunately acoustical waves are not clearly distinguishable objects and such a task is not trivial in real environments even if real-life sources could be approximated by points (MacCabe 1994). Numerous studies have attempted to determine the mechanisms used by humans to achieve dimensional hearing (Carr 1966) 1 Copyright 1998, <p> the sphere (Capel 1978) <ref> (Carr 1966) </ref>. Unfortunately acoustical waves are not clearly distinguishable objects and such a task is not trivial in real environments even if real-life sources could be approximated by points (MacCabe 1994). Numerous studies have attempted to determine the mechanisms used by humans to achieve dimensional hearing (Carr 1966) 1 Copyright 1998, American Association for Artificial Intelligence (www.aaai.org). All rights reserved. (Hartmann 1990) (Hartmann 1989). Most phenomena have been reasonably explained in principle, although many aspects of human dimensional hearing need further study.
Reference: <author> Champagne, B.; Bedard, S., and Stephenne, A. </author> <year> 1996. </year> <title> Performance of time-delay estimation in the presence of room reverberation. </title> <journal> IEEE Transactions on Speech and Audio Processing 4(2) </journal> <pages> 148-152. </pages>
Reference-contexts: We should note a very useful side effect, which is closely related to the precedence effect in human auditory perception (Hartmann 1990) (Hartmann 1989). In case there are reflections of the incoming sound from incident surfaces (echoes, reverberation <ref> (Champagne & Stephenne 1996) </ref>) a secondary maximum will appear in the correlation curve. However, with the presented approach, it will be ignored because the coherency of that signal will be significantly lower and thus it will correlate less well (lower and wider peak).
Reference: <author> Chan, Y.; Hattin, R., and Plant, J. </author> <year> 1978. </year> <title> The least squares estimation of time delay and its use in signal detection. </title> <journal> IEEE Trans. Acoust., Speech, Signal Processing 26(3) </journal> <pages> 217-222. </pages>
Reference-contexts: The clear geometrical representation of the problem makes it the favorite feature to be used when approaching such a task by a machine setup (Brandstein 1997b) (Brandstein 1997a) (Brand-stein & Silverman 1995) (Bub & Weibel 1995) <ref> (Chan & Plant 1978) </ref> (Ianiello 1982) (Knapp 1976) (Rabinkin 1996).
Reference: <author> Guentchev, K. </author> <year> 1997. </year> <title> Learning-based three dimensional sound localization using a compact noncoplanar array of microphones. </title> <type> Master's thesis, </type> <institution> Dept.of Computer Science, Michigan State University. </institution>
Reference-contexts: Depending on the sign of the difference, one of the sheets contains the source location. Then from equation (2), for the matching ILD, it is less obvious but it can be shown the locus is a sphere <ref> (Guentchev 1997) </ref> (Albert 1966) (Bell 1918) (Sommerville 1929). The intersection of these surfaces, defined by a number of detector couples, will determine the solution. <p> In the experiment, an equal-side tetrahedron with a 20cm side was used. Methodology As outlined above, efficient sound localization can be performed after having extracted the necessary differential features. It can be shown <ref> (Guentchev 1997) </ref> that the minimum number of detectors required to obtain unambiguously a solution in three dimensional space is four and that it is unique. In order to fully solve the problem of three dimensional sound localization two main steps need to be performed.
Reference: <author> Hartmann, W.M.; Rakerd, B. </author> <year> 1989. </year> <title> Localization of Sound in Rooms IV: The Franssen Effect. </title> <journal> J. Acoust. Soc. Am. </journal> <volume> 86(4) </volume> <pages> 1366-1373. </pages>
Reference-contexts: Numerous studies have attempted to determine the mechanisms used by humans to achieve dimensional hearing (Carr 1966) 1 Copyright 1998, American Association for Artificial Intelligence (www.aaai.org). All rights reserved. (Hartmann 1990) <ref> (Hartmann 1989) </ref>. Most phenomena have been reasonably explained in principle, although many aspects of human dimensional hearing need further study. It is known that two of the most important cues used by humans are the interaural differences: in time and level (ITD, ILD) (MacCabe 1994) (Wight-man 1992) (Yost 1987). <p> The precision depends on various characteristics of the perceived sound: spectral contents, envelope variability as a function of time, volume level, reverberation and echo, etc. It can be disappointingly low and in some instances totally inconclusive (Hart-mann 1990). Sometimes it can be convincingly wrong (e.g. Franssen effect) <ref> (Hartmann 1989) </ref>. One major difference between human and engineering setup is the number of sensors available. Most authors distinguish a single parameter as the most significant factor for dimensional sound localization. It is the interaural time difference (ITD) of the sound as perceived by two sensors. <p> We should note a very useful side effect, which is closely related to the precedence effect in human auditory perception (Hartmann 1990) <ref> (Hartmann 1989) </ref>. In case there are reflections of the incoming sound from incident surfaces (echoes, reverberation (Champagne & Stephenne 1996)) a secondary maximum will appear in the correlation curve.
Reference: <author> Hartmann, W. </author> <year> 1990. </year> <title> Localization of a source of sound in a room. </title> <booktitle> In Proc. AES 8th International Conference, </booktitle> <pages> 27-32. </pages>
Reference-contexts: Numerous studies have attempted to determine the mechanisms used by humans to achieve dimensional hearing (Carr 1966) 1 Copyright 1998, American Association for Artificial Intelligence (www.aaai.org). All rights reserved. <ref> (Hartmann 1990) </ref> (Hartmann 1989). Most phenomena have been reasonably explained in principle, although many aspects of human dimensional hearing need further study. <p> We should note a very useful side effect, which is closely related to the precedence effect in human auditory perception <ref> (Hartmann 1990) </ref> (Hartmann 1989). In case there are reflections of the incoming sound from incident surfaces (echoes, reverberation (Champagne & Stephenne 1996)) a secondary maximum will appear in the correlation curve.
Reference: <author> Hobbs, S. </author> <year> 1992. </year> <title> Asymptotic statistics for location estimates of acoustic signals. </title> <journal> J. Acoust. Soc. Am. </journal> <volume> 91(3) </volume> <pages> 1538-1544. </pages>
Reference-contexts: Finally the processing of the extracted features is one of the dominating factors for the success of a localiza tion procedure. Most works determine the ITD and then use either an iterative search algorithm to minimize a certain objective function <ref> (Hobbs 1992) </ref> (Martin 1995) (Rabinkin 1996), or an approximation model for which a closed-form solution can be derived (Brand-stein 1997a) (Brandstein & Silverman 1997). The former is relatively slow and thus, it may not reach real time speed.
Reference: <author> Ianiello, J. </author> <year> 1982. </year> <title> Time delay estimation via cross-correlation in the presence of large estimation errors. </title> <journal> IEEE Trans. Acoust., Speech, Signal Processing 30(6) </journal> <pages> 998-1003. </pages>
Reference-contexts: The clear geometrical representation of the problem makes it the favorite feature to be used when approaching such a task by a machine setup (Brandstein 1997b) (Brandstein 1997a) (Brand-stein & Silverman 1995) (Bub & Weibel 1995) (Chan & Plant 1978) <ref> (Ianiello 1982) </ref> (Knapp 1976) (Rabinkin 1996).
Reference: <author> Knapp, C.; Carter, C. </author> <year> 1976. </year> <title> The generalized correlation method for estimation of time delay. </title> <journal> IEEE Trans. Acoust., Speech, Signal Processing 24(4) </journal> <pages> 320-327. </pages>
Reference-contexts: The clear geometrical representation of the problem makes it the favorite feature to be used when approaching such a task by a machine setup (Brandstein 1997b) (Brandstein 1997a) (Brand-stein & Silverman 1995) (Bub & Weibel 1995) (Chan & Plant 1978) (Ianiello 1982) <ref> (Knapp 1976) </ref> (Rabinkin 1996).
Reference: <author> MacCabe, C.J.; Furlong, D. </author> <year> 1994. </year> <title> Virtual imaging capabilities of surround sound systems. </title> <journal> J. Audio Eng. Soc. </journal> 42(1/2):38-48. 
Reference-contexts: Unfortunately acoustical waves are not clearly distinguishable objects and such a task is not trivial in real environments even if real-life sources could be approximated by points <ref> (MacCabe 1994) </ref>. Numerous studies have attempted to determine the mechanisms used by humans to achieve dimensional hearing (Carr 1966) 1 Copyright 1998, American Association for Artificial Intelligence (www.aaai.org). All rights reserved. (Hartmann 1990) (Hartmann 1989). <p> All rights reserved. (Hartmann 1990) (Hartmann 1989). Most phenomena have been reasonably explained in principle, although many aspects of human dimensional hearing need further study. It is known that two of the most important cues used by humans are the interaural differences: in time and level (ITD, ILD) <ref> (MacCabe 1994) </ref> (Wight-man 1992) (Yost 1987). Other cues relate to the spectral variations caused by diffractions at the head and pinnae (Blauert 1969). For sounds with longer duration, cognitive processes start playing an important role, including dynamic head adjustments, high-level reasoning, etc. (Yost 1987). <p> Another cue known to have notable importance in human dimensional hearing is the interaural level differences (ILD). Surprisingly ILD have seldom been used in actual system implementations because they are believed to have unfavorable frequency dependence and unreliability <ref> (MacCabe 1994) </ref> (Martin 1995). Another reason is the lack of an explicit and stable relationship between ILD and source location which will allow for a simple algorithmic solution to be derived (MacCabe 1994). <p> have seldom been used in actual system implementations because they are believed to have unfavorable frequency dependence and unreliability <ref> (MacCabe 1994) </ref> (Martin 1995). Another reason is the lack of an explicit and stable relationship between ILD and source location which will allow for a simple algorithmic solution to be derived (MacCabe 1994). The learning approach used in this study does not have such limitations and it benefits from the added cues. Finally the processing of the extracted features is one of the dominating factors for the success of a localiza tion procedure.
Reference: <author> Martin, K. </author> <year> 1995. </year> <title> Estimating azimuth and elevation from interaural differences. </title> <booktitle> In 1995 IEEE Mo-honk workshop on Applications of Signal Processing to Acoustics and Audio. </booktitle>
Reference-contexts: In other studies a human model has been followed to some degree resulting in constraints in applicability and limited accuracy <ref> (Martin 1995) </ref>. <p> Another cue known to have notable importance in human dimensional hearing is the interaural level differences (ILD). Surprisingly ILD have seldom been used in actual system implementations because they are believed to have unfavorable frequency dependence and unreliability (MacCabe 1994) <ref> (Martin 1995) </ref>. Another reason is the lack of an explicit and stable relationship between ILD and source location which will allow for a simple algorithmic solution to be derived (MacCabe 1994). The learning approach used in this study does not have such limitations and it benefits from the added cues. <p> Finally the processing of the extracted features is one of the dominating factors for the success of a localiza tion procedure. Most works determine the ITD and then use either an iterative search algorithm to minimize a certain objective function (Hobbs 1992) <ref> (Martin 1995) </ref> (Rabinkin 1996), or an approximation model for which a closed-form solution can be derived (Brand-stein 1997a) (Brandstein & Silverman 1997). The former is relatively slow and thus, it may not reach real time speed. The latter introduces model errors and cannot use more feature types for better accuracy.
Reference: <author> Rabinkin, D. e. a. </author> <year> 1996. </year> <title> A DSP Implementation of Source Location Using Microphone Arrays. </title> <booktitle> In 131st meeting of the Acoustical Society of America, </booktitle> <address> Indi-anapolis, Indiana, </address> <month> 15 May </month> <year> 1996. </year>
Reference-contexts: For example, a number of microphone subarrays have been placed on the walls with a goal to pick up the location of a speaker in a room (Brandstein 1997a) (Brandstein & Silverman 1997) (Brandstein & Silverman 1995) <ref> (Rabinkin 1996) </ref>. In other studies a human model has been followed to some degree resulting in constraints in applicability and limited accuracy (Martin 1995). <p> A significant amount of work has been devoted to devices with a limited functionality (e.g. constrained to localization in a single half-plane while still using large sensor structures) (Bub & Weibel 1995) <ref> (Rabinkin 1996) </ref> or the help of a non-acoustical modality has been used (e.g. vision)(Bub & Weibel 1995). <p> The clear geometrical representation of the problem makes it the favorite feature to be used when approaching such a task by a machine setup (Brandstein 1997b) (Brandstein 1997a) (Brand-stein & Silverman 1995) (Bub & Weibel 1995) (Chan & Plant 1978) (Ianiello 1982) (Knapp 1976) <ref> (Rabinkin 1996) </ref>. <p> Finally the processing of the extracted features is one of the dominating factors for the success of a localiza tion procedure. Most works determine the ITD and then use either an iterative search algorithm to minimize a certain objective function (Hobbs 1992) (Martin 1995) <ref> (Rabinkin 1996) </ref>, or an approximation model for which a closed-form solution can be derived (Brand-stein 1997a) (Brandstein & Silverman 1997). The former is relatively slow and thus, it may not reach real time speed. The latter introduces model errors and cannot use more feature types for better accuracy.
Reference: <author> Sommerville, D. </author> <year> 1929. </year> <institution> Analytical Conics. London: Bell. </institution>
Reference-contexts: Depending on the sign of the difference, one of the sheets contains the source location. Then from equation (2), for the matching ILD, it is less obvious but it can be shown the locus is a sphere (Guentchev 1997) (Albert 1966) (Bell 1918) <ref> (Sommerville 1929) </ref>. The intersection of these surfaces, defined by a number of detector couples, will determine the solution. It is clear that, apart from some special cases, with three couples the intersection is two points located symmetrically on both sides of the plane passing through the three detectors.
Reference: <author> Weng, J. </author> <year> 1996a. </year> <title> Cresceptron and SHOSLIF: Toward comprehensive visual learning. </title> <editor> In Nayar, S.K.; Pog-gio, T., ed., </editor> <title> Early Visual Learning. </title> <address> New York: </address> <publisher> Oxford University Press. </publisher> <pages> 183-214. </pages>
Reference-contexts: The obtained results were used to evaluate the correctness and the performance of the SHOSLIF procedure. SHOSLIF achieves a high speed of retrieval due to its logarithmic time complexity O (log (n)), where n is the number of cases learned and stored as necessary <ref> (Weng 1996a) </ref> (Weng 1996b). It was found that the results produced by SHOSLIF had identical precision with that of the linear search, while its performance time was nearly 5 times faster. <p> We should also note the extreme complexity of the actual mapping function. It is appropriate then to use learning as an efficient way of approximating complex high-dimensional functions <ref> (Weng 1996a) </ref>. Being able to explicitly model the function would significanlty increase the accuracy of prediction. However, in the current case, when considering all the presented variables, it is very difficult to establish the form of such a model. <p> The goal of the SHOSLIF recursive partition tree (RPT) is to find the top k nearest neighbors in O (log (n)) time, where n is the number of training samples learned and stored. The SHOSLIF scheme <ref> (Weng 1996a) </ref> (Weng 1996b) automatically generates the RPT, which recursively partitions the input space X into smaller and smaller cells. Each leaf node contains exactly one training sample (x i ; y i ), where x i *X and y i *Y .
Reference: <author> Weng, J.J.; Chen, S. </author> <year> 1996b. </year> <title> Incremental learning for vision-based navigation. </title> <booktitle> In Proc. International Conference on Pattern Recognition, </booktitle> <address> Vienna, Austria, </address> <month> Aug. </month> <journal> 1996, </journal> <volume> volume 4, </volume> <pages> 45-49. </pages>
Reference-contexts: The obtained results were used to evaluate the correctness and the performance of the SHOSLIF procedure. SHOSLIF achieves a high speed of retrieval due to its logarithmic time complexity O (log (n)), where n is the number of cases learned and stored as necessary (Weng 1996a) <ref> (Weng 1996b) </ref>. It was found that the results produced by SHOSLIF had identical precision with that of the linear search, while its performance time was nearly 5 times faster. <p> The goal of the SHOSLIF recursive partition tree (RPT) is to find the top k nearest neighbors in O (log (n)) time, where n is the number of training samples learned and stored. The SHOSLIF scheme (Weng 1996a) <ref> (Weng 1996b) </ref> automatically generates the RPT, which recursively partitions the input space X into smaller and smaller cells. Each leaf node contains exactly one training sample (x i ; y i ), where x i *X and y i *Y .
Reference: <author> Wightman, F.L.; Kistler, D. </author> <year> 1992. </year> <title> The dominant role of low-frequency interaural time differences in sound localization. </title> <journal> J. Acoust. Soc. Am. </journal> <volume> 91(3) </volume> <pages> 1648-1661. </pages>
Reference-contexts: Most authors distinguish a single parameter as the most significant factor for dimensional sound localization. It is the interaural time difference (ITD) of the sound as perceived by two sensors. Numerous studies report the ITD as the main cue in human dimensional hearing <ref> (Wightman 1992) </ref>. The clear geometrical representation of the problem makes it the favorite feature to be used when approaching such a task by a machine setup (Brandstein 1997b) (Brandstein 1997a) (Brand-stein & Silverman 1995) (Bub & Weibel 1995) (Chan & Plant 1978) (Ianiello 1982) (Knapp 1976) (Rabinkin 1996).
Reference: <author> Yost, W.A.; Gourevitch, G. </author> <year> 1987. </year> <title> Directional hearing. </title> <address> New York: </address> <publisher> Springer-Verlag. </publisher>
Reference-contexts: Most phenomena have been reasonably explained in principle, although many aspects of human dimensional hearing need further study. It is known that two of the most important cues used by humans are the interaural differences: in time and level (ITD, ILD) (MacCabe 1994) (Wight-man 1992) <ref> (Yost 1987) </ref>. Other cues relate to the spectral variations caused by diffractions at the head and pinnae (Blauert 1969). For sounds with longer duration, cognitive processes start playing an important role, including dynamic head adjustments, high-level reasoning, etc. (Yost 1987). <p> interaural differences: in time and level (ITD, ILD) (MacCabe 1994) (Wight-man 1992) <ref> (Yost 1987) </ref>. Other cues relate to the spectral variations caused by diffractions at the head and pinnae (Blauert 1969). For sounds with longer duration, cognitive processes start playing an important role, including dynamic head adjustments, high-level reasoning, etc. (Yost 1987). The problem of sound localization by machine Sound localization can be used in many different applications: robot hearing, human-machine interfaces, monitoring devices, handicappers' aids, etc, where other means fail for different reasons.
References-found: 26

