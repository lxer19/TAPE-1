URL: http://www.cs.wisc.edu/~fischer/ftp/pub/tech-reports/ncstrl.uwmadison/CS-TR-91-996/CS-TR-91-996.ps.Z
Refering-URL: http://www.cs.wisc.edu/~fischer/ftp/pub/tech-reports/ncstrl.uwmadison/CS-TR-91-996/
Root-URL: http://www.cs.wisc.edu
Email: hollings@cs.wisc.edu rbi@cs.wisc.edu bart@cs.wisc.edu  
Title: The Integration of Application and System Based Metrics in a Parallel Program Performance Tool  
Author: Jeffrey K. Hollingsworth R. Bruce Irvin Barton P. Miller 
Note: Research supported in part by National Science Foundation grant CCR-8815928, Office of Naval Research grant N00014-89-J-1222, a grant from Sequent Computer Systems Inc., and a Digital Equipment Corporation External Research Grant. To appear in Proceedings of the 1991 ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming.  
Address: 1210 W. Dayton Street Madison, Wisconsin 53706  
Affiliation: Computer Sciences Department University of Wisconsin-Madison  
Abstract: The IPS-2 parallel program measurement tools provide performance data from application programs, the operating system, hardware, network, and other sources. Previous versions of IPS-2 allowed programmers to collect information about an application based only on what could be collected by software instrumentation inserted into the program (and system call libraries). We have developed an open interface, called the ``external time histogram'', providing a graceful way to include external data from many sources. The user can tell IPS-2 of new sources of performance data through an extensible metric description language. The data from these external sources is automatically collected when the application program is run. IPS-2 provides a library to simplify constructing the external data collectors. The new version of IPS-2 can measure shared-memory and message-passing parallel programs running on a heterogeneous collection of machines. Data from C or Fortran programs, and data from simulations can be processed by the same tool. As a result of including the new external performance data, IPS-2 now can report on a whole new set of performance problems. We describe the results of using IPS-2 on two real applications: a shared-memory database join utility, and a multi-processor interconnection network simulator. Even though these applications previously went through careful tuning, we were able to precisely identify performance problems and extract additional performance improvements of about 30%. hhhhhhhhhhhhhhhhhh
Abstract-found: 1
Intro-found: 1
Reference: 1. <author> B. P. Miller, M. Clark, J. Hollingsworth, S. Kierstead, S. Lim and T. Torzewski, "IPS-2: </author> <title> The Second Generation of a Parallel Program Measurement System", </title> <journal> IEEE Transactions on Parallel and Distributed Systems 1, </journal> <month> 2 (April </month> <year> 1990), </year> <pages> pp. 206-217. </pages>
Reference-contexts: 1. Introduction A performance tool should provide as complete a picture as possible of a program's execution. The IPS-2 parallel program performance tools <ref> [1, 2] </ref> allow a programmer to monitor and analyze the performance of application programs running in both shared-memory and message-passing environments. The performance of an application program can be studied at varying levels of detail, from the whole program down to an individual procedure or synchronization variable.
Reference: 2. <author> J. Hollingsworth, B. P. Miller and R. B. Irvin, </author> <note> "IPS User's Guide", Computer Sciences Technical Report, </note> <month> December </month> <year> 1989. </year>
Reference-contexts: 1. Introduction A performance tool should provide as complete a picture as possible of a program's execution. The IPS-2 parallel program performance tools <ref> [1, 2] </ref> allow a programmer to monitor and analyze the performance of application programs running in both shared-memory and message-passing environments. The performance of an application program can be studied at varying levels of detail, from the whole program down to an individual procedure or synchronization variable.
Reference: 3. <author> T. E. Anderson and E. D. Lazowska, "Quartz: </author> <title> A Tool for Tuning Parallel Program Performance", </title> <booktitle> Proc. of the 1990 SIGMETRICS Conference on Measurement and Modeling of Computer Systems, </booktitle> <address> Boston, </address> <month> May </month> <year> 1990, </year> <pages> pp. 115-125. </pages> - -- 
Reference-contexts: The performance metrics are available in several types of displays: tabular summary, time histograms, Critical Path profiles, and sorted profile tables. In addition, an experimental version of IPS-2 exists that provides an implementation of the Quartz Normalized Processor Time metric <ref> [3] </ref>. The simplest kind of metric in IPS-2 is a table metric. A table metric has a single value for the entire length of the program's execution.
Reference: 4. <author> C. Yang and B. P. Miller, </author> <title> "Critical Path Analysis for the Execution of Parallel and Distributed Programs", </title> <booktitle> 8th Int'l Conf. on Distributed Computing Systems, </booktitle> <address> San Jose, Calif., </address> <month> June </month> <year> 1988, </year> <pages> pp. 366-375. </pages>
Reference-contexts: Several histograms can be plotted on the same axis. This permits correlation of different metrics. Like tables, histograms are available at any level in the program tree hierarchy. Critical Path analysis is based on identifying the path through the program's execution that consumed the most time <ref> [4] </ref>. This technique identifies the part of the program that was responsible for its length of execution. To calculate a program's Critical Path, we build a graph of the program's execution.
Reference: 5. <author> S. L. Graham, P. B. Kessler and M. K. McKusick, </author> <title> "gprof: a Call Graph Execution Profiler", </title> <booktitle> Proc. of the 1982 SIGPLAN Symposium on Compiler Construction, </booktitle> <address> Boston, </address> <month> June </month> <year> 1982, </year> <pages> pp. 120-126. </pages>
Reference-contexts: This only provides an approximation of the new run time of the program, but it does help to assess the impact of improving sections of code. Profile tables are similar to the type of information presented by the standard UNIX profiling tool Gprof <ref> [5] </ref>. At the procedure level, profile tables list procedures sorted by the amount of CPU time used. In addition, profile tables list the time each procedure spent in synchronization waits. At higher levels, profile tables summarize the information from the lower levels.
Reference: 6. <institution> Symmetry Technical Summary, Sequent Computer Systems, Inc., </institution> <year> 1988. </year>
Reference-contexts: We anticipate a core set of data collectors that will become standard, and will be provided with the IPS-2 tools. Additional collectors will be created by users for more specialized uses. The Sequent Symmetry <ref> [6] </ref> is a shared memory multi-processor. Each processor has a set-associative cache, and connects to main memory via the system bus. The hardware data collector (``hwEDCU'') collects metrics from special hardware monitors in the Sequent Symmetry, including utilization of the system bus, and cache miss information for each processor.
Reference: 7. <author> D. DeWitt and R. Gerber, </author> <title> "Multiprocessor Hash-Based Join Algorithms", </title> <booktitle> Proc. of the 1985 VLDB Conference, </booktitle> <address> Stockholm, Sweden, </address> <month> August </month> <year> 1985, </year> <pages> pp. 151-164. </pages>
Reference-contexts: The values presented in the percent improvement tables are based on programs compiled without IPS-2 instrumentation and with the compiler's optimizer enabled. 6.1. Shared Memory Join Program The shared memory join application (shm_join) is an implementation of the join function for a relational database. It implements a hash-join algorithm <ref> [7] </ref> using shared memory for inter-process communication. The program was written to study shared-memory and shared-nothing join algorithms and, as a result, contains a large number of tunable parameters that effect the algorithm's performance.
Reference: 8. <author> B. Bitton, D. DeWitt and C. Turbyfill, </author> <title> "Benchmarking Database Systems- A Systematic Approach", </title> <booktitle> Proc. of the 1983 VLDB Conference, </booktitle> <address> Florance, Italy, </address> <month> October </month> <year> 1983, </year> <pages> pp. 8-19. </pages>
Reference-contexts: For the purpose of our performance study, we used a fixed set of parameters and attempted to improve the algorithm's implementation. IPS-2 can also be used to tune the performance of the program by adjusting these parameters. The test data consisted of the Wisconsin Benchmark Join ABPrime <ref> [8] </ref> query with a 50,000 tuple inner and a 50,000 tuple outer relation. the shm_join application running on a four processor Histogram of the initial version of the shared memory join application showing built-in (CPU time) and external metrics. Sequent Symmetry.
Reference: 9. <author> E. D. Brooks III, </author> <title> "The indirect k-ary n-cube for a vector processing environment", </title> <booktitle> Parallel Computing 6, 3 (1988), </booktitle> <pages> pp. 339-348. </pages>
Reference-contexts: Network Simulator The network simulator application (psim) simulates an indirect k-ary n-cube processor-memory interconnection network. The program has been used previously to evaluate the performance of such networks <ref> [9] </ref> and to study cache coherence protocols for shared memory multi-processors [10]. Psim is an integer intensive application that parallel-izes well, exhibiting nearly linear speedup for 20 processors. The network simulated by psim consists of a request sub-network and a response sub-network.
Reference: 10. <author> S. S. Thakkar, </author> <title> Performance of Parallel Applications on a Shared-Memory Multiprocessor System, in Performance Instrumentation and Visualization, </title> <editor> M. Simmons and R. Koskela (ed.), </editor> <publisher> ACM Press, </publisher> <year> 1990, </year> <pages> 233-256. </pages>
Reference-contexts: Network Simulator The network simulator application (psim) simulates an indirect k-ary n-cube processor-memory interconnection network. The program has been used previously to evaluate the performance of such networks [9] and to study cache coherence protocols for shared memory multi-processors <ref> [10] </ref>. Psim is an integer intensive application that parallel-izes well, exhibiting nearly linear speedup for 20 processors. The network simulated by psim consists of a request sub-network and a response sub-network. The program allocates half of the available processors to each subnetwork.
References-found: 10

