URL: http://www.cs.duke.edu/~mlittman/docs/pomdp-ai2.ps
Refering-URL: http://www.cs.duke.edu/~mlittman/topics/pomdp-page.html
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Title: Planning and Acting in Partially Observable Stochastic Domains  
Author: Leslie Pack Kaelbling Michael L. Littman Anthony R. Cassandra 
Note: This work was supported in part by NSF grants IRI-9453383 and IRI-9312395. This work was supported in part by Bellcore.  
Date: January 13, 1997  
Abstract: In this paper, we bring techniques from operations research to bear on the problem of choosing optimal actions in partially observable stochastic domains. We begin by introducing the theory of Markov decision processes (mdps) and partially observable mdps (pomdps). We then outline a novel algorithm for solving pomdps off line and show how, in some cases, a finite-memory controller can be extracted from the solution to a pomdp. We conclude with a discussion of how our approach relates to previous work, the complexity of finding exact solutions to pomdps, and of some possibilities for finding approximate solutions. Consider the problem of a robot navigating in a large office building. The robot can move from hallway intersection to intersection and can make local observations of its world. Its actions are not completely reliable, however. Sometimes, when it intends to move, it stays where it is or goes too far; sometimes, when it intends to turn, it overshoots. It has similar problems with observation. Sometimes a corridor looks like a corner; sometimes a T-junction looks like an L-junction. How can such an error-plagued robot navigate, even given a map of the corridors? In general, the robot will have to remember something about its history of actions and observations and use this information, together with its knowledge of the underlying dynamics of the world (the map and other information), to maintain an estimate of its location. Many engineering applications follow this approach, using methods like the Kalman filter [18] to maintain a running estimate of the robot's spatial uncertainty, expressed as an ellipsoid or normal distribution in Cartesian space. This approach will not do for our robot, though. Its uncertainty may be discrete: it might be almost certain that it is in the north-east corner of either the fourth or the seventh floors, though it admits a chance that it is on the fifth floor, as well. Then, given an uncertain estimate of its location, the robot has to decide what actions to take. In some cases, it might be sufficient to ignore its uncertainty and take actions that would be appropriate for the most likely location. In other cases, it might be better for 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> K. J. Astrom. </author> <title> Optimal control of Markov decision processes with incomplete state estimation. </title> <journal> Journal of Mathematical Analysis and Applications, </journal> <volume> 10 </volume> <pages> 174-205, </pages> <year> 1965. </year>
Reference-contexts: This belief mdp is such that an optimal policy for it, coupled with the correct state estimator, will give rise to optimal behavior (in the discounted infinite-horizon sense) for the original pomdp <ref> [50, 1] </ref>. The remaining problem, then, is to solve this mdp.
Reference: [2] <author> Fahiem Bacchus, Craig Boutilier, and Adam Grove. </author> <title> Rewarding behaviors. </title> <booktitle> In Proceedings of the Thirteenth National Conference on Artificial Intelligence, </booktitle> <pages> pages 1160-1167. </pages> <publisher> AAAI Press/The MIT Press, </publisher> <year> 1996. </year>
Reference-contexts: Haddawy et al. [15] looked at a broad family of decision-theoretic objectives that make it possible to specify trade-offs between partially satisfying goals quickly and satisfying them completely. Bacchus, Boutilier, and Grove <ref> [2] </ref> show how some richer objectives based on evaluations of sequences of actions can actually be converted to total-reward problems.
Reference: [3] <author> Dimitri P. Bertsekas. </author> <title> Dynamic Programming and Optimal Control. </title> <publisher> Athena Scientific, </publisher> <address> Belmont, Massachusetts, </address> <year> 1995. </year> <note> Volumes 1 and 2. </note>
Reference-contexts: Markov decision processes are described in depth in a variety of texts <ref> [3, 40] </ref>; we will just briefly cover the necessary background. 2.1 Basic Framework A Markov decision process can be described as a tuple hS; A; T; Ri, where * S is a finite set of states of the world; * A is a finite set of actions; * T : S
Reference: [4] <author> Avrim Blum and Merrick Furst. </author> <title> Fast planning through planning graph analysis. </title> <booktitle> In Proceedings of the 14th International Joint Conference on Artificial Intelligence (IJCAI), </booktitle> <pages> pages 1636-1642, </pages> <month> August </month> <year> 1995. </year>
Reference-contexts: Traditional plans are simple sequences of actions. They are sufficient when the initial state is known and all actions are deterministic. A slightly more elaborate structure is the partially ordered plan (generated, for example, by snlp and ucpop), or the parallel plan <ref> [4] </ref>. In this type of plan, actions can be left unordered if all orderings are equivalent. When actions are stochastic, partially ordered plans can still be used (as in Buridan), but contingent plans can be more effective.
Reference: [5] <author> Jim Blythe. </author> <title> Planning with external events. </title> <booktitle> In Proceedings of the Tenth Conference on Uncertainty in Artificial Intelligence, </booktitle> <pages> pages 94-101, </pages> <year> 1994. </year>
Reference-contexts: This type of action model is used in mdps and pomdps as well as in Buridan and C-Buridan. Other work <ref> [5, 13] </ref> has used representations that can be used to compute probability distributions over future states. 6.4 Observation Model When the starting state is known and actions are deterministic, there is no need to get feedback from the environment when executing a plan.
Reference: [6] <author> Craig Boutilier and David Poole. </author> <title> Computing optimal policies for partially observable decision processes using compact representations. </title> <booktitle> In Proceedings of the Thirteenth National Conference on Artificial Intelligence, </booktitle> <pages> pages 1168-1175. </pages> <publisher> AAAI Press/The MIT Press, </publisher> <year> 1996. </year>
Reference-contexts: However, this work has served as a substrate for development of more complex and efficient representations <ref> [6] </ref>. Section 6 describes the relation between the present approach and prior research in more detail. One important facet of the pomdp approach is that there is no distinction drawn between actions taken to change the state of the world and actions taken to gain information. <p> The main advantage comes from their compactness|combined with operator schemata, which can represent many individual actions in a single operator, propositional representations can be exponentially more concise than a fully expanded state-based transition matrix for an mdp. Algorithms for manipulating compact (or factored) pomdps have begun to appear <ref> [12, 6] </ref>|this is a promising area for future research. At present, however, there is no evidence that these algorithms result in improved planning time significantly over the use of a "flat" representation of the state space. 6.7 Plan Structures Planning systems differ in the structure of the plans they produce.
Reference: [7] <author> Anthony R. Cassandra, Leslie Pack Kaelbling, and Michael L. Littman. </author> <title> Acting optimally in partially observable stochastic domains. </title> <booktitle> In Proceedings of the Twelfth National Conference on Artificial Intelligence, </booktitle> <pages> pages 1023-1028, </pages> <address> Seattle, WA, </address> <year> 1994. </year>
Reference-contexts: Other examples are explored in an earlier paper <ref> [7] </ref>. 5.1 The Tiger Problem Imagine an agent standing in front of two closed doors. Behind one of the doors is a tiger and behind the other is a large reward. <p> Each node of the graph is labeled with an action and there is one labeled outgoing edge for each possible outcome of the action. It is possible to generate this type of plan graph for some pomdps <ref> [38, 50, 7, 16] </ref>. For completely observable problems with a high branching factor, a more convenient representation is a policy which maps the current state (situation) to a choice of action. Because there is an action choice specified for all possible initial states, policies are also called universal plans [45].
Reference: [8] <author> Hsien-Te Cheng. </author> <title> Algorithms for Partially Observable Markov Decision Processes. </title> <type> PhD thesis, </type> <institution> University of British Columbia, British Columbia, Canada, </institution> <year> 1988. </year>
Reference-contexts: If we could do this, we might be able to reach a computation time per iteration that is polynomial in jSj, jAj, jj, jV t1 j, and jV t j. Cheng <ref> [8] </ref> and Smallwood and Sondik [48] also try to avoid generating all of V + t by constructing V t directly. However, their algorithms still have worst-case running times exponential in at least one of the problem parameters [25].
Reference: [9] <author> Lonnie Chrisman. </author> <title> Reinforcement learning with perceptual aliasing: The perceptual distinctions approach. </title> <booktitle> In Proceedings of the Tenth National Conference on Artificial Intelligence, </booktitle> <pages> pages 183-188, </pages> <address> San Jose, California, 1992. </address> <publisher> AAAI Press. </publisher> <pages> 31 </pages>
Reference-contexts: This approach has the potential significant advantage of being able to learn a model that is complex enough to support optimal (or good) behavior without making irrelevant distinctions; this idea has been pursued by Chrisman <ref> [9] </ref> and McCallum [31, 32]. 29 A Appendix Theorem 1 Let U be a non-empty set of useful policy trees, and Q a t be the complete set of useful policy trees.
Reference: [10] <author> Anne Condon. </author> <title> The complexity of stochastic games. </title> <journal> Information and Computation, </journal> <volume> 96(2) </volume> <pages> 203-224, </pages> <month> February </month> <year> 1992. </year>
Reference-contexts: The same holds for finite-horizon partially observable domains. Interestingly, a more complicated transformation holds in the opposite direction: any total expected discounted reward problem (completely observable or finite horizon) can be transformed into a goal-achievement problem of similar size <ref> [10, 57] </ref>. Roughly, the transformation simulates the discount factor by introducing an absorbing state with a small probability of being entered on each step.
Reference: [11] <author> Thomas Dean, Leslie Pack Kaelbling, Jak Kirman, and Ann Nicholson. </author> <title> Planning under time constraints in stochastic domains. </title> <journal> Artificial Intelligence, </journal> <volume> 76(1-2):35-74, </volume> <year> 1995. </year>
Reference-contexts: In many cases, we may not want a full policy; methods for developing partial policies and conditional plans for completely observable domains are the subject of much current interest <ref> [13, 11, 22] </ref>. A weakness of the methods described in this paper is that they require the states of the world to be represented enumeratively, rather than through compositional representations such as Bayes nets or probabilistic operator descriptions.
Reference: [12] <author> Denise Draper, Steve Hanks, and Dan Weld. </author> <title> Probabilistic planning with information gathering and contingent execution. </title> <type> Technical Report 93-12-04, </type> <institution> University of Washing-ton, </institution> <address> Seattle, WA, </address> <month> December </month> <year> 1993. </year>
Reference-contexts: The most closely related work to our own is that of Kushmerick, Hanks, and Weld [22] on the Buridan system, and Draper, Hanks and Weld <ref> [12] </ref> on the C-Buridan system. 6.1 Imperfect Knowledge Plans generated using standard mdp algorithms and classical strips-like planning algorithms assume that the underlying state of the process will be known with certainty during plan execution. <p> The main advantage comes from their compactness|combined with operator schemata, which can represent many individual actions in a single operator, propositional representations can be exponentially more concise than a fully expanded state-based transition matrix for an mdp. Algorithms for manipulating compact (or factored) pomdps have begun to appear <ref> [12, 6] </ref>|this is a promising area for future research. At present, however, there is no evidence that these algorithms result in improved planning time significantly over the use of a "flat" representation of the state space. 6.7 Plan Structures Planning systems differ in the structure of the plans they produce.
Reference: [13] <author> Mark Drummond and John Bresina. </author> <title> Anytime synthetic projection. </title> <booktitle> In Proceedings of the Eighth National Conference on Artificial Intelligence, </booktitle> <pages> pages 138-144. </pages> <publisher> Morgan Kaufmann, </publisher> <year> 1990. </year>
Reference-contexts: In many cases, we may not want a full policy; methods for developing partial policies and conditional plans for completely observable domains are the subject of much current interest <ref> [13, 11, 22] </ref>. A weakness of the methods described in this paper is that they require the states of the world to be represented enumeratively, rather than through compositional representations such as Bayes nets or probabilistic operator descriptions. <p> This type of action model is used in mdps and pomdps as well as in Buridan and C-Buridan. Other work <ref> [5, 13] </ref> has used representations that can be used to compute probability distributions over future states. 6.4 Observation Model When the starting state is known and actions are deterministic, there is no need to get feedback from the environment when executing a plan. <p> Bacchus, Boutilier, and Grove [2] show how some richer objectives based on evaluations of sequences of actions can actually be converted to total-reward problems. Other objectives considered in planning systems, aside from simple goals of achievement, include goals of maintenance and goals of prevention <ref> [13] </ref>; these types of goals can typically be represented using immediate rewards as well. 6.6 Representation of Problems The propositional representations most often used in planning have a number of advantages over the flat state-space representations associated with mdps and pomdps.
Reference: [14] <author> Emmanuel Fernandez-Gaucherand, Aristotle Arapostathis, and Steven I. Marcus. </author> <title> On the average cost optimality equation and the structure of optimal policies for partially observable Markov processes. </title> <journal> Annals of Operations Research, </journal> <volume> 29 </volume> <pages> 471-512, </pages> <year> 1991. </year>
Reference-contexts: Given the inter-representability results between goal-probability problems and discounted-optimality problems, it is hard to make technical sense of this difference. In fact, many pomdp models should probably be addressed in an average-reward context <ref> [14] </ref>. Using a discounted-optimal policy in a truly infinite-duration setting is a convenient approximation, similar to the use of a situation-action mapping from a finite-horizon policy in receding horizon control.
Reference: [15] <author> Peter Haddawy and Steve Hanks. </author> <title> Utility models for goal-directed decision-theoretic planners. </title> <type> Technical Report 93-06-04, </type> <institution> Department of Computer Science and Engineering, University of Washington, </institution> <month> June </month> <year> 1993. </year>
Reference-contexts: Koenig and Simmons [20] examine risk-sensitive planning and showed how planners for the total-reward criterion could be used to optimize risk-sensitive behavior. Haddawy et al. <ref> [15] </ref> looked at a broad family of decision-theoretic objectives that make it possible to specify trade-offs between partially satisfying goals quickly and satisfying them completely. Bacchus, Boutilier, and Grove [2] show how some richer objectives based on evaluations of sequences of actions can actually be converted to total-reward problems.
Reference: [16] <author> Eric A. Hansen. </author> <title> Cost-effective sensing during plan execution. </title> <booktitle> In Proceedings of the Twelfth National Conference on Artificial Intelligence, </booktitle> <pages> pages 1029-1035. </pages> <publisher> AAAI Press/The MIT Press, </publisher> <year> 1994. </year>
Reference-contexts: Each node of the graph is labeled with an action and there is one labeled outgoing edge for each possible outcome of the action. It is possible to generate this type of plan graph for some pomdps <ref> [38, 50, 7, 16] </ref>. For completely observable problems with a high branching factor, a more convenient representation is a policy which maps the current state (situation) to a choice of action. Because there is an action choice specified for all possible initial states, policies are also called universal plans [45].
Reference: [17] <author> Ronald A. Howard. </author> <title> Dynamic Programming and Markov Processes. </title> <publisher> The MIT Press, </publisher> <address> Cambridge, Massachusetts, </address> <year> 1960. </year>
Reference-contexts: Howard <ref> [17] </ref> showed that there exists a stationary policy, fl , that is optimal for every starting state.
Reference: [18] <author> R. E. </author> <title> Kalman. A new approach to linear filtering and prediction problems. </title> <journal> Transactions of the American Society of Mechanical Engineers, Journal of Basic Engineering, </journal> <volume> 82 </volume> <pages> 35-45, </pages> <month> March </month> <year> 1960. </year>
Reference: [19] <author> Sven Koenig. </author> <title> Optimal probabilistic and decision-theoretic planning using Markovian decision theory. </title> <type> Technical Report UCB/CSD 92/685, </type> <institution> Berkeley, </institution> <month> May </month> <year> 1992. </year>
Reference-contexts: The exponentially discounted sum of these rewards over the execution of a plan (finite or infinite horizon) constitutes the value of the plan. This objective is used extensively in most work with mdps and pomdps, including ours. Several authors (for example, Koenig <ref> [19] </ref>) have pointed out that, given a completely observable problem stated as one of goal achievement, reward functions can be constructed so that a policy that maximizes reward can be used to maximize the probability of goal attainment in the original problem.
Reference: [20] <author> Sven Koenig and Reid G. Simmons. </author> <title> Risk-sensitive planning with probabilistic decision graphs. </title> <booktitle> In Proceedings of the 4th International Conference on Principles of Knowledge Representation and Reasoning, </booktitle> <pages> pages 363-373, </pages> <year> 1994. </year>
Reference-contexts: Littman [27] catalogs some alternatives to the total-reward criterion, all of which are based on the idea that the objective value for a plan is based on a summary of immedi 27 ate rewards over the duration of a run. Koenig and Simmons <ref> [20] </ref> examine risk-sensitive planning and showed how planners for the total-reward criterion could be used to optimize risk-sensitive behavior. Haddawy et al. [15] looked at a broad family of decision-theoretic objectives that make it possible to specify trade-offs between partially satisfying goals quickly and satisfying them completely.
Reference: [21] <author> John R. Koza. </author> <title> Genetic Programming: On the Programming of Computers by Means of Natural Selection. </title> <publisher> The MIT Press, </publisher> <year> 1992. </year>
Reference-contexts: This argues that, in the limit, a plan is actually a program. Several techniques have been proposed recently for searching for good program-like controllers in pomdps <ref> [44, 21] </ref> We restrict our attention to the simpler finite-horizon case and a small set of infinite-horizon problems that have optimal finite-state plans. 7 Extensions and Conclusions The pomdp model provides a firm foundation for work on planning under uncertainty in action and observation.
Reference: [22] <author> Nicholas Kushmerick, Steve Hanks, and Daniel S. Weld. </author> <title> An algorithm for probabilistic planning. </title> <journal> Artificial Intelligence, </journal> <volume> 76(1-2):239-286, </volume> <month> September </month> <year> 1995. </year>
Reference-contexts: In many cases, we may not want a full policy; methods for developing partial policies and conditional plans for completely observable domains are the subject of much current interest <ref> [13, 11, 22] </ref>. A weakness of the methods described in this paper is that they require the states of the world to be represented enumeratively, rather than through compositional representations such as Bayes nets or probabilistic operator descriptions. <p> Our comparison focusses on issues of imperfect knowledge, uncertainty in initial state, the transition model, the observation model, the objective of planning, the representation of domains, and plan structures. The most closely related work to our own is that of Kushmerick, Hanks, and Weld <ref> [22] </ref> on the Buridan system, and Draper, Hanks and Weld [12] on the C-Buridan system. 6.1 Imperfect Knowledge Plans generated using standard mdp algorithms and classical strips-like planning algorithms assume that the underlying state of the process will be known with certainty during plan execution.
Reference: [23] <author> Shieu-Hong Lin and Thomas Dean. </author> <title> Generating optimal policies for high-level plans with conditional branches and loops. </title> <booktitle> In Proceedings of the Third European Workshop on Planning, </booktitle> <pages> pages 205-218, </pages> <year> 1995. </year> <month> 32 </month>
Reference-contexts: C-Buridan uses a representation of contingent plans that also allows for structure sharing (although of a different type than our DAG-structured plans). Our work on pomdps finds DAG-structured plans for finite-horizon problems. For infinite-horizon problems, it is necessary to introduce loops into the plan representation <ref> [37, 23] </ref>. (Loops might also be useful in long finite-horizon pomdps for representational 28 succinctness.) A simple loop-based plan representation depicts a plan as a labeled directed graph.
Reference: [24] <author> Michael L. Littman. </author> <title> Memoryless policies: Theoretical limitations and practical results. </title> <editor> In Dave Cliff, Philip Husbands, Jean-Arcady Meyer, and Stewart W. Wilson, editors, </editor> <booktitle> From Animals to Animats 3: Proceedings of the Third International Conference on Simulation of Adaptive Behavior, </booktitle> <address> Cambridge, MA, 1994. </address> <publisher> The MIT Press. </publisher>
Reference-contexts: Randomness effectively allows the agent to sometimes choose different actions in different locations with the same appearance, increasing the probability that it might choose a good action; in practice deterministic observation-action mappings are prone to getting trapped in deterministic loops <ref> [24] </ref>. In order to behave truly effectively in a partially observable world, it is necessary to use memory of previous actions and observations to aid in the disambiguation of the states of the world.
Reference: [25] <author> Michael L. Littman, Anthony R. Cassandra, and Leslie Pack Kaelbling. </author> <title> Efficient dynamic-programming updates in partially observable Markov decision processes. </title> <note> Submitted to Operations Research, </note> <year> 1995. </year>
Reference-contexts: Cheng [8] and Smallwood and Sondik [48] also try to avoid generating all of V + t by constructing V t directly. However, their algorithms still have worst-case running times exponential in at least one of the problem parameters <ref> [25] </ref>. In fact, the existence of an algorithm that runs in time polynomial in jSj, jAj, jj, jV t1 j, and jV t j would settle the long-standing complexity-theoretic question "Does NP=RP?" in the affirmative [25], so we will pursue a slightly different approach. <p> algorithms still have worst-case running times exponential in at least one of the problem parameters <ref> [25] </ref>. In fact, the existence of an algorithm that runs in time polynomial in jSj, jAj, jj, jV t1 j, and jV t j would settle the long-standing complexity-theoretic question "Does NP=RP?" in the affirmative [25], so we will pursue a slightly different approach. Instead of computing V t directly, we will compute, for each action a, a set Q a t of t-step policy trees that have action a at their root. <p> In what sense is the witness algorithm superior to previous algorithms for solving pomdps, then? Experiments indicate that the witness algorithm is faster in practice over a wide range of problem sizes <ref> [25] </ref>. The primary complexity-theoretic difference is that the witness algorithm runs in polynomial time in the number of policy trees in Q a t . <p> Figure 9 illustrates the relationship between p and p new . Now we can state the witness theorem <ref> [25] </ref>: The true Q-function, Q a t , differs from the approximate Q-function, ^ Q a t , if and only if there is some p 2 U , o 2 , and p 0 2 V t1 for which there is some b such that V p new (b) &gt; <p> It gives a uniform treatment of action to gain information and action to change the world. Although they are derived through the domain of continuous belief spaces, elegant finite-state controllers may sometimes be constructed using algorithms such as the witness algorithm. However, experimental results <ref> [25] </ref> suggest, even the witness algorithm becomes impractical for problems of modest size (jSj &gt; 15 and jj &gt; 15).
Reference: [26] <author> Michael L. Littman, Anthony R. Cassandra, and Leslie Pack Kaelbling. </author> <title> Learning policies for partially observable environments: Scaling up. </title> <editor> In Armand Prieditis and Stuart Russell, editors, </editor> <booktitle> Proceedings of the Twelfth International Conference on Machine Learning, </booktitle> <pages> pages 362-370, </pages> <address> San Francisco, CA, 1995. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Our current work explores the use of function-approximation methods for representing value functions and the use of simulation in order to concentrate the approximations on the frequently visited parts of the belief space <ref> [26] </ref>. The results of this work are encouraging and have allowed us to get a very good solution to an 89 state, 16 observation instance of a hallway navigation problem similar to the one described in the introduction.
Reference: [27] <author> Michael Lederman Littman. </author> <title> Algorithms for Sequential Decision Making. </title> <type> PhD thesis, </type> <institution> Department of Computer Science, Brown University, </institution> <month> February </month> <year> 1996. </year> <note> Also Technical Report CS-96-09. </note>
Reference-contexts: On each iteration, we can enumerate all of the t-step policy trees, then compute the maximum 13 of their value functions to get V t . If the value functions are represented by sets of policy trees, the test for termination can be implemented exactly using linear programming <ref> [27] </ref>. This is, of course, hopelessly computationally intractable. Each t-step policy tree contains (jj t 1)=(jj 1) nodes (the branching factor is jj, the number of possible observations). <p> In fact, many pomdp models should probably be addressed in an average-reward context [14]. Using a discounted-optimal policy in a truly infinite-duration setting is a convenient approximation, similar to the use of a situation-action mapping from a finite-horizon policy in receding horizon control. Littman <ref> [27] </ref> catalogs some alternatives to the total-reward criterion, all of which are based on the idea that the objective value for a plan is based on a summary of immedi 27 ate rewards over the duration of a run.
Reference: [28] <author> William S. Lovejoy. </author> <title> A survey of algorithmic methods for partially observable Markov decision processes. </title> <journal> Annals of Operations Research, </journal> <volume> 28(1) </volume> <pages> 47-65, </pages> <year> 1991. </year>
Reference-contexts: Much of the content of this paper is a recapitulation of work in the operations-research literature <ref> [28, 33, 48, 50, 54] </ref>. We have developed new ways of viewing the problem that are, perhaps, more consistent with the AI perspective; for example, we give a novel development of exact finite-horizon pomdp algorithms in terms of "policy trees" instead of the classical algebraic approach [48].
Reference: [29] <author> T. M. Mansell. </author> <title> A method for planning given uncertain and incomplete information. </title> <booktitle> In Proceedings of the 9th Conference on Uncertainty in Artificial Intelligence, </booktitle> <pages> pages 350-358. </pages> <publisher> Morgan Kaufmann Publishers, </publisher> <month> July </month> <year> 1993. </year>
Reference-contexts: An exception is the U-Plan <ref> [29] </ref> system, which creates a separate 25 plan for each possible initial state with the aim of making these plans easy to merge to form a single plan. Conditional planners typically have some aspects of the initial state unknown.
Reference: [30] <author> David McAllester and David Rosenblitt. </author> <title> Systematic nonlinear planning. </title> <booktitle> In Proceedings of the 9th National Conference on Artificial Intelligence, </booktitle> <pages> pages 634-639, </pages> <year> 1991. </year>
Reference-contexts: In the mdp framework, the agent is informed of the current state each time it takes an action. In many strips-like planners (e.g., snlp <ref> [30] </ref>, ucpop [36]), the current state can be calculated trivially from the known initial state and knowledge of the deterministic operators. The assumption of perfect knowledge is not valid in many domains.
Reference: [31] <author> R. Andrew McCallum. </author> <title> Overcoming incomplete perception with utile distinction memory. </title> <booktitle> In Proceedings of the Tenth International Conference on Machine Learning, </booktitle> <pages> pages 190-196, </pages> <address> Amherst, Massachusetts, 1993. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: This approach has the potential significant advantage of being able to learn a model that is complex enough to support optimal (or good) behavior without making irrelevant distinctions; this idea has been pursued by Chrisman [9] and McCallum <ref> [31, 32] </ref>. 29 A Appendix Theorem 1 Let U be a non-empty set of useful policy trees, and Q a t be the complete set of useful policy trees.
Reference: [32] <author> R. Andrew McCallum. </author> <title> Instance-based utile distinctions for reinforcement learning with hidden state. </title> <booktitle> In Proceedings of the Twelfth International Conference on Machine Learning, </booktitle> <pages> pages 387-395, </pages> <address> San Francisco, CA, 1995. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: This approach has the potential significant advantage of being able to learn a model that is complex enough to support optimal (or good) behavior without making irrelevant distinctions; this idea has been pursued by Chrisman [9] and McCallum <ref> [31, 32] </ref>. 29 A Appendix Theorem 1 Let U be a non-empty set of useful policy trees, and Q a t be the complete set of useful policy trees.
Reference: [33] <author> George E. Monahan. </author> <title> A survey of partially observable Markov decision processes: Theory, models, and algorithms. </title> <journal> Management Science, </journal> <volume> 28(1) </volume> <pages> 1-16, </pages> <month> January </month> <year> 1982. </year>
Reference-contexts: Much of the content of this paper is a recapitulation of work in the operations-research literature <ref> [28, 33, 48, 50, 54] </ref>. We have developed new ways of viewing the problem that are, perhaps, more consistent with the AI perspective; for example, we give a novel development of exact finite-horizon pomdp algorithms in terms of "policy trees" instead of the classical algebraic approach [48]. <p> We will call the elements of this set the useful policy trees. The ability to find the set of useful policy trees serves as a basis for a more efficient version of the value-iteration algorithm <ref> [33] </ref>.
Reference: [34] <author> Robert C. Moore. </author> <title> A formal theory of knowledge and action. </title> <editor> In Jerry R. Hobbs and Robert C. Moore, editors, </editor> <booktitle> Formal Theories of the Commonsense World, </booktitle> <pages> pages 319-358. </pages> <publisher> Ablex Publishing Company, </publisher> <address> Norwood, New Jersey, </address> <year> 1985. </year>
Reference-contexts: This is essentially a planning problem: given a complete and correct model of the world dynamics and a reward structure, find an optimal way to behave. In the artificial intelligence (AI) literature, a deterministic version of this problem has been addressed by adding knowledge preconditions to traditional planning systems <ref> [34] </ref>. Because we are interested in stochastic domains, however, we must depart from the traditional AI planning model. <p> In many strips-like planners (e.g., snlp [30], ucpop [36]), the current state can be calculated trivially from the known initial state and knowledge of the deterministic operators. The assumption of perfect knowledge is not valid in many domains. Research on epistemic logic <ref> [34, 35, 43] </ref> relaxes this assumption by making it possible to reason about what is and is not known at a given time. Unfortunately, epistemic logics have not been used as a representation in automatic planning systems, perhaps because the richness of representation they provide makes efficient reasoning very difficult.
Reference: [35] <author> L. Morgenstern. </author> <title> Knowledge preconditions for actions and plans. </title> <booktitle> In Proceedings of the 10th International Joint Conference on Artificial Intelligence, </booktitle> <pages> pages 867-874, </pages> <year> 1987. </year>
Reference-contexts: In many strips-like planners (e.g., snlp [30], ucpop [36]), the current state can be calculated trivially from the known initial state and knowledge of the deterministic operators. The assumption of perfect knowledge is not valid in many domains. Research on epistemic logic <ref> [34, 35, 43] </ref> relaxes this assumption by making it possible to reason about what is and is not known at a given time. Unfortunately, epistemic logics have not been used as a representation in automatic planning systems, perhaps because the richness of representation they provide makes efficient reasoning very difficult.
Reference: [36] <author> J. S. Penberthy and D. Weld. UCPOP: </author> <title> A sound, complete, partial order planner for ADL. </title> <booktitle> In Proceedings of the third international conference on principles of knowledge representation and reasoning, </booktitle> <pages> pages 103-114, </pages> <year> 1992. </year> <month> 33 </month>
Reference-contexts: In the mdp framework, the agent is informed of the current state each time it takes an action. In many strips-like planners (e.g., snlp [30], ucpop <ref> [36] </ref>), the current state can be calculated trivially from the known initial state and knowledge of the deterministic operators. The assumption of perfect knowledge is not valid in many domains.
Reference: [37] <author> Mark A. Peot and David E. Smith. </author> <title> Conditional nonlinear planning. </title> <booktitle> In Proceedings of the First International Conference on Artificial Intelligence Planning Systems, </booktitle> <pages> pages 189-197, </pages> <year> 1992. </year>
Reference-contexts: A step towards trying to build a working planning system that reasons about knowledge is to relax the generality of the logic-based schemes. The approach of cnlp <ref> [37] </ref> uses three-valued propositions where, in addition to true and false, there is a value unknown, which represents the state when the truth of the proposition is not known. <p> Many domains are not easily modeled with deterministic actions, since the outcome of an action can have different results, even when applied in exactly the same state. Extensions to classical planning, such as cnlp <ref> [37] </ref> and Cassandra [39] have considered operators with nondeterministic effects. For each operator, there is a set of possible next states that could occur. A drawback of this approach is that it gives no information about the relative likelihood of the possible outcomes. <p> C-Buridan uses a representation of contingent plans that also allows for structure sharing (although of a different type than our DAG-structured plans). Our work on pomdps finds DAG-structured plans for finite-horizon problems. For infinite-horizon problems, it is necessary to introduce loops into the plan representation <ref> [37, 23] </ref>. (Loops might also be useful in long finite-horizon pomdps for representational 28 succinctness.) A simple loop-based plan representation depicts a plan as a labeled directed graph.
Reference: [38] <author> Loren K. Platzman. </author> <title> A feasible computational approach to infinite-horizon partially-observed Markov decision problems. </title> <type> Technical report, </type> <institution> Georgia Institute of Technology, </institution> <address> Atlanta, GA, </address> <month> January </month> <year> 1981. </year>
Reference-contexts: Each node of the graph is labeled with an action and there is one labeled outgoing edge for each possible outcome of the action. It is possible to generate this type of plan graph for some pomdps <ref> [38, 50, 7, 16] </ref>. For completely observable problems with a high branching factor, a more convenient representation is a policy which maps the current state (situation) to a choice of action. Because there is an action choice specified for all possible initial states, policies are also called universal plans [45].
Reference: [39] <author> Louise Pryor and Gregg Collins. </author> <title> Planning for contingencies: A decision-based approach. </title> <journal> Journal of Artificial Intelligence Research, </journal> <volume> 4 </volume> <pages> 287-339, </pages> <year> 1996. </year>
Reference-contexts: Many domains are not easily modeled with deterministic actions, since the outcome of an action can have different results, even when applied in exactly the same state. Extensions to classical planning, such as cnlp [37] and Cassandra <ref> [39] </ref> have considered operators with nondeterministic effects. For each operator, there is a set of possible next states that could occur. A drawback of this approach is that it gives no information about the relative likelihood of the possible outcomes.
Reference: [40] <author> Martin L. Puterman. </author> <title> Markov Decision Processes|Discrete Stochastic Dynamic Programming. </title> <publisher> John Wiley & Sons, Inc., </publisher> <address> New York, NY, </address> <year> 1994. </year>
Reference-contexts: Markov decision processes are described in depth in a variety of texts <ref> [3, 40] </ref>; we will just briefly cover the necessary background. 2.1 Basic Framework A Markov decision process can be described as a tuple hS; A; T; Ri, where * S is a finite set of states of the world; * A is a finite set of actions; * T : S <p> That is, max jV V t (s) V fl (s)j &lt; 2* 1 fl It is often the case that V t = fl long before V t is near V fl ; tighter bounds may be obtained using the span semi-norm on the value function <ref> [40] </ref>. 3 Partial Observability For mdps we can compute the optimal policy and use it to act by simply executing (s) for current state s.
Reference: [41] <author> Lawrence R. Rabiner. </author> <title> A tutorial on hidden Markov models and selected applications in speech recognition. </title> <booktitle> Proceedings of the IEEE, </booktitle> <volume> 77(2) </volume> <pages> 257-286, </pages> <month> February </month> <year> 1989. </year>
Reference-contexts: We are optimistic and hope to extend these techniques (and others) to get good solutions to large problems. Another area that is not addressed in this paper is the acquisition of a world model. One approach is to extend techniques for learning hidden Markov models <ref> [41, 51] </ref> to learn pomdp models. Then, we could apply algorithms of the type described in this paper to the learned models. Another approach is to combine the learning of the model with the computation of the policy.
Reference: [42] <author> Katsushige Sawaki and Akira Ichikawa. </author> <title> Optimal control for partially observable Markov decision processes over an infinite horizon. </title> <journal> Journal of the Operations Research Society of Japan, </journal> <volume> 21(1) </volume> <pages> 1-14, </pages> <month> March </month> <year> 1978. </year>
Reference-contexts: This is not necessarily true for the infinite-horizon discounted value function; it remains convex [53], but may have infinitely many facets. Still, the optimal infinite-horizon discounted value function can be approximated arbitrarily closely by a finite-horizon value function for a sufficiently long horizon <ref> [50, 42] </ref>. <p> At this point, we have an approximately optimal value function for the infinite-horizon discounted problem. This pomdp has the property that the optimal infinite-horizon value function has a finite number of linear segments. An associated optimal policy has a finite description and is called finitely transient <ref> [49, 42] </ref>. Pomdps with optimal finitely transient policies can sometimes be solved in finite time using value iteration. In pomdps with optimal policies that are not finitely transient, the infinite-horizon value function has an infinite number of segments; on these problems the sets V t grow with each iteration.
Reference: [43] <author> R. B. Scherl and H. J. Levesque. </author> <title> The frame problem and knowledge-producing actions. </title> <booktitle> In Proceedings of the 11th National Conference on Artificial Intelligence, </booktitle> <pages> pages 689-697, </pages> <year> 1993. </year>
Reference-contexts: In many strips-like planners (e.g., snlp [30], ucpop [36]), the current state can be calculated trivially from the known initial state and knowledge of the deterministic operators. The assumption of perfect knowledge is not valid in many domains. Research on epistemic logic <ref> [34, 35, 43] </ref> relaxes this assumption by making it possible to reason about what is and is not known at a given time. Unfortunately, epistemic logics have not been used as a representation in automatic planning systems, perhaps because the richness of representation they provide makes efficient reasoning very difficult.
Reference: [44] <author> Jieyu Zhao Jurgen H. Schmidhuber. </author> <title> Incremental self-improvement for life-time multi-agent reinforcement learning. </title> <editor> In Pattie Maes, Maja J. Mataric, Jean-Arcady Meyer, Jordan Pollack, and Stewart W. Wilson, editors, </editor> <booktitle> From Animals to Animats: Proceedings of the Fourth International Conference on Simulation of Adaptive Behavior, </booktitle> <pages> pages 516-525. </pages> <publisher> The MIT Press, </publisher> <year> 1996. </year>
Reference-contexts: This argues that, in the limit, a plan is actually a program. Several techniques have been proposed recently for searching for good program-like controllers in pomdps <ref> [44, 21] </ref> We restrict our attention to the simpler finite-horizon case and a small set of infinite-horizon problems that have optimal finite-state plans. 7 Extensions and Conclusions The pomdp model provides a firm foundation for work on planning under uncertainty in action and observation.
Reference: [45] <author> Marcel J. Schoppers. </author> <title> Universal plans for reactive robots in unpredictable environments. </title> <booktitle> In Proceedings of the International Joint Conference on Artificial Intelligence 10, </booktitle> <pages> pages 1039-1046, </pages> <year> 1987. </year>
Reference-contexts: For completely observable problems with a high branching factor, a more convenient representation is a policy which maps the current state (situation) to a choice of action. Because there is an action choice specified for all possible initial states, policies are also called universal plans <ref> [45] </ref>. This representation is not appropriate for pomdps, since the underlying state is not fully observable. However, pomdp policies can be viewed as universal plans over belief space. It is interesting to note that there are infinite-horizon pomdps for which no finite-state plan is sufficient.
Reference: [46] <author> Alexander Schrijver. </author> <title> Theory of Linear and Integer Programming. </title> <publisher> Wiley-Interscience, </publisher> <address> New York, NY, </address> <year> 1986. </year>
Reference-contexts: Note that we must assume that the number of bits of precision used in specifying the model is polynomial in these quantities since the polynomial running time of linear programming is expressed as a function of the input precision <ref> [46] </ref>. 5 Understanding Policies In this section we introduce a very simple example and use it to illustrate some properties of pomdp policies. Other examples are explored in an earlier paper [7]. 5.1 The Tiger Problem Imagine an agent standing in front of two closed doors.
Reference: [47] <author> Satinder Pal Singh, Tommi Jaakkola, and Michael I. Jordan. </author> <title> Model-free reinforcement learning for non-Markovian decision problems. </title> <booktitle> In Proceedings of the Eleventh International Conference on Machine Learning, </booktitle> <pages> pages 284-292, </pages> <address> San Francisco, California, 1994. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Somewhat better results can be obtained by adding randomness to the agent's behavior: a policy can be a mapping from observations to probability distributions over actions <ref> [47] </ref>. Randomness effectively allows the agent to sometimes choose different actions in different locations with the same appearance, increasing the probability that it might choose a good action; in practice deterministic observation-action mappings are prone to getting trapped in deterministic loops [24].
Reference: [48] <author> Richard D. Smallwood and Edward J. Sondik. </author> <title> The optimal control of partially observable Markov processes over a finite horizon. </title> <journal> Operations Research, </journal> <volume> 21 </volume> <pages> 1071-1088, </pages> <year> 1973. </year>
Reference-contexts: Much of the content of this paper is a recapitulation of work in the operations-research literature <ref> [28, 33, 48, 50, 54] </ref>. We have developed new ways of viewing the problem that are, perhaps, more consistent with the AI perspective; for example, we give a novel development of exact finite-horizon pomdp algorithms in terms of "policy trees" instead of the classical algebraic approach [48]. <p> We have developed new ways of viewing the problem that are, perhaps, more consistent with the AI perspective; for example, we give a novel development of exact finite-horizon pomdp algorithms in terms of "policy trees" instead of the classical algebraic approach <ref> [48] </ref>. We begin by introducing the theory of Markov decision processes (mdps) and pomdps. We then outline a novel algorithm for solving pomdps off line and show how, in some cases, a finite-memory controller can be extracted from the solution to a pomdp. <p> If we could do this, we might be able to reach a computation time per iteration that is polynomial in jSj, jAj, jj, jV t1 j, and jV t j. Cheng [8] and Smallwood and Sondik <ref> [48] </ref> also try to avoid generating all of V + t by constructing V t directly. However, their algorithms still have worst-case running times exponential in at least one of the problem parameters [25].
Reference: [49] <author> Edward Sondik. </author> <title> The Optimal Control of Partially Observable Markov Processes. </title> <type> PhD thesis, </type> <institution> Stanford University, </institution> <year> 1971. </year> <month> 34 </month>
Reference-contexts: At this point, we have an approximately optimal value function for the infinite-horizon discounted problem. This pomdp has the property that the optimal infinite-horizon value function has a finite number of linear segments. An associated optimal policy has a finite description and is called finitely transient <ref> [49, 42] </ref>. Pomdps with optimal finitely transient policies can sometimes be solved in finite time using value iteration. In pomdps with optimal policies that are not finitely transient, the infinite-horizon value function has an infinite number of segments; on these problems the sets V t grow with each iteration. <p> The only finite-time algorithm that has been described for solving pomdps with finitely transient optimal policies over the infinite horizon is a version of policy iteration described by Sondik <ref> [49] </ref>. 5.4 Plan Graphs One drawback of the pomdp approach is that the agent must maintain a belief state and use it to select an optimal action on every step; if the underlying state space or V is large, then this computation can be expensive.
Reference: [50] <author> Edward J. Sondik. </author> <title> The optimal control of partially observable Markov processes over the infinite horizon: Discounted costs. </title> <journal> Operations Research, </journal> <volume> 26(2) </volume> <pages> 282-304, </pages> <year> 1978. </year>
Reference-contexts: Much of the content of this paper is a recapitulation of work in the operations-research literature <ref> [28, 33, 48, 50, 54] </ref>. We have developed new ways of viewing the problem that are, perhaps, more consistent with the AI perspective; for example, we give a novel development of exact finite-horizon pomdp algorithms in terms of "policy trees" instead of the classical algebraic approach [48]. <p> This belief mdp is such that an optimal policy for it, coupled with the correct state estimator, will give rise to optimal behavior (in the discounted infinite-horizon sense) for the original pomdp <ref> [50, 1] </ref>. The remaining problem, then, is to solve this mdp. <p> This is not necessarily true for the infinite-horizon discounted value function; it remains convex [53], but may have infinitely many facets. Still, the optimal infinite-horizon discounted value function can be approximated arbitrarily closely by a finite-horizon value function for a sufficiently long horizon <ref> [50, 42] </ref>. <p> In many cases, it is possible to encode the policy in a graph that can be used to select actions without any explicit representation of the belief state <ref> [50] </ref>; we refer to such graphs as plan graphs. Recall Figure 14, in which the algorithm has nearly converged upon an infinite-horizon policy for the tiger problem. <p> Each node of the graph is labeled with an action and there is one labeled outgoing edge for each possible outcome of the action. It is possible to generate this type of plan graph for some pomdps <ref> [38, 50, 7, 16] </ref>. For completely observable problems with a high branching factor, a more convenient representation is a policy which maps the current state (situation) to a choice of action. Because there is an action choice specified for all possible initial states, policies are also called universal plans [45].
Reference: [51] <author> Andreas Stolcke and Stephen Omohundro. </author> <title> Hidden Markov model induction by Bayesian model merging. </title> <editor> In Stephen Jose Hanson, Jack D. Cowan, and C. Lee Giles, editors, </editor> <booktitle> Advances in Neural Information Processing Systems 5, </booktitle> <pages> pages 11-18, </pages> <address> San Mateo, California, 1993. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: We are optimistic and hope to extend these techniques (and others) to get good solutions to large problems. Another area that is not addressed in this paper is the acquisition of a world model. One approach is to extend techniques for learning hidden Markov models <ref> [41, 51] </ref> to learn pomdp models. Then, we could apply algorithms of the type described in this paper to the learned models. Another approach is to combine the learning of the model with the computation of the policy.
Reference: [52] <author> Paul Tseng. </author> <title> Solving H-horizon, stationary Markov decision problems in time proportional to log(H). </title> <journal> Operations Research Letters, </journal> <volume> 9(5) </volume> <pages> 287-297, </pages> <year> 1990. </year>
Reference-contexts: The algorithm terminates when the maximum difference between two successive value functions (known as the Bellman error magnitude) is less than some *. It can be shown <ref> [52] </ref> that there exists a t fl , polynomial in jSj, jAj, the magnitude of the largest value of R (s; a), and 1=(1 fl), such that the greedy policy with respect to V t fl is equal to the optimal infinite-horizon policy, fl .
Reference: [53] <author> C. C. White and D. Harrington. </author> <title> Application of Jensen's inequality for adaptive suboptimal design. </title> <journal> Journal of Optimization Theory and Applications, </journal> <volume> 32(1) </volume> <pages> 89-99, </pages> <year> 1980. </year>
Reference-contexts: This is not necessarily true for the infinite-horizon discounted value function; it remains convex <ref> [53] </ref>, but may have infinitely many facets. Still, the optimal infinite-horizon discounted value function can be approximated arbitrarily closely by a finite-horizon value function for a sufficiently long horizon [50, 42].
Reference: [54] <author> Chelsea C. White, III. </author> <title> Partially observed Markov decision processes: A survey. </title> <journal> Annals of Operations Research, </journal> <volume> 32, </volume> <year> 1991. </year>
Reference-contexts: Much of the content of this paper is a recapitulation of work in the operations-research literature <ref> [28, 33, 48, 50, 54] </ref>. We have developed new ways of viewing the problem that are, perhaps, more consistent with the AI perspective; for example, we give a novel development of exact finite-horizon pomdp algorithms in terms of "policy trees" instead of the classical algebraic approach [48].
Reference: [55] <author> Ronald J. Williams and Leemon C. Baird, III. </author> <title> Tight performance bounds on greedy policies based on imperfect value functions. </title> <type> Technical Report NU-CCS-93-14, </type> <institution> Northeastern University, College of Computer Science, </institution> <address> Boston, MA, </address> <month> November </month> <year> 1993. </year>
Reference-contexts: Rather than calculating a bound on t fl in advance and running value iteration for that long, we instead use the following result regarding the Bellman error magnitude <ref> [55] </ref> in order to terminate with a near-optimal policy. If jV t (s) V t1 (s)j &lt; * for all s, then the value of the greedy policy with respect to V t does not differ from V fl by more than 2*fl=(1 fl) at any state.
Reference: [56] <author> Nevin L. Zhang and Wenju Liu. </author> <title> Planning in stochastic domains: Problem characteristics and approximation. </title> <type> Technical Report HKUST-CS96-31, </type> <institution> Department of Computer Science, Hong Kong University of Science and Technology, </institution> <year> 1996. </year>
Reference-contexts: That means, if we restrict ourselves to problems in which jQ a t j is polynomial, that the resulting running time is poly-3 A more recent algorithm by Zhang <ref> [56] </ref>, inspired by the witness algorithm, has the same asymptotic complexity but appears to be the current fastest algorithm empirically for this problem. 15 V 1 := fh0; 0; : : : ; 0ig loop t := t + 1 foreach a in A Q a t := witness (V t1
Reference: [57] <author> Uri Zwick and Mike Paterson. </author> <title> The complexity of mean payoff games on graphs. </title> <booktitle> Theoretical Computer Science, </booktitle> <address> 158(1-2):343-359, </address> <year> 1996. </year> <month> 35 </month>
Reference-contexts: The same holds for finite-horizon partially observable domains. Interestingly, a more complicated transformation holds in the opposite direction: any total expected discounted reward problem (completely observable or finite horizon) can be transformed into a goal-achievement problem of similar size <ref> [10, 57] </ref>. Roughly, the transformation simulates the discount factor by introducing an absorbing state with a small probability of being entered on each step.
References-found: 57

