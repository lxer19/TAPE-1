URL: ftp://ftp.cs.sandia.gov/pub/papers/dewombl/parallel_io_dags93.ps.Z
Refering-URL: http://www.cs.dartmouth.edu/pario/examples.html
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Title: BEYOND CORE: MAKING PARALLEL COMPUTER I/O PRACTICAL  
Author: DAVID WOMBLE DAVID GREENBERG STEPHEN WHEAT AND ROLF RIESEN 
Address: 1993.)  
Affiliation: Dartmouth Institute for Advanced Graduate Studies,  
Note: (Proceedings of the  
Abstract: The solution of Grand Challenge Problems will require computations which are too large to fit in the memories of even the largest machines. Inevitably, new designs of I/O systems will be necessary to support them. Through our implementations of an out-of-core LU factorization we have learned several important lessons about what I/O systems should be like. In particular we believe that the I/O system must provide the programmer with the ability to explicitly manage storage. One method of doing so is to have a partitioned secondary storage in which each processor owns a logical disk. Along with operating system enhancements which allow overheads such as buffer copying to be avoided, this sort of I/O system meets the needs of high performance computing. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> C. M. Burns, R. H. Kuhn, and E. J. Werme, </author> <title> Low copy message passing on the alliant CAMPUS/800, </title> <booktitle> in Proceedings of Supercomputer '92, </booktitle> <year> 1992, </year> <pages> pp. 760-769. </pages>
Reference-contexts: A potential pitfall in memory management is excessive system buffering. Because of the large volume of data that can be transferred, extra system buffering can soak up both time and memory. The cost of buffering in both I/O and message passing is well known <ref> [1] </ref>. PUMA supplies the user with the ability to create portals. A portal is a section of memory for which the user makes a "contract" with the OS. The contract specifies ways in which the system and the user can share access to the section of memory.
Reference: [2] <author> T. H. Cormen, </author> <title> Virtual Memory for Data-Parallel Computing, </title> <type> PhD thesis, </type> <institution> MIT, </institution> <year> 1993. </year>
Reference-contexts: When data is needed in a new format, the interconnection network can be used to reorder the data, and then write to the disks in the new format (see Cormen <ref> [2] </ref>). In our most efficient implementation of column-oriented LU factorization, we use two formats. The first is used for the unfactored blocks of the matrix where the matrix is stored by columns. The second is used to store intermediate results and the final (factored) matrix. <p> I/O complexity. The has been quite a bit of work done on the I/O complexity of several problems, including permutation, sorting, FFT's, and matrix-matrix multiplies <ref> [2, 6, 8] </ref>. Most of the work has dealt with the shared memory model. The bound on the amount of I/O can be used in the distributed memory case if an I/O operation consists of all processors reading in their contribution to a data block si multaneously. <p> The number of disk operations (I/O complexity) for this algorithm is O (n 3 =B p M ), where n is the size of the matrix, M is the size of memory, and B is the total size of one I/O request (across all processors) <ref> [2, 8] </ref>.
Reference: [3] <author> J. M. del Rosario, R. Bordawekar, and A. Choudhary, </author> <title> Improving parallel I/O performance using a two-phase access strategy, </title> <type> Tech. Report SCCS-406, </type> <institution> Northeast Parallel Architectures Center, </institution> <year> 1993. </year>
Reference-contexts: Several reports have examined this issue and found that pre-or post-permutations of data lead to a substantially reduced running time for many computations <ref> [3] </ref>. Sometimes a single computation will require data in different formats for different subcomputations. At other times, as noted above, the input or output interface may require data in a different format from that which is optimal for the computation. Thus the ability to convert between formats can be important.
Reference: [4] <author> G. H. Golub and C. F. Van Loan, </author> <title> Matrix Computations, </title> <publisher> Johns Hopkins University Press, </publisher> <editor> 2nd ed., </editor> <year> 1989. </year>
Reference-contexts: If the submatrices are too large to fit in memory, the matrix is recursively subdivided [8]. A similar procedure can be used to recursively subdivide LU factorization without pivoting <ref> [4] </ref>. If pivoting is necessary, however, entire columns must be held in memory. A recursive procedure can also be designed in this case (which we discuss below in the section on I/O complexity), but the more natural decomposition is to treat blocks of columns as the basic I/O unit.
Reference: [5] <author> B. A. Hendrickson and D. E. Womble, </author> <title> The torus-wrap mapping for dense matrix calculations on massively parallel computers, </title> <journal> SIAM J. Sci. Stat. Comput., </journal> <note> (to appear). </note>
Reference-contexts: the block server implementation of PSS now incorporated into the PUMA operating system, * use a column-block decomposition of the matrix for disk I/O and overlap I/O and computations as described in the section on complexity (a torus-wrap decomposition of matrix elements to processors is used within the column blocks <ref> [5] </ref>), * permute a column block after its LU factors have been computed to make the elements of the lower triangular portion contiguous within each processor's partitioned storage, and * use partial pivoting to ensure numerical stability.
Reference: [6] <author> J.-W. Hong and H. T. Kung, </author> <title> I/O complexity: The red-blue pebble game, </title> <booktitle> in Proceedings of the Symposium on the Theory of Computing, </booktitle> <year> 1981, </year> <pages> pp. 326-332. </pages>
Reference-contexts: I/O complexity. The has been quite a bit of work done on the I/O complexity of several problems, including permutation, sorting, FFT's, and matrix-matrix multiplies <ref> [2, 6, 8] </ref>. Most of the work has dealt with the shared memory model. The bound on the amount of I/O can be used in the distributed memory case if an I/O operation consists of all processors reading in their contribution to a data block si multaneously.
Reference: [7] <author> A. B. Maccabe and S. R. Wheat, </author> <title> Message passing in SUNMOS. Overview of the Sandia/University of New Mexico OS, now called PUMA., </title> <month> Jan. </month> <year> 1993. </year>
Reference-contexts: Systems Implementations. We do not intend to recommend any particular implementation of an I/O system. So far we have merely argued for the inclusion of a high performance option that looks like a PSS. We have, however, been using a particular OS called PUMA <ref> [7] </ref>. PUMA was designed by Sandia National Laboratories and the University of New Mexico initially for the nCUBE, and there are plans to port it to the Intel Paragon. Because PUMA is our own operating system we have been able to incorporate several features which we have found particularly useful.
Reference: [8] <author> J. S. Vitter and E. A. M. Shriver, </author> <title> Algorithms for parallel memory I: Two-level memories, </title> <type> Tech. Report CS-92-04, </type> <institution> Brown University, </institution> <year> 1992. </year>
Reference-contexts: C 1;1 = A 1;1 B 1;1 + A 1;2 B 2;1 C 2;1 = A 2;1 B 1;1 + A 2;2 B 2;1 where the subscripts designate the location of the submatrices. If the submatrices are too large to fit in memory, the matrix is recursively subdivided <ref> [8] </ref>. A similar procedure can be used to recursively subdivide LU factorization without pivoting [4]. If pivoting is necessary, however, entire columns must be held in memory. <p> I/O complexity. The has been quite a bit of work done on the I/O complexity of several problems, including permutation, sorting, FFT's, and matrix-matrix multiplies <ref> [2, 6, 8] </ref>. Most of the work has dealt with the shared memory model. The bound on the amount of I/O can be used in the distributed memory case if an I/O operation consists of all processors reading in their contribution to a data block si multaneously. <p> The number of disk operations (I/O complexity) for this algorithm is O (n 3 =B p M ), where n is the size of the matrix, M is the size of memory, and B is the total size of one I/O request (across all processors) <ref> [2, 8] </ref>. <p> This can be explained by observing that the complexity of rearranging the rows of a matrix is O (n 2 =B) <ref> [8] </ref>, which is a lower order term. We also note that the upper bound for LU factorization is the same as that for matrix-matrix multiplication. The recursive algorithm for LU factorization can be difficult to implement so we now describe a simpler algorithm and discuss its complexity.
References-found: 8

