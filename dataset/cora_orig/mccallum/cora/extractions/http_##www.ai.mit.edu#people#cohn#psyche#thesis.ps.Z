URL: http://www.ai.mit.edu/people/cohn/psyche/thesis.ps.Z
Refering-URL: http://www.ai.mit.edu/people/cohn/psyche/
Root-URL: 
Title: Separating Formal Bounds from Practical Performance in Learning Systems  
Author: by David Cohn 
Degree: A dissertation submitted in partial fulfillment of the requirements for the degree of Doctor of Philosophy  Approved by (Chairperson of Supervisory Committee)  
Note: Program Authorized to Offer Degree Date  
Date: 1992  
Affiliation: University of Washington  
Abstract-found: 0
Intro-found: 0
Reference: [AAC + 89] <author> M. Aggoune, L. Atlas, D. Cohn, M. Damborg, M. El-Sharkawi, and R. Marks II. </author> <title> Artificial neural networks for power system static security assessment. </title> <booktitle> In Proceedings of the IEEE International Symposium on Circuits and Systems, </booktitle> <year> 1989. </year>
Reference-contexts: Otherwise it risks thermal overload and brown-out. Previous research determined that this problem was amenable to neural network learning, and that neural networks solutions were competitive with those of conventional statistical techniques <ref> [AAC + 89, ACM + 90, ACD91] </ref>. However, random sampling of the problem domain was inefficient in terms of examples needed, and all methods tested required a large body of training data.
Reference: [ACD91] <author> L. Atlas, J. Connor, and M. Damborg. </author> <title> Comparisons of conventional techniques and neural networks in power system load forecasting and stability. </title> <booktitle> In Proceedings of the American Power Conference, </booktitle> <pages> pages 1196-1200, </pages> <address> Chicago, IL, </address> <month> April 29 May 1 </month> <year> 1991. </year>
Reference-contexts: Otherwise it risks thermal overload and brown-out. Previous research determined that this problem was amenable to neural network learning, and that neural networks solutions were competitive with those of conventional statistical techniques <ref> [AAC + 89, ACM + 90, ACD91] </ref>. However, random sampling of the problem domain was inefficient in terms of examples needed, and all methods tested required a large body of training data.
Reference: [ACM + 90] <author> L. Atlas, R. Cole, Y. Muthusamy, A. Lippman, J. Connor, D. Park, M. </author> <title> El-Sharkawi, and R.J. Marks. A performance comparison of trained multilayer perceptrons and trained classification trees. </title> <booktitle> Proceedings of the IEEE, </booktitle> <month> October </month> <year> 1990. </year>
Reference-contexts: Otherwise it risks thermal overload and brown-out. Previous research determined that this problem was amenable to neural network learning, and that neural networks solutions were competitive with those of conventional statistical techniques <ref> [AAC + 89, ACM + 90, ACD91] </ref>. However, random sampling of the problem domain was inefficient in terms of examples needed, and all methods tested required a large body of training data.
Reference: [Ahm88] <author> S. Ahmad. </author> <title> A study of scaling and generalization in neural networks. </title> <type> Technical Report UILU-ENG-88-1759, </type> <institution> University of Illinois at Urbana-Champaign, </institution> <year> 1988. </year>
Reference-contexts: Empirical support for these theories in fact predates the theories themselves. Work by Ahmad and Tesauro <ref> [Ahm88, AT88] </ref> indicates that single-layered networks trained on the linearly-separable "majority" function exhibit an exponential-like decrease in generalization error with increasing training set size. These results provide encouragement that it might be possible to surpass the worst-case VC bound by a substantial margin in at least some cases. <p> The experiments described here are a follow-up and extension of the work originally reported in <ref> [Ahm88, AT88] </ref>.
Reference: [AK91] <author> D. Angluin and M. Kharitonov. </author> <title> When won't membership queries help? In Proc. </title> <booktitle> of 23rd Annual ACM Symposium on the Theory of Computing, </booktitle> <pages> pages 444-454, </pages> <address> New York, 1991. </address> <publisher> ACM Press. </publisher>
Reference-contexts: Although in the worst case, self-directed learning may do no better than random sampling <ref> [AK91] </ref>, in many formal problems self-directed learning is provably more 89 powerful than passively learning from randomly given examples. A simple example is that of locating a boundary on the unit line interval. <p> Additionally, given certain smoothness constraints on the distribution, they describe how 108 queries may be used to learn the class of initial segments on the unit line. Angluin and Kharitonov <ref> [AK91] </ref> have also used cryptographic analysis to describe concept classes over which no learner can do better than learning by random sampling. Actual implementations of querying systems for learning have only recently been explored.
Reference: [Ams88] <author> J. </author> <title> Amsterdam. Extending the Valiant learning model. </title> <booktitle> In Proceedings of the Fifth International Conference on Machine Learning, </booktitle> <pages> pages 381-394, </pages> <address> San Jose, 1988. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Here, the learner must specifically ask for information that it thinks will be useful. It may make queries to an oracle [Ang88], e.g. it may pose an example and ask for its proper labeling, which the oracle provides in some fixed amount of time. In research by Amsterdam <ref> [Ams88] </ref>, this is described as the process of the learner "doing experiments" on the environment and observing the results. Chapters 2 and 3 are concerned with the first, and best-studied of these three forms of learning: learning from random examples. <p> In a membership query, the learner queries a point in the input domain and an oracle returns the classification of that point. Much work in formal learning theory has been directed to the study of queries (see e.g.: <ref> [Ang86, Val84, Ams88] </ref>), but only very recently have queries been examined with respect to their role in improving generalization behavior [CAL90, Eis91]. <p> This means that the efficiency of the learning process also approaches zero; eventually, most examples we draw will provide us with no information about the concept we are trying to learn. Amsterdam <ref> [Ams88] </ref> describes a formalism for an "EXPERIMENT" oracle, which allows redefining an input distribution such that random examples are drawn only from portions of a domain meeting some criterion specified by the learner. <p> Valiant [Val84] considers various classes that are learnable using a variety of forms of directed learning. Amsterdam <ref> [Ams88] </ref> proposed the EXPERIMENT oracle discussed in Section 4.2.2, incorporating Valiant's notion of an EXAMPLE and MEMBER or acles. Work described in [Eis91] puts bounds on the degree to which membership queries examples can help generalization when the underlying distribution is unknown.
Reference: [Ang86] <author> D. Angluin. </author> <title> Learning regular sets from queries and counter-examples. </title> <type> Technical Report YALEU/DCS/TR-64, </type> <institution> Yale University, </institution> <month> March </month> <year> 1986. </year>
Reference-contexts: Angluin and Smith [AS83] summarize early work in this field. Below, we briefly describe the exact identification problem for learning minimal finite state automata. In work by Gold [Gol78] and Angluin <ref> [Ang86] </ref>, a learner attempts to build a minimal finite automaton that is functionally identical to an unknown target automaton. <p> In a membership query, the learner queries a point in the input domain and an oracle returns the classification of that point. Much work in formal learning theory has been directed to the study of queries (see e.g.: <ref> [Ang86, Val84, Ams88] </ref>), but only very recently have queries been examined with respect to their role in improving generalization behavior [CAL90, Eis91]. <p> Angluin <ref> [Ang86] </ref> showed that while minimal finite state automata were not poly-nomially learnable (in the Valiant sense) from examples alone, they could be learned using a polynomial number of queries to an oracle that provides counter-examples.
Reference: [Ang88] <author> D. Angluin. </author> <title> Queries and concept learning. </title> <journal> Machine Learning, </journal> <volume> 2, </volume> <year> 1988. </year> <month> 111 </month>
Reference-contexts: Learning from queries. Here, the learner must specifically ask for information that it thinks will be useful. It may make queries to an oracle <ref> [Ang88] </ref>, e.g. it may pose an example and ask for its proper labeling, which the oracle provides in some fixed amount of time. In research by Amsterdam [Ams88], this is described as the process of the learner "doing experiments" on the environment and observing the results.
Reference: [AS83] <author> D. Angluin and C. Smith. </author> <title> Inductive inference: Theory and methods. </title> <journal> Computing Surveys, </journal> <volume> 15(3) </volume> <pages> 237-269, </pages> <year> 1983. </year>
Reference-contexts: The general idea is that a concept is "learnable" if, in the limit of large training sets, a learner is able to exactly identify the target concept. Angluin and Smith <ref> [AS83] </ref> summarize early work in this field. Below, we briefly describe the exact identification problem for learning minimal finite state automata. In work by Gold [Gol78] and Angluin [Ang86], a learner attempts to build a minimal finite automaton that is functionally identical to an unknown target automaton.
Reference: [Ash89] <author> T. Ash. </author> <title> Dynamic node creation in backpropagation networks. </title> <type> Technical Report ICS-8901, </type> <institution> Institute for Cognitive Science, University of California, </institution> <address> San Diego, </address> <month> February </month> <year> 1989. </year>
Reference-contexts: Although there are network training algorithms that involve changing a network's topology during training (e.g. <ref> [Ash89] </ref>), we consider here only those with fixed topologies that train by weight adjustment. The theory and methods described here should, with some modification, be equally applicable to other trainable classifiers.
Reference: [AT88] <author> S. Ahmad and G. Tesauro. </author> <title> Scaling and generalization in neural networks: a case study. </title> <editor> In D. Touretzky, J. Elman, T. Sejnowski, and G. Hinton, editors, </editor> <booktitle> Proceedings of the 1988 Connectionist Models Summer School, </booktitle> <address> San Jose, 1988. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Empirical support for these theories in fact predates the theories themselves. Work by Ahmad and Tesauro <ref> [Ahm88, AT88] </ref> indicates that single-layered networks trained on the linearly-separable "majority" function exhibit an exponential-like decrease in generalization error with increasing training set size. These results provide encouragement that it might be possible to surpass the worst-case VC bound by a substantial margin in at least some cases. <p> The experiments described here are a follow-up and extension of the work originally reported in <ref> [Ahm88, AT88] </ref>.
Reference: [Bau90] <author> E. Baum. </author> <title> When are k-nearest neighbor and back propagation accurate for feasible sized sets of examples? In L. </title> <editor> Almeida and C. Wellekens, editors, </editor> <booktitle> Neural Networks EURASIP Workshop. </booktitle> <publisher> Springer-Verlag, </publisher> <year> 1990. </year>
Reference-contexts: This suggests an upper bound of O (W=m) for generalization error in neural networks. Baum <ref> [Bau90] </ref> describes work in which neural networks were trained to emulate a prototype neural network. The error of the networks with respect to the prototype behaved roughly as W=m, giving credence to the above hypothesis.
Reference: [BDB85] <author> R. Bethea, B. Duran, and T. Boullion. </author> <title> Statistical methods for engineers and scientist. </title> <publisher> Marcel Dekker, </publisher> <address> New York, </address> <year> 1985. </year>
Reference-contexts: The value of r 2 varies between 0 and 1: 0 means that knowing x gives no information about y; 1 means that x gives perfect information about the location of y (see <ref> [BDB85] </ref> for details). When comparing the fit of two models to a set of empirical data, we may thus gauge the relative fit of the models by comparing the r 2 values of the linearized fits.
Reference: [BEHW89] <author> A. Blumer, A. Ehrenfeucht, D. Haussler, and M. K. Warmuth. </author> <title> Learn-ability and the Vapnik-Chervonenkis dimension. </title> <journal> Journal of the ACM, </journal> <volume> 36(4) </volume> <pages> 929-965, </pages> <month> October </month> <year> 1989. </year>
Reference-contexts: In the past ten years, both formal research in computational learning theory and empirical research in machine learning systems has blossomed as powerful theories (such as those described in <ref> [Vap82, Val84, BEHW89] </ref>), and powerful learning algorithms (such as those in [RHW86, LBG80]), have come into widespread use. Unfortunately, very little work has addressed the relationship between the theory and practice of machine learning. <p> The second question concerns the computational complexity of a learning problem, and has been the focus of extensive work, some of which is described in <ref> [BEHW89, BR89, Jud88] </ref>. Below, I will be concerned with computational complexity only to the extent that it renders some learning problems intractable. 4 The data that an empirical learner works with may be divided into two types: training data and test data. <p> It may be given only positive examples, only negative examples, or both. The expectation is that the distribution over which the error will be measured is the same as the distribution from which the training examples were drawn <ref> [Val84, BEHW89] </ref>. 2. Learning by being taught. Here, a benevolent teacher provides pedagogical examples designed to lead the learner to the correct solution [RS88]. <p> For example, if we apply our genealogical algorithm to learning the concept mammal, there is a high probability that we will err 2 Valiant's theory used only a single parameter ffi as both an error bound and confidence bound. <ref> [BEHW89] </ref> demonstrated that the more manageable two-parameter model was formally equivalent to Valiant's original one. 7 in our classification when we are shown a duck-billed platypus for the first time. <p> We can think of the concept class as being the set of concepts that our learner is willing/able 8 to consider. Typically, a class is defined in terms of some representation language, with concepts being represented as strings in that language. Drawing on <ref> [BEHW89] </ref>, we make the following assumptions: 1. Each concept c 2 C has a unique string s c representing it in the language. 2. There exists an algorithm that will decide if a given string s is in the language in time polynomial in the length of s. 3. <p> P and labeled according to t, where m is polynomial in 1=* and 1=ffi, 2. outputs a concept c 2 C that is consistent with P m such that *(c; (t;P)) * with confidence at least 1 ffi, and 9 3. halts in time polynomial in 1=*, 1=ffi and m <ref> [BEHW89] </ref>. Example: The concept class of 2-CNF (conjunctions of clauses with at most two lit erals per clause) with n variables is learnable from examples using m &gt; 1 * 8n 3 + ln 1 labeled samples [Val84, BEHW89]. <p> Example: The concept class of 2-CNF (conjunctions of clauses with at most two lit erals per clause) with n variables is learnable from examples using m &gt; 1 * 8n 3 + ln 1 labeled samples <ref> [Val84, BEHW89] </ref>. Example: The concept class of axis-parallel rectangles in a plane is learnable from examples using m &gt; 4 * ln 4 ffi labeled samples. Note that in this case the size of the concept to be learned is constant [BEHW89]. <p> Example: The concept class of axis-parallel rectangles in a plane is learnable from examples using m &gt; 4 * ln 4 ffi labeled samples. Note that in this case the size of the concept to be learned is constant <ref> [BEHW89] </ref>. Work by Valiant [Val84], Judd [Jud88], and Blum and Rivest [BR89] has demonstrated that very few useful concept classes are actually polynomially learnable from examples. Even the problem of training a neural network with only one hidden layer is NP-complete. <p> measures the generalization spectrum near perfect performance for one of the single-layer networks, in an attempt to find the theoretically predicted "gap." Finally, the concluding section discusses the implications of our results, and possible directions for future work along these lines. 19 2.1.1 Worst-case behavior: Vapnik-Chervonenkis bounds Blumer et al. <ref> [BEHW89] </ref> demonstrated that the results in [VC71] and [Vap82] could be used to provide upper bounds on the generalization error of a concept in the PAC model. <p> However, this dependence does not appear until the training set size exceeds roughly 200, the same place where the shape of the generalization curve changes. 2.5 Comparison to Theory We examined how the observed generalization curves compared to the theoretical upper bounds described in <ref> [Vap82, BEHW89, HLW90, HKS91] </ref>. This comparison is illustrated in Figure 2.9. For the N -input, single-layer networks, the VC-dimension of the network is N + 1. <p> We then show how these bounds may be used to bound the difference in training and test performance of a VQ codebook. 48 3.2.1 Pattern classification and the VC-dimension The results in <ref> [BEHW89, Vap82, VC71] </ref> concern the asymptotic performance of learning systems. More specifically, these results bound the difference between the empirically observed performance of a system and its "true" performance as a function of the number of inputs over which the empirical performance was observed. <p> The VC-dimension of the class is the size m of the largest set P m that can be shattered by C. For example, the VC-dimension of the class of balls in k-dimensional Euclidean space is 3k <ref> [BEHW89] </ref>. Given the empirical error *(c; (t; P m )), for c selected from a class with VC-dimension d, the theorems of Vapnik and Chervonenkis bound the probability that the *(c; (t; P)) will exceed some value. <p> The simplest upper bound we can place on the VC-dimension of a binary VQ codebook class is derived by a combinatorial argument from <ref> [BEHW89] </ref>. In order to shatter m blocks, there must be a codebook that encodes correctly each of the 2 m subsets of those blocks. This requires that the class include at least 2 m distinct codebooks. <p> In many problems, though, we can still make use of distribution information without having to pay the full cost of drawing and classifying an example. Rather than assuming that the drawing of a classified exam ple is an atomic operation (as in <ref> [Val84, BEHW89] </ref>), we may divide the operation into two steps: first, that of drawing an unclassified example from the distribution, and second, querying the classification of that point. <p> The results (Figure 4.10) indicate a much steeper learning curve for selective sampling. Plotting the generalization error against the number of training examples m, networks trained on the randomly sampled data exhibited a roughly polynomial curve, as would be expected following Blumer et al. <ref> [BEHW89] </ref>. Using simple linear regression on 1 * , the error data fit * = (a m + b) 1 (for a = 0:0514 and b = 0:076) with a coefficient of determination (r 2 ) of 0:987.
Reference: [BH89] <author> E. Baum and D. Haussler. </author> <title> What size net gives valid generalization? Neural Computation, </title> <type> 1(1), </type> <year> 1989. </year>
Reference-contexts: Indeed, there does not seem to be any generalized algorithm for determining the VC-dimension of an arbitrary concept class. For feed-forward neural networks, the VC-dimension can be calculated exactly in the single-layer case, and can be bounded above and below in the multilayer case <ref> [BH89] </ref>. The lower bound on the dimension is just the number of weights in the network, and this is often used as a rough estimate of the actual VC-dimension. <p> It has been shown that, under certain conditions discussed in [RRK + 90, Wan90], a neural network classifier approximates a Bayes-optimal decision boundary, but only over its training set. Work by Baum and Haussler <ref> [BH89] </ref> shows that the VC-dimension of a neural network is bounded from below by W , and above by 2W log (eN ), where W is the total number of weights in the network and N is the number of nodes. <p> This comparison is illustrated in Figure 2.9. For the N -input, single-layer networks, the VC-dimension of the network is N + 1. In the higher-order case we have used the total number of weights as an estimate of the VC-dimension; this is a lower bound on the dimension, following <ref> [BH89] </ref>. <p> Self-Directed Learning Most neural network generalization problems are studied only with respect to random sampling: the training examples are chosen at random, and the network is simply a passive learner. This approach is generally referred to as "learning from examples." Baum and Haussler <ref> [BH89] </ref>, examine the problem analytically for neural networks; Chapter 2 of this dissertation provides an empirical study of neural network generalization when learning from examples.
Reference: [BL91] <author> E. Baum and K. Lang. </author> <title> Constructing hidden units using examples and queries. </title> <editor> In R. Lippmann, J. Moody, and D. Touretzky, editors, </editor> <booktitle> Advances in Neural Information Processing Systems 3, </booktitle> <address> San Jose, 1991. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: To obviate this problem, most work on such 11 domains, including the work described later in this dissertation, uses the paradigm of learning from both membership queries and random examples <ref> [BL91, Eis91] </ref>. 1.3 Two Learning Systems In this section, we discuss two learning systems with which we will be concerned in the following chapters: feedforward neural networks, and vector quantizers. <p> A variation on this approach reported in [Yam91] selects training examples based on the sensitivity of a partially-trained network to perturbations of the pattern. This combines the advantage of using distribution information with the practicality demonstrated in [HCOM90]. An algorithm due to Baum and Lang <ref> [BL91] </ref>, uses queries to reduce the computational costs of training a single hidden-layer neural network. Their algorithm makes queries that allow the network to efficiently determine the connection weights from the input layer to the hidden layer.
Reference: [BR89] <author> A. Blum and R. Rivest. </author> <title> Training a 3-node neural network is NP-complete. </title> <editor> In D. Touretzky, editor, </editor> <booktitle> Advances in Neural Information Processing Systems 1, </booktitle> <address> San Jose, 1989. </address> <publisher> Morgan Kaufmann. </publisher> <pages> 112 </pages>
Reference-contexts: The second question concerns the computational complexity of a learning problem, and has been the focus of extensive work, some of which is described in <ref> [BEHW89, BR89, Jud88] </ref>. Below, I will be concerned with computational complexity only to the extent that it renders some learning problems intractable. 4 The data that an empirical learner works with may be divided into two types: training data and test data. <p> Note that in this case the size of the concept to be learned is constant [BEHW89]. Work by Valiant [Val84], Judd [Jud88], and Blum and Rivest <ref> [BR89] </ref> has demonstrated that very few useful concept classes are actually polynomially learnable from examples. Even the problem of training a neural network with only one hidden layer is NP-complete.
Reference: [CAL90] <author> D. Cohn, L. Atlas, and R. Ladner. </author> <title> Training connectionist networks with queries and selective sampling. </title> <editor> In D. Touretzky, editor, </editor> <booktitle> Advances in Neural Information Processing Systems 2, </booktitle> <address> San Jose, 1990. </address> <publisher> Morgan Kauf-mann. </publisher>
Reference-contexts: Much work in formal learning theory has been directed to the study of queries (see e.g.: [Ang86, Val84, Ams88]), but only very recently have queries been examined with respect to their role in improving generalization behavior <ref> [CAL90, Eis91] </ref>. Although in the worst case, self-directed learning may do no better than random sampling [AK91], in many formal problems self-directed learning is provably more 89 powerful than passively learning from randomly given examples. A simple example is that of locating a boundary on the unit line interval.
Reference: [CCZD84] <author> J. B. Cadwallader-Cohen, W. W. Zysiczk, and R. B. Donnelly. </author> <title> The chaostron: an important advance in learning machines. </title> <journal> Communications of the ACM, </journal> <volume> 27(4) </volume> <pages> 356-357, </pages> <year> 1984. </year>
Reference-contexts: Section 1.4, contains a capsule summary of the results of this dissertation: the relation between bounds predicted by formal learning theory and the practical performance observed in these learning systems. There are almost as many definitions of "learning" as there are authors writing about it <ref> [Tan87, CF82, CCZD84] </ref>. Most of these definitions, and most systems which "learn" according to their criteria, may be said to follow one of two approaches: deductive (top-down) or inductive (bottom-up) inference.
Reference: [CDS90] <author> Y. Le Cun, J. Denker, and S. Solla. </author> <title> Optimal brain damage. </title> <editor> In D. Touret-zky, editor, </editor> <booktitle> Advances in Neural Information Processing Systems 2, </booktitle> <address> San Jose, 1990. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: We define the training error as *(c; (t; P m )) = m x2P m &lt; 0 if c (x) = t (x) 1 otherwise. Recent work (e.g. <ref> [HP89, Cha89, CDS90, Mac91] </ref>), minimizes this error subject to complexity constraints: the algorithm is willing to accept a slightly higher training 10 error in order to select a much simpler concept. In the work discussed here, however, we only consider classes of fixed complexity, so this issue does not arise. <p> There have also been a number of empirical efforts, such as those of Le Cun et al. <ref> [CDS90] </ref>, aimed at improving neural network generalization when learning from examples. Learning from examples is not, however, a universally applicable paradigm. Many natural learning systems are not simply passive, but instead make use of at least some form of directed learning to actively examine the problem domain.
Reference: [CF82] <author> P. Cohen and E. Feigenbaum. </author> <booktitle> Handbook of Artificial Intelligence. </booktitle> <publisher> Morgan Kaufmann, </publisher> <address> San Jose, </address> <year> 1982. </year>
Reference-contexts: Section 1.4, contains a capsule summary of the results of this dissertation: the relation between bounds predicted by formal learning theory and the practical performance observed in these learning systems. There are almost as many definitions of "learning" as there are authors writing about it <ref> [Tan87, CF82, CCZD84] </ref>. Most of these definitions, and most systems which "learn" according to their criteria, may be said to follow one of two approaches: deductive (top-down) or inductive (bottom-up) inference.
Reference: [Cha89] <author> Y. Chauvin. </author> <title> A back-propagation algorithm with optimal use of hidden units. </title> <editor> In D.S. Touretzky, editor, </editor> <booktitle> Advances in Neural Information Processing Systems 1, </booktitle> <address> San Jose, 1989. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: We define the training error as *(c; (t; P m )) = m x2P m &lt; 0 if c (x) = t (x) 1 otherwise. Recent work (e.g. <ref> [HP89, Cha89, CDS90, Mac91] </ref>), minimizes this error subject to complexity constraints: the algorithm is willing to accept a slightly higher training 10 error in order to select a much simpler concept. In the work discussed here, however, we only consider classes of fixed complexity, so this issue does not arise.
Reference: [CPP + 91] <author> P. Cosman, K. Perlmutter, S. Permutter, R. A. Olshen, and R. M. Gray. </author> <title> Training sequence size and vector quantizer performance. </title> <booktitle> In Proceedings of 25th Asilomar conference on signals, systems, and computers, </booktitle> <address> Asilo-mar, CA, </address> <month> November </month> <year> 1991. </year>
Reference-contexts: If the training is for a set of 10 images rather than a single image, then again only 5,000 vectors are needed in the training set which may represent only 3% of potential training vectors. The problem of training set size has also been studied independently at Stanford University <ref> [CPP + 91] </ref> with different but consistent results. We summarize the remainder of the chapter as follows. Section 3.2.2 provides a brief introduction to vector quantization and details the derivation of bounds ap propriate to the training of a VQ codebook.
Reference: [CT92] <author> D. Cohn and G. Tesauro. </author> <title> How tight are the Vapnik-Chervonenkis bounds? Neural Computation, </title> <type> 4(2), </type> <year> 1992. </year>
Reference-contexts: I am also deeply indebted to Professors Les Atlas and Eve Riskin, and to Dr. Gerry Tesauro for providing support, encouragement, and insight all along the way. Chapter 2 describes work done jointly with Gerry Tesauro which recently appeared in Neural Computation <ref> [CT92] </ref>. The work in Chapter 3 was done jointly with Richard Ladner, Eve Riskin, and Les Atlas. Chapter 4 describes work done with Richard Ladner and Les Atlas, and has been submitted to Machine Learning.
Reference: [CY90] <author> J. Crutchfield and K. Young. </author> <title> Computation at the onset of chaos. </title> <editor> In W. Zurek, editor, </editor> <title> Complexity, Entropy, </title> <booktitle> and the Physics of Information, SFI Stutdies in the Science of Complexity, </booktitle> <volume> volume 8, </volume> <pages> pages 223-269. </pages> <publisher> Addison-Wesley, </publisher> <year> 1990. </year>
Reference-contexts: This would provide an exact bound on worst-case learning complexity. Related work along these lines reported in <ref> [CY90] </ref> on the "statistical complexity" of stochastic processes appears to be applicable here, and we are in the process of trying to establish a formal link between these two lines of research. 2.
Reference: [Dud78] <author> R. M. Dudley. </author> <title> Central limit theorems for empirical measures. </title> <journal> Ann. Prob., </journal> <volume> 6(6) </volume> <pages> 899-929, </pages> <year> 1978. </year> <month> 113 </month>
Reference: [Eco89] <editor> U. Eco. Foucault's Pendulum. </editor> <publisher> Harcourt Brace Jovanovich, </publisher> <address> San Diego, </address> <year> 1989. </year>
Reference-contexts: It's an ideal principle, which can be verified only under ideal conditions. Which means never. But it's still true. -Amparo <ref> [Eco89] </ref> 1.1 Introduction Computational learning theory is concerned with the amount of data and computation needed to solve a learning problem.
Reference: [EHKV88] <author> A. Ehrenfeucht, D. Haussler, M. Kearns, and L. Valiant. </author> <title> A general lower bound on the number of examples needed for learning. </title> <editor> In D. Haussler and L. Pitt, editors, </editor> <booktitle> Proc. of the 1988 Workshop on Computational Learning Theory, </booktitle> <address> San Jose, 1988. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: If the training error is zero, then the above equation simplifies drastically to *(c; (t; P)) m 2m + 1) m A complementary worst-case lower bound in <ref> [EHKV88] </ref> demonstrates that there exist classes for which * O ( d m ), and thus bounds the worst case to within a logarithmic factor. <p> Despite the lack of a strict connection to this bound, the observed performance follows it remarkably well: one may note in Figure 2.9 that the observed curves lie below, but 38 within a small numerical constant of the bound. The worst-case lower bound in <ref> [EHKV88] </ref> argues that for any learning algorithm, there exists a learning problem such that, even if the learner classifies all m training examples correctly, it cannot guarantee an error of less than " with high confidence unless " (d 1)=(32m).
Reference: [Eis91] <author> B. Eisenberg. </author> <title> On the sample complexity of pac-learning using random and chosen examples. </title> <type> Master's thesis, </type> <institution> Massachusetts Institute of Technology, </institution> <year> 1991. </year>
Reference-contexts: To obviate this problem, most work on such 11 domains, including the work described later in this dissertation, uses the paradigm of learning from both membership queries and random examples <ref> [BL91, Eis91] </ref>. 1.3 Two Learning Systems In this section, we discuss two learning systems with which we will be concerned in the following chapters: feedforward neural networks, and vector quantizers. <p> Much work in formal learning theory has been directed to the study of queries (see e.g.: [Ang86, Val84, Ams88]), but only very recently have queries been examined with respect to their role in improving generalization behavior <ref> [CAL90, Eis91] </ref>. Although in the worst case, self-directed learning may do no better than random sampling [AK91], in many formal problems self-directed learning is provably more 89 powerful than passively learning from randomly given examples. A simple example is that of locating a boundary on the unit line interval. <p> Valiant [Val84] considers various classes that are learnable using a variety of forms of directed learning. Amsterdam [Ams88] proposed the EXPERIMENT oracle discussed in Section 4.2.2, incorporating Valiant's notion of an EXAMPLE and MEMBER or acles. Work described in <ref> [Eis91] </ref> puts bounds on the degree to which membership queries examples can help generalization when the underlying distribution is unknown. Additionally, given certain smoothness constraints on the distribution, they describe how 108 queries may be used to learn the class of initial segments on the unit line.
Reference: [FK87] <author> A. Fernald and P. Kuhl. </author> <title> Acoustic determinants of infant preference for motherese speech. Infant Behavior and Development, </title> <type> 10, </type> <year> 1987. </year>
Reference-contexts: In natural systems (such as humans), this phenomenon is exhibited at both high levels (e.g. active examination of objects) and low, subconscious levels (e.g. Fernald and Kuhl's <ref> [FK87] </ref> work on infant reactions to "Motherese" speech). Within the broad definition of self-directed learning, we will restrict our attention to the simple and intuitive form of concept learning via membership queries.
Reference: [FS75] <author> R. Floyd and L. Steinberg. </author> <title> An adaptive algorithm for spatial grey scale. </title> <booktitle> SID Int. Sym. Digest of Tech. Papers, </booktitle> <pages> pages 36-37, </pages> <year> 1975. </year>
Reference-contexts: The resulting halftoned image quality depends both on the size and entries of the dithering matrix. Error diffusion <ref> [FS75] </ref> is a more complex halftoning process than dithering but leads to higher image quality.
Reference: [Fur92] <author> M. Furst. </author> <type> personal communication, </type> <year> 1992. </year>
Reference-contexts: If this is the case, then we can expect that with advances in statistical learning theory we will see theories that account for this behavior without relying on a gap in representation or solution space. One hypothesis (due to Furst <ref> [Fur92] </ref>), currently being investigated is that the distribution of examples over the input space is responsible for difference in perfor mance.
Reference: [GG92] <author> A. Gersho and R. M. Gray. </author> <title> Vector Quantization and Signal Compression. </title> <publisher> Kluwer Academic Publishers, </publisher> <address> Boston, </address> <year> 1992. </year>
Reference-contexts: Below, we will use c to denote both the concept c and the network ~c that implements it. 1.3.2 Vector quantizers Vector quantization (VQ) <ref> [GG92, Gra84] </ref> is a data compression technique that can be used to reduce the storage or transmission costs of binary and grayscale images. It is lossy in that the compressed/uncompressed image is a degraded copy of the original image. <p> There are many variations on the VQ codebook scheme described here, such as tree-structured codebooks, predictive coders and trellis encoders, but in the following chapters I will consider only the basic scheme described above (see <ref> [GG92, Gra84] </ref> for descriptions of several other methods). 1.4 Overview of the dissertation In this last section, we give a brief overview of each of the remaining chapters in this dissertation.
Reference: [GJ79] <author> M. Gary and D. Johnson. </author> <title> Computers and Intractability. </title> <editor> W. H. Freemand, </editor> <address> New York, </address> <year> 1979. </year>
Reference-contexts: If the hypothesized automaton 1 It is generally thought that P 6= N P , but this is one of the great unproven conjectures of computer science. For more information, see <ref> [GJ79] </ref>. 6 is functionally identical to the target, then the oracle informs the learner that it is done.
Reference: [Gol67] <author> E. M. Gold. </author> <title> Language identification in the limit. </title> <journal> Information and Control, </journal> <volume> 10 </volume> <pages> 447-474, </pages> <year> 1967. </year>
Reference-contexts: I will then introduce Valiant's notion of probably-approximately-correct learning, which will serve as the basic definition of "learning" for problems discussed in this dissertation. Exact identification The idea of learning as inductive inference was first formalized by Gold <ref> [Gol67] </ref> who defined the problem as identification in the limit. The general idea is that a concept is "learnable" if, in the limit of large training sets, a learner is able to exactly identify the target concept. Angluin and Smith [AS83] summarize early work in this field.
Reference: [Gol78] <author> E. M. Gold. </author> <title> Complexity of automaton identification from given data. </title> <journal> Information and Control, </journal> <volume> 37, </volume> <year> 1978. </year>
Reference-contexts: Angluin and Smith [AS83] summarize early work in this field. Below, we briefly describe the exact identification problem for learning minimal finite state automata. In work by Gold <ref> [Gol78] </ref> and Angluin [Ang86], a learner attempts to build a minimal finite automaton that is functionally identical to an unknown target automaton.
Reference: [GP92] <author> D. Geiger and R. Pereira. </author> <title> Learning how to teach, or selecting minimal surface data. </title> <editor> In J. Moody, S. Hanson, and R. Lippmann, editors, </editor> <booktitle> Advances in Neural Information Processing Systems 4, </booktitle> <address> San Jose, 1992. </address> <publisher> Morgan Kaufmann. </publisher> <pages> 114 </pages>
Reference-contexts: Here, a benevolent teacher provides pedagogical examples designed to lead the learner to the correct solution [RS88]. A variant of this problem is the "data selection problem": given a large body of properly labeled examples, a teacher selects a useful subset of them to present to A <ref> [PW91, GP92] </ref>. 3. Learning from queries. Here, the learner must specifically ask for information that it thinks will be useful. It may make queries to an oracle [Ang88], e.g. it may pose an example and ask for its proper labeling, which the oracle provides in some fixed amount of time.
Reference: [Gra84] <author> R. M. Gray. </author> <title> Vector quantization. </title> <journal> IEEE ASSP Magazine, </journal> <volume> 1 </volume> <pages> 4-29, </pages> <month> April </month> <year> 1984. </year>
Reference-contexts: Below, we will use c to denote both the concept c and the network ~c that implements it. 1.3.2 Vector quantizers Vector quantization (VQ) <ref> [GG92, Gra84] </ref> is a data compression technique that can be used to reduce the storage or transmission costs of binary and grayscale images. It is lossy in that the compressed/uncompressed image is a degraded copy of the original image. <p> The algorithm proceeds as follows: 1. Select an initial set of N codewords fv 0 ; v 1 ; : : : ; v N1 g. This may be done randomly, by arbitrarily selecting N of the training vectors, or by one of the techniques described in <ref> [Gra84] </ref>. 2. Map each training vector x to the codeword v i that minimizes the distortion of x. We will denote as X i the set of training vectors that are mapped to codeword v i . <p> There are many variations on the VQ codebook scheme described here, such as tree-structured codebooks, predictive coders and trellis encoders, but in the following chapters I will consider only the basic scheme described above (see <ref> [GG92, Gra84] </ref> for descriptions of several other methods). 1.4 Overview of the dissertation In this last section, we give a brief overview of each of the remaining chapters in this dissertation.
Reference: [Hau87] <author> D. Haussler. </author> <title> Learning conjunctive concepts in structural domains. </title> <booktitle> In Proceedings, AAAI, </booktitle> <year> 1987. </year>
Reference-contexts: With more realistic, complicated classes, representing R (P m ) exactly can easily become a difficult, if not impossible, task: results by Haus-sler <ref> [Hau87] </ref> indicate that in the worst case, representing R (P m ) may require space exponential in the size of the training set. Using a good approximation of R (P m ) may, however, be sufficient to allow selective sampling.
Reference: [HCOM90] <author> J. N. Hwang, J. Choi, S. Oh, and R. Marks. </author> <title> Query learning based on boundary search and gradient computation of trained multilayer perceptrons. </title> <booktitle> In Proceedings, IJCNN, </booktitle> <year> 1990. </year>
Reference-contexts: uncertain, and may thus define it to be R fl (P m ), our approximation to the region of uncertainty (Figure 4.4). the network's transition area between 0 and 1 to represent the part of the domain where the network is "uncertain." Although this approach was used very successfully in <ref> [HCOM90] </ref> to learn mostly convex, connected regions, there are problems in applying it to general concept learning. This approach requires settling on a single network configuration and measuring the "uncertainty" in that configuration. <p> Thus, since the classification of a point is much more expensive than the determination of an input distribution, the problem is amenable to solution 106 by selective sampling. The baseline case of random sampling in four dimensions studied by Hwang et al. <ref> [HCOM90] </ref>, was used for comparison. In our experiments, we ran six sets of networks on the initial, random training sets (with 500 data points) and added a single iteration of selective sampling. <p> Angluin and Kharitonov [AK91] have also used cryptographic analysis to describe concept classes over which no learner can do better than learning by random sampling. Actual implementations of querying systems for learning have only recently been explored. Work done by Hwang et al. <ref> [HCOM90] </ref> implements querying for neural networks by means of inverting the activation of a trained network to determine where it is uncertain. This approach shows promise for concept learning in cases with relatively compact, connected concepts, and has already produced impressive results on the power system static security problem. <p> It is, however, susceptible to the pathology discussed in Section 4.3.1. A variation on this approach reported in [Yam91] selects training examples based on the sensitivity of a partially-trained network to perturbations of the pattern. This combines the advantage of using distribution information with the practicality demonstrated in <ref> [HCOM90] </ref>. An algorithm due to Baum and Lang [BL91], uses queries to reduce the computational costs of training a single hidden-layer neural network. Their algorithm makes queries that allow the network to efficiently determine the connection weights from the input layer to the hidden layer.
Reference: [HKS91] <author> D. Haussler, M. Kearns, and R. Schapire. </author> <title> Bounds on the sample complexity of Bayesian learning theory using information theory and the VC dimension. </title> <booktitle> In Proceedings of the 4th Annual Workshop on Computational Learning Theory, </booktitle> <pages> pages 61-74, </pages> <address> San Mateo, CA, 1991. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: First, I examine the expected generalization of simple neural networks training on randomly drawn training data, and compare the results to the performance predicted by Vapnik and Chervonenkis [VC71, Vap82], Haussler et al. <ref> [HKS91] </ref>, and Schwartz 2 et al. [SSSD90]. I then consider a non-traditional learning problem: the design of a vector quan-tizer codebook. <p> The results of these first two studies demonstrate that, except for specific algorithms and under very specific assumptions (such as those in <ref> [HKS91] </ref>), the current bounds are too loose to provide accurate prediction of generalization. In most cases, however, the bounds appear to be of the proper functional form, providing insight into the nature of the learning problem. <p> We con 16 sider neural networks as concept learners and empirically measure their performance quantitatively and qualitatively against a number of theoretical predictions. We find that the worst-case bounds using the Vapnik-Chervonenkis (VC) dimension [Vap82] are too loose to provide useful guidance, but that a variation of this bound <ref> [HKS91] </ref>, which requires the (unknown) assumption of Bayes optimality on the part of the learner, reflects the observed generalization performance of the neural networks very closely. We also examine the generalization performance of these networks with respect to the recent statistical learning theories described in [TLS89, SSSD90]. <p> A hypothesis concept is then selected based on this final distribution. This may be selected at random according to the distribution, or may be chosen according to some other criterion, such as Bayes optimality (e.g. <ref> [HKS91] </ref>). (x , t (x )) 11 3 (x , t (x )) 3 initially accessible space ( ) space accessible after four training examples ( ) r 0 that represent concepts not consistent with the labelings of the examples. <p> However, this dependence does not appear until the training set size exceeds roughly 200, the same place where the shape of the generalization curve changes. 2.5 Comparison to Theory We examined how the observed generalization curves compared to the theoretical upper bounds described in <ref> [Vap82, BEHW89, HLW90, HKS91] </ref>. This comparison is illustrated in Figure 2.9. For the N -input, single-layer networks, the VC-dimension of the network is N + 1. <p> Since this bound is a distribution-free worst-case upper bound for any algorithm that classifies the training data correctly, it is not surprising that it lies considerably above any of the observed curves, and, in the case of the higher-order functions, is completely off the scale. The bound in <ref> [HLW90, HKS91] </ref> is a distribution-free upper bound for the expected generalization performance of a Bayes-optimal learning algorithm. If such an algorithm classifies its m randomly drawn training examples correctly, then it will, on average, have a generalization error of at most " d=m. <p> We then conclude by briefly recapitulating the results of this chapter and suggesting directions for future research. 82 3.6.1 Implications of empirical results for theory The bounds derived in Section 3.2.2 are worst-case bounds, which hold regardless of the input distribution and codebook-design algorithm. It has been shown in <ref> [HKS91] </ref> that under certain circumstances, much tighter bounds may be derived if something is known about the design algorithm. Consider the case of an arbitrary classifier that can achieve zero training error on m training examples.
Reference: [HLW90] <author> D. Haussler, N. Littlestone, and M. Warmuth. </author> <title> Predicting f0, 1g-functions on randomly drawn points. </title> <type> Technical Report UCSC-CRL-90-54, </type> <institution> Univ. of California at Santa Cruz, </institution> <year> 1990. </year>
Reference-contexts: Attempts at obtaining tighter bounds using the VC-dimension have been made by making algorithm-specific assumptions. The Bayes-optimal bound described in Haussler et al. <ref> [HLW90] </ref> argues that any Bayes-optimal learning procedure (with suitable priors) will have an upper bound on generalization error of * d=m. It has been shown that, under certain conditions discussed in [RRK + 90, Wan90], a neural network classifier approximates a Bayes-optimal decision boundary, but only over its training set. <p> However, this dependence does not appear until the training set size exceeds roughly 200, the same place where the shape of the generalization curve changes. 2.5 Comparison to Theory We examined how the observed generalization curves compared to the theoretical upper bounds described in <ref> [Vap82, BEHW89, HLW90, HKS91] </ref>. This comparison is illustrated in Figure 2.9. For the N -input, single-layer networks, the VC-dimension of the network is N + 1. <p> Since this bound is a distribution-free worst-case upper bound for any algorithm that classifies the training data correctly, it is not surprising that it lies considerably above any of the observed curves, and, in the case of the higher-order functions, is completely off the scale. The bound in <ref> [HLW90, HKS91] </ref> is a distribution-free upper bound for the expected generalization performance of a Bayes-optimal learning algorithm. If such an algorithm classifies its m randomly drawn training examples correctly, then it will, on average, have a generalization error of at most " d=m.
Reference: [HP89] <author> S. Hanson and L. Pratt. </author> <title> Comparing biases for minimal network construction with back-propagation. </title> <editor> In D. S. Touretzky, editor, </editor> <booktitle> Advances in Neural Information Processing Systems 1, </booktitle> <pages> pages 177-185, </pages> <address> San Jose, 1989. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: We define the training error as *(c; (t; P m )) = m x2P m &lt; 0 if c (x) = t (x) 1 otherwise. Recent work (e.g. <ref> [HP89, Cha89, CDS90, Mac91] </ref>), minimizes this error subject to complexity constraints: the algorithm is willing to accept a slightly higher training 10 error in order to select a much simpler concept. In the work discussed here, however, we only consider classes of fixed complexity, so this issue does not arise.
Reference: [IS68] <author> F. Itakura and S. Saito. </author> <title> Analysis synthesis telephony based on the maximum likelihood method. </title> <booktitle> In Proceedings of the 6th Iternational Congress on Acoustics, </booktitle> <year> 1968. </year>
Reference-contexts: Typical distortion measures are the mean-squared error, the weighted mean-squared error, and the Itakura-Saito distortion (for speech) <ref> [IS68] </ref>. Below, we first introduce the pattern classification problem and formal bounds that have been derived for it using the Vapnik-Chervonenkis dimension.
Reference: [Jud88] <author> S. Judd. </author> <title> On the complexity of loading shallow neural networks. </title> <journal> Journal of Complexity, </journal> <volume> 4, </volume> <year> 1988. </year>
Reference-contexts: The second question concerns the computational complexity of a learning problem, and has been the focus of extensive work, some of which is described in <ref> [BEHW89, BR89, Jud88] </ref>. Below, I will be concerned with computational complexity only to the extent that it renders some learning problems intractable. 4 The data that an empirical learner works with may be divided into two types: training data and test data. <p> Example: The concept class of axis-parallel rectangles in a plane is learnable from examples using m &gt; 4 * ln 4 ffi labeled samples. Note that in this case the size of the concept to be learned is constant [BEHW89]. Work by Valiant [Val84], Judd <ref> [Jud88] </ref>, and Blum and Rivest [BR89] has demonstrated that very few useful concept classes are actually polynomially learnable from examples. Even the problem of training a neural network with only one hidden layer is NP-complete. <p> This "training" proceeds until the error on all examples than some specified threshold, at which point the examples are considered to have been "learned." At this point, we need to draw attention to the distinction between a neural network's architecture and its configuration (the terminology is that of Judd <ref> [Jud88] </ref>). The architecture of a neural network refers to those parameters of the network that do not change during training; in our case, this will be the network's topology and transfer functions.
Reference: [LBG80] <author> Y. Linde, A. Buzo, and R. M. Gray. </author> <title> An algorithm for vector quantizer design. </title> <journal> IEEE Transactions on Communications, </journal> <volume> 28 </volume> <pages> 84-95, </pages> <month> January </month> <year> 1980. </year>
Reference-contexts: In the past ten years, both formal research in computational learning theory and empirical research in machine learning systems has blossomed as powerful theories (such as those described in [Vap82, Val84, BEHW89]), and powerful learning algorithms (such as those in <ref> [RHW86, LBG80] </ref>), have come into widespread use. Unfortunately, very little work has addressed the relationship between the theory and practice of machine learning. <p> This process constitutes a major part of the computational cost of VQ. Below, I briefly describe the Generalized Lloyd Algorithm (GLA), a popular method for designing codebooks from training data <ref> [LBG80] </ref>. In the statistics community, this algorithm is known as the k-means algorithm. 15 Given an initial (e.g. random) set of codewords, and a set of training data, the GLA iteratively perturbs the codewords to minimize the distortion that they impose on the data. <p> Given an image (or set of images), a fixed block size, and a fixed codebook size, iterative algorithms such as the Generalized Lloyd Algorithm (GLA) select codebook vectors which locally optimize some image degradation measure <ref> [LBG80] </ref>. Typical distortion measures are the mean-squared error, the weighted mean-squared error, and the Itakura-Saito distortion (for speech) [IS68]. Below, we first introduce the pattern classification problem and formal bounds that have been derived for it using the Vapnik-Chervonenkis dimension. <p> A training set S m of m blocks is selected at random from this set, drawn either with or without replacement, depending on the paradigm being used. 2. The training set is used as input to a program running the Generalized Lloyd Algorithm (described in <ref> [LBG80] </ref>), which attempts to find a codebook c that yields a local minimum in the Hamming or mean squared error (MSE) distortion over its input. We denote the codebook designed from training set S m as c (S m ). 3.
Reference: [LV91] <author> J. Lin and J. Vitter. </author> <title> *-approximations with minimum constraint violation. </title> <type> Unpublished manuscript, </type> <month> November </month> <year> 1991. </year> <month> 115 </month>
Reference-contexts: We have completely ignored the implications of this work for achieving minimum distortion for a fixed bit rate (or minimum bit rate for fixed distortion), given a fixed allowable training time. Recent work described in <ref> [LV91] </ref> and elsewhere, addresses the problem of minimizing training distortion. From a bound on training distortion, with the work here bounding the difference (test train), it should be possible to directly bound the test distortion of an image as a function of its codebook training set size. 3.
Reference: [Mac91] <author> D. MacKay. </author> <title> Bayesian methods for adaptive models. </title> <type> PhD thesis, </type> <institution> Dept. of Computation and Neural Systems, California Institute of Technology, </institution> <year> 1991. </year>
Reference-contexts: We define the training error as *(c; (t; P m )) = m x2P m &lt; 0 if c (x) = t (x) 1 otherwise. Recent work (e.g. <ref> [HP89, Cha89, CDS90, Mac91] </ref>), minimizes this error subject to complexity constraints: the algorithm is willing to accept a slightly higher training 10 error in order to select a much simpler concept. In the work discussed here, however, we only consider classes of fixed complexity, so this issue does not arise. <p> One method for avoiding this problem is the use of Bayesian probabilities to measure the degree of utility in querying various parts of the region of uncertainty. This approach has recently been studied by MacKay <ref> [Mac91] </ref>, and is discussed briefly in the following section. 4.6 Related Work There is a large body of work studying the effects of queries from the strict learning theory viewpoint, primarily with respect to learning formal concepts such as Boolean expressions and finite state automata. <p> An algorithm due to Baum and Lang [BL91], uses queries to reduce the computational costs of training a single hidden-layer neural network. Their algorithm makes queries that allow the network to efficiently determine the connection weights from the input layer to the hidden layer. Recent work by David MacKay <ref> [Mac91] </ref> pursues a logical extension of selective sampling to Bayesian analysis. By assigning prior probabilities to each concept (or each network configuration) one can determine the utility of querying various parts of R (P m ).
Reference: [Mit82] <author> T. Mitchell. </author> <title> Generalization as search. </title> <journal> Artificial Intelligence, </journal> <volume> 18(2), </volume> <year> 1982. </year>
Reference-contexts: The process may then be repeated as necessary. This approximation has a number of properties that derive from its relation to the version space paradigm described by Tom Mitchell <ref> [Mit82] </ref>. Mitchell describes a learning procedure based on the partial ordering in generality of the concepts being learned. Some concept c 1 is "more general" than another concept c 2 if and only if c 2 c 1 .
Reference: [NH88] <author> A. N. Netravali and B. G. </author> <title> Haskell. Digital Pictures Representation and Compression. </title> <publisher> Plenum Press, </publisher> <address> New York and London, </address> <year> 1988. </year>
Reference-contexts: Halftoning is the process of rendering grayscale images so that they can be printed on binary devices. Pixels are assigned to either black or white to create the illusion of continuous shades of gray. Ordered dither <ref> [NH88, Uli87] </ref> is a simple halftoning technique that works independently on each subblock of a grayscale image. Each input grayscale pixel is compared to the appropriate threshold value in a dither matrix and is set to white if its intensity is above the threshold and is set to black otherwise.
Reference: [Pol84] <author> D. Pollard. </author> <title> Convergence of Stochastic Processes. </title> <publisher> Springer-Verlag, </publisher> <year> 1984. </year>
Reference: [Pra92] <author> L. Y. Pratt. </author> <title> Transfer of information among inductive learners. </title> <note> In Submitted to AAAI, </note> <year> 1992. </year>
Reference-contexts: We have found that in practice, with large training set sizes, it is often most efficient to simply retrain the entire network from scratch when new examples are added. Recent work by Pratt <ref> [Pra92] </ref> offers hope that this retraining may be made more efficient by use of "information transfer" strategies between iterations. 4.4 Experimental Results Experiments using selective sampling were run on three types of problems: solving a simple boundary-recognition problem in two dimensions, learning a 25-input real-valued threshold function, and recognizing the secure
Reference: [PW91] <author> M. Plutowski and H. White. </author> <title> Active selection of training examples for network learning in noiseless environments. </title> <type> Technical report, </type> <institution> Department of Computer Science, University of California, </institution> <address> San Diego, </address> <month> February </month> <year> 1991. </year>
Reference-contexts: Here, a benevolent teacher provides pedagogical examples designed to lead the learner to the correct solution [RS88]. A variant of this problem is the "data selection problem": given a large body of properly labeled examples, a teacher selects a useful subset of them to present to A <ref> [PW91, GP92] </ref>. 3. Learning from queries. Here, the learner must specifically ask for information that it thinks will be useful. It may make queries to an oracle [Ang88], e.g. it may pose an example and ask for its proper labeling, which the oracle provides in some fixed amount of time.
Reference: [RHW86] <author> D. Rumelhart, G. Hinton, and R. Williams. </author> <title> Learning internal representations by error propagation. </title> <editor> In D. Rumelhart and J. McClelland, editors, </editor> <booktitle> Parallel Distributed Processing, </booktitle> <volume> volume 1, chapter 8. </volume> <publisher> MIT Press, </publisher> <address> Cambridge, </address> <year> 1986. </year>
Reference-contexts: In the past ten years, both formal research in computational learning theory and empirical research in machine learning systems has blossomed as powerful theories (such as those described in [Vap82, Val84, BEHW89]), and powerful learning algorithms (such as those in <ref> [RHW86, LBG80] </ref>), have come into widespread use. Unfortunately, very little work has addressed the relationship between the theory and practice of machine learning. <p> Even on this limited task, the variety of neural network learning algorithms in use today is bewildering; below, we briefly describe the use of one of the most common of these: the backpropagation algorithm on simple feedforward networks <ref> [RHW86] </ref>. output from neuron 1 output from neuron 2 output from neuron 3 neuron j w j,1 j,2 w 3 o i summed, then normalized to the range (0; 1) using a sigmoidal "squashing" function. <p> We define the error of the output node n as ffi n (x) = (o n (x) t (x)) 2 . This error value is propagated back through the network (see <ref> [RHW86] </ref> for details), so that each neuron j has an error term ffi j (x). <p> Networks trained on these tasks used sigmoidal units and had standard feed-forward fully-connected structures with at most a single hidden layer. The training algorithm was standard backpropagation with momentum <ref> [RHW86] </ref>. A simulator run consisted of training a randomly initialized network on a training set of m examples of the target function, chosen uniformly from the input space. Networks were trained until all examples were classified within a specified training threshold of the correct classification.
Reference: [RRK + 90] <author> D. Ruck, S. Rogers, M. Kabrisky, M. Oxley, and B. Suter. </author> <title> The multilayer perceptron as an approximation to a bayes optimal discriminant function. </title> <journal> IEEE Transactions on Neural Networks, </journal> <volume> 1(4), </volume> <year> 1990. </year>
Reference-contexts: The Bayes-optimal bound described in Haussler et al. [HLW90] argues that any Bayes-optimal learning procedure (with suitable priors) will have an upper bound on generalization error of * d=m. It has been shown that, under certain conditions discussed in <ref> [RRK + 90, Wan90] </ref>, a neural network classifier approximates a Bayes-optimal decision boundary, but only over its training set. <p> short dashed lines indicate the standard error of the mean for each training set size. data, is Bayes-optimal over the training inputs to the extent that the algorithm is able to find a global error minimum, and to the extent that the network is able to represent the Bayes-optimal function <ref> [RRK + 90, Wan90] </ref>. Without a "prior" over the set of functions learnable by our networks, however, nothing is known about the optimality of backpropagation on inputs outside of the training set, which is what we are interested in.
Reference: [RS88] <author> R. Rivest and R. Sloan. </author> <title> Learning complicated concepts reliably and usefully. </title> <booktitle> In Proc. of AAAI-88, </booktitle> <year> 1988. </year>
Reference-contexts: The expectation is that the distribution over which the error will be measured is the same as the distribution from which the training examples were drawn [Val84, BEHW89]. 2. Learning by being taught. Here, a benevolent teacher provides pedagogical examples designed to lead the learner to the correct solution <ref> [RS88] </ref>. A variant of this problem is the "data selection problem": given a large body of properly labeled examples, a teacher selects a useful subset of them to present to A [PW91, GP92]. 3. Learning from queries.
Reference: [SSSD90] <author> D. Schwartz, V. Samalam, S. Solla, and J. Denker. </author> <title> Exhaustive learning. </title> <journal> Neural Computation, </journal> <volume> 2, </volume> <year> 1990. </year>
Reference-contexts: First, I examine the expected generalization of simple neural networks training on randomly drawn training data, and compare the results to the performance predicted by Vapnik and Chervonenkis [VC71, Vap82], Haussler et al. [HKS91], and Schwartz 2 et al. <ref> [SSSD90] </ref>. I then consider a non-traditional learning problem: the design of a vector quan-tizer codebook. <p> We also examine the generalization performance of these networks with respect to the recent statistical learning theories described in <ref> [TLS89, SSSD90] </ref>. These theories suggest that surpassing the worst-case bounds might be possible if the spectrum of possible generalizations has a "gap" near perfect performance. <p> Chapter 2 THEORY VERSUS PRACTICE OF NEURAL NETWORK GENERALIZATION 2.1 Introduction In this chapter, we examine the relevance of the Vapnik-Chervonenkis (VC) dimension bounds to the problem of generalization in a number of simple neural network problems. We also attempt to determine the applicability of an "average-case" statistical learning <ref> [SSSD90] </ref> to learning using the backpropagation algorithm. We find that, in some cases, the average generalization is significantly better than the VC-dimension bound: the approach to perfect performance is exponential in the number of examples m, rather than the 1=m result of the bound. <p> In this first section, we introduce the worst-case bounds that have been derived using the Vapnik-Chervonenkis dimension, and describe briefly a statistical formalism due to Schwartz et al. <ref> [SSSD90] </ref> that attempts to predict average-case performance. <p> they still leave us with the question of whether or not the observed empirical adherence to the VC-dimension bound is representative of "average case performance." 2.1.2 Average-case behavior: statistical theories A recent theoretical approach enables the calculation of expected performance of learning systems, rather than worst-case performance under certain conditions <ref> [TLS89, SSSD90] </ref>. These theories, which we call "statistical learning theories," calculate expected performance by modeling learning as a statistical process of choosing a concept from a distribution of concepts that are consistent with a training set. <p> One may think of 0 (g) as the generalization spectrum of the initial distribution, indicating the density of concepts with generalization g. The statistical formalism of Schwartz et al. <ref> [SSSD90] </ref> makes a prediction about generalization performance after m examples based on the continuity of 0 (g) near g = 1: if 0 (g) is continuous near g = 1, then the expected generalization error will fall off as 1=m, in agreement with the VC bound. <p> It would be of interest to extend the theory to discrete input spaces, to see if it could account for our exponential learning curves for the binary majority function. 2.6 Examination of "The Gap" As described in Section 2.1.2, the theories described in <ref> [TLS89, SSSD90] </ref> predict that if there is a gap in the spectrum of possible generalizations near the level of perfect generalization, then the shape of the learning curve will be exponentially decreasing. <p> This indicates that the worst-case bounds may be more relevant to expected performance than has been previously realized. The statistical theories of Tishy et al. and Schwartz et al. <ref> [TLS89, SSSD90] </ref> predict the two classes of behavior seen in our experiments, but there appears to be a discrepancy with theory in that we have been unable to find evidence of a "gap" in the generalization spectrum that would explain the observed exponential behavior.
Reference: [STS90] <author> H. Sompolinsky, N. Tishby, and H. Seung. </author> <title> Learning from examples in large neural networks. </title> <journal> Phys. Rev. Lett., </journal> <volume> 65, </volume> <year> 1990. </year> <month> 116 </month>
Reference-contexts: Note that for the XOR functions, the Blumer et al. bound is so high as to be off the graph. It is also of interest to compare our results with recent theoretical work of Som-polinsky et al. <ref> [STS90] </ref>, which uses a statistical mechanics formalism to calculate the expected generalization error of a single-layer perceptron.
Reference: [Tan87] <author> S. </author> <title> Tanimoto. </title> <booktitle> The Elements of Artificial Intelligence. </booktitle> <publisher> Computer Science Press, </publisher> <address> Rockville, MD, </address> <year> 1987. </year>
Reference-contexts: Section 1.4, contains a capsule summary of the results of this dissertation: the relation between bounds predicted by formal learning theory and the practical performance observed in these learning systems. There are almost as many definitions of "learning" as there are authors writing about it <ref> [Tan87, CF82, CCZD84] </ref>. Most of these definitions, and most systems which "learn" according to their criteria, may be said to follow one of two approaches: deductive (top-down) or inductive (bottom-up) inference.
Reference: [TLS89] <author> N. Tishby, E. Levin, and S. Solla. </author> <title> Consistent inference of probabilities in layered networks: Predictions and generalizations. </title> <booktitle> In IJCNN Proceedings. IEEE, </booktitle> <year> 1989. </year>
Reference-contexts: We also examine the generalization performance of these networks with respect to the recent statistical learning theories described in <ref> [TLS89, SSSD90] </ref>. These theories suggest that surpassing the worst-case bounds might be possible if the spectrum of possible generalizations has a "gap" near perfect performance. <p> they still leave us with the question of whether or not the observed empirical adherence to the VC-dimension bound is representative of "average case performance." 2.1.2 Average-case behavior: statistical theories A recent theoretical approach enables the calculation of expected performance of learning systems, rather than worst-case performance under certain conditions <ref> [TLS89, SSSD90] </ref>. These theories, which we call "statistical learning theories," calculate expected performance by modeling learning as a statistical process of choosing a concept from a distribution of concepts that are consistent with a training set. <p> It would be of interest to extend the theory to discrete input spaces, to see if it could account for our exponential learning curves for the binary majority function. 2.6 Examination of "The Gap" As described in Section 2.1.2, the theories described in <ref> [TLS89, SSSD90] </ref> predict that if there is a gap in the spectrum of possible generalizations near the level of perfect generalization, then the shape of the learning curve will be exponentially decreasing. <p> This indicates that the worst-case bounds may be more relevant to expected performance than has been previously realized. The statistical theories of Tishy et al. and Schwartz et al. <ref> [TLS89, SSSD90] </ref> predict the two classes of behavior seen in our experiments, but there appears to be a discrepancy with theory in that we have been unable to find evidence of a "gap" in the generalization spectrum that would explain the observed exponential behavior.
Reference: [Uli87] <author> R. Ulichney. </author> <title> Digital halftoning. </title> <publisher> MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1987. </year>
Reference-contexts: Halftoning is the process of rendering grayscale images so that they can be printed on binary devices. Pixels are assigned to either black or white to create the illusion of continuous shades of gray. Ordered dither <ref> [NH88, Uli87] </ref> is a simple halftoning technique that works independently on each subblock of a grayscale image. Each input grayscale pixel is compared to the appropriate threshold value in a dither matrix and is set to white if its intensity is above the threshold and is set to black otherwise.
Reference: [Val84] <author> L. Valiant. </author> <title> A theory of the learnable. </title> <journal> Communications of the ACM, </journal> <volume> 27, </volume> <year> 1984. </year>
Reference-contexts: In the past ten years, both formal research in computational learning theory and empirical research in machine learning systems has blossomed as powerful theories (such as those described in <ref> [Vap82, Val84, BEHW89] </ref>), and powerful learning algorithms (such as those in [RHW86, LBG80]), have come into widespread use. Unfortunately, very little work has addressed the relationship between the theory and practice of machine learning. <p> It may be given only positive examples, only negative examples, or both. The expectation is that the distribution over which the error will be measured is the same as the distribution from which the training examples were drawn <ref> [Val84, BEHW89] </ref>. 2. Learning by being taught. Here, a benevolent teacher provides pedagogical examples designed to lead the learner to the correct solution [RS88]. <p> Probably-approximately-correct learning In 1984, Leslie Valiant proposed an intuitively simple, and widely applicable definition for a type of concept learning that has come to be known as "PAC" or "probably approximately correct" learning <ref> [Val84] </ref>. <p> Example: The concept class of 2-CNF (conjunctions of clauses with at most two lit erals per clause) with n variables is learnable from examples using m &gt; 1 * 8n 3 + ln 1 labeled samples <ref> [Val84, BEHW89] </ref>. Example: The concept class of axis-parallel rectangles in a plane is learnable from examples using m &gt; 4 * ln 4 ffi labeled samples. Note that in this case the size of the concept to be learned is constant [BEHW89]. <p> Example: The concept class of axis-parallel rectangles in a plane is learnable from examples using m &gt; 4 * ln 4 ffi labeled samples. Note that in this case the size of the concept to be learned is constant [BEHW89]. Work by Valiant <ref> [Val84] </ref>, Judd [Jud88], and Blum and Rivest [BR89] has demonstrated that very few useful concept classes are actually polynomially learnable from examples. Even the problem of training a neural network with only one hidden layer is NP-complete. <p> In a membership query, the learner queries a point in the input domain and an oracle returns the classification of that point. Much work in formal learning theory has been directed to the study of queries (see e.g.: <ref> [Ang86, Val84, Ams88] </ref>), but only very recently have queries been examined with respect to their role in improving generalization behavior [CAL90, Eis91]. <p> In many problems, though, we can still make use of distribution information without having to pay the full cost of drawing and classifying an example. Rather than assuming that the drawing of a classified exam ple is an atomic operation (as in <ref> [Val84, BEHW89] </ref>), we may divide the operation into two steps: first, that of drawing an unclassified example from the distribution, and second, querying the classification of that point. <p> Angluin [Ang86] showed that while minimal finite state automata were not poly-nomially learnable (in the Valiant sense) from examples alone, they could be learned using a polynomial number of queries to an oracle that provides counter-examples. Valiant <ref> [Val84] </ref> considers various classes that are learnable using a variety of forms of directed learning. Amsterdam [Ams88] proposed the EXPERIMENT oracle discussed in Section 4.2.2, incorporating Valiant's notion of an EXAMPLE and MEMBER or acles.
Reference: [Vap82] <author> V. Vapnik. </author> <title> Estimation of Dependencies based on empirical data. </title> <publisher> Springer-Verlag, </publisher> <address> New York, </address> <year> 1982. </year>
Reference-contexts: In the past ten years, both formal research in computational learning theory and empirical research in machine learning systems has blossomed as powerful theories (such as those described in <ref> [Vap82, Val84, BEHW89] </ref>), and powerful learning algorithms (such as those in [RHW86, LBG80]), have come into widespread use. Unfortunately, very little work has addressed the relationship between the theory and practice of machine learning. <p> First, I examine the expected generalization of simple neural networks training on randomly drawn training data, and compare the results to the performance predicted by Vapnik and Chervonenkis <ref> [VC71, Vap82] </ref>, Haussler et al. [HKS91], and Schwartz 2 et al. [SSSD90]. I then consider a non-traditional learning problem: the design of a vector quan-tizer codebook. <p> I then consider a non-traditional learning problem: the design of a vector quan-tizer codebook. I derive formal bounds on the generalization performance of such a codebook as a function of the size of its training set (using methods from <ref> [Vap82] </ref>), and compare these bounds to the actual generalization seen using the Generalized Lloyd Algorithm, a popular codebook design technique. <p> We con 16 sider neural networks as concept learners and empirically measure their performance quantitatively and qualitatively against a number of theoretical predictions. We find that the worst-case bounds using the Vapnik-Chervonenkis (VC) dimension <ref> [Vap82] </ref> are too loose to provide useful guidance, but that a variation of this bound [HKS91], which requires the (unknown) assumption of Bayes optimality on the part of the learner, reflects the observed generalization performance of the neural networks very closely. <p> We adapt the theoretical results from <ref> [Vap82] </ref> to this problem and compare the derived theory to the empirical generalization performance we get when designing codebooks using the Generalized Lloyd Algorithm (GLA). <p> one of the single-layer networks, in an attempt to find the theoretically predicted "gap." Finally, the concluding section discusses the implications of our results, and possible directions for future work along these lines. 19 2.1.1 Worst-case behavior: Vapnik-Chervonenkis bounds Blumer et al. [BEHW89] demonstrated that the results in [VC71] and <ref> [Vap82] </ref> could be used to provide upper bounds on the generalization error of a concept in the PAC model. <p> Specifically, if m &gt; d=2, then with probability 1 ffi *(c; (t; P)) *(c; (t; P m )) + q @ 1 + 1 + q A ; (2.1) where q = 2 (d (ln d ffi ) <ref> [Vap82] </ref>. <p> However, this dependence does not appear until the training set size exceeds roughly 200, the same place where the shape of the generalization curve changes. 2.5 Comparison to Theory We examined how the observed generalization curves compared to the theoretical upper bounds described in <ref> [Vap82, BEHW89, HLW90, HKS91] </ref>. This comparison is illustrated in Figure 2.9. For the N -input, single-layer networks, the VC-dimension of the network is N + 1. <p> We then show how these bounds may be used to bound the difference in training and test performance of a VQ codebook. 48 3.2.1 Pattern classification and the VC-dimension The results in <ref> [BEHW89, Vap82, VC71] </ref> concern the asymptotic performance of learning systems. More specifically, these results bound the difference between the empirically observed performance of a system and its "true" performance as a function of the number of inputs over which the empirical performance was observed.
Reference: [VC71] <author> V. Vapnik and A. Chervonenkis. </author> <title> On the uniform convergence of relative frequencies of events to their probabilities. </title> <journal> Theory of Probability and its Applications, </journal> <volume> 16(2) </volume> <pages> 264-280, </pages> <year> 1971. </year>
Reference-contexts: First, I examine the expected generalization of simple neural networks training on randomly drawn training data, and compare the results to the performance predicted by Vapnik and Chervonenkis <ref> [VC71, Vap82] </ref>, Haussler et al. [HKS91], and Schwartz 2 et al. [SSSD90]. I then consider a non-traditional learning problem: the design of a vector quan-tizer codebook. <p> performance for one of the single-layer networks, in an attempt to find the theoretically predicted "gap." Finally, the concluding section discusses the implications of our results, and possible directions for future work along these lines. 19 2.1.1 Worst-case behavior: Vapnik-Chervonenkis bounds Blumer et al. [BEHW89] demonstrated that the results in <ref> [VC71] </ref> and [Vap82] could be used to provide upper bounds on the generalization error of a concept in the PAC model. <p> While it is relatively straightforward to define the VC-dimension, it is an alto gether different matter to determine the VC-dimension of an arbitrary concept class. For some classes, determining the VC-dimension is intuitively obvious, while for others, extensive mathematics are needed (see <ref> [VC71] </ref> for examples). Indeed, there does not seem to be any generalized algorithm for determining the VC-dimension of an arbitrary concept class. For feed-forward neural networks, the VC-dimension can be calculated exactly in the single-layer case, and can be bounded above and below in the multilayer case [BH89]. <p> We then show how these bounds may be used to bound the difference in training and test performance of a VQ codebook. 48 3.2.1 Pattern classification and the VC-dimension The results in <ref> [BEHW89, Vap82, VC71] </ref> concern the asymptotic performance of learning systems. More specifically, these results bound the difference between the empirically observed performance of a system and its "true" performance as a function of the number of inputs over which the empirical performance was observed.
Reference: [Wan90] <author> E. Wan. </author> <title> Neural network classification: A Bayesian interpretation. </title> <journal> IEEE Transactions on Neural Networks, </journal> <volume> 1(4), </volume> <year> 1990. </year>
Reference-contexts: The Bayes-optimal bound described in Haussler et al. [HLW90] argues that any Bayes-optimal learning procedure (with suitable priors) will have an upper bound on generalization error of * d=m. It has been shown that, under certain conditions discussed in <ref> [RRK + 90, Wan90] </ref>, a neural network classifier approximates a Bayes-optimal decision boundary, but only over its training set. <p> short dashed lines indicate the standard error of the mean for each training set size. data, is Bayes-optimal over the training inputs to the extent that the algorithm is able to find a global error minimum, and to the extent that the network is able to represent the Bayes-optimal function <ref> [RRK + 90, Wan90] </ref>. Without a "prior" over the set of functions learnable by our networks, however, nothing is known about the optimality of backpropagation on inputs outside of the training set, which is what we are interested in.
Reference: [WK91] <author> S. Weiss and C. </author> <title> Kulikowski. Computer Systems that Learn. </title> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, CA, </address> <year> 1991. </year>
Reference-contexts: We write this as *(c; (t; P m )) = m i=1 &lt; 0 h (x i ) = t (x i ) 1 otherwise. (3.2) Other more involved methods of estimating empirical error are discussed in <ref> [WK91] </ref>. Typically, our hypothesis is chosen according to some rule, such as "all points that are within Euclidean distance 1 of point z." This rule defines a concept class, C. <p> The only noticeable deviations from the polynomial model are for small training set sizes or when large codebooks are used. This is consistent with observations made in <ref> [WK91] </ref>, point ing out that when a learner is sufficiently powerful and a training set is sufficiently small, rather than learning to generalize, the learner simply memorizes data to some extent. Memorization is a qualitatively different phenomenon from generalization, and is beyond the scope of this study.
Reference: [Yam91] <author> K. Yamada. </author> <title> Learning of category boundaries based on inverse recall by multi-layer neural networks. </title> <booktitle> In Proceedings, IJCNN, </booktitle> <year> 1991. </year>
Reference-contexts: This approach shows promise for concept learning in cases with relatively compact, connected concepts, and has already produced impressive results on the power system static security problem. It is, however, susceptible to the pathology discussed in Section 4.3.1. A variation on this approach reported in <ref> [Yam91] </ref> selects training examples based on the sensitivity of a partially-trained network to perturbations of the pattern. This combines the advantage of using distribution information with the practicality demonstrated in [HCOM90].
Reference: [Zeh70] <author> P. Zehna. </author> <title> Probability Distributions and Statistics. </title> <publisher> Allyn and Bacon, </publisher> <address> Boston, </address> <year> 1970. </year> <pages> pp. 286, 289. </pages>
References-found: 68

