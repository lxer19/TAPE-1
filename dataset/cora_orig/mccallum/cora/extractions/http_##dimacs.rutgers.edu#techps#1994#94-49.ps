URL: http://dimacs.rutgers.edu/techps/1994/94-49.ps
Refering-URL: http://dimacs.rutgers.edu/TechnicalReports/1994.html
Root-URL: http://www.cs.rutgers.edu
Email: ferreira@lip.ens-lyon.fr  
Title: with hypercube multiprocessors  
Author: by Afonso Ferreira ;; CNRS-URA n ffi 
Address: 69364 Lyon Cedex 07 France  
Affiliation: CNRS LIP  ENS Lyon  
Note: Issues in parallel computing  DIMACS is a cooperative project of Rutgers University, Princeton University, AT&T Bell Laboratories and Bellcore. DIMACS is an NSF Science and Technology Center, funded under contract STC-91-19999; and also receives support from the New Jersey Commission on Science and Technology.  
Abstract: DIMACS Technical Report 94-49 October 1994 1 Visiting Member, partially supported by Dimacs NSF grant STC-91-199999 and the NJ Commis sion on Science and Technology. 2 Partially supported by the PRC PRS and Math-Info of the French CNRS. 3 A revised version of this report will appear in the Handbook of Parallel and Distributed Computing, to be published by McGraw-Hill. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> S. G. Akl. </author> <title> Parallel Sorting Algorithms. </title> <publisher> Academic Press, </publisher> <address> Orlando, FL, </address> <year> 1985. </year>
Reference-contexts: BPC permutation on H (d) is specified by a permutation vector p = [p d1 ; p d2 ; : : : ; p 0 ], where d p i d, such that [jp d1 j; jp d2 j; : : : ; jp 0 j] is a permutation of <ref> [0; 1; : : : ; d 1] </ref>. <p> Do in parallel BitonicSortffirst half of the list in ascending orderg BitonicSortfsecond half of the list in descending orderg BitonicMergefthe two listsg - 14 - For more details on both algorithms, such as proof of correctness and time complexity, we refer the interested reader to <ref> [1, 24] </ref>. In the case where there are more computing elements than data elements to be sorted, the sparse enumeration sort of [31] is able to sort N elements in a hypercube with N 1+1=k processors, 1 k log N , in time O (k log N ).
Reference: [2] <author> S.G. Akl and K. Lyons. </author> <title> Parallel Computational Geometry. </title> <publisher> Prentice Hall, </publisher> <year> 1993. </year>
Reference-contexts: For instance, in Section 4 we saw several time-optimal dense algorithms. The classic problems we are going to address in this Section have serial time complexity less or equal to that of sorting. For further algorithms see, e.g., <ref> [2, 28] </ref>. Efficient O (log 2 N ) time dense algorithms were introduced in [35] for several geometric problems like convex hull, ECDF search, and all points closest neighbors. <p> overall time complexity of O ((k log N k + (log log N ) 2 ) log N ) to answer m queries simultaneously. 5.2.2 Summary The problems that could be solved through the use of the paradigms cited above are as follows. (For more details on the algorithms, see <ref> [2, 28] </ref>.) * trapezoidal decomposition and triangulation of polygons of size N in time O (log 2 N ) with a hypercube of size N log N [15]. * parallel branch and bound on hypercube multiprocessors [12]. * multiple stabbing of a simple polygonal path, i.e., determining all k intersections of
Reference: [3] <author> M. J. Atallah and D. Z. Chen. </author> <title> Optimal parallel hypercube algorithms for polygon problems. </title> <journal> IEEE Transaction on Computers, </journal> <note> to appear. </note>
Reference: [4] <author> M. J. Atallah and A. Fabri. </author> <title> On the multisearching problem for hypercubes. </title> <booktitle> In Proc. 6th Parallel Architectures and Languages Europe, Lecture Notes in Computer Science. </booktitle> <publisher> Springer Verlag, </publisher> <year> 1994. </year> <note> To appear in Computational Geometry Theory and Applications. </note>
Reference-contexts: Recently, it was shown that the M-way search paradigm can be used to solve some query problems even faster. In the conference version of <ref> [4] </ref> an O (log N (log log N ) 3 ) time algorithm is described to solve the multiple point location in a slab, and in the journal version of this paper, it is shown how to apply the same techniques to solve the trapezoidal decomposition in time O (log N
Reference: [5] <author> P. Berthome and A. Ferreira. </author> <title> Efficiently solving geometric problems on large hypercube multiprocessors. </title> <editor> In S. Tzafestas, P. Borne, and L. Grandinetti, editors, </editor> <booktitle> Parallel and Distributed Computing in Engineering Systems, </booktitle> <pages> pages 123-128. </pages> <publisher> IMACS North Holland, </publisher> <year> 1992. </year> <note> To appear in Parallel Algorithms and Applications. </note>
Reference-contexts: Such search operations can be accomplished in logarithmic time using the standard operations described in Section 4. 5.1.2 Time-optimal sparse algorithms A different approach for designing time-optimal sparse algorithms can be found in <ref> [5] </ref>, where ECDF search, all points closest neighbors, and convex hull are solved in time O (k log N ) with N 1+1=k processors, 1 k log N .
Reference: [6] <author> P. Berthome, A. Ferreira, S. Perennes, G. Plaxton, and B. Maggs. </author> <title> Sorting-based selection algorithms on hypercubic networks. </title> <booktitle> In Proceedings of the 7th IEEE International Parallel Processing Symposium, </booktitle> <pages> pages 89-95. </pages> <publisher> IEEE Press, </publisher> <year> 1993. </year>
Reference-contexts: The problem is the exponential complexity of the algorithm building such tables. This is the reason why we preferred to list here the time complexity of the uniform version. Finally, the sparse version of share sort was studied in <ref> [6] </ref>. <p> The first two values are computed recursively. Selection A sorting based O (log N log fl N ) time dense algorithm 9 for selecting the k-th smallest element out of N unsorted items was given in <ref> [6] </ref>. The algorithm is a succession of phases where finer approximations for the k-th smallest element are obtained, so that the correct element is eventually computed. Each such approximate selection phase is identical to the others: sort sublists, then choose splitters as the new sublist.
Reference: [7] <author> L. N. Bhuyan and D. P. Agrawal. </author> <title> Generalized hypercube and hyperbus structures for a computer network. </title> <journal> IEEE Transactions on Computers, </journal> <volume> 4(C-33):323-334, </volume> <year> 1984. </year> <month> - 24 </month> - 
Reference-contexts: For oblivious algorithms the routing path of a packet is uniquely determined from its source and final destination. Both the commonly used greedy algorithm, which uniformly corrects hypercube dimensions in either increasing or decreasing order, and also a variant which corrects dimensions in random order belong to this class <ref> [7, 37] </ref>. Although these strategies are simple, their worst-case time complexity is ( p N ) [39].
Reference: [8] <author> B. Chaselle and L. J. Guibas. Fractional cascading: I. </author> <title> A data structuring technique. </title> <journal> Algorithmica, </journal> <volume> 1(2) </volume> <pages> 133-162, </pages> <year> 1986. </year>
Reference-contexts: The solution shown in [14] presents a slowdown factor of only O (log N ) with respect to the sequential version, where just one query line is answered. With fractional cascading <ref> [8] </ref>, the serial solution of the multiple stabbing problem is based on the following observation: a line l intersects a simple polygonal path P if and only if l intersects the convex hull, CH (P ), of P . <p> Let T 0 (l) be the subtree of T that shall be traversed in the sequential solution with fractional cascading for answering the query associated with l. In <ref> [8] </ref> it was shown that jT 0 (l)j = O (k log s k ), where k is the number of intersections between l and P .
Reference: [9] <author> E. Cohen, R. Miller, E.M. Sarraf, and Q. Stout. </author> <title> Efficient convexity and domination algorithms for fine- and medium-grain hypercube computers. </title> <journal> Algorithmica, </journal> <volume> 7 </volume> <pages> 51-75, </pages> <year> 1992. </year>
Reference-contexts: This is the case of the solutions for the convex hull algorithm from [29], triangulation and visibility from a point in [26], and the algorithms for maximal points and smallest enclosing box in <ref> [9] </ref>, that we describe in the following. Convex hull The algorithm for convex hull starts by sorting the points by x coordinates. The points are then divided into N 1=4 groups of size N 3=4 , to be stored in consecutive subhypercubes. For each group the solution is computed recursively.
Reference: [10] <author> M. Cosnard and A. Ferreira. </author> <title> On the real power of loosely coupled parallel architectures. </title> <journal> Parallel Processing Letters, </journal> <volume> 1(2) </volume> <pages> 103-111, </pages> <year> 1991. </year>
Reference-contexts: can imagine that the memory positions m 1 ; m 2 ; : : : ; m n are such that P E i can only access memory positions m j , where j = i, or the vertices v j and v i of V are neighbors in G <ref> [10] </ref>. In the remainder of this report, we shall try to answer most of the questions that a non-specialist reader may have. However, some areas of "hypercube computing" will not be covered here.
Reference: [11] <author> R. E. Cypher and C. G. Plaxton. </author> <title> Deterministic sorting in nearly logarithmic time on the hypercube and related computers. </title> <booktitle> In Proceedings of the 22nd Annual ACM Symposium on Theory of Computing, </booktitle> <pages> pages 193-203, </pages> <month> May </month> <year> 1990. </year>
Reference-contexts: Moreover, it is very simple to describe and implement, with a fairly small constant (approximately 1=2) hidden in its time complexity of O (log 2 N ). Nevertheless, the asymptotical time complexity of the bitonic sort was finally improved by the share sort algorithm proposed in <ref> [11] </ref>, that has another version described in [25]. To the best of our knowledge, the share sort is the asymptotically fastest existing sorting algorithm for sorting N data elements with N hypercube processors, in O (log N (log log N ) 2 ) time 8 . <p> For further algorithms see, e.g., [2, 28]. Efficient O (log 2 N ) time dense algorithms were introduced in [35] for several geometric problems like convex hull, ECDF search, and all points closest neighbors. Notice that, before the advent of the share sort <ref> [11] </ref>, such algorithms matched the best known bound for sorting on a hypercube.
Reference: [12] <author> F. Dehne, A. Ferreira, and A. Rau-Chaplin. </author> <title> Parallel branch and bound on fine grained hypercube multiprocessors. </title> <booktitle> Parallel Computing, </booktitle> <address> 15(1-3):201-209, </address> <year> 1990. </year>
Reference-contexts: A powerful extension to this technique was proposed in [14]. Hypercube cascading is based on the dynamic M-way search in <ref> [12] </ref>, which is in turn an extension of the M-way search that copes with dynamic structures. With hypercube cascading, most data structures represented by directed acyclic graphs can be traversed by concurrent queries that synchronously advance in their paths, step by step. <p> the paradigms cited above are as follows. (For more details on the algorithms, see [2, 28].) * trapezoidal decomposition and triangulation of polygons of size N in time O (log 2 N ) with a hypercube of size N log N [15]. * parallel branch and bound on hypercube multiprocessors <ref> [12] </ref>. * multiple stabbing of a simple polygonal path, i.e., determining all k intersections of m lines with a simple polygonal path of length l, in time O (k log N=k+(log log N ) 2 ) log N ) with a hypercube of size N , where N = maxfl; mg
Reference: [13] <author> F. Dehne, A. Ferreira, and A. Rau-Chaplin. </author> <title> Efficient parallel construction and manipulation of pointer based quadtrees. </title> <booktitle> In Proceedings of the International Conference on Parallel Processing, </booktitle> <pages> pages 255-262, </pages> <address> St. Charles (Ill), </address> <year> 1991. </year>
Reference-contexts: This can be done in time O (k log N=k + (log log N ) 2 ) log N ) with a hypercube of size N , where N = maxfl; mg [14]. * parallel processing of pointer-based quadtrees of height h in hypercube multiprocessors of size N <ref> [13] </ref>.
Reference: [14] <author> F. Dehne, A. Ferreira, and A. Rau-Chaplin. </author> <title> Parallel fractional cascading on hypercube multiprocessors. </title> <journal> Computational Geometry Theory and Applications, </journal> <volume> 2 </volume> <pages> 141-167, </pages> <year> 1992. </year>
Reference-contexts: A powerful extension to this technique was proposed in <ref> [14] </ref>. Hypercube cascading is based on the dynamic M-way search in [12], which is in turn an extension of the M-way search that copes with dynamic structures. <p> we have a bounded degree monotone data structure of size s, then we can answer m iterative search queries whose search paths are no longer than h, in time O ((h + (log N log N ) 2 ) log N ) in hypercubes with N = maxfs; mg processors <ref> [14] </ref>. The only condition imposed on the data structure so that this technique applies is that it should be monotone, as follows (see figure 8). <p> In the following we briefly describe an application of hypercube cascading. 5.2.1 Solving the multiple stabbing problem This problem consists of determining all intersections of m lines with a simple polygonal path of size s (see figure 9). The solution shown in <ref> [14] </ref> presents a slowdown factor of only O (log N ) with respect to the sequential version, where just one query line is answered. <p> * multiple stabbing of a simple polygonal path, i.e., determining all k intersections of m lines with a simple polygonal path of length l, in time O (k log N=k+(log log N ) 2 ) log N ) with a hypercube of size N , where N = maxfl; mg <ref> [14] </ref>. - 22 - * multiple slanted range search. Let S be a set of l points in the plane. An aligned trapezoid is a trapezoid with one side on the x-axis and the two adjacent sides parallel to the y-axis. <p> This can be done in time O (k log N=k + (log log N ) 2 ) log N ) with a hypercube of size N , where N = maxfl; mg <ref> [14] </ref>. * parallel processing of pointer-based quadtrees of height h in hypercube multiprocessors of size N [13].
Reference: [15] <author> F. Dehne and A. Rau-Chaplin. </author> <title> Implementing data structures on a hypercube multiprocessor and applications in parallel computational geometry. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 8(4) </volume> <pages> 367-375, </pages> <year> 1990. </year>
Reference-contexts: One of the first tools for elegant and theoretically efficient use of data structures in hypercubes was the Multi-Way Search (M-way search, for short) proposed in <ref> [15] </ref>. With the M-way search, several queries | whose maximum search path size is h |, can traverse tree-like data structures simultaneously, with an overhead of only O (log N ) with respect to CREW P RAM algorithms. <p> problems that could be solved through the use of the paradigms cited above are as follows. (For more details on the algorithms, see [2, 28].) * trapezoidal decomposition and triangulation of polygons of size N in time O (log 2 N ) with a hypercube of size N log N <ref> [15] </ref>. * parallel branch and bound on hypercube multiprocessors [12]. * multiple stabbing of a simple polygonal path, i.e., determining all k intersections of m lines with a simple polygonal path of length l, in time O (k log N=k+(log log N ) 2 ) log N ) with a hypercube
Reference: [16] <author> A. Ferreira and M. Grammatikakis. </author> <title> Improved probabilistic routing in generalized hy-percubes. </title> <booktitle> In Proc. 6th Parallel Architectures and Languages Europe, Lecture Notes in Computer Science. </booktitle> <publisher> Springer Verlag, </publisher> <year> 1994. </year>
Reference-contexts: Notice that, at the end of the first phase, packets are distributed randomly. The role of randomization is to reduce the gap between the average and the worst-case time complexities by avoiding worst-case congestions. In the context of generalized hypercubes 7 , this result has been slightly improved in <ref> [16] </ref>. Adaptive routing algorithms can use much more information about the messages and the system congestion, like the destination of many packets for instance. They are usually disguised as sorting algorithms.
Reference: [17] <author> A. Ferreira and J. Peters. </author> <title> Finding the smallest path in a rectilinear polygon on a hypercube multiprocessor. </title> <booktitle> In Proceedings of the Third Canadian Conference on Computational Geometry, </booktitle> <pages> pages 162-165, </pages> <address> Vancouver (Ca), </address> <year> 1991. </year>
Reference-contexts: The best combination of the sorting algorithms yields h = O (log fl N ). Polygons Finding shortest paths in rectilinear polygons was solved in time O (T t + log N ) with P t processors in <ref> [17] </ref>, where T t and P t are, respectively, the time and the number of processors required for solving trapezoidal decomposition (see Section 5.2).
Reference: [18] <author> P. Fraigniaud and E. Lazard. </author> <title> Methods and problems of communication in usual networks. </title> <note> Discrete Applied Mathematics (special issue on broadcasting), (to appear). </note>
Reference-contexts: This is the constant model [20]. Another important parameter for routing problems is the concurrency of communication - 7 - links, i.e., the number of links that can be used concurrently by each processor <ref> [18] </ref>. Algorithms that, in a single step, allow communication with only one neighbor are said to respect the 1-port model. If, on the other hand, communication with all the d neighbors is allowed, then they are said to respect the d-port model. <p> The spanning tree used is the Spanning Binomial Tree (see figure 6), and the arc-disjoint spanning tree is the Rotated Spanning Binomial Tree (see <ref> [18, 19, 34, and references therein] </ref> for more details). Recall that, because of the vertex symmetry of the hypercube discussed in Section 2.1, one needs to find only one spanning tree, rooted in any node, say i.
Reference: [19] <author> M. D. Grammatikakis, D. F. Hsu, and F. K. Hwang. </author> <title> Adaptive and oblivious routing on the d-cube. </title> <booktitle> In Proceedings of the 4th Annual International Symposium on Algorithms and Computation, to appear in Lecture Notes in Computer Science. </booktitle> <publisher> Springer-Verlag, </publisher> <year> 1994. </year>
Reference-contexts: The spanning tree used is the Spanning Binomial Tree (see figure 6), and the arc-disjoint spanning tree is the Rotated Spanning Binomial Tree (see <ref> [18, 19, 34, and references therein] </ref> for more details). Recall that, because of the vertex symmetry of the hypercube discussed in Section 2.1, one needs to find only one spanning tree, rooted in any node, say i. <p> An optimal oblivious algorithm, based on many to one and one to many routing in subhypercubes, achieves a worst-case time delay of fi ( p Even though optimal deterministic oblivious routing schemes on the N -node binary hypercube, could be constructed for N 64 <ref> [19] </ref>, of major importance is the randomized permutation routing algorithm, first proposed by Valiant and Brebner [39], and later improved by Valiant [38].
Reference: [20] <author> M. D. Grammatikakis, D. F. Hsu, and M. Kraetzl. </author> <title> Multicomputer routing. </title> <type> Technical Report 94-5, </type> <institution> School of Mathematics and Statistics, Curtin University of Technology, </institution> <address> Perth, Australia, </address> <year> 1994. </year> <month> - 25 </month> - 
Reference-contexts: Then, it could be assumed that a sending-to-neighbor operation takes constant time, regardless of the size of the message to be sent. This is the constant model <ref> [20] </ref>. Another important parameter for routing problems is the concurrency of communication - 7 - links, i.e., the number of links that can be used concurrently by each processor [18]. Algorithms that, in a single step, allow communication with only one neighbor are said to respect the 1-port model.
Reference: [21] <author> C.-T. Ho and S. L. Johnson. </author> <title> Optimum broadcasting and personalized communication in hypercubes. </title> <journal> IEEE Transactions on Computers, </journal> <volume> 38(9) </volume> <pages> 1249-1268, </pages> <year> 1989. </year>
Reference-contexts: By pipelining packets through different arc-disjoint spanning trees, the following can be shown. * Under the 1-port model, broadcasting completes in ( p p dfi) 2 steps <ref> [21] </ref>. * Under the d-port model, broadcasting completes in ( q d + (d 1)fi) 2 steps [36].
Reference: [22] <author> C. Kaklamanis, D. Krizanc, and T. Tsantilas. </author> <title> Tight bounds for oblivious routing on the hypercube. </title> <booktitle> In Proceedings of the 2nd Annual ACM Symposium of Parallel Algorithms and Architectures, </booktitle> <pages> pages 31-36, </pages> <year> 1990. </year>
Reference: [23] <author> R. M. Karp and V. Ramachandran. </author> <title> A survey of parallel algorithms for shared-memory machines. </title> <editor> In J. van Leeuwen, editor, </editor> <booktitle> Handbook of Theoretical Computer Science, Volume A: Algorithms and Complexity, </booktitle> <pages> pages 869-941. </pages> <publisher> Elsevier/MIT Press, </publisher> <year> 1990. </year>
Reference-contexts: Conflicts among processors arise when some or all of them try to access the same cell of the shared memory, and the different rules used to resolve such conflicts define the different models of P RAM s <ref> [23] </ref>. Unfortunately, implementing a shared memory in a parallel computer becomes infeasible when the number of processors grows, due to the complexity of the hardware required to allow all processors to have access to every memory cell. Thus, in large-scale parallel systems, the memory is distributed among the processors.
Reference: [24] <author> D. E. Knuth. </author> <booktitle> The Art of Computer Programming, </booktitle> <volume> volume 3. </volume> <publisher> Addison-Wesley, </publisher> <address> Reading, MA, </address> <year> 1973. </year>
Reference-contexts: In that case, bit positions 3 and 4 are considered as fixed in each of the copies. It is interesting to notice that one can use these decompositions in order to implement divide & conquer algorithms <ref> [24] </ref> in hypercubes. Moreover, they yield a recursive construction of the hypercube, as follows. A 0-dimensional hypercube is composed of one node only. In order to build H (1), we take two copies of H (0) and join the two nodes by an edge. <p> Do in parallel BitonicSortffirst half of the list in ascending orderg BitonicSortfsecond half of the list in descending orderg BitonicMergefthe two listsg - 14 - For more details on both algorithms, such as proof of correctness and time complexity, we refer the interested reader to <ref> [1, 24] </ref>. In the case where there are more computing elements than data elements to be sorted, the sparse enumeration sort of [31] is able to sort N elements in a hypercube with N 1+1=k processors, 1 k log N , in time O (k log N ).
Reference: [25] <author> F. T. Leighton. </author> <title> Introduction to Parallel Algorithms and Architectures: Arrays, Trees and Hypercubes. </title> <publisher> Morgan-Kaufmann, </publisher> <address> San Mateo, CA, </address> <year> 1991. </year>
Reference-contexts: every pair of edges (v i ; v j ) and (u r ; u s ) of H (d), one can find an automorphism 2 f of H (d) such that (f (v i ); f (v j )) = (u r ; u s ) (see figure 3) <ref> [25, 33] </ref>. Very informally, we could say that, as far as the neighbors are concerned, the hypercube looks the same from every node. 2.2 Embedding Another attractive characteristic of this architecture is that it has a large number of other architectures as its subgraphs. <p> It is known how to embed them in H (h + 2) with dilation 1, although it is impossible to find such an embedding in H (h + 1), its optimal hypercube <ref> [25, 34] </ref>. On the other hand, a slightly different tree, namely the double rooted complete binary tree DRT (h), with 2 h+1 nodes, is a subgraph of H (h + 1) (see figure 5). <p> Several more results can be found in the literature, related to the embedding of different topologies in the hypercube, such as meshes, meshes of trees, pyramids, and others. For a thorough discussion on embedding in general, we refer the reader to <ref> [25, 34] </ref>. 5 A graph is said to be Hamiltonian if it has a Hamiltonian cycle, i.e., there is a path containing all vertices exactly once, such that the last vertex in the path is a neighbor of the first. - 6 - 3 Communication issues In parallel applications, a large <p> Algorithms where at each time step, only the edges associated with a single dimension are used, and consecutive dimensions are used on consecutive steps, are called normal algorithms <ref> [25, Section 3.1.4] </ref>. From the architectural point of view, one of the main drawbacks of the hypercube stems from its logarithmic degree, due to the large number of wires required to interconnect the processors in big systems. <p> Any normal algorithm designed for the hypercube can be simulated in any of these networks with only a constant slowdown. The most studied members of this class, besides the hypercube itself, are the butterfly, the shu*e-exchange, and the de Bruijn graphs (see <ref> [25] </ref> for details on such interconnection networks). <p> Nevertheless, the asymptotical time complexity of the bitonic sort was finally improved by the share sort algorithm proposed in [11], that has another version described in <ref> [25] </ref>. To the best of our knowledge, the share sort is the asymptotically fastest existing sorting algorithm for sorting N data elements with N hypercube processors, in O (log N (log log N ) 2 ) time 8 .
Reference: [26] <author> P.D. MacKenzie and Q.F. Stout. </author> <title> Asymptotically efficient hypercube algorithms for computational geometry. </title> <booktitle> In Proc. 3rd Symposium Frontiers of Massively Parallel Computation, </booktitle> <pages> pages 8-11, </pages> <year> 1990. </year>
Reference-contexts: On the other hand, and in view of the time/processor tradeoff obtained by the sparse share sort, these are also time-optimal sparse algorithms. This is the case of the solutions for the convex hull algorithm from [29], triangulation and visibility from a point in <ref> [26] </ref>, and the algorithms for maximal points and smallest enclosing box in [9], that we describe in the following. Convex hull The algorithm for convex hull starts by sorting the points by x coordinates. <p> are computed through the bitonic merge, and the extra dimensions are used to avoid collisions during the routing steps, ensuring that no more than O (log N ) time is required in each one of the k steps. 5.1.3 Suboptimal dense algorithms ECDF search Other dense algorithms were proposed in <ref> [26] </ref>. They solve ECDF search, all points closest neighbors, and 3-dimensional maxima in time O (T s log log N ). The idea is again to recursively solve problems whose size is a root function of the original one.
Reference: [27] <author> E. W. Mayr and R. Werchner. </author> <title> Optimal tree contraction on the hypercube and related networks. </title> <editor> In J. Flanagan, Y. Matias, and V. Ramachandran, editors, </editor> <title> DIMACS Workshop on Parallel Algorithms: from solving combinatorial problems to solving grand challenge problems, </title> <type> page 36, </type> <month> November </month> <year> 1993. </year>
Reference: [28] <author> R. Miller, A. Rau-Chaplin, and Q. Stout. </author> <title> Parallel Algorithms for Regular Architectures. </title> <publisher> MIT Press, </publisher> <address> Cambridge, MA, </address> <note> to appear. </note>
Reference-contexts: For instance, in Section 4 we saw several time-optimal dense algorithms. The classic problems we are going to address in this Section have serial time complexity less or equal to that of sorting. For further algorithms see, e.g., <ref> [2, 28] </ref>. Efficient O (log 2 N ) time dense algorithms were introduced in [35] for several geometric problems like convex hull, ECDF search, and all points closest neighbors. <p> overall time complexity of O ((k log N k + (log log N ) 2 ) log N ) to answer m queries simultaneously. 5.2.2 Summary The problems that could be solved through the use of the paradigms cited above are as follows. (For more details on the algorithms, see <ref> [2, 28] </ref>.) * trapezoidal decomposition and triangulation of polygons of size N in time O (log 2 N ) with a hypercube of size N log N [15]. * parallel branch and bound on hypercube multiprocessors [12]. * multiple stabbing of a simple polygonal path, i.e., determining all k intersections of
Reference: [29] <author> R. Miller and Q. Stout. </author> <title> Efficient parallel convex hull algorithms. </title> <journal> IEEE Transactions on Computers, </journal> <volume> 37(12) </volume> <pages> 1605-1618, </pages> <year> 1988. </year>
Reference-contexts: On the other hand, and in view of the time/processor tradeoff obtained by the sparse share sort, these are also time-optimal sparse algorithms. This is the case of the solutions for the convex hull algorithm from <ref> [29] </ref>, triangulation and visibility from a point in [26], and the algorithms for maximal points and smallest enclosing box in [9], that we describe in the following. Convex hull The algorithm for convex hull starts by sorting the points by x coordinates.
Reference: [30] <author> D. Nassimi and S. Sahni. </author> <title> Data broadcasting in SIMD computers. </title> <journal> IEEE Transactions on Computers, </journal> <volume> C-30:101-107, </volume> <year> 1981. </year>
Reference-contexts: For the sake of clarity, we shall show the detailed implementation of some of these procedures. They all use the same strategy, correcting the bits in consecutive dimensions either from right to left, or from left to right. The interested reader is referred to <ref> [30, 33] </ref> for more details. 4.1 Semigroup operations A semigroup computation applies an associative operation to all selected data items. The procedures that follow are computed in optimal time, i.e., they complete in O (log N ) time. Procedure Count. Input: One flag per processor. <p> takes two registers per P E i , block (i) and r (i), as input, and that computes L b e j=b b r (j), where L is an associative operation. 4.2 Data-movement operations The procedures that follow can be seen as special cases of routing, and were studied in <ref> [30] </ref>. Again, they are all time-optimal. Procedure Concentrate. Input: One register per processor, associated to a flag. Output: All registers whose flag was up are concentrated at the beginning of the hypercube, one per processor. The initial relative order remains unchanged. <p> Let ID (i)=i be used to keep track of the reading processors. Suppose further that each processor shall work with tuples containing four variables, namely, A (), ID (), D (), and S (). The algorithm implementing random access read is as follows <ref> [30] </ref>. 1. Sort tuples by A (). All valid addresses are now concentrated at the beginning of the hypercube. Let S () take the value of the current address. 2. IdentifyEndOfBlock. Since we are implementing concurrent read, several processors may have the same value in A ().
Reference: [31] <author> D. Nassimi and S. Sahni. </author> <title> Parallel permutation and sorting algorithms and a new generalized connection network. </title> <journal> J. of the ACM, </journal> <volume> 29 </volume> <pages> 642-667, </pages> <year> 1982. </year>
Reference-contexts: In the case where there are more computing elements than data elements to be sorted, the sparse enumeration sort of <ref> [31] </ref> is able to sort N elements in a hypercube with N 1+1=k processors, 1 k log N , in time O (k log N ). It is worth mentioning that the bitonic sort remained the best sorting algorithm for hypercubes for more than 20 years.
Reference: [32] <author> J.G. Peters and M. Syska. </author> <title> Circuit-switched broadcasting in torus networks. </title> <type> Tech. Rep. TR 93-4, </type> <institution> School of Computing Science, Simon Fraser Univ., </institution> <year> 1993. </year>
Reference-contexts: Nevertheless, some results exist concerning specific topologies. 2 An automorphism of H (d) is a bijection from V onto V which induces a bijection from E onto E. 3 A 2 p fi 2 q torus is a 2 p fi 2 q mesh with wraparound connections <ref> [32] </ref>. 4 An embedding of a graph G 1 (V 1 ; E 1 ) into a graph G 2 (V 2 ; E 2 ), jV 2 j jV 1 j, is an injective mapping f : V 1 ! V 2 , along with a mapping g : E <p> On the other hand, under the d-port model, the lower bound for this kind of problems is usually blog d+1 N c = (log N= log log N ). For optimal broadcasting, for instance, a tiling of the hypercube, such as the tiling of the 2-dimensional torus proposed in <ref> [32] </ref>, has to be found. 4 Useful algorithmic tools This Section and the next focus on synchronous algorithms for fine-grain hypercubes.
Reference: [33] <author> S. Ranka and S. Sahni. </author> <title> Hypercube Algorithms, with applications to image processing and pattern recognition. </title> <publisher> Springer-Verlag, </publisher> <year> 1990. </year>
Reference-contexts: every pair of edges (v i ; v j ) and (u r ; u s ) of H (d), one can find an automorphism 2 f of H (d) such that (f (v i ); f (v j )) = (u r ; u s ) (see figure 3) <ref> [25, 33] </ref>. Very informally, we could say that, as far as the neighbors are concerned, the hypercube looks the same from every node. 2.2 Embedding Another attractive characteristic of this architecture is that it has a large number of other architectures as its subgraphs. <p> Notice that we need queues of size two in order to store data that are to be routed along the next routing dimension <ref> [33] </ref>. 3.1.2 Linear model Under the linear model, the idea is to use pipelining to hide the diameter of the hypercube. For instance, the technique for broadcasting is based on the existence and construction of height h spanning trees 6 of the hypercube (see figure 6). <p> For the sake of clarity, we shall show the detailed implementation of some of these procedures. They all use the same strategy, correcting the bits in consecutive dimensions either from right to left, or from left to right. The interested reader is referred to <ref> [30, 33] </ref> for more details. 4.1 Semigroup operations A semigroup computation applies an associative operation to all selected data items. The procedures that follow are computed in optimal time, i.e., they complete in O (log N ) time. Procedure Count. Input: One flag per processor.
Reference: [34] <author> J. Rumeur. </author> <title> Communications dans les Reseaux d'Interconnexions. </title> <publisher> Masson, </publisher> <address> Paris, </address> <year> 1994. </year> <month> - 26 </month> - 
Reference-contexts: In case the hypercube is the smallest one such that N jV j, it is called optimal hypercube with regard to G. The general case of deciding whether a given graph is a subgraph of the hypercube is NP-Complete <ref> [34] </ref>. <p> It is known how to embed them in H (h + 2) with dilation 1, although it is impossible to find such an embedding in H (h + 1), its optimal hypercube <ref> [25, 34] </ref>. On the other hand, a slightly different tree, namely the double rooted complete binary tree DRT (h), with 2 h+1 nodes, is a subgraph of H (h + 1) (see figure 5). <p> Several more results can be found in the literature, related to the embedding of different topologies in the hypercube, such as meshes, meshes of trees, pyramids, and others. For a thorough discussion on embedding in general, we refer the reader to <ref> [25, 34] </ref>. 5 A graph is said to be Hamiltonian if it has a Hamiltonian cycle, i.e., there is a path containing all vertices exactly once, such that the last vertex in the path is a neighbor of the first. - 6 - 3 Communication issues In parallel applications, a large <p> The spanning tree used is the Spanning Binomial Tree (see figure 6), and the arc-disjoint spanning tree is the Rotated Spanning Binomial Tree (see <ref> [18, 19, 34, and references therein] </ref> for more details). Recall that, because of the vertex symmetry of the hypercube discussed in Section 2.1, one needs to find only one spanning tree, rooted in any node, say i.
Reference: [35] <author> I. Stojmenovic. </author> <title> Computational geometry on a hypercube. </title> <booktitle> In Proceedings of the International Conference on Parallel Processing, Vol. III Algorithms and Applications, </booktitle> <pages> pages 100-103, </pages> <address> St. Charles (Ill), </address> <year> 1988. </year>
Reference-contexts: The classic problems we are going to address in this Section have serial time complexity less or equal to that of sorting. For further algorithms see, e.g., [2, 28]. Efficient O (log 2 N ) time dense algorithms were introduced in <ref> [35] </ref> for several geometric problems like convex hull, ECDF search, and all points closest neighbors. Notice that, before the advent of the share sort [11], such algorithms matched the best known bound for sorting on a hypercube.
Reference: [36] <author> Q. F. Stout and B. Wagar. </author> <title> Intensive hypercube communication: prearranged communication in link-bound machines. </title> <journal> Journal of Parallel and Distributed Computing, </journal> (10):167-181, 1990. 
Reference-contexts: By pipelining packets through different arc-disjoint spanning trees, the following can be shown. * Under the 1-port model, broadcasting completes in ( p p dfi) 2 steps [21]. * Under the d-port model, broadcasting completes in ( q d + (d 1)fi) 2 steps <ref> [36] </ref>. The spanning tree used is the Spanning Binomial Tree (see figure 6), and the arc-disjoint spanning tree is the Rotated Spanning Binomial Tree (see [18, 19, 34, and references therein] for more details).
Reference: [37] <author> T. Szymanski. </author> <title> On the permutation capability of a circuit-switched hypercube. </title> <booktitle> In Proceedings of the IEEE International Conference on Parallel Processing, </booktitle> <volume> Vol. I, </volume> <pages> pages 103-110, </pages> <year> 1989. </year>
Reference-contexts: For oblivious algorithms the routing path of a packet is uniquely determined from its source and final destination. Both the commonly used greedy algorithm, which uniformly corrects hypercube dimensions in either increasing or decreasing order, and also a variant which corrects dimensions in random order belong to this class <ref> [7, 37] </ref>. Although these strategies are simple, their worst-case time complexity is ( p N ) [39].
Reference: [38] <author> L. G. Valiant. </author> <title> A scheme for fast parallel communication. </title> <journal> SIAM J. of Computing, </journal> <volume> 2(11) </volume> <pages> 350-361, </pages> <year> 1985. </year>
Reference-contexts: achieves a worst-case time delay of fi ( p Even though optimal deterministic oblivious routing schemes on the N -node binary hypercube, could be constructed for N 64 [19], of major importance is the randomized permutation routing algorithm, first proposed by Valiant and Brebner [39], and later improved by Valiant <ref> [38] </ref>. For this algorithm, the probability that all packets have been routed correctly within c log N time is proved greater than (1 e ffi log N ), where ffi is a constant depending only on c.
Reference: [39] <author> L. G. Valiant and G. J. Brebner. </author> <title> Universal schemes for parallel communication. </title> <booktitle> In Proceedings of the 13th Annual ACM Symposium on Theory of Computing, </booktitle> <pages> pages 263-277, </pages> <month> May </month> <year> 1981. </year>
Reference-contexts: Both the commonly used greedy algorithm, which uniformly corrects hypercube dimensions in either increasing or decreasing order, and also a variant which corrects dimensions in random order belong to this class [7, 37]. Although these strategies are simple, their worst-case time complexity is ( p N ) <ref> [39] </ref>. <p> one to many routing in subhypercubes, achieves a worst-case time delay of fi ( p Even though optimal deterministic oblivious routing schemes on the N -node binary hypercube, could be constructed for N 64 [19], of major importance is the randomized permutation routing algorithm, first proposed by Valiant and Brebner <ref> [39] </ref>, and later improved by Valiant [38]. For this algorithm, the probability that all packets have been routed correctly within c log N time is proved greater than (1 e ffi log N ), where ffi is a constant depending only on c.
Reference: [40] <author> C. A. Wang and Y. H. Tsin. </author> <title> An O(log n) time parallel algorithm for triangulating a set of points in the plane. </title> <journal> Information Processing Letters, </journal> <volume> 25(1) </volume> <pages> 55-60, </pages> <year> 1987. </year>
Reference-contexts: Once the points are sorted, the time complexity is given by T (N ) = T (N 3=4 ) + O (log N ) = O (log N ). Triangulation The solution for triangulation is based on the PRAM algorithm from <ref> [40] </ref>. First the points are sorted by x coordinates, and the partial solutions, of size N 3=4 each, are found recursively. The triangulation of the partial solutions uses the supporting lines between the convex hulls of each set.
References-found: 40

