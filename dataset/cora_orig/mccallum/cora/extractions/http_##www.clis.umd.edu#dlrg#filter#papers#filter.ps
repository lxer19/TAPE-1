URL: http://www.clis.umd.edu/dlrg/filter/papers/filter.ps
Refering-URL: http://www.umiacs.umd.edu/research/CLIP/filter2.html
Root-URL: 
Email: foardjmarchg@glue.umd.edu  
Title: A Conceptual Framework for Text Filtering  
Author: Douglas W. Oard Gary Marchionini 
Address: College Park, MD 20742  
Affiliation: Medical Informatics and Computational Intelligence Laboratory Electrical Engineering Department and  Human-Computer Interaction Laboratory Center for Automation Research and Digital Library Research Group College of Library and Information Services University of Maryland,  
Date: May, 1996  
Pubnum: EE-TR-96-25 CAR-TR-830 CLIS-TR-96-02 CS-TR-3643  
Abstract: This report develops a conceptual framework for text filtering practice and research, and reviews present practice in the field. Text filtering is an information seeking process in which documents are selected from a dynamic text stream to satisfy a relatively stable and specific information need. A model of the information seeking process is introduced and specialized to define information filtering. The historical development of text filtering is then reviewed and case studies of recent work are used to highlight important design characteristics of modern text filtering systems. Specific techniques drawn from information retrieval, user modeling, machine learning and other related fields are described, and the report concludes with observations on the present state of the art and implications for future research on text filtering. fl The research reported herein was supported in part by NSF grant IRI-9357731, DOD grant MDA9043C7217, NIH grant S10RR06460-1, a Department of Education technology challenge grant, and the Logos Corporation 
Abstract-found: 1
Intro-found: 1
Reference: [1] <editor> Overview of the third Text REtrieval Conference (TREC-3). In D. K. Harman, editor, </editor> <booktitle> Overview of the Third Text REtrieval Conference (TREC-3), </booktitle> <pages> pages 1-19. </pages> <institution> NIST, U.S. Department of Commerce, </institution> <year> 1994. </year> <note> NIST Special Publication 500-225. http://potomac.ncsl.nist.gov/TREC. </note>
Reference-contexts: The TREC evaluation has provided an unprecedented venue for exactly this type of performance evaluation. Conducted annually since 1992, the most recent conference (TREC-4) attracted participation from 24 universities and 12 corporations <ref> [1] </ref>. NIST provides each participant with fifty topics and a large set (typically thousands) of training documents and relevance assessments on those documents 8 for each 8 Relevance assessments for the TREC "routing" (text filtering) training documents generally are derived 11 information need. <p> The domain of the document representation function d is D, the collection of documents, and its range is also R. The domain of the comparison function c is R fi R and its range is <ref> [0; 1] </ref> n , the set of n-tuples of real numbers between zero and one. In an ideal text filtering system, c (p (info need); d (doc)) = j (info need; doc); 8info need 2 I ; 8doc 2 D; where j : I fi D 7! [0; 1] n represents <p> its range is <ref> [0; 1] </ref> n , the set of n-tuples of real numbers between zero and one. In an ideal text filtering system, c (p (info need); d (doc)) = j (info need; doc); 8info need 2 I ; 8doc 2 D; where j : I fi D 7! [0; 1] n represents the user's judgement of some relationships between an interest and a document, measured on n ordinal scales (e.g., topical similarity or degree of constraint satisfaction). 15 As we saw in section 4, the representation can exploit information derived from the content of the document, annotations made by
Reference: [2] <author> Paul E. Baclace. </author> <title> Competitive agents for information filtering. </title> <journal> Communications of the ACM, </journal> <volume> 35(12):50, </volume> <month> December </month> <year> 1992. </year>
Reference-contexts: A year later, in December of 1992, expanded versions of nine papers from that workshop appeared in a special issue of the Communications of the ACM <ref> [2, 3, 5, 11, 12, 24, 31, 36, 37] </ref>. 4 Case Studies The recent surge of interest in information filtering has actually contributed to the flood of information, since there is now more being published in the field than any single individual could hope to read.
Reference: [3] <author> Nicholas J. Belkin and W. Bruce Croft. </author> <title> Information filtering and information retrieval: </title> <journal> Two sides of the same coin? Communications of the ACM, </journal> <volume> 35(12) </volume> <pages> 29-38, </pages> <month> December </month> <year> 1992. </year>
Reference-contexts: In one of the classic works on information filtering, this observation led Belkin and Croft to suggest that the information filtering process would be an attractive application for techniques that had already developed for information retrieval systems <ref> [3] </ref>. The distinction between process and system is fundamental to understanding the 2 difference between information filtering and information retrieval. By "process" we mean an activity conducted by humans, perhaps with the assistance of a machine. <p> A year later, in December of 1992, expanded versions of nine papers from that workshop appeared in a special issue of the Communications of the ACM <ref> [2, 3, 5, 11, 12, 24, 31, 36, 37] </ref>. 4 Case Studies The recent surge of interest in information filtering has actually contributed to the flood of information, since there is now more being published in the field than any single individual could hope to read. <p> filtering practice, however, is not the techniques themselves, but rather the way in which the techniques drawn from these fields are integrated to support a text filtering process. 5.1 Information Retrieval As Belkin and Croft observed, content-based text selection techniques have been extensively evaluated in the context of information retrieval <ref> [3] </ref>.
Reference: [4] <author> D. C. Blair. </author> <title> Language and Representation in Information Retrieval. </title> <publisher> Elsevier, </publisher> <address> Amsterdam, </address> <year> 1990. </year>
Reference-contexts: The distinguishing feature of the database retrieval process is that the output will be information, 1 while in information filtering (or retrieval), the output is a set of entities (e.g., documents) which contain the information which is sought <ref> [4] </ref>. For example, using an library catalog to find the title of a book would be a database access process. Using the same system to discover whether any new books about a particular topic have been added to the collection would be an information filtering process.
Reference: [5] <author> T. F. Bowen, G. Gopal, G. Herman, T.Hickey, K.C. Lee, W. H. Mansfield, J. Raitz, and A. Weiribnrib. </author> <title> The datacycle architecture. </title> <journal> Communications of the ACM, </journal> <volume> 35(12) </volume> <pages> 71-80, </pages> <month> December </month> <year> 1992. </year>
Reference-contexts: A year later, in December of 1992, expanded versions of nine papers from that workshop appeared in a special issue of the Communications of the ACM <ref> [2, 3, 5, 11, 12, 24, 31, 36, 37] </ref>. 4 Case Studies The recent surge of interest in information filtering has actually contributed to the flood of information, since there is now more being published in the field than any single individual could hope to read.
Reference: [6] <author> David L. Chaum. </author> <title> Untraceable electronic mail, return addresses, and digital pseudonyms. </title> <journal> Communications of the ACM, </journal> <volume> 24(2) </volume> <pages> 84-88, </pages> <month> February </month> <year> 1981. </year>
Reference-contexts: Preventing such an attack would require that unauthorized observers be denied access to information about the sources and destinations of individual messages. In the computer security field, this is known as the "traffic analysis problem," and cryptographic techniques which address it have been devised (c.f., <ref> [6, 7] </ref>). In the case of collaborative filtering, the situation is further complicated by the imperative to share document annotations. A simple approach (which is used by GroupLens) is to allow each user to adopt a pseudonym.
Reference: [7] <author> David A. Cooper and Kenneth P. Birman. </author> <title> Preserving provacy in a network of mobile computers. </title> <booktitle> In Proceedings of the 1995 IEEE Symposium on Security and Privacy, </booktitle> <pages> pages 26-38. </pages> <publisher> IEEE Computer Society, </publisher> <month> May </month> <year> 1995. </year> <note> http://cs-tr.cs.cornell.edu. </note>
Reference-contexts: Preventing such an attack would require that unauthorized observers be denied access to information about the sources and destinations of individual messages. In the computer security field, this is known as the "traffic analysis problem," and cryptographic techniques which address it have been devised (c.f., <ref> [6, 7] </ref>). In the case of collaborative filtering, the situation is further complicated by the imperative to share document annotations. A simple approach (which is used by GroupLens) is to allow each user to adopt a pseudonym.
Reference: [8] <author> Peter J. Denning. </author> <title> Electronic junk. </title> <journal> Communications of the ACM, </journal> <volume> 25(3) </volume> <pages> 163-165, </pages> <month> March </month> <year> 1992. </year>
Reference-contexts: Early descriptions of the information filtering problem implicitly assumed passive collection (c.f. <ref> [8, 18] </ref>). As the amount of electronically accessible information has exploded, active collection has become increasingly important (c.f. [44]). <p> Denning coined the term "information filtering" in his ACM President's Letter that appeared in the Communications of the ACM in March of 1982 <ref> [8] </ref>. Introducing the new ACM Transactions on Office Information Systems, Denning's objective was to broaden a discussion which had traditionally focused on generation of information to include reception of information as well.
Reference: [9] <editor> Barbara Denton. </editor> <title> Ten ways to control dialog alert costs. </title> <journal> Online, </journal> <volume> 19(2) </volume> <pages> 47-48, </pages> <month> March </month> <year> 1995. </year>
Reference-contexts: On the other hand, users of commercial text filtering systems have developed profile construction techniques which which recognize differing costs for different aspects of access to intellectual property (e.g., selective purchase of limited redistribution rights) <ref> [9] </ref>. Commercial text filtering systems typically require explicit profiles, however, and we are not aware of any research on implicit user models for text filtering which exploit cost information.
Reference: [10] <author> Peter W. Foltz. </author> <title> Using latent semantic indexing for information filtering. </title> <editor> In Frederick H. Lochovsky and Robert B. Allen, editors, </editor> <booktitle> Conference on Office Information Systems, </booktitle> <pages> pages 40-47. </pages> <publisher> ACM, </publisher> <month> April </month> <year> 1990. </year> <note> http://www-psych.nmsu.edu/~pfoltz/cois/filtering-cois.html. </note>
Reference-contexts: Over the subsequent decade, occasional papers on information filtering applications appeared in the literature. While electronic mail was the original domain about which Denning had written, subsequent papers have addressed newswire articles, Internet "News" articles, 3 and broader network resources <ref> [10, 19, 30, 43] </ref>. The most influential paper of this period was published in the Communications of the ACM by Malone and others in 1987 [26]. <p> These rules (e.g., select if newsgroup is rec.sewing and "bobbin" appears in the subject field) are learned using heuristics which can be modified by the user. Foltz applied an instance based learning technique to selection of Internet News articles <ref> [10] </ref>. He retained representations of about 100 articles from a training collection which the user designated as interesting, and then ranked new articles by the cosine between their representation and the nearest retained representation.
Reference: [11] <author> Peter W. Foltz and Susan T. Dumais. </author> <title> Personalized information delivery: An analysis of information filtering methods. </title> <journal> Communications of the ACM, </journal> <volume> 35(12) </volume> <pages> 51-60, </pages> <month> December </month> <year> 1992. </year> <note> http://www-psych.nmsu.edu/~pfoltz/cacm/cacm.html. </note>
Reference-contexts: A year later, in December of 1992, expanded versions of nine papers from that workshop appeared in a special issue of the Communications of the ACM <ref> [2, 3, 5, 11, 12, 24, 31, 36, 37] </ref>. 4 Case Studies The recent surge of interest in information filtering has actually contributed to the flood of information, since there is now more being published in the field than any single individual could hope to read.
Reference: [12] <author> David Goldberg, David Nicholas, Brian M. Oki, and Douglas Terry. </author> <title> Using collaborative filtering to weave an information tapestry. </title> <journal> Communications of the ACM, </journal> <volume> 35(12) </volume> <pages> 61-70, </pages> <month> December </month> <year> 1992. </year>
Reference-contexts: A year later, in December of 1992, expanded versions of nine papers from that workshop appeared in a special issue of the Communications of the ACM <ref> [2, 3, 5, 11, 12, 24, 31, 36, 37] </ref>. 4 Case Studies The recent surge of interest in information filtering has actually contributed to the flood of information, since there is now more being published in the field than any single individual could hope to read. <p> the TREC-3 evaluation, for example, 25 text filtering systems were evaluated and average precision was observed to vary between 0.25 and 0.41. 4.2 Social Filtering The Tapestry text filtering system, developed by Nichols and others at the Xerox Palo Alto Research Center (PARC), was the first to include social filtering <ref> [12, 40] </ref>. Designed to filter personal electronic mail, messages received from mailing lists, Internet News articles, and newswire stories, Tapestry allowed users to manually construct profiles based both on document content and on annotations made regarding those documents by other users.
Reference: [13] <author> Donna Harman. </author> <title> The DARPA TIPSTER project. </title> <journal> ACM SIGIR Forum, </journal> <volume> 26(2) </volume> <pages> 26-28, </pages> <month> Fall </month> <year> 1992. </year>
Reference-contexts: In 1990, DARPA launched the TIPSTER project to fund the research efforts of several of the MUC participants <ref> [13] </ref>. TIPSTER added an emphasis on the use of statistical techniques to preselect messages that could then be subjected to more sophisticated natural language processing.
Reference: [14] <author> Donna Harman. </author> <title> Overview of the first TREC conference. </title> <editor> In Robert Korfhage, Edie Rasmussen, and Peter Willett, editors, </editor> <booktitle> Proceedings of the Sixteenth Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, </booktitle> <pages> pages 36-47. </pages> <publisher> ACM, </publisher> <month> June </month> <year> 1993. </year>
Reference-contexts: In TIPSTER, this the preselection process is known as "document detection." In 1992 The National Institute of Standards and Technology (NIST) capitalized on this research by co-sponsoring (with DARPA) an annual Text REtrieval Conference (TREC) focused specifically on text filtering and retrieval <ref> [14] </ref>. So for the first decade after Denning identified networked information as an important application for filtering technology, information filtering was either addressed episodically or included as part of a broader research effort.
Reference: [15] <author> W. C. Hill, J. D. Hollan, D. Wroblewski, and T. McCandless. Read wear and edit wear. </author> <booktitle> In Proceedings of ACM Conference on Human Factors in Computing Systems, CHI '92, </booktitle> <pages> pages 3-9. </pages> <publisher> ACM Press, </publisher> <year> 1992. </year> <month> 29 </month>
Reference-contexts: Hill and his colleagues have reported that readers find it useful to know which portions of a document receive the most attention from other readers. In an analogy to the tendency of well-used paper documents to acquire characteristics which convey similar information, they call this concept "read wear" <ref> [15] </ref>. Coarser measurements such as Morita and Shinoda's reading time metric, or the save and reply decisions explored by Stevens, may also prove to be useful bases for social filtering in some applications.
Reference: [16] <author> Will Hill, Mark Rosenstein, and Larry Stead. </author> <title> Community and history--of-use navigation. </title> <booktitle> In Electronic Proceedings of the Second World Wide Web Conference '94. National Center For Supercomputer Applications, Software Development Group, </booktitle> <month> October </month> <year> 1994. </year> <note> Not available in print. http://community.bellcore.com/navigation/home-page.html. </note>
Reference-contexts: One such application which appears to have reached the critical mass necessary for effective use of annotations is a home video recommendation service developed by Hill and his colleagues at Bellcore in which users' tastes in movies were matched using techniques similar to those implemented in GroupLens <ref> [16] </ref>. Populated with a large and relatively stable set of movie titles, stable interests could be matched against that database for some time before exhausting the set of movies that might be of interest to a user. <p> Furthermore, Hill has observed that users choosing which information to examine may find it useful to know the identity (not merely the pseudonym) of the users who made the annotations <ref> [16] </ref>. While encrypted transmission of annotations to other authorized users is a possibility in such cases, significantly limiting the user group in that way may prevent a social filtering system from reaching the necessary critical mass.
Reference: [17] <author> Lynette Hirschman. </author> <title> Comparing MUCK-II and MUC-3: Assessing the difficulty of different tasks. </title> <booktitle> In Proceedings, Third Message Understanding Conference (MUC-3), </booktitle> <pages> pages 25-30. </pages> <publisher> DARPA, Morgan Kaufmann, </publisher> <month> May </month> <year> 1991. </year>
Reference-contexts: Large-scale government-sponsored research on information filtering also began in this period. In 1989 the United States Defense Advanced Research Projects Agency (DARPA) sponsored the first of an ongoing series of Message Understanding Conferences (MUC) <ref> [23, 17] </ref>. 5 The principal thrust of those conferences has been use of information extraction techniques to support the selection of messages. In 1990, DARPA launched the TIPSTER project to fund the research efforts of several of the MUC participants [13].
Reference: [18] <author> Edward M. Housman. </author> <title> Survey of current systems for selective dissemination of information. Technical Report SIG/SDI-1, </title> <booktitle> American Society for Information Science Special Interest Group on SDI, </booktitle> <address> Washington, DC, </address> <month> June </month> <year> 1969. </year>
Reference-contexts: Early descriptions of the information filtering problem implicitly assumed passive collection (c.f. <ref> [8, 18] </ref>). As the amount of electronically accessible information has exploded, active collection has become increasingly important (c.f. [44]). <p> A decade later, widespread interest in Selective Dissemination of Information (SDI) resulted in creation of the Special Interest Group on SDI (SIG-SDI) of the American Society for Information Science. Houseman's 1969 survey for that organization identified 60 operational systems, nine of which served over 1,000 users each <ref> [18] </ref>. These systems generally followed Luhn's model, although only four of the 60 implemented 7 automatic profile updating, with the rest about evenly split between manual mainte-nance of the profiles by professional support staff or by the users themselves.
Reference: [19] <author> Paul S. Jacobs and Lisa F. Rau. SCISOR: </author> <title> Extracting information from on-line news. </title> <journal> Communications of the ACM, </journal> <volume> 33(11) </volume> <pages> 88-97, </pages> <month> November </month> <year> 1990. </year>
Reference-contexts: Over the subsequent decade, occasional papers on information filtering applications appeared in the literature. While electronic mail was the original domain about which Denning had written, subsequent papers have addressed newswire articles, Internet "News" articles, 3 and broader network resources <ref> [10, 19, 30, 43] </ref>. The most influential paper of this period was published in the Communications of the ACM by Malone and others in 1987 [26].
Reference: [20] <author> Zhenglian Jiang. </author> <title> Understanding information filtering and providing and information filtering system model. </title> <type> Master's thesis, </type> <institution> University of Missouri, Kansas City, </institution> <month> December </month> <year> 1993. </year>
Reference-contexts: This report has reviewed progress in the field with particular emphasis on the selection component of the filtering process. Other useful perspectives are offered by Jiang <ref> [20] </ref>, Mock [28], Stevens [38], and Wyle [44].
Reference: [21] <author> Jussi Karlgren, Kristina Hook, Ann Lantz, Jacob Palme, and Daniel Pargman. </author> <title> The glass box user model for filtering. </title> <type> Technical Report T94:09, </type> <institution> Swedish Institute of Computer Science, </institution> <month> July </month> <year> 1994. </year> <note> http://mars.dsv.su.se/~fk/if Doc/JPfilter-filer/Glassbox1.1.ps.Z. </note>
Reference-contexts: used to construct feature vectors, it would be reasonable to assume that the "read or ignore" decision would be nearly as useful 11 As Karlgren and his colleagues have observed, it is also important to construct systems whose operation conforms with the user's mental model of the information filtering process <ref> [21] </ref>.
Reference: [22] <author> Pat Langley. </author> <title> Elements of Machine Learning. </title> <publisher> Morgan Kaufmann, </publisher> <address> San Francisco, </address> <year> 1996. </year>
Reference-contexts: much information retrieval research precludes consideration of techniques such as the regression used by Hill and his colleagues. 5.2 User Modeling Machine learning, the study of algorithms that improve their performance with experience, offers a source of techniques that are designed to exploit multiple training instances to improve selection effectiveness <ref> [22] </ref>. <p> observations after the fact, in the absence of some bias in the induction technique, any values could reasonably be predicted. 14 Langley identifies three ways in which this necessary bias can be introduced in a machine learning system: in the representation, in the search technique, and as explicit domain knowledge. <ref> [22] </ref> The vector space method, in which profiles are represented as a single vector and documents are ranked based on the angular similarity of their representation with that vector, combines both representation bias and search bias. <p> This dimensionality reduction is an example of "feature selection." Feature selection can be an important issue when applying machine learning techniques to vector representations. Langley has observed that "many algorithms scale poorly to domains with large numbers of irrelevant features," <ref> [22] </ref> and it is not uncommon to have thousands of terms in the vocabulary of a text filtering system.
Reference: [23] <editor> Wendy Lehnert and Beth Sundheim. </editor> <title> A performance evaluation of text analysis technologies. </title> <journal> AI Magazine, </journal> <volume> 12(3) </volume> <pages> 81-94, </pages> <month> Fall </month> <year> 1991. </year>
Reference-contexts: Large-scale government-sponsored research on information filtering also began in this period. In 1989 the United States Defense Advanced Research Projects Agency (DARPA) sponsored the first of an ongoing series of Message Understanding Conferences (MUC) <ref> [23, 17] </ref>. 5 The principal thrust of those conferences has been use of information extraction techniques to support the selection of messages. In 1990, DARPA launched the TIPSTER project to fund the research efforts of several of the MUC participants [13].
Reference: [24] <author> Shoshana Loeb. </author> <title> Architecting personalized delivery of multimedia information. </title> <journal> Communications of the ACM, </journal> <volume> 35(12) </volume> <pages> 39-48, </pages> <month> December </month> <year> 1992. </year>
Reference-contexts: A year later, in December of 1992, expanded versions of nine papers from that workshop appeared in a special issue of the Communications of the ACM <ref> [2, 3, 5, 11, 12, 24, 31, 36, 37] </ref>. 4 Case Studies The recent surge of interest in information filtering has actually contributed to the flood of information, since there is now more being published in the field than any single individual could hope to read.
Reference: [25] <author> H. P. Luhn. </author> <title> A business intelligence system. </title> <journal> IBM Journal of Research and Development, </journal> <volume> 2(4) </volume> <pages> 314-319, </pages> <month> October </month> <year> 1958. </year>
Reference-contexts: But that usage occasionally appears as well. We shall avoid confusion on this subject by using only the term "profile" when referring to the compromised information need in the context of information filtering. 3 Historical Development Luhn introduced the idea of a "Business Intelligence System" in 1958 <ref> [25] </ref>. In Luhn's concept, library workers would create profiles for individual users, and then those profiles would be used in an exact-match text selection system to produce lists of new documents for each user. Orders for specific documents would be recorded and used to automatically update the requester's profile.
Reference: [26] <author> Thomas W. Malone, Kenneth R. Grant, Franklyn A. Turbak, Steven A. Brobst, and Michael D. Cohen. </author> <title> Intelligent information sharing systems. </title> <journal> Communications of the ACM, </journal> <volume> 30(5) </volume> <pages> 390-402, </pages> <month> May </month> <year> 1987. </year>
Reference-contexts: The most influential paper of this period was published in the Communications of the ACM by Malone and others in 1987 <ref> [26] </ref>.
Reference: [27] <author> Gary Marchionini. </author> <title> Information Seeking in Electronic Environments. </title> <publisher> Cambridge University Press, </publisher> <address> Cambridge, </address> <year> 1995. </year>
Reference-contexts: This report reviews existing work on text filtering, a type of "information seeking." Here we use "information seeking" as an overarching term to describe any processes by which users seek to obtain information from automated information systems <ref> [27] </ref>. Table 1 shows common types of information seeking processes. In the "information filtering" process the user is assumed to be seeking information which addresses a specific long-term interest.
Reference: [28] <author> Kenrick Jefferson Mock. </author> <title> Intelligent Information Filtering via Hybrid Techniques: Hill Climbing, Case-Based Reasoning, Index Patterns, and Genetic Algorithms. </title> <type> PhD thesis, </type> <institution> Univeristy of California Davis, </institution> <year> 1996. </year> <note> http://phobos.cs.ucdavis.edu:8001/~mock/infos/infos.html. </note>
Reference-contexts: This report has reviewed progress in the field with particular emphasis on the selection component of the filtering process. Other useful perspectives are offered by Jiang [20], Mock <ref> [28] </ref>, Stevens [38], and Wyle [44]. Text filtering systems must develop representations of both documents and user interests, they must be endowed with some way of comparing documents with interests, and they must possess some way of using the results of those comparisons to assist the user with document selection.
Reference: [29] <author> Masahiro Morita and Yoichi Shinoda. </author> <title> Information filtering based on user behavior analysis and best match text retrieval. </title> <editor> In W. Bruce Croft and C.J. van Rijsber-gen, editors, </editor> <booktitle> Proceedings of the Seventeenth Annual International ACM-SIGIR Conference on Research and Development in Information Retrieval, </booktitle> <pages> pages 272-281. </pages> <publisher> Springer-Verlag, </publisher> <month> July </month> <year> 1994. </year> <note> http://www.jaist.ac.jp/jaist/is/labs/shinoda-lab/papers/1994/sigir-94.ps. 30 </note>
Reference-contexts: InfoScope did, however, allow explicit feedback as well. Morita and Shinoda also investigated implicit feedback for filtering Internet News articles, using both save and reply evidence but substituting reading duration for Info-Scope's "read or ignore" evidence <ref> [29] </ref>. In a six week study of eight users, they found a strong positive correlation between reading time and explicit feedback provided by the user on a four-level scale.
Reference: [30] <author> Stephen Pollock. </author> <title> A rule-based message filtering system. </title> <journal> ACM Transactions on Office Information Systems, </journal> <volume> 6(3) </volume> <pages> 232-254, </pages> <month> July </month> <year> 1988. </year>
Reference-contexts: Over the subsequent decade, occasional papers on information filtering applications appeared in the literature. While electronic mail was the original domain about which Denning had written, subsequent papers have addressed newswire articles, Internet "News" articles, 3 and broader network resources <ref> [10, 19, 30, 43] </ref>. The most influential paper of this period was published in the Communications of the ACM by Malone and others in 1987 [26].
Reference: [31] <author> Ashwin Ram. </author> <title> Natural language understanding for information filtering systems. </title> <journal> Communications of the ACM, </journal> <volume> 35(12) </volume> <pages> 80-81, </pages> <month> December </month> <year> 1992. </year> <month> ftp://ftp.cc.gatech.edu/ai/ram/er-92-08.ps.Z. </month>
Reference-contexts: A year later, in December of 1992, expanded versions of nine papers from that workshop appeared in a special issue of the Communications of the ACM <ref> [2, 3, 5, 11, 12, 24, 31, 36, 37] </ref>. 4 Case Studies The recent surge of interest in information filtering has actually contributed to the flood of information, since there is now more being published in the field than any single individual could hope to read.
Reference: [32] <author> Paul Resnick, Neophytos Iacovou, Mitesh Suchak, Peter Bergstrom, and John Riedl. GroupLens: </author> <title> An open architecture for collaborative filtering of netnews. </title> <editor> In Richard K. Faruta and Christine M. Neuwirth, editors, </editor> <booktitle> Proceedings of the Conference on Computer Supported Cooperative Work, </booktitle> <pages> pages 175-186. </pages> <publisher> ACM, </publisher> <month> October </month> <year> 1994. </year> <note> http://www.cs.umn.edu/Research/GroupLens/cscwpaper/paper.html. </note>
Reference-contexts: The GroupLens project of Miller and others at the University of Minnesota is presently the most ambitious attempt to reach a critical mass on a dynamic information source <ref> [32] </ref>. GroupLens is designed to filter Internet News, a freely redistributable text source. Like Tapestry, GroupLens is built on a client-server model. GroupLens uses two types of servers, content servers (which are simply standard Internet News servers) and annotation servers (which have been developed for the project).
Reference: [33] <author> E. A. Rich. </author> <title> User modeling via stereotypes. </title> <journal> Cognitive Science, </journal> <volume> 3 </volume> <pages> 329-354, </pages> <year> 1979. </year>
Reference-contexts: Machine learning is one component of "user modeling," a discipline which is concerned with both how information about 18 users can be acquired and used by automated systems. 11 The models we consider in this report are what Rich has called "individual user, long-term user models" <ref> [33] </ref>. 5.2.1 Sources of Information About the User Before describing how machine learning techniques have been applied to text filtering it is useful to consider more carefully how information about the user can be acquired. <p> Rich defined a distinction between "explicit" models which are "constructed explicitly by the user" and "implicit" models which are "abstracted by the system on the basis of the user's behavior" <ref> [33] </ref>. Both implicit and explicit user models are found in text filtering systems (SIFT, for example, uses an explicit model). The machine learning techniques we describe in section can be used to create what Rich called implicit models.
Reference: [34] <author> Gerard Salton and Michael J. McGill. </author> <title> Introduction to Modern Information Retrieval. </title> <publisher> McGraw-Hill, </publisher> <address> New York, </address> <year> 1983. </year>
Reference-contexts: Table 2 illustrates these relationships. In TREC, almost all of the filtering systems produce ranked output. Accordingly, precision and fallout at several values of recall are reported, and "average precision" (the area under the precision-recall curve) is reported for use when a single measure of effectiveness is needed <ref> [34] </ref>. Average precision is computed by choosing successively larger sets of documents from the top of the ranked list that result in evenly spaced values of recall between zero and one.
Reference: [35] <author> Hinrich Schutze, David A. Hull, and Jan O. Pedersen. </author> <title> A comparison of classifiers and document representations for the routing problem. </title> <editor> In Edward A. Fox, Peter Ingwersen, and Raya Fidel, editors, </editor> <booktitle> Proceedings of the 18th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, </booktitle> <pages> pages 229-237, </pages> <month> July </month> <year> 1995. </year>
Reference-contexts: Schutze and others at Xerox PARC applied two rank reduction techniques, one using the best 200 terms found with a 2 measure of dependence between terms and relevant documents, and the other using a variation of the LSI dimension-reduction technique used by Foltz <ref> [35] </ref>. Four each of these feature selection techniques they evaluated four machine learning techniques, linear discriminant analysis (a statistical decision theory technique), logistic regression, a two-layer (linear) neural network, and three-layer (nonlinear) neural network using training and evaluation collections from TREC. <p> In this light, the work of Schutze and his colleagues suggests that machine learning techniques which effectively exploit multiple sources of evidence can be found <ref> [35] </ref>. Content-based and social filtering will almost certainly prove to be complementary in other, less easily measured ways as well. A perfect content-based technique would never find anything novel, limiting the range of applications for which it would be useful.
Reference: [36] <author> Irene Stadnyk and Robers Kass. </author> <title> Modeling users' interests in information filters. </title> <journal> Communications of the ACM, </journal> <volume> 35(12) </volume> <pages> 49-50, </pages> <month> December </month> <year> 1992. </year>
Reference-contexts: A year later, in December of 1992, expanded versions of nine papers from that workshop appeared in a special issue of the Communications of the ACM <ref> [2, 3, 5, 11, 12, 24, 31, 36, 37] </ref>. 4 Case Studies The recent surge of interest in information filtering has actually contributed to the flood of information, since there is now more being published in the field than any single individual could hope to read.
Reference: [37] <author> Curt Stevens. </author> <title> Automating the creation of information filters. </title> <journal> Communications of the ACM, </journal> <volume> 35(12):48, </volume> <month> December </month> <year> 1992. </year> <note> http://www.holodeck.com/curt/mypapers/ CACM-12-92.ps. </note>
Reference-contexts: A year later, in December of 1992, expanded versions of nine papers from that workshop appeared in a special issue of the Communications of the ACM <ref> [2, 3, 5, 11, 12, 24, 31, 36, 37] </ref>. 4 Case Studies The recent surge of interest in information filtering has actually contributed to the flood of information, since there is now more being published in the field than any single individual could hope to read.
Reference: [38] <author> Curt Stevens. </author> <title> Knowledge-Based Assistance for Accessing Large, Poorly Structured Information Spaces. </title> <type> PhD thesis, </type> <institution> University of Colorado, Department of Computer Science, Boulder, </institution> <year> 1992. </year> <note> http://www.cs.colorado.edu/homes/stevens/ public html/mypapers/Thesis-tech-report.ps. </note>
Reference-contexts: Stevens developed a system called InfoScope which used automatic profile learning to minimize the complexity of exploiting information about the context in which words were used <ref> [38] </ref>. Also designed to filter Internet News, InfoScope deduced exact-match rules and offered them for approval (possibly with modifications) by the user. These suggestions were based on simple observable actions such as the time spent reading a newsgroup or whether an individual message was saved for future reference. <p> Automatic techniques are needed to make this wealth of information accessible, since information that cannot be found is no better than information which does not exist. Rather than simply removing unwanted information, information filtering actually gives consumers the ability to reorganize the information space <ref> [38] </ref>. For economic reasons, information spaces have traditionally been organized by producers and, in some cases, reorganized by intermediaries. In book publishing, for example, authors and publishers work together to assign titles to books and to announce their availability. <p> This report has reviewed progress in the field with particular emphasis on the selection component of the filtering process. Other useful perspectives are offered by Jiang [20], Mock [28], Stevens <ref> [38] </ref>, and Wyle [44]. Text filtering systems must develop representations of both documents and user interests, they must be endowed with some way of comparing documents with interests, and they must possess some way of using the results of those comparisons to assist the user with document selection.
Reference: [39] <author> Robert S. Taylor. </author> <title> The process of asking questions. </title> <journal> American Documentation, </journal> <volume> 13(4) </volume> <pages> 391-396, </pages> <month> October </month> <year> 1962. </year>
Reference-contexts: referring to all of these variations as "information filtering." Taylor defined four types of information need (visceral, conscious, formalized, and compromised) that reflected the process of moving from the actual (but perhaps unrecognized) need for information to an expression of the need which could be represented in an information system <ref> [39] </ref>. In common use, however, application of the terminology is unfortunately not nearly so precise.
Reference: [40] <author> Douglas B. Terry. </author> <title> A tour through tapestry. </title> <booktitle> In Proceedings of the ACM Conference on Organizational Computing Systems (COOCS), </booktitle> <pages> pages 21-30, </pages> <month> November </month> <year> 1993. </year>
Reference-contexts: the TREC-3 evaluation, for example, 25 text filtering systems were evaluated and average precision was observed to vary between 0.25 and 0.41. 4.2 Social Filtering The Tapestry text filtering system, developed by Nichols and others at the Xerox Palo Alto Research Center (PARC), was the first to include social filtering <ref> [12, 40] </ref>. Designed to filter personal electronic mail, messages received from mailing lists, Internet News articles, and newswire stories, Tapestry allowed users to manually construct profiles based both on document content and on annotations made regarding those documents by other users.
Reference: [41] <author> Howard Turtle and W. Bruce Croft. </author> <title> Inference networks for document retrieval. </title> <editor> In Jean-Luc Vidick, editor, </editor> <booktitle> Proceedings of the 13th International Conference on Research and Development in Information Retrieval, </booktitle> <pages> pages 1-24. </pages> <booktitle> ACM SIGIR, </booktitle> <month> September </month> <year> 1990. </year>
Reference-contexts: Bayesian inference networks have proven to be a useful technique for computing this conditional probability <ref> [41] </ref>. Since it is possible to construct a Bayesian inference net which computes the cosine of the angle between two vectors, the vector space method can be interpreted as a special case of the probabilistic method [42].
Reference: [42] <author> Howard R. Turtle and W. Bruce Croft. </author> <title> A comparison of text retrieval models. </title> <journal> The Computer Journal, </journal> <volume> 35(3) </volume> <pages> 279-290, </pages> <year> 1992. </year>
Reference-contexts: Since it is possible to construct a Bayesian inference net which computes the cosine of the angle between two vectors, the vector space method can be interpreted as a special case of the probabilistic method <ref> [42] </ref>. Since the comparison function can produce a multiple-valued result, the display module can be designed to exploit the results of both exact match and ranked output techniques.
Reference: [43] <author> M.F. </author> <title> Wyle and H.P. Frei. Retrieving highly dynamic, widely distributed information. </title> <editor> In N. J. Belkin and C.J. van Rijsbergen, editors, </editor> <booktitle> Proceedings of the Twelfth Annual International ACMSIGIR Conference on Research and Development in Information Retrieval, </booktitle> <pages> pages 108-115. </pages> <publisher> ACM, </publisher> <month> June </month> <year> 1989. </year>
Reference-contexts: Over the subsequent decade, occasional papers on information filtering applications appeared in the literature. While electronic mail was the original domain about which Denning had written, subsequent papers have addressed newswire articles, Internet "News" articles, 3 and broader network resources <ref> [10, 19, 30, 43] </ref>. The most influential paper of this period was published in the Communications of the ACM by Malone and others in 1987 [26].
Reference: [44] <author> Mitchell F. Wyle. </author> <title> Effective Dissemination of WAN Information. </title> <type> PhD thesis, </type> <institution> LaSalle University, Mandeville, LA, </institution> <year> 1995. </year> <note> http://vhdl.org/~wyle/diss/diss.html. 31 </note>
Reference-contexts: Early descriptions of the information filtering problem implicitly assumed passive collection (c.f. [8, 18]). As the amount of electronically accessible information has exploded, active collection has become increasingly important (c.f. <ref> [44] </ref>). Active collection techniques can benefit from a close coupling between the collection and selection modules because they exploit both user and network models to perform information seeking actions in a network on behalf of the user. <p> This report has reviewed progress in the field with particular emphasis on the selection component of the filtering process. Other useful perspectives are offered by Jiang [20], Mock [28], Stevens [38], and Wyle <ref> [44] </ref>. Text filtering systems must develop representations of both documents and user interests, they must be endowed with some way of comparing documents with interests, and they must possess some way of using the results of those comparisons to assist the user with document selection.
Reference: [45] <author> Tak W. Yan and Hector Garcia-Molina. </author> <title> Distributed selective dissemination of information. </title> <booktitle> In Proceedings of the Third International Conference on Parallel and Distributed Information Systems, </booktitle> <pages> pages 89-98. </pages> <publisher> IEEE Computer Society, </publisher> <address> Septem-ber 1994. ftp://db.stanford.edu/pub/yan/1994/dsdi.ps. </address>
Reference-contexts: In general, he goal of distributed computation is to optimize the tradeoff between distributing the workload and minimizing communication requirements. Yan studied this issue rigorously in conjunction with his work on SIFT, developing optimal assignments of computational tasks among a group of cooperating servers <ref> [45] </ref>. The GroupLens project has chosen an alternative approach that exploits an existing infrastructure for document distribution. By augmenting this infrastructure with distributed annotation servers, GroupLens expects to achieve acceptable efficiency in a manner compatible with the existing physical and social structure for Internet News.

References-found: 45

