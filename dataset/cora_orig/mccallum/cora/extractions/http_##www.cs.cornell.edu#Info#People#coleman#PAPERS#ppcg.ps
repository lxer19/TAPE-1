URL: http://www.cs.cornell.edu/Info/People/coleman/PAPERS/ppcg.ps
Refering-URL: http://www.cs.cornell.edu/Info/People/coleman/papers.html
Root-URL: 
Title: Linearly Constrained Optimization and Projected Preconditioned Conjugate Gradients  
Author: Thomas F. Coleman 
Date: October 18, 1996  
Abstract: We consider how to apply the preconditioned conjugate gradient process to generate feasible iterates for large-scale nonlinear minimization in the presence of linear constraints, including non-negativity conditions. A new and useful viewpoint is proffered; important computational linear algebra issues are discussed.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> T. Carpenter and D. Shanno, </author> <title> An interior point method for quadratic programs based on conjugate gradients, </title> <journal> Computational Optimization and Applications, </journal> <volume> 2 (1993), </volume> <pages> pp. 5-28. </pages>
Reference-contexts: If C k is diagonal then ~ A k has the same sparsity as A; this choice yields the Carpenter/Shanno technique <ref> [1] </ref>, used within an interior-point method for positive definite quadratic programming. Of course restricting the preconditioner to be diagonal is limiting in some cases a diagonal preconditioner may not be effective.
Reference: [2] <author> T. F. Coleman and Y. Li, </author> <title> An interior trust region approach for nonlinear minimization subject to bounds, </title> <type> Tech. Rep. TR 93-1342, </type> <institution> Computer Science Department, Cornell University, </institution> <year> 1993. </year>
Reference-contexts: H k and g k above, assign p = Z p. (To update x k and remain feasible it is necessary to introduce a line search parameter ff k such that x k+1 = x k + ff k p k &gt; 0 and ff k ! 1 sufficiently fast <ref> [2, 3] </ref>.) How can we effectively use the projected preconditioning ideas discussed above to compute p k , when it is clear that H k , as defined in (8), is unbounded as x k ! x fl ? A good answer is to define a preconditioner that isolates the source <p> Finally, we remark that under reasonable assumptions, Algorithm PPCG can be used, in conjunction with a line step procedure, described in <ref> [2, 3] </ref>, to generate a strictly feasible sequence fx k g converging to x fl with a local superlinear rate. 4 Concluding Remarks In summary, algorithm PPCG allows for the direct application of preconditioned conjugate gradient ideas to large-scale linearly constrained optimization, including problems with non-negativity constraints.
Reference: [3] <author> T. F. Coleman and J. Liu, </author> <title> An interior Newton method for quadratic programming, </title> <type> Tech. Rep. 93-1388, </type> <institution> Computer Science Department, Cornell University, </institution> <year> 1993. </year>
Reference-contexts: H k and g k above, assign p = Z p. (To update x k and remain feasible it is necessary to introduce a line search parameter ff k such that x k+1 = x k + ff k p k &gt; 0 and ff k ! 1 sufficiently fast <ref> [2, 3] </ref>.) How can we effectively use the projected preconditioning ideas discussed above to compute p k , when it is clear that H k , as defined in (8), is unbounded as x k ! x fl ? A good answer is to define a preconditioner that isolates the source <p> Finally, we remark that under reasonable assumptions, Algorithm PPCG can be used, in conjunction with a line step procedure, described in <ref> [2, 3] </ref>, to generate a strictly feasible sequence fx k g converging to x fl with a local superlinear rate. 4 Concluding Remarks In summary, algorithm PPCG allows for the direct application of preconditioned conjugate gradient ideas to large-scale linearly constrained optimization, including problems with non-negativity constraints.
Reference: [4] <author> T. F. Coleman and A. Pothen, </author> <title> The null space problem II: Algorithms, </title> <journal> SIAM J. Alg. & Disc. Meth., </journal> <volume> 8 (1987), </volume> <pages> pp. 544-563. </pages>
Reference-contexts: First, is the reduced matrix Z T H k Z formed explicitly? If so, it is imperative that both Z and Z T H k Z be sparse when n m is large. The problem of finding a sparse null basis Z has been studied with some success, e.g., <ref> [4, 9] </ref>; however, the problem of determining a basis Z for the null space of A where both Z and Z T H k Z are sparse is uncharted territory.
Reference: [5] <author> I. Duff, N. I. M. Gould, J. K. Reid, J. A. Scott, and K. Turner, </author> <title> The factorization of sparse symmetric indefinite matrices, </title> <journal> IMA Journal of Numerical Analysis, </journal> <volume> 11 (1991), </volume> <pages> pp. 181-204. </pages>
Reference-contexts: A Full-Space Approach: This approach is the least restrictive with regard to the choice of C k . The idea is to merely treat (5) as a sparse symmetric system and use a general sparse factorization/solver for such systems, e.g. <ref> [5] </ref>. Certainly C k must be sparse to allow for a sparse factorization of the augmented system but there are no further restrictions on the sparsity of C k .
Reference: [6] <author> I. S. Duff, A. Erisman, and J. K. Reid, </author> <title> Direct methods for sparse matrices, </title> <publisher> Clarendon Press, </publisher> <address> Oxford UK, </address> <year> 1986. </year>
Reference-contexts: Therefore, subsequent applications of Algorithm Range-Project, within 1 For simplicity we supress the role of permutation matrices, typically used to reduce fill in sparse matrix factorizations. In practise they are important and must be used. See George and Liu [7] or Duff, Erisman, and Reid <ref> [6] </ref> for an introduction to this important area. 4 Coleman a major iteration, may need to rely on normal equations to do the least squares solve, step 3, using the upper-triangular matrix R k (saved from the sparse QR-factorization of ~ A T k ).
Reference: [7] <author> A. George and J. W.-H. Liu, </author> <title> Computer Solution of Large Sparse Positive Definite Systems, </title> <publisher> Prentice-Hall, </publisher> <year> 1981. </year>
Reference-contexts: Therefore, subsequent applications of Algorithm Range-Project, within 1 For simplicity we supress the role of permutation matrices, typically used to reduce fill in sparse matrix factorizations. In practise they are important and must be used. See George and Liu <ref> [7] </ref> or Duff, Erisman, and Reid [6] for an introduction to this important area. 4 Coleman a major iteration, may need to rely on normal equations to do the least squares solve, step 3, using the upper-triangular matrix R k (saved from the sparse QR-factorization of ~ A T k ).
Reference: [8] <author> J. George and M. Heath, </author> <title> Solution of sparse least squares problems using Givens rotations, </title> <journal> SIAM J. Numer. Anal., </journal> <volume> 34 (1980), </volume> <pages> pp. 69-83. </pages>
Reference-contexts: One possibility is to compute a sparse QR-factorization of ~ A T k , when ~ A k is formed, and save the factor R k . Numerous serial and parallel algorithms for computing a sparse QR-factorization exist, e.g., <ref> [8, 13] </ref>. However, it is usually not feasible to explicitly compute or save Q k . Therefore, subsequent applications of Algorithm Range-Project, within 1 For simplicity we supress the role of permutation matrices, typically used to reduce fill in sparse matrix factorizations.
Reference: [9] <author> J. R. Gilbert and M. Heath, </author> <title> Computing a sparse basis for the nullspace, </title> <journal> SIAM J. Alg. & Disc. Meth., </journal> <volume> 8 (1987), </volume> <pages> pp. 446-459. </pages>
Reference-contexts: First, is the reduced matrix Z T H k Z formed explicitly? If so, it is imperative that both Z and Z T H k Z be sparse when n m is large. The problem of finding a sparse null basis Z has been studied with some success, e.g., <ref> [4, 9] </ref>; however, the problem of determining a basis Z for the null space of A where both Z and Z T H k Z are sparse is uncharted territory.
Reference: [10] <author> P. Gill, W. Murray, D. Ponceleon, and M. Saunders, </author> <title> Preconditioners for indefinite systems arising in optimization, </title> <journal> SIAM J. Matrix Anal. Appl., </journal> <volume> 13 (1992), </volume> <pages> pp. </pages> <address> 292 -311. </address>
Reference-contexts: An alternative approach, based on an indefinite iterative scheme applied to the optimality conditions, is given in <ref> [10] </ref>. Our approach is different: we are concerned with the direct application of the PCG process to the linearly constrained setting. We consider two situations.
Reference: [11] <author> S. G. Nash and A. Sofer, </author> <title> Preconditioning of reduced matrices, </title> <type> Tech. Rep. 93-01, </type> <institution> Dept. of Operations Research and Engineering, George Mason University, </institution> <year> 1993. </year>
Reference-contexts: Second, if the reduced matrix Z T H k Z is not formed, two concerns remain. There is still the requirement that a sparse (or low-storage) null basis Z be determined, and how do we precondition system (4) when the matrix elements are not accessible? Nash and Sofer <ref> [11] </ref> make some preconditioning suggestions based on an approximation to (4). However, it is clear that preconditioning strategies that require sparsity and direct access to the matrix elements are not viable in this situation. We can circumvent these problems with an elegant and general solution.
Reference: [12] <author> T. Steihaug, </author> <title> The conjugate gradient method and trust regions in large scale optimization, </title> <journal> SIAM J. Numer. Anal., </journal> <volume> 20 (1983), </volume> <pages> pp. 626-637. </pages>
Reference-contexts: In the latter case d can be used to help further decrease the objective function f , e.g., Steihaug <ref> [12] </ref>. In algorithm PCG below, initialize scalar fi = 0; initialize vectors r = g k , p = 0, d = 0, and z = P k (r).
Reference: [13] <author> C. Sun, </author> <title> Parallel sparse orthogonal factorizations on distributed-memory multiprocessors. </title> <note> In preparation, </note> <year> 1993. </year>
Reference-contexts: One possibility is to compute a sparse QR-factorization of ~ A T k , when ~ A k is formed, and save the factor R k . Numerous serial and parallel algorithms for computing a sparse QR-factorization exist, e.g., <ref> [8, 13] </ref>. However, it is usually not feasible to explicitly compute or save Q k . Therefore, subsequent applications of Algorithm Range-Project, within 1 For simplicity we supress the role of permutation matrices, typically used to reduce fill in sparse matrix factorizations.
References-found: 13

