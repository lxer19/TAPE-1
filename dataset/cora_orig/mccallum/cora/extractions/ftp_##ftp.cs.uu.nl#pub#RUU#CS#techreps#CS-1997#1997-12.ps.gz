URL: ftp://ftp.cs.uu.nl/pub/RUU/CS/techreps/CS-1997/1997-12.ps.gz
Refering-URL: http://www.cs.ruu.nl/docs/research/publication/TechList1.html
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Title: A Decade of Combinatorial Optimization  
Author: Karen Aardal Stan van Hoesel Jan Karel Lenstra Leen Stougie 
Address: CWI, Amsterdam  Amsterdam  
Affiliation: Utrecht University  University of Maastricht  Eindhoven University of Technology;  University of  
Abstract: This paper offers a brief overview of the developments in combinatorial optimization during the past decade. We discuss improvements in polynomial-time algorithms for problems on graphs and networks, and review the methodological and computational progress in linear and integer optimization. Some of the more prominent software packages in these areas are mentioned. With respect to obtaining approximate solutions to NP-hard problems, we survey recent positive and negative results on polynomial-time approximability and summarize the advances in local search.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> K. Aardal, </author> <title> C.P.M. van Hoesel (1996). Polyhedral techniques in combinatorial optimization I: </title> <journal> Theory. Statist. </journal> <volume> Neerlandica 50, </volume> <pages> 3-26. </pages>
Reference-contexts: During the past ten years, however, an enormous amount of more problem specific results have been obtained. Moreover, surprisingly large instances have been solved using a mixture of cutting plane algorithms and branch-and-bound. For recent surveys we refer to Aardal and Van Hoesel <ref> [1] </ref>, [2], and to Chapter 3 of [22]. Similar developments have been attained for column generation methods, which can be viewed as dual to cutting plane techniques. For a survey we refer to Barnhart et al. [11].
Reference: [2] <author> K. Aardal, S. </author> <title> van Hoesel (1995). Polyhedral Techniques in Combinatorial Optimization II: Computations, </title> <type> Report UU-CS-1995-42, </type> <institution> Utrecht University. </institution>
Reference-contexts: During the past ten years, however, an enormous amount of more problem specific results have been obtained. Moreover, surprisingly large instances have been solved using a mixture of cutting plane algorithms and branch-and-bound. For recent surveys we refer to Aardal and Van Hoesel [1], <ref> [2] </ref>, and to Chapter 3 of [22]. Similar developments have been attained for column generation methods, which can be viewed as dual to cutting plane techniques. For a survey we refer to Barnhart et al. [11].
Reference: [3] <author> E.H.L. Aarts, J.K. Lenstra (eds.) </author> <year> (1997). </year> <title> Local Search in Combinatorial Optimization, </title> <publisher> Wiley, </publisher> <address> Chichester. </address>
Reference-contexts: Neural networks have many applications, which, however, seem to fall outside the realm of optimization. Many aspects of local search are discussed in the book edited by Aarts and Lenstra <ref> [3] </ref>. We see three main lines of advance. First, a theory of the computational complexity and performance analysis of local search is now emerging; see Chapters 2 and 3 of [3]. Second, neighborhoods embodying problem-specific knowledge and data structures supporting incremental computations are being used in rather sophisticated implementations. <p> Many aspects of local search are discussed in the book edited by Aarts and Lenstra <ref> [3] </ref>. We see three main lines of advance. First, a theory of the computational complexity and performance analysis of local search is now emerging; see Chapters 2 and 3 of [3]. Second, neighborhoods embodying problem-specific knowledge and data structures supporting incremental computations are being used in rather sophisticated implementations. Third, some of the more successful search strategies are hybrids, which combine local search with a constructive method, with tree search or, again, with local search. <p> The shifting bottleneck procedure for job shop scheduling of Adams et al. [4] is a constructive rule that reoptimizes partial schedules along the way. The shop scheduling algorithms of Nowicki and Smutnicki (see, e.g., [41] and Chapter 11 of <ref> [3] </ref>) apply tabu search and jump back to previously considered promising but rejected moves; many other combinations of local search and tree search have been proposed. Johnson's iterated Lin-Kernighan algorithm for the TSP (see Chapter 8 of [3]) is a nested form of local search, which applies 4-exchanges to local optima <p> scheduling algorithms of Nowicki and Smutnicki (see, e.g., [41] and Chapter 11 of <ref> [3] </ref>) apply tabu search and jump back to previously considered promising but rejected moves; many other combinations of local search and tree search have been proposed. Johnson's iterated Lin-Kernighan algorithm for the TSP (see Chapter 8 of [3]) is a nested form of local search, which applies 4-exchanges to local optima resulting from variable-depth search. 7
Reference: [4] <author> J. Adams, E. Balas, D. </author> <month> Zawack </month> <year> (1988). </year> <title> The shifting bottleneck procedure for job shop scheduling. </title> <journal> Management Sci. </journal> <volume> 34, </volume> <pages> 391-401. </pages>
Reference-contexts: Third, some of the more successful search strategies are hybrids, which combine local search with a constructive method, with tree search or, again, with local search. The shifting bottleneck procedure for job shop scheduling of Adams et al. <ref> [4] </ref> is a constructive rule that reoptimizes partial schedules along the way.
Reference: [5] <author> A. Aggarwal, J.K. </author> <title> Park (1993). Improved algorithms for economic lot size problems. </title> <journal> Oper. Res. </journal> <volume> 41, </volume> <pages> 549-571. </pages>
Reference-contexts: It lasted more than thirty years before a better algorithm was found. In the early 1990's three groups simultaneously developed algorithms with running time O (T log T ); see <ref> [5] </ref>, [24], [52]. Important for the implementation of graph related algorithms is the availability of software packages. The most prominent software library is LEDA, A Library of Efficient Datatypes and Algorithms, developed by Melhorn and Naher [38].
Reference: [6] <author> R.K. Ahuja, T.L. Magnanti, J.B. </author> <title> Orlin (1993). Network Flows; Theory, Algorithms, and Applications, </title> <publisher> Prentice Hall, </publisher> <address> Englewoord Cliffs. </address>
Reference-contexts: A thorough treatment of these results is given by Ahuja et al. <ref> [6] </ref>. This book also describes improvements in solution times for problems such as shortest path and matching problems. The minimum cut problem of finding a minimum weight set of arcs in a network whose removal would disconnect the network is dual to the maximum flow problem.
Reference: [7] <author> D.J. </author> <month> Aldous </month> <year> (1981). </year> <title> Random walks on finite groups and rapidly mixing Markov chains. Seminaire de Probabilites XVII, </title> <booktitle> Lecture Notes in Mathematics 986, </booktitle> <publisher> Springer, </publisher> <address> New York, </address> <pages> 243-297. </pages>
Reference-contexts: Jerrum et al. 6 [30] showed the equivalence between approximate counting and approximate sampling for a wide class of combinatorial structures. Building on work by Aldous <ref> [7] </ref>, they use Markov chains to simulate random (uniform) sampling of the structures, and proved that these "mix rapidly".
Reference: [8] <author> S. </author> <title> Arora (1996). Polynomial time approximation schemes for Euclidean PST and other geometric problems. </title> <booktitle> Proc. 37th Annual Symp. Foundations of Computer Science, </booktitle> <pages> 2-11. </pages>
Reference-contexts: A remarkable result was obtained by Arora <ref> [8] </ref>. He developed a polynomial-time approximation scheme for the traveling salesman problem (TSP) in the Euclidean space.
Reference: [9] <author> S. Arora, C. Lund, R. Motwani, M. Sudan, M. </author> <title> Szegedy (1992). Proof verification and hardness of approximation problems. </title> <booktitle> Proc. 33rd Annual Symp. Foundations of Computer Science, </booktitle> <pages> 14-23. </pages>
Reference-contexts: Given a Boolean expression in conjunctive normal form, the problem MAXSAT of finding a truth assignment to the variables that satisfies the maximum number of clauses is complete for this class. Arora et al. <ref> [9] </ref> gave a strong justification for investigating these concepts. They showed that there cannot exist a polynomial-time approximation scheme for MAXSAT unless P = N P . The proof is based on an alternative definition of NP in terms of randomized certificate verification based on fingerprinting methodology.
Reference: [10] <author> E. Balas, S. Ceria, G. </author> <month> Cornuejols </month> <year> (1993). </year> <title> A lift-and-project cutting plane algorithm for mixed 0-1 programs. Math. </title> <booktitle> Programming 58, </booktitle> <pages> 295-324. </pages>
Reference-contexts: This linear formulation is finally projected onto the space of the original variables. Lovasz and Schrijver showed that this procedure needs to be repeated at most n times before the convex hull of feasible solutions is obtained. Balas et al. <ref> [10] </ref> showed that it is sufficient to multiply each constraint by a single variable x j and its complement at a time. For branch-and-bound algorithms powerful and quite flexible software packages have been developed. We mention MINTO [47] and ABACUS [50].
Reference: [11] <author> C. Barnhart, E.L. Johnson, G.L. Nemhauser, M.W.P. Savelsbergh, P.H. </author> <title> Vance (1994). Branch-and-price: column generation for solving huge integer programs. </title> <editor> In: J.R. Birge, K.G. Murty (eds.). </editor> <booktitle> Mathematical Programming: State of the Art 1994, </booktitle> <institution> University of Michigan, </institution> <address> Ann Arbor, </address> <pages> 186-207. </pages>
Reference-contexts: For recent surveys we refer to Aardal and Van Hoesel [1], [2], and to Chapter 3 of [22]. Similar developments have been attained for column generation methods, which can be viewed as dual to cutting plane techniques. For a survey we refer to Barnhart et al. <ref> [11] </ref>. A new development of the last decade is the theoretical quality analysis of cutting planes. Negative results for some classes of cutting planes have been reported by Goemans [26].
Reference: [12] <author> A.I. </author> <month> Barvinok </month> <year> (1994). </year> <title> A polynomial time algorithm for counting integral points. </title> <journal> Math. Oper. Res. </journal> <volume> 19, </volume> <pages> 769-779. </pages>
Reference-contexts: The advantage of their method is that less information is lost, the disadvantage is that it uses considerably more computational steps. Cook et al. [20] implemented the Lovasz-Scarf algorithm and solved some previously unsolved integer programming problems. Barvinok <ref> [12] </ref> generalized Lenstra's result and proved that the number of integral points in a polyhedron can be counted in polynomial time if the dimension is fixed.
Reference: [13] <author> D. Bayer, M. Stillman. </author> <title> MACAULAY: A Computer Algebra System for Algebraic Geometry, </title> <note> Available by anonymous ftp from zariski.harvard.edu. </note>
Reference-contexts: Due to their structure such methods have advantages over other more conventional IP methods in solving stochastic integer programming problems; see Schultz et al. [48]. Computer packages for computing Grobner bases are available, e.g., CoCoa [16] and MACAULAY <ref> [13] </ref>. 5 Polynomial-time approximation As an alternative to solving NP-hard combinatorial optimization problems to optimality, which may be very time consuming, a stream of research has concentrated on designing polynomial-time algorithms that aim at good approximations for such problems.
Reference: [14] <author> J. Bisschop, R. </author> <month> Entriken </month> <year> (1993). </year> <title> AIMMS: The Modeling System, </title> <note> Available at http://www.paragon.nl/aimms220.html. </note>
Reference-contexts: CPLEX also contains an interior point method. To enhance user-friendliness of software for linear and integer optimization, modeling languages that allow for representation of variables and constraints in a set-based format are very useful. Leading computer packages for modeling are AMPL [25] and AIMMS <ref> [14] </ref>. 3 4 Integer optimization The most commonly used technique for solving (mixed) integer programs is still branch-and-bound. The quality of the available upper and lower bounds on the optimal value of the considered instance is the decisive factor for success of this tree search technique.
Reference: [15] <author> H.L. </author> <title> Bodlaender (1993). A tourist guide through treewidth. </title> <journal> Acta Cybernet. </journal> <volume> 11, </volume> <pages> 1-21. </pages>
Reference-contexts: The proof is non-constructive, and the algorithm may involve a large 2 constant (of the order of the number of forbidden minors). Bodlaender <ref> [15] </ref> gives a good overview of the techniques involved together with some applications. Next to these classical graph problems, there are several problems in planning and scheduling that can be viewed as problems on graphs. One well-studied problem of this kind is the uncapacitated lot sizing problem.
Reference: [16] <author> A. Capani, G. </author> <month> Niesi </month> <year> (1995). </year> <note> CoCoa User's Manual, Release 3.0b, Edition 1995, </note> <institution> Department of Mathematics, University of Genova. </institution>
Reference-contexts: Due to their structure such methods have advantages over other more conventional IP methods in solving stochastic integer programming problems; see Schultz et al. [48]. Computer packages for computing Grobner bases are available, e.g., CoCoa <ref> [16] </ref> and MACAULAY [13]. 5 Polynomial-time approximation As an alternative to solving NP-hard combinatorial optimization problems to optimality, which may be very time consuming, a stream of research has concentrated on designing polynomial-time algorithms that aim at good approximations for such problems.
Reference: [17] <author> C.S. Chekuri, A.V. Goldberg, D.R. Karger, M.S. Levine, C. </author> <title> Stein (1996). Experimental study of minimum cut algorithms, </title> <type> Technical report, </type> <institution> NEC Research Institute, Princeton. </institution> <month> 8 </month>
Reference-contexts: Nagamochi and Ibaraki [40], for instance, use edge contraction in their algorithm. Randomized edge contraction, introduced by Karger and Stein [32], leads to the fastest algorithm so far. An overview of these algorithms with a computational study is given by Chekuri et al. <ref> [17] </ref>. Interesting results have been obtained in determining polynomially solvable subclasses of generally NP-hard problems.
Reference: [18] <author> N. </author> <title> Christofides (1976). Worst-case analysis of a new heuristic for the travelling salesman problem, </title> <type> Report 388, </type> <institution> GSIA, Carnegie Mellon University. </institution>
Reference-contexts: A remarkable result was obtained by Arora [8]. He developed a polynomial-time approximation scheme for the traveling salesman problem (TSP) in the Euclidean space. Here we notice that Christofides' algorithm of 1976 <ref> [18] </ref>, with its performance guarantee of 3/2, is still the best polynomial approximation algorithm for the TSP whose distances are symmetric and satisfy the triangle inequality.
Reference: [19] <author> P. Conti, C. </author> <title> Traverso (1991). Buchberger algorithm and integer programming. </title> <booktitle> Proceedings of AAECC 9, </booktitle> <address> New Orleans, </address> <booktitle> Lecture Notes in Computer Science 539, </booktitle> <publisher> Springer, Berlin, </publisher> <pages> 130-139. </pages>
Reference-contexts: Another new technique, based on the theory of Grobner bases, was already known in computational algebraic geometry, and was introduced for solving integer optimization problems by Conti and Traverso <ref> [19] </ref>. It amounts to translating the integer programming problem into an algebraic membership problem. The Grobner bases are used to guide the generalized division that decides the membership. Advances in applicability of these methods are due mainly to Thomas [51].
Reference: [20] <author> W. Cook, T. Rutherford, H.E. Scarf, D. </author> <month> Shallcross </month> <year> (1993). </year> <title> An implementation of the generalized basis reduction algorithm for integer programming. </title> <journal> ORSA J. Comput. </journal> <volume> 5, </volume> <pages> 206-212. </pages>
Reference-contexts: Lovasz and Scarf [36] designed a "generalized" basis reduction algorithm, which works directly on the polyhedron instead of using approximations such as Lenstra does. The advantage of their method is that less information is lost, the disadvantage is that it uses considerably more computational steps. Cook et al. <ref> [20] </ref> implemented the Lovasz-Scarf algorithm and solved some previously unsolved integer programming problems. Barvinok [12] generalized Lenstra's result and proved that the number of integral points in a polyhedron can be counted in polynomial time if the dimension is fixed.
Reference: [21] <institution> CPLEX Optimization, Inc. </institution> <year> (1994). </year> <title> Using the CPLEX callable library, </title> <note> Version 3.0. </note>
Reference-contexts: For a review of research in this direction we refer to Chapter 9 of the book by Motwani and Raghavan [39]. With respect to deterministic simplex algorithms, many improvements in practical performance have been achieved. Many of these improvements have been implemented in the state-of-the-art software package CPLEX <ref> [21] </ref>. CPLEX also contains an interior point method. To enhance user-friendliness of software for linear and integer optimization, modeling languages that allow for representation of variables and constraints in a set-based format are very useful.
Reference: [22] <author> M. Dell'Amico, F. Maffioli, S. Martello (eds.) </author> <year> (1997). </year> <title> Annotated Bibliographies in Combinatorial Optimization, </title> <publisher> Wiley, </publisher> <address> Chichester. </address>
Reference-contexts: Moreover, surprisingly large instances have been solved using a mixture of cutting plane algorithms and branch-and-bound. For recent surveys we refer to Aardal and Van Hoesel [1], [2], and to Chapter 3 of <ref> [22] </ref>. Similar developments have been attained for column generation methods, which can be viewed as dual to cutting plane techniques. For a survey we refer to Barnhart et al. [11]. A new development of the last decade is the theoretical quality analysis of cutting planes. <p> An investigation of the complexity of finding very short schedules yielded lower bounds on the polynomial-time approximability of several scheduling problems, including the job shop scheduling problem. For specific results and references, we refer to Chapter 12 of <ref> [22] </ref>. The previous paragraphs concerned the worst-case approach to approximation. A complementary approach is average-case or probabilistic analysis, a research field that started more than twenty years ago.
Reference: [23] <author> M.E. Dyer, A.M. Frieze, R. </author> <title> Kannan (1991). A random polynomial algorithm for approximating the volume of convex bodies. </title> <journal> J. Assoc. Comput. Mach. </journal> <volume> 38, </volume> <pages> 1-17. </pages>
Reference-contexts: The non-dense graph case is still open. Another prominent result in this direction is an FPRAS for computing volumes of convex bodies by Dyer et al. <ref> [23] </ref>. A series of subsequent papers have given schemes with increasingly better running times. For an overview we refer to Chapter 12 of [28]. 6 Local search For many years heuristic search approaches have been used throughout science and engineering.
Reference: [24] <author> A. Federgruen, M. </author> <month> Tzur </month> <year> (1991). </year> <title> A simple forward algorithm to solve general dynamic lot sizing models with n periods in O(n log n) or O(n) time. </title> <journal> Management Sci. </journal> <volume> 37, </volume> <pages> 909-925. </pages>
Reference-contexts: It lasted more than thirty years before a better algorithm was found. In the early 1990's three groups simultaneously developed algorithms with running time O (T log T ); see [5], <ref> [24] </ref>, [52]. Important for the implementation of graph related algorithms is the availability of software packages. The most prominent software library is LEDA, A Library of Efficient Datatypes and Algorithms, developed by Melhorn and Naher [38].
Reference: [25] <author> R. Fourer, D.M. Gay, B.W. </author> <title> Kernighan (1993). AMPL: A Modeling Language for Mathematical Programming, </title> <publisher> Duxbury Press. </publisher>
Reference-contexts: CPLEX also contains an interior point method. To enhance user-friendliness of software for linear and integer optimization, modeling languages that allow for representation of variables and constraints in a set-based format are very useful. Leading computer packages for modeling are AMPL <ref> [25] </ref> and AIMMS [14]. 3 4 Integer optimization The most commonly used technique for solving (mixed) integer programs is still branch-and-bound. The quality of the available upper and lower bounds on the optimal value of the considered instance is the decisive factor for success of this tree search technique.
Reference: [26] <author> M.X. </author> <title> Goemans (1995). Worst-case comparison of valid inequalities for the TSP. Math. </title> <booktitle> Programming 69, </booktitle> <pages> 335-349. </pages>
Reference-contexts: For a survey we refer to Barnhart et al. [11]. A new development of the last decade is the theoretical quality analysis of cutting planes. Negative results for some classes of cutting planes have been reported by Goemans <ref> [26] </ref>. He evaluated the worst-case improvement resulting from adding several of the known classes of facets for the traveling salesman polytope to the subtour polyhedron, i.e., the set of vectors satisfying the so-called subtour elimination constraints.
Reference: [27] <author> M.X. Goemans, </author> <title> D.P. Williamson (1994). .878-Approximation algorithms for MAX CUT and MAX 2SAT. </title> <booktitle> Proc. 26th Annual ACM Symp. Theory of Computing, </booktitle> <pages> 422-431. </pages>
Reference-contexts: A comprehensive and up-to-date survey of the theory of approximation algorithms is provided in the book edited by Hochbaum [28]. Some of the major achievements in this field are based on a combination of relaxation and randomization. Goemans and Williamson (see <ref> [27] </ref> and Chapter 11 of [28]) designed approximation algorithms that solve appropriately chosen relaxations of mathematical programming formulations of the considered combinatorial problems, and then round the obtained solution in a randomized way. The rounding can be derandomized yielding deterministic approximation algorithms.
Reference: [28] <editor> D.S. Hochbaum (ed.) </editor> <year> (1996). </year> <title> Approximation Algorithms for NP-Hard problems, </title> <publisher> PWS Publishing Company, </publisher> <address> Boston. </address>
Reference-contexts: A widely accepted quality measure of such approximations is the performance guarantee, i.e., an upper bound on the ratio between the approximate solution value and the optimal one. A comprehensive and up-to-date survey of the theory of approximation algorithms is provided in the book edited by Hochbaum <ref> [28] </ref>. Some of the major achievements in this field are based on a combination of relaxation and randomization. Goemans and Williamson (see [27] and Chapter 11 of [28]) designed approximation algorithms that solve appropriately chosen relaxations of mathematical programming formulations of the considered combinatorial problems, and then round the obtained solution <p> A comprehensive and up-to-date survey of the theory of approximation algorithms is provided in the book edited by Hochbaum <ref> [28] </ref>. Some of the major achievements in this field are based on a combination of relaxation and randomization. Goemans and Williamson (see [27] and Chapter 11 of [28]) designed approximation algorithms that solve appropriately chosen relaxations of mathematical programming formulations of the considered combinatorial problems, and then round the obtained solution in a randomized way. The rounding can be derandomized yielding deterministic approximation algorithms. <p> This important result implies that for any MAXSNP-complete problem there must be a threshold value strictly greater than 1 on the achievable polynomial-time performance guarantee. For an overview of specific results in this direction, we refer to Chapter 10 of <ref> [28] </ref>. In sequencing and scheduling, techniques based on linear programming and rounding led to surprising performance guarantees for the off-line and on-line minimization of total (weighted) completion time on a single machine and on parallel machines, and for the minimization of makespan on parallel machines subject to communication delays. <p> Martingale theory allowed for relatively elegant asymptotic characterizations of optimal solution values of several problems; see Rhee and Talagrand [44]. Finally we mention the rather complete probabilistic analysis of bin-packing algorithms, presented in Chapter 2 of <ref> [28] </ref>. Next to these developments for optimization problems, a breakthrough in approximation was accomplished for counting problems, again based on randomization. Counting combinatorial structures such as the number of Hamiltonian cycles in a graph is obviously harder than just deciding on the presence of the structure. <p> The non-dense graph case is still open. Another prominent result in this direction is an FPRAS for computing volumes of convex bodies by Dyer et al. [23]. A series of subsequent papers have given schemes with increasingly better running times. For an overview we refer to Chapter 12 of <ref> [28] </ref>. 6 Local search For many years heuristic search approaches have been used throughout science and engineering.
Reference: [29] <author> M.R. Jerrum, A. </author> <title> Sinclair (1988). Approximating the permanent. </title> <journal> SIAM J. Comput. </journal> <volume> 18, </volume> <pages> 1149-1178. </pages>
Reference-contexts: Building on work by Aldous [7], they use Markov chains to simulate random (uniform) sampling of the structures, and proved that these "mix rapidly". As a first result Jerrum and Sinclair <ref> [29] </ref> devised a fully polynomial randomized approximation scheme (FPRAS) for counting perfect matchings in dense graphs, whose vertices have degree at least half of the total number of vertices. The non-dense graph case is still open.
Reference: [30] <author> M.R. Jerrum, L.G. Valiant, V.V. </author> <title> Vazirani (1986). Random generation of combinatorial structures from a uniform distribution. </title> <journal> Theoret. Comput. Sci. </journal> <volume> 43, </volume> <pages> 169-188. </pages>
Reference-contexts: Next to these developments for optimization problems, a breakthrough in approximation was accomplished for counting problems, again based on randomization. Counting combinatorial structures such as the number of Hamiltonian cycles in a graph is obviously harder than just deciding on the presence of the structure. Jerrum et al. 6 <ref> [30] </ref> showed the equivalence between approximate counting and approximate sampling for a wide class of combinatorial structures. Building on work by Aldous [7], they use Markov chains to simulate random (uniform) sampling of the structures, and proved that these "mix rapidly".
Reference: [31] <author> G. </author> <title> Kalai (1992). A subexponential randomized simplex algorithm. </title> <booktitle> Proc. 24th Annual ACM Symp. Theory of Computing, </booktitle> <pages> 475-482. </pages>
Reference-contexts: The main open question here is if there exist randomized algorithms that solve linear optimization problems in strongly polynomial expected running time. Though this question has not been resolved yet, major steps have been taken. The fastest randomized algorithm is due to Kalai <ref> [31] </ref>, and has expected running time O (n 2 m + b p n log n log m), where n is the number of variables, m the number of constraints, and b a constant independent of the input. It is in essence a randomized simplex algorithm.
Reference: [32] <author> D.R. Karger, C. </author> <title> Stein (1993). An O(n 2 ) algorithm for minimum cuts. </title> <booktitle> Proc. 25th Annual ACM Symp. Theory of Computing, </booktitle> <pages> 757-765. </pages>
Reference-contexts: Recently, new algorithms have been developed for this problem that do not exploit this duality. Nagamochi and Ibaraki [40], for instance, use edge contraction in their algorithm. Randomized edge contraction, introduced by Karger and Stein <ref> [32] </ref>, leads to the fastest algorithm so far. An overview of these algorithms with a computational study is given by Chekuri et al. [17]. Interesting results have been obtained in determining polynomially solvable subclasses of generally NP-hard problems.
Reference: [33] <author> A.V. </author> <title> Karmarkar (1984). A new polynomial-time algorithm for linear programming. </title> <type> Combinatorica 4, </type> <pages> 373-395. </pages>
Reference-contexts: It is implemented by a C++ class library, and incorporates many efficient data structures and algorithms. LEDA is available at ftp://ftp.mpi-sb.mpg.de in directory pub/LEDA. 3 Linear optimization The main developments in linear optimization have sprouted from the work of Karmarkar <ref> [33] </ref>, who started a wave of research on so-called interior point methods. Both theoretical and practical advances were accomplished over the past ten years, and by now some interior point methods are competitive with the celebrated simplex method.
Reference: [34] <author> A.K. Lenstra, H.W. Lenstra, Jr., L. </author> <title> Lovasz (1982). Factoring polynomials with rational coefficients. </title> <journal> Math. Ann. </journal> <volume> 261, </volume> <pages> 515-534 </pages>
Reference-contexts: The number of such hyperplanes can be proved to be bounded by a constant depending only on n. For any lattice such a basis exists and can be found in polynomial time starting from an arbitrary basis by using basis reduction; see Lenstra et al. <ref> [34] </ref>. Lovasz and Scarf [36] designed a "generalized" basis reduction algorithm, which works directly on the polyhedron instead of using approximations such as Lenstra does. The advantage of their method is that less information is lost, the disadvantage is that it uses considerably more computational steps.
Reference: [35] <author> H.W. Lenstra, Jr. </author> <year> (1983). </year> <title> Integer programming with a fixed number of variables. </title> <journal> Math. Oper. Res. </journal> <volume> 8, </volume> <pages> 538-548. </pages>
Reference-contexts: Apart from the further development of existing solution techniques, also two new techniques for integer optimization received much attention in the last decade. The first algorithm we mention, developed by H.W. Lenstra <ref> [35] </ref>, is older than ten years, but 4 served as an inspiration for further developments.
Reference: [36] <author> L. Lovasz, </author> <title> H.E. Scarf (1992). The generalized basis reduction algorithm. </title> <journal> Math. Oper. Res. </journal> <volume> 17, </volume> <pages> 751-764. 9 </pages>
Reference-contexts: The number of such hyperplanes can be proved to be bounded by a constant depending only on n. For any lattice such a basis exists and can be found in polynomial time starting from an arbitrary basis by using basis reduction; see Lenstra et al. [34]. Lovasz and Scarf <ref> [36] </ref> designed a "generalized" basis reduction algorithm, which works directly on the polyhedron instead of using approximations such as Lenstra does. The advantage of their method is that less information is lost, the disadvantage is that it uses considerably more computational steps.
Reference: [37] <author> L. Lovasz, A. </author> <title> Schrijver (1991). Cones of matrices and set-functions and 0-1 optimization. </title> <journal> SIAM J. Optimization 1, </journal> <pages> 166-190. </pages>
Reference-contexts: He evaluated the worst-case improvement resulting from adding several of the known classes of facets for the traveling salesman polytope to the subtour polyhedron, i.e., the set of vectors satisfying the so-called subtour elimination constraints. Another surprising theoretical result in polyhedral combinatorics is due to Lovasz and Schrijver <ref> [37] </ref>, who developed an algorithm for obtaining a sequence of tighter and tighter relaxations of integer 0-1 programs. The algorithm iterates the following steps.
Reference: [38] <author> K. Mehlhorn, S. </author> <title> Naher (to appear). The LEDA Platform of Combinatorial and Geometric Computing, </title> <publisher> Cambridge University Press, </publisher> <address> Cambridge. </address>
Reference-contexts: Important for the implementation of graph related algorithms is the availability of software packages. The most prominent software library is LEDA, A Library of Efficient Datatypes and Algorithms, developed by Melhorn and Naher <ref> [38] </ref>. It is implemented by a C++ class library, and incorporates many efficient data structures and algorithms.
Reference: [39] <author> R. Motwani, P. </author> <title> Raghavan (1995). Randomized Algorithms, </title> <publisher> Cambridge University Press, </publisher> <address> New York. </address>
Reference-contexts: We highlight a few here. Quite a number of these results have been obtained via randomized algorithms, for which we refer to the book by Motwani and Raghavan <ref> [39] </ref>. Much research on designing faster algorithms for the maximum flow problem and the minimum cost flow problem was initiated by the work of Tardos [49], who found the first strongly polynomial algorithm for the minimum cost flow problem. <p> It is in essence a randomized simplex algorithm. For a review of research in this direction we refer to Chapter 9 of the book by Motwani and Raghavan <ref> [39] </ref>. With respect to deterministic simplex algorithms, many improvements in practical performance have been achieved. Many of these improvements have been implemented in the state-of-the-art software package CPLEX [21]. CPLEX also contains an interior point method.
Reference: [40] <author> H. Nagamochi, T. </author> <title> Ibaraki (1992). Computing edge-connectivity in multigraphs and capacitated graphs. </title> <journal> SIAM J. Discrete Math. </journal> <volume> 5, </volume> <pages> 54-66. </pages>
Reference-contexts: The minimum cut problem of finding a minimum weight set of arcs in a network whose removal would disconnect the network is dual to the maximum flow problem. Recently, new algorithms have been developed for this problem that do not exploit this duality. Nagamochi and Ibaraki <ref> [40] </ref>, for instance, use edge contraction in their algorithm. Randomized edge contraction, introduced by Karger and Stein [32], leads to the fastest algorithm so far. An overview of these algorithms with a computational study is given by Chekuri et al. [17].
Reference: [41] <author> E. Nowicki, C. </author> <month> Smutnicki </month> <year> (1996). </year> <title> A fast taboo search algorithm for the job shop problem. </title> <journal> Management Sci. </journal> <volume> 42, </volume> <pages> 797-813. </pages>
Reference-contexts: The shifting bottleneck procedure for job shop scheduling of Adams et al. [4] is a constructive rule that reoptimizes partial schedules along the way. The shop scheduling algorithms of Nowicki and Smutnicki (see, e.g., <ref> [41] </ref> and Chapter 11 of [3]) apply tabu search and jump back to previously considered promising but rejected moves; many other combinations of local search and tree search have been proposed.
Reference: [42] <author> C.H. Papadimitriou, M. </author> <title> Yannakakis (1991). Optimization, approximation, and complexity classes. </title> <journal> J. Comput. System Sci. </journal> <volume> 28, </volume> <pages> 425-440. </pages>
Reference-contexts: Apart from the above positive sounds on approximation, there has also been a breakthrough on the negative side, in the sense of non-approximability of optimal solutions of some problems. Papadimitriou and Yannakakis <ref> [42] </ref> defined a class of maximization problems for the purpose of distinguishing problems whose optimal solutions are hard to approximate within arbitrarily small ratio. This class called MAXSNP has a two-sided polynomial reduction defined on it under which it is closed.
Reference: [43] <author> N. </author> <month> Piersma </month> <year> (1993). </year> <title> Combinatorial Optimization and Empirical Processes, </title> <booktitle> Tinbergen Institute Research Series 52, </booktitle> <address> Amsterdam. </address>
Reference-contexts: The main developments in this field during the last decade were based on discovering the possibility to exploit existing results from probability theory. Empirical process theory provided tools for the analysis of the optimal solution value of a series of number problems; see Piersma <ref> [43] </ref>. Martingale theory allowed for relatively elegant asymptotic characterizations of optimal solution values of several problems; see Rhee and Talagrand [44]. Finally we mention the rather complete probabilistic analysis of bin-packing algorithms, presented in Chapter 2 of [28].
Reference: [44] <author> W.T. Rhee, M. </author> <title> Talagrand (1989). Martingale inequalities, interpolation and NP-complete problems. </title> <journal> Math. Oper. Res. </journal> <volume> 14, </volume> <pages> 91-96. </pages>
Reference-contexts: Empirical process theory provided tools for the analysis of the optimal solution value of a series of number problems; see Piersma [43]. Martingale theory allowed for relatively elegant asymptotic characterizations of optimal solution values of several problems; see Rhee and Talagrand <ref> [44] </ref>. Finally we mention the rather complete probabilistic analysis of bin-packing algorithms, presented in Chapter 2 of [28]. Next to these developments for optimization problems, a breakthrough in approximation was accomplished for counting problems, again based on randomization.
Reference: [45] <author> N. Robertson, </author> <title> P.D. Seymour (1996). Graph minors XV: giant steps. </title> <journal> J. Combin. Theory Ser. </journal> <volume> B 68, </volume> <pages> 112-148. </pages>
Reference-contexts: Randomized edge contraction, introduced by Karger and Stein [32], leads to the fastest algorithm so far. An overview of these algorithms with a computational study is given by Chekuri et al. [17]. Interesting results have been obtained in determining polynomially solvable subclasses of generally NP-hard problems. Robertson and Seymour <ref> [45] </ref> proved an old conjecture of Wagner: for each set of graphs that is closed under taking minors, there exists a finite set of graphs that are forbidden to be minors of any graph in the set.
Reference: [46] <author> C. Roos, T. Terlaky, J.P. </author> <title> Vial (1997). Theory and Algorithms for Linear Optimization: An Interior Point Approach, </title> <publisher> Wiley, </publisher> <address> Chichester. </address>
Reference-contexts: An interesting overview and discussion of the use of simplex and interior point methods can be found in the ORSA Journal on Computing 6.1 (1994). The book by Roos et al. <ref> [46] </ref> gives a comprehensive treatment of interior point methods for linear optimization. Interior point methods have also been developed for convex optimization problems.
Reference: [47] <author> M.W.P. Savelsbergh, </author> <title> G.C. Sigismondi, G.L. Nemhauser (1994). Functional description of MINTO, a Mixed INTeger Optimizer. </title> <journal> Oper. Res. Lett. </journal> <volume> 15, </volume> <pages> 47-58. </pages>
Reference-contexts: Balas et al. [10] showed that it is sufficient to multiply each constraint by a single variable x j and its complement at a time. For branch-and-bound algorithms powerful and quite flexible software packages have been developed. We mention MINTO <ref> [47] </ref> and ABACUS [50]. MINTO contains more tools such as preprocessing and generic valid inequalities, whereas ABACUS has the advantage that it is written in C++. Apart from the further development of existing solution techniques, also two new techniques for integer optimization received much attention in the last decade.
Reference: [48] <author> R. Schultz, L. Stougie, M.H. van der Vlerk (1995). </author> <title> Solving stochastic programs with integer recourse by enumeration: a framework using Grobner basis reductions, </title> <type> Discussion paper TI 95-216, </type> <institution> Tinbergen Institute, </institution> <address> Amsterdam. </address>
Reference-contexts: Their current practical power is restricted by the size of the Grobner bases, which is large for most problems. Due to their structure such methods have advantages over other more conventional IP methods in solving stochastic integer programming problems; see Schultz et al. <ref> [48] </ref>.
Reference: [49] <author> E. </author> <title> Tardos (1985). A strongly polynomial minimum cost circulation algorithm. </title> <type> Combinatorica 5, </type> <pages> 247-255. </pages>
Reference-contexts: Quite a number of these results have been obtained via randomized algorithms, for which we refer to the book by Motwani and Raghavan [39]. Much research on designing faster algorithms for the maximum flow problem and the minimum cost flow problem was initiated by the work of Tardos <ref> [49] </ref>, who found the first strongly polynomial algorithm for the minimum cost flow problem. Scaling of the input parameters and prefixing flows are the main ingredients of most of these new algorithms, but the design of efficient data structures has also had an important impact.
Reference: [50] <author> S. </author> <month> Thienel </month> <year> (1995). </year> <title> ABACUS, A Branch-And-CUt System, </title> <type> PhD thesis, </type> <institution> Institut fur Informatik, Universitat zu Koln, Germany. </institution>
Reference-contexts: Balas et al. [10] showed that it is sufficient to multiply each constraint by a single variable x j and its complement at a time. For branch-and-bound algorithms powerful and quite flexible software packages have been developed. We mention MINTO [47] and ABACUS <ref> [50] </ref>. MINTO contains more tools such as preprocessing and generic valid inequalities, whereas ABACUS has the advantage that it is written in C++. Apart from the further development of existing solution techniques, also two new techniques for integer optimization received much attention in the last decade.
Reference: [51] <author> R.R. </author> <title> Thomas (1995). A geometric Buchberger algorithm for integer programming. </title> <journal> Math. Oper. Res. </journal> <volume> 20, </volume> <pages> 864-884. </pages>
Reference-contexts: It amounts to translating the integer programming problem into an algebraic membership problem. The Grobner bases are used to guide the generalized division that decides the membership. Advances in applicability of these methods are due mainly to Thomas <ref> [51] </ref>. Their current practical power is restricted by the size of the Grobner bases, which is large for most problems. Due to their structure such methods have advantages over other more conventional IP methods in solving stochastic integer programming problems; see Schultz et al. [48].
Reference: [52] <author> A. Wagelmans, S. van Hoesel, A. Kolen. </author> <title> Economic lot sizing: an O(n log n) algorithm that runs in linear time in the Wagner-Whitin case. </title> <journal> Oper. Res. </journal> <volume> 40, S145-S156. </volume> <pages> 10 </pages>
Reference-contexts: It lasted more than thirty years before a better algorithm was found. In the early 1990's three groups simultaneously developed algorithms with running time O (T log T ); see [5], [24], <ref> [52] </ref>. Important for the implementation of graph related algorithms is the availability of software packages. The most prominent software library is LEDA, A Library of Efficient Datatypes and Algorithms, developed by Melhorn and Naher [38].
References-found: 52

