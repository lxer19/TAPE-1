URL: ftp://ftp.cs.orst.edu/pub/tgd/papers/ml95-ebrl.ps.gz
Refering-URL: http://www.ai.univie.ac.at/~juffi/lig/lig-bib.html
Root-URL: 
Email: tgd@cs.orst.edu  flann@nick.cs.usu.edu  
Title: Explanation-Based Learning and Reinforcement Learning: A Unified View  
Author: Thomas G. Dietterich Nicholas S. Flann 
Address: Corvallis, OR 97331  Logan, UT 84322-4205  
Affiliation: Department of Computer Science Oregon State University  Department of Computer Science Utah State University  
Abstract: In speedup-learning problems, where full descriptions of operators are always known, both explanation-based learning (EBL) and reinforcement learning (RL) can be applied. This paper shows that both methods involve fundamentally the same process of propagating information backward from the goal toward the starting state. RL performs this propagation on a state-by-state basis, while EBL computes the weakest preconditions of operators, and hence, performs this propagation on a region-by-region basis. Based on the observation that RL is a form of asynchronous dynamic programming, this paper shows how to develop a dynamic programming version of EBL, which we call Explanation-Based Reinforcement Learning (EBRL). The paper compares batch and online versions of EBRL to batch and online versions of RL and to standard EBL. The results show that EBRL combines the strengths of EBL (fast learning and the ability to scale to large state spaces) with the strengths of RL (learning of optimal policies). Results are shown in chess endgames and in synthetic maze tasks.
Abstract-found: 1
Intro-found: 1
Reference: <author> Atkeson, C. G. </author> <year> (1990). </year> <title> Using local models to control movement. </title> <booktitle> In Advances in Neural Information Processing Systems, </booktitle> <volume> Vol. 2, </volume> <pages> pp. 316-323. </pages> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Instead, various ad hoc methods have been developed to ameliorate this problem. learn the policy for many states based on experience with only a few states. Perhaps the most popular approach is to represent the value function by some function approximation method, such as local weighted regression <ref> (Atkeson, 1990) </ref> or a feed-forward neural network (Tesauro, 1992; Sutton, 1988; Lin, 1992). A closely related line of research attempts to partition the state space into regions having the same (or similar) values for the value function (Chapman & Kael-bling, 1991; Moore, 1993).
Reference: <author> Bellman, R. E. </author> <year> (1957). </year> <title> Dynamic Programming. </title> <publisher> Princeton University Press. </publisher>
Reference: <author> Bern, M. </author> <year> (1990). </year> <title> Hidden surface removal for rectangles. </title> <journal> J. Comp. and Sys. Sci., </journal> <volume> 40, </volume> <pages> 49-69. </pages>
Reference: <author> Chapman, D., & Kaelbling, L. P. </author> <year> (1991). </year> <title> Input generalization in delayed reinforcement learning: An algorithm and performance comparisons. </title> <booktitle> In Proc. 12th IJCAI, </booktitle> <pages> pp. 726-731. </pages> <publisher> Morgan Kauf-mann. </publisher>
Reference: <author> Cormen, T. H., Leiserson, C. E., & Rivest, R. L. </author> <year> (1990). </year> <title> Introduction to Algorithms. </title> <publisher> MIT Press. </publisher>
Reference: <author> Dietterich, T. G., & Flann, N. S. </author> <year> (1994). </year> <title> Explanation-based learning and reinforcement learning: A unified view. </title> <type> Tech. rep., </type> <institution> Oregon State University, Corvallis, </institution> <address> OR. </address>
Reference: <author> Dijkstra, E. W. </author> <year> (1959). </year> <title> A note on two problems in connexion with graphs. </title> <journal> Numerische Mathematik, </journal> <volume> 1, </volume> <pages> 269-271. </pages>
Reference: <author> Edelsbrunner, H. </author> <year> (1983). </year> <title> A new approach to rectangle intersections. </title> <journal> Int. J. Comp. Math., </journal> <volume> 13, </volume> <pages> 209-219. </pages>
Reference: <author> Erdmann, M. </author> <year> (1986). </year> <title> Using backprojections for fine motion planning with uncertainty. </title> <journal> Int. J. Rob. Res., </journal> <volume> 5 (1), </volume> <pages> 19-45. </pages>
Reference: <author> Flann, N. S. </author> <year> (1992). </year> <title> Correct Abstraction in Counter-planning: A Knowledge Compilation Approach. </title> <type> Ph.D. thesis, </type> <institution> Oregon State University. </institution>
Reference-contexts: A further reason why the region-based abstraction is so effective in this domain is because of the "funnel" property of the rook moves. Similar results have been generated for other, more complicated endings in chess and in checkers <ref> (see Flann, 1992, for more details) </ref>. 4 Discussion The results clearly show that explanation-based reinforcement learning (rect-dp and ebrl) out-performs point-based dynamic programming (point-dp and rl) and explanation-based learning (ebl) in 2-D maze tasks.
Reference: <author> Laird, J., Rosenbloom, P., & Newell, A. </author> <year> (1986). </year> <title> Chunking in Soar: the anatomy of a general learning mechanism. </title> <journal> Mach. Learn., </journal> <volume> 1 (1), </volume> <pages> 11-46. </pages>
Reference-contexts: The meta-predicate match (e1,e2) is true if expression e1 matches expres sion e2 through unification of variables. The effectiveness of EBL is determined by the cost of performing goal regression and the cost of matching the learned rules. The Prodigy (Minton, 1988) and SOAR <ref> (Laird, Rosenbloom, & Newell, 1986) </ref> architectures incorporate many features designed to minimize these costs. Reinforcement learning (RL) is another approach to learning policies for state space search. RL algorithms assume that the state space includes a reward function, R.
Reference: <author> Lin, L.-J. </author> <year> (1992). </year> <title> Self-improving reactive agents based on reinforcement learning, planning and teaching. </title> <journal> Mach. Learn., </journal> <volume> 8, </volume> <pages> 293-322. </pages>
Reference: <author> Lozano-Perez, T., Mason, M. M., & Taylor, R. H. </author> <year> (1984). </year> <title> Automatic synthesis of fine-motion strategies for robots. </title> <journal> Int. J. Rob. Res., </journal> <volume> 3. </volume>
Reference: <author> Minton, S. </author> <year> (1988). </year> <title> Learning effective search control knowledge: An explanation-based approach. </title> <type> Ph.D. thesis, </type> <institution> Carnegie-Mellon University. </institution> <type> Tech. Rep. </type> <institution> CMU-CS-88-133. </institution>
Reference-contexts: The meta-predicate match (e1,e2) is true if expression e1 matches expres sion e2 through unification of variables. The effectiveness of EBL is determined by the cost of performing goal regression and the cost of matching the learned rules. The Prodigy <ref> (Minton, 1988) </ref> and SOAR (Laird, Rosenbloom, & Newell, 1986) architectures incorporate many features designed to minimize these costs. Reinforcement learning (RL) is another approach to learning policies for state space search. RL algorithms assume that the state space includes a reward function, R. <p> This ability to reason with regions has permitted EBL to be applied to problems with infinite state spaces, such as traditional AI planning and scheduling domains, where state-based reinforcement learning would be inapplicable <ref> (Minton, 1988) </ref>. These observations concerning the relationship between EBL and RL suggest that it would be interesting to investigate a hybrid algorithm that could perform region-based Bellman backups. These backups would combine the region-based reasoning of EBL with the value function approach of RL. <p> These high costs are the primary cause of the "Utility Problem" of explanation-based learning (Minton, 1990; Subramanian & Feldman, 1990). Some researchers have explored algorithms that combine region-based policies with a default policy <ref> (Minton, 1988) </ref>. This has the advantage of reducing the number of rectangles that need to be stored and manipulated (but at the cost of eliminating the ability to learn an optimal policy).
Reference: <author> Minton, S. </author> <year> (1990). </year> <title> Quantitative results concerning the utility of explanation-based learning. Art. </title> <journal> Int., </journal> <volume> 42, </volume> <pages> 363-392. </pages>
Reference: <author> Mitchell, T. M., Keller, R. M., & Kedar-Cabelli, S. T. </author> <year> (1986). </year> <title> Explanation-based generalization: a unifying view. </title> <journal> Mach. Learn., </journal> <volume> 1 (1), </volume> <pages> 47-80. </pages>
Reference-contexts: The process of computing the set P |called the preimage of the goal G with respect to the operator sequence S|is called goal regression, because a description of the goal is "regressed" through models of each of the operators in S. Consider for example, the LEX2 system <ref> (Mitchell, Keller, & Kedar-Cabelli, 1986) </ref>, which applies EBL to speed up symbolic integration. A state in LEX2 is an expression, such as R 5x 2 dx. The goal is to transform this expression to one that does not contain the integral sign.
Reference: <author> Moore, A. W. </author> <year> (1993). </year> <title> The Parti-game algorithm for variable resolution reinforcement learning in multidimensional state-spaces. </title> <booktitle> In Advances in Neural Information Processing, </booktitle> <volume> Vol. 6, </volume> <pages> pp. </pages> <address> 711-718 San Mateo, CA. </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Subramanian, D., & Feldman, R. </author> <year> (1990). </year> <title> The utility of ebl in recursive domain theories. </title> <booktitle> In Proc. </booktitle> <address> AAAI-90 Menlo Park, CA. </address> <publisher> AAAI Press. </publisher>
Reference: <author> Sutton, R. S. </author> <year> (1988). </year> <title> Learning to predict by the methods of temporal differences. </title> <journal> Mach. Learn., </journal> <volume> 3 (1), </volume> <pages> 9-44. </pages>
Reference: <author> Tambe, M., Newell, A., & Rosenbloom, P. </author> <year> (1990). </year> <title> The problem of expensive chunks and its solution by restricting expressiveness. </title> <journal> Mach. Learn., </journal> <volume> 5, </volume> <pages> 299-348. </pages>
Reference-contexts: Edelsbrun-ner (1983) describes a rectangle-tree data structure that provides reasonably efficient reasoning with d-dimensional rectangles. However, in discrete-valued spaces with many dimensions, the costs may be much higher <ref> (Tambe, Newell, & Rosenbloom, 1990) </ref>. These high costs are the primary cause of the "Utility Problem" of explanation-based learning (Minton, 1990; Subramanian & Feldman, 1990). Some researchers have explored algorithms that combine region-based policies with a default policy (Minton, 1988).
Reference: <author> Tesauro, G. </author> <year> (1992). </year> <title> Practical issues in temporal difference learning. </title> <journal> Mach. Learn., </journal> <volume> 8, </volume> <pages> 257-278. </pages>
References-found: 21

