URL: http://polaris.cs.uiuc.edu/reports/1329.ps.gz
Refering-URL: http://polaris.cs.uiuc.edu/polaris/rep2.html
Root-URL: http://www.cs.uiuc.edu
Title: The Privatizing DOALL Test: A Run-Time Technique for DOALL Loop Identification and Array Privatization  
Author: Lawrence Rauchwerger and David Padua 
Address: 1308 W. Main St., Urbana, IL 61801  
Affiliation: Center for Supercomputing Research and Development University of Illinois at Urbana-Champaign  
Abstract: Current parallelizing compilers cannot identify a significant fraction of fully parallel loops because they have complex or statically insufficiently defined access patterns. For this reason, we have developed the Privatizing DOALL test a technique for identifying fully parallel loops at run-time, and dynamically privatizing scalars and arrays. The test itself is fully parallel, and can be applied to any loop, regardless of the structure of its data and/or control flow. The technique can be utilized in two modes: (i) the test is performed before executing the loop and indicates whether the loop can be executed as a DOALL; (ii) speculatively the loop and the test are executed simultaneously, and it is determined later if the loop was in fact parallel. The test can also be used for debugging parallel programs. We discuss how the test can be inserted automatically by the compiler and outline a cost/performance analysis that can be performed to decide when to use the test. Our conclusion is that the test should almost always be applied because, as we show, the expected speedup for fully parallel loops is significant, and the cost of a failed test (a not fully parallel loop), is minimal. We present some experimental results on loops from the PERFECT Benchmarks which confirm our conclusion that this test can lead to significant speedups. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <institution> Alliant Computer Systems Corporation, 42 Nagog Park, Ac-ton, Massachusetts 01720. FX/Series Architecture Manual, </institution> <year> 1986. </year> <title> Part Number: </title> <publisher> 300-00001-B. </publisher>
Reference-contexts: dependence.) (c) Else if tw (A) = tm (A), then the loop IS a DOALL and the phase ends. (Since we never overwrite any memory location, there are no output depen dences.) 1 any returns the "OR" of its vector operand's elements, i.e., any (v [1 : n]) = (v <ref> [1] </ref> _ v [2] _ : : : _ v [n]). (d) Otherwise, it IS NOT a DOALL. (There are output dependences since we overwrite at least one memory location.) We now give a few examples of the DOALL test. Consider the loop shown in Fig. 2. <p> index 1 2 3 4 5 6 7 8 9 10 11 12 A w [1 : 12] 1 0 1 1 1 1 1 0 0 0 0 1 Prefix Sums 1 1 2 3 4 5 6 6 6 6 6 7 P A [1 : 7] A <ref> [1] </ref> A [3] A [4] A [5] A [6] A [7] A [12] P R1 [1 : 8] 2 2 2 10 8 8 8 10 P W [1 : 8] 1+d 2+d 4+d 3+d 6+d 2+d 5+d 7+d P R2 [1 : 8] 1+d 2+d 2 10 6+d 2+d 8 <p> The additional overhead mentioned in Section 3.1 (state save/restore) must be considered when deciding whether to use speculative execution. 5 Experimental Results In this section we present experimental results obtained on two modestly parallel machines with 8 (Alliant FX/80 <ref> [1] </ref>) and 14 processors (Alliant FX/2800 [2]) using a Cedar Fortran [10] implementation of the DOALL tests. It should be pointed out that our results scale with the number of processors and the data size and that they should be extrapolated for MPPs, the actual target of our run-time methods.
Reference: [2] <institution> Alliant Computers Systems Corporation. Alliant FX/2800 Series System Description, </institution> <year> 1991. </year>
Reference-contexts: if tw (A) = tm (A), then the loop IS a DOALL and the phase ends. (Since we never overwrite any memory location, there are no output depen dences.) 1 any returns the "OR" of its vector operand's elements, i.e., any (v [1 : n]) = (v [1] _ v <ref> [2] </ref> _ : : : _ v [n]). (d) Otherwise, it IS NOT a DOALL. (There are output dependences since we overwrite at least one memory location.) We now give a few examples of the DOALL test. Consider the loop shown in Fig. 2. <p> The DOALL test allocates, and initializes to zero, the write and read shadow arrays, A w [1 : 12] and A r [1 : 12], respectively. We obtain the results depicted in the table. Because A w <ref> [2] </ref> = A r [2] = 1, we know there exists at least one flow or anti dependence. Since the number marked does not equal the number written, we know that there are output dependences. Therefore, the loop cannot be executed in parallel. <p> The DOALL test allocates, and initializes to zero, the write and read shadow arrays, A w [1 : 12] and A r [1 : 12], respectively. We obtain the results depicted in the table. Because A w <ref> [2] </ref> = A r [2] = 1, we know there exists at least one flow or anti dependence. Since the number marked does not equal the number written, we know that there are output dependences. Therefore, the loop cannot be executed in parallel. <p> The additional overhead mentioned in Section 3.1 (state save/restore) must be considered when deciding whether to use speculative execution. 5 Experimental Results In this section we present experimental results obtained on two modestly parallel machines with 8 (Alliant FX/80 [1]) and 14 processors (Alliant FX/2800 <ref> [2] </ref>) using a Cedar Fortran [10] implementation of the DOALL tests. It should be pointed out that our results scale with the number of processors and the data size and that they should be extrapolated for MPPs, the actual target of our run-time methods.
Reference: [3] <author> U. Banerjee. </author> <title> Dependence Analysis for Supercomputing. </title> <publisher> Kluwer. </publisher> <address> Boston, MA., </address> <year> 1988. </year>
Reference-contexts: In order to determine whether or not the execution order of the data accesses affects the semantics of the loop, the data dependence relations between the statements in the loop body must be analyzed <ref> [3, 11, 17, 27, 30] </ref>. There are three possible types of dependences between two statements that access the same memory location: flow (read after write), anti (write after read), and output (write after write). <p> 2 3 4 5 6 7 8 9 10 11 12 A w [1 : 12] 1 0 1 1 1 1 1 0 0 0 0 1 Prefix Sums 1 1 2 3 4 5 6 6 6 6 6 7 P A [1 : 7] A [1] A <ref> [3] </ref> A [4] A [5] A [6] A [7] A [12] P R1 [1 : 8] 2 2 2 10 8 8 8 10 P W [1 : 8] 1+d 2+d 4+d 3+d 6+d 2+d 5+d 7+d P R2 [1 : 8] 1+d 2+d 2 10 6+d 2+d 8 7+d Table
Reference: [4] <author> M. Berry, D. Chen, P. Koss, D. Kuck, S. Lo, Y. Pang, R. Roloff, A. Sameh, E. Clementi, S. Chin, D. Schneider, G. Fox, P. Messina, D. Walker, C. Hsiung, J. Schwarzmeier, K. Lue, S. Orzag, F. Seidl, O. Johnson, G. Swanson, R. Goodrum, and J. Martin. </author> <title> The PERFECT club benchmarks: Effective performance evaluation of supercomputers. </title> <type> Technical Report CSRD-827, </type> <institution> Center for Supercomputing Research and Development, University of Illinois, Urbana, IL, </institution> <month> May </month> <year> 1989. </year>
Reference-contexts: 4 5 6 7 8 9 10 11 12 A w [1 : 12] 1 0 1 1 1 1 1 0 0 0 0 1 Prefix Sums 1 1 2 3 4 5 6 6 6 6 6 7 P A [1 : 7] A [1] A [3] A <ref> [4] </ref> A [5] A [6] A [7] A [12] P R1 [1 : 8] 2 2 2 10 8 8 8 10 P W [1 : 8] 1+d 2+d 4+d 3+d 6+d 2+d 5+d 7+d P R2 [1 : 8] 1+d 2+d 2 10 6+d 2+d 8 7+d Table 1: Allocating <p> We can determine the positions of the privatized elements of A in P A from the prefix sums of A w [1 : 12], e.g., the private version of A [5] is contained in P A <ref> [4] </ref> since the prefix sum value of A w [5] = 4 (see Table 1). In general, on each access to a shared array element A [k], it must be determined whether or not A [k] has been privatized, e.g., by checking A w [k]. <p> It should be pointed out that our results scale with the number of processors and the data size and that they should be extrapolated for MPPs, the actual target of our run-time methods. We considered five loops contained in the PERFECT Benchmarks <ref> [4] </ref> that could not be parallelized by any compiler available to us. A summary of our results is given in Table 2. For each loop, the methods applied and the speedup obtained are reported.
Reference: [5] <author> H. Berryman and J. Saltz. </author> <title> A manual for PARTI runtime primitives. </title> <type> Interim Report 90-13, </type> <institution> ICASE, </institution> <year> 1990. </year>
Reference-contexts: The boolean expression in the if statement typically tests the value of a scalar variable. During the last few years, new techniques have been developed for the run-time analysis and scheduling of loops with cross-iteration dependences <ref> [5, 13, 16, 19, 20, 21, 28, 29] </ref>. Most of this work has focussed on developing run-time methods for constructing parallel schedules for DOACROSS loops. Unfortunately, these methods have significant sequential components, rely heavily on global synchronizations (communication), or do not extract the maximum available parallelism (they make conservative assumptions). <p> 6 7 8 9 10 11 12 A w [1 : 12] 1 0 1 1 1 1 1 0 0 0 0 1 Prefix Sums 1 1 2 3 4 5 6 6 6 6 6 7 P A [1 : 7] A [1] A [3] A [4] A <ref> [5] </ref> A [6] A [7] A [12] P R1 [1 : 8] 2 2 2 10 8 8 8 10 P W [1 : 8] 1+d 2+d 4+d 3+d 6+d 2+d 5+d 7+d P R2 [1 : 8] 1+d 2+d 2 10 6+d 2+d 8 7+d Table 1: Allocating the private <p> We can determine the positions of the privatized elements of A in P A from the prefix sums of A w [1 : 12], e.g., the private version of A <ref> [5] </ref> is contained in P A [4] since the prefix sum value of A w [5] = 4 (see Table 1). <p> We can determine the positions of the privatized elements of A in P A from the prefix sums of A w [1 : 12], e.g., the private version of A <ref> [5] </ref> is contained in P A [4] since the prefix sum value of A w [5] = 4 (see Table 1). In general, on each access to a shared array element A [k], it must be determined whether or not A [k] has been privatized, e.g., by checking A w [k].
Reference: [6] <author> W. Blume and R. Eigenmann. </author> <title> Performance Analysis of Paral-lelizing Compilers on the Perfect Benchmarks T M Programs. </title> <journal> IEEE Trans. of Parallel and Distributed Systems, </journal> <volume> 3(6) </volume> <pages> 643-656, </pages> <month> November </month> <year> 1992. </year>
Reference-contexts: From this work it has become clear that, for a class of programs, compile-time analysis has to be complemented with run-time techniques if a significant fraction of the implicit parallelism is to be detected <ref> [6, 8] </ref>. The main reason for this is that the access pattern of some programs cannot be determined statically, either because of limitations of the current analysis algorithms or because the access pattern is a function of the input data. <p> 8 9 10 11 12 A w [1 : 12] 1 0 1 1 1 1 1 0 0 0 0 1 Prefix Sums 1 1 2 3 4 5 6 6 6 6 6 7 P A [1 : 7] A [1] A [3] A [4] A [5] A <ref> [6] </ref> A [7] A [12] P R1 [1 : 8] 2 2 2 10 8 8 8 10 P W [1 : 8] 1+d 2+d 4+d 3+d 6+d 2+d 5+d 7+d P R2 [1 : 8] 1+d 2+d 2 10 6+d 2+d 8 7+d Table 1: Allocating the private variables, and
Reference: [7] <author> M. Burke, R. Cytron, J. Ferrante, and W. Hsieh. </author> <title> Automatic generation of nested, fork-join parallelism. </title> <journal> Journal of Supercomputing, </journal> <pages> pages 71-88, </pages> <year> 1989. </year>
Reference-contexts: In order to remove certain types of anti and output dependences a transformation called privatization can be applied to the loop. Privatization creates, for each processor cooperating on the execution of the loop, private copies of the program variables that give rise to anti or output dependences (see, e.g., <ref> [7, 14, 15, 25, 26] </ref>). <p> 10 11 12 A w [1 : 12] 1 0 1 1 1 1 1 0 0 0 0 1 Prefix Sums 1 1 2 3 4 5 6 6 6 6 6 7 P A [1 : 7] A [1] A [3] A [4] A [5] A [6] A <ref> [7] </ref> A [12] P R1 [1 : 8] 2 2 2 10 8 8 8 10 P W [1 : 8] 1+d 2+d 4+d 3+d 6+d 2+d 5+d 7+d P R2 [1 : 8] 1+d 2+d 2 10 6+d 2+d 8 7+d Table 1: Allocating the private variables, and creating private
Reference: [8] <author> R. Eigenmann and W. Blume. </author> <title> An effectiveness study of paral-lelizing compiler techniques. </title> <booktitle> In Proc. of the 1991 Int'l. Conf. on Parallel Processing, </booktitle> <pages> pages 17-25, </pages> <month> August </month> <year> 1991. </year>
Reference-contexts: From this work it has become clear that, for a class of programs, compile-time analysis has to be complemented with run-time techniques if a significant fraction of the implicit parallelism is to be detected <ref> [6, 8] </ref>. The main reason for this is that the access pattern of some programs cannot be determined statically, either because of limitations of the current analysis algorithms or because the access pattern is a function of the input data.
Reference: [9] <author> P. A. Emrath, S. Ghosh, and D. A. Padua. </author> <title> Detecting nonde-terminacy in parallel programs. </title> <journal> IEEE Software, </journal> <pages> pages 69-77, </pages> <month> January </month> <year> 1992. </year>
Reference-contexts: Another important area of application for these tests is to detect race conditions <ref> [9] </ref> in loops that the programmer identifies as parallel. In fact, the DOALL test can be used as an efficient on-the-fly test [22] for the cases when there are no synchronization operations between parallel loop iterations.
Reference: [10] <author> M. Guzzi, D. Padua, J. Hoeflinger, and D. Lawrie. </author> <title> Cedar fortran and other vector and parallel fortran dialects. </title> <journal> Journal of Supercomputing, </journal> <volume> 4(1) </volume> <pages> 37-62, </pages> <month> March </month> <year> 1990. </year>
Reference-contexts: The additional overhead mentioned in Section 3.1 (state save/restore) must be considered when deciding whether to use speculative execution. 5 Experimental Results In this section we present experimental results obtained on two modestly parallel machines with 8 (Alliant FX/80 [1]) and 14 processors (Alliant FX/2800 [2]) using a Cedar Fortran <ref> [10] </ref> implementation of the DOALL tests. It should be pointed out that our results scale with the number of processors and the data size and that they should be extrapolated for MPPs, the actual target of our run-time methods.
Reference: [11] <author> D. J. Kuck, R. H. Kuhn, D. A. Padua, B. Leasure, and M. Wolfe. </author> <title> Dependence graphs and compiler optimizations. </title> <booktitle> In Proc. of the 8th ACM Symp. on Principles of Programming Languages, </booktitle> <pages> pages 207-218, </pages> <month> January </month> <year> 1981. </year>
Reference-contexts: In order to determine whether or not the execution order of the data accesses affects the semantics of the loop, the data dependence relations between the statements in the loop body must be analyzed <ref> [3, 11, 17, 27, 30] </ref>. There are three possible types of dependences between two statements that access the same memory location: flow (read after write), anti (write after read), and output (write after write).
Reference: [12] <author> F. Thomson Leighton. </author> <title> Introduction to Parallel Algorithms and Architectures: Arrays, Trees, Hypercubes. </title> <publisher> Morgan Kauf-mann, </publisher> <year> 1992. </year>
Reference-contexts: 12 A w [1 : 12] 1 0 1 1 1 1 1 0 0 0 0 1 Prefix Sums 1 1 2 3 4 5 6 6 6 6 6 7 P A [1 : 7] A [1] A [3] A [4] A [5] A [6] A [7] A <ref> [12] </ref> P R1 [1 : 8] 2 2 2 10 8 8 8 10 P W [1 : 8] 1+d 2+d 4+d 3+d 6+d 2+d 5+d 7+d P R2 [1 : 8] 1+d 2+d 2 10 6+d 2+d 8 7+d Table 1: Allocating the private variables, and creating private subscript arrays. <p> The counting in Step 2 (a) can be done in parallel by giving each processor s=p values to add within its private memory, and then summing the p resulting values in global storage; 5 this method takes O (s=p + log p) time <ref> [12] </ref>. The comparisons in Step 2 (b) (2 (d)) of the A w and A r (A np ) shadow arrays will take at most O (s=p + log p) time. <p> Since A w is computed during the test itself, the only additional information needed is the prefix sums, which can be computed in time O (s=p+log p) by recursive doubling <ref> [12] </ref>. In fact, the prefix sums can be computed when determining tm (A) without much extra work. If the entire array is not privatized, then in the special case of subscripted subscripts, private copies of the subscript arrays are also created.
Reference: [13] <author> S. Leung and J. Zahorjan. </author> <title> Improving the performance of run-time parallelization. </title> <booktitle> In Proc. 4th ACM SIGPLAN Symp. on Principles and Practice of Parallel Programming (PPOPP), </booktitle> <pages> pages 83-91, </pages> <month> May </month> <year> 1993. </year>
Reference-contexts: More powerful analysis techniques could remove this last limitation when the index arrays are computed using only statically-known values. However, nothing can be done at compile-time when the index arrays are a function of the input data <ref> [13, 21, 29] </ref>. Run-time techniques have been used practically from the beginning of parallel computing. During the 1960s, relatively simple run-time techniques, used to detect parallelism between scalar operations, were implemented in the hardware of the CDC 6600 and the IBM 360/91 [23, 24]. <p> The boolean expression in the if statement typically tests the value of a scalar variable. During the last few years, new techniques have been developed for the run-time analysis and scheduling of loops with cross-iteration dependences <ref> [5, 13, 16, 19, 20, 21, 28, 29] </ref>. Most of this work has focussed on developing run-time methods for constructing parallel schedules for DOACROSS loops. Unfortunately, these methods have significant sequential components, rely heavily on global synchronizations (communication), or do not extract the maximum available parallelism (they make conservative assumptions).
Reference: [14] <author> Z. Li. </author> <title> Array privatization for parallel execution of loops. </title> <booktitle> In Proc. of the 19th Int'l. Symp. on Computer Architecture, </booktitle> <pages> pages 313-322, </pages> <year> 1992. </year>
Reference-contexts: In order to remove certain types of anti and output dependences a transformation called privatization can be applied to the loop. Privatization creates, for each processor cooperating on the execution of the loop, private copies of the program variables that give rise to anti or output dependences (see, e.g., <ref> [7, 14, 15, 25, 26] </ref>). <p> If only the elements that are written are privatized, it is possible that this technique might yield performance gains over traditional compile time privatization techniques, which usually privatize the entire array <ref> [14, 15, 25, 26] </ref>. In fact, if the data access pattern is sparse enough, it is even conceivable that the reduction in the size of the working set could lead to superlinear speedups due to cache effects.
Reference: [15] <author> D. E. Maydan, S. P. Amarasinghe, and M. S. Lam. </author> <title> Data dependence and data-flow analysis of arrays. </title> <booktitle> In Proc. 5th Workshop on Programming Languages and Compilers for Parallel Computing, </booktitle> <month> August </month> <year> 1992. </year>
Reference-contexts: In order to remove certain types of anti and output dependences a transformation called privatization can be applied to the loop. Privatization creates, for each processor cooperating on the execution of the loop, private copies of the program variables that give rise to anti or output dependences (see, e.g., <ref> [7, 14, 15, 25, 26] </ref>). <p> If only the elements that are written are privatized, it is possible that this technique might yield performance gains over traditional compile time privatization techniques, which usually privatize the entire array <ref> [14, 15, 25, 26] </ref>. In fact, if the data access pattern is sparse enough, it is even conceivable that the reduction in the size of the working set could lead to superlinear speedups due to cache effects.
Reference: [16] <author> S. Midkiff and D. Padua. </author> <title> Compiler algorithms for synchronization. </title> <journal> IEEE Trans. Comput., </journal> <volume> C-36(12):1485-1495, </volume> <year> 1987. </year>
Reference-contexts: The boolean expression in the if statement typically tests the value of a scalar variable. During the last few years, new techniques have been developed for the run-time analysis and scheduling of loops with cross-iteration dependences <ref> [5, 13, 16, 19, 20, 21, 28, 29] </ref>. Most of this work has focussed on developing run-time methods for constructing parallel schedules for DOACROSS loops. Unfortunately, these methods have significant sequential components, rely heavily on global synchronizations (communication), or do not extract the maximum available parallelism (they make conservative assumptions).
Reference: [17] <author> D. A. Padua and M. J. Wolfe. </author> <title> Advanced compiler optimizations for supercomputers. </title> <journal> Commun. of the ACM, </journal> <volume> 29 </volume> <pages> 1184-1201, </pages> <month> December </month> <year> 1986. </year>
Reference-contexts: 1 Introduction During the last two decades, compiler techniques for the automatic detection of parallelism have been studied extensively <ref> [17, 27] </ref>. From this work it has become clear that, for a class of programs, compile-time analysis has to be complemented with run-time techniques if a significant fraction of the implicit parallelism is to be detected [6, 8]. <p> In order to determine whether or not the execution order of the data accesses affects the semantics of the loop, the data dependence relations between the statements in the loop body must be analyzed <ref> [3, 11, 17, 27, 30] </ref>. There are three possible types of dependences between two statements that access the same memory location: flow (read after write), anti (write after read), and output (write after write).
Reference: [18] <author> C. Polychronopoulos. </author> <title> Compiler Optimizations for Enhancing Parallelism and Their Imp act on Architecture Design. </title> <journal> IEEE Trans. on Comput., </journal> <volume> C-37(8):991-1004, </volume> <month> August </month> <year> 1988. </year>
Reference: [19] <author> J. Saltz and R. Mirchandaney. </author> <title> The preprocessed doacross loop. In Dr. </title> <editor> H.D. Schwetman, editor, </editor> <booktitle> Proc. of the 1991 Int'l. Conf. on Parallel Processing, </booktitle> <address> St. Charles, Illinois, </address> <month> August 12-16, </month> <pages> pages 174-178. </pages> <publisher> CRC Press, Inc., </publisher> <year> 1991. </year> <title> Vol. </title> <booktitle> II Software. </booktitle>
Reference-contexts: The boolean expression in the if statement typically tests the value of a scalar variable. During the last few years, new techniques have been developed for the run-time analysis and scheduling of loops with cross-iteration dependences <ref> [5, 13, 16, 19, 20, 21, 28, 29] </ref>. Most of this work has focussed on developing run-time methods for constructing parallel schedules for DOACROSS loops. Unfortunately, these methods have significant sequential components, rely heavily on global synchronizations (communication), or do not extract the maximum available parallelism (they make conservative assumptions).
Reference: [20] <author> J. Saltz, R. Mirchandaney, and K. Crowley. </author> <title> The doconsider loop. </title> <booktitle> In Proc. of the 1989 ACM Int'l. Conf. on Supercomputing , Crete, </booktitle> <address> Greece, </address> <pages> pages 29-40, </pages> <month> June </month> <year> 1989. </year>
Reference-contexts: The boolean expression in the if statement typically tests the value of a scalar variable. During the last few years, new techniques have been developed for the run-time analysis and scheduling of loops with cross-iteration dependences <ref> [5, 13, 16, 19, 20, 21, 28, 29] </ref>. Most of this work has focussed on developing run-time methods for constructing parallel schedules for DOACROSS loops. Unfortunately, these methods have significant sequential components, rely heavily on global synchronizations (communication), or do not extract the maximum available parallelism (they make conservative assumptions).
Reference: [21] <author> J. Saltz, R. Mirchandaney, and K. Crowley. </author> <title> Run-time par-allelization and scheduling of loops. </title> <journal> IEEE Trans. Comput., </journal> <volume> 40(5), </volume> <month> May </month> <year> 1991. </year>
Reference-contexts: More powerful analysis techniques could remove this last limitation when the index arrays are computed using only statically-known values. However, nothing can be done at compile-time when the index arrays are a function of the input data <ref> [13, 21, 29] </ref>. Run-time techniques have been used practically from the beginning of parallel computing. During the 1960s, relatively simple run-time techniques, used to detect parallelism between scalar operations, were implemented in the hardware of the CDC 6600 and the IBM 360/91 [23, 24]. <p> The boolean expression in the if statement typically tests the value of a scalar variable. During the last few years, new techniques have been developed for the run-time analysis and scheduling of loops with cross-iteration dependences <ref> [5, 13, 16, 19, 20, 21, 28, 29] </ref>. Most of this work has focussed on developing run-time methods for constructing parallel schedules for DOACROSS loops. Unfortunately, these methods have significant sequential components, rely heavily on global synchronizations (communication), or do not extract the maximum available parallelism (they make conservative assumptions). <p> This is a simple illustration of the schedule reuse technique, in which a correct execution schedule is determined once, and subsequently reused if all of the defining conditions remain invariant (see, e.g., <ref> [21] </ref>). If it can be determined at compile time that the data access pattern is invariant across different executions of the same loop, then no additional computation is required.
Reference: [22] <author> E. Schonberg. </author> <title> On-the-fly detection of access anomalies. </title> <booktitle> In Proc. SigPlan Conf. Programming Languages Design and Implementation (PLDI)., </booktitle> <pages> pages 285-297. </pages> <publisher> ACM Press, </publisher> <address> New York, </address> <year> 1989. </year>
Reference-contexts: Another important area of application for these tests is to detect race conditions [9] in loops that the programmer identifies as parallel. In fact, the DOALL test can be used as an efficient on-the-fly test <ref> [22] </ref> for the cases when there are no synchronization operations between parallel loop iterations. When used for this purpose, the marking phase could be run in parallel by incorporating it in the body of the parallel loop, and the analysis phase could be done after the completion of the loop.
Reference: [23] <author> J. E. Thornton. </author> <title> Design of a Computer:The Control Data 6600. </title> <type> Scott, </type> <institution> Foresman, Glenview, Illinois, </institution> <year> 1971. </year>
Reference-contexts: Run-time techniques have been used practically from the beginning of parallel computing. During the 1960s, relatively simple run-time techniques, used to detect parallelism between scalar operations, were implemented in the hardware of the CDC 6600 and the IBM 360/91 <ref> [23, 24] </ref>. Some of today's parallelizing compilers postpone part of the analysis to run-time by generating two-version loops. These consist of an if statement that selects either the original serial loop or its parallel version. The boolean expression in the if statement typically tests the value of a scalar variable.
Reference: [24] <author> R. M. Tomasulo. </author> <title> An efficient algorithm for exploiting multiple arithmetic units. </title> <journal> IBM Journal of Research and Development, </journal> <volume> 11 </volume> <pages> 25-33, </pages> <month> January </month> <year> 1967. </year>
Reference-contexts: Run-time techniques have been used practically from the beginning of parallel computing. During the 1960s, relatively simple run-time techniques, used to detect parallelism between scalar operations, were implemented in the hardware of the CDC 6600 and the IBM 360/91 <ref> [23, 24] </ref>. Some of today's parallelizing compilers postpone part of the analysis to run-time by generating two-version loops. These consist of an if statement that selects either the original serial loop or its parallel version. The boolean expression in the if statement typically tests the value of a scalar variable.
Reference: [25] <author> P. Tu and D. Padua. </author> <title> Array privatization for shared and distributed memory machines. </title> <booktitle> In Proc. 2nd Workshop on Languages, Compilers, and Run-Time Environments for Distributed Memory Machines, </booktitle> <month> September </month> <year> 1992. </year>
Reference-contexts: In order to remove certain types of anti and output dependences a transformation called privatization can be applied to the loop. Privatization creates, for each processor cooperating on the execution of the loop, private copies of the program variables that give rise to anti or output dependences (see, e.g., <ref> [7, 14, 15, 25, 26] </ref>). <p> If only the elements that are written are privatized, it is possible that this technique might yield performance gains over traditional compile time privatization techniques, which usually privatize the entire array <ref> [14, 15, 25, 26] </ref>. In fact, if the data access pattern is sparse enough, it is even conceivable that the reduction in the size of the working set could lead to superlinear speedups due to cache effects.
Reference: [26] <author> P. Tu and D. Padua. </author> <title> Automatic array privatization. </title> <booktitle> In Proc. 6th Annual Workshop on Languages and Compilers for Parallel Computing, </booktitle> <address> Portland, OR, </address> <month> August </month> <year> 1992. </year> <month> 10 </month>
Reference-contexts: In order to remove certain types of anti and output dependences a transformation called privatization can be applied to the loop. Privatization creates, for each processor cooperating on the execution of the loop, private copies of the program variables that give rise to anti or output dependences (see, e.g., <ref> [7, 14, 15, 25, 26] </ref>). <p> If only the elements that are written are privatized, it is possible that this technique might yield performance gains over traditional compile time privatization techniques, which usually privatize the entire array <ref> [14, 15, 25, 26] </ref>. In fact, if the data access pattern is sparse enough, it is even conceivable that the reduction in the size of the working set could lead to superlinear speedups due to cache effects.
Reference: [27] <author> M. Wolfe. </author> <title> Optimizing Compilers for Supercomputers. </title> <publisher> The MIT Press, </publisher> <address> Boston, MA, </address> <year> 1989. </year>
Reference-contexts: 1 Introduction During the last two decades, compiler techniques for the automatic detection of parallelism have been studied extensively <ref> [17, 27] </ref>. From this work it has become clear that, for a class of programs, compile-time analysis has to be complemented with run-time techniques if a significant fraction of the implicit parallelism is to be detected [6, 8]. <p> In order to determine whether or not the execution order of the data accesses affects the semantics of the loop, the data dependence relations between the statements in the loop body must be analyzed <ref> [3, 11, 17, 27, 30] </ref>. There are three possible types of dependences between two statements that access the same memory location: flow (read after write), anti (write after read), and output (write after write).
Reference: [28] <author> J. Wu, J. Saltz, S. Hiranandani, and H. Berryman. </author> <title> Run-time compilation methods for multicomputers. In Dr. </title> <editor> H.D. Schwetman, editor, </editor> <booktitle> Proc. of the 1991 Int'l. Conf. on Parallel Processing, </booktitle> <address> St. Charles, Illinois, </address> <month> August 12-16, </month> <pages> pages 26-30. </pages> <publisher> CRC Press, Inc., </publisher> <year> 1991. </year> <title> Vol. </title> <booktitle> II Software. </booktitle>
Reference-contexts: The boolean expression in the if statement typically tests the value of a scalar variable. During the last few years, new techniques have been developed for the run-time analysis and scheduling of loops with cross-iteration dependences <ref> [5, 13, 16, 19, 20, 21, 28, 29] </ref>. Most of this work has focussed on developing run-time methods for constructing parallel schedules for DOACROSS loops. Unfortunately, these methods have significant sequential components, rely heavily on global synchronizations (communication), or do not extract the maximum available parallelism (they make conservative assumptions).
Reference: [29] <author> C. Zhu and P. Yew. </author> <title> A scheme to enforce data dependence on large multiprocessor systems. </title> <journal> IEEE Trans. Softw. Eng., </journal> <volume> 13(6) </volume> <pages> 726-739, </pages> <month> June </month> <year> 1987. </year>
Reference-contexts: More powerful analysis techniques could remove this last limitation when the index arrays are computed using only statically-known values. However, nothing can be done at compile-time when the index arrays are a function of the input data <ref> [13, 21, 29] </ref>. Run-time techniques have been used practically from the beginning of parallel computing. During the 1960s, relatively simple run-time techniques, used to detect parallelism between scalar operations, were implemented in the hardware of the CDC 6600 and the IBM 360/91 [23, 24]. <p> The boolean expression in the if statement typically tests the value of a scalar variable. During the last few years, new techniques have been developed for the run-time analysis and scheduling of loops with cross-iteration dependences <ref> [5, 13, 16, 19, 20, 21, 28, 29] </ref>. Most of this work has focussed on developing run-time methods for constructing parallel schedules for DOACROSS loops. Unfortunately, these methods have significant sequential components, rely heavily on global synchronizations (communication), or do not extract the maximum available parallelism (they make conservative assumptions).
Reference: [30] <author> H. Zima. </author> <title> Supercompilers for Parallel and Vector Computers. </title> <publisher> ACM Press, </publisher> <address> New York, New York, </address> <year> 1991. </year> <month> 11 </month>
Reference-contexts: In order to determine whether or not the execution order of the data accesses affects the semantics of the loop, the data dependence relations between the statements in the loop body must be analyzed <ref> [3, 11, 17, 27, 30] </ref>. There are three possible types of dependences between two statements that access the same memory location: flow (read after write), anti (write after read), and output (write after write).
References-found: 30

