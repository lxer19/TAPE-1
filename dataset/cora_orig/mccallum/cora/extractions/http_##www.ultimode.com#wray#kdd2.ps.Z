URL: http://www.ultimode.com/wray/kdd2.ps.Z
Refering-URL: http://www.ultimode.com/wray/kdd2/
Root-URL: 
Title: 4 Graphical Models for Discovering Knowledge a consequence, the power to perform in an application
Author: Wray Buntine 
Note: 4.1 Introduction As  
Address: search Center  
Affiliation: Research Institute for Advanced Computing Sciences, Computational Sciences Division, NASA Ames Re  
Abstract: There are many different ways of representing knowledge, and for each of these ways there are many different discovery algorithms. How can we compare different representations? How can we mix, match and merge representations and algorithms on new problems with their own unique requirements? This chapter introduces probabilistic modeling as a philosophy for addressing these questions and presents graphical models for representing probabilistic models. Probabilistic graphical models are a unified qualitative and quantitative framework for representing and reasoning with probabilities and independencies. Perhaps one common element of the discovery systems described in this and previous books on knowledge discovery is that they are all different. Since the class of discovery problems is a challenging one, we cannot write a single program to address all of knowledge discovery. The KEFIR discovery system applied to health care by Matheus, Piatetsky-Shapiro, and McNeill (1995), for instance, is carefully tailored for a particular class of situations and could not have been easily used on the SKICAT application (Fayyad, Djorgovski, and Weir 1995). I do not know of a universal learning or discovery algorithm (Buntine 1990), and a universal problem description for discovery is arguably too broad to be used as a program specification. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Berka, P., and Ivanek, J. </author> <year> 1994. </year> <title> Automated knowledge acquisition for PROSPECTOR-like expert systems. </title> <booktitle> In Proceedings of the European Conference on Machine Learning, </booktitle> <pages> 339-342. </pages>
Reference: <author> Bernardo, J.M., and Smith, A.F.M. </author> <year> 1994. </year> <title> Bayesian Theory. </title> <address> Chichester: </address> <publisher> John Wiley. </publisher>
Reference-contexts: Algorithms for learning them are described Four categories of models. in (Buntine 1994), and references therein. The simplest category of learning models have exact, closed form solutions to the learning problem. This category is the exponential family of distributions, which includes the Gaussian, the multinomial, and other basic distributions <ref> (Bernardo and Smith 1994) </ref>, but also the decision tree or Gaussian Bayesian network of known fixed structure, and linear regression with Gaussian error described in Section 4.5.1.
Reference: <author> Boulton, D.M., and Wallace, C.S. </author> <year> 1990. </year> <title> A program for numerical classification. </title> <journal> The Computer Journal, </journal> <volume> 13(1) </volume> <pages> 63-69. </pages>
Reference: <author> Brachman, R.J., and Anand, T. </author> <year> 1995. </year> <title> The process of knowledge discovery in databases: A first sketch. In Advances in Knowledge Discovery and Data Mining, </title> <editor> eds. </editor> <publisher> U.M. </publisher>
Reference: <editor> Fayyad, G. Piatetsky-Shapiro, P. Smyth, and R.S. Uthurasamy. </editor> <publisher> MIT Press. </publisher>
Reference: <author> Buntine, W.L. </author> <year> 1994. </year> <title> Operations for learning with graphical models. </title> <journal> Journal of Artificial Intelligence Research, </journal> <volume> 2 </volume> <pages> 159-225. </pages>
Reference-contexts: They are flexible enough to represent supervised and unsupervised learning systems, neural networks, and many hybrids. * They come with well understood techniques for key tasks in the discovery process: problem formulation and decomposition, designing a learning algorithm <ref> (Buntine 1994) </ref>, identification of valuable knowledge (using decision theory), and generation of explanations (Madigan, Mosurski, and Almond 1995). Only a simple form of graphical model is considered in this chapter, the Bayesian network. <p> proceed with some parts of model fixed? This is not a difficult problem in the sense that standard algorithm schemes like the expectation max 74 Buntine Hybrid unsupervised model of the aircraft incident domain. imization (EM) algorithm used in SNOB and Autoclass are known to handle learning in this context <ref> (Buntine 1994) </ref>. Software suited to this exact task is not currently available, however. <p> There are many other models for unsupervised learning that can be similarly represented with probabilistic graphs. Sometimes this includes undirected graphs or mixtures of directed and undirected graphs <ref> (Buntine 1994) </ref>. <p> Four categories considered here are represented by the models they address, given in Figure 4.9. This section briefly explains these categories. Algorithms for learning them are described Four categories of models. in <ref> (Buntine 1994) </ref>, and references therein. The simplest category of learning models have exact, closed form solutions to the learning problem.
Reference: <author> Buntine, W.L. </author> <year> 1991a. </year> <title> Learning classification trees. </title> <booktitle> In Artificial Intelligence Frontiers in Statistics, </booktitle> <editor> ed. D.J. Hand, </editor> <address> 182-201. London: </address> <publisher> Chapman & Hall. </publisher> <address> 84 Buntine Buntine, W.L. </address> <year> 1991b. </year> <title> Theory refinement of Bayesian networks. </title> <booktitle> In Uncertainty in Artificial Intelligence: Proceedings of the Seventh Conference, </booktitle> <editor> eds. B.D. D'Ambrosio, P. Smets, and P.P. </editor> <booktitle> Bonissone, </booktitle> <pages> 52-60. </pages> <address> San Mateo, California: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Buntine, W.L., </author> <year> 1990. </year> <title> Myths and legends in learning classification rules. </title> <booktitle> In Eighth National Conference on Artificial Intelligence, </booktitle> <pages> 736-742. </pages> <address> Boston, Massachusetts: </address> <publisher> AAAI Press. </publisher>
Reference-contexts: I do not know of a universal learning or discovery algorithm <ref> (Buntine 1990) </ref>, and a universal problem description for discovery is arguably too broad to be used as a program specification. As a consequence, the power to perform in an application lies in the way knowledge about the application is obtained, used, represented and modified.
Reference: <author> Casella G., and Berger, R.L. </author> <year> 1990. </year> <title> Statistical Inference. </title> <address> Belmont, California: </address> <publisher> Wadsworth & Brooks/Cole. </publisher>
Reference: <author> Chan, B.Y. and Shachter, R.D. </author> <year> 1992. </year> <title> Structural controllability and observability in influence diagrams. </title> <booktitle> In Uncertainty in Artificial Intelligence: Proceedings of the Eight Conference, </booktitle> <editor> eds. D. Dubois, M.P. Wellman, B.D. D'Ambrosio and P. Smets, </editor> <address> 25-32. Stanford, California: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Cheeseman, P., and Stutz, J. </author> <year> 1995. </year> <title> Bayesian clustering. In Advances in Knowledge Discovery and Data Mining, </title> <editor> eds. U.M. Fayyad, G. Piatetsky-Shapiro, P. Smyth, and R.S. Uthurasamy. </editor> <publisher> MIT Press. </publisher>
Reference-contexts: A well known example in discovery is the Autoclass application to the IRAS star database <ref> (Cheeseman and Stutz 1995) </ref>. While these applications of unsupervised learning sometimes proceed routinely, it is more often the case that discovery is an iterative process. Initial exploration reveals some details and the discovery algorithm is modified as a result.
Reference: <author> Dagum, P., Galper, A., Horvitz, E., and Seiver, A. </author> <year> 1995. </year> <title> Uncertain reasoning and forecasting. </title> <journal> International Journal of Forecasting. Forthcoming. </journal>
Reference: <author> Dean T.L., and Wellman, </author> <title> M.P. 1991. Planning and Control. </title> <address> San Mateo, California: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Duda, R.O., Hart, P.E., and Nilsson, N.J. </author> <year> 1976. </year> <title> Subjective Bayesian methods for rule-based inference systems. </title> <booktitle> In National Computer Conference (AFIPS Conference Proceedings, </booktitle> <volume> Vol. 45), </volume> <pages> 1075-1082. </pages>
Reference: <author> Fayyad, U.M., Djorgovski, S., and Weir, N. </author> <year> 1995. </year> <title> The SKICAT system for sky survey cataloging and analysis. In Advances in Knowledge Discovery and Data Mining, </title> <editor> eds. U.M. Fayyad, G. Piatetsky-Shapiro, P. Smyth, and R.S. Uthurasamy. </editor> <publisher> MIT Press. </publisher>
Reference-contexts: The KEFIR discovery system applied to health care by Matheus, Piatetsky-Shapiro, and McNeill (1995), for instance, is carefully tailored for a particular class of situations and could not have been easily used on the SKICAT application <ref> (Fayyad, Djorgovski, and Weir 1995) </ref>. I do not know of a universal learning or discovery algorithm (Buntine 1990), and a universal problem description for discovery is arguably too broad to be used as a program specification.
Reference: <author> Gilks, W.R., Thomas, A., and Spiegelhalter, D.J. </author> <year> 1993. </year> <title> A language and program for complex Bayesian modelling. </title> <journal> The Statistician, </journal> <volume> 43 </volume> <pages> 169-178. </pages>
Reference-contexts: Graphical models have been used in domains such as diagnosis, probabilistic expert systems, in planning and control (Dean 1 Unless there was some kind of time delay and feedback involved. Graphical Models for Discovering Knowledge 69 and Wellman 1991; Chan and Shachter 1992), and in statistical analysis of data <ref> (Gilks, Thomas, and Spiegelhalter 1993) </ref>, which is often more goal directed than typical knowledge discovery. <p> Probability and decision theory are used to decompose a problem into a computational prescription, and then search and optimization techniques are used to fill the prescription. A software tool exists that implements a special case of this conceptual framework using Gibbs sampling as the computational scheme <ref> (Gilks et al. 1993) </ref>. The Gibbs sampler is but one family of algorithms, and many more can be fit into this general framework. As explained by Buntine (1994), the framework of Figure 4.10 can use the categories of learning models described in Section 4.6 as its basis.
Reference: <author> Ginsberg, A., Weiss, S.M., and Politakis, P. </author> <year> 1988. </year> <title> Automatic knowledge base refinement for classification systems. </title> <journal> Artificial Intelligence, </journal> <volume> 35(2) </volume> <pages> 197-226. </pages>
Reference: <author> Hanson, R., Stutz, J., and Cheeseman, P. </author> <title> 1991 Bayesian classification with correlation and inheritance. </title> <booktitle> In International Joint Conference on Artificial Intelligence, </booktitle> <pages> 692-698. </pages> <address> San Mateo, </address> <publisher> California: </publisher> <editor> Morgan Kaufmann. </editor> <title> Graphical Models for Discovering Knowledge 85 Heckerman, </title> <address> D. </address> <year> 1995. </year> <title> Bayesian networks for knowledge representation and learning. In Advances in Knowledge Discovery and Data Mining, </title> <editor> eds. U.M. Fayyad, </editor> <publisher> G. </publisher>
Reference-contexts: This includes the stochastic networks used in Hopfield models and others in neural networks Hertz, Krogh, and Palmer (1991), 80 Buntine more complex unsupervised learning systems such as Autoclass IV which has a variety of covariances <ref> (Hanson, Stutz, and Cheeseman 1991) </ref>, and systems with multiple classes. 4.6 Learning algorithms Methods have been developed for learning simple discrete and Gaussian Bayesian networks from data, and for learning simple unsupervised models such as those mentioned in Section 4.5.4.
Reference: <author> Piatetsky-Shapiro, P. Smyth, </author> <title> and R.S. </title> <publisher> Uthurasamy. MIT Press. </publisher>
Reference: <author> Heckerman, D. </author> <year> 1990. </year> <title> Probabilistic similarity networks. </title> <journal> Networks, </journal> <volume> 20 </volume> <pages> 607-636. </pages>
Reference-contexts: Methods for combining probabilities from multiple 72 Buntine networks can involve more complex schemes. A method developed for medical diagnosis that is suitable to the topic spotting problem considered here is similarity networks <ref> (Heckerman 1990) </ref>. This is based on many graphs of the form of Figure 4.2 (c) used to distinguish pairs of topics. There are number of interesting questions for this decomposition approach.
Reference: <author> Henrion, M., Breese, J.S., and Horvitz, E.J. </author> <year> 1991. </year> <title> Decision analysis and expert systems. </title> <journal> AI Magazine, </journal> <volume> 12(4) </volume> <pages> 64-91. </pages>
Reference: <author> Hertz, J.A., and Krogh, A.S., and Palmer, R.G. </author> <year> 1991. </year> <title> Introduction to the Theory of Neural Computation. </title> <publisher> Addison-Wesley. </publisher>
Reference: <author> Jordan, M.I., and Jacobs, R.I. </author> <year> 1993. </year> <title> Supervised learning and divide-and-conquer: A statistical approach. </title> <booktitle> In Machine Learning: Proc. of the Tenth International Conference, </booktitle> <pages> 159-166. </pages> <address> San Mateo, California: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Kjruff, U. </author> <year> 1992. </year> <title> A computational scheme for reasoning in dynamic probabilistic networks. </title> <booktitle> In Uncertainty in Artificial Intelligence: Proceedings of the Eight Conference, </booktitle> <editor> eds. D. Dubois, M.P. Wellman, B.D. D'Ambrosio and P. Smets, </editor> <address> 121-129. San Mateo, California: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Kraft, P., and Buntine, W.L. </author> <year> 1993. </year> <title> Initial exploration of the ASRS database. </title> <booktitle> In Seventh International Symposium on Aviation Psychology, </booktitle> <address> Columbus, Ohio. </address>
Reference: <author> Lauritzen, S.L., Dawid, A.P., Larsen, B.N., and Leimer, H.-G. </author> <year> 1990. </year> <title> Independence properties of directed Markov fields. </title> <journal> Networks, </journal> <volume> 20 </volume> <pages> 491-505. </pages>
Reference: <author> McCullagh, P., and Nelder, J.A. </author> <year> 1989. </year> <title> Generalized Linear Models. </title> <publisher> London: Chapman and Hall. Second edition. </publisher>
Reference: <author> Madigan, D., Mosurski, K., and Almond, R.G. </author> <year> 1995. </year> <title> Explanation in belief networks. </title> <booktitle> StatSci research report 33, </booktitle> <address> StatSci/Mathsoft, Seattle, Washington. </address> <note> (Submitted for publication.) </note> <author> Matheus, C., Piatetsky-Shapiro, G., and McNeill, D. </author> <year> 1995. </year> <title> Key findings reporter for the analysis of healthcare information. In Advances in Knowledge Discovery and Data Mining, </title> <editor> eds. U.M. Fayyad, G. Piatetsky-Shapiro, P. Smyth, and R.S. Uthurasamy. </editor> <publisher> MIT Press. </publisher>
Reference-contexts: are flexible enough to represent supervised and unsupervised learning systems, neural networks, and many hybrids. * They come with well understood techniques for key tasks in the discovery process: problem formulation and decomposition, designing a learning algorithm (Buntine 1994), identification of valuable knowledge (using decision theory), and generation of explanations <ref> (Madigan, Mosurski, and Almond 1995) </ref>. Only a simple form of graphical model is considered in this chapter, the Bayesian network.
Reference: <author> Ousterhout, J.K. </author> <year> 1994. </year> <title> Tcl and the Tk Toolkit. </title> <publisher> Addison-Wesley. </publisher>
Reference-contexts: Initial exploration reveals some details and the discovery algorithm is modified as a result. Here, the discovery process parallels the iterative refinement strategies popular in software engineering. These strategies are made possible by rapid prototyping software such as Tcl/Tk used for developing interfaces <ref> (Ousterhout 1994) </ref>. This aspect of discovery is discussed further by Brachman and Anand (1995). The application of iterative refinement to knowledge discovery and knowledge acquisition is one way of viewing knowledge refinement (Ginsberg, Weiss, and Politakis 1988; Towell, Shavlik, and Noordewier 1990).
Reference: <author> Pearl, J. </author> <year> 1988. </year> <title> Probabilistic Reasoning in Intelligent Systems. </title> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Poland, W.B. </author> <year> 1994. </year> <title> Decision Analysis with Continuous and Discrete Variables: A Mixture Distribution Approach. </title> <type> Ph.D. </type> <institution> diss., Dept. of Engineering Economic Systems, Stanford Univ. </institution> <note> 86 Buntine Rabiner, L.R., </note> <author> and Juang, B.H. </author> <year> 1986. </year> <title> An introduction to hidden Markov models. </title> <journal> IEEE ASSP Magazine, </journal> <volume> January:4-16, </volume> <year> 1986. </year>
Reference-contexts: Graphical Models for Discovering Knowledge 69 and Wellman 1991; Chan and Shachter 1992), and in statistical analysis of data (Gilks, Thomas, and Spiegelhalter 1993), which is often more goal directed than typical knowledge discovery. Graphical models also generalize some aspects of Kalman filters <ref> (Poland 1994) </ref> used in control and hidden Markov models, the basic tool used in speech recognition (Rabiner and Juang 1986) and fault diagnosis (Smyth and Mellstrom 1992). Therefore graphical models are also used for dynamic systems and forecasting (Kjruff 1992; Dagum et al. 1995).
Reference: <author> Shachter, R.D., Andersen, S.K., and Szolovits, P. </author> <year> 1994. </year> <title> Global conditioning for probabilistic inference in belief networks. </title> <booktitle> In Uncertainty in Artificial Intelligence: Proceedings of the Tenth Conference, </booktitle> <editor> eds, R. Lopez de Mantaras and D. </editor> <booktitle> Poole, </booktitle> <pages> 514-522. </pages> <address> San Mateo, California: </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Probability models such as these are used primarily for performing inference on new problems. Graphical models are useful here because many kinds of inference can be performed on them. Basic inference involves calculating probabilities for arbitrary sets of variables <ref> (Shachter, Andersen and Szolovits 1994) </ref>. Graphical models have been used in domains such as diagnosis, probabilistic expert systems, in planning and control (Dean 1 Unless there was some kind of time delay and feedback involved.
Reference: <author> Shachter, R.D., and Heckerman, D. </author> <year> 1987. </year> <title> Thinking backwards for knowledge acquisition. </title> <journal> AI Magazine, 8(Fall):55-61. </journal>
Reference: <author> Shachter, R.D. </author> <year> 1986. </year> <title> Evaluating influence diagrams. </title> <journal> Operations Research, </journal> <volume> 34(6) </volume> <pages> 871-882. </pages>
Reference-contexts: Only a simple form of graphical model is considered in this chapter, the Bayesian network. Reasoning about the value of knowledge on Bayesian networks can be done by adding "value" nodes, and using the tools of influence diagrams and utility theory <ref> (Shachter 1986) </ref>, part of modern decision theory. This is not covered in this chapter.
Reference: <author> Smyth, P, and Mellstrom, J. </author> <year> 1992. </year> <title> Detecting novel classes with applications to fault diagnosis. </title> <booktitle> In Ninth International Conference on Machine Learning. </booktitle> <address> San Mateo, California: </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Graphical models also generalize some aspects of Kalman filters (Poland 1994) used in control and hidden Markov models, the basic tool used in speech recognition (Rabiner and Juang 1986) and fault diagnosis <ref> (Smyth and Mellstrom 1992) </ref>. Therefore graphical models are also used for dynamic systems and forecasting (Kjruff 1992; Dagum et al. 1995). Various methods for learning simple kinds of graphical models from data also exist (Heckerman 1995).
Reference: <author> Spiegelhalter, D.J., Dawid, A.P., Lauritzen, S.L., and Cowell, R.G. </author> <year> 1993. </year> <title> Bayesian analysis in expert systems. </title> <journal> Statistical Science, </journal> <volume> 8(3) </volume> <pages> 219-283. </pages>
Reference-contexts: Graphical models have been used in domains such as diagnosis, probabilistic expert systems, in planning and control (Dean 1 Unless there was some kind of time delay and feedback involved. Graphical Models for Discovering Knowledge 69 and Wellman 1991; Chan and Shachter 1992), and in statistical analysis of data <ref> (Gilks, Thomas, and Spiegelhalter 1993) </ref>, which is often more goal directed than typical knowledge discovery.
Reference: <author> Titterington, D.M., Smith, A.F.M., and Makov, U.E. </author> <year> 1985. </year> <title> Statistical Analysis of Finite Mixture Distributions. </title> <address> Chichester: </address> <publisher> John Wiley & Sons. </publisher>
Reference-contexts: The graphical model of Figure 4.7 therefore yields the following conditional probability: p (cjx; 1 ; 2j1 ; 12 ) = g 1 X log-lin (g 2 ; x; 2jg 1 ) log-lin (c; x; g 1 g 2 ) : This is a mixture model <ref> (Titterington, Smith, and Makov 1985) </ref>, in the sense that it sums over hidden variables g 1 and g 2 , where the basic joint probability p (c; g 1 ; g 2 jx; 1 ; 2j1 ; 12 ) is in a standard form. <p> This category of models are used to model unsupervised learning, incomplete data in the classification problems, robust regression, and general density estimation <ref> (Titterington et al. 1985) </ref>. The mixture model category can often be learned using the EM algorithm. The EM algorithm has an inner loop using the closed form solution found for the underlying exponential family model.
Reference: <author> Towell, G.G., Shavlik, J.W., and Noordewier, M.O. </author> <year> 1990. </year> <title> Refinement of approximate domain theories by knowledge-based neural networks. </title> <booktitle> In Eighth National Conference on Artificial Intelligence, </booktitle> <pages> 861-866. </pages> <address> Boston, Massachusetts: </address> <publisher> AAAI Press. </publisher>
Reference: <author> Whittaker, J. </author> <year> 1990. </year> <title> Graphical Models in Applied Multivariate Statistics. </title> <publisher> Wiley. </publisher>
References-found: 39

