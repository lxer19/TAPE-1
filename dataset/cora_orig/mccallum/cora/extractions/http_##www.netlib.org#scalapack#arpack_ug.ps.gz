URL: http://www.netlib.org/scalapack/arpack_ug.ps.gz
Refering-URL: http://www.netlib.org/scalapack/
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Title: ARPACK USERS GUIDE: Solution of Large Scale Eigenvalue Problems by Implicitly Restarted Arnoldi Methods.  
Author: R. B. Lehoucq, D. C. Sorensen, C. Yang 
Date: -DRAFT- 31 July 96  
Abstract-found: 0
Intro-found: 1
Reference: [1] <author> E. Anderson, Z. Bai, C. Bischof, J. Demmel, J. Dongarra, J. Du Croz, A. Green-baum, S. Hammarling, A. McKenney, S. Ostrouchov, and D. Sorensen. </author> <title> LAPACK Users' Guide. </title> <publisher> SIAM, </publisher> <address> Philadelphia, PA, </address> <note> second edition, </note> <year> 1995. </year>
Reference-contexts: The goal is to provide some understanding of the underlying algorithm, expected behavior, additional references, and capabilities as well as limitations of the software. 1.7 Dependence on LAPACK and BLAS ARPACK is dependent upon a number of subroutines from LAPACK <ref> [1] </ref> and the BLAS [10, 9, 19]. The necessary routines are distributed along with the ARPACK software. Whenever possible, BLAS routines that have been optimized for the given machine should be used in place of the ones provided with ARPACK. <p> The auxiliary routine Xlahqr implements the standard double shift form of the QR algorithm for determining the eigenvalues and Schur decomposition. For further details and information, see Chapter 2 and Appendices A and B in <ref> [1] </ref>. Tables 5.2 and 5.3 list all the LAPACK routines used by ARPACK. The current release of LAPACK used is version 2.0. -DRAFT- 31 July 96 5.2. LAPACK ROUTINES USED BY ARPACK 78 Table 5.3: Description of the LAPACK auxiliary routines used by ARPACK.
Reference: [2] <author> W. E. </author> <title> Arnoldi. The principle of minimized iterations in the solution of the matrix eigenvalue problem. </title> <journal> Quart. J. Applied Mathematics, </journal> <volume> 9 </volume> <pages> 17-29, </pages> <year> 1951. </year>
Reference-contexts: The second observation leads to the Lanczos/Arnoldi process <ref> [2, 18] </ref>. 4.3 The Arnoldi Factorization Definition : If A 2 C nfin then a relation of the form AV k = V k H k + f k e T -DRAFT- 31 July 96 CHAPTER 4.
Reference: [3] <author> Z. Bai, J. Demmel, and A. Mckenney. </author> <title> On computing condition numbers for the nonsymmetric eigenproblem. </title> <journal> ACM Transactions on Mathematical Software, </journal> <volume> 19(2) </volume> <pages> 202-223, </pages> <month> June </month> <year> 1993. </year> <note> LAPACK Working Note. </note>
Reference-contexts: If ' is the positive angle between x and ^ x then ' sep (; R 22 ) F ): Proof : See x 5 in <ref> [3] </ref>. The definition of the quantity sep in this theorem is sep (; R 22 ) min kz H z H R 22 k F ; and the norm kEk F = (trace E H E) 1 2 is the Frobenius norm.
Reference: [4] <author> J. Cullum. </author> <title> The simultaneous computation of a few of the algebraically largest and smallest eigenvalues of a large, symmetric, sparse matrix. </title> <journal> BIT, </journal> <volume> 18 </volume> <pages> 265-275, </pages> <year> 1978. </year>
Reference: [5] <author> J. Cullum and W. E. Donath. </author> <title> A block Lanczos algorithm for computing the q algebraically largest eigenvalues and a corresponding eigenspace for large, sparse symmetric matrices. </title> <booktitle> In Proceedings of the 1974 IEEE Conference on Decision and Control, </booktitle> <pages> pages 505-509, </pages> <address> New York, </address> <year> 1974. </year>
Reference-contexts: An iteration is defined by a repeatly restarting until the current Arnoldi factorization contains the desired information. Saad's ideas were based on similar ones developed for the Lanczos process by Paige [29], Cullum and Donath <ref> [5] </ref>, and Golub and Underwood [15]. It appears that Karush [17] proposed the first example of a restarted iteration. The ARPACK software is based upon another approach to restarting that offers a more efficient and numerically stable formulation.
Reference: [6] <author> J. Cullum and R. A. Willoughby. </author> <title> Computing eigenvalues of very large symmetric matrices|an implementation of a Lanczos algorithm with no reorthogonaliza-tion. </title> <journal> Journal of Computational Physics, </journal> <volume> 434 </volume> <pages> 329-358, </pages> <year> 1981. </year>
Reference: [7] <author> J. Daniel, W. B. Gragg, L. Kaufman, and G. W. Stewart. </author> <title> Reorthogonaliza-tion and stable algorithms for updating the Gram-Schmidt QR factorization. </title> <journal> Mathematics of Computation, </journal> <volume> 30 </volume> <pages> 772-795, </pages> <year> 1976. </year>
Reference-contexts: In finite precision arithmetic, care must be taken to assure that the computed vectors are orthogonal to working precision. The method proposed by Daniel, Gragg, Kaufman and Stewart (DGKS) in <ref> [7] </ref> provides an excellent way to construct a vector f j+1 that is numerically orthogonal to V j+1 . It amounts to computing a correction c = V H just after Step (a2.4) if necessary. A simple test is used to avoid this DGKS correction if it is not needed.
Reference: [8] <author> J. J. Dongarra, I. S. Duff, D. C. Sorensen, and H. A. Van der Vorst. </author> <title> Solving Linear systems on Vector and shared memory computers. </title> <publisher> SIAM, </publisher> <address> Philadelphia, PA., </address> <year> 1991. </year>
Reference-contexts: This is quite important for performance on vector, and parallel-vector supercomputers. The Level 2 BLAS operation GEMV is easily parallelized and vectorized and has a much better ratio of floating point computation to data movement <ref> [10, 8] </ref> than the Level 1 BLAS operations. The information obtained through this process is completely determined by the choice of the starting vector. Eigen-information of interest may not appear until k gets very large.
Reference: [9] <author> J.J. Dongarra, J. DuCroz, I. S. Duff, and S. Hammarling. </author> <title> A set of level 3 basic linear algebra subprograms. </title> <journal> ACM Transactions on Mathematical Software, </journal> <volume> 16(1) </volume> <pages> 1-17, </pages> <year> 1990. </year> <note> BIBLIOGRAPHY 126 </note>
Reference-contexts: The goal is to provide some understanding of the underlying algorithm, expected behavior, additional references, and capabilities as well as limitations of the software. 1.7 Dependence on LAPACK and BLAS ARPACK is dependent upon a number of subroutines from LAPACK [1] and the BLAS <ref> [10, 9, 19] </ref>. The necessary routines are distributed along with the ARPACK software. Whenever possible, BLAS routines that have been optimized for the given machine should be used in place of the ones provided with ARPACK. A list of routines required from these two sources is available in Chapter 5.
Reference: [10] <author> J.J. Dongarra, J. DuCroz, S. Hammarling, and R. J. Hanson. </author> <title> An extended set of Fortran basic linear algebra subprograms. </title> <journal> ACM Trans. on Math. Software, </journal> <volume> 14(1) </volume> <pages> 1-17, </pages> <year> 1988. </year>
Reference-contexts: The goal is to provide some understanding of the underlying algorithm, expected behavior, additional references, and capabilities as well as limitations of the software. 1.7 Dependence on LAPACK and BLAS ARPACK is dependent upon a number of subroutines from LAPACK [1] and the BLAS <ref> [10, 9, 19] </ref>. The necessary routines are distributed along with the ARPACK software. Whenever possible, BLAS routines that have been optimized for the given machine should be used in place of the ones provided with ARPACK. A list of routines required from these two sources is available in Chapter 5. <p> This is quite important for performance on vector, and parallel-vector supercomputers. The Level 2 BLAS operation GEMV is easily parallelized and vectorized and has a much better ratio of floating point computation to data movement <ref> [10, 8] </ref> than the Level 1 BLAS operations. The information obtained through this process is completely determined by the choice of the starting vector. Eigen-information of interest may not appear until k gets very large.
Reference: [11] <author> T. Ericsson and A. Ruhe. </author> <title> The spectral transformation Lanczos method for the numerical solution of large sparse generalized symmetric eigenvalue problems. </title> <journal> Mathematics of Computation, </journal> <volume> 35 </volume> <pages> 1251-1268, </pages> <month> October </month> <year> 1980. </year>
Reference-contexts: This correction was originally suggested by Ericsson and Ruhe <ref> [11] </ref> as a mean of performing a formal step of the power method with S: The residual error of the computed Ritz vector with respect to the original problem is kAx Mxk = kMf k k sj where = + 1=: Keeping in mind that under the spectral transformation jj is <p> This may be incorporated into ARPACK at a future date. Spectral transformations were studied extensively by Ericsson and Ruhe <ref> [11] </ref> and the first eigenvector purification strategy was developed in [28]. Shift and invert techniques play an essential role in the block Lanczos code developed by Grimes, Lewis, and Simon and the many nuances of this technique in practical applications are discussed thoroughly in [16].
Reference: [12] <author> J. G. F. Francis. </author> <title> The QR transformation|part 1. </title> <journal> The Computer Journal, </journal> <volume> 4 </volume> <pages> 265-271, </pages> <month> October </month> <year> 1961. </year>
Reference-contexts: The columns of Q are called Schur vectors in general and these are eigenvectors of A if and only if A is normal. For purposes of algorithmic development this structure is fundamental. In fact, the well known Implicitly Shifted QR-Algorithm <ref> [12, 13] </ref> is designed to produce a sequence of unitary similarity transformations Q j that iteratively reduce A to upper triangular form.
Reference: [13] <author> J. G. F. Francis. </author> <title> The QR transformation|part 2. </title> <journal> The Computer Journal, </journal> <volume> 4 </volume> <pages> 332-345, </pages> <month> January </month> <year> 1962. </year>
Reference-contexts: The columns of Q are called Schur vectors in general and these are eigenvectors of A if and only if A is normal. For purposes of algorithmic development this structure is fundamental. In fact, the well known Implicitly Shifted QR-Algorithm <ref> [12, 13] </ref> is designed to produce a sequence of unitary similarity transformations Q j that iteratively reduce A to upper triangular form.
Reference: [14] <author> G. H. Golub and C. F. Van Loan. </author> <title> Matrix Computations. </title> <publisher> Johns Hopkins, </publisher> <address> Balti-more, </address> <note> second edition, </note> <year> 1989. </year>
Reference-contexts: The elegant details of an efficient and stable implementation would be too much of a digression here. They may be found in <ref> [14] </ref>. The convergence -DRAFT- 31 July 96 CHAPTER 4. THE IMPLICITLY RESTARTED ARNOLDI METHOD 53 behavior of this iteration is fascinating. The columns of V converge to Schur vectors at various rates.
Reference: [15] <author> G. H. Golub and R. Underwood. </author> <title> The block Lanczos method for computing eigenvalues. </title> <editor> In J. R. Rice, editor, </editor> <booktitle> Mathematical Software III, </booktitle> <pages> pages 361-377, </pages> <address> New York, 1977. </address> <publisher> Academic Press. </publisher>
Reference-contexts: An iteration is defined by a repeatly restarting until the current Arnoldi factorization contains the desired information. Saad's ideas were based on similar ones developed for the Lanczos process by Paige [29], Cullum and Donath [5], and Golub and Underwood <ref> [15] </ref>. It appears that Karush [17] proposed the first example of a restarted iteration. The ARPACK software is based upon another approach to restarting that offers a more efficient and numerically stable formulation.
Reference: [16] <author> R. G. Grimes, J. G. Lewis, and H. D. Simon. </author> <title> A shifted block Lanczos algorithm for solving sparse symmetric generalized eigenproblems. </title> <journal> SIAM J. Matrix Analysis and Applications, </journal> <volume> 15(1) </volume> <pages> 228-272, </pages> <month> January </month> <year> 1994. </year>
Reference-contexts: Shift and invert techniques play an essential role in the block Lanczos code developed by Grimes, Lewis, and Simon and the many nuances of this technique in practical applications are discussed thoroughly in <ref> [16] </ref>. The development presented here and the eigenvector purification through implicit restarting is due to Meerbergen and Spence [26]. 4.6 Stopping Criteria This section considers the important question of determining when a length m Arnoldi factorization has computed approximate eigenvalues of acceptable accuracy. -DRAFT- 31 July 96 4.6.
Reference: [17] <author> W. Karush. </author> <title> An iterative method for finding characteristics vectors of a symmetric matrix. </title> <journal> Pacific J. Mathematics, </journal> <volume> 1 </volume> <pages> 233-248, </pages> <year> 1951. </year>
Reference-contexts: An iteration is defined by a repeatly restarting until the current Arnoldi factorization contains the desired information. Saad's ideas were based on similar ones developed for the Lanczos process by Paige [29], Cullum and Donath [5], and Golub and Underwood [15]. It appears that Karush <ref> [17] </ref> proposed the first example of a restarted iteration. The ARPACK software is based upon another approach to restarting that offers a more efficient and numerically stable formulation.
Reference: [18] <author> C. </author> <title> Lanczos. An iteration method for the solution of the eigenvalue problem of linear differential and integral operators. </title> <journal> J. Research of the National Bureau of Standards, </journal> <volume> 45(4) </volume> <pages> 255-282, </pages> <month> October </month> <year> 1950. </year> <note> Research Paper 2133. </note>
Reference-contexts: The second observation leads to the Lanczos/Arnoldi process <ref> [2, 18] </ref>. 4.3 The Arnoldi Factorization Definition : If A 2 C nfin then a relation of the form AV k = V k H k + f k e T -DRAFT- 31 July 96 CHAPTER 4.
Reference: [19] <author> C. L. Lawson, R. J. Hanson, D. R. Kincaid, and F. T. Krogh. </author> <title> Basic linear algebra subprograms for Fortran usage. </title> <journal> ACM Transactions on Mathematical Software, </journal> <volume> 5(3) </volume> <pages> 308-323, </pages> <year> 1979. </year>
Reference-contexts: The goal is to provide some understanding of the underlying algorithm, expected behavior, additional references, and capabilities as well as limitations of the software. 1.7 Dependence on LAPACK and BLAS ARPACK is dependent upon a number of subroutines from LAPACK [1] and the BLAS <ref> [10, 9, 19] </ref>. The necessary routines are distributed along with the ARPACK software. Whenever possible, BLAS routines that have been optimized for the given machine should be used in place of the ones provided with ARPACK. A list of routines required from these two sources is available in Chapter 5.
Reference: [20] <author> R. B. Lehoucq. </author> <title> Analysis and Implementation of an Implicitly Restarted Iteration. </title> <type> PhD thesis, </type> <institution> Rice University, Houston, Texas, </institution> <month> May </month> <year> 1995. </year> <note> Also available as Technical Report TR95-13, </note> <institution> Dept. of Computational and Applied Mathematics. </institution>
Reference-contexts: This discussion is intended to give a broad overview of the theory and to develop a high level description of the algorithms. Specific implementation details concerned with efficiency and numerical stability are treated in Chapter 5. Further information may found in <ref> [38, 23, 39, 20, 21, 27] </ref>. The basic iteration of the IRAM is outlined in Figure 4.1 for those familiar with Krylov subspace methods and basic dense eigenvalue methods. <p> However, these details are extremely important to the success of this iteration in difficult cases. Complete details of these numerical refinements may be found in <ref> [23, 20] </ref>. The above iteration can be used to apply any known polynomial restart. If the roots of the polynomial are not known there is an alternative implementation that only requires one to compute q 1 = (H m )e 1 where is the desired degree p polynomial. <p> In theory, the implicit restarting mechanism would obviate the need for this. However, computing in finite precision arithmetic (as usual) complicates the issue and make these final reorderings mandatory. See Chapter 5 in <ref> [20] </ref> and [23] for further information.
Reference: [21] <author> R. B. Lehoucq. </author> <title> Restarting an Arnoldi reduction. </title> <type> Preprint MCS-P591-0496, </type> <institution> Argonne National Laboratory, Argonne, IL, </institution> <year> 1996. </year>
Reference-contexts: This discussion is intended to give a broad overview of the theory and to develop a high level description of the algorithms. Specific implementation details concerned with efficiency and numerical stability are treated in Chapter 5. Further information may found in <ref> [38, 23, 39, 20, 21, 27] </ref>. The basic iteration of the IRAM is outlined in Figure 4.1 for those familiar with Krylov subspace methods and basic dense eigenvalue methods. <p> Thus the exact shift strategy can be viewed both as a means to damp unwanted components from the starting vector and also as directly forcing the starting vector to be a linear combination of wanted eigenvectors. See <ref> [21, 38] </ref> further information convergence of an IRAM and other possible shift strategies. 4.5 The Generalized Eigenvalue Problem A typical source of large scale eigenproblems is through a discrete form of a continuous problem. The resulting finite dimensional problems become large due to accuracy requirements and spatial dimensionality.
Reference: [22] <author> R. B. Lehoucq and J. A. Scott. </author> <title> An evaluation of software for computing eigen-values of sparse nonsymmetric matrices. </title> <type> Preprint MCS-P547-1195, </type> <institution> Argonne National Laboratory, Argonne, IL, </institution> <year> 1995. </year> <note> -DRAFT- 31 July 96 BIBLIOGRAPHY 127 </note>
Reference-contexts: The backward error is defined as the smallest, in norm, perturbation A such that the Ritz pair is an eigenpair for A + A. The recent study <ref> [22] </ref> presents a thorough discussion of the many issues involved in determining stopping criteria for the nonsymmetric eigenvalue problem. In ARPACK we are more stringent than just asking for a small backward error relative to kAk.
Reference: [23] <author> R. B. Lehoucq and D. C. Sorensen. </author> <title> Deflation techniques for an implicitly restarted arnoldi iteration. </title> <journal> SIAM J. Matrix Analysis and Applications, </journal> <note> 1996. To appear. </note>
Reference-contexts: This discussion is intended to give a broad overview of the theory and to develop a high level description of the algorithms. Specific implementation details concerned with efficiency and numerical stability are treated in Chapter 5. Further information may found in <ref> [38, 23, 39, 20, 21, 27] </ref>. The basic iteration of the IRAM is outlined in Figure 4.1 for those familiar with Krylov subspace methods and basic dense eigenvalue methods. <p> However, these details are extremely important to the success of this iteration in difficult cases. Complete details of these numerical refinements may be found in <ref> [23, 20] </ref>. The above iteration can be used to apply any known polynomial restart. If the roots of the polynomial are not known there is an alternative implementation that only requires one to compute q 1 = (H m )e 1 where is the desired degree p polynomial. <p> In theory, the implicit restarting mechanism would obviate the need for this. However, computing in finite precision arithmetic (as usual) complicates the issue and make these final reorderings mandatory. See Chapter 5 in [20] and <ref> [23] </ref> for further information.
Reference: [24] <author> T. A. Manteuffel. </author> <title> Adaptive procedure for estimating parameters for the nonsymmetric Tchebychev iteration. </title> <journal> Numerische Mathematik, </journal> <volume> 31 </volume> <pages> 183-208, </pages> <year> 1978. </year>
Reference-contexts: Implicit Restarting A restarting alternative has been proposed by Saad based upon the polynomial ac celeration scheme developed by Manteuffel <ref> [24] </ref> for the iterative solution of linear -DRAFT- 31 July 96 4.4. RESTARTING THE ARNOLDI METHOD 58 systems. Saad [34] proposed to restart the factorization with a vector that has been preconditioned so that it is more nearly in a k-dimensional invariant subspace of interest.
Reference: [25] <author> K. J. Maschhoff and D. C. Sorensen. </author> <title> A portable implementation of ARPACK for distributed memory parallel architectures. </title> <booktitle> In Proceedings of the Copper Mountain Conference on Iterative Methods, </booktitle> <month> April 9-13, </month> <journal> 1996., </journal> <volume> volume 1, </volume> <year> 1996. </year>
Reference-contexts: PARPACK has been installed on CRAY-T3D, Intel Delta and Paragon, IBM-SP2, an SGI cluster and a network of Sun workstations. The package runs efficiently in each of these environments. More detailed information about Parallel ARPACK is available in the report by Maschhoff and Sorensen <ref> [25] </ref>. 1.10 Trouble Shooting and Problems An up to date list of known problems is available in pub/people/sorensen/ARPACK/Known_Problems Any difficulties with using the software should be reported to arpack@caam.rice.edu 1.11 Research Funding of ARPACK Financial support for this work was provided in part by the National Science Foundation cooperative agreement CCR-912008,
Reference: [26] <author> Karl Meerbergen and Alastair Spence. </author> <title> Implicitly restarted Arnoldi with purification for the shift-invert transformation. </title> <journal> Mathematics of Computation, </journal> <note> 1996. To appear. </note>
Reference-contexts: This is the purification used in ARPACK. Another recent suggestion due to Meerbergen and Spence is to use implicit restarting with a zero shift <ref> [26] </ref>. Recall that implicit restarting with ` zero shifts is equivalent to starting the M-Arnoldi process with a starting vector of S ` v 1 and all the resulting Ritz vectors will be multiplied by S ` as well. <p> The development presented here and the eigenvector purification through implicit restarting is due to Meerbergen and Spence <ref> [26] </ref>. 4.6 Stopping Criteria This section considers the important question of determining when a length m Arnoldi factorization has computed approximate eigenvalues of acceptable accuracy. -DRAFT- 31 July 96 4.6.
Reference: [27] <author> R. B. Morgan. </author> <title> On restarting the Arnoldi method for large nonsymmetric eigenvalue problems. </title> <journal> Mathematics of Computation, </journal> <volume> 65(215) </volume> <pages> 1213-1230, </pages> <month> July </month> <year> 1996. </year>
Reference-contexts: This discussion is intended to give a broad overview of the theory and to develop a high level description of the algorithms. Specific implementation details concerned with efficiency and numerical stability are treated in Chapter 5. Further information may found in <ref> [38, 23, 39, 20, 21, 27] </ref>. The basic iteration of the IRAM is outlined in Figure 4.1 for those familiar with Krylov subspace methods and basic dense eigenvalue methods.
Reference: [28] <author> B. Nour-Omid, B. N. Parlett, and Thomas Ericsson Paul S. Jensen. </author> <title> How to implement the spectral transformation. </title> <journal> Mathematics of Computation, </journal> <volume> 48(178) </volume> <pages> 663-673, </pages> <month> April </month> <year> 1987. </year>
Reference-contexts: This may be incorporated into ARPACK at a future date. Spectral transformations were studied extensively by Ericsson and Ruhe [11] and the first eigenvector purification strategy was developed in <ref> [28] </ref>. Shift and invert techniques play an essential role in the block Lanczos code developed by Grimes, Lewis, and Simon and the many nuances of this technique in practical applications are discussed thoroughly in [16].
Reference: [29] <author> C. C. Paige. </author> <title> The computation of eigenvalues and eigenvectors of very large sparse matrices. </title> <type> PhD thesis, </type> <institution> University of London, </institution> <address> London, England, </address> <year> 1971. </year>
Reference-contexts: An iteration is defined by a repeatly restarting until the current Arnoldi factorization contains the desired information. Saad's ideas were based on similar ones developed for the Lanczos process by Paige <ref> [29] </ref>, Cullum and Donath [5], and Golub and Underwood [15]. It appears that Karush [17] proposed the first example of a restarted iteration. The ARPACK software is based upon another approach to restarting that offers a more efficient and numerically stable formulation.
Reference: [30] <author> B. N. Parlett. </author> <title> The Symmetric Eigenvalue Problem. </title> <publisher> Prentice-Hall, </publisher> <address> Englewood Cliffs, N.J., </address> <year> 1980. </year>
Reference-contexts: (assuming ksk = 1) and the associated Rayleigh Quotient residual r (x) = Ax x satisfies kr (x)k = jfi k e T When A is Hermitian, this relation may be used to provide computable rigorous bounds on the accuracy of the eigenvalues of H k as approximations to eigenval-ues <ref> [30] </ref> of A: When A is non-Hermitian the possibility of non-normality precludes such bounds and one can only say that the Rayleigh Quotient residual is small if jfi k e T k sj is small without further information. <p> However, an analogous statment in the non-Hermaitian case is not possible without further information concerning non-normalilty and defectiveness. We shall develop a crude but effective assessment of accuracy based upon this estimate. Far more sophisticated analysis is available for the symmetric problem in <ref> [30] </ref> and in [35] for the non-symmetric case.
Reference: [31] <author> B. N. Parlett and W. G. Poole. </author> <title> A geometric theory for the QR, LU, and power iterations. </title> <journal> SIAM J. Numerical Analysis, </journal> <volume> 10(2) </volume> <pages> 389-412, </pages> <month> April </month> <year> 1973. </year>
Reference-contexts: The convergence -DRAFT- 31 July 96 CHAPTER 4. THE IMPLICITLY RESTARTED ARNOLDI METHOD 53 behavior of this iteration is fascinating. The columns of V converge to Schur vectors at various rates. These rates are fundamentally linked to the simple power method and its rapidly convergent variant, inverse iteration <ref> [31, 41] </ref>. Despite the extremely fast rate of convergence and the efficient use of storage, the implicitly shifted QR method is not suitable for large scale problems and it has proved to be extremely difficult to parallelize.
Reference: [32] <author> B. N. Parlett and D. Scott. </author> <title> The Lanczos algorithm with selective orthogonaliza-tion. </title> <journal> Mathematics of Computation, </journal> <volume> 33 </volume> <pages> 217-238, </pages> <year> 1979. </year>
Reference: [33] <author> Y. Saad. </author> <title> Variations on Arnoldi's method for computing eigenelements of large unsymmetric matrices. </title> <journal> Linear Algebra and Its Applications, </journal> <volume> 34 </volume> <pages> 269-295, </pages> <year> 1980. </year>
Reference: [34] <author> Y. Saad. </author> <title> Chebyshev acceleration techniques for solving nonsymmetric eigenvalue problems. </title> <journal> Mathematics of Computation, </journal> <volume> 42 </volume> <pages> 567-588, </pages> <year> 1984. </year>
Reference-contexts: Implicit Restarting A restarting alternative has been proposed by Saad based upon the polynomial ac celeration scheme developed by Manteuffel [24] for the iterative solution of linear -DRAFT- 31 July 96 4.4. RESTARTING THE ARNOLDI METHOD 58 systems. Saad <ref> [34] </ref> proposed to restart the factorization with a vector that has been preconditioned so that it is more nearly in a k-dimensional invariant subspace of interest. This preconditioning takes the form of a polynomial applied to the starting vector that is constructed to damp unwanted components from the eigenvector expansion. <p> Typically, an ellipse that encloses the set u but excludes the set w is constructed and the Chebyshev polynomial of degree p that is small on the ellipse is then specified <ref> [34] </ref>. 5.1.3 XYaitr Subroutine XYaitr is responsible for all the work associated with building the needed factorization. It implements Algorithm 2 of Chapter 4 using the classical Gram-Schmidt procedure with possible re-orthogonalization by the DGKS scheme.
Reference: [35] <author> Y. Saad. </author> <title> Numerical Methods for Large Eigenvalue Problems. </title> <publisher> Halsted Press, </publisher> <year> 1992. </year>
Reference-contexts: A thorough discussion is given by Saad in <ref> [35] </ref> and in his earlier papers. These facts have important algorithmic consequences. <p> However, an analogous statment in the non-Hermaitian case is not possible without further information concerning non-normalilty and defectiveness. We shall develop a crude but effective assessment of accuracy based upon this estimate. Far more sophisticated analysis is available for the symmetric problem in [30] and in <ref> [35] </ref> for the non-symmetric case.
Reference: [36] <author> Y. Saad and M. H. Schultz. </author> <title> GMRES: A generalized minimal residual algorithm for solving nonsymmetric linear systems. </title> <journal> SIAM Journal on Scientific and Statistical Computing, </journal> <volume> 7(3) </volume> <pages> 856-869, </pages> <month> July </month> <year> 1986. </year> <note> -DRAFT- 31 July 96 BIBLIOGRAPHY 128 </note>
Reference-contexts: v k+1 ) H k k where fi k = kf k k and v k+1 = 1 f k : This factorization may be used to obtain approximate solutions to a linear system Ax = b if b = v 1 fi 0 and this underlies the GMRES method <ref> [36] </ref>. However, the purpose here is to investigate the use of this factorization to obtain approximate eigenvalues and eigenvectors.
Reference: [37] <author> H. Simon. </author> <title> Analysis of the symmetric Lanczos algorithm with reorthogonalization methods. </title> <journal> Linear Algebra and Its Applications, </journal> <volume> 61 </volume> <pages> 101-131, </pages> <year> 1984. </year>
Reference: [38] <author> D. C. Sorensen. </author> <title> Implicit application of polynomial filters in a k-step Arnoldi method. </title> <journal> SIAM J. Matrix Analysis and Applications, </journal> <volume> 13(1) </volume> <pages> 357-385, </pages> <month> January </month> <year> 1992. </year>
Reference-contexts: This discussion is intended to give a broad overview of the theory and to develop a high level description of the algorithms. Specific implementation details concerned with efficiency and numerical stability are treated in Chapter 5. Further information may found in <ref> [38, 23, 39, 20, 21, 27] </ref>. The basic iteration of the IRAM is outlined in Figure 4.1 for those familiar with Krylov subspace methods and basic dense eigenvalue methods. <p> Full details may be found in <ref> [38] </ref>. <p> Thus the exact shift strategy can be viewed both as a means to damp unwanted components from the starting vector and also as directly forcing the starting vector to be a linear combination of wanted eigenvectors. See <ref> [21, 38] </ref> further information convergence of an IRAM and other possible shift strategies. 4.5 The Generalized Eigenvalue Problem A typical source of large scale eigenproblems is through a discrete form of a continuous problem. The resulting finite dimensional problems become large due to accuracy requirements and spatial dimensionality.
Reference: [39] <author> D. C. Sorensen. </author> <title> Implicitly restarted Arnoldi/Lanczos methods for large scale eigenvalue calculations. </title> <editor> In D. E. Keyes, A. Sameh, and V. Venkatakrishnan, editors, </editor> <booktitle> Parallel Numerical Algorithms, </booktitle> <address> Dordrecht, </address> <year> 1995. </year> <note> Kluwer. To appear. </note>
Reference-contexts: This discussion is intended to give a broad overview of the theory and to develop a high level description of the algorithms. Specific implementation details concerned with efficiency and numerical stability are treated in Chapter 5. Further information may found in <ref> [38, 23, 39, 20, 21, 27] </ref>. The basic iteration of the IRAM is outlined in Figure 4.1 for those familiar with Krylov subspace methods and basic dense eigenvalue methods.
Reference: [40] <author> J. M. Varah. </author> <title> On the separation of two matrices. </title> <journal> SIAM Journal on Numerical Analysis, </journal> <volume> 16(2) </volume> <pages> 216-222, </pages> <month> April </month> <year> 1979. </year>
Reference-contexts: This is a more refined indicator of eigenvector sensitivity that accounts for non normality as well as clustering of eigenvalues. Varah <ref> [40] </ref> shows that sep (; R 22 ) min j i j; jy H xj 1 jy H xj 2 where the latter bound is only defined for nonzero r 12 .
Reference: [41] <author> D. S. Watkins and L. Elsner. </author> <title> Convergence of algorithms of decomposition type for the eigenvalue problem. </title> <journal> Linear Algebra and Its Applications, </journal> <volume> 143 </volume> <pages> 19-47, </pages> <year> 1991. </year>
Reference-contexts: The convergence -DRAFT- 31 July 96 CHAPTER 4. THE IMPLICITLY RESTARTED ARNOLDI METHOD 53 behavior of this iteration is fascinating. The columns of V converge to Schur vectors at various rates. These rates are fundamentally linked to the simple power method and its rapidly convergent variant, inverse iteration <ref> [31, 41] </ref>. Despite the extremely fast rate of convergence and the efficient use of storage, the implicitly shifted QR method is not suitable for large scale problems and it has proved to be extremely difficult to parallelize.
Reference: [42] <author> J. H. Wilkinson. </author> <title> The Algebraic Eigenvalue Problem. </title> <publisher> Clarendon Press, Oxford, </publisher> <address> UK, </address> <year> 1965. </year> <month> -DRAFT- 31 July 96 </month>
References-found: 42

