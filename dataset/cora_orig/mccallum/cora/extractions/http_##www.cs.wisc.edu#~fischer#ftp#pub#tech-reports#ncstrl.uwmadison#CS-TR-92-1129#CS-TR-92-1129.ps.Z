URL: http://www.cs.wisc.edu/~fischer/ftp/pub/tech-reports/ncstrl.uwmadison/CS-TR-92-1129/CS-TR-92-1129.ps.Z
Refering-URL: http://www.cs.wisc.edu/~fischer/ftp/pub/tech-reports/ncstrl.uwmadison/CS-TR-92-1129/
Root-URL: http://www.cs.wisc.edu
Title: Mathematical Programming in Neural Networks  
Author: O. L. Mangasarian 
Date: December 11, 1992  
Abstract: This paper highlights the role of mathematical programming, particularly linear programming, in training neural networks. A neural network description is given in terms of separating planes in the input space that suggests the use of linear programming for determining these planes. A more standard description in terms of a mean square error in the output space is also given, which leads to the use of unconstrained minimization techniques for training a neural network. The linear programming approach is demonstrated by a brief description of a system for breast cancer diagnosis that has been in use for the last four years at a major medical facility.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> E. Aarts and J. Korst. </author> <title> Simulated Annealing and Boltzman Machines. </title> <publisher> John Wiley & Sons, </publisher> <address> Chichester, </address> <year> 1990. </year>
Reference-contexts: Whether the converse is also true, is an interesting but not completely settled question [23, pages 76-79], even though there have been a number of interesting applications of neural networks to optimization problems, for example <ref> [37, 1, 14] </ref> and [23, pages 71-87]. Acknowledgement I wish to thank my colleagues Yann le Cun, Renato De Leone, Laurence Dixon, Greg Madey, Jim Noyes, Boris Polyak, Steve Robinsosn and Jude Shavlik for helpful references that they furnished, some of which have been cited.
Reference: [2] <author> R. Battiti. </author> <title> First- and second-order methods for learning: Between steepest descent and New-ton's method. </title> <journal> Neural Computation, </journal> <volume> 4 </volume> <pages> 141-166, </pages> <year> 1992. </year>
Reference-contexts: In fact, by considering the whole objective function f instead of its components f i separately, one can apply a whole variety of first and second order methods to the problem of minimizing f . First and second order methods for various BP algorithms are given in <ref> [2, 3, 36] </ref>.
Reference: [3] <author> S. Becker and Y. le Cun. </author> <title> Improving the convergence of backpropagation learning with second order methods. </title> <editor> In D.S. Touretzky, editor, </editor> <booktitle> Proceedings of 1988 Connectionist Models Summer School, </booktitle> <pages> pages 29-37, </pages> <address> San Mateo, California, 1988. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: In fact, by considering the whole objective function f instead of its components f i separately, one can apply a whole variety of first and second order methods to the problem of minimizing f . First and second order methods for various BP algorithms are given in <ref> [2, 3, 36] </ref>.
Reference: [4] <author> K. P. Bennett. </author> <title> Decision tree construction via linear programming. </title> <editor> In M. Evans, editor, </editor> <booktitle> Proceedings of the 4th Midwest Artificial Intelligence and Cognitive Science Society Conference, </booktitle> <pages> pages 97-101, </pages> <year> 1992. </year>
Reference-contexts: (4 0); z = (0 0) (7) (We note in passing that w = (0 0); = 0; y = (1 1); z = (1 1) (8) is also a solution but is neither unique in w nor is it the one given by MINOS.) The multisurface method tree (MSMT) <ref> [4] </ref> can be employed to obtain the complete separation of Figure 3 as follows. <p> We describe now in more detail the greedy linear-programming-based algorithm MSMT (mul-tisurface method tree), for constructing polyhedral regions in R n similar to those of Figure 7, that will discriminate between two disjoint point sets A and B in R n <ref> [4] </ref>. The essence of the method consists of applying the linear program (6) first to points contained in R n and then in succession to points contained in appropriate intersections of complementary halfspaces. <p> Algorithm 2.1 MSMT (Multisurface Method-Tree) <ref> [4] </ref>. Let A and B be disjoint points in R n ; represented by the m fi n and k fi n matrices A and B respectively.
Reference: [5] <author> K. P. Bennett and O. L. Mangasarian. </author> <title> Bilinear separation of two sets in n-space. </title> <institution> Computer Sciences Department Technical Report 1109, University of Wisconsin, Madison, Wisconsin, </institution> <year> 1992. </year>
Reference-contexts: Unfortunately, the problem of deciding whether two sets are separable by as few planes as two is NP-complete [31, 10]. Although there are effective bilinear programming algorithms for solving the bilinear separability problem <ref> [5] </ref>, there are no methods for directly obtaining piecewise-linear separators other than the proposed greedy MSM algorithms [28, 29, 8].
Reference: [6] <author> K. P. Bennett and O. L. Mangasarian. </author> <title> Multicategory separation via linear programming. </title> <institution> Computer Sciences Department Technical Report 1127, University of Wisconsin, Madison, Wisconsin, </institution> <year> 1992. </year>
Reference-contexts: Smith [46] proposed solving k linear programs separating each set from the remaining k 1 sets. However, in <ref> [6] </ref> a single linear program is solved to obtain a convex k-piece piecewise-linear surface that exactly separates k disjoint sets under certain conditions, otherwise an error-minimizing approximate separation is obtained. <p> For more details see <ref> [6] </ref>. 3 Neural Networks as Unconstrained Minimization Problems In this section we cast the problem of determining the weights and thresholds of a feedforward neural network with a single hidden layer as an unconstrained optimization problem, and relate this problem to the standard backpropagation algorithm [41, 44, 23] for training such
Reference: [7] <author> K. P. Bennett and O. L. Mangasarian. </author> <title> Neural network training via linear programming. </title> <editor> In P. M. Pardalos, editor, </editor> <booktitle> Advances in Optimization and Parallel Computing, </booktitle> <pages> pages 56-67, </pages> <address> Amsterdam, 1992. </address> <publisher> North Holland. </publisher>
Reference-contexts: Curiously enough, however, it should be noted that even before Minsky-Papert proposed their classical XOR counterexample, a linear-programming-based piecewise-linear separator was proposed in 1968 [28] that could easily and correctly handle this problem, and which in fact can be represented as a neural network <ref> [7] </ref>. (See Figure 14 and discussion following Algorithm 2.2 below.) We shall now use this example to motivate a general multisurface method (MSM) for separating the sets A and B of the XOR example or any other disjoint sets A and B in R n and will present a neural network <p> We remark that the separation achieved by the MSM algorithm and depicted in Figure 12 and 13 can also be represented as a single-hidden layer neural network <ref> [7] </ref> that is depicted in Figure 14, which we interpret now. <p> A similar remark applies to 2 1 and 2 0 : See <ref> [7] </ref> for more explanation of this neural network representation of MSM. It should be pointed out here that the MSM Algorithm 2.2 is stated in its simplest form above for exposition purposes. <p> Although MSM has produced very effective practical results <ref> [49, 29, 7] </ref>, optimality of the separating surface, as measured by the number of planes constituting it, cannot be guaranteed. Unfortunately, the problem of deciding whether two sets are separable by as few planes as two is NP-complete [31, 10]. <p> Most of these applications use neural networks trained by backpropagation or variants thereof. We shall describe here a successful linear programming-based application to breast cancer diagnosis that is in current operation at University of Wisconsin Hospitals. Its cumulative correctness rate has been 98% over the past four years <ref> [49, 29, 7] </ref>. The diagnosis system consists of an LP-trained neural network made up of seven hidden units that was originally trained on 369 samples, each consisting of a 9-dimensional vector, and retrained once. <p> The weights of the incoming arcs to the hidden units and the thresholds of the hidden units correspond respectively to the normals and distance from the origin of 4 pairs of parallel planes in R 9 and their relative distance from the origin <ref> [29, 7] </ref>. The first pair of planes separates some malignant points from some benign points but leaving a mixture of benign and malignant points between the planes.
Reference: [8] <author> K. P. Bennett and O. L. Mangasarian. </author> <title> Robust linear programming discrimination of two linearly inseparable sets. </title> <journal> Optimization Methods and Software, </journal> <volume> 1 </volume> <pages> 23-34, </pages> <year> 1992. </year>
Reference-contexts: For the latter case, what we expect, of course, is some error-minimizing linear separation that does not suffer from the shortcoming of the linear program (1.2), that is a null solution that does not provide any error-minimizing separation. For this purpose, we utilize the linear program introduced recently in <ref> [8] </ref> and which has the following desirable features not all of which are possessed by any other previous linear programming formulation [11, 27, 28, 45, 20, 19]: (i) A strict separating plane (that is neither set lies on the separating plane) for linearly separable sets A and B (ii) An error-minimizing <p> It is easy to show that (5) is equivalent to the linear program <ref> [8] </ref> min m ez j Aw e + y e; Bw + e + z e; y 0; z 0 (6) Important properties of the linear program (6) are summarized in the following theorem. Theorem 2.1 Exact and approximate separation of sets by linear programs [8]. <p> equivalent to the linear program <ref> [8] </ref> min m ez j Aw e + y e; Bw + e + z e; y 0; z 0 (6) Important properties of the linear program (6) are summarized in the following theorem. Theorem 2.1 Exact and approximate separation of sets by linear programs [8]. <p> Although there are effective bilinear programming algorithms for solving the bilinear separability problem [5], there are no methods for directly obtaining piecewise-linear separators other than the proposed greedy MSM algorithms <ref> [28, 29, 8] </ref>. We note that in these greedy MSM algorithms, each piece of the separating surface can be nonlinear as long as it is linear in its parameters such as a quadratic surface for instance.
Reference: [9] <author> H.D. Block and S.A. Levin. </author> <title> On the boundedness of an iterative procedure for solving a system of linear inequalities. </title> <journal> Proceedings of the American Mathematical Society, </journal> <volume> 26 </volume> <pages> 229-235, </pages> <year> 1970. </year>
Reference-contexts: 2 f0; 1g T hreshold 2 R LT U w W eight w 2 R n Input : x 2 R n by the weight vector w in R n and threshold in R: 2 if the sets A and B are linearly separable [32, pages 164-175], [35, Chapter 5], <ref> [9] </ref>. However if A and B are linearly inseparable the algorithm need not terminate but the iterates are bounded [32, pages 181-187], [9] and hence merely have an accumulation point. <p> by the weight vector w in R n and threshold in R: 2 if the sets A and B are linearly separable [32, pages 164-175], [35, Chapter 5], <ref> [9] </ref>. However if A and B are linearly inseparable the algorithm need not terminate but the iterates are bounded [32, pages 181-187], [9] and hence merely have an accumulation point.
Reference: [10] <author> A. Blum and R.L. Rivest. </author> <title> Training a 3-node neural network is NP-complete. </title> <editor> In D.S. Touretzky, editor, </editor> <booktitle> Advances in Neural Information Processing Systems I, </booktitle> <pages> pages 494-501, </pages> <address> San Mateo, California, 1989. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Although MSM has produced very effective practical results [49, 29, 7], optimality of the separating surface, as measured by the number of planes constituting it, cannot be guaranteed. Unfortunately, the problem of deciding whether two sets are separable by as few planes as two is NP-complete <ref> [31, 10] </ref>. Although there are effective bilinear programming algorithms for solving the bilinear separability problem [5], there are no methods for directly obtaining piecewise-linear separators other than the proposed greedy MSM algorithms [28, 29, 8].
Reference: [11] <author> A. Charnes. </author> <title> Some fundamental theorems of perceptron theory and their geometry. </title> <editor> In J. T. Lou and R. H. Wilcox, editors, </editor> <booktitle> Computer and Information Sciences, </booktitle> <pages> pages 67-74, </pages> <address> Washington, 1964. </address> <publisher> Spartan Books. </publisher>
Reference-contexts: However this difficulty can be easily circumvented, in order to obtain some "approximate" error-minimizing linear separation, by considering a slightly different linear program (6) as shown by Theorem 2.1 below. The first linear programming formulations for the linearly separable case were given in 1964 and 1965 <ref> [11, 27] </ref>, but they also suffered from the null-solution difficulty for the linearly inseparable case. In order to handle the linearly inseparable case one has to employ a more complex map than that provided by an LTU. <p> For this purpose, we utilize the linear program introduced recently in [8] and which has the following desirable features not all of which are possessed by any other previous linear programming formulation <ref> [11, 27, 28, 45, 20, 19] </ref>: (i) A strict separating plane (that is neither set lies on the separating plane) for linearly separable sets A and B (ii) An error-minimizing plane is obtained when the sets A and B are linearly inseparable. (iii) No extraneous constraints are used to exclude the
Reference: [12] <author> J.E. Dennis and R.B. Schnabel. </author> <title> Numerical Methods for Unconstrained Optimization and Nonlinear Equations. </title> <publisher> Prentice-Hall, </publisher> <address> Englewood Cliffs, N.J., </address> <year> 1983. </year>
Reference-contexts: Recently, however, Grippo [21] has proposed a promising convergence proof under certain assumptions and based on line search techniques similar to those of [22]. Batch BP on the other hand can be treated as an ordinary unconstrained minimization gradient method, and all the machinery of search methods <ref> [12] </ref> can be applied to it. In fact, by considering the whole objective function f instead of its components f i separately, one can apply a whole variety of first and second order methods to the problem of minimizing f .
Reference: [13] <author> L. DeSilets, B. Golden, Q. Wang, and R. Kumar. </author> <title> Predicting salinity in the Chesapeake Bay using backpropagation. </title> <journal> Computers and Operations Research, </journal> <volume> 19 </volume> <pages> 277-285, </pages> <year> 1992. </year>
Reference-contexts: See for example [44, 23], references therein and <ref> [43, 13] </ref>. Most of these applications use neural networks trained by backpropagation or variants thereof. We shall describe here a successful linear programming-based application to breast cancer diagnosis that is in current operation at University of Wisconsin Hospitals.
Reference: [14] <author> L.C. Dixon and D.J. Mills. </author> <title> Neural nets for massively parallel optimisation. </title> <type> Technical Report Numerical Optimisation Centre Technical Report No. 259, </type> <institution> Hatfield Polytechnic, Hatfield, </institution> <address> Hertforshire, England, </address> <year> 1992. </year> <month> 23 </month>
Reference-contexts: Whether the converse is also true, is an interesting but not completely settled question [23, pages 76-79], even though there have been a number of interesting applications of neural networks to optimization problems, for example <ref> [37, 1, 14] </ref> and [23, pages 71-87]. Acknowledgement I wish to thank my colleagues Yann le Cun, Renato De Leone, Laurence Dixon, Greg Madey, Jim Noyes, Boris Polyak, Steve Robinsosn and Jude Shavlik for helpful references that they furnished, some of which have been cited.
Reference: [15] <editor> Yu. Ermoliev and R.J.-B. Wets (editors). </editor> <title> Numerical Techniques for Stochastic Optimization Problems. </title> <publisher> Springer-Verlag, </publisher> <address> Berlin, </address> <year> 1988. </year>
Reference-contexts: First and second order methods for various BP algorithms are given in [2, 3, 36]. More recently Gaivoronski [16] gave a convergence proof for online BP (17) under minimal conditions using stochastic gradient ideas <ref> [15] </ref> in which he established convergence of fk rf (z ` ) kg to zero as well as the stationarity of each accumulation point z of fz ` g, that is rf (z) = 0.
Reference: [16] <author> A.A. Gaivoronski. </author> <title> Private communication, </title> <month> November </month> <year> 1992. </year>
Reference-contexts: First and second order methods for various BP algorithms are given in [2, 3, 36]. More recently Gaivoronski <ref> [16] </ref> gave a convergence proof for online BP (17) under minimal conditions using stochastic gradient ideas [15] in which he established convergence of fk rf (z ` ) kg to zero as well as the stationarity of each accumulation point z of fz ` g, that is rf (z) = 0.
Reference: [17] <author> A.A. Gaivoronski. </author> <title> Stochastic gradient methods for neural networks. In O.L. </title> <editor> Mangasarian and R.R. Meyer, editors, </editor> <booktitle> Parallel Optimization 3, Philadelphia, 1994. SIAM. Proceedings of Symposium on Parallel Optimization 3, </booktitle> <address> Madison July 7-9, </address> <year> 1993. </year>
Reference-contexts: The complete result will appear in <ref> [17] </ref>. We note also that, in a little known paper, Kibardin [25] used similar conditions to (19) for establishing convergence of (17) for convex f i (z); i = 1; : : :k. Unfortunately the convexity assumption does not hold here.
Reference: [18] <author> G.M. Georgiou. </author> <title> Comments on hidden nodes in neural nets. </title> <journal> IEEE Transactions on Circuits and Systems, </journal> <volume> 38:1410, </volume> <year> 1991. </year>
Reference-contexts: More generally, we can think of a feedforward neural network with a single hidden layer of h LTU's as a representation of h planes in R n that divide the space into p polyhedral regions, p i=0 h ! <ref> [18] </ref>, each containing elements of only one set A or B: These planes, which are represented by the vector-weighted incoming arcs to the hidden LTU's together with their threshold values, map each polyhedral region into the vertices of the unit cube C h = fr j r 2 R h ;
Reference: [19] <author> F. Glover. </author> <title> Improved linear programming models for discriminant analysis. </title> <journal> Decision Sciences, </journal> <volume> 21 </volume> <pages> 771-785, </pages> <year> 1990. </year>
Reference-contexts: For this purpose, we utilize the linear program introduced recently in [8] and which has the following desirable features not all of which are possessed by any other previous linear programming formulation <ref> [11, 27, 28, 45, 20, 19] </ref>: (i) A strict separating plane (that is neither set lies on the separating plane) for linearly separable sets A and B (ii) An error-minimizing plane is obtained when the sets A and B are linearly inseparable. (iii) No extraneous constraints are used to exclude the
Reference: [20] <author> R.C. Grinold. </author> <title> Mathematical methods for pattern classification. </title> <journal> Management Science, </journal> <volume> 19 </volume> <pages> 272-289, </pages> <year> 1972. </year>
Reference-contexts: For this purpose, we utilize the linear program introduced recently in [8] and which has the following desirable features not all of which are possessed by any other previous linear programming formulation <ref> [11, 27, 28, 45, 20, 19] </ref>: (i) A strict separating plane (that is neither set lies on the separating plane) for linearly separable sets A and B (ii) An error-minimizing plane is obtained when the sets A and B are linearly inseparable. (iii) No extraneous constraints are used to exclude the
Reference: [21] <author> L. Grippo. </author> <title> Private communication, </title> <month> September </month> <year> 1992. </year>
Reference-contexts: This is a curious fact in view of the wide 20 acceptability and success of the method. Recently, however, Grippo <ref> [21] </ref> has proposed a promising convergence proof under certain assumptions and based on line search techniques similar to those of [22]. Batch BP on the other hand can be treated as an ordinary unconstrained minimization gradient method, and all the machinery of search methods [12] can be applied to it.
Reference: [22] <author> L. Grippo, F. Lampariello, and S. Lucidi. </author> <title> Global convergence and stabilization of unconstrained minimization methods without derivatives. </title> <journal> Journal of Optimization Theory and Applications, </journal> <volume> 56 </volume> <pages> 385-406, </pages> <year> 1988. </year>
Reference-contexts: This is a curious fact in view of the wide 20 acceptability and success of the method. Recently, however, Grippo [21] has proposed a promising convergence proof under certain assumptions and based on line search techniques similar to those of <ref> [22] </ref>. Batch BP on the other hand can be treated as an ordinary unconstrained minimization gradient method, and all the machinery of search methods [12] can be applied to it.
Reference: [23] <author> J. Hertz, A. Krogh, and R. G. Palmer. </author> <title> Introduction to the Theory of Neural Computation. </title> <publisher> Addison-Wesley, </publisher> <address> Redwood City, California, </address> <year> 1991. </year>
Reference-contexts: For more details see [6]. 3 Neural Networks as Unconstrained Minimization Problems In this section we cast the problem of determining the weights and thresholds of a feedforward neural network with a single hidden layer as an unconstrained optimization problem, and relate this problem to the standard backpropagation algorithm <ref> [41, 44, 23] </ref> for training such a neural network. <p> One approach is to obtain a least value of h for which an approximate solution to Problem 3.1 is satisfactory in the sense that y (x) has a tolerable error in it <ref> [26, 23] </ref>. Note that both of our Algorithms MSMT 2.1 and MSM 2.2 essentially take this approach. In the sequel, however, we shall assume that h is fixed. An interesting empirical study of generalization in machine learning has been carried out on several real world data sets in [43]. <p> We shall assume from now on that s () = (): Once this is done the problem reduces to that of finding a stationary point of the nonconvex but differentiable function f: The classical backpropagation (BP) method <ref> [41, 23] </ref> for solving this problem is a gradient-type method applied to its k components sequentially (online BP) or to the whole function f (batch BP). <p> See for example <ref> [44, 23] </ref>, references therein and [43, 13]. Most of these applications use neural networks trained by backpropagation or variants thereof. We shall describe here a successful linear programming-based application to breast cancer diagnosis that is in current operation at University of Wisconsin Hospitals. <p> We conclude that optimization theory and algorithms play a significant role in the algorithmic and applied development of the burgeoning neural network field of machine learning [42]. Whether the converse is also true, is an interesting but not completely settled question <ref> [23, pages 76-79] </ref>, even though there have been a number of interesting applications of neural networks to optimization problems, for example [37, 1, 14] and [23, pages 71-87]. <p> Whether the converse is also true, is an interesting but not completely settled question [23, pages 76-79], even though there have been a number of interesting applications of neural networks to optimization problems, for example [37, 1, 14] and <ref> [23, pages 71-87] </ref>. Acknowledgement I wish to thank my colleagues Yann le Cun, Renato De Leone, Laurence Dixon, Greg Madey, Jim Noyes, Boris Polyak, Steve Robinsosn and Jude Shavlik for helpful references that they furnished, some of which have been cited.
Reference: [24] <author> K. Hornik, M. Stinchcombe, and H. White. </author> <title> Multilayer feedforward networks are universal approximators. </title> <booktitle> Neural Networks, </booktitle> <volume> 2 </volume> <pages> 359-366, </pages> <year> 1989. </year>
Reference-contexts: The term "hidden layer" refers to units hidden from both the input and output layers. The presence of such a hidden layer is crucial and endows a neural network with a complexity that is not possessed by a single layer of LTU's. In fact, it can be shown <ref> [24, 29] </ref> that such a neural network can separate any two disjoint sets in R n given a sufficient number of hidden units. (See Theorem 2.2 below.) This is equivalent to the multisurface method separating such disjoint sets given a sufficient number of planes [28]. <p> In terms of neural network terminology this is equivalent to the following. Theorem 2.2 Neural Network as Universal Separator <ref> [24, 29] </ref> A neural network with a single hidden layer and sufficient number of hidden LTU's can completely separate any two disjoint points sets in R n : We note that MSM is a greedy algorithm that generates various pieces of a separating surface between the sets A and B sequentially.
Reference: [25] <author> V.M. Kibardin. </author> <title> Decomposition into functions in the minimization problems. </title> <journal> Automation and Remote Control, </journal> <volume> 40 </volume> <pages> 1311-1323, </pages> <year> 1980. </year>
Reference-contexts: The complete result will appear in [17]. We note also that, in a little known paper, Kibardin <ref> [25] </ref> used similar conditions to (19) for establishing convergence of (17) for convex f i (z); i = 1; : : :k. Unfortunately the convexity assumption does not hold here.
Reference: [26] <author> Y. le Cun, J.S. Denker, and S.A. Solla. </author> <title> Optimal brain damage. </title> <editor> In D.S. Touretzky, editor, </editor> <booktitle> Advances in Neural Information Processing Systems II (Denver 1989), </booktitle> <pages> pages 598-605, </pages> <address> San Mateo, California, 1990. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: One approach is to obtain a least value of h for which an approximate solution to Problem 3.1 is satisfactory in the sense that y (x) has a tolerable error in it <ref> [26, 23] </ref>. Note that both of our Algorithms MSMT 2.1 and MSM 2.2 essentially take this approach. In the sequel, however, we shall assume that h is fixed. An interesting empirical study of generalization in machine learning has been carried out on several real world data sets in [43].
Reference: [27] <author> O. L. Mangasarian. </author> <title> Linear and nonlinear separation of patterns by linear programming. </title> <journal> Operations Research, </journal> <volume> 13 </volume> <pages> 444-452, </pages> <year> 1965. </year>
Reference-contexts: However this difficulty can be easily circumvented, in order to obtain some "approximate" error-minimizing linear separation, by considering a slightly different linear program (6) as shown by Theorem 2.1 below. The first linear programming formulations for the linearly separable case were given in 1964 and 1965 <ref> [11, 27] </ref>, but they also suffered from the null-solution difficulty for the linearly inseparable case. In order to handle the linearly inseparable case one has to employ a more complex map than that provided by an LTU. <p> For this purpose, we utilize the linear program introduced recently in [8] and which has the following desirable features not all of which are possessed by any other previous linear programming formulation <ref> [11, 27, 28, 45, 20, 19] </ref>: (i) A strict separating plane (that is neither set lies on the separating plane) for linearly separable sets A and B (ii) An error-minimizing plane is obtained when the sets A and B are linearly inseparable. (iii) No extraneous constraints are used to exclude the <p> We note that in these greedy MSM algorithms, each piece of the separating surface can be nonlinear as long as it is linear in its parameters such as a quadratic surface for instance. The separation can still be achieved by solving a linear program <ref> [27, 40] </ref> for each piece. We turn now to the case of multicategory discrimination, that is, discriminating between the elements of k disjoint point sets in R n ; and show how it can be set as a single linear program.
Reference: [28] <author> O. L. Mangasarian. </author> <title> Multi-surface method of pattern separation. </title> <journal> IEEE Transactions on Information Theory, </journal> <volume> IT-14:801-807, </volume> <year> 1968. </year>
Reference-contexts: Curiously enough, however, it should be noted that even before Minsky-Papert proposed their classical XOR counterexample, a linear-programming-based piecewise-linear separator was proposed in 1968 <ref> [28] </ref> that could easily and correctly handle this problem, and which in fact can be represented as a neural network [7]. (See Figure 14 and discussion following Algorithm 2.2 below.) We shall now use this example to motivate a general multisurface method (MSM) for separating the sets A and B of <p> In fact, it can be shown [24, 29] that such a neural network can separate any two disjoint sets in R n given a sufficient number of hidden units. (See Theorem 2.2 below.) This is equivalent to the multisurface method separating such disjoint sets given a sufficient number of planes <ref> [28] </ref>. Hopefully, it should be clear now that the mapping that we are after from R n into f0; 1g can be rather complex depending on the sets A and B: The two representations that have been described, neural networks and multisurface separation, are both valid representations of this mapping. <p> For this purpose, we utilize the linear program introduced recently in [8] and which has the following desirable features not all of which are possessed by any other previous linear programming formulation <ref> [11, 27, 28, 45, 20, 19] </ref>: (i) A strict separating plane (that is neither set lies on the separating plane) for linearly separable sets A and B (ii) An error-minimizing plane is obtained when the sets A and B are linearly inseparable. (iii) No extraneous constraints are used to exclude the <p> We describe now another multisurface method that is a variant of the original MSM method <ref> [28, 29] </ref>, which is based on solving the single linear program (6) at each step instead of solving 2n linear programs as originally proposed [28, 29]. We first give a geometric description of the method and refer to Figure 12. <p> We describe now another multisurface method that is a variant of the original MSM method <ref> [28, 29] </ref>, which is based on solving the single linear program (6) at each step instead of solving 2n linear programs as originally proposed [28, 29]. We first give a geometric description of the method and refer to Figure 12. The method obtains the piecewise-linear separator, depicted by a heavy line in Figure 12, as follows. <p> We now formally describe the MSM algorithm, omitting a rarely needed antidegeneracy procedure <ref> [28, 29] </ref> that guards against the cases when some pair of planes does not separate any points, that is A i+1 = A i and B i+1 = B i for some i in the terminology of the MSM algorithm below. Algorithm 2.2 MSM (Multisurface Method) [28, 29] Let A and <p> rarely needed antidegeneracy procedure <ref> [28, 29] </ref> that guards against the cases when some pair of planes does not separate any points, that is A i+1 = A i and B i+1 = B i for some i in the terminology of the MSM algorithm below. Algorithm 2.2 MSM (Multisurface Method) [28, 29] Let A and B be disjoint point sets in R n ; represented by the m fi n and k fi n matrices A and B respectively. <p> In order to cover possible degenerate cases that are not usually encountered in practice, one has to provide for the possibility of both A i = A i+1 and B i = B i+1 for some i <ref> [28, 29] </ref>. <p> from A i but no points from B i ; or to construct a plane xw i = i 0 that chops off one or more points from B i but no points from A i : With the Degeneracy Procedure in place, one can assert, as was done in <ref> [28, 29] </ref> that MSM can discriminate between any two disjoint point sets in R n ; provided that a sufficient number of planes are used. In terms of neural network terminology this is equivalent to the following. <p> Although there are effective bilinear programming algorithms for solving the bilinear separability problem [5], there are no methods for directly obtaining piecewise-linear separators other than the proposed greedy MSM algorithms <ref> [28, 29, 8] </ref>. We note that in these greedy MSM algorithms, each piece of the separating surface can be nonlinear as long as it is linear in its parameters such as a quadratic surface for instance. <p> The fourth and last pair of planes which coalesce into a single plane, completely separate the mixture of points between the third pair of planes <ref> [28] </ref>, thus giving a compete separation of the training set. These planes which generate a piecewise-linear separator, were obtained by a linear programming approach [28, 29] slightly different from MSM described in Section 2. <p> The fourth and last pair of planes which coalesce into a single plane, completely separate the mixture of points between the third pair of planes [28], thus giving a compete separation of the training set. These planes which generate a piecewise-linear separator, were obtained by a linear programming approach <ref> [28, 29] </ref> slightly different from MSM described in Section 2. The 9-dimensional vector consists of 9 cellular attributes measured microscopically by a surgical oncologist on a needle aspirate taken from the patient's breast. Malignant diagnosis is confirmed by subsequent biopsy of breast tissue.
Reference: [29] <author> O.L. Mangasarian, R. Setiono, </author> <title> and W.H. Wolberg. Pattern recognition via linear programming: Theory and application to medical diagnosis. </title> <editor> In T. F. Coleman and Y. Li, editors, </editor> <booktitle> Proceedings of the Workshop on Large-Scale Numerical Optimization, </booktitle> <institution> Cornell University, </institution> <address> Ithaca, New York, </address> <month> October 19-20, </month> <year> 1989, </year> <pages> pages 22-31, </pages> <address> Philadelphia, Pennsylvania, </address> <year> 1990. </year> <note> SIAM. </note>
Reference-contexts: The term "hidden layer" refers to units hidden from both the input and output layers. The presence of such a hidden layer is crucial and endows a neural network with a complexity that is not possessed by a single layer of LTU's. In fact, it can be shown <ref> [24, 29] </ref> that such a neural network can separate any two disjoint sets in R n given a sufficient number of hidden units. (See Theorem 2.2 below.) This is equivalent to the multisurface method separating such disjoint sets given a sufficient number of planes [28]. <p> We describe now another multisurface method that is a variant of the original MSM method <ref> [28, 29] </ref>, which is based on solving the single linear program (6) at each step instead of solving 2n linear programs as originally proposed [28, 29]. We first give a geometric description of the method and refer to Figure 12. <p> We describe now another multisurface method that is a variant of the original MSM method <ref> [28, 29] </ref>, which is based on solving the single linear program (6) at each step instead of solving 2n linear programs as originally proposed [28, 29]. We first give a geometric description of the method and refer to Figure 12. The method obtains the piecewise-linear separator, depicted by a heavy line in Figure 12, as follows. <p> We now formally describe the MSM algorithm, omitting a rarely needed antidegeneracy procedure <ref> [28, 29] </ref> that guards against the cases when some pair of planes does not separate any points, that is A i+1 = A i and B i+1 = B i for some i in the terminology of the MSM algorithm below. Algorithm 2.2 MSM (Multisurface Method) [28, 29] Let A and <p> rarely needed antidegeneracy procedure <ref> [28, 29] </ref> that guards against the cases when some pair of planes does not separate any points, that is A i+1 = A i and B i+1 = B i for some i in the terminology of the MSM algorithm below. Algorithm 2.2 MSM (Multisurface Method) [28, 29] Let A and B be disjoint point sets in R n ; represented by the m fi n and k fi n matrices A and B respectively. <p> In order to cover possible degenerate cases that are not usually encountered in practice, one has to provide for the possibility of both A i = A i+1 and B i = B i+1 for some i <ref> [28, 29] </ref>. <p> from A i but no points from B i ; or to construct a plane xw i = i 0 that chops off one or more points from B i but no points from A i : With the Degeneracy Procedure in place, one can assert, as was done in <ref> [28, 29] </ref> that MSM can discriminate between any two disjoint point sets in R n ; provided that a sufficient number of planes are used. In terms of neural network terminology this is equivalent to the following. <p> In terms of neural network terminology this is equivalent to the following. Theorem 2.2 Neural Network as Universal Separator <ref> [24, 29] </ref> A neural network with a single hidden layer and sufficient number of hidden LTU's can completely separate any two disjoint points sets in R n : We note that MSM is a greedy algorithm that generates various pieces of a separating surface between the sets A and B sequentially. <p> Although MSM has produced very effective practical results <ref> [49, 29, 7] </ref>, optimality of the separating surface, as measured by the number of planes constituting it, cannot be guaranteed. Unfortunately, the problem of deciding whether two sets are separable by as few planes as two is NP-complete [31, 10]. <p> Although there are effective bilinear programming algorithms for solving the bilinear separability problem [5], there are no methods for directly obtaining piecewise-linear separators other than the proposed greedy MSM algorithms <ref> [28, 29, 8] </ref>. We note that in these greedy MSM algorithms, each piece of the separating surface can be nonlinear as long as it is linear in its parameters such as a quadratic surface for instance. <p> Most of these applications use neural networks trained by backpropagation or variants thereof. We shall describe here a successful linear programming-based application to breast cancer diagnosis that is in current operation at University of Wisconsin Hospitals. Its cumulative correctness rate has been 98% over the past four years <ref> [49, 29, 7] </ref>. The diagnosis system consists of an LP-trained neural network made up of seven hidden units that was originally trained on 369 samples, each consisting of a 9-dimensional vector, and retrained once. <p> The weights of the incoming arcs to the hidden units and the thresholds of the hidden units correspond respectively to the normals and distance from the origin of 4 pairs of parallel planes in R 9 and their relative distance from the origin <ref> [29, 7] </ref>. The first pair of planes separates some malignant points from some benign points but leaving a mixture of benign and malignant points between the planes. <p> The fourth and last pair of planes which coalesce into a single plane, completely separate the mixture of points between the third pair of planes [28], thus giving a compete separation of the training set. These planes which generate a piecewise-linear separator, were obtained by a linear programming approach <ref> [28, 29] </ref> slightly different from MSM described in Section 2. The 9-dimensional vector consists of 9 cellular attributes measured microscopically by a surgical oncologist on a needle aspirate taken from the patient's breast. Malignant diagnosis is confirmed by subsequent biopsy of breast tissue.
Reference: [30] <author> W. McCulloch and W. Pitts. </author> <title> A logical calculus of the ideas immanent in nervous activity. </title> <journal> Bulletin of Mathematical Biophysics, </journal> <volume> 7 </volume> <pages> 115-133, </pages> <year> 1943. </year>
Reference-contexts: These units are similar to the human neuron in that they fire when their input exceeds their threshold. The simplest and earliest neural network is the linear threshold unit (LTU), first proposed by McCulloch and Pitts in 1943 <ref> [30] </ref> and for which Rosenblatt [39, 38] proposed his iterative perceptron training algorithm in 1957.
Reference: [31] <author> N. Megiddo. </author> <title> On the complexity of polyhedral separability. </title> <journal> Discrete and Computational Geometry, </journal> <volume> 3 </volume> <pages> 325-337, </pages> <year> 1988. </year> <month> 24 </month>
Reference-contexts: Although MSM has produced very effective practical results [49, 29, 7], optimality of the separating surface, as measured by the number of planes constituting it, cannot be guaranteed. Unfortunately, the problem of deciding whether two sets are separable by as few planes as two is NP-complete <ref> [31, 10] </ref>. Although there are effective bilinear programming algorithms for solving the bilinear separability problem [5], there are no methods for directly obtaining piecewise-linear separators other than the proposed greedy MSM algorithms [28, 29, 8].
Reference: [32] <author> M. Minsky and S. Papert. </author> <title> Perceptrons: An Introduction to Computational Geometry. </title> <publisher> MIT Press, </publisher> <address> Cambridge, Massachusetts, </address> <year> 1969. </year>
Reference-contexts: CCR-9101801. 1 Output : y (x) 2 f0; 1g T hreshold 2 R LT U w W eight w 2 R n Input : x 2 R n by the weight vector w in R n and threshold in R: 2 if the sets A and B are linearly separable <ref> [32, pages 164-175] </ref>, [35, Chapter 5], [9]. However if A and B are linearly inseparable the algorithm need not terminate but the iterates are bounded [32, pages 181-187], [9] and hence merely have an accumulation point. <p> However if A and B are linearly inseparable the algorithm need not terminate but the iterates are bounded <ref> [32, pages 181-187] </ref>, [9] and hence merely have an accumulation point. <p> In order to handle the linearly inseparable case one has to employ a more complex map than that provided by an LTU. This was made evident in the early days of neural network development by Minsky and Papert in 1969 <ref> [32] </ref> when they presented their now-classical exclusive-or (XOR) counterexample which is not linearly separable and hence for which no LTU will work. (See Figure 2).
Reference: [33] <author> T. S. Motzkin and I. J. </author> <title> Schoenberg. The relaxation method for linear inequalities. </title> <journal> Canadian Journal of Mathematics, </journal> <volume> 6 </volume> <pages> 393-404, </pages> <year> 1954. </year>
Reference-contexts: We term such sets linearly separable. Among the earliest algorithms for obtaining such a separating plane or an LTU was Rosenblatt's perceptron algorithm, which turns out to be a version of the Motzkin-Schoenberg iterative algorithm <ref> [33] </ref> for solving a system of linear inequalities. This algorithm terminates in a finite number of steps fl Computer Sciences Department, University of Wisconsin, 1210 West Dayton Street, Madison, WI 53706, email: olvi@cs.wisc.edu.
Reference: [34] <author> B.A. Murtagh and M.A. Saunders. </author> <title> MINOS 5.0 user's guide. </title> <type> Technical Report SOL 83.20, </type> <institution> Stanford University, </institution> <month> December </month> <year> 1983. </year>
Reference-contexts: We shall demonstrate this application by obtaining the multisurface separation depicted in Figure 3 for the XOR example and hence the equivalent neural network of Figure 4. By letting A = 1 0 # " 1 1 and solving the linear program (6) using MINOS 5.3 <ref> [34] </ref> we obtain the solution w = (2 + 2); = 1; y = (4 0); z = (0 0) (7) (We note in passing that w = (0 0); = 0; y = (1 1); z = (1 1) (8) is also a solution but is neither unique in w
Reference: [35] <author> N. J. Nilsson. </author> <title> Learning Machines. </title> <publisher> MIT Press, </publisher> <address> Cambridge, Massachusetts, </address> <year> 1966. </year>
Reference-contexts: : y (x) 2 f0; 1g T hreshold 2 R LT U w W eight w 2 R n Input : x 2 R n by the weight vector w in R n and threshold in R: 2 if the sets A and B are linearly separable [32, pages 164-175], <ref> [35, Chapter 5] </ref>, [9]. However if A and B are linearly inseparable the algorithm need not terminate but the iterates are bounded [32, pages 181-187], [9] and hence merely have an accumulation point.
Reference: [36] <author> J.L. Noyes. </author> <title> Neural network optimization methods. </title> <booktitle> In Proceedings of the Fourth Conference on Neural Networks and Parallel Distributed Processing, </booktitle> <pages> pages 1-12, </pages> <address> Fort Wayne, Indiana, </address> <year> 1991. </year> <institution> Indiana-Purdue University. </institution>
Reference-contexts: In fact, by considering the whole objective function f instead of its components f i separately, one can apply a whole variety of first and second order methods to the problem of minimizing f . First and second order methods for various BP algorithms are given in <ref> [2, 3, 36] </ref>.
Reference: [37] <author> K.E. Nygard, P. Juell, and N. Kadaba. </author> <title> Neural networks for selecting vehicle routing heuristics. </title> <journal> ORSA Journal on Computing, </journal> <volume> 4 </volume> <pages> 353-364, </pages> <year> 1990. </year>
Reference-contexts: Whether the converse is also true, is an interesting but not completely settled question [23, pages 76-79], even though there have been a number of interesting applications of neural networks to optimization problems, for example <ref> [37, 1, 14] </ref> and [23, pages 71-87]. Acknowledgement I wish to thank my colleagues Yann le Cun, Renato De Leone, Laurence Dixon, Greg Madey, Jim Noyes, Boris Polyak, Steve Robinsosn and Jude Shavlik for helpful references that they furnished, some of which have been cited.
Reference: [38] <author> F. Rosenblatt. </author> <title> Principles of Neurodynamics. </title> <publisher> Spartan Books, </publisher> <address> New York, New York, </address> <year> 1962. </year>
Reference-contexts: These units are similar to the human neuron in that they fire when their input exceeds their threshold. The simplest and earliest neural network is the linear threshold unit (LTU), first proposed by McCulloch and Pitts in 1943 [30] and for which Rosenblatt <ref> [39, 38] </ref> proposed his iterative perceptron training algorithm in 1957.
Reference: [39] <author> F. Rosenblatt. </author> <title> The perceptron-a perceiving and recognizing automaton. </title> <type> Technical Report 85-460-1, </type> <institution> Cornell Aeronautical Laboratory, </institution> <address> Itahca, New York, </address> <month> January </month> <year> 1957. </year>
Reference-contexts: These units are similar to the human neuron in that they fire when their input exceeds their threshold. The simplest and earliest neural network is the linear threshold unit (LTU), first proposed by McCulloch and Pitts in 1943 [30] and for which Rosenblatt <ref> [39, 38] </ref> proposed his iterative perceptron training algorithm in 1957.
Reference: [40] <author> A. Roy and S. Mukhopadhyay. </author> <title> Pattern classification using linear programming. </title> <journal> ORSA Journal of Computing, </journal> <volume> 3 </volume> <pages> 66-80, </pages> <year> 1990. </year>
Reference-contexts: We note that in these greedy MSM algorithms, each piece of the separating surface can be nonlinear as long as it is linear in its parameters such as a quadratic surface for instance. The separation can still be achieved by solving a linear program <ref> [27, 40] </ref> for each piece. We turn now to the case of multicategory discrimination, that is, discriminating between the elements of k disjoint point sets in R n ; and show how it can be set as a single linear program.
Reference: [41] <author> D. E. Rumelhart and J. L. McClelland. </author> <title> Parallel Distributed Processing. </title> <publisher> MIT Press, </publisher> <address> Cambridge, Massachusetts, </address> <year> 1986. </year>
Reference-contexts: This essentially brought the early development of neural networks to a halt until it was realized <ref> [47, 41] </ref> that a more complex function than that represented by an LTU was needed to correctly map these simple four points into the set f0; 1g. <p> For more details see [6]. 3 Neural Networks as Unconstrained Minimization Problems In this section we cast the problem of determining the weights and thresholds of a feedforward neural network with a single hidden layer as an unconstrained optimization problem, and relate this problem to the standard backpropagation algorithm <ref> [41, 44, 23] </ref> for training such a neural network. <p> We shall assume from now on that s () = (): Once this is done the problem reduces to that of finding a stationary point of the nonconvex but differentiable function f: The classical backpropagation (BP) method <ref> [41, 23] </ref> for solving this problem is a gradient-type method applied to its k components sequentially (online BP) or to the whole function f (batch BP).
Reference: [42] <editor> J.W. Shavlik and T.G. Dietterich (editors). </editor> <booktitle> Readings in Machine Learning. </booktitle> <publisher> Morgan Kaufman, </publisher> <address> San Mateo, California, </address> <year> 1990. </year>
Reference-contexts: We also gave a brief description of a real-world application of the linear-programming-based approach to an important medical diagnosis problem. We conclude that optimization theory and algorithms play a significant role in the algorithmic and applied development of the burgeoning neural network field of machine learning <ref> [42] </ref>. Whether the converse is also true, is an interesting but not completely settled question [23, pages 76-79], even though there have been a number of interesting applications of neural networks to optimization problems, for example [37, 1, 14] and [23, pages 71-87].
Reference: [43] <author> J.W. Shavlik, R.J. Mooney, and G.G. Towell. </author> <title> Symbolic and neural network learning algorithms: An experimental comparison. </title> <journal> Machine Learning, </journal> <volume> 6 </volume> <pages> 111-143, </pages> <year> 1991. </year>
Reference-contexts: Note that both of our Algorithms MSMT 2.1 and MSM 2.2 essentially take this approach. In the sequel, however, we shall assume that h is fixed. An interesting empirical study of generalization in machine learning has been carried out on several real world data sets in <ref> [43] </ref>. <p> See for example [44, 23], references therein and <ref> [43, 13] </ref>. Most of these applications use neural networks trained by backpropagation or variants thereof. We shall describe here a successful linear programming-based application to breast cancer diagnosis that is in current operation at University of Wisconsin Hospitals.
Reference: [44] <author> P.K. Simpson. </author> <booktitle> Artificial Neural Systems. </booktitle> <publisher> Pergamon Press, </publisher> <address> New York, </address> <year> 1990. </year>
Reference-contexts: For more details see [6]. 3 Neural Networks as Unconstrained Minimization Problems In this section we cast the problem of determining the weights and thresholds of a feedforward neural network with a single hidden layer as an unconstrained optimization problem, and relate this problem to the standard backpropagation algorithm <ref> [41, 44, 23] </ref> for training such a neural network. <p> See for example <ref> [44, 23] </ref>, references therein and [43, 13]. Most of these applications use neural networks trained by backpropagation or variants thereof. We shall describe here a successful linear programming-based application to breast cancer diagnosis that is in current operation at University of Wisconsin Hospitals.
Reference: [45] <author> F. W. Smith. </author> <title> Pattern classifier design by linear programming. </title> <journal> IEEE Transactions on Computers, </journal> <volume> C-17:367-372, </volume> <year> 1968. </year>
Reference-contexts: For this purpose, we utilize the linear program introduced recently in [8] and which has the following desirable features not all of which are possessed by any other previous linear programming formulation <ref> [11, 27, 28, 45, 20, 19] </ref>: (i) A strict separating plane (that is neither set lies on the separating plane) for linearly separable sets A and B (ii) An error-minimizing plane is obtained when the sets A and B are linearly inseparable. (iii) No extraneous constraints are used to exclude the
Reference: [46] <author> F. W. Smith. </author> <title> Design of multicategory pattern classifiers with two-category classifier design procedures. </title> <journal> IEEE Transactions on Computers, </journal> <volume> 18 </volume> <pages> 367-372, </pages> <year> 1969. </year>
Reference-contexts: We turn now to the case of multicategory discrimination, that is, discriminating between the elements of k disjoint point sets in R n ; and show how it can be set as a single linear program. Smith <ref> [46] </ref> proposed solving k linear programs separating each set from the remaining k 1 sets. However, in [6] a single linear program is solved to obtain a convex k-piece piecewise-linear surface that exactly separates k disjoint sets under certain conditions, otherwise an error-minimizing approximate separation is obtained.
Reference: [47] <author> P.J. Werbos. </author> <title> Beyond Regression: New Tools for Prediction and Analysis in the Behaviorial Sciences. </title> <type> PhD thesis, </type> <institution> Harvard University, </institution> <year> 1974. </year>
Reference-contexts: This essentially brought the early development of neural networks to a halt until it was realized <ref> [47, 41] </ref> that a more complex function than that represented by an LTU was needed to correctly map these simple four points into the set f0; 1g.
Reference: [48] <author> W.N.Street, </author> <title> W.H. Wolberg, and O.L. Mangasarian. Nuclear feature extraction for breast tumor diagnosis. </title> <booktitle> In IS&T/SPIE 1993 International Symposium on Electronic Imaging: Science and Technology, volume 105, </booktitle> <address> San Jose, California, </address> <year> 1993. </year>
Reference-contexts: Although this system has been quite successful, it requires the services of an experienced oncologist for making the measurements. An automated system has been developed, and recently put into use, that completely eliminates the subjective assessment by the oncologist <ref> [48] </ref>. Instead vision techniques are used to draw boundaries around the nuclei of a few cells from which 30 numerical features are extracted.
Reference: [49] <author> W. H. Wolberg and O.L. Mangasarian. </author> <title> Multisurface method of pattern separation for medical diagnosis applied to breast cytology. </title> <booktitle> Proceedings of the National Academy of Sciences,U.S.A., </booktitle> <volume> 87 </volume> <pages> 9193-9196, </pages> <year> 1990. </year>
Reference-contexts: Although MSM has produced very effective practical results <ref> [49, 29, 7] </ref>, optimality of the separating surface, as measured by the number of planes constituting it, cannot be guaranteed. Unfortunately, the problem of deciding whether two sets are separable by as few planes as two is NP-complete [31, 10]. <p> Most of these applications use neural networks trained by backpropagation or variants thereof. We shall describe here a successful linear programming-based application to breast cancer diagnosis that is in current operation at University of Wisconsin Hospitals. Its cumulative correctness rate has been 98% over the past four years <ref> [49, 29, 7] </ref>. The diagnosis system consists of an LP-trained neural network made up of seven hidden units that was originally trained on 369 samples, each consisting of a 9-dimensional vector, and retrained once.
References-found: 49

