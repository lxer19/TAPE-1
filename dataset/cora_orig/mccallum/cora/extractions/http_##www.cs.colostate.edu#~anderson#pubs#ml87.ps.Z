URL: http://www.cs.colostate.edu/~anderson/pubs/ml87.ps.Z
Refering-URL: http://www.cs.colostate.edu/~anderson/
Root-URL: 
Email: anderson@cs.colostate.edu  
Title: Strategy Learning with Multilayer Connectionist Representations 1  
Author: Charles W. Anderson 
Web: http://www.cs.colostate.edu/~anderson  
Address: Fort Collins, CO 80523  
Affiliation: Department of Computer Science Colorado State University  
Abstract: Results are presented that demonstrate the learning and fine-tuning of search strategies using connectionist mechanisms. Previous studies of strategy learning within the symbolic, production-rule formalism have not addressed fine-tuning behavior. Here a two-layer connectionist system is presented that develops its search from a weak to a task-specific strategy and fine-tunes its performance. The system is applied to a simulated, real-time, balance-control task. We compare the performance of one-layer and two-layer networks, showing that the ability of the two-layer network to discover new features and thus enhance the original representation is critical to solving the balancing task. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Anderson, C. W. </author> <year> (1982). </year> <title> Feature generation and selection by a layered network of reinforcement learning elements: some initial experiments. </title> <type> (Technical Report COINS 82-12). </type> <institution> Amherst, MA: University of Massachusetts, Department of Computer and Information Science. </institution>
Reference: <author> Anderson, C. W. </author> <year> (1986). </year> <title> Learning and problem solving with multilayered connectionist systems. </title> <type> Doctoral Dissertation, </type> <institution> Department of Computer and Information Science, University of Massachusetts, </institution> <address> Amherst, MA. </address>
Reference-contexts: With minor modifications, this learning system has also been applied to the Tower of Hanoi puzzle <ref> (Anderson, 1986) </ref>, demonstrating that it is not limited to numerical control tasks. A question not addressed here is how well the learned solution transfers to related tasks. Selfridge, Sutton, & Barto (1985) considered transfer in the one-layer system.
Reference: <author> Anzai, Y. </author> <year> (1987). </year> <title> Doing, understanding, and production systems. </title> <editor> In D. Klahr, P. Langley, & R. Neches (Eds.), </editor> <title> Production system models of learning and development, </title> <address> Cambridge, MA: </address> <publisher> MIT Press. </publisher>
Reference-contexts: Not included in these studies is the minute adjustment of strategies exhibited by experts performing real-time control tasks <ref> (Anzai, 1987) </ref>. Such fine-tuning is often relegated to numerical representations, with the assumption that parameter adjustment is useful only after a good representation and the fundamental steps for a successful strategy have been found. <p> In this section the computations are summarized (see Anderson, 1986, Barto et al., 1983, and Rumelhart, Hinton, & Williams, 1986, for further explanations). 3.1 Generation of Behavior|Search The pole-balancing task differs from those commonly used to demonstrate strategy-learning methods in that it is a real-time task <ref> (Anzai, 1987) </ref>. The learning system cannot control the timing of the task's state transitions, therefore the response time of the learning system is critical.
Reference: <author> Barto, A. G. </author> <year> (1985). </year> <title> Learning by statistical cooperation of self-interested neuron-like computing elements. </title> <journal> Human Neurobiology, </journal> <volume> 4, </volume> <pages> 229-256. </pages>
Reference: <author> Barto, A. G., Sutton, R. S., & Anderson, C. W. </author> <year> (1983). </year> <title> Neuronlike elements that can solve difficult learning control problems. </title> <journal> IEEE Trans. on Systems, Man, and Cybernetics, </journal> <volume> 13, </volume> <pages> 835-846. </pages>
Reference: <author> Barto, A. G., Sutton, R. S., & Brouwer, P. S. </author> <year> (1981). </year> <title> Associative search network: a reinforcement learning associative memory. </title> <journal> Biological Cybernetics, </journal> <volume> 40, </volume> <pages> 201-211. </pages>
Reference: <author> Bush, R. R. </author> & <title> Estes, </title> <editor> W. K. (Eds.). </editor> <booktitle> (1959). Studies in mathematical learning theory, </booktitle> <publisher> Stanford University Press. 10 Cannon, </publisher> <editor> R. H., Jr. </editor> <year> (1967). </year> <title> Dynamics of Physical Systems. </title> <publisher> McGraw-Hill, Inc. </publisher>
Reference-contexts: Learning must be based on the inaccurate, time-varying, and delayed evaluations of the adaptive evaluation function. The numerical approach to learning with such performance feedback is called reinforcement learning. Reinforcement-learning methods have been developed for mathematical learning theory <ref> (Bush & Estes, 1959) </ref>, learning automata (Narendra and Thathachar, 1974), learning control (Mendel and McLaren, 1970), and connectionist systems (Barto, 1985; Barto, Sutton, & Brouwer, 1981). The reinforcement-learning method used here operates as follows.
Reference: <author> Cheok, K. C. & Loh, N. K. </author> <year> (1987). </year> <title> A ball-balancing demonstration of optimal and disturbance-accommodating control. </title> <journal> IEEE Control Systems Magazine, </journal> <month> February, </month> <year> 1987, </year> <pages> 54-57. </pages>
Reference: <author> Klahr, D., Langley, P., </author> & <title> Neches, </title> <editor> R. (Eds.). </editor> <year> (1987). </year> <title> Production system models of learning and development, </title> <address> Cambridge, MA: </address> <publisher> MIT Press. </publisher>
Reference-contexts: Most research on the learning of strategies has used symbolic forms of representation, such as production rules, which have been useful in modeling some of the conscious steps that humans appear to follow in acquiring strategies <ref> (Klahr, Langley, & Neches, 1987) </ref>. Not included in these studies is the minute adjustment of strategies exhibited by experts performing real-time control tasks (Anzai, 1987).
Reference: <author> Langley, P. </author> <year> (1985). </year> <title> Learning to search: from weak methods to domain-specific heuristics. </title> <journal> Cognitive Science, </journal> <volume> 9, </volume> <pages> 217-260. </pages>
Reference: <author> Mendel, J. M. & McLaren, R. W. </author> <year> (1970). </year> <title> Reinforcement learning control and pattern recognition systems. </title> <editor> In J. M. Mendel & K. S. Fu (EDS.), </editor> <title> Adaptive, learning and pattern recognition systems: theory and applications. </title> <publisher> Academic Press. </publisher>
Reference-contexts: The numerical approach to learning with such performance feedback is called reinforcement learning. Reinforcement-learning methods have been developed for mathematical learning theory (Bush & Estes, 1959), learning automata (Narendra and Thathachar, 1974), learning control <ref> (Mendel and McLaren, 1970) </ref>, and connectionist systems (Barto, 1985; Barto, Sutton, & Brouwer, 1981). The reinforcement-learning method used here operates as follows.
Reference: <author> Narendra, K. S. & Thathachar, M. A. L. </author> <year> (1974). </year> <title> Learning automata|a survey. </title> <journal> IEEE Transactions on Systems, Man, and Cybernetics, </journal> <volume> 4, </volume> <pages> 323-334. </pages>
Reference-contexts: Learning must be based on the inaccurate, time-varying, and delayed evaluations of the adaptive evaluation function. The numerical approach to learning with such performance feedback is called reinforcement learning. Reinforcement-learning methods have been developed for mathematical learning theory (Bush & Estes, 1959), learning automata <ref> (Narendra and Thathachar, 1974) </ref>, learning control (Mendel and McLaren, 1970), and connectionist systems (Barto, 1985; Barto, Sutton, & Brouwer, 1981). The reinforcement-learning method used here operates as follows.
Reference: <author> Plaut, D. C., Nowlan, S. J., & Hinton, G. E. </author> <year> (1986). </year> <title> Experiments on learning by back propagation. </title> <type> (Technical Report CMU-CS-86-126). </type> <institution> Pittsburgh, PA: Carnegie-Mellon University, Department of Computer Science. </institution>
Reference: <author> Rumelhart, D. E., Hinton, G. E., & Williams, R. </author> <year> (1986). </year> <title> Learning internal representations by error propagation. </title> <editor> In D. E. Rumelhart, J. L. McClelland, </editor> & <booktitle> the PDP research group (Eds.), Parallel distributed processing: explorations in the microstructure of cognition, </booktitle> <address> Cambridge, MA: </address> <publisher> Bradford Books. </publisher>
Reference: <author> Samuel, A. L. </author> <year> (1959). </year> <title> Some studies in machine learning using the game of checkers. </title> <journal> IBM Journal on Research and Development, </journal> <volume> 3, </volume> <pages> 210-229. </pages>
Reference: <author> Selfridge O., Sutton, R. S. & Barto, A. G. </author> <year> (1985). </year> <title> Training and tracking in robotics. </title> <booktitle> Proceedings of the Ninth International Joint Conference on Artificial Intelligence, </booktitle> <address> Los Angeles, CA. </address>
Reference: <author> Sutton, R. S. </author> <year> (1984). </year> <title> Temporal aspects of credit assignment in reinforcement learning. </title> <type> Doctoral Dissertation, </type> <institution> Department of Computer and Information Science, University of Massachusetts, </institution> <address> Amherst, MA. </address>
Reference-contexts: 1 ; t 2 ] = g ( j=1 v [t 1 ; t 2 ] = i=1 5 X c i [t 1 ]y i [t 1 ; t 2 ]; 1 + e s ; Double time dependencies are used to avoid instabilities in the updating of weights <ref> (Sutton, 1984) </ref> Action Evaluation: Failure Signal Plus Change in State Evaluation ^r [t + 1] = &gt; &gt; &gt; &gt; &gt; &lt; 0; if state at time t + 1 is a start state; r [t + 1] if state at time t + 1 v [t; t]; is a failure
Reference: <author> Sutton, R. S. </author> <year> (1987). </year> <title> Learning to predict by the methods of temporal differences. </title> <type> (Technical Report TR87-509.1). </type> <institution> Waltham, MA: GTE Laboratories Incorporated. </institution>
Reference-contexts: The evaluation network learns this function using a generalization of Samuel's (1959) method called the Adaptive Heuristic Critic (AHC), a member of the class of prediction methods called temporal difference methods <ref> (Sutton, 1987) </ref>. The AHC algorithm develops an evaluation function whose value v for a given state is a prediction of future discounted failure signals. Changes in v due to problem-state transitions are combined with the failure signal r to form ^r.
Reference: <author> Sutton, R. S. & Pinette, B. </author> <year> (1985). </year> <title> The learning of world models by connectionist networks. </title> <booktitle> In Proceedings of the Seventh Annual Conference of the Cognitive Science Society (pp. </booktitle> <pages> 54-64). </pages> <address> Irvine, CA. </address>
Reference-contexts: Such a model can be used either to search for actions in an off-line, look-ahead type search, or to form a prediction of future evaluations <ref> (Sutton & Pinette, 1985) </ref>. Appendix: Cart-pole Simulation The dynamics of the cart-pole system are given by the following equations of motion.
References-found: 19

