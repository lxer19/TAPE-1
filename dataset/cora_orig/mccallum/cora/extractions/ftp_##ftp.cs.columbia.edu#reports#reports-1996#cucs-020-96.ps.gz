URL: ftp://ftp.cs.columbia.edu/reports/reports-1996/cucs-020-96.ps.gz
Refering-URL: http://www.cs.columbia.edu/~library/1996.html
Root-URL: http://www.cs.columbia.edu
Title: Explicit Cost Bounds of Algorithms for Multivariate Tensor Product Problems  
Author: Grzegorz W. Wasilkowski and Henryk Wozniakowski 
Note: (c(d) 2) fi 1 fi 2 fi 3 d  n("; d) 7:26 2:454 8 d; 1: The first author was partially supported by the the National Science Foundation, and the second by the National Science Foundation and the Air Force Office of Scientific Research.  
Date: March, 1994  1  
Affiliation: Columbia University Computer Science Department  
Pubnum: Report CUCS-020-96  
Abstract: We study multivariate tensor product problems in the worst case and average case settings. They are defined on functions of d variables. For arbitrary d, we provide explicit upper bounds on the costs of algorithms which compute an "-approximation to the solution. The cost bounds are of the form fi 5 Here c(d) is the cost of one function evaluation (or one linear functional evaluation), and fi i 's do not depend on d; they are determined by the properties of the problem for d = 1. For certain tensor product problems, these cost bounds do not exceed c(d) K " p for some numbers K and p, both independent of d. We apply these general estimates to certain integration and approximation problems in the worst and average case settings. We also obtain an upper bound, which is independent of d, for the number, n("; d), of points for which discrepancy (with unequal weights) is at most ",
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> N. Aronszajn, </author> <title> Theory of reproducing kernels, </title> <journal> Trans. Amer. Math. Soc., </journal> <volume> 68, </volume> <pages> p. 337-404, </pages> <year> 1950. </year>
Reference-contexts: Furthermore, it is not necessarily clear why we should insist on kS 1 k set = 1. For instance, consider the worst case setting with the unit ball in the Sobolev space F 1 of absolutely continuous functions defined over <ref> [0; 1] </ref>, vanishing at zero and with the first derivative in G 1 = L 2 ([0; 1]). The norm in F 1 is ( R 1 0 jf 0 (t)j 2 dt) 1=2 . Let S 1 (f ) = ff f . <p> Furthermore, by Q (q; d) we mean Q (q; d) = ~ i = [i 1 ; i 2 ; : : : ; i d ] : ~ 1 ~ i; j ~ i j q ; with ~ 1 = <ref> [1; 1; : : : ; 1] </ref>: The cardinality of the set Q (q; d) is equal to the binomial coefficient d . <p> We now discuss the class fl std . Recall that in the worst case setting we assume that F d is a Hilbert space. Then continuity of functionals in fl std is equivalent to F d being a reproducing kernel space, see <ref> [1] </ref>. Obviously, F d is a reproducing kernel space iff F 1 has this property. In the average case setting, the continuity of function evaluations is equivalent to continuity of the covariance kernel function of the measure d . <p> Such a root is unique. Indeed, for fl 1 1, h 0 (y) 0; and for fl 1 &lt; 1, h does not have a root in <ref> [fl 1 ; 1] </ref> since h 0 is positive and h (fl 1 ) &gt; 0. <p> For t 2 <ref> [3m i fi=(3m i +1); 1] </ref> we set U i (f)(t) f (3m i fi=(3m i + It is known, see [9], that the algorithm U i is optimal and its average case error is e (U i ) = kS 1 U i k 1 = q ; i 0: <p> This holds for all ff and the norm of S 1 can be arbitrary large for sufficiently large ff. We now present an example of a linear functional S 1 with norm 1 which yields an intractable tensor product problem. Let h 1 and h 2 be defined on <ref> [0; 1] </ref> with the support (0; 1=2) and (1=2; 1), respectively. Take F 1 = linfh 1 ; h 2 g with inner-product defined by the condition that h i are orthonormal. Define the linear functional S 1 such that Sh i = 1= p 2. <p> It is known that e fl (n; d) = sup j hf; h d i j: We select a special f to bound e fl (n; d) from below. Let x j;k denote the kth component of x j . Choose f k : <ref> [0; 1] </ref> ! IR such that f k (x j;k ) = 0 for j = 1; 2 : : : ; n, kf k k = 1 and e k = S 1 f e fl (n; 1).
Reference: [2] <author> K. I. Babenko, </author> <title> On the approximation of a class of periodic functions of several variables by trigonometric polynomials, </title> <journal> Dokl. Akad. Nauk SSSR (English trans. in Soviet Math. Dokl, </journal> <volume> 1, </volume> <year> 1960), </year> <pages> 132, </pages> <address> p. </address> <pages> 247-250, 982-985, </pages> <year> 1960. </year>
Reference-contexts: The algorithms studied in this paper were presented by Smolyak [19] who studied tensor product problems in the worst case setting. Antecedents of these algorithms may be found 2 in <ref> [2] </ref>. Later, these algorithms have been used in many papers for specific problems; we only mention some recent papers [7, 13, 14, 15, 20, 21, 22, 26, 30]. <p> The solution set of (16) forms part of a hyperbolic cross. That is why the information N q;d is called hyperbolic cross information. The study and power of such hyperbolic cross information has been initiated by Babenko <ref> [2] </ref> who studied approximation of periodic functions by polynomials that use Fourier coefficients whose indices are from a hyperbolic cross. There are many papers where the power of hyperbolic cross information has been studied for a number of problems in different settings.
Reference: [3] <author> J. Beck and W. W. L. Chen, </author> <title> Irregularities of Distribution, </title> <publisher> Cambridge University Press, </publisher> <address> Cambridge, </address> <year> 1987. </year>
Reference-contexts: This problem is defined as finding n points in the d dimensional unit cube which approximate the volumes of rectangles with minimal error, see [11] for the precise definition and basic properties. Discrepancy has been extensively studied in number theory, see e.g., <ref> [3, 11, 17, 18] </ref>. It has recently been applied in computer science, see [4, 5] and the references given there. Discrepancy is related to multivariate integration.
Reference: [4] <author> B. Chazelle, </author> <title> Geometric Discrepancy Revisited, </title> <booktitle> 34th Annual Symposium on Foundations of Computer Science, p. </booktitle> <pages> 392-399, </pages> <year> 1993. </year>
Reference-contexts: Discrepancy has been extensively studied in number theory, see e.g., [3, 11, 17, 18]. It has recently been applied in computer science, see <ref> [4, 5] </ref> and the references given there. Discrepancy is related to multivariate integration. Indeed, discrepancy is an upper bound on the worst case integration error of functions whose variation in the sense of Hardy and Krause is at most one, see [11].
Reference: [5] <author> D. P. Dobkin and D. P. Mitchell, </author> <title> Random-Edge Discrepancy of Supersampling Patterns, Graphics Interface '93, </title> <address> York, Ontario, </address> <year> 1993. </year>
Reference-contexts: Discrepancy has been extensively studied in number theory, see e.g., [3, 11, 17, 18]. It has recently been applied in computer science, see <ref> [4, 5] </ref> and the references given there. Discrepancy is related to multivariate integration. Indeed, discrepancy is an upper bound on the worst case integration error of functions whose variation in the sense of Hardy and Krause is at most one, see [11]. <p> However, not every index ~ j satisfying (16) must correspond to some L ~ i; ~ j . For example, take d = F = 2 and m i = F i with q = 5. Then ~ j = <ref> [5; 5] </ref> is a counter-example. Thus, the indices of the functionals of the information N q;d are a subset of the solution of the hyperbolic inequality (16). The solution set of (16) forms part of a hyperbolic cross. That is why the information N q;d is called hyperbolic cross information.
Reference: [6] <author> I. S. Gradshteyn and I. Ryzhik, </author> <title> Table of Integrals, Series and Products, </title> <publisher> Academic Press, </publisher> <address> New York, </address> <year> 1980. </year> <month> 52 </month>
Reference-contexts: Using the form of a (q 1 i; d 1 i) and the identity qi1 X d 2 i k = q d ! see 0.151.1 of <ref> [6] </ref>, we obtain the needed estimate. <p> Since p q 1, the cardinality m (q; d) is bounded by q1 X p 1 ! Hence m (q; d) m (q 1; d) = F qd (F 1) d p=d1 p 1 ! d 1 ; the latter equality follows from 0.151.1 of <ref> [6] </ref>. Hence m (q; d) = (F 1) d j=0 d 1 (F 1) d1 F qd+1 q 1 ! We summarize the cost estimate in the following Lemma. 25 Lemma 7 Let (42) hold and q d.
Reference: [7] <author> S. Heinrich, </author> <title> Complexity of integral equations and relations to s-numbers, </title> <journal> J. Complexity, </journal> <volume> 9, </volume> <pages> p. 141-153, </pages> <year> 1993. </year>
Reference-contexts: The algorithms studied in this paper were presented by Smolyak [19] who studied tensor product problems in the worst case setting. Antecedents of these algorithms may be found 2 in [2]. Later, these algorithms have been used in many papers for specific problems; we only mention some recent papers <ref> [7, 13, 14, 15, 20, 21, 22, 26, 30] </ref>. In all these papers, only asymptotic cost (or error) bounds are provided, i.e., " is assumed to be sufficiently small whereas d is arbitrary but fixed. In this paper we will obtain explicit cost bounds for all d and ". <p> There are many papers where the power of hyperbolic cross information has been studied for a number of problems in different settings. The reader is referred to <ref> [7, 13, 14, 15, 16, 19, 20, 21, 22, 26, 30] </ref>. 4 Error Analysis 4.1 Worst Case We analyze the error of the algorithm (10) in terms of its error for d = 1.
Reference: [8] <author> H.-H. Kuo, H.-H. </author> <title> Gaussian Measures in Banach Spaces, </title> <booktitle> Lectures Notes in Mathematics, </booktitle> <volume> 463, </volume> <publisher> Springer Verlag, </publisher> <address> Berlin, </address> <year> 1975. </year>
Reference-contexts: f , not necessarily from the unit ball, kS d (f ) A (f )k G d e (A)kf k F d : 2.2 Average Case Setting In the average case setting, we assume that the Banach space F 1 is equipped with a zero-mean Gaussian measure 1 , see <ref> [8, 25] </ref>. To preserve the tensor product structure, we assume that F d is equipped with a tensor product measure d , d = 1 1 .
Reference: [9] <author> D. Lee, </author> <title> Approximation of linear operators on a Wiener space, Rocky Mount. </title> <journal> J. Math, </journal> <volume> 16, </volume> <pages> p. 641-659, </pages> <year> 1986. </year>
Reference-contexts: for d = 1 the information N i (f) = f 2fi ! 2m i + 1 ; : : : ; f 2m i fi !# and the algorithms U i (f ) = 2m i + 1 j=1 2m i + 1 : (63) It is known, see <ref> [9] </ref>, that the algorithm U i is optimal and its average case error is e (U i ) = kS 1 U i k 1 = p ; i 0: Observe that for m i = 2 3 i 1 the information N i is nested, and the assumptions (26) and <p> For t 2 [3m i fi=(3m i +1); 1] we set U i (f)(t) f (3m i fi=(3m i + It is known, see <ref> [9] </ref>, that the algorithm U i is optimal and its average case error is e (U i ) = kS 1 U i k 1 = q ; i 0: Observe that for m i = 3 4 i 1 the information N i is nested, and the assumptions (26) and
Reference: [10] <author> C. A. Micchelli and T. J. </author> <title> Rivlin , A survey of optimal recovery, in Optimal Estimation in Approximation Theory (C. </title> <editor> A. Micchelli and T.J. Rivlin, eds.) p. </editor> <address> 1-54, </address> <publisher> Plenum Press, </publisher> <address> New York, </address> <year> 1977. </year>
Reference-contexts: In fact, the projection form implies additional error properties such as minimizing all local errors, see, e.g., <ref> [10, 24, 23] </ref>. This is why such algorithms are sometimes referred to as central or strongly optimal. 2 We now improve the estimate of Lemma 2 when the following additional assumption is made.
Reference: [11] <author> H. Niederreiter, </author> <title> Random number generator and quasi-Monte Carlo methods, </title> <journal> CBMS-NSF Reg. Conf. Series Appl. Math., </journal> <volume> 63, </volume> <publisher> SIAM, </publisher> <address> Philadelphia, </address> <year> 1992. </year>
Reference-contexts: Another interesting application is discrepancy (with unequal weights) in the L 2 norm. This problem is defined as finding n points in the d dimensional unit cube which approximate the volumes of rectangles with minimal error, see <ref> [11] </ref> for the precise definition and basic properties. Discrepancy has been extensively studied in number theory, see e.g., [3, 11, 17, 18]. It has recently been applied in computer science, see [4, 5] and the references given there. Discrepancy is related to multivariate integration. <p> This problem is defined as finding n points in the d dimensional unit cube which approximate the volumes of rectangles with minimal error, see [11] for the precise definition and basic properties. Discrepancy has been extensively studied in number theory, see e.g., <ref> [3, 11, 17, 18] </ref>. It has recently been applied in computer science, see [4, 5] and the references given there. Discrepancy is related to multivariate integration. <p> Discrepancy is related to multivariate integration. Indeed, discrepancy is an upper bound on the worst case integration error of functions whose variation in the sense of Hardy and Krause is at most one, see <ref> [11] </ref>. It is also known, see [29], that discrepancy (with optimally chosen weights) is equal to the minimal average case integration error of continuous functions defined over the d dimensional unit cube and equipped with the Wiener sheet measure. <p> The norm is given by kfk H 1 = R fi 1=2 8.3 Discrepancy We utilize relations between integration in the average case setting with the Wiener sheet measure and discrepancy in the L 2 norm, see <ref> [11] </ref> for the definition and basic properties of discrepancy. It was shown in [29] that the minimal discrepancy (with optimal weights) of n points in the d dimensional unit cube is equal to the minimal average case error of integration algorithms.
Reference: [12] <author> A. Papageorgiou and G. W. Wasilkowski, </author> <title> On the average complexity of multivariate problems, </title> <journal> J. Complexity, </journal> <volume> 6, </volume> <pages> p 1-23, </pages> <year> 1990. </year>
Reference-contexts: Even if we use equally-spaced sample points for d = 1 then the position of sample points in d dimensions is very different from grid points. In fact, it is known that grid points are a very poor choice of sample points for tensor product problems, see e.g., <ref> [12, 33] </ref>. Information used by the algorithms is called hyperbolic cross information and have been successfully applied for a number of problems, see Section 3 for details and references. We summarize the result of Smolyak [19] more precisely. He studied the worst case setting only.
Reference: [13] <author> S. Paskov, </author> <title> Average case complexity of multivariate integration for smooth functions, </title> <journal> J. Complexity, </journal> <volume> 9, p.291-312, </volume> <year> 1993. </year>
Reference-contexts: The algorithms studied in this paper were presented by Smolyak [19] who studied tensor product problems in the worst case setting. Antecedents of these algorithms may be found 2 in [2]. Later, these algorithms have been used in many papers for specific problems; we only mention some recent papers <ref> [7, 13, 14, 15, 20, 21, 22, 26, 30] </ref>. In all these papers, only asymptotic cost (or error) bounds are provided, i.e., " is assumed to be sufficiently small whereas d is arbitrary but fixed. In this paper we will obtain explicit cost bounds for all d and ". <p> the complexity comp ("; d) = K fl d (c (d) + 2) " fi fl 5 (ln 1=") fi fl 4 (d1) (1 + o (1)) as " ! 0 (3) with known fi fl 4 and fi fl 5 , but unknown K fl d , see e.g., <ref> [13, 15, 16, 20, 22, 29, 30] </ref>. The limited knowledge of the behavior of the complexity makes the question of optimality of the cost bound (2) even harder. A partial answer is provided by comparing the exponents fi 5 ; fi 4 with fi fl 4 . <p> There are many papers where the power of hyperbolic cross information has been studied for a number of problems in different settings. The reader is referred to <ref> [7, 13, 14, 15, 16, 19, 20, 21, 22, 26, 30] </ref>. 4 Error Analysis 4.1 Worst Case We analyze the error of the algorithm (10) in terms of its error for d = 1.
Reference: [14] <author> S. V. Pereverzev, </author> <title> On the complexity of the problem of finding solutions of Fredholm equations of the second kind with differentiable kernels (in Russian), Ukrain. </title> <journal> Mat. Sh., </journal> <volume> 41, </volume> <pages> p. 1422-1425, </pages> <year> 1989. </year>
Reference-contexts: The algorithms studied in this paper were presented by Smolyak [19] who studied tensor product problems in the worst case setting. Antecedents of these algorithms may be found 2 in [2]. Later, these algorithms have been used in many papers for specific problems; we only mention some recent papers <ref> [7, 13, 14, 15, 20, 21, 22, 26, 30] </ref>. In all these papers, only asymptotic cost (or error) bounds are provided, i.e., " is assumed to be sufficiently small whereas d is arbitrary but fixed. In this paper we will obtain explicit cost bounds for all d and ". <p> There are many papers where the power of hyperbolic cross information has been studied for a number of problems in different settings. The reader is referred to <ref> [7, 13, 14, 15, 16, 19, 20, 21, 22, 26, 30] </ref>. 4 Error Analysis 4.1 Worst Case We analyze the error of the algorithm (10) in terms of its error for d = 1.
Reference: [15] <author> K. Ritter, G. W. Wasilkowski, and H. Wozniakowski, </author> <title> On multivariate integration for stochastic processes, in Numerical Integration (H. Bra, </title> <editor> G. Hammerlin, eds.), p. </editor> <booktitle> 331-347, International Series of Numerical Mathematics, </booktitle> <volume> vol. 112, </volume> <publisher> Birkhauser, </publisher> <address> Basel, </address> <year> 1993. </year>
Reference-contexts: The algorithms studied in this paper were presented by Smolyak [19] who studied tensor product problems in the worst case setting. Antecedents of these algorithms may be found 2 in [2]. Later, these algorithms have been used in many papers for specific problems; we only mention some recent papers <ref> [7, 13, 14, 15, 20, 21, 22, 26, 30] </ref>. In all these papers, only asymptotic cost (or error) bounds are provided, i.e., " is assumed to be sufficiently small whereas d is arbitrary but fixed. In this paper we will obtain explicit cost bounds for all d and ". <p> the complexity comp ("; d) = K fl d (c (d) + 2) " fi fl 5 (ln 1=") fi fl 4 (d1) (1 + o (1)) as " ! 0 (3) with known fi fl 4 and fi fl 5 , but unknown K fl d , see e.g., <ref> [13, 15, 16, 20, 22, 29, 30] </ref>. The limited knowledge of the behavior of the complexity makes the question of optimality of the cost bound (2) even harder. A partial answer is provided by comparing the exponents fi 5 ; fi 4 with fi fl 4 . <p> There are many papers where the power of hyperbolic cross information has been studied for a number of problems in different settings. The reader is referred to <ref> [7, 13, 14, 15, 16, 19, 20, 21, 22, 26, 30] </ref>. 4 Error Analysis 4.1 Worst Case We analyze the error of the algorithm (10) in terms of its error for d = 1.
Reference: [16] <author> K. Ritter, G. W. Wasilkowski, and H. Wozniakowski, </author> <title> Multivariate integration and approximation for random fields satisfying Sacks-Ylvisaker conditions, </title> <note> submitted for publication, </note> <year> 1993. </year>
Reference-contexts: the complexity comp ("; d) = K fl d (c (d) + 2) " fi fl 5 (ln 1=") fi fl 4 (d1) (1 + o (1)) as " ! 0 (3) with known fi fl 4 and fi fl 5 , but unknown K fl d , see e.g., <ref> [13, 15, 16, 20, 22, 29, 30] </ref>. The limited knowledge of the behavior of the complexity makes the question of optimality of the cost bound (2) even harder. A partial answer is provided by comparing the exponents fi 5 ; fi 4 with fi fl 4 . <p> There are many papers where the power of hyperbolic cross information has been studied for a number of problems in different settings. The reader is referred to <ref> [7, 13, 14, 15, 16, 19, 20, 21, 22, 26, 30] </ref>. 4 Error Analysis 4.1 Worst Case We analyze the error of the algorithm (10) in terms of its error for d = 1.
Reference: [17] <author> K. F. Roth, </author> <title> On irregularities of distribution, </title> <journal> Mathematika, </journal> <volume> 1, </volume> <pages> p. 73-79, </pages> <year> 1954. </year>
Reference-contexts: This problem is defined as finding n points in the d dimensional unit cube which approximate the volumes of rectangles with minimal error, see [11] for the precise definition and basic properties. Discrepancy has been extensively studied in number theory, see e.g., <ref> [3, 11, 17, 18] </ref>. It has recently been applied in computer science, see [4, 5] and the references given there. Discrepancy is related to multivariate integration.
Reference: [18] <author> K. F. Roth, </author> <title> On irregularities of distribution, IV, </title> <journal> Acta Arith., </journal> <volume> 37, </volume> <pages> p. 67-75, </pages> <year> 1980. </year>
Reference-contexts: This problem is defined as finding n points in the d dimensional unit cube which approximate the volumes of rectangles with minimal error, see [11] for the precise definition and basic properties. Discrepancy has been extensively studied in number theory, see e.g., <ref> [3, 11, 17, 18] </ref>. It has recently been applied in computer science, see [4, 5] and the references given there. Discrepancy is related to multivariate integration.
Reference: [19] <author> S. A. Smolyak, </author> <title> Quadrature and interpolation formulas for tensor products of certain classes of functions, </title> <journal> Dokl. Akad. Nauk SSSR, </journal> <volume> p. </volume> <pages> 240-243, </pages> <year> 1964. </year> <month> 53 </month>
Reference-contexts: The algorithms studied in this paper were presented by Smolyak <ref> [19] </ref> who studied tensor product problems in the worst case setting. Antecedents of these algorithms may be found 2 in [2]. Later, these algorithms have been used in many papers for specific problems; we only mention some recent papers [7, 13, 14, 15, 20, 21, 22, 26, 30]. <p> Information used by the algorithms is called hyperbolic cross information and have been successfully applied for a number of problems, see Section 3 for details and references. We summarize the result of Smolyak <ref> [19] </ref> more precisely. He studied the worst case setting only. Let A " (d) denote an algorithm which computes an "-approximation for the d dimensional case. For simplicity we assume that " " 0 &lt; 1. <p> As already indicated in the introduction, these algorithms were analyzed by Smolyak <ref> [19] </ref> in the worst case setting but without studying the dependence on d. The essence of these algorithms is that, for arbitrary d 2, they are given by certain combinations of tensor products of one dimensional (d = 1) algorithms. <p> There are many papers where the power of hyperbolic cross information has been studied for a number of problems in different settings. The reader is referred to <ref> [7, 13, 14, 15, 16, 19, 20, 21, 22, 26, 30] </ref>. 4 Error Analysis 4.1 Worst Case We analyze the error of the algorithm (10) in terms of its error for d = 1.
Reference: [20] <author> V. N. Temlyakov, </author> <title> Approximate recovery of periodic functions of several variables, </title> <journal> Math. USSR Sbornik, </journal> <volume> 56, </volume> <pages> p. 249-261, </pages> <year> 1987. </year>
Reference-contexts: The algorithms studied in this paper were presented by Smolyak [19] who studied tensor product problems in the worst case setting. Antecedents of these algorithms may be found 2 in [2]. Later, these algorithms have been used in many papers for specific problems; we only mention some recent papers <ref> [7, 13, 14, 15, 20, 21, 22, 26, 30] </ref>. In all these papers, only asymptotic cost (or error) bounds are provided, i.e., " is assumed to be sufficiently small whereas d is arbitrary but fixed. In this paper we will obtain explicit cost bounds for all d and ". <p> the complexity comp ("; d) = K fl d (c (d) + 2) " fi fl 5 (ln 1=") fi fl 4 (d1) (1 + o (1)) as " ! 0 (3) with known fi fl 4 and fi fl 5 , but unknown K fl d , see e.g., <ref> [13, 15, 16, 20, 22, 29, 30] </ref>. The limited knowledge of the behavior of the complexity makes the question of optimality of the cost bound (2) even harder. A partial answer is provided by comparing the exponents fi 5 ; fi 4 with fi fl 4 . <p> There are many papers where the power of hyperbolic cross information has been studied for a number of problems in different settings. The reader is referred to <ref> [7, 13, 14, 15, 16, 19, 20, 21, 22, 26, 30] </ref>. 4 Error Analysis 4.1 Worst Case We analyze the error of the algorithm (10) in terms of its error for d = 1.
Reference: [21] <author> V. N. Temlyakov, </author> <title> On a way of obtaining lower estimates for the errors of quadrature formulas errors, </title> <journal> Math. USSR Sbornik, </journal> <volume> 181, </volume> <pages> p. 1403-1413, </pages> <note> 1990 (in Russian), Math. USSR Sbornik, 71, p. 247-257, 1992 (in English). </note>
Reference-contexts: The algorithms studied in this paper were presented by Smolyak [19] who studied tensor product problems in the worst case setting. Antecedents of these algorithms may be found 2 in [2]. Later, these algorithms have been used in many papers for specific problems; we only mention some recent papers <ref> [7, 13, 14, 15, 20, 21, 22, 26, 30] </ref>. In all these papers, only asymptotic cost (or error) bounds are provided, i.e., " is assumed to be sufficiently small whereas d is arbitrary but fixed. In this paper we will obtain explicit cost bounds for all d and ". <p> There are many papers where the power of hyperbolic cross information has been studied for a number of problems in different settings. The reader is referred to <ref> [7, 13, 14, 15, 16, 19, 20, 21, 22, 26, 30] </ref>. 4 Error Analysis 4.1 Worst Case We analyze the error of the algorithm (10) in terms of its error for d = 1.
Reference: [22] <author> V. N. Temlyakov, </author> <title> On approximate recovery of functions with bounded mixed derivative, </title> <journal> J. Complexity, </journal> <volume> 9, </volume> <pages> p. 41-59, </pages> <year> 1993. </year>
Reference-contexts: The algorithms studied in this paper were presented by Smolyak [19] who studied tensor product problems in the worst case setting. Antecedents of these algorithms may be found 2 in [2]. Later, these algorithms have been used in many papers for specific problems; we only mention some recent papers <ref> [7, 13, 14, 15, 20, 21, 22, 26, 30] </ref>. In all these papers, only asymptotic cost (or error) bounds are provided, i.e., " is assumed to be sufficiently small whereas d is arbitrary but fixed. In this paper we will obtain explicit cost bounds for all d and ". <p> the complexity comp ("; d) = K fl d (c (d) + 2) " fi fl 5 (ln 1=") fi fl 4 (d1) (1 + o (1)) as " ! 0 (3) with known fi fl 4 and fi fl 5 , but unknown K fl d , see e.g., <ref> [13, 15, 16, 20, 22, 29, 30] </ref>. The limited knowledge of the behavior of the complexity makes the question of optimality of the cost bound (2) even harder. A partial answer is provided by comparing the exponents fi 5 ; fi 4 with fi fl 4 . <p> There are many papers where the power of hyperbolic cross information has been studied for a number of problems in different settings. The reader is referred to <ref> [7, 13, 14, 15, 16, 19, 20, 21, 22, 26, 30] </ref>. 4 Error Analysis 4.1 Worst Case We analyze the error of the algorithm (10) in terms of its error for d = 1.
Reference: [23] <author> J. F. Traub, G. W. Wasilkowski, and H. Wozniakowski, </author> <title> Information-based Complexity, </title> <publisher> Academic Press, </publisher> <address> New York, </address> <year> 1988. </year>
Reference-contexts: The optimal choice of sample points and algorithms is a challenging problem for many multivariate problems. We add that the optimal choice of arbitrary continuous linear functionals and algorithms is known, see Chapters 4 and 6 of <ref> [23] </ref>. We assume that the cost of computing L (f) is c (d) 1. We also assume that we can perform combinatory operations such as arithmetic operations and comparisons at unit cost. <p> Without loss of generality, we restrict ourselves to nonadaptive information and linear algorithms since for the settings considered in this paper, adaption and nonlinear algorithms are not essentially better, see <ref> [28, 23] </ref>. <p> In fact, the projection form implies additional error properties such as minimizing all local errors, see, e.g., <ref> [10, 24, 23] </ref>. This is why such algorithms are sometimes referred to as central or strongly optimal. 2 We now improve the estimate of Lemma 2 when the following additional assumption is made. <p> We also assume that the algorithms U i are optimal, i.e., they minimize the average case error among all algorithms that use the information N i . We now recall the form of optimal algorithms in the average case setting for arbitrary d. It is known, see, e.g., <ref> [23] </ref>, that for Gaussian measures and a linear operator S, the optimal algorithm that uses information N equals Sm (y), where m (y) is the mean of the conditional measure given y = N (f ). <p> N i (f ) = f fi ! m i + 1 ; : : : ; f m i fi !# and U i as the trapezoidal algorithm, U i (f) = m i j=1 m i + 1 : From Sections 2.1 of Chapters 5 and 7 of <ref> [23] </ref>, it follows that the algorithm U i is optimal and its average case error is given by e (U i ) = kS 1 U i k 1 = (m i + 1) r+1 ; i 0; where C r = q jB 2r+2 j=(2r + 2)! with the Bernoulli
Reference: [24] <author> J. F. Traub and H. Wozniakowski, </author> <title> A General Theory of Optimal Algorithms, </title> <publisher> Academic Press, </publisher> <address> New York, </address> <year> 1980. </year>
Reference-contexts: In fact, the projection form implies additional error properties such as minimizing all local errors, see, e.g., <ref> [10, 24, 23] </ref>. This is why such algorithms are sometimes referred to as central or strongly optimal. 2 We now improve the estimate of Lemma 2 when the following additional assumption is made.
Reference: [25] <author> N. N. Vakhania, V. I. Tarieladze, and S. A. Chobanyan, </author> <title> Probability Distributions on Banach Spaces, </title> <publisher> Reidel, </publisher> <address> Dordrecht, </address> <year> 1987. </year>
Reference-contexts: f , not necessarily from the unit ball, kS d (f ) A (f )k G d e (A)kf k F d : 2.2 Average Case Setting In the average case setting, we assume that the Banach space F 1 is equipped with a zero-mean Gaussian measure 1 , see <ref> [8, 25] </ref>. To preserve the tensor product structure, we assume that F d is equipped with a tensor product measure d , d = 1 1 .
Reference: [26] <author> G. Wahba, </author> <title> Interpolating Surfaces: High Order Convergence Rates and Their Associated Designs, with Application to X-Ray Image Reconstruction, </title> <institution> Dept. of Statistics, University of Wisconsin, </institution> <year> 1978. </year>
Reference-contexts: The algorithms studied in this paper were presented by Smolyak [19] who studied tensor product problems in the worst case setting. Antecedents of these algorithms may be found 2 in [2]. Later, these algorithms have been used in many papers for specific problems; we only mention some recent papers <ref> [7, 13, 14, 15, 20, 21, 22, 26, 30] </ref>. In all these papers, only asymptotic cost (or error) bounds are provided, i.e., " is assumed to be sufficiently small whereas d is arbitrary but fixed. In this paper we will obtain explicit cost bounds for all d and ". <p> There are many papers where the power of hyperbolic cross information has been studied for a number of problems in different settings. The reader is referred to <ref> [7, 13, 14, 15, 16, 19, 20, 21, 22, 26, 30] </ref>. 4 Error Analysis 4.1 Worst Case We analyze the error of the algorithm (10) in terms of its error for d = 1.
Reference: [27] <author> G. Wahba, </author> <title> Spline Models for Observational Data, </title> <journal> CBMS-NSF Reg. Conf. Series Appl. Math., </journal> <volume> 59, </volume> <publisher> SIAM, </publisher> <address> Philadelphia, </address> <year> 1990. </year>
Reference-contexts: That is, the average case setting for the space F 1 with the measure 1 corresponds to the worst case setting for the unit ball of the reproducing kernel Hilbert space H 1 whose reproducing kernel is the covariance function of the measure 1 , see e.g., <ref> [27] </ref>.
Reference: [28] <author> G. W. Wasilkowski, </author> <title> Information of varying cardinality, </title> <journal> J. of Complexity, </journal> <volume> 2, </volume> <pages> p. 204-228, </pages> <year> 1986. </year>
Reference-contexts: Without loss of generality, we restrict ourselves to nonadaptive information and linear algorithms since for the settings considered in this paper, adaption and nonlinear algorithms are not essentially better, see <ref> [28, 23] </ref>.
Reference: [29] <author> H. Wozniakowski, </author> <title> Average Case Complexity of Multivariate Integration, </title> <journal> Bull. Amer. Math. Soc. (N.S), </journal> <volume> 24, </volume> <pages> p. 185-194, </pages> <year> 1991. </year>
Reference-contexts: the complexity comp ("; d) = K fl d (c (d) + 2) " fi fl 5 (ln 1=") fi fl 4 (d1) (1 + o (1)) as " ! 0 (3) with known fi fl 4 and fi fl 5 , but unknown K fl d , see e.g., <ref> [13, 15, 16, 20, 22, 29, 30] </ref>. The limited knowledge of the behavior of the complexity makes the question of optimality of the cost bound (2) even harder. A partial answer is provided by comparing the exponents fi 5 ; fi 4 with fi fl 4 . <p> Discrepancy is related to multivariate integration. Indeed, discrepancy is an upper bound on the worst case integration error of functions whose variation in the sense of Hardy and Krause is at most one, see [11]. It is also known, see <ref> [29] </ref>, that discrepancy (with optimally chosen weights) is equal to the minimal average case integration error of continuous functions defined over the d dimensional unit cube and equipped with the Wiener sheet measure. <p> subsection are also valid for the worst case with the tensor product problem generated by fH 1 ; IR; S 1 g. 3 This follows from the fact that periodicity does not change the dependence on ", and without periodicity the bound on the average case complexity is derived in <ref> [29] </ref>. 41 8.2 Integration of Nonperiodic Functions In this subsection we also consider integration in the average case setting. We define fF 1 ; G 1 ; S 1 g as in Subsection 8.1 with r = 0 but without assuming periodicity of functions. <p> 2, Theorem 1 yields cost (A " (d)) 3:304 (c (d) + 2) 1:12167 + ln 1=" ! 1:5 (d1) " We compare this bound with the average case complexity fi (" 1 (ln " 1 ) (d1)=2 ), where the factors in the fi notation depend on d, see <ref> [29] </ref>. As in Subsection 8.1, the exponent of 1=" in the cost estimate of the algorithm A (q; d)) agrees with the power of 1=" in the average case complexity, however, the power of ln 1=" is too large. <p> It was shown in <ref> [29] </ref> that the minimal discrepancy (with optimal weights) of n points in the d dimensional unit cube is equal to the minimal average case error of integration algorithms. This, of course, allows us to apply the bounds obtained in Subsection 8.2 to discrepancy.
Reference: [30] <author> H. Wozniakowski, </author> <title> Average case complexity of linear multivariate problems, Part 1: Theory, Part 2: </title> <journal> Applications, J. Complexity, </journal> <volume> 8, </volume> <pages> p. 337-372, 373-392, </pages> <year> 1992. </year>
Reference-contexts: The algorithms studied in this paper were presented by Smolyak [19] who studied tensor product problems in the worst case setting. Antecedents of these algorithms may be found 2 in [2]. Later, these algorithms have been used in many papers for specific problems; we only mention some recent papers <ref> [7, 13, 14, 15, 20, 21, 22, 26, 30] </ref>. In all these papers, only asymptotic cost (or error) bounds are provided, i.e., " is assumed to be sufficiently small whereas d is arbitrary but fixed. In this paper we will obtain explicit cost bounds for all d and ". <p> the complexity comp ("; d) = K fl d (c (d) + 2) " fi fl 5 (ln 1=") fi fl 4 (d1) (1 + o (1)) as " ! 0 (3) with known fi fl 4 and fi fl 5 , but unknown K fl d , see e.g., <ref> [13, 15, 16, 20, 22, 29, 30] </ref>. The limited knowledge of the behavior of the complexity makes the question of optimality of the cost bound (2) even harder. A partial answer is provided by comparing the exponents fi 5 ; fi 4 with fi fl 4 . <p> There are many papers where the power of hyperbolic cross information has been studied for a number of problems in different settings. The reader is referred to <ref> [7, 13, 14, 15, 16, 19, 20, 21, 22, 26, 30] </ref>. 4 Error Analysis 4.1 Worst Case We analyze the error of the algorithm (10) in terms of its error for d = 1. <p> 1 yields cost (A " (d)) 0:8489 (c (d) + 2) 0:9189 + ln 1=" ! 2 (d1) " : We compare this bound with the average case complexity fi (" 2 (ln " 1 ) 2 (d1) ), where the factors in the fi notation depend on d, see <ref> [30] </ref>. Hence, the exponents of 1=" as well as of ln 1=" in the cost estimate of the algorithm A " (d) agree with the corresponding exponents in the average case complexity.
Reference: [31] <author> H. Wozniakowski, </author> <title> Tractability and strong tractability of multivariate tensor product problems, </title> <booktitle> in Proceedings of 2nd Gauss Symposium, </booktitle> <address> Muenchen, </address> <year> 1993, </year> <note> to appear in J. Computing and Information. </note>
Reference-contexts: To alleviate this difficulty, and yet to get some insight concerning the optimality of (2) and the dependence on d, we follow the approach of tractability and strong tractability for linear multivariate problems introduced in <ref> [31, 32] </ref>. Tractability means that the complexity is bounded by c (d) K (d; ") where K (d; ") is a polynomial in d and 1=". Strong tractability means that K (d; ") is a polynomial in 1=" and does not depend on d. <p> This means that the dependence on d is only through the cost c (d). This holds even if only function values are used. Previously, such algorithms were known to exist but the proof was non-constructive, see <ref> [31, 32] </ref>. We stress that the cost bound (2) is usually much smaller than (5). To obtain a bound of the form (5) we must permit the worst relation between d and 1=". Such a relation between d and 1=" does not usually hold in computational practice. <p> In the worst case setting, strong tractability holds iff 1 = kS 1 k &lt; 1 and n = O (n k ) for some positive k. Here, 1 2 are eigenvalues of W 1 , see Theorem 3.1 of <ref> [31] </ref>. 32 In the average case setting, strong tractability holds iff kS 1 k 2 1 X fl n &lt; 1 and fl n = O (n 1k ) for some positive k. <p> by cost (A " (d)) (c (d) + 2) F ff 1 + ff 2 d 1 B 1=r where ff 1 = B 1=r e (F 1) 1 + r ln F ; B 1=r e (F 1): We now compare this estimate with the worst case complexity, see <ref> [31] </ref>, comp ("; d) = B d=r " ln " (1 + o (1)) ; as " ! 0: Hence, the exponents of 1=" and ln 1=" are the same. <p> This holds in the worst case and average case settings for both classes fl all and fl std . Obviously, it is enough to show that tractability implies strong tractability. We begin with the worst case setting. For the class fl all it is proven in <ref> [31] </ref>. For the class fl std we proceed as follows. Tractability in fl std implies tractability in fl all . The latter yields that B = kS 1 k &lt; 1, see (i) of Theorem 3.1 of [31]. <p> For the class fl all it is proven in <ref> [31] </ref>. For the class fl std we proceed as follows. Tractability in fl std implies tractability in fl all . The latter yields that B = kS 1 k &lt; 1, see (i) of Theorem 3.1 of [31]. Tractability in fl std means that for d = 1 there exists a one dimensional algorithm U n which uses n function values with error O (n p ) for some positive p.
Reference: [32] <author> H. Wozniakowski, </author> <title> Tractability and strong tractability of linear multivariate problems, </title> <note> to appear in J. Complexity, </note> <month> March, </month> <year> 1994. </year> <month> 54 </month>
Reference-contexts: To alleviate this difficulty, and yet to get some insight concerning the optimality of (2) and the dependence on d, we follow the approach of tractability and strong tractability for linear multivariate problems introduced in <ref> [31, 32] </ref>. Tractability means that the complexity is bounded by c (d) K (d; ") where K (d; ") is a polynomial in d and 1=". Strong tractability means that K (d; ") is a polynomial in 1=" and does not depend on d. <p> This means that the dependence on d is only through the cost c (d). This holds even if only function values are used. Previously, such algorithms were known to exist but the proof was non-constructive, see <ref> [31, 32] </ref>. We stress that the cost bound (2) is usually much smaller than (5). To obtain a bound of the form (5) we must permit the worst relation between d and 1=". Such a relation between d and 1=" does not usually hold in computational practice. <p> Still, as we shall see, for certain important problems the assumption B &lt; 1 holds. One may feel uneasy accepting the assumption (4) and prefer to normalize the tensor product problem by insisting that kS 1 k set = 1. Then some counter-intuitive things happen, see <ref> [32] </ref> and (iv) of the Appendix. For instance, this normalization criterion contradicts a natural property that the sum of two strongly tractable problems is strongly tractable. Furthermore, it is not necessarily clear why we should insist on kS 1 k set = 1. <p> We prefer not to choose sides in this selection of parameters. We add that if one normalizes the problem by taking kS 1 k = 1 then the problem becomes intractable and the complexity depends roughly on d fi 6 ln 1=" for some positive fi 6 , see <ref> [32] </ref>. Still, if " is not too small, this is acceptable even for large d. Obviously, we can always use the cost bound (2) which is reasonable for modest d. We illustrate the analysis of this paper by a number of applications. <p> This is discussed in the next section. 31 7 Strong Tractability In this section we utilize the concept of strong tractability, see <ref> [32] </ref>. A tensor product problem fF d ; G d ; S d g is strongly tractable iff the complexity of computing an "-approximation is bounded by c (d)K (") where K (") is a polynomial in 1=". <p> 2;d is a bound of the function f (t) = R d (t; t) in the L 1 (D d ) norm, where R d is the reproducing kernel of the space F d in the worst case setting, and the covariance kernel function in the average case setting, see <ref> [32] </ref>. For the class fl all , it is known which problems are strongly tractable. Obviously, if S 1 is a continuous linear functional then the problem is strongly tractable. <p> This contradicts tractability. Hence, B &lt; 1, as claimed. Tractability in fl all implies that for d = 1 we have fl j;1 = O (j 1k ) for some positive k, see Theorem 5.1 of <ref> [32] </ref>. We now take a one dimensional algorithm U n that uses n inner products such that its average case error is q P 1 j=n+1 fl j;1 = O (n k=2 ). Since B &lt; 1, the construction of Section 3 yields a strongly polynomial-time algorithm.
Reference: [33] <author> D. Ylvisaker, </author> <title> Designs on random fields, in A Survey of Statistical Design and Linear Models (J. Srivastava, </title> <editor> ed.), p. </editor> <address> 593-607, </address> <publisher> North-Holland, </publisher> <address> Amsterdam, </address> <year> 1975. </year> <title> Author's Addresses: Grzegorz W. </title> <type> Wasilkowski, </type> <institution> Department of Computer Science, University of Kentucky, Lexington, KY 40506, USA, email: greg@ms.uky.edu Henryk Wozniakowski, Department of Computer Science, Columbia University, </institution> <address> New York, NY 10027, USA, </address> <institution> and Institute of Applied Mathematics, University of Warsaw, </institution> <address> ul. Banacha 2, 02-097 Warszawa, Poland, email: henryk@cs.columbia.edu 55 </address>
Reference-contexts: Even if we use equally-spaced sample points for d = 1 then the position of sample points in d dimensions is very different from grid points. In fact, it is known that grid points are a very poor choice of sample points for tensor product problems, see e.g., <ref> [12, 33] </ref>. Information used by the algorithms is called hyperbolic cross information and have been successfully applied for a number of problems, see Section 3 for details and references. We summarize the result of Smolyak [19] more precisely. He studied the worst case setting only.
References-found: 33

