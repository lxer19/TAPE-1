URL: http://www.cs.umd.edu/~tseng/papers/crpc91121.ps
Refering-URL: http://www.cs.umd.edu/~tseng/papers.html
Root-URL: 
Title: An Overview of the Fortran D Programming System  
Author: Seema Hiranandani Ken Kennedy Charles Koelbel Ulrich Kremer Chau-Wen Tseng 
Note: Center for Research on Parallel Computation  In Fourth Workshop on Languages and Compilers  August 1991. Springer-Verlag.  
Date: March 1991  
Address: 91121  P.O. Box 1892 Houston, TX 77251-1892  Santa Clara, CA,  
Affiliation: CRPC-TR  Rice University  for Parallel Computing,  
Abstract-found: 0
Intro-found: 1
Reference: [1] <author> V. Balasundaram. </author> <title> Translating control parallelism to data parallelism. </title> <booktitle> In Proceedings of the Fifth SIAM Conference on Parallel Processing for Scientific Computing, </booktitle> <address> Houston, TX, </address> <month> March </month> <year> 1991. </year>
Reference-contexts: Computing the result on the processor owning the rhs and then sending the result to the owner of the lhs could reduce the amount of data communicated. This optimization is a simple case of the owner stores rule proposed by Balasundaram <ref> [1] </ref>. In particular, it may be desirable for the Fortran D compiler to partition loops amongst processors so that each loop iteration is executed on 5 a single processor, such as in Kali [22] and Parti [33].
Reference: [2] <author> V. Balasundaram, G. Fox, K. Kennedy, and U. Kremer. </author> <title> An interactive environment for data partitioning and distribution. </title> <booktitle> In Proceedings of the 5th Distributed Memory Computing Conference, </booktitle> <address> Charleston, SC, </address> <month> April </month> <year> 1990. </year>
Reference-contexts: The Fortran D compiler will use data dependence information to determine whether communication may be inserted at some outer loop, vec-torizing messages by combining many small messages. The algorithm to calculate the appropriate loop level for each message is described by Bala-sundaram et al. and Gerndt <ref> [2, 10] </ref>. A major goal of the Fortran D compiler is to aggressively optimize communications. We intend to apply techniques proposed by Li and Chen to recognize regular computation patterns that can utilize collective communications primitives [24]. It will be especially important to recognize reduction operations. <p> The Fortran D programming system, shown in Figure 3, provides such an environment. The main components of the environment are a static performance estimator and an automatic data partitioner <ref> [2, 3] </ref>. Since the Fortran D programming system is built on top of ParaScope, it also provides program analysis, transformation, and editing capabilities that 7 allow users to restructure their programs accord-ing to a data-parallel programming style.
Reference: [3] <author> V. Balasundaram, G. Fox, K. Kennedy, and U. Kremer. </author> <title> A static performance estimator to guide data partitioning decisions. </title> <booktitle> In Proceedings of the Third ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming, </booktitle> <address> Williamsburg, VA, </address> <month> April </month> <year> 1991. </year>
Reference-contexts: The Fortran D programming system, shown in Figure 3, provides such an environment. The main components of the environment are a static performance estimator and an automatic data partitioner <ref> [2, 3] </ref>. Since the Fortran D programming system is built on top of ParaScope, it also provides program analysis, transformation, and editing capabilities that 7 allow users to restructure their programs accord-ing to a data-parallel programming style. <p> It predicts the performance of a node program using Express communication routines for different numbers of processors and data sizes [27]. The prototype performance estimator has proved quite precise, especially in predicting the relative performances of different data decompositions <ref> [3] </ref>. A screen snapshot during a typical performance 8 estimation session is shown in Figure 4. The user can select a program segment such as a do loop and invoke the performance estimator by clicking on the Estimate Performance button.
Reference: [4] <author> D. Callahan, K. Cooper, R. Hood, K. Kennedy, and L. Torczon. </author> <title> ParaScope: A parallel programming environment. </title> <journal> The International Journal of Supercomputer Applications, </journal> <volume> 2(4) </volume> <pages> 84-99, </pages> <month> Winter </month> <year> 1988. </year>
Reference-contexts: A major advantage of programming in Fortran D will be the ability to utilize advanced compiler techniques to automatically generate node programs with explicit communication, based on the data decompositions specified in the program. The prototype compiler is being developed in the context of the ParaScope parallel programming environment <ref> [4] </ref>, and will take advantage of the analysis and 3 transformation capabilities of the ParaScope Editor [19, 20]. The main goal of the Fortran D compiler is to derive from the data decomposition a parallel node program that minimizes load imbalance and communication costs.
Reference: [5] <author> D. Callahan and K. Kennedy. </author> <title> Compiling programs for distributed-memory multiprocessors. </title> <journal> Journal of Supercomputing, </journal> <volume> 2 </volume> <pages> 151-169, </pages> <month> October </month> <year> 1988. </year>
Reference-contexts: Our approach is to convert Fortran D programs into single-program, multiple-data (SPMD) form with explicit message-passing that executes directly on the nodes of the distributed-memory machine. Our basic strategy is to partition the program using the owner computes rule, where every processor only performs computation on data it owns <ref> [5, 29, 34] </ref>. However, we will relax the rule where it prevents the compiler from achieving good load balance or reducing communication costs. The Fortran D compiler bears similarities to Arf [33], Aspar [18], Id Nouveau [29], Kali [22], Mimdizer [13], and Superb [34].
Reference: [6] <author> D. Callahan, K. Kennedy, and U. Kremer. </author> <title> A dynamic study of vectorization in PFC. </title> <type> Technical Report TR89-97, </type> <institution> Dept. of Computer Science, Rice University, </institution> <month> July </month> <year> 1989. </year>
Reference-contexts: Automatic vectorization and other compiler technologies have made it possible for the scientist to structure Fortran loops according the well-understood rules of "vectorizable style" and expect the resulting program to be compiled to efficient code on any vector machine <ref> [6, 32] </ref>. Compare this with the current situation for parallel machines.
Reference: [7] <author> B. Chapman, H. Herbeck, and H. Zima. </author> <title> Automatic support for data distribution. </title> <booktitle> In Proceedings of the 6th Distributed Memory Computing Conference, </booktitle> <address> Portland, OR, </address> <month> April </month> <year> 1991. </year>
Reference-contexts: Zima and others at Vienna are working on a similar tool to support data decomposition decisions using automatic techniques <ref> [7] </ref>. Gupta and Banerjee propose automatic data decomposition techniques based on assumptions about a proposed Parafrase-2 distributed-memory compiler [11]. 4.1 Static Performance Estimator It is clearly impractical to use dynamic performance information to choose between data decompositions in our programming environment.
Reference: [8] <author> G. Fox, S. Hiranandani, K. Kennedy, C. Koelbel, U. Kremer, C. Tseng, and M. Wu. </author> <title> Fortran D language specification. </title> <type> Technical Report TR90-141, </type> <institution> Dept. of Computer Science, Rice University, </institution> <month> De-cember </month> <year> 1990. </year>
Reference-contexts: In addition, we believe that our two-phase strategy for specifying data decomposition is natural and conducive to writing modular and portable code. Fortran D bears similarities to both CM Fortran [31] and Kali [22]. The complete language is described in detail elsewhere <ref> [8] </ref>. 3 Fortran D Compiler As we have stated previously, two major steps in writing a data-parallel program are selecting a data decomposition, and then using it to derive node programs with explicit communications to access nonlocal data.
Reference: [9] <author> G. Fox, M. Johnson, G. Lyzenga, S. Otto, J. Salmon, and D. Walker. </author> <title> Solving Problems on Concurrent Processors, volume 1. </title> <publisher> Prentice-Hall, </publisher> <address> Englewood Cliffs, NJ, </address> <year> 1988. </year>
Reference-contexts: Instead, it only needs to accurately predict the performance relative to other data decompositions. A prototype of the machine module has been implemented for a common class of loosely synchronous scientific problems <ref> [9] </ref>. It predicts the performance of a node program using Express communication routines for different numbers of processors and data sizes [27]. The prototype performance estimator has proved quite precise, especially in predicting the relative performances of different data decompositions [3].
Reference: [10] <author> M. Gerndt. </author> <title> Updating distributed variables in local computations. </title> <journal> Concurrency|Practice & Experi 11 ence, </journal> <volume> 2(3) </volume> <pages> 171-193, </pages> <month> September </month> <year> 1990. </year>
Reference-contexts: The Fortran D compiler will use data dependence information to determine whether communication may be inserted at some outer loop, vec-torizing messages by combining many small messages. The algorithm to calculate the appropriate loop level for each message is described by Bala-sundaram et al. and Gerndt <ref> [2, 10] </ref>. A major goal of the Fortran D compiler is to aggressively optimize communications. We intend to apply techniques proposed by Li and Chen to recognize regular computation patterns that can utilize collective communications primitives [24]. It will be especially important to recognize reduction operations. <p> There are several different storage schemes, described below: * Overlaps, developed by Gerndt, are expansions of local array sections to accommodate neighboring nonlocal elements <ref> [10] </ref>. They are useful for programs with high locality of reference, but may waste storage when nonlocal accesses are distant. * Buffers are designed to overcome the contiguous nature of overlaps.
Reference: [11] <author> M. Gupta and P. Banerjee. </author> <title> Automatic data partitioning on distributed memory multiprocessors. </title> <booktitle> In Proceedings of the 6th Distributed Memory Computing Conference, </booktitle> <address> Portland, OR, </address> <month> April </month> <year> 1991. </year>
Reference-contexts: Zima and others at Vienna are working on a similar tool to support data decomposition decisions using automatic techniques [7]. Gupta and Banerjee propose automatic data decomposition techniques based on assumptions about a proposed Parafrase-2 distributed-memory compiler <ref> [11] </ref>. 4.1 Static Performance Estimator It is clearly impractical to use dynamic performance information to choose between data decompositions in our programming environment. Instead, a static performance estimator is needed that can accurately predict the performance of a Fortran D program on the target machine.
Reference: [12] <author> P. Havlak and K. Kennedy. </author> <title> An implementation of interprocedural bounded regular section analysis. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 2(3) </volume> <pages> 350-360, </pages> <month> July </month> <year> 1991. </year>
Reference-contexts: These sets represent the iterations for which data must be sent or received by t p . The Fortran D compiler summarizes the array locations accessed on the send or receive iterations using rectangular or triangular regions known as regular sections <ref> [12] </ref>; they are used to generate calls to communication primitives. 3.3 Communication Optimization A naive approach for introducing communication is to insert send and receive operations directly preceding each reference causing a nonlocal data access. This generates many small messages that may prove inefficient due to communication overhead.
Reference: [13] <author> R. Hill. MIMDizer: </author> <title> A new tool for parallelization. </title> <journal> Supercomputing Review, </journal> <volume> 3(4) </volume> <pages> 26-28, </pages> <month> April </month> <year> 1990. </year>
Reference-contexts: However, we will relax the rule where it prevents the compiler from achieving good load balance or reducing communication costs. The Fortran D compiler bears similarities to Arf [33], Aspar [18], Id Nouveau [29], Kali [22], Mimdizer <ref> [13] </ref>, and Superb [34]. The current prototype generates code for a subset of the decompositions allowed in Fortran D, namely those with BLOCK distributions.
Reference: [14] <author> S. Hiranandani, K. Kennedy, and C. Tseng. </author> <title> Compiler optimizations for Fortran D on MIMD distributed-memory machines. </title> <booktitle> In Proceedings of Supercomputing '91, </booktitle> <address> Albuquerque, NM, </address> <month> November </month> <year> 1991. </year>
Reference-contexts: Storage management and other parts of the Fortran D compiler are described in more detail elsewhere <ref> [14, 15] </ref>. 4 Fortran D Programming Environment Choosing a decomposition for the fundamental data structures used in the program is a pivotal step in developing data-parallel applications. Once selected, the data decomposition usually completely determines the parallelism and data movement in the resulting program.
Reference: [15] <author> S. Hiranandani, K. Kennedy, and C. Tseng. </author> <title> Compiler support for machine-independent parallel programming in Fortran D. </title> <type> Technical Report TR90-149, </type> <institution> Dept. of Computer Science, Rice University, </institution> <month> January </month> <year> 1991. </year> <note> To appear in J. </note> <editor> Saltz and P. Mehro-tra, editors, </editor> <title> Compilers and Runtime Software for Scalable Multiprocessors, </title> <publisher> Elsevier, </publisher> <year> 1991. </year>
Reference-contexts: Storage management and other parts of the Fortran D compiler are described in more detail elsewhere <ref> [14, 15] </ref>. 4 Fortran D Programming Environment Choosing a decomposition for the fundamental data structures used in the program is a pivotal step in developing data-parallel applications. Once selected, the data decomposition usually completely determines the parallelism and data movement in the resulting program.
Reference: [16] <author> S. Hiranandani, J. Saltz, P. Mehrotra, and H. Berryman. </author> <title> Performance of hashed cache data migration schemes on multicomputers. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 12(4), </volume> <month> August </month> <year> 1991. </year>
Reference-contexts: This is the case in many irregular computations. Hash tables provide a quick lookup mechanism for arbi trary sets of nonlocal values <ref> [16] </ref>. Once the storage type for all nonlocal data is determined, the compiler needs to analyze the space required by the various storage structures and generate code so that nonlocal data is accessed from its correct location.
Reference: [17] <author> D. Hudak and S. Abraham. </author> <title> Compiler techniques for data partitioning of sequentially iterated parallel loops. </title> <booktitle> In Proceedings of the 1990 ACM International Conference on Supercomputing, </booktitle> <address> Amsterdam, The Netherlands, </address> <month> June </month> <year> 1990. </year>
Reference-contexts: The process is prohibitively difficult without the assistance of a compiler to automatically generate node programs based on the data decomposition. Several researchers have proposed techniques to automatically derive data decompositions based on simple machine models <ref> [17, 28, 30] </ref>.
Reference: [18] <author> K. Ikudome, G. Fox, A. Kolawa, and J. Flower. </author> <title> An automatic and symbolic parallelization system for distributed memory parallel computers. </title> <booktitle> In Proceedings of the 5th Distributed Memory Computing Conference, </booktitle> <address> Charleston, SC, </address> <month> April </month> <year> 1990. </year>
Reference-contexts: However, we will relax the rule where it prevents the compiler from achieving good load balance or reducing communication costs. The Fortran D compiler bears similarities to Arf [33], Aspar <ref> [18] </ref>, Id Nouveau [29], Kali [22], Mimdizer [13], and Superb [34]. The current prototype generates code for a subset of the decompositions allowed in Fortran D, namely those with BLOCK distributions.
Reference: [19] <author> K. Kennedy, K. S. M c Kinley, and C. Tseng. </author> <title> Analysis and transformation in the ParaScope Editor. </title> <booktitle> In Proceedings of the 1991 ACM International Conference on Supercomputing, </booktitle> <address> Cologne, Germany, </address> <month> June </month> <year> 1991. </year>
Reference-contexts: The prototype compiler is being developed in the context of the ParaScope parallel programming environment [4], and will take advantage of the analysis and 3 transformation capabilities of the ParaScope Editor <ref> [19, 20] </ref>. The main goal of the Fortran D compiler is to derive from the data decomposition a parallel node program that minimizes load imbalance and communication costs.
Reference: [20] <author> K. Kennedy, K. S. M c Kinley, and C. Tseng. </author> <title> Interactive parallel programming using the ParaScope Editor. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 2(3) </volume> <pages> 329-341, </pages> <month> July </month> <year> 1991. </year>
Reference-contexts: The prototype compiler is being developed in the context of the ParaScope parallel programming environment [4], and will take advantage of the analysis and 3 transformation capabilities of the ParaScope Editor <ref> [19, 20] </ref>. The main goal of the Fortran D compiler is to derive from the data decomposition a parallel node program that minimizes load imbalance and communication costs.
Reference: [21] <author> K. Knobe, J. Lukas, and G. Steele, Jr. </author> <title> Data optimization: Allocation of arrays to reduce communication on SIMD machines. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 8(2) </volume> <pages> 102-118, </pages> <month> Febru-ary </month> <year> 1990. </year>
Reference-contexts: Alignment analysis is largely machine-independent; it is performed by analyzing the array access patterns of computations in the phase. We intend to build on the inter-dimensional and intra-dimensional alignment techniques of Li and Chen [23] and Knobe et al. <ref> [21] </ref>. Distribution analysis follows alignment analysis. It applies heuristics to prune unprofitable choices in the search space of possible distributions. The efficiency of a data distribution is determined by machine-dependent aspects such as topology, number of processors, and communication costs.
Reference: [22] <author> C. Koelbel and P. Mehrotra. </author> <title> Compiling global name-space parallel loops for distributed execution. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 2(4), </volume> <month> October </month> <year> 1991. </year>
Reference-contexts: As a result, it should be quite usable by computational scientists. In addition, we believe that our two-phase strategy for specifying data decomposition is natural and conducive to writing modular and portable code. Fortran D bears similarities to both CM Fortran [31] and Kali <ref> [22] </ref>. The complete language is described in detail elsewhere [8]. 3 Fortran D Compiler As we have stated previously, two major steps in writing a data-parallel program are selecting a data decomposition, and then using it to derive node programs with explicit communications to access nonlocal data. <p> However, we will relax the rule where it prevents the compiler from achieving good load balance or reducing communication costs. The Fortran D compiler bears similarities to Arf [33], Aspar [18], Id Nouveau [29], Kali <ref> [22] </ref>, Mimdizer [13], and Superb [34]. The current prototype generates code for a subset of the decompositions allowed in Fortran D, namely those with BLOCK distributions. <p> This optimization is a simple case of the owner stores rule proposed by Balasundaram [1]. In particular, it may be desirable for the Fortran D compiler to partition loops amongst processors so that each loop iteration is executed on 5 a single processor, such as in Kali <ref> [22] </ref> and Parti [33]. This technique may improve communication and provide greater control over load balance, especially for irregular computations. <p> Other algorithms, such as fast multipole algorithms, make heavy use of index arrays that the compiler cannot analyze. In these cases, the communications analysis must be performed at run-time. The Fortran D project supports dynamic irregular distributions. The inspector/executor strategy to generate efficient communications has been adapted from Kali <ref> [22] </ref> and Parti [25]. The inspector is a transformation of the original Fortran D loop that builds a list of nonlocal elements, known as the In set, that will be received during the execution of the loop.
Reference: [23] <author> J. Li and M. Chen. </author> <title> Index domain alignment: Minimizing cost of cross-referencing between distributed arrays. </title> <booktitle> In Frontiers90: The 3rd Symposium on the Frontiers of Massively Parallel Computation, </booktitle> <address> College Park, MD, </address> <month> October </month> <year> 1990. </year>
Reference-contexts: Alignment analysis is largely machine-independent; it is performed by analyzing the array access patterns of computations in the phase. We intend to build on the inter-dimensional and intra-dimensional alignment techniques of Li and Chen <ref> [23] </ref> and Knobe et al. [21]. Distribution analysis follows alignment analysis. It applies heuristics to prune unprofitable choices in the search space of possible distributions. The efficiency of a data distribution is determined by machine-dependent aspects such as topology, number of processors, and communication costs.
Reference: [24] <author> J. Li and M. Chen. </author> <title> Compiling communication-efficient programs for massively parallel machines. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 2(3) </volume> <pages> 361-376, </pages> <month> July </month> <year> 1991. </year>
Reference-contexts: A major goal of the Fortran D compiler is to aggressively optimize communications. We intend to apply techniques proposed by Li and Chen to recognize regular computation patterns that can utilize collective communications primitives <ref> [24] </ref>. It will be especially important to recognize reduction operations. For regular communication patterns, we plan to employ the collective communications routines found in Express [27]. For unstructured computations with irregular communications, we will incorporate the Parti primitives of Saltz et al. [33].
Reference: [25] <author> R. Mirchandaney, J. Saltz, R. Smith, D. Nicol, and K. Crowley. </author> <title> Principles of runtime support for parallel processors. </title> <booktitle> In Proceedings of the Second International Conference on Supercomputing, </booktitle> <address> St. Malo, France, </address> <month> July </month> <year> 1988. </year>
Reference-contexts: In these cases, the communications analysis must be performed at run-time. The Fortran D project supports dynamic irregular distributions. The inspector/executor strategy to generate efficient communications has been adapted from Kali [22] and Parti <ref> [25] </ref>. The inspector is a transformation of the original Fortran D loop that builds a list of nonlocal elements, known as the In set, that will be received during the execution of the loop.
Reference: [26] <author> C. Pancake and D. Bergmark. </author> <title> Do parallel languages respond to the needs of scientific programmers? IEEE Computer, </title> <booktitle> 23(12) </booktitle> <pages> 13-23, </pages> <month> December </month> <year> 1990. </year>
Reference-contexts: For these reasons, we have chosen a different approach. We believe that selecting a data decomposition is one of the most important intellectual step in developing data-parallel scientific codes. However, current parallel programming languages provide little support for data decomposition <ref> [26] </ref>. We have therefore developed an enhanced version of Fortran that introduces data decomposition specifications. We call the extended language Fortran D, where "D" suggests data, decomposition, or distribution.
Reference: [27] <institution> Parasoft Corporation. </institution> <note> Express User's Manual, </note> <year> 1989. </year>
Reference-contexts: We intend to apply techniques proposed by Li and Chen to recognize regular computation patterns that can utilize collective communications primitives [24]. It will be especially important to recognize reduction operations. For regular communication patterns, we plan to employ the collective communications routines found in Express <ref> [27] </ref>. For unstructured computations with irregular communications, we will incorporate the Parti primitives of Saltz et al. [33]. The Fortran D compiler may utilize data decomposition and dependence information to guide program transformations that improve communication patterns. <p> A prototype of the machine module has been implemented for a common class of loosely synchronous scientific problems [9]. It predicts the performance of a node program using Express communication routines for different numbers of processors and data sizes <ref> [27] </ref>. The prototype performance estimator has proved quite precise, especially in predicting the relative performances of different data decompositions [3]. A screen snapshot during a typical performance 8 estimation session is shown in Figure 4.
Reference: [28] <author> J. Ramanujam and P. Sadayappan. </author> <title> A methodology for parallelizing programs for multicomputers and complex memory multiprocessors. </title> <booktitle> In Proceedings of Supercomputing '89, </booktitle> <address> Reno, NV, </address> <month> November </month> <year> 1989. </year>
Reference-contexts: The process is prohibitively difficult without the assistance of a compiler to automatically generate node programs based on the data decomposition. Several researchers have proposed techniques to automatically derive data decompositions based on simple machine models <ref> [17, 28, 30] </ref>.
Reference: [29] <author> A. Rogers and K. Pingali. </author> <title> Process decomposition through locality of reference. </title> <booktitle> In Proceedings of the SIGPLAN '89 Conference on Program Language Design and Implementation, </booktitle> <address> Portland, OR, </address> <month> June </month> <year> 1989. </year>
Reference-contexts: Our approach is to convert Fortran D programs into single-program, multiple-data (SPMD) form with explicit message-passing that executes directly on the nodes of the distributed-memory machine. Our basic strategy is to partition the program using the owner computes rule, where every processor only performs computation on data it owns <ref> [5, 29, 34] </ref>. However, we will relax the rule where it prevents the compiler from achieving good load balance or reducing communication costs. The Fortran D compiler bears similarities to Arf [33], Aspar [18], Id Nouveau [29], Kali [22], Mimdizer [13], and Superb [34]. <p> However, we will relax the rule where it prevents the compiler from achieving good load balance or reducing communication costs. The Fortran D compiler bears similarities to Arf [33], Aspar [18], Id Nouveau <ref> [29] </ref>, Kali [22], Mimdizer [13], and Superb [34]. The current prototype generates code for a subset of the decompositions allowed in Fortran D, namely those with BLOCK distributions.
Reference: [30] <author> L. Snyder and D. Socha. </author> <title> An algorithm producing balanced partitionings of data arrays. </title> <booktitle> In Proceedings of the 5th Distributed Memory Computing Conference, </booktitle> <address> Charleston, SC, </address> <month> April </month> <year> 1990. </year>
Reference-contexts: The process is prohibitively difficult without the assistance of a compiler to automatically generate node programs based on the data decomposition. Several researchers have proposed techniques to automatically derive data decompositions based on simple machine models <ref> [17, 28, 30] </ref>.
Reference: [31] <institution> Thinking Machines Corporation, </institution> <address> Cambridge, MA. </address> <note> CM Fortran Reference Manual, version 5.2-0.6 edition, </note> <month> September </month> <year> 1989. </year>
Reference-contexts: As a result, it should be quite usable by computational scientists. In addition, we believe that our two-phase strategy for specifying data decomposition is natural and conducive to writing modular and portable code. Fortran D bears similarities to both CM Fortran <ref> [31] </ref> and Kali [22]. The complete language is described in detail elsewhere [8]. 3 Fortran D Compiler As we have stated previously, two major steps in writing a data-parallel program are selecting a data decomposition, and then using it to derive node programs with explicit communications to access nonlocal data.
Reference: [32] <author> M. J. Wolfe. </author> <title> Semi-automatic domain decomposition. </title> <booktitle> In Proceedings of the 4th Conference on Hypercube Concurrent Computers and Applications, </booktitle> <address> Monterey, CA, </address> <month> March </month> <year> 1989. </year>
Reference-contexts: Automatic vectorization and other compiler technologies have made it possible for the scientist to structure Fortran loops according the well-understood rules of "vectorizable style" and expect the resulting program to be compiled to efficient code on any vector machine <ref> [6, 32] </ref>. Compare this with the current situation for parallel machines.
Reference: [33] <author> J. Wu, J. Saltz, S. Hiranandani, and H. Berryman. </author> <title> Runtime compilation methods for multicomputers. </title> <booktitle> In Proceedings of the 1991 International Conference on Parallel Processing, </booktitle> <address> St. Charles, IL, </address> <month> August </month> <year> 1991. </year>
Reference-contexts: However, we will relax the rule where it prevents the compiler from achieving good load balance or reducing communication costs. The Fortran D compiler bears similarities to Arf <ref> [33] </ref>, Aspar [18], Id Nouveau [29], Kali [22], Mimdizer [13], and Superb [34]. The current prototype generates code for a subset of the decompositions allowed in Fortran D, namely those with BLOCK distributions. <p> It will be especially important to recognize reduction operations. For regular communication patterns, we plan to employ the collective communications routines found in Express [27]. For unstructured computations with irregular communications, we will incorporate the Parti primitives of Saltz et al. <ref> [33] </ref>. The Fortran D compiler may utilize data decomposition and dependence information to guide program transformations that improve communication patterns. We are considering the usefulness of several transformations, particularly loop interchanging, strip mining, loop distribution, and loop alignment. <p> In particular, it may be desirable for the Fortran D compiler to partition loops amongst processors so that each loop iteration is executed on 5 a single processor, such as in Kali [22] and Parti <ref> [33] </ref>. This technique may improve communication and provide greater control over load balance, especially for irregular computations. <p> The executor uses the computed sets to control the actual communication. Performance results using the Parti primitives indicate that the inspector can be implemented with acceptable overhead, particularly if the results are saved for future executions of the original loop <ref> [33] </ref>. 3.6 Storage Management Once guards and communication have been calculated, the Fortran D compiler must select and manage storage for all nonlocal array references received from other processors.
Reference: [34] <author> H. Zima, H.-J. Bast, and M. Gerndt. </author> <title> SUPERB: A tool for semi-automatic MIMD/SIMD paralleliza-tion. </title> <journal> Parallel Computing, </journal> <volume> 6 </volume> <pages> 1-18, </pages> <year> 1988. </year> <month> 12 </month>
Reference-contexts: Our approach is to convert Fortran D programs into single-program, multiple-data (SPMD) form with explicit message-passing that executes directly on the nodes of the distributed-memory machine. Our basic strategy is to partition the program using the owner computes rule, where every processor only performs computation on data it owns <ref> [5, 29, 34] </ref>. However, we will relax the rule where it prevents the compiler from achieving good load balance or reducing communication costs. The Fortran D compiler bears similarities to Arf [33], Aspar [18], Id Nouveau [29], Kali [22], Mimdizer [13], and Superb [34]. <p> However, we will relax the rule where it prevents the compiler from achieving good load balance or reducing communication costs. The Fortran D compiler bears similarities to Arf [33], Aspar [18], Id Nouveau [29], Kali [22], Mimdizer [13], and Superb <ref> [34] </ref>. The current prototype generates code for a subset of the decompositions allowed in Fortran D, namely those with BLOCK distributions.
References-found: 34

