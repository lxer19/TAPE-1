URL: http://arch.cs.ucdavis.edu/~chong/250C/multigrain/CSL-TR-94-632.ps
Refering-URL: http://arch.cs.ucdavis.edu/~chong/250C/multigrain/
Root-URL: http://www.cs.ucdavis.edu
Title: The Benefits of Clustering in Shared Address Space Multiprocessors: An Applications-Driven Investigation  
Author: Andrew Erlichson, Basem A. Nayfeh, Jaswinder P. Singh, and Kunle Olukotun 
Keyword: Key Words and Phrases: Clustering, Applications, Shared Memory.  
Address: Stanford, CA 94305  
Affiliation: Computer Systems Lab Stanford University  
Pubnum: Technical Report: CSL-TR-94-632  
Abstract: Clustering processors together at a level of the memory hierarchy in shared address space multiprocessors appears to be an attractive technique from several standpoints: Resources are shared, packaging technologies are exploited, and processors within a cluster can share data more effectively. We investigate the performance benefits that can be obtained by clustering on a range of important scientific and engineering applications. We find that in general clustering is not very effective in reducing the inherent communication to computation ratios. Clustering is more useful in reducing working set requirements in unstructured applications, and can improve performance substantially when small first level caches are clustered in these cases. This suggests that clustering at the first level cache might be useful in highly-integrated, relatively fine-grained environments. For less integrated machines such as current distributed shared memory multiprocessors, our results suggest that clustering is not very useful in improving application performance, and the decision about whether or not to cluster should be made on the basis of engineering and packaging constraints. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> David R. Cheriton, Hendrik A. Goosen and Patrick D. Boyle. </author> <title> ParaDiGM: A Highly Scalable Shared-Memory Multi-Computer Architecture. </title> <booktitle> IEEE Computer, </booktitle> <month> February </month> <year> 1991. </year>
Reference: [2] <author> Jim Christy and David Wilkins. </author> <title> Parallel Ray Tracing Without Database Duplication. </title> <type> CS315B Project Report, </type> <institution> Stanford University, </institution> <month> June </month> <year> 1992. </year>
Reference: [3] <author> Helen Davis, Stephen Goldschmidt and John L. Hennessy. </author> <title> Multiprocessor Simulation and Tracing using Tango. </title> <booktitle> Proc. Intl. Conf. on Parallel Processing, </booktitle> <month> August </month> <year> 1991. </year>
Reference: [4] <author> Peter J. Denning. </author> <title> The working set model of program behavior. </title> <journal> Communications of the ACM, </journal> <volume> vol. 11, no. 5, </volume> <month> May </month> <year> 1968, </year> <pages> pp. 323-333 </pages>
Reference: [5] <author> S. Goldschmidt, </author> <title> "Simulation of Multiprocessors: Accuracy and Performance," </title> <type> Ph.D. Thesis, </type> <institution> Stanford University, </institution> <year> 1993. </year>
Reference-contexts: For this reason we do not want to include the effect of conflict misses that are due to limited associativity. The event-driven simulator used to generate our performance results is based on Tango-lite <ref> [5] </ref>. This simulator produces application execution times by simulating with single cycle cache hits. We use an estimation procedure to account for the increase in hit times from the shared cache.
Reference: [6] <author> J. Hennessy and D. Patterson. </author> <title> Computer Architecture: A Quantitative Approach. </title> <publisher> Morgan Kaufmann Publishers, </publisher> <address> San Mateo, CA, </address> <year> 1990. </year>
Reference: [7] <author> Daniel E. Lenoski et al. </author> <title> The directory-based cache coherence protocol for the DASH multiprocessor. </title> <booktitle> In Proc. 17th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 148-159, </pages> <year> 1990. </year>
Reference: [8] <author> Jason Nieh and Marc Levoy. </author> <title> Volume Rendering on Scalable Shared Memory MIMD Architectures. </title> <booktitle> Proc. Boston Workshop on Volume Visualization, </booktitle> <month> October </month> <year> 1992. </year>
Reference: [9] <author> Basem Nayfeh and Kunle Olukotun. </author> <title> Exploring the Design Space for a Shared-Cache Multiprocessor. </title> <booktitle> In Proc. 21st Annual International Symposium on Computer Architecture, </booktitle> <address> 1994.. </address>
Reference-contexts: In a shared cache system, providing the cache bandwidth to support multiple cache accesses per cycle requires a multiple-port, non-blocking, multi-banked cache [15]. Such caches have higher hit times than single ported, single bank caches <ref> [9] </ref>. Furthermore, when bank conflicts occur there are further increases in the hit time. Longer primary cache hit times will degrade performance by increasing the CPU cycle 3 time or increasing the latency of loads in CPU cycles. <p> The primary cache hit latency varies between one and four clock cycles to account for the increases in the hit time of the shared cache as the number of processors per cluster increases. These latency values are based on previous studies of shared primary cache architectures <ref> [9] </ref>. The latencies of local and remote misses are consistent with the relative processor speeds, DRAM speeds and network speeds of modern systems. In Table 1, home refers to the cluster at which the memory associated with the reference is allocated. <p> This simulator produces application execution times by simulating with single cycle cache hits. We use an estimation procedure to account for the increase in hit times from the shared cache. To apply this procedure, we assume that the shared cache has four banks for each processor in the cluster <ref> [9] </ref>. For example, a four processor cluster has shared cache that is 16-way interleaved. We further assume that the processor emits a memory reference to a bank picked at random every cycle and that the processor stalls for a cycle if there is contention for that bank with another processor.
Reference: [10] <author> Edward Rothberg, Jaswinder Pal Singh and Anoop Gupta. </author> <title> Working Sets, Cache Sizes, and Node Granularity for Large-Scale Multiprocessors. </title> <booktitle> In Proc. 20th Annual International Symposium on Computer Architecture, </booktitle> <year> 1993. </year>
Reference-contexts: For small to moderate numbers of processors, the communication to computation ratios of the structured applications is typically very low, so the miss rates in infinite caches are low as well <ref> [10] </ref>. The results for LU show that the eight processor cluster has over 98% of the execution time of the single processor cluster. Communication occurs in blocked LU decomposition when processors access the diagonal or perimeter blocks. <p> For a cache size close to the uniprocessor working set, clustering the caches causes a steep drop in execution time at the point where the overlapped working set suddenly fits in the cache. This is because scientific and engineering applications often have sharply defined working sets <ref> [10] </ref>. <p> All the unstructured applications (other than Raytrace) have relatively small working sets that grow slowly with problem size (see Table 2 and <ref> [10] </ref>). For example, we can see from the figures that the working set of FMM is at something close to the 4KB cache size, for Barnes is somewhat less than 16K per processor, Volrend is near the 16KB range, and for Raytrace is larger.
Reference: [11] <author> J.P. Singh, W.-D. Weber, and Anoop Gupta. </author> <title> SPLASH: Stanford Parallel Applications for Shared-Memory. </title> <journal> Computer Architecture News, </journal> <volume> 20(1) </volume> <pages> 5-44, </pages> <month> March </month> <year> 1992. </year>
Reference: [12] <author> Jaswinder Pal Singh, John L. Hennessy, and Anoop Gupta. </author> <title> Scaling parallel programs for multiprocessors: methodology and examples. </title> <booktitle> IEEE Computer, </booktitle> <month> July </month> <year> 1993. </year>
Reference: [13] <author> Jaswinder Pal Singh et al. </author> <title> Load balancing and data locality in parallel hierarchial N-body simulation. </title> <type> Technical Report CSL-TR-92-505, </type> <institution> Stanford University, </institution> <month> February </month> <year> 1992. </year> <note> To appear in Journal of Parallel and Distributed Computing. </note>
Reference: [14] <author> M. D. Smith, </author> <title> "Tracing with Pixie," </title> <type> Technical CSL-TR-91-497, </type> <institution> Stanford University, Computer Systems Laboratory, </institution> <month> November </month> <year> 1991. </year>
Reference-contexts: The basic block profiling tool Pixie can be used to find the relative increase in execution time of increasing the load latency from 1 to 2 cycles, 1 to 3 cycles, and 1 to 4 cycles <ref> [14] </ref>.
Reference: [15] <author> Sohi and M. Franklin, </author> <title> High Bandwidth Data Memory for Superscalar Processors, </title> <booktitle> in Proc. 4th Int. Conf. Architectural Support for Programming Languages and Operating Systems (ASPLOS-IV), </booktitle> <pages> 53-62, </pages> <year> 1991. </year>
Reference-contexts: In a shared cache system, providing the cache bandwidth to support multiple cache accesses per cycle requires a multiple-port, non-blocking, multi-banked cache <ref> [15] </ref>. Such caches have higher hit times than single ported, single bank caches [9]. Furthermore, when bank conflicts occur there are further increases in the hit time.
Reference: [16] <author> Susan Spach and Ronald Pulleyblank. </author> <title> Parallel Raytraced Image Generation. </title> <journal> Hewlett-Packard Journal, </journal> <volume> vol. 43, no. 3, </volume> <pages> pages 76-83, </pages> <month> June </month> <year> 1992 </year>
Reference: [17] <author> Per Stenstrom, Truman Joe, and Anoop Gupta, </author> <title> Comparative Performance Evaluation of Cache-Coherent NUMA and COMA Architectures. </title> <booktitle> In Proc. 19th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 80-91, </pages> <month> May </month> <year> 1992. </year>
Reference-contexts: A shared main memory cluster has individual processor caches connected by a snoopy bus with the backing shared main memory. The main memory is assumed to be an effectively infinite attraction memory of a flat COMA style machine <ref> [17] </ref>. We consider the benefits of clustering in these two types of systems before narrowing our investigations to non-COMA shared cache clusters with distributed directories.
Reference: [18] <author> Andrew A. W. Wilson, Jr. </author> <title> Hierarchical Cache/Bus Architecture for Shared Memory Multiprocessors. </title> <booktitle> In Proc. 14th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 244-252, </pages> <year> 1987 </year>
References-found: 18

