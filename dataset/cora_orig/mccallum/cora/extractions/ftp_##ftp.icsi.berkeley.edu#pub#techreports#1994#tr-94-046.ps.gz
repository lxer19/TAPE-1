URL: ftp://ftp.icsi.berkeley.edu/pub/techreports/1994/tr-94-046.ps.gz
Refering-URL: http://www.icsi.berkeley.edu/techreports/1994.html
Root-URL: http://www.icsi.berkeley.edu
Title: Object Oriented Design of a BP Neural Network Simulator and Implementation on the Connection Machine (CM-5)  
Phone: (510) 643-9153 FAX (510) 643-7684  
Author: J.M.Adamo D.Anguita 
Date: September 1994  
Address: I 1947 Center St. Suite 600 Berkeley, California 94704-1198  Genova, Italy  
Affiliation: INTERNATIONAL COMPUTER SCIENCE INSTITUTE  International Computer Science Institute, Berkeley, USA Universite Claude Bernard, Lyon, France Department of Biophysical and Electronic Engineering, University of  
Pubnum: TR-94-46  
Abstract: In this paper we describe the implementation of the backpropagation algorithm by means of an object oriented library (ARCH). The use of this library relieve the user from the details of a specific parallel programming paradigm and at the same time allows a greater portability of the generated code. To provide a comparision with existing solutions, we survey the most relevant implementations of the algorithm proposed so far in the literature, both on dedicated and general purpose computers. Extensive experimental results show that the use of the library does not hurt the performance of our simulator, on the contrary our implementation on a Connection Machine (CM-5) is comparable with the fastest in its category. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <institution> SNAP - SIMD Numerical Array Processor. </institution> <address> HNC, 5930 Cornerstone Court West, S.Diego, CA, </address> <year> 1994. </year>
Reference-contexts: 12 P N [13] Sony GCN-860 (128) 1000 1 256 fi 80 fi 32; 5120 - Y [12] Sandy/8 (256) 118=567 1 NETtalk/peak P Y [16] TMC CM-2 (64k) 350 128 fi 128 fi 128; 65536 E Y [29] HNC SNAP (16/64) 80:4=302 512 fi 512 fi 512 - Y <ref> [1] </ref> MUSIC (60) 247 - P Y [21, 22] MANTRA I (1600) 133 1 - P N [30] RAP (40) 102 640 fi 640 fi 640 P Y [20] SPERT 100 1 512 fi 512 fi 512 P N [32, 6] TMC CM-5 (512) 76 256 fi 256 fi 131072; 111
Reference: [2] <author> J.M.Adamo. </author> <title> Object-Oriented Parallel Programming: Design and Development of an Object-Oriented Library for SPMD Programming. </title> <type> ICSI Technical Report TR-94-011, </type> <month> February </month> <year> 1994. </year>
Reference-contexts: Obviously, all the general purpose systems make use of the floating-point format. 3 Implementation of the simulator with ARCH. 3.1 The algorithm. Our implementation is based on an object-oriented library for parallel computing (ARCH) developed by one of the authors. Details of the library can be found in <ref> [2, 3] </ref>. Before detailing the implementation of the backpropagation, we will summarize here the algorithm in terms of matrix and vector operations. In particular we will examine both the on-line and batch versions. <p> In this section we will detail the implementation of the algorithm using the ARCH library. For more details on the ARCH library see <ref> [2, 3] </ref>. The following code is the declaration of the class Neural net for the batch algorithm. Whenever possible, the same notation of the algorithm described in the preceding section has been used.
Reference: [3] <author> J.M.Adamo. </author> <title> Development of Parallel BLAS with ARCH Object-Oriented Parallel Library, Implementation on CM-5. </title> <type> ICSI Technical Report TR-94-045, </type> <month> August </month> <year> 1994. </year>
Reference-contexts: Obviously, all the general purpose systems make use of the floating-point format. 3 Implementation of the simulator with ARCH. 3.1 The algorithm. Our implementation is based on an object-oriented library for parallel computing (ARCH) developed by one of the authors. Details of the library can be found in <ref> [2, 3] </ref>. Before detailing the implementation of the backpropagation, we will summarize here the algorithm in terms of matrix and vector operations. In particular we will examine both the on-line and batch versions. <p> In this section we will detail the implementation of the algorithm using the ARCH library. For more details on the ARCH library see <ref> [2, 3] </ref>. The following code is the declaration of the class Neural net for the batch algorithm. Whenever possible, the same notation of the algorithm described in the preceding section has been used.
Reference: [4] <author> D.Anguita, G.Parodi, and R.Zunino. </author> <title> An Efficient Implementation of BP on RISC-based Workstations. </title> <type> Neurocomputing 6, </type> <year> 1994, </year> <pages> pp. 57-65. </pages>
Reference-contexts: Y-MP (2) 40 256 fi 256 fi 131072; 111 P Y [17] TMC CM-2 (4k/64k) 2:5=40 1 256 fi 128 fi 256; 64 B Y [33] Cray X-MP (4) 18 256 fi 256 fi 131072; 111 P Y [17] IBM 6000/550 17:6 500 fi 500 fi 1; 1000 E Y <ref> [4] </ref> Intel iPSC/860 (32) 11 NETtalk B Y [14] Cray 2 (4) 10 256 fi 256 fi 131072; 111 P Y [17] DEC Alpha 3:2 - P Y [21] Sun SparcStation 10 1:1 - P Y [21] Inmos T800 (16) 0:7 192 units (3 layers), 128 B Y [24] PC486 0:47
Reference: [5] <author> K.Asanovic, J.Beck, T.Callahan, J.Feldman, B.Irissou, B.Kingsbury, P.Kohn, J.Lazzaro, N.Morgan, D.Stoutamire and J.Wawrzynek. </author> <title> CNS-1 Architecture Specification. </title> <type> ICSI Technical Report TR-93-021, </type> <month> April </month> <year> 1993. </year>
Reference-contexts: In Table 1 some of the best-known solutions proposed in literature are presented. Table 1: General purpose and dedicated implementations of the backpropagation. Computer MCUPS Problem size Alg. FP Ref. CNS-1 (128/1024) 22000 1 =166000 1 - P N <ref> [5, 23] </ref> Adapt. Sol.
Reference: [6] <author> K.Asanovic, J.Beck, B.E.D.Kingsbury, P.Kohn, N.Morgan, J.Wawrzynek. SPERT: </author> <title> A VLIW/SIMD Microprocessor for Artificial Neural Network Computations. </title> <type> ICSI Tech. Rep. </type> <institution> TR-91-072, </institution> <month> January </month> <year> 1992. </year>
Reference-contexts: HNC SNAP (16/64) 80:4=302 512 fi 512 fi 512 - Y [1] MUSIC (60) 247 - P Y [21, 22] MANTRA I (1600) 133 1 - P N [30] RAP (40) 102 640 fi 640 fi 640 P Y [20] SPERT 100 1 512 fi 512 fi 512 P N <ref> [32, 6] </ref> TMC CM-5 (512) 76 256 fi 256 fi 131072; 111 P Y [17] FUJITSU VP-2400/10 60 NETtalk P Y [26] A.C.A. (4225) 51:4 1 NETtalk P N [10] Cray Y-MP (2) 40 256 fi 256 fi 131072; 111 P Y [17] TMC CM-2 (4k/64k) 2:5=40 1 256 fi 128
Reference: [7] <author> A.Corana, C.Rolando, S.Ridella. </author> <title> A Highly Efficient Implementation of Backpropagation Algorithm on SIMD Computers. In High Performance Computing, </title> <editor> J.- L.Delhaye and E.Gelenbe (Eds.), </editor> <publisher> Elsevier, </publisher> <year> 1989, </year> <month> pp.181-190. </month>
Reference-contexts: It has been showed that the core of the backpropagation algorithm can be seen as a sequence of vector-matrix or matrix-matrix multiplications. Therefore, the problem of mapping the algorithm on a particular architecture could be easily addressed considering the mapping of these operations <ref> [7, 8] </ref>. Unfortunately, the neural community often favor a natural mapping of the neurons on the processor (s) and this approach leads to questionable results. 3 affects directly the maximum speed achievable by an implementation.
Reference: [8] <author> A.Corana, C.Rolando, S.Ridella. </author> <title> Use of Level 3 BLAS Kernels in Neural Networks: The Back-propagation algorithm. </title> <booktitle> Parallel Computing 89, </booktitle> <year> 1990, </year> <month> pp.269-274. </month>
Reference-contexts: It has been showed that the core of the backpropagation algorithm can be seen as a sequence of vector-matrix or matrix-matrix multiplications. Therefore, the problem of mapping the algorithm on a particular architecture could be easily addressed considering the mapping of these operations <ref> [7, 8] </ref>. Unfortunately, the neural community often favor a natural mapping of the neurons on the processor (s) and this approach leads to questionable results. 3 affects directly the maximum speed achievable by an implementation.
Reference: [9] <author> J.Dongarra. </author> <title> Linear Algebra Library for High-Performance Computers. </title> <booktitle> Frontiers of Supercomputing II. </booktitle> <editor> K.R.Ames and A.Brenner (Eds.), </editor> <publisher> University of California Press, </publisher> <year> 1994. </year>
Reference-contexts: As mentioned before, the on-line version can be implemented through matrix-vector multiplication, while the batch version requires matrix-matrix multiplications. As showed in <ref> [9] </ref> the latter can be implemented more efficiently on the majority of systems. Finally, the column labeled FP indicates the arithmetic used by an implementation. As can be seen, the fastest system can obtain such astonishing performances using fixed-point math instead of floating-point.
Reference: [10] <author> B.Faure, G.Mazare. </author> <title> Implementation of back-propagation on a VLSI asynchronous cellular architecture. </title> <booktitle> Proc. of Int. NN Conf., </booktitle> <address> July 9-13, 1990, Paris, France, </address> <pages> pp. 631-634. </pages>
Reference-contexts: RAP (40) 102 640 fi 640 fi 640 P Y [20] SPERT 100 1 512 fi 512 fi 512 P N [32, 6] TMC CM-5 (512) 76 256 fi 256 fi 131072; 111 P Y [17] FUJITSU VP-2400/10 60 NETtalk P Y [26] A.C.A. (4225) 51:4 1 NETtalk P N <ref> [10] </ref> Cray Y-MP (2) 40 256 fi 256 fi 131072; 111 P Y [17] TMC CM-2 (4k/64k) 2:5=40 1 256 fi 128 fi 256; 64 B Y [33] Cray X-MP (4) 18 256 fi 256 fi 131072; 111 P Y [17] IBM 6000/550 17:6 500 fi 500 fi 1; 1000 E
Reference: [11] <author> K.A.Grajski, G.Chinn, C.Chen, C.Kuszmaul, S.Tomboulian. </author> <title> Neural Network Simulation on the MasPar MP-1 Massively Parallel Processor. </title> <booktitle> Proc. of Int. NN Conf., </booktitle> <address> July 9-13, 1990, Paris, France, p.673. </address>
Reference-contexts: 2 (4) 10 256 fi 256 fi 131072; 111 P Y [17] DEC Alpha 3:2 - P Y [21] Sun SparcStation 10 1:1 - P Y [21] Inmos T800 (16) 0:7 192 units (3 layers), 128 B Y [24] PC486 0:47 - P Y [21] MasPar MP-1 0:3 - Y <ref> [11] </ref> In the first column, the name of the system on which a backpropagation implementation has been realized is reported. The number of processors (if greater than one) is reported in parenthesis.
Reference: [12] <author> A.Hiraiwa, S.Kurosu, S.Arisawa, M.Inoue. </author> <title> A two level pipeline RISC processor array for ANN. </title> <booktitle> Proc. of the Int. Joint Conf. on NN, </booktitle> <month> January 15-19, </month> <year> 1990, </year> <title> Washinghton, </title> <address> DC, pp.II137-II140. </address>
Reference-contexts: Computer MCUPS Problem size Alg. FP Ref. CNS-1 (128/1024) 22000 1 =166000 1 - P N [5, 23] Adapt. Sol. CNAPS (512) 2379 1900 fi 500 fi 12 P N [13] Sony GCN-860 (128) 1000 1 256 fi 80 fi 32; 5120 - Y <ref> [12] </ref> Sandy/8 (256) 118=567 1 NETtalk/peak P Y [16] TMC CM-2 (64k) 350 128 fi 128 fi 128; 65536 E Y [29] HNC SNAP (16/64) 80:4=302 512 fi 512 fi 512 - Y [1] MUSIC (60) 247 - P Y [21, 22] MANTRA I (1600) 133 1 - P N [30]
Reference: [13] <author> P.Ienne. </author> <title> Architectures for Neuro-Computers: Review and Performance Evaluation. </title> <type> Technical Report 93/21, </type> <institution> Swiss Federal Institute of Technology, Lausanne, </institution> <month> January </month> <year> 1993. </year>
Reference-contexts: Table 1: General purpose and dedicated implementations of the backpropagation. Computer MCUPS Problem size Alg. FP Ref. CNS-1 (128/1024) 22000 1 =166000 1 - P N [5, 23] Adapt. Sol. CNAPS (512) 2379 1900 fi 500 fi 12 P N <ref> [13] </ref> Sony GCN-860 (128) 1000 1 256 fi 80 fi 32; 5120 - Y [12] Sandy/8 (256) 118=567 1 NETtalk/peak P Y [16] TMC CM-2 (64k) 350 128 fi 128 fi 128; 65536 E Y [29] HNC SNAP (16/64) 80:4=302 512 fi 512 fi 512 - Y [1] MUSIC (60) 247
Reference: [14] <author> D.Jackson, D.Hammerstrom. </author> <title> Distributing Back Propagation Networks Over the Intel iPSC/860 Hypercube. </title> <booktitle> Proc. of the Int. Joint Conf. on NN, </booktitle> <address> July 8-12, 1991, Seattle, WA, </address> <pages> pp. </pages> <month> I569-I574. </month>
Reference-contexts: 111 P Y [17] TMC CM-2 (4k/64k) 2:5=40 1 256 fi 128 fi 256; 64 B Y [33] Cray X-MP (4) 18 256 fi 256 fi 131072; 111 P Y [17] IBM 6000/550 17:6 500 fi 500 fi 1; 1000 E Y [4] Intel iPSC/860 (32) 11 NETtalk B Y <ref> [14] </ref> Cray 2 (4) 10 256 fi 256 fi 131072; 111 P Y [17] DEC Alpha 3:2 - P Y [21] Sun SparcStation 10 1:1 - P Y [21] Inmos T800 (16) 0:7 192 units (3 layers), 128 B Y [24] PC486 0:47 - P Y [21] MasPar MP-1 0:3 -
Reference: [15] <author> T.T.Jervis, W.J.Fitzgerald. </author> <title> Optimizations Schemes for Neural Networks. </title> <note> Technical Re--port CUED/F-INFENG/TR 144. </note>
Reference-contexts: Many other variations to this standard procedure can be found in literature <ref> [27, 15] </ref>. For the batch version we have implemented the Vogl's acceleration technique [31] that adapts both the learning step and the momentum term at each iteration.
Reference: [16] <author> H.Kato, H.Yoshizawa, H.Iciki, K.Asakawa. </author> <title> A Parallel Neurocomputer Architecture towards Billion Connection Update Per Second. </title> <booktitle> Proc. of the Int. Joint Conf. on NN, </booktitle> <month> January 15-19, </month> <year> 1990, </year> <title> Washinghton, </title> <address> DC, pp.II47-II50. </address>
Reference-contexts: FP Ref. CNS-1 (128/1024) 22000 1 =166000 1 - P N [5, 23] Adapt. Sol. CNAPS (512) 2379 1900 fi 500 fi 12 P N [13] Sony GCN-860 (128) 1000 1 256 fi 80 fi 32; 5120 - Y [12] Sandy/8 (256) 118=567 1 NETtalk/peak P Y <ref> [16] </ref> TMC CM-2 (64k) 350 128 fi 128 fi 128; 65536 E Y [29] HNC SNAP (16/64) 80:4=302 512 fi 512 fi 512 - Y [1] MUSIC (60) 247 - P Y [21, 22] MANTRA I (1600) 133 1 - P N [30] RAP (40) 102 640 fi 640 fi 640
Reference: [17] <author> X.Liu and G.L.Wilcox. </author> <title> Benchmarking of the CM-5 and the Cray Machines with a Very Large Backpropagation Neural Network. </title> <booktitle> Proc. of IEEE Int. Conf. on NN, </booktitle> <address> June 28 - July 2, 1994, Orlando, FL, pp.22-27. </address>
Reference-contexts: 247 - P Y [21, 22] MANTRA I (1600) 133 1 - P N [30] RAP (40) 102 640 fi 640 fi 640 P Y [20] SPERT 100 1 512 fi 512 fi 512 P N [32, 6] TMC CM-5 (512) 76 256 fi 256 fi 131072; 111 P Y <ref> [17] </ref> FUJITSU VP-2400/10 60 NETtalk P Y [26] A.C.A. (4225) 51:4 1 NETtalk P N [10] Cray Y-MP (2) 40 256 fi 256 fi 131072; 111 P Y [17] TMC CM-2 (4k/64k) 2:5=40 1 256 fi 128 fi 256; 64 B Y [33] Cray X-MP (4) 18 256 fi 256 fi <p> 1 512 fi 512 fi 512 P N [32, 6] TMC CM-5 (512) 76 256 fi 256 fi 131072; 111 P Y <ref> [17] </ref> FUJITSU VP-2400/10 60 NETtalk P Y [26] A.C.A. (4225) 51:4 1 NETtalk P N [10] Cray Y-MP (2) 40 256 fi 256 fi 131072; 111 P Y [17] TMC CM-2 (4k/64k) 2:5=40 1 256 fi 128 fi 256; 64 B Y [33] Cray X-MP (4) 18 256 fi 256 fi 131072; 111 P Y [17] IBM 6000/550 17:6 500 fi 500 fi 1; 1000 E Y [4] Intel iPSC/860 (32) 11 NETtalk B Y [14] Cray 2 (4) <p> P Y [26] A.C.A. (4225) 51:4 1 NETtalk P N [10] Cray Y-MP (2) 40 256 fi 256 fi 131072; 111 P Y <ref> [17] </ref> TMC CM-2 (4k/64k) 2:5=40 1 256 fi 128 fi 256; 64 B Y [33] Cray X-MP (4) 18 256 fi 256 fi 131072; 111 P Y [17] IBM 6000/550 17:6 500 fi 500 fi 1; 1000 E Y [4] Intel iPSC/860 (32) 11 NETtalk B Y [14] Cray 2 (4) 10 256 fi 256 fi 131072; 111 P Y [17] DEC Alpha 3:2 - P Y [21] Sun SparcStation 10 1:1 - P Y [21] Inmos T800 <p> 256; 64 B Y [33] Cray X-MP (4) 18 256 fi 256 fi 131072; 111 P Y <ref> [17] </ref> IBM 6000/550 17:6 500 fi 500 fi 1; 1000 E Y [4] Intel iPSC/860 (32) 11 NETtalk B Y [14] Cray 2 (4) 10 256 fi 256 fi 131072; 111 P Y [17] DEC Alpha 3:2 - P Y [21] Sun SparcStation 10 1:1 - P Y [21] Inmos T800 (16) 0:7 192 units (3 layers), 128 B Y [24] PC486 0:47 - P Y [21] MasPar MP-1 0:3 - Y [11] In the first column, the name of the system on which
Reference: [18] <author> R.Means, L.Lisenbee. </author> <title> Extensible Linear Floating Point SIMD Neurocomputer Array Processor. </title> <booktitle> Proc. of the Int. Joint Conf. on NN, </booktitle> <address> July 8-12, 1991, Seattle, WA, </address> <pages> pp. </pages> <month> I587-I592. </month>
Reference: [19] <author> M.Moller. </author> <title> Supervised Learning on Large Redundant Training Sets. </title> <journal> Int. J. of Neural Systems, Vol.4, </journal> <volume> No.1, </volume> <year> 1993. </year>
Reference-contexts: The consequences of the different versions are both on the computational requirement and on the speed of convergence. The last one is outside of the scope of this paper and won't be addressed here (see for example <ref> [19] </ref>). The effect on the computation 2 The MasPar figure has been reported in the Table only as simple curiosity. This shows how the implementation of the backpropagation algorithm is not widely very well understood. 3 Obviously, the quality of the implementations summarized here is not the same.
Reference: [20] <author> N.Morgan, J.Beck, P.Kohn, J.Bilmes, E.Allman, J.Beer. </author> <title> The Ring Array Processor: </title>
Reference-contexts: (64k) 350 128 fi 128 fi 128; 65536 E Y [29] HNC SNAP (16/64) 80:4=302 512 fi 512 fi 512 - Y [1] MUSIC (60) 247 - P Y [21, 22] MANTRA I (1600) 133 1 - P N [30] RAP (40) 102 640 fi 640 fi 640 P Y <ref> [20] </ref> SPERT 100 1 512 fi 512 fi 512 P N [32, 6] TMC CM-5 (512) 76 256 fi 256 fi 131072; 111 P Y [17] FUJITSU VP-2400/10 60 NETtalk P Y [26] A.C.A. (4225) 51:4 1 NETtalk P N [10] Cray Y-MP (2) 40 256 fi 256 fi 131072; 111
References-found: 20

