URL: http://www.cs.cornell.edu/Info/Projects/Bernoulli/papers/tpds94.ps
Refering-URL: http://www.cs.cornell.edu/Info/Projects/Bernoulli/
Root-URL: http://www.cs.cornell.edu
Title: Compiling for distributed memory architectures  
Author: Anne Rogers and Keshav Pingali 
Date: June 10, 1991  
Abstract-found: 0
Intro-found: 1
Reference: [1] <author> F. Allen, M. Burke, P. Charles, R. Cytron, and J. Ferrante. </author> <title> An overview of the ptran analysis system for multiprocessing. </title> <journal> J. of Parallel and Distributed Computing, </journal> <volume> 5 </volume> <pages> 617-640, </pages> <year> 1988. </year>
Reference-contexts: Given this data decomposition, the compiler performs process decomposition by analyzing the program and specializing it, for each processor, to the data that resides on that processor. Thus, our approach to process decomposition is "data-driven" rather than "program-driven" as are more traditional approaches <ref> [1, 19] </ref>. 1.1 System Overview Our compiler takes a sequential program and a domain decomposition and generates C code for the Intel iPSC/2. The system generates code for each process based on the data residing in that process.
Reference: [2] <author> V. Balasundaram, G. Fox, K. Kennedy, and U. Kremer. </author> <title> An interactive environment for data partitioning and distribution. </title> <booktitle> In to appear in Proceedings of the Fifth Distributed Memory Computing Conference, </booktitle> <month> April </month> <year> 1990. </year>
Reference: [3] <author> S. Bokhari. </author> <title> Assignment Problems in Parallel and Distributed Computing. </title> <publisher> Kluwer Academic Publishing, </publisher> <year> 1987. </year>
Reference-contexts: This approach runs counter to more traditional approaches to programming distributed memory machines that focus on mapping the topology of problems (rings, trees, etc.) to the topology of machines (hypercube, shu*e-exchange etc.) to exploit nearest-neighbor communication <ref> [3, 23] </ref>. Exploiting this kind of locality is important for architectures with multi-level memory hierarchies like the Intel iPSC/1, but it is not important to our two-level hierarchy.
Reference: [4] <author> L. Bomans and D. Roose. </author> <title> Benchmarking the iPSC/2 hypercube multiprocessor. </title> <journal> Concurrency Practice and Experience, </journal> <volume> 1(1), </volume> <month> September </month> <year> 1989. </year>
Reference-contexts: For messages under 100 bytes, t startup = 350s and t send = 0:8s, and t startup = 660s and t send = 0:8s for longer messages <ref> [4] </ref>. Most of the cost in sending a message is software overhead 1 Strictly speaking, the iPSC/2 permits multiple processes to execute on a processor but we can take that into account simply by increasing the number of processors in our model. <p> This is not always true. When a block mapping is used, the expression that describes the reader can take on different values. For example, given the statement make-send (A [i-1, 1..7], &lt;t, fBM (i-1, 1..7)g&gt;), process 0 will need to send A <ref> [4, 1..3] </ref> to Process 2 and A [4, 4] to Process 3 from a single vector send. When this occurs, we generate multiple vector sends. One for each of the possible value of the reader expression. The same holds for vector receives. <p> This is not always true. When a block mapping is used, the expression that describes the reader can take on different values. For example, given the statement make-send (A [i-1, 1..7], &lt;t, fBM (i-1, 1..7)g&gt;), process 0 will need to send A [4, 1..3] to Process 2 and A <ref> [4, 4] </ref> to Process 3 from a single vector send. When this occurs, we generate multiple vector sends. One for each of the possible value of the reader expression. The same holds for vector receives. <p> We estimate the cost of a floating point operation as C = :33 fl 8:52 + :66 fl 6:64. The operation costs used are the costs reported for a the multiplication/addition of two double length floating point numbers <ref> [4] </ref>. This estimate of the floating point work does not taken into account pipelining. Communication overhead must be measured with respect to a particular mapping. To estimate this overhead, we counted the number of messages and the size of each message in a four process system.
Reference: [5] <author> D. Callahan and K. Kennedy. </author> <title> Compiling programs for distributed memory multiprocessors. </title> <journal> The Journal of Supercomputing, </journal> <volume> 2(2), </volume> <month> October </month> <year> 1988. </year>
Reference-contexts: . . .. . . .. . . .. . . .. . .. . . .. . .. . .. . .. . .. .. . .. . .. .. . .. .. .. . .. .. .. .. .. . .. .. 11 Related Work Several other groups <ref> [5, 6, 14, 21, 25, 26] </ref> have concurrently developed compilers that use code generation methods similar to our run-time and compile-time resolution. We briefly describe the different ways these groups have built upon the basic code generation schemes. <p> We briefly describe the different ways these groups have built upon the basic code generation schemes. Rice A group led by Ken Kennedy <ref> [5] </ref> at Rice University is considering the question of how to determine good mappings. In Balasundaram et al.[2] they propose an interactive tool to assist the user in selecting the partition and distribution of each array in a program.
Reference: [6] <author> M. Chen, Y. Choo, and J. Li. </author> <title> Theory and pragmatics of compiling efficient parallel code. </title> <type> Technical Report YALEU/DCS/TR-790, </type> <institution> Yale Univeristy, </institution> <month> December </month> <year> 1989. </year>
Reference-contexts: . . .. . . .. . . .. . . .. . .. . . .. . .. . .. . .. . .. .. . .. . .. .. . .. .. .. . .. .. .. .. .. . .. .. 11 Related Work Several other groups <ref> [5, 6, 14, 21, 25, 26] </ref> have concurrently developed compilers that use code generation methods similar to our run-time and compile-time resolution. We briefly describe the different ways these groups have built upon the basic code generation schemes. <p> The disadvantage is that when the data dependences in a program are too complex for the compiler to analyze, it will generate code that errs on the side of sending any data that might be needed. The cost of sending unneeded values could be high. Crystal The Crystal project <ref> [6, 16] </ref> led by Marina Chen at Yale has designed a compiler that focuses on reducing communication overhead. Program references are analyzed and matched against a set of syntactic patterns that represent a set of aggregate communication routines that can be implemented efficiently. These aggregates include broadcast, reduction, and shift.
Reference: [7] <author> W.P. Crowley, C.P. Hendrickson, and T.E. Rudy. </author> <title> The SIMPLE code. </title> <type> Technical Report UCID-17715, </type> <institution> Lawrence Livermore Laboratory, </institution> <year> 1978. </year>
Reference-contexts: In addition, he specifies the domain decomposition | a mapping of the data structures on to the multiprocessor. In most programs we have looked at (such as matrix algorithms and SIMPLE <ref> [7] </ref>), this is quite straightforward since the programmer thinks naturally in terms of decompositions by columns, rows, blocks, and so on. Given this data decomposition, the compiler performs process decomposition by analyzing the program and specializing it, for each processor, to the data that resides on that processor. <p> These optimizations attempt to strike a balance between reducing message overhead and increasing parallelism. An important optimization is the recognition and exploitation of reduction operations. Section 9 discusses how this is done in our compiler. Section 10 presents the results of using the compiler to parallelize SIMPLE <ref> [7] </ref>, a large heat conduction/hydrodynamics benchmark from Los Alamos. We discuss related work in Section 11.
Reference: [8] <author> R. Cytron, J. Ferrante, B. K. Rosen, M. N. Wegman, and F.K. Zadeck. </author> <title> An efficient method of computing static single assignment form. </title> <booktitle> In Proceedings of the ACM Symposium on the Principles of Programming Languages, </booktitle> <year> 1989. </year>
Reference-contexts: Some of these quantities are difficult to obtain because the time measurement tools on the iPSC/2 are primitive. 6 We started with an implementation of written in Id [17] by K. Ekanadham <ref> [8] </ref>. This program makes heavy use of higher-order functions. Since our compiler does not yet handle such functions, we replaced all higher-order functions by equivalent first-order code. Also, our compiler can handle only flat arrays.
Reference: [9] <author> K. Ekanadham and Arvind. </author> <title> SIMPLE Part I: An exercise in future scientific programming. </title> <type> Technical Report RC12686, </type> <institution> IBM, </institution> <month> April </month> <year> 1987. </year>
Reference: [10] <author> M. Gerndt. </author> <title> Automatic Parallelization for Distributed-Memory Multiprocessing Systems. </title> <type> PhD thesis, </type> <institution> Univeristy of Bonn, </institution> <year> 1989. </year>
Reference-contexts: In Balasundaram et al.[2] they propose an interactive tool to assist the user in selecting the partition and distribution of each array in a program. The basic idea is to estimate statically the performance of a program under various distributions. Superb The Superb project <ref> [10, 26] </ref> led by Hans Zima at the University of Vienna has built a compiler that includes a vectorization of communication optimization, as well as the standard code generation schemes. This optimization is built upon the concept of an overlap area.
Reference: [11] <author> A. Karp. </author> <title> Programming for parallelism. </title> <journal> IEEE Computer, </journal> <volume> 20(5), </volume> <month> May </month> <year> 1987. </year>
Reference-contexts: Such machines are called Non-uniform Memory Access (NUMA) machines and exploiting locality of reference is important on these machines as well. The importance of combining parallelism detection with proper data distribution to achieve locality of reference has been eloquently summarized by Karp <ref> [11] </ref> as follows: 1 : : : we see that data organization is the key to parallel algorithms even on shared memory systems. It will take some retraining to get programmers to plan their data first and their program flow later.
Reference: [12] <author> C. Koelbel. </author> <title> Compiling Programs for Nonshared Memory Machines. </title> <type> PhD thesis, </type> <institution> Purdue University, West Lafayette, IN, </institution> <month> August </month> <year> 1990. </year>
Reference-contexts: The overlap areas as defined do not extend nicely to wrapped mappings. For instance, the overlap area for an array used in the wrapped version of Gauss-Seidel would be the whole array! Kali Koelbel, Mehrotra and van Rosendale <ref> [12, 13, 14] </ref> have developed Kali, a system that compiles a functional language with a forall construct into a language that includes constructs for explicit process creation, data storage layout, and interprocessor communication and synchronization.
Reference: [13] <author> C. Koelbel, P. Mehrotra, and J. van Rosendale. </author> <title> Semi-automatic domain decomposition in Blaze. </title> <booktitle> In Proceedings of the International Conference on Parallel Processing, </booktitle> <year> 1987. </year>
Reference-contexts: The overlap areas as defined do not extend nicely to wrapped mappings. For instance, the overlap area for an array used in the wrapped version of Gauss-Seidel would be the whole array! Kali Koelbel, Mehrotra and van Rosendale <ref> [12, 13, 14] </ref> have developed Kali, a system that compiles a functional language with a forall construct into a language that includes constructs for explicit process creation, data storage layout, and interprocessor communication and synchronization.
Reference: [14] <author> C. Koelbel, P. Mehrotra, and J. van Rosendale. </author> <title> Supporting shared data structures on distributed memory architectures. </title> <booktitle> In Proceedings of the Second ACM SIGPLAN Symposium on the Principles and Practice of Parallel Programming, </booktitle> <year> 1990. </year>
Reference-contexts: . . .. . . .. . . .. . . .. . .. . . .. . .. . .. . .. . .. .. . .. . .. .. . .. .. .. . .. .. .. .. .. . .. .. 11 Related Work Several other groups <ref> [5, 6, 14, 21, 25, 26] </ref> have concurrently developed compilers that use code generation methods similar to our run-time and compile-time resolution. We briefly describe the different ways these groups have built upon the basic code generation schemes. <p> The overlap areas as defined do not extend nicely to wrapped mappings. For instance, the overlap area for an array used in the wrapped version of Gauss-Seidel would be the whole array! Kali Koelbel, Mehrotra and van Rosendale <ref> [12, 13, 14] </ref> have developed Kali, a system that compiles a functional language with a forall construct into a language that includes constructs for explicit process creation, data storage layout, and interprocessor communication and synchronization.
Reference: [15] <author> M. Lam. </author> <title> A Systolic Array Optimizing Compiler. </title> <type> PhD thesis, </type> <institution> Carnegie Mellon University, </institution> <year> 1987. </year>
Reference-contexts: They exploit repeated patterns by saving information about the loop's communication patterns between executions. Rather than recomputing the communication pattern, they try to use the saved information. Saltz et al.[22] have taken a similar approach. AL P.S. Tseng [24, 25] has written a compiler that generates W2 <ref> [15] </ref> code for the Warp, a programmable linear systolic array, from programs written in a simple sequential language called AL. This approach may require more than just domain decomposition information from the programmer.
Reference: [16] <author> J. Li and M. Chen. </author> <title> Synthesis of explicit communication from shared-memory program references. </title> <type> Technical Report YALEU/DCS/TR-755, </type> <institution> Yale Univeristy, </institution> <month> May </month> <year> 1990. </year>
Reference-contexts: The disadvantage is that when the data dependences in a program are too complex for the compiler to analyze, it will generate code that errs on the side of sending any data that might be needed. The cost of sending unneeded values could be high. Crystal The Crystal project <ref> [6, 16] </ref> led by Marina Chen at Yale has designed a compiler that focuses on reducing communication overhead. Program references are analyzed and matched against a set of syntactic patterns that represent a set of aggregate communication routines that can be implemented efficiently. These aggregates include broadcast, reduction, and shift.
Reference: [17] <author> R. Nikhil, K. Pingali, and Arvind. </author> <title> Id Nouveau. </title> <type> Technical Report CSG Memo 265, </type> <institution> MIT Laboratory for Computer Science, </institution> <year> 1986. </year> <month> 37 </month>
Reference-contexts: We discuss related work in Section 11. Section 12 is a discussion of the scope and limitations of our approach and opportunities for further work. 2 Language and Machine Model The example programs in this paper are written in a language similar to Id <ref> [17] </ref>. The programs differ from Id in two major ways: data distributions must be specified for most variables and the parallel programs include communication statements. <p> Some of these quantities are difficult to obtain because the time measurement tools on the iPSC/2 are primitive. 6 We started with an implementation of written in Id <ref> [17] </ref> by K. Ekanadham [8]. This program makes heavy use of higher-order functions. Since our compiler does not yet handle such functions, we replaced all higher-order functions by equivalent first-order code. Also, our compiler can handle only flat arrays.
Reference: [18] <author> K. Pingali and A. Rogers. </author> <title> Compiler parallelization of SIMPLE for a distributed memory machine. </title> <type> Technical Report 90-1084, </type> <institution> Department of Computer Science, Cornell University, </institution> <year> 1990. </year>
Reference-contexts: For lack of space, we will not describe the details of the various computations in SIMPLE here, and refer the interested reader to an associated technical report <ref> [18] </ref>. 29 0 4 8 12 16 20 24 28 32 Number of Processes 0 8 16 Time (sec.) * * * We ran a set of experiments using an implementation 6 of one iteration of SIMPLE for a 64 by 64 grid.
Reference: [19] <author> C. Polychronopoulos, M. Girkar, M. Haghighat, C.L. Lee, B. Leung, and D. Schouten. </author> <title> Parafrase-2: An environment for parallelizing, partitioning, synchronizing, and scheduling programs on multiprocessors. </title> <booktitle> In Proceedings of the 1989 International Conference on Parallel Processing, </booktitle> <month> August </month> <year> 1989. </year>
Reference-contexts: Given this data decomposition, the compiler performs process decomposition by analyzing the program and specializing it, for each processor, to the data that resides on that processor. Thus, our approach to process decomposition is "data-driven" rather than "program-driven" as are more traditional approaches <ref> [1, 19] </ref>. 1.1 System Overview Our compiler takes a sequential program and a domain decomposition and generates C code for the Intel iPSC/2. The system generates code for each process based on the data residing in that process.
Reference: [20] <author> A. Rogers. </author> <title> Compiling for Locality of Reference. </title> <type> PhD thesis, </type> <institution> Cornell University, </institution> <month> August </month> <year> 1990. </year>
Reference-contexts: By keeping this summary information, we generate the right code without actually unrolling the loop and then specializing the body of the unrolled loop. The reader who is interested in the details is referred to <ref> [20] </ref>. Unlike wrapped mappings, block mappings yield loops in which the step size is 1. An expression (e 1 ) will equal a process number, P, for some range of integer values (x..y). To generate code given a set of such expressions, we need to determine where these ranges overlap.
Reference: [21] <author> M. Rosing, R. Schnabel, and R. Weaver. </author> <title> The DINO parallel programming language. </title> <type> Technical Report CU-CS-457-90, </type> <institution> Univeristy of Colorado at Boulder, </institution> <month> April </month> <year> 1990. </year>
Reference-contexts: . . .. . . .. . . .. . . .. . .. . . .. . .. . .. . .. . .. .. . .. . .. .. . .. .. .. . .. .. .. .. .. . .. .. 11 Related Work Several other groups <ref> [5, 6, 14, 21, 25, 26] </ref> have concurrently developed compilers that use code generation methods similar to our run-time and compile-time resolution. We briefly describe the different ways these groups have built upon the basic code generation schemes. <p> Ideally, the compiler will be able to generate an efficient implementation for the target architecture. Crystal includes a meta-language that allows the programmer to direct the transformation process. Practically, this transformation process may require insight from the programmer. 33 Dino The Dino Project <ref> [21] </ref> led by R. Schnabel at Colorado takes a more language oriented approach to the problem of programming distributed memory machines. The Dino language consists of a set of parallel constructs added to C.
Reference: [22] <author> J. Saltz, K. Crowley, R. Mirchandaney, and H. Berryman. </author> <title> Run-time scheduling and execution of loops on message passing machines. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 8 </volume> <pages> 303-312, </pages> <year> 1990. </year>
Reference-contexts: One approach to these problems is to use variations on scatter and gather, as suggested by Saltz et al <ref> [22] </ref>. Problems in which load 34 balancing and locality of reference are in conflict, such as particle-in-the-cell codes, may also not yield to our techniques.
Reference: [23] <author> H. Stone. </author> <title> Multiprocessor scheduling with the aid of network flow algorithms. </title> <journal> ACM Transactions on Software Engineering, </journal> <year> 1977. </year>
Reference-contexts: This approach runs counter to more traditional approaches to programming distributed memory machines that focus on mapping the topology of problems (rings, trees, etc.) to the topology of machines (hypercube, shu*e-exchange etc.) to exploit nearest-neighbor communication <ref> [3, 23] </ref>. Exploiting this kind of locality is important for architectures with multi-level memory hierarchies like the Intel iPSC/1, but it is not important to our two-level hierarchy.
Reference: [24] <author> P.S. Tseng. </author> <title> A Parallelizing Compiler for distributed memory parallel computers. </title> <type> PhD thesis, </type> <institution> Carnegie-Mellon University, </institution> <month> May </month> <year> 1989. </year>
Reference-contexts: They exploit repeated patterns by saving information about the loop's communication patterns between executions. Rather than recomputing the communication pattern, they try to use the saved information. Saltz et al.[22] have taken a similar approach. AL P.S. Tseng <ref> [24, 25] </ref> has written a compiler that generates W2 [15] code for the Warp, a programmable linear systolic array, from programs written in a simple sequential language called AL. This approach may require more than just domain decomposition information from the programmer.
Reference: [25] <author> P.S. Tseng. </author> <title> Compiling programs for a linear systolic array. </title> <booktitle> In Proceeding of ACM Symposium on Programming Language Design and Implementation, </booktitle> <month> June </month> <year> 1990. </year>
Reference-contexts: . . .. . . .. . . .. . . .. . .. . . .. . .. . .. . .. . .. .. . .. . .. .. . .. .. .. . .. .. .. .. .. . .. .. 11 Related Work Several other groups <ref> [5, 6, 14, 21, 25, 26] </ref> have concurrently developed compilers that use code generation methods similar to our run-time and compile-time resolution. We briefly describe the different ways these groups have built upon the basic code generation schemes. <p> They exploit repeated patterns by saving information about the loop's communication patterns between executions. Rather than recomputing the communication pattern, they try to use the saved information. Saltz et al.[22] have taken a similar approach. AL P.S. Tseng <ref> [24, 25] </ref> has written a compiler that generates W2 [15] code for the Warp, a programmable linear systolic array, from programs written in a simple sequential language called AL. This approach may require more than just domain decomposition information from the programmer. <p> This approach may require more than just domain decomposition information from the programmer. The compiler can infer which portion of the data is needed by one process but resides on another, in some cases <ref> [25] </ref>. When the compiler cannot infer this information, the user must supply it. With this information the AL compiler can generate code using a method similar to ours.
Reference: [26] <author> H. Zima, H. Bast, and M. Gerndt. </author> <title> SUPERB:a tool for semi-automatic MIMD/SIMD paral-lelization. </title> <journal> Parallel Computing, </journal> <volume> 6(1) </volume> <pages> 1-18, </pages> <year> 1988. </year> <month> 38 </month>
Reference-contexts: . . .. . . .. . . .. . . .. . .. . . .. . .. . .. . .. . .. .. . .. . .. .. . .. .. .. . .. .. .. .. .. . .. .. 11 Related Work Several other groups <ref> [5, 6, 14, 21, 25, 26] </ref> have concurrently developed compilers that use code generation methods similar to our run-time and compile-time resolution. We briefly describe the different ways these groups have built upon the basic code generation schemes. <p> In Balasundaram et al.[2] they propose an interactive tool to assist the user in selecting the partition and distribution of each array in a program. The basic idea is to estimate statically the performance of a program under various distributions. Superb The Superb project <ref> [10, 26] </ref> led by Hans Zima at the University of Vienna has built a compiler that includes a vectorization of communication optimization, as well as the standard code generation schemes. This optimization is built upon the concept of an overlap area.
References-found: 26

