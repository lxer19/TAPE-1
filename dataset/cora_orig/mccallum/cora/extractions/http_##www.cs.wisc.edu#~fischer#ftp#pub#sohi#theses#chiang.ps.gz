URL: http://www.cs.wisc.edu/~fischer/ftp/pub/sohi/theses/chiang.ps.gz
Refering-URL: http://www.cs.wisc.edu/~fischer/ftp/pub/sohi/theses/
Root-URL: http://www.cs.wisc.edu
Abstract-found: 0
Intro-found: 1
Reference: [1] <institution> ``Introduction to the iAPX-432 Architecture,'' </institution> <note> Intel Manual No. 171821-001, </note> <year> 1981. </year>
Reference: [2] <author> A. Agarwal, R. L. Sites, and M. Horowitz, ``ATUM: </author> <title> A New Technique for Capturing Address Traces Using Microcode,'' </title> <booktitle> in Proc. 13th Annual Symposium on Computer Architecture, </booktitle> <address> Tokyo, Japan, </address> <pages> pp. 119-127, </pages> <month> June </month> <year> 1986. </year>
Reference-contexts: Using workloads from these two distinct architectures would make the evaluation of design choices a fairly complete study. However, for CISC architectures better traces are publicly available. Traces generated using the Address Tracing Using Microcode (ATUM) technique are more representative because they contain operating system activity <ref> [2] </ref>. Moreover, to put the evaluation results in perspective, the workloads should have been used previously for uniprocessor cache studies. The ATUM traces, as well as the TITAN traces, fulfill this requirement.
Reference: [3] <author> A. Agarwal, J. Hennessy, and M. Horowitz, </author> <title> ``Cache Performance of Operating Systems and Multiprogramming Workloads,'' </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> vol. 6, </volume> <pages> pp. 393-431, </pages> <month> November </month> <year> 1988. </year>
Reference-contexts: The ATUM traces that we use are gathered via microcode patches on a VAX 8200 by Agarwal and Sites. These traces are distributed by DEC, are considered to be the best public-domain traces for a multiprogrammed, multi-user environment, and they have been widely used in recent cache studies <ref> [3, 5, 34, 35, 56] </ref>. By passing the ATUM traces through a uniprocessor cache simulator we obtain the values of M, 37 M ref , P r , P w and P v . The microcode based ATUM technique obviously can not be applied to RISC processors.
Reference: [4] <author> A. Agarwal, R. Simoni, J. Hennessy, and M. Horowitz, </author> <title> ``An Evaluation of Directory Schemes for Cache Coherence,'' </title> <booktitle> in Proc. 15th Annual Symposium on Computer Architecture, </booktitle> <address> Honolulu, HI, </address> <pages> pp. 280-289, </pages> <month> June </month> <year> 1988. </year>
Reference-contexts: The performance of different cache coherence protocols was studied using analytical modeling [69, 70], trace driven simulation with real traces <ref> [4, 18, 23, 24] </ref>, or pseudo-traces [9]. Real parallel trace can either be derived from hardware monitoring [60], or software means such as the ptrace method. <p> Invalidation Pattern In this section the invalidation pattern of using different synchronization methods are studied. Analyzing the invalidation pattern is important in evaluating different directory schemes for large scale, non-bus based, cache coherent multiprocessors <ref> [4, 72] </ref>. Previous study of invalidation patterns uses either analytical modeling [17, 21], or trace driven simulation [4, 72]. <p> Analyzing the invalidation pattern is important in evaluating different directory schemes for large scale, non-bus based, cache coherent multiprocessors <ref> [4, 72] </ref>. Previous study of invalidation patterns uses either analytical modeling [17, 21], or trace driven simulation [4, 72]. One drawback of using the trace driven simulation method is that, since the timing of the memory system to be evaluated is unlikely to be the same as the timing of the system where the trace is derived, the result of trace-driven simulation can be inaccurate.
Reference: [5] <author> A. Agarwal, M. Horowitz, and J. Hennessy, </author> <title> ``An Analytical Cache Model,'' </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> vol. 7, </volume> <pages> pp. 184-215, </pages> <month> May </month> <year> 1989. </year>
Reference-contexts: The ATUM traces that we use are gathered via microcode patches on a VAX 8200 by Agarwal and Sites. These traces are distributed by DEC, are considered to be the best public-domain traces for a multiprogrammed, multi-user environment, and they have been widely used in recent cache studies <ref> [3, 5, 34, 35, 56] </ref>. By passing the ATUM traces through a uniprocessor cache simulator we obtain the values of M, 37 M ref , P r , P w and P v . The microcode based ATUM technique obviously can not be applied to RISC processors.
Reference: [6] <author> Thomas E. Anderson, </author> <title> ``The Performance of Spin Lock Alternatives for Shared-Memory Multiprocessors,'' </title> <journal> IEEE Trans. on Parallel and Distributed System, </journal> <volume> vol. 1, </volume> <month> January </month> <year> 1990. </year>
Reference-contexts: For example, to reduce the synchronization traffic, test&test&set has 64 been introduced to reduce the number of test&set memory accesses generated by waiting processors. The Software Queue method further cuts down the amount of unnecessary synchronization traffic <ref> [6, 58] </ref>. Reducing the processor waiting time during a synchronization event is more difficult, and may require the cooperation from the compiler and the operating system. <p> Third, a completely different synchronization method can be developed to replace TTS, or even the entire critical section. In this chapter the performance of the Software Queue <ref> [6] </ref>, the Lock Table, and the Restricted Fetch&Add [63] methods will be evaluated. The software queue method belongs to the second approach, while the other two methods belong to the third. The rest of this chapter is organized as follows. <p> To reduce the contention a processor can delay itself for a certain amount of time after each failed attempt, before rejoining the queue of the critical section, as in the collision avoidance method <ref> [6] </ref>. The potential contention in a single critical section can be spread to multiple critical sections by creating a hierarchy (a tree) of lock variables, as in the tournament locks method [32]. But the most promising has been the software queue method [6, 32, 48]. <p> The potential contention in a single critical section can be spread to multiple critical sections by creating a hierarchy (a tree) of lock variables, as in the tournament locks method [32]. But the most promising has been the software queue method <ref> [6, 32, 48] </ref>. The software queue method enqueues all processors that try to enter the same critical section, with the head of the queue being the processor that is currently in the critical section. <p> A similar but more complex hardware queueing mechanism is proposed to further implement non-exclusive critical sections by adding more cache line states [41]. The hardware queueing mechanism can also be simulated by software. The SyncBits in cache lines can be simulated by an array of lock variables <ref> [6] </ref>, or a software queue data structure [48]. The major advantage of these software derivatives is that they can run on the existing multiprocessors which only support the simpler primitive of test&set and compare&swap, and immediately improve the performance of synchronization. <p> It is apparent from these figures that, if TTS is the only available synchronization mechanism, the software queue scheduling method always performs better, when many processors are used. This result is similar to what was reported in <ref> [6, 32] </ref>. The result here also shows that the performance can be further improved if the sequencing critical section is supported by the restricted combining mechanism.
Reference: [7] <author> Marco Annaratone and Roland Ruhl, </author> <title> ``Performance Measurements of a Commercial Multiprocessor Running Parallel Code,'' </title> <booktitle> in The 16th International Symposium on Computer Architecture, Jerusalem, Israel, </booktitle> <pages> pp. 307-314, </pages> <month> May </month> <year> 1989. </year>
Reference-contexts: These methods can also be used to evaluate the performance of multiprocessors. For example, the performance of different software synchronization algorithms was measured on a Sequent Symmetry multiprocessor [32], and the performance of different cache organizations on different models of Symmetry multiprocessors <ref> [7] </ref>. The performance of different cache coherence protocols was studied using analytical modeling [69, 70], trace driven simulation with real traces [4, 18, 23, 24], or pseudo-traces [9]. Real parallel trace can either be derived from hardware monitoring [60], or software means such as the ptrace method.
Reference: [8] <author> J. Archibald and J. -L. Baer, </author> <title> ``An Evaluation of Cache Coherence Solutions in Shared-Bus Multiprocessors,'' </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> vol. 4, </volume> <pages> pp. 273-298, </pages> <month> November </month> <year> 1986. </year>
Reference-contexts: Multiprocessors with arbitrary interconnection networks have been the subject of several previous studies [39, 45, 46, 49, 51]. Studies of bus-based multiprocessor design issues have used trace-driven simulation [22], parameterized simulation <ref> [8] </ref>, as well as analytical modeling [69, 70]. For a system as complex as a multi, ideally a system designer would like to use an accurate analytical model to explore the design space with a minimal computational requirement. <p> In these papers analytical models are used and the probabilistic input dictates that all processors to have equal access to the shared data. Therefore shared data are more likely subjected to fine grain sharing, in which the write-broadcast should perform well. A similar result is also in <ref> [8] </ref>, where the write broadcast protocols are found to do well when the contention to shared data is high. Write broadcast does worse when the shared data are not subjected to fine grain sharing (or high contention).
Reference: [9] <author> J. Archibald and J. -L. Baer, </author> <title> ``High Performance Cache Coherence Protocols For Shared-Bus Multiprocessors,'' </title> <type> Technical Report 86-06-02, </type> <institution> Department of Computer Sciences, University of Washington, </institution> <address> Seattle, WA 98195, </address> <month> June </month> <year> 1986. </year>
Reference-contexts: The performance of different cache coherence protocols was studied using analytical modeling [69, 70], trace driven simulation with real traces [4, 18, 23, 24], or pseudo-traces <ref> [9] </ref>. Real parallel trace can either be derived from hardware monitoring [60], or software means such as the ptrace method. The parallel trace itself was analyzed to evaluate the performance of cache snooping protocols [22], or different design trades-off of the directory-based cache coherence scheme [72].
Reference: [10] <author> H. B. Bakoglu and T. Whiteside, </author> <title> ``RISC System/6000 Hardware Overview,'' in IBM RISC System/6000 Technology, </title> <address> Austin, TX, </address> <pages> pp. 8-15, </pages> <year> 1990. </year>
Reference-contexts: A larger block may also be desired if we assume that, after a cache miss the requested data in the missing block is always returned first, and the processor can resume execution as soon as the requested data arrives at the cache <ref> [10] </ref>. The need for set associativity in the caches was also considered. Although the importance of set associativity in uniprocessor caches diminishes when the cache size is as large as 1M bytes, set associativity was found to be desirable in multiprocessors even with such large caches.
Reference: [11] <author> B. Beck, B. Kasten, and S. Thakker, </author> <title> ``VLSI Assist for a Multiprocessor,'' </title> <booktitle> Proc. ASPLOS II, </booktitle> <pages> pp. 10-20, </pages> <month> Oc-tober </month> <year> 1987. </year>
Reference-contexts: The bus is released by the master and is made available to other bus requesters. When the slave device is ready to respond to a request, it obtains bus mastership and transfers data to the requesting device. An STP bus is used in the Sequent Balance and Symmetry multiprocessors <ref> [11, 25] </ref>, and is also being considered for the IEEE hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh 3 More precisely, pipelined buses are only a restricted form of split transaction buses. With a pipelined bus the time between sending a request through the bus and receiving the corresponding reply from the bus is a constant.
Reference: [12] <author> Carl Josef Beckmann, </author> <title> ``Reducing Synchronization and Scheduling Overhead in Parallel Loops,'' </title> <type> CSRD Report Number 922, </type> <institution> Center for Supercomputing Research and Development, University of Illinois, Urbana, </institution> <address> IL 61801, </address> <month> October </month> <year> 1989. </year>
Reference-contexts: Note that the three benchmarks are chosen because in these benchmarks the amount of computation in a loop iteration is very small, and does not increase with the problem size. So they are more likely to benefit from the use of the GSS method <ref> [12, 54] </ref>. hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh 2 Assuming the critical section implements the simple-self-scheduling policy, i.e., allocating one loop iteration at a time. 129 I will first discuss the performance improvement resulting from using lock tables or restricted combining instead of test&test&set.
Reference: [13] <author> C. G. Bell, ``Multis: </author> <title> a New Class of Multiprocessor Computers,'' </title> <journal> Science, </journal> <volume> vol. 228, </volume> <pages> pp. 462-467, </pages> <month> April </month> <year> 1985. </year>
Reference-contexts: Introduction 1.1. Motivation for this Dissertation Low-cost microprocessors have led to the construction of small- to medium-scale shared memory multiprocessors with a shared bus interconnect. Such multiprocessors, which have been referred to as multis by Bell <ref> [13] </ref>, are popular for two reasons: i) the shared bus interconnect is easy to implement and ii) the shared bus interconnect allows an easy solution to the cache coherence problem [29]. Currently, many major computer manufacturers have commercial products that use the multi paradigm.
Reference: [14] <author> J. K. Bennett, J. B. Carter, and W. Zwaenepoel, </author> <title> ``Adaptive Software Cache Management for Distributed Shared Memory Architectures,'' </title> <booktitle> in Proceedings of the 17th International Symposium on Computer Architecture, </booktitle> <address> Seattle, Washington, </address> <pages> pp. 125-134, </pages> <month> May </month> <year> 1990. </year>
Reference-contexts: Therefore if we can distinguish between the shared data that are subjected to fine grain sharing from those that are not, one strategy is to use the right cache coherence protocol for each type of the shared data <ref> [14] </ref>. This has become the basic idea behind the competitive snooping [37], where a write broadcast protocol is switched to write-invalidate when the break even point in bus-related coherency traffic between the two protocols is reached.
Reference: [15] <author> Anita Borg, R. E. Kessler, and David W. Wall, </author> <title> ``Generation and Analysis of Very Long Address Traces,'' </title> <booktitle> Proceedings of the 17th Annual International Symposium on Computer Architecture, </booktitle> <pages> pp. 270-279, </pages> <year> 1990. </year>
Reference-contexts: For each benchmark, a memory trace is collected for 1 million instructions executed. During this period 1.3 to 1.6 million memory accesses are made by these programs. 21 The second set is the TITAN traces <ref> [15] </ref>. The traces are generated from Titan processors, which have a 32--bit, load/store (``RISC'') architecture designed by DEC. <p> The microcode based ATUM technique obviously can not be applied to RISC processors. Although the software method used in collecting the TITAN traces is capable of extracting memory trace from operating system functions, the traces currently available do not include any operating system activity <ref> [15] </ref>. Nevertheless the TITAN traces still represent the memory access behavior from a state-of-the-art, new generation RISC architecture. <p> The VAX traces contain totally about 2.6M memory accesses. The TITAN traces used are the same as used in <ref> [15] </ref>, and contain a total of more than 500M memory accesses. The biggest caches of which the performance is evaluated by the VAX traces are limited to 256K bytes. The TITAN traces, on the other hand, are used to evaluate the caches as large as 1M bytes.
Reference: [16] <author> P. Borrill and J. Theus, </author> <title> ``An Advanced Communication Protocol for the Proposed IEEE 896 Futurebus,'' </title> <booktitle> IEEE Micro, </booktitle> <pages> pp. 42-56, </pages> <month> August </month> <year> 1984. </year>
Reference-contexts: Such a switching strategy is used in most existing bus designs. For exam ple, the block read and block write transactions in the IEEE Futurebus employ a circuit switched protocol <ref> [16] </ref>. In an STP bus, the bus is not held by the master if the slave device is unable to respond to the request immediately. The bus is released by the master and is made available to other bus requesters.
Reference: [17] <author> L. M. Censier and P. Feautier, </author> <title> ``A New Solution to Coherence Problems in Multicache Systems,'' </title> <journal> IEEE Transactions on Computers, </journal> <volume> vol. C-27, </volume> <pages> pp. 1112-1118, </pages> <month> December </month> <year> 1978. </year>
Reference-contexts: Invalidation Pattern In this section the invalidation pattern of using different synchronization methods are studied. Analyzing the invalidation pattern is important in evaluating different directory schemes for large scale, non-bus based, cache coherent multiprocessors [4, 72]. Previous study of invalidation patterns uses either analytical modeling <ref> [17, 21] </ref>, or trace driven simulation [4, 72].
Reference: [18] <author> H. Cheong and A. V. Veidenbaum, </author> <title> ``A Cache Coherence Scheme with Fast Selective Invalidation,'' </title> <booktitle> in Proc. 15th Annual Symposium on Computer Architecture, </booktitle> <address> Honolulu, HI, </address> <pages> pp. 299-307, </pages> <month> June </month> <year> 1988. </year>
Reference-contexts: The performance of different cache coherence protocols was studied using analytical modeling [69, 70], trace driven simulation with real traces <ref> [4, 18, 23, 24] </ref>, or pseudo-traces [9]. Real parallel trace can either be derived from hardware monitoring [60], or software means such as the ptrace method.
Reference: [19] <author> Conte and De Boor, </author> <title> Elementary Numerical Analysis An Algorithmic Aprroach. </title> <address> New York: </address> <publisher> McGraw-Hill, </publisher> <year> 1980. </year>
Reference-contexts: In the parallel algorithm the forward elimination step is modified so that a diagonal instead of an upper triangular matrix is created at the end. This modification makes it easier to distribute computational work evenly to processors. The pivot strategy used in the algorithm is scaled partial pivoting <ref> [19] </ref>. The complete computation involves multiple sweeps of the matrix. Each sweep starts with a parallel phase, in which a weight array is calculated from the matrix A for the latter pivot selection. <p> Serial loop - Serial /* two arrays exchange roles; calculate some loop invariants */ Parallel Loop (SI) /* transformation of array */ Barrier - The input to the FFT (Fast Fourier Transform) problem is a motion in the time domain, and the output is the frequency content of the motion <ref> [19] </ref>. The input as well as the output is an array of numbers. The data in the input 84 array undergoes multiple steps of transformation to generate the output at the end.
Reference: [20] <author> R. Cytron, </author> <title> ``Doacross: Beyond Vectorization for Multiprocessors (Extended Abstract),'' </title> <booktitle> Proceedings of the 1986 International Conference on Parallel Processing, </booktitle> <pages> pp. 836-844, </pages> <month> August, </month> <year> 1986. </year> <month> 148 </month>
Reference-contexts: My thesis study is restricted to programs with this type of parallel loops. Parallel loops may also be the DOACROSS type, where dependency relationship exists between loop iterations <ref> [20] </ref>. 70 computation. The advantage of static scheduling is small scheduling overhead during run time, while dynamic scheduling offers better load balancing under different input conditions and uneven progress of processes created by the operating system scheduler [53].
Reference: [21] <author> M. Dubois and F. A. Briggs, </author> <title> ``Effects of Cache Coherence in Multiprocessors,'' </title> <journal> IEEE Trans. on Computers, </journal> <volume> vol. C-31, </volume> <pages> pp. 1083-1089, </pages> <month> November </month> <year> 1982. </year>
Reference-contexts: Invalidation Pattern In this section the invalidation pattern of using different synchronization methods are studied. Analyzing the invalidation pattern is important in evaluating different directory schemes for large scale, non-bus based, cache coherent multiprocessors [4, 72]. Previous study of invalidation patterns uses either analytical modeling <ref> [17, 21] </ref>, or trace driven simulation [4, 72].
Reference: [22] <author> S. J. Eggers and R. H. Katz, </author> <title> ``A Characterization of Sharing in Parallel Programs and its Application to Coherency Protocol Evalaution,'' </title> <booktitle> in Proc. 15th Annual Symposium on Computer Architecture, </booktitle> <address> Honolulu, HI, </address> <pages> pp. 373-382, </pages> <month> June </month> <year> 1988. </year>
Reference-contexts: Multiprocessors with arbitrary interconnection networks have been the subject of several previous studies [39, 45, 46, 49, 51]. Studies of bus-based multiprocessor design issues have used trace-driven simulation <ref> [22] </ref>, parameterized simulation [8], as well as analytical modeling [69, 70]. For a system as complex as a multi, ideally a system designer would like to use an accurate analytical model to explore the design space with a minimal computational requirement. <p> Real parallel trace can either be derived from hardware monitoring [60], or software means such as the ptrace method. The parallel trace itself was analyzed to evaluate the performance of cache snooping protocols <ref> [22] </ref>, or different design trades-off of the directory-based cache coherence scheme [72]. However, these methods are not always as effective as for uniprocessor systems. For example, the accuracy of these methods depends on the accuracy of the parallel traces, or the accurate characterization of the traces.
Reference: [23] <author> S. J. Eggers and R. H. Katz, </author> <title> ``Evaluating the Performance of Four Snooping Cache Coherence Proctcols,'' </title> <booktitle> Proc. of 16th Int'l Symp. Computer Architecture, </booktitle> <pages> pp. 2-15, </pages> <year> 1989. </year>
Reference-contexts: The performance of different cache coherence protocols was studied using analytical modeling [69, 70], trace driven simulation with real traces <ref> [4, 18, 23, 24] </ref>, or pseudo-traces [9]. Real parallel trace can either be derived from hardware monitoring [60], or software means such as the ptrace method.
Reference: [24] <author> S. J. Eggers and R. H. Katz, </author> <title> ``The Effect of Sharing on the Cache and Bus Performance of Parallel Programs,'' </title> <booktitle> in Proc. </booktitle> <address> ASPLOS-III, Boston, MA, </address> <pages> pp. 257-270, </pages> <month> April </month> <year> 1989. </year>
Reference-contexts: The performance of different cache coherence protocols was studied using analytical modeling [69, 70], trace driven simulation with real traces <ref> [4, 18, 23, 24] </ref>, or pseudo-traces [9]. Real parallel trace can either be derived from hardware monitoring [60], or software means such as the ptrace method.
Reference: [25] <author> G. N. Fielland, </author> <title> ``Symmetry: A Second Generation Practical Parallel,'' </title> <booktitle> Digest of Papers, COMPCON Spring 1988, </booktitle> <pages> pp. 114-115, </pages> <month> February </month> <year> 1988. </year>
Reference-contexts: The bus is released by the master and is made available to other bus requesters. When the slave device is ready to respond to a request, it obtains bus mastership and transfers data to the requesting device. An STP bus is used in the Sequent Balance and Symmetry multiprocessors <ref> [11, 25] </ref>, and is also being considered for the IEEE hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh 3 More precisely, pipelined buses are only a restricted form of split transaction buses. With a pipelined bus the time between sending a request through the bus and receiving the corresponding reply from the bus is a constant.
Reference: [26] <author> S. J. Frank, </author> <title> ``Tightly Coupled Multiprocessor System Speeds Memory-Access TImes,'' </title> <journal> Electronics, </journal> <volume> vol. 57, 1, </volume> <pages> pp. 164-169, </pages> <month> January </month> <year> 1984. </year>
Reference-contexts: But introducing private caches to a multiprocessor also creates the cache coherence problem. Many cache coherence protocols have been proposed, and the trade-off between complexity and performance under different workload conditions analyzed. For bus based multiprocessors a particular simple and efficient solution employs the snooping strategy <ref> [26, 29, 38, 47, 50, 66] </ref>. Bus traffic induced by cache coherence events can be substantial. The exact amount of coherence traffic generated is sensitive to the type of snooping protocols used, and the sharing patterns of the parallel workload.
Reference: [27] <author> Eric Freudenthal and Allan Gottlieb, </author> <title> ``Process Coordination with Fetch-and-Increment,'' </title> <booktitle> in ASPLOS-IV Proceedings, </booktitle> <address> Santa Clara, CA, </address> <pages> pp. 260-268, </pages> <month> April, </month> <year> 1991. </year>
Reference-contexts: Although it can be used like test&set to implement critical sections, it actually obviates the need of a critical section when the only operation in the critical section is a fetch& F. The restricted form of combining fetch& F has a surprisingly large number of applications <ref> [27] </ref>. For example, in the parallel benchmarks all operations in scheduling critical sections can be converted into the restricted 128 form of fetch&decrement 2 . Restricted combining also find its way into assisting the sequencing operation in the software queue method, improving the performance of large critical sections. 10.3.
Reference: [28] <author> Andy Glew and Wen-Mei Hwu, </author> <title> ``Snoopy Cache Test-and-Test-and-Set Without Excessive Bus Contention,'' </title> <journal> Computer Architecture News, </journal> <volume> vol. 18, </volume> <pages> pp. 25-32, </pages> <month> June </month> <year> 1990. </year>
Reference-contexts: Theoretically the bus traffic generated from scheduling one processor can be as high as O (p 2 ), p being the number of processors in the waiting queue. But with the FCFS bus arbitration policy, about O (p) of bus accesses are likely to occur <ref> [28] </ref>. Whether a queue will form or not depends on the ratio of the time to schedule a unit of work and the time to process it. The scheduled work is processed in the independent computing and communication phases. <p> Since processors waiting to enter the critical section can generate a lot of bus traffic (the amount of bus traffic generated by scheduling one loop iteration is proportional to the queue length. See Section 6.2.1) if the critical section is implemented with test&test&set <ref> [28] </ref>, it is not surprising to see in Table 8.1 and Figure 8.1 that for each benchmark the surge in total bus demand coincides with the configuration when the scheduling critical section starts, or is about to saturate, and also coincides with a relatively large, if not the largest increase in <p> Various hardware measures were proposed to minimize the number of these accesses. For example, write or read broadcast reduces the number of reads [37, 57], and test&set abandonment reduces the number of test&set bus requests <ref> [28] </ref>. With a combination of these measures the number of bus accesses generated by scheduling one processor can be reduced from O (p) to O (1), where p is the queue length of, or the number of processors waiting to access, the critical section. <p> The amount of bus traffic generated is proportional to the number of processors actively contending for the lock variable, or the queue length of the critical section <ref> [28] </ref>. To reduce the contention a processor can delay itself for a certain amount of time after each failed attempt, before rejoining the queue of the critical section, as in the collision avoidance method [6].
Reference: [29] <author> J. R. Goodman, </author> <title> ``Using Cache Memory to Reduce Processor-Memory Traffic,'' </title> <booktitle> Proc. 10th Annual Symposium on Computer Architecture, </booktitle> <pages> pp. 124-131, </pages> <month> June </month> <year> 1983. </year>
Reference-contexts: Such multiprocessors, which have been referred to as multis by Bell [13], are popular for two reasons: i) the shared bus interconnect is easy to implement and ii) the shared bus interconnect allows an easy solution to the cache coherence problem <ref> [29] </ref>. Currently, many major computer manufacturers have commercial products that use the multi paradigm. A typical shared bus, shared memory multiprocessor (hereafter called a multi in this thesis) is shown in Figure 1. The multi consists of several processors (typically microprocessors) connected together to a memory system. <p> However, larger block sizes also cause more traffic on the cache-memory interconnect. Since this additional traffic uses more bus bandwidth and since the bandwidth of the shared bus is the critical resource in a multi, Goodman has suggested that small block sizes are preferable for cache memories in multis <ref> [29] </ref>. As Smith points out, minimizing the bus traffic alone is not the correct optimization procedure in multiprocessors and neither is a minimization of the miss ratio, independent of the other parameters of the memory system [62]. <p> But introducing private caches to a multiprocessor also creates the cache coherence problem. Many cache coherence protocols have been proposed, and the trade-off between complexity and performance under different workload conditions analyzed. For bus based multiprocessors a particular simple and efficient solution employs the snooping strategy <ref> [26, 29, 38, 47, 50, 66] </ref>. Bus traffic induced by cache coherence events can be substantial. The exact amount of coherence traffic generated is sensitive to the type of snooping protocols used, and the sharing patterns of the parallel workload. <p> Direct Implementation of Critical Section: Lock Table 10.2.3.1. Motivation and Function Description Cache coherence protocols can be broadly divided into two categories: write broadcast [47, 66], and invalidation protocols <ref> [29, 38, 50] </ref>. Write broadcast cache coherence protocols have been found to be superior to write invalidation protocols [69, 70]. In these papers analytical models are used and the probabilistic input dictates that all processors to have equal access to the shared data.
Reference: [30] <author> J. R. Goodman and P. J. Woest, </author> <title> ``The Wisconsin Multicube: A New Large-Scale Cache-Coherent Multiprocessor,'' </title> <booktitle> in Proc. 15th Annual Symposium on Computer Architecture, </booktitle> <address> Honolulu, HI, </address> <pages> pp. 422-431, </pages> <month> June </month> <year> 1988. </year>
Reference-contexts: The primitive enqueues a processor to a lock variable (SyncBit), which is part of a cache line, to enforce a first-come first-serve discipline for the lock variable (and its associated critical section). The primitive was originally proposed for a large scale, cache-coherent shared memory multiprocessor Multicube <ref> [30] </ref>, though it can be adapted to a simpler bus based multiprocessor. The QOSB mechanism has inspired some variations. A similar but more complex hardware queueing mechanism is proposed to further implement non-exclusive critical sections by adding more cache line states [41].
Reference: [31] <author> J. R. Goodman, M. K. Vernon, and P. J. Woest, </author> <title> ``A Set of Efficient Synchronization Primitives for a Large-Scale Shared-Memory Multiprocessor,'' </title> <booktitle> in Proc. </booktitle> <address> ASPLOS-III, Boston, MA, </address> <month> April </month> <year> 1989. </year>
Reference-contexts: A simpler multiprocessor, in terms of both hardware and system software, can be built upon the lock table mechanism. 10.2.3.4.2. Queue_on_SyncBit (QOSB) QOSB is a synchronization primitive that can be used to implement critical sections <ref> [31] </ref>. The primitive enqueues a processor to a lock variable (SyncBit), which is part of a cache line, to enforce a first-come first-serve discipline for the lock variable (and its associated critical section).
Reference: [32] <author> Gary Graunke and Shreekant Thakkar, </author> <title> ``Synchronization Algorithms for Shared-Memory Multiprocessors,'' </title> <journal> Computer, </journal> <volume> vol. 23, </volume> <pages> pp. 60-69, </pages> <month> June </month> <year> 1990. </year>
Reference-contexts: These methods can also be used to evaluate the performance of multiprocessors. For example, the performance of different software synchronization algorithms was measured on a Sequent Symmetry multiprocessor <ref> [32] </ref>, and the performance of different cache organizations on different models of Symmetry multiprocessors [7]. The performance of different cache coherence protocols was studied using analytical modeling [69, 70], trace driven simulation with real traces [4, 18, 23, 24], or pseudo-traces [9]. <p> The potential contention in a single critical section can be spread to multiple critical sections by creating a hierarchy (a tree) of lock variables, as in the tournament locks method <ref> [32] </ref>. But the most promising has been the software queue method [6, 32, 48]. The software queue method enqueues all processors that try to enter the same critical section, with the head of the queue being the processor that is currently in the critical section. <p> The potential contention in a single critical section can be spread to multiple critical sections by creating a hierarchy (a tree) of lock variables, as in the tournament locks method [32]. But the most promising has been the software queue method <ref> [6, 32, 48] </ref>. The software queue method enqueues all processors that try to enter the same critical section, with the head of the queue being the processor that is currently in the critical section. <p> It is apparent from these figures that, if TTS is the only available synchronization mechanism, the software queue scheduling method always performs better, when many processors are used. This result is similar to what was reported in <ref> [6, 32] </ref>. The result here also shows that the performance can be further improved if the sequencing critical section is supported by the restricted combining mechanism.
Reference: [33] <author> Rajiv Gupta, </author> <title> ``The Fuzzy Barrier: A Mechanism for High Speed Synchronization of Processors,'' </title> <booktitle> in Proc. ASPLOS IV, </booktitle> <address> Santa Clara, CA, </address> <month> April </month> <year> 1991. </year>
Reference-contexts: A compiler can schedule a balanced workload to each processor, while the operation system can adjust the run time process scheduling policy to minimize the process waiting time [75]. Even the synchronization mechanism can be redesign to tolerate the difference in processor arrival times of a synchronization event <ref> [33] </ref>. Synchronization can also create a unique memory access problem called hot-spot [52]. The problem is effectively dealt with by the combing strategy [52, 63, 74]. 5.2.2.
Reference: [34] <author> M. D. Hill, </author> <title> ``Aspects of Cache Memory and Instruction Buffer Performance,'' </title> <type> Technical Report UCB/CSD 87/381, </type> <institution> University of California at Berkeley, Berkeley, </institution> <address> CA, </address> <month> November </month> <year> 1987. </year>
Reference-contexts: If M is sufficiently small (because of a large cache size, for example), the decrease in T C P due to a lower set associativity can easily overcome an increase in MT m C <ref> [34, 35, 56] </ref>. In a multiprocessor, however, T m C contains a queuing delay, which can be a large fraction of T m C if the bus utilization is high. <p> The ATUM traces that we use are gathered via microcode patches on a VAX 8200 by Agarwal and Sites. These traces are distributed by DEC, are considered to be the best public-domain traces for a multiprogrammed, multi-user environment, and they have been widely used in recent cache studies <ref> [3, 5, 34, 35, 56] </ref>. By passing the ATUM traces through a uniprocessor cache simulator we obtain the values of M, 37 M ref , P r , P w and P v . The microcode based ATUM technique obviously can not be applied to RISC processors. <p> To account for the impact of set associativity on processor throughput, T C P of caches with set associativities of 2, 4 and 8 is 10% greater than T C P for a direct mapped cache <ref> [34] </ref>. Two trends are obvious from Figure 4.3. The first trend is that as cache size increases, the block size that results in the best uniprocessor throughput increases. Furthermore, the throughput tends to ``flatten'' out, indicating that several block sizes may give roughly the same performance. <p> These data suggest that 2-way or 4-way set associativity may be warranted in a multi even when the cache size is fairly large (256K bytes). This is unlike uniprocessor caches where the need for set associativity diminishes significantly as the cache size increases <ref> [34, 35, 56] </ref>. The reason why a larger associativity is favored for the multiprocessor caches is due to the fact that caches with a larger associativity lower the miss ratio as well as the per-processor utilization of the shared bus.
Reference: [35] <author> M. D. Hill, </author> <title> ``A Case for Direct-Mapped Caches,'' </title> <journal> IEEE Computer, </journal> <volume> vol. 21, </volume> <pages> pp. 25-40, </pages> <month> December </month> <year> 1988. </year>
Reference-contexts: If M is sufficiently small (because of a large cache size, for example), the decrease in T C P due to a lower set associativity can easily overcome an increase in MT m C <ref> [34, 35, 56] </ref>. In a multiprocessor, however, T m C contains a queuing delay, which can be a large fraction of T m C if the bus utilization is high. <p> The ATUM traces that we use are gathered via microcode patches on a VAX 8200 by Agarwal and Sites. These traces are distributed by DEC, are considered to be the best public-domain traces for a multiprogrammed, multi-user environment, and they have been widely used in recent cache studies <ref> [3, 5, 34, 35, 56] </ref>. By passing the ATUM traces through a uniprocessor cache simulator we obtain the values of M, 37 M ref , P r , P w and P v . The microcode based ATUM technique obviously can not be applied to RISC processors. <p> Both trends apparent in Figure 4.3 are well known and have been described in detail in the literature on uniprocessor caches <ref> [35, 62] </ref>. The purpose of presenting them here is again to show that neither trend may occur for multiprocessor caches, to be discussed in the upcoming sections. 4.2.2. <p> These data suggest that 2-way or 4-way set associativity may be warranted in a multi even when the cache size is fairly large (256K bytes). This is unlike uniprocessor caches where the need for set associativity diminishes significantly as the cache size increases <ref> [34, 35, 56] </ref>. The reason why a larger associativity is favored for the multiprocessor caches is due to the fact that caches with a larger associativity lower the miss ratio as well as the per-processor utilization of the shared bus.
Reference: [36] <author> R. Jog, G. S. Sohi, and M. K. Vernon, </author> <title> ``The TREEBus Architecture and Its Analysis,'' </title> <type> Computer Sciences Technical Report #747, </type> <institution> University of Wisconsin-Madison, Madison, WI 53706, </institution> <month> February </month> <year> 1988. </year>
Reference-contexts: Both analytical modeling as well as actual trace-driven simulation will be used in this study. The analytical models are based on a ``customized'' mean value analysis technique that has been proposed in [70] and applied in <ref> [36, 43, 71] </ref>. Trace-driven simulation is used to study a few thousand cases and, more importantly, build confidence in the analytical models. Once the validity of the analytical models has been established, the models will be used to evaluate the design choices in the next chapter. 3.2. <p> Once the validity of the analytical models has been established, the models will be used to evaluate the design choices in the next chapter. 3.2. Customized Mean Value Analysis (CMVA) The CMVA models build on similar models developed to study bus-based multiprocessors <ref> [36, 43, 70, 71] </ref>. The CMVA method is appealing because it is simple and intuitive.
Reference: [37] <author> A. Karlin and et al, </author> <title> ``Competitive Snoopy Caching,'' </title> <booktitle> Proc. of 27th Ann. Symp. Foundations of Computer Science, </booktitle> <pages> pp. 244-254, </pages> <year> 1986. </year>
Reference-contexts: Various hardware measures were proposed to minimize the number of these accesses. For example, write or read broadcast reduces the number of reads <ref> [37, 57] </ref>, and test&set abandonment reduces the number of test&set bus requests [28]. <p> Therefore if we can distinguish between the shared data that are subjected to fine grain sharing from those that are not, one strategy is to use the right cache coherence protocol for each type of the shared data [14]. This has become the basic idea behind the competitive snooping <ref> [37] </ref>, where a write broadcast protocol is switched to write-invalidate when the break even point in bus-related coherency traffic between the two protocols is reached. In this method the sharing pattern of a data is dynamically determined during the run time of a program.
Reference: [38] <author> R. H. Katz, S. J. Eggers, D. A. Wood, C. L. Perkins, and R. G. Sheldon, </author> <title> ``Implementing a Cache Consistency Protocol,'' </title> <booktitle> Proc. 12th Annual Symposium on Computer Architecture, </booktitle> <pages> pp. 276-283, </pages> <month> June </month> <year> 1985. </year>
Reference-contexts: Each bus trace record stores the time of generation (still an ideal time) and the type of the bus request. The Berkeley Ownership protocol is used for generating the invalidation requests <ref> [38] </ref>, though any other protocol could be used in a straightforward manner. Bus traces from several benchmark programs are then used to drive a bus simulator which simulates the operation of the shared bus and the main memory. <p> But introducing private caches to a multiprocessor also creates the cache coherence problem. Many cache coherence protocols have been proposed, and the trade-off between complexity and performance under different workload conditions analyzed. For bus based multiprocessors a particular simple and efficient solution employs the snooping strategy <ref> [26, 29, 38, 47, 50, 66] </ref>. Bus traffic induced by cache coherence events can be substantial. The exact amount of coherence traffic generated is sensitive to the type of snooping protocols used, and the sharing patterns of the parallel workload. <p> Direct Implementation of Critical Section: Lock Table 10.2.3.1. Motivation and Function Description Cache coherence protocols can be broadly divided into two categories: write broadcast [47, 66], and invalidation protocols <ref> [29, 38, 50] </ref>. Write broadcast cache coherence protocols have been found to be superior to write invalidation protocols [69, 70]. In these papers analytical models are used and the probabilistic input dictates that all processors to have equal access to the shared data.
Reference: [39] <author> T. Lang, M. Valero, and I. </author> <title> Alegre, ``Bandwidth of Crossbar and Multiple-Bus Connections for Multiprocessors,'' </title> <journal> IEEE Transactions on Computers, </journal> <volume> vol. C-31, </volume> <pages> pp. 1227-1234, </pages> <month> December </month> <year> 1982. </year>
Reference-contexts: Finally, one can develop an analytical model. Analytical models generally are much cheaper computationally than trace-driven simulation and consequently allow the designer to explore a much larger design space. Multiprocessors with arbitrary interconnection networks have been the subject of several previous studies <ref> [39, 45, 46, 49, 51] </ref>. Studies of bus-based multiprocessor design issues have used trace-driven simulation [22], parameterized simulation [8], as well as analytical modeling [69, 70].
Reference: [40] <author> E. D. Lazowska, J. Zahorjan, G. S. Graham, and K. C. Sevcik, </author> <title> Quantitative System Performance, Computer System Analysis Using Queueing Network Models. </title> <address> Englewood Cliffs, New Jersey: </address> <publisher> Prentice Hall, </publisher> <year> 1984. </year>
Reference-contexts: hhhhhhh T a + T xo ; x = r, v T w = (1 - NU - U hhhhhhhh ) T a + T wo = 1 - U 1 NU hhhhhhh T a + T wo Waiting Time Equations Using the mean value technique for queuing network models <ref> [40] </ref>, The waiting time of an arriving request is decomposed into three components based on the types of the requests that delay the service of the new request. <p> rv where K rv I (Q x - B x ) T x + B x Re x M ; x = r, v w = N L hh O The residual service time for the request that is being serviced when a new read or invalidation request arrives is <ref> [40] </ref>: 2 The probabilities that the bus is busy servicing the request from a particular cache when a new read or invalidation request arrives can be approximated as: B x = U x hhhhhhh ; x = r, v, w where 15 U x = P x T x hhhhh ;
Reference: [41] <author> J. Lee and U. Ramachandran, </author> <title> ``Synchronization with Multiprocessor Caches,'' </title> <booktitle> in Proceedings of the 17th International Symposium on Computer Architecture, </booktitle> <address> Seattle, Washington, </address> <pages> pp. 27-37, </pages> <month> May </month> <year> 1990. </year>
Reference-contexts: The QOSB mechanism has inspired some variations. A similar but more complex hardware queueing mechanism is proposed to further implement non-exclusive critical sections by adding more cache line states <ref> [41] </ref>. The hardware queueing mechanism can also be simulated by software. The SyncBits in cache lines can be simulated by an array of lock variables [6], or a software queue data structure [48].
Reference: [42] <author> R. De Leone and O. L. Mangasarian, </author> <title> ``Asynchronous Parallel Successive Overrelaxation For The Symmetric Linear Complementarity Problem,'' </title> <type> Computer Sciences Technical Report #755, </type> <institution> University of Wisconsin-Madison, Madison, WI 53706, </institution> <month> Feb. </month> <year> 1988. </year> <month> 149 </month>
Reference-contexts: loop invariants */ Parallel Loop (SIM) /* update the array z */ Barrier - Serial /* check to see if further relaxation is necessary */ - The SOR (Successive Over-Relaxation) method is used to solve the symmetric linear complementarity problem A particular version of asynchronous SOR is used for analysis <ref> [42] </ref>. Matrix M is a read-only sparse matrix. Elements of the matrix are compressed and stored in an array, with separate arrays to store the row and column indices.
Reference: [43] <author> S. Leutenegger and M. K. Vernon, </author> <title> ``A Mean-Value Performance Analysis of a New Multiprocessor Archi--tecture,'' </title> <booktitle> Proc. ACM SIGMETRICS Conference on Measurement and Modeling of Computer Systems, </booktitle> <month> May </month> <year> 1988. </year>
Reference-contexts: Both analytical modeling as well as actual trace-driven simulation will be used in this study. The analytical models are based on a ``customized'' mean value analysis technique that has been proposed in [70] and applied in <ref> [36, 43, 71] </ref>. Trace-driven simulation is used to study a few thousand cases and, more importantly, build confidence in the analytical models. Once the validity of the analytical models has been established, the models will be used to evaluate the design choices in the next chapter. 3.2. <p> Once the validity of the analytical models has been established, the models will be used to evaluate the design choices in the next chapter. 3.2. Customized Mean Value Analysis (CMVA) The CMVA models build on similar models developed to study bus-based multiprocessors <ref> [36, 43, 70, 71] </ref>. The CMVA method is appealing because it is simple and intuitive.
Reference: [44] <author> D. Lilja, D. Marcovitz, and P.-C. Yew, </author> <title> ``Memory Reference Behavior and Cache Performance in a Shared Memory Multiprocessor,'' </title> <type> CSRD Report Number 836, </type> <institution> Center for Supercomputing Research and Development, University of Illinois, Urbana, </institution> <address> IL 61801-2932, </address> <month> December </month> <year> 1988. </year>
Reference-contexts: The value of M depends on the number of processors on which the parallel program is executing, as a result of a phenomenon called reference spreading <ref> [44] </ref>. Consequently, a larger cache may not necessarily be able to lower the value of M. <p> Reference Spreading Reference spreading is related to shared read accesses <ref> [44] </ref>. When a loop is parallelized, each processor executes some iterations of the loop. Certain shared data, mainly loop invariants, may be read in all or some iterations before the data is updated again.
Reference: [45] <author> M. A. Marsan and M. Gerla, </author> <title> ``Markov Models for Multiple-Bus Multiprocessor Systems,'' </title> <journal> IEEE Transactions on Computers, </journal> <volume> vol. C-31, </volume> <pages> pp. 239-248, </pages> <month> December </month> <year> 1982. </year>
Reference-contexts: Finally, one can develop an analytical model. Analytical models generally are much cheaper computationally than trace-driven simulation and consequently allow the designer to explore a much larger design space. Multiprocessors with arbitrary interconnection networks have been the subject of several previous studies <ref> [39, 45, 46, 49, 51] </ref>. Studies of bus-based multiprocessor design issues have used trace-driven simulation [22], parameterized simulation [8], as well as analytical modeling [69, 70].
Reference: [46] <author> M. A. Marsan, G. Balbo, G. Conte, and F. Gregoretti, </author> <title> ``Modeling Bus Contention and Memory Interference in a Multiprocessor System,'' </title> <journal> IEEE Transactions on Computers, </journal> <volume> vol. C-32, </volume> <pages> pp. 60-72, </pages> <month> January </month> <year> 1983. </year>
Reference-contexts: Finally, one can develop an analytical model. Analytical models generally are much cheaper computationally than trace-driven simulation and consequently allow the designer to explore a much larger design space. Multiprocessors with arbitrary interconnection networks have been the subject of several previous studies <ref> [39, 45, 46, 49, 51] </ref>. Studies of bus-based multiprocessor design issues have used trace-driven simulation [22], parameterized simulation [8], as well as analytical modeling [69, 70].
Reference: [47] <author> E. McCreight, </author> <title> ``The DRAGON Computer System: An Early Overview,'' </title> <booktitle> in NATO Advanced Study Institute on Microarchitecture of VLSI Computer, </booktitle> <address> Urbino, Italy, </address> <month> July </month> <year> 1984. </year>
Reference-contexts: But introducing private caches to a multiprocessor also creates the cache coherence problem. Many cache coherence protocols have been proposed, and the trade-off between complexity and performance under different workload conditions analyzed. For bus based multiprocessors a particular simple and efficient solution employs the snooping strategy <ref> [26, 29, 38, 47, 50, 66] </ref>. Bus traffic induced by cache coherence events can be substantial. The exact amount of coherence traffic generated is sensitive to the type of snooping protocols used, and the sharing patterns of the parallel workload. <p> Direct Implementation of Critical Section: Lock Table 10.2.3.1. Motivation and Function Description Cache coherence protocols can be broadly divided into two categories: write broadcast <ref> [47, 66] </ref>, and invalidation protocols [29, 38, 50]. Write broadcast cache coherence protocols have been found to be superior to write invalidation protocols [69, 70]. In these papers analytical models are used and the probabilistic input dictates that all processors to have equal access to the shared data.
Reference: [48] <author> J. M. Mellor-Crummey and M. L. Scott, </author> <title> ``Algorithmms for Scalable Synchronization on Shared-Memory Multiprocessors,'' </title> <journal> ACM Transactions on Computers Systems, </journal> <volume> vol. 9, </volume> <pages> pp. 21-65, </pages> <month> February </month> <year> 1991. </year>
Reference-contexts: The potential contention in a single critical section can be spread to multiple critical sections by creating a hierarchy (a tree) of lock variables, as in the tournament locks method [32]. But the most promising has been the software queue method <ref> [6, 32, 48] </ref>. The software queue method enqueues all processors that try to enter the same critical section, with the head of the queue being the processor that is currently in the critical section. <p> The hardware queueing mechanism can also be simulated by software. The SyncBits in cache lines can be simulated by an array of lock variables [6], or a software queue data structure <ref> [48] </ref>. The major advantage of these software derivatives is that they can run on the existing multiprocessors which only support the simpler primitive of test&set and compare&swap, and immediately improve the performance of synchronization.
Reference: [49] <author> T. N. Mudge, J. P. Hayes, G. D. Buzzard, and D. C. </author> <title> Windsor, ``Analysis of Multiple Bus Interconnection Networks,'' </title> <booktitle> Proc. 1984 International Conference on Parallel Processing, </booktitle> <pages> pp. 228-232, </pages> <month> August </month> <year> 1984. </year>
Reference-contexts: Finally, one can develop an analytical model. Analytical models generally are much cheaper computationally than trace-driven simulation and consequently allow the designer to explore a much larger design space. Multiprocessors with arbitrary interconnection networks have been the subject of several previous studies <ref> [39, 45, 46, 49, 51] </ref>. Studies of bus-based multiprocessor design issues have used trace-driven simulation [22], parameterized simulation [8], as well as analytical modeling [69, 70].
Reference: [50] <author> M. S. Papamarcos and J. H. Patel, </author> <title> ``A Low-Overhead Coherence Solution for Multiprocessors with Private Cache Memories,'' </title> <booktitle> Proc. 11th Annual Symposium on Computer Architecture, </booktitle> <pages> pp. 348-354, </pages> <month> June </month> <year> 1984. </year>
Reference-contexts: But introducing private caches to a multiprocessor also creates the cache coherence problem. Many cache coherence protocols have been proposed, and the trade-off between complexity and performance under different workload conditions analyzed. For bus based multiprocessors a particular simple and efficient solution employs the snooping strategy <ref> [26, 29, 38, 47, 50, 66] </ref>. Bus traffic induced by cache coherence events can be substantial. The exact amount of coherence traffic generated is sensitive to the type of snooping protocols used, and the sharing patterns of the parallel workload. <p> The memory access pattern is strongly related to the cache coherence protocol used. In the following discussion an invalidation based cache coherence protocol is always assumed. The protocol is similar to Illinois Protocol and has been use by Symmetry machines <ref> [50] </ref>. The protocol employs four cache block states: readonly-private, readonly-shared, dirty and invalid. More detail about the protocol can be seen when the multiprocessor simulator is discussed in the next chapter. 6.1.1. Definitions Before going further a few terms that may have different meanings in other places are defined. <p> Cache The private cache uses an ownership based, invalidation type of cache coherence protocol that is similar to the Illinois protocol <ref> [50] </ref>, and the one used in Symmetry. A cache block can be in one of the four states: invalid, readonly-private, readonly-shared and dirty. <p> Direct Implementation of Critical Section: Lock Table 10.2.3.1. Motivation and Function Description Cache coherence protocols can be broadly divided into two categories: write broadcast [47, 66], and invalidation protocols <ref> [29, 38, 50] </ref>. Write broadcast cache coherence protocols have been found to be superior to write invalidation protocols [69, 70]. In these papers analytical models are used and the probabilistic input dictates that all processors to have equal access to the shared data.
Reference: [51] <author> J. H. Patel, </author> <title> ``Analysis of Multiprocessors With Private Cache Memories,'' </title> <journal> IEEE Transactions on Computers, </journal> <volume> vol. C-31, </volume> <pages> pp. 296-304, </pages> <month> April </month> <year> 1982. </year>
Reference-contexts: Finally, one can develop an analytical model. Analytical models generally are much cheaper computationally than trace-driven simulation and consequently allow the designer to explore a much larger design space. Multiprocessors with arbitrary interconnection networks have been the subject of several previous studies <ref> [39, 45, 46, 49, 51] </ref>. Studies of bus-based multiprocessor design issues have used trace-driven simulation [22], parameterized simulation [8], as well as analytical modeling [69, 70].
Reference: [52] <author> G. F. Pfister and V. A. Norton, </author> <title> ``'Hot-Spot' Contention and Combining in Multistage Interconnection Networks,'' </title> <journal> IEEE Transactions on Computers, </journal> <volume> vol. C-34, </volume> <pages> pp. 943-948, </pages> <month> October </month> <year> 1985. </year>
Reference-contexts: Even the synchronization mechanism can be redesign to tolerate the difference in processor arrival times of a synchronization event [33]. Synchronization can also create a unique memory access problem called hot-spot <ref> [52] </ref>. The problem is effectively dealt with by the combing strategy [52, 63, 74]. 5.2.2. Memory Access Pattern Except for the added synchronization code, the total number of instructions executed and memory accesses made by a program should not change too much by parallelizing the program. <p> Even the synchronization mechanism can be redesign to tolerate the difference in processor arrival times of a synchronization event [33]. Synchronization can also create a unique memory access problem called hot-spot [52]. The problem is effectively dealt with by the combing strategy <ref> [52, 63, 74] </ref>. 5.2.2. Memory Access Pattern Except for the added synchronization code, the total number of instructions executed and memory accesses made by a program should not change too much by parallelizing the program. However, distributing computational work among multiple processors can drastically change the memory access pattern.
Reference: [53] <author> C. C. Polychronopoulos, </author> <title> ``On Program Restructuring, Scheduling and Communication for Parallel Processor Systems,'' in Ph. </title> <address> D. </address> <institution> dissertation, Center for Supercomputing Research and Development, University of Illi-nois at Urbana-Champaign, </institution> <month> August </month> <year> 1986. </year>
Reference-contexts: If static scheduling is used, a fixed set of loop iterations is assinged to each processor before execution of the loop. The alternative is dynamical scheduling, which comes in various complexity and costs <ref> [53, 64, 65] </ref>. <p> The advantage of static scheduling is small scheduling overhead during run time, while dynamic scheduling offers better load balancing under different input conditions and uneven progress of processes created by the operating system scheduler <ref> [53] </ref>. The choice of a scheduling strategy can also affect the execution of other computational phases. The exact effect will be discussed when the relevant computational phases are encountered. Here only the scheduling overhead of dynamic scheduling is analyzed.
Reference: [54] <author> C. D. Polychronopoulos and D. J. Kuck, </author> <title> ``Guided Self-Scheduling: A Practical Scheduling Scheme for Parallel Supercomputers,'' </title> <journal> IEEE Transaction on Computer, </journal> <month> December </month> <year> 1987. </year>
Reference-contexts: In the new version the Simple-Self Scheduling strategy, which schedules one loop iteration at a time, is changed to Guided-Self-Scheduling (GSS) <ref> [54] </ref>, which uses heuristics to schedule multiple loop iterations at a time. The program code with the GSS scheduling critical section is then modified so that a software queue can be generated at run time. <p> Note that the three benchmarks are chosen because in these benchmarks the amount of computation in a loop iteration is very small, and does not increase with the problem size. So they are more likely to benefit from the use of the GSS method <ref> [12, 54] </ref>. hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh 2 Assuming the critical section implements the simple-self-scheduling policy, i.e., allocating one loop iteration at a time. 129 I will first discuss the performance improvement resulting from using lock tables or restricted combining instead of test&test&set.
Reference: [55] <author> Richard D. Pribnow, </author> <title> ``System For Multiprocessor Communication Using Local and Common Semaphore and Information Register,'' </title> <type> U.S. Patent No. </type> <institution> 4,754,398, </institution> <month> June </month> <year> 1985. </year>
Reference-contexts: Comparisons to Other Synchronization Schemes 10.2.3.4.1. Semaphore Registers The idea of using special hardware to assist the critical section operation is also used in the semaphore register method <ref> [55] </ref>. The differences between the lock tables and semaphore registers are in many ways similar to those between the ordinary data caches and data registers.
Reference: [56] <author> S. Przybylski, M. Horowitz, and J. Hennessy, </author> <title> ``Performance Tradeoffs in Cache Design,'' </title> <booktitle> Proc. 15th Annual Symposium on Computer Architecture, </booktitle> <pages> pp. 290-298, </pages> <month> June </month> <year> 1988. </year>
Reference-contexts: If M is sufficiently small (because of a large cache size, for example), the decrease in T C P due to a lower set associativity can easily overcome an increase in MT m C <ref> [34, 35, 56] </ref>. In a multiprocessor, however, T m C contains a queuing delay, which can be a large fraction of T m C if the bus utilization is high. <p> The ATUM traces that we use are gathered via microcode patches on a VAX 8200 by Agarwal and Sites. These traces are distributed by DEC, are considered to be the best public-domain traces for a multiprogrammed, multi-user environment, and they have been widely used in recent cache studies <ref> [3, 5, 34, 35, 56] </ref>. By passing the ATUM traces through a uniprocessor cache simulator we obtain the values of M, 37 M ref , P r , P w and P v . The microcode based ATUM technique obviously can not be applied to RISC processors. <p> These data suggest that 2-way or 4-way set associativity may be warranted in a multi even when the cache size is fairly large (256K bytes). This is unlike uniprocessor caches where the need for set associativity diminishes significantly as the cache size increases <ref> [34, 35, 56] </ref>. The reason why a larger associativity is favored for the multiprocessor caches is due to the fact that caches with a larger associativity lower the miss ratio as well as the per-processor utilization of the shared bus.
Reference: [57] <author> L. Rudolph and Z. Segall, </author> <title> ``Dynamic Decentralized Cache Schemes for MIMD Parallel Processors,'' </title> <booktitle> Proc. 11th Annual Symposium on Computer Architecture, </booktitle> <pages> pp. 340-347, </pages> <month> June </month> <year> 1984. </year>
Reference-contexts: Various hardware measures were proposed to minimize the number of these accesses. For example, write or read broadcast reduces the number of reads <ref> [37, 57] </ref>, and test&set abandonment reduces the number of test&set bus requests [28].
Reference: [58] <author> Michael L. Scott and John M. Mellor-Crummey, </author> <title> ``Synchronization without Contention,'' </title> <booktitle> in Proc. ASPLOS IV, </booktitle> <address> Santa Clara, CA, </address> <month> April </month> <year> 1991. </year>
Reference-contexts: For example, to reduce the synchronization traffic, test&test&set has 64 been introduced to reduce the number of test&set memory accesses generated by waiting processors. The Software Queue method further cuts down the amount of unnecessary synchronization traffic <ref> [6, 58] </ref>. Reducing the processor waiting time during a synchronization event is more difficult, and may require the cooperation from the compiler and the operating system.
Reference: [59] <author> Y. Shiloach and U. Vishkin, </author> <title> ``An O(log n) parallel connectivity algorithm,'' </title> <editor> J. </editor> <booktitle> Algorithms, </booktitle> <pages> pp. 57-63, </pages> <year> 1982. </year>
Reference-contexts: */ Barrier Parallel Loop (I) /* if a hooking is possible, determine where to hook */ Barrier Parallel Loop (SI) /* actual hooking of two trees */ Barrier - TREE derives an unrooted spanning tree from an undirected connected graph by using a modification of the Shiloach-Vishkin connected components algorithm <ref> [59] </ref>. Finding an arbitrary or qualified spanning tree from a graph is a common step in many graph algorithms. The serial algorithm visits each node and edge in turn and takes O (N) time.
Reference: [60] <author> R. L. Sites and A. Agarwal, </author> <title> ``Multiproccessor Cache Analysis Using ATUM,'' </title> <booktitle> in Proc. 15th Annual Symposium on Computer Architecture, </booktitle> <address> Honolulu, HI, </address> <pages> pp. 186-195, </pages> <month> June </month> <year> 1988. </year>
Reference-contexts: The performance of different cache coherence protocols was studied using analytical modeling [69, 70], trace driven simulation with real traces [4, 18, 23, 24], or pseudo-traces [9]. Real parallel trace can either be derived from hardware monitoring <ref> [60] </ref>, or software means such as the ptrace method. The parallel trace itself was analyzed to evaluate the performance of cache snooping protocols [22], or different design trades-off of the directory-based cache coherence scheme [72]. However, these methods are not always as effective as for uniprocessor systems.
Reference: [61] <author> A. J. Smith, </author> <title> ``Cache Memories,'' </title> <journal> ACM Computing Surveys, </journal> <volume> vol. 14, </volume> <pages> pp. 473-530, </pages> <month> Sept. </month> <year> 1982. </year>
Reference-contexts: Cache set associativity is also an important design consideration. It is well known that a larger set associativity reduces M (in most cases) and, if T C P and T m C are constant, a larger set associativity is preferable, subject to implementation constraints <ref> [61] </ref>. However, as several researchers have observed, T C P is not independent of the cache set associativity since a large set associativity requires a more complex implementation and consequently has a higher T C P .
Reference: [62] <author> A. J. Smith, </author> <title> ``Line (Block) Size Choice for CPU Cache Memories,'' </title> <journal> IEEE Transactions on Computers, </journal> <volume> vol. C-36, </volume> <pages> pp. 1063-1075, </pages> <month> September </month> <year> 1987. </year>
Reference-contexts: In a uniprocessor, T m C can be approximated as T m C = a + b . B, where B is the cache block size and a and b are constants that represent the fixed overhead and the unit transfer cost of transferring a cache block <ref> [62] </ref>. In a multi, T m C cannot be approximated simply as T m C = a + b . B. This is because T m C includes a queuing delay that can have a significant overall contribution to T m P . <p> In a detailed study, Smith mentions several architectural factors that influence the choice of a block size and evaluates them in a uniprocessor environment <ref> [62] </ref>. Smith's main result of interest to us is that the miss ratio decreases with increasing block size up to a point at which the internal cache interference increases the miss ratio. However, larger block sizes also cause more traffic on the cache-memory interconnect. <p> As Smith points out, minimizing the bus traffic alone is not the correct optimization procedure in multiprocessors and neither is a minimization of the miss ratio, independent of the other parameters of the memory system <ref> [62] </ref>. This point, which is also apparent from equation (2.2), is central to the performance study of multiprocessors and cannot be overemphasized. <p> Both trends apparent in Figure 4.3 are well known and have been described in detail in the literature on uniprocessor caches <ref> [35, 62] </ref>. The purpose of presenting them here is again to show that neither trend may occur for multiprocessor caches, to be discussed in the upcoming sections. 4.2.2.
Reference: [63] <author> Gurindar S. Sohi, James E. Smith, and James R. Goodman, </author> <title> ``Restricted Fetch&F Operations for Paralle Processing,'' </title> <booktitle> in Proc. 3rd International Conference on Supercomputing, </booktitle> <address> Crete, Greece, </address> <month> June </month> <year> 1989. </year>
Reference-contexts: Even the synchronization mechanism can be redesign to tolerate the difference in processor arrival times of a synchronization event [33]. Synchronization can also create a unique memory access problem called hot-spot [52]. The problem is effectively dealt with by the combing strategy <ref> [52, 63, 74] </ref>. 5.2.2. Memory Access Pattern Except for the added synchronization code, the total number of instructions executed and memory accesses made by a program should not change too much by parallelizing the program. However, distributing computational work among multiple processors can drastically change the memory access pattern. <p> Third, a completely different synchronization method can be developed to replace TTS, or even the entire critical section. In this chapter the performance of the Software Queue [6], the Lock Table, and the Restricted Fetch&Add <ref> [63] </ref> methods will be evaluated. The software queue method belongs to the second approach, while the other two methods belong to the third. The rest of this chapter is organized as follows. <p> Restricted Combining For bus-based multiprocessors a restricted form of combining fetch& F operations was proposed <ref> [63] </ref>. The restricted form of combining allows an easy and efficient implementation, requiring only that all processors participating in the combining have an identical fetch& F operation.
Reference: [64] <author> P. Tang and P.-C. Yew, </author> <title> ``Processor Self-Scheduling for Multiple-Nested Parallel Loops,'' </title> <booktitle> Proceededings of the 1986 International Conference on Parallel Processing, </booktitle> <pages> pp. 528-535, </pages> <month> August </month> <year> 1986. </year>
Reference-contexts: If static scheduling is used, a fixed set of loop iterations is assinged to each processor before execution of the loop. The alternative is dynamical scheduling, which comes in various complexity and costs <ref> [53, 64, 65] </ref>.
Reference: [65] <author> P. Tang, P.-C. Yew, and C.-Q. Zhu, </author> <title> ``Impact of Self-Scheduling Order on Performance of Multiprocessor Systems,'' </title> <booktitle> in ACM International Conferece of Supercomputing, Malo, France, </booktitle> <pages> pp. 593-603, </pages> <month> July </month> <year> 1988. </year> <month> 150 </month>
Reference-contexts: If static scheduling is used, a fixed set of loop iterations is assinged to each processor before execution of the loop. The alternative is dynamical scheduling, which comes in various complexity and costs <ref> [53, 64, 65] </ref>.
Reference: [66] <author> C. P. Thacker, L. C. Stewart, and E. H. Satterthwaite, Jr., ``Firefly: </author> <title> A Multiprocessor Workstation,'' </title> <journal> IEEE Transactions on COmputers, </journal> <volume> vol. C-37, </volume> <pages> pp. 909-920, </pages> <month> August </month> <year> 1988. </year>
Reference-contexts: But introducing private caches to a multiprocessor also creates the cache coherence problem. Many cache coherence protocols have been proposed, and the trade-off between complexity and performance under different workload conditions analyzed. For bus based multiprocessors a particular simple and efficient solution employs the snooping strategy <ref> [26, 29, 38, 47, 50, 66] </ref>. Bus traffic induced by cache coherence events can be substantial. The exact amount of coherence traffic generated is sensitive to the type of snooping protocols used, and the sharing patterns of the parallel workload. <p> Direct Implementation of Critical Section: Lock Table 10.2.3.1. Motivation and Function Description Cache coherence protocols can be broadly divided into two categories: write broadcast <ref> [47, 66] </ref>, and invalidation protocols [29, 38, 50]. Write broadcast cache coherence protocols have been found to be superior to write invalidation protocols [69, 70]. In these papers analytical models are used and the probabilistic input dictates that all processors to have equal access to the shared data.
Reference: [67] <author> S. Thakkar, P. Gifford, and G. Fielland, </author> <title> ``Balance: A Shard Memory Multiprocessor System,'' </title> <booktitle> Proc. 2nd Int. Conf. on Supercomputing, </booktitle> <pages> pp. 93-101, </pages> <year> 1987. </year>
Reference-contexts: One solution is to employ the idea of implementing large number of virtual or logical locks with a smaller number of hardware locks, as in Sequent Balance multiprocessors <ref> [67] </ref>. Since multiple virtual semaphore registers can be guarded by one hardware semaphore register, many more virtual semaphore registers are available and the allocation problem is alleviated. But the performance of the virtual semaphores is not so good as the hardware ones.
Reference: [68] <author> Kishor Shridharbhai Trivedi, </author> <title> Probability and Statistics with Reliability, Queuing and Computer Science Application. </title> <publisher> Prentics-Hall Inc., </publisher> <year> 1982. </year>
Reference-contexts: forms in the scheduling critical section (see Section 6.2.1), the increase of the execution time of the barrier-spin wait phase is also faster than linearly with the number of processors. hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh 3 Var [X + Y] = Var [X] + Var [Y], if X and Y are independent random variables <ref> [68] </ref>. 76 6.2.6. Serial Phase Serial code can be executed in two ways. The first uses one processor to compute while others spin wait. The second way allows all processors to do the same computation, even though every processor expects to get the same result.
Reference: [69] <author> M. K. Vernon and M. Holliday, </author> <title> ``Performance Analysis of Multiprocessor Cache Consistency Protocols Using Generalized Timed Petri Nets,'' </title> <booktitle> Proc. SIGMETRICS International Symposium on Computer Performance Modeling, Measurement and Evaluation, </booktitle> <pages> pp. 9-17, </pages> <month> May </month> <year> 1986. </year>
Reference-contexts: Multiprocessors with arbitrary interconnection networks have been the subject of several previous studies [39, 45, 46, 49, 51]. Studies of bus-based multiprocessor design issues have used trace-driven simulation [22], parameterized simulation [8], as well as analytical modeling <ref> [69, 70] </ref>. For a system as complex as a multi, ideally a system designer would like to use an accurate analytical model to explore the design space with a minimal computational requirement. Both analytical modeling as well as actual trace-driven simulation will be used in this study. <p> For example, the performance of different software synchronization algorithms was measured on a Sequent Symmetry multiprocessor [32], and the performance of different cache organizations on different models of Symmetry multiprocessors [7]. The performance of different cache coherence protocols was studied using analytical modeling <ref> [69, 70] </ref>, trace driven simulation with real traces [4, 18, 23, 24], or pseudo-traces [9]. Real parallel trace can either be derived from hardware monitoring [60], or software means such as the ptrace method. <p> Direct Implementation of Critical Section: Lock Table 10.2.3.1. Motivation and Function Description Cache coherence protocols can be broadly divided into two categories: write broadcast [47, 66], and invalidation protocols [29, 38, 50]. Write broadcast cache coherence protocols have been found to be superior to write invalidation protocols <ref> [69, 70] </ref>. In these papers analytical models are used and the probabilistic input dictates that all processors to have equal access to the shared data. Therefore shared data are more likely subjected to fine grain sharing, in which the write-broadcast should perform well.
Reference: [70] <author> M. K. Vernon, E. D. Lazowska, and J. Zahorjan, </author> <title> ``An Accurate and Efficient Performance Analysis Technique for Multiprocessor Snooping Cache-Consistency Protocols,'' </title> <booktitle> in Proc. 15th Annual Symposium on Computer Architecture, </booktitle> <address> Honolulu, HI, </address> <pages> pp. 308-315, </pages> <month> June </month> <year> 1988. </year>
Reference-contexts: Multiprocessors with arbitrary interconnection networks have been the subject of several previous studies [39, 45, 46, 49, 51]. Studies of bus-based multiprocessor design issues have used trace-driven simulation [22], parameterized simulation [8], as well as analytical modeling <ref> [69, 70] </ref>. For a system as complex as a multi, ideally a system designer would like to use an accurate analytical model to explore the design space with a minimal computational requirement. Both analytical modeling as well as actual trace-driven simulation will be used in this study. <p> Both analytical modeling as well as actual trace-driven simulation will be used in this study. The analytical models are based on a ``customized'' mean value analysis technique that has been proposed in <ref> [70] </ref> and applied in [36, 43, 71]. Trace-driven simulation is used to study a few thousand cases and, more importantly, build confidence in the analytical models. <p> Once the validity of the analytical models has been established, the models will be used to evaluate the design choices in the next chapter. 3.2. Customized Mean Value Analysis (CMVA) The CMVA models build on similar models developed to study bus-based multiprocessors <ref> [36, 43, 70, 71] </ref>. The CMVA method is appealing because it is simple and intuitive. <p> Therefore, the probability that the bus is busy when a blocking request arrives from a cache is 1 - U rv NU - U rv hhhhhhhhh , and the probability that the bus is idle is (1 - 1 - U rv NU - U rv hhhhhhhhh ) <ref> [70] </ref>. 14 For a non-blocking request (write back) this probability becomes (1 - 1 - U NU - U hhhhhhhh ). <p> For example, the performance of different software synchronization algorithms was measured on a Sequent Symmetry multiprocessor [32], and the performance of different cache organizations on different models of Symmetry multiprocessors [7]. The performance of different cache coherence protocols was studied using analytical modeling <ref> [69, 70] </ref>, trace driven simulation with real traces [4, 18, 23, 24], or pseudo-traces [9]. Real parallel trace can either be derived from hardware monitoring [60], or software means such as the ptrace method. <p> Direct Implementation of Critical Section: Lock Table 10.2.3.1. Motivation and Function Description Cache coherence protocols can be broadly divided into two categories: write broadcast [47, 66], and invalidation protocols [29, 38, 50]. Write broadcast cache coherence protocols have been found to be superior to write invalidation protocols <ref> [69, 70] </ref>. In these papers analytical models are used and the probabilistic input dictates that all processors to have equal access to the shared data. Therefore shared data are more likely subjected to fine grain sharing, in which the write-broadcast should perform well.
Reference: [71] <author> M. K. Vernon, R. Jog, and G. S. Sohi, </author> <title> ``Performance Analysis of Hierarchical Cache-Consistent Multiprocessors,'' </title> <journal> Performance Evaluation, </journal> <volume> vol. 9, </volume> <pages> pp. 287-302, </pages> <year> 1989. </year>
Reference-contexts: Both analytical modeling as well as actual trace-driven simulation will be used in this study. The analytical models are based on a ``customized'' mean value analysis technique that has been proposed in [70] and applied in <ref> [36, 43, 71] </ref>. Trace-driven simulation is used to study a few thousand cases and, more importantly, build confidence in the analytical models. Once the validity of the analytical models has been established, the models will be used to evaluate the design choices in the next chapter. 3.2. <p> Once the validity of the analytical models has been established, the models will be used to evaluate the design choices in the next chapter. 3.2. Customized Mean Value Analysis (CMVA) The CMVA models build on similar models developed to study bus-based multiprocessors <ref> [36, 43, 70, 71] </ref>. The CMVA method is appealing because it is simple and intuitive.
Reference: [72] <author> Wolf-Dietrich Weber and Anoop Gupta, </author> <title> ``Analysis of Cache Invalidation Patterns in Multiprocessors,'' </title> <booktitle> in Proceedins ASPLOS-III, </booktitle> <address> Boston, MA, </address> <month> April, </month> <year> 1989. </year>
Reference-contexts: Real parallel trace can either be derived from hardware monitoring [60], or software means such as the ptrace method. The parallel trace itself was analyzed to evaluate the performance of cache snooping protocols [22], or different design trades-off of the directory-based cache coherence scheme <ref> [72] </ref>. However, these methods are not always as effective as for uniprocessor systems. For example, the accuracy of these methods depends on the accuracy of the parallel traces, or the accurate characterization of the traces. <p> These statistics serve as an approximation of a certain component of performance. But the information are not enough in pin-pointing the exact cause of performance problems. A more sophisticated approach has been taken in <ref> [72] </ref>, which tried to link the invalidation traffic pattern to the behavior of high-level data objects in parallel programs. From the invalidation pattern the authors were able to suggest, for example, efficient ways to reduce invalidation traffic generated by synchronization objects. <p> Invalidation Pattern In this section the invalidation pattern of using different synchronization methods are studied. Analyzing the invalidation pattern is important in evaluating different directory schemes for large scale, non-bus based, cache coherent multiprocessors <ref> [4, 72] </ref>. Previous study of invalidation patterns uses either analytical modeling [17, 21], or trace driven simulation [4, 72]. <p> Analyzing the invalidation pattern is important in evaluating different directory schemes for large scale, non-bus based, cache coherent multiprocessors <ref> [4, 72] </ref>. Previous study of invalidation patterns uses either analytical modeling [17, 21], or trace driven simulation [4, 72]. One drawback of using the trace driven simulation method is that, since the timing of the memory system to be evaluated is unlikely to be the same as the timing of the system where the trace is derived, the result of trace-driven simulation can be inaccurate.
Reference: [73] <author> P. J. Woest and J. Goodman, </author> <title> ``An Analysis of Synchronization Mechanisms in Shared-Memory Multiprocessors,'' </title> <booktitle> in Proceedings of the International Symposium of Shared Memory Multiprocessing, </booktitle> <address> Tokyo, Japan, </address> <pages> pp. 152-165, </pages> <month> April </month> <year> 1991. </year>
Reference-contexts: However, their performance, such as the bus traffic generated, is not likely comparable to the hardware QOSB, especially for large multiprocessor configurations <ref> [73] </ref>. The comparisons between the lock table and QOSB methods are based on the following criteria: functionality, performance, implementation, and scalability. Both the lock table and QOSB methods directly support critical sec 126 tion operation.
Reference: [74] <author> P.-C. Yew, N.-F. Tzeng, and D. H. Lawrie, </author> <title> ``Distributing Hot-Spot Addressing in Large Scale Multiprocessors,'' </title> <journal> IEEE Transactions on Computers, </journal> <volume> vol. C-36, </volume> <pages> pp. 388-395, </pages> <month> April </month> <year> 1987. </year>
Reference-contexts: Even the synchronization mechanism can be redesign to tolerate the difference in processor arrival times of a synchronization event [33]. Synchronization can also create a unique memory access problem called hot-spot [52]. The problem is effectively dealt with by the combing strategy <ref> [52, 63, 74] </ref>. 5.2.2. Memory Access Pattern Except for the added synchronization code, the total number of instructions executed and memory accesses made by a program should not change too much by parallelizing the program. However, distributing computational work among multiple processors can drastically change the memory access pattern.
Reference: [75] <author> John Zahorjan, Edward D. Lazowska, and Derek L. Eager, </author> <title> ``The Effet of Scheduling Discipline on Spin Overhead in Shared Memory Parallel Systems,'' </title> <month> July </month> <year> 1989. </year> <month> 151 </month>
Reference-contexts: A compiler can schedule a balanced workload to each processor, while the operation system can adjust the run time process scheduling policy to minimize the process waiting time <ref> [75] </ref>. Even the synchronization mechanism can be redesign to tolerate the difference in processor arrival times of a synchronization event [33]. Synchronization can also create a unique memory access problem called hot-spot [52]. The problem is effectively dealt with by the combing strategy [52, 63, 74]. 5.2.2. <p> But processor utilization is not a major concern here. The interested reader, however, can find a thorough study of the topic in <ref> [75] </ref>. 75 When static scheduling is used the execution time of the first time component is affected by the difference in the execution times of different iterations in the loop. When fewer processors are used each processor takes more units of work to compute in the independent computing phase.
References-found: 75

