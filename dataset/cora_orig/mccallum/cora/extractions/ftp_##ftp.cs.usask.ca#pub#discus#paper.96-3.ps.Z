URL: ftp://ftp.cs.usask.ca/pub/discus/paper.96-3.ps.Z
Refering-URL: http://www.cs.wustl.edu/~jxh/research/related.html
Root-URL: 
Email: fmfa126,careyg@cs.usask.ca  
Title: Web Server Workload Characterization: The Search for Invariants (Extended Version) study, emphasis is placed on
Author: Martin F. Arlitt Carey L. Williamson 
Note: Throughout the  
Date: March 15, 1996  
Address: 57 Campus Drive Saskatoon, SK, CANADA S7N 5A9  
Affiliation: Department of Computer Science University of Saskatchewan  
Abstract: Fundamental to the goal of improving WWW performance is an understanding of WWW workloads. This paper presents a workload characterization study for Internet Web servers. Six different data sets are used in this study: three from academic environments, two from scientific research organizations, and one from a commercial Internet provider. These data sets represent three different orders of magnitude in server activity, and two different orders of magnitude in time duration, ranging from one week of activity to one year of activity. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> M. </author> <title> Andreessen, "NCSA Mosaic Technical Summary", </title> <booktitle> National Center for Supercomputing Applications, </booktitle> <year> 1993. </year>
Reference-contexts: 1 Introduction The popularity of the World Wide Web <ref> [1, 22] </ref> (also called WWW, or the Web) has made Web traffic the fl A shorter version of this paper is to appear at the 1996 ACM SIGMETRICS Conference, Philadelphia, PA, May 1996. fastest growing component of packet and byte traffic on the NSFNET network backbone [14].
Reference: [2] <author> T. Berners-Lee, L. Masinter and M. McCahill, </author> <title> "Uniform Resource Locators", </title> <type> RFC 1738, </type> <month> Decem-ber </month> <year> 1994. </year>
Reference-contexts: When the user selects a document to retrieve (usually by clicking a mouse on a hyperlink), the browser creates a request to be sent to the corresponding Web server. The request includes: the name of the requested document, expressed as a Uniform Resource Locator (URL) <ref> [2] </ref>; a set of HyperText request headers, indicating which data formats the client will accept; and user authentication information, which tells the server which documents the client has permission to retrieve. Once the request has been sent to the Web server, the client machine waits for a response.
Reference: [3] <author> A. Bestavros, R. Carter, M. Crovella, C. Cunha, A. Heddaya and S. Mirdad, </author> <title> "Application-Level Document Caching in the Internet", </title> <booktitle> Proceedings of the Second International Workshop on Services in Distributed and Networked Environments (SDNE '95), </booktitle> <address> Whistler, BC, Canada, </address> <pages> pp. 166-173, </pages> <month> June </month> <year> 1995. </year>
Reference-contexts: The phenomenal and alarming growth in Web traffic has sparked much research activity on "improving" the World Wide Web. For example, researchers have proposed caching strategies for Web clients <ref> [3] </ref>, caching strategies for Web servers [4], regional file caching strategies for large internetworks [8], and improved protocols for Web interaction [15, 20]. Much of this recent research activity has been aimed at improving Web performance and scalability. <p> Fundamental to the goal of improving Web performance is a solid understanding of WWW workloads. While there are several studies reported in the literature <ref> [3, 4, 6, 7, 12] </ref>, most studies present data from only one measurement site, making it difficult to generalize results to other sites. Furthermore, most studies focus on characterizing Web clients, rather than Web servers. <p> Efficient Web browsers (clients) can use caching of documents to reduce the loads that they put on Web servers and network links, thereby improving the performance of the Web. A recent study at Boston University <ref> [3] </ref> studied the effects of client-level caching on Web performance. Several other researchers have studied the use of file caching to reduce network traffic and server loads [4, 8, 10]. Web performance can also be improved by enhancing client-server communication [15, 20].
Reference: [4] <author> H. Braun and K. Claffy, </author> <title> "Web Traffic Characterization: An Assessment of the Impact of Caching Documents from NCSA's Web Server", </title> <booktitle> Electronic Proceedings of the Second World Wide Web Conference '94: Mosaic and the Web, </booktitle> <address> Chicago, Illinois, </address> <month> October </month> <year> 1994. </year>
Reference-contexts: The phenomenal and alarming growth in Web traffic has sparked much research activity on "improving" the World Wide Web. For example, researchers have proposed caching strategies for Web clients [3], caching strategies for Web servers <ref> [4] </ref>, regional file caching strategies for large internetworks [8], and improved protocols for Web interaction [15, 20]. Much of this recent research activity has been aimed at improving Web performance and scalability. <p> Fundamental to the goal of improving Web performance is a solid understanding of WWW workloads. While there are several studies reported in the literature <ref> [3, 4, 6, 7, 12] </ref>, most studies present data from only one measurement site, making it difficult to generalize results to other sites. Furthermore, most studies focus on characterizing Web clients, rather than Web servers. <p> A recent study at Boston University [3] studied the effects of client-level caching on Web performance. Several other researchers have studied the use of file caching to reduce network traffic and server loads <ref> [4, 8, 10] </ref>. Web performance can also be improved by enhancing client-server communication [15, 20]. Although the primary focus of this paper is workload characterization for Web servers, several relevant issues affecting server caching and performance are discussed in Section 5. <p> Both of these papers reported that over 90% of client requests were for either HTML or image documents. Table 5 also indicates that most transferred documents are quite small, which is a third invariant. This phenomenon was also observed by Braun and Claffy <ref> [4] </ref> for requests to the NCSA's Web server. Despite the fact that Web browsers provide support for the use of multimedia objects like sound and video, documents of these types accounted for only 0.01-1.2% of the requests in the six data sets. <p> First, only 0.3-2.1% of the requests and 0.4-5.1% of the bytes transferred are for distinct documents. This observation implies that caching documents (at the server, at the client, or within the network) could greatly improve the performance of the server, as has been pointed out by Claffy and Braun <ref> [4] </ref>. Second, in all six data sets, approximately one-third (e.g., 22.6-42.1%) of all the distinct documents are requested only once, and one-third (e.g., 14.3-42.5%) of the distinct bytes are transferred only once. <p> This distribution is consistent with the file size distribution reported by Braun and Claffy <ref> [4] </ref>. A more rigourous study shows that the observed file size distributions match well with the Pareto distribution [11, 17], for ff &lt; 1. This observation has been noted in the literature [6, 16], and is confirmed in all six of our data sets. <p> The NCSA data set shows the most concentration, while the Calgary data set shows the least. This concentration phenomenon is another invariant in our Web server logs, and is thus added to Table 1. Braun and Claffy have reported similar results for NCSA's Web server in an earlier study <ref> [4] </ref>. 4.2.2 Mean Inter-Reference Times Our next analysis focuses on the inter-reference time for documents that are accessed more than once.
Reference: [5] <author> R. Caceres, P. Danzig, S. Jamin and D. Mitzel, </author> <title> "Characteristics of Wide-Area TCP/IP Conversations", </title> <booktitle> Proceedings of ACM SIGCOMM '91, </booktitle> <address> Zurich, Switzerland, </address> <pages> pp. 101-112, </pages> <month> September </month> <year> 1991. </year>
Reference-contexts: times are exponentially Times distributed and independent 9 Remote Remote sites account for 70% of the accesses Requests to the server, and 60% of the bytes transferred 10 Wide Area Web servers are accessed by 1000's of domains, Usage with 10% of the domains accounting for 75% of usage traffic <ref> [5] </ref>. Six different Web server access logs are used in this study: three from academic environments, two from scientific research institutions, and one from a commercial Internet provider.
Reference: [6] <author> M. Crovella and A. Bestavros, </author> <title> "Explaining World Wide Web Traffic Self-Similarity", </title> <booktitle> Proceedings of the 1996 ACM SIGMETRICS Conference, </booktitle> <address> Philadelphia, PA, </address> <month> May </month> <year> 1996. </year>
Reference-contexts: Fundamental to the goal of improving Web performance is a solid understanding of WWW workloads. While there are several studies reported in the literature <ref> [3, 4, 6, 7, 12] </ref>, most studies present data from only one measurement site, making it difficult to generalize results to other sites. Furthermore, most studies focus on characterizing Web clients, rather than Web servers. <p> This distribution is consistent with the file size distribution reported by Braun and Claffy [4]. A more rigourous study shows that the observed file size distributions match well with the Pareto distribution [11, 17], for ff &lt; 1. This observation has been noted in the literature <ref> [6, 16] </ref>, and is confirmed in all six of our data sets. In particular, the tails of the distributions (for file sizes larger than 1024 bytes) are Pareto with 0:40 ff 0:63. This characteristic is present in all six data sets, and is thus added to Table 1. <p> % All Bytes 18.3 36.6 24.8 2.7 1.6 0.5 Remote Hosts Item Waterloo Calgary Saskatchewan NASA ClarkNet NCSA % All Requests 77.7 53.6 75.1 93.7 98.1 98.8 % All Bytes 81.7 63.4 75.2 97.3 98.4 99.5 4.3 Self-Similarity Recent work has suggested that World Wide Web traffic may be self-similar <ref> [6] </ref>. This section briefly describes the tests that were performed to check for self-similarity in Web server workloads. <p> Self-similarity does not appear to be an invariant in all Web server workloads, though it does appear to be a property when Web traffic is heavy, as reported in <ref> [6] </ref>. 4.4 Aborted Connections Several Web documents appeared in an access log multiple times, with the same URL each time, but with different transfer sizes at different points in the log. There are two possible causes for these "anomalies" in the Web server access logs.
Reference: [7] <author> C. Cunha, A. Bestavros and M. Crovella, </author> <title> "Characteristics of WWW Client-Based Traces", </title> <type> Technical Report BU-CS-95-010, </type> <institution> Boston University Computer Science Department, </institution> <year> 1995. </year>
Reference-contexts: Fundamental to the goal of improving Web performance is a solid understanding of WWW workloads. While there are several studies reported in the literature <ref> [3, 4, 6, 7, 12] </ref>, most studies present data from only one measurement site, making it difficult to generalize results to other sites. Furthermore, most studies focus on characterizing Web clients, rather than Web servers. <p> Using Table 5, we can identify a second invariant in Web server workloads. Across the six data sets, HTML and Image documents accounted for 90-100% of the total requests to the server. 4 This observation is consistent with results reported by Sedayao [19] and by Cunha, Bestavros and Crovella <ref> [7] </ref>. Both of these papers reported that over 90% of client requests were for either HTML or image documents. Table 5 also indicates that most transferred documents are quite small, which is a third invariant.
Reference: [8] <author> P. Danzig, M. Schwartz and R. Hall, </author> <title> "A Case for Caching File Objects Inside Internetworks", </title> <booktitle> Proceedings of ACM SIGCOMM '93, </booktitle> <address> San Francisco, California, </address> <pages> pp. 239-248, </pages> <month> September </month> <year> 1993. </year>
Reference-contexts: The phenomenal and alarming growth in Web traffic has sparked much research activity on "improving" the World Wide Web. For example, researchers have proposed caching strategies for Web clients [3], caching strategies for Web servers [4], regional file caching strategies for large internetworks <ref> [8] </ref>, and improved protocols for Web interaction [15, 20]. Much of this recent research activity has been aimed at improving Web performance and scalability. <p> A recent study at Boston University [3] studied the effects of client-level caching on Web performance. Several other researchers have studied the use of file caching to reduce network traffic and server loads <ref> [4, 8, 10] </ref>. Web performance can also be improved by enhancing client-server communication [15, 20]. Although the primary focus of this paper is workload characterization for Web servers, several relevant issues affecting server caching and performance are discussed in Section 5.
Reference: [9] <author> K. Froese and R. Bunt, </author> <title> "The Effect of Client Caching on File Server Workloads", </title> <booktitle> Proceedings of the Twenty-Ninth Hawaii International Conference on System Sciences, </booktitle> <month> January </month> <year> 1996. </year>
Reference-contexts: Client-side caching mechanisms may also serve to remove temporal locality from the reference stream seen at the server, as has been shown in other client-server environments <ref> [9] </ref>. 4.2.4 Geographic Distribution Our final analysis of file referencing behaviour examines the geographic distribution of document requests. This analysis makes use of the IP addresses of the requesting hosts in the access log.
Reference: [10] <author> S. Glassman, </author> <title> "A Caching Relay for the World Wide Web", </title> <booktitle> First International Conference on the World Wide Web, </booktitle> <address> Geneva, Switzerland, </address> <month> May </month> <year> 1994. </year>
Reference-contexts: A recent study at Boston University [3] studied the effects of client-level caching on Web performance. Several other researchers have studied the use of file caching to reduce network traffic and server loads <ref> [4, 8, 10] </ref>. Web performance can also be improved by enhancing client-server communication [15, 20]. Although the primary focus of this paper is workload characterization for Web servers, several relevant issues affecting server caching and performance are discussed in Section 5.
Reference: [11] <author> N. Johnson and S. Kotz, Editors-in-Chief, </author> <title> Encyclopedia of Statistical Sciences, Volumes 6 and 9, </title> <publisher> John Wiley & Sons, Inc., </publisher> <address> New York, </address> <year> 1988. </year>
Reference-contexts: This distribution is consistent with the file size distribution reported by Braun and Claffy [4]. A more rigourous study shows that the observed file size distributions match well with the Pareto distribution <ref> [11, 17] </ref>, for ff &lt; 1. This observation has been noted in the literature [6, 16], and is confirmed in all six of our data sets. In particular, the tails of the distributions (for file sizes larger than 1024 bytes) are Pareto with 0:40 ff 0:63.
Reference: [12] <author> T. Kwan, R. McGrath, and D. Reed, </author> <title> "NCSA's World Wide Web Server: Design and Performance", </title> <journal> IEEE Computer, </journal> <volume> Vol. 28, No. 11, </volume> <pages> pp. 68-74, </pages> <month> November </month> <year> 1995. </year>
Reference-contexts: Fundamental to the goal of improving Web performance is a solid understanding of WWW workloads. While there are several studies reported in the literature <ref> [3, 4, 6, 7, 12] </ref>, most studies present data from only one measurement site, making it difficult to generalize results to other sites. Furthermore, most studies focus on characterizing Web clients, rather than Web servers. <p> The ClarkNet Web server is a SUNsparc10 with two 60 MHz processors, providing Internet access for 5,000 people (as of August 1995). This machine is running Netscape's Commerce Server 1.1. The NCSA server consists of 8 HP 735 workstations, used in a round-robin fashion to provide Web service <ref> [12] </ref>. Each workstation has 96 MB RAM and a 130 MB local disk cache. The workstations all run NCSA httpd, and use AFS for file access over an FDDI ring. 3.2 Raw Data Table 2 summarizes the raw data from the six access logs.
Reference: [13] <author> W. Leland, M. Taqqu, W. Willinger and D. Wil-son, </author> <title> "On the Self-Similar Nature of Ethernet Traffic (Extended Version)", </title> <journal> IEEE/ACM Transactions on Networking, </journal> <volume> Vol. 2, No. 1, </volume> <pages> pp. 1-15, </pages> <month> February </month> <year> 1994. </year>
Reference-contexts: One aspect of self-similarity is the absence of a characteristic size of a traffic burst. To assess this effect, the server workload data (i.e., bytes transferred per unit of time) were plotted and inspected visually, as in Leland et al <ref> [13] </ref>. The results are shown in Figure 6, for four different time scales. The topmost graph (1000 second intervals) shows the full week of ClarkNet data, in which a distinct daily usage pattern is seen. <p> Moving from the bottom plot to the top plot in Figure 6, burstiness clearly exists across several different time scales. The results suggest that there is some evidence of self-similarity in this workload. 6 A more rigourous analysis confirms this intuition. The analysis, as described in <ref> [13] </ref>, makes use of three statistical tests: autocorrelation, variance-time plot, and R/S analysis. These tests were performed for both one hour and eight hour periods of the data. sample. The autocorrelation plot in Figure 7 (a) shows that the autocorrelation coefficients are small, but nonzero, even at large lags.
Reference: [14] <institution> NSFNET Statistics. </institution> <note> Data available by anonymous ftp from nic.merit.edu/statistics/nsfnet. </note>
Reference-contexts: the World Wide Web [1, 22] (also called WWW, or the Web) has made Web traffic the fl A shorter version of this paper is to appear at the 1996 ACM SIGMETRICS Conference, Philadelphia, PA, May 1996. fastest growing component of packet and byte traffic on the NSFNET network backbone <ref> [14] </ref>. WWW traffic has increased from 74 Megabytes per month in December 1992 to 3.2 Terabytes per month in December 1994. There are many reasons behind this explosive growth in Web traffic.
Reference: [15] <author> V. Padmanabhan and J. Mogul, </author> <title> "Improving HTTP Latency", </title> <booktitle> Electronic Proceedings of the Second World Wide Web Conference '94: Mosaic and the Web, </booktitle> <address> Chicago, Illinois, </address> <month> October </month> <year> 1994. </year>
Reference-contexts: For example, researchers have proposed caching strategies for Web clients [3], caching strategies for Web servers [4], regional file caching strategies for large internetworks [8], and improved protocols for Web interaction <ref> [15, 20] </ref>. Much of this recent research activity has been aimed at improving Web performance and scalability. The key performance factors to consider are how to reduce the volume of network traffic produced by Web clients and servers, and how to improve the response time for WWW users. <p> The server parses the request, and issues a response. Once the response is complete, the TCP connection between the Web client and server is closed. This process is repeated each time a client wishes to retrieve a document from a Web server <ref> [15] </ref>. 2.2 Web Clients A human user can gain access to the information on the World Wide Web by using a Web browser, such as netscape, mosaic, or lynx [22]. <p> The response includes a status code to inform the client if the request succeeded. If the request was successful, then the response includes the requested document. If the request was unsuccessful, a reason for the failure is returned to the client <ref> [15] </ref>. Once the Web server has sent its response and terminated the TCP connection with the client, the server repeats the cycle and begins listening for its next request. 2.4 Server Logs Web servers can be configured to record information about all client requests. <p> A recent study at Boston University [3] studied the effects of client-level caching on Web performance. Several other researchers have studied the use of file caching to reduce network traffic and server loads [4, 8, 10]. Web performance can also be improved by enhancing client-server communication <ref> [15, 20] </ref>. Although the primary focus of this paper is workload characterization for Web servers, several relevant issues affecting server caching and performance are discussed in Section 5.
Reference: [16] <author> V. Paxson, </author> <title> "Empirically-Derived Analytic Models of Wide-Area TCP Connections", </title> <journal> IEEE/ACM Transactions on Networking, </journal> <volume> Vol. 2, No. 4, </volume> <pages> pp. 316-336, </pages> <month> August </month> <year> 1994. </year>
Reference-contexts: This distribution is consistent with the file size distribution reported by Braun and Claffy [4]. A more rigourous study shows that the observed file size distributions match well with the Pareto distribution [11, 17], for ff &lt; 1. This observation has been noted in the literature <ref> [6, 16] </ref>, and is confirmed in all six of our data sets. In particular, the tails of the distributions (for file sizes larger than 1024 bytes) are Pareto with 0:40 ff 0:63. This characteristic is present in all six data sets, and is thus added to Table 1.
Reference: [17] <author> V. Paxson and S. Floyd, </author> <title> "Wide-Area Traffic: The Failure of Poisson Modeling", </title> <booktitle> Proceedings of ACM SIGCOMM '94 London, England, </booktitle> <pages> pp. 257-268, </pages> <month> August </month> <year> 1994. </year>
Reference-contexts: This distribution is consistent with the file size distribution reported by Braun and Claffy [4]. A more rigourous study shows that the observed file size distributions match well with the Pareto distribution <ref> [11, 17] </ref>, for ff &lt; 1. This observation has been noted in the literature [6, 16], and is confirmed in all six of our data sets. In particular, the tails of the distributions (for file sizes larger than 1024 bytes) are Pareto with 0:40 ff 0:63.
Reference: [18] <author> V. Paxson, </author> <title> "Growth Trends in Wide Area TCP Connections", </title> <journal> IEEE Network, </journal> <volume> Vol. 8, No. 4, </volume> <pages> pp. 8-17, </pages> <month> July/August </month> <year> 1994. </year>
Reference-contexts: trend among researchers, educational institutions, and commercial organizations to make the Web the standard mechanism for disseminating information in a timely fashion; the machine-independent nature of the languages and protocols used for constructing and exchanging Web documents; and a continuing exponential increase in the number of Internet hosts and users <ref> [18] </ref>. The phenomenal and alarming growth in Web traffic has sparked much research activity on "improving" the World Wide Web.
Reference: [19] <author> J. Sedayao, </author> <title> "Mosaic Will Kill My Network!", </title> <booktitle> Electronic Proceedings of the Second World Wide Web Conference '94: Mosaic and the Web, </booktitle> <address> Chicago, Illi-nois, </address> <month> October </month> <year> 1994. </year>
Reference-contexts: Using Table 5, we can identify a second invariant in Web server workloads. Across the six data sets, HTML and Image documents accounted for 90-100% of the total requests to the server. 4 This observation is consistent with results reported by Sedayao <ref> [19] </ref> and by Cunha, Bestavros and Crovella [7]. Both of these papers reported that over 90% of client requests were for either HTML or image documents. Table 5 also indicates that most transferred documents are quite small, which is a third invariant.
Reference: [20] <author> M. Spasojevic, M. Bowman and A. Spector, </author> <title> "Using a Wide-Area File System Within the World-Wide Web", </title> <booktitle> Electronic Proceedings of the Second World Wide Web Conference '94: Mosaic and the Web, </booktitle> <address> Chicago, Illinois, </address> <month> October </month> <year> 1994. </year>
Reference-contexts: For example, researchers have proposed caching strategies for Web clients [3], caching strategies for Web servers [4], regional file caching strategies for large internetworks [8], and improved protocols for Web interaction <ref> [15, 20] </ref>. Much of this recent research activity has been aimed at improving Web performance and scalability. The key performance factors to consider are how to reduce the volume of network traffic produced by Web clients and servers, and how to improve the response time for WWW users. <p> A recent study at Boston University [3] studied the effects of client-level caching on Web performance. Several other researchers have studied the use of file caching to reduce network traffic and server loads [4, 8, 10]. Web performance can also be improved by enhancing client-server communication <ref> [15, 20] </ref>. Although the primary focus of this paper is workload characterization for Web servers, several relevant issues affecting server caching and performance are discussed in Section 5.
Reference: [21] <author> A. Tanenbaum, </author> <title> Computer Networks, Second Edition, </title> <publisher> Prentice Hall, </publisher> <address> Englewood Cliffs, New Jersey, </address> <year> 1988. </year>
Reference-contexts: Communication is always in the form of request-response pairs, and is always initiated by the client. Web clients and servers communicate using the HyperText Transfer Protocol (HTTP). HTTP runs on top of TCP, a reliable bidirectional byte stream protocol at the transport layer <ref> [21] </ref>. Communication between a Web client and a Web server is carried out in the following manner. When a client has a request to make of a particular Web server, the client must contact that server.
Reference: [22] <editor> World Wide Web Frequently Asked Questions, </editor> <title> URL: http://www.io.org/faq/www/index.html For More Information Martin Arlitt's M.Sc. thesis will be available via URL http://www.cs.usask.ca/projects/discus/ in May 1996. The C programs used in this study to process Web server logs will be available on an "as is" basis from the same site. We hope to make one or more of our Web server access logs available to other researchers via the Internet Traffic Archive (ITA), </title> <note> located at URL http://town.hall.org/Archives/pub/ITA/ </note>
Reference-contexts: 1 Introduction The popularity of the World Wide Web <ref> [1, 22] </ref> (also called WWW, or the Web) has made Web traffic the fl A shorter version of this paper is to appear at the 1996 ACM SIGMETRICS Conference, Philadelphia, PA, May 1996. fastest growing component of packet and byte traffic on the NSFNET network backbone [14]. <p> This process is repeated each time a client wishes to retrieve a document from a Web server [15]. 2.2 Web Clients A human user can gain access to the information on the World Wide Web by using a Web browser, such as netscape, mosaic, or lynx <ref> [22] </ref>. When the user selects a document to retrieve (usually by clicking a mouse on a hyperlink), the browser creates a request to be sent to the corresponding Web server.
References-found: 22

