URL: http://www.cs.yale.edu/HTML/YALE/CS/HyPlans/lovasz/mixing.ps
Refering-URL: http://www.cs.yale.edu/HTML/YALE/CS/HyPlans/lovasz/survey.html
Root-URL: http://www.cs.yale.edu
Title: Mixing Times  
Author: Laszlo Lovasz and Peter Winkler 
Note: Contents  
Abstract: The critical issue in the complexity of Markov chain sampling techniques has been "mixing time", the number of steps of the chain needed to reach its stationary distribution. It turns out that there are many ways to define mixing time|more than a dozen are considered here|but they fall into a small number of classes. The parameters in each class lie within constant multiples of one another, independent of the chain. Furthermore, there are interesting connections between these classes related to time reversal. This work is more in the nature of a long research paper than a survey, with many new results and proofs. Some of the results have appeared in recent articles or were known previously for the important special case of reversible chains. 
Abstract-found: 1
Intro-found: 1
Reference: [AF] <author> D.J. Aldous and J. Fill, </author> <title> Reversible Markov Chains and Random Walks on Graphs (book), </title> <note> to appear. URL for draft at http://www.stat.Berkeley.EDU/users/aldous/book.html. </note>
Reference-contexts: length of ;t is given by N (; t ) = i;j When the target is the stationary distribution for the chain, E ; is independent of the starting distribution , on account of what we call the "Random Target Identity" (also known as the right averaging principle, e.g. in <ref> [AF] </ref>) which says that there is a constant N for which N (; ) = N (2.1) for every starting distribution . In particular, we have N = j X j H (i; j)(2.2) for every state i. 6 L ASZL O LOV ASZ AND PETER WINKLER 2.2. Access times. <p> Groups of mixing times. The Mixing Group is a small group containing the most important mixing measure, the mixing time H. The following theorem shows that the approximate filling time is also in this group (see <ref> [AF] </ref> for this result and many others in the setting of reversible Markov chains). While either one of the forget and reset times may be much smaller than the mixing time, their maximum (or, equivalently, sum) is also in this group. Theorem 3.1. <p> Exit frequencies are a special case of what Pitman [P] calls "pre-T occupation measures" for stopping times T . Indeed, versions of Lemma 4.2 and Theorem 4.4 below are proved in [P] using Pitman's "occupation measure identity". Exit frequencies for the naive rule are easy to find; see <ref> [AF] </ref>. Several related formulas could be derived using relations to electrical networks, as in [CRRST], [DS], or [T]. Lemma 4.1. <p> middle term, we use 4.2: X z k (t; t t ) = k2U m=0 k k ) m=0 Substituting these bounds, the theorem follows. (See [LW4] for a simple direct proof.) Theorem 4.22 asserts closeness in the total variation distance; approaching pointwise (as noted, for the reversible case, in <ref> [AF] </ref>) turns out to be somewhat harder in general. Below we establish results about the pointwise distance of distributions obtained by various averaging rules. However, we shall have to use the worst-case bound on the mixing time. <p> Next we describe another simple averaging rule that achieves this. The result below also follows by adaptation of the "multiplicativity property" in Aldous <ref> [AF] </ref>. Let M &gt; 0, t = 8dHe, and let X be the sum of M independent random variables Y 1 ; : : : ; Y M , distributed uniformly in f0; : : : ; t 1g. Stop at v X . <p> Next we prove inequalities for reversible chains. The next lemma was proved by Aldous and Fill <ref> [AF] </ref>: Lemma 5.18. For every time-reversible Markov chain, ~ S L : We have a similar inequality for ~ Z: MIXING TIMES 45 Lemma 5.19. For every time-reversible Markov chain, ~ Z 8L: Proof.
Reference: [A1] <author> D.J. Aldous, </author> <title> Some inequalities for reversible Markov chains, </title> <journal> J. London Math. Soc. </journal> <volume> 25 (1982), </volume> <pages> 564-576. </pages>
Reference: [A2] <author> D.J. Aldous, </author> <title> Applications of random walks on graphs, </title> <note> preprint (1989). </note>
Reference-contexts: 1. Introduction and preliminaries In the past ten years there have been numerous applications (see, e.g., <ref> [A2] </ref>, [JS], [DFK]) for sampling via finite Markov chains. A Markov chain is constructed from whose stationary distribution one wishes to sample, and then the chain is run for a fixed number of steps after which the distribution of the current state is "nearly" stationary. <p> The main result is that we can place these numbers into a few equivalence classes, within which numbers differ only by constant factors independent of the chain (see Section 3). For the case of reversible chains, such results were obtained by Aldous <ref> [A2] </ref>; other results in this direction were published in [ALW, LW4, LW5]. To prove the main results, we develop a calculus of "exit frequencies", study the behavior of mixing properties under time reversal, and make use of some linear MIXING TIMES 3 algebraic tools.
Reference: [A3] <author> D.J. Aldous, </author> <title> The random walk construction for spanning trees and uniform labelled trees, </title> <journal> SIAM J. Discrete Math. </journal> <volume> 3 (1990), </volume> <pages> 450-465. </pages>
Reference-contexts: However, we will see later that this shu*ing rule is not optimal. Without going into details, let us remark that the algorithm of Aldous <ref> [A3] </ref> and Broder [BR] can be regarded as a stopping rule for a Markov chain on trees that generates the uniform distribution.
Reference: [AD] <author> D.J. Aldous and P. Diaconis, </author> <title> Shu*ing cards and stopping times, </title> <journal> Amer. Math. </journal> <volume> Monthly 93 #5 (1986), </volume> <pages> 333-348. </pages>
Reference-contexts: Example 1.3 (Card Shu*ing). A classic application of Markov chain mixing is shu*ing a deck of playing cards; see e.g. [BD] where Bayer and Diaconis argue that seven ri*e shu*es are necessary and sufficient to mix a deck of 52 cards. In Aldous and Diaconis <ref> [AD] </ref>, the following simple shu*ing algorithm is analyzed: a card is removed from the top of an n-card deck and replaced with equal probability in any of the n slots among the remaining n1 cards. <p> Again, this method takes about n ln n steps on the average, and also as in the cube case, constitutes a "strong" stopping rule in the sense of <ref> [AD] </ref> (the ending distribution is uniform even when conditioned on the stopping rule having taken some fixed number of steps). However, we will see later that this shu*ing rule is not optimal. <p> Unfortunately, for chains that are not time-reversible, there are several ways to define this number, and these play different roles in mixing. We take the easy route and do not define it at all. Strong rules. We mention strong stopping rules, introduced by Aldous and Di-aconis <ref> [AD] </ref>, which have been used very successfully to estimate mixing times. A stopping rule is said to be strong if the final state v is independent of the number of steps . <p> This is indeed true in several situations, but not always. Example 2.4 illustrates that the dependence on the error parameter can be quite tricky. A nice clean result is due to Aldous and Diaconis <ref> [AD] </ref>. Let e (t) = max d ( t ; t ), where the maximum is taken over all starting distributions and . Clearly the maximum is attained when and are concentrated on singletons.
Reference: [ALW] <author> D.J. Aldous, L. Lovasz and P. Winkler, </author> <title> Mixing times for uniformly ergodic Markov chains, Stochastic Processes and their Applications, </title> <note> to appear. </note>
Reference-contexts: For the case of reversible chains, such results were obtained by Aldous [A2]; other results in this direction were published in <ref> [ALW, LW4, LW5] </ref>. To prove the main results, we develop a calculus of "exit frequencies", study the behavior of mixing properties under time reversal, and make use of some linear MIXING TIMES 3 algebraic tools. We hope that these tools shed some light on the mechanism of mixing. Acknowledgement. <p> Preliminaries. Throughout this paper we will assume a fixed irreducible Markov chain with transition matrix M = fp ij g, with a finite state space V of finite cardinality n (see <ref> [ALW] </ref> for extensions to infinite state space). If we say "distribution" without specifying an underlying set, we mean distribution on V . If is a distribution on V and A V , then (A) = P i2A i is the probability of A.
Reference: [AGT] <author> S. Asmussen, P.W. Glynn and H. Thorisson, </author> <title> Stationary detection in the initial transient problem, </title> <booktitle> ACM Transactions on Modeling and Computer Simulation 2 (1992), </booktitle> <pages> 130-157. </pages>
Reference: [BC] <author> J.R. Baxter and R.V. Chacon, </author> <title> Stopping times for recurrent Markov processes, </title> <journal> Illinois J. Math. </journal> <volume> 20 (1976), </volume> <pages> 467-475. </pages>
Reference: [BD] <author> D. Bayer and P. Diaconis, </author> <title> Trailing the dovetail shu*e to its lair, </title> <journal> Ann. Appl. Probab. </journal> <volume> 2 (1992), </volume> <pages> 294-313. </pages>
Reference-contexts: We will see that it is in fact optimal. Example 1.3 (Card Shu*ing). A classic application of Markov chain mixing is shu*ing a deck of playing cards; see e.g. <ref> [BD] </ref> where Bayer and Diaconis argue that seven ri*e shu*es are necessary and sufficient to mix a deck of 52 cards.
Reference: [BL] <author> A. Beveridge and L. Lovasz, </author> <title> Random walks and the regeneration time, </title> <journal> J. </journal> <note> Graph Theory (to appear) </note>
Reference-contexts: This is only formally similar to Formula (2.2) for N ; in general, N is much larger. (Instead of , one might want to generate independent samples from some other distribution. This leads to the notion of the regeneration time max ff I (ff; ff), studied in <ref> [BL] </ref>; however, this turns out to be closely related to commute times rather than mixing times, and will not be discussed here.) We also introduce an approximate version of the independence time.
Reference: [BR] <author> A. Broder, </author> <title> Generating random spanning trees, </title> <booktitle> Proc. 30th Annual Symp. on Found. </booktitle> <institution> of Computer Science, IEEE Computer Soc. </institution> <year> (1989), </year> <pages> 442-447. </pages>
Reference-contexts: However, we will see later that this shu*ing rule is not optimal. Without going into details, let us remark that the algorithm of Aldous [A3] and Broder <ref> [BR] </ref> can be regarded as a stopping rule for a Markov chain on trees that generates the uniform distribution.
Reference: [CO] <author> R.V. Chacon and D.S. Ornstein, </author> <title> A general ergodic theorem, </title> <journal> Illinois J. Math. </journal> <volume> 4 (1960), </volume> <pages> 153-160. </pages>
Reference: [CRRST] <author> A.K. Chandra, P. Raghavan, W.L. Ruzzo, R. Smolensky, and P. Tiwari, </author> <title> The Electrical Resistance of a Graph Captures its Commute and Cover Times, </title> <booktitle> Proceedings of the 21st Annual ACM Symposium on Theory of Computing, </booktitle> <month> May </month> <year> (1989). </year>
Reference-contexts: Indeed, versions of Lemma 4.2 and Theorem 4.4 below are proved in [P] using Pitman's "occupation measure identity". Exit frequencies for the naive rule are easy to find; see [AF]. Several related formulas could be derived using relations to electrical networks, as in <ref> [CRRST] </ref>, [DS], or [T]. Lemma 4.1.
Reference: [CTW] <author> D. Coppersmith, P. Tetali, and P. Winkler, </author> <title> Collisions among Random Walks on a Graph, </title> <note> SIAM J. on Discrete Mathematics 6 #3 (1993), 363-374. </note>
Reference-contexts: MIXING TIMES 7 The maximum in Theorem 2.1 is achieved precisely when j is a halting state of some optimal rule which stops at t when started at . In the case of time-reversible chains, we can use the cycle-reversing identity from <ref> [CTW] </ref> to obtain the following expression: H (; t ) = i j i We will be concerned mostly with the case when t = and is concentrated at a single state i.
Reference: [DS] <author> P.G. Doyle and J.L. Snell, </author> <title> Random Walks and Electric Networks, </title> <publisher> Mathematical Assoc. of America, </publisher> <address> Washington, DC 1984. </address>
Reference-contexts: Indeed, versions of Lemma 4.2 and Theorem 4.4 below are proved in [P] using Pitman's "occupation measure identity". Exit frequencies for the naive rule are easy to find; see [AF]. Several related formulas could be derived using relations to electrical networks, as in [CRRST], <ref> [DS] </ref>, or [T]. Lemma 4.1.
Reference: [D] <author> L.E. Dubins, </author> <title> On a theorem of Skorokhod, </title> <journal> Ann. Math. Statist. </journal> <volume> 39 (1968), </volume> <pages> 2094-2097. </pages>
Reference: [DFK] <author> M. Dyer, A. Frieze and R. Kannan, </author> <title> A random polynomial time algorithm for estimating volumes of convex bodies, </title> <booktitle> Proc. 21st Annual ACM Symposium on the Theory of Computing (1989), </booktitle> <pages> 375-381. </pages>
Reference-contexts: 1. Introduction and preliminaries In the past ten years there have been numerous applications (see, e.g., [A2], [JS], <ref> [DFK] </ref>) for sampling via finite Markov chains. A Markov chain is constructed from whose stationary distribution one wishes to sample, and then the chain is run for a fixed number of steps after which the distribution of the current state is "nearly" stationary.
Reference: [JS] <author> M. Jerrum and A. Sinclair, </author> <title> Conductance and the rapid mixing property for Markov chains: the approximation of the permanent resolved, </title> <booktitle> Proc. 20nd Annual ACM Symposium on Theory of Computing (1988), </booktitle> <pages> 235-243. </pages>
Reference-contexts: 1. Introduction and preliminaries In the past ten years there have been numerous applications (see, e.g., [A2], <ref> [JS] </ref>, [DFK]) for sampling via finite Markov chains. A Markov chain is constructed from whose stationary distribution one wishes to sample, and then the chain is run for a fixed number of steps after which the distribution of the current state is "nearly" stationary.
Reference: [KLS] <author> R. Kannan, L. Lovasz and M. Simonovits: </author> <title> Random walks and an O fl (n 5 ) volume algorithm, Random Structures and Algorithms (1997). </title>
Reference-contexts: are "-independent, if for every two sets A V and B V , we have fi fi P (X 2 A; Y 2 B) P (X 2 A)P (Y 2 B) fi This is a very weak notion of independence, but in some applications of Markov chain techniques to sampling <ref> [KLS] </ref>, this is exactly what is needed. <p> Also, if (1 + ") then clearly Y (1 + "). This lemma is proved (essentially) in <ref> [KLS] </ref>. Proof. We have to show that fi fi P (v 0 2 A; v Y 2 B) P (v 0 2 A)P (v Y 2 B) fi for every two sets A and B of states.
Reference: [LS] <author> L. Lovasz and M. Simonovits, </author> <title> Random walks in a convex body and an improved volume algorithm, Random Structures and Alg. </title> <booktitle> 4 (1993), </booktitle> <pages> 359-412. </pages>
Reference-contexts: So here a (randomized) stopping rule is considered which, in many respects, has better properties than the "stop after t steps" rule. A similar remark applies to stopping a "lazy" walk as defined, e.g., in <ref> [LS] </ref>. 2. Access times and mixing times 2.1. General stopping rules. <p> For example, the uniform averaging rule (to be discussed below) of walking u steps and then choosing one of the exited points uniformly can be viewed as a blind rule: after t steps, we stop with probability 1=(ut). Lazy random walks have been considered (see Lovasz and Simonovits <ref> [LS] </ref>) because they have better convergence to the stationary distribution; the lazy version of a Markov chain is obtained by flipping a coin before each move and staying where we are if we see "heads".
Reference: [LW1] <author> L. Lovasz and P. Winkler, </author> <title> A note on the last new vertex visited by a random walk, </title> <editor> J. </editor> <booktitle> Graph Theory 17 (1993), </booktitle> <pages> 593-596. </pages>
Reference-contexts: This particular method does not generalize; in fact, apart from the complete graph, the cycle is the only graph which enjoys this property (see <ref> [LW1] </ref>). 4 L ASZL O LOV ASZ AND PETER WINKLER Example 1.2 (Cube). Consider another quite simple graph, the cube, which we view as the graph of vertices and edges of [0; 1] n .
Reference: [LW2] <author> L. Lovasz and P. Winkler, </author> <title> Efficient stopping rules for Markov chains, </title> <booktitle> Proc. 27th ACM Symp. on the Theory of Computing (1995), </booktitle> <pages> 76-82. </pages>
Reference-contexts: Thus by definition we stop immediately if and when any halting state is entered. (Of course we may stop in other states too, just not all the time.) The following theorem from <ref> [LW2] </ref> and [LW4] is very handy for determining optimality of stopping rules. Theorem 2.2. A stopping rule is mean-optimal if and only if it has a halting state. <p> Thus if we hit a state past the release time we stop; otherwise we continue, except that if we're within 1, we use the fractional part to randomize. Theorems 4.9 and 4.10, from <ref> [LW2] </ref> and [LW4], establish the universality and max-optimality of threshold rules. Theorem 4.9. For every starting distribution and target distribution t , there exists a threshold function h which gives an optimal stopping rule for generating t . Proof. We define this rule recursively as follows.
Reference: [LW3] <author> L. Lovasz and P. Winkler, </author> <title> Exact mixing in an unknown Markov chain, </title> <journal> Electronic J. Comb. </journal> <volume> 2, </volume> <year> (1995), </year> <note> Paper R15. </note>
Reference-contexts: By considering the naive rule ;t , we get the inequality H (; t ) N (; t ) = i;j This may be quite far from equality; for example, H (; ) = 0 for any . The following formula for access times was given in <ref> [LW3] </ref>: Theorem 2.1. For all distributions and t , H (; t ) = max By (2.4), we can write this as H (; t ) = max X ( i t i )H (i; j) : The inequality in the theorem is trivial by the triangle inequality. <p> The naive rule is in general not bounded, but bounded stopping rules are available when the target distribution t has sufficiently large support, thus in particular when t = (see <ref> [LW3] </ref>). A stopping rule is max-optimal (for given and t ) if max () is minimal. The maximum length of a max-optimal stopping rule from to t will be denoted by M (; t ).
Reference: [LW4] <author> L. Lovasz and P. Winkler, </author> <title> Mixing of random walks and other diffusions on a graph, </title> <booktitle> Surveys in Combinatorics, </booktitle> <year> 1995, </year> <editor> P. Rowlinson, ed., </editor> <booktitle> London Math. Soc. Lecture Note Series 218, </booktitle> <address> Cambridge U. </address> <publisher> Press (1995), </publisher> <pages> 119-154. </pages>
Reference-contexts: For the case of reversible chains, such results were obtained by Aldous [A2]; other results in this direction were published in <ref> [ALW, LW4, LW5] </ref>. To prove the main results, we develop a calculus of "exit frequencies", study the behavior of mixing properties under time reversal, and make use of some linear MIXING TIMES 3 algebraic tools. We hope that these tools shed some light on the mechanism of mixing. Acknowledgement. <p> Thus by definition we stop immediately if and when any halting state is entered. (Of course we may stop in other states too, just not all the time.) The following theorem from [LW2] and <ref> [LW4] </ref> is very handy for determining optimality of stopping rules. Theorem 2.2. A stopping rule is mean-optimal if and only if it has a halting state. <p> Blind stopping rules will be used to generate the stationary distribution, or at least approximations of it. Unfortunately, blind rules to achieve the stationary distribution exist only for a rather restricted class of chains, as the following result shows, stated here and in <ref> [LW4] </ref> without proof. 10 L ASZL O LOV ASZ AND PETER WINKLER Theorem 2.3. There is a finite blind stopping rule that generates from any starting state if and only if no eigenvalue of the transition matrix is a positive real number. <p> This is also easily seen by observing that p ij x i () is the expected number of passes from i to j while following . The next theorem, found also in <ref> [LW4] </ref>, implies that the exit frequencies of a stopping rule are "almost" determined by the starting and target distributions. Theorem 4.3. Fix two distributions and t , and let and 0 be two finite stopping rules from to t . <p> Thus if we hit a state past the release time we stop; otherwise we continue, except that if we're within 1, we use the fractional part to randomize. Theorems 4.9 and 4.10, from [LW2] and <ref> [LW4] </ref>, establish the universality and max-optimality of threshold rules. Theorem 4.9. For every starting distribution and target distribution t , there exists a threshold function h which gives an optimal stopping rule for generating t . Proof. We define this rule recursively as follows. <p> For the middle term, we use 4.2: X z k (t; t t ) = k2U m=0 k k ) m=0 Substituting these bounds, the theorem follows. (See <ref> [LW4] </ref> for a simple direct proof.) Theorem 4.22 asserts closeness in the total variation distance; approaching pointwise (as noted, for the reversible case, in [AF]) turns out to be somewhat harder in general. Below we establish results about the pointwise distance of distributions obtained by various averaging rules.
Reference: [LW5] <author> L. Lovasz and P. Winkler, </author> <title> Reversal of Markov chains and the forget time, Combinatorics, </title> <journal> Probability and Computing, </journal> <note> to appear. </note>
Reference-contexts: For the case of reversible chains, such results were obtained by Aldous [A2]; other results in this direction were published in <ref> [ALW, LW4, LW5] </ref>. To prove the main results, we develop a calculus of "exit frequencies", study the behavior of mixing properties under time reversal, and make use of some linear MIXING TIMES 3 algebraic tools. We hope that these tools shed some light on the mechanism of mixing. Acknowledgement. <p> To get the time needed to generate an independent sample state, we let both the default initial and target distributions for an independent rule be : I := I (; ) = i obtaining what we call the reset time of the chain (see <ref> [LW5] </ref>). This is only formally similar to Formula (2.2) for N ; in general, N is much larger. (Instead of , one might want to generate independent samples from some other distribution. <p> It was proved in <ref> [LW5] </ref> that this distribution is uniquely determined; see [LW5] for an explicit formula for . We can define approximate versions of the forget time, by F " := min t H ( (1 ")t ) etc. Discrepancy. <p> It was proved in <ref> [LW5] </ref> that this distribution is uniquely determined; see [LW5] for an explicit formula for . We can define approximate versions of the forget time, by F " := min t H ( (1 ")t ) etc. Discrepancy. This quantity is quite closely related to mixing measures, although its definition seems somewhat artificial. <p> We conclude by stating a theorem that implies that the mixing, maxing and relaxation groups are invariant under time reversal, while the forget and reset groups are interchanged. Parts (a) and (c) appear, with proofs, in <ref> [LW5] </ref>. Theorem 3.8. For every finite Markov chain, (a) H = H; (c) I = F and F = I. If the chain is time-reversible, then I = F , and hence by Theorem 3.1, F H. <p> Here we study exit frequencies and access times in reverse chains. The key to the proof of many properties of the reverse chain is the following general "duality formula", proved in <ref> [LW5] </ref>. Lemma 4.11. Let ff; fi; fl; ffi be four distributions on the states. Then X (fi i ff i ) y i (fl; ffi) = i We recall some consequences of this lemma. <p> Proof of Theorem 3.8. Parts (a) and (c) were proved in <ref> [LW5] </ref>. Part (b) is straightforward linear algebra. For part (d), note that Corollary 4.12 implies that H (i; j) H (; j) = H (j; i) H (; i), and hence the identity follows from formulas (2.9) and (2.10). 48 L ASZL O LOV ASZ AND PETER WINKLER
Reference: [P] <author> J.W. </author> <title> Pitman, Occupation measures for Markov chains, </title> <journal> Adv. Appl. Prob. </journal> <volume> 9 (1977), </volume> <pages> 69-86. </pages>
Reference-contexts: In the finite case, the use of x i or y i is only a matter of taste; in the case of Markov chains with infinite state space, the difference between x i and y i is quite significant. Exit frequencies are a special case of what Pitman <ref> [P] </ref> calls "pre-T occupation measures" for stopping times T . Indeed, versions of Lemma 4.2 and Theorem 4.4 below are proved in [P] using Pitman's "occupation measure identity". Exit frequencies for the naive rule are easy to find; see [AF]. <p> Exit frequencies are a special case of what Pitman <ref> [P] </ref> calls "pre-T occupation measures" for stopping times T . Indeed, versions of Lemma 4.2 and Theorem 4.4 below are proved in [P] using Pitman's "occupation measure identity". Exit frequencies for the naive rule are easy to find; see [AF]. Several related formulas could be derived using relations to electrical networks, as in [CRRST], [DS], or [T]. Lemma 4.1. <p> j) + H (j; k) H (i; k) : More generally, the exit frequencies for the naive stopping rule ;t are given by ~x k = k i;j = k @ i;j 1 Exit frequencies are related to the starting and ending distributions by a simple formula, found in Pitman <ref> [P] </ref>, which can be obtained by counting entrances and exits at a given state: Lemma 4.2.
Reference: [R] <author> D.H. </author> <title> Root, The existence of certain stopping times on Brownian motion, </title> <journal> Ann. Math. Statist. </journal> <volume> 40 (1969), </volume> <pages> 715-718. </pages>
Reference: [S] <author> A. Skorokhod, </author> <title> Studies in the Theory of Random Processes, </title> <publisher> orig. pub. Addison-Wesley (1965), 2nd ed. Dover, </publisher> <address> New York 1982. </address> <note> MIXING TIMES 49 </note>
Reference: [T] <author> P. Tetali, </author> <title> Random walks and effective resistance of networks, </title> <journal> J. Theoretical Prob. </journal> <volume> #1 (1991), </volume> <pages> 101-109. </pages> <institution> Dept. of Computer Science, Yale University, New Haven, CT 06510 E-mail address: lovasz@cs.yale.edu Bell Laboratories, </institution> <address> 700 Mountain Ave., Murray Hill, NJ 07974 E-mail address: pw@lucent.com </address>
Reference-contexts: Indeed, versions of Lemma 4.2 and Theorem 4.4 below are proved in [P] using Pitman's "occupation measure identity". Exit frequencies for the naive rule are easy to find; see [AF]. Several related formulas could be derived using relations to electrical networks, as in [CRRST], [DS], or <ref> [T] </ref>. Lemma 4.1. <p> Lemma 5.16. Let t = M (; ). Choose a random starting state from . Choose a random integer Y uniformly from the interval <ref> [t; 2t 1] </ref>, and walk for Y steps. Then the probability of being at state i is at most 2 i . Proof.
References-found: 29

