URL: http://www.cs.utexas.edu/users/rvdg/class/iCC_SHPCC94.ps
Refering-URL: http://www.cs.utexas.edu/users/rvdg/class/materials.html
Root-URL: 
Title: Interprocessor Collective Communication Library (InterCom)  
Author: Mike Barnett Satya Gupta David G. Payne Lance Shuler Robert van de Geijn Jerrell Watts 
Address: Moscow, Idaho 83844-1010 15201 N.W. Greenbrier Pkwy Beaverton, Oregon 97006  Austin, Texas 78712-1188 Austin, Texas 78758-4497  
Affiliation: Department of Computer Science Supercomputer Systems Division University of Idaho Intel Corporation  Department of Computer Sciences Center for High Performance Computing The University of Texas at Austin The University of Texas  
Abstract: In this paper, we outline a unified approach for building a library of collective communication operations that performs well on a cross-section of problems encountered in real applications. The target architecture is a two-dimensional mesh with worm-hole routing, but the techniques also apply to higher dimensional meshes and hypercubes. We stress a general approach, addressing the need for implementations that perform well for various sized vectors and grid dimensions, including non-power-of-two grids. This requires the development of general techniques for building hybrid algorithms. Finally, our approach also supports collective communication within a group of nodes, which is required by many scalable algorithms. Results from the Intel Paragon system are included. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> M. Barnett, R. Littlefield, D.G. Payne, and R. van de Geijn. </author> <title> Efficient Communication Primitives on Mesh Architectures with Hardware Routing, </title> <booktitle> Sixth SIAM Conf. on Par. Proc. for Sci. Comp., </booktitle> <address> Norfolk, Virginia, March 22-24, </address> <year> 1993. </year>
Reference-contexts: For a general purpose library, it is crucial that an implementation performs well for all vector lengths. In our previous papers on collective communication, we studied individual communication operations and their implementation, including possible hybrid approaches <ref> [1, 2, 3, 5] </ref>. It is through this progression of studies that we have discovered that all the aforementioned collective communication operations can be built from similar primitives. It is this observation that has led us to propose a unified approach to hybrid design.
Reference: [2] <author> M. Barnett, R. Littlefield, D.G. Payne, and R. van de Geijn, </author> <title> Global Combine on Mesh Architectures with Wormhole Routing, </title> <booktitle> 7th International Parallel Processing Symposium, </booktitle> <pages> pages 156-162, </pages> <publisher> IEEE Computer Society Press, </publisher> <address> Newport Beach, CA, </address> <month> April 13-16, </month> <year> 1993. </year>
Reference-contexts: For a general purpose library, it is crucial that an implementation performs well for all vector lengths. In our previous papers on collective communication, we studied individual communication operations and their implementation, including possible hybrid approaches <ref> [1, 2, 3, 5] </ref>. It is through this progression of studies that we have discovered that all the aforementioned collective communication operations can be built from similar primitives. It is this observation that has led us to propose a unified approach to hybrid design.
Reference: [3] <author> M. Barnett, D.G. Payne, R. van de Geijn, and J. Watts, </author> <title> Broadcasting on Meshes with WormHole Routing, </title> <journal> Journal of Parallel and Distributed Computing, </journal> <note> submitted. </note>
Reference-contexts: For a general purpose library, it is crucial that an implementation performs well for all vector lengths. In our previous papers on collective communication, we studied individual communication operations and their implementation, including possible hybrid approaches <ref> [1, 2, 3, 5] </ref>. It is through this progression of studies that we have discovered that all the aforementioned collective communication operations can be built from similar primitives. It is this observation that has led us to propose a unified approach to hybrid design.
Reference: [4] <author> C.-T. Ho and S. L. Johnsson, </author> <title> Distributed Routing Algorithms for Broadcasting and Personalized Communication in Hypercubes, </title> <booktitle> In Proceedings of the 1986 International Conference on Parallel Processing, </booktitle> <pages> pages 640-648, </pages> <publisher> IEEE, </publisher> <year> 1986. </year>
Reference-contexts: All results are for a 16fi32 mesh of nodes. 7 Other algorithms It should be noted that for some of the communications, optimal algorithms for long vectors exist that in theory outperform our approach. For example, on hypercubes Ho and Johnsson's EDST broadcast <ref> [4] </ref> will outperform our scatter/collect broadcast by a factor of two for long vectors. However, it is our experience that such pipelined algorithms are generally difficult to implement and are extremely architecture dependent.
Reference: [5] <author> R. A. van de Geijn. </author> <title> Efficient Global Combine Operations. </title> <booktitle> In Sixth Distributed Memory Computing Conference Proceedings, </booktitle> <pages> pages 291-294. </pages> <publisher> IEEE Computer Society Press, </publisher> <year> 1991. </year>
Reference-contexts: For a general purpose library, it is crucial that an implementation performs well for all vector lengths. In our previous papers on collective communication, we studied individual communication operations and their implementation, including possible hybrid approaches <ref> [1, 2, 3, 5] </ref>. It is through this progression of studies that we have discovered that all the aforementioned collective communication operations can be built from similar primitives. It is this observation that has led us to propose a unified approach to hybrid design.
Reference: [6] <author> D. W. Walker, </author> <title> The Design of a Standard Message Passing Interface for Distributed Memory Concurrent Computers. </title> <note> to appear in Parallel Computing, </note> <month> April </month> <year> 1994. </year> <title> Upto Date information about the MPI standard is available from netlib, directory mpi. </title>
Reference-contexts: 1 Introduction The Interprocessor Collective Communication (InterCom) Project is a comprehensive study of techniques for a high performance implementation of commonly used collective communication algorithms. It is the emphasis on a high performance implementation that sets it aside from the MPI effort <ref> [6] </ref>, which tries to standardize the interface to communication libraries.
References-found: 6

