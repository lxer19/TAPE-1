URL: ftp://ftp-pubs.lcs.mit.edu/pub/lcs-pubs/tr.outbox/MIT-LCS-TR-655.ps.gz
Refering-URL: ftp://ftp-pubs.lcs.mit.edu/pub/lcs-pubs/listings/tr600.html
Root-URL: 
Title: Reducing Synchronization Overhead in Parallel Simulation  
Author: by Ulana Legedza 
Address: Cambridge, Massachusetts 02139  
Affiliation: Massachusetts Institute of Technology Laboratory for Computer Science  
Note: c Massachusetts Institute of Technology 1995 This work was supported in part by the Advanced Research Projects Agency under Contract N00014-94-1-0985, by grants from IBM and AT&T, and by an equipment grant from DEC. Ulana Legedza was supported by a National Science Foundation Graduate Fellowship.  
Abstract: Technical Report MIT/LCS/TR-655 May 1995 
Abstract-found: 1
Intro-found: 1
Reference: [AR92] <author> Rassul Ayani and Hassan Rajaei. </author> <title> Parallel simulation using conservative time windows. </title> <booktitle> In Proceedings of the 1992 Winter Simulation Conference, </booktitle> <month> December </month> <year> 1992. </year>
Reference-contexts: Barrier synchronizations are used to separate iterations, while operation is asynchronous between barriers. These algorithms avoid deadlock and guarantee the same semantics and correctness as the BCM approaches. Examples of synchronous algorithms can be found in [Lub88] [Aya89] <ref> [AR92] </ref> [CS89] [SBW88] [Ste91] (plus 59 one more reference: yawns). 7.2 Nearest Neighbor Synchronization Synchronous techniques often suffer in performance because of long waiting times at global synchronizations. As described previously, nearest neighbor synchronization can be useful in alleviating this problem.
Reference: [ASU86] <author> Alfred A. Aho, Ravi Sethi, and Jeffrey D. Ullman. </author> <booktitle> Compilers: Principles, Techniques, and Tools. </booktitle> <publisher> Addison-Wesley, </publisher> <year> 1986. </year>
Reference-contexts: In order to compute all the minimum distances to non-local operations from all the basic blocks in the graph, the following iterative dataflow algorithm (based on algorithm 10.2 in <ref> [ASU86] </ref>) is used: FOREACH basic block B in graph estimate (B) = sizeof (B) ENDFOR DO change = false FOREACH basic block B in graph 45 old estimate = estimate (B) if (B ends in procedure call) then estimate (B) = sizeof (B) else if (B has 1 successor) then estimate
Reference: [Aya89] <author> Rassul Ayani. </author> <title> A parallel simulation scheme based on distances between objects. </title> <booktitle> In Proceedings of the 1989 SCS Multiconference on Distributed Simulation, </booktitle> <month> January </month> <year> 1989. </year>
Reference-contexts: Barrier synchronizations are used to separate iterations, while operation is asynchronous between barriers. These algorithms avoid deadlock and guarantee the same semantics and correctness as the BCM approaches. Examples of synchronous algorithms can be found in [Lub88] <ref> [Aya89] </ref> [AR92] [CS89] [SBW88] [Ste91] (plus 59 one more reference: yawns). 7.2 Nearest Neighbor Synchronization Synchronous techniques often suffer in performance because of long waiting times at global synchronizations. As described previously, nearest neighbor synchronization can be useful in alleviating this problem.
Reference: [BB94] <author> Eric A. Brewer and Robert Blumofe. Strata: </author> <title> A multi-layer communications library, version 2.0 beta, </title> <month> February </month> <year> 1994. </year>
Reference-contexts: The following pseudocode illustrates how periodic global barriers are implemented in Parallel Proteus: DO Poll Network; difference = msgs sent - msgs received; result = Strata Global Add (difference); WHILE (result != 0); Implemented using Strata CM5 control network reduction functions <ref> [BB94] </ref>, each global synchronization takes 150 cycles (1 cycle = 30 nanoseconds) for each global reduction performed (usually 1 or 2), plus waiting time. As in described in the previous section, the synchronization time quantum Q for the networks Parallel Proteus simulates is equal to two cycles of simulated time. <p> The next barrier is scheduled for this time. The first step, synchronization, is performed exactly as in periodic global barriers, with a Strata CM5 control network reduction function <ref> [BB94] </ref>. This step includes synchronizing all host processors as well as waiting for the host network to drain. The second step involves searching the event queue. In Parallel Proteus, as in sequential Proteus, the event queue is implemented as an array.
Reference: [BD92] <author> Eric A. Brewer and Chrysanthos N. Dellarocas. </author> <title> Proteus user documentation, </title> <note> version 0.5, </note> <month> December </month> <year> 1992. </year>
Reference: [BDCW91] <author> Eric A. Brewer, Chrysanthos N. Dellarocas, Adrian Colbrook, and William E. Weihl. Proteus: </author> <title> A high-performance parallel architecture simulator. </title> <type> MIT/LCS 516, </type> <institution> Massachusetts Institute of Technology, </institution> <month> September </month> <year> 1991. </year>
Reference-contexts: Predictive barrier scheduling, a novel technique which schedules synchronizations using both compile-time and runtime analysis, reduces the frequency of synchronization operations. This thesis also reports on the design and implementation of Parallel Proteus, a parallel simulator of message-passing multicomputers. It is based on the fast sequential simulator, Proteus <ref> [BDCW91] </ref>. Like all sequential simulators, sequential Proteus is time- and space-limited. It is adequate for simulating small and short applications running on machines not larger than 512 nodes (more memory would help capacity but not speed). Large simulations either take too long or run out of memory. <p> As its name suggests, it is a parallel version of Proteus, the sequential simulator developed at MIT by Brewer and Dellarocas <ref> [BDCW91] </ref>. Parallel Proteus runs on a Thinking Machines' CM5. This section describes some of the techniques used in Parallel Proteus to provide accurate simulation. Some of these are very similar to or the same as those in sequential Proteus, while others are unique to the parallel version.
Reference: [Bed95] <author> Robert C. Bedichek. Talisman: </author> <title> fast and accurate multicomputer simulation. </title> <booktitle> In Proceedings of SIGMETRICS '95, </booktitle> <year> 1995. </year>
Reference-contexts: While the local barrier approach and predictive barrier scheduling have been evaluated in the context of a direct-execution simulator, they are certainly also applicable to simulators which simulate application code execution in a different way (e.g., Bedichek's threaded code simulator 57 <ref> [Bed95] </ref>). First, local barriers do not depend on direct execution in any way. Second, while my implementation of predictive barrier scheduling does rely on Parallel Proteus's support for direct execution, this is not essential to the technique.
Reference: [Bre92] <author> Eric A. Brewer. </author> <title> Aspects of a parallel-architecture simulator. </title> <type> MIT/LCS 527, </type> <institution> Mas-sachusetts Institute of Technology, </institution> <month> February </month> <year> 1992. </year>
Reference-contexts: Local instructions, which usually account for most of the instructions in application code, are executed directly. In order to correctly account for execution time of local instructions, application code is augmented with cycle-counting instructions. This is done with the Proteus utility augment <ref> [Bre92] </ref>. Augment reads the assembly language version of the application, divides it into basic blocks, and generates augmented assembly language code.
Reference: [Bry77] <author> R. E. Bryant. </author> <title> Simulation of packet communication architecture computer systems. </title> <type> MIT/LCS 188, </type> <institution> Massachusetts Institute of Technology, </institution> <month> November </month> <year> 1977. </year>
Reference-contexts: Chapter 7 Related Work This chapter discusses the evolution of parallel simulation techniques, the use of nearest neighbor synchronization and lookahead in other parallel simulators, and the techniques used in other parallel simulators of parallel computers. 7.1 Parallel Simulation Techniques Bryant, Chandy, and Misra developed the first conservative PDES algorithms <ref> [Bry77] </ref> [CM79] [CM81] . In both the Bryant and Chandy-Misra approaches to conservative PDES, LP's do not synchronize regularly, but only during the transmission of event transfer messages called for by communication in the simulated system.
Reference: [BT] <author> Bertsekas and Tsitsikus. </author> <title> Parallel and Distributed Computing Numerical Methods. </title> <type> 70 </type>
Reference-contexts: A more detailed description can be found in <ref> [BT] </ref>. I used grid sizes of 2 11 entries to 2 20 entries, and up to 625 target processors. I set the algorithm to perform 10 iterations.
Reference: [BW95] <author> Douglas C. Burger and David A. Wood. </author> <title> Accuracy vs. performance in parallel simulation of interconnection networks. </title> <booktitle> In Proceedings of the 9th International Parallel Processing Symposium, </booktitle> <month> April </month> <year> 1995. </year>
Reference-contexts: WWT achieves good performance in part because of this large time quantum (a consequence of a very simple network simulation model). In more recent work on the Wisconsin Wind Tunnel, several more accurate network simulation modules have been added <ref> [BW95] </ref>. The most accurate ones (Approximate and Baseline) exploit lookahead proportional to the minimum message size in the simulated system. The first scheme (Approximate) distributes network simulation among the processors of the host machine (as does the hop-by-hop model in Parallel Proteus), and models contention and message delivery times approximately.
Reference: [CGHM93] <author> David R. Cheriton, Hendrik A. Goosen, Hugh Holbrook, and Philip Machanick. </author> <title> Restructuring a parallel simulation to improve behavior in a shared-memory multiprocessor: The value of distributed synchronization. </title> <booktitle> In Proceedings of the Seventh Workshop on Parallel and Distributed Simulation, </booktitle> <pages> pages 159-162, </pages> <month> May </month> <year> 1993. </year>
Reference-contexts: Cheriton et al: have looked into the problem of long barrier waiting times in their particle-based wind tunnel simulation which runs on a shared memory multiprocessor <ref> [CGHM93] </ref>. They observed better performance with nearest-neighbor (local barrier) synchronization for this application. Lubachevsky improved the performance of an Ising spin model simulation with a technique similar to local barriers [Lub89a]. His algorithm involves several steps. First, the minimum simulation time over all the nodes is computed and broadcast.
Reference: [CM79] <author> K. Mani Chandy and Jayadev Misra. </author> <title> Distributed simulation: A case study in design and verification of distributed programs. </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> SE-5(5):440-452, </volume> <month> September </month> <year> 1979. </year>
Reference-contexts: 7 Related Work This chapter discusses the evolution of parallel simulation techniques, the use of nearest neighbor synchronization and lookahead in other parallel simulators, and the techniques used in other parallel simulators of parallel computers. 7.1 Parallel Simulation Techniques Bryant, Chandy, and Misra developed the first conservative PDES algorithms [Bry77] <ref> [CM79] </ref> [CM81] . In both the Bryant and Chandy-Misra approaches to conservative PDES, LP's do not synchronize regularly, but only during the transmission of event transfer messages called for by communication in the simulated system.
Reference: [CM81] <author> K. Mani Chandy and Jayadev Misra. </author> <title> Asynchronous distributed simulation via a sequence of parallel computations. </title> <journal> Communications of the ACM, </journal> <volume> 24(11) </volume> <pages> 198-206, </pages> <month> April </month> <year> 1981. </year>
Reference-contexts: Related Work This chapter discusses the evolution of parallel simulation techniques, the use of nearest neighbor synchronization and lookahead in other parallel simulators, and the techniques used in other parallel simulators of parallel computers. 7.1 Parallel Simulation Techniques Bryant, Chandy, and Misra developed the first conservative PDES algorithms [Bry77] [CM79] <ref> [CM81] </ref> . In both the Bryant and Chandy-Misra approaches to conservative PDES, LP's do not synchronize regularly, but only during the transmission of event transfer messages called for by communication in the simulated system.
Reference: [Cor92] <institution> Thinking Machines Corporation. </institution> <type> CM-5 Technical Summary. </type> <year> 1992. </year>
Reference: [CS89] <author> K. M. Chandy and R. Sherman. </author> <title> The conditional event approach to distributed simulation. </title> <booktitle> In Proceedings of the 1989 SCS Multiconference on Distributed Simulation, </booktitle> <month> January </month> <year> 1989. </year>
Reference-contexts: Barrier synchronizations are used to separate iterations, while operation is asynchronous between barriers. These algorithms avoid deadlock and guarantee the same semantics and correctness as the BCM approaches. Examples of synchronous algorithms can be found in [Lub88] [Aya89] [AR92] <ref> [CS89] </ref> [SBW88] [Ste91] (plus 59 one more reference: yawns). 7.2 Nearest Neighbor Synchronization Synchronous techniques often suffer in performance because of long waiting times at global synchronizations. As described previously, nearest neighbor synchronization can be useful in alleviating this problem.
Reference: [DHN94] <author> Philip M. Dickens, Philip Heidelberger, and David M. Nicol. </author> <title> Parallelized direct execution simulation of message-passing parallel programs. </title> <type> Technical Report 94-50, </type> <institution> ICASE, </institution> <month> June </month> <year> 1994. </year>
Reference-contexts: Because all events are non-local in this simulation, each host processor determines the length of each opaque period by calculating the minimum of the times of the next events on each of its neighbors. LAPSE is a conservative parallel simulator of the message-passing Intel Paragon <ref> [DHN94] </ref>. It achieves good performance by exploiting two sources of lookahead. First, like barrier collapsing, LAPSE lets some application code execute in advance of the simulation of its timing.
Reference: [Fuj88] <author> Richard M. Fujimoto. </author> <title> Performance measurements of distributed simulation. </title> <booktitle> In Proceedings of the SCS Multiconference on Distributed Simulation, </booktitle> <pages> pages 14-20, </pages> <month> February </month> <year> 1988. </year>
Reference-contexts: This has been documented by [Nic91] and [LL89]. Fujimoto also demonstrated the importance of lookahead in achieving good performance with experiments on synthetic workloads using the Chandy-Misra deadlock avoidance algorithm <ref> [Fuj88] </ref>. The following are two examples of conservative parallel simulators whose good performance is due to improved lookahead. Lubachevsky's opaque periods improve the performance of his Ising spin model simulator by increasing the number of safe events found at each iteration of his algorithm [Lub89a].
Reference: [Fuj90] <author> Richard M Fujimoto. </author> <title> Parallel discrete event simulation. </title> <journal> Communiations of the ACM, </journal> <volume> 33(10) </volume> <pages> 30-53, </pages> <month> October </month> <year> 1990. </year>
Reference-contexts: The ability to predict the future behavior of the simulated system is called lookahead. A simulator is said to have lookahead L if at simulated time t it can predict all events it will generate up to simulated time t + L <ref> [Fuj90] </ref>. This ability to predict future events and event transfer messages makes lookahead useful in determining how long simulator LP's can run between synchronizations. A large amount of lookahead enables a simulator to avoid synchronizing during long periods of simulated time during which the simulated entities are not communicating.
Reference: [Gol93] <author> Stephen R. Goldschmidt. </author> <title> Simulation of Multiprocessors: Accuracy and Performance. </title> <type> PhD thesis, </type> <institution> Stanford University, </institution> <month> June </month> <year> 1993. </year> <month> 71 </month>
Reference-contexts: They contend that fast approximate simulators are more valuable than slow accurate ones. 7.4.2 Parallel Tango Lite Parallel Tango Lite is a parallel simulator of shared memory architectures which runs on the DASH machine <ref> [Gol93] </ref>. It uses the same simple network model and conservative PDES algorithm as the Wisconsin Wind Tunnel, also synchronizing every 100 timesteps. Experiments were done using small workloads (24 target processors) on small host machine configurations (up to 12 host processors).
Reference: [JCM93] <author> Kirk Johnson, David Chaiken, and Alan Mainwaring. Nwo-p: </author> <title> Parallel simulation of the alewife machine. </title> <booktitle> In Proceedings of the 1993 MIT Student Workshop on Supercomputing Technologies, </booktitle> <month> August </month> <year> 1993. </year>
Reference-contexts: because no synchronization is performed during the simulation process; instead, synchronization markers are inserted into the trace files and later used to correctly order the memory references listed in the traces. 7.4.4 NWO-P NWO-P is the parallel version of NWO, a detailed sequential simulator of the MIT Alewife distributed-memory multiprocessor <ref> [JCM93] </ref>. NWO-P operates in lock-step, synchronizing every cycle of simulated time.
Reference: [Joh95] <author> Kirk Johnson. </author> <type> Personal communication. </type> <month> March </month> <year> 1995. </year>
Reference-contexts: NWO-P operates in lock-step, synchronizing every cycle of simulated time. Because NWO-P simulates each cycle of Alewife in a great amount of detail, the overhead for simulating each cycle is very high and far outweighs the synchronization overhead <ref> [Joh95] </ref>. 7.5 Discussion Of all the work described in this chapter, the Wisconsin Wind Tunnel (WWT), Parallel Tango Lite (PTL), and LAPSE are the most closely related to Parallel Proteus.
Reference: [LL89] <author> Yi-Bing Lin and Edward D. Lazowska. </author> <title> Exploiting lookahead in parallel simulation. </title> <type> Technical Report 89-10-06, </type> <institution> University of Washington Department of Computer Science and Engineering, </institution> <month> October </month> <year> 1989. </year>
Reference-contexts: They attribute this improvement in part to the elimination of unnecessary waiting [YA93]. 60 7.3 Improving Lookahead Lookahead, the ability to predict the future communication behavior of a simulated system, can be quite useful in reducing synchronization overhead in conservative parallel simulators. This has been documented by [Nic91] and <ref> [LL89] </ref>. Fujimoto also demonstrated the importance of lookahead in achieving good performance with experiments on synthetic workloads using the Chandy-Misra deadlock avoidance algorithm [Fuj88]. The following are two examples of conservative parallel simulators whose good performance is due to improved lookahead.
Reference: [Lub88] <author> Boris D. Lubachevsky. </author> <title> Bounded lag distributed discrete event simulation. </title> <booktitle> In Proceedings of the SCS Multiconference on Distributed Simulation, </booktitle> <pages> pages 183-191, </pages> <month> January </month> <year> 1988. </year>
Reference-contexts: These involve iteratively determining which events are safe to process before executing them. Barrier synchronizations are used to separate iterations, while operation is asynchronous between barriers. These algorithms avoid deadlock and guarantee the same semantics and correctness as the BCM approaches. Examples of synchronous algorithms can be found in <ref> [Lub88] </ref> [Aya89] [AR92] [CS89] [SBW88] [Ste91] (plus 59 one more reference: yawns). 7.2 Nearest Neighbor Synchronization Synchronous techniques often suffer in performance because of long waiting times at global synchronizations. As described previously, nearest neighbor synchronization can be useful in alleviating this problem.
Reference: [Lub89a] <author> Boris D. Lubachevsky. </author> <title> Efficient distributed event-driven simulations of multiple-loop networks. </title> <journal> Communications of the ACM, </journal> <volume> 32(1) </volume> <pages> 111-123, </pages> <month> January </month> <year> 1989. </year>
Reference-contexts: They observed better performance with nearest-neighbor (local barrier) synchronization for this application. Lubachevsky improved the performance of an Ising spin model simulation with a technique similar to local barriers <ref> [Lub89a] </ref>. His algorithm involves several steps. First, the minimum simulation time over all the nodes is computed and broadcast. <p> The following are two examples of conservative parallel simulators whose good performance is due to improved lookahead. Lubachevsky's opaque periods improve the performance of his Ising spin model simulator by increasing the number of safe events found at each iteration of his algorithm <ref> [Lub89a] </ref>. Because all events are non-local in this simulation, each host processor determines the length of each opaque period by calculating the minimum of the times of the next events on each of its neighbors. LAPSE is a conservative parallel simulator of the message-passing Intel Paragon [DHN94].
Reference: [Lub89b] <author> Boris D. Lubachevsky. </author> <title> Scalability of the bounded lag distributed discrete event simulation. </title> <booktitle> In Proceedings of the SCS Multiconference on Distributed Simulation, </booktitle> <pages> pages 100-107, </pages> <month> March </month> <year> 1989. </year>
Reference: [Nic91] <author> David M. Nicol. </author> <title> Performance bounds on parallel self-initiating discrete-event simulations. </title> <journal> ACM Transactions on Modeling and Computer Simulations, </journal> <volume> 1(1) </volume> <pages> 24-50, </pages> <year> 1991. </year>
Reference-contexts: They attribute this improvement in part to the elimination of unnecessary waiting [YA93]. 60 7.3 Improving Lookahead Lookahead, the ability to predict the future communication behavior of a simulated system, can be quite useful in reducing synchronization overhead in conservative parallel simulators. This has been documented by <ref> [Nic91] </ref> and [LL89]. Fujimoto also demonstrated the importance of lookahead in achieving good performance with experiments on synthetic workloads using the Chandy-Misra deadlock avoidance algorithm [Fuj88]. The following are two examples of conservative parallel simulators whose good performance is due to improved lookahead.
Reference: [RHL + 93] <author> Steven K. Reinhardt, Mark D. Hill, James R. Larus, Alvin R. Lebeck, James C.Lewis, and David A. Wood. </author> <title> The wisconsin wind tunnel: Virtual prototyping of parallel compuers. </title> <booktitle> In Proceedings of the 1993 ACM SIGMETRICS Conference, </booktitle> <month> May </month> <year> 1993. </year>
Reference-contexts: As a result, less accurate simulation in Parallel Proteus requires 50 times fewer synchronizations than the hop by hop model. This technique is also used by the Wisconsin Wind Tunnel parallel simulator <ref> [RHL + 93] </ref>. 19 3.2.5 Synchronization To avoid causality errors in Parallel Proteus, host processors are synchronized with periodic global barriers. <p> since synchronization will be needed much more frequently. 61 7.4 Other Parallel Simulators of Parallel Computers This section describes several other parallel simulators of multicomputers (in addition to LAPSE). 7.4.1 Wisconsin Wind Tunnel The Wisconsin Wind Tunnel is a parallel simulator of shared memory architectures which runs on the CM5 <ref> [RHL + 93] </ref>. It uses a synchronous conservative PDES algorithm with periodic global barriers and a synchronization time quantum of 100 cycles. WWT achieves good performance in part because of this large time quantum (a consequence of a very simple network simulation model).
Reference: [SBW88] <author> Lisa M. Sokol, Duke P. Briscoe, and Alexis P. Wieland. Mtw: </author> <title> A strategy for scheduling discrete simulation events for concurrent execution. </title> <booktitle> In Proceedings of the 1988 SCS multiconference on distributed simulation, </booktitle> <month> January </month> <year> 1988. </year>
Reference-contexts: Barrier synchronizations are used to separate iterations, while operation is asynchronous between barriers. These algorithms avoid deadlock and guarantee the same semantics and correctness as the BCM approaches. Examples of synchronous algorithms can be found in [Lub88] [Aya89] [AR92] [CS89] <ref> [SBW88] </ref> [Ste91] (plus 59 one more reference: yawns). 7.2 Nearest Neighbor Synchronization Synchronous techniques often suffer in performance because of long waiting times at global synchronizations. As described previously, nearest neighbor synchronization can be useful in alleviating this problem.
Reference: [SDRG + 86] <author> K. So, F. Darema-Rogers, D.A. George, V.A. Norton, </author> <title> and G.F. Pfister. Psimul - a system for parallel simulation of the execution of parallel programs. </title> <type> RC 11674, </type> <institution> IBM T.J. Watson Research Center, </institution> <month> January </month> <year> 1986. </year> <month> 72 </month>
Reference-contexts: The performance of the simulator improved slightly when multiple target processors were simulated on each host processor. 7.4.3 PSIMUL PSIMUL is a parallel simulator of shared memory architectures which runs on the 2-CPU IBM System 3081 <ref> [SDRG + 86] </ref>. It is used to collect memory traces of parallel programs.
Reference: [Ste91] <author> J. Steinman. Speedes: </author> <title> synchronous parallel environment for emulation and discrete event simulation. </title> <booktitle> Proceedings of the SCS western multiconference on advances in parallel and distributed simulation, </booktitle> <volume> 23(1) </volume> <pages> 95-103, </pages> <year> 1991. </year>
Reference-contexts: Barrier synchronizations are used to separate iterations, while operation is asynchronous between barriers. These algorithms avoid deadlock and guarantee the same semantics and correctness as the BCM approaches. Examples of synchronous algorithms can be found in [Lub88] [Aya89] [AR92] [CS89] [SBW88] <ref> [Ste91] </ref> (plus 59 one more reference: yawns). 7.2 Nearest Neighbor Synchronization Synchronous techniques often suffer in performance because of long waiting times at global synchronizations. As described previously, nearest neighbor synchronization can be useful in alleviating this problem.
Reference: [vECGS92] <author> T. von Eiken, D.E. Culler, S.C. Goldstein, and K.E. Schauser. </author> <title> Active messages: a mechanism for integrated communication and computation. </title> <booktitle> In Proceedings of the 19th International Symposium on Computer Architecture, </booktitle> <pages> pages 256-266, </pages> <month> May </month> <year> 1992. </year>
Reference-contexts: Each processor node consists of a processor, memory, and a network chip. The network chip provides the processor with access to the network, and acts as a router in the network. Each processor runs one or more application threads which communicate with threads on other processors via active messages <ref> [vECGS92] </ref> (or inter-processor interrupts). An active message is a packet of data which travels through the network and causes an asynchronous interrupt upon reception at the target processor.
Reference: [YA93] <author> Donald Yeung and Anant Agarwal. </author> <title> Experience with fine-grain synchronization in mimd machines for preconditioned conjugate gradient. </title> <booktitle> In Proceedings of the 4th Symposium on principles and practice of parallel programming, </booktitle> <month> May </month> <year> 1993. </year> <month> 73 </month>
Reference-contexts: Nearest neighbor synchronization can be viewed as a form of fine grain synchronization. Yeung and Agarwal report a significant performance improvement for preconditioned conjugate gradient when using fine grain synchronization instead of global synchronization. They attribute this improvement in part to the elimination of unnecessary waiting <ref> [YA93] </ref>. 60 7.3 Improving Lookahead Lookahead, the ability to predict the future communication behavior of a simulated system, can be quite useful in reducing synchronization overhead in conservative parallel simulators. This has been documented by [Nic91] and [LL89].
References-found: 33

