URL: ftp://ftp.cs.umass.edu/pub/osl/papers/Umass-TR97-65.ps.gz
Refering-URL: http://spa-www.cs.umass.edu/bibliography.html
Root-URL: 
Email: dropsho@cs.umass.edu weems@cs.umass.edu  
Title: Comparing Caching Techniques for Multitasking Real-Time Systems  
Author: Steve Dropsho Chip Weems 
Date: April 30, 1997  
Affiliation: Computer Science Department University of Massachusetts-Amherst  
Abstract: Correctness in real-time computing depends on the logical result and the time when it is available. Real-time operating systems need to know the timing behavior of applications to ensure correct real-time system behavior. Thus, predictability in the underlying hardware operation is required. Unfortunately, standard, embedded cache management policies in microprocessors are designed for excellent probabilistic behavior but lack predictability, especially in a multitasking environment. In this article we examine the two popular cache management policies that support predictable cache behavior in a multitasking environment and quantitatively compare them. Using a novel application of an existing analytical cache model we show that neither policy is best in general and delimit the system characteristics where each is most effective. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> J.E. Gillis an dG.H. Weiss. </author> <title> Expected Number of Distinct Sites Visited by a Random Walk with an Infinite Variance. </title> <journal> J. Math. Phys., </journal> <pages> pages 1307-1312, </pages> <month> April </month> <year> 1970. </year>
Reference-contexts: The value C c is called the critical cache size and is defined in equation 2, x is the cache size. C c marks the transition from the cache warm-up phase to steady state behavior. Equations 1 and 2 are motivated by previous work of Gillis and Weiss <ref> [1] </ref> on hyperbolic random walks and supporting empirical observations by Kobayashi and MacDougall [3] and Thi bault [16].
Reference: [2] <author> N.C. Audsley and K.W. Tindell. </author> <title> On Priorities in Fixed Priority Scheduling. </title> <type> Technical Report TR 95-???, </type> <institution> Dept of CS, Uppsala University of Sweden, </institution> <month> May </month> <year> 1995. </year>
Reference-contexts: Partitioning only works if each application maps into a cache region without contention from another application. In a real-time system there may be hundreds of tasks, however, this does not necessitate hundreds of partitions. Real-time tasks are typically grouped by priority levels. Audsley and Tinsdell <ref> [2] </ref> show that tasks at the same priority level are scheduled non-preemptively relative to each other in an otherwise preemptive system with FIFO scheduling at the same priority level. Thus, there only needs to be one partition per priority level.
Reference: [3] <author> M. Dobayashi and M. MacDougall. </author> <title> The Stack Growth Function: Cache Line Refence Models. </title> <journal> IEEE Transactions on Computers, </journal> <pages> pages 789-805, </pages> <month> June </month> <year> 1989. </year>
Reference-contexts: C c marks the transition from the cache warm-up phase to steady state behavior. Equations 1 and 2 are motivated by previous work of Gillis and Weiss [1] on hyperbolic random walks and supporting empirical observations by Kobayashi and MacDougall <ref> [3] </ref> and Thi bault [16].
Reference: [4] <author> Steve Dropsho. </author> <title> RISC Processor Worst-Case Execution Time Penalties. </title> <type> Technical Report TR-95-110, </type> <institution> University of Massachusetts- Amherst, </institution> <year> 1995. </year>
Reference-contexts: The lack of predictability forces WCET estimates that are overly pessimistic, possibly by an order of magnitude or more <ref> [4] </ref>. One solution is to bypass the cache and access memory directly. While this removes caching variability, it also restricts processor performance to the speed of main memory and limits the ability of real-time systems to track performance gains due to new processor technology.
Reference: [5] <author> Christopher A. Healy, David B. Whalley, and Marion G. Harmon. </author> <title> Integrating the Timing Analysis of Pipelining and Instruction Caching. </title> <booktitle> Proc. of the IEEE Real-Time Systems Symposium, </booktitle> <month> December </month> <year> 1995. </year>
Reference-contexts: cache policy effectiveness is highly system dependent, and delimits the conditions where each policy works well. 2 Related Work Much of the research in real-time predictable cache behavior has focussed on analyzing an application's behavior with respect to a uniprocess system that does not suffer interprocess contention for the cache <ref> [13, 10, 14, 7, 5] </ref>. For a multitasking environment, two basic methods have been proposed for controlling interprocess cache interference for predictable cache behavior. The first method relies on dividing the cache into distinct partitions and the other restricts where context switches can occur.
Reference: [6] <author> D.B. Kirk. </author> <title> SMART (Strategic Memory Allocation for Real-Time) Cache Design. </title> <booktitle> Proceedings of the Tenth IEEE Real-Time Systems Symposium, </booktitle> <pages> pages 229-237, </pages> <month> December </month> <year> 1989. </year>
Reference-contexts: For a multitasking environment, two basic methods have been proposed for controlling interprocess cache interference for predictable cache behavior. The first method relies on dividing the cache into distinct partitions and the other restricts where context switches can occur. Kirk and Strosnider <ref> [6] </ref> detail a hardware design for the 2 MIPS R3000 that allows a cache to be partitioned among processes. Mueller [9] implemented a compiler that partitions the cache strictly through software, via positioning of code. <p> Mueller [9] presents details of a software only method that logically partitions the cache, thus avoiding the need of hardware support that physical partitioning would require <ref> [6] </ref>. In his scheme the compiler breaks application code into blocks and maps them into memory such that the application becomes restricted to only a portion of the cache via the normal cache mapping process. The compiler adds branches to skip over the gaps created by the code positioning.
Reference: [7] <author> Jyh-Charn Liu and Hung-Ju Lee. </author> <title> Deterministic Upperbounds of the Worst-Case Execution Times of Cached Programs. </title> <booktitle> IEEE Real-Time Systems Symposium, </booktitle> <year> 1994. </year>
Reference-contexts: cache policy effectiveness is highly system dependent, and delimits the conditions where each policy works well. 2 Related Work Much of the research in real-time predictable cache behavior has focussed on analyzing an application's behavior with respect to a uniprocess system that does not suffer interprocess contention for the cache <ref> [13, 10, 14, 7, 5] </ref>. For a multitasking environment, two basic methods have been proposed for controlling interprocess cache interference for predictable cache behavior. The first method relies on dividing the cache into distinct partitions and the other restricts where context switches can occur.
Reference: [8] <author> J.J. Molini, S.K. Maimon, and P.H. Watson. </author> <title> Real-Time System Scenarios. </title> <booktitle> Real-Time Systems Symposium, </booktitle> <pages> pages 214-225, </pages> <month> December </month> <year> 1990. </year>
Reference-contexts: Current research shows significant progress in predicting cache behavior for a single task [11]. However, complex real-time systems can involve multitasking with processes sharing the CPU via timeslicing <ref> [8] </ref>. Within a single process a compiler might be expected to predetermine the code's behavior with respect to the cache. Across multiple tasks with interrupts on a dynamic and changing schedule, accurate prediction of cache behavior is intractable, in general.
Reference: [9] <author> Frank Mueller. </author> <title> Compiler Support for Software-Based Cache Partitioning. </title> <booktitle> ACM Sigplan Workshop on Languages, Compilers and Tools for Real-Time Systems, </booktitle> <pages> pages 125-133, </pages> <month> June </month> <year> 1995. </year>
Reference-contexts: While such a tradeoff is generally unattractive in non-real-time systems, real-time designers worried about WCET view it as a significant improvement. This article analyzes two fundamental methods of managing caches for predictable behavior. The two methods are software-based and have been presented previously in the literature <ref> [12, 9] </ref>. We quantitatively compare the policies by leveraging an existing analytical cache model in a novel manner that allows exploration of system performance across a wide range of designs. <p> The first method relies on dividing the cache into distinct partitions and the other restricts where context switches can occur. Kirk and Strosnider [6] detail a hardware design for the 2 MIPS R3000 that allows a cache to be partitioned among processes. Mueller <ref> [9] </ref> implemented a compiler that partitions the cache strictly through software, via positioning of code. On the other hand, the Spring Real-Time System [12] controls where context switches can occur using a software technique that defines regions of code during which interrupts are masked. <p> For this reason, we will use the more descriptive term critical code regions (CCR) synonymously with SOS policy. 3.1.2 TOS Implementation A TOS policy requires partitioning of the cache to prevent processes from conflicting on cache lines. Mueller <ref> [9] </ref> presents details of a software only method that logically partitions the cache, thus avoiding the need of hardware support that physical partitioning would require [6].
Reference: [10] <author> Frank Mueller and David B. Whalley. </author> <title> Fast Instruction Cache Analysis via Static Cache Simulation. </title> <type> Technical report, </type> <institution> Florida State University, </institution> <year> 1994. </year> <booktitle> To appear in Proceedings of the 28th Annual Simulation Symposium, </booktitle> <month> April </month> <year> 1995. </year>
Reference-contexts: cache policy effectiveness is highly system dependent, and delimits the conditions where each policy works well. 2 Related Work Much of the research in real-time predictable cache behavior has focussed on analyzing an application's behavior with respect to a uniprocess system that does not suffer interprocess contention for the cache <ref> [13, 10, 14, 7, 5] </ref>. For a multitasking environment, two basic methods have been proposed for controlling interprocess cache interference for predictable cache behavior. The first method relies on dividing the cache into distinct partitions and the other restricts where context switches can occur.
Reference: [11] <author> Frank Mueller, David B. Whalley, and Marion Harmon. </author> <title> Predicting Instruction Cache Behavior. </title> <booktitle> ACM SIGPLAN Workshop on Language, Compiler and Tool Support for Real-Time Systems, </booktitle> <month> June </month> <year> 1994. </year>
Reference-contexts: While this removes caching variability, it also restricts processor performance to the speed of main memory and limits the ability of real-time systems to track performance gains due to new processor technology. Current research shows significant progress in predicting cache behavior for a single task <ref> [11] </ref>. However, complex real-time systems can involve multitasking with processes sharing the CPU via timeslicing [8]. Within a single process a compiler might be expected to predetermine the code's behavior with respect to the cache.
Reference: [12] <author> D. Niehaus, E. Nahum, and J.A. Stankovic. </author> <title> Predictable Real-Time Caching in the Spring System. </title> <booktitle> IFAC Real-Time Programming, </booktitle> <pages> pages 79-83, </pages> <year> 1992. </year>
Reference-contexts: While such a tradeoff is generally unattractive in non-real-time systems, real-time designers worried about WCET view it as a significant improvement. This article analyzes two fundamental methods of managing caches for predictable behavior. The two methods are software-based and have been presented previously in the literature <ref> [12, 9] </ref>. We quantitatively compare the policies by leveraging an existing analytical cache model in a novel manner that allows exploration of system performance across a wide range of designs. <p> Kirk and Strosnider [6] detail a hardware design for the 2 MIPS R3000 that allows a cache to be partitioned among processes. Mueller [9] implemented a compiler that partitions the cache strictly through software, via positioning of code. On the other hand, the Spring Real-Time System <ref> [12] </ref> controls where context switches can occur using a software technique that defines regions of code during which interrupts are masked. <p> By turning off interrupts in such a code segment, a compiler can accurately estimate the cache behavior between the entry and exit points. The larger the code region, the more efficient is the cache usage. This cache policy is precisely that of the Real-Time Spring System <ref> [12] </ref>. The reader may note the similarity of the SOS policy to using critical code regions for protecting access to shared variables and resources.
Reference: [13] <author> D. B. Whalley R. Arnold, F. Mueller and M. Harmon. </author> <title> Bounding Worst-Case Instruction Cache Performance. </title> <booktitle> IEEE Symposium on Real-Time Systems, </booktitle> <pages> pages 172-181, </pages> <month> Dec. </month> <year> 1994. </year>
Reference-contexts: cache policy effectiveness is highly system dependent, and delimits the conditions where each policy works well. 2 Related Work Much of the research in real-time predictable cache behavior has focussed on analyzing an application's behavior with respect to a uniprocess system that does not suffer interprocess contention for the cache <ref> [13, 10, 14, 7, 5] </ref>. For a multitasking environment, two basic methods have been proposed for controlling interprocess cache interference for predictable cache behavior. The first method relies on dividing the cache into distinct partitions and the other restricts where context switches can occur.
Reference: [14] <author> Jai Rawat. </author> <title> Static Analysis of Cache Performance for Real-Time Programming. </title> <type> Technical Report TR93-19, </type> <institution> Iowa State University of Science and Technology, </institution> <month> November </month> <year> 1993. </year>
Reference-contexts: cache policy effectiveness is highly system dependent, and delimits the conditions where each policy works well. 2 Related Work Much of the research in real-time predictable cache behavior has focussed on analyzing an application's behavior with respect to a uniprocess system that does not suffer interprocess contention for the cache <ref> [13, 10, 14, 7, 5] </ref>. For a multitasking environment, two basic methods have been proposed for controlling interprocess cache interference for predictable cache behavior. The first method relies on dividing the cache into distinct partitions and the other restricts where context switches can occur.
Reference: [15] <author> J.A. Stankovic. </author> <title> A Serious Problem for Next-Generation Systems. </title> <booktitle> Computer, </booktitle> <year> 1988. </year>
Reference-contexts: Real-time systems have the characteristic that missing a timing constraint, or deadline, can result in a catastrophic failure. The real-time operating system (RTOS) must schedule a complex set of tasks so they meet their deadlines <ref> [15] </ref>. To do so, the RTOS must have estimates of the resources required by each task and, in particular, their execution times. In systems that can experience catastrophic failures, the latter must be worst-case execution time (WCET) estimates that the task is guaranteed not to exceed.
Reference: [16] <author> Dominique Thiebaut. </author> <title> On the Fractal Dimension of Computer Programs and its Application to the Computation of the Cache Miss-Ratio. </title> <journal> IEEE Transactions on Computers, </journal> <pages> pages 1012-1026, </pages> <month> July </month> <year> 1989. </year>
Reference-contexts: C c marks the transition from the cache warm-up phase to steady state behavior. Equations 1 and 2 are motivated by previous work of Gillis and Weiss [1] on hyperbolic random walks and supporting empirical observations by Kobayashi and MacDougall [3] and Thi bault <ref> [16] </ref>.
Reference: [17] <author> Dominique Thiebaut, Joel L. Wolf, and Harold S. Stone. </author> <title> Synthetic Traces for Trace-Driven Simulation of Cache Memories. </title> <journal> IEEE Transactions on Computers, </journal> <pages> pages 388-410, </pages> <month> April </month> <year> 1992. </year> <month> 16 </month>
Reference-contexts: Each of these three variables has a range of potential values resulting in a large number of combinations, each of which can be viewed as defining a particular system. We refer to this set of values as the system space. 4.1 Cache Model Thiebaut et al. <ref> [17] </ref> developed an analytical model of fully associative caching behavior. The intended use of the model was to drive a synthetic address generator for cache simulations. They provide a method of measuring three parameter values- A, , and M m of actual applications that completely defines their cache behavior. <p> The reader is referred back to the original paper <ref> [17] </ref> for additional details. It is important to note that the cache size is measured in lines rather than words. The effects of the cache line size are embodied in values of the application definition parameters A, , and M m .
References-found: 17

