URL: http://www.cs.wisc.edu/~fischer/ftp/pub/tech-reports/ncstrl.uwmadison/CS-TR-89-874/CS-TR-89-874.ps.Z
Refering-URL: http://www.cs.wisc.edu/~fischer/ftp/pub/tech-reports/ncstrl.uwmadison/CS-TR-89-874/
Root-URL: http://www.cs.wisc.edu
Email: choi@cs.wisc.edu  bart@cs.wisc.edu  
Title: Code Generation and Separate Compilation in a Parallel Program Debugger  
Author: Jong-Deok Choi Barton P. Miller Jong-Deok Choi, Barton P. Miller. 
Note: Research supported in part by National Science Foundation grants CCR-8703373 and CCR-8815928, Office of Naval Research grant N00014-89-J-1222, and a Digital Equipment Corporation External Research Grant. Copyright 1989  Appears in Research Monographs in Parallel and Distributed Computing, MIT Press.  
Address: 1210 W. Dayton Street Madison, Wisconsin 53706  
Affiliation: Computer Sciences Department University of Wisconsin-Madison  
Abstract-found: 0
Intro-found: 1
Reference: 1. <author> Allen, R. and Kennedy, K., </author> <title> ``Automatic Translation of FORTRAN Programs to Vector Form,'' </title> <journal> ACM Transactions on Programming Languages and Systems 9(4) pp. </journal> <month> 491-542 (October </month> <year> 1987). </year>
Reference-contexts: 1. Introduction Separate compilation introduces significant complexity to compilers that perform interpro-cedural data-flow analysis for optimization [5] and for automatic parallelization <ref> [1] </ref>. This complexity often defeats the benefits of separate compilation. The Parallel Program Debugger (PPD), a debugging system for parallel programs running on shared memory multi-processors (SMMP) [3, 9], does dependence and data-flow analysis similar to parallelizing compilers, and must also contend with separate compilation.
Reference: 2. <author> Balzer, R. M., </author> <title> ``EXDAMS-EXtendable Debugging and Monitoring System,'' </title> <booktitle> Proc. of AFIPS Spring Joint Computer Conf. </booktitle> <pages> 34 pp. </pages> <month> 567-580 </month> <year> (1969). </year>
Reference-contexts: This complexity often defeats the benefits of separate compilation. The Parallel Program Debugger (PPD), a debugging system for parallel programs running on shared memory multi-processors (SMMP) [3, 9], does dependence and data-flow analysis similar to parallelizing compilers, and must also contend with separate compilation. PPD uses flowback analysis <ref> [2] </ref> to provide information on the causal relationships between events in a program's execution without re-executing the program during debugging. In flowback analysis, the programmer sees, either backward or forward, how information flowed through the program to produce the events of interest.
Reference: 3. <author> Choi, J. D., Miller, B. P., and Netzer, R., </author> <title> ``Techniques for Debugging Parallel Programs with Flowback Analysis,'' </title> <type> Computer Sciences Technical Report #786, </type> <institution> Univ. of Wisconsin-Madison, </institution> <month> (August </month> <year> 1988). </year>
Reference-contexts: 1. Introduction Separate compilation introduces significant complexity to compilers that perform interpro-cedural data-flow analysis for optimization [5] and for automatic parallelization [1]. This complexity often defeats the benefits of separate compilation. The Parallel Program Debugger (PPD), a debugging system for parallel programs running on shared memory multi-processors (SMMP) <ref> [3, 9] </ref>, does dependence and data-flow analysis similar to parallelizing compilers, and must also contend with separate compilation. PPD uses flowback analysis [2] to provide information on the causal relationships between events in a program's execution without re-executing the program during debugging. <p> A more detailed description of the static graph is given in <ref> [3, 4] </ref>. Execution Phase During this phase, the object code generates the normal program output and a log that contains dynamic information about program execution. The log is used, along with the emulation package, during the debugging phase to generate fine traces for the flowback analysis. <p> The compiler then generates logging code for shared variables and identifies the places in the assembly code where the logging code should be inserted. The resulting information is also recorded in PPD/nudfile. Postlogs generated by the same e-block are linked in a list <ref> [3] </ref>. ``E-pointers'' is an array that contains pointers to the last log entry made by each e-block and is updated during program execution. The e-pointer array needs to be written to a file when the execution terminates.
Reference: 4. <author> Choi, J. D., </author> <title> ``Parallel Program Debugging with Flowback Analysis,'' </title> <type> Ph.D. Thesis - Computer Sciences Tech Report #871, </type> <institution> Univ. of Wisconsin-Madison, </institution> <month> (August </month> <year> 1989). </year>
Reference-contexts: Three Phases in Debugging We divide debugging into three phases: preparatory phase, execution phase, and debugging phase. In the remainder of this section, we briefly describe each of these three phases. A more detailed description of the three phases is given in <ref> [4, 9] </ref>. <p> A more detailed description of the static graph is given in <ref> [3, 4] </ref>. Execution Phase During this phase, the object code generates the normal program output and a log that contains dynamic information about program execution. The log is used, along with the emulation package, during the debugging phase to generate fine traces for the flowback analysis. <p> Such additional information usually includes the values of the shared variables. We identify the additional information that we have to generate and where in the program we have to generate that information using the simplified static graph <ref> [4, 9] </ref> built at compile time. The simplified static graph is a subset of static graph that abstracts out everything except for the potential interactions between processes. There is one simplified static graph for each subroutine in the program. <p> The synchronization unit roughly corresponds to the set of basic blocks that might be executed between two synchronization operations such as P and V semaphore operations. We apply interprocedural analysis in computing the synchronization units of a program <ref> [4] </ref>. Thus, each synchronization operation is associated with two synchronization units: one starting from that synchronization operation and the other terminating at that synchronization operation. PPD compiler generates code after each synchronization operation to produce an additional log entry for the synchronization unit starting from that synchronization operation. <p> Thus, the PPD compiler shows relatively large overhead when a source module of multiple-module programs needs to be re-compiled. A detailed measurement results of compiler overhead for this case is given in <ref> [4] </ref>. <p> However, by using a more sophisticated analysis of dependences for complex data objects <ref> [4] </ref>, we should be able to make the compiler smart enough to generate a log entry containing the particular row of the matrix that is actually accessed instead of the entire matrix. Array logging can also cause some interesting performance anomalies. <p> However, the larger increases in execution time come from test programs that access only parts of arrays in loops. With a more sophisticated dependence analysis for complex data objects <ref> [4] </ref>, we expect substantial improvements to be possible for such programs. Execution-time trace sizes are generally small (less than 1 Mbytes in all cases).
Reference: 5. <author> Cooper, K., Kennedy, K., and Torczon, L., </author> <title> ``The Impact of Interprocedural Analysis and Optimization in the R n Programming Environment,'' </title> <journal> ACM Trans. on Programming Languages and Systems 8(4) pp. </journal> <month> 491-523 (October </month> <year> 1986). </year>
Reference-contexts: 1. Introduction Separate compilation introduces significant complexity to compilers that perform interpro-cedural data-flow analysis for optimization <ref> [5] </ref> and for automatic parallelization [1]. This complexity often defeats the benefits of separate compilation. The Parallel Program Debugger (PPD), a debugging system for parallel programs running on shared memory multi-processors (SMMP) [3, 9], does dependence and data-flow analysis similar to parallelizing compilers, and must also contend with separate compilation. <p> The log also enables us to detect data races in an execution instance of a parallel program and to ensure repeatable execution behavior of a race-free parallel program. We reduce the run time overhead of producing the log by applying interprocedural analysis <ref> [5] </ref> and data-flow analysis [7] techniques commonly used in optimizing compilers. However, there is a trade-off between the trace size during execution and response time during debugging [9]; in general, small log size means low execution overhead, but with long response time during debugging.
Reference: 6. <author> Horowitz, E. and Sahni, S., </author> <title> Fundamentals of Data Structures, </title> <publisher> Computer Science Press (1983). </publisher>
Reference-contexts: The subroutine does not contain a loop or any accesses to static variables, making it a target of log optimization. SH_PATH_1 computes the shortest paths from a city to 99 other cities using an algorithm described by Horowitz and Sahni <ref> [6] </ref>. SH_PATH_2 is the same as SH_PATH_1 except that it computes the shortest paths from all of the 100 cities to all of the other cities. CLASS is a program that emulates course registration for students, such as registering for courses and dropping courses.
Reference: 7. <author> Kennedy, K., </author> <title> ``A Survey of Data-flow Analysis Techniques,'' Program Flow Analysis: Theory and Applications, </title> <editor> S. S. Muchnick and N. D. Jones, </editor> <booktitle> Eds., </booktitle> <pages> pp. 5-54 Prentice-Hall, </pages> <address> Englewood Cliffs, N.J., </address> <year> (1981). </year>
Reference-contexts: The log also enables us to detect data races in an execution instance of a parallel program and to ensure repeatable execution behavior of a race-free parallel program. We reduce the run time overhead of producing the log by applying interprocedural analysis [5] and data-flow analysis <ref> [7] </ref> techniques commonly used in optimizing compilers. However, there is a trade-off between the trace size during execution and response time during debugging [9]; in general, small log size means low execution overhead, but with long response time during debugging.
Reference: 8. <author> Kernighan, B. and Ritchie, D., </author> <title> The C Programming Language, </title> <publisher> Prentice Hall, Inc., </publisher> <address> Englewood Cliffs, N.J. </address> <year> (1978). </year>
Reference-contexts: While we are not addressing automatic parallelism, many of our techniques might be extended to such systems. The techniques in this paper are described in terms of the C programming language <ref> [8] </ref>, but they should generalize to many procedural languages. We address a large part of the C language, including primitives for synchronization, but we do not discuss pointers and dynamic (heap) variables; that is a topic of current investigation. This paper is organized as follows.
Reference: 9. <author> Miller, B. P. and Choi, J. D., </author> <title> ``A Mechanism for Efficient Debugging of Parallel Programs,'' Procs. </title> <booktitle> of the SIGPLAN Conf on Prog. Language Design and Implementation, </booktitle> <pages> pp. </pages> <address> 135-144 Atlanta, GA, </address> <month> (June </month> <year> 1988). </year> <booktitle> Also appeared in the Proc. of the SIGPLAN/SIGOPS Workshop on Parallel and Distributed Debugging, </booktitle> <address> Madison, </address> <month> June </month> <year> 1988. </year>
Reference-contexts: 1. Introduction Separate compilation introduces significant complexity to compilers that perform interpro-cedural data-flow analysis for optimization [5] and for automatic parallelization [1]. This complexity often defeats the benefits of separate compilation. The Parallel Program Debugger (PPD), a debugging system for parallel programs running on shared memory multi-processors (SMMP) <ref> [3, 9] </ref>, does dependence and data-flow analysis similar to parallelizing compilers, and must also contend with separate compilation. PPD uses flowback analysis [2] to provide information on the causal relationships between events in a program's execution without re-executing the program during debugging. <p> In flowback analysis, the programmer sees, either backward or forward, how information flowed through the program to produce the events of interest. In this way, the programmer can easily locate bugs that led to the detected errors. By using a method called incremental tracing <ref> [9] </ref>, PPD is able to keep the program execution overhead of applying flowback analysis relatively low, allowing us to generate only a small amount of trace during program execution. The cornerstone idea of the incremental tracing is to generate a small amount of information, called a log, during execution. <p> We reduce the run time overhead of producing the log by applying interprocedural analysis [5] and data-flow analysis [7] techniques commonly used in optimizing compilers. However, there is a trade-off between the trace size during execution and response time during debugging <ref> [9] </ref>; in general, small log size means low execution overhead, but with long response time during debugging. <p> Three Phases in Debugging We divide debugging into three phases: preparatory phase, execution phase, and debugging phase. In the remainder of this section, we briefly describe each of these three phases. A more detailed description of the three phases is given in <ref> [4, 9] </ref>. <p> The log entries and tracing are described in more detail in Section 3. Debugging Phase The goal of the debugging phase is to build a graph of the dynamic dependences in an execution instance of a program, called the dynamic program dependence graph (dynamic graph) <ref> [9] </ref>. Figure 2.1 shows an example dynamic graph. The debugging phase assembles information from the previous phases: the static graph and program database generated during the preparation phase, and the log generated during the execution phase. <p> the dynamic graph. : control dependence edge d=a+b; sq=sqrt (-d); else sq=sqrt (d); if (d&gt;0) s5 s3 s2 a b (d&gt;0):s2 sq:s5 sqrt:s5 : data dependence edge : flow edge : singular node : sub-graph node The Controller also builds the parallel dynamic program dependence graph (or parallel dynamic graph) <ref> [9] </ref> to debug parallel programs. The parallel dynamic graph is a subset of the dynamic graph that shows the interactions between processes while hiding the detailed dependences of local events. 3. Run-Time Traces In this section, we describe the run-time trace (log) in more detail. <p> The object code generated by the PPD compiler contains code to generate these log entries. By using semantic analysis, we divide the program into numerous segments of code called emulation blocks (e-blocks) <ref> [9] </ref>. A subroutine is a good example of an emulation block. An e-block is also the unit of incremental tracing during debugging. The USED set of an e-block is the set of variables that might be read-accessed by statements of the e-block. <p> Such additional information usually includes the values of the shared variables. We identify the additional information that we have to generate and where in the program we have to generate that information using the simplified static graph <ref> [4, 9] </ref> built at compile time. The simplified static graph is a subset of static graph that abstracts out everything except for the potential interactions between processes. There is one simplified static graph for each subroutine in the program.
References-found: 9

