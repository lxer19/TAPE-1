URL: http://www.robotics.stanford.edu/~koller/papers/aaai98kp.ps
Refering-URL: http://www.robotics.stanford.edu/~koller/papers/aaai98kp.html
Root-URL: http://www.robotics.stanford.edu
Email: koller@cs.stanford.edu  avi@cs.stanford.edu  
Title: Probabilistic frame-based systems  
Author: Daphne Koller Avi Pfeffer 
Affiliation: Computer Science Department Stanford University  Computer Science Department Stanford University  
Date: July 1998  
Address: Madison, Wisconsin,  Gates Building, 1A Stanford, CA 94305-9010  Gates Building, 1A Stanford, CA 94305-9010  
Note: In Proceedings of the Fifteenth National Conference on Artificial Intelligence (AAAI-98), pages 580-587,  
Abstract: Two of the most important threads of work in knowledge representation today are frame-based representation systems (FRS's) and Bayesian networks (BNs). FRS's provide an excellent representation for the organizational structure of large complex domains, but their applicability is limited because of their inability to deal with uncertainty and noise. BNs provide an intuitive and coherent probabilistic representation of our uncertainty, but are very limited in their ability to handle complex structured domains. In this paper, we provide a language that cleanly integrates these approaches, preserving the advantages of both. Our approach allows us to provide natural and compact definitions of probability models for a class, in a way that is local to the class frame. These models can be instantiated for any set of interconnected instances, resulting in a coherent probability distribution over the instance properties. Our language also allows us to represent important types of uncertainty that cannot be accomodated within the framework of traditional BNs: uncertainty over the set of entities present in our model, and uncertainty about the relationships between these entities. We provide an inference algorithm for our language via a reduction to inference in standard Bayesian networks. We describe an implemented system that allows most of the main frame systems in existence today to annotate their knowledge bases with probabilistic information, and to use that information in answering probabilistic queries. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> C. Boutilier, N. Friedman, M. Goldszmidt, and D. Koller. </author> <title> Context-specific independence in Bayesian networks. </title> <booktitle> In Proc. </booktitle> <address> UAI, </address> <year> 1996. </year>
Reference-contexts: The value of I:B: is equal to the value of K []:. In other words, I:ref (B) selects the value of I:B: from a set of possibilities. Therefore, the node I:B: is a multiplexer node <ref> [1] </ref>; it has as parents the node I:ref (B) and all nodes K []:B:, and it uses the value of I:ref (B) to select, as its value, the value of one of its appropriate parents. <p> Consider a quantifier slot I:Q over A:. The value of I:Q is fully determined by I:num (A) and the value of in each of the possible values of A. Let n be the maximum number of such values, and suppose that A is assigned m values J <ref> [1] </ref>; : : : ; J [m] in I. In addition to these, there can be up to n m other instances that are values for A; we build a frame for each of them, J [m + 1]; : : : ; J [n]. <p> The node I:Q depends on I:num (A) and on the appropriate subset of the variables J [i]:; i.e., if num (A) is k, then only J <ref> [1] </ref>; : : : ; J [k] will influence I:Q. The exact form of the dependence will depend on the form of the quantifier. For example, a 8 quantifier slot will be a deterministic conjunction of the appropriate subset of its parents. <p> For example, a 8 quantifier slot will be a deterministic conjunction of the appropriate subset of its parents. ProcessQuantifier (I:Q) /* a quantifier over A: */ AddParent (I:Q, I:num (A)) Let n be the maximum value of I:num (A) Let J <ref> [1] </ref>; : : : ; J [m] be the assigned values to I:A For i = m + 1 to n J [i] = X [i][I:A], where X is the type of I:A case. Models for only two of the fifty possible papers are shown. <p> Models for only two of the fifty possible papers are shown. Paper [i] is shorthand for paper [Gump.papers][i], and Conf [i] is short for conf [paper [Gump.papers.conference][i]]. For i = 1 to n AddParent (I:Q, J [i]:) Set the CPT for I:Q to depend on J <ref> [1] </ref>:; : : : ; J [num (A)]: To illustrate the algorithm, Figure 2 shows part of the BN constructed for a KB concerning the tenure case of one Prof. F. Gump, an instance of ai-assistant-professor. The node Gump.will-get-tenure depends on Gump. 10 (papers.impact:high).
Reference: [2] <author> V.K. Chaudhri, A. Farquhar, R. Fikes, P.D. Karp, and J.P. Rice. </author> <title> Open knowledge base connectivity 2.0.2. </title> <note> Available from http://www.ai.sri.com/okbc, 1998. </note>
Reference-contexts: This property is important, since it allows our approach to be used with virtually any frame system, and thereby to annotate existing KBs with probabilistic information. In particular, we have implemented a system based on our approach, capable of interacting with most existing FRS's via OKBC <ref> [2] </ref>, an emerging standard for FRS interoperability. Our work is a signficant improvement over previous approaches to combining first-order logic and Bayesian networks. Most of the attempts in this direction (e.g., [12, 11, 9]) use probabilistic Horn clauses as the basic representation. <p> The terminology varies widely from system to system. In this paper we adopt the language and basic knowledge model of the OKBC protocol <ref> [2] </ref>. The basic unit of discourse in a frame system is a frame. A frame has a set of slots, each of which may have slot values or fillers.
Reference: [3] <author> A. Farquhar, R. Fikes, and J. Rice. </author> <title> The Ontolingua server: A tool for collaborative ontology construction. </title> <type> Technical report, </type> <address> Stanford KSL 96-26, </address> <year> 1996. </year>
Reference-contexts: 1 Introduction Frame representation systems (FRS's) are currently the primary technology used for large scale knowledge representation in AI <ref> [8, 3, 7] </ref>. Their modular organization according to cognitively meaningful entities and their ability to capture patterns common to many individuals provide a convenient language for representing complex structured domain models. <p> The editor/browser interacts with the underlying FRS using OKBC, thus allowing its use with many of the existing frame systems (e.g., <ref> [3, 8, 7] </ref>). Our inference component also connects to the FRS via OKBC; it extracts the relevant information about frames, instances, and their probabilistic models, constructs the BN corresponding to the KB, and utilizes the BN to answer probabilistic queries. <p> Our inference component also connects to the FRS via OKBC; it extracts the relevant information about frames, instances, and their probabilistic models, constructs the BN corresponding to the KB, and utilizes the BN to answer probabilistic queries. The system has been integrated successfully with the Ontolingua frame system <ref> [3] </ref> and was used for representing simple models of vehicle movement patterns in a military setting.
Reference: [4] <author> M. Henrion. </author> <title> Propagation of uncertainty in Bayesian networks by probabilistic logic sampling. </title> <booktitle> In Proc. </booktitle> <address> UAI, </address> <year> 1988. </year>
Reference-contexts: If, during the sampling process, a value is generated for a slot which is inconsistent with the observed value, we simply discard the entire partial value generated up to that point. It is easy to see <ref> [4] </ref> that the relative probability with which a partial value is generated in this data generating process is exactly the same as its probability conditioned on the observed slot values. As we discussed, our sampling procedure builds up a partial value # piece by piece, as the pieces are needed.
Reference: [5] <author> D. Koller, A. Levy, and A. Pfeffer. P-classic: </author> <title> A tractable probabilistic description logic. </title> <booktitle> In Proc. AAAI, </booktitle> <year> 1997. </year>
Reference-contexts: But, like all propositional systems, the applicability of BNs is largely limited to situations that can be encoded, in advance, using a fixed set of attributes. Thus, they are inadequate for large-scale complex KR tasks. Building on our recent work <ref> [6, 5] </ref>, we propose a representation language that integrates frame-representation systems and Bayesian networks, thereby providing the first bridge between these two very different threads of work in KR. The key component in our representation is the annotation of a frame with a probability model. <p> Moreover, the use of structural uncertainty in this framework typically causes combinatorial blowup of the resulting models, leading most approaches to outlaw it entirely. Our framework also overcomes some major limitations of our earlier proposals <ref> [6, 5] </ref>, by allowing both structural uncertainty (absent in the first) and probabilistic dependencies between instances (absent in the second). <p> In particular, it fails to exploit encapsulation of frames within other frames and the reuse of class models among several objects. These ideas are put to good use in <ref> [6, 5] </ref>, and it is an important research topic to apply them in our richer framework. We believe that by exploiting these features in our inference as well as in our representation, we will be able to effectively represent and reason in large uncertain domains.
Reference: [6] <author> D. Koller and A. Pfeffer. </author> <title> Object-oriented Bayesian networks. </title> <booktitle> In Proc. </booktitle> <address> UAI, </address> <year> 1997. </year>
Reference-contexts: But, like all propositional systems, the applicability of BNs is largely limited to situations that can be encoded, in advance, using a fixed set of attributes. Thus, they are inadequate for large-scale complex KR tasks. Building on our recent work <ref> [6, 5] </ref>, we propose a representation language that integrates frame-representation systems and Bayesian networks, thereby providing the first bridge between these two very different threads of work in KR. The key component in our representation is the annotation of a frame with a probability model. <p> Moreover, the use of structural uncertainty in this framework typically causes combinatorial blowup of the resulting models, leading most approaches to outlaw it entirely. Our framework also overcomes some major limitations of our earlier proposals <ref> [6, 5] </ref>, by allowing both structural uncertainty (absent in the first) and probabilistic dependencies between instances (absent in the second). <p> In particular, it fails to exploit encapsulation of frames within other frames and the reuse of class models among several objects. These ideas are put to good use in <ref> [6, 5] </ref>, and it is an important research topic to apply them in our richer framework. We believe that by exploiting these features in our inference as well as in our representation, we will be able to effectively represent and reason in large uncertain domains.
Reference: [7] <author> D.B. Lenat and R.V. Guha. </author> <title> Building Large Knowledge-Based Systems: Representation and Inference in the CYC Project. </title> <publisher> Addison-Wesley, </publisher> <year> 1990. </year>
Reference-contexts: 1 Introduction Frame representation systems (FRS's) are currently the primary technology used for large scale knowledge representation in AI <ref> [8, 3, 7] </ref>. Their modular organization according to cognitively meaningful entities and their ability to capture patterns common to many individuals provide a convenient language for representing complex structured domain models. <p> The editor/browser interacts with the underlying FRS using OKBC, thus allowing its use with many of the existing frame systems (e.g., <ref> [3, 8, 7] </ref>). Our inference component also connects to the FRS via OKBC; it extracts the relevant information about frames, instances, and their probabilistic models, constructs the BN corresponding to the KB, and utilizes the BN to answer probabilistic queries.
Reference: [8] <author> R. MacGregor. </author> <title> The evolving technology of classification-based knowledge representation systems. </title> <editor> In J. Sowa, editor, </editor> <booktitle> Principles of semantic networks, </booktitle> <pages> pages 385-400. </pages> <publisher> Morgan Kaufmann, </publisher> <year> 1991. </year>
Reference-contexts: 1 Introduction Frame representation systems (FRS's) are currently the primary technology used for large scale knowledge representation in AI <ref> [8, 3, 7] </ref>. Their modular organization according to cognitively meaningful entities and their ability to capture patterns common to many individuals provide a convenient language for representing complex structured domain models. <p> The editor/browser interacts with the underlying FRS using OKBC, thus allowing its use with many of the existing frame systems (e.g., <ref> [3, 8, 7] </ref>). Our inference component also connects to the FRS via OKBC; it extracts the relevant information about frames, instances, and their probabilistic models, constructs the BN corresponding to the KB, and utilizes the BN to answer probabilistic queries.
Reference: [9] <author> L. Ngo and P. Haddawy. </author> <title> Answering queries from context-sensitive probabilistic knowledge bases. </title> <booktitle> Theoretical Computer Science, </booktitle> <year> 1996. </year>
Reference-contexts: Our work is a signficant improvement over previous approaches to combining first-order logic and Bayesian networks. Most of the attempts in this direction (e.g., <ref> [12, 11, 9] </ref>) use probabilistic Horn clauses as the basic representation. The choice of Horn clauses as an underlying language already dictates some of the properties of the representation, e.g., its inability to encapsulate an object and its properties within a cognitively meaningful frame.
Reference: [10] <author> J. Pearl. </author> <title> Probabilistic Reasoning in Intelligent Systems. </title> <publisher> Morgan Kaufmann, </publisher> <year> 1988. </year>
Reference-contexts: In the propositional setting, this problem has largely been resolved over the past decade by the development of Copyright 1998, American Association for Artificial Intelligence (www.aaai.org). All rights reserved. probabilistic reasoning systems, and particularly Bayesian networks <ref> [10] </ref>. A Bayesian network (BN) is a representation of a full joint distribution over a set of random variables; it can be used to answer queries about any of its variables given any evidence.
Reference: [11] <author> D. Poole. </author> <title> Probabilistic Horn abduction and Bayesian networks. </title> <journal> Artificial Intelligence, </journal> <volume> 64(1) </volume> <pages> 81-129, </pages> <month> November </month> <year> 1993. </year>
Reference-contexts: Our work is a signficant improvement over previous approaches to combining first-order logic and Bayesian networks. Most of the attempts in this direction (e.g., <ref> [12, 11, 9] </ref>) use probabilistic Horn clauses as the basic representation. The choice of Horn clauses as an underlying language already dictates some of the properties of the representation, e.g., its inability to encapsulate an object and its properties within a cognitively meaningful frame.
Reference: [12] <author> M.P. Wellman, J.S. Breese, and R.P. Goldman. </author> <title> From knowledge bases to decision models. </title> <journal> The Knowledge Engineering Review, </journal> <volume> 7(1) </volume> <pages> 35-53, </pages> <month> November </month> <year> 1992. </year>
Reference-contexts: Our work is a signficant improvement over previous approaches to combining first-order logic and Bayesian networks. Most of the attempts in this direction (e.g., <ref> [12, 11, 9] </ref>) use probabilistic Horn clauses as the basic representation. The choice of Horn clauses as an underlying language already dictates some of the properties of the representation, e.g., its inability to encapsulate an object and its properties within a cognitively meaningful frame. <p> Our algorithm can handle any instance-based query, i.e., queries about the values of slots of instances. For simplicity, we restrict attention to simple slots of named instances, as other queries can easily be reduced to these. The algorithm, called ConstructBN, is based on knowledge-based model construction <ref> [12] </ref>, the process of taking a KB and deriving a BN B representing the same probability model. Standard BN inference can then be used to answer queries.
References-found: 12

