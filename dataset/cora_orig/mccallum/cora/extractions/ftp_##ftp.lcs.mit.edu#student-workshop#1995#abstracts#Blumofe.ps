URL: ftp://ftp.lcs.mit.edu/student-workshop/1995/abstracts/Blumofe.ps
Refering-URL: http://www.cag.lcs.mit.edu/student95/proceedings.html
Root-URL: 
Email: rdb@theory.lcs.mit.edu  
Title: Cilk: An Efficient Multithreaded Runtime System Other members of the Cilk team, present and past,
Author: Robert Blumofe Frigo, Michael Halbherr, Chris Joerg, Bradley Kuszmaul, Rob Miller, Keith Randall, and Yuli Zhou 
Note: This research was supervised by Professor Charles E. Leiserson and was supported in part by the Advanced Research Projects Agency under contract N00014-94-1-0985 and Project SCOUT (contract MDA97292-J-1032), and by an ARPA High-Performance Computing Graduate Fellowship.  
Address: Cambridge, MA 02139  
Affiliation: MIT Laboratory for Computer Science  
Abstract: Multithreading has become an increasingly popular way to implement dynamic, highly asynchronous, concurrent programs. A multithreaded system provides the programmer with a means to create, synchronize, and schedule threads. Although the schedulers in many runtime systems seem to perform well in practice, none provide users with a guarantee of application performance. Cilk (pronounced silk) is a runtime system whose work-stealing scheduler is efficient in theory as well as in practice. Moreover, it gives the user an algorithmic model of application performance based on the measures of work and critical path which can be used to predict the runtime of a Cilk program accurately. The Cilk language is an extension to C that provides an abstraction of threads in explicit continuation-passing style. A Cilk program is preprocessed to C and then linked with a runtime library to run on the Connection Machine CM5 MPP, the Intel Paragon MPP, the Silicon Graphics Power Challenge SMP, or the MIT Phish network of workstations. To date, the applications we have programmed include protein folding, graphic rendering, backtrack search, and the ?Socrates chess program, which won third prize in the 1994 ACM International Computer Chess Championship. A Cilk multithreaded computation can be viewed as a directed acyclic graph (dag) that unfolds dynamically. A Cilk program consists of a collection of Cilk threads, which form the vertices of the dag. Each thread is a nonblocking C function, which means that it can run to completion without waiting or suspending once it has been invoked. As a thread runs, it can spawn a child thread. A spawn is like a subroutine call, except that the calling thread may execute concurrently with its child, possibly spawning additional children. Since threads cannot block in the Cilk model, a thread cannot spawn children and then wait for values to be returned. Rather, the thread must additionally spawn a successor thread to receive the children's return values when they are produced. Return values, and other values sent from one thread to another, induce data dependencies among the threads, where a thread receiving a value cannot begin until another thread sends the value. The execution time of any Cilk program on a parallel computer with P processors is constrained by two parameters of the computation: the work and the critical path. The work, denoted T 1 , is the time used by a one-processor execution of the program, which corresponds to the sum of the execution times of all the threads. The critical path length, denoted T 1 , is the total amount of time required by an infinite-processor execution, which corresponds to the largest sum of thread execution times along any path in the dag. With P processors, the execution time T P cannot be less than T 1 =P (perfect linear speedup) or less than T 1 . A good scheduler should meet each of these lower bounds as closely as possible. The Cilk scheduler uses random work stealing [2] to achieve execution time very near to the sum of these two lower bounds. Off-line techniques for computing such efficient schedules have been known for a long time, but this efficiency has been difficult to achieve on-line in a distributed environment while simultaneously using small amounts of space and communication. In [1], we used algorithmic and empirical techniques to show that for the class of fully strict programs, Cilk's work-stealing scheduler is efficient with respect to space, time, and communication. A fully strict program is one for which each thread sends arguments only to its parent's successor threads. (All of the applications we have coded to date are fully strict.) Analytically, we showed that for any number P of processors and any fully strict computation with work T 1 and critical path T 1 , the Cilk scheduling algorithm achieves execution time T P = O(T 1 =P + T 1 ) with high probability while simultaneously using amounts of space and communication that are existentially optimal to within a small constant factor. The remainder of this paper focuses on empirical measurements of execution time on the CM5 for our ?Socrates chess application. Figure 1 shows the outcome of many experiments of running ?Socrates on a variety of chess positions. The figure plots the speedup T 1 =T P for each run against the machine size P for that run. In order to compare the outcomes for runs with different parameters, we have normalized the data by dividing the plotted values by the average parallelism T 1 =T 1 . Thus, the horizontal position of each datum is P=(T 1 =T 1 ), and the vertical position of each datum is (T 1 =T P )=(T 1 =T 1 ) = T 1 =T P . Consequently, on the horizontal axis, the normalized machine-size is 1:0 when the average parallelism is equal to the machine size. On the ver 
Abstract-found: 1
Intro-found: 0
Reference: [1] <author> Robert D. Blumofe, Christopher F. Joerg, Bradley C. Kusz-maul, Charles E. Leiserson, Keith H. Randall, and Yuli Zhou. Cilk: </author> <title> An efficient multithreaded runtime system. </title> <booktitle> In Proceedings of the Fifth ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming (PPoPP), </booktitle> <address> Santa Barbara, California, </address> <month> July </month> <year> 1995. </year> <note> To appear. </note>
Reference: [2] <author> Robert D. Blumofe and Charles E. Leiserson. </author> <title> Scheduling multithreaded computations by work stealing. </title> <booktitle> In Proceedings of the 35th Annual Symposium on Foundations of Computer Science, </booktitle> <pages> pages 356-368, </pages> <address> Santa Fe, New Mexico, </address> <month> November </month> <year> 1994. </year>
References-found: 2

