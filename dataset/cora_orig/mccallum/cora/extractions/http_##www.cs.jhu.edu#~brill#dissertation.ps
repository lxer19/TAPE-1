URL: http://www.cs.jhu.edu/~brill/dissertation.ps
Refering-URL: http://www.cs.jhu.edu/~brill/acadpubs.html
Root-URL: http://www.cs.jhu.edu
Title: A Corpus-Based Approach to Language Learning  
Author: Eric Brill Mitchell Marcus Mark Steedman 
Degree: A Dissertation in Department of Computer and Information Science Presented to the Faculties of the University of Pennsylvania in Partial Fulfillment of the Requirements for the Degree of Doctor of Philosophy 1993  Supervisor of Dissertation  
Affiliation: Graduate Group Chairperson  
Abstract-found: 0
Intro-found: 0
Reference: [1] <author> S. Abney. </author> <title> The English noun phrase in its sentential aspects. </title> <publisher> Unpublished MIT Dissertation, </publisher> <year> 1987. </year>
Reference-contexts: Even the structural description of the simple phrase the boy is in question. It is unclear whether this phrase is a projection of the noun boy, making it a noun phrase, or the projection of the determiner the, making it a determiner phrase <ref> [1] </ref>. With no clear picture of the correct structure of sentences, how can we hope to make progress toward a system capable of learning the information necessary to assign structural descriptions? This question has to be answered in the context of the current state of the art of language processing. <p> ($word,$tag); g # Alter the score for the transformation involving the currently # processed tag and last letter. while (($key,$val) = each %transformation) f # Go through all recorded transformations and find # the one with the best score. @temp = split (/ns+/,$key); $tag = $temp [0]; $letter = $temp <ref> [1] </ref>; if ($val &gt; $bestscore) f $bestscore = $val; $bestrule = "Change to $tag if last letter is $letternn";ggg print "++$RULENUMBER $bestrule nn"; %updatecorpus; g 67 information was later learned using the contextual training corpus, which consisted of the second 1,000 sentences of the corpus (see figure 6.7).
Reference: [2] <author> L. Bahl, P. Brown, P. DeSouza, and R. Mercer. </author> <title> A tree-based statistical language model for natural language recognition. </title> <booktitle> Readings in Speech Recognition, </booktitle> <year> 1990. </year>
Reference-contexts: Most successful language models are n-gram models, basing the probability of a word on the probability of the preceding n words, or classes of these words (e.g. [65]). Language models based on context free grammars (e.g. [88]) and decision trees (e.g. <ref> [2] </ref>) have also been proposed. <p> In natural language, decision trees have been applied to language modelling <ref> [2] </ref> and part of speech tagging [7].
Reference: [3] <author> J. Baker. </author> <title> Trainable grammars for speech recognition. </title> <booktitle> In Speech communication papers presented at the 97th Meeting of the Acoustical Society of America, </booktitle> <year> 1979. </year>
Reference-contexts: Since finding the optimal, or highest probability, combination of subtrees would result in a parser requiring exponential time, a Monte Carlo technique [50] is used to find a good guess at the optimal combination of subtrees when parsing fresh text. The inside-outside algorithm <ref> [3] </ref> is a method for training stochastic context-free grammars. It is an extension of the Baum-Welch [4] algorithm for training stochastic finite state automata. 2 A number of recent papers have explored the potential of using the i-o algorithm to automatically learn a grammar [75, 104, 88, 26, 31, 102]. <p> The most promising results to date have been based on the inside-outside algorithm, which can be used to train stochastic context-free grammars. The inside-outside algorithm is an extension of the finite-state based Hidden Markov model (by <ref> [3] </ref>), which has been applied successfully in many areas, including speech recognition and part of speech tagging. A number of recent papers have explored the potential of using the inside-outside algorithm to automatically learn a grammar [75, 104, 88, 26, 31, 102].
Reference: [4] <author> L. Baum. </author> <title> An inequality and associated maximization technique in statistical estimation for probabilistic functions of a Markov process. </title> <journal> Inequalities, </journal> <volume> 3 </volume> <pages> 1-8, </pages> <year> 1972. </year>
Reference-contexts: The taggers described in [39, 65, 73] are of this type. They use the Baum-Welch algorithm <ref> [4] </ref> to train the model, and then use this trained model for tagging fresh text. It is not clear whether this approach of training on untagged text provides an effective and portable method of tagging. <p> The inside-outside algorithm [3] is a method for training stochastic context-free grammars. It is an extension of the Baum-Welch <ref> [4] </ref> algorithm for training stochastic finite state automata. 2 A number of recent papers have explored the potential of using the i-o algorithm to automatically learn a grammar [75, 104, 88, 26, 31, 102]. A probabilistic context free grammar begins with some initial, possibly random, probabilities.
Reference: [5] <author> E. Black, S. Abney, D. Flickenger, C. Gdaniec, R. Grishman, P. Harrison, D. Hindle, R. Ingria, F. Jelinek, J. Klavans, M. Liberman, M. Marcus, S. Roukos, B. Santorini, and T. Strzalkowski. </author> <title> A procedure for quantitatively comparing the syntactic coverage of English grammars. </title> <booktitle> In Proceedings of Fourth DARPA Speech and Natural Language Workshop, </booktitle> <pages> pages 306-311, </pages> <year> 1991. </year>
Reference-contexts: In computational linguistics, many researchers have been using annotated corpora to train stochastic part of speech taggers and parsers (e.g. [36, 102]). Structurally annotated corpora are being used as the gold standard by which different parsers can be objectively compared <ref> [5] </ref>. Currently, researchers are limited by the existing annotated corpora and the structural descriptions provided in those corpora, or by sentences that can be successfully annotated by existing taggers and parsers.
Reference: [6] <author> E. Black, F. Jelinek, J. Lafferty, D. Magerman, R. Mercer, and S. Roukos. </author> <title> Towards history-based grammars: Using richer models for probabilistic parsing. </title> <booktitle> In Proceedings of the 31st Annual Meeting of the Association for Computational Linguistics, 1993. </booktitle> <address> Columbus, Ohio. </address>
Reference-contexts: There are many different ways one could try to construct a language learner. In [65], a self-organizing language learner is proposed to be used for language modelling. In <ref> [6] </ref>, a method of combining a large manually constructed grammar with statistical information obtained from a large corpus is discussed.
Reference: [7] <author> E. Black, F. Jelinek, J. Lafferty, R. Mercer, and S.Roukos. </author> <title> Decision tree models applied to the labeling of text with parts-of-speech. </title> <booktitle> In Darpa Workshop on Speech and Natural Language, 1992. </booktitle> <address> Harriman, N.Y. </address> <month> 145 </month>
Reference-contexts: In [57], part of speech tagging rules are discovered automatically within a sophisticated Marcus-style parser [81]. Rules make reference to the state of the parser during the processing of the word being tagged. In [16], a simple rule-based tagger is described. In <ref> [7] </ref>, a decision tree is used in tagging. In all of these approaches, contextual information is used to disambiguate from an already known set of allowable part of speech tags. A problem arises when a word is encountered for which no part of speech information is known. <p> In light of the comparable performance achieved by all taggers, that described in [16] and below is much simpler than the others. For example, contextual information is captured in fewer than 100 rules in [16], compared to a 30,000 to 40,000 leaf decision tree in <ref> [7] </ref> and a table of tens of thousands of contextual probabilities in [36]. 3.2 Learning Lexical Information Distributional techniques have also been useful in helping a lexicographer uncover lexical information about words that he might not have been able to think of through introspection. <p> In natural language, decision trees have been applied to language modelling [2] and part of speech tagging <ref> [7] </ref>. One crucial difference between training decision trees and training the transformation-based learner is that when training a decision tree, each time the depth of the tree is increased, the average amount of training material available per node at that new depth is halved (for a binary tree). <p> In addition, the resulting learned information is much more compact in transformation-based learning. For example, in the application of part of speech tagging, the decision-tree tagger described in <ref> [7] </ref> outputs a tree with 30,000 to 40,000 leaves, whereas the transformation-based learner outputs a list of fewer than 200 transformations. A decision list [95] is similar to a decision tree, except that it is restricted to being binary-branching and right-linear. <p> This problem could be resolved, or at least lessened, by using a second training corpus to prune transformations. However, because of the nature of the learner, overtraining is not as harmful in transformation-based learning as it is in other learning paradigms being applied to corpus-based learning (for instance, see <ref> [88, 7] </ref>). At every stage of learning, the transformation-based learner learns the transformation that results in the greatest error reduction. <p> Yet, we have some evidence that at least in the domain of tagging, the two methods have comparable error rates (see <ref> [7] </ref>). 81 demonstrated that with fewer than 100 such symbolic contextual transformations, perfor-mance was obtained that was comparable to stochastic taggers that capture contextual information in tens of thousands of contextual probabilities and use smoothing techniques for overcoming the problem of estimated probabilities of zero. <p> In [84], an accuracy of 96.3% is obtained when training on one million words. However, the lexicon was closed in the sense that it was built from both the training and test set, and so there were no unknown words. In <ref> [7] </ref> an accuracy of 95.4% was obtained training on over 4.4 million words. We have achieved results that are competitive with results quoted in the literature for stochastic taggers trained on large corpora.
Reference: [8] <author> E. Black, J. Lafferty, and S. Roukos. </author> <title> Development and evaluation of a broad-coverage probabilistic grammar of English-language computer manuals. </title> <booktitle> In Proceedings of the 30th Annual Meeting of the Association for Computational Linguistics, </booktitle> <year> 1992. </year> <institution> Newark, De. </institution>
Reference-contexts: The lack of success to this date in building a robust parser (see <ref> [8] </ref>) is an indication that perhaps these methods never will. A system that automatically extracts linguistic generalizations from an annotated corpus has two strong advantages over introspection and trial-and-error. First, automating the development of the knowledge base could greatly reduce the total development time of a system. <p> Given the current level of sophistication of state of the art sentence processors, it is unlikely that progress will currently be hampered by our lack of a detailed understanding of the structure of sentences. As reported in <ref> [8] </ref>, an experiment was recently run in which four large-coverage parsers were presented a number of sentences, all containing fewer than fourteen words. <p> The algorithm is guaranteed to find a locally optimal assignment of rule probabilities, but not a globally optimal assignment. In [88, 102], it is shown that the inside-outside algorithm can be used to bracket text with high accuracy, with very weak initial knowledge of the grammar. In <ref> [8] </ref>, the inside-outside algorithm is used to convert a grammar written by a linguist into a probabilistic grammar where the hope is that the most probable parse is often the correct parse. 2 For a good tutorial on the inside-outside algorithm, see [66]. 30 In this thesis, we will discuss an <p> The measure we have chosen for our experiments is the same measure described in [88], which is a variation of a measure which arose out of various meetings on parser evaluation <ref> [8] </ref>. The measure is the percentage of constituents (strings of words between matching parentheses) from sentences output by our system which do not cross any constituents in the Penn Treebank structural description of the sentence.
Reference: [9] <author> L. Bloomfield. </author> <title> Language. </title> <publisher> Holt, </publisher> <address> New York, </address> <year> 1933. </year>
Reference-contexts: He proposed using a form of distributional analysis. As an example, a linguist analyzing English could determine that m and n are not allophones by noting that the sounds mail and nail convey different meaning. Boas' work was followed by that of Leonard Bloomfield <ref> [9] </ref>. Bloomfield also worked on uncovering descriptions of unfamiliar languages. And like Boas, Bloomfield believed that when studying an unfamiliar language, one had to be extremely careful not to allow any preconceived notions to creep into the study.
Reference: [10] <author> A. Blumer, A. Ehrenfeucht, D. Haussler, and M. Warmuth. </author> <title> Occam's razor. </title> <journal> In Information Processing Letters, </journal> <volume> volume 24, </volume> <year> 1987. </year>
Reference-contexts: The only potential parameter is the threshold value above which a transformation must score for it to be learned. The performance of the system with respect to this one parameter 2 This is described a bit more formally in <ref> [10] </ref>. 134 can easily be observed by learning a set of transformations with the threshold set to zero. Then text can be annotated using this transformation list.
Reference: [11] <author> F. Boas. </author> <title> Handbook of American Indian Languages, Part 1. </title> <publisher> Smithsonian Institution, </publisher> <address> Washington, D.C., </address> <year> 1911. </year> <journal> Bureau of American Ethnology, Bulletin 40. </journal>
Reference-contexts: Because of this relationship, we will now briefly examine some past work done in structural linguistics. According to Sampson [99], Franz Boas is the father of linguistic structuralism. Boas was interested in determining the structure of a number of different languages <ref> [11, 62] </ref>. Providing an accurate description of each language was the primary goal of this work. From this work, Boas thought, research could be done to determine the relationship between languages based upon their structural similarity.
Reference: [12] <author> R. </author> <title> Bod. Using an annotated corpus as a stochastic grammar. </title> <booktitle> In Proceedings of European ACL, </booktitle> <year> 1993. </year>
Reference-contexts: Parsing is carried out by repeatedly reducing a pair of tags to a single tag in a way that maximizes the similarity of the two items involved in every reduction. In <ref> [12] </ref>, statistics are calculated on all possible subtrees contained in a structurally annotated training corpus. <p> A rule such as pronoun ! determiner noun would have a good score, since a pronoun and a noun phrase are distributionally similar. None of these methods were tested in a way that allows them to be readily compared to other methods. In <ref> [12] </ref>, statistics are calculated on all possible subtrees contained in a structurally annotated training corpus. A Monte Carlo technique [50] is then used to combine subtrees when parsing fresh text. <p> After applying all learned transformations to the test corpus, 60% of the sentences had no crossing constituents, 74% had fewer than two crossing constituents, and 85% had fewer than three. In <ref> [12] </ref>, training and testing were also done on the ATIS corpus, with 96% of sentences in the test set were parsed exactly correctly (a much more difficult task) when training on 675 sentences. However, when training on only 150 sentences, accuracy is approximately 30%.
Reference: [13] <author> L. Breiman, J. Friedman, R. Olshen, and C. Stone. </author> <title> Classification and regression trees. </title> <publisher> Wadsworth and Brooks, </publisher> <year> 1984. </year>
Reference-contexts: In addition, transformations are learned in the transformation-based learner, whereas the rules of GPS are prespecified. The technique employed by the learner is also similar to that used in decision trees <ref> [13, 91, 92] </ref>. A decision tree is trained on a set of preclassified entities and outputs a set of questions that can be asked about an entity to determine its proper classification.
Reference: [14] <author> M. Brent. </author> <title> Automatic acquisition of subcategorization frames from untagged text. </title> <booktitle> In Proceedings of the 29th Annual Meeting of the Association for Computational Linguistics, </booktitle> <address> Berkeley, Ca., </address> <year> 1991. </year>
Reference-contexts: In [47], a semi-automated procedure is described for learning what classes of objects in a subdomain can enter into a subject-verb-object relationship. However, the procedure 28 needs a reliable parser, and a great deal of human intervention. <ref> [14] </ref> describes a procedure for extracting verb subcategorization information from an unannotated text. The verb subcategorization frames that it finds include: direct object, direct object and clause, direct object and infinitive, clause, and infinitive. This procedure works without a parser and, once written, needs no human supervision.
Reference: [15] <author> E. Brill. </author> <title> Discovering the lexical features of a language. </title> <booktitle> In Proceedings of the 29th Annual Meeting of the Association for Computational Linguistics, </booktitle> <address> Berkeley, Ca., </address> <year> 1991. </year>
Reference-contexts: Other areas include machine translation [27], word sense disambiguation [28], word clustering <ref> [22, 15, 29, 89] </ref>, and pronoun resolution [30]. 31 Chapter 4 Transformation-Based Error-Driven Learning Applied to Natural Language 4.1 Introduction In this section, we describe a framework for learning which has been effectively applied to a number of language learning problems. We call this framework transformation-based error-driven learning. <p> However, for a human to do this from scratch would probably require some linguistic knowledge as well as some familiarity with the corpus being processed. 3 Earlier versions of this work appear in <ref> [22, 15] </ref>. 53 D (P jjQ) = x P (x) The divergence of P and Q is then defined as: Div (P; Q) = Div (Q; P ) = D (P jjQ) + D (QjjP ) For two words x and y, let P left;x (z) be the probability of word <p> Therefore, whether the trigger applies for a particular word type is computed based on the words and word pairs occurring in the large unannotated training corpus. Note also that while bigram statistics have been used often to characterize words (eg. <ref> [22, 15, 24, 29, 37] </ref>), the approach taken here is different in that unlike all of these 6 For reasons of processing efficiency, Y is constrained to be one of the n most frequently occurring words, where n was arbitrarily set to 200 in all experiments described here. 63 systems we
Reference: [16] <author> E. Brill. </author> <title> A simple rule-based part of speech tagger. </title> <booktitle> In Proceedings of the Third Conference on Applied Natural Language Processing, ACL, </booktitle> <address> Trento, Italy, </address> <year> 1992. </year>
Reference-contexts: In [57], part of speech tagging rules are discovered automatically within a sophisticated Marcus-style parser [81]. Rules make reference to the state of the parser during the processing of the word being tagged. In <ref> [16] </ref>, a simple rule-based tagger is described. In [7], a decision tree is used in tagging. In all of these approaches, contextual information is used to disambiguate from an already known set of allowable part of speech tags. <p> These variables can significantly effect performance, and without factoring this in, we cannot be sure if we are measuring the success of a tagging method, or merely the success of the extra information provided. In light of the comparable performance achieved by all taggers, that described in <ref> [16] </ref> and below is much simpler than the others. For example, contextual information is captured in fewer than 100 rules in [16], compared to a 30,000 to 40,000 leaf decision tree in [7] and a table of tens of thousands of contextual probabilities in [36]. 3.2 Learning Lexical Information Distributional techniques <p> In light of the comparable performance achieved by all taggers, that described in <ref> [16] </ref> and below is much simpler than the others. For example, contextual information is captured in fewer than 100 rules in [16], compared to a 30,000 to 40,000 leaf decision tree in [7] and a table of tens of thousands of contextual probabilities in [36]. 3.2 Learning Lexical Information Distributional techniques have also been useful in helping a lexicographer uncover lexical information about words that he might not have been able to <p> To help solidify the ideas described in this section, we will briefly outline the error-driven part of speech tagger we have developed <ref> [16] </ref> (which we discuss in more detail in a later chapter). In this system, the initial state is an algorithm that tags every word with its most probable tag in isolation, along with a guessing procedure for unknown words. <p> To do this, we once again use a transformation-based learner. In <ref> [16] </ref>, we describe a transformation-based part of speech tagger. 15 This tagger works by first tagging every word with its most probable part of speech estimated from a large corpus of annotated text, and then automatically learning a small set of contextually triggered transformations to improve tagging performance. <p> Although only a small annotated corpus was needed for learning context-triggered transformations in <ref> [16] </ref>, a very large annotated corpus (over one million words) was used for training the most-likely-tag tagger as well as the procedure for tagging unknown words. In addition, this tagger included one manually created rule for distinguishing between common nouns and proper nouns. <p> But, all specific information, such as cues to be used for distinguishing between proper and common nouns, is learned. This makes the system much more portable. In addition, in the transformation-based tagger described in <ref> [16] </ref>, the tagging of a word in the test set that was also seen in the training set is only changed to X if the word was tagged with X somewhere in the training corpus. <p> In [102], it is shown that a grammar produced this way is ineffective, in fact performing worse than assigning right linear structure to input sentences. 5 This is the same output given by systems described in <ref> [79, 16, 88, 102] </ref>. 6 Note that we do not necessarily have to begin in a naive start state.
Reference: [17] <author> E. Brill. </author> <title> Automatic grammar induction and parsing free text: A transformation-based approach. </title> <booktitle> In Proceedings of the 31st Meeting of the Association of Computational Linguistics, </booktitle> <address> Columbus, Oh., </address> <year> 1993. </year>
Reference-contexts: They can be tagged using the minimal resource tagger described in the previous section, or any of the many other available taggers if sufficient training material is available. 2 This work has also been reported in <ref> [17, 18, 19] </ref>. 97 the left and strings on the right is likely to be a phrase boundary. [100] defines a function to score the quality of parse trees and a move set, and then uses simulated annealing to heuristically explore the entire space of possible parses for a given sentence.
Reference: [18] <author> E. Brill. </author> <title> Automatic grammar induction and parsing free text: A transformation-based approach. </title> <booktitle> In Proceedings of the ARPA Human Language Technology Workshop, </booktitle> <address> Princeton, N.J., </address> <year> 1993. </year> <month> 146 </month>
Reference-contexts: They can be tagged using the minimal resource tagger described in the previous section, or any of the many other available taggers if sufficient training material is available. 2 This work has also been reported in <ref> [17, 18, 19] </ref>. 97 the left and strings on the right is likely to be a phrase boundary. [100] defines a function to score the quality of parse trees and a move set, and then uses simulated annealing to heuristically explore the entire space of possible parses for a given sentence.
Reference: [19] <author> E. Brill. </author> <title> Transformation-based error-driven parsing. </title> <booktitle> In Proceedings of the Third International Workshop on Parsing Technologies, </booktitle> <address> Tilburg, The Netherlands, </address> <year> 1993. </year>
Reference-contexts: They can be tagged using the minimal resource tagger described in the previous section, or any of the many other available taggers if sufficient training material is available. 2 This work has also been reported in <ref> [17, 18, 19] </ref>. 97 the left and strings on the right is likely to be a phrase boundary. [100] defines a function to score the quality of parse trees and a move set, and then uses simulated annealing to heuristically explore the entire space of possible parses for a given sentence.
Reference: [20] <author> E. Brill, E. Haeberli, and T. Kroch. </author> <title> Adventures in tagging Old English. </title> <type> Manuscript, </type> <year> 1993. </year>
Reference-contexts: We have used three different manually annotated corpora: the Penn Treebank [22, 83] and original Brown Corpus [44] for experiments in English and a manually annotated corpus of Old English <ref> [20] </ref>. Note that the main expense in writing and training the learning programs in this learning paradigm is in creating the small annotated corpus. Fortunately, the learning methods do not require a great amount of annotated text for learning.
Reference: [21] <author> E. Brill and S. Kapur. </author> <title> An information-theoretic solution to parameter setting. </title> <type> Technical report, </type> <institution> Institute for Research in Cognitive Science, University of Pennsylvania Number IRCS-93-07, </institution> <year> 1993. </year>
Reference-contexts: If a complex description is needed to fully explain a phenomenon, we can ask to what extent the phenomenon can be explained or captured by a simple analysis of surface structure. For example, there are certainly cases where 1 See <ref> [21] </ref> for an example of using information-theoretic corpus-based techniques to learn more complex phenomena that are not surface-apparent.
Reference: [22] <author> E. Brill, D. Magerman, M. Marcus, and B. Santorini. </author> <title> Deducing linguistic structure from the statistics of large corpora. </title> <booktitle> In Proceedings of the DARPA Speech and Natural Language Workshop, </booktitle> <pages> pages 275-282, </pages> <year> 1990. </year>
Reference-contexts: Tools to automatically tag a text with parts of speech have been very successful. Using these tools to tag text and then having people correct mistakes 24 manually has resulted in very fast and accurate tagging of large amounts of text <ref> [22, 83] </ref>. Although a bit circular, building larger corpora provides training material to build more accurate automatic annotators which can then be used in applications that require annotated input, or to build even larger annotated corpora. <p> Other areas include machine translation [27], word sense disambiguation [28], word clustering <ref> [22, 15, 29, 89] </ref>, and pronoun resolution [30]. 31 Chapter 4 Transformation-Based Error-Driven Learning Applied to Natural Language 4.1 Introduction In this section, we describe a framework for learning which has been effectively applied to a number of language learning problems. We call this framework transformation-based error-driven learning. <p> After the text is annotated by the initial state annotator, it is then compared to the true annotation as indicated by the annotation assigned in the manually annotated training corpus. We have used three different manually annotated corpora: the Penn Treebank <ref> [22, 83] </ref> and original Brown Corpus [44] for experiments in English and a manually annotated corpus of Old English [20]. Note that the main expense in writing and training the learning programs in this learning paradigm is in creating the small annotated corpus. <p> However, for a human to do this from scratch would probably require some linguistic knowledge as well as some familiarity with the corpus being processed. 3 Earlier versions of this work appear in <ref> [22, 15] </ref>. 53 D (P jjQ) = x P (x) The divergence of P and Q is then defined as: Div (P; Q) = Div (Q; P ) = D (P jjQ) + D (QjjP ) For two words x and y, let P left;x (z) be the probability of word <p> Therefore, whether the trigger applies for a particular word type is computed based on the words and word pairs occurring in the large unannotated training corpus. Note also that while bigram statistics have been used often to characterize words (eg. <ref> [22, 15, 24, 29, 37] </ref>), the approach taken here is different in that unlike all of these 6 For reasons of processing efficiency, Y is constrained to be one of the n most frequently occurring words, where n was arbitrarily set to 200 in all experiments described here. 63 systems we
Reference: [23] <author> E. Brill and M. Marcus. </author> <title> Automatically acquiring phrase structure using distributional analysis. </title> <booktitle> In Darpa Workshop on Speech and Natural Language, </booktitle> <address> Harriman, N.Y., </address> <year> 1992. </year>
Reference-contexts: Then a set of moves is defined, which includes changing the nonterminal label of a node and restructuring a tree. Parsing is then carried out using simulated annealing to move through the search space in hope of ending up with a high scoring tree. In <ref> [23] </ref>, distributional analysis techniques similar to those described in [111] are used to automatically learn scored context-free rules. <p> For instance, if a speech system is to properly pronounce the word record, it must know whether the word is being used as a noun or as a verb. Since a number of fairly reliable part of speech taggers have been developed recently (e.g. <ref> [23, 36, 40, 45, 39] </ref>), one may ask why we bother exploring the possibility of creating a part of speech tagger with minimal human supervision. First, it has been shown that a tagger trained on one corpus will perform much worse on a different corpus. <p> In <ref> [23] </ref>, distributional analysis techniques are applied to a large corpus to learn a context-free grammar. Rules are of the form a ! b c, where a, b and c are all part of speech tags.
Reference: [24] <author> E. Brill and M. Marcus. </author> <title> Tagging an unfamiliar text with minimal human supervision. </title> <booktitle> In Proceedings of the Fall Symposium on Probabilistic Approaches to Natural Language. American Association for Artificial Intelligence (AAAI), </booktitle> <year> 1992. </year>
Reference-contexts: We will demonstrate below that the transformation-based approach significantly outperforms this statistical method both on tagging unknown words and overall when both are trained on a much smaller training corpus and obtains comparable performance overall on large corpora. In <ref> [24] </ref>, a different statistical approach was taken to determining the class of unknown words. An informant first listed the open class tags for the corpus. Next, for each open class tag a small list of exemplar words (5-8) was given. <p> Therefore, whether the trigger applies for a particular word type is computed based on the words and word pairs occurring in the large unannotated training corpus. Note also that while bigram statistics have been used often to characterize words (eg. <ref> [22, 15, 24, 29, 37] </ref>), the approach taken here is different in that unlike all of these 6 For reasons of processing efficiency, Y is constrained to be one of the n most frequently occurring words, where n was arbitrarily set to 200 in all experiments described here. 63 systems we
Reference: [25] <author> E. Brill and P. </author> <title> Resnik. A transformation-based approach to prepositional phrase attachment. </title> <type> Technical report, </type> <institution> Department of Computer and Information Science, University of Pennsylvania, </institution> <year> 1993. </year> <month> Forthcoming. </month>
Reference-contexts: A postprocessor could be used whose set of allowable transformations allows for the movement of prepositional phrases to different attachment locations. The prepositional phrase attachment module (also discussed in <ref> [25] </ref>) learns transformations from a corpus of 4-tuples of the form: (v n1 p n2), where v is the matrix verb, n1 is the head of the object noun phrase, p is the preposition and n2 is the head of the noun phrase governed by the preposition (for example, see/v the
Reference: [26] <author> T. Briscoe and N. Waegner. </author> <title> Robust stochastic parsing using the inside-outside algorithm. </title> <booktitle> In Workshop notes from the AAAI Statistically-Based NLP Techniques Workshop, </booktitle> <year> 1992. </year>
Reference-contexts: The inside-outside algorithm [3] is a method for training stochastic context-free grammars. It is an extension of the Baum-Welch [4] algorithm for training stochastic finite state automata. 2 A number of recent papers have explored the potential of using the i-o algorithm to automatically learn a grammar <ref> [75, 104, 88, 26, 31, 102] </ref>. A probabilistic context free grammar begins with some initial, possibly random, probabilities. The inside-outside algorithm is an estimation maximization algorithm which iteratively changes the rule probabilities to increase the probability of the training corpus. <p> A number of recent papers have explored the potential of using the inside-outside algorithm to automatically learn a grammar <ref> [75, 104, 88, 26, 31, 102] </ref>. In the inside-outside algorithm, context-free rule probabilities are incrementally altered in a way that increases the probability of the training corpus.
Reference: [27] <author> P. Brown, J. Cocke, S. Della Pietra, V. Della Pietra, F. Jelinek, J. Lafferty, R. Mercer, and P. Roossin. </author> <title> A statistical approach to machine translation. </title> <journal> Computational Linguistics, </journal> <volume> 16(2), </volume> <year> 1990. </year> <month> 147 </month>
Reference-contexts: Other areas include machine translation <ref> [27] </ref>, word sense disambiguation [28], word clustering [22, 15, 29, 89], and pronoun resolution [30]. 31 Chapter 4 Transformation-Based Error-Driven Learning Applied to Natural Language 4.1 Introduction In this section, we describe a framework for learning which has been effectively applied to a number of language learning problems.
Reference: [28] <author> P. Brown, J. Lai, and R. Mercer. </author> <title> Word-sense disambiguation using statistical meth-ods. </title> <booktitle> In Proceedings of the 29th Annual Meeting of the Association for Computational Linguistics, </booktitle> <address> Berkeley, Ca., </address> <year> 1991. </year>
Reference-contexts: Other areas include machine translation [27], word sense disambiguation <ref> [28] </ref>, word clustering [22, 15, 29, 89], and pronoun resolution [30]. 31 Chapter 4 Transformation-Based Error-Driven Learning Applied to Natural Language 4.1 Introduction In this section, we describe a framework for learning which has been effectively applied to a number of language learning problems.
Reference: [29] <author> P. Brown, V. Della Pietra, S. Della Pietra, and R. Mercer. </author> <title> Class-based n-gram models of natural language. </title> <booktitle> Computational Linguistics, </booktitle> <year> 1992. </year>
Reference-contexts: Other areas include machine translation [27], word sense disambiguation [28], word clustering <ref> [22, 15, 29, 89] </ref>, and pronoun resolution [30]. 31 Chapter 4 Transformation-Based Error-Driven Learning Applied to Natural Language 4.1 Introduction In this section, we describe a framework for learning which has been effectively applied to a number of language learning problems. We call this framework transformation-based error-driven learning. <p> We have chosen against pursuing (a) because the amount of manual labor necessary in the semi-automatic method is so small, we do not see a need for a fully automatic system. Approach (b) has been attempted elsewhere <ref> [29] </ref>; we believe that decoupling has the advantage of intelligently using a small amount of human supervision to guide the learning process in a way that should lead to more intuitive classes. <p> The method proposed by Kiss is not fully automatic. He manually chooses the set of words that will be clustered. The experiments of [97] and [70] left open the question of whether these techniques could succeed on free text. <ref> [29] </ref> describes another method of classifying words based upon distributional similarity of words in adjacent-word environments. They attempt to find the assignment of words to classes which results in the smallest loss of average mutual information between immediately adjacent word classes in the corpus. <p> Therefore, whether the trigger applies for a particular word type is computed based on the words and word pairs occurring in the large unannotated training corpus. Note also that while bigram statistics have been used often to characterize words (eg. <ref> [22, 15, 24, 29, 37] </ref>), the approach taken here is different in that unlike all of these 6 For reasons of processing efficiency, Y is constrained to be one of the n most frequently occurring words, where n was arbitrarily set to 200 in all experiments described here. 63 systems we
Reference: [30] <author> C. Cardie. </author> <title> Corpus-based acquisition of relative pronoun disambiguation heuristics. </title> <booktitle> In Proceedings of the 31st Annual Meeting of the Association for Computational Linguistics, </booktitle> <year> 1993. </year>
Reference-contexts: Other areas include machine translation [27], word sense disambiguation [28], word clustering [22, 15, 29, 89], and pronoun resolution <ref> [30] </ref>. 31 Chapter 4 Transformation-Based Error-Driven Learning Applied to Natural Language 4.1 Introduction In this section, we describe a framework for learning which has been effectively applied to a number of language learning problems. We call this framework transformation-based error-driven learning.
Reference: [31] <author> G. Carroll and E. Charniak. </author> <title> Learning probabilistic dependency grammars from labelled text. </title> <booktitle> In Proceedings of the Fall Symposium on Probabilistic Approaches to Natural Language. American Association for Artificial Intelligence (AAAI), </booktitle> <year> 1992. </year>
Reference-contexts: The inside-outside algorithm [3] is a method for training stochastic context-free grammars. It is an extension of the Baum-Welch [4] algorithm for training stochastic finite state automata. 2 A number of recent papers have explored the potential of using the i-o algorithm to automatically learn a grammar <ref> [75, 104, 88, 26, 31, 102] </ref>. A probabilistic context free grammar begins with some initial, possibly random, probabilities. The inside-outside algorithm is an estimation maximization algorithm which iteratively changes the rule probabilities to increase the probability of the training corpus. <p> A number of recent papers have explored the potential of using the inside-outside algorithm to automatically learn a grammar <ref> [75, 104, 88, 26, 31, 102] </ref>. In the inside-outside algorithm, context-free rule probabilities are incrementally altered in a way that increases the probability of the training corpus.
Reference: [32] <author> E. Charniak. </author> <title> A parser with something for everyone. </title> <publisher> Academic Press, </publisher> <year> 1983. </year> <title> In Parsing Natural Language, M. King editor. </title>
Reference-contexts: For example, see <ref> [32] </ref>. 39 For error-driven learning to succeed, it must be the case that a set of ordered transfor-mations can be learned whose application significantly improves performance over accuracy obtained by simply using start-state information.
Reference: [33] <author> S. Chatman. </author> <title> Immediate constituents and expansion analysis. Word, </title> <type> 11, </type> <year> 1955. </year>
Reference-contexts: environments, he must be familiar with the language being studied, or have access to an informant. 2.3 Discovering Significant Morpheme Strings In the work of the American Structuralists, we find a number of suggestions as to how an immediate constituent analysis can be performed on a sentence. 1 Seymour Chatman <ref> [33] </ref> and Charles Hockett [61] have suggested using a measure somewhat similar to entropy as a tool for breaking a sentence into phrases. <p> A number of proposals came out of the American Structural linguistics school on how a field linguist could determine the phrase structure of sentences in an unfamiliar language <ref> [33, 61, 52, 53, 111] </ref>. We described these approaches in an earlier section. All of these approaches require a trained linguist working with an informant to tease out the structural information of a sentence.
Reference: [34] <author> N. Chomsky. </author> <title> Aspects of the Theory of Syntax. </title> <publisher> MIT Press, </publisher> <address> Cambridge, </address> <year> 1965. </year>
Reference-contexts: The search for universals may also provide an explanation for how a child learns language. It is now commonly believed that language learning cannot proceed as a purely inductive process with no a priori knowledge of the target grammar. Some of the roots of this belief (outlined in <ref> [34] </ref>) include: 1. Poverty of the stimulus: the quantity and quality of evidence in the environment is not conducive to learning. 2.
Reference: [35] <author> N. Chomsky. </author> <title> Language and problems of knowledge. </title> <type> Manuscript, </type> <year> 1987. </year>
Reference-contexts: have compiled a 1779 page reference book entitled "A Comprehensive Grammar of the English Language" which records superficial facts about English, and even this book is no doubt incomplete. 2 1.2 Understanding Human Language Learning Since the work presented in this report and the research program of generative grammarians (e.g. <ref> [35] </ref>) share as a primary goal an explanation of how language can be learned, it is important to be clear about the differences between the two approaches. Although both approaches address language learning, the focus of the two approaches is quite different. <p> Some knowledge of language appears to be shared by many diverse languages. The Principles and Parameters approach <ref> [35] </ref> was offered as a model of how a child could come to acquire the skills which allow her to use language productively. In this model, language is divided into two parts: core and periphery.
Reference: [36] <author> K. Church. </author> <title> A stochastic parts program and noun phrase parser for unrestricted text. </title> <booktitle> In Proceedings of the Second Conference on Applied Natural Language Processing, ACL, </booktitle> <year> 1988. </year>
Reference-contexts: Taylor and Kroch [107] use tagged and bracketed corpora of Middle English and Old English for studying diachronic linguistic phenomena. In computational linguistics, many researchers have been using annotated corpora to train stochastic part of speech taggers and parsers (e.g. <ref> [36, 102] </ref>). Structurally annotated corpora are being used as the gold standard by which different parsers can be objectively compared [5]. <p> When a tagger is trained on tagged text, the state transitions are visible, and so the transition probabilities and the emit probabilities are easy to estimate from the training corpus. The taggers described in <ref> [36, 40, 45, 84] </ref> are trained on tagged text. From a large corpus of tagged text, a set of lexical and contextual probabilities are estimated. Lexical probabilities are P (W jT ), the probability of a word given a part of speech tag. <p> All statistical taggers must 26 deal with smoothing in some way, since an empirical probability estimate of zero can often lead to errors. One problem with current successful approaches to tagging is that none of them handle unknown words in a way that is completely portable. In <ref> [36] </ref>, Church hard-codes a complex procedure for classifying unknown words, including a frequency-dependent procedure for detecting proper nouns, a domain-dependent procedure for classifying words with dashes, a list of abbreviations, a large list of informative suffixes, and a great deal of additional information. <p> For example, contextual information is captured in fewer than 100 rules in [16], compared to a 30,000 to 40,000 leaf decision tree in [7] and a table of tens of thousands of contextual probabilities in <ref> [36] </ref>. 3.2 Learning Lexical Information Distributional techniques have also been useful in helping a lexicographer uncover lexical information about words that he might not have been able to think of through introspection. Recently developed techniques use mutual information, a measure of how the cooccur-rence of two elements compares with chance. <p> For instance, if a speech system is to properly pronounce the word record, it must know whether the word is being used as a noun or as a verb. Since a number of fairly reliable part of speech taggers have been developed recently (e.g. <ref> [23, 36, 40, 45, 39] </ref>), one may ask why we bother exploring the possibility of creating a part of speech tagger with minimal human supervision. First, it has been shown that a tagger trained on one corpus will perform much worse on a different corpus. <p> Currently, training an accurate tagger requires a great deal of human labor. For example, in the tagger described in <ref> [36] </ref>, the program includes: * Statistics gathered from one million words of manually tagged text. * Rules discovered via experimentation for dealing with hard tagging distinctions such as proper noun vs. common noun. 51 * A manually encoded list of dates, unlikely proper nouns, titles, states. * A module for dealing <p> Since we are interested in a system that can be easily trained and retrained for new domains or languages, we will not discuss methods that have a great deal of domain-dependent knowledge built in (such as <ref> [36, 45] </ref>). In [73, 84], a probabilistic method of tagging unknown words is discussed. Since the two methods are fairly similar, 59 60 we will only describe the algorithm presented in [84]. The tagger used is a Markov-model based tagger trained on tagged text.
Reference: [37] <author> K. Church, W. Gale, P. Hanks, and D. Hindle. </author> <title> Parsing, word associations and typical predicate-argument relations. </title> <booktitle> In Proceedings of the International Workshop on Parsing Technologies, </booktitle> <year> 1989. </year>
Reference-contexts: If I (x; y) &lt; 0, then they occur together less than chance would predict. One would expect I (clouds,rain) to be positive, I (rain, sunshine) to be negative, and I (even numbered day, rain) to be zero. In <ref> [37] </ref> it is shown how one can use the mutual information statistic to uncover lexical information. In this paper, they compute the mutual information of strong and powerful for all words that occur next to these two words in the 1988 Associated Press newswire. <p> Therefore, whether the trigger applies for a particular word type is computed based on the words and word pairs occurring in the large unannotated training corpus. Note also that while bigram statistics have been used often to characterize words (eg. <ref> [22, 15, 24, 29, 37] </ref>), the approach taken here is different in that unlike all of these 6 For reasons of processing efficiency, Y is constrained to be one of the n most frequently occurring words, where n was arbitrarily set to 200 in all experiments described here. 63 systems we
Reference: [38] <author> T. Cover and J. Thomas. </author> <title> Elements of Information Theory. </title> <publisher> Wiley and Sons, </publisher> <address> New York, </address> <year> 1991. </year>
Reference-contexts: A number of different similarity measures could be used. We chose to use relative entropy, also known as the Kullback-Leibler distance <ref> [72, 38] </ref>. The Kullback Leibler distance from probability distribution P to probability distribution Q is defined as: is necessary, as it might not accomplish anything that could not be done rapidly by human introspection.
Reference: [39] <author> D. Cutting, J. Kupiec, J. Pedersen, and P. Sibun. </author> <title> A practical part-of-speech tagger. </title> <booktitle> In Proceedings of the Third Conference on Applied Natural Language Processing, ACL, </booktitle> <address> Trento, Italy, </address> <year> 1992. </year> <month> 148 </month>
Reference-contexts: The taggers described in <ref> [39, 65, 73] </ref> are of this type. They use the Baum-Welch algorithm [4] to train the model, and then use this trained model for tagging fresh text. It is not clear whether this approach of training on untagged text provides an effective and portable method of tagging. <p> For instance, if a speech system is to properly pronounce the word record, it must know whether the word is being used as a noun or as a verb. Since a number of fairly reliable part of speech taggers have been developed recently (e.g. <ref> [23, 36, 40, 45, 39] </ref>), one may ask why we bother exploring the possibility of creating a part of speech tagger with minimal human supervision. First, it has been shown that a tagger trained on one corpus will perform much worse on a different corpus.
Reference: [40] <author> S. Derose. </author> <title> Grammatical category disambiguation by statistical optimization. </title> <journal> Com--putational Linguistics, </journal> <volume> 14, </volume> <year> 1988. </year>
Reference-contexts: When a tagger is trained on tagged text, the state transitions are visible, and so the transition probabilities and the emit probabilities are easy to estimate from the training corpus. The taggers described in <ref> [36, 40, 45, 84] </ref> are trained on tagged text. From a large corpus of tagged text, a set of lexical and contextual probabilities are estimated. Lexical probabilities are P (W jT ), the probability of a word given a part of speech tag. <p> For instance, if a speech system is to properly pronounce the word record, it must know whether the word is being used as a noun or as a verb. Since a number of fairly reliable part of speech taggers have been developed recently (e.g. <ref> [23, 36, 40, 45, 39] </ref>), one may ask why we bother exploring the possibility of creating a part of speech tagger with minimal human supervision. First, it has been shown that a tagger trained on one corpus will perform much worse on a different corpus.
Reference: [41] <author> G. Ernst and A. Newell. </author> <title> GPS: A case study in generality and problem solving. </title> <publisher> Academic Press, </publisher> <year> 1969. </year>
Reference-contexts: The first transformation is applied to the entire corpus, resulting in Corpus-1. The second transformation is applied to Corpus-1, and so on until the list of transformations is exhausted. 36 37 Transformation-based error-driven learning is a degenerate instance of means-ends anal-ysis. GPS (General Problem Solver) <ref> [41, 86] </ref> is probably the earliest successful implementation of a means-ends analysis system. In GPS, a set of rules is specified. Rules have two parts: the preconditions that must be satisfied to trigger a rule, and the effect of carrying out the rule.
Reference: [42] <author> R. Quirk et al. </author> <title> A Comprehensive Grammar of the English Language. </title> <publisher> Longman, </publisher> <address> London, </address> <year> 1985. </year>
Reference-contexts: Much of language understanding involves mastering the many superficial, but highly idiosyncratic, rules of the language. As testament to the vastness of superficial knowledge, Quirk and his colleagues <ref> [42] </ref> have compiled a 1779 page reference book entitled "A Comprehensive Grammar of the English Language" which records superficial facts about English, and even this book is no doubt incomplete. 2 1.2 Understanding Human Language Learning Since the work presented in this report and the research program of generative grammarians (e.g.
Reference: [43] <author> S. Fong. </author> <title> Computational Properties of Principle Based Grammatical Theories. </title> <type> PhD thesis, </type> <institution> Department of Electrical Engineering and Computer Science, MIT, </institution> <year> 1991. </year>
Reference-contexts: While people studying Principles and Parameters are exploring what facts about language could be accounted for by innate linguistic constraints, we are setting out to explore what facts about language are learnable by applying a learning algorithm to a 3 This is not entirely true. <ref> [43] </ref> describes one attempt at providing an algorithmic account of learning under this formalism. 4 And at this point, superficial. 5 sample of language. The sorts of phenomena the two approaches attempt to explain, as well as the motivations for choosing these phenomena, are also different.
Reference: [44] <author> W. Francis and H. Kucera. </author> <title> Frequency analysis of English usage: Lexicon and grammar. </title> <publisher> Houghton Mi*in, </publisher> <address> Boston, </address> <year> 1982. </year>
Reference-contexts: After the text is annotated by the initial state annotator, it is then compared to the true annotation as indicated by the annotation assigned in the manually annotated training corpus. We have used three different manually annotated corpora: the Penn Treebank [22, 83] and original Brown Corpus <ref> [44] </ref> for experiments in English and a manually annotated corpus of Old English [20]. Note that the main expense in writing and training the learning programs in this learning paradigm is in creating the small annotated corpus. <p> tokens and 3 types. 43 correct dictionary entries, then assuming a text size of one million words, giving the two lists of 50 words would give an accuracy of only 50%. 7 To give a concrete example of Zipfian behavior in a natural language corpus, in the Brown Corpus 8 <ref> [44] </ref>, two percent of the word types account for sixty nine percent of the word tokens. About seventy five percent of the word types occur five or fewer times in the corpus. Fifty eight percent of word types occur two or fewer times, and forty four percent only occur once. <p> the accuracy of this learning module, we tested it on two different English corpora and one corpus of Old English. 8 Three different part of speech sets were used: Penn Treebank tags for the Brown Corpus and Wall Street Journal [83], the original Brown tag set for the Brown Corpus <ref> [44] </ref>, and a tag set which is a derivative of the Penn Treebank tags derived by Eric Haeberli and Tony Kroch for the Old English corpus. Wall Street Journal The Wall Street Journal corpus is a set of stories from the Wall Street Journal sorted in chronological order. <p> We next ran the same experiment on the Brown Corpus using the original Brown Corpus part of speech tags <ref> [44] </ref>. 12 The original Brown Corpus tag set is considerably larger than the Penn Treebank tag set. In the Brown corpus, 65 Penn Treebank tags occur more than once, whereas 159 original Brown Corpus tags occur more than once.
Reference: [45] <editor> R. Garside, G. Leech, and G. Sampson. </editor> <title> The Computational Analysis of English: A Corpus-Based Approach. </title> <publisher> Longman, </publisher> <address> London, </address> <year> 1987. </year>
Reference-contexts: When a tagger is trained on tagged text, the state transitions are visible, and so the transition probabilities and the emit probabilities are easy to estimate from the training corpus. The taggers described in <ref> [36, 40, 45, 84] </ref> are trained on tagged text. From a large corpus of tagged text, a set of lexical and contextual probabilities are estimated. Lexical probabilities are P (W jT ), the probability of a word given a part of speech tag. <p> For instance, if a speech system is to properly pronounce the word record, it must know whether the word is being used as a noun or as a verb. Since a number of fairly reliable part of speech taggers have been developed recently (e.g. <ref> [23, 36, 40, 45, 39] </ref>), one may ask why we bother exploring the possibility of creating a part of speech tagger with minimal human supervision. First, it has been shown that a tagger trained on one corpus will perform much worse on a different corpus. <p> Since we are interested in a system that can be easily trained and retrained for new domains or languages, we will not discuss methods that have a great deal of domain-dependent knowledge built in (such as <ref> [36, 45] </ref>). In [73, 84], a probabilistic method of tagging unknown words is discussed. Since the two methods are fairly similar, 59 60 we will only describe the algorithm presented in [84]. The tagger used is a Markov-model based tagger trained on tagged text.
Reference: [46] <author> D. Goldberg. </author> <title> Genetic algorithms in search, optimization, and learning. </title> <publisher> Addison-Wesley, </publisher> <year> 1989. </year>
Reference-contexts: One could use other control strategies, such as search with a look-ahead of greater than one transformation, or other strategies for dealing with a large search space such as simulated annealing [69] or a genetic algorithm <ref> [46] </ref>. In transformation-based error-driven learning, there are two pieces of knowledge that need to be prespecified: the start state annotation algorithm and the set of transformation templates. The prespecified knowledge is very cheap to create.
Reference: [47] <author> R. Grishman, L. Hirschman, and N. </author> <title> Nhan. Discovery procedures for sublanguage selectional patterns: Initial experiments. </title> <journal> Computational Linguistics, </journal> <volume> 12(3), </volume> <year> 1986. </year>
Reference-contexts: From a list such as this, a lexicographer could uncover subtle differences between words that he may not have thought of without the aid of such a list. In <ref> [47] </ref>, a semi-automated procedure is described for learning what classes of objects in a subdomain can enter into a subject-verb-object relationship. However, the procedure 28 needs a reliable parser, and a great deal of human intervention. [14] describes a procedure for extracting verb subcategorization information from an unannotated text.
Reference: [48] <author> H. Guiter and M. Arapov, </author> <title> editors. Studies on Zipf 's Law. </title> <editor> Studienverlag Dr. N. Brochmeyer, Bochum, </editor> <booktitle> 1982. Quantitative Linguistics, </booktitle> <volume> Vol. </volume> <pages> 16. </pages>
Reference-contexts: For instance, if city populations were to obey Zipf's law, that would mean that if the most populous city has population n, then the second largest city would have population n/2, the third largest n/3 and so on. Figure 4.4 (reproduced from <ref> [48] </ref>) demonstrates this phenomenon over actual city census data. Zipf observed that this law seemed to hold for frequency data from a number of disparate areas, including city populations and word frequencies in texts written in various languages.
Reference: [49] <author> R. Haigh and G. Sampson adn E. Atwell. </author> <title> Project APRIL a progress report. </title> <booktitle> In Proceedings of the Annual Meeting of the Association for Computational Linguistics, </booktitle> <address> Buffalo, N.Y., </address> <year> 1988. </year>
Reference-contexts: In [106], using mutual information (called interword predictability by Stolz) to discover phrases is suggested, with the crucial insight being that local minima in interword predictability correlate well with phrase boundaries. In [79], this idea is elaborated upon and tested on a large corpus. In <ref> [49, 100] </ref>, simulated annealing is used to parse a sentence. First, a scoring function 29 is defined that can take any tree structure as input and score the quality of that tree.
Reference: [50] <author> J. Hammersley and D. Handscomb. </author> <title> Monte Carlo Methods. </title> <publisher> Chapman and Hall, </publisher> <year> 1964. </year>
Reference-contexts: In [12], statistics are calculated on all possible subtrees contained in a structurally annotated training corpus. Since finding the optimal, or highest probability, combination of subtrees would result in a parser requiring exponential time, a Monte Carlo technique <ref> [50] </ref> is used to find a good guess at the optimal combination of subtrees when parsing fresh text. The inside-outside algorithm [3] is a method for training stochastic context-free grammars. <p> None of these methods were tested in a way that allows them to be readily compared to other methods. In [12], statistics are calculated on all possible subtrees contained in a structurally annotated training corpus. A Monte Carlo technique <ref> [50] </ref> is then used to combine subtrees when parsing fresh text. This technique has been shown to be effective on a corpus from a very constrained domain, but it may not be possible to scale it up to effectively parse richer domains.
Reference: [51] <author> D. Hardt. </author> <title> An algorithm for VP ellipsis. </title> <booktitle> In Proceedings of the 29th Annual Meeting of the Association for Computational Linguistics, </booktitle> <year> 1992. </year>
Reference-contexts: Linguists are using structurally annotated corpora to study a number of linguistic phenomena. Hardt <ref> [51] </ref> uses tagged corpora for a study on VP ellipsis. Niv [87] uses a syntactically annotated corpus to develop a theory about how humans resolve syntactic ambiguity in parsing. Taylor and Kroch [107] use tagged and bracketed corpora of Middle English and Old English for studying diachronic linguistic phenomena.
Reference: [52] <author> Z. Harris. </author> <title> From morpheme to utterance. </title> <booktitle> Language, </booktitle> <volume> 22, </volume> <year> 1946. </year> <month> 149 </month>
Reference-contexts: This method of finding phrase boundaries is similar to that proposed by Harris [54] for determining the morphemes of a language. A different approach to immediate constituent analysis has been suggested by Zellig Harris <ref> [52, 53] </ref> and Rulon Wells [111]. Since the work of Wells incorporates and expands upon the ideas of Harris, we will only discuss Wells' work here. <p> A number of proposals came out of the American Structural linguistics school on how a field linguist could determine the phrase structure of sentences in an unfamiliar language <ref> [33, 61, 52, 53, 111] </ref>. We described these approaches in an earlier section. All of these approaches require a trained linguist working with an informant to tease out the structural information of a sentence.
Reference: [53] <author> Z. Harris. </author> <title> Structural Linguistics. </title> <publisher> University of Chicago Press, </publisher> <address> Chicago, </address> <year> 1951. </year>
Reference-contexts: Zellig Harris attempted to describe the structuralist idea of language analysis with sufficient rigor so that it could conceivably be written as a computer program. Harris 18 developed rules that a linguist doing field work could use to uncover the structure of an unfamiliar language <ref> [53] </ref>. In addition to the hope of eventually automating the process, Harris was troubled by the lack of rigor in the analysis linguists carried out on data collected from field work. <p> Below we describe three different discovery procedures posed by Harris and his contemporaries. 2.1 Discovering Morpheme and Word Boundaries Harris proposed a method to discover morpheme boundaries within a word and word boundaries within a sentence <ref> [53, 54] </ref>. The procedure is given a sentence as input, transcribed either in phonemes or letters. For each prefix of the sentence, the number of phonemes that can follow is computed. <p> This method of finding phrase boundaries is similar to that proposed by Harris [54] for determining the morphemes of a language. A different approach to immediate constituent analysis has been suggested by Zellig Harris <ref> [52, 53] </ref> and Rulon Wells [111]. Since the work of Wells incorporates and expands upon the ideas of Harris, we will only discuss Wells' work here. <p> We have developed a learning algorithm which we believe to be quite successful at learning a considerable amount of structural information about language. In building the programs that comprise the learning system, we follow Harris' layered approach <ref> [53] </ref>, first addressing the learning of word classes and then learning phrase structure. This is done in part because mapping words into classes can help get around the sparse data problem in phrase structure learning. <p> The work is based upon the hypothesis that whenever two words are syntactically or semantically dissimilar, this difference will manifest itself in the syntax via lexical distribution, an idea suggested in <ref> [53] </ref>. <p> The only information used is the boolean value of whether a particular bigram was seen at all in the training corpus. This is similar to the nonstatistical use of distributional environments in the theory of Harris <ref> [53] </ref>. Harris states that since using the sum total of all allowable short environments an entity is licensed to appear in may not be a good way to classify words, a linguist could find diagnostic environments that can be used to test if a word belongs to a particular class. <p> A number of proposals came out of the American Structural linguistics school on how a field linguist could determine the phrase structure of sentences in an unfamiliar language <ref> [33, 61, 52, 53, 111] </ref>. We described these approaches in an earlier section. All of these approaches require a trained linguist working with an informant to tease out the structural information of a sentence.
Reference: [54] <author> Z. Harris. </author> <title> From phoneme to morpheme. </title> <booktitle> Language, </booktitle> <volume> 31, </volume> <year> 1955. </year>
Reference-contexts: Below we describe three different discovery procedures posed by Harris and his contemporaries. 2.1 Discovering Morpheme and Word Boundaries Harris proposed a method to discover morpheme boundaries within a word and word boundaries within a sentence <ref> [53, 54] </ref>. The procedure is given a sentence as input, transcribed either in phonemes or letters. For each prefix of the sentence, the number of phonemes that can follow is computed. <p> When extracting information from a corpus, we have an estimate of the probability of an entity appearing in that environment. This method of finding phrase boundaries is similar to that proposed by Harris <ref> [54] </ref> for determining the morphemes of a language. A different approach to immediate constituent analysis has been suggested by Zellig Harris [52, 53] and Rulon Wells [111]. Since the work of Wells incorporates and expands upon the ideas of Harris, we will only discuss Wells' work here.
Reference: [55] <author> Z. Harris. </author> <title> String Analysis of Language Structure. </title> <publisher> Mouton and Co., </publisher> <address> The Hague, </address> <year> 1962. </year>
Reference-contexts: There have been a number of attempts at rule-based tagging as well. Rule-based taggers date back as far as <ref> [55, 71] </ref>, but only recently, with the availability of fast computers and large corpora, have these taggers been able to tag with extremely high accuracy. In [57], part of speech tagging rules are discovered automatically within a sophisticated Marcus-style parser [81].
Reference: [56] <author> C. Hemphill, J. Godfrey, and G. Doddington. </author> <title> The ATIS spoken language systems pilot corpus. </title> <booktitle> In Proceedings of the DARPA Speech and Natural Language Workshop, </booktitle> <year> 1990. </year>
Reference-contexts: structure: ( ( ( We ( called them ) ) ( , ( but ( they left ) ) ) ) . ) 7.1.2 Results Using Manually Tagged Text In the first experiment we ran, training and testing were done on the Texas Instruments Air Travel Information System (ATIS) corpus <ref> [56] </ref>. 11 In table 7.1, we compare results obtained using transformation-based error-driven learning to results cited in [88] using the inside-outside algorithm on the same corpus. Accuracy is measured in terms of the percentage of noncrossing constituents in the test corpus, as described above.
Reference: [57] <author> D. Hindle. </author> <title> Acquiring disambiguation rules from text. </title> <booktitle> In Proceedings of the 27th Annual Meeting of the Association for Computational Linguistics, </booktitle> <year> 1989. </year>
Reference-contexts: There have been a number of attempts at rule-based tagging as well. Rule-based taggers date back as far as [55, 71], but only recently, with the availability of fast computers and large corpora, have these taggers been able to tag with extremely high accuracy. In <ref> [57] </ref>, part of speech tagging rules are discovered automatically within a sophisticated Marcus-style parser [81]. Rules make reference to the state of the parser during the processing of the word being tagged. In [16], a simple rule-based tagger is described. In [7], a decision tree is used in tagging.
Reference: [58] <author> D. Hindle. </author> <title> Noun classification from predicate-argument structures. </title> <booktitle> In Proceedings of the 28th Annual Meeting of the Association for Computational Linguistics, </booktitle> <address> Pitts-burgh, Pa., </address> <year> 1990. </year>
Reference-contexts: For instance, in <ref> [58] </ref>, nouns are classified based on the mutual information between them and verbs they are the argument of. From a list such as this, a lexicographer could uncover subtle differences between words that he may not have thought of without the aid of such a list.
Reference: [59] <author> D. Hindle and M. Rooth. </author> <title> Structural ambiguity and lexical relations. </title> <booktitle> In Proceedings of the 29th Annual Meeting of the Association for Computational Linguistics, </booktitle> <address> Berkeley, Ca., </address> <year> 1991. </year>
Reference-contexts: This experiment also demonstrates how any feature based lexicon can trivially be incorporated into the learner, by extending transformations to allow them to make reference to a word and any of its features. In <ref> [59] </ref>, Hindle and Rooth use t-score statistics to measure the strength of lexical associations between the preposition and the noun and between the preposition and the verb, and attach prepositional phrases according to these scores.
Reference: [60] <author> L. Hirschman, R. Grishman, and N. Sager. </author> <title> Grammatically-based automatic word class formation. </title> <booktitle> Information Processing and Management, </booktitle> <volume> 11, </volume> <year> 1975. </year>
Reference-contexts: In <ref> [60] </ref>, words are classed according to their distribution in subject-verb-object relationships. For such a method to succeed, one would need to be able to accurately parse the text being analyzed prior to word classification.
Reference: [61] <author> C. Hockett. </author> <title> Review of the mathematical theory of communication by Claude Shannon and Warren Weaver. </title> <booktitle> Language, </booktitle> <volume> 29, </volume> <year> 1953. </year>
Reference-contexts: familiar with the language being studied, or have access to an informant. 2.3 Discovering Significant Morpheme Strings In the work of the American Structuralists, we find a number of suggestions as to how an immediate constituent analysis can be performed on a sentence. 1 Seymour Chatman [33] and Charles Hockett <ref> [61] </ref> have suggested using a measure somewhat similar to entropy as a tool for breaking a sentence into phrases. <p> A number of proposals came out of the American Structural linguistics school on how a field linguist could determine the phrase structure of sentences in an unfamiliar language <ref> [33, 61, 52, 53, 111] </ref>. We described these approaches in an earlier section. All of these approaches require a trained linguist working with an informant to tease out the structural information of a sentence.
Reference: [62] <author> P. Holder, editor. Franz Boas: </author> <title> Introduction to Handbook of American Indian Languages and J.W. </title> <type> Powell: </type> <institution> Indian Linguistic Families of America North of Mexico. University of Nebraska Press, Lincoln, Ne., </institution> <year> 1966. </year>
Reference-contexts: Because of this relationship, we will now briefly examine some past work done in structural linguistics. According to Sampson [99], Franz Boas is the father of linguistic structuralism. Boas was interested in determining the structure of a number of different languages <ref> [11, 62] </ref>. Providing an accurate description of each language was the primary goal of this work. From this work, Boas thought, research could be done to determine the relationship between languages based upon their structural similarity.
Reference: [63] <author> O. Jaeggli and K. Safir, </author> <title> editors. The null subject parameter. </title> <publisher> Foris, </publisher> <address> Dordrecht, </address> <year> 1989. </year>
Reference-contexts: The core contains innate linguistic universals. To account for the differences between languages, some of the rules (or constraints) in the core are parameterized. One such rule is pro-drop <ref> [63] </ref>. In English, the subject of a sentence is necessary in all but imperative sentences; in Spanish, the subject is optional.
Reference: [64] <author> F. Jelinek. </author> <title> Continuous speech recognition by statistical methods. </title> <journal> Proc. IEEE, </journal> <volume> 64 </volume> <pages> 532-556, </pages> <year> 1976. </year>
Reference-contexts: Language models can be used in real-time speech recognition to predict the next word of a stream of language, based upon what last appeared in the stream (e.g. <ref> [64] </ref>) or to rank alternate theories of what was uttered for filtering or for outputting the most probable theory. Most successful language models are n-gram models, basing the probability of a word on the probability of the preceding n words, or classes of these words (e.g. [65]).
Reference: [65] <author> F. Jelinek. </author> <title> Impact of Processing Techniques on Communication. </title> <address> Dordrecht, </address> <year> 1985. </year> <title> In Impact of Processing Techniques on Communication, </title> <editor> J. Skwirzinski, ed. </editor> <volume> 150 </volume>
Reference-contexts: Most successful language models are n-gram models, basing the probability of a word on the probability of the preceding n words, or classes of these words (e.g. <ref> [65] </ref>). Language models based on context free grammars (e.g. [88]) and decision trees (e.g. [2]) have also been proposed. <p> If it is indeed the case that systems that learn are the solution to building robust natural language processing systems, then the process of automated language learning deserves further study. There are many different ways one could try to construct a language learner. In <ref> [65] </ref>, a self-organizing language learner is proposed to be used for language modelling. In [6], a method of combining a large manually constructed grammar with statistical information obtained from a large corpus is discussed. <p> The taggers described in <ref> [39, 65, 73] </ref> are of this type. They use the Baum-Welch algorithm [4] to train the model, and then use this trained model for tagging fresh text. It is not clear whether this approach of training on untagged text provides an effective and portable method of tagging.
Reference: [66] <author> F. Jelinek, J. Lafferty, and R. Mercer. </author> <title> Basic methods of probabilistic context free grammars. </title> <type> Technical report, </type> <institution> IBM, Yorktown Heights, </institution> <year> 1990. </year> <type> Technical Report RC 16374 (72684). </type>
Reference-contexts: In [8], the inside-outside algorithm is used to convert a grammar written by a linguist into a probabilistic grammar where the hope is that the most probable parse is often the correct parse. 2 For a good tutorial on the inside-outside algorithm, see <ref> [66] </ref>. 30 In this thesis, we will discuss an error-driven approach to learning a grammar for bracketing text. The approach works by beginning with a very naive parser, and then learning a set of transformations that can be applied to the output of the parser to make parsing more accurate.
Reference: [67] <author> A. Joshi and L. Levy. </author> <title> Phrase structure trees bear more fruit than you would have thought. </title> <journal> American Journal of Computational Linguistics, </journal> <volume> 8(1), </volume> <year> 1982. </year>
Reference-contexts: Next, we present results obtained from training the system on a corpus annotated with part of speech tags using the lexical and contextual learning modules described in the previous section. 3 In <ref> [77, 67] </ref>, it is shown that nonterminals are not necessary in the sense that for every context free grammar there is a skeletal generating system (a set of trees without nonterminal labels and tree rewriting rules) that generates the same set of strings. 99 7.1.1 The Algorithm The learning algorithm is
Reference: [68] <author> J. Kimball. </author> <title> Seven principles of surface structure parsing in natural language. </title> <journal> Cognition, </journal> <volume> 2, </volume> <year> 1973. </year>
Reference-contexts: In this experiment, the attachment choice for prepositional phrases was between the object noun and the matrix verb. In the initial state annotator, all prepositional phrases are attached to the object noun. 21 This is the attachment predicted by right association <ref> [68] </ref>. The allowable transformations are: * Change the attachment location from X to Y if: - n1 is Z - v is Z 19 This work was done with Philip Resnik. 20 These were extracted by Philip Resnik using tgrep, a tool written by Rich Pito.
Reference: [69] <author> S. Kirkpatrick, C. Gelatt, and M. Vecchi. </author> <title> Optimization by simulated annealing. </title> <journal> Science, </journal> <volume> 220, </volume> <year> 1983. </year>
Reference-contexts: One could use other control strategies, such as search with a look-ahead of greater than one transformation, or other strategies for dealing with a large search space such as simulated annealing <ref> [69] </ref> or a genetic algorithm [46]. In transformation-based error-driven learning, there are two pieces of knowledge that need to be prespecified: the start state annotation algorithm and the set of transformation templates. The prespecified knowledge is very cheap to create.
Reference: [70] <author> G. Kiss. </author> <title> Grammatical word classes: A learning process and its simulation. </title> <journal> Psychology of Learning and Motivation, </journal> <volume> 7, </volume> <year> 1973. </year>
Reference-contexts: In [60], words are classed according to their distribution in subject-verb-object relationships. For such a method to succeed, one would need to be able to accurately parse the text being analyzed prior to word classification. The classification method we described above requires no structural information. [97] and <ref> [70] </ref> both attempt to classify words based upon their immediate neighbors. <p> The method proposed by Kiss is not fully automatic. He manually chooses the set of words that will be clustered. The experiments of [97] and <ref> [70] </ref> left open the question of whether these techniques could succeed on free text. [29] describes another method of classifying words based upon distributional similarity of words in adjacent-word environments.
Reference: [71] <author> S. Klein and R. Simmons. </author> <title> A computational approach to grammatical coding of English words. </title> <journal> JACM, </journal> <volume> 10, </volume> <year> 1963. </year>
Reference-contexts: There have been a number of attempts at rule-based tagging as well. Rule-based taggers date back as far as <ref> [55, 71] </ref>, but only recently, with the availability of fast computers and large corpora, have these taggers been able to tag with extremely high accuracy. In [57], part of speech tagging rules are discovered automatically within a sophisticated Marcus-style parser [81].
Reference: [72] <author> S. Kullback. </author> <title> Information Theory and Statistics. </title> <publisher> John Wiley and Sons, </publisher> <address> New York, </address> <year> 1959. </year>
Reference-contexts: A number of different similarity measures could be used. We chose to use relative entropy, also known as the Kullback-Leibler distance <ref> [72, 38] </ref>. The Kullback Leibler distance from probability distribution P to probability distribution Q is defined as: is necessary, as it might not accomplish anything that could not be done rapidly by human introspection. <p> Unknown words are then classified by comparing their distributional fingerprint to that of each open class and assigning it to the class which is most similar. The similarity measure used was relative entropy <ref> [72] </ref>. We have found that the transformation-based approach significantly outperforms this distributional approach for classifying unknown words. In the transformation-based system, the lexicon will initially be built from the small manually annotated corpus.
Reference: [73] <author> J. Kupiec. </author> <title> Robust part-of-speech tagging using a hidden Markov model. </title> <booktitle> Computer speech and language, </booktitle> <volume> 6, </volume> <year> 1992. </year>
Reference-contexts: The taggers described in <ref> [39, 65, 73] </ref> are of this type. They use the Baum-Welch algorithm [4] to train the model, and then use this trained model for tagging fresh text. It is not clear whether this approach of training on untagged text provides an effective and portable method of tagging. <p> They use the Baum-Welch algorithm [4] to train the model, and then use this trained model for tagging fresh text. It is not clear whether this approach of training on untagged text provides an effective and portable method of tagging. For example, in <ref> [73] </ref>, performance comparable to that obtained by taggers trained on tagged corpora is obtained. However, to obtain this performance, a large dictionary with part of speech and inflectional information was needed, and a number of higher-order procedures were manually built based on manual error analysis. <p> In [36], Church hard-codes a complex procedure for classifying unknown words, including a frequency-dependent procedure for detecting proper nouns, a domain-dependent procedure for classifying words with dashes, a list of abbreviations, a large list of informative suffixes, and a great deal of additional information. In <ref> [73] </ref>, Kupiec provides a list of closed class tags, and assumes that the external dictionary will always list all closed class items. <p> Since we are interested in a system that can be easily trained and retrained for new domains or languages, we will not discuss methods that have a great deal of domain-dependent knowledge built in (such as [36, 45]). In <ref> [73, 84] </ref>, a probabilistic method of tagging unknown words is discussed. Since the two methods are fairly similar, 59 60 we will only describe the algorithm presented in [84]. The tagger used is a Markov-model based tagger trained on tagged text.
Reference: [74] <author> G. Lakoff. Women, </author> <title> Fire and Dangerous Things: What Categories Reveal About the Mind. </title> <publisher> University of Chicago Press, </publisher> <address> Chicago, </address> <year> 1987. </year>
Reference-contexts: It is different with the other parts of speech. No one of these is imperatively required for the life of language." Lakoff <ref> [74] </ref> describes a language in which the class woman-or-fire-or-dangerous-thing exists. This class is based upon ancient folklore of the society in which it is used.
Reference: [75] <author> K. Lari and S. Young. </author> <title> The estimation of stochastic context-free grammars using the inside-outside algorithm. </title> <booktitle> Computer Speech and Language, </booktitle> <volume> 4, </volume> <year> 1990. </year>
Reference-contexts: The inside-outside algorithm [3] is a method for training stochastic context-free grammars. It is an extension of the Baum-Welch [4] algorithm for training stochastic finite state automata. 2 A number of recent papers have explored the potential of using the i-o algorithm to automatically learn a grammar <ref> [75, 104, 88, 26, 31, 102] </ref>. A probabilistic context free grammar begins with some initial, possibly random, probabilities. The inside-outside algorithm is an estimation maximization algorithm which iteratively changes the rule probabilities to increase the probability of the training corpus. <p> A number of recent papers have explored the potential of using the inside-outside algorithm to automatically learn a grammar <ref> [75, 104, 88, 26, 31, 102] </ref>. In the inside-outside algorithm, context-free rule probabilities are incrementally altered in a way that increases the probability of the training corpus.
Reference: [76] <author> H. Lasnik and M. Saito. </author> <title> On the nature of proper government. </title> <journal> Linguistic Inquiry, </journal> <volume> 15(2), </volume> <year> 1984. </year>
Reference-contexts: In addition to subtle word classes, research in modern syntax has uncovered many nonsuperficial restrictions on what structural relationships can hold in a sentence. As one example of a relationship that is not surface-apparent, let us examine the that-trace effect <ref> [76] </ref>.
Reference: [77] <author> L. Levy and A. Joshi. </author> <title> Skeletal structural descriptions. </title> <journal> Information and Control, </journal> <volume> 39(2), </volume> <year> 1978. </year>
Reference-contexts: Next, we present results obtained from training the system on a corpus annotated with part of speech tags using the lexical and contextual learning modules described in the previous section. 3 In <ref> [77, 67] </ref>, it is shown that nonterminals are not necessary in the sense that for every context free grammar there is a skeletal generating system (a set of trees without nonterminal labels and tree rewriting rules) that generates the same set of strings. 99 7.1.1 The Algorithm The learning algorithm is
Reference: [78] <author> B. MacWhinney and C. Snow. </author> <title> The child language data exchange system. </title> <journal> Journal of Child Language, </journal> <volume> 12, </volume> <year> 1985. </year>
Reference-contexts: This means that issues such as providing good probability estimates for observed frequencies of zero need not be addressed. The same experiment was run on a corpus of roughly 1.7 million words of transcribed utterances addressed by parents to their young children <ref> [78] </ref>. The thirty most similar pairs in that corpus are listed in figure 6.2.
Reference: [79] <author> D. Magerman and M. Marcus. </author> <title> Parsing a natural language using mutual information statistics. </title> <booktitle> In Proceedings, Eighth National Conference on Artificial Intelligence (AAAI 90), </booktitle> <year> 1990. </year>
Reference-contexts: A number of papers from the school of structural linguistics addressed this issue. In [106], using mutual information (called interword predictability by Stolz) to discover phrases is suggested, with the crucial insight being that local minima in interword predictability correlate well with phrase boundaries. In <ref> [79] </ref>, this idea is elaborated upon and tested on a large corpus. In [49, 100], simulated annealing is used to parse a sentence. First, a scoring function 29 is defined that can take any tree structure as input and score the quality of that tree. <p> In <ref> [106, 79] </ref>, a statistic based on mutual information is used to find phrase boundaries. The key idea used in these papers is that a position between two words with relatively low mutual information between strings on 1 Of course, the input sentences need not be manually tagged. <p> In [102], it is shown that a grammar produced this way is ineffective, in fact performing worse than assigning right linear structure to input sentences. 5 This is the same output given by systems described in <ref> [79, 16, 88, 102] </ref>. 6 Note that we do not necessarily have to begin in a naive start state.
Reference: [80] <author> B. Mandelbrot. </author> <title> An information theory of the statistical structure of language. </title> <address> Lon-don, </address> <year> 1953. </year> <title> In Communication Theory, </title> <editor> W. Jackson, </editor> <publisher> ed. </publisher>
Reference-contexts: Subsequent to Zipf's claim of uncovering a universal property of human nature, a number of later publications demonstrated that Zipf's Law is a necessary consequent of assuming that the source of the language from which the frequency data is taken is a simple stochastic process <ref> [80, 105] </ref>. In the introduction to [115], George Miller elegantly puts it: Suppose that we acquired a dozen monkeys and chained them to typewriters until they had produced some very long and random sequence of characters.
Reference: [81] <author> M. Marcus. </author> <title> A theory of syntactic recognition for natural language. </title> <publisher> MIT Press, </publisher> <year> 1980. </year>
Reference-contexts: Rule-based taggers date back as far as [55, 71], but only recently, with the availability of fast computers and large corpora, have these taggers been able to tag with extremely high accuracy. In [57], part of speech tagging rules are discovered automatically within a sophisticated Marcus-style parser <ref> [81] </ref>. Rules make reference to the state of the parser during the processing of the word being tagged. In [16], a simple rule-based tagger is described. In [7], a decision tree is used in tagging.
Reference: [82] <author> M. Marcus. </author> <title> Some Inadequate Theories of Human Language Processing. 1984. In: Talking Minds: The Study of Language in the Cognitive Sciences, Bever, </title> <editor> T., Carroll, J. and Miller, L. </editor> <publisher> eds. </publisher>
Reference-contexts: We will briefly discuss each of these applications in turn. 1.3.1 Extracting Meaning From a Sentence In <ref> [82] </ref>, Marcus argues that it would not be possible to extract meaning from a sentence in general without first obtaining syntactic information. The alternate approach is to assume that the meaning of a sentence can be obtained without recourse to syntactic structure. However, there are many problems with this approach.
Reference: [83] <author> M. Marcus, B. Santorini, and M. Marcinkiewicz. </author> <title> Building a large annotated corpus of English: the Penn Treebank. </title> <note> To appear in Computational Linguistics, </note> <year> 1993. </year>
Reference-contexts: Even if an adequate annotation accuracy level cannot be obtained using automated procedures, an automated annotator could still be used to bootstrap the process of manually annotating a corpus. In <ref> [83] </ref>, it is shown that manually correcting the output of an automated tagger results in greater speed and accuracy than manually annotating from scratch. <p> Tools to automatically tag a text with parts of speech have been very successful. Using these tools to tag text and then having people correct mistakes 24 manually has resulted in very fast and accurate tagging of large amounts of text <ref> [22, 83] </ref>. Although a bit circular, building larger corpora provides training material to build more accurate automatic annotators which can then be used in applications that require annotated input, or to build even larger annotated corpora. <p> After the text is annotated by the initial state annotator, it is then compared to the true annotation as indicated by the annotation assigned in the manually annotated training corpus. We have used three different manually annotated corpora: the Penn Treebank <ref> [22, 83] </ref> and original Brown Corpus [44] for experiments in English and a manually annotated corpus of Old English [20]. Note that the main expense in writing and training the learning programs in this learning paradigm is in creating the small annotated corpus. <p> It is much faster to correct annotation errors than to annotate from scratch <ref> [83] </ref>. 1 This could possibly be cut in half. Currently, the lexical and contextual modules are trained on separate annotated corpora. This is so the behavior of unknown words when training the contextual module will be similar to that of fresh text. <p> is O (jT j fl jnj). 6.2.1 Results To assess the accuracy of this learning module, we tested it on two different English corpora and one corpus of Old English. 8 Three different part of speech sets were used: Penn Treebank tags for the Brown Corpus and Wall Street Journal <ref> [83] </ref>, the original Brown tag set for the Brown Corpus [44], and a tag set which is a derivative of the Penn Treebank tags derived by Eric Haeberli and Tony Kroch for the Old English corpus.
Reference: [84] <author> M. Meteer, R. Schwartz, and R. Weischedel. </author> <title> Empirical studies in part of speech labelling. </title> <booktitle> In Proceedings of the fourth DARPA Workshop on Speech and Natural Language, </booktitle> <year> 1991. </year>
Reference-contexts: When a tagger is trained on tagged text, the state transitions are visible, and so the transition probabilities and the emit probabilities are easy to estimate from the training corpus. The taggers described in <ref> [36, 40, 45, 84] </ref> are trained on tagged text. From a large corpus of tagged text, a set of lexical and contextual probabilities are estimated. Lexical probabilities are P (W jT ), the probability of a word given a part of speech tag. <p> In addition, he provides a set of derivational and inflectional suffixes and then trains a probabilistic method for determining their part of speech, trained on the dictionary and a corpus of unannotated text. In <ref> [84] </ref>, a probabilistic procedure is also employed for unknown words. This procedure also requires that an informative set of affixes be provided, as well as other likely cues. In a later chapter, we will discuss a transformation-based learner that automatically learns to tag unfamiliar words with no prior language-specific knowledge. <p> First, it has been shown that a tagger trained on one corpus will perform much worse on a different corpus. In <ref> [84] </ref>, an experiment is run where a tagger is first trained on the Wall Street Journal and tested on the MUC corpus, a corpus of texts on terrorism in Latin America. Next, both training and testing were done using the MUC corpus. <p> Since we are interested in a system that can be easily trained and retrained for new domains or languages, we will not discuss methods that have a great deal of domain-dependent knowledge built in (such as [36, 45]). In <ref> [73, 84] </ref>, a probabilistic method of tagging unknown words is discussed. Since the two methods are fairly similar, 59 60 we will only describe the algorithm presented in [84]. The tagger used is a Markov-model based tagger trained on tagged text. <p> In [73, 84], a probabilistic method of tagging unknown words is discussed. Since the two methods are fairly similar, 59 60 we will only describe the algorithm presented in <ref> [84] </ref>. The tagger used is a Markov-model based tagger trained on tagged text. For known words, P (W jT ) is estimated from the corpus. <p> In figure 6.5, the entropy of the tag distribution for unknown words ( P X2T ags P (XjU nknown) fl log (P (XjU nknown))) is graphed as a function of training corpus size for the same Wall Street Journal samples used in the previous figure. In <ref> [84] </ref>, unknown words are first addressed by attempting to tag using a trigram model 61 by adding P (Unknown WordjT ) for all open class tags T, with training carried out on about one million words of tagged text from the Wall Street Journal. 5 In other words, only general lexical <p> See table 6.1. Keep in mind that these results are obtained prior to using any token-based contextual information. We then compared our results to the results cited in <ref> [84] </ref> for tagging unknown words, using the lexical probabilities: P (w i jt j ) = P (Unknown Wordjt i ) fl P (Capital Featurejt i ) fl P (Endings/Hyphenationjt i ) 71 Training Corpus Unknown Wd. <p> Initially tagging all unknown words as singular common nouns results in an unknown word accuracy of 28.3%. 207 11 The difference in accuracy using the probabilistic method quoted in <ref> [84] </ref> (85%) and the accuracy obtained in our implementation is due to the much smaller training set used to train our implementation. 74 transformations are learned, whose application to the test set results in an unknown word accuracy of 74.7%. <p> In total, 175 transformations were learned. In table 6.3, we show tagging accuracy both before and after applying contextual transformations and compare these 84 results to results obtained using the probabilistic approach of <ref> [84] </ref> which was described above. <p> The transformation-based tagger contains absolutely no prespecified language-specific or corpus-specific information, and relies on no external aids such as dictionaries or affix lists. Two other results are quoted in the literature for stochastic taggers trained on much larger training samples of the Penn Treebank. In <ref> [84] </ref>, an accuracy of 96.3% is obtained when training on one million words. However, the lexicon was closed in the sense that it was built from both the training and test set, and so there were no unknown words.
Reference: [85] <author> G. Miller. </author> <title> Wordnet: an on-line lexical database. </title> <journal> International Journal of Lexicography, </journal> <year> 1990. </year>
Reference-contexts: If transformations only make reference to particular words, sparse data problems may be encountered. One solution to this problem would be to use a manually created lexical hierarchy such as Wordnet <ref> [85] </ref>, and allow transformations to make reference to words and to the word classes each word belongs to. A drawback of this approach is that manually created hierarchies are expensive and time-consuming to create. <p> There are a couple of ways to address the sparse data problem. In this case, mapping words to part of speech will not help. Instead, semantic class information is necessary. One method is to use a manually constructed semantic hierarchy such as that described in <ref> [85] </ref>. Every word can then be expanded to a list of classes it occurs in, and transformations can be triggered by words and word classes. <p> Each node name is a feature, and for each feature x, a word is +x if it is a descendent of the node labelled x, and is x otherwise. We incorporated the idea of using semantic information in the following way. Using the Wordnet noun hierarchy <ref> [85] </ref>, each noun in the training and test set was replaced by a set containing the noun and the name of every class that noun appears in.
Reference: [86] <author> A. Newell and H. Simon. </author> <title> Human Problem Solving. </title> <publisher> Prentice-Hall, </publisher> <year> 1972. </year>
Reference-contexts: The first transformation is applied to the entire corpus, resulting in Corpus-1. The second transformation is applied to Corpus-1, and so on until the list of transformations is exhausted. 36 37 Transformation-based error-driven learning is a degenerate instance of means-ends anal-ysis. GPS (General Problem Solver) <ref> [41, 86] </ref> is probably the earliest successful implementation of a means-ends analysis system. In GPS, a set of rules is specified. Rules have two parts: the preconditions that must be satisfied to trigger a rule, and the effect of carrying out the rule.
Reference: [87] <author> M. Niv. </author> <title> Resolution of syntactic ambiguity: the case of new subjects. </title> <booktitle> In Proceedings of the 15th Annual Meeting of the Cognitive Science Society, </booktitle> <year> 1993. </year>
Reference-contexts: Linguists are using structurally annotated corpora to study a number of linguistic phenomena. Hardt [51] uses tagged corpora for a study on VP ellipsis. Niv <ref> [87] </ref> uses a syntactically annotated corpus to develop a theory about how humans resolve syntactic ambiguity in parsing. Taylor and Kroch [107] use tagged and bracketed corpora of Middle English and Old English for studying diachronic linguistic phenomena.
Reference: [88] <author> F. Pereira and Y. Schabes. </author> <title> Inside-outside reestimation from partially bracketed corpora. </title> <booktitle> In Proceedings of the 30th Annual Meeting of the Association for Computational Linguistics, </booktitle> <address> Newark, De., </address> <year> 1992. </year>
Reference-contexts: Most successful language models are n-gram models, basing the probability of a word on the probability of the preceding n words, or classes of these words (e.g. [65]). Language models based on context free grammars (e.g. <ref> [88] </ref>) and decision trees (e.g. [2]) have also been proposed. <p> The inside-outside algorithm [3] is a method for training stochastic context-free grammars. It is an extension of the Baum-Welch [4] algorithm for training stochastic finite state automata. 2 A number of recent papers have explored the potential of using the i-o algorithm to automatically learn a grammar <ref> [75, 104, 88, 26, 31, 102] </ref>. A probabilistic context free grammar begins with some initial, possibly random, probabilities. The inside-outside algorithm is an estimation maximization algorithm which iteratively changes the rule probabilities to increase the probability of the training corpus. <p> The inside-outside algorithm is an estimation maximization algorithm which iteratively changes the rule probabilities to increase the probability of the training corpus. The algorithm is guaranteed to find a locally optimal assignment of rule probabilities, but not a globally optimal assignment. In <ref> [88, 102] </ref>, it is shown that the inside-outside algorithm can be used to bracket text with high accuracy, with very weak initial knowledge of the grammar. <p> This problem could be resolved, or at least lessened, by using a second training corpus to prune transformations. However, because of the nature of the learner, overtraining is not as harmful in transformation-based learning as it is in other learning paradigms being applied to corpus-based learning (for instance, see <ref> [88, 7] </ref>). At every stage of learning, the transformation-based learner learns the transformation that results in the greatest error reduction. <p> A number of recent papers have explored the potential of using the inside-outside algorithm to automatically learn a grammar <ref> [75, 104, 88, 26, 31, 102] </ref>. In the inside-outside algorithm, context-free rule probabilities are incrementally altered in a way that increases the probability of the training corpus. <p> The inside-outside algorithm can be used to assign probabilities to a symbolic grammar written by a grammarian, or to learn a grammar automatically. In <ref> [88, 102] </ref>, an initial grammar consisting of all possible binary 98 rules (for a particular set of preterminals and nonterminals) is built, and each rule is as-signed a random probability. <p> In [102], it is shown that a grammar produced this way is ineffective, in fact performing worse than assigning right linear structure to input sentences. 5 This is the same output given by systems described in <ref> [79, 16, 88, 102] </ref>. 6 Note that we do not necessarily have to begin in a naive start state. <p> With transformation-based learning, there can be a tighter relationship between the measure that guides learning and the final measure of success. The measure we have chosen for our experiments is the same measure described in <ref> [88] </ref>, which is a variation of a measure which arose out of various meetings on parser evaluation [8]. <p> ) ) ) ) . ) 7.1.2 Results Using Manually Tagged Text In the first experiment we ran, training and testing were done on the Texas Instruments Air Travel Information System (ATIS) corpus [56]. 11 In table 7.1, we compare results obtained using transformation-based error-driven learning to results cited in <ref> [88] </ref> using the inside-outside algorithm on the same corpus. Accuracy is measured in terms of the percentage of noncrossing constituents in the test corpus, as described above. <p> numbers presented above allow us to compare the transformation learner with systems trained and tested on comparable corpora, these results are all based upon the assumption that the test data is tagged fairly reliably (manually tagged text was used in all of these experiments, as well in the experiments of <ref> [88, 102] </ref>.) When parsing free text, 116 we cannot assume that the text will be tagged with the accuracy of a human annotator. Instead, an automatic tagger would have to be used to first tag the text before parsing.
Reference: [89] <author> F. Pereira, N. Tishby, and L. Lee. </author> <title> Distributional clustering of English words. </title> <booktitle> In Proceedings of the 31st Annual Meeting of the Association for Computational Linguistics, </booktitle> <year> 1993. </year> <month> 152 </month>
Reference-contexts: Other areas include machine translation [27], word sense disambiguation [28], word clustering <ref> [22, 15, 29, 89] </ref>, and pronoun resolution [30]. 31 Chapter 4 Transformation-Based Error-Driven Learning Applied to Natural Language 4.1 Introduction In this section, we describe a framework for learning which has been effectively applied to a number of language learning problems. We call this framework transformation-based error-driven learning.
Reference: [90] <author> S. Pinker. </author> <title> Learnability and Cognition. </title> <publisher> MIT Press, </publisher> <address> Cambridge, </address> <year> 1989. </year>
Reference-contexts: However, there are a number of classes and relationships whose existence is not transparent. Pinker <ref> [90] </ref> discusses the class of dativizable verbs. In (1b), the verb gave has undergone dativization. (1a) John gave a painting to the museum. (1b) John gave the museum a painting.
Reference: [91] <author> J. Quinlan. </author> <title> Induction of decision trees. </title> <journal> Machine Learning, </journal> <volume> 1 </volume> <pages> 81-106, </pages> <year> 1986. </year>
Reference-contexts: In addition, transformations are learned in the transformation-based learner, whereas the rules of GPS are prespecified. The technique employed by the learner is also similar to that used in decision trees <ref> [13, 91, 92] </ref>. A decision tree is trained on a set of preclassified entities and outputs a set of questions that can be asked about an entity to determine its proper classification. <p> We 15 What we are doing is similar to decision tree learning <ref> [91] </ref>, but with a decision tree sparse data problems abound, since (in a binary decision tree) every level deeper in the tree has only half the training material (on the average) of the next level up.
Reference: [92] <author> J. Quinlan and R. Rivest. </author> <title> Inferring decision trees using the minimum description length principle. </title> <journal> Information and Computation, </journal> <volume> 80, </volume> <year> 1989. </year>
Reference-contexts: In addition, transformations are learned in the transformation-based learner, whereas the rules of GPS are prespecified. The technique employed by the learner is also similar to that used in decision trees <ref> [13, 91, 92] </ref>. A decision tree is trained on a set of preclassified entities and outputs a set of questions that can be asked about an entity to determine its proper classification.
Reference: [93] <author> L. Rabiner. </author> <title> A tutorial on Hidden Markov models and selected applications in speech recognition. In Readings in speech recognition, </title> <note> 1990. </note> <author> A Waibel and K. Lee, </author> <title> Editors. </title>
Reference-contexts: Contextual probabilities are computed as P (T i jT i1 ) or P (T i jT i1 T i2 ), depending upon the size of the context window being used. Once the 1 For a good introduction to Markov models, see <ref> [93] </ref>. 25 system is trained, new text can be tagged by assigning the string of tags which maximizes P (W jT ) fl P (T i jT i1 ) for a sentence. This optimal tagging can easily be computed using dynamic programming [109].
Reference: [94] <author> P. </author> <title> Resnik. Semantic classes and syntactic ambiguity. </title> <booktitle> In ARPA Workshop on Human Language Technology, </booktitle> <year> 1993. </year>
Reference-contexts: The transformation set is modified so that instead of asking if a noun is X, it can ask if X is a member of the noun's class set. 22 In <ref> [94] </ref>, a method is proposed for using Wordnet in conjunction with a corpus to obtain class-based statistics. Our method here is much simpler, in that we are only using boolean values to indicate the classes a word can be a member of. <p> Next, the training set was expanded to include the entire Wall Street Journal corpus (including unambiguous attachments but excluding the test set). Accuracy improved to 75.8% using the larger training set, still significantly lower than accuracy obtained using the transformation-based approach. 24 Using the technique described in <ref> [94] </ref> to attach prepositional phrases based on semantic similarity estimated using Wordnet, an accuracy of 72% was obtained using the same training and test sets. Using the semantic approach in conjunction with the method described by Hindle and Rooth [94] (backing off from the Hindle/Rooth method to the semantic based method) <p> obtained using the transformation-based approach. 24 Using the technique described in <ref> [94] </ref> to attach prepositional phrases based on semantic similarity estimated using Wordnet, an accuracy of 72% was obtained using the same training and test sets. Using the semantic approach in conjunction with the method described by Hindle and Rooth [94] (backing off from the Hindle/Rooth method to the semantic based method) resulted in an accuracy of 76.0%, still lower than the results obtained using transformations. Since the t-score approach did not make reference to n2, we reran the transformation-learner disallowing all transformations that make reference to n2.
Reference: [95] <author> R. Rivest. </author> <title> Learning decision lists. </title> <journal> Machine Learning, </journal> <volume> 2, </volume> <year> 1987. </year>
Reference-contexts: For example, in the application of part of speech tagging, the decision-tree tagger described in [7] outputs a tree with 30,000 to 40,000 leaves, whereas the transformation-based learner outputs a list of fewer than 200 transformations. A decision list <ref> [95] </ref> is similar to a decision tree, except that it is restricted to being binary-branching and right-linear.
Reference: [96] <author> R. Robins. </author> <title> Noun and verb in universal grammar. </title> <booktitle> Language, </booktitle> <volume> 28(3), </volume> <year> 1953. </year>
Reference-contexts: There is strong evidence that the set of possible classes which can be distinguished in a language is unbounded. Sapir thought that only the classes of noun and verb were fundamental to language. He wrote ([101], quoted in <ref> [96] </ref>): "No language wholly fails to distinguish noun and verb, though in particular cases the nature of the distinction may be an elusive one. It is different with the other parts of speech.
Reference: [97] <author> A. Rosenfeld, H. Huang, and V. Schneider. </author> <title> An application of cluster detection to text and picture processing. </title> <journal> IEEE Transactions on Information Theory, </journal> <volume> 15(6), </volume> <year> 1969. </year>
Reference-contexts: In [60], words are classed according to their distribution in subject-verb-object relationships. For such a method to succeed, one would need to be able to accurately parse the text being analyzed prior to word classification. The classification method we described above requires no structural information. <ref> [97] </ref> and [70] both attempt to classify words based upon their immediate neighbors. They use a similar definition of environment as used in our system, but use different measures of similarity. [97] ran a small-scale experiment, running the learning procedure over 105 basic English sentences from a simple introductory English language <p> The classification method we described above requires no structural information. <ref> [97] </ref> and [70] both attempt to classify words based upon their immediate neighbors. They use a similar definition of environment as used in our system, but use different measures of similarity. [97] ran a small-scale experiment, running the learning procedure over 105 basic English sentences from a simple introductory English language text containing a total of 131 different words. The method proposed by Kiss is not fully automatic. He manually chooses the set of words that will be clustered. <p> The method proposed by Kiss is not fully automatic. He manually chooses the set of words that will be clustered. The experiments of <ref> [97] </ref> and [70] left open the question of whether these techniques could succeed on free text. [29] describes another method of classifying words based upon distributional similarity of words in adjacent-word environments.
Reference: [98] <author> A. Rouvret and J. Vergnaud. </author> <title> Specifying reference to the subject. </title> <journal> Linguistic Inquiry, </journal> <volume> 11(1), </volume> <year> 1980. </year>
Reference-contexts: First, a list of verbs is extracted from the corpus. This is done using a simple automaton that assumes a word is a verb "if it is adjacent to a pronoun or proper name that would otherwise lack case." The verb finding algorithm is based upon the Case Filter <ref> [98] </ref>, which states that for a noun phrase to get case in English, it must occur in one of a small number of possible positions in a sentence: immediately to the left of a tensed verb, to the right of a main verb, or to the right of a preposition.
Reference: [99] <author> G. Sampson. </author> <title> Schools of Linguistics. </title> <publisher> Stanford University Press, </publisher> <year> 1980. </year>
Reference-contexts: Both research communities have the structural description of a language as one of the goals of their labor, although the motivations behind this goal are very different. Because of this relationship, we will now briefly examine some past work done in structural linguistics. According to Sampson <ref> [99] </ref>, Franz Boas is the father of linguistic structuralism. Boas was interested in determining the structure of a number of different languages [11, 62]. Providing an accurate description of each language was the primary goal of this work.
Reference: [100] <author> G. Sampson. </author> <title> A stochastic approach to parsing. </title> <booktitle> In Proceedings of COLING 1986, </booktitle> <address> Bonn, </address> <year> 1986. </year>
Reference-contexts: In [106], using mutual information (called interword predictability by Stolz) to discover phrases is suggested, with the crucial insight being that local minima in interword predictability correlate well with phrase boundaries. In [79], this idea is elaborated upon and tested on a large corpus. In <ref> [49, 100] </ref>, simulated annealing is used to parse a sentence. First, a scoring function 29 is defined that can take any tree structure as input and score the quality of that tree. <p> using the minimal resource tagger described in the previous section, or any of the many other available taggers if sufficient training material is available. 2 This work has also been reported in [17, 18, 19]. 97 the left and strings on the right is likely to be a phrase boundary. <ref> [100] </ref> defines a function to score the quality of parse trees and a move set, and then uses simulated annealing to heuristically explore the entire space of possible parses for a given sentence. In [23], distributional analysis techniques are applied to a large corpus to learn a context-free grammar.
Reference: [101] <author> E. </author> <title> Sapir. </title> <booktitle> Language. </booktitle> <address> New York, </address> <year> 1921. </year>
Reference: [102] <author> Y. Schabes, M. Roth, and R. Osborne. </author> <title> Parsing the Wall Street Journal with the inside-outside algorithm. </title> <booktitle> In Proceedings of the 1993 European ACL, </booktitle> <address> Uterich, The Netherlands, </address> <year> 1993. </year>
Reference-contexts: Taylor and Kroch [107] use tagged and bracketed corpora of Middle English and Old English for studying diachronic linguistic phenomena. In computational linguistics, many researchers have been using annotated corpora to train stochastic part of speech taggers and parsers (e.g. <ref> [36, 102] </ref>). Structurally annotated corpora are being used as the gold standard by which different parsers can be objectively compared [5]. <p> The inside-outside algorithm [3] is a method for training stochastic context-free grammars. It is an extension of the Baum-Welch [4] algorithm for training stochastic finite state automata. 2 A number of recent papers have explored the potential of using the i-o algorithm to automatically learn a grammar <ref> [75, 104, 88, 26, 31, 102] </ref>. A probabilistic context free grammar begins with some initial, possibly random, probabilities. The inside-outside algorithm is an estimation maximization algorithm which iteratively changes the rule probabilities to increase the probability of the training corpus. <p> The inside-outside algorithm is an estimation maximization algorithm which iteratively changes the rule probabilities to increase the probability of the training corpus. The algorithm is guaranteed to find a locally optimal assignment of rule probabilities, but not a globally optimal assignment. In <ref> [88, 102] </ref>, it is shown that the inside-outside algorithm can be used to bracket text with high accuracy, with very weak initial knowledge of the grammar. <p> A number of recent papers have explored the potential of using the inside-outside algorithm to automatically learn a grammar <ref> [75, 104, 88, 26, 31, 102] </ref>. In the inside-outside algorithm, context-free rule probabilities are incrementally altered in a way that increases the probability of the training corpus. <p> The inside-outside algorithm can be used to assign probabilities to a symbolic grammar written by a grammarian, or to learn a grammar automatically. In <ref> [88, 102] </ref>, an initial grammar consisting of all possible binary 98 rules (for a particular set of preterminals and nonterminals) is built, and each rule is as-signed a random probability. <p> In <ref> [102] </ref>, it is shown that a grammar produced this way is ineffective, in fact performing worse than assigning right linear structure to input sentences. 5 This is the same output given by systems described in [79, 16, 88, 102]. 6 Note that we do not necessarily have to begin in a <p> In [102], it is shown that a grammar produced this way is ineffective, in fact performing worse than assigning right linear structure to input sentences. 5 This is the same output given by systems described in <ref> [79, 16, 88, 102] </ref>. 6 Note that we do not necessarily have to begin in a naive start state. <p> A graph showing parsing performance for the WSJ run trained on a 500-sentence training corpus (training and testing on sentences of length 2-15) is shown in figure 7.7. In <ref> [102] </ref>, an experiment was done using the inside-outside algorithm to train a context-free grammar from the partially bracketed Wall Street Journal corpus. <p> numbers presented above allow us to compare the transformation learner with systems trained and tested on comparable corpora, these results are all based upon the assumption that the test data is tagged fairly reliably (manually tagged text was used in all of these experiments, as well in the experiments of <ref> [88, 102] </ref>.) When parsing free text, 116 we cannot assume that the text will be tagged with the accuracy of a human annotator. Instead, an automatic tagger would have to be used to first tag the text before parsing.
Reference: [103] <author> S. Seneff. Tina: </author> <title> A natural language system for spoken language applications. </title> <booktitle> Computational Linguistics, </booktitle> <year> 1992. </year>
Reference-contexts: Structural annotation allows for a more complete well-formedness check on a sentence. Checking well-formedness is useful in a number of applications. To give one example, in some speech recognition systems (e.g. <ref> [103] </ref>), the recognizer outputs a list of the n-best guesses that is then passed to a parser to filter out those sentences that are not well-formed. 7 The more accurately well-formedness can be assessed, the better these systems that rely on filtering out bad sentences from n-best lists can perform.
Reference: [104] <author> R. Sharman, F. Jelinek, and R. Mercer. </author> <title> Generating a grammar for statistical training. </title> <booktitle> In Proceedings of the 1990 Darpa Speech and Natural Language Workshop, </booktitle> <year> 1990. </year> <month> 153 </month>
Reference-contexts: The inside-outside algorithm [3] is a method for training stochastic context-free grammars. It is an extension of the Baum-Welch [4] algorithm for training stochastic finite state automata. 2 A number of recent papers have explored the potential of using the i-o algorithm to automatically learn a grammar <ref> [75, 104, 88, 26, 31, 102] </ref>. A probabilistic context free grammar begins with some initial, possibly random, probabilities. The inside-outside algorithm is an estimation maximization algorithm which iteratively changes the rule probabilities to increase the probability of the training corpus. <p> A number of recent papers have explored the potential of using the inside-outside algorithm to automatically learn a grammar <ref> [75, 104, 88, 26, 31, 102] </ref>. In the inside-outside algorithm, context-free rule probabilities are incrementally altered in a way that increases the probability of the training corpus.
Reference: [105] <author> H. Simon. </author> <title> On a class of skew distribution functions. </title> <journal> Biometrika, </journal> <volume> 42, </volume> <year> 1955. </year>
Reference-contexts: Subsequent to Zipf's claim of uncovering a universal property of human nature, a number of later publications demonstrated that Zipf's Law is a necessary consequent of assuming that the source of the language from which the frequency data is taken is a simple stochastic process <ref> [80, 105] </ref>. In the introduction to [115], George Miller elegantly puts it: Suppose that we acquired a dozen monkeys and chained them to typewriters until they had produced some very long and random sequence of characters.
Reference: [106] <author> W. Stolz. </author> <title> A probabilistic procedure for grouping words into phrases. Language and Speech, </title> <type> 8, </type> <year> 1965. </year>
Reference-contexts: A number of papers from the school of structural linguistics addressed this issue. In <ref> [106] </ref>, using mutual information (called interword predictability by Stolz) to discover phrases is suggested, with the crucial insight being that local minima in interword predictability correlate well with phrase boundaries. In [79], this idea is elaborated upon and tested on a large corpus. <p> In <ref> [106, 79] </ref>, a statistic based on mutual information is used to find phrase boundaries. The key idea used in these papers is that a position between two words with relatively low mutual information between strings on 1 Of course, the input sentences need not be manually tagged.
Reference: [107] <author> A. Taylor and T. Kroch. </author> <title> The Penn parsed corpus of Middle English: a syntactically annotated database. </title> <booktitle> Presented at the Georgetown University Roundtable on Languages and Linguistics Pre-session on Corpus-Based Linguistics, </booktitle> <year> 1993. </year>
Reference-contexts: Linguists are using structurally annotated corpora to study a number of linguistic phenomena. Hardt [51] uses tagged corpora for a study on VP ellipsis. Niv [87] uses a syntactically annotated corpus to develop a theory about how humans resolve syntactic ambiguity in parsing. Taylor and Kroch <ref> [107] </ref> use tagged and bracketed corpora of Middle English and Old English for studying diachronic linguistic phenomena. In computational linguistics, many researchers have been using annotated corpora to train stochastic part of speech taggers and parsers (e.g. [36, 102]).
Reference: [108] <author> R. Thomason, </author> <title> editor. Formal Philosophy: Selected Papers of Richard Montague. </title> <publisher> Yale University Press, </publisher> <year> 1974. </year>
Reference-contexts: the meaning of a sentence without referring to structural descriptions, Marcus states: "The purpose of the process of understanding human language is to determine the meanings of utterances, but syntactic structures appear to be a necessary stop along the way." Theories of compositional semantics such as that proposed by Montague <ref> [108] </ref> are based 6 upon the assumption that semantic rules are tied to syntactic rules.
Reference: [109] <author> A. </author> <title> Viterbi. Error bounds for convolutional codes and an asymptotically optimal decoding algorithm. </title> <journal> IEEE Trans. Inform. Theory, </journal> <volume> IT-13, </volume> <year> 1967. </year>
Reference-contexts: This optimal tagging can easily be computed using dynamic programming <ref> [109] </ref>. The second set of stochastic taggers does not require tagged text for training. The same underlying model is assumed, namely a Hidden Markov model, but in this case training is more difficult because the set of state transitions used to generate the training corpus is no longer visible.
Reference: [110] <author> L. Wall and R. Schwartz. </author> <title> Programming perl. </title> <publisher> O'Reilly and Associates, </publisher> <year> 1991. </year>
Reference: [111] <author> R. Wells. </author> <title> Immediate constituents. </title> <booktitle> Language, </booktitle> <volume> 23, </volume> <year> 1947. </year>
Reference-contexts: This method of finding phrase boundaries is similar to that proposed by Harris [54] for determining the morphemes of a language. A different approach to immediate constituent analysis has been suggested by Zellig Harris [52, 53] and Rulon Wells <ref> [111] </ref>. Since the work of Wells incorporates and expands upon the ideas of Harris, we will only discuss Wells' work here. <p> Parsing is then carried out using simulated annealing to move through the search space in hope of ending up with a high scoring tree. In [23], distributional analysis techniques similar to those described in <ref> [111] </ref> are used to automatically learn scored context-free rules. The score for the rule: noun ! determiner noun receives as its score the distributional similarity, based on words immediately to the left and right, of the single part of speech noun to the part of speech pair determiner noun. <p> A number of proposals came out of the American Structural linguistics school on how a field linguist could determine the phrase structure of sentences in an unfamiliar language <ref> [33, 61, 52, 53, 111] </ref>. We described these approaches in an earlier section. All of these approaches require a trained linguist working with an informant to tease out the structural information of a sentence.
Reference: [112] <author> B. Whorf. </author> <title> Language, </title> <booktitle> Thought and Reality: Selected Writings of Benjamin Lee Whorf. </booktitle> <publisher> MIT Press, </publisher> <address> Cambridge, </address> <year> 1956. </year>
Reference-contexts: Providing an accurate description of each language was the primary goal of this work. From this work, Boas thought, research could be done to determine the relationship between languages based upon their structural similarity. Also, Boas held a view similar to Whorf <ref> [112] </ref> that the structure of a language influences a person's behavior and therefore saw language study as being important because "the peculiar characteristics of languages are clearly reflected in the views and customs of the peoples of the world" ([62], page 69).
Reference: [113] <author> T. Wonnacott and R. Wonnacott. </author> <title> Introductory Statistics for Business and Economics. </title> <publisher> Wiley, </publisher> <year> 1984. </year>
Reference-contexts: Our system was tested by using the training set to learn a set of transformations, and then applying these transformations to the test set and scoring the resulting output. We then used the jacknife approach (see <ref> [113] </ref>) to estimate the variance of our result from a single run of learning and applying transformations. Doing so, we compute that the 95% confidence interval for this experiment is 91:1% 2:1%.
Reference: [114] <author> G. Zipf. </author> <title> Selected Studies of the Principle of Relative Frequency in Language. </title> <publisher> Harvard University Press, </publisher> <year> 1932. </year>
Reference-contexts: It is sufficient to note that empirically, Zipf's law seems to roughly hold for linguistic frequency data of many sorts across many different languages <ref> [114] </ref>.
Reference: [115] <author> G. Zipf. </author> <title> The Psycho-Biology of Language. </title> <publisher> Houghton Mi*in, </publisher> <year> 1935. </year>
Reference-contexts: Zipf observed that this law seemed to hold for frequency data from a number of disparate areas, including city populations and word frequencies in texts written in various languages. He attributed this phenomenon to what he called the Principle of Least Effort <ref> [115, 116] </ref>. Subsequent to Zipf's claim of uncovering a universal property of human nature, a number of later publications demonstrated that Zipf's Law is a necessary consequent of assuming that the source of the language from which the frequency data is taken is a simple stochastic process [80, 105]. <p> In the introduction to <ref> [115] </ref>, George Miller elegantly puts it: Suppose that we acquired a dozen monkeys and chained them to typewriters until they had produced some very long and random sequence of characters.
Reference: [116] <author> G. Zipf. </author> <title> Human behavior and the principle of least effort. Hafner, </title> <address> New York, </address> <year> 1949. </year> <month> 154 </month>
Reference-contexts: We will now turn to an examination of Zipf's law, an empirical observation that the rank-frequency plot of many different language-phenomena in many different languages, is highly skewed. 4.2 A Word on Zipf 's Law Zipf's law <ref> [116] </ref> is an empirical observation that in many different domains, the rank of an element divided by the frequency of occurrence of that element is constant. <p> Zipf observed that this law seemed to hold for frequency data from a number of disparate areas, including city populations and word frequencies in texts written in various languages. He attributed this phenomenon to what he called the Principle of Least Effort <ref> [115, 116] </ref>. Subsequent to Zipf's claim of uncovering a universal property of human nature, a number of later publications demonstrated that Zipf's Law is a necessary consequent of assuming that the source of the language from which the frequency data is taken is a simple stochastic process [80, 105].
References-found: 116

