URL: ftp://www.cs.rutgers.edu/pub/technical-reports/hpcd-tr-1.ps.Z
Refering-URL: http://www.cs.rutgers.edu/pub/technical-reports/
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Title: Efficient Algorithms and a Software Tool for Scheduling Parallel Computation parallelize a sequential program and
Author: Apostolos Gerasoulis, P. Chretienne, E. G. Coffman, J. K. Lenstra, Z. Liu (Eds.) 
Note: partition and  c fl1994 John Wiley Sons Ltd  
Address: 0.1 Introduction  
Affiliation: Rutgers University Tao Yang, University of California at Santa Barbara  
Abstract: In this chapter, we consider the partitioning and scheduling problem for directed acyclic program task graphs (DAG). We emphasize algorithms for scheduling parallel architectures based on the asynchronous message passing paradigm for communication. Such architectures are becoming increasingly popular but programming them is very difficult since both the data and the program must be partitioned and distributed to the processors. The following problems are of major importance for distributed memory architectures: (1) The program and data partitioning and the identification of parallelism. (2) The mapping of the data and program onto an architecture. (3) The scheduling and co-ordination of the task execution. From a theoretical point of view all problems above are extremely difficult in the sense that finding the optimum scheduling solution is NP-complete in general [Ch89a, CD73, LR78, PY90]. In practice, however, parallel programs are written routinely for distributed memory architectures with excellent performance. Thus one of the grand challenges in parallel processing is if a compiler can be built that will automatically 
Abstract-found: 1
Intro-found: 1
Reference: [ACD74] <author> T. Adam, K.M. Chandy, and J. R. Dickson, </author> <title> A Comparison of List Schedules for Parallel Processing Systems, </title> <journal> CACM, </journal> <volume> 17:12, </volume> <year> 1974, </year> <pages> pp. 685-690. </pages>
Reference-contexts: For example, it is optimum for tree DAGs with equal weights and for any arbitrary DAG with equal weights on 2 processors [CD73]. For arbitrary DAGs and p processors any list scheduling including CP is within 50% of the optimum. Moreover, the experimental results by Adam et. al. <ref> [ACD74] </ref> show that CP is near optimum in practice in the sense that it is within 5% of the optimum in 90% of randomly generated DAGs. Unfortunately, these nice properties do not carry over to the case of nonzero communication cost.
Reference: [Bok90] <author> S.H. Bokhari, </author> <title> Assignment Problems in Parallel and Distributed Computing, </title> <publisher> Kluwer Academic Publisher, </publisher> <year> 1990. </year>
Reference-contexts: In Fig. 0.16 (c) we show one physical mapping to a 4-node hypercube with F (CC) = 24 and another mapping is shown in (d) with F (CC) = 21. Currently we use a heuristic algorithm due to Bokhari <ref> [Bok90] </ref>. This algorithm starts from an initial assignment, then performs a series of pairwise interchanges so that the F (CC) reduces monotonically as shown in the example above. Task ordering: Once the physical mapping has been decided then a task ordering is needed to define the scheduling.
Reference: [CK88] <author> D. Callahan and K. Kennedy, </author> <title> Compiling Programs for Distributed-memory Multiprocessors, </title> <journal> Journal of Supercomputing, </journal> <volume> Vol. 2, </volume> <year> 1988, </year> <pages> pp. 151-169. </pages>
Reference-contexts: The above theorems provide an explanation of the advantages of linear clustering which has been widely used in the literature particularly for coarse grain dataflow graphs, e.g. [GH86, KB88, Kun88, Ort88, Saa86]. We present an example. Example. A widely used assumption for clustering is "the owner computes rule" <ref> [CK88] </ref>, i.e. a processor executes a computation unit if this unit modifies the data that the processor owns. This rule can perform well for certain regular problems but in general it could result in workload imbalances especially for unstructured problems.
Reference: [Ch89a] <author> P. Chretienne, </author> <title> Task Scheduling over Distributed Memory Machines, </title> <booktitle> Proc. of Inter. Workshop on Parallel and Distributed Algorithms, </booktitle> <publisher> North Holland, </publisher> <year> 1989. </year>
Reference-contexts: From a theoretical point of view all problems above are extremely difficult in the sense that finding the optimum scheduling solution is NP-complete in general <ref> [Ch89a, CD73, LR78, PY90] </ref>. In practice, however, parallel programs are written routinely for distributed memory architectures with excellent performance. <p> The Gantt chart completely describes the schedule since it defines both P A (n j ) and ST (n j ). The scheduling problem with communication delay has been shown to be NP-complete for a general task graph in most cases, Sarkar [Sar89], Chretienne <ref> [Ch89a] </ref> and Papadimitriou and Yannakakis [PY90]. 1 0 P Gantt chart n 1 n 3 2 n 5 1 2 3 4 5 6 7 Time (a) 2 2 2 n 6 n 3 n 2 n 4 2 n 1 n 8 n 7 n 5 (c) The Gantt chart <p> The clustering problem is identical to processor assignment part of scheduling. Sarkar [Sar89] calls it an internalization pre-pass. Clustering is also NP-complete for the minimization of the parallel time <ref> [Ch89a, Sar89] </ref>. A clustering is called nonlinear if two independent tasks are mapped in the same cluster, otherwise is called linear. <p> More details can be found in [YG91]. The NP-completeness of clustering for parallel time minimization has been shown by Sarkar [Sar89], Chretienne <ref> [Ch89a] </ref> and Papadimitriou and Yannakakis [PY90].
Reference: [Ch89b] <author> P. Chretienne, </author> <title> A Polynomial Algorithm to Optimally Schedule Tasks over an ideal Distributed System under Tree-like Precedence Constraints, </title> <journal> European Journal of Operational Research, </journal> <note> 2:43 (1989), pp225-230. </note>
Reference-contexts: More details can be found in [YG91]. The NP-completeness of clustering for parallel time minimization has been shown by Sarkar [Sar89], Chretienne [Ch89a] and Papadimitriou and Yannakakis [PY90]. Chre-tienne <ref> [Ch89b] </ref> shows that the problems of scheduling a join, fork DAG or coarse grain tree DAG are solvable in a polynomial time, but the complexity jumps to NP-complete for scheduling fine-grain tree DAGs and a DAG structure obtained by concatenating a fork and a join together.
Reference: [CD73] <author> E. G. Coffman and P. J. Denning, </author> <title> Operating Systems Theory, </title> <publisher> Prentice Hall, </publisher> <year> 1973. </year>
Reference-contexts: From a theoretical point of view all problems above are extremely difficult in the sense that finding the optimum scheduling solution is NP-complete in general <ref> [Ch89a, CD73, LR78, PY90] </ref>. In practice, however, parallel programs are written routinely for distributed memory architectures with excellent performance. <p> The CP list scheduling possesses many nice properties when communication cost is zero. For example, it is optimum for tree DAGs with equal weights and for any arbitrary DAG with equal weights on 2 processors <ref> [CD73] </ref>. For arbitrary DAGs and p processors any list scheduling including CP is within 50% of the optimum.
Reference: [Cos88] <author> M. Cosnard, M. Marrakchi, Y. Robert, and D. Trystram, </author> <title> Parallel Gaussian Elimination on an MIMD Computer, </title> <journal> Parallel Computing, </journal> <volume> vol. 6, </volume> <year> 1988, </year> <pages> pp. 275-296. </pages>
Reference: [DS87] <author> J. J. Dongarra and D. C. Sorensen, </author> <title> SCHEDULE: Tools for Developing and Analyzing Parallel Fortran Programs, in The Characteristics of Parallel Algorithms, D.B. </title> <editor> Gannon, L.H. Jamieson and R.J. Douglass (Eds), </editor> <publisher> MIT Press, </publisher> <year> 1987, </year> <month> pp363-394. </month>
Reference-contexts: A more detailed description of PYRROS is given in [YG92]. There are several other systems related to PYRROS. PARAFRASE-2 [Pol90] by Polychronopoulos et. al., is a parallelizing compiler system that performs dependence analysis, partitioning and dynamic scheduling on shared memory machines. SCHED-ULER by Dongarra and Sorensen <ref> [DS87] </ref> uses centralized dynamic scheduling for a shared memory machine. KALI by Koelbel and Mehrota [KM90] addresses code generation and is currently targeted at DOALL parallelism. Kennedy's group [HKT91] is also working on code generation for FORTRAN D for distributed-memory machines.
Reference: [Dun91] <author> T. H. Dunigan, </author> <title> Performance of the INTEL iPSC/860 and nCUBE 6400 Hypercube, </title> <institution> ORNL/TM-11790, Oak Ridge National Lab., TN, </institution> <year> 1991. </year>
Reference-contexts: This linear communication model is a good approximation to most currently available message passing architectures, see Dunigan <ref> [Dun91] </ref>. For the nCUBE-II hypercube we have ff = 160s and fi = 2:4s per word transfer for single precision arithmetic.
Reference: [EL90] <author> H. El-Rewini and T. G. Lewis, </author> <title> Scheduling Parallel Program Tasks onto Arbitrary Target Machines, </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> Vol. 9, </volume> <year> 1990, </year> <pages> pp. 138-153. </pages>
Reference-contexts: We assume that tasks are convex, which means that once a task starts its execution it can run to completion without interrupting for communications, Sarkar [Sar89]. * The static macro-dataflow model of execution, Sarkar [Sar89], Wu and Gajski [WG88], El-Rewini and Lewis <ref> [EL90] </ref>. This is similar to the dataflow model. The data flow through the graph and a task waits to receive all data in parallel before it starts its execution. <p> PARTI by Saltz's group [Sal90] focuses on run-time DOALL parallelism with irregular distribution of data and optimizes performance by precomputing data accessing patterns. HYPERTOOL by Wu and Gajski [WG88] and TASKGRAPHER by El-Rewini and Lewis <ref> [EL90] </ref> use the same task model as PYRROS. The time complexity of these two systems is over O (v 2 ). 0.5.1 Task graph language The PYRROS system uses a simple language for defining task graphs.
Reference: [GH86] <author> Geist, G.A. and Heath,M.T., </author> <title> Matrix Factorization on a Hypercube Multiprocessor, Hypercube Multiprocessors, </title> <publisher> SIAM, </publisher> <year> 1986, </year> <pages> pp. </pages> <month> 161-180. </month> <title> 9/1/1994 14:42|PAGE PROOFS for John Wiley & Sons Ltd (using jwcbmc01, Vers 01.02 OCT 1993)|newbchapter Scheduling for Parallel Computation 29 </title>
Reference-contexts: The above theorems provide an explanation of the advantages of linear clustering which has been widely used in the literature particularly for coarse grain dataflow graphs, e.g. <ref> [GH86, KB88, Kun88, Ort88, Saa86] </ref>. We present an example. Example. A widely used assumption for clustering is "the owner computes rule" [CK88], i.e. a processor executes a computation unit if this unit modifies the data that the processor owns. <p> This rule can perform well for certain regular problems but in general it could result in workload imbalances especially for unstructured problems. The "owner compute rule" has been used to cluster both the U-DAG and the T-DAG in Fig. 0.4, see Saad [Saa86], Geist and Heath <ref> [GH86] </ref> and Ortega [Ort88]. <p> PYRROS uses a variation of work profiling method suggested by George et. al. [Geo86] for cluster merging. This method is simple and has been shown to work well in practice, e.g. Saad [Saa86], Geist and Heath <ref> [GH86] </ref>, Ortega [Ort88], Gerasoulis and Nelken [GN89]. The complexity of this algorithm is O (u log u + v), which is less than O (v log v). 1. Compute the arithmetic load LM j for each cluster. 2. Sort the clusters in an increasing order of their loads. 3. <p> We have that LM j = i=1 j 2 : These clusters can be approxmiately load balanced by using the wrap or reflection mapping, V P (j) = (j 2) mod p, Geist and Heath <ref> [GH86] </ref>. For the example in Fig. 0.14 (a) with 3 clusters and 2 processors, the result of merging is two clusters shown in Fig. 0.14 (b). Physical mapping: We now have p virtual processors (or clusters) and p physical processors.
Reference: [GN89] <author> A. Gerasoulis and I. Nelken, </author> <title> Static Scheduling for Linear Algebra DAGs, </title> <booktitle> Proc. of HCCA 4, </booktitle> <year> 1989, </year> <pages> pp. 671-674. </pages>
Reference-contexts: PYRROS uses a variation of work profiling method suggested by George et. al. [Geo86] for cluster merging. This method is simple and has been shown to work well in practice, e.g. Saad [Saa86], Geist and Heath [GH86], Ortega [Ort88], Gerasoulis and Nelken <ref> [GN89] </ref>. The complexity of this algorithm is O (u log u + v), which is less than O (v log v). 1. Compute the arithmetic load LM j for each cluster. 2. Sort the clusters in an increasing order of their loads. 3.
Reference: [GY93] <author> A. Gerasoulis and T. Yang, </author> <title> On the Granularity and Clustering of Directed Acyclic Task Graphs, </title> <journal> IEEE Trans. on Parallel and Distributed Systems, </journal> <volume> Vol. 4, No. 6, </volume> <month> June </month> <year> 1993, </year> <pages> pp. 686-701. </pages>
Reference-contexts: An interesting question arises: can this analysis be generalized to arbitrary DAGs. In Gerasoulis and Yang <ref> [GY93] </ref> we have introduced a new notion of granularity using a ratio of the computation to communication costs taken over all fork and joins subgraphs of a task graph. <p> Continuing we finally determine the granularity as the minimum grain over all nodes of the graph which in our case is g = 1=5. In <ref> [GY93] </ref> we prove the following theorems: Theorem 1 For a coarse grain task graph, there exists a linear clustering that mini mizes the parallel time. 9/1/1994 14:42|PAGE PROOFS for John Wiley & Sons Ltd (using jwcbmc01, Vers 01.02 OCT 1993)|newbchapter Scheduling for Parallel Computation 11 The above theorem is true only <p> We demonstrate the basic idea of the proof by using the example in Fig. 0.8. We show in <ref> [GY93] </ref> that for any nonlinear clustering we can extract a linear clustering whose parallel time is less than or equal to the nonlinear clustering. If we assume that w c in Fig. 0.8, then the parallel time of the nonlinear clustering in Fig. 0.8 (b) is 3w.
Reference: [GY92] <author> A. Gerasoulis and T. Yang, </author> <title> A Comparison of Clustering Heuristics for Scheduling DAGs on Multiprocessors', </title> <journal> Journal of Parallel and Distributed Computing, special issue on scheduling and load balancing, </journal> <volume> Vol. 16, No. 4, </volume> <pages> pp. </pages> <month> 276-291 (Dec. </month> <year> 1992). </year>
Reference-contexts: The DSC clustering algorithm: Sarkar's clustering algorithm has a complexity of O (e (v + e)). Furthermore, zeroing the highest communication edges is not the best approach since this edge might not belong in the DS and as a result the parallel time cannot be reduced. In <ref> [YG91, GY92] </ref> we have proposed a new clustering algorithm called the DSC algorithm which has been shown to outperform other algorithms from the literature, both in terms of complexity and parallel time. The DSC algorithm is based on the following heuristic: * The parallel time is determined by the DS.
Reference: [Geo86] <author> A. George, M.T. Heath, and J. Liu, </author> <title> Parallel Cholesky Factorization on a Shared Memory Processor, </title> <journal> Lin. Algebra Appl., </journal> <volume> Vol. 77, </volume> <year> 1986, </year> <pages> pp. 165-187. </pages>
Reference-contexts: Cluster merging: The cost of the Sarkar cluster merging and scheduling algorithm is O (pv (v + e)) which is time-consuming for a large graph. PYRROS uses a variation of work profiling method suggested by George et. al. <ref> [Geo86] </ref> for cluster merging. This method is simple and has been shown to work well in practice, e.g. Saad [Saa86], Geist and Heath [GH86], Ortega [Ort88], Gerasoulis and Nelken [GN89].
Reference: [GP88] <author> M. Girkar and C. </author> <title> Polychronopoulos Partitioning Programs for Parallel Execution, </title> <booktitle> Proc. of ACM Inter. Conf. on Supercomputing, </booktitle> <address> St. Malo, France, </address> <year> 1988. </year>
Reference: [HVV92] <author> J.A. Hoogeveen, S.L. Van de Velde, and B. Veltman, </author> <title> Complexity of scheduling multiprocessor tasks with prespecified processor allocations, </title> <publisher> CWI, </publisher> <address> Report BS-R9211 June 1992, Netherlands. </address>
Reference-contexts: In Fig. 0.17 (b) we show one ordering with P T = 12 and in (c) another ordering in which the parallel time increases to P T = 15. Finding a task ordering that minimizes the parallel time is NP-hard <ref> [HVV92] </ref>. We have proposed a modification to the CP heuristic for the ordering problem in Yang and Gerasoulis [YG95]. <p> The case is similar in the third processor for scheduling n 3 and n 6 . The resulting schedule is shown in Fig. 0.17 (b) and its parallel time is P T = 12. The task ordering problem is NP-hard even for chains of tasks <ref> [HVV92] </ref>, however, in [YG95] we prove that fork and join DAGs are tractable.
Reference: [HR88] <author> M.T. Heath and C. H. Romine, </author> <title> Parallel Solution of Triangular Systems on Distributed Memory Multiprocessors, </title> <journal> SIAM J. Sci. Statist. Comput., </journal> <volume> Vol. 9, </volume> <year> 1988, </year> <pages> pp. 558-588. </pages>
Reference-contexts: It is therefore the communication and computation costs derived by a partitioning that will determine the "useful parallelism " which minimizes the parallel time. This has been recognized in the literature as it can be seen by the following quote from Heath and Romine <ref> [HR88] </ref> p. 559: " Another important characteristic determining the overall efficiency of parallel algorithms is the relative cost of communication and computation.
Reference: [HKT91] <author> S. Hiranandani, K. Kennedy, and C.W. Tseng, </author> <title> Compiler Optimizations for Fortran D on MIMD Distributed-Memory Machines, </title> <booktitle> Proc. of Supercomputing '91, IEEE, </booktitle> <pages> pp. 86-100. </pages>
Reference-contexts: SCHED-ULER by Dongarra and Sorensen [DS87] uses centralized dynamic scheduling for a shared memory machine. KALI by Koelbel and Mehrota [KM90] addresses code generation and is currently targeted at DOALL parallelism. Kennedy's group <ref> [HKT91] </ref> is also working on code generation for FORTRAN D for distributed-memory machines. PARTI by Saltz's group [Sal90] focuses on run-time DOALL parallelism with irregular distribution of data and optimizes performance by precomputing data accessing patterns.
Reference: [KM90] <author> C. Koelbel and P. Mehrotra, </author> <title> Supporting Shared Data Structures on Distributed Memory Architectures, </title> <booktitle> Proc. of ACM SIGPLAN Sympos. on Principles and Practice of Parallel Programming, </booktitle> <year> 1990, </year> <pages> pp. 177-186. </pages>
Reference-contexts: PARAFRASE-2 [Pol90] by Polychronopoulos et. al., is a parallelizing compiler system that performs dependence analysis, partitioning and dynamic scheduling on shared memory machines. SCHED-ULER by Dongarra and Sorensen [DS87] uses centralized dynamic scheduling for a shared memory machine. KALI by Koelbel and Mehrota <ref> [KM90] </ref> addresses code generation and is currently targeted at DOALL parallelism. Kennedy's group [HKT91] is also working on code generation for FORTRAN D for distributed-memory machines. PARTI by Saltz's group [Sal90] focuses on run-time DOALL parallelism with irregular distribution of data and optimizes performance by precomputing data accessing patterns.
Reference: [LR78] <author> J.K. Lenstra and A.H.G. Rinnooy Kan, </author> <title> Complexity of Scheduling under Precedence Constraints, Operation Research, </title> <month> 26:1 </month> <year> (1978). </year>
Reference-contexts: From a theoretical point of view all problems above are extremely difficult in the sense that finding the optimum scheduling solution is NP-complete in general <ref> [Ch89a, CD73, LR78, PY90] </ref>. In practice, however, parallel programs are written routinely for distributed memory architectures with excellent performance.
Reference: [KB88] <author> S.J. Kim and J.C. Browne, </author> <title> A General Approach to Mapping of Parallel Computation upon Multiprocessor Architectures, </title> <booktitle> Proc. of Inter. Conf. on Parallel Processing, </booktitle> <volume> Vol. 3, </volume> <year> 1988, </year> <pages> pp. 1-8. </pages>
Reference-contexts: The above theorems provide an explanation of the advantages of linear clustering which has been widely used in the literature particularly for coarse grain dataflow graphs, e.g. <ref> [GH86, KB88, Kun88, Ort88, Saa86] </ref>. We present an example. Example. A widely used assumption for clustering is "the owner computes rule" [CK88], i.e. a processor executes a computation unit if this unit modifies the data that the processor owns. <p> communication cost between tasks will become zero if they are allocated in the same processor. (b) CP, PT=2w+c. (a) n 1 w n n 1 2 0 (c) MCP, PT=2w+c. n 3 n P P 1 It has been argued in the literature by Sarkar [Sar89] and Kim and Browne <ref> [KB88] </ref> that a better approach to scheduling when communication is present is to perform scheduling in more than one steps.
Reference: [Kun88] <author> S.Y. Kung, </author> <title> VLSI Array Processors, </title> <publisher> Prentice Hall, </publisher> <year> 1988. </year>
Reference-contexts: The above theorems provide an explanation of the advantages of linear clustering which has been widely used in the literature particularly for coarse grain dataflow graphs, e.g. <ref> [GH86, KB88, Kun88, Ort88, Saa86] </ref>. We present an example. Example. A widely used assumption for clustering is "the owner computes rule" [CK88], i.e. a processor executes a computation unit if this unit modifies the data that the processor owns.
Reference: [Mol86] <author> C.Moler, </author> <title> Matrix Computation on Distributed Memory Multiprocessors, Hypercube Multiprocessors 1986, </title> <publisher> SIAM, </publisher> <pages> pp. 181-195. </pages>
Reference-contexts: The block size is the dimension of a submatrix. n=450 n=450 n=1000 block size=5 block size=10 block size=10 p=2 1.97 1.9 1.99 p=8 7.3 6.9 7.8 p=32 19.0 12.9 25.7 with cyclic wrap mapping along the gray code of a hypercube following the algorithm of Moler <ref> [Mol86] </ref> and Saad [Saa86]. Tasks that modify the same column block are mapped in the same processor. The broadcasting uses a function provided by the nCUBE-II library.
Reference: [Ort88] <author> J. M. Ortega, </author> <title> Introduction to Parallel and Vector Solution of Linear Systems, </title> <publisher> Plenum (New York), </publisher> <year> 1988. </year>
Reference-contexts: The above theorems provide an explanation of the advantages of linear clustering which has been widely used in the literature particularly for coarse grain dataflow graphs, e.g. <ref> [GH86, KB88, Kun88, Ort88, Saa86] </ref>. We present an example. Example. A widely used assumption for clustering is "the owner computes rule" [CK88], i.e. a processor executes a computation unit if this unit modifies the data that the processor owns. <p> This rule can perform well for certain regular problems but in general it could result in workload imbalances especially for unstructured problems. The "owner compute rule" has been used to cluster both the U-DAG and the T-DAG in Fig. 0.4, see Saad [Saa86], Geist and Heath [GH86] and Ortega <ref> [Ort88] </ref>. <p> PYRROS uses a variation of work profiling method suggested by George et. al. [Geo86] for cluster merging. This method is simple and has been shown to work well in practice, e.g. Saad [Saa86], Geist and Heath [GH86], Ortega <ref> [Ort88] </ref>, Gerasoulis and Nelken [GN89]. The complexity of this algorithm is O (u log u + v), which is less than O (v log v). 1. Compute the arithmetic load LM j for each cluster. 2. Sort the clusters in an increasing order of their loads. 3.
Reference: [PY90] <author> C. Papadimitriou and M. Yannakakis, </author> <title> Towards on an Architecture-Independent Analysis of Parallel Algorithms, </title> <journal> SIAM J. Comput., </journal> <volume> Vol. 19, </volume> <year> 1990, </year> <pages> pp. 322-328. </pages>
Reference-contexts: From a theoretical point of view all problems above are extremely difficult in the sense that finding the optimum scheduling solution is NP-complete in general <ref> [Ch89a, CD73, LR78, PY90] </ref>. In practice, however, parallel programs are written routinely for distributed memory architectures with excellent performance. <p> The Gantt chart completely describes the schedule since it defines both P A (n j ) and ST (n j ). The scheduling problem with communication delay has been shown to be NP-complete for a general task graph in most cases, Sarkar [Sar89], Chretienne [Ch89a] and Papadimitriou and Yannakakis <ref> [PY90] </ref>. 1 0 P Gantt chart n 1 n 3 2 n 5 1 2 3 4 5 6 7 Time (a) 2 2 2 n 6 n 3 n 2 n 4 2 n 1 n 8 n 7 n 5 (c) The Gantt chart of a schedule. <p> More details can be found in [YG91]. The NP-completeness of clustering for parallel time minimization has been shown by Sarkar [Sar89], Chretienne [Ch89a] and Papadimitriou and Yannakakis <ref> [PY90] </ref>. Chre-tienne [Ch89b] shows that the problems of scheduling a join, fork DAG or coarse grain tree DAG are solvable in a polynomial time, but the complexity jumps to NP-complete for scheduling fine-grain tree DAGs and a DAG structure obtained by concatenating a fork and a join together.
Reference: [Pic92] <author> C. Picouleau, </author> <title> New complexity results on the UET-UCT scheduling algorithms, </title> <booktitle> Proc. of Summer School on Scheduling Theory and its Applications, </booktitle> <address> Chateau De Bonas, France, </address> <year> 1992, </year> <pages> pp. 487-502. </pages>
Reference-contexts: We can always perform this extraction as long as the task graph is coarse grain. Theorem 1 shows that the problem of finding an optimal solution for a coarse grain DAG is equivalent to that of finding an optimal linear clustering. Picouleau <ref> [Pic92] </ref> has shown that the scheduling problem for coarse grain DAGs is NP-complete, therefore optimal linear clustering is NP-complete. Theorem 2 Determining the optimum linear clustering is NP-complete.
Reference: [Pol90] <author> C. Polychronopoulos, M. Girkar, M. Haghighat, C. Lee, B. Leung, and D. Schouten, </author> <title> The Structure of Parafrase-2: an Advanced Parallelizing Compiler for C and Fortran, in Languages and Compilers for Parallel Computing, </title> <editor> D. Gelernter, A. Nicolau and D. Padua (Eds.), </editor> <year> 1990. </year>
Reference-contexts: A more detailed description of PYRROS is given in [YG92]. There are several other systems related to PYRROS. PARAFRASE-2 <ref> [Pol90] </ref> by Polychronopoulos et. al., is a parallelizing compiler system that performs dependence analysis, partitioning and dynamic scheduling on shared memory machines. SCHED-ULER by Dongarra and Sorensen [DS87] uses centralized dynamic scheduling for a shared memory machine.
Reference: [Sal90] <author> J. Saltz, K. Crowley, R. Mirchandaney, and H. Berryman, </author> <title> Run-Time Scheduling and Execution of Loops on Message Passing Machines, </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> Vol. 8, </volume> <year> 1990, </year> <pages> pp. 303-312. </pages>
Reference-contexts: For such cases run-time scheduling techniques are useful, e.g. <ref> [Sal90] </ref>. 0.3 Granularity and the Impact of Partitioning on Scheduling 0.3.1 Scheduling and clustering definitions Scheduling is defined by a processor assignment mapping, P A (n j ), of the tasks onto the p processors and by a starting times mapping, ST (n j ), of all nodes onto the real <p> KALI by Koelbel and Mehrota [KM90] addresses code generation and is currently targeted at DOALL parallelism. Kennedy's group [HKT91] is also working on code generation for FORTRAN D for distributed-memory machines. PARTI by Saltz's group <ref> [Sal90] </ref> focuses on run-time DOALL parallelism with irregular distribution of data and optimizes performance by precomputing data accessing patterns. HYPERTOOL by Wu and Gajski [WG88] and TASKGRAPHER by El-Rewini and Lewis [EL90] use the same task model as PYRROS.
Reference: [Saa86] <author> Y. Saad, </author> <title> Gaussian Elimination on Hypercubes, in Parallel Algorithms and Architectures, </title> <editor> Cosnard, M. et al. (Eds.), </editor> <publisher> Elsevier Science Publishers, North-Holland, </publisher> <year> 1986. </year>
Reference-contexts: The above theorems provide an explanation of the advantages of linear clustering which has been widely used in the literature particularly for coarse grain dataflow graphs, e.g. <ref> [GH86, KB88, Kun88, Ort88, Saa86] </ref>. We present an example. Example. A widely used assumption for clustering is "the owner computes rule" [CK88], i.e. a processor executes a computation unit if this unit modifies the data that the processor owns. <p> This rule can perform well for certain regular problems but in general it could result in workload imbalances especially for unstructured problems. The "owner compute rule" has been used to cluster both the U-DAG and the T-DAG in Fig. 0.4, see Saad <ref> [Saa86] </ref>, Geist and Heath [GH86] and Ortega [Ort88]. <p> PYRROS uses a variation of work profiling method suggested by George et. al. [Geo86] for cluster merging. This method is simple and has been shown to work well in practice, e.g. Saad <ref> [Saa86] </ref>, Geist and Heath [GH86], Ortega [Ort88], Gerasoulis and Nelken [GN89]. The complexity of this algorithm is O (u log u + v), which is less than O (v log v). 1. Compute the arithmetic load LM j for each cluster. 2. <p> The block size is the dimension of a submatrix. n=450 n=450 n=1000 block size=5 block size=10 block size=10 p=2 1.97 1.9 1.99 p=8 7.3 6.9 7.8 p=32 19.0 12.9 25.7 with cyclic wrap mapping along the gray code of a hypercube following the algorithm of Moler [Mol86] and Saad <ref> [Saa86] </ref>. Tasks that modify the same column block are mapped in the same processor. The broadcasting uses a function provided by the nCUBE-II library.
Reference: [Sar89] <author> V. Sarkar, </author> <title> Partitioning and Scheduling Parallel Programs for Execution on Multiprocessors, </title> <publisher> MIT Press, </publisher> <year> 1989. </year>
Reference-contexts: We assume that tasks are convex, which means that once a task starts its execution it can run to completion without interrupting for communications, Sarkar <ref> [Sar89] </ref>. * The static macro-dataflow model of execution, Sarkar [Sar89], Wu and Gajski [WG88], El-Rewini and Lewis [EL90]. This is similar to the dataflow model. The data flow through the graph and a task waits to receive all data in parallel before it starts its execution. <p> We assume that tasks are convex, which means that once a task starts its execution it can run to completion without interrupting for communications, Sarkar <ref> [Sar89] </ref>. * The static macro-dataflow model of execution, Sarkar [Sar89], Wu and Gajski [WG88], El-Rewini and Lewis [EL90]. This is similar to the dataflow model. The data flow through the graph and a task waits to receive all data in parallel before it starts its execution. <p> One widely used cost function is the minimization of the parallel time. Unfortunately, for this cost function the partitioning problem is NP-complete in most cases, <ref> [Sar89] </ref>. However, instead of searching for the optimum partitioning, we can search for a partitioning that has sufficient parallelism for the given architecture and also satisfies additional constraints. The additional constraints must be chosen so that the search space is reduced. <p> The additional constraints must be chosen so that the search space is reduced. An example of such a constraint is to search for tasks of a given maximum size that have no cycles. This is known as the convexity constraint in the literature, <ref> [Sar89] </ref>. <p> On the other hand, T-DAG task partitioning in Fig. 0.4 is consistent with column data partitioning since each task T j k only accesses 2 columns (k and j) for each update. 0.2.5 Computing the weights for the DAG. Sarkar <ref> [Sar89] </ref> on page 139 has proposed a methodology for the estimation of the communication and computation cost for the macro dataflow task model. The computation cost is the time E for a task to execute on a processor. The communication cost consists of two components: 1. <p> The Gantt chart completely describes the schedule since it defines both P A (n j ) and ST (n j ). The scheduling problem with communication delay has been shown to be NP-complete for a general task graph in most cases, Sarkar <ref> [Sar89] </ref>, Chretienne [Ch89a] and Papadimitriou and Yannakakis [PY90]. 1 0 P Gantt chart n 1 n 3 2 n 5 1 2 3 4 5 6 7 Time (a) 2 2 2 n 6 n 3 n 2 n 4 2 n 1 n 8 n 7 n 5 (c) The <p> Clusters are not tasks, since tasks that belong to a cluster are permitted to communicate with the tasks of other clusters immediately after completion of their execution. The clustering problem is identical to processor assignment part of scheduling. Sarkar <ref> [Sar89] </ref> calls it an internalization pre-pass. Clustering is also NP-complete for the minimization of the parallel time [Ch89a, Sar89]. A clustering is called nonlinear if two independent tasks are mapped in the same cluster, otherwise is called linear. <p> The clustering problem is identical to processor assignment part of scheduling. Sarkar [Sar89] calls it an internalization pre-pass. Clustering is also NP-complete for the minimization of the parallel time <ref> [Ch89a, Sar89] </ref>. A clustering is called nonlinear if two independent tasks are mapped in the same cluster, otherwise is called linear. <p> Processor P 0 has tasks n 1 and n 2 with starting times ST (n 1 ) = 0 and ST (n 2 ) = 1. If we modify the clustered DAG as in <ref> [Sar89] </ref> by adding a zero-weighted pseudo edge between any pair of nodes n x and n y in a cluster, if n y is executed immediately after n x and there is no data dependence edge between n x and n y , then we obtain what we call a scheduled <p> information is non-deterministic because the communication cost between tasks will become zero if they are allocated in the same processor. (b) CP, PT=2w+c. (a) n 1 w n n 1 2 0 (c) MCP, PT=2w+c. n 3 n P P 1 It has been argued in the literature by Sarkar <ref> [Sar89] </ref> and Kim and Browne [KB88] that a better approach to scheduling when communication is present is to perform scheduling in more than one steps. We discuss this approach next. 0.4.2 Multistep scheduling methods 0.4.2.1 Sarkar's approach Sarkar's heuristic [Sar89] is based on the assumption that a scheduling pre-pass is needed <p> P 1 It has been argued in the literature by Sarkar <ref> [Sar89] </ref> and Kim and Browne [KB88] that a better approach to scheduling when communication is present is to perform scheduling in more than one steps. We discuss this approach next. 0.4.2 Multistep scheduling methods 0.4.2.1 Sarkar's approach Sarkar's heuristic [Sar89] is based on the assumption that a scheduling pre-pass is needed to cluster tasks with high communication between them. Then the clusters are scheduled on p available processors. <p> Determine a clustering of the task graph by using scheduling on an unbounded number of processors and a clique architecture. 2. Schedule the clusters on the given architecture with a bounded number of proces sors. Sarkar <ref> [Sar89] </ref> uses the following heuristics for the two steps above: 1. Zero the edge with the highest communication cost. If the parallel time does not increase then accept this zeroing. Continue with the next highest edge until all edges have been visited. 2. <p> If the parallel time was not computed incrementally, then the total cost would be greater than O (v 2 ) which will not be practical for large graphs. More details can be found in [YG91]. The NP-completeness of clustering for parallel time minimization has been shown by Sarkar <ref> [Sar89] </ref>, Chretienne [Ch89a] and Papadimitriou and Yannakakis [PY90].
Reference: [Sto87] <author> H. Stone, </author> <title> High-Performance Computer Architectures, </title> <publisher> Addison-Wesley, </publisher> <year> 1987. </year>
Reference-contexts: If all task weights are equal to R and all edge weights are equal to C then the granularity reduces to R=C which is the same as Stone's <ref> [Sto87] </ref>. For coarse grain DAGs each task receives or sends data with a small amount of communication cost compared to the computation cost.
Reference: [VRK93] <author> T. Varvarigou, V. Roychowdhury, and T. Kailath, </author> <title> Scheduling In and out Forests in the Presence of communication delays, </title> <note> to appear in IEEE Trans. on Parallel and Distr. Systems., Short version appeared in Proc. of International Parallel Processing Symp., </note> <month> April </month> <year> 1993, </year> <title> CA. </title> <publisher> 9/1/1994 14:42|PAGE PROOFS for John Wiley & Sons Ltd (using jwcbmc01, </publisher> <address> Vers 01.02 OCT 1993)|newbchapter 30 Gerasoulis and Yang </address>
Reference: [WG88] <author> M.Y. Wu and D. Gajski, </author> <title> A Programming Aid for Hypercube Architectures, </title> <journal> Journal of Supercomputing, </journal> <volume> Vol. 2, </volume> <year> 1988, </year> <pages> pp. 349-372. </pages>
Reference-contexts: We assume that tasks are convex, which means that once a task starts its execution it can run to completion without interrupting for communications, Sarkar [Sar89]. * The static macro-dataflow model of execution, Sarkar [Sar89], Wu and Gajski <ref> [WG88] </ref>, El-Rewini and Lewis [EL90]. This is similar to the dataflow model. The data flow through the graph and a task waits to receive all data in parallel before it starts its execution. <p> One is the classical list scheduling and another is the Modified Critical Path (MCP) heuristic proposed by Wu and Gajski <ref> [WG88] </ref>. 0.4.1.1 The classical list scheduling heuristic The classical list scheduling schedules free 1 tasks by scanning a priority list from left to right. More specifically the following steps are performed: 1. Determine a priority list. 2. <p> We now present a modification to the CP heuristic. 9/1/1994 14:42|PAGE PROOFS for John Wiley & Sons Ltd (using jwcbmc01, Vers 01.02 OCT 1993)|newbchapter Scheduling for Parallel Computation 15 0.4.1.2 The modified critical path (MCP) heuristic Wu and Gajski <ref> [WG88] </ref> have proposed a modification to the CP heuristic. Instead of scheduling a free task in an available processor, the free task is scheduled in the available processor that allows the task to start its execution at the earliest possible time. <p> Kennedy's group [HKT91] is also working on code generation for FORTRAN D for distributed-memory machines. PARTI by Saltz's group [Sal90] focuses on run-time DOALL parallelism with irregular distribution of data and optimizes performance by precomputing data accessing patterns. HYPERTOOL by Wu and Gajski <ref> [WG88] </ref> and TASKGRAPHER by El-Rewini and Lewis [EL90] use the same task model as PYRROS. The time complexity of these two systems is over O (v 2 ). 0.5.1 Task graph language The PYRROS system uses a simple language for defining task graphs.
Reference: [YG91] <author> T. Yang and A. Gerasoulis, </author> <title> A Fast Static Scheduling Algorithm for DAGs on an Unbounded Number of Processors, </title> <booktitle> Proc. of Supercomputing '91, IEEE, </booktitle> <pages> pp. 633-642. </pages> <note> A longer version will appear in IEEE Trans. on Parallel and Distributed Systems. </note>
Reference-contexts: The DSC clustering algorithm: Sarkar's clustering algorithm has a complexity of O (e (v + e)). Furthermore, zeroing the highest communication edges is not the best approach since this edge might not belong in the DS and as a result the parallel time cannot be reduced. In <ref> [YG91, GY92] </ref> we have proposed a new clustering algorithm called the DSC algorithm which has been shown to outperform other algorithms from the literature, both in terms of complexity and parallel time. The DSC algorithm is based on the following heuristic: * The parallel time is determined by the DS. <p> Thus the total complexity is O ((v + e) log v). If the parallel time was not computed incrementally, then the total cost would be greater than O (v 2 ) which will not be practical for large graphs. More details can be found in <ref> [YG91] </ref>. The NP-completeness of clustering for parallel time minimization has been shown by Sarkar [Sar89], Chretienne [Ch89a] and Papadimitriou and Yannakakis [PY90]. <p> Chre-tienne [Ch89b] shows that the problems of scheduling a join, fork DAG or coarse grain tree DAG are solvable in a polynomial time, but the complexity jumps to NP-complete for scheduling fine-grain tree DAGs and a DAG structure obtained by concatenating a fork and a join together. In <ref> [YG91] </ref>, we show that DSC performs well for general DAGs by examining a set of randomly generated DAGs but also produces the following optimal solutions. Theorem 5 DSC is optimal for fork, join and coarse grain tree DAGs.
Reference: [YG95] <author> T. Yang and A. Gerasoulis, </author> <title> List Scheduling with and without Communication Delay, </title> <note> To appear in Parallel Computing, </note> <year> 1995. </year>
Reference-contexts: Finding a task ordering that minimizes the parallel time is NP-hard [HVV92]. We have proposed a modification to the CP heuristic for the ordering problem in Yang and Gerasoulis <ref> [YG95] </ref>. <p> The case is similar in the third processor for scheduling n 3 and n 6 . The resulting schedule is shown in Fig. 0.17 (b) and its parallel time is P T = 12. The task ordering problem is NP-hard even for chains of tasks [HVV92], however, in <ref> [YG95] </ref> we prove that fork and join DAGs are tractable.
Reference: [YG92] <author> T. Yang and A. Gerasoulis, </author> <title> PYRROS: Static Task Scheduling and Code Generation for Message-Passing Multiprocessors, </title> <booktitle> Proc. of 6th ACM Inter. Confer. on Supercomputing, </booktitle> <address> Washington D.C., </address> <year> 1992, </year> <pages> pp. 428-437. </pages> <publisher> 9/1/1994 14:42|PAGE PROOFS for John Wiley & Sons Ltd (using jwcbmc01, </publisher> <address> Vers 01.02 OCT 1993)|newbchapter </address>
Reference-contexts: We emphasize static scheduling over dynamic, because it is still an open problem how to reduce the run-time overhead of dynamic scheduling for distributed memory architectures. We have addressed the issues of static scheduling and developed algorithms along with a software system named PYRROS <ref> [YG92] </ref>. PYRROS takes as an input a task graph and produces schedules for message passing architectures such as nCUBE-II. The current PYRROS prototype has a low complexity and can handle task graphs with millions of tasks. An automatic system for scheduling and code generation is useful in many ways. <p> Thus n 3 remains in P 0 . Finally we have a schedule shown in Fig. 0.14 (c). 0.4.2.2 PYRROS's multistep scheduling algorithms The PYRROS tool <ref> [YG92] </ref> uses a multistep approach to scheduling: 1. Perform clustering using the Dominant Sequence Algorithm (DSC). 2. Merge the u clusters into p completely connected virtual processors if u &gt; p. 3. Map the p virtual processors into p physical processors. 4. Order the execution of tasks in each processor. <p> A more detailed description of PYRROS is given in <ref> [YG92] </ref>. There are several other systems related to PYRROS. PARAFRASE-2 [Pol90] by Polychronopoulos et. al., is a parallelizing compiler system that performs dependence analysis, partitioning and dynamic scheduling on shared memory machines. SCHED-ULER by Dongarra and Sorensen [DS87] uses centralized dynamic scheduling for a shared memory machine.
References-found: 37

