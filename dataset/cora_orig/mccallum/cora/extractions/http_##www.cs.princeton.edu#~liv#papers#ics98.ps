URL: http://www.cs.princeton.edu/~liv/papers/ics98.ps
Refering-URL: http://www.cs.princeton.edu/~liv/papers/papers.html
Root-URL: http://www.cs.princeton.edu
Email: fcliao, djg@cs.princeton.edu iftode@cs.rutgers.edu mrm@ee.princeton.edu doug@cs.princeton.edu  
Title: Monitoring Shared Virtual Memory Performance on a Myrinet-based PC Cluster  
Author: Cheng Liao, Dongming Jiang, Liviu Iftode Margaret Martonosi, and Douglas W. Clark 
Address: Princeton, NJ 08544 Piscataway, NJ 08855  
Affiliation: Princeton University Rutgers University  
Abstract: Network-connected clusters of PCs or workstations are becoming a widespread parallel computing platform. Performance methodologies that use either simulation or high-level software instrumentation cannot adequately measure the detailed behavior of such systems. The availability of new network technologies based on programmable network interfaces opens a new avenue of research in analyzing and improving the performance of software shared memory protocols. We have developed monitoring firmware embedded in the programmable network interfaces of a Myrinet-based PC cluster. Timestamps on network packets facilitate the collection of low-level statistics on, e.g., network latencies, interrupt handler times and inter-node synchronization. This paper describes our use of the low-level software performance monitor to measure and understand the performance of a Shared Virtual Memory (SVM) system implemented on a Myrinet-based cluster, running the SPLASH-2 benchmarks. We measured time spent in various communication stages during the main protocol operations: remote page fetch, remote lock synchronization, and barriers. These data show that remote request contention in the network interface and hosts can serialize their handling and artificially increase the page miss time. This increase then dilates the critical section within which it occurs, increasing lock contention and causing lock serialization. Furthermore, lock serialization is reflected in the waiting time at barriers. These results of our study sharpen and deepen similar but higher-level speculations in previous simulation-based SVM performance research. Moreover, the insights about different layers, including communication architecture, SVM protocol, and applications, on real systems provide guidelines for better designs in those layers. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> R. Bianchini, L. Kontothanassis, R. Pinto, et al. </author> <title> Hiding communication latency and coherence overhead in software DSMs. </title> <booktitle> In The 7th Intl. Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <month> Oct. </month> <year> 1996. </year>
Reference-contexts: Our HLRC implementation uses the VMMC-2 library [9] and in this sense is similar to the HLRC implementation on SHRIMP [11] which also uses a memory-mapped communication library. SVM performance has been studied, both in simulations [3, 13, 14, 17] and on real systems <ref> [1, 15, 16] </ref>. But our work is the first, to our knowledge, in incorporating the SVM system and a performance debugging tool embedded into Myrinet firmware for performance study on a real system.
Reference: [2] <author> A. Bilas, L. Iftode, R. Samanta, and J. P. Singh. </author> <title> Supporting a coherent shared address space across SMP nodes: An application-driven investigation. </title> <note> IMA Volumes in Mathematics and its Applications, </note> <year> 1998. </year>
Reference-contexts: This increase then dilates the critical section within which it occurs, increasing lock contention and thus causing lock serialization. Furthermore, lock serialization is also reflected in the waiting time at barriers. Although some of these effects have been speculated from higher-level statistics and detailed simulations <ref> [2, 3, 12, 13, 14, 15, 17, 24] </ref>, the initial interrupt delay and the exact succession of this cascade of effects could not have been traced exactly without the performance monitor on real systems.
Reference: [3] <author> A. Bilas and J. P. Singh. </author> <title> The effects of communication parameters on end performance of shared virtual memory clusters. </title> <booktitle> In In Proc. of Supercomputing 97, </booktitle> <address> San Jose, CA, </address> <month> Nov. </month> <year> 1997. </year>
Reference-contexts: Prototype evaluations in these studies indicate that the communication-related costs plus the software overhead are responsible for limiting the performance of the software shared memory approach. Simulations have been the principal vehicle for much previous research on SVM performance <ref> [3, 13, 14, 17] </ref>. However, the simu lation approach has several limitations which constrain one's trust in its results. Namely, fast simulators usually make simplifying assumptions about certain costs and behaviors, such as protocol costs or bus and network contention. <p> This increase then dilates the critical section within which it occurs, increasing lock contention and thus causing lock serialization. Furthermore, lock serialization is also reflected in the waiting time at barriers. Although some of these effects have been speculated from higher-level statistics and detailed simulations <ref> [2, 3, 12, 13, 14, 15, 17, 24] </ref>, the initial interrupt delay and the exact succession of this cascade of effects could not have been traced exactly without the performance monitor on real systems. <p> Since the network-layer performance characteristics are crucial to the design and utilization of SVM systems, they were studied through simulation in previous research <ref> [3, 13, 14] </ref>. Most current SVM simulators are inadequate in modeling contention effects on buses, network links, and at the network interfaces. They are also slow. <p> The HLRC implementation on Typhoon-zero used active messages for communication support. Our HLRC implementation uses the VMMC-2 library [9] and in this sense is similar to the HLRC implementation on SHRIMP [11] which also uses a memory-mapped communication library. SVM performance has been studied, both in simulations <ref> [3, 13, 14, 17] </ref> and on real systems [1, 15, 16]. But our work is the first, to our knowledge, in incorporating the SVM system and a performance debugging tool embedded into Myrinet firmware for performance study on a real system.
Reference: [4] <author> M. Blumrich, K. Li, R. Alpert, C. Dubnicki, E. Felten, and J. Sand-berg. </author> <title> A virtual memory mapped network interface for the SHRIMP multicomputer. </title> <booktitle> In Proc. of the 21st Annual Symposium on Computer Architecture, </booktitle> <month> Apr. </month> <year> 1994. </year>
Reference-contexts: Based on high-level timings of the SVM system, we preview its performance. We also show that some crucial performance characteristics are hard to understand using high-level profiling alone. 2.1 Myrinet-based PC Cluster The Myrinet-based PC cluster implements the Virtual Memory-Mapped Communication model (VMMC) <ref> [4] </ref> on a Myrinet network of PCI-based PCs. Myrinet is a highspeed local/system area network for computer systems [6]. A Myrinet network is composed of point-to-point links that connect hosts and switches.
Reference: [5] <author> M. A. Blumrich, R. D. Alpert, Y. Chen, et al. </author> <title> Design choices in the SHRIMP system: An empirical study. </title> <booktitle> In Proc. of the 25th Annual Symposium on Computer Architecture, </booktitle> <month> June </month> <year> 1998. </year>
Reference-contexts: Our SVM system implements an HLRC protocol on a Myrinet-based PC cluster. The same HLRC protocol was previously implemented on Intel-Paragon [24], Wisconsin Typhoon-Zero [10] and SHRIMP <ref> [5] </ref> (a PC-based cluster interconnected with a cus-tom designed network interface). The HLRC implementation on Typhoon-zero used active messages for communication support. Our HLRC implementation uses the VMMC-2 library [9] and in this sense is similar to the HLRC implementation on SHRIMP [11] which also uses a memory-mapped communication library.
Reference: [6] <author> N. J. Boden, D. Cohen, R. E. Felderman, et al. Myrinet: </author> <title> A gigabit-per-second local area network. </title> <journal> IEEE Micro, </journal> <volume> 15(1):2936, </volume> <month> Feb. </month> <year> 1995. </year>
Reference-contexts: The alternative to simulation is to collect detailed information by monitoring the protocol execution in a real system. In this paper we describe a software performance monitor which we have used to understand the performance of a software shared memory prototype implemented on a Myrinet-based <ref> [6] </ref> PC cluster. We have embedded additional monitoring and global clock synchronization code into the communication firmware running on the Myrinet network interface card. This allows us to gather low-level statistics on, e.g., network latencies, interrupt handler times and inter-node synchronization. <p> We also show that some crucial performance characteristics are hard to understand using high-level profiling alone. 2.1 Myrinet-based PC Cluster The Myrinet-based PC cluster implements the Virtual Memory-Mapped Communication model (VMMC) [4] on a Myrinet network of PCI-based PCs. Myrinet is a highspeed local/system area network for computer systems <ref> [6] </ref>. A Myrinet network is composed of point-to-point links that connect hosts and switches. The Myrinet-based PC cluster used in this study consists of 8 PCI PCs connected to a Myrinet switch via Myrinet PCI network interfaces.
Reference: [7] <author> F. Christian. </author> <title> Probabilistic Clock Synchronization. </title> <journal> Distributed Computing, </journal> <volume> vol 3:146158, </volume> <year> 1989. </year>
Reference-contexts: the data can be post-processed to be presented in a number of ways, including 3D plots of latency statistics versus packet size, sending node, etc. 3.1 Global Clock Synchronization To keep time consistent across the loosely coupled nodes, the monitor employs a global clock synchronization algorithm based on Christian's algorithm <ref> [7] </ref>. In our current system, Node 0 is always responsible for collecting and distributing global clock information. Periodically, it contacts other nodes querying them for their current time; when the answer is returned, Node 0 computes the time difference between the pair.
Reference: [8] <author> D. Culler et al. </author> <title> Assessing fast network interfaces. </title> <booktitle> IEEE Micro, </booktitle> <month> Feb. </month> <year> 1996. </year>
Reference-contexts: Also, our work is different from other performance evaluations of real (not simulated) Myrinet-based systems, which have relied on microbenchmarks and higher-level system measurements <ref> [8, 21, 22] </ref>. This limits their ability to make lower-level determinations of how different components of communication impact performance. 7 Conclusions This paper has presented an empirical discussion of an HLRC-based SVM system and a software performance monitor residing in low-level Myrinet firmware.
Reference: [9] <author> C. Dubnicki, A. Bilas, Y. Chen, S. Damianakis, and K. Li. VMMC-2: </author> <title> Efficient support for reliable, connection-oriented communication. </title> <booktitle> In Proceedings of Hot Interconnects, </booktitle> <month> Aug. </month> <year> 1997. </year>
Reference-contexts: Finally, Section 6 discusses related work and Section 7 offers our conclusions. 2 Implementing SVM On A Myrinet-based PC Cluster In this section, we describe the SVM prototype we implemented on a Myrinet-based PC cluster <ref> [9] </ref> built at Princeton. Based on high-level timings of the SVM system, we preview its performance. <p> The same HLRC protocol was previously implemented on Intel-Paragon [24], Wisconsin Typhoon-Zero [10] and SHRIMP [5] (a PC-based cluster interconnected with a cus-tom designed network interface). The HLRC implementation on Typhoon-zero used active messages for communication support. Our HLRC implementation uses the VMMC-2 library <ref> [9] </ref> and in this sense is similar to the HLRC implementation on SHRIMP [11] which also uses a memory-mapped communication library. SVM performance has been studied, both in simulations [3, 13, 14, 17] and on real systems [1, 15, 16].
Reference: [10] <author> M. D. Hill, Y. Zhou, I. Schoinas, et al. </author> <title> Relaxed consistency and coherence granularity in DSM systems: A performance evaluation. </title> <type> Technical Report TR-535-96, </type> <institution> Department of Computer Science, Princeton University, </institution> <month> Dec. </month> <year> 1996. </year>
Reference-contexts: Our SVM system implements an HLRC protocol on a Myrinet-based PC cluster. The same HLRC protocol was previously implemented on Intel-Paragon [24], Wisconsin Typhoon-Zero <ref> [10] </ref> and SHRIMP [5] (a PC-based cluster interconnected with a cus-tom designed network interface). The HLRC implementation on Typhoon-zero used active messages for communication support.
Reference: [11] <author> L. Iftode, M. Blumrich, C. Dubnicki, et al. </author> <title> Shared Virtual Memory with Automatic Update Support. </title> <type> Technical Report TR-575-98, </type> <institution> Department of Computer Science, Princeton University, </institution> <year> 1998. </year>
Reference-contexts: The HLRC implementation on Typhoon-zero used active messages for communication support. Our HLRC implementation uses the VMMC-2 library [9] and in this sense is similar to the HLRC implementation on SHRIMP <ref> [11] </ref> which also uses a memory-mapped communication library. SVM performance has been studied, both in simulations [3, 13, 14, 17] and on real systems [1, 15, 16].
Reference: [12] <author> L. Iftode, C. Dubnicki, E. Felten, and K. Li. </author> <title> Improving release-consistent shared virtual memory using automatic update. </title> <booktitle> In The 2nd Symposium on High-Performance Computer Architecture, </booktitle> <month> Feb. </month> <year> 1996. </year>
Reference-contexts: To reduce the hardware cost of these systems, cache coherence hardware is often foregone in favor of software layers implementing Shared Virtual Memory (SVM) [18] as illustrated in Figure 1. Although SVM implementations have been the focus of several past performance studies <ref> [12, 15, 17, 24] </ref>, data-gathering approaches have varied widely. At the programming and protocol layer, run-time tools and software instrumentation have been used to detect high-level contributors to execution time: e.g., counts of synchronizations, page faults, and messages. <p> This increase then dilates the critical section within which it occurs, increasing lock contention and thus causing lock serialization. Furthermore, lock serialization is also reflected in the waiting time at barriers. Although some of these effects have been speculated from higher-level statistics and detailed simulations <ref> [2, 3, 12, 13, 14, 15, 17, 24] </ref>, the initial interrupt delay and the exact succession of this cascade of effects could not have been traced exactly without the performance monitor on real systems.
Reference: [13] <author> L. Iftode, J. P. Singh, and K. Li. </author> <title> Understanding the performance of shared virtual memory from an applications perspective. </title> <booktitle> In Proc. of the 23rd Annual Symposium on Computer Architecture, </booktitle> <month> May </month> <year> 1996. </year>
Reference-contexts: Prototype evaluations in these studies indicate that the communication-related costs plus the software overhead are responsible for limiting the performance of the software shared memory approach. Simulations have been the principal vehicle for much previous research on SVM performance <ref> [3, 13, 14, 17] </ref>. However, the simu lation approach has several limitations which constrain one's trust in its results. Namely, fast simulators usually make simplifying assumptions about certain costs and behaviors, such as protocol costs or bus and network contention. <p> This increase then dilates the critical section within which it occurs, increasing lock contention and thus causing lock serialization. Furthermore, lock serialization is also reflected in the waiting time at barriers. Although some of these effects have been speculated from higher-level statistics and detailed simulations <ref> [2, 3, 12, 13, 14, 15, 17, 24] </ref>, the initial interrupt delay and the exact succession of this cascade of effects could not have been traced exactly without the performance monitor on real systems. <p> Since the network-layer performance characteristics are crucial to the design and utilization of SVM systems, they were studied through simulation in previous research <ref> [3, 13, 14] </ref>. Most current SVM simulators are inadequate in modeling contention effects on buses, network links, and at the network interfaces. They are also slow. <p> The HLRC implementation on Typhoon-zero used active messages for communication support. Our HLRC implementation uses the VMMC-2 library [9] and in this sense is similar to the HLRC implementation on SHRIMP [11] which also uses a memory-mapped communication library. SVM performance has been studied, both in simulations <ref> [3, 13, 14, 17] </ref> and on real systems [1, 15, 16]. But our work is the first, to our knowledge, in incorporating the SVM system and a performance debugging tool embedded into Myrinet firmware for performance study on a real system.
Reference: [14] <author> D. Jiang, H. Shan, and J. P. Singh. </author> <title> Application restructuring and performance portability on shared virtual memory and hardware-coherent multiprocessors. </title> <booktitle> In 6th ACM Symposium on Principles and Practice of Parallel Programming, </booktitle> <month> June </month> <year> 1997. </year>
Reference-contexts: Prototype evaluations in these studies indicate that the communication-related costs plus the software overhead are responsible for limiting the performance of the software shared memory approach. Simulations have been the principal vehicle for much previous research on SVM performance <ref> [3, 13, 14, 17] </ref>. However, the simu lation approach has several limitations which constrain one's trust in its results. Namely, fast simulators usually make simplifying assumptions about certain costs and behaviors, such as protocol costs or bus and network contention. <p> This increase then dilates the critical section within which it occurs, increasing lock contention and thus causing lock serialization. Furthermore, lock serialization is also reflected in the waiting time at barriers. Although some of these effects have been speculated from higher-level statistics and detailed simulations <ref> [2, 3, 12, 13, 14, 15, 17, 24] </ref>, the initial interrupt delay and the exact succession of this cascade of effects could not have been traced exactly without the performance monitor on real systems. <p> Since the network-layer performance characteristics are crucial to the design and utilization of SVM systems, they were studied through simulation in previous research <ref> [3, 13, 14] </ref>. Most current SVM simulators are inadequate in modeling contention effects on buses, network links, and at the network interfaces. They are also slow. <p> One solution is to choose homes of locks dynamically rather than assigning them statically. Another solution is to return the lock to its home each time the lock is released (cheap with a smart network interface on Myrinet). 5.3 Application Layer A previous simulation study <ref> [14] </ref> shows that understanding how an application behaves and restructuring it properly can dramatically improve performance on SVM systems. This however is not always easy, especially on real systems. <p> The HLRC implementation on Typhoon-zero used active messages for communication support. Our HLRC implementation uses the VMMC-2 library [9] and in this sense is similar to the HLRC implementation on SHRIMP [11] which also uses a memory-mapped communication library. SVM performance has been studied, both in simulations <ref> [3, 13, 14, 17] </ref> and on real systems [1, 15, 16]. But our work is the first, to our knowledge, in incorporating the SVM system and a performance debugging tool embedded into Myrinet firmware for performance study on a real system.
Reference: [15] <author> P. Keleher, A. Cox, S. Dwarkadas, and W. Zwaenepoel. Treadmarks: </author> <title> Distributed shared memory on standard workstations and operating systems. </title> <booktitle> In Proc. of the Winter USENIX Conference, </booktitle> <month> Jan. </month> <year> 1994. </year>
Reference-contexts: To reduce the hardware cost of these systems, cache coherence hardware is often foregone in favor of software layers implementing Shared Virtual Memory (SVM) [18] as illustrated in Figure 1. Although SVM implementations have been the focus of several past performance studies <ref> [12, 15, 17, 24] </ref>, data-gathering approaches have varied widely. At the programming and protocol layer, run-time tools and software instrumentation have been used to detect high-level contributors to execution time: e.g., counts of synchronizations, page faults, and messages. <p> This increase then dilates the critical section within which it occurs, increasing lock contention and thus causing lock serialization. Furthermore, lock serialization is also reflected in the waiting time at barriers. Although some of these effects have been speculated from higher-level statistics and detailed simulations <ref> [2, 3, 12, 13, 14, 15, 17, 24] </ref>, the initial interrupt delay and the exact succession of this cascade of effects could not have been traced exactly without the performance monitor on real systems. <p> Our HLRC implementation uses the VMMC-2 library [9] and in this sense is similar to the HLRC implementation on SHRIMP [11] which also uses a memory-mapped communication library. SVM performance has been studied, both in simulations [3, 13, 14, 17] and on real systems <ref> [1, 15, 16] </ref>. But our work is the first, to our knowledge, in incorporating the SVM system and a performance debugging tool embedded into Myrinet firmware for performance study on a real system.
Reference: [16] <editor> L. Kontothanassis, G. Hunt, R. Stets, et al. </editor> <booktitle> VM-based shared memory on low-latency, remote-memory-access networks. In Proc. of the 24th Annual Symposium on Computer Architecture, </booktitle> <month> June </month> <year> 1997. </year>
Reference-contexts: Our HLRC implementation uses the VMMC-2 library [9] and in this sense is similar to the HLRC implementation on SHRIMP [11] which also uses a memory-mapped communication library. SVM performance has been studied, both in simulations [3, 13, 14, 17] and on real systems <ref> [1, 15, 16] </ref>. But our work is the first, to our knowledge, in incorporating the SVM system and a performance debugging tool embedded into Myrinet firmware for performance study on a real system.
Reference: [17] <author> L. Kontothanassis and M. Scott. </author> <title> Using memory-mapped network interfaces to improve the performance of distributed shared memory. </title> <booktitle> In The 2nd Symposium on High-Performance Computer Architecture, </booktitle> <month> Feb. </month> <year> 1996. </year>
Reference-contexts: To reduce the hardware cost of these systems, cache coherence hardware is often foregone in favor of software layers implementing Shared Virtual Memory (SVM) [18] as illustrated in Figure 1. Although SVM implementations have been the focus of several past performance studies <ref> [12, 15, 17, 24] </ref>, data-gathering approaches have varied widely. At the programming and protocol layer, run-time tools and software instrumentation have been used to detect high-level contributors to execution time: e.g., counts of synchronizations, page faults, and messages. <p> Prototype evaluations in these studies indicate that the communication-related costs plus the software overhead are responsible for limiting the performance of the software shared memory approach. Simulations have been the principal vehicle for much previous research on SVM performance <ref> [3, 13, 14, 17] </ref>. However, the simu lation approach has several limitations which constrain one's trust in its results. Namely, fast simulators usually make simplifying assumptions about certain costs and behaviors, such as protocol costs or bus and network contention. <p> This increase then dilates the critical section within which it occurs, increasing lock contention and thus causing lock serialization. Furthermore, lock serialization is also reflected in the waiting time at barriers. Although some of these effects have been speculated from higher-level statistics and detailed simulations <ref> [2, 3, 12, 13, 14, 15, 17, 24] </ref>, the initial interrupt delay and the exact succession of this cascade of effects could not have been traced exactly without the performance monitor on real systems. <p> The HLRC implementation on Typhoon-zero used active messages for communication support. Our HLRC implementation uses the VMMC-2 library [9] and in this sense is similar to the HLRC implementation on SHRIMP [11] which also uses a memory-mapped communication library. SVM performance has been studied, both in simulations <ref> [3, 13, 14, 17] </ref> and on real systems [1, 15, 16]. But our work is the first, to our knowledge, in incorporating the SVM system and a performance debugging tool embedded into Myrinet firmware for performance study on a real system.
Reference: [18] <author> K. Li and P. Hudak. </author> <title> Memory coherence in shared virtual memory systems. </title> <booktitle> In Proc. of the 5th Annual ACM Symposium on Principles of Distributed Computing, </booktitle> <pages> pages 229239, </pages> <month> Aug. </month> <year> 1986. </year>
Reference-contexts: 1 Introduction Network-connected clusters of PCs or workstations are becoming a widespread parallel computing platform. To reduce the hardware cost of these systems, cache coherence hardware is often foregone in favor of software layers implementing Shared Virtual Memory (SVM) <ref> [18] </ref> as illustrated in Figure 1. Although SVM implementations have been the focus of several past performance studies [12, 15, 17, 24], data-gathering approaches have varied widely.
Reference: [19] <author> C. Liao, M. Martonosi, and D. W. Clark. </author> <title> Performance monitoring in a Myrinet-connected SHRIMP cluster. </title> <booktitle> In Proc. of 2nd SIGMETRICS Symposium on Parallel and Distributed Tools, </booktitle> <month> Aug. </month> <year> 1998. </year> <note> To appear. </note>
Reference-contexts: Network-level monitoring also allows us to build monitoring infrastructure that applies to a range of different higher-level programming models. We have used our strategy to performance debug several message passing applications in the past <ref> [19] </ref>; this paper concentrates on its use for SVM applications. Copies of the monitoring system run on each node's network interface. The monitor software adds time-stamps and sequencing information to messages outgoing from a particular node, and also processes this extra information as packets arrive at a node.
Reference: [20] <author> M. Martonosi, D. Ofelt, and M. Heinrich. </author> <title> Integrating Performance Monitoring and Communication in Parallel Computers. </title> <booktitle> In Proc. ACM SIGMETRICS Conf. on Meas. and Modeling of Computer Systems, </booktitle> <month> May </month> <year> 1996. </year>
Reference-contexts: In particular, by successively deeper detections, the monitor helps the programmers spot the sources of the performance bottlenecks. 6 Related Work This research builds on prior work on performance tools and SVM systems. Our performance monitoring approach is similar to techniques from the FlashPoint system <ref> [20] </ref>. That performance monitoring system embedded extra monitoring code into the cache coherence protocol handlers running on the FLASH multiprocessor's MAGIC chip.
Reference: [21] <author> S. Pakin et al. </author> <title> Fast messages: efficient, portable communication for workstation clusters and MPPs. </title> <journal> IEEE Concurrency, </journal> <month> Apr. </month> <year> 1997. </year>
Reference-contexts: Also, our work is different from other performance evaluations of real (not simulated) Myrinet-based systems, which have relied on microbenchmarks and higher-level system measurements <ref> [8, 21, 22] </ref>. This limits their ability to make lower-level determinations of how different components of communication impact performance. 7 Conclusions This paper has presented an empirical discussion of an HLRC-based SVM system and a software performance monitor residing in low-level Myrinet firmware.
Reference: [22] <author> T. von Eicken, D. Culler, et al. </author> <title> Active messages: a mechanism for integrated communication and computation. </title> <booktitle> In Proc. 19th Annual Intl. Symposium on Computer Architecture, </booktitle> <month> May </month> <year> 1992. </year>
Reference-contexts: Also, our work is different from other performance evaluations of real (not simulated) Myrinet-based systems, which have relied on microbenchmarks and higher-level system measurements <ref> [8, 21, 22] </ref>. This limits their ability to make lower-level determinations of how different components of communication impact performance. 7 Conclusions This paper has presented an empirical discussion of an HLRC-based SVM system and a software performance monitor residing in low-level Myrinet firmware.
Reference: [23] <author> S. Woo, M. Ohara, E. Torrie, J. Singh, and A. Gupta. </author> <title> Methodological considerations and characterization of the SPLASH-2 parallel application suite. </title> <booktitle> In Proc. of the 23rd Annual Symposium on Computer Architecture, </booktitle> <month> May </month> <year> 1996. </year>
Reference-contexts: Although we initially designed this monitor to analyze and tune message passing programs, this paper demonstrates the flexibility of the approach by describing our experiments using it to understand and tune a collection of SVM applications from the SPLASH-2 <ref> [23] </ref> suite. The performance monitor interacts with the SVM protocol via a simple and efficient interface. By interfacing the performance monitor with the SVM protocol we can track the time spent in various communication stages during main protocol operations: remote page fetch, remote lock synchronization, and barriers (global synchronization). <p> This study uses 8 SPLASH-2 <ref> [23] </ref> applications with a range of characteristics. Table 1 shows the problem sizes of the 8 benchmarks and their speedups on the SVM system with 8 processors. We use important computational kernels as well as real applications, both regular and irregular.
Reference: [24] <author> Y. Zhou, L. Iftode, and K. Li. </author> <title> Performance evaluation of two home-based lazy release consistency protocols for shared virtual memory systems. </title> <booktitle> In Proc. of the Operating Systems Design and Implementation Symposium, </booktitle> <month> Oct. </month> <year> 1996. </year>
Reference-contexts: To reduce the hardware cost of these systems, cache coherence hardware is often foregone in favor of software layers implementing Shared Virtual Memory (SVM) [18] as illustrated in Figure 1. Although SVM implementations have been the focus of several past performance studies <ref> [12, 15, 17, 24] </ref>, data-gathering approaches have varied widely. At the programming and protocol layer, run-time tools and software instrumentation have been used to detect high-level contributors to execution time: e.g., counts of synchronizations, page faults, and messages. <p> This increase then dilates the critical section within which it occurs, increasing lock contention and thus causing lock serialization. Furthermore, lock serialization is also reflected in the waiting time at barriers. Although some of these effects have been speculated from higher-level statistics and detailed simulations <ref> [2, 3, 12, 13, 14, 15, 17, 24] </ref>, the initial interrupt delay and the exact succession of this cascade of effects could not have been traced exactly without the performance monitor on real systems. <p> Each network interface is controlled by a 33MHz LANai processor running communication firmware. We embed our performance monitor in this code. 2.2 SVM Implementation The SVM system implements an all-software, home-based lazy release consistency (HLRC) protocol <ref> [24] </ref>. The HLRC protocol assigns each page to a home node. To alleviate the false-sharing problem at page granularity, HLRC implements a multiple-writer protocol based on using twins and diffs. <p> Our SVM system implements an HLRC protocol on a Myrinet-based PC cluster. The same HLRC protocol was previously implemented on Intel-Paragon <ref> [24] </ref>, Wisconsin Typhoon-Zero [10] and SHRIMP [5] (a PC-based cluster interconnected with a cus-tom designed network interface). The HLRC implementation on Typhoon-zero used active messages for communication support.
References-found: 24

