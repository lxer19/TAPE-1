URL: ftp://ftp.cs.yale.edu/pub/hager/tutorial_notes.ps.gz
Refering-URL: http://www.cs.yale.edu/HTML/YALE/CS/HyPlans/hager.html
Root-URL: http://www.cs.yale.edu
Email: Email: hager@cs.yale.edu  seth@mingus.cs.uiuc.edu  pic@brb.dmt.csiro.au  
Phone: Phone: (203) 432-6432  Phone: (217) 244-5570  
Title: Tutorial TT3: A Tutorial on Visual Servo Control  
Author: Gregory D. Hager Seth Hutchinson Peter Corke 
Address: New Haven, CT 06520-8285  Urbana, IL 61801  P.O. Box 883, Kenmore. Australia, 4069.  
Affiliation: Department of Computer Science Yale University  Electrical and Computer Engineering Department University of Illinois  CSIRO Division of Manufacturing Technology  
Abstract-found: 0
Intro-found: 0
Reference: [1] <author> Y. Shirai and H. Inoue, </author> <title> "Guiding a robot by visual feedback in assembling tasks," </title> <journal> Pattern Recognition, </journal> <volume> vol. 5, </volume> <pages> pp. 99-108, </pages> <year> 1973. </year>
Reference: [2] <author> J. Hill and W. T. Park, </author> <title> "Real time control of a robot with a mobile camera," </title> <booktitle> in Proc. 9th ISIR, </booktitle> <address> (Washington, DC), </address> <pages> pp. 233-246, </pages> <month> Mar. </month> <year> 1979. </year>
Reference-contexts: Many of these problems can be circumvented by sensing target pose directly using a 3D sensor. Active 3D sensors based on structured lighting are now compact and fast enough to use for visual servoing. If the sensor is small and mounted on the robot <ref> [2, 101, 33] </ref> the depth and orientation information can be used for position-based visual servoing. 1.5 Image-Based Control As described in Section 1.3, in image-based visual servo control the error signal is defined directly in terms of image feature parameters (in contrast to position-based methods that define the error signal in
Reference: [3] <author> P. Corke, </author> <title> "Visual control of robot manipulators | a review," </title> <editor> in Visual Servoing (K. Hashimoto, ed.), </editor> <volume> vol. </volume> <booktitle> 7 of Robotics and Automated Systems, </booktitle> <pages> pp. 1-31, </pages> <publisher> World Scientific, </publisher> <year> 1993. </year>
Reference-contexts: A comprehensive review of the literature in this field, as well the history and applications reported to date, is given by Corke <ref> [3] </ref> and includes a large bibliography. Visual servoing is the fusion of results from many elemental areas including high-speed image processing, kinematics, dynamics, control theory, and real-time computing. <p> Other authors use extremely task-specific clues: e:g: Allen [53] uses motion detection for locating a moving object to be grasped, and welding systems commonly use special filters that isolate the image of the welding tip. A review of tracking approaches used by researchers in this field is given in <ref> [3] </ref>. In less structured situations, vision has typically relied on the extraction of sharp contrast changes, referred to as "corners" or "edges", to indicate the presence of object boundaries or surface markings in an image. <p> Even with these improvements, [15] reports that a special signal processor is required to attain frame-rate performance. It is also possible to solve (1.59) using continuous optimization methods <ref> [29, 3, 33, 21] </ref>.
Reference: [4] <author> A. C. Sanderson and L. E. Weiss, </author> <title> "Image-based visual servo control using relational graph error signals," </title> <booktitle> Proc. IEEE, </booktitle> <pages> pp. 1074-1077, </pages> <year> 1980. </year>
Reference-contexts: The remainder of this article is structured as follows. Section 1.2 establishes a consistent nomenclature and reviews the relevant fundamentals of coordinate transformations, pose representation, and image formation. In Section 1.3, we present a taxonomy of visual servo control systems (adapted from <ref> [4] </ref>). The two major classes of systems, position-based visual servo systems and image-based visual servo systems, are discussed in Sections 1.4 and 1.5 respectively. <p> Calibration is a long standing research issue in the computer vision community (good solutions to the calibration problem can be found in a number of references, e.g., [24, 25, 26]). 1.3 Servoing Architectures In 1980, Sanderson and Weiss <ref> [4] </ref> introduced a taxonomy of visual servo systems, into which all subsequent visual servo systems can be categorized. Their scheme essentially poses two questions: 1. <p> Hence, visual servoing pre-supposes the solution to a set of potentially difficult static and dynamic vision problems. To this end many reported implementations contrive the vision problem to be simple: e:g: painting objects white, using artificial targets, and so forth <ref> [54, 38, 12, 4] </ref>. Other authors use extremely task-specific clues: e:g: Allen [53] uses motion detection for locating a moving object to be grasped, and welding systems commonly use special filters that isolate the image of the welding tip.
Reference: [5] <author> J. C. Latombe, </author> <title> Robot Motion Planning. </title> <address> Boston: </address> <publisher> Kluwer Academic Publishers, </publisher> <year> 1991. </year>
Reference-contexts: Since the task space is merely the configuration space of the robot tool, the task space is a smooth m-manifold (see, e.g., <ref> [5] </ref>). If the tool is a single rigid body moving arbitrarily in a three-dimensional workspace, then T = SE 3 = &lt; 3 fi SO 3 , and m = 6. In some applications, the task space may be restricted to a subspace of SE 3 .
Reference: [6] <author> J. J. Craig, </author> <title> Introduction to Robotics. </title> <address> Menlo Park: </address> <publisher> Addison Wesley, </publisher> <editor> second ed., </editor> <year> 1986. </year>
Reference-contexts: In this case, we often prefer to parameterize a pose using a translation vector and three angles, (e.g., roll, pitch and yaw <ref> [6] </ref>). Although such parameterizations are inherently local, it is often convenient to represent a pose by a vector r 2 &lt; 6 , rather than by x e 2 T . This notation can easily be adapted to the case where T SE 3 . <p> x c ffi c ^ x t : In order to compute a velocity screw, we first note that the rotation matrix e fl R e can be represented as a rotation through an angle e fl e about an axis defined by a unit vector e fl k e <ref> [6] </ref>. Thus, we can define = k 1 e fl ^ k e (1.31) e fl where t e is the origin of the end-effector frame in base coordinates.
Reference: [7] <author> B. K. P. Horn, </author> <title> Robot Vision. </title> <publisher> MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1986. </year>
Reference-contexts: This is illustrated in - 7 - Tutorial TT3: Visual Servo Control Hager/Hutchinson/Corke Perspective Projection. Assuming that the projective geometry of the camera is modeled by perspective projection (see, e.g., <ref> [7] </ref>), a point, c P = [x; y; z] T , whose coordinates are expressed with respect to the camera coordinate frame, will project onto the image plane with coordinates p = [u; v] T , given by (x; y; z) = u # " y (1.15) If the coordinates of
Reference: [8] <author> W. Jang, K. Kim, M. Chung, and Z. </author> <title> Bien, "Concepts of augmented image space and transformed feature space for efficient visual servoing of an "eye-in-hand robot"," </title> <journal> Robotica, </journal> <volume> vol. 9, </volume> <pages> pp. 203-212, </pages> <year> 1991. </year>
Reference: [9] <author> J. Feddema and O. Mitchell, </author> <title> "Vision-guided servoing with feature-based trajectory generation," </title> <journal> IEEE Trans. Robot. Autom., </journal> <volume> vol. 5, </volume> <pages> pp. 691-700, </pages> <month> Oct. </month> <year> 1989. </year>
Reference: [10] <author> B. Espiau, F. Chaumette, and P. Rives, </author> <title> "A New Approach to Visual Servoing in Robotics," </title> <journal> IEEE Transactions on Robotics and Automation, </journal> <volume> vol. 8, </volume> <pages> pp. 313-326, </pages> <year> 1992. </year>
Reference-contexts: Most commonly the coordinates of a feature point or a region centroid are used. A good feature point is one that can be located unambiguously in different views of the scene, such as a hole in a gasket [38] or a contrived pattern <ref> [10, 29] </ref>. <p> Image feature parameters that have been used for visual servo control include the image plane coordinates of points in the image <ref> [12, 10, 48, 14, 15, 93, 17] </ref>, the distance between two points in the image plane and the orientation of the line connecting those two points [38, 36], perceived edge length [19], the area of a projected surface and the relative areas of two projected surfaces [19], the centroid and higher <p> of the line connecting those two points [38, 36], perceived edge length [19], the area of a projected surface and the relative areas of two projected surfaces [19], the centroid and higher order moments of a projected surface [20, 21, 19, 22], the parameters of lines in the image plane <ref> [10] </ref>, and the parameters of an ellipse in the image plane [10]. In this tutorial we will restrict our attention to point features whose parameters are their image plane coordinates. In order to perform visual servo control, we must select a set of image feature parameters. <p> length [19], the area of a projected surface and the relative areas of two projected surfaces [19], the centroid and higher order moments of a projected surface [20, 21, 19, 22], the parameters of lines in the image plane <ref> [10] </ref>, and the parameters of an ellipse in the image plane [10]. In this tutorial we will restrict our attention to point features whose parameters are their image plane coordinates. In order to perform visual servo control, we must select a set of image feature parameters. <p> The value d will be referred to as the degree of the constraint. As noted by Espiau et al. <ref> [33, 10] </ref>, the kinematic error function can be thought of as representing a virtual kinematic constraint between the end-effector and the target. <p> Thus, the number of columns in the image Jacobian will vary depending on the task. The image Jacobian was first introduced by Weiss et al. [19], who referred to it as the feature sensitivity matrix. It is also referred to as the interaction matrix <ref> [10] </ref> and the B matrix [14, 15]. Other applications of the image Jacobian include [12, 38, 48, 22]. The relationship given by (1.35) describes how image feature parameters change with respect to changing manipulator pose.
Reference: [11] <author> M. L. Cyros, </author> <title> "Datacube at the space shuttle's launch pad," </title> <journal> Datacube World Review, </journal> <volume> vol. 2, </volume> <pages> pp. 1-3, </pages> <month> Sept. </month> <year> 1988. </year> <institution> Datacube Inc., </institution> <address> 4 Dearborn Road, Peabody, MA. </address>
Reference: [12] <author> A. Castano and S. A. Hutchinson, </author> <title> "Visual compliance: </title> <journal> Task-directed visual servo control," IEEE Transactions on Robotics and Automation, </journal> <volume> vol. 10, </volume> <pages> pp. 334-342, </pages> <month> June </month> <year> 1994. </year>
Reference-contexts: Image feature parameters that have been used for visual servo control include the image plane coordinates of points in the image <ref> [12, 10, 48, 14, 15, 93, 17] </ref>, the distance between two points in the image plane and the orientation of the line connecting those two points [38, 36], perceived edge length [19], the area of a projected surface and the relative areas of two projected surfaces [19], the centroid and higher <p> The image Jacobian was first introduced by Weiss et al. [19], who referred to it as the feature sensitivity matrix. It is also referred to as the interaction matrix [10] and the B matrix [14, 15]. Other applications of the image Jacobian include <ref> [12, 38, 48, 22] </ref>. The relationship given by (1.35) describes how image feature parameters change with respect to changing manipulator pose. <p> The null space of the image Jacobian plays a significant role in hybrid methods, in which some degrees of freedom are controlled using visual servo, while the remaining degrees of freedom are controlled using some other modality <ref> [12] </ref>. 1.5.4 Resolved-Rate Methods The earliest approaches to image-based visual servo control [19, 38] were based on resolved-rate motion control [28], which we will briefly describe here. Suppose that the goal of a particular task is to reach a desired image feature parameter vector, f d . <p> For the case of a non-square image Jacobian, the techniques described in Section 1.5.3 would be used to compute for u. Similar results have been presented in <ref> [48, 12] </ref>. 1.5.5 Example Servoing Tasks In this section, we revisit the problems that were described in Section 1.4.1. Here, we describe image-based solutions for these problems. Point to Point Positioning Consider the task of bringing some point P on the manipulator to a desired stationing point S. <p> Hence, visual servoing pre-supposes the solution to a set of potentially difficult static and dynamic vision problems. To this end many reported implementations contrive the vision problem to be simple: e:g: painting objects white, using artificial targets, and so forth <ref> [54, 38, 12, 4] </ref>. Other authors use extremely task-specific clues: e:g: Allen [53] uses motion detection for locating a moving object to be grasped, and welding systems commonly use special filters that isolate the image of the welding tip. <p> These include control issues, such as adaptive visual servo control [85, 14], hybrid control (e.g., hybrid vision/position control <ref> [12] </ref>, or hybrid force/vision control), and multi-rate system theory [86]; issues related to automatic planning of visually controlled robot motions [87, 88]; applications in mobile robotics, including non-holonmoic systems [83]; and, feature selection [33, 36].
Reference: [13] <author> K. Hashimoto, T. Kimoto, T. Ebine, and H. Kimura, </author> <title> "Manipulator control with image-based visual servo," </title> <booktitle> in Proc. IEEE Int. Conf. Robotics and Automation, </booktitle> <pages> pp. 2267-2272, </pages> <year> 1991. </year> <title> Tutorial TT3: Visual Servo Control Hager/Hutchinson/Corke </title>
Reference-contexts: The pixel values for all x 2 X are copied into a two-dimensional array that is subsequently treated as a rectangular image. Such acquisitions can be implemented extremely efficiently using line-drawing and region-fill algorithms commonly developed for graphics applications <ref> [13] </ref>. In the second stage, the windows are processed to locate features. Using feature measurements, a new set of window parameters are computed. These parameters may be modified using external geometric constraints or temporal prediction, and the cycle repeats.
Reference: [14] <author> N. P. Papanikolopoulos and P. K. Khosla, </author> <title> "Adaptive Robot Visual Tracking: Theory and Experiments," </title> <journal> IEEE Transactions on Automatic Control, </journal> <volume> vol. 38, no. 3, </volume> <pages> pp. 429-445, </pages> <year> 1993. </year>
Reference-contexts: Image feature parameters that have been used for visual servo control include the image plane coordinates of points in the image <ref> [12, 10, 48, 14, 15, 93, 17] </ref>, the distance between two points in the image plane and the orientation of the line connecting those two points [38, 36], perceived edge length [19], the area of a projected surface and the relative areas of two projected surfaces [19], the centroid and higher <p> Thus, the number of columns in the image Jacobian will vary depending on the task. The image Jacobian was first introduced by Weiss et al. [19], who referred to it as the feature sensitivity matrix. It is also referred to as the interaction matrix [10] and the B matrix <ref> [14, 15] </ref>. Other applications of the image Jacobian include [12, 38, 48, 22]. The relationship given by (1.35) describes how image feature parameters change with respect to changing manipulator pose. <p> In practice, the unknown parameter for Jacobian calculation is distance from the camera. Some recent papers present adaptive approaches for estimating <ref> [14] </ref> this depth value, or develop feedback methods which do not use depth in the feedback formulation [67]. There are often computational advantages to image-based control, particularly in ECL configurations. <p> These include control issues, such as adaptive visual servo control <ref> [85, 14] </ref>, hybrid control (e.g., hybrid vision/position control [12], or hybrid force/vision control), and multi-rate system theory [86]; issues related to automatic planning of visually controlled robot motions [87, 88]; applications in mobile robotics, including non-holonmoic systems [83]; and, feature selection [33, 36].
Reference: [15] <author> N. P. Papanikolopoulos, P. K. Khosla, and T. Kanade, </author> <title> "Visual Tracking of a Moving Target by a Camera Mounted on a Robot: A Combination of Vision and Control," </title> <journal> IEEE Transactions on Robotics and Automation, </journal> <volume> vol. 9, no. 1, </volume> <pages> pp. 14-35, </pages> <year> 1993. </year>
Reference-contexts: Image feature parameters that have been used for visual servo control include the image plane coordinates of points in the image <ref> [12, 10, 48, 14, 15, 93, 17] </ref>, the distance between two points in the image plane and the orientation of the line connecting those two points [38, 36], perceived edge length [19], the area of a projected surface and the relative areas of two projected surfaces [19], the centroid and higher <p> Thus, the number of columns in the image Jacobian will vary depending on the task. The image Jacobian was first introduced by Weiss et al. [19], who referred to it as the feature sensitivity matrix. It is also referred to as the interaction matrix [10] and the B matrix <ref> [14, 15] </ref>. Other applications of the image Jacobian include [12, 38, 48, 22]. The relationship given by (1.35) describes how image feature parameters change with respect to changing manipulator pose. <p> This burden can be reduced by performing the optimization starting at low resolution and proceeding to higher resolution, and by ordering the candidates in D from most to least likely and terminating the search once a candidate with an acceptably low SSD value is found <ref> [15] </ref>. Once the discrete minimum is found, the location can be refined to subpixel accuracy by interpolation of the SSD values about the minimum. Even with these improvements, [15] reports that a special signal processor is required to attain frame-rate performance. <p> in D from most to least likely and terminating the search once a candidate with an acceptably low SSD value is found <ref> [15] </ref>. Once the discrete minimum is found, the location can be refined to subpixel accuracy by interpolation of the SSD values about the minimum. Even with these improvements, [15] reports that a special signal processor is required to attain frame-rate performance. It is also possible to solve (1.59) using continuous optimization methods [29, 3, 33, 21].
Reference: [16] <author> S. Skaar, W. Brockman, and R. Hanson, </author> <title> "Camera-space manipulation," </title> <journal> Int. J. Robot. Res., </journal> <volume> vol. 6, no. 4, </volume> <pages> pp. 20-32, </pages> <year> 1987. </year>
Reference: [17] <author> S. B. Skaar, W. H. Brockman, and W. S. Jang, </author> <title> "Three-Dimensional Camera Space Manipulation," </title> <journal> International Journal of Robotics Research, </journal> <volume> vol. 9, no. 4, </volume> <pages> pp. 22-39, </pages> <year> 1990. </year>
Reference-contexts: Image feature parameters that have been used for visual servo control include the image plane coordinates of points in the image <ref> [12, 10, 48, 14, 15, 93, 17] </ref>, the distance between two points in the image plane and the orientation of the line connecting those two points [38, 36], perceived edge length [19], the area of a projected surface and the relative areas of two projected surfaces [19], the centroid and higher
Reference: [18] <author> J. T. Feddema, C. S. G. Lee, and O. R. Mitchell, </author> <title> "Weighted selection of image features for resolved rate visual feedback control," </title> <journal> IEEE Trans. Robot. Autom., </journal> <volume> vol. 7, </volume> <pages> pp. 31-47, </pages> <month> Feb. </month> <year> 1991. </year>
Reference-contexts: Hence, the positioning accuracy of EOL systems depends directly on the accuracy of the hand-eye calibration. Conversely, systems that observe the end-effector as well as target features can perform with accuracy that is independent of hand-eye calibration error <ref> [30, 31, 18] </ref>. Note also that ECL systems can easily deal with tasks that involve the positioning of objects within the end-effector, whereas EOL systems must use an inferred object location. From a theoretical perspective, it would appear that ECL systems would always be preferable to EOL systems.
Reference: [19] <author> A. C. Sanderson, L. E. Weiss, and C. P. Neuman, </author> <title> "Dynamic sensor-based control of robots with visual feedback," </title> <journal> IEEE Trans. Robot. Autom., </journal> <volume> vol. RA-3, </volume> <pages> pp. 404-417, </pages> <month> Oct. </month> <year> 1987. </year>
Reference-contexts: parameters that have been used for visual servo control include the image plane coordinates of points in the image [12, 10, 48, 14, 15, 93, 17], the distance between two points in the image plane and the orientation of the line connecting those two points [38, 36], perceived edge length <ref> [19] </ref>, the area of a projected surface and the relative areas of two projected surfaces [19], the centroid and higher order moments of a projected surface [20, 21, 19, 22], the parameters of lines in the image plane [10], and the parameters of an ellipse in the image plane [10]. <p> points in the image [12, 10, 48, 14, 15, 93, 17], the distance between two points in the image plane and the orientation of the line connecting those two points [38, 36], perceived edge length <ref> [19] </ref>, the area of a projected surface and the relative areas of two projected surfaces [19], the centroid and higher order moments of a projected surface [20, 21, 19, 22], the parameters of lines in the image plane [10], and the parameters of an ellipse in the image plane [10]. <p> the distance between two points in the image plane and the orientation of the line connecting those two points [38, 36], perceived edge length [19], the area of a projected surface and the relative areas of two projected surfaces [19], the centroid and higher order moments of a projected surface <ref> [20, 21, 19, 22] </ref>, the parameters of lines in the image plane [10], and the parameters of an ellipse in the image plane [10]. In this tutorial we will restrict our attention to point features whose parameters are their image plane coordinates. <p> Thus, the number of columns in the image Jacobian will vary depending on the task. The image Jacobian was first introduced by Weiss et al. <ref> [19] </ref>, who referred to it as the feature sensitivity matrix. It is also referred to as the interaction matrix [10] and the B matrix [14, 15]. Other applications of the image Jacobian include [12, 38, 48, 22]. <p> The null space of the image Jacobian plays a significant role in hybrid methods, in which some degrees of freedom are controlled using visual servo, while the remaining degrees of freedom are controlled using some other modality [12]. 1.5.4 Resolved-Rate Methods The earliest approaches to image-based visual servo control <ref> [19, 38] </ref> were based on resolved-rate motion control [28], which we will briefly describe here. Suppose that the goal of a particular task is to reach a desired image feature parameter vector, f d . <p> Discussion of the issues related to feature selection for visual servo control applications can be found in <ref> [19, 36] </ref>. The "right" image feature tracking method to use is extremely application dependent. For example, if the goal is to track a single special pattern or surface marking that is approximately planar and moving at slow to moderate speeds, then SSD tracking is appropriate.
Reference: [20] <author> R. L. Andersson, </author> <title> A Robot Ping-Pong Player. Experiment in Real-Time Intelligent Control. </title> <publisher> MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1988. </year>
Reference-contexts: the distance between two points in the image plane and the orientation of the line connecting those two points [38, 36], perceived edge length [19], the area of a projected surface and the relative areas of two projected surfaces [19], the centroid and higher order moments of a projected surface <ref> [20, 21, 19, 22] </ref>, the parameters of lines in the image plane [10], and the parameters of an ellipse in the image plane [10]. In this tutorial we will restrict our attention to point features whose parameters are their image plane coordinates. <p> Here we use the term "direct visual servo" to avoid confusion. - 11 - Tutorial TT3: Visual Servo Control Hager/Hutchinson/Corke challenging control problem. Using internal feedback with a high sampling rate generally presents the visual controller with idealized axis dynamics <ref> [20] </ref>. Second, many robots already have an interface for accepting Cartesian velocity or incremental position commands. This simplifies the construction of the visual servo system, and also makes the methods more portable. <p> A good example of this is the common Unimate Puma robot whose position loops operate at a sample interval of 14 or 28 ms while vision systems operate at sample intervals of 33 or 40 ms for RS 170 or CCIR video respectively <ref> [20] </ref>. It is well known that a feedback system including delay will become unstable as the loop gain is increased. Many visual closed-loop systems are tuned empirically, increasing the loop gain until overshoot or oscillation becomes intolerable. <p> Other issues for consideration include whether or not the vision system should `close the loop' around robot axes which are position, velocity or torque controlled. A detailed discussion of these dynamic issues in visual servo systems is given by Corke <ref> [20, 82] </ref>. - 38 - Tutorial TT3: Visual Servo Control Hager/Hutchinson/Corke 1.7.3 Mobile robots The discussion above has assumed that the moving camera is mounted on an arm type robot manipulator.
Reference: [21] <author> M. Lei and B. K. Ghosh, </author> <title> "Visually-Guided Robotic Motion Tracking," </title> <booktitle> in Proc. Thirtieth Annual Allerton Conference on Communication, Control, and Computing, </booktitle> <pages> pp. 712-721, </pages> <year> 1992. </year>
Reference-contexts: the distance between two points in the image plane and the orientation of the line connecting those two points [38, 36], perceived edge length [19], the area of a projected surface and the relative areas of two projected surfaces [19], the centroid and higher order moments of a projected surface <ref> [20, 21, 19, 22] </ref>, the parameters of lines in the image plane [10], and the parameters of an ellipse in the image plane [10]. In this tutorial we will restrict our attention to point features whose parameters are their image plane coordinates. <p> Even with these improvements, [15] reports that a special signal processor is required to attain frame-rate performance. It is also possible to solve (1.59) using continuous optimization methods <ref> [29, 3, 33, 21] </ref>. <p> First, a single updating cycle is usually faster to compute. For example, (1.62) can be computed and solved in less than 5 ms on a Sparc II computer <ref> [21] </ref>. Second, it is easy to incorporate other window parameters such as rotation and scaling into the system without greatly increasing the computation time [33, 21]. <p> For example, (1.62) can be computed and solved in less than 5 ms on a Sparc II computer [21]. Second, it is easy to incorporate other window parameters such as rotation and scaling into the system without greatly increasing the computation time <ref> [33, 21] </ref>. It is also easy to show that including parameters for contrast and brightness in (1.60) makes SSD tracking equivalent to finding the maximum correlation between the two image regions [29].
Reference: [22] <author> B. Yoshimi and P. K. Allen, </author> <title> "Active, uncalibrated visual servoing," </title> <booktitle> in Proc. IEEE International Conference on Robotics and Automation, </booktitle> <address> (San Diego, CA), </address> <pages> pp. 156-161, </pages> <month> May </month> <year> 1994. </year>
Reference-contexts: the distance between two points in the image plane and the orientation of the line connecting those two points [38, 36], perceived edge length [19], the area of a projected surface and the relative areas of two projected surfaces [19], the centroid and higher order moments of a projected surface <ref> [20, 21, 19, 22] </ref>, the parameters of lines in the image plane [10], and the parameters of an ellipse in the image plane [10]. In this tutorial we will restrict our attention to point features whose parameters are their image plane coordinates. <p> The image Jacobian was first introduced by Weiss et al. [19], who referred to it as the feature sensitivity matrix. It is also referred to as the interaction matrix [10] and the B matrix [14, 15]. Other applications of the image Jacobian include <ref> [12, 38, 48, 22] </ref>. The relationship given by (1.35) describes how image feature parameters change with respect to changing manipulator pose.
Reference: [23] <author> B. Nelson and P. K. Khosla, </author> <title> "Integrating Sensor Placement and Visual Tracking Strategies," </title> <booktitle> in Proc. IEEE International Conference on Robotics and Automation, </booktitle> <pages> pp. 1351-1356, </pages> <year> 1994. </year>
Reference-contexts: A variant of this is for the camera to be agile, mounted on another robot or pan/tilt head in order to observe the visually controlled robot from the best vantage <ref> [23] </ref>. For either choice of camera configuration, prior to the execution of visual servo tasks, camera calibration must be performed. For the eye-in-hand case, this amounts to determining e x c . For the fixed camera case, calibration is used to determine 0 x c .
Reference: [24] <author> I. E. Sutherland, </author> <title> "Three-dimensional data input by tablet," </title> <journal> Proc. IEEE, </journal> <volume> vol. 62, </volume> <pages> pp. 453-461, </pages> <month> Apr. </month> <year> 1974. </year>
Reference-contexts: For the fixed camera case, calibration is used to determine 0 x c . Calibration is a long standing research issue in the computer vision community (good solutions to the calibration problem can be found in a number of references, e.g., <ref> [24, 25, 26] </ref>). 1.3 Servoing Architectures In 1980, Sanderson and Weiss [4] introduced a taxonomy of visual servo systems, into which all subsequent visual servo systems can be categorized. Their scheme essentially poses two questions: 1.
Reference: [25] <author> R. Tsai and R. Lenz, </author> <title> "A new technique for fully autonomous and efficient 3D robotics hand/eye calibra tion," </title> <journal> IEEE Trans. Robot. Autom., </journal> <volume> vol. 5, </volume> <pages> pp. 345-358, </pages> <month> June </month> <year> 1989. </year>
Reference-contexts: For the fixed camera case, calibration is used to determine 0 x c . Calibration is a long standing research issue in the computer vision community (good solutions to the calibration problem can be found in a number of references, e.g., <ref> [24, 25, 26] </ref>). 1.3 Servoing Architectures In 1980, Sanderson and Weiss [4] introduced a taxonomy of visual servo systems, into which all subsequent visual servo systems can be categorized. Their scheme essentially poses two questions: 1.
Reference: [26] <author> R. Tsai, </author> <title> "A versatile camera calibration technique for high accuracy 3-D machine vision m etrology using off-the-shelf TV cameras and lenses," </title> <journal> IEEE Trans. Robot. Autom., </journal> <volume> vol. 3, </volume> <pages> pp. 323-344, </pages> <month> Aug. </month> <year> 1987. </year>
Reference-contexts: For the fixed camera case, calibration is used to determine 0 x c . Calibration is a long standing research issue in the computer vision community (good solutions to the calibration problem can be found in a number of references, e.g., <ref> [24, 25, 26] </ref>). 1.3 Servoing Architectures In 1980, Sanderson and Weiss [4] introduced a taxonomy of visual servo systems, into which all subsequent visual servo systems can be categorized. Their scheme essentially poses two questions: 1.
Reference: [27] <author> P. I. Corke, </author> <title> High-Performance Visual Closed-Loop Robot Control. </title> <type> PhD thesis, </type> <institution> University of Melbourne, Dept.Mechanical and Manufacturing Engineering, </institution> <month> July </month> <year> 1994. </year> <title> Tutorial TT3: Visual Servo Control Hager/Hutchinson/Corke </title>
Reference: [28] <author> D. E. Whitney, </author> <title> "The mathematics of coordinated control of prosthetic arms and manipulators," </title> <journal> Journal of Dynamic Systems, Measurement and Control, </journal> <volume> vol. 122, </volume> <pages> pp. 303-309, </pages> <month> Dec. </month> <year> 1972. </year>
Reference-contexts: This simplifies the construction of the visual servo system, and also makes the methods more portable. Thirdly, look-and-move separates the kinematic singularities of the mechanism from the visual controller, allowing the robot to be considered as an ideal Cartesian motion device. Since many resolved rate <ref> [28] </ref> controllers have specialized mechanisms for dealing with kinematic singularities [29], the system design is again greatly simplified. In this article, we will utilize the look-and-move model exclusively. The second major classification of systems distinguishes position-based control from image-based control. <p> plays a significant role in hybrid methods, in which some degrees of freedom are controlled using visual servo, while the remaining degrees of freedom are controlled using some other modality [12]. 1.5.4 Resolved-Rate Methods The earliest approaches to image-based visual servo control [19, 38] were based on resolved-rate motion control <ref> [28] </ref>, which we will briefly describe here. Suppose that the goal of a particular task is to reach a desired image feature parameter vector, f d .
Reference: [29] <author> S. Chieaverini, L. Sciavicco, and B. Siciliano, </author> <title> "Control of robotic systems through singularities," in Proc. Int. Workshop on Nonlinear and Adaptive Control: Issues i n Robotics (C. </title> <editor> C. de Wit, ed.), </editor> <publisher> Springer-Verlag, </publisher> <year> 1991. </year>
Reference-contexts: Most commonly the coordinates of a feature point or a region centroid are used. A good feature point is one that can be located unambiguously in different views of the scene, such as a hole in a gasket [38] or a contrived pattern <ref> [10, 29] </ref>. <p> Thirdly, look-and-move separates the kinematic singularities of the mechanism from the visual controller, allowing the robot to be considered as an ideal Cartesian motion device. Since many resolved rate [28] controllers have specialized mechanisms for dealing with kinematic singularities <ref> [29] </ref>, the system design is again greatly simplified. In this article, we will utilize the look-and-move model exclusively. The second major classification of systems distinguishes position-based control from image-based control. <p> Even with these improvements, [15] reports that a special signal processor is required to attain frame-rate performance. It is also possible to solve (1.59) using continuous optimization methods <ref> [29, 3, 33, 21] </ref>. <p> It is also easy to show that including parameters for contrast and brightness in (1.60) makes SSD tracking equivalent to finding the maximum correlation between the two image regions <ref> [29] </ref>.
Reference: [30] <author> S. Wijesoma, D. Wolfe, and R. Richards, </author> <title> "Eye-to-hand coordination for vision-guided robot control applications," </title> <journal> International Journal of Robotics Research, </journal> <volume> vol. 12, no. 1, </volume> <pages> pp. 65-78, </pages> <year> 1993. </year>
Reference-contexts: Hence, the positioning accuracy of EOL systems depends directly on the accuracy of the hand-eye calibration. Conversely, systems that observe the end-effector as well as target features can perform with accuracy that is independent of hand-eye calibration error <ref> [30, 31, 18] </ref>. Note also that ECL systems can easily deal with tasks that involve the positioning of objects within the end-effector, whereas EOL systems must use an inferred object location. From a theoretical perspective, it would appear that ECL systems would always be preferable to EOL systems.
Reference: [31] <author> N. Hollinghurst and R. Cipolla, </author> <title> "Uncalibrated stereo hand eye coordination," </title> <journal> Image and Vision Computing, </journal> <volume> vol. 12, no. 3, </volume> <pages> pp. 187-192, </pages> <year> 1994. </year>
Reference-contexts: Hence, the positioning accuracy of EOL systems depends directly on the accuracy of the hand-eye calibration. Conversely, systems that observe the end-effector as well as target features can perform with accuracy that is independent of hand-eye calibration error <ref> [30, 31, 18] </ref>. Note also that ECL systems can easily deal with tasks that involve the positioning of objects within the end-effector, whereas EOL systems must use an inferred object location. From a theoretical perspective, it would appear that ECL systems would always be preferable to EOL systems. <p> For example, Allen [53] shows a system which can grasp a toy train using stereo vision. Rizzi [54] demonstrates a system which can bounce a ping-pong ball. All of these systems are EOL. Cipolla <ref> [31] </ref> describes an ECL system using free-standing stereo cameras. One novel feature of this system is the use of the affine projection model (Section 1.2.3) for the imaging geometry. This leads to linear calibration and control at the cost of some system performance. <p> However not all pixels in the image are of interest, and computation time can be greatly reduced if only a small region around each image feature is processed. Thus, a promising technique for making vision cheap and tractable is to use window-based tracking techniques <ref> [70, 54, 31] </ref>. Window-based methods have several advantages, among them; computational simplicity, little requirement for special hardware, and easy reconfiguration for different applications.
Reference: [32] <author> G. D. Hager, W.-C. Chang, and A. S. Morse, </author> <title> "Robot hand-eye coordination based on stereo vision," </title> <journal> IEEE Control Systems Magazine, </journal> <month> Feb. </month> <year> 1995. </year>
Reference: [33] <author> C. Samson, M. Le Borgne, and B. Espiau, </author> <title> Robot Control: The Task Function Approach. </title> <publisher> Oxford, </publisher> <address> England: </address> <publisher> Clarendon Press, </publisher> <year> 1992. </year>
Reference-contexts: The value d will be referred to as the degree of the constraint. As noted by Espiau et al. <ref> [33, 10] </ref>, the kinematic error function can be thought of as representing a virtual kinematic constraint between the end-effector and the target. <p> Many of these problems can be circumvented by sensing target pose directly using a 3D sensor. Active 3D sensors based on structured lighting are now compact and fast enough to use for visual servoing. If the sensor is small and mounted on the robot <ref> [2, 101, 33] </ref> the depth and orientation information can be used for position-based visual servoing. 1.5 Image-Based Control As described in Section 1.3, in image-based visual servo control the error signal is defined directly in terms of image feature parameters (in contrast to position-based methods that define the error signal in <p> Even with these improvements, [15] reports that a special signal processor is required to attain frame-rate performance. It is also possible to solve (1.59) using continuous optimization methods <ref> [29, 3, 33, 21] </ref>. <p> For example, (1.62) can be computed and solved in less than 5 ms on a Sparc II computer [21]. Second, it is easy to incorporate other window parameters such as rotation and scaling into the system without greatly increasing the computation time <ref> [33, 21] </ref>. It is also easy to show that including parameters for contrast and brightness in (1.60) makes SSD tracking equivalent to finding the maximum correlation between the two image regions [29]. <p> include control issues, such as adaptive visual servo control [85, 14], hybrid control (e.g., hybrid vision/position control [12], or hybrid force/vision control), and multi-rate system theory [86]; issues related to automatic planning of visually controlled robot motions [87, 88]; applications in mobile robotics, including non-holonmoic systems [83]; and, feature selection <ref> [33, 36] </ref>. Many of these are describe in the proceedings of a recent workshop on visual servo control [89]. - 39 - Tutorial TT3: Visual Servo Control Hager/Hutchinson/Corke 1.7.6 The future The future for applications of visual servoing should be bright.
Reference: [34] <author> G. Franklin, J. Powell, and A. Emami-Naeini, </author> <title> Feedback Control of Dynamic Systems. </title> <publisher> Addison-Wesley, </publisher> <editor> 2nd ed., </editor> <year> 1991. </year>
Reference-contexts: This regulator produces at every time instant a desired end-effector velocity screw u 2 &lt; 6 which is sent to the robot control subsystem. For the purposes of this section, we use simple proportional control methods for linear and linearized systems to compute u <ref> [34] </ref>. These methods are illustrated below, and are discussed in more detail in Section 1.5. We now present examples of positioning tasks for end-effector and fixed cameras in both ECL and EOL configurations. In Section 1.4.1, several examples of positioning tasks based on directly observable features are presented. <p> pp ( ^ x e ; ^ x c ffi c b S; e P) = k x e ffi e P ^ x c ffi c b S : (1.20) will drive the system to an equilibrium state in which the estimated value of the error function is zero <ref> [34] </ref>. The value k &gt; 0 is a proportional feedback gain. Note that - 14 - Tutorial TT3: Visual Servo Control Hager/Hutchinson/Corke we have written ^ x e in the feedback law to emphasize the fact that this value is also subject to errors.
Reference: [35] <author> G. D. Hager, </author> <title> "Six DOF visual control of relative position," </title> <institution> DCS RR-1038, Yale University, </institution> <address> New Haven, CT, </address> <month> June </month> <year> 1994. </year>
Reference-contexts: Full six degree-of-freedom positioning can be attained by enforcing another point-to-line constraint using an additional point on the end-effector and an additional point in the world. See <ref> [35] </ref> for details. These formulations can be adjusted for end-effector mounted camera and can be implemented as ECL or EOL systems.
Reference: [36] <author> T. S. Huang and A. N. Netravali, </author> <title> "Motion and structure from feature correspondences: A review," </title> <journal> IEEE Proceeding, </journal> <volume> vol. 82, no. 2, </volume> <pages> pp. 252-268, </pages> <year> 1994. </year>
Reference-contexts: Image feature parameters that have been used for visual servo control include the image plane coordinates of points in the image [12, 10, 48, 14, 15, 93, 17], the distance between two points in the image plane and the orientation of the line connecting those two points <ref> [38, 36] </ref>, perceived edge length [19], the area of a projected surface and the relative areas of two projected surfaces [19], the centroid and higher order moments of a projected surface [20, 21, 19, 22], the parameters of lines in the image plane [10], and the parameters of an ellipse in <p> This encompasses problems including structure from motion, exterior orientation, stereo reconstruction, and absolute orientation. A comprehensive discussion of these topics can be found in a recent review article <ref> [36] </ref>. We divide the estimation problems that arise into single-camera and multiple-camera situations which will be discussed in the following sections. Single Camera As noted previously, it follows from (1.15) that a point in a single camera image corresponds to a line in space. <p> Single Camera As noted previously, it follows from (1.15) that a point in a single camera image corresponds to a line in space. Although it is possible to perform geometric reconstruction using a single moving camera, the equations governing this process are often ill-conditioned, leading to stability problems <ref> [36] </ref> Better results can be achieved if target features have some internal structure, or the features come from a known object. Below, we briefly describe methods for performing both point estimation and pose estimation with a single camera assuming such information is available. <p> We now discuss each of these. When k = m and J v is nonsingular, J 1 v exists. Therefore, in this case, _ r = J 1 v Such an approach has been used by Feddema <ref> [36] </ref>, who also describes an automated approach to image feature selection in order to minimize the condition number of J v . When k 6= m, J 1 v does not exist. <p> Discussion of the issues related to feature selection for visual servo control applications can be found in <ref> [19, 36] </ref>. The "right" image feature tracking method to use is extremely application dependent. For example, if the goal is to track a single special pattern or surface marking that is approximately planar and moving at slow to moderate speeds, then SSD tracking is appropriate. <p> include control issues, such as adaptive visual servo control [85, 14], hybrid control (e.g., hybrid vision/position control [12], or hybrid force/vision control), and multi-rate system theory [86]; issues related to automatic planning of visually controlled robot motions [87, 88]; applications in mobile robotics, including non-holonmoic systems [83]; and, feature selection <ref> [33, 36] </ref>. Many of these are describe in the proceedings of a recent workshop on visual servo control [89]. - 39 - Tutorial TT3: Visual Servo Control Hager/Hutchinson/Corke 1.7.6 The future The future for applications of visual servoing should be bright.
Reference: [37] <author> W. Wilson, </author> <title> "Visual servo control of robots using kalman filter estimates of robot pose relative to work-pieces," in Visual Servoing (K. Hashimoto, </title> <publisher> ed.), </publisher> <pages> pp. 71-104, </pages> <publisher> World Scientific, </publisher> <year> 1994. </year>
Reference-contexts: Object Pose Accurate object pose estimation is possible if the vision system observes features of a known object, and uses those features to estimate object pose. This approach has been recently demonstrated by Wilson <ref> [37] </ref> for six DOF control of end-effector pose. A similar approach was recently reported in [38]. Briefly, such an approach proceeds as follows. <p> A particularly elegant formulation of this updating procedure results by application of statistical techniques such as the extended Kalman filter [52]. The reader is referred to <ref> [37] </ref> for details. Multiple Cameras Many systems utilizing position-based control with stereo vision from free-standing cameras have been demonstrated. For example, Allen [53] shows a system which can grasp a toy train using stereo vision. Rizzi [54] demonstrates a system which can bounce a ping-pong ball. <p> Computation time for the relative orientation problem is often cited as a disadvantage of position-based methods. However recent results show that solutions can be computed in only a few milliseconds even using iteration [39] or Kalman filtering <ref> [37] </ref>. - 20 - Tutorial TT3: Visual Servo Control Hager/Hutchinson/Corke Endpoint closed-loop systems are demonstrably less sensitive to calibration. However, particularly in stereo systems, small rotational errors between the cameras can lead to reconstruction errors which do impact the positioning accuracy of the system. <p> For example, Rizzi [54] describes the use of a Newtonian flight dynamics model to make it possible to track a ping-pong ball during flight. Predictors based on ff fi tracking filters and Kalman filters have also been used <ref> [53, 37, 72] </ref>. Multiresolution techniques can be used provide further performance improvements, particularly when a dynamic model is not available and large search windows must be used. 1.6.4 Discussion Prior to executing or planning visually controlled motions, a specific set of visual features must be chosen.
Reference: [38] <author> C. Fagerer, D. Dickmanns, and E. Dickmanns, </author> <title> "Visual grasping with long delay time of a free floating object in orbit," </title> <booktitle> Autonomous Robots, </booktitle> <volume> vol. 1, no. 1, </volume> <year> 1994. </year>
Reference-contexts: Most commonly the coordinates of a feature point or a region centroid are used. A good feature point is one that can be located unambiguously in different views of the scene, such as a hole in a gasket <ref> [38] </ref> or a contrived pattern [10, 29]. <p> Image feature parameters that have been used for visual servo control include the image plane coordinates of points in the image [12, 10, 48, 14, 15, 93, 17], the distance between two points in the image plane and the orientation of the line connecting those two points <ref> [38, 36] </ref>, perceived edge length [19], the area of a projected surface and the relative areas of two projected surfaces [19], the centroid and higher order moments of a projected surface [20, 21, 19, 22], the parameters of lines in the image plane [10], and the parameters of an ellipse in <p> Object Pose Accurate object pose estimation is possible if the vision system observes features of a known object, and uses those features to estimate object pose. This approach has been recently demonstrated by Wilson [37] for six DOF control of end-effector pose. A similar approach was recently reported in <ref> [38] </ref>. Briefly, such an approach proceeds as follows. Let t P 1 ; t P 2 ; : : : t P n be a set of points expressed in an object coordinate system with unknown pose c x t relative to an observing camera. <p> The image Jacobian was first introduced by Weiss et al. [19], who referred to it as the feature sensitivity matrix. It is also referred to as the interaction matrix [10] and the B matrix [14, 15]. Other applications of the image Jacobian include <ref> [12, 38, 48, 22] </ref>. The relationship given by (1.35) describes how image feature parameters change with respect to changing manipulator pose. <p> The null space of the image Jacobian plays a significant role in hybrid methods, in which some degrees of freedom are controlled using visual servo, while the remaining degrees of freedom are controlled using some other modality [12]. 1.5.4 Resolved-Rate Methods The earliest approaches to image-based visual servo control <ref> [19, 38] </ref> were based on resolved-rate motion control [28], which we will briefly describe here. Suppose that the goal of a particular task is to reach a desired image feature parameter vector, f d . <p> By observing visual cues such as the ball, the arm's pivot point, and another point on the arm, the interception task can be specified, even if the relationship between camera and arm is not known a priori. Feddema <ref> [38] </ref> uses a feature space trajectory generator to interpolate feature parameter values due to the low update rate of the vision system used. 1.6 Image Feature Extraction and Tracking Irrespective of the control approach used, a vision system is required to extract the information needed to perform the servoing task. <p> Hence, visual servoing pre-supposes the solution to a set of potentially difficult static and dynamic vision problems. To this end many reported implementations contrive the vision problem to be simple: e:g: painting objects white, using artificial targets, and so forth <ref> [54, 38, 12, 4] </ref>. Other authors use extremely task-specific clues: e:g: Allen [53] uses motion detection for locating a moving object to be grasped, and welding systems commonly use special filters that isolate the image of the welding tip.
Reference: [39] <author> C. Lu, E. J. Mjolsness, and G. D. Hager, </author> <title> "Online computation of exterior orientation with application to hand-eye calibration," </title> <institution> DCS RR-1046, Yale University, </institution> <address> New Haven, CT, </address> <month> Aug. </month> <year> 1994. </year> <note> To appear in Mathematical and Computer Modeling. </note>
Reference-contexts: Numerous methods of solution have been proposed and <ref> [39] </ref> provides a recent review of several techniques. Broadly speaking, solutions divide into analytic solutions and least-squares solutions which employ a variety of - 18 - Tutorial TT3: Visual Servo Control Hager/Hutchinson/Corke simplifications and/or iterative methods. <p> Computation time for the relative orientation problem is often cited as a disadvantage of position-based methods. However recent results show that solutions can be computed in only a few milliseconds even using iteration <ref> [39] </ref> or Kalman filtering [37]. - 20 - Tutorial TT3: Visual Servo Control Hager/Hutchinson/Corke Endpoint closed-loop systems are demonstrably less sensitive to calibration. However, particularly in stereo systems, small rotational errors between the cameras can lead to reconstruction errors which do impact the positioning accuracy of the system.
Reference: [40] <author> M. A. Fischler and R. C. Bolles, </author> <title> "Random sample consensus: a paradigm for model fitting with applicatio ns to image analysis and automated cartography," </title> <journal> Communications of the ACM, </journal> <volume> vol. 24, </volume> <pages> pp. 381-395, </pages> <month> June </month> <year> 1981. </year>
Reference-contexts: Broadly speaking, solutions divide into analytic solutions and least-squares solutions which employ a variety of - 18 - Tutorial TT3: Visual Servo Control Hager/Hutchinson/Corke simplifications and/or iterative methods. Analytic solutions for three and four points are given by <ref> [40, 41, 42, 43, 44] </ref>. Unique solutions exist for four coplanar, but not collinear, points. Least-squares solutions can be found in [45, 46, 47, 48, 49, 50, 51]. Six or more points always yield unique solutions.
Reference: [41] <author> R. M. Haralick, C. Lee, K. Ottenberg, and M. Nolle, </author> <title> "Analysis and solutions of the three point perspective pose estimation problem," </title> <booktitle> in Proc. IEEE Conf. Computer Vision Pat. Rec., </booktitle> <pages> pp. 592-598, </pages> <year> 1991. </year>
Reference-contexts: Broadly speaking, solutions divide into analytic solutions and least-squares solutions which employ a variety of - 18 - Tutorial TT3: Visual Servo Control Hager/Hutchinson/Corke simplifications and/or iterative methods. Analytic solutions for three and four points are given by <ref> [40, 41, 42, 43, 44] </ref>. Unique solutions exist for four coplanar, but not collinear, points. Least-squares solutions can be found in [45, 46, 47, 48, 49, 50, 51]. Six or more points always yield unique solutions.
Reference: [42] <author> D. DeMenthon and L. S. Davis, </author> <title> "Exact and approximate solutions of the perspective-three-point problem," </title> <journal> IEEE Trans. Pat. Anal. Machine Intell., </journal> <volume> no. 11, </volume> <pages> pp. 1100-1105, </pages> <year> 1992. </year> <title> Tutorial TT3: Visual Servo Control Hager/Hutchinson/Corke </title>
Reference-contexts: Broadly speaking, solutions divide into analytic solutions and least-squares solutions which employ a variety of - 18 - Tutorial TT3: Visual Servo Control Hager/Hutchinson/Corke simplifications and/or iterative methods. Analytic solutions for three and four points are given by <ref> [40, 41, 42, 43, 44] </ref>. Unique solutions exist for four coplanar, but not collinear, points. Least-squares solutions can be found in [45, 46, 47, 48, 49, 50, 51]. Six or more points always yield unique solutions.
Reference: [43] <author> R. Horaud, B. Canio, and O. Leboullenx, </author> <title> "An analytic solution for the perspective 4-point problem," </title> <journal> Computer Vis. Graphics. Image Process, </journal> <volume> no. 1, </volume> <pages> pp. 33-44, </pages> <year> 1989. </year>
Reference-contexts: Broadly speaking, solutions divide into analytic solutions and least-squares solutions which employ a variety of - 18 - Tutorial TT3: Visual Servo Control Hager/Hutchinson/Corke simplifications and/or iterative methods. Analytic solutions for three and four points are given by <ref> [40, 41, 42, 43, 44] </ref>. Unique solutions exist for four coplanar, but not collinear, points. Least-squares solutions can be found in [45, 46, 47, 48, 49, 50, 51]. Six or more points always yield unique solutions.
Reference: [44] <author> M. Dhome, M. Richetin, J. Lapreste, and G. Rives, </author> <title> "Determination of the attitude of 3-D objects from a single perspective view," </title> <journal> IEEE Trans. Pat. Anal. Machine Intell., </journal> <volume> no. 12, </volume> <pages> pp. 1265-1278, </pages> <year> 1989. </year>
Reference-contexts: Broadly speaking, solutions divide into analytic solutions and least-squares solutions which employ a variety of - 18 - Tutorial TT3: Visual Servo Control Hager/Hutchinson/Corke simplifications and/or iterative methods. Analytic solutions for three and four points are given by <ref> [40, 41, 42, 43, 44] </ref>. Unique solutions exist for four coplanar, but not collinear, points. Least-squares solutions can be found in [45, 46, 47, 48, 49, 50, 51]. Six or more points always yield unique solutions.
Reference: [45] <author> G. H. </author> <title> Rosenfield, "The problem of exterior orientation in photogrammetry," </title> <booktitle> Photogrammetric Engineering, </booktitle> <pages> pp. 536-553, </pages> <year> 1959. </year>
Reference-contexts: Analytic solutions for three and four points are given by [40, 41, 42, 43, 44]. Unique solutions exist for four coplanar, but not collinear, points. Least-squares solutions can be found in <ref> [45, 46, 47, 48, 49, 50, 51] </ref>. Six or more points always yield unique solutions. The camera calibration matrix can be computed from features on the target, then decomposed [49] to yield the target's pose. The least-squares solution proceeds as follows.
Reference: [46] <author> D. G. Lowe, </author> <title> "Fitting parametrized three-dimensional models to images," </title> <journal> IEEE Trans. Pat. Anal. Machine Intell., </journal> <volume> no. 5, </volume> <pages> pp. 441-450, </pages> <year> 1991. </year>
Reference-contexts: Analytic solutions for three and four points are given by [40, 41, 42, 43, 44]. Unique solutions exist for four coplanar, but not collinear, points. Least-squares solutions can be found in <ref> [45, 46, 47, 48, 49, 50, 51] </ref>. Six or more points always yield unique solutions. The camera calibration matrix can be computed from features on the target, then decomposed [49] to yield the target's pose. The least-squares solution proceeds as follows.
Reference: [47] <author> R. Goldberg, </author> <title> "Constrained pose refinement of parametric objects," </title> <journal> Intl. J. Computer Vision, </journal> <volume> no. 2, </volume> <pages> pp. 181-211, </pages> <year> 1994. </year>
Reference-contexts: Analytic solutions for three and four points are given by [40, 41, 42, 43, 44]. Unique solutions exist for four coplanar, but not collinear, points. Least-squares solutions can be found in <ref> [45, 46, 47, 48, 49, 50, 51] </ref>. Six or more points always yield unique solutions. The camera calibration matrix can be computed from features on the target, then decomposed [49] to yield the target's pose. The least-squares solution proceeds as follows.
Reference: [48] <author> R. Kumar, </author> <title> "Robust methods for estimating pose and a sensitivity analysis," CVGIP: </title> <booktitle> Image Understanding, </booktitle> <volume> no. 3, </volume> <pages> pp. 313-342, </pages> <year> 1994. </year>
Reference-contexts: Image feature parameters that have been used for visual servo control include the image plane coordinates of points in the image <ref> [12, 10, 48, 14, 15, 93, 17] </ref>, the distance between two points in the image plane and the orientation of the line connecting those two points [38, 36], perceived edge length [19], the area of a projected surface and the relative areas of two projected surfaces [19], the centroid and higher <p> Analytic solutions for three and four points are given by [40, 41, 42, 43, 44]. Unique solutions exist for four coplanar, but not collinear, points. Least-squares solutions can be found in <ref> [45, 46, 47, 48, 49, 50, 51] </ref>. Six or more points always yield unique solutions. The camera calibration matrix can be computed from features on the target, then decomposed [49] to yield the target's pose. The least-squares solution proceeds as follows. <p> The image Jacobian was first introduced by Weiss et al. [19], who referred to it as the feature sensitivity matrix. It is also referred to as the interaction matrix [10] and the B matrix [14, 15]. Other applications of the image Jacobian include <ref> [12, 38, 48, 22] </ref>. The relationship given by (1.35) describes how image feature parameters change with respect to changing manipulator pose. <p> For the case of a non-square image Jacobian, the techniques described in Section 1.5.3 would be used to compute for u. Similar results have been presented in <ref> [48, 12] </ref>. 1.5.5 Example Servoing Tasks In this section, we revisit the problems that were described in Section 1.4.1. Here, we describe image-based solutions for these problems. Point to Point Positioning Consider the task of bringing some point P on the manipulator to a desired stationing point S.
Reference: [49] <author> S. Ganapathy, </author> <title> "Decomposition of transformation matrices for robot vision," </title> <journal> Pattern Recognition Letters, </journal> <pages> pp. 401-412, </pages> <year> 1989. </year>
Reference-contexts: Analytic solutions for three and four points are given by [40, 41, 42, 43, 44]. Unique solutions exist for four coplanar, but not collinear, points. Least-squares solutions can be found in <ref> [45, 46, 47, 48, 49, 50, 51] </ref>. Six or more points always yield unique solutions. The camera calibration matrix can be computed from features on the target, then decomposed [49] to yield the target's pose. The least-squares solution proceeds as follows. <p> Unique solutions exist for four coplanar, but not collinear, points. Least-squares solutions can be found in [45, 46, 47, 48, 49, 50, 51]. Six or more points always yield unique solutions. The camera calibration matrix can be computed from features on the target, then decomposed <ref> [49] </ref> to yield the target's pose. The least-squares solution proceeds as follows.
Reference: [50] <author> M. Fischler and R. C. Bolles, </author> <title> "Random sample consensus: A paradigm for model fitting and automatic cartography," </title> <journal> Commun. ACM, </journal> <volume> no. 6, </volume> <pages> pp. 381-395, </pages> <year> 1981. </year>
Reference-contexts: Analytic solutions for three and four points are given by [40, 41, 42, 43, 44]. Unique solutions exist for four coplanar, but not collinear, points. Least-squares solutions can be found in <ref> [45, 46, 47, 48, 49, 50, 51] </ref>. Six or more points always yield unique solutions. The camera calibration matrix can be computed from features on the target, then decomposed [49] to yield the target's pose. The least-squares solution proceeds as follows.
Reference: [51] <author> Y. Liu, T. S. Huang, and O. D. Faugeras, </author> <title> "Determination of camera location from 2-D to 3-D line and point correspondences," </title> <journal> IEEE Trans. Pat. Anal. Machine Intell., </journal> <volume> no. 1, </volume> <pages> pp. 28-37, </pages> <year> 1990. </year>
Reference-contexts: Taken to the extreme, machine vision can provide closed-loop position control for a robot end-effector | this is referred to as visual servoing. This term appears to have been first introduced by Hill and Park <ref> [51] </ref> in 1979 to distinguish their approach from earlier `blocks world' experiments where the system alternated between picture taking and moving. Prior to the introduction of this term, the less specific term visual feedback was generally used. <p> Analytic solutions for three and four points are given by [40, 41, 42, 43, 44]. Unique solutions exist for four coplanar, but not collinear, points. Least-squares solutions can be found in <ref> [45, 46, 47, 48, 49, 50, 51] </ref>. Six or more points always yield unique solutions. The camera calibration matrix can be computed from features on the target, then decomposed [49] to yield the target's pose. The least-squares solution proceeds as follows.
Reference: [52] <author> A. Gelb, ed., </author> <title> Applied Optimal Estimation. </title> <address> Cambridge, MA: </address> <publisher> MIT Press, </publisher> <year> 1974. </year>
Reference-contexts: A particularly elegant formulation of this updating procedure results by application of statistical techniques such as the extended Kalman filter <ref> [52] </ref>. The reader is referred to [37] for details. Multiple Cameras Many systems utilizing position-based control with stereo vision from free-standing cameras have been demonstrated. For example, Allen [53] shows a system which can grasp a toy train using stereo vision.
Reference: [53] <author> P. K. Allen, A. Timcenko, B. Yoshimi, and P. Michelman, </author> <title> "Automated Tracking and Grasping of a Moving Object with a Robotic Hand-Eye System," </title> <journal> IEEE Transactions on Robotics and Automation, </journal> <volume> vol. 9, no. 2, </volume> <pages> pp. 152-165, </pages> <year> 1993. </year>
Reference-contexts: A particularly elegant formulation of this updating procedure results by application of statistical techniques such as the extended Kalman filter [52]. The reader is referred to [37] for details. Multiple Cameras Many systems utilizing position-based control with stereo vision from free-standing cameras have been demonstrated. For example, Allen <ref> [53] </ref> shows a system which can grasp a toy train using stereo vision. Rizzi [54] demonstrates a system which can bounce a ping-pong ball. All of these systems are EOL. Cipolla [31] describes an ECL system using free-standing stereo cameras. <p> To this end many reported implementations contrive the vision problem to be simple: e:g: painting objects white, using artificial targets, and so forth [54, 38, 12, 4]. Other authors use extremely task-specific clues: e:g: Allen <ref> [53] </ref> uses motion detection for locating a moving object to be grasped, and welding systems commonly use special filters that isolate the image of the welding tip. A review of tracking approaches used by researchers in this field is given in [3]. <p> For example, Rizzi [54] describes the use of a Newtonian flight dynamics model to make it possible to track a ping-pong ball during flight. Predictors based on ff fi tracking filters and Kalman filters have also been used <ref> [53, 37, 72] </ref>. Multiresolution techniques can be used provide further performance improvements, particularly when a dynamic model is not available and large search windows must be used. 1.6.4 Discussion Prior to executing or planning visually controlled motions, a specific set of visual features must be chosen.
Reference: [54] <author> A. Rizzi and D. Koditschek, </author> <title> "An active visual estimator for dexterous manipulation," </title> <booktitle> in Proceedings, IEEE International Conference on Robotics and Automaton, </booktitle> <year> 1994. </year>
Reference-contexts: The reader is referred to [37] for details. Multiple Cameras Many systems utilizing position-based control with stereo vision from free-standing cameras have been demonstrated. For example, Allen [53] shows a system which can grasp a toy train using stereo vision. Rizzi <ref> [54] </ref> demonstrates a system which can bounce a ping-pong ball. All of these systems are EOL. Cipolla [31] describes an ECL system using free-standing stereo cameras. One novel feature of this system is the use of the affine projection model (Section 1.2.3) for the imaging geometry. <p> Hence, visual servoing pre-supposes the solution to a set of potentially difficult static and dynamic vision problems. To this end many reported implementations contrive the vision problem to be simple: e:g: painting objects white, using artificial targets, and so forth <ref> [54, 38, 12, 4] </ref>. Other authors use extremely task-specific clues: e:g: Allen [53] uses motion detection for locating a moving object to be grasped, and welding systems commonly use special filters that isolate the image of the welding tip. <p> However not all pixels in the image are of interest, and computation time can be greatly reduced if only a small region around each image feature is processed. Thus, a promising technique for making vision cheap and tractable is to use window-based tracking techniques <ref> [70, 54, 31] </ref>. Window-based methods have several advantages, among them; computational simplicity, little requirement for special hardware, and easy reconfiguration for different applications. <p> For example, Rizzi <ref> [54] </ref> describes the use of a Newtonian flight dynamics model to make it possible to track a ping-pong ball during flight. Predictors based on ff fi tracking filters and Kalman filters have also been used [53, 37, 72]. <p> If the contrast between the interior of the opening and area around it is high, then binary thresholding followed by a calculation of the first and second central moments can be used to localize the feature <ref> [54] </ref>. 2. If the ambient illumination changes greatly over time, but the brightness of the opening and the brightness of the surrounding region are roughly constant, a circular template could be localized using SSD methods augmented with brightness and contrast parameters. <p> The two most common problems are occlusion of features and and visual singularities. Solutions to the former include intelligent observers that note the disappearance of features and continue to predict their locations based on dynamics and/or feedforward information <ref> [54] </ref>, or redundant feature specifications that can perform even with some loss of information. Solution to the latter require some combination of intelligent path planning and/or intelligent acquisition and focus-of-attention to maintain the controllability of the system.
Reference: [55] <author> J. Pretlove and G. Parker, </author> <title> "The development of a real-time stereo-vision system to aid robot guidance in carrying out a typical manufacturing task," </title> <booktitle> in Proc. 22nd ISRR, </booktitle> <address> (Detroit), </address> <pages> pp. </pages> <address> 21.1-21.23, </address> <year> 1991. </year>
Reference: [56] <author> B. K. P. Horn, H. M. Hilden, and S. Negahdaripour, </author> <title> "Closed-form solution of absolute orientation using orthonomal matrices," </title> <journal> J. Opt. Soc. Amer., </journal> <volume> vol. A-5, </volume> <pages> pp. 1127-1135, 198. </pages>
Reference-contexts: we have c x t ffi ( t P i t C) ( c b P i c C) = ( c R t Note that the final expression depends only on c R t : The corresponding least-squares problem can either be solved explicitly for c R t (see <ref> [56, 57, 58] </ref>), or solved incrementally using linearization.
Reference: [57] <author> K. S. Arun, T. S. Huang, and S. D. Blostein, </author> <title> "Least-squares fitting of two 3-D point sets," </title> <journal> IEEE Trans. Pat. Anal. Machine Intell., </journal> <volume> vol. 9, </volume> <pages> pp. 698-700, </pages> <year> 1987. </year>
Reference-contexts: we have c x t ffi ( t P i t C) ( c b P i c C) = ( c R t Note that the final expression depends only on c R t : The corresponding least-squares problem can either be solved explicitly for c R t (see <ref> [56, 57, 58] </ref>), or solved incrementally using linearization.
Reference: [58] <author> B. K. P. Horn, </author> <title> "Closed-form solution of absolute orientation using unit quater-nion," </title> <journal> J. Opt. Soc. Amer., </journal> <volume> vol. A-4, </volume> <pages> pp. 629-642, </pages> <year> 1987. </year> - <title> 44 - Tutorial TT3: Visual Servo Control Hager/Hutchinson/Corke </title>
Reference-contexts: we have c x t ffi ( t P i t C) ( c b P i c C) = ( c R t Note that the final expression depends only on c R t : The corresponding least-squares problem can either be solved explicitly for c R t (see <ref> [56, 57, 58] </ref>), or solved incrementally using linearization.
Reference: [59] <author> G. D. Hager, G. Grunwald, and G. Hirzinger, </author> <title> "Feature-based visual servoing and its application to telerobotics," </title> <institution> DCS RR-1010, Yale University, </institution> <address> New Haven, CT, </address> <month> Jan. </month> <year> 1994. </year> <note> To appear at the 1994 IROS Conference. </note>
Reference-contexts: We define an image feature parameter to be any real-valued quantity that can be calculated from one or more image features. Examples include, moments, relationships between regions or vertices, and polygon face areas. Jang <ref> [59] </ref> provides a formal definition of what we term feature paramters as image functionals. Most commonly the coordinates of a feature point or a region centroid are used. <p> However, it still may cause problems when both cameras are free to move relative to one another. Feature-based approaches tend to be more appropriate to tasks where there is no prior model of the geometry of the task, for example in teleoperation applications <ref> [59] </ref>. Pose-based approaches inherently depend on an existing object model. The pose estimation problems inherent in many position-based servoing problems requires solution to a potentially difficult correspondence problem.
Reference: [60] <author> G. Agin, </author> <title> "Calibration and use of a light stripe range sensor mounted on the hand of a robot," </title> <booktitle> in Proc. IEEE Int. Conf. Robotics and Automation, </booktitle> <pages> pp. 680-685, </pages> <year> 1985. </year>
Reference: [61] <author> S. Venkatesan and C. Archibald, </author> <title> "Realtime tracking in five degrees of freedom using two wrist-mounted laser range finders," </title> <booktitle> in Proc. IEEE Int. Conf. Robotics and Automation, </booktitle> <pages> pp. 2004-2010, </pages> <year> 1990. </year>
Reference: [62] <author> J. Dietrich, G. Hirzinger, B. Gombert, and J. Schott, </author> <title> "On a unified concept for a new generation of light-weight robots," in Experimental Robotics 1 (V. </title> <editor> Hay-ward and O. Khatib, eds.), </editor> <volume> vol. </volume> <booktitle> 139 of Lecture Notes in Control and Information Sciences, </booktitle> <pages> pp. 287-295, </pages> <publisher> Springer-Verlag, </publisher> <year> 1989. </year>
Reference: [63] <author> J. Aloimonos and D. P. Tsakiris, </author> <title> "On the mathematics of visual tracking," </title> <journal> Image and Vision Computing, </journal> <volume> vol. 9, </volume> <pages> pp. 235-251, </pages> <month> Aug. </month> <year> 1991. </year>
Reference-contexts: Alternative derivations for this example can be found in a number of references including <ref> [63, 64] </ref>.
Reference: [64] <author> R. M. Haralick and L. G. Shapiro, </author> <title> Computer and Robot Vision. </title> <publisher> Addison Wesley, </publisher> <year> 1993. </year>
Reference-contexts: Alternative derivations for this example can be found in a number of references including <ref> [63, 64] </ref>. <p> In this case, the solution is given by (1.47). For example, as shown in <ref> [64] </ref>, the null space of the image Jacobian given in (1.45), is spanned by the four vectors 2 6 6 6 6 u 0 3 7 7 7 7 2 6 6 6 6 0 0 v 3 7 7 7 7 2 6 6 6 6 uvz vz 0 3 <p> A window, R (x) = R (x; c ; ; t); extracted with these parameters would then have a vertical edge segment within it. Isolated step edges can be localized by determining the location of the maximum of the first derivative of the signal <ref> [64, 72, 74] </ref>. However, since derivatives tend to increase the noise in an image, most edge detection methods combine spatial derivatives with a smoothing operation to suppress spurious maxima. Both derivatives and smoothing are linear operations that can be computed using convolution operators. <p> Another edge detector which can be implemented without floating point arithmetic is the derivative of a triangle (DOT) kernal. In one dimension the DOT is defined as g (x) = signum (x): For a kernal three pixels wide, this is also known as the Prewitt operator <ref> [64] </ref>. Although the latter is not optimal from a signal processing point of view, convolution by the DOT can be implemented using only four additions per pixel. Thus, it is extremely fast to execute on simple hardware. Returning to detecting edge segments, convolutions are employed as follows.
Reference: [65] <author> F. W. Warner, </author> <title> Foundations of Differentiable Manifolds and Lie Groups. </title> <address> New York: </address> <publisher> Springer-Verlag, </publisher> <year> 1983. </year>
Reference-contexts: The least squares solution gives a value for _ r that minimizes the norm k _ f J v _ rk. We first consider the case k &gt; m, that is, there are more feature parameters than task degrees of freedom. By the implicit function theorem <ref> [65] </ref>, if, in some neighborhood of r, m k and rank (J v ) = m (i.e., J v is full rank), we can express the coordinates f m+1 : : : f k as smooth functions of f 1 : : : f m . &gt;From this, we deduce that
Reference: [66] <author> G. D. Hager, </author> <title> "Calibration-free visual control using projective invariance," </title> <institution> DCS RR-1046, Yale University, </institution> <address> New Haven, CT, </address> <month> Dec. </month> <year> 1994. </year> <note> To appear Proc. ICCV '95. </note>
Reference-contexts: It is interesting to note that these solutions to the point-to-line problem perform with an accuracy that is independent of calibration, whereas the position-based versions do not <ref> [66] </ref>. 1.5.6 Discussion One of the chief advantages to image-based control over position-based control is that the positioning accuracy of the system is less sensitive camera calibration. This is particularly true for ECL image-based systems. <p> This is particularly true for ECL image-based systems. For example, it is interesting to note that the ECL image-based solutions to the point-to-line positioning problem perform with an accuracy that is independent of calibration, whereas the position-based versions do not <ref> [66] </ref>. It is important to note, however, that most of the image-based control methods appearing in the literature still rely on an estimate of point position or target pose to parameterize the Jacobian. In practice, the unknown parameter for Jacobian calculation is distance from the camera.
Reference: [67] <author> D. Kim, A. Rizzi, G. Hager, and D. Koditschek, </author> <title> "A "robust" convergent visual servoing system." </title> <note> Submitted to Intelligent Robots and Systems 1995, </note> <year> 1994. </year>
Reference-contexts: In practice, the unknown parameter for Jacobian calculation is distance from the camera. Some recent papers present adaptive approaches for estimating [14] this depth value, or develop feedback methods which do not use depth in the feedback formulation <ref> [67] </ref>. There are often computational advantages to image-based control, particularly in ECL configurations. For example, a position-based relative pose solution for an ECL single-camera system must perform two nonlinear least squares optimizations in order to compute the error function.
Reference: [68] <author> W. Jang and Z. </author> <title> Bien, "Feature-based visual servoing of an eye-in-hand robot with improved tracking performance," </title> <booktitle> in Proc. IEEE Int. Conf. Robotics and Automation, </booktitle> <pages> pp. 2254-2260, </pages> <year> 1991. </year>
Reference: [69] <author> R. L. Anderson, </author> <title> "Dynamic sensing in a ping-pong playing robot," </title> <journal> IEEE Transaction on Robotics and Automation, </journal> <volume> vol. 5, no. 6, </volume> <pages> pp. 723-739, </pages> <year> 1989. </year>
Reference: [70] <author> G. D. Hager, </author> <title> "The "X-Vision" system: A general purpose substrate for real-time vision-based robotics." </title> <booktitle> Submitted to the 1995 Workshop on Vision for Robotics, </booktitle> <month> Feb. </month> <year> 1995. </year>
Reference-contexts: However not all pixels in the image are of interest, and computation time can be greatly reduced if only a small region around each image feature is processed. Thus, a promising technique for making vision cheap and tractable is to use window-based tracking techniques <ref> [70, 54, 31] </ref>. Window-based methods have several advantages, among them; computational simplicity, little requirement for special hardware, and easy reconfiguration for different applications. <p> In this case, (1.59) must also include parameters for scaling and aspect ratio <ref> [70] </ref>. 3. The opening could be selected in an initial image, and subsequently located using SSD methods. This differs from the previous method in that this calculation does not compute the center of the opening, only its correlation with the starting image. <p> It is probably safe to say that image processing presents the greatest challenge to general-purpose hand-eye coordination. As an effort to help overcome this obstacle, the methods described above and other related methods have been incorporated into a publically available "toolkit." The interested reader is referred to <ref> [70] </ref> for details. 1.7 Related Issues In this section, we briefly discuss a number of related issues that were not addressed in the tutorial. 1.7.1 Image-Based versus Position-Based Control The taxonomy of visual servo introduced in Section 1.1 has four major architectural classes.
Reference: [71] <author> E. Dickmanns and V. Graefe, </author> <title> "Dynamic monocular machine vision," </title> <journal> Machine Vision and Applications, </journal> <volume> vol. 1, </volume> <pages> pp. 223-240, </pages> <year> 1988. </year>
Reference: [72] <author> O. Faugeras, </author> <title> Three-Dimensional Computer Vision. </title> <address> Cambridge, MA: </address> <publisher> MIT Press, </publisher> <year> 1993. </year>
Reference-contexts: In keeping with the minimalist approach of this tutorial, we concentrate on describing the window-based approach to tracking of features in an image. A discussion of methods which use specialized hardware combined with temporal and geometric constraints can be found in <ref> [72] </ref>. The remainder of this section is organized as follows. Section 1.6.1 describes how window-based methods can be used to implement fast detection of edge segments, a common low-level primitive for vision applications. Section 1.6.2 describe an approach based on temporally correlating image regions over time. <p> A window, R (x) = R (x; c ; ; t); extracted with these parameters would then have a vertical edge segment within it. Isolated step edges can be localized by determining the location of the maximum of the first derivative of the signal <ref> [64, 72, 74] </ref>. However, since derivatives tend to increase the noise in an image, most edge detection methods combine spatial derivatives with a smoothing operation to suppress spurious maxima. Both derivatives and smoothing are linear operations that can be computed using convolution operators. <p> For example, Rizzi [54] describes the use of a Newtonian flight dynamics model to make it possible to track a ping-pong ball during flight. Predictors based on ff fi tracking filters and Kalman filters have also been used <ref> [53, 37, 72] </ref>. Multiresolution techniques can be used provide further performance improvements, particularly when a dynamic model is not available and large search windows must be used. 1.6.4 Discussion Prior to executing or planning visually controlled motions, a specific set of visual features must be chosen.
Reference: [73] <author> J. Foley, A. van Dam, S. Feiner, and J. Hughes, </author> <title> Computer Graphics. </title> <publisher> Addison Wesley, </publisher> <year> 1993. </year>
Reference: [74] <author> D. Ballard and C. Brown, </author> <title> Computer Vision. </title> <address> Englewood Cliffs, NJ: </address> <publisher> Prentice-Hall, </publisher> <year> 1982. </year> <title> Tutorial TT3: Visual Servo Control Hager/Hutchinson/Corke </title>
Reference-contexts: A window, R (x) = R (x; c ; ; t); extracted with these parameters would then have a vertical edge segment within it. Isolated step edges can be localized by determining the location of the maximum of the first derivative of the signal <ref> [64, 72, 74] </ref>. However, since derivatives tend to increase the noise in an image, most edge detection methods combine spatial derivatives with a smoothing operation to suppress spurious maxima. Both derivatives and smoothing are linear operations that can be computed using convolution operators.
Reference: [75] <author> J. Canny, </author> <title> "A computational approach to edge detection," </title> <journal> IEEE Trans. Pattern Anal. Mach. Intell., </journal> <pages> pp. 679-98, </pages> <month> Nov. </month> <year> 1986. </year>
Reference-contexts: Hence, smoothing and differentiation can be combined into a single convolution template. An extremely popular convolution kernel is the derivative of a Gaussian (DOG) <ref> [75] </ref>. In one dimension, the DOG is defined as g (x) = x exp (x 2 = 2 ) where is a design parameter governing the amount of smoothing that takes place. Although the DOG has been demonstrated to be the optimal filter for detecting step edges [75], it requires floating <p> a Gaussian (DOG) <ref> [75] </ref>. In one dimension, the DOG is defined as g (x) = x exp (x 2 = 2 ) where is a design parameter governing the amount of smoothing that takes place. Although the DOG has been demonstrated to be the optimal filter for detecting step edges [75], it requires floating point arithmetic to be computed accurately. Another edge detector which can be implemented without floating point arithmetic is the derivative of a triangle (DOT) kernal.
Reference: [76] <author> B. D. Lucas and T. Kanade, </author> <title> "An iterative image registration technique with an application to stereo vision," </title> <booktitle> in Proc. International Joint Conference on Artificial Intelligence, </booktitle> <pages> pp. 674-679, </pages> <year> 1981. </year>
Reference: [77] <author> P. Anandan, </author> <title> "A computational framework and an algorithm for the measurement of structure from motion," </title> <journal> International Journal of Computer Vision, </journal> <volume> vol. 2, </volume> <pages> pp. 283-310, </pages> <year> 1989. </year>
Reference: [78] <author> J. Shi and C. Tomasi, </author> <title> "Good features to track," </title> <booktitle> in Proc. IEEE Computer Society Conference on Computer Vision and Pattern Recognition, </booktitle> <pages> pp. 593-600, </pages> <publisher> IEEE Computer Society Press, </publisher> <year> 1994. </year>
Reference: [79] <author> J. Huang and G. D. Hager, </author> <title> "Tracking tools for vision-based navigation," </title> <institution> DCS RR-1046, Yale University, </institution> <address> New Haven, CT, </address> <month> Dec. </month> <year> 1994. </year> <note> Submitted to IROS '95. </note>
Reference: [80] <author> M. Kass, A. Witkin, and D. Terzopoulos, "Snakes: </author> <title> active contour models," </title> <journal> International journal of Computer Vision, </journal> <volume> vol. 1, no. 1, </volume> <pages> pp. 321-331, </pages> <year> 1987. </year>
Reference-contexts: Such situations call for the use of more global task constraints (e:g: the geometry of several edges), more global tracking (e:g: extended contours or snakes <ref> [80] </ref>), or improved or specialized detection methods. To illustrate these tradeoffs, suppose a visual servoing task relies on tracking the image of a circular opening over time. In general, the opening will project to an ellipse in the camera.
Reference: [81] <author> B. Bishop, S. A. Hutchinson, and M. W. Spong, </author> <title> "Camera modelling for visual servo control applications," </title> <booktitle> Mathematical and Computer Modelling Special issue on Modelling Issues in Visual Sensing. </booktitle>
Reference-contexts: One novel feature of this system is the use of the affine projection model (Section 1.2.3) for the imaging geometry. This leads to linear calibration and control at the cost of some system performance. The development of a position-based stereo eye-in-hand servoing system has also been reported <ref> [81] </ref>. Multiple cameras greatly simplify the reconstruction process as illustrated below. <p> Weiss's proposed image-based direct visual-servoing structure does away entirely with axis sensors | dynamics and kinematics are controlled adaptively based on visual feature data. This concept has a certain appeal but in practice is overly complex to implement and appears to lack robustness (see, e.g., <ref> [81] </ref> for an analysis of the effects of various image distortions on such control schemes). The concepts have only ever been demonstrated in simulation for up to 3-DOF and then with simplistic models of axis dynamics which ignore `real world' effects such as Coulomb friction and stiction.
Reference: [82] <author> P. Corke and M. </author> <title> Good, "Dynamic effects in visual closed-loop systems," </title> <journal> Submitted to IEEE Transactions on Robotics and Automation, </journal> <year> 1995. </year>
Reference-contexts: Other issues for consideration include whether or not the vision system should `close the loop' around robot axes which are position, velocity or torque controlled. A detailed discussion of these dynamic issues in visual servo systems is given by Corke <ref> [20, 82] </ref>. - 38 - Tutorial TT3: Visual Servo Control Hager/Hutchinson/Corke 1.7.3 Mobile robots The discussion above has assumed that the moving camera is mounted on an arm type robot manipulator.
Reference: [83] <author> S. B. Skaar, Y. Yalda-Mooshabad, and W. H. Brockman, </author> <title> "Nonholonomic camera-space manipulation," </title> <journal> IEEE Transactions on Robotics and Automation, </journal> <volume> vol. 8, </volume> <pages> pp. 464-479, </pages> <month> Aug. </month> <year> 1992. </year>
Reference-contexts: Most of the techniques described above are directly applicable to the mobile robot case. Visual servoing can be used for navigation with respect to landmarks or obstacle and to control docking (see, e.g., <ref> [83] </ref>). 1.7.4 A Light-Weight Tracking and Servoing Environment The design of many task-specific visual tracking and vision-based feedback systems used in visual servoing places a strong emphasis on system modularity and reconfig-urability. This has motivated the development of a modular, software-based visual tracking system for experimental vision-based robotic applications [84]. <p> These include control issues, such as adaptive visual servo control [85, 14], hybrid control (e.g., hybrid vision/position control [12], or hybrid force/vision control), and multi-rate system theory [86]; issues related to automatic planning of visually controlled robot motions [87, 88]; applications in mobile robotics, including non-holonmoic systems <ref> [83] </ref>; and, feature selection [33, 36]. Many of these are describe in the proceedings of a recent workshop on visual servo control [89]. - 39 - Tutorial TT3: Visual Servo Control Hager/Hutchinson/Corke 1.7.6 The future The future for applications of visual servoing should be bright.
Reference: [84] <author> G. D. Hager, S. Puri, and K. Toyama, </author> <title> "A framework for real-time vision-based tracking using off-the-shelf hardware," </title> <institution> DCS RR-988, Yale University, </institution> <address> New Haven, CT, </address> <month> Sept. </month> <year> 1993. </year>
Reference-contexts: This has motivated the development of a modular, software-based visual tracking system for experimental vision-based robotic applications <ref> [84] </ref>. The system design emphasizes flexibility and efficiency on standard scientific workstations and PC's. The system is intended to be a portable, inexpensive tool for rapid prototyping and experimentation for teaching and research. The system is written as a set of classes in C++.
Reference: [85] <author> A. C. Sanderson and L. E. Weiss, </author> <title> "Adaptive visual servo control of robots," in Robot Vision (A. Pugh, </title> <publisher> ed.), </publisher> <pages> pp. 107-116, IFS, </pages> <year> 1983. </year>
Reference-contexts: These include control issues, such as adaptive visual servo control <ref> [85, 14] </ref>, hybrid control (e.g., hybrid vision/position control [12], or hybrid force/vision control), and multi-rate system theory [86]; issues related to automatic planning of visually controlled robot motions [87, 88]; applications in mobile robotics, including non-holonmoic systems [83]; and, feature selection [33, 36].
Reference: [86] <author> N. Mahadevamurty, T.-C. Tsao, and S. Hutchinson, </author> <title> "Multi-rate analysis and design of visual feedback digital servo control systems," ASME Journal of Dynamic Systems, </title> <booktitle> Measurement and Control, </booktitle> <pages> pp. 45-55, </pages> <month> Mar. </month> <year> 1994. </year>
Reference-contexts: These include control issues, such as adaptive visual servo control [85, 14], hybrid control (e.g., hybrid vision/position control [12], or hybrid force/vision control), and multi-rate system theory <ref> [86] </ref>; issues related to automatic planning of visually controlled robot motions [87, 88]; applications in mobile robotics, including non-holonmoic systems [83]; and, feature selection [33, 36].
Reference: [87] <author> R. Sharma and S. A. Hutchinson, </author> <title> "On the observability of robot motion under active camera control," </title> <booktitle> in Proc. IEEE International Conference on Robotics and Automation, </booktitle> <pages> pp. 162-167, </pages> <month> May </month> <year> 1994. </year>
Reference-contexts: These include control issues, such as adaptive visual servo control [85, 14], hybrid control (e.g., hybrid vision/position control [12], or hybrid force/vision control), and multi-rate system theory [86]; issues related to automatic planning of visually controlled robot motions <ref> [87, 88] </ref>; applications in mobile robotics, including non-holonmoic systems [83]; and, feature selection [33, 36].
Reference: [88] <author> A. Fox and S. Hutchinson, </author> <title> "Exploiting visual constraints in the synthesis of uncertainty-tolerant motion plans," </title> <journal> IEEE Transactions on Robotics and Automation, </journal> <volume> vol. 11, </volume> <pages> pp. 56-71, </pages> <year> 1995. </year>
Reference-contexts: These include control issues, such as adaptive visual servo control [85, 14], hybrid control (e.g., hybrid vision/position control [12], or hybrid force/vision control), and multi-rate system theory [86]; issues related to automatic planning of visually controlled robot motions <ref> [87, 88] </ref>; applications in mobile robotics, including non-holonmoic systems [83]; and, feature selection [33, 36].

Reference: [1] <author> P. Adsit. </author> <title> Real-Time Intelligent Control of a Vision-Servoed Fruit-Picking Robot. </title> <type> PhD thesis, </type> <institution> University of Florida, </institution> <year> 1989. </year>
Reference: [2] <author> G. Agin. </author> <title> Calibration and use of a light stripe range sensor mounted on the hand of a robot. </title> <booktitle> In Proc. IEEE Int. Conf. Robotics and Automation, </booktitle> <pages> pages 680-685, </pages> <year> 1985. </year>
Reference-contexts: Many of these problems can be circumvented by sensing target pose directly using a 3D sensor. Active 3D sensors based on structured lighting are now compact and fast enough to use for visual servoing. If the sensor is small and mounted on the robot <ref> [2, 101, 33] </ref> the depth and orientation information can be used for position-based visual servoing. 1.5 Image-Based Control As described in Section 1.3, in image-based visual servo control the error signal is defined directly in terms of image feature parameters (in contrast to position-based methods that define the error signal in
Reference: [3] <author> R. Ahluwalia and L. Fogwell. </author> <title> A modular approach to visual servoing. </title> <booktitle> In Proc. IEEE Int. Conf. Robotics and Automation, </booktitle> <pages> pages 943-950, </pages> <year> 1986. </year>
Reference-contexts: A comprehensive review of the literature in this field, as well the history and applications reported to date, is given by Corke <ref> [3] </ref> and includes a large bibliography. Visual servoing is the fusion of results from many elemental areas including high-speed image processing, kinematics, dynamics, control theory, and real-time computing. <p> Other authors use extremely task-specific clues: e:g: Allen [53] uses motion detection for locating a moving object to be grasped, and welding systems commonly use special filters that isolate the image of the welding tip. A review of tracking approaches used by researchers in this field is given in <ref> [3] </ref>. In less structured situations, vision has typically relied on the extraction of sharp contrast changes, referred to as "corners" or "edges", to indicate the presence of object boundaries or surface markings in an image. <p> Even with these improvements, [15] reports that a special signal processor is required to attain frame-rate performance. It is also possible to solve (1.59) using continuous optimization methods <ref> [29, 3, 33, 21] </ref>.
Reference: [4] <author> J. S. Albus. </author> <title> Brains, behavior and robotics. </title> <publisher> Byte Books, </publisher> <year> 1981. </year>
Reference-contexts: The remainder of this article is structured as follows. Section 1.2 establishes a consistent nomenclature and reviews the relevant fundamentals of coordinate transformations, pose representation, and image formation. In Section 1.3, we present a taxonomy of visual servo control systems (adapted from <ref> [4] </ref>). The two major classes of systems, position-based visual servo systems and image-based visual servo systems, are discussed in Sections 1.4 and 1.5 respectively. <p> Calibration is a long standing research issue in the computer vision community (good solutions to the calibration problem can be found in a number of references, e.g., [24, 25, 26]). 1.3 Servoing Architectures In 1980, Sanderson and Weiss <ref> [4] </ref> introduced a taxonomy of visual servo systems, into which all subsequent visual servo systems can be categorized. Their scheme essentially poses two questions: 1. <p> Hence, visual servoing pre-supposes the solution to a set of potentially difficult static and dynamic vision problems. To this end many reported implementations contrive the vision problem to be simple: e:g: painting objects white, using artificial targets, and so forth <ref> [54, 38, 12, 4] </ref>. Other authors use extremely task-specific clues: e:g: Allen [53] uses motion detection for locating a moving object to be grasped, and welding systems commonly use special filters that isolate the image of the welding tip.
Reference: [5] <author> P. K. Allen, A. Timcenko, B. Yoshimi, and P. Michelman. </author> <title> Real-time visual servoing. </title> <booktitle> In Proc. IEEE Int. Conf. Robotics and Automation, </booktitle> <pages> pages 1850-1856, </pages> <year> 1992. </year>
Reference-contexts: Since the task space is merely the configuration space of the robot tool, the task space is a smooth m-manifold (see, e.g., <ref> [5] </ref>). If the tool is a single rigid body moving arbitrarily in a three-dimensional workspace, then T = SE 3 = &lt; 3 fi SO 3 , and m = 6. In some applications, the task space may be restricted to a subspace of SE 3 .
Reference: [6] <author> P. K. Allen, B. Yoshimi, and A. Timcenko. </author> <title> Real-time visual servoing. </title> <booktitle> In Proc. IEEE Int. Conf. Robotics and Automation, </booktitle> <pages> pages 851-856, </pages> <year> 1991. </year>
Reference-contexts: In this case, we often prefer to parameterize a pose using a translation vector and three angles, (e.g., roll, pitch and yaw <ref> [6] </ref>). Although such parameterizations are inherently local, it is often convenient to represent a pose by a vector r 2 &lt; 6 , rather than by x e 2 T . This notation can easily be adapted to the case where T SE 3 . <p> x c ffi c ^ x t : In order to compute a velocity screw, we first note that the rotation matrix e fl R e can be represented as a rotation through an angle e fl e about an axis defined by a unit vector e fl k e <ref> [6] </ref>. Thus, we can define = k 1 e fl ^ k e (1.31) e fl where t e is the origin of the end-effector frame in base coordinates.
Reference: [7] <author> N. Andersen, O. Ravn, and A. Strensen. </author> <title> Real-time vision based control of ser-vomechanical systems. </title> <booktitle> In Proc. 2nd International Symposium on Experimental Robotics, </booktitle> <address> Toulouse, France, </address> <month> June </month> <year> 1991. </year>
Reference-contexts: This is illustrated in - 7 - Tutorial TT3: Visual Servo Control Hager/Hutchinson/Corke Perspective Projection. Assuming that the projective geometry of the camera is modeled by perspective projection (see, e.g., <ref> [7] </ref>), a point, c P = [x; y; z] T , whose coordinates are expressed with respect to the camera coordinate frame, will project onto the image plane with coordinates p = [u; v] T , given by (x; y; z) = u # " y (1.15) If the coordinates of
Reference: [8] <author> C. H. Anderson, P. J. Burt, and G. S. van der Wal. </author> <title> Change detection and tracking using pyramid transform techniques. </title> <booktitle> In Proceeding of SPIE, </booktitle> <volume> volume 579, </volume> <pages> pages 72-78, </pages> <address> Cambridge, Mass., </address> <month> September </month> <year> 1985. </year> <pages> SPIE. </pages>
Reference: [9] <author> R. L. Andersson. </author> <title> Real-time gray-scale video processing using a moment-generating chip. </title> <journal> IEEE Trans. Robot. Autom., </journal> <volume> RA-1(2):79-85, </volume> <month> June </month> <year> 1985. </year>
Reference: [10] <author> R. Andersson. </author> <title> Real Time Expert System to Control a Robot Ping-Pong Player. </title> <type> PhD thesis, </type> <institution> University of Pennsylvania, </institution> <month> June </month> <year> 1987. </year>
Reference-contexts: Most commonly the coordinates of a feature point or a region centroid are used. A good feature point is one that can be located unambiguously in different views of the scene, such as a hole in a gasket [38] or a contrived pattern <ref> [10, 29] </ref>. <p> Image feature parameters that have been used for visual servo control include the image plane coordinates of points in the image <ref> [12, 10, 48, 14, 15, 93, 17] </ref>, the distance between two points in the image plane and the orientation of the line connecting those two points [38, 36], perceived edge length [19], the area of a projected surface and the relative areas of two projected surfaces [19], the centroid and higher <p> of the line connecting those two points [38, 36], perceived edge length [19], the area of a projected surface and the relative areas of two projected surfaces [19], the centroid and higher order moments of a projected surface [20, 21, 19, 22], the parameters of lines in the image plane <ref> [10] </ref>, and the parameters of an ellipse in the image plane [10]. In this tutorial we will restrict our attention to point features whose parameters are their image plane coordinates. In order to perform visual servo control, we must select a set of image feature parameters. <p> length [19], the area of a projected surface and the relative areas of two projected surfaces [19], the centroid and higher order moments of a projected surface [20, 21, 19, 22], the parameters of lines in the image plane <ref> [10] </ref>, and the parameters of an ellipse in the image plane [10]. In this tutorial we will restrict our attention to point features whose parameters are their image plane coordinates. In order to perform visual servo control, we must select a set of image feature parameters. <p> The value d will be referred to as the degree of the constraint. As noted by Espiau et al. <ref> [33, 10] </ref>, the kinematic error function can be thought of as representing a virtual kinematic constraint between the end-effector and the target. <p> Thus, the number of columns in the image Jacobian will vary depending on the task. The image Jacobian was first introduced by Weiss et al. [19], who referred to it as the feature sensitivity matrix. It is also referred to as the interaction matrix <ref> [10] </ref> and the B matrix [14, 15]. Other applications of the image Jacobian include [12, 38, 48, 22]. The relationship given by (1.35) describes how image feature parameters change with respect to changing manipulator pose.
Reference: [11] <author> R. Andersson. </author> <title> A low-latency 60Hz stereo vision system for real-time visual control. </title> <booktitle> Proc. 5th Int.Symp. on Intelligent Control, </booktitle> <pages> pages 165-170, </pages> <year> 1990. </year>
Reference: [12] <author> D. H. Ballard and C. M. Brown. </author> <title> Computer Vision. </title> <publisher> Prentice Hall, </publisher> <year> 1982. </year>
Reference-contexts: Image feature parameters that have been used for visual servo control include the image plane coordinates of points in the image <ref> [12, 10, 48, 14, 15, 93, 17] </ref>, the distance between two points in the image plane and the orientation of the line connecting those two points [38, 36], perceived edge length [19], the area of a projected surface and the relative areas of two projected surfaces [19], the centroid and higher <p> The image Jacobian was first introduced by Weiss et al. [19], who referred to it as the feature sensitivity matrix. It is also referred to as the interaction matrix [10] and the B matrix [14, 15]. Other applications of the image Jacobian include <ref> [12, 38, 48, 22] </ref>. The relationship given by (1.35) describes how image feature parameters change with respect to changing manipulator pose. <p> The null space of the image Jacobian plays a significant role in hybrid methods, in which some degrees of freedom are controlled using visual servo, while the remaining degrees of freedom are controlled using some other modality <ref> [12] </ref>. 1.5.4 Resolved-Rate Methods The earliest approaches to image-based visual servo control [19, 38] were based on resolved-rate motion control [28], which we will briefly describe here. Suppose that the goal of a particular task is to reach a desired image feature parameter vector, f d . <p> For the case of a non-square image Jacobian, the techniques described in Section 1.5.3 would be used to compute for u. Similar results have been presented in <ref> [48, 12] </ref>. 1.5.5 Example Servoing Tasks In this section, we revisit the problems that were described in Section 1.4.1. Here, we describe image-based solutions for these problems. Point to Point Positioning Consider the task of bringing some point P on the manipulator to a desired stationing point S. <p> Hence, visual servoing pre-supposes the solution to a set of potentially difficult static and dynamic vision problems. To this end many reported implementations contrive the vision problem to be simple: e:g: painting objects white, using artificial targets, and so forth <ref> [54, 38, 12, 4] </ref>. Other authors use extremely task-specific clues: e:g: Allen [53] uses motion detection for locating a moving object to be grasped, and welding systems commonly use special filters that isolate the image of the welding tip. <p> These include control issues, such as adaptive visual servo control [85, 14], hybrid control (e.g., hybrid vision/position control <ref> [12] </ref>, or hybrid force/vision control), and multi-rate system theory [86]; issues related to automatic planning of visually controlled robot motions [87, 88]; applications in mobile robotics, including non-holonmoic systems [83]; and, feature selection [33, 36].
Reference: [13] <author> P. Besl. </author> <title> Active, optical range imaging sensors. </title> <journal> Machine Vision and Applications, </journal> <volume> 1 </volume> <pages> 127-152, </pages> <year> 1988. </year> - <title> 70 - Tutorial TT3: Visual Servo Control Hager/Hutchinson/Corke </title>
Reference-contexts: The pixel values for all x 2 X are copied into a two-dimensional array that is subsequently treated as a rectangular image. Such acquisitions can be implemented extremely efficiently using line-drawing and region-fill algorithms commonly developed for graphics applications <ref> [13] </ref>. In the second stage, the windows are processed to locate features. Using feature measurements, a new set of window parameters are computed. These parameters may be modified using external geometric constraints or temporal prediction, and the cycle repeats.
Reference: [14] <author> R. C. Bolles. </author> <title> Verification vision for programmable assembly. </title> <booktitle> In Proc 5th International Joint Conference on Artificial Intelligence, </booktitle> <pages> pages 569-575, </pages> <address> Cambridge, MA, </address> <year> 1977. </year>
Reference-contexts: Image feature parameters that have been used for visual servo control include the image plane coordinates of points in the image <ref> [12, 10, 48, 14, 15, 93, 17] </ref>, the distance between two points in the image plane and the orientation of the line connecting those two points [38, 36], perceived edge length [19], the area of a projected surface and the relative areas of two projected surfaces [19], the centroid and higher <p> Thus, the number of columns in the image Jacobian will vary depending on the task. The image Jacobian was first introduced by Weiss et al. [19], who referred to it as the feature sensitivity matrix. It is also referred to as the interaction matrix [10] and the B matrix <ref> [14, 15] </ref>. Other applications of the image Jacobian include [12, 38, 48, 22]. The relationship given by (1.35) describes how image feature parameters change with respect to changing manipulator pose. <p> In practice, the unknown parameter for Jacobian calculation is distance from the camera. Some recent papers present adaptive approaches for estimating <ref> [14] </ref> this depth value, or develop feedback methods which do not use depth in the feedback formulation [67]. There are often computational advantages to image-based control, particularly in ECL configurations. <p> These include control issues, such as adaptive visual servo control <ref> [85, 14] </ref>, hybrid control (e.g., hybrid vision/position control [12], or hybrid force/vision control), and multi-rate system theory [86]; issues related to automatic planning of visually controlled robot motions [87, 88]; applications in mobile robotics, including non-holonmoic systems [83]; and, feature selection [33, 36].
Reference: [15] <author> M. Bowman and A. Forrest. </author> <title> Visual detection of differential movement: Applications to robotics. </title> <journal> Robotica, </journal> <volume> 6 </volume> <pages> 7-12, </pages> <year> 1988. </year>
Reference-contexts: Image feature parameters that have been used for visual servo control include the image plane coordinates of points in the image <ref> [12, 10, 48, 14, 15, 93, 17] </ref>, the distance between two points in the image plane and the orientation of the line connecting those two points [38, 36], perceived edge length [19], the area of a projected surface and the relative areas of two projected surfaces [19], the centroid and higher <p> Thus, the number of columns in the image Jacobian will vary depending on the task. The image Jacobian was first introduced by Weiss et al. [19], who referred to it as the feature sensitivity matrix. It is also referred to as the interaction matrix [10] and the B matrix <ref> [14, 15] </ref>. Other applications of the image Jacobian include [12, 38, 48, 22]. The relationship given by (1.35) describes how image feature parameters change with respect to changing manipulator pose. <p> This burden can be reduced by performing the optimization starting at low resolution and proceeding to higher resolution, and by ordering the candidates in D from most to least likely and terminating the search once a candidate with an acceptably low SSD value is found <ref> [15] </ref>. Once the discrete minimum is found, the location can be refined to subpixel accuracy by interpolation of the SSD values about the minimum. Even with these improvements, [15] reports that a special signal processor is required to attain frame-rate performance. <p> in D from most to least likely and terminating the search once a candidate with an acceptably low SSD value is found <ref> [15] </ref>. Once the discrete minimum is found, the location can be refined to subpixel accuracy by interpolation of the SSD values about the minimum. Even with these improvements, [15] reports that a special signal processor is required to attain frame-rate performance. It is also possible to solve (1.59) using continuous optimization methods [29, 3, 33, 21].
Reference: [16] <author> R. Bukowski, L. Haynes, Z. Geng, N. Coleman, A. Santucci, K. Lam, A. Paz, R. May, and M. DeVito. </author> <title> Robot hand-eye coordination rapid prototyping environment. </title> <booktitle> In Proc. ISIR, </booktitle> <pages> pages 16.15-16.28, </pages> <month> October </month> <year> 1991. </year>
Reference: [17] <author> G. Buttazzo, B. Allotta, and F. Fanizza. Mousebuster: </author> <title> a robot system for catching fast moving objects by vision. </title> <booktitle> In Proc. IEEE Int. Conf. Robotics and Automation, </booktitle> <pages> pages 932-937, </pages> <year> 1993. </year>
Reference-contexts: Image feature parameters that have been used for visual servo control include the image plane coordinates of points in the image <ref> [12, 10, 48, 14, 15, 93, 17] </ref>, the distance between two points in the image plane and the orientation of the line connecting those two points [38, 36], perceived edge length [19], the area of a projected surface and the relative areas of two projected surfaces [19], the centroid and higher
Reference: [18] <author> F. Chaumette, P. Rives, and B. Espiau. </author> <title> Positioning of a robot with respect to an object, tracking it and estimating its velocity by visual servoing. </title> <booktitle> In Proc. IEEE Int. Conf. Robotics and Automation, </booktitle> <pages> pages 2248-2253, </pages> <year> 1991. </year>
Reference-contexts: Hence, the positioning accuracy of EOL systems depends directly on the accuracy of the hand-eye calibration. Conversely, systems that observe the end-effector as well as target features can perform with accuracy that is independent of hand-eye calibration error <ref> [30, 31, 18] </ref>. Note also that ECL systems can easily deal with tasks that involve the positioning of objects within the end-effector, whereas EOL systems must use an inferred object location. From a theoretical perspective, it would appear that ECL systems would always be preferable to EOL systems.
Reference: [19] <author> W. F. Clocksin, J. S. E. Bromley, P. G. Davey, A. R. Vidler, and C. G. Morgan. </author> <title> An implementation of model-based visual feedback for robot arc welding of thin sheet steel. </title> <journal> Int. J. Robot. Res., </journal> <volume> 4(1) </volume> <pages> 13-26, </pages> <month> Spring </month> <year> 1985. </year>
Reference-contexts: parameters that have been used for visual servo control include the image plane coordinates of points in the image [12, 10, 48, 14, 15, 93, 17], the distance between two points in the image plane and the orientation of the line connecting those two points [38, 36], perceived edge length <ref> [19] </ref>, the area of a projected surface and the relative areas of two projected surfaces [19], the centroid and higher order moments of a projected surface [20, 21, 19, 22], the parameters of lines in the image plane [10], and the parameters of an ellipse in the image plane [10]. <p> points in the image [12, 10, 48, 14, 15, 93, 17], the distance between two points in the image plane and the orientation of the line connecting those two points [38, 36], perceived edge length <ref> [19] </ref>, the area of a projected surface and the relative areas of two projected surfaces [19], the centroid and higher order moments of a projected surface [20, 21, 19, 22], the parameters of lines in the image plane [10], and the parameters of an ellipse in the image plane [10]. <p> the distance between two points in the image plane and the orientation of the line connecting those two points [38, 36], perceived edge length [19], the area of a projected surface and the relative areas of two projected surfaces [19], the centroid and higher order moments of a projected surface <ref> [20, 21, 19, 22] </ref>, the parameters of lines in the image plane [10], and the parameters of an ellipse in the image plane [10]. In this tutorial we will restrict our attention to point features whose parameters are their image plane coordinates. <p> Thus, the number of columns in the image Jacobian will vary depending on the task. The image Jacobian was first introduced by Weiss et al. <ref> [19] </ref>, who referred to it as the feature sensitivity matrix. It is also referred to as the interaction matrix [10] and the B matrix [14, 15]. Other applications of the image Jacobian include [12, 38, 48, 22]. <p> The null space of the image Jacobian plays a significant role in hybrid methods, in which some degrees of freedom are controlled using visual servo, while the remaining degrees of freedom are controlled using some other modality [12]. 1.5.4 Resolved-Rate Methods The earliest approaches to image-based visual servo control <ref> [19, 38] </ref> were based on resolved-rate motion control [28], which we will briefly describe here. Suppose that the goal of a particular task is to reach a desired image feature parameter vector, f d . <p> Discussion of the issues related to feature selection for visual servo control applications can be found in <ref> [19, 36] </ref>. The "right" image feature tracking method to use is extremely application dependent. For example, if the goal is to track a single special pattern or surface marking that is approximately planar and moving at slow to moderate speeds, then SSD tracking is appropriate.
Reference: [20] <author> P. I. Corke. </author> <title> High-Performance Visual Closed-Loop Robot Control. </title> <type> PhD thesis, </type> <institution> University of Melbourne, Dept. Mechanical and Manufacturing Engineering, </institution> <month> July </month> <year> 1994. </year>
Reference-contexts: the distance between two points in the image plane and the orientation of the line connecting those two points [38, 36], perceived edge length [19], the area of a projected surface and the relative areas of two projected surfaces [19], the centroid and higher order moments of a projected surface <ref> [20, 21, 19, 22] </ref>, the parameters of lines in the image plane [10], and the parameters of an ellipse in the image plane [10]. In this tutorial we will restrict our attention to point features whose parameters are their image plane coordinates. <p> Here we use the term "direct visual servo" to avoid confusion. - 11 - Tutorial TT3: Visual Servo Control Hager/Hutchinson/Corke challenging control problem. Using internal feedback with a high sampling rate generally presents the visual controller with idealized axis dynamics <ref> [20] </ref>. Second, many robots already have an interface for accepting Cartesian velocity or incremental position commands. This simplifies the construction of the visual servo system, and also makes the methods more portable. <p> A good example of this is the common Unimate Puma robot whose position loops operate at a sample interval of 14 or 28 ms while vision systems operate at sample intervals of 33 or 40 ms for RS 170 or CCIR video respectively <ref> [20] </ref>. It is well known that a feedback system including delay will become unstable as the loop gain is increased. Many visual closed-loop systems are tuned empirically, increasing the loop gain until overshoot or oscillation becomes intolerable. <p> Other issues for consideration include whether or not the vision system should `close the loop' around robot axes which are position, velocity or torque controlled. A detailed discussion of these dynamic issues in visual servo systems is given by Corke <ref> [20, 82] </ref>. - 38 - Tutorial TT3: Visual Servo Control Hager/Hutchinson/Corke 1.7.3 Mobile robots The discussion above has assumed that the moving camera is mounted on an arm type robot manipulator.
Reference: [21] <author> P. Corke. </author> <title> Experiments in high-performance robotic visual servoing. </title> <booktitle> In Proc. International Symposium on Experimental Robotics, </booktitle> <pages> pages 194-200, </pages> <address> Kyoto, </address> <month> Oc-tober </month> <year> 1993. </year>
Reference-contexts: the distance between two points in the image plane and the orientation of the line connecting those two points [38, 36], perceived edge length [19], the area of a projected surface and the relative areas of two projected surfaces [19], the centroid and higher order moments of a projected surface <ref> [20, 21, 19, 22] </ref>, the parameters of lines in the image plane [10], and the parameters of an ellipse in the image plane [10]. In this tutorial we will restrict our attention to point features whose parameters are their image plane coordinates. <p> Even with these improvements, [15] reports that a special signal processor is required to attain frame-rate performance. It is also possible to solve (1.59) using continuous optimization methods <ref> [29, 3, 33, 21] </ref>. <p> First, a single updating cycle is usually faster to compute. For example, (1.62) can be computed and solved in less than 5 ms on a Sparc II computer <ref> [21] </ref>. Second, it is easy to incorporate other window parameters such as rotation and scaling into the system without greatly increasing the computation time [33, 21]. <p> For example, (1.62) can be computed and solved in less than 5 ms on a Sparc II computer [21]. Second, it is easy to incorporate other window parameters such as rotation and scaling into the system without greatly increasing the computation time <ref> [33, 21] </ref>. It is also easy to show that including parameters for contrast and brightness in (1.60) makes SSD tracking equivalent to finding the maximum correlation between the two image regions [29].
Reference: [22] <author> P. Corke and M. </author> <title> Good. Dynamic effects in high-performance visual servoing. </title> <booktitle> In Proc. IEEE Int. Conf. Robotics and Automation, </booktitle> <pages> pages 1838-1843, </pages> <address> Nice, </address> <month> May </month> <year> 1992. </year>
Reference-contexts: the distance between two points in the image plane and the orientation of the line connecting those two points [38, 36], perceived edge length [19], the area of a projected surface and the relative areas of two projected surfaces [19], the centroid and higher order moments of a projected surface <ref> [20, 21, 19, 22] </ref>, the parameters of lines in the image plane [10], and the parameters of an ellipse in the image plane [10]. In this tutorial we will restrict our attention to point features whose parameters are their image plane coordinates. <p> The image Jacobian was first introduced by Weiss et al. [19], who referred to it as the feature sensitivity matrix. It is also referred to as the interaction matrix [10] and the B matrix [14, 15]. Other applications of the image Jacobian include <ref> [12, 38, 48, 22] </ref>. The relationship given by (1.35) describes how image feature parameters change with respect to changing manipulator pose.
Reference: [23] <author> P. Corke and M. </author> <title> Good. Controller design for high-performance visual servoing. </title> <booktitle> In Proc. IFAC 12th World Congress, </booktitle> <pages> pages 9-395 to 9-398, </pages> <address> Sydney, </address> <year> 1993. </year>
Reference-contexts: A variant of this is for the camera to be agile, mounted on another robot or pan/tilt head in order to observe the visually controlled robot from the best vantage <ref> [23] </ref>. For either choice of camera configuration, prior to the execution of visual servo tasks, camera calibration must be performed. For the eye-in-hand case, this amounts to determining e x c . For the fixed camera case, calibration is used to determine 0 x c .
Reference: [24] <author> P. Corke and R. Kirkham. </author> <title> The ARCL robot programming system. </title> <booktitle> In Proc.Int.Conf. of Australian Robot Association, </booktitle> <pages> pages 484-493, </pages> <address> Brisbane, </address> <month> July </month> <year> 1993. </year> <institution> Australian Robot Association, Mechanical Engineering Publications (Lon-don). </institution>
Reference-contexts: For the fixed camera case, calibration is used to determine 0 x c . Calibration is a long standing research issue in the computer vision community (good solutions to the calibration problem can be found in a number of references, e.g., <ref> [24, 25, 26] </ref>). 1.3 Servoing Architectures In 1980, Sanderson and Weiss [4] introduced a taxonomy of visual servo systems, into which all subsequent visual servo systems can be categorized. Their scheme essentially poses two questions: 1.
Reference: [25] <author> P. Corke and R. Paul. </author> <title> Video-rate visual servoing for robots. </title> <editor> In V. Hayward and O. Khatib, editors, </editor> <booktitle> Experimental Robotics 1, volume 139 of Lecture Notes in Control and Information Sciences, </booktitle> <pages> pages 429-451. </pages> <publisher> Springer-Verlag, </publisher> <year> 1989. </year>
Reference-contexts: For the fixed camera case, calibration is used to determine 0 x c . Calibration is a long standing research issue in the computer vision community (good solutions to the calibration problem can be found in a number of references, e.g., <ref> [24, 25, 26] </ref>). 1.3 Servoing Architectures In 1980, Sanderson and Weiss [4] introduced a taxonomy of visual servo systems, into which all subsequent visual servo systems can be categorized. Their scheme essentially poses two questions: 1.
Reference: [26] <author> P. Corke and R. Paul. </author> <title> Video-rate visual servoing for robots. </title> <type> Technical Report MS-CIS-89-18, </type> <institution> GRASP Lab, University of Pennsylvania, </institution> <month> February </month> <year> 1989. </year>
Reference-contexts: For the fixed camera case, calibration is used to determine 0 x c . Calibration is a long standing research issue in the computer vision community (good solutions to the calibration problem can be found in a number of references, e.g., <ref> [24, 25, 26] </ref>). 1.3 Servoing Architectures In 1980, Sanderson and Weiss [4] introduced a taxonomy of visual servo systems, into which all subsequent visual servo systems can be categorized. Their scheme essentially poses two questions: 1.
Reference: [27] <author> P. Y. Coulon and M. Nougaret. </author> <title> Use of a TV camera system in closed-loop position control of mechnisms. </title> <editor> In A. Pugh, editor, </editor> <booktitle> International Trends in - 71 - Tutorial TT3: Visual Servo Control Hager/Hutchinson/Corke Manufacturing Technology ROBOT VISION, </booktitle> <pages> pages 117-127. </pages> <publisher> IFS Publications, </publisher> <year> 1983. </year>
Reference: [28] <author> R. Cunningham. </author> <title> Segmenting binary images. </title> <booktitle> Robotics Age, </booktitle> <pages> pages 4-19, </pages> <month> July </month> <year> 1981. </year>
Reference-contexts: This simplifies the construction of the visual servo system, and also makes the methods more portable. Thirdly, look-and-move separates the kinematic singularities of the mechanism from the visual controller, allowing the robot to be considered as an ideal Cartesian motion device. Since many resolved rate <ref> [28] </ref> controllers have specialized mechanisms for dealing with kinematic singularities [29], the system design is again greatly simplified. In this article, we will utilize the look-and-move model exclusively. The second major classification of systems distinguishes position-based control from image-based control. <p> plays a significant role in hybrid methods, in which some degrees of freedom are controlled using visual servo, while the remaining degrees of freedom are controlled using some other modality [12]. 1.5.4 Resolved-Rate Methods The earliest approaches to image-based visual servo control [19, 38] were based on resolved-rate motion control <ref> [28] </ref>, which we will briefly describe here. Suppose that the goal of a particular task is to reach a desired image feature parameter vector, f d .
Reference: [29] <author> M. L. Cyros. </author> <title> Datacube at the space shuttle's launch pad. </title> <journal> Datacube World Review, </journal> <volume> 2(5) </volume> <pages> 1-3, </pages> <month> September </month> <year> 1988. </year> <institution> Datacube Inc., </institution> <address> 4 Dearborn Road, Peabody, MA. </address>
Reference-contexts: Most commonly the coordinates of a feature point or a region centroid are used. A good feature point is one that can be located unambiguously in different views of the scene, such as a hole in a gasket [38] or a contrived pattern <ref> [10, 29] </ref>. <p> Thirdly, look-and-move separates the kinematic singularities of the mechanism from the visual controller, allowing the robot to be considered as an ideal Cartesian motion device. Since many resolved rate [28] controllers have specialized mechanisms for dealing with kinematic singularities <ref> [29] </ref>, the system design is again greatly simplified. In this article, we will utilize the look-and-move model exclusively. The second major classification of systems distinguishes position-based control from image-based control. <p> Even with these improvements, [15] reports that a special signal processor is required to attain frame-rate performance. It is also possible to solve (1.59) using continuous optimization methods <ref> [29, 3, 33, 21] </ref>. <p> It is also easy to show that including parameters for contrast and brightness in (1.60) makes SSD tracking equivalent to finding the maximum correlation between the two image regions <ref> [29] </ref>.
Reference: [30] <author> E. Dickmanns and V. Graefe. </author> <title> Applications of dynamic monocular machine vision. </title> <journal> Machine Vision and Applications, </journal> <volume> 1 </volume> <pages> 241-261, </pages> <year> 1988. </year>
Reference-contexts: Hence, the positioning accuracy of EOL systems depends directly on the accuracy of the hand-eye calibration. Conversely, systems that observe the end-effector as well as target features can perform with accuracy that is independent of hand-eye calibration error <ref> [30, 31, 18] </ref>. Note also that ECL systems can easily deal with tasks that involve the positioning of objects within the end-effector, whereas EOL systems must use an inferred object location. From a theoretical perspective, it would appear that ECL systems would always be preferable to EOL systems.
Reference: [31] <author> E. Dickmanns and V. Graefe. </author> <title> Dynamic monocular machine vision. </title> <journal> Machine Vision and Applications, </journal> <volume> 1 </volume> <pages> 223-240, </pages> <year> 1988. </year>
Reference-contexts: Hence, the positioning accuracy of EOL systems depends directly on the accuracy of the hand-eye calibration. Conversely, systems that observe the end-effector as well as target features can perform with accuracy that is independent of hand-eye calibration error <ref> [30, 31, 18] </ref>. Note also that ECL systems can easily deal with tasks that involve the positioning of objects within the end-effector, whereas EOL systems must use an inferred object location. From a theoretical perspective, it would appear that ECL systems would always be preferable to EOL systems. <p> For example, Allen [53] shows a system which can grasp a toy train using stereo vision. Rizzi [54] demonstrates a system which can bounce a ping-pong ball. All of these systems are EOL. Cipolla <ref> [31] </ref> describes an ECL system using free-standing stereo cameras. One novel feature of this system is the use of the affine projection model (Section 1.2.3) for the imaging geometry. This leads to linear calibration and control at the cost of some system performance. <p> However not all pixels in the image are of interest, and computation time can be greatly reduced if only a small region around each image feature is processed. Thus, a promising technique for making vision cheap and tractable is to use window-based tracking techniques <ref> [70, 54, 31] </ref>. Window-based methods have several advantages, among them; computational simplicity, little requirement for special hardware, and easy reconfiguration for different applications.
Reference: [32] <author> E. Dickmanns and F.-R. Schell. </author> <title> Autonomous landing of airplanes by dynamic machine vision. </title> <booktitle> In Proc. IEEE Workshop on Applications of Computer Vision, </booktitle> <pages> pages 172-179. </pages> <publisher> IEEE Comput. Soc. Press, </publisher> <month> November </month> <year> 1992. </year>
Reference: [33] <author> J. Dietrich, G. Hirzinger, B. Gombert, and J. Schott. </author> <title> On a unified concept for a new generation of light-weight robots. </title> <editor> In V. Hayward and O. Khatib, editors, </editor> <booktitle> Experimental Robotics 1, volume 139 of Lecture Notes in Control and Information Sciences, </booktitle> <pages> pages 287-295. </pages> <publisher> Springer-Verlag, </publisher> <year> 1989. </year>
Reference-contexts: The value d will be referred to as the degree of the constraint. As noted by Espiau et al. <ref> [33, 10] </ref>, the kinematic error function can be thought of as representing a virtual kinematic constraint between the end-effector and the target. <p> Many of these problems can be circumvented by sensing target pose directly using a 3D sensor. Active 3D sensors based on structured lighting are now compact and fast enough to use for visual servoing. If the sensor is small and mounted on the robot <ref> [2, 101, 33] </ref> the depth and orientation information can be used for position-based visual servoing. 1.5 Image-Based Control As described in Section 1.3, in image-based visual servo control the error signal is defined directly in terms of image feature parameters (in contrast to position-based methods that define the error signal in <p> Even with these improvements, [15] reports that a special signal processor is required to attain frame-rate performance. It is also possible to solve (1.59) using continuous optimization methods <ref> [29, 3, 33, 21] </ref>. <p> For example, (1.62) can be computed and solved in less than 5 ms on a Sparc II computer [21]. Second, it is easy to incorporate other window parameters such as rotation and scaling into the system without greatly increasing the computation time <ref> [33, 21] </ref>. It is also easy to show that including parameters for contrast and brightness in (1.60) makes SSD tracking equivalent to finding the maximum correlation between the two image regions [29]. <p> include control issues, such as adaptive visual servo control [85, 14], hybrid control (e.g., hybrid vision/position control [12], or hybrid force/vision control), and multi-rate system theory [86]; issues related to automatic planning of visually controlled robot motions [87, 88]; applications in mobile robotics, including non-holonmoic systems [83]; and, feature selection <ref> [33, 36] </ref>. Many of these are describe in the proceedings of a recent workshop on visual servo control [89]. - 39 - Tutorial TT3: Visual Servo Control Hager/Hutchinson/Corke 1.7.6 The future The future for applications of visual servoing should be bright.
Reference: [34] <author> K. A. Dzialo and R. J. Schalkoff. </author> <title> Control implications in tracking moving objects using time-varying perspective-projective imagery. </title> <journal> IEEE Trans. Ind. Electron., </journal> <volume> IE-33(3):247-253, </volume> <month> August </month> <year> 1986. </year>
Reference-contexts: This regulator produces at every time instant a desired end-effector velocity screw u 2 &lt; 6 which is sent to the robot control subsystem. For the purposes of this section, we use simple proportional control methods for linear and linearized systems to compute u <ref> [34] </ref>. These methods are illustrated below, and are discussed in more detail in Section 1.5. We now present examples of positioning tasks for end-effector and fixed cameras in both ECL and EOL configurations. In Section 1.4.1, several examples of positioning tasks based on directly observable features are presented. <p> pp ( ^ x e ; ^ x c ffi c b S; e P) = k x e ffi e P ^ x c ffi c b S : (1.20) will drive the system to an equilibrium state in which the estimated value of the error function is zero <ref> [34] </ref>. The value k &gt; 0 is a proportional feedback gain. Note that - 14 - Tutorial TT3: Visual Servo Control Hager/Hutchinson/Corke we have written ^ x e in the feedback law to emphasize the fact that this value is also subject to errors.
Reference: [35] <author> H. Fassler, H. Beyer, and J. Wen. </author> <title> A robot pong pong player: optimized mechanics, high performance 3D vision, and intelligent sensor control. </title> <journal> Robotersysteme, </journal> <volume> 6 </volume> <pages> 161-170, </pages> <year> 1990. </year>
Reference-contexts: Full six degree-of-freedom positioning can be attained by enforcing another point-to-line constraint using an additional point on the end-effector and an additional point in the world. See <ref> [35] </ref> for details. These formulations can be adjusted for end-effector mounted camera and can be implemented as ECL or EOL systems.
Reference: [36] <author> J. T. Feddema, C. S. G. Lee, and O. R. Mitchell. </author> <title> Weighted selection of image features for resolved rate visual feedback control. </title> <journal> IEEE Trans. Robot. Autom., </journal> <volume> 7(1) </volume> <pages> 31-47, </pages> <month> February </month> <year> 1991. </year>
Reference-contexts: Image feature parameters that have been used for visual servo control include the image plane coordinates of points in the image [12, 10, 48, 14, 15, 93, 17], the distance between two points in the image plane and the orientation of the line connecting those two points <ref> [38, 36] </ref>, perceived edge length [19], the area of a projected surface and the relative areas of two projected surfaces [19], the centroid and higher order moments of a projected surface [20, 21, 19, 22], the parameters of lines in the image plane [10], and the parameters of an ellipse in <p> This encompasses problems including structure from motion, exterior orientation, stereo reconstruction, and absolute orientation. A comprehensive discussion of these topics can be found in a recent review article <ref> [36] </ref>. We divide the estimation problems that arise into single-camera and multiple-camera situations which will be discussed in the following sections. Single Camera As noted previously, it follows from (1.15) that a point in a single camera image corresponds to a line in space. <p> Single Camera As noted previously, it follows from (1.15) that a point in a single camera image corresponds to a line in space. Although it is possible to perform geometric reconstruction using a single moving camera, the equations governing this process are often ill-conditioned, leading to stability problems <ref> [36] </ref> Better results can be achieved if target features have some internal structure, or the features come from a known object. Below, we briefly describe methods for performing both point estimation and pose estimation with a single camera assuming such information is available. <p> We now discuss each of these. When k = m and J v is nonsingular, J 1 v exists. Therefore, in this case, _ r = J 1 v Such an approach has been used by Feddema <ref> [36] </ref>, who also describes an automated approach to image feature selection in order to minimize the condition number of J v . When k 6= m, J 1 v does not exist. <p> Discussion of the issues related to feature selection for visual servo control applications can be found in <ref> [19, 36] </ref>. The "right" image feature tracking method to use is extremely application dependent. For example, if the goal is to track a single special pattern or surface marking that is approximately planar and moving at slow to moderate speeds, then SSD tracking is appropriate. <p> include control issues, such as adaptive visual servo control [85, 14], hybrid control (e.g., hybrid vision/position control [12], or hybrid force/vision control), and multi-rate system theory [86]; issues related to automatic planning of visually controlled robot motions [87, 88]; applications in mobile robotics, including non-holonmoic systems [83]; and, feature selection <ref> [33, 36] </ref>. Many of these are describe in the proceedings of a recent workshop on visual servo control [89]. - 39 - Tutorial TT3: Visual Servo Control Hager/Hutchinson/Corke 1.7.6 The future The future for applications of visual servoing should be bright.
Reference: [37] <author> J. Feddema. </author> <title> Real Time Visual Feedback Control for Hand-Eye Coordinated Robotic Systems. </title> <type> PhD thesis, </type> <institution> Purdue University, </institution> <year> 1989. </year>
Reference-contexts: Object Pose Accurate object pose estimation is possible if the vision system observes features of a known object, and uses those features to estimate object pose. This approach has been recently demonstrated by Wilson <ref> [37] </ref> for six DOF control of end-effector pose. A similar approach was recently reported in [38]. Briefly, such an approach proceeds as follows. <p> A particularly elegant formulation of this updating procedure results by application of statistical techniques such as the extended Kalman filter [52]. The reader is referred to <ref> [37] </ref> for details. Multiple Cameras Many systems utilizing position-based control with stereo vision from free-standing cameras have been demonstrated. For example, Allen [53] shows a system which can grasp a toy train using stereo vision. Rizzi [54] demonstrates a system which can bounce a ping-pong ball. <p> Computation time for the relative orientation problem is often cited as a disadvantage of position-based methods. However recent results show that solutions can be computed in only a few milliseconds even using iteration [39] or Kalman filtering <ref> [37] </ref>. - 20 - Tutorial TT3: Visual Servo Control Hager/Hutchinson/Corke Endpoint closed-loop systems are demonstrably less sensitive to calibration. However, particularly in stereo systems, small rotational errors between the cameras can lead to reconstruction errors which do impact the positioning accuracy of the system. <p> For example, Rizzi [54] describes the use of a Newtonian flight dynamics model to make it possible to track a ping-pong ball during flight. Predictors based on ff fi tracking filters and Kalman filters have also been used <ref> [53, 37, 72] </ref>. Multiresolution techniques can be used provide further performance improvements, particularly when a dynamic model is not available and large search windows must be used. 1.6.4 Discussion Prior to executing or planning visually controlled motions, a specific set of visual features must be chosen.
Reference: [38] <author> J. Feddema and O. Mitchell. </author> <title> Vision-guided servoing with feature-based trajectory generation. </title> <journal> IEEE Trans. Robot. Autom., </journal> <volume> 5(5) </volume> <pages> 691-700, </pages> <month> October </month> <year> 1989. </year>
Reference-contexts: Most commonly the coordinates of a feature point or a region centroid are used. A good feature point is one that can be located unambiguously in different views of the scene, such as a hole in a gasket <ref> [38] </ref> or a contrived pattern [10, 29]. <p> Image feature parameters that have been used for visual servo control include the image plane coordinates of points in the image [12, 10, 48, 14, 15, 93, 17], the distance between two points in the image plane and the orientation of the line connecting those two points <ref> [38, 36] </ref>, perceived edge length [19], the area of a projected surface and the relative areas of two projected surfaces [19], the centroid and higher order moments of a projected surface [20, 21, 19, 22], the parameters of lines in the image plane [10], and the parameters of an ellipse in <p> Object Pose Accurate object pose estimation is possible if the vision system observes features of a known object, and uses those features to estimate object pose. This approach has been recently demonstrated by Wilson [37] for six DOF control of end-effector pose. A similar approach was recently reported in <ref> [38] </ref>. Briefly, such an approach proceeds as follows. Let t P 1 ; t P 2 ; : : : t P n be a set of points expressed in an object coordinate system with unknown pose c x t relative to an observing camera. <p> The image Jacobian was first introduced by Weiss et al. [19], who referred to it as the feature sensitivity matrix. It is also referred to as the interaction matrix [10] and the B matrix [14, 15]. Other applications of the image Jacobian include <ref> [12, 38, 48, 22] </ref>. The relationship given by (1.35) describes how image feature parameters change with respect to changing manipulator pose. <p> The null space of the image Jacobian plays a significant role in hybrid methods, in which some degrees of freedom are controlled using visual servo, while the remaining degrees of freedom are controlled using some other modality [12]. 1.5.4 Resolved-Rate Methods The earliest approaches to image-based visual servo control <ref> [19, 38] </ref> were based on resolved-rate motion control [28], which we will briefly describe here. Suppose that the goal of a particular task is to reach a desired image feature parameter vector, f d . <p> By observing visual cues such as the ball, the arm's pivot point, and another point on the arm, the interception task can be specified, even if the relationship between camera and arm is not known a priori. Feddema <ref> [38] </ref> uses a feature space trajectory generator to interpolate feature parameter values due to the low update rate of the vision system used. 1.6 Image Feature Extraction and Tracking Irrespective of the control approach used, a vision system is required to extract the information needed to perform the servoing task. <p> Hence, visual servoing pre-supposes the solution to a set of potentially difficult static and dynamic vision problems. To this end many reported implementations contrive the vision problem to be simple: e:g: painting objects white, using artificial targets, and so forth <ref> [54, 38, 12, 4] </ref>. Other authors use extremely task-specific clues: e:g: Allen [53] uses motion detection for locating a moving object to be grasped, and welding systems commonly use special filters that isolate the image of the welding tip.
Reference: [39] <author> J. P. Foith, C. Eisenbarth, E. Enderle, H. Geisselmann, H. Ringschauser, and G. Zimmermann. </author> <title> Real-time processing of binary images for industrial applications. </title> <editor> In L. Bolc and Z. Kulpa, editors, </editor> <booktitle> Digital Image Processing Systems, </booktitle> <pages> pages 61-168. </pages> <publisher> Springer-Verlag, </publisher> <address> Germany, </address> <year> 1981. </year>
Reference-contexts: Numerous methods of solution have been proposed and <ref> [39] </ref> provides a recent review of several techniques. Broadly speaking, solutions divide into analytic solutions and least-squares solutions which employ a variety of - 18 - Tutorial TT3: Visual Servo Control Hager/Hutchinson/Corke simplifications and/or iterative methods. <p> Computation time for the relative orientation problem is often cited as a disadvantage of position-based methods. However recent results show that solutions can be computed in only a few milliseconds even using iteration <ref> [39] </ref> or Kalman filtering [37]. - 20 - Tutorial TT3: Visual Servo Control Hager/Hutchinson/Corke Endpoint closed-loop systems are demonstrably less sensitive to calibration. However, particularly in stereo systems, small rotational errors between the cameras can lead to reconstruction errors which do impact the positioning accuracy of the system.
Reference: [40] <author> G. Franklin and J. Powell. </author> <title> Digital Control of dynamic systems. </title> <publisher> Addison-Wesley, </publisher> <year> 1980. </year>
Reference-contexts: Broadly speaking, solutions divide into analytic solutions and least-squares solutions which employ a variety of - 18 - Tutorial TT3: Visual Servo Control Hager/Hutchinson/Corke simplifications and/or iterative methods. Analytic solutions for three and four points are given by <ref> [40, 41, 42, 43, 44] </ref>. Unique solutions exist for four coplanar, but not collinear, points. Least-squares solutions can be found in [45, 46, 47, 48, 49, 50, 51]. Six or more points always yield unique solutions.
Reference: [41] <author> S. Ganapathy. </author> <title> Real-time motion tracking using a single camera. </title> <type> Technical Memorandum 11358-841105-21-TM, </type> <institution> AT&T Bell Laboratories, </institution> <month> November </month> <year> 1984. </year> - <title> 72 - Tutorial TT3: Visual Servo Control Hager/Hutchinson/Corke </title>
Reference-contexts: Broadly speaking, solutions divide into analytic solutions and least-squares solutions which employ a variety of - 18 - Tutorial TT3: Visual Servo Control Hager/Hutchinson/Corke simplifications and/or iterative methods. Analytic solutions for three and four points are given by <ref> [40, 41, 42, 43, 44] </ref>. Unique solutions exist for four coplanar, but not collinear, points. Least-squares solutions can be found in [45, 46, 47, 48, 49, 50, 51]. Six or more points always yield unique solutions.
Reference: [42] <author> C. Geschke. </author> <title> A robot task using visual tracking. </title> <booktitle> Robotics Today, </booktitle> <pages> pages 39-43, </pages> <month> Winter </month> <year> 1981. </year>
Reference-contexts: Broadly speaking, solutions divide into analytic solutions and least-squares solutions which employ a variety of - 18 - Tutorial TT3: Visual Servo Control Hager/Hutchinson/Corke simplifications and/or iterative methods. Analytic solutions for three and four points are given by <ref> [40, 41, 42, 43, 44] </ref>. Unique solutions exist for four coplanar, but not collinear, points. Least-squares solutions can be found in [45, 46, 47, 48, 49, 50, 51]. Six or more points always yield unique solutions.
Reference: [43] <author> C. C. Geschke. </author> <title> A system for programming and controlling sensor-based robot manipulators. </title> <journal> IEEE Trans. Pattern Anal. Mach. Intell., </journal> <volume> PAMI-5(1):1-7, </volume> <month> Jan-uary </month> <year> 1983. </year>
Reference-contexts: Broadly speaking, solutions divide into analytic solutions and least-squares solutions which employ a variety of - 18 - Tutorial TT3: Visual Servo Control Hager/Hutchinson/Corke simplifications and/or iterative methods. Analytic solutions for three and four points are given by <ref> [40, 41, 42, 43, 44] </ref>. Unique solutions exist for four coplanar, but not collinear, points. Least-squares solutions can be found in [45, 46, 47, 48, 49, 50, 51]. Six or more points always yield unique solutions.
Reference: [44] <author> A. Gilbert, M. Giles, G. Flachs, R. Rogers, and H. Yee. </author> <title> A real-time video tracking system. </title> <journal> IEEE Trans. Pattern Anal. Mach. Intell., </journal> <volume> 2(1) </volume> <pages> 47-56, </pages> <month> January </month> <year> 1980. </year>
Reference-contexts: Broadly speaking, solutions divide into analytic solutions and least-squares solutions which employ a variety of - 18 - Tutorial TT3: Visual Servo Control Hager/Hutchinson/Corke simplifications and/or iterative methods. Analytic solutions for three and four points are given by <ref> [40, 41, 42, 43, 44] </ref>. Unique solutions exist for four coplanar, but not collinear, points. Least-squares solutions can be found in [45, 46, 47, 48, 49, 50, 51]. Six or more points always yield unique solutions.
Reference: [45] <author> R. M. Haralick and L. G. Shapiro. </author> <title> Survey: Image segmentation techniques. Computer Vision, </title> <journal> Graphics, and Image Processing, </journal> <volume> 29 </volume> <pages> 100-132, </pages> <year> 1985. </year>
Reference-contexts: Analytic solutions for three and four points are given by [40, 41, 42, 43, 44]. Unique solutions exist for four coplanar, but not collinear, points. Least-squares solutions can be found in <ref> [45, 46, 47, 48, 49, 50, 51] </ref>. Six or more points always yield unique solutions. The camera calibration matrix can be computed from features on the target, then decomposed [49] to yield the target's pose. The least-squares solution proceeds as follows.
Reference: [46] <author> R. C. Harrell, D. C. Slaughter, and P. D. Adsit. </author> <title> A fruit-tracking system for robotic harvesting. </title> <journal> Machine Vision and Applications, </journal> <volume> 2 </volume> <pages> 69-80, </pages> <year> 1989. </year>
Reference-contexts: Analytic solutions for three and four points are given by [40, 41, 42, 43, 44]. Unique solutions exist for four coplanar, but not collinear, points. Least-squares solutions can be found in <ref> [45, 46, 47, 48, 49, 50, 51] </ref>. Six or more points always yield unique solutions. The camera calibration matrix can be computed from features on the target, then decomposed [49] to yield the target's pose. The least-squares solution proceeds as follows.
Reference: [47] <author> H. Hashimoto, T. Kubota, W.-C. Lo, and F. Harashima. </author> <title> A control scheme of visual servo control of robotic manipulators using artificial neural network. </title> <booktitle> In Proc. IEEE Int.Conf. Control and Applications, pages TA-3-6, </booktitle> <address> Jerusalem, </address> <year> 1989. </year>
Reference-contexts: Analytic solutions for three and four points are given by [40, 41, 42, 43, 44]. Unique solutions exist for four coplanar, but not collinear, points. Least-squares solutions can be found in <ref> [45, 46, 47, 48, 49, 50, 51] </ref>. Six or more points always yield unique solutions. The camera calibration matrix can be computed from features on the target, then decomposed [49] to yield the target's pose. The least-squares solution proceeds as follows.
Reference: [48] <author> K. Hashimoto, T. Kimoto, T. Ebine, and H. Kimura. </author> <title> Manipulator control with image-based visual servo. </title> <booktitle> In Proc. IEEE Int. Conf. Robotics and Automation, </booktitle> <pages> pages 2267-2272, </pages> <year> 1991. </year>
Reference-contexts: Image feature parameters that have been used for visual servo control include the image plane coordinates of points in the image <ref> [12, 10, 48, 14, 15, 93, 17] </ref>, the distance between two points in the image plane and the orientation of the line connecting those two points [38, 36], perceived edge length [19], the area of a projected surface and the relative areas of two projected surfaces [19], the centroid and higher <p> Analytic solutions for three and four points are given by [40, 41, 42, 43, 44]. Unique solutions exist for four coplanar, but not collinear, points. Least-squares solutions can be found in <ref> [45, 46, 47, 48, 49, 50, 51] </ref>. Six or more points always yield unique solutions. The camera calibration matrix can be computed from features on the target, then decomposed [49] to yield the target's pose. The least-squares solution proceeds as follows. <p> The image Jacobian was first introduced by Weiss et al. [19], who referred to it as the feature sensitivity matrix. It is also referred to as the interaction matrix [10] and the B matrix [14, 15]. Other applications of the image Jacobian include <ref> [12, 38, 48, 22] </ref>. The relationship given by (1.35) describes how image feature parameters change with respect to changing manipulator pose. <p> For the case of a non-square image Jacobian, the techniques described in Section 1.5.3 would be used to compute for u. Similar results have been presented in <ref> [48, 12] </ref>. 1.5.5 Example Servoing Tasks In this section, we revisit the problems that were described in Section 1.4.1. Here, we describe image-based solutions for these problems. Point to Point Positioning Consider the task of bringing some point P on the manipulator to a desired stationing point S.
Reference: [49] <author> M. Hatamian. </author> <title> A real-time two-dimensional moment generating algorithm and its single chip implementation. </title> <journal> IEEE Trans. Acoust. Speech Signal Process., </journal> <volume> 34(3) </volume> <pages> 546-553, </pages> <month> June </month> <year> 1986. </year>
Reference-contexts: Analytic solutions for three and four points are given by [40, 41, 42, 43, 44]. Unique solutions exist for four coplanar, but not collinear, points. Least-squares solutions can be found in <ref> [45, 46, 47, 48, 49, 50, 51] </ref>. Six or more points always yield unique solutions. The camera calibration matrix can be computed from features on the target, then decomposed [49] to yield the target's pose. The least-squares solution proceeds as follows. <p> Unique solutions exist for four coplanar, but not collinear, points. Least-squares solutions can be found in [45, 46, 47, 48, 49, 50, 51]. Six or more points always yield unique solutions. The camera calibration matrix can be computed from features on the target, then decomposed <ref> [49] </ref> to yield the target's pose. The least-squares solution proceeds as follows.
Reference: [50] <author> V. Hayward and R. P. Paul. </author> <title> Robot manipulator control under UNIX | RCCL: a Robot Control C Library. </title> <journal> Int. J. Robot. Res., </journal> <volume> 5(4) </volume> <pages> 94-111, </pages> <year> 1986. </year>
Reference-contexts: Analytic solutions for three and four points are given by [40, 41, 42, 43, 44]. Unique solutions exist for four coplanar, but not collinear, points. Least-squares solutions can be found in <ref> [45, 46, 47, 48, 49, 50, 51] </ref>. Six or more points always yield unique solutions. The camera calibration matrix can be computed from features on the target, then decomposed [49] to yield the target's pose. The least-squares solution proceeds as follows.
Reference: [51] <author> J. Hill and W. T. Park. </author> <title> Real time control of a robot with a mobile camera. </title> <booktitle> In Proc. 9th ISIR, </booktitle> <pages> pages 233-246, </pages> <address> Washington, DC, </address> <month> March </month> <year> 1979. </year>
Reference-contexts: Taken to the extreme, machine vision can provide closed-loop position control for a robot end-effector | this is referred to as visual servoing. This term appears to have been first introduced by Hill and Park <ref> [51] </ref> in 1979 to distinguish their approach from earlier `blocks world' experiments where the system alternated between picture taking and moving. Prior to the introduction of this term, the less specific term visual feedback was generally used. <p> Analytic solutions for three and four points are given by [40, 41, 42, 43, 44]. Unique solutions exist for four coplanar, but not collinear, points. Least-squares solutions can be found in <ref> [45, 46, 47, 48, 49, 50, 51] </ref>. Six or more points always yield unique solutions. The camera calibration matrix can be computed from features on the target, then decomposed [49] to yield the target's pose. The least-squares solution proceeds as follows.
Reference: [52] <author> K. Hosoda and M. Asada. </author> <title> Versatile visual servoing without knowledge of true Jacobian. </title> <booktitle> In Proc. IROS, </booktitle> <month> September </month> <year> 1994. </year>
Reference-contexts: A particularly elegant formulation of this updating procedure results by application of statistical techniques such as the extended Kalman filter <ref> [52] </ref>. The reader is referred to [37] for details. Multiple Cameras Many systems utilizing position-based control with stereo vision from free-standing cameras have been demonstrated. For example, Allen [53] shows a system which can grasp a toy train using stereo vision.
Reference: [53] <author> N. Houshangi. </author> <title> Control of a robotic manipulator to grasp a moving target using vision. </title> <booktitle> In Proc. IEEE Int. Conf. Robotics and Automation, </booktitle> <pages> pages 604-609, </pages> <year> 1990. </year>
Reference-contexts: A particularly elegant formulation of this updating procedure results by application of statistical techniques such as the extended Kalman filter [52]. The reader is referred to [37] for details. Multiple Cameras Many systems utilizing position-based control with stereo vision from free-standing cameras have been demonstrated. For example, Allen <ref> [53] </ref> shows a system which can grasp a toy train using stereo vision. Rizzi [54] demonstrates a system which can bounce a ping-pong ball. All of these systems are EOL. Cipolla [31] describes an ECL system using free-standing stereo cameras. <p> To this end many reported implementations contrive the vision problem to be simple: e:g: painting objects white, using artificial targets, and so forth [54, 38, 12, 4]. Other authors use extremely task-specific clues: e:g: Allen <ref> [53] </ref> uses motion detection for locating a moving object to be grasped, and welding systems commonly use special filters that isolate the image of the welding tip. A review of tracking approaches used by researchers in this field is given in [3]. <p> For example, Rizzi [54] describes the use of a Newtonian flight dynamics model to make it possible to track a ping-pong ball during flight. Predictors based on ff fi tracking filters and Kalman filters have also been used <ref> [53, 37, 72] </ref>. Multiresolution techniques can be used provide further performance improvements, particularly when a dynamic model is not available and large search windows must be used. 1.6.4 Discussion Prior to executing or planning visually controlled motions, a specific set of visual features must be chosen.
Reference: [54] <author> M. K. Hu. </author> <title> Visual pattern recognition by moment invariants. </title> <journal> IRE Trans. Info. Theory, </journal> <volume> 8 </volume> <pages> 179-187, </pages> <month> February </month> <year> 1962. </year>
Reference-contexts: The reader is referred to [37] for details. Multiple Cameras Many systems utilizing position-based control with stereo vision from free-standing cameras have been demonstrated. For example, Allen [53] shows a system which can grasp a toy train using stereo vision. Rizzi <ref> [54] </ref> demonstrates a system which can bounce a ping-pong ball. All of these systems are EOL. Cipolla [31] describes an ECL system using free-standing stereo cameras. One novel feature of this system is the use of the affine projection model (Section 1.2.3) for the imaging geometry. <p> Hence, visual servoing pre-supposes the solution to a set of potentially difficult static and dynamic vision problems. To this end many reported implementations contrive the vision problem to be simple: e:g: painting objects white, using artificial targets, and so forth <ref> [54, 38, 12, 4] </ref>. Other authors use extremely task-specific clues: e:g: Allen [53] uses motion detection for locating a moving object to be grasped, and welding systems commonly use special filters that isolate the image of the welding tip. <p> However not all pixels in the image are of interest, and computation time can be greatly reduced if only a small region around each image feature is processed. Thus, a promising technique for making vision cheap and tractable is to use window-based tracking techniques <ref> [70, 54, 31] </ref>. Window-based methods have several advantages, among them; computational simplicity, little requirement for special hardware, and easy reconfiguration for different applications. <p> For example, Rizzi <ref> [54] </ref> describes the use of a Newtonian flight dynamics model to make it possible to track a ping-pong ball during flight. Predictors based on ff fi tracking filters and Kalman filters have also been used [53, 37, 72]. <p> If the contrast between the interior of the opening and area around it is high, then binary thresholding followed by a calculation of the first and second central moments can be used to localize the feature <ref> [54] </ref>. 2. If the ambient illumination changes greatly over time, but the brightness of the opening and the brightness of the surrounding region are roughly constant, a circular template could be localized using SSD methods augmented with brightness and contrast parameters. <p> The two most common problems are occlusion of features and and visual singularities. Solutions to the former include intelligent observers that note the disappearance of features and continue to predict their locations based on dynamics and/or feedforward information <ref> [54] </ref>, or redundant feature specifications that can perform even with some loss of information. Solution to the latter require some combination of intelligent path planning and/or intelligent acquisition and focus-of-attention to maintain the controllability of the system.
Reference: [55] <author> H. Inoue, T. Tachikawa, and M. Inaba. </author> <title> Robot vision server. </title> <booktitle> In Proc. 20th ISIR, </booktitle> <pages> pages 195-202, </pages> <year> 1989. </year>
Reference: [56] <author> H. Inoue, T. Tachikawa, and M. Inaba. </author> <title> Robot vision system with a correlation chip for real-time tracking, optical flow, and depth map generation. </title> <booktitle> In Proc. IEEE Int. Conf. Robotics and Automation, </booktitle> <pages> pages 1621-1626, </pages> <year> 1992. </year> - <title> 73 - Tutorial TT3: Visual Servo Control Hager/Hutchinson/Corke </title>
Reference-contexts: we have c x t ffi ( t P i t C) ( c b P i c C) = ( c R t Note that the final expression depends only on c R t : The corresponding least-squares problem can either be solved explicitly for c R t (see <ref> [56, 57, 58] </ref>), or solved incrementally using linearization.
Reference: [57] <author> M. Jagersand, O. Fuentes, and R. Nelson. </author> <title> Experimental evaluation of uncalibrated visual servoing for precision manipulation. </title> <journal> In Proc. IEEE Int. Conf. Robotics and Automation, </journal> <note> page to appear, </note> <year> 1996. </year>
Reference-contexts: we have c x t ffi ( t P i t C) ( c b P i c C) = ( c R t Note that the final expression depends only on c R t : The corresponding least-squares problem can either be solved explicitly for c R t (see <ref> [56, 57, 58] </ref>), or solved incrementally using linearization.
Reference: [58] <author> W. Jang and Z. </author> <title> Bien. Feature-based visual servoing of an eye-in-hand robot with improved tracking performance. </title> <booktitle> In Proc. IEEE Int. Conf. Robotics and Automation, </booktitle> <pages> pages 2254-2260, </pages> <year> 1991. </year>
Reference-contexts: we have c x t ffi ( t P i t C) ( c b P i c C) = ( c R t Note that the final expression depends only on c R t : The corresponding least-squares problem can either be solved explicitly for c R t (see <ref> [56, 57, 58] </ref>), or solved incrementally using linearization.
Reference: [59] <author> W. Jang, K. Kim, M. Chung, and Z. </author> <title> Bien. Concepts of augmented image space and transformed feature space for efficient visual servoing of an "eye-in-hand robot". </title> <journal> Robotica, </journal> <volume> 9 </volume> <pages> 203-212, </pages> <year> 1991. </year>
Reference-contexts: We define an image feature parameter to be any real-valued quantity that can be calculated from one or more image features. Examples include, moments, relationships between regions or vertices, and polygon face areas. Jang <ref> [59] </ref> provides a formal definition of what we term feature paramters as image functionals. Most commonly the coordinates of a feature point or a region centroid are used. <p> However, it still may cause problems when both cameras are free to move relative to one another. Feature-based approaches tend to be more appropriate to tasks where there is no prior model of the geometry of the task, for example in teleoperation applications <ref> [59] </ref>. Pose-based approaches inherently depend on an existing object model. The pose estimation problems inherent in many position-based servoing problems requires solution to a potentially difficult correspondence problem.
Reference: [60] <author> M. Kabuka, J. Desoto, and J. Miranda. </author> <title> Robot vision tracking system. </title> <journal> IEEE Trans. Ind. Electron., </journal> <volume> 35(1) </volume> <pages> 40-51, </pages> <month> February </month> <year> 1988. </year>
Reference: [61] <author> M. Kabuka, E. McVey, and P. Shironoshita. </author> <title> An adaptive approach to video tracking. </title> <journal> IEEE Trans. Robot. Autom., </journal> <volume> 4(2) </volume> <pages> 228-236, </pages> <month> April </month> <year> 1988. </year>
Reference: [62] <author> P. R. Kalata. </author> <title> The tracking index: a generalized parameter for ff fi and ff fi fl target trackers. </title> <journal> IEEE Trans. Aerosp. Electron. Syst., </journal> <volume> AES-20(2):174-182, </volume> <month> March </month> <year> 1984. </year>
Reference: [63] <author> P. Khosla, N. Papanikolopoulos, and B. Nelson. </author> <title> Dynamic sensor placement using controlled active vision. </title> <booktitle> In Proc. IFAC 12th World Congress, pages 9.419-9.422, </booktitle> <address> Sydney, </address> <year> 1993. </year>
Reference-contexts: Alternative derivations for this example can be found in a number of references including <ref> [63, 64] </ref>.
Reference: [64] <author> R. Kohler. </author> <title> A segmentation system based on thresholding. </title> <booktitle> Computer Graphics and Image Processing, </booktitle> <pages> pages 319-338, </pages> <year> 1981. </year>
Reference-contexts: Alternative derivations for this example can be found in a number of references including <ref> [63, 64] </ref>. <p> In this case, the solution is given by (1.47). For example, as shown in <ref> [64] </ref>, the null space of the image Jacobian given in (1.45), is spanned by the four vectors 2 6 6 6 6 u 0 3 7 7 7 7 2 6 6 6 6 0 0 v 3 7 7 7 7 2 6 6 6 6 uvz vz 0 3 <p> A window, R (x) = R (x; c ; ; t); extracted with these parameters would then have a vertical edge segment within it. Isolated step edges can be localized by determining the location of the maximum of the first derivative of the signal <ref> [64, 72, 74] </ref>. However, since derivatives tend to increase the noise in an image, most edge detection methods combine spatial derivatives with a smoothing operation to suppress spurious maxima. Both derivatives and smoothing are linear operations that can be computed using convolution operators. <p> Another edge detector which can be implemented without floating point arithmetic is the derivative of a triangle (DOT) kernal. In one dimension the DOT is defined as g (x) = signum (x): For a kernal three pixels wide, this is also known as the Prewitt operator <ref> [64] </ref>. Although the latter is not optimal from a signal processing point of view, convolution by the DOT can be implemented using only four additions per pixel. Thus, it is extremely fast to execute on simple hardware. Returning to detecting edge segments, convolutions are employed as follows.
Reference: [65] <author> M. Kuperstein. </author> <title> Generalized neural model for adaptive sensory-motor control of single postures. </title> <booktitle> In Proc. IEEE Int. Conf. Robotics and Automation, </booktitle> <pages> pages 140-143, </pages> <year> 1988. </year>
Reference-contexts: The least squares solution gives a value for _ r that minimizes the norm k _ f J v _ rk. We first consider the case k &gt; m, that is, there are more feature parameters than task degrees of freedom. By the implicit function theorem <ref> [65] </ref>, if, in some neighborhood of r, m k and rank (J v ) = m (i.e., J v is full rank), we can express the coordinates f m+1 : : : f k as smooth functions of f 1 : : : f m . &gt;From this, we deduce that
Reference: [66] <author> M. Leahy, V. Milholen, and R. Shipman. </author> <title> Robotic aircraft refueling: a concept demonstration. </title> <booktitle> In Proc. National Aerospace and Electronics Conf., </booktitle> <pages> pages 1145-50, </pages> <month> May </month> <year> 1990. </year>
Reference-contexts: It is interesting to note that these solutions to the point-to-line problem perform with an accuracy that is independent of calibration, whereas the position-based versions do not <ref> [66] </ref>. 1.5.6 Discussion One of the chief advantages to image-based control over position-based control is that the positioning accuracy of the system is less sensitive camera calibration. This is particularly true for ECL image-based systems. <p> This is particularly true for ECL image-based systems. For example, it is interesting to note that the ECL image-based solutions to the point-to-line positioning problem perform with an accuracy that is independent of calibration, whereas the position-based versions do not <ref> [66] </ref>. It is important to note, however, that most of the image-based control methods appearing in the literature still rely on an estimate of point position or target pose to parameterize the Jacobian. In practice, the unknown parameter for Jacobian calculation is distance from the camera.
Reference: [67] <author> Z. Lin, V. Zeman, and R. V. Patel. </author> <title> On-line robot trajectory planning for catching a moving object. </title> <booktitle> In Proc. IEEE Int. Conf. Robotics and Automation, </booktitle> <pages> pages 1726-1731, </pages> <year> 1989. </year>
Reference-contexts: In practice, the unknown parameter for Jacobian calculation is distance from the camera. Some recent papers present adaptive approaches for estimating [14] this depth value, or develop feedback methods which do not use depth in the feedback formulation <ref> [67] </ref>. There are often computational advantages to image-based control, particularly in ECL configurations. For example, a position-based relative pose solution for an ECL single-camera system must perform two nonlinear least squares optimizations in order to compute the error function.
Reference: [68] <author> A. G. Makhlin. </author> <title> Stability and sensitivity of servo vision systems. </title> <booktitle> Proc 5th Int Conf on Robot Vision and Sensory Controls - RoViSeC 5, </booktitle> <pages> pages 79-89, </pages> <month> October </month> <year> 1985. </year>
Reference: [69] <author> B. </author> <title> Mel. Connectionist Robot Motion Planning. </title> <publisher> Academic Press, </publisher> <year> 1990. </year>
Reference: [70] <author> W. Miller. </author> <title> Sensor-based control of robotic manipulators using a general learning algorithm. </title> <journal> IEEE Trans. Robot. Autom., </journal> <volume> 3(2) </volume> <pages> 157-165, </pages> <month> April </month> <year> 1987. </year>
Reference-contexts: However not all pixels in the image are of interest, and computation time can be greatly reduced if only a small region around each image feature is processed. Thus, a promising technique for making vision cheap and tractable is to use window-based tracking techniques <ref> [70, 54, 31] </ref>. Window-based methods have several advantages, among them; computational simplicity, little requirement for special hardware, and easy reconfiguration for different applications. <p> In this case, (1.59) must also include parameters for scaling and aspect ratio <ref> [70] </ref>. 3. The opening could be selected in an initial image, and subsequently located using SSD methods. This differs from the previous method in that this calculation does not compute the center of the opening, only its correlation with the starting image. <p> It is probably safe to say that image processing presents the greatest challenge to general-purpose hand-eye coordination. As an effort to help overcome this obstacle, the methods described above and other related methods have been incorporated into a publically available "toolkit." The interested reader is referred to <ref> [70] </ref> for details. 1.7 Related Issues In this section, we briefly discuss a number of related issues that were not addressed in the tutorial. 1.7.1 Image-Based versus Position-Based Control The taxonomy of visual servo introduced in Section 1.1 has four major architectural classes.
Reference: [71] <author> J. Mochizuki, M. Takahashi, and S. Hata. </author> <title> Unpositioned workpieces handling robot with visual and force sensors. </title> <journal> IEEE Trans. Ind. Electron., </journal> <volume> 34(1) </volume> <pages> 1-4, </pages> <month> February </month> <year> 1987. </year> - <title> 74 - Tutorial TT3: Visual Servo Control Hager/Hutchinson/Corke </title>
Reference: [72] <author> S. Negahdaripour and J. Fox. </author> <title> Undersea optical stationkeeping: Improved methods. J. Robot. </title> <journal> Syst., </journal> <volume> 8(3) </volume> <pages> 319-338, </pages> <year> 1991. </year>
Reference-contexts: In keeping with the minimalist approach of this tutorial, we concentrate on describing the window-based approach to tracking of features in an image. A discussion of methods which use specialized hardware combined with temporal and geometric constraints can be found in <ref> [72] </ref>. The remainder of this section is organized as follows. Section 1.6.1 describes how window-based methods can be used to implement fast detection of edge segments, a common low-level primitive for vision applications. Section 1.6.2 describe an approach based on temporally correlating image regions over time. <p> A window, R (x) = R (x; c ; ; t); extracted with these parameters would then have a vertical edge segment within it. Isolated step edges can be localized by determining the location of the maximum of the first derivative of the signal <ref> [64, 72, 74] </ref>. However, since derivatives tend to increase the noise in an image, most edge detection methods combine spatial derivatives with a smoothing operation to suppress spurious maxima. Both derivatives and smoothing are linear operations that can be computed using convolution operators. <p> For example, Rizzi [54] describes the use of a Newtonian flight dynamics model to make it possible to track a ping-pong ball during flight. Predictors based on ff fi tracking filters and Kalman filters have also been used <ref> [53, 37, 72] </ref>. Multiresolution techniques can be used provide further performance improvements, particularly when a dynamic model is not available and large search windows must be used. 1.6.4 Discussion Prior to executing or planning visually controlled motions, a specific set of visual features must be chosen.
Reference: [73] <author> R. Nevatia. </author> <title> Depth measurement by motion stereo. </title> <journal> Computer Graphics and Image Processing, </journal> <volume> 5 </volume> <pages> 203-214, </pages> <year> 1976. </year>
Reference: [74] <author> N. Papanikolopoulos, P. Khosla, and T. Kanade. </author> <title> Adaptive robot visual tracking. </title> <booktitle> In Proc. American Control Conference, </booktitle> <pages> pages 962-967, </pages> <year> 1991. </year>
Reference-contexts: A window, R (x) = R (x; c ; ; t); extracted with these parameters would then have a vertical edge segment within it. Isolated step edges can be localized by determining the location of the maximum of the first derivative of the signal <ref> [64, 72, 74] </ref>. However, since derivatives tend to increase the noise in an image, most edge detection methods combine spatial derivatives with a smoothing operation to suppress spurious maxima. Both derivatives and smoothing are linear operations that can be computed using convolution operators.
Reference: [75] <author> N. Papanikolopoulos, P. Khosla, and T. Kanade. </author> <title> Vision and control techniques for robotic visual tracking. </title> <booktitle> In Proc. IEEE Int. Conf. Robotics and Automation, </booktitle> <pages> pages 857-864, </pages> <year> 1991. </year>
Reference-contexts: Hence, smoothing and differentiation can be combined into a single convolution template. An extremely popular convolution kernel is the derivative of a Gaussian (DOG) <ref> [75] </ref>. In one dimension, the DOG is defined as g (x) = x exp (x 2 = 2 ) where is a design parameter governing the amount of smoothing that takes place. Although the DOG has been demonstrated to be the optimal filter for detecting step edges [75], it requires floating <p> a Gaussian (DOG) <ref> [75] </ref>. In one dimension, the DOG is defined as g (x) = x exp (x 2 = 2 ) where is a design parameter governing the amount of smoothing that takes place. Although the DOG has been demonstrated to be the optimal filter for detecting step edges [75], it requires floating point arithmetic to be computed accurately. Another edge detector which can be implemented without floating point arithmetic is the derivative of a triangle (DOT) kernal.
Reference: [76] <author> N. Papanikolopoulos and P. Khosla. </author> <title> Shared and traded telerobotic visual control. </title> <booktitle> In Proc. IEEE Int. Conf. Robotics and Automation, </booktitle> <pages> pages 878-885, </pages> <year> 1992. </year>
Reference: [77] <author> R. P. Paul. </author> <title> Robot Manipulators: Mathematics, Programming, and Control. </title> <publisher> MIT Press, </publisher> <address> Cambridge, Massachusetts, </address> <year> 1981. </year>
Reference: [78] <author> R. P. Paul and H. Zhang. </author> <title> Design of a robot force/motion server. </title> <booktitle> In Proc. IEEE Int. Conf. Robotics and Automation, </booktitle> <volume> volume 3, </volume> <pages> pages 1878-83, </pages> <address> Washington , USA, </address> <year> 1986. </year>
Reference: [79] <author> T. </author> <title> Pool. Motion Control of a Citrus-Picking Robot. </title> <type> PhD thesis, </type> <institution> University of Florida, </institution> <year> 1989. </year>
Reference: [80] <author> R. E. Prajoux. </author> <title> Visual tracking. </title> <editor> In D. Nitzan et al., editors, </editor> <booktitle> Machine intelligence research applied to industrial automation, </booktitle> <pages> pages 17-37. </pages> <institution> SRI International, </institution> <month> August </month> <year> 1979. </year>
Reference-contexts: Such situations call for the use of more global task constraints (e:g: the geometry of several edges), more global tracking (e:g: extended contours or snakes <ref> [80] </ref>), or improved or specialized detection methods. To illustrate these tradeoffs, suppose a visual servoing task relies on tracking the image of a circular opening over time. In general, the opening will project to an ellipse in the camera.
Reference: [81] <author> J. Pretlove and G. Parker. </author> <title> The development of a real-time stereo-vision system to aid robot guidance in carrying out a typical manufacturing task. </title> <booktitle> In Proc. 22nd ISRR, pages 21.1-21.23, </booktitle> <address> Detroit, </address> <year> 1991. </year>
Reference-contexts: One novel feature of this system is the use of the affine projection model (Section 1.2.3) for the imaging geometry. This leads to linear calibration and control at the cost of some system performance. The development of a position-based stereo eye-in-hand servoing system has also been reported <ref> [81] </ref>. Multiple cameras greatly simplify the reconstruction process as illustrated below. <p> Weiss's proposed image-based direct visual-servoing structure does away entirely with axis sensors | dynamics and kinematics are controlled adaptively based on visual feature data. This concept has a certain appeal but in practice is overly complex to implement and appears to lack robustness (see, e.g., <ref> [81] </ref> for an analysis of the effects of various image distortions on such control schemes). The concepts have only ever been demonstrated in simulation for up to 3-DOF and then with simplistic models of axis dynamics which ignore `real world' effects such as Coulomb friction and stiction.
Reference: [82] <author> P. Rives, F. Chaumette, and B. Espiau. </author> <title> Positioning of a robot with respect to an object, tracking it and estimating its velocity by visual servoing. </title> <editor> In V. Hayward and O. Khatib, editors, </editor> <booktitle> Experimental Robotics 1, volume 139 of Lecture Notes in Control and Information Sciences, </booktitle> <pages> pages 412-428. </pages> <publisher> Springer-Verlag, </publisher> <year> 1989. </year>
Reference-contexts: Other issues for consideration include whether or not the vision system should `close the loop' around robot axes which are position, velocity or torque controlled. A detailed discussion of these dynamic issues in visual servo systems is given by Corke <ref> [20, 82] </ref>. - 38 - Tutorial TT3: Visual Servo Control Hager/Hutchinson/Corke 1.7.3 Mobile robots The discussion above has assumed that the moving camera is mounted on an arm type robot manipulator.
Reference: [83] <author> A. Rizzi and D. Koditschek. </author> <title> Preliminary experiments in spatial robot juggling. </title> <booktitle> In Proc. 2nd International Symposium on Experimental Robotics, </booktitle> <address> Toulouse, France, </address> <month> June </month> <year> 1991. </year>
Reference-contexts: Most of the techniques described above are directly applicable to the mobile robot case. Visual servoing can be used for navigation with respect to landmarks or obstacle and to control docking (see, e.g., <ref> [83] </ref>). 1.7.4 A Light-Weight Tracking and Servoing Environment The design of many task-specific visual tracking and vision-based feedback systems used in visual servoing places a strong emphasis on system modularity and reconfig-urability. This has motivated the development of a modular, software-based visual tracking system for experimental vision-based robotic applications [84]. <p> These include control issues, such as adaptive visual servo control [85, 14], hybrid control (e.g., hybrid vision/position control [12], or hybrid force/vision control), and multi-rate system theory [86]; issues related to automatic planning of visually controlled robot motions [87, 88]; applications in mobile robotics, including non-holonmoic systems <ref> [83] </ref>; and, feature selection [33, 36]. Many of these are describe in the proceedings of a recent workshop on visual servo control [89]. - 39 - Tutorial TT3: Visual Servo Control Hager/Hutchinson/Corke 1.7.6 The future The future for applications of visual servoing should be bright.
Reference: [84] <author> M. Roberts. </author> <title> Control of resonant robotic systems. </title> <type> Master's thesis, </type> <institution> University of Newcastle, Australia, </institution> <month> March </month> <year> 1991. </year>
Reference-contexts: This has motivated the development of a modular, software-based visual tracking system for experimental vision-based robotic applications <ref> [84] </ref>. The system design emphasizes flexibility and efficiency on standard scientific workstations and PC's. The system is intended to be a portable, inexpensive tool for rapid prototyping and experimentation for teaching and research. The system is written as a set of classes in C++.
Reference: [85] <author> C. Rosen et al. </author> <title> Machine intelligence research applied to industrial automation. </title> <type> Sixth report. Technical report, </type> <institution> SRI International, </institution> <year> 1976. </year>
Reference-contexts: These include control issues, such as adaptive visual servo control <ref> [85, 14] </ref>, hybrid control (e.g., hybrid vision/position control [12], or hybrid force/vision control), and multi-rate system theory [86]; issues related to automatic planning of visually controlled robot motions [87, 88]; applications in mobile robotics, including non-holonmoic systems [83]; and, feature selection [33, 36].
Reference: [86] <author> C. Rosen et al. </author> <title> Machine intelligence research applied to industrial automation. </title> <type> Eighth report. Technical report, </type> <institution> SRI International, </institution> <year> 1978. </year>
Reference-contexts: These include control issues, such as adaptive visual servo control [85, 14], hybrid control (e.g., hybrid vision/position control [12], or hybrid force/vision control), and multi-rate system theory <ref> [86] </ref>; issues related to automatic planning of visually controlled robot motions [87, 88]; applications in mobile robotics, including non-holonmoic systems [83]; and, feature selection [33, 36].
Reference: [87] <author> A. Rosenfeld and A. C. Kak. </author> <title> Digital Picture Processing. </title> <publisher> Academic Press, </publisher> <year> 1982. </year> - <title> 75 - Tutorial TT3: Visual Servo Control Hager/Hutchinson/Corke </title>
Reference-contexts: These include control issues, such as adaptive visual servo control [85, 14], hybrid control (e.g., hybrid vision/position control [12], or hybrid force/vision control), and multi-rate system theory [86]; issues related to automatic planning of visually controlled robot motions <ref> [87, 88] </ref>; applications in mobile robotics, including non-holonmoic systems [83]; and, feature selection [33, 36].
Reference: [88] <author> P. K. Sahoo, S. Soltani, and A. K. C. Wong. </author> <title> A survey of thresholding techniques. Computer Vision, </title> <journal> Graphics, and Image Processing, </journal> <volume> 41 </volume> <pages> 233-260, </pages> <year> 1988. </year>
Reference-contexts: These include control issues, such as adaptive visual servo control [85, 14], hybrid control (e.g., hybrid vision/position control [12], or hybrid force/vision control), and multi-rate system theory [86]; issues related to automatic planning of visually controlled robot motions <ref> [87, 88] </ref>; applications in mobile robotics, including non-holonmoic systems [83]; and, feature selection [33, 36].
Reference: [89] <author> T. Sakaguchi, M. Fujita, H. Watanabe, and F. Miyazaki. </author> <title> Motion planning and control for a robot performer. </title> <booktitle> In Proc. IEEE Int. Conf. Robotics and Automation, </booktitle> <pages> pages 925-931, </pages> <year> 1993. </year>
Reference-contexts: Many of these are describe in the proceedings of a recent workshop on visual servo control <ref> [89] </ref>. - 39 - Tutorial TT3: Visual Servo Control Hager/Hutchinson/Corke 1.7.6 The future The future for applications of visual servoing should be bright. Camera's are relatively inexpensive devices and the cost of image processing systems continues to fall.
Reference: [90] <author> S. Sawano, J. Ikeda, N. Utsumi, H. Kiba, Y. Ohtani, and A. Kikuchi. </author> <title> A sealing robot system with visual seam tracking. </title> <booktitle> In Proc. Int. Conf. on Advanced Robotics, </booktitle> <pages> pages 351-8, </pages> <address> Tokyo, </address> <month> September </month> <year> 1983. </year> <institution> Japan Ind. Robot Assoc., </institution> <address> Tokyo, Japan. </address>
Reference: [91] <author> P. Sharkey, I. Reid, P. McLauchlan, and D. Murray. </author> <title> Real-time control of a reactive stereo head/eye platform. </title> <booktitle> Proc. 29th CDC, </booktitle> <pages> pages CO.1.2.1-CO.1.2.5, </pages> <year> 1992. </year>
Reference: [92] <author> Y. Shirai and H. Inoue. </author> <title> Guiding a robot by visual feedback in assembling tasks. </title> <journal> Pattern Recognition, </journal> <volume> 5 </volume> <pages> 99-108, </pages> <year> 1973. </year>
Reference-contexts: Vision is a useful robotic sensor since it mimics the human sense of vision and allows for non-contact measurement of the environment. Since the seminal work of Shirai and Inoue <ref> [92] </ref> (who describe how a visual feedback loop can be used to correct the position of a robot to increase task accuracy), considerable effort has been devoted to the visual control of robot manipulators. Robot controllers with fully integrated vision systems are now available from a number of vendors.
Reference: [93] <author> S. Skaar, W. Brockman, and R. Hanson. </author> <title> Camera-space manipulation. </title> <journal> Int. J. Robot. Res., </journal> <volume> 6(4) </volume> <pages> 20-32, </pages> <year> 1987. </year>
Reference-contexts: Image feature parameters that have been used for visual servo control include the image plane coordinates of points in the image <ref> [12, 10, 48, 14, 15, 93, 17] </ref>, the distance between two points in the image plane and the orientation of the line connecting those two points [38, 36], perceived edge length [19], the area of a projected surface and the relative areas of two projected surfaces [19], the centroid and higher
Reference: [94] <author> G. Skofteland and G. Hirzinger. </author> <title> Computing position and orientation of a freefly-ing polyhedron from 3D data. </title> <booktitle> In Proc. IEEE Int. Conf. Robotics and Automation, </booktitle> <pages> pages 150-155, </pages> <year> 1991. </year>
Reference: [95] <author> M. Srinivasan, M. Lehrer, S. Zhang, and G. Horridge. </author> <title> How honeybees measure their distance from objects of unknown size. </title> <journal> J. Comp. Physiol. A, </journal> <volume> 165 </volume> <pages> 605-613, </pages> <year> 1989. </year>
Reference: [96] <author> G. Stange, M. Srinivasan, and J. Dalczynski. </author> <title> Rangefinder based on intensity gradient measurement. </title> <journal> Applied Optics, </journal> <volume> 30(13) </volume> <pages> 1695-1700, </pages> <month> May </month> <year> 1991. </year>
Reference: [97] <author> A. R. Tate. </author> <title> Closed loop force control for a robotic grinding system. </title> <type> Master's thesis, </type> <institution> Massachusetts Institute of Technology, </institution> <address> Cambridge, Massachsetts, </address> <year> 1986. </year>
Reference: [98] <author> F. Tendick, J. Voichick, G. Tharp, and L. Stark. </author> <title> A supervisory telerobotic control system using model-based vision feedback. </title> <booktitle> In Proc. IEEE Int. Conf. Robotics and Automation, </booktitle> <pages> pages 2280-2285, </pages> <year> 1991. </year>
Reference: [99] <author> M. Tomizuka. </author> <title> Zero phase error tracking algorithm for digital control. </title> <journal> Journal of Dynamic Systems, Measurement and Control, </journal> <volume> 109 </volume> <pages> 65-68, </pages> <month> March </month> <year> 1987. </year>
Reference: [100] <author> J. Urban, G. Motyl, and J. Gallice. </author> <title> Real-time visual servoing using controlled illumination. </title> <journal> Int. J. Robot. Res., </journal> <volume> 13(1) </volume> <pages> 93-100, </pages> <month> February </month> <year> 1994. </year>
Reference: [101] <author> S. Venkatesan and C. Archibald. </author> <title> Realtime tracking in five degrees of freedom using two wrist-mounted laser range finders. </title> <booktitle> In Proc. IEEE Int. Conf. Robotics and Automation, </booktitle> <pages> pages 2004-2010, </pages> <year> 1990. </year>
Reference-contexts: Many of these problems can be circumvented by sensing target pose directly using a 3D sensor. Active 3D sensors based on structured lighting are now compact and fast enough to use for visual servoing. If the sensor is small and mounted on the robot <ref> [2, 101, 33] </ref> the depth and orientation information can be used for position-based visual servoing. 1.5 Image-Based Control As described in Section 1.3, in image-based visual servo control the error signal is defined directly in terms of image feature parameters (in contrast to position-based methods that define the error signal in
Reference: [102] <author> A. Verri and T. Poggio. </author> <title> Motion field and optical flow: Qualitative properties. </title> <journal> IEEE Trans. Pattern Anal. Mach. Intell., </journal> <volume> 11(5) </volume> <pages> 490-498, </pages> <month> May </month> <year> 1989. </year>
Reference: [103] <institution> Vision Systems Limited, Technology Park, Adelaide. APA-512MX Area Parameter Accelerator User Manual, </institution> <month> October </month> <year> 1987. </year> - <title> 76 - Tutorial TT3: Visual Servo Control Hager/Hutchinson/Corke </title>
Reference: [104] <author> P. Vuylsteke, P. Defraeye, A. Oosterlinck, and H. V. den Berghe. </author> <title> Video rate recognition of plane objects. </title> <journal> Sensor Review, </journal> <pages> pages 132-135, </pages> <month> July </month> <year> 1981. </year>
Reference: [105] <author> J. Wang and G. Beni. </author> <title> Connectivity analysis of multi-dimensional multi-valued images. </title> <booktitle> In Proc. IEEE Int. Conf. Robotics and Automation, </booktitle> <pages> pages 1731-1736, </pages> <year> 1987. </year>
Reference: [106] <author> J. Wang and W. J. Wilson. </author> <title> Three-D relative position and orientation estimation using Kalman filter for robot control. </title> <booktitle> In Proc. IEEE Int. Conf. Robotics and Automation, </booktitle> <pages> pages 2638-2645, </pages> <year> 1992. </year>
Reference: [107] <author> A. Wavering, J. Fiala, K. Roberts, and R. Lumia. Triclops: </author> <title> A high-performance trinocular active vision system. </title> <booktitle> In Proc. IEEE Int. Conf. Robotics and Automation, </booktitle> <pages> pages 410-417, </pages> <year> 1993. </year>
Reference: [108] <author> T. Webber and R. Hollis. </author> <title> A vision based correlator to actively damp vibrations of a coarse-fine manipulator. </title> <type> RC 14147 (63381), </type> <institution> IBM T.J. Watson Research Center, </institution> <month> October </month> <year> 1988. </year>
Reference: [109] <author> L. Weiss. </author> <title> Dynamic Visual Servo Control of Robots: an Adaptive Image-Based Approach. </title> <type> PhD thesis, </type> <institution> Carnegie-Mellon University, </institution> <year> 1984. </year>
Reference: [110] <author> D. B. Westmore and W. J. Wilson. </author> <title> Direct dynamic control of a robot using an end-point mounted camera and Kalman filter position estimation. </title> <booktitle> In Proc. IEEE Int. Conf. Robotics and Automation, </booktitle> <pages> pages 2376-2384, </pages> <year> 1991. </year>
Reference: [111] <author> J. S. Weszka. </author> <title> A survey of threshold selection techniques. </title> <journal> Computer Graphics and Image Processing, </journal> <volume> 7 </volume> <pages> 259-265, </pages> <year> 1978. </year>
Reference: [112] <author> W. Wichman. </author> <title> Use of optical feedback in the computer control of an arm. </title> <type> AI memo 55, </type> <institution> Stanford AI project, </institution> <month> August </month> <year> 1967. </year>
Reference: [113] <author> J. Wilf and R. Cunningham. </author> <title> Computing region moments from boundary representations. </title> <type> JPL 79-45, </type> <institution> NASA JPL, </institution> <month> November </month> <year> 1979. </year>
Reference: [114] <author> W. Wilson. </author> <title> Visual servo control of robots using Kalman filter estimates of relative pose. </title> <booktitle> In Proc. IFAC 12th World Congress, </booktitle> <pages> pages 9-399 to 9-404, </pages> <address> Sydney, </address> <year> 1993. </year>
Reference: [115] <author> P. Wolf. </author> <title> Elements of Photogrammetry. </title> <publisher> McGraw-Hill, </publisher> <year> 1974. </year>
Reference: [116] <author> J.-C. Yuan. </author> <title> A general photogrammetric method for determining object position and orientation. </title> <journal> IEEE Trans. Robot. Autom., </journal> <volume> 5(2) </volume> <pages> 129-142, </pages> <month> April </month> <year> 1989. </year>
Reference: [117] <author> J.-C. Yuan, F. Keung, and R. MacDonald. </author> <title> Telerobotic tracker. </title> <type> Patent EP 0 323 681 A1, European Patent Office, </type> <month> Filed </month> <year> 1988. </year>

References-found: 205

