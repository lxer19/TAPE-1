URL: http://www.cs.duke.edu/~mlittman/docs/rls-compact.ps
Refering-URL: 
Root-URL: 
Email: (lpk@cs.brown.edu)  (mlittman@cs.brown.edu)  (awm@cs.cmu.edu)  
Title: Reinforcement Learning: A Survey  
Author: Leslie Pack Kaelbling Michael L. Littman Andrew W. Moore 
Date: September 7, 1995  
Abstract: This paper surveys the field of reinforcement learning from a computer-science perspective. It is written to be accessible to researchers familiar with machine learning. Both the historical basis of the field and a broad selection of current work are summarized. Reinforcement learning is the problem faced by an agent that learns behavior through trial-and-error interactions with a dynamic environment. The work described here has a resemblance to work in psychology, but differs considerably in the details and in the use of the word "reinforcement." The paper discusses central issues of reinforcement learning, including trading off exploration and exploitation, establishing the foundations of the field via Markov decision theory, learning from delayed reinforcement, constructing empirical models to accelerate learning, making use of generalization and hierarchy, and coping with hidden state. It concludes with a survey of some implemented systems and an assessment of the practical utility of current methods for reinforcement learning.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> David H. Ackley and Michael L. Littman. </author> <title> Generalization and scaling in reinforcement learning. </title> <editor> In D. S. Touretzky, editor, </editor> <booktitle> Advances in Neural Information Processing Systems, </booktitle> <volume> volume 2, </volume> <pages> pages 550-557, </pages> <address> San Mateo, CA, 1990. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: The reinforcement-learning architectures and algorithms discussed above have included the storage of a variety of mappings, including S ! A (policies), S ! &lt; (value functions), S fi A ! &lt; (Q functions and rewards), S fiA ! S (deterministic transitions), and S fiAfiS ! <ref> [0; 1] </ref> (transition probabilities). Some of these mappings, such as transitions and immediate rewards, can be learned using straightforward supervised learning, and can be handled using any of the wide variety of function-approximation techniques for supervised learning that support noisy training examples. <p> Such algorithms can and have been used in the context of a delayed reinforcement, for instance, as the RL component in the AHC architecture described in Section 4.1. They can also be generalized to real-valued reward through reward comparison methods [89]. CRBP The complementary reinforcement backpropagation algorithm <ref> [1] </ref> (crbp) consists of a feed-forward network mapping an encoding of the state to an encoding of the action.
Reference: [2] <author> James S. Albus. </author> <title> Brains, Behavior, and Robotics. </title> <publisher> BYTE Books, Subsidiary of McGraw-Hill, </publisher> <address> Peterborough, New Hampshire, </address> <year> 1981. </year>
Reference-contexts: Some of these mappings, such as transitions and immediate rewards, can be learned using straightforward supervised learning, and can be handled using any of the wide variety of function-approximation techniques for supervised learning that support noisy training examples. Popular methods include various neural-network methods [76], CMAC <ref> [2] </ref>, and local memory-based methods [67], such as generalizations of nearest neighbor methods.
Reference: [3] <author> Charles W. Anderson. </author> <title> Learning and Problem Solving with Multilayer Connectionist Systems. </title> <type> PhD thesis, </type> <institution> University of Massachusetts, </institution> <address> Amherst, MA, </address> <year> 1986. </year>
Reference-contexts: The second network is trained in a standard supervised mode to estimate r as a function of the input state s. Variations of this approach have been used in a variety of applications <ref> [3, 7, 46, 89] </ref>. reinforce Algorithms Williams [104, 105] studied the problem of choosing actions to maximize immedate reward. He identified a broad class of update rules that perform gradient descent on the expected reward and showed how to integrate these rules with backpropagation.
Reference: [4] <author> Rachita (Ronny) Ashar. </author> <title> Hierarchical learning in stochastic domains. </title> <type> Master's thesis, </type> <institution> Brown University, </institution> <address> Providence, Rhode Island, </address> <year> 1994. </year>
Reference-contexts: The HDG algorithm works by analogy with navigation in a harbor. The environment is partitioned (a priori, but more recent work addresses the case of learning the partition <ref> [4] </ref>) into a set of regions whose centers are known as "landmarks." If the agent is currently in the same region as the goal, then it uses low-level actions to move to the goal.
Reference: [5] <author> Leemon C. Baird and A. H. Klopf. </author> <title> Reinforcement learning with high-dimensional, continuous actions. </title> <type> Technical report, </type> <institution> Wright-Patterson Air Force Base Ohio: Wright Laboratory, </institution> <year> 1993. </year> <note> Technical Report WL-TR-93-1147. </note>
Reference-contexts: Training such a network is not conceptually difficult, but using the network to find the optimal action can be a challenge. One method is to do a local gradient-ascent search on the action in order to find one with high value <ref> [5] </ref>. Gullapalli [31, 32] has developed a "neural" reinforcement-learning unit for use in continuous action spaces. The unit generates actions with a normal distribution; it adjusts the mean and variance based on previous experience.
Reference: [6] <author> Andrew G. Barto, S. J. Bradtke, and Satinder P. Singh. </author> <title> Learning to act using real-time dynamic programming. </title> <journal> Artificial Intelligence, </journal> <volume> 72(1) </volume> <pages> 81-138, </pages> <year> 1995. </year>
Reference-contexts: RTDP (real-time dynamic programming) <ref> [6] </ref> is another model-based method that uses Q-learning to concentrate computational effort on the areas of the state-space that the agent is most likely to occupy.
Reference: [7] <author> Andrew G. Barto, Richard S. Sutton, and Charles W. Anderson. </author> <title> Neuronlike adaptive elements that can solve difficult learning control problems. </title> <journal> IEEE Transactions on Systems, Man, and Cybernetics, </journal> <volume> SMC-13(5):834-846, </volume> <year> 1983. </year>
Reference-contexts: This class of algorithms is known as temporal difference methods [90]. We will consider two different temporal-difference learning strategies for the discounted infinite-horizon model. 12 4.1 Adaptive Heuristic Critic and T D () The adaptive heuristic critic algorithm is a learning analog of policy iteration <ref> [7] </ref> in which the value-function computation is no longer implemented by solving a set of linear equations, but is instead computed by an algorithm called T D (0). A block diagram for this approach is given in component (labeled RL). <p> The second network is trained in a standard supervised mode to estimate r as a function of the input state s. Variations of this approach have been used in a variety of applications <ref> [3, 7, 46, 89] </ref>. reinforce Algorithms Williams [104, 105] studied the problem of choosing actions to maximize immedate reward. He identified a broad class of update rules that perform gradient descent on the expected reward and showed how to integrate these rules with backpropagation.
Reference: [8] <author> Richard Bellman. </author> <title> Dynamic Programming. </title> <publisher> Princeton University Press, </publisher> <address> Princeton, NJ, </address> <year> 1957. </year>
Reference-contexts: The reward function specifies expected instantaneous reward as a function of the current state and action. The model is Markov if the state transitions are independent of any previous environment states or agent actions. There are many good references to MDP models <ref> [8, 11, 35, 73] </ref>. 3.2 Finding a Policy Given a Model Before we consider algorithms for learning to behave in MDP environments, we will explore techniques for determining the optimal policy given a correct model. These techniques will serve as the foundation and inspiration for the learning algorithms to follow. <p> It can be determined by a simple iterative algorithm called value iteration that can be shown to converge to the correct V fl values <ref> [8, 11] </ref>. initialize V (s) arbitrarily loop until policy good enough loop for s 2 S Q (s; a) := R (s; a) + fl s 0 2S T (s; a; s 0 )V (s 0 ) end loop end loop It is not obvious when to stop the value iteration
Reference: [9] <author> Donald A. Berry and Bert Fristedt. </author> <title> Bandit Problems: Sequential Allocation of Experiments. </title> <publisher> Chapman and Hall, </publisher> <address> London, UK, </address> <year> 1985. </year>
Reference-contexts: A more appropriate measure, then, is the expected decrease in reward gained due to executing the learning algorithm instead of behaving optimally from the very beginning. This measure is known as regret <ref> [9] </ref>. It penalizes mistakes wherever they occur during the run. <p> for Q-learning (described in Section 4.2) that sheds some light on the connection between the two views. 2 Exploitation versus Exploration: The Single-State Case In this section we consider the k-armed bandit problem, which been the subject of a great deal of study in the statistics and applied mathematics literature <ref> [9] </ref>. The agent is in a room with a collection of k gambling machines (each called a "one-armed bandit" in colloquial English). The agent is permitted a fixed number of pulls, h. Any arm may be pulled on each turn. <p> There is a wide variety of solutions to this problem. We will consider a representative selection of them, but for a deeper discussion and a number of important theoretical results, see the book by Berry and Fristedt <ref> [9] </ref>. We use the term "action" to indicate the agent's choice of arm to pull. This eases the transition into delayed reinforcement models in Section 3. <p> use in practice, and can be applied (with similar lack of guarantee) to the general case. 2.1 Formally-justified Techniques 2.1.1 Dynamic-Programming Approach If the agent is going to be playing the game for a total of h steps, it can use basic Bayesian reasoning to solve for an optimal strategy <ref> [9] </ref>. This requires an assumed prior joint distribution for the parameters fp i g, the most natural of which is that each p i is independently uniformly distributed between 0 and 1. We compute a mapping from belief states (summaries of the agent's experiences during this run) to actions.
Reference: [10] <author> D. P. Bertsekas and D. A. Casta~non. </author> <title> Adaptive aggregation for infinite horizon dynamic programming. </title> <journal> IEEE Transactions on Automatic Control, </journal> <volume> 34(6) </volume> <pages> 589-598, </pages> <year> 1989. </year> <month> 33 </month>
Reference-contexts: Multigrid methods can be used to quickly seed a good initial approximation to a high resolution value function by initially performing value iteration at a coarser resolution [75]. State aggregation works by collapsing groups of states to a single meta-state solving the abstracted problem <ref> [10] </ref>. 4 Learning an Optimal Policy: Model-free Methods In the previous section we reviewed methods for obtaining an optimal policy for an MDP assuming that we already had a model.
Reference: [11] <author> Dmitri P. Bertsekas. </author> <title> Dynamic Programming: Deterministic and Stochastic Models. </title> <publisher> Prentice--Hall, </publisher> <address> Englewood Cliffs, N.J., </address> <year> 1987. </year>
Reference-contexts: The reward function specifies expected instantaneous reward as a function of the current state and action. The model is Markov if the state transitions are independent of any previous environment states or agent actions. There are many good references to MDP models <ref> [8, 11, 35, 73] </ref>. 3.2 Finding a Policy Given a Model Before we consider algorithms for learning to behave in MDP environments, we will explore techniques for determining the optimal policy given a correct model. These techniques will serve as the foundation and inspiration for the learning algorithms to follow. <p> It can be determined by a simple iterative algorithm called value iteration that can be shown to converge to the correct V fl values <ref> [8, 11] </ref>. initialize V (s) arbitrarily loop until policy good enough loop for s 2 S Q (s; a) := R (s; a) + fl s 0 2S T (s; a; s 0 )V (s 0 ) end loop end loop It is not obvious when to stop the value iteration <p> This provides an effective stopping criterion for the algorithm. Another important result is that the greedy policy is guaranteed to be optimal in some finite number of steps even though the value function may not have converged <ref> [11] </ref>. And in practice, the greedy policy is often optimal long before the value function has converged. Value iteration is very flexible.
Reference: [12] <author> Dmitri P. Bertsekas and John N. Tsitsiklis. </author> <title> Parallel and Distributed Computation: Numerical Methods. </title> <publisher> Prentice-Hall, </publisher> <address> Englewood Cliffs, NJ, </address> <year> 1989. </year>
Reference-contexts: Value iteration is very flexible. The assignments to V need not be done in strict order as shown above, but instead can occur asynchronously in parallel provided that the value value of every state gets updated infinitely often on an infinite run. These issues are treated extensively by Bertsekas <ref> [12] </ref>, who also proves convergence results. Updates based on Equation 1 are known as full backups since they make use of information from all possible successor states.
Reference: [13] <author> G. E. P. Box and N. R. Draper. </author> <title> Empirical Model-Building and Response Surfaces. </title> <publisher> Wiley, </publisher> <year> 1987. </year>
Reference-contexts: Smaller values of the ff parameter encourage greater exploration. The method works very well in empirical trials. It is also related to a certain class of statistical techniques known as experiment design methods <ref> [13] </ref>, which are used for comparing multiple treatments (for example, fertilizers or drugs) to determine which treatment (if any) is best in as small a set of experiments as possible. 3 Delayed Reward In the general case of the reinforcement learning problem, the agent's actions determine not only its immediate reward,
Reference: [14] <author> Justin A. Boyan and Andrew W. Moore. </author> <title> Generalization in reinforcement learning: Safely approximating the value function. </title> <booktitle> In Neural Information Processing Systems 7, </booktitle> <year> 1995. </year>
Reference-contexts: Here, a function approximator is used to represent the value function by mapping a state description to a value. Many reseachers have experimented with this approach: Boyan and Moore <ref> [14] </ref> used local memory-based methods in conjunction with value iteration; Lin [44] used backpropagation networks for Q-learning; Watkins [101] used CMAC for Q-learning; Tesauro [93, 95] used backpropagation for learning the value function in backgammon (described in Section 8.1); Zhang and Dietterich [109] used backpropagation and T D () to learn <p> This guarantee no longer holds when generalization is used. These issues are discussed by Boyan and Moore <ref> [14] </ref>, who give some simple examples of value function errors growing arbitrarily large when generalization is used with value iteration.
Reference: [15] <author> Anthony R. Cassandra, Leslie Pack Kaelbling, and Michael L. Littman. </author> <title> Acting optimally in partially observable stochastic domains. </title> <booktitle> In Proceedings of the Twelfth National Conference on Artificial Intelligence, </booktitle> <address> Seattle, WA, </address> <year> 1994. </year>
Reference-contexts: POMDP Approach Another strategy consists of using hidden Markov model (HMM) techniques to learn a model of the environment, including the hidden state, then to use that model to construct a perfect memory controller <ref> [15, 52, 62] </ref>. Chrisman [17] showed how the forward-backward algorithm for learning HMMs could be adapted to learning POMDPs. He, and later McCallum [59], also gave heuristic state-splitting rules to attempt to learn the smallest possible model for a given environment. <p> A standard approach from the operations-research literature is to solve for the optimal policy (or a close approximation thereof) based on its representation as a piecewise-linear and convex function over the belief space. This method is computationally intractable, but may serve as inspiration for methods that make further approximations <ref> [15, 48] </ref>. 27 8 Practical Reinforcement Learning One reason that reinforcement learning is popular is that is serves as a theoretical tool for studying the principles of agents learning to act.
Reference: [16] <author> David Chapman and Leslie Pack Kaelbling. </author> <title> Input generalization in delayed reinforcement learning: An algorithm and performance comparisons. </title> <booktitle> In Proceedings of the International Joint Conference on Artificial Intelligence, </booktitle> <address> Sydney, Australia, </address> <year> 1991. </year>
Reference-contexts: Decision Trees In environments that are characterized by a set of boolean or discrete-valued variables, it is possible to learn compact decision trees for representing Q values. The G-learning algorithm <ref> [16] </ref>, works as follows. It starts by assuming that no partitioning is necessary and tries to learn Q values for the entire environment as if it were one state.
Reference: [17] <author> Lonnie Chrisman. </author> <title> Reinforcement learning with perceptual aliasing: The perceptual distinctions approach. </title> <booktitle> In Proceedings of the Tenth National Conference on Artificial Intelligence, </booktitle> <pages> pages 183-188, </pages> <address> San Jose, CA, 1992. </address> <publisher> AAAI Press. </publisher>
Reference-contexts: POMDP Approach Another strategy consists of using hidden Markov model (HMM) techniques to learn a model of the environment, including the hidden state, then to use that model to construct a perfect memory controller [15, 52, 62]. Chrisman <ref> [17] </ref> showed how the forward-backward algorithm for learning HMMs could be adapted to learning POMDPs. He, and later McCallum [59], also gave heuristic state-splitting rules to attempt to learn the smallest possible model for a given environment. <p> Now we are left with the problem of finding a policy mapping belief states into action. This problem can be formulated as an MDP, but it is difficult to solve using the techniques described earlier, because the input space is continuous. Chrisman's approach <ref> [17] </ref> does not take into account future uncertainty, but yields a policy after a small amount of computation.
Reference: [18] <author> Lonnie Chrisman. </author> <title> Hidden state and short-term memory, 1993. Presentation at Reinforcement Learning Workshop, </title> <booktitle> Machine Learning Conference. </booktitle>
Reference-contexts: But how well can it do? The resulting problem is not Markovian, and Q-learning cannot be guaranteed to converge. Small breaches of the Markov requirement are well handled by Q-learning, but it is possible to construct simple environments that cause Q-learning to oscillate <ref> [18] </ref>. It is possible to use a model-based approach, however; act according to some policy and gather statistics about the transitions between observations, then solve for the optimal policy based on those observations.
Reference: [19] <author> Pawel Cichosz and Jan J. Mulawka. </author> <title> Fast and efficient reinforcement learning with truncated temporal differences. </title> <editor> In Armand Prieditis and Stuart Russell, editors, </editor> <booktitle> Proceedings of the Twelfth International Conference on Machine Learning, </booktitle> <pages> pages 99-107, </pages> <address> San Francisco, CA, 1995. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: It is computationally more expensive to execute the general T D (), though it often converges considerably faster for large [23, 25]. There has been some recent work on making the updates more efficient <ref> [19] </ref> and on changing the definition to make T D () more consistent with the certainty-equivalent method (discussed in Section 5.1 [85]). 4.2 Q-learning The work of the two components of AHC can be accomplished in a unified manner by Watkins' Q-learning algorithm [101, 102].
Reference: [20] <author> W. S. Cleveland and S. J. Delvin. </author> <title> Locally weighted regression: An approach to regression analysis by local fitting. </title> <journal> Journal of the American Statistical Association, </journal> <volume> 83(403) </volume> <pages> 596-610, </pages> <month> September </month> <year> 1988. </year>
Reference-contexts: A typical human learning the task requires an order of magnitude more practice to achieve proficiency at mere tens of hits. The juggling robot learned a world model from experience, which was generalized to unvisited states by a function approximation scheme known as locally weighted regression <ref> [20, 65] </ref>. Between each trial, a form of dynamic programming specific to linear control policies and locally linear transitions was used to improve the policy. The form of dynamic programming is known as linear-quadratic-regulator design [77]. 2.
Reference: [21] <author> Dave Cliff and Susi Ross. </author> <title> Adding temporary memory to zcs. </title> <booktitle> Adaptive Behavior, </booktitle> <volume> 3(2) </volume> <pages> 101-150, </pages> <year> 1995. </year>
Reference-contexts: In spite of some early successes, the original design does not appear to handle partially observed environments robustly. Recently, this approach has been reexamined using insights from the reinforcement-learning literature, with some success. Cliff and Ross <ref> [21] </ref> start with Wilson's zeroth-level classifier system [108] and add one and two-bit memory registers. They find that, although their system can learn to use short-term memory registers effectively, the approach is unlikely to scale to more complex environments.
Reference: [22] <author> J. Connell and S. Mahadevan. </author> <title> Rapid task learning for real robots. In Robot Learning. </title> <publisher> Kluwer Academic Publishers, </publisher> <year> 1993. </year>
Reference-contexts: Most interesting examples of robotic reinforcement learning employ this technique to some extent <ref> [22] </ref>. reflexes: One thing that keeps agents that know nothing from learning anything is that they have a hard time even finding the interesting parts of the space; they wander around at random never getting near the goal, or they are always "killed" immediately.
Reference: [23] <author> Peter Dayan. </author> <title> The convergence of TD() for general . Machine Learning, </title> <booktitle> 8(3) </booktitle> <pages> 341-362, </pages> <year> 1992. </year>
Reference-contexts: Note that we can update the eligibility online as follows: e (s) := fle (s) + 1 if s = current state fle (s) otherwise . It is computationally more expensive to execute the general T D (), though it often converges considerably faster for large <ref> [23, 25] </ref>.
Reference: [24] <author> Peter Dayan and Geoffrey E. Hinton. </author> <title> Feudal reinforcement learning. </title> <booktitle> In Advances in Neural Information Processing Systems 5, </booktitle> <address> San Mateo, CA, 1993. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Lin [45] used this approach, first training the behaviors and then training the gating function. Many of the other hierarchical learning methods can be cast in this framework. 6.4.1 Feudal Q-learning Feudal Q-learning <ref> [24, 101] </ref> involves a hierarchy of learning modules. In the simplest case, there is a high-level master and a low-level slave. The master receives reinforcement from the external 24 environment. Its actions consist of commands that it can give to the low-level learner.
Reference: [25] <author> Peter Dayan and Terrence J. Sejnowski. </author> <title> TD() converges with probability 1. </title> <journal> Machine Learning, </journal> <volume> 14(3), </volume> <year> 1994. </year>
Reference-contexts: Note that we can update the eligibility online as follows: e (s) := fle (s) + 1 if s = current state fle (s) otherwise . It is computationally more expensive to execute the general T D (), though it often converges considerably faster for large <ref> [23, 25] </ref>.
Reference: [26] <author> Thomas Dean, Leslie Pack Kaelbling, Jak Kirman, and Ann Nicholson. </author> <title> Planning with deadlines in stochastic domains. </title> <booktitle> In Proceedings of the Eleventh National Conference on Artificial Intelligence, </booktitle> <address> Washington, DC, </address> <year> 1993. </year> <month> 34 </month>
Reference-contexts: By taking into account the start state, it can find a short path from the start to the goal, without necessarily visiting the rest of the state space. The Plexus planning system <ref> [26, 42] </ref> exploits a similar intuition. It starts by making an approximate version of the MDP which is much smaller than the original one. The approximate MDP contains a set of states, called the envelope, that includes the agent's current state and the goal state, if there is one.
Reference: [27] <author> Claude-Nicolas Fiechter. </author> <title> Efficient reinforcement learning. </title> <booktitle> In Proceedings of the Seventh Annual ACM Conference on Computational Learning Theory, </booktitle> <pages> pages 88-97. </pages> <institution> Association of Computing Machinery, </institution> <year> 1994. </year>
Reference-contexts: This measure is known as regret [9]. It penalizes mistakes wherever they occur during the run. In spite of the mismatch between embedded reinforcement learning and the train/test perspec-tive, Fiechter <ref> [27] </ref> provides a PAC analysis for Q-learning (described in Section 4.2) that sheds some light on the connection between the two views. 2 Exploitation versus Exploration: The Single-State Case In this section we consider the k-armed bandit problem, which been the subject of a great deal of study in the statistics
Reference: [28] <author> J. C. Gittins. </author> <title> Multi-armed bandit allocation indices. Wiley-Interscience series in systems and optimization. </title> <publisher> Wiley, </publisher> <address> Chichester, New York, </address> <year> 1989. </year>
Reference-contexts: is linear in the number of belief states times actions, and thus exponential in the horizon (although for fixed k, it is polynomial in the horizon). 2.1.2 Gittins Allocation Indices Gittins gives an "allocation index" method for finding the optimal choice of action at each step in k-armed bandit problems <ref> [28] </ref>. The technique only applies under the discounted expected reward criterion. For each action, consider the number of times it has been chosen, n, versus the number of times it has paid off, w.
Reference: [29] <author> D. Goldberg. </author> <title> Genetic algorithms in search, optimization, and machine learning. </title> <publisher> Addison-Wesley, </publisher> <address> MA, </address> <year> 1989. </year>
Reference-contexts: This approach has been used by a number of researchers [61, 47, 81]. It seems to work effectively on simple problems, but can suffer from convergence to local optima on more complex problems. Classifier Systems Classifier systems <ref> [34, 29] </ref> were explicitly developed to problems with delayed reward, including those requiring short-term memory. In spite of some early successes, the original design does not appear to handle partially observed environments robustly. Recently, this approach has been reexamined using insights from the reinforcement-learning literature, with some success.
Reference: [30] <author> Geoffrey J. Gordon. </author> <title> Stable function approximation in dynamic programming. </title> <editor> In Armand Prieditis and Stuart Russell, editors, </editor> <booktitle> Proceedings of the Twelfth International Conference on Machine Learning, </booktitle> <pages> pages 261-268, </pages> <address> San Francisco, CA, 1995. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Thrun and Schwartz [97] theorize that function approximation of value functions is also dangerous because the errors in value functions due to generalization can become compounded by the "max" operator in the definition of the value function. Several recent results <ref> [30, 99] </ref> show how the appropriate choice of function approximator can guarantee convergence, though not necessarily to the optimal values.
Reference: [31] <author> Vijay Gullapalli. </author> <title> A stochastic reinforcement learning algorithm for learning real-valued functions. </title> <booktitle> Neural Networks, </booktitle> <volume> 3 </volume> <pages> 671-692, </pages> <year> 1990. </year>
Reference-contexts: Training such a network is not conceptually difficult, but using the network to find the optimal action can be a challenge. One method is to do a local gradient-ascent search on the action in order to find one with high value [5]. Gullapalli <ref> [31, 32] </ref> has developed a "neural" reinforcement-learning unit for use in continuous action spaces. The unit generates actions with a normal distribution; it adjusts the mean and variance based on previous experience.
Reference: [32] <author> Vijay Gullapalli. </author> <title> Reinforcement learning and its application to control. </title> <type> PhD thesis, </type> <institution> University of Massachusetts, </institution> <address> Amherst, MA, </address> <year> 1992. </year>
Reference-contexts: Training such a network is not conceptually difficult, but using the network to find the optimal action can be a challenge. One method is to do a local gradient-ascent search on the action in order to find one with high value [5]. Gullapalli <ref> [31, 32] </ref> has developed a "neural" reinforcement-learning unit for use in continuous action spaces. The unit generates actions with a normal distribution; it adjusts the mean and variance based on previous experience.
Reference: [33] <author> Ernest R. Hilgard and Gordon H. Bower. </author> <title> Theories of Learning. </title> <publisher> Prentice-Hall, </publisher> <address> Englewood Cliffs, NJ, </address> <note> fourth edition, </note> <year> 1975. </year>
Reference-contexts: The probabilities of taking different actions would be adjusted according to their previous successes and failures. An example, which stands among a set of algorithms independently developed in the mathematical psychology literature <ref> [33] </ref>, is the linear reward-inaction algorithm. <p> The necessary bias can come in a variety of forms, including the following: shaping: The technique of shaping is used in training animals <ref> [33] </ref>; a teacher presents very simple problems to solve first, then gradually exposes the learner to more complex problems.
Reference: [34] <author> John H. Holland. </author> <title> Adaptation in Natural and Artificial Systems. </title> <publisher> University of Michigan Press, </publisher> <address> Ann Arbor, MI, </address> <year> 1975. </year>
Reference-contexts: This approach has been used by a number of researchers [61, 47, 81]. It seems to work effectively on simple problems, but can suffer from convergence to local optima on more complex problems. Classifier Systems Classifier systems <ref> [34, 29] </ref> were explicitly developed to problems with delayed reward, including those requiring short-term memory. In spite of some early successes, the original design does not appear to handle partially observed environments robustly. Recently, this approach has been reexamined using insights from the reinforcement-learning literature, with some success.
Reference: [35] <author> Ronald A. Howard. </author> <title> Dynamic Programming and Markov Processes. </title> <publisher> The MIT Press, </publisher> <address> Cam-bridge, MA, </address> <year> 1960. </year>
Reference-contexts: The reward function specifies expected instantaneous reward as a function of the current state and action. The model is Markov if the state transitions are independent of any previous environment states or agent actions. There are many good references to MDP models <ref> [8, 11, 35, 73] </ref>. 3.2 Finding a Policy Given a Model Before we consider algorithms for learning to behave in MDP environments, we will explore techniques for determining the optimal policy given a correct model. These techniques will serve as the foundation and inspiration for the learning algorithms to follow.
Reference: [36] <author> Tommi Jaakkola, Michael I. Jordan, and Satinder P. Singh. </author> <title> On the convergence of stochastic iterative dynamic programming algorithms. </title> <journal> Neural Computation, </journal> <volume> 6(6), </volume> <month> November </month> <year> 1994. </year>
Reference-contexts: If each action is executed in each state an infinite number of times on an infinite run and ff is decayed appropriately, the Q values will converge with probability 1 to Q fl <ref> [101, 99, 36] </ref>. Q-learning can also be extended to update states that occurred more than one step previously, as in T D () [71]. Q-learning is the most popular and seems to be the most effective model-free algorithm for learning from delayed reinforcement.
Reference: [37] <author> Tommi Jaakkola, Satinder Pal Singh, and Michael I. Jordan. </author> <title> Monte-carlo reinforcement learning in non-Markovian decision problems. </title> <booktitle> In Proc. Conf. Neural Information Processing Systems (NIPS7). </booktitle> <publisher> Morgan Kaufmann, </publisher> <year> 1995. </year>
Reference-contexts: With that in mind, researchers have studied the problem of learning optimal average-reward policies. Mahadevan [55] surveyed model-based average-reward algorithms from a reinforcement-learning perspective and found several difficulties with existing algorithms. Jaakkola et al. <ref> [37] </ref> described an average-reward learning algorithm with guaranteed convergence properties. It uses a Monte-Carlo component to estimate the expected future reward for each state as the agent moves through the environment. <p> If there is randomness in the agent's actions, it will not get stuck in the woods forever. Jaakkola, Singh, and Jordan <ref> [37] </ref> have developed an algorithm for finding locally-optimal stochastic policies, but finding a globally optimal policy is still NP hard.
Reference: [38] <author> Leslie Pack Kaelbling. </author> <title> Hierarchical learning in stochastic domains: Preliminary results. </title> <booktitle> In Proceedings of the Tenth International Conference on Machine Learning, </booktitle> <address> Amherst, MA, 1993. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Kaelbling's HDG algorithm <ref> [38] </ref> uses a hierarchical approach to solving problems when goals of achievement (the agent should get to a particular state as quickly as possible) are given to an agent dynamically. The HDG algorithm works by analogy with navigation in a harbor.
Reference: [39] <author> Leslie Pack Kaelbling. </author> <title> Learning in Embedded Systems. </title> <publisher> The MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1993. </year>
Reference-contexts: This still has a measurable danger of starving an optimal but unlucky action, but the risk of this can be made arbitrarily small. Techniques like this have been used in several reinforcement learning algorithms including the interval exploration method <ref> [39] </ref> (described shortly), the exploration bonus in Dyna [91] and the exploration mechanism in prioritized sweeping [66]. 2.2.2 Randomized Strategies In reinforcement-learning practice, some simple, ad hoc strategies have been popular. <p> It may also converge unnecessarily slowly unless the temperature schedule is manually tuned with great care. 2.2.3 Interval-based Techniques Exploration is often more efficient when it is based on second-order information about the certainty or variance of the estimated values of actions. Kaelbling's interval estimation algorithm <ref> [39] </ref> stores statistics for each action a i : w i is the number of successes and n i the number of trials. <p> In general, however, that approach suffers from the problem of very unreliable reinforcement: if a single learner generates an inappropriate output bit, all of the learners receive a low reinforcement value. The cascade method <ref> [39] </ref> allows a collection of learners to be trained collectively to generate appropriate joint outputs; it is considerably more reliable, but can require additional computational effort. 6.1.2 Delayed Reward Another method to allow reinforcement-learning techniques to be applied in large state spaces is modeled on value iteration and Q-learning.
Reference: [40] <author> Leslie Pack Kaelbling. </author> <title> Associative reinforcement learning: A generate and test algorithm. </title> <journal> Machine Learning, </journal> <volume> 15(3), </volume> <year> 1994. </year>
Reference-contexts: Taking inspiration from mainstream machine learning work, Kaelbling developed two algorithms for learning boolean functions from reinforcement: one uses the bias of k-DNF to drive the generalization process [41]; the other searches the space of syntactic descriptions of functions using a simple generate-and-test method <ref> [40] </ref>. The restriction to a single boolean output makes these techniques difficult to apply. In very benign learning situations, it is possible to extend this approach to use a collection of learners to independently learn the individual bits that make up a complex output.
Reference: [41] <author> Leslie Pack Kaelbling. </author> <title> Associative reinforcement learning: Functions in k-DNF. </title> <journal> Machine Learning, </journal> <volume> 15(3), </volume> <year> 1994. </year>
Reference-contexts: A boolean function has a vector of boolean inputs and a single boolean output. Taking inspiration from mainstream machine learning work, Kaelbling developed two algorithms for learning boolean functions from reinforcement: one uses the bias of k-DNF to drive the generalization process <ref> [41] </ref>; the other searches the space of syntactic descriptions of functions using a simple generate-and-test method [40]. The restriction to a single boolean output makes these techniques difficult to apply.
Reference: [42] <author> Jak Kirman. </author> <title> Predicting Real-Time Planner Performance by Domain Characterization. </title> <type> PhD thesis, </type> <institution> Department of Computer Science, Brown University, </institution> <year> 1994. </year> <month> 35 </month>
Reference-contexts: By taking into account the start state, it can find a short path from the start to the goal, without necessarily visiting the rest of the state space. The Plexus planning system <ref> [26, 42] </ref> exploits a similar intuition. It starts by making an approximate version of the MDP which is much smaller than the original one. The approximate MDP contains a set of states, called the envelope, that includes the agent's current state and the goal state, if there is one.
Reference: [43] <author> Sven Koenig and Reid G. Simmons. </author> <title> Complexity analysis of real-time reinforcement learning. </title> <booktitle> In Proceedings of the Eleventh National Conference on Artificial Intelligence, </booktitle> <pages> pages 99-105, </pages> <address> Menlo Park, California, 1993. </address> <publisher> AAAI Press/MIT Press. </publisher>
Reference-contexts: the learning phase and the acting phase. * How should it gather data about the environment initially? Random exploration might be dangerous, and in some environments is an immensely inefficient method of gathering data, requiring exponentially more data [103] than a system that interleaves experience gathering with policy-building more tightly <ref> [43] </ref>. See Figure 5 for an example. * The possibility of changes in the environment is also problematic.
Reference: [44] <author> Long-Ji Lin. </author> <title> Programming robots using reinforcement learning and teaching. </title> <booktitle> In Proceedings of the Ninth National Conference on Artificial Intelligence, </booktitle> <year> 1991. </year>
Reference-contexts: Here, a function approximator is used to represent the value function by mapping a state description to a value. Many reseachers have experimented with this approach: Boyan and Moore [14] used local memory-based methods in conjunction with value iteration; Lin <ref> [44] </ref> used backpropagation networks for Q-learning; Watkins [101] used CMAC for Q-learning; Tesauro [93, 95] used backpropagation for learning the value function in backgammon (described in Section 8.1); Zhang and Dietterich [109] used backpropagation and T D () to learn good strategies for job-shop scheduling. <p> Shaping has been used in supervised-learning systems, and can be used to train hierarchical reinforcement-learning systems from the bottom up <ref> [44] </ref>, and to alleviate problems of delayed reinforcement by decreasing the delay until the problem is well understood. local reinforcement signals: Whenever possible, agents should be given reinforcement signals that are local. <p> In applications in which it is possible to compute a gradient, rewarding the agent for taking steps up the gradient, rather than just for achieving the final goal, can speed learning significantly [58]. imitation: An agent can learn by "watching" another agent perform the task <ref> [44] </ref>. For real robots, this requires perceptual abilities that are not yet available.
Reference: [45] <author> Long-Ji Lin. </author> <title> Hierachical learning of robot skills by reinforcement. </title> <booktitle> In Proceedings of the International Conference on Neural Networks, </booktitle> <year> 1993. </year>
Reference-contexts: Mahadevan and Connell [57] used the dual approach: they fixed the gating function, and supplied reinforcement functions for the individual behaviors, which were learned. Lin <ref> [45] </ref> used this approach, first training the behaviors and then training the gating function. Many of the other hierarchical learning methods can be cast in this framework. 6.4.1 Feudal Q-learning Feudal Q-learning [24, 101] involves a hierarchy of learning modules.
Reference: [46] <author> Long-Ji Lin. </author> <title> Reinforcement Learning for Robots Using Neural Networks. </title> <type> PhD thesis, </type> <institution> Carnegie Mellon University, </institution> <address> Pittsburgh, PA, </address> <year> 1993. </year>
Reference-contexts: The second network is trained in a standard supervised mode to estimate r as a function of the input state s. Variations of this approach have been used in a variety of applications <ref> [3, 7, 46, 89] </ref>. reinforce Algorithms Williams [104, 105] studied the problem of choosing actions to maximize immedate reward. He identified a broad class of update rules that perform gradient descent on the expected reward and showed how to integrate these rules with backpropagation.
Reference: [47] <author> Long-Ji Lin and Tom M. Mitchell. </author> <title> Memory approaches to reinforcement learning in non-Markovian domains. </title> <type> Technical Report CMU-CS-92-138, </type> <institution> Carnegie Mellon University, School of Computer Science, </institution> <month> May </month> <year> 1992. </year>
Reference-contexts: The network can be trained using backpropagation through time (or some other suitable technique) and learns to retain "history features" to predict value. This approach has been used by a number of researchers <ref> [61, 47, 81] </ref>. It seems to work effectively on simple problems, but can suffer from convergence to local optima on more complex problems. Classifier Systems Classifier systems [34, 29] were explicitly developed to problems with delayed reward, including those requiring short-term memory. <p> Finite-history-window Approach One way to restore the Markov property is the allow decisions to be based on the history of recent observations and perhaps actions. Lin and Mitchell <ref> [47] </ref> used a fixed-width finite history window to learn a pole balancing task. McCallum [60] describes the "utile suffix memory" which learns a variable-width window that serves simultaneously as a model of the environment and a finite-memory policy.
Reference: [48] <author> Michael Littman, Anthony Cassandra, and Leslie Kaelbling. </author> <title> Learning policies for partially observable environments: Scaling up. </title> <editor> In Armand Prieditis and Stuart Russell, editors, </editor> <booktitle> Proceedings of the Twelfth International Conference on Machine Learning, </booktitle> <pages> pages 362-370, </pages> <address> San Francisco, CA, 1995. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: A standard approach from the operations-research literature is to solve for the optimal policy (or a close approximation thereof) based on its representation as a piecewise-linear and convex function over the belief space. This method is computationally intractable, but may serve as inspiration for methods that make further approximations <ref> [15, 48] </ref>. 27 8 Practical Reinforcement Learning One reason that reinforcement learning is popular is that is serves as a theoretical tool for studying the principles of agents learning to act.
Reference: [49] <author> Michael L. Littman. </author> <title> Markov games as a framework for multi-agent reinforcement learning. </title> <booktitle> In Proceedings of the Eleventh International Conference on Machine Learning, </booktitle> <pages> pages 157-163, </pages> <address> San Francisco, CA, 1994. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Nonetheless, reinforcement-learning algorithms can be adapted to work for a very general class of games <ref> [49] </ref> and many researchers have used reinforcement learning in these environments. One application, spectacularly far ahead of its time, was Samuel's checkers playing system [79].
Reference: [50] <author> Michael L. Littman. </author> <title> Memoryless policies: Theoretical limitations and practical results. </title> <editor> In Dave Cliff, Philip Husbands, Jean-Arcady Meyer, and Stewart W. Wilson, editors, </editor> <booktitle> From Animals to Animats 3: Proceedings of the Third International Conference on Simulation of Adaptive Behavior, </booktitle> <address> Cambridge, MA, 1994. </address> <publisher> MIT Press. </publisher>
Reference-contexts: This approach may yield plausible results in some cases, but again, there are no guarantees. It is reasonable, though, to ask what the optimal policy (mapping from observations to actions, in this case) is. It is NP-hard <ref> [50] </ref> to find this mapping, and even the best mapping can have very poor performance.
Reference: [51] <author> Michael L. Littman, Thomas L. Dean, and Leslie Pack Kaelbling. </author> <title> On the complexity of solving Markov decision problems. </title> <booktitle> In Proceedings of the Eleventh Annual Conference on Uncertainty in Artificial Intelligence (UAI-95), </booktitle> <address> Montreal, Quebec, Canada, </address> <year> 1995. </year>
Reference-contexts: However, in the worst case the number of iterations grows polynomially in 1=(1 fl), so the convergence rate slows considerably as the discount factor approaches 1 <ref> [51] </ref>. 3.2.2 Policy Iteration The policy iteration algorithm manipulates the policy directly, rather than finding it indirectly via the optimal value function. <p> However, it is an important open question how many iterations policy iteration takes in the worst case. It is known that the running time is pseudopolynomial and that for any fixed discount factor, there is a polynomial bound in the total size of the MDP <ref> [51] </ref>. 3.2.3 Enhancement to Value Iteration and Policy Iteration In practice, value iteration is much faster per iteration, but policy iteration takes fewer iterations. Arguments have been put forth to the effect that each approach is better for large problems.
Reference: [52] <author> William S. Lovejoy. </author> <title> A survey of algorithmic methods for partially observable Markov decision processes. </title> <journal> Annals of Operations Research, </journal> <volume> 28 </volume> <pages> 47-66, </pages> <year> 1991. </year>
Reference-contexts: POMDP Approach Another strategy consists of using hidden Markov model (HMM) techniques to learn a model of the environment, including the hidden state, then to use that model to construct a perfect memory controller <ref> [15, 52, 62] </ref>. Chrisman [17] showed how the forward-backward algorithm for learning HMMs could be adapted to learning POMDPs. He, and later McCallum [59], also gave heuristic state-splitting rules to attempt to learn the smallest possible model for a given environment.
Reference: [53] <author> Pattie Maes and Rodney A. Brooks. </author> <title> Learning to coordinate behaviors. </title> <booktitle> In Proceedings Eighth National Conference on Artificial Intelligence, </booktitle> <pages> pages 796-802. </pages> <publisher> Morgan Kaufmann, </publisher> <year> 1990. </year>
Reference-contexts: There is a collection of behaviors that map environment states into low-level actions and a gating function that decides, based on the state of the environment, which behavior's actions should be switched through and actually executed. Maes and Brooks <ref> [53] </ref> used a version of this architecture in which the individual behaviors were fixed a priori and the gating function was learned from reinforcement. Mahadevan and Connell [57] used the dual approach: they fixed the gating function, and supplied reinforcement functions for the individual behaviors, which were learned.
Reference: [54] <author> Sridhar Mahadevan. </author> <title> To discount or not to discount in reinforcement learning: A case study comparing R learning and Q learning. </title> <booktitle> In Proceedings of the Eleventh International Conference on Machine Learning, </booktitle> <pages> pages 164-172, </pages> <address> San Francisco, CA, 1994. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Although his R-learning algorithm seems to exhibit convergence problems for some MDPs, several researchers have found the average-reward criterion closer to the true problem they wish to solve than a discounted criterion and therefore prefer R-learning to Q-learning <ref> [54] </ref>. With that in mind, researchers have studied the problem of learning optimal average-reward policies. Mahadevan [55] surveyed model-based average-reward algorithms from a reinforcement-learning perspective and found several difficulties with existing algorithms. Jaakkola et al. [37] described an average-reward learning algorithm with guaranteed convergence properties.
Reference: [55] <author> Sridhar Mahadevan. </author> <title> Average reward reinforcement learning: Foundations, algorithms, and empirical results. </title> <journal> Machine Learning, </journal> <note> To appear. </note>
Reference-contexts: With that in mind, researchers have studied the problem of learning optimal average-reward policies. Mahadevan <ref> [55] </ref> surveyed model-based average-reward algorithms from a reinforcement-learning perspective and found several difficulties with existing algorithms. Jaakkola et al. [37] described an average-reward learning algorithm with guaranteed convergence properties. It uses a Monte-Carlo component to estimate the expected future reward for each state as the agent moves through the environment.
Reference: [56] <author> Sridhar Mahadevan and Jonathan Connell. </author> <title> Automatic programming of behavior-based robots using reinforcement learning. </title> <booktitle> In Proceedings of the Ninth National Conference on Artificial Intelligence, </booktitle> <address> Anaheim, CA, </address> <year> 1991. </year> <month> 36 </month>
Reference-contexts: Between each trial, a form of dynamic programming specific to linear control policies and locally linear transitions was used to improve the policy. The form of dynamic programming is known as linear-quadratic-regulator design [77]. 2. Mahadevan and Connell <ref> [56] </ref> discuss a task in which a mobile robot pushes large boxes for extended periods of time. Box-pushing is a well-known difficult robotics problem, characterized by immense uncertainty in the results of actions.
Reference: [57] <author> Sridhar Mahadevan and Jonathan Connell. </author> <title> Scaling reinforcement learning to robotics by exploiting the subsumption architecture. </title> <booktitle> In Proceedings of the Eighth International Workshop on Machine Learning, </booktitle> <pages> pages 328-332, </pages> <year> 1991. </year>
Reference-contexts: Maes and Brooks [53] used a version of this architecture in which the individual behaviors were fixed a priori and the gating function was learned from reinforcement. Mahadevan and Connell <ref> [57] </ref> used the dual approach: they fixed the gating function, and supplied reinforcement functions for the individual behaviors, which were learned. Lin [45] used this approach, first training the behaviors and then training the gating function.
Reference: [58] <editor> Maja J. Mataric. </editor> <title> Reward functions for accelerated learning. </title> <editor> In W. W. Cohen and H. Hirsh, editors, </editor> <booktitle> Proceedings of the Eleventh International Conference on Machine Learning. </booktitle> <publisher> Morgan Kaufmann, </publisher> <year> 1994. </year>
Reference-contexts: The robot learned to perform competitively with the performance of a human-programmed solution. Another aspect of this work, mentioned in Section 6.4, was a pre-programmed breakdown of the monolithic task description into a set of lower level tasks to be learned. 3. Mataric <ref> [58] </ref> describes a robotics experiment with, from the viewpoint of theoretical reinforcement learning, an unthinkably high dimensional state space, containing many dozens of degrees of freedom. Four mobile robots traveled within an enclosure collecting small disks 30 and transporting them to a destination region. <p> In applications in which it is possible to compute a gradient, rewarding the agent for taking steps up the gradient, rather than just for achieving the final goal, can speed learning significantly <ref> [58] </ref>. imitation: An agent can learn by "watching" another agent perform the task [44]. For real robots, this requires perceptual abilities that are not yet available. <p> These problems can be ameliorated by programming a set of "reflexes" that cause the agent to act initially in some way that is reasonable <ref> [58, 84] </ref>. These reflexes can eventually be overridden by more detailed and accurate learned knowledge, but they at least keep the agent alive and pointed in the right direction while it is trying to learn.
Reference: [59] <author> R. Andrew McCallum. </author> <title> Overcoming incomplete perception with utile distinction memory. </title> <booktitle> In Proceedings of the Tenth International Conference on Machine Learning, </booktitle> <pages> pages 190-196, </pages> <address> Amherst, Massachusetts, 1993. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Chrisman [17] showed how the forward-backward algorithm for learning HMMs could be adapted to learning POMDPs. He, and later McCallum <ref> [59] </ref>, also gave heuristic state-splitting rules to attempt to learn the smallest possible model for a given environment.
Reference: [60] <author> R. Andrew McCallum. </author> <title> Instance-based utile distinctions for reinforcement learning with hidden state. </title> <booktitle> In Proceedings of the Twelfth International Conference Machine Learning, </booktitle> <pages> pages 387-395, </pages> <address> San Francisco, CA, 1995. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Figure 7c shows the second trial, started from a slightly different position. This is a very fast algorithm, learning policies in spaces of up to nine dimensions in less than a minute. The restriction of the current implementation to deterministic environments limits its applicability, however. McCallum <ref> [60] </ref> suggests some related tree-structured methods. 6.2 Generalization over Actions The networks described in Section 6.1.1 generalize over state descriptions presented as inputs. They also produce outputs in a discrete, factored representation and thus could be seen as generalizing over actions as well. <p> Finite-history-window Approach One way to restore the Markov property is the allow decisions to be based on the history of recent observations and perhaps actions. Lin and Mitchell [47] used a fixed-width finite history window to learn a pole balancing task. McCallum <ref> [60] </ref> describes the "utile suffix memory" which learns a variable-width window that serves simultaneously as a model of the environment and a finite-memory policy.
Reference: [61] <author> Lisa Meeden, G. McGraw, and D. Blank. </author> <title> Emergent control and planning in an autonomous vehicle. </title> <editor> In D.S. Touretsky, editor, </editor> <booktitle> Proceedings of the Fifteenth Annual Meeting of the Cognitive Science Society, </booktitle> <pages> pages 735-740. </pages> <publisher> Lawerence Erlbaum Associates, </publisher> <address> Hillsdale, NJ, </address> <year> 1993. </year>
Reference-contexts: The network can be trained using backpropagation through time (or some other suitable technique) and learns to retain "history features" to predict value. This approach has been used by a number of researchers <ref> [61, 47, 81] </ref>. It seems to work effectively on simple problems, but can suffer from convergence to local optima on more complex problems. Classifier Systems Classifier systems [34, 29] were explicitly developed to problems with delayed reward, including those requiring short-term memory.
Reference: [62] <author> George E. Monahan. </author> <title> A survey of partially observable Markov decision processes: Theory, models, and algorithms. </title> <journal> Management Science, </journal> <volume> 28 </volume> <pages> 1-16, </pages> <month> January </month> <year> 1982. </year>
Reference-contexts: POMDP Approach Another strategy consists of using hidden Markov model (HMM) techniques to learn a model of the environment, including the hidden state, then to use that model to construct a perfect memory controller <ref> [15, 52, 62] </ref>. Chrisman [17] showed how the forward-backward algorithm for learning HMMs could be adapted to learning POMDPs. He, and later McCallum [59], also gave heuristic state-splitting rules to attempt to learn the smallest possible model for a given environment.
Reference: [63] <author> Andrew W. Moore. </author> <title> Variable resolution dynamic programming: Efficiently learning action maps in multivariate real-valued spaces. </title> <booktitle> In Proc. Eighth International Machine Learning Workshop, </booktitle> <year> 1991. </year>
Reference-contexts: It outperformed Q-learning with backpropagation in a simple video-game environment. It cannot, however, acquire partitions in which attributes are only significant in combination (such as those needed to solve parity problems). Variable Resolution Dynamic Programming The VRDP algorithm <ref> [63] </ref> enables conventional dynamic programming to be performed in real-valued multivariate state-spaces where straightforward discretization would fall prey to the curse of dimensionality. A kd-tree (similar to a decision tree) is used to partition state space into coarse regions.
Reference: [64] <author> Andrew W. Moore. </author> <title> The parti-game algorithm for variable resolution reinforcement learning in multidimensional state spaces. </title> <editor> In S. J. Hanson, J. D Cowan, and C. L. Giles, editors, </editor> <booktitle> Advances in Neural Information Processing Systems 6, </booktitle> <address> San Mateo, CA, 1994. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: This algorithm proved effective on a number of problems for which full high-resolution arrays would have been impractical. It has the disadvantage of requiring a guess at an initially valid trajectory through state-space. PartiGame Algorithm Moore's PartiGame algorithm <ref> [64] </ref> is another solution to the problem of learning to achieve goal configurations in deterministic high-dimensional continuous spaces by learning an adaptive-resolution model.
Reference: [65] <author> Andrew W. Moore and Christopher G. Atkeson. </author> <title> An investigation of memory-based function approximators for learning control. </title> <type> Technical report, </type> <institution> MIT Artifical Intelligence Laboratory, </institution> <address> Cambridge, MA, </address> <year> 1992. </year>
Reference-contexts: A typical human learning the task requires an order of magnitude more practice to achieve proficiency at mere tens of hits. The juggling robot learned a world model from experience, which was generalized to unvisited states by a function approximation scheme known as locally weighted regression <ref> [20, 65] </ref>. Between each trial, a form of dynamic programming specific to linear control policies and locally linear transitions was used to improve the policy. The form of dynamic programming is known as linear-quadratic-regulator design [77]. 2.
Reference: [66] <author> Andrew W. Moore and Christopher G. Atkeson. </author> <title> Prioritized sweeping: Reinforcement learning with less data and less real time. </title> <journal> Machine Learning, </journal> <volume> 13, </volume> <year> 1993. </year>
Reference-contexts: Techniques like this have been used in several reinforcement learning algorithms including the interval exploration method [39] (described shortly), the exploration bonus in Dyna [91] and the exploration mechanism in prioritized sweeping <ref> [66] </ref>. 2.2.2 Randomized Strategies In reinforcement-learning practice, some simple, ad hoc strategies have been popular. They are rarely, if ever, the best choice for the models of optimality we have used, but they may be viewed as reasonable, computationally tractable, heuristics. Thrun [98] has surveyed a variety of these techniques. <p> It is particularly unhelpful when the goal has just been reached or when the agent is stuck in a dead end; it continues to update random state-action pairs, rather than concentrating on the "interesting" parts of the state space. These problems are addressed by prioritized sweeping <ref> [66] </ref> and Queue-Dyna [70], which are two independently-developed but very similar techniques. We will describe prioritized sweeping in some detail.
Reference: [67] <author> Andrew W. Moore, Christopher G. Atkeson, and S. Schaal. </author> <title> Memory-based learning for control. </title> <type> Technical Report CMU-RI-TR-95-18, </type> <institution> CMU Robotics Institute, </institution> <year> 1995. </year>
Reference-contexts: Popular methods include various neural-network methods [76], CMAC [2], and local memory-based methods <ref> [67] </ref>, such as generalizations of nearest neighbor methods. Other mappings, especially the policy mapping, typically need specialized algorithms because training sets of input-output pairs are not available. 6.1 Generalization over Input A reinforcement-learning agent's current state plays a central role in its selection of reward-maximizing actions.
Reference: [68] <author> Kumpati Narendra and M. A. L. Thathachar. </author> <title> Learning Automata: An Introduction. </title> <publisher> Prentice-Hall, </publisher> <address> Englewood Cliffs, NJ, </address> <year> 1989. </year>
Reference-contexts: Unfortunately, no one has yet been able to find an analog of index values for delayed reinforcement problems. 2.1.3 Learning Automata A branch of the theory of adaptive control is devoted to learning automata (see Narendra and Thathachar <ref> [68] </ref> for a survey), which were originally described explicitly as finite state automata. The Tsetlin automaton shown in Figure 3 provides an example that solves a 2-armed bandit arbitrarily near optimally as N approaches infinity.
Reference: [69] <author> Kumpati S. Narendra and M. A. L. Thathachar. </author> <title> Learning automata|a survey. </title> <journal> IEEE Transactions on Systems, Man, and Cybernetics, </journal> <volume> 4(4) </volume> <pages> 323-334, </pages> <month> July </month> <year> 1974. </year>
Reference-contexts: Unfortunately, it does not always converge to the correct action; but the probability that it converges to the wrong one can be made arbitrarily small by making ff small <ref> [69] </ref>. There is no literature on the regret of this algorithm. 2.2 Ad-Hoc Techniques 2.2.1 Greedy Strategies The first strategy that comes to mind is to always choose the action with the highest estimated payoff.
Reference: [70] <author> Jing Peng and Ronald J. Williams. </author> <title> Efficient learning and planning within the Dyna framework. </title> <booktitle> Adaptive Behavior, </booktitle> <volume> 1(4) </volume> <pages> 437-454, </pages> <year> 1993. </year>
Reference-contexts: It is particularly unhelpful when the goal has just been reached or when the agent is stuck in a dead end; it continues to update random state-action pairs, rather than concentrating on the "interesting" parts of the state space. These problems are addressed by prioritized sweeping [66] and Queue-Dyna <ref> [70] </ref>, which are two independently-developed but very similar techniques. We will describe prioritized sweeping in some detail. The algorithm is similar to Dyna, except that updates are no longer chosen at random and values are now associated with states (as in value iteration) instead of state-action pairs (as in Q-learning).
Reference: [71] <author> Jing Peng and Ronald J. Williams. </author> <title> Incremental multi-step Q-learning. </title> <booktitle> In Proceedings of the Eleventh International Conference on Machine Learning, </booktitle> <pages> pages 226-232, </pages> <address> San Francisco, CA, 1994. </address> <publisher> Morgan Kaufmann. </publisher> <pages> 37 </pages>
Reference-contexts: Q-learning can also be extended to update states that occurred more than one step previously, as in T D () <ref> [71] </ref>. Q-learning is the most popular and seems to be the most effective model-free algorithm for learning from delayed reinforcement. It does not, however, address any of the issues involved in generalizing over large state and/or action spaces.
Reference: [72] <author> Dean A. Pomerleau. </author> <title> Neural network perception for mobile robot guidance. </title> <publisher> Kluwer Academic Publishing, </publisher> <year> 1993. </year>
Reference-contexts: For real robots, this requires perceptual abilities that are not yet available. But another strategy is to have a human supply appropriate motor commands to a robot through a joystick or steering wheel <ref> [72] </ref>. 32 problem decomposition: Decomposing a huge learning problem into a collection of smaller ones, and providing useful reinforcement signals for the subproblems is a very powerful technique for biasing learning.
Reference: [73] <author> Martin L. Puterman. </author> <title> Markov Decision Processes|Discrete Stochastic Dynamic Programming. </title> <publisher> John Wiley & Sons, Inc., </publisher> <address> New York, NY, </address> <year> 1994. </year>
Reference-contexts: The reward function specifies expected instantaneous reward as a function of the current state and action. The model is Markov if the state transitions are independent of any previous environment states or agent actions. There are many good references to MDP models <ref> [8, 11, 35, 73] </ref>. 3.2 Finding a Policy Given a Model Before we consider algorithms for learning to behave in MDP environments, we will explore techniques for determining the optimal policy given a correct model. These techniques will serve as the foundation and inspiration for the learning algorithms to follow. <p> When no improvements are possible, then the policy is guaranteed to be optimal. 11 Since there are at most jAj jSj distinct policies, and the sequence of policies improves at each step, this algorithm terminates in at most an exponential number of iterations <ref> [73] </ref>. However, it is an important open question how many iterations policy iteration takes in the worst case.
Reference: [74] <author> Martin L. Puterman and Moon Chirl Shin. </author> <title> Modified policy iteration algorithms for discounted Markov decision processes. </title> <journal> Management Science, </journal> <volume> 24 </volume> <pages> 1127-1137, </pages> <year> 1978. </year>
Reference-contexts: Arguments have been put forth to the effect that each approach is better for large problems. Puterman's modified policy iteration algorithm <ref> [74] </ref> provides a method for trading iteration time for iteration improvement in a smoother way. The basic idea is that the expensive part of policy iteration is solving for the exact value of V .
Reference: [75] <author> Ulrich Rude. </author> <title> Mathematical and computational techniques for multilevel adaptive methods. </title> <institution> Society for Industrial and Applied Mathematics, </institution> <address> Philadelphia, Pennsylvania, </address> <year> 1993. </year>
Reference-contexts: Several standard numerical-analysis techniques that speed the convergence of dynamic programming can be used to accelerate value and policy iteration. Multigrid methods can be used to quickly seed a good initial approximation to a high resolution value function by initially performing value iteration at a coarser resolution <ref> [75] </ref>. State aggregation works by collapsing groups of states to a single meta-state solving the abstracted problem [10]. 4 Learning an Optimal Policy: Model-free Methods In the previous section we reviewed methods for obtaining an optimal policy for an MDP assuming that we already had a model.
Reference: [76] <editor> D. E. Rumelhart and J. L. McClelland, editors. </editor> <booktitle> Parallel Distributed Processing: Explorations in the microstructures of cognition. Volume 1: Foundations. </booktitle> <publisher> The MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1986. </year>
Reference-contexts: Some of these mappings, such as transitions and immediate rewards, can be learned using straightforward supervised learning, and can be handled using any of the wide variety of function-approximation techniques for supervised learning that support noisy training examples. Popular methods include various neural-network methods <ref> [76] </ref>, CMAC [2], and local memory-based methods [67], such as generalizations of nearest neighbor methods.
Reference: [77] <author> A. P. Sage and C. C. White. </author> <title> Optimum Systems Control. </title> <publisher> Prentice Hall, </publisher> <year> 1977. </year>
Reference-contexts: Between each trial, a form of dynamic programming specific to linear control policies and locally linear transitions was used to improve the policy. The form of dynamic programming is known as linear-quadratic-regulator design <ref> [77] </ref>. 2. Mahadevan and Connell [56] discuss a task in which a mobile robot pushes large boxes for extended periods of time. Box-pushing is a well-known difficult robotics problem, characterized by immense uncertainty in the results of actions.
Reference: [78] <author> Marcos Salganicoff and Lyle H. Ungar. </author> <title> Active exploration and learning in real-valued spaces using multi-armed bandit allocation indices. </title> <editor> In Armand Prieditis and Stuart Russell, editors, </editor> <booktitle> Proceedings of the Twelfth International Conference on Machine Learning, </booktitle> <pages> pages 480-487, </pages> <address> San Francisco, CA, 1995. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Because of the guarantee of optimal exploration and the simplicity of the technique (given the table of index values), this approach holds a great deal of promise for use in more complex applications. This method proved useful in an application to robotic manipulation with immediate reward <ref> [78] </ref>.
Reference: [79] <author> A. L. Samuel. </author> <title> Some studies in machine learning using the game of checkers. </title> <journal> IBM Journal of Research and Development, </journal> <volume> 3 </volume> <pages> 211-229, </pages> <year> 1959. </year> <note> Reprinted in E. </note> <editor> A. Feigenbaum and J. Feldman, editors, </editor> <booktitle> Computers and Thought, </booktitle> <publisher> McGraw-Hill, </publisher> <address> New York 1963. </address>
Reference-contexts: Nonetheless, reinforcement-learning algorithms can be adapted to work for a very general class of games [49] and many researchers have used reinforcement learning in these environments. One application, spectacularly far ahead of its time, was Samuel's checkers playing system <ref> [79] </ref>. This learned a value function represented by a linear function approximator, and employed a training scheme similar to the updates used in value iteration, temporal differences and Q-learning. More recently, Tesauro [93, 94, 95] applied the temporal difference algorithm to backgammon.
Reference: [80] <author> S. Schaal and Christopher Atkeson. </author> <title> Robot juggling: An implementation of memory-based learning. </title> <journal> Control Systems Magazine, </journal> <volume> 14, </volume> <year> 1994. </year>
Reference-contexts: Here we will concentrate on the following four examples, although many other interesting ongoing robotics investigations are underway. 1. Schaal and Atkeson <ref> [80] </ref> constructed a two-armed robot, shown in Figure 11, that learns to juggle a device known as a devil-stick. This is a complex non-linear control task involving a six-dimensional state space and less than 200 msecs per control decision.
Reference: [81] <author> Jurgen H. Schmidhuber. </author> <title> Reinforcement learning in Markovian and non-Markovian environments. </title> <editor> In D. S. Lippman, J. E. Moody, and D. S. Touretzky, editors, </editor> <booktitle> Advances in Neural Information Processing Systems 3, </booktitle> <pages> pages 500-506, </pages> <address> San Mateo, CA, 1991. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: The network can be trained using backpropagation through time (or some other suitable technique) and learns to retain "history features" to predict value. This approach has been used by a number of researchers <ref> [61, 47, 81] </ref>. It seems to work effectively on simple problems, but can suffer from convergence to local optima on more complex problems. Classifier Systems Classifier systems [34, 29] were explicitly developed to problems with delayed reward, including those requiring short-term memory.
Reference: [82] <author> Nicol N. Schraudolph, Peter Dayan, and Terrence J. Sejnowski. </author> <title> Using the T D() algorithm to learn an evaluation function for the game of Go. </title> <booktitle> In Advances in Neural Information Processing Systems 6, </booktitle> <address> San Mateo, CA, 1994. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Basic TD-Gammon played respectably, but not at a professional standard. Although experiments with other games have in some cases produced interesting learning behavior, no success close to that of TD-Gammon has been repeated. Other games that have been studied include Go <ref> [82] </ref> and Chess [96]. It is still an open question as to if and how the success of TD-Gammon can be repeated in other domains. 29 of the two hand sticks. The task is to keep the devil stick from falling for as many hits as possible.
Reference: [83] <author> Anton Schwartz. </author> <title> A reinforcement learning method for maximizing undiscounted rewards. </title> <booktitle> In Proceedings of the Tenth International Conference on Machine Learning, </booktitle> <pages> pages 298-305, </pages> <address> Amherst, Massachusetts, 1993. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: It can also be applied to undiscounted problems as long as the optimal policy is guaranteed to reach a reward-free absorbing state and the state is periodically reset. Schwartz <ref> [83] </ref> examined the problem of adapting Q-learning to an average-reward framework. Although his R-learning algorithm seems to exhibit convergence problems for some MDPs, several researchers have found the average-reward criterion closer to the true problem they wish to solve than a discounted criterion and therefore prefer R-learning to Q-learning [54].
Reference: [84] <author> Satinder P. Singh. </author> <title> Robust reinforcement learning in motion planning. </title> <editor> In S. J. Hanson, J. D Cowan, and C. L. Giles, editors, </editor> <booktitle> Advances in Neural Information Processing Systems 6. </booktitle> <publisher> Morgan Kaufmann, </publisher> <month> April </month> <year> 1994. </year>
Reference-contexts: These problems can be ameliorated by programming a set of "reflexes" that cause the agent to act initially in some way that is reasonable <ref> [58, 84] </ref>. These reflexes can eventually be overridden by more detailed and accurate learned knowledge, but they at least keep the agent alive and pointed in the right direction while it is trying to learn.
Reference: [85] <author> Satinder P. Singh and Richard S. Sutton. </author> <title> Reinforcement learning with replacing eligibility traces. </title> <journal> Machine Learning, </journal> <note> To appear. </note>
Reference-contexts: There has been some recent work on making the updates more efficient [19] and on changing the definition to make T D () more consistent with the certainty-equivalent method (discussed in Section 5.1 <ref> [85] </ref>). 4.2 Q-learning The work of the two components of AHC can be accomplished in a unified manner by Watkins' Q-learning algorithm [101, 102]. Q-learning is typically easier to implement. In order to understand Q-learning, we have to develop some additional notation.
Reference: [86] <author> Satinder Pal Singh. </author> <title> Reinforcement learning with a hierarchy of abstract models. </title> <booktitle> In Proceedings of the Tenth National Conference on Artificial Intelligence, </booktitle> <pages> pages 202-207, </pages> <address> San Jose, CA, 1992. </address> <publisher> AAAI Press. </publisher> <pages> 38 </pages>
Reference-contexts: The reinforcement functions for the individual behaviors (commands) are given, but learning takes place simultaneously at both the high and low levels. 6.4.2 Compositional Q-learning Singh's compositional Q-learning <ref> [87, 86] </ref> (C-QL) consists of a hierarchy based on the temporal sequencing of subgoals. The elemental tasks are behaviors that achieve some recognizable condition. The high-level goal of the system is to achieve some set of conditions in sequential order.
Reference: [87] <author> Satinder Pal Singh. </author> <title> Transfer of learning by composing solutions of elemental sequential tasks. </title> <journal> Machine Learning, </journal> <volume> 8(3) </volume> <pages> 323-340, </pages> <year> 1992. </year>
Reference-contexts: The reinforcement functions for the individual behaviors (commands) are given, but learning takes place simultaneously at both the high and low levels. 6.4.2 Compositional Q-learning Singh's compositional Q-learning <ref> [87, 86] </ref> (C-QL) consists of a hierarchy based on the temporal sequencing of subgoals. The elemental tasks are behaviors that achieve some recognizable condition. The high-level goal of the system is to achieve some set of conditions in sequential order.
Reference: [88] <author> Satinder Pal Singh. </author> <title> Learning to Solve Markovian Decision Processes. </title> <type> PhD thesis, </type> <institution> Department of Computer Science, University of Massachusetts, </institution> <year> 1993. </year> <note> also, CMPSCI Technical Report 93-77. </note>
Reference-contexts: This type of sample backup <ref> [88] </ref> is critical to the operation of the model-free methods discussed in the next section. The computational complexity of the value-iteration algorithm with full backups, per iteration, is quadratic in the number of states and linear in the number of actions.
Reference: [89] <author> Richard S. Sutton. </author> <title> Temporal Credit Assignment in Reinforcement Learning. </title> <type> PhD thesis, </type> <institution> University of Massachusetts, </institution> <address> Amherst, MA, </address> <year> 1984. </year>
Reference-contexts: Such algorithms can and have been used in the context of a delayed reinforcement, for instance, as the RL component in the AHC architecture described in Section 4.1. They can also be generalized to real-valued reward through reward comparison methods <ref> [89] </ref>. CRBP The complementary reinforcement backpropagation algorithm [1] (crbp) consists of a feed-forward network mapping an encoding of the state to an encoding of the action. <p> The hope is that the random distribution will generate an action that works better, and then that action will be reinforced. ARC The associative reinforcement comparison (arc) algorithm <ref> [89] </ref> is an instance of the ahc architecture for the case of boolean actions, consisting of two feed-forward networks. One learns the value of situations, the other learns a policy. These can be simple linear networks or can have hidden units. <p> The second network is trained in a standard supervised mode to estimate r as a function of the input state s. Variations of this approach have been used in a variety of applications <ref> [3, 7, 46, 89] </ref>. reinforce Algorithms Williams [104, 105] studied the problem of choosing actions to maximize immedate reward. He identified a broad class of update rules that perform gradient descent on the expected reward and showed how to integrate these rules with backpropagation.
Reference: [90] <author> Richard S. Sutton. </author> <title> Learning to predict by the method of temporal differences. </title> <journal> Machine Learning, </journal> <volume> 3(1) </volume> <pages> 9-44, </pages> <year> 1988. </year>
Reference-contexts: Instead, we will use insights from value iteration to adjust the estimated value of a state based on the immediate reward and the estimated value of the next state. This class of algorithms is known as temporal difference methods <ref> [90] </ref>. <p> Here s is the agent's state before the transition, a is its choice of action, r the instantaneous reward it receives, and s 0 its resulting state. The value of a policy is learned using Sutton's T D (0) algorithm <ref> [90] </ref> which uses the update rule V (s) := V (s) + ff (r + flV (s 0 ) V (s)) : Whenever a state s is visited, its estimated value is updated to be closer to r + flV (s 0 ), since r is the instantaneous reward received and
Reference: [91] <author> Richard S. Sutton. </author> <title> Integrated architectures for learning, planning, and reacting based on approximating dynamic programming. </title> <booktitle> In Proceedings of the Seventh International Conference on Machine Learning, </booktitle> <address> Austin, TX, 1990. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: This still has a measurable danger of starving an optimal but unlucky action, but the risk of this can be made arbitrarily small. Techniques like this have been used in several reinforcement learning algorithms including the interval exploration method [39] (described shortly), the exploration bonus in Dyna <ref> [91] </ref> and the exploration mechanism in prioritized sweeping [66]. 2.2.2 Randomized Strategies In reinforcement-learning practice, some simple, ad hoc strategies have been popular. They are rarely, if ever, the best choice for the models of optimality we have used, but they may be viewed as reasonable, computationally tractable, heuristics. <p> Fortunately, there are a number of other model-based algorithms that are more practical. 5.2 Dyna Sutton's Dyna architecture <ref> [91, 92] </ref> exploits a middle ground, yielding strategies that are both more effective than model-free learning and more computationally efficient than the certainty-equivalence approach.
Reference: [92] <author> Richard S. Sutton. </author> <title> Planning by incremental dynamic programming. </title> <booktitle> In Proceedings of the Eighth International Workshop on Machine Learning, </booktitle> <pages> pages 353-357. </pages> <publisher> Morgan Kaufmann, </publisher> <year> 1991. </year>
Reference-contexts: Fortunately, there are a number of other model-based algorithms that are more practical. 5.2 Dyna Sutton's Dyna architecture <ref> [91, 92] </ref> exploits a middle ground, yielding strategies that are both more effective than model-free learning and more computationally efficient than the certainty-equivalence approach.
Reference: [93] <author> Gerald Tesauro. </author> <title> Practical issues in temporal difference learning. </title> <journal> Machine Learning, </journal> <volume> 8 </volume> <pages> 257-277, </pages> <year> 1992. </year>
Reference-contexts: Many reseachers have experimented with this approach: Boyan and Moore [14] used local memory-based methods in conjunction with value iteration; Lin [44] used backpropagation networks for Q-learning; Watkins [101] used CMAC for Q-learning; Tesauro <ref> [93, 95] </ref> used backpropagation for learning the value function in backgammon (described in Section 8.1); Zhang and Dietterich [109] used backpropagation and T D () to learn good strategies for job-shop scheduling. <p> One application, spectacularly far ahead of its time, was Samuel's checkers playing system [79]. This learned a value function represented by a linear function approximator, and employed a training scheme similar to the updates used in value iteration, temporal differences and Q-learning. More recently, Tesauro <ref> [93, 94, 95] </ref> applied the temporal difference algorithm to backgammon. Backgammon has approximately 10 20 states, making table-based reinforcement learning impossible.
Reference: [94] <author> Gerald Tesauro. </author> <title> TD-Gammon, a self-teaching backgammon program, achieves master-level play. </title> <journal> Neural Computation, </journal> <volume> 6(2) </volume> <pages> 215-219, </pages> <year> 1994. </year>
Reference-contexts: One application, spectacularly far ahead of its time, was Samuel's checkers playing system [79]. This learned a value function represented by a linear function approximator, and employed a training scheme similar to the updates used in value iteration, temporal differences and Q-learning. More recently, Tesauro <ref> [93, 94, 95] </ref> applied the temporal difference algorithm to backgammon. Backgammon has approximately 10 20 states, making table-based reinforcement learning impossible.
Reference: [95] <author> Gerald Tesauro. </author> <title> Temporal difference learning and TD-Gammon. </title> <journal> Communications of the ACM, </journal> <pages> pages 58-67, </pages> <month> March </month> <year> 1995. </year>
Reference-contexts: Many reseachers have experimented with this approach: Boyan and Moore [14] used local memory-based methods in conjunction with value iteration; Lin [44] used backpropagation networks for Q-learning; Watkins [101] used CMAC for Q-learning; Tesauro <ref> [93, 95] </ref> used backpropagation for learning the value function in backgammon (described in Section 8.1); Zhang and Dietterich [109] used backpropagation and T D () to learn good strategies for job-shop scheduling. <p> One application, spectacularly far ahead of its time, was Samuel's checkers playing system [79]. This learned a value function represented by a linear function approximator, and employed a training scheme similar to the updates used in value iteration, temporal differences and Q-learning. More recently, Tesauro <ref> [93, 94, 95] </ref> applied the temporal difference algorithm to backgammon. Backgammon has approximately 10 20 states, making table-based reinforcement learning impossible.
Reference: [96] <author> Sebastian Thrun. </author> <title> Learning to play the game of chess. </title> <booktitle> In Neural Information Processing Systems 7, </booktitle> <year> 1995. </year>
Reference-contexts: Basic TD-Gammon played respectably, but not at a professional standard. Although experiments with other games have in some cases produced interesting learning behavior, no success close to that of TD-Gammon has been repeated. Other games that have been studied include Go [82] and Chess <ref> [96] </ref>. It is still an open question as to if and how the success of TD-Gammon can be repeated in other domains. 29 of the two hand sticks. The task is to keep the devil stick from falling for as many hits as possible.
Reference: [97] <author> Sebastian Thrun and Anton Schwartz. </author> <title> Issues in using function approximation for reinforcement learning. </title> <editor> In M. Mozer, P. Smolensky, D. Touretzky, J. Elman, and A. Weigend, editors, </editor> <booktitle> Proceedings of the 1993 Connectionist Models Summer School, </booktitle> <address> Hillsdale, NJ, 1993. </address> <publisher> Lawrence Erlbaum. </publisher>
Reference-contexts: Their solution to this, applicable only to certain classes of problems, discourages such divergence by only permitting updates whose estimated values can be shown to be near-optimal via a battery of Monte-Carlo experiments. Thrun and Schwartz <ref> [97] </ref> theorize that function approximation of value functions is also dangerous because the errors in value functions due to generalization can become compounded by the "max" operator in the definition of the value function.
Reference: [98] <author> Sebastian B. Thrun. </author> <title> The role of exploration in learning control. </title> <editor> In David A. White and Donald A. Sofge, editors, </editor> <title> Handbook of Intelligent Control: Neural, Fuzzy, and Adaptive Approaches. </title> <publisher> Van Nostrand Reinhold, </publisher> <address> New York, NY, </address> <year> 1992. </year>
Reference-contexts: They are rarely, if ever, the best choice for the models of optimality we have used, but they may be viewed as reasonable, computationally tractable, heuristics. Thrun <ref> [98] </ref> has surveyed a variety of these techniques. The simplest exploration strategy is to take the action with the best estimated expected reward by default, but with probability p, choose an action at random.
Reference: [99] <author> John N. Tsitsikilis. </author> <title> Asynchronous stochastic approximation and Q-learning. </title> <journal> Machine Learning, </journal> <volume> 16(3), </volume> <month> September </month> <year> 1994. </year>
Reference-contexts: If each action is executed in each state an infinite number of times on an infinite run and ff is decayed appropriately, the Q values will converge with probability 1 to Q fl <ref> [101, 99, 36] </ref>. Q-learning can also be extended to update states that occurred more than one step previously, as in T D () [71]. Q-learning is the most popular and seems to be the most effective model-free algorithm for learning from delayed reinforcement. <p> Thrun and Schwartz [97] theorize that function approximation of value functions is also dangerous because the errors in value functions due to generalization can become compounded by the "max" operator in the definition of the value function. Several recent results <ref> [30, 99] </ref> show how the appropriate choice of function approximator can guarantee convergence, though not necessarily to the optimal values.
Reference: [100] <author> L. G. Valiant. </author> <title> A theory of the learnable. </title> <journal> Communications of the ACM, </journal> <volume> 27(11) </volume> <pages> 1134-1142, </pages> <month> November </month> <year> 1984. </year>
Reference-contexts: It should be noted that here we have another difference between reinforcement learning and conventional supervised learning. In the latter, expected future predictive accuracy or statistical efficiency are the prime concerns. For example, in the well-known PAC framework <ref> [100] </ref>, there is a learning period during which mistakes do not count, then a performance period during which they do. That is usually an inappropriate view for an agent with a long existence in a complex environment. Measures related to speed of learning have an additional weakness.
Reference: [101] <author> Christopher J. C. H. Watkins. </author> <title> Learning from Delayed Rewards. </title> <type> PhD thesis, </type> <institution> King's College, </institution> <address> Cambridge, UK, </address> <year> 1989. </year>
Reference-contexts: work on making the updates more efficient [19] and on changing the definition to make T D () more consistent with the certainty-equivalent method (discussed in Section 5.1 [85]). 4.2 Q-learning The work of the two components of AHC can be accomplished in a unified manner by Watkins' Q-learning algorithm <ref> [101, 102] </ref>. Q-learning is typically easier to implement. In order to understand Q-learning, we have to develop some additional notation. Let Q fl (s; a) be the expected discounted reinforcement of taking action a in state s, then continuing by choosing actions optimally. <p> If each action is executed in each state an infinite number of times on an infinite run and ff is decayed appropriately, the Q values will converge with probability 1 to Q fl <ref> [101, 99, 36] </ref>. Q-learning can also be extended to update states that occurred more than one step previously, as in T D () [71]. Q-learning is the most popular and seems to be the most effective model-free algorithm for learning from delayed reinforcement. <p> Here, a function approximator is used to represent the value function by mapping a state description to a value. Many reseachers have experimented with this approach: Boyan and Moore [14] used local memory-based methods in conjunction with value iteration; Lin [44] used backpropagation networks for Q-learning; Watkins <ref> [101] </ref> used CMAC for Q-learning; Tesauro [93, 95] used backpropagation for learning the value function in backgammon (described in Section 8.1); Zhang and Dietterich [109] used backpropagation and T D () to learn good strategies for job-shop scheduling. <p> Lin [45] used this approach, first training the behaviors and then training the gating function. Many of the other hierarchical learning methods can be cast in this framework. 6.4.1 Feudal Q-learning Feudal Q-learning <ref> [24, 101] </ref> involves a hierarchy of learning modules. In the simplest case, there is a high-level master and a low-level slave. The master receives reinforcement from the external 24 environment. Its actions consist of commands that it can give to the low-level learner.
Reference: [102] <author> Christopher J. C. H. Watkins and Peter Dayan. </author> <title> Q-learning. </title> <journal> Machine Learning, </journal> <volume> 8(3) </volume> <pages> 279-292, </pages> <year> 1992. </year>
Reference-contexts: We would also like to be able to evaluate the quality of learning itself. There are several incompatible measures in use. * Eventual convergence to optimal. Many algorithms come with a provable guarantee of asymptotic convergence to optimal behavior <ref> [102] </ref>. This is reassuring, but useless in practical terms. An agent that quickly reaches a plateau at 99% of optimality may, in many applications, be preferable to an agent that has a guarantee of eventual optimality but a sluggish early learning rate. * Speed of convergence to optimality. <p> work on making the updates more efficient [19] and on changing the definition to make T D () more consistent with the certainty-equivalent method (discussed in Section 5.1 [85]). 4.2 Q-learning The work of the two components of AHC can be accomplished in a unified manner by Watkins' Q-learning algorithm <ref> [101, 102] </ref>. Q-learning is typically easier to implement. In order to understand Q-learning, we have to develop some additional notation. Let Q fl (s; a) be the expected discounted reinforcement of taking action a in state s, then continuing by choosing actions optimally.
Reference: [103] <author> Steven D. Whitehead. </author> <title> Complexity and cooperation in Q-learning. </title> <booktitle> In Proceedings of the Eighth International Workshop on Machine Learning, </booktitle> <address> Evanston, IL, 1991. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: serious objections to this method: * It makes an arbitrary division between the learning phase and the acting phase. * How should it gather data about the environment initially? Random exploration might be dangerous, and in some environments is an immensely inefficient method of gathering data, requiring exponentially more data <ref> [103] </ref> than a system that interleaves experience gathering with policy-building more tightly [43]. See Figure 5 for an example. * The possibility of changes in the environment is also problematic.
Reference: [104] <author> Ronald J. Williams. </author> <title> A class of gradient-estimating algorithms for reinforcement learning in neural networks. </title> <booktitle> In Proceedings of the IEEE First International Conference on Neural Networks, </booktitle> <address> San Diego, CA, </address> <year> 1987. </year>
Reference-contexts: The second network is trained in a standard supervised mode to estimate r as a function of the input state s. Variations of this approach have been used in a variety of applications [3, 7, 46, 89]. reinforce Algorithms Williams <ref> [104, 105] </ref> studied the problem of choosing actions to maximize immedate reward. He identified a broad class of update rules that perform gradient descent on the expected reward and showed how to integrate these rules with backpropagation.
Reference: [105] <author> Ronald J. Williams. </author> <title> Simple statistical gradient-following algorithms for connectionist reinforcement learning. </title> <journal> Machine Learning, </journal> <volume> 8(3) </volume> <pages> 229-256, </pages> <year> 1992. </year>
Reference-contexts: The second network is trained in a standard supervised mode to estimate r as a function of the input state s. Variations of this approach have been used in a variety of applications [3, 7, 46, 89]. reinforce Algorithms Williams <ref> [104, 105] </ref> studied the problem of choosing actions to maximize immedate reward. He identified a broad class of update rules that perform gradient descent on the expected reward and showed how to integrate these rules with backpropagation.
Reference: [106] <author> Ronald J. Williams and Leemon C. Baird, III. </author> <title> Analysis of some incremental variants of policy iteration: First steps toward understanding actor-critic learning systems. </title> <type> Technical Report NU-CCS-93-11, </type> <institution> Northeastern University, College of Computer Science, </institution> <address> Boston, MA, </address> <month> September </month> <year> 1993. </year>
Reference-contexts: In most implementations, however, both components operate simultaneously. Only the alternating implementation can be guaranteed to converge to the optimal policy, under appropriate conditions. Williams and Baird explored the convergence properties of a class of AHC-related algorithms they call "incremental variants of policy iteration" <ref> [106] </ref>. It remains to explain how the critic can learn the value of a policy. We define hs; a; r; s 0 i to be an experience tuple summarizing a single transition in the environment.
Reference: [107] <author> Ronald J. Williams and Leemon C. Baird, III. </author> <title> Tight performance bounds on greedy policies based on imperfect value functions. </title> <type> Technical Report NU-CCS-93-14, </type> <institution> Northeastern University, College of Computer Science, </institution> <address> Boston, MA, </address> <month> November </month> <year> 1993. </year>
Reference-contexts: One important result bounds the performance of the current greedy policy as a function of the Bellman residual of the current value function <ref> [107] </ref>.
Reference: [108] <author> Stewart Wilson. </author> <title> Classifier fitness based on accuracy. </title> <journal> Evolutionary Computation, </journal> <volume> 3(2) </volume> <pages> 147-173, </pages> <year> 1995. </year>
Reference-contexts: In spite of some early successes, the original design does not appear to handle partially observed environments robustly. Recently, this approach has been reexamined using insights from the reinforcement-learning literature, with some success. Cliff and Ross [21] start with Wilson's zeroth-level classifier system <ref> [108] </ref> and add one and two-bit memory registers. They find that, although their system can learn to use short-term memory registers effectively, the approach is unlikely to scale to more complex environments.
Reference: [109] <author> W. Zhang and T. G. Dietterich. </author> <title> A reinforcement learning approach to job-shop scheduling. </title> <booktitle> In IJCAI95, </booktitle> <year> 1995. </year> <note> To appear. 40 </note>
Reference-contexts: have experimented with this approach: Boyan and Moore [14] used local memory-based methods in conjunction with value iteration; Lin [44] used backpropagation networks for Q-learning; Watkins [101] used CMAC for Q-learning; Tesauro [93, 95] used backpropagation for learning the value function in backgammon (described in Section 8.1); Zhang and Dietterich <ref> [109] </ref> used backpropagation and T D () to learn good strategies for job-shop scheduling. Although there have been some positive examples, in general there are unfortunate interactions between function approximation and the learning rules.
References-found: 109

