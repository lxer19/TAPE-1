URL: http://www.cs.brown.edu/people/arc/papers/ml95.ps.gz
Refering-URL: http://www.cs.brown.edu/people/arc/papers/papers.html
Root-URL: 
Email: mlittman@cs.brown.edu  arc@cs.brown.edu  lpk@cs.brown.edu  
Title: Learning policies for partially observable environments: Scaling up  
Author: Michael L. Littman Anthony R. Cassandra Leslie Pack Kaelbling 
Address: Providence, RI 02912-1910  
Affiliation: Department of Computer Science Brown University  
Abstract: Partially observable Markov decision processes (pomdp's) model decision problems in which an agent tries to maximize its reward in the face of limited and/or noisy sensor feedback. While the study of pomdp's is motivated by a need to address realistic problems, existing techniques for finding optimal behavior do not appear to scale well and have been unable to find satisfactory policies for problems with more than a dozen states. After a brief review of pomdp's, this paper discusses several simple solution methods and shows that all are capable of finding near-optimal policies for a selection of extremely small pomdp's taken from the learning literature. In contrast, we show that none are able to solve a slightly larger and noisier problem based on robot navigation. We find that a combination of two novel approaches performs well on these problems and suggest methods for scaling to even larger and more complicated domains.
Abstract-found: 1
Intro-found: 1
Reference: <author> Astrom, K. J. </author> <year> (1965). </year> <title> Optimal control of Markov decision processes with incomplete state estimation. </title>
Reference: <author> J. </author> <title> Math. </title> <journal> Anal. Appl., </journal> <volume> 10 </volume> <pages> 174-205. </pages>
Reference: <author> Bertsekas, D. P. </author> <year> (1987). </year> <title> Dynamic Programming: Deterministic and Stochastic Models. </title> <publisher> Prentice-Hall. </publisher>
Reference-contexts: We will only consider the case in which these sets are finite. The functions T and R define a Markov decision process (mdp) <ref> (Bertsekas, 1987) </ref> with which the agent interacts without direct information as to the current state. <p> forward and seeing walls in all directions except behind, the agent is sure of where it is: b ( North in Room a ) = 1: Since the agent's belief state is an accurate summary of all the relevant past information, it is a sufficient statistic for choosing optimal actions <ref> (Bertsekas, 1987) </ref>. That is, an agent that can choose the optimal action for any given belief state is acting optimally in the environment.
Reference: <author> Boutilier, C., Dearden, R., and Goldszmidt, M. </author> <year> (1995). </year> <title> Exploiting structure in policy construction. </title> <booktitle> In Proceedings of the International Joint Conference on Artificial Intelligence. </booktitle>
Reference: <author> Cassandra, A. </author> <year> (1994). </year> <title> Optimal policies for partially observable Markov decision processes. </title> <type> Technical Report CS-94-14, </type> <institution> Brown University, Department of Computer Science, Providence RI. </institution>
Reference-contexts: From a known starting belief state, it is easy to use the transition and observation probabilities to incorporate new information into the belief state <ref> (Cassandra et al., 1994) </ref>. As an example, consider an agent that is started in any of the 12 non-goal states of the tiny navigation environment with equal probability: b (s) = 1=12 for all non-goal states. <p> During learning, actions were selected to maximize the current Q functions with Name jSj jAj jj Noise Shuttle (Chrisman, 1992) 8 3 5 T /O Cheese Maze (McCallum, 1992) 11 4 7 - Part Painting (Kushmerick et al., 1993) 4 4 2 T /O 4x4 Grid <ref> (Cassandra et al., 1994) </ref> 16 4 2 - Tiger (Cassandra et al., 1994) 2 3 2 O 4x3 Grid (Parr and Russell, 1995) 11 4 6 T Table 1: A suite of extremely small pomdp's. <p> to maximize the current Q functions with Name jSj jAj jj Noise Shuttle (Chrisman, 1992) 8 3 5 T /O Cheese Maze (McCallum, 1992) 11 4 7 - Part Painting (Kushmerick et al., 1993) 4 4 2 T /O 4x4 Grid <ref> (Cassandra et al., 1994) </ref> 16 4 2 - Tiger (Cassandra et al., 1994) 2 3 2 O 4x3 Grid (Parr and Russell, 1995) 11 4 6 T Table 1: A suite of extremely small pomdp's.
Reference: <author> Cassandra, A. R., Kaelbling, L. P., and Littman, M. L. </author> <year> (1994). </year> <title> Acting optimally in partially observable stochastic domains. </title> <booktitle> In Proceedings of the Twelfth National Conference on Artificial Intelligence, </booktitle> <address> Seattle, WA. </address>
Reference-contexts: From a known starting belief state, it is easy to use the transition and observation probabilities to incorporate new information into the belief state <ref> (Cassandra et al., 1994) </ref>. As an example, consider an agent that is started in any of the 12 non-goal states of the tiny navigation environment with equal probability: b (s) = 1=12 for all non-goal states. <p> During learning, actions were selected to maximize the current Q functions with Name jSj jAj jj Noise Shuttle (Chrisman, 1992) 8 3 5 T /O Cheese Maze (McCallum, 1992) 11 4 7 - Part Painting (Kushmerick et al., 1993) 4 4 2 T /O 4x4 Grid <ref> (Cassandra et al., 1994) </ref> 16 4 2 - Tiger (Cassandra et al., 1994) 2 3 2 O 4x3 Grid (Parr and Russell, 1995) 11 4 6 T Table 1: A suite of extremely small pomdp's. <p> to maximize the current Q functions with Name jSj jAj jj Noise Shuttle (Chrisman, 1992) 8 3 5 T /O Cheese Maze (McCallum, 1992) 11 4 7 - Part Painting (Kushmerick et al., 1993) 4 4 2 T /O 4x4 Grid <ref> (Cassandra et al., 1994) </ref> 16 4 2 - Tiger (Cassandra et al., 1994) 2 3 2 O 4x3 Grid (Parr and Russell, 1995) 11 4 6 T Table 1: A suite of extremely small pomdp's.
Reference: <author> Cheng, H.-T. </author> <year> (1988). </year> <title> Algorithms for Partially Observable Markov Decision Processes. </title> <type> PhD thesis, </type> <institution> University of British Columbia, British Columbia, Canada. </institution>
Reference-contexts: A variety of algorithms have been developed for solving pomdp's (Lovejoy, 1991), but because the problem is so computationally challenging (Papadimitriou and Tsitsiklis, 1987), most techniques are too inefficient to be used on all but the smallest problems (2 to 5 states <ref> (Cheng, 1988) </ref>). Recently, the Witness algorithm (Cassandra, 1994; Littman, 1994) has been used to solve pomdp's with up to 16 states. While this problem size is considerably larger than prior state of the art, the algorithm is not efficient enough to be used for larger pomdp's.
Reference: <author> Chrisman, L. </author> <year> (1992). </year> <title> Reinforcement learning with perceptual aliasing: The perceptual distinctions approach. </title> <booktitle> In Proc. Tenth National Conference on AI (AAAI). </booktitle>
Reference-contexts: For each of 21 runs, we performed 75,000 steps of learning starting from the problem-specific belief state. During learning, actions were selected to maximize the current Q functions with Name jSj jAj jj Noise Shuttle <ref> (Chrisman, 1992) </ref> 8 3 5 T /O Cheese Maze (McCallum, 1992) 11 4 7 - Part Painting (Kushmerick et al., 1993) 4 4 2 T /O 4x4 Grid (Cassandra et al., 1994) 16 4 2 - Tiger (Cassandra et al., 1994) 2 3 2 O 4x3 Grid (Parr and Russell, 1995)
Reference: <author> Connell, J. and Mahadevan, S. </author> <year> (1993). </year> <title> Rapid task learning for real robots. In Robot Learning. </title> <publisher> Kluwer Academic Publishers. </publisher>
Reference: <author> Jaakkola, T., Jordan, M. I., and Singh, S. P. </author> <year> (1994). </year> <title> On the convergence of stochastic iterative dynamic programming algorithms. </title> <booktitle> Neural Computation, </booktitle> <pages> 6(6). </pages>
Reference: <author> Kushmerick, N., Hanks, S., and Weld, D. </author> <year> (1993). </year> <title> An Algorithm for Probabilistic Planning. </title> <type> Technical Report 93-06-03, </type> <institution> University of Washington Department of Computer Science and Engineering. </institution> <note> To appear in Artificial Intelligence. </note>
Reference-contexts: During learning, actions were selected to maximize the current Q functions with Name jSj jAj jj Noise Shuttle (Chrisman, 1992) 8 3 5 T /O Cheese Maze (McCallum, 1992) 11 4 7 - Part Painting <ref> (Kushmerick et al., 1993) </ref> 4 4 2 T /O 4x4 Grid (Cassandra et al., 1994) 16 4 2 - Tiger (Cassandra et al., 1994) 2 3 2 O 4x3 Grid (Parr and Russell, 1995) 11 4 6 T Table 1: A suite of extremely small pomdp's.
Reference: <author> Littman, M., Cassandra, A., and Kaelbling, L. </author> <year> (1995). </year> <title> Learning policies for partially observable environments: Scaling up. </title> <type> Technical Report CS-95-11, </type> <institution> Brown University, Department of Computer Science, Providence RI. </institution>
Reference-contexts: Our preliminary experiments indicate that full backups do not appear to speed convergence (at least not consistently across pomdp's) and require significant computational overhead <ref> (Littman et al., 1995) </ref>. More study will be necessary to fully address this issue. All of the results reported here use sample backups. 3.5 EMPIRICAL COMPARISON ON EXTREMELY SMALL PROBLEMS We ran each of the above methods on a battery of pomdp's selected from the literature, summarized in Table 1. <p> We used a discount factor of 0:95 for all problems. The column of Table 1 labeled "Noise" indicates whether there is noise in the transitions, observations, or both. The part-painting problem has been adapted from its original form <ref> (Littman et al., 1995) </ref>. The 4x3 grid problem was introduced by Rus-sell and Norvig (1994) and the version here includes a discounted criterion and returns to the initial belief state after a goal instead of entering an absorbing state. <p> The latter occurs because of the determinism in the state transitions and the relatively small probability of taking random actions; this problem can be easily fixed by adjusting the random-action probability <ref> (Littman et al., 1995) </ref>. <p> The other has 89 states (4 orientations in 22 rooms, plus a goal) and 17 observations (all combinations of walls, plus "star"). Both include 5 actions (stay in place, move forward, turn right, turn left, turn around) and have extremely noisy transitions and observations <ref> (Littman et al., 1995) </ref>. We ran the same collection of algorithms on these two environments with a slight change: truncated exact value iteration was given roughly 1000 seconds. Performance was measured slightly differently. <p> The fact that the agent needs to take an action to gain information and then execute the same action (forward) regardless of the outcome, is sufficient to destroy any single-vector-per-action approximation of the optimal policy <ref> (Littman et al., 1995) </ref>.
Reference: <author> Littman, M. L. </author> <year> (1994). </year> <title> The Witness algorithm: Solving partially observable Markov decision processes. </title> <type> Technical Report CS-94-40, </type> <institution> Brown University, Department of Computer Science, Providence, RI. </institution>
Reference: <author> Lovejoy, W. S. </author> <year> (1991). </year> <title> A survey of algorithmic methods for partially observable Markov decision processes. </title> <journal> Annals of Operations Research, </journal> <volume> 28 </volume> <pages> 47-66. </pages>
Reference-contexts: The theory of partially observable Markov decision processes (pomdp's) (Astrom, 1965; Small-wood and Sondik, 1973; Cassandra et al., 1994) models this situation and provides a basis for computing optimal behavior. A variety of algorithms have been developed for solving pomdp's <ref> (Lovejoy, 1991) </ref>, but because the problem is so computationally challenging (Papadimitriou and Tsitsiklis, 1987), most techniques are too inefficient to be used on all but the smallest problems (2 to 5 states (Cheng, 1988)).
Reference: <author> McCallum, R. A. </author> <year> (1992). </year> <title> First results with utile distinction memory for reinforcement learning. </title> <type> Technical Report 446, </type> <institution> Dept. Comp. Sci., Univ. Rochester. </institution> <note> See also Proceedings of Machine Learning Conference 1993. </note>
Reference-contexts: For each of 21 runs, we performed 75,000 steps of learning starting from the problem-specific belief state. During learning, actions were selected to maximize the current Q functions with Name jSj jAj jj Noise Shuttle (Chrisman, 1992) 8 3 5 T /O Cheese Maze <ref> (McCallum, 1992) </ref> 11 4 7 - Part Painting (Kushmerick et al., 1993) 4 4 2 T /O 4x4 Grid (Cassandra et al., 1994) 16 4 2 - Tiger (Cassandra et al., 1994) 2 3 2 O 4x3 Grid (Parr and Russell, 1995) 11 4 6 T Table 1: A suite of
Reference: <author> Moore, A. W. </author> <year> (1994). </year> <title> The parti-game algorithm for variable resolution reinforcement learning in multidimensional state spaces. </title> <booktitle> In Advances in Neural Information Processing Systems 6, </booktitle> <address> San Mateo, CA. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: An important consequence is that the belief states, in combination with the updating rule, form a completely observable Markov decision process (mdp) with a continuous state space, similar to problems addressed in the reinforcement-learning literature <ref> (Moore, 1994) </ref>. Our goal will be to find an approximation of the Q function over the continuous space of belief states and to use this as a basis for action in the environment.
Reference: <author> Nicholson, A. and Kaelbling, L. P. </author> <year> (1994). </year> <title> Toward approximate planning in very large stochastic domains. </title> <booktitle> In Proceedings of the AAAI Spring Symposium on Decision Theoretic Planning, </booktitle> <address> Stanford, California. </address>
Reference: <author> Papadimitriou, C. H. and Tsitsiklis, J. N. </author> <year> (1987). </year> <title> The complexity of Markov decision processes. </title> <journal> Mathematics of Operations Research, </journal> <volume> 12(3) </volume> <pages> 441-450. </pages>
Reference-contexts: A variety of algorithms have been developed for solving pomdp's (Lovejoy, 1991), but because the problem is so computationally challenging <ref> (Papadimitriou and Tsitsiklis, 1987) </ref>, most techniques are too inefficient to be used on all but the smallest problems (2 to 5 states (Cheng, 1988)). Recently, the Witness algorithm (Cassandra, 1994; Littman, 1994) has been used to solve pomdp's with up to 16 states.
Reference: <author> Parr, R. and Russell, S. </author> <year> (1995). </year> <title> Approximating opti-mal policies for partially observable stochastic domains. </title> <booktitle> In Proceedings of the International Joint Conference on Artificial Intelligence. </booktitle>
Reference-contexts: jj Noise Shuttle (Chrisman, 1992) 8 3 5 T /O Cheese Maze (McCallum, 1992) 11 4 7 - Part Painting (Kushmerick et al., 1993) 4 4 2 T /O 4x4 Grid (Cassandra et al., 1994) 16 4 2 - Tiger (Cassandra et al., 1994) 2 3 2 O 4x3 Grid <ref> (Parr and Russell, 1995) </ref> 11 4 6 T Table 1: A suite of extremely small pomdp's.
Reference: <author> Puterman, M. L. </author> <year> (1994). </year> <title> Markov Decision Processes| Discrete Stochastic Dynamic Programming. </title> <publisher> John Wiley & Sons, Inc., </publisher> <address> New York, NY. </address>
Reference-contexts: These values can be computed extremely efficiently for problems with dozens to thousands of states and a va riety of approaches are available <ref> (Puterman, 1994) </ref>. With the Q MDP values in hand, we can treat all the Q MDP values for each action as a single linear function and estimate the Q value for a belief state b as Q a (b) = s b (s) Q MDP (s; a).
Reference: <author> Ross, S. M. </author> <year> (1983). </year> <title> Introduction to Stochastic Dynamic Programming. </title> <publisher> Academic Press, </publisher> <address> New York. </address>
Reference-contexts: We restrict our attention to stationary, deterministic policies on the belief state, since this class is relatively simple and we are assured that it includes an optimal policy <ref> (Ross, 1983) </ref>. 2.3 PIECEWISE-LINEAR CONVEX FUNCTIONS A particularly powerful result of Sondik's is that the optimal value function for any pomdp can be approximated arbitrarily well by a piecewise-linear and convex (pwlc) function (Smallwood and Sondik, 1973; Littman, 1994).
Reference: <author> Rumelhart, D. E., Hinton, G. E., and Williams, R. J. </author> <year> (1986). </year> <title> Learning internal representations by error backpropagation. </title> <editor> In Rumelhart, D. E. and McClelland, J. L., editors, </editor> <booktitle> Parallel Distributed Processing: Explorations in the microstructures of cognition. Volume 1: Foundations, chapter 8. </booktitle> <publisher> The MIT Press, </publisher> <address> Cambridge, MA. </address>
Reference: <author> Russell, S. J. and Norvig, P. </author> <year> (1994). </year> <title> Artificial Intelligence: A Modern Approach. </title> <publisher> Prentice-Hall, </publisher> <address> En-glewood Cliffs, NJ. </address>
Reference: <author> Smallwood, R. D. and Sondik, E. J. </author> <year> (1973). </year> <title> The optimal control of partially observable Markov processes over a finite horizon. </title> <journal> Operations Research, </journal> <volume> 21 </volume> <pages> 1071-1088. </pages>
Reference: <author> Sondik, E. J. </author> <year> (1978). </year> <title> The optimal control of partially observable Markov processes over the infinite horizon: Discounted costs. </title> <journal> Operations Research, </journal> <volume> 26(2). </volume>
Reference-contexts: Further, there is a class of pomdp's that have value functions that are exactly pwlc <ref> (Sondik, 1978) </ref>. These results apply to the optimal Q functions as well: the Q function for action a, Q a (b) is the expected reward for a policy that starts in belief state b, takes action a, and then behaves optimally.
Reference: <author> Tsitsikilis, J. N. </author> <year> (1994). </year> <title> Asynchronous stohcastic aproximation and Q-learning. </title> <journal> Machine Learning, </journal> <volume> 16(3). </volume>
Reference: <author> Watkins, C. J. </author> <year> (1989). </year> <title> Learning with Delayed Rewards. </title> <type> PhD thesis, </type> <institution> Cambridge University. </institution>
Reference-contexts: At the same time that their algorithms attempt to learn the transition and observation probabilities, they used an extension of Q-learning <ref> (Watkins, 1989) </ref> to learn approximate Q functions for the learned pomdp model. Although it was not the emphasis of their work, their "replicated Q-learning" rule is of independent interest.
Reference: <author> Williams, R. J. and Baird, L. C. I. </author> <year> (1993). </year> <title> Tight performance bounds on greedy policies based on imperfect value functions. </title> <type> Technical Report NU-CCS-93-13, </type> <institution> Northeastern University, College of Computer Science, </institution> <address> Boston, MA. </address>
Reference-contexts: After its k-th iteration, the algorithm returns the exact k-step Q functions as collections of vectors, L a , for each action, a. The algorithm can be used to find arbitrarily accurate approximations to the optimal infinite-horizon Q functions and therefore policies that are arbitrarily close to optimal <ref> (Williams and Baird, 1993) </ref>. Unfortunately, the algorithm can take many, many iterations to find an approximately optimal value function, and for problems with a large number of observations, the size of the L a sets can grow explosively from iteration to iteration.
References-found: 28

