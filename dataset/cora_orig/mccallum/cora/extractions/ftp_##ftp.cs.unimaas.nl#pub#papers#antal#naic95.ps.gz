URL: ftp://ftp.cs.unimaas.nl/pub/papers/antal/naic95.ps.gz
Refering-URL: http://www.cs.rulimburg.nl/~antal/nn-pub.html
Root-URL: 
Email: fantal,weijters,herikg@cs.rulimburg.nl  
Title: Scaling Effects with Greedy and Lazy Machine-Learning Algorithms  
Author: Antal van den Bosch, Ton Weijters, Jaap van den Herik 
Address: PO Box 616 6200 MD Maastricht  
Affiliation: Department of Computer Science University of Maastricht  
Abstract: We report on a series of experiments in which three machine-learning algorithms are trained to hyphenate English words, viz. the backpropagation algorithm, the cascade-backpropagation algorithm, and the information-gain-tree algorithm (IG-Tree). English hyphenation is an interesting testing ground for machine learning: it has a few underlying principles, and a large amount of exceptions. A successful learning algorithm must be able to deal with both. The three learning algorithms vary in the way they compress the training material: by extracting a limited number of general rules (greedy learning), or by storing large amounts of instances (lazy learning). Our experiments show that the lazy learning algorithm (i.e., the IG-Tree algorithm) scales up better than the two greedy learning algorithms (the backpropagation algorithm and the cascade-backpropagation algorithm). Moreover, our results call for including very large problem sets in collections of machine-learning benchmark problems.
Abstract-found: 1
Intro-found: 1
Reference: [DV92] <author> W. Daelemans and A. van den Bosch. </author> <title> A neural network for hyphenation. </title> <editor> In I. Aleksander and J. Taylor, editors, </editor> <booktitle> Artificial Neural Networks 2, </booktitle> <volume> volume 2, </volume> <pages> pages 1647-1650. </pages> <address> Amsterdam: </address> <publisher> North-Holland, </publisher> <year> 1992. </year>
Reference-contexts: Each input pattern is associated with an output of 1 if the focus letter may follow a hyphen, and 0 otherwise (i.e., the problem is presented as a boundary decision task; cf. <ref> [DV92] </ref> for an earlier account of such a hyphenation experiment). In the case of the full data set, each partitioned training set contained about 676,000 instances, and each partitioned test set about 75,000 instances.
Reference: [FL90] <author> S. E. Fahlman and C. Lebiere. </author> <title> The cascade-correlation learning architecture. </title> <type> Technical Report CMU-CS-90-100, </type> <institution> School of Computer Science, Carnegie-Mellon University, </institution> <address> Pittsburgh, PA, </address> <year> 1990. </year>
Reference-contexts: For completeness' sake we mention three approaches similar to CBP: constructive learning by specialisation [RV91], the cascade-correlation algorithm <ref> [FL90] </ref>, and divide and conquer [RH93]. 2.3 Information-Gain-Tree The information-gain-tree (IG-Tree) algorithm is a data-oriented, symbolic, inductive decision-tree algorithm which can be seen as a basic, though powerful variant of ID3 [Qui86]. For a detailed description of the IG-Tree algorithm, we refer to [VD95]. <p> Future research should include elaborations on the CBP algorithm (e.g., different parameter settings, no connections between hidden units). Moreover, comparisons with other connectionist algorithms with growing architectures, such as Cascade-Correlation <ref> [FL90] </ref>, are envisaged in order to clarify why BP and CBP perform so similarly. In general, we would like to stress that scaling is an essential problem for the application of machine-learning techniques to natural, complex problems.
Reference: [MP88] <author> M. L. Minsky and S. A. Papert. </author> <title> Perceptrons: Expanded edition. </title> <address> Cambridge, MA: </address> <publisher> The MIT Press, </publisher> <year> 1988. </year> <note> First published in 1969. </note>
Reference-contexts: The largest benchmark problem set in PROBEN1 contains 8,124 instances, whereas several publicly accessible natural data sets contain many more. The aspect of the number of problem instances, is directly related to the scaling problem (cf. <ref> [MP88] </ref>).
Reference: [Pre94] <author> L. Prechelt. Proben1: </author> <title> A set of neural network benchmark problems and bench-marking rules. </title> <type> Technical Report 19/94, </type> <institution> Fakult at f ur Informatik, Universit at Karlsruhe, Germany, </institution> <year> 1994. </year>
Reference-contexts: To this purpose, publicly accessible collections of benchmark problems have been built. See, e.g., the PROBEN1 collection compiled and described by Prechelt <ref> [Pre94] </ref>. Application of a specific learning algorithm to these problems renders performance results, which can be used for categorising the learning algorithm. Benchmark collections should contain problems that differ in many aspects. Obviously, the various problems ask for different learning strategies. <p> The PROBEN1 collection <ref> [Pre94] </ref> is a good example of such a variety. However, one aspect seems to have been overlooked, viz. the number of problem instances. The largest benchmark problem set in PROBEN1 contains 8,124 instances, whereas several publicly accessible natural data sets contain many more.
Reference: [Qui86] <author> J. R. Quinlan. </author> <title> Induction of decision trees. </title> <journal> Machine Learning, </journal> <volume> 1 </volume> <pages> 81-206, </pages> <year> 1986. </year>
Reference-contexts: For completeness' sake we mention three approaches similar to CBP: constructive learning by specialisation [RV91], the cascade-correlation algorithm [FL90], and divide and conquer [RH93]. 2.3 Information-Gain-Tree The information-gain-tree (IG-Tree) algorithm is a data-oriented, symbolic, inductive decision-tree algorithm which can be seen as a basic, though powerful variant of ID3 <ref> [Qui86] </ref>. For a detailed description of the IG-Tree algorithm, we refer to [VD95]. The idea is that the IG-Tree algorithm compresses a training set of problem instances into a decision-tree structure.
Reference: [Qui94] <author> J. R. Quinlan. </author> <title> Comparing connectionist and symbolic learning methods. </title> <editor> In S. J. Hanson, G. A. Drastal, and R. L. Rivest, editors, </editor> <booktitle> Computational Learning Theory and Natural Learning Systems, volume 1: Constraints and Prospects, chapter 15, </booktitle> <pages> pages 445-456. </pages> <publisher> MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1994. </year>
Reference-contexts: These aspects include, among others, whether the problem is a classification task with a small or large number of output classes, whether the problem has a few or many exceptions, and whether solving the problem implies taking into account many input features simultaneously or examining them one by one <ref> [Qui94] </ref>. The PROBEN1 collection [Pre94] is a good example of such a variety. However, one aspect seems to have been overlooked, viz. the number of problem instances. The largest benchmark problem set in PROBEN1 contains 8,124 instances, whereas several publicly accessible natural data sets contain many more. <p> Finally, a third drawback is that learning with BP can become extremely slow when learning complex problems (particularly problems in which the relevance of an input feature depends on the value of other input features <ref> [Qui94] </ref>). 2.2 Cascade Backpropagation Cascade Backpropagation (CBP) is a variant of BP in that it uses the same connection-strength adjustment rules. The first difference is that CBP starts off without any hidden layer, i.e., only the input layer and the output layer are fully connected to each other.
Reference: [RH93] <author> S. G. Romaniuk and L. O. Hall. </author> <title> Divide and conquer neural networks. </title> <booktitle> Neural Networks, </booktitle> <volume> 6 </volume> <pages> 1105-1116, </pages> <year> 1993. </year>
Reference-contexts: For completeness' sake we mention three approaches similar to CBP: constructive learning by specialisation [RV91], the cascade-correlation algorithm [FL90], and divide and conquer <ref> [RH93] </ref>. 2.3 Information-Gain-Tree The information-gain-tree (IG-Tree) algorithm is a data-oriented, symbolic, inductive decision-tree algorithm which can be seen as a basic, though powerful variant of ID3 [Qui86]. For a detailed description of the IG-Tree algorithm, we refer to [VD95].
Reference: [RHW86] <author> D. E. Rumelhart, G. E. Hinton, and R. J. Williams. </author> <title> Learning internal representations by error propagation. </title> <editor> In D. E. Rumelhart and J. L. McClelland, editors, </editor> <booktitle> Parallel Distributed Processing: Explorations in the Microstructure of Cognition, volume 1: Foundations, </booktitle> <pages> pages 318-362. </pages> <address> Cambridge, MA: </address> <publisher> The MIT Press, </publisher> <year> 1986. </year>
Reference-contexts: In Section 3, we describe the training material in detail, and the regimen of the experiments. In Section 4, we present the results of our experiments. Finally, Section 5 contains our conclusions. 2 The learning algorithms 2.1 Backpropagation Backpropagation <ref> [RHW86] </ref> is a popular gradient-descent learning algorithm. It operates on multi-layered perceptrons, viz. networks of interconnected layers of units, in which an input layer is indirectly connected to an output layer via one or more hidden layers.
Reference: [RV91] <author> A. N. Refenes and S. Vithlani. </author> <title> Constructive learning by specialisation. </title> <editor> In T. Kohonen, K. M akisara, O. Simula, and J. Kangas, editors, </editor> <booktitle> Proceedings of ICANN-91, </booktitle> <address> Espoo, Finland, </address> <pages> pages 923-929. </pages> <address> Amsterdam: </address> <publisher> North Holland, </publisher> <year> 1991. </year>
Reference-contexts: For completeness' sake we mention three approaches similar to CBP: constructive learning by specialisation <ref> [RV91] </ref>, the cascade-correlation algorithm [FL90], and divide and conquer [RH93]. 2.3 Information-Gain-Tree The information-gain-tree (IG-Tree) algorithm is a data-oriented, symbolic, inductive decision-tree algorithm which can be seen as a basic, though powerful variant of ID3 [Qui86]. For a detailed description of the IG-Tree algorithm, we refer to [VD95].
Reference: [SR87] <author> T. J. Sejnowski and C. S. Rosenberg. </author> <title> Parallel networks that learn to pronounce english text. </title> <journal> Complex Systems, </journal> <volume> 1 </volume> <pages> 145-168, </pages> <year> 1987. </year>
Reference-contexts: have standardised, fixed-length input encodings of problem instances, we converted the hyphenated words in each training and test set into fixed-width patterns, consisting of a focus letter surrounded by 3 left context letters and 3 right context letters, including word boundaries (an encoding technique which can be found in, e.g., <ref> [SR87] </ref>). Each input pattern is associated with an output of 1 if the focus letter may follow a hyphen, and 0 otherwise (i.e., the problem is presented as a boundary decision task; cf. [DV92] for an earlier account of such a hyphenation experiment). <p> The training-set partitionings from the smaller subset contained about 151,500 instances; the test-set partitionings from that subset contained about 17,000 instances. In our experiments with BP and CBP, we adopted the encoding technique used by Sejnowski and Rosenberg <ref> [SR87] </ref>, i.e., a local coding of each of the 7 input letters. Given the fact that we distinguished between 42 letters, the input layer of the BP and CBP networks contained 42 fl 7 = 294 units.
Reference: [TZ90] <author> R. Treiman and A. Zukowski. </author> <title> Toward an understanding of english syllabification. </title> <journal> Journal of Memory and Language, </journal> <volume> 29 </volume> <pages> 66-85, </pages> <year> 1990. </year>
Reference-contexts: English hyphenation contains a fairly large number of cases that obey to a few simple, pronunciation-based principles <ref> [TZ90] </ref>. However, in English a principle exists that introduces an large amount of periphery, viz. the Morphological Principle, stating that morphological boundaries must receive hyphens, regardless of any other applicable principle.
Reference: [VD93] <author> A. van den Bosch and W. Daelemans. </author> <title> Data-oriented methods for grapheme-to-phoneme conversion. </title> <booktitle> In Proceedings of the 6th Conference of the EACL, </booktitle> <pages> pages 45-53. </pages>
Reference-contexts: Nevertheless, for the problem under consideration (i.e., hyphenation) and for some problems to which IG-Tree was applied previously [VCDD95], learning and classification is very fast compared to that of BP and CBP. Furthermore, IG-Tree's generalisation performance on grapheme-phoneme conversion was shown to be better than that of BP <ref> [VD93] </ref>. 3 Experimental Settings The problem which we have adopted for our experiments is English hyphenation, i.e., the problem of finding the positions in a spelling word at which a hyphen (`-') can be placed.
Reference: [VCDD95] <author> A. van den Bosch, A. Content, W. Daelemans, and B. de Gelder. </author> <title> Measuring the complexity of writing systems. </title> <journal> Journal of Quantitative Linguistics, </journal> <volume> 1(3), </volume> <year> 1995. </year>
Reference-contexts: This laziness may result in very large trees when the problem to be learned is complex. Nevertheless, for the problem under consideration (i.e., hyphenation) and for some problems to which IG-Tree was applied previously <ref> [VCDD95] </ref>, learning and classification is very fast compared to that of BP and CBP.
Reference: [VD95] <author> A. van den Bosch and W. Daelemans. </author> <title> A distributed, yet symbolic model of text-to-speech processing. </title> <note> to appear in P. </note> <editor> Broeder and J. Murre (Eds.), </editor> <booktitle> Models of language learning: inductive and deductive approaches. </booktitle> <address> Cambridge, MA: </address> <publisher> MIT Press, forthcoming. </publisher>
Reference-contexts: For a detailed description of the IG-Tree algorithm, we refer to <ref> [VD95] </ref>. The idea is that the IG-Tree algorithm compresses a training set of problem instances into a decision-tree structure. <p> In the case of CBP, all experiments were run up to the point where 9 attempted new units did not result in a lower mean-squared-output error. For IG-Tree, the standard algorithm was used <ref> [VD95] </ref>. Being a symbolic learning algorithm, the 7-letter input patterns were not encoded by binary values, but by the letters themselves.
References-found: 14

