URL: ftp://synapse.cs.byu.edu/pub/papers/martinez_88c.ps
Refering-URL: ftp://synapse.cs.byu.edu/pub/papers/details.html
Root-URL: 
Title: ASOCS: A Multilayered Connectionist Network with Guaranteed Learning of Arbitrary Mappings  
Author: Tony R. Martinez 
Address: Provo, Utah 84602  
Affiliation: Computer Science Dept., 230 TMCB Brigham Young University,  
Note: Presented at 2nd IEEE International Conference on Neural Networks, 1988.  
Abstract: This paper reviews features of a new class of multilayer connectionist architectures known as ASOCS (Adaptive Self-Organizing Concurrent Systems). ASOCS is similar to most decision-making neural network models in that it attempts to learn an adaptive set of arbitrary vector mappings. However, it differs dramatically in its mechanisms. ASOCS is based on networks of adaptive digital elements which self-modify using local information. Function specification is entered incrementally by use of rules, rather than complete input-output vectors, such that a processing network is able to extract critical features from a large environment and give output in a parallel fashion. Learning also uses parallelism and self-organization such that a new rule is completely learned in time linear with the depth of the network. The model guarantees learning of any arbitrary mapping of boolean input-output vectors. The model is also stable in that learning does not erase any previously learned mappings except those explicitly contradicted. 
Abstract-found: 1
Intro-found: 1
Reference: 1. <author> Carpenter, G.A., and Grossberg, S., </author> <title> A massively parallel architecture fo a self-organizing neural pattern recognition machine, Computer Vision, </title> <journal> Graphics, and Image Processing, </journal> <volume> 37, </volume> <pages> pp. 54-115, </pages> <year> (1987). </year>
Reference-contexts: Although a simplistic and incomplete dichotomy, I classify current artificial neural systems into categorization systems and decision systems. Examples of categorization systems include adaptive resonance theory <ref> [1] </ref> and self-organizing feature maps [4]. These systems do unsupervised learning of categories and classifications of arbitrary inputs. They seek solutions in the important domain of building up reasonable categories from an unknown input environment, mapping similar things into similar classes.
Reference: 2. <author> Chang, J. and J. J. Vidal, </author> <title> Inferencing in Hardware, </title> <booktitle> Proceedings of the MCC-University Research Symposium, </booktitle> <address> Austin, TX, </address> <month> (July </month> <year> 1987). </year>
Reference-contexts: A number of specific ASOCS architectures have been proposed [6] and VLSI implementation and testing are underway <ref> [2] </ref>. An ASOCS system solves any arbitrary dynamic mapping of boolean input vectors to boolean output vectors. The atomic knowledge input to the system is known as an instance and is made up of a variable length boolean input vector and an associated target output. <p> The second class of algorithms [5,6] require only that the control unit store a static list of the variables upon which its DPLM currently functions. (Even this small portion of memory can be obviated by alternate broadcast techniques <ref> [2] </ref>). Each node can then do a direct comparison of its variable list with that of the broadcast new instance, and immediately fulfill a deterministic modification from a small set of rules depending on how it matches with the new instance.
Reference: 3. <author> Hinton, G., Sejnowski, T, and D. Ackley, </author> <title> Boltzmann Machines: Constraint Satisfaction Networks that Learn,"Tech. </title> <type> Rep CMU-CS-84-119, </type> <address> CMU, Pittsburgh, PA, </address> <year> (1984). </year>
Reference-contexts: This is the realm of decision systems. These systems are required to do more arbitrary mappings and require some type of supervised learning (teacher, environment reward, etc.) Attempts at this type of system include backpropagation techniques [9] and Boltzmann machines <ref> [3] </ref>. There is certainly overlap between categorization and decision systems, and classifying a model as one or the other is a matter of degree. ASOCS falls into the realm of decision making systems, and thus can be most accurately compared and measured againstthat class of models.
Reference: 4. <author> Kohonen, T., </author> <title> Self-organization and associative memory, </title> <publisher> Springer Verlag, </publisher> <address> New York, </address> <year> (1984). </year>
Reference-contexts: Although a simplistic and incomplete dichotomy, I classify current artificial neural systems into categorization systems and decision systems. Examples of categorization systems include adaptive resonance theory [1] and self-organizing feature maps <ref> [4] </ref>. These systems do unsupervised learning of categories and classifications of arbitrary inputs. They seek solutions in the important domain of building up reasonable categories from an unknown input environment, mapping similar things into similar classes.
Reference: 5. <author> Martinez, T. R., </author> <title> Adaptive Self-Organizing Logic Networks, </title> <type> Ph.D. Dissertation, Technical Report - CSD 860093, </type> <institution> University of California, </institution> <address> Los Angeles, CA (May 1986). </address>
Reference: 6. <author> Martinez T. R., </author> <title> Models of Parallel Adaptive Logic, </title> <booktitle> Proceedings of the 1987 IEEE Systems Man and Cybernetics Conference, </booktitle> <pages> pp. 290-296, </pages> <month> (October, </month> <year> 1987). </year>
Reference-contexts: A new class of multilayer connectionist systems, ASOCS (adaptive self-organizing concurrent systems), was created with the goal of fulfilling the desired functionality of connectionist methods, while overcoming some of the drawbacks of current models. A number of specific ASOCS architectures have been proposed <ref> [6] </ref> and VLSI implementation and testing are underway [2]. An ASOCS system solves any arbitrary dynamic mapping of boolean input vectors to boolean output vectors.
Reference: 7. <author> Martinez, T. R. and J. J. Vidal, </author> <title> Adaptive Parallel Logic Networks, </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> Vol. 5, </volume> <pages> pp. 26-58, </pages> <year> (1988). </year>
Reference-contexts: The control unit also has the ability to read the broadcast mechanism, and send messages on the bidirectional control paths between its closest neighbors. To this point, learning algorithms come from two broad classes. The first <ref> [7] </ref> requires some memory in the control unit of the individual nodes, storing the manner in which the node reacts to learned instances.
Reference: 8. <author> Martinez, T.R., </author> <title> Digital Neural Networks, </title> <booktitle> Proceedings of the 1988 IEEE Systems Man and Cybernetics Conference, </booktitle> <month> (August, </month> <year> 1988). </year>
Reference: 9. <editor> Rumelhart, D. and McClelland, J., </editor> <booktitle> Parallel Distributed Processing:, </booktitle> <volume> Vol. I, </volume> <pages> pp. 318-362, </pages> <publisher> MIT Press, </publisher> <year> (1986). </year>
Reference-contexts: This is the realm of decision systems. These systems are required to do more arbitrary mappings and require some type of supervised learning (teacher, environment reward, etc.) Attempts at this type of system include backpropagation techniques <ref> [9] </ref> and Boltzmann machines [3]. There is certainly overlap between categorization and decision systems, and classifying a model as one or the other is a matter of degree. ASOCS falls into the realm of decision making systems, and thus can be most accurately compared and measured againstthat class of models.
References-found: 9

