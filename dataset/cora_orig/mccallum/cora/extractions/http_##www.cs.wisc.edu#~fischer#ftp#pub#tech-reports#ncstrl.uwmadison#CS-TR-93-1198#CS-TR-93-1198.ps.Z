URL: http://www.cs.wisc.edu/~fischer/ftp/pub/tech-reports/ncstrl.uwmadison/CS-TR-93-1198/CS-TR-93-1198.ps.Z
Refering-URL: http://www.cs.wisc.edu/~fischer/ftp/pub/tech-reports/ncstrl.uwmadison/CS-TR-93-1198/
Root-URL: http://www.cs.wisc.edu
Title: Designing Memory Consistency Models For Shared-Memory Multiprocessors  
Author: Sarita V. Adve 
Abstract: Computer Sciences Technical Report #1198 University of Wisconsin-Madison December 1993 
Abstract-found: 1
Intro-found: 1
Reference: [AdH90a] <author> S. V. ADVE and M. D. HILL, </author> <title> Implementing Sequential Consistency in Cache-Based Systems, </title> <booktitle> Intl. Conf. on Parallel Processing, </booktitle> <month> August </month> <year> 1990, </year> <month> I47-I50. </month>
Reference-contexts: once ownership of the requested line is obtained (other processors could still have a stale copy of the line); sequential consistency is maintained by ensuring that the effects of the subsequent operations of the writing processor are not made visible to any other processor until the write is globally performed <ref> [AdH90a] </ref>. Gharachorloo et al. describe the use of non-binding hardware prefetching and speculative execution for overlapped memory accesses with sequential consistency in cache-based systems [GGH91b]. <p> All of the above schemes that allow a processor to overlap or reorder its memory accesses without software support, however, either require complex or restricted hardware (e.g., hardware prefetching and rollback for [GGH91b] and restricted networks for [Col84-92, LHH91]) or the gains are expected to be small (e.g., <ref> [AdH90a, LHH91] </ref>). Further, the optimizations of these schemes can be exploited by hardware (or the runtime system software), but cannot be exploited by compilers. A relevant scheme from distributed message passing systems is Jefferson's virtual time scheme using the time warp mechanism [Jef85]. <p> No practical implementations of concurrent consistency that are not sequentially consistent are given. The above sufficient conditions were originally stated as the definition of strong ordering and claimed to be sufficient for sequential consistency [DSB86]. However, there are programs for which the conditions violate sequential consistency <ref> [AdH90a] </ref> and strong ordering was later redefined as the hardware quality of a multiprocessor which guarantees sequential consistency without further software aid [DuS90, Sch89]. Hutto and Ahamad have hierarchically characterized various weak models [HuA90].
Reference: [AdH90b] <author> S. V. ADVE and M. D. HILL, </author> <title> Weak Ordering ANew Definition, </title> <booktitle> Proc. 17th Annual Intl. Symp. on Computer Architecture, </booktitle> <month> May </month> <year> 1990, </year> <pages> 2-14. </pages>
Reference-contexts: These observations lead to a specification methodology called the sequential consistency normal form or SCNF described below. 3.2. Sequential Consistency Normal Form (SCNF) The sequential consistency normal form (SCNF) method of specifying memory models (earlier called weak ordering with respect to a synchronization model <ref> [AdH90b] </ref>) is as follows. Definition 3.1: A memory model is in sequential consistency normal form (SCNF) iff it guarantees sequential consistency to a set of formally-characterized programs. Further, the program characteriza tion should not consider non-sequentially consistent executions. <p> Nevertheless, Chapter 4 informally looks at general systems and executions, and informally uses the notion that the values of the shared-memory reads capture the result. - -- Chapter 4 An SCNF Memory Model: Data-Race-Free-0 This chapter defines an SCNF model data-race-free-0 <ref> [AdH90b] </ref> that is based on the intuition of weak ordering. <p> Definition of the Data-Race-Free-0 Memory Model Section 4.1.1 and 4.1.2 respectively motivate and define the data-race-free-0 model (first presented in <ref> [AdH90b] </ref>). Section 4.1.3 describes the support needed in programming languages for data-race-free-0, and how the definition of data-race-free-0 can be adapted to the available support. 4.1.1. Motivation for Data-Race-Free-0 The problem of maintaining sequential consistency manifests itself when two or more processors interact through memory operations on common variables. <p> There are several ways to formalize data-race-free-0 programs. The following method was used in our original definition of the data-race-free models <ref> [AdH90b, AdH93] </ref>. This method uses a happens-before relation to indicate when two memory operations are ordered by intervening synchronization operations. <p> The following first shows how weak ordering meets this requirement, and then shows how data-race-free-0 allows more aggressive implementations. (This work is based on material in <ref> [AdH90b, AdH93] </ref>.) Weak ordering requires a processor to stall on every synchronization operation until its preceding operations complete. Thus, in figure 4.6 (b), weak ordering would prohibit P 1 from issuing its write of Valid, and all following operations, until the data write on A completes. <p> The first of these implementations maintains sequential consistency by ensuring that when P 2 issues its read of Valid, P 1 will not allow the read to return the updated value of Valid until P 1 's operations preceding the write of Valid complete <ref> [AdH90b] </ref>. This implies that P 1 need never delay its memory operations; specifically, its write of Valid executes earlier than with the implementations discussed above. <p> Similarly, hardware can actually execute more operations than specified by an execution as long as the effects of these extra operations are not seen. This is in contrast to many earlier specifications <ref> [AdH90b, DSB86, GLL90, ScD87] </ref> which impose constraints on the real time ordering of events. <p> Sections 5.3.2 and 5.3.3 give one system-centric specification and implementation for the synchronization and control requirements. For the sake of simplicity, the synchronization and control specifications given here are not very aggressive. (Parts of this work were originally presented in <ref> [AdH90b, AdH92, AdH93] </ref>.) - -- The proofs that the new low-level system-centric specifications of the data and synchronization requirements specify executions that also satisfy the system-centric specification of Condition 5.10 are straightforward, but the corresponding proof for the control requirement is fairly complex. <p> However, this complexity is manageable for software-based shared virtual memory systems, as demonstrated by recent implementations based on similar ideas [KCZ92]. (These are further discussed at the end of this section.) The following method is based on the proposal in <ref> [AdH90b] </ref>. Low-Level System-Centric Specification for Data Requirement. <p> The remote service mechanism should be included in the implementation proposal presented in our previous work <ref> [AdH90b] </ref>. - -- hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh P1 P2 A = 1 B = 1data data synchronizationsynchronization A = 1B = 2 Consider an execution where the synchronization writes are allowed to get ownership of their lines before the preceding data writes. <p> This is an important contribution because in the past, determining when a certain optimization was safe to use involved lengthy and complex proofs that required reasoning about executions on the optimized system and proving that such an execution would appear sequentially consistent <ref> [AdH90b, AdH93, GLL90, GMG91, GiM92] </ref>; our mapping largely eliminates that complexity. The mapping between program information and optimizations leads to a characterization of SCNF memory models that determines their performance and programmability.
Reference: [AHM91] <author> S. V. ADVE, M. D. HILL, B. P. MILLER and R. H. B. NETZER, </author> <title> Detecting Data Races on Weak Memory Systems, </title> <booktitle> Proc. 18th Annual Intl. Symp. on Computer Architecture, </booktitle> <month> May </month> <year> 1991, </year> <pages> 234-243. </pages>
Reference-contexts: If a program is not data-race-free, on the other hand, an execution of it on a data-race-free system may not be sequentially con-hhhhhhhhhhhhhhhhhh 33. Most of this chapter (except Section 8.5) is taken verbatim from a paper jointly written with others <ref> [AHM91] </ref>. The material is copyrighted by ACM and reproduced here with the permission of all the co-authors. - -- sistent. Applying the dynamic techniques to such an execution may produce unpredictable results. This chapter develops a system-centric specification for data-race-free systems that allows data races to be dynamically detected. <p> The actual technique for detecting the data races in an execution on a system that obeys our specification was primarily developed by other authors of this joint work. This chapter summarizes the technique and briefly mentions its limitations; a more detailed description appears in <ref> [AHM91] </ref>. The rest of this chapter is organized as follows. Section 8.1 discusses the potential problems in applying dynamic data race detection techniques to data-race-free systems. Section 8.2 develops the system-centric specification to overcome these problems. <p> Thus, the set of data races that are not affected by any other data race in E form a valid reportable set of data races that also occur in some sequentially consistent execution. hhhhhhhhhhhhhhhhhh 34. The definition of an SCP in <ref> [AHM91] </ref> required x,y to be a data race in E if and only if it is also a data race in Eseq. <p> Detecting Data Races on Data-Race-Free Systems This section summarizes how Condition 8.4 can be used to dynamically detect sequentially consistent data races on data-race-free systems, and briefly mentions the limitations of our technique. A detailed description of this material appears in <ref> [AHM91] </ref>. A post-mortem approach can be used to locate sets of data races, where each set contains at least one data race that belongs to a specific SCP. <p> The other limitations of this work are that we do not detect sequentially consistent data races that are not in the first partitions, and the race detection overhead is quite high. We argue in <ref> [AHM91] </ref> how all the limitations of our technique are analogous to the limitations of dynamic techniques for sequentially consistent systems. 8.5. Related Work To the best of our knowledge, the only other work related to debugging on non-sequentially consistent systems is by Gharachorloo and Gibbons [GhG91]. <p> This is a natural choice considering the simplicity of sequential consistency, and considering that a large body of algorithms has been developed (often implicitly) assuming sequential consistency. Our general approach outlined hhhhhhhhhhhhhhhhhh 36. The data race detection work is joint work with others <ref> [AHM91] </ref>; PLpc1 and PLpc2 are derived from joint work with others on the PLpc model [GAG92]. - -- above, however, is valid with any other base model as well.
Reference: [AGG91] <author> S. V. ADVE, K. GHARACHORLOO, A. GUPTA, J. L. HENNESSY and M. D. HILL, </author> <title> Memory Models: An Evaluation And A Unification, Presentation in the Second Workshop on Scalable Shared-Memory Multiprocessors, </title> <institution> Toronto, </institution> <month> May </month> <year> 1991. </year>
Reference-contexts: Thus, programmers must reason with many different (and fairly complex) interfaces, making porting programs between different models difficult. Figure 1.3 captures some of this variety of interfaces by showing several currently implemented hardware-centric models (to be defined further in Chapter 2) and their relationships with each other <ref> [AGG91] </ref>.
Reference: [AdH92] <author> S. V. ADVE and M. D. HILL, </author> <title> Sufficient Conditions for Implementing the Data-Race-Free-1 Memory Model, </title> <type> Computer Sciences Technical Report #1107, </type> <institution> University of Wisconsin, Madison, </institution> <month> September </month> <year> 1992. </year>
Reference-contexts: Our previous work describes how the above formalism represents real systems as follows <ref> [AdH92] </ref>. ``Although real systems do not usually provide physical copies of the entire memory to any processor, a logical copy of memory can be assumed to be associated with every processor. <p> A write sub-operation, W (i), corresponds to the write W performing with respect to processor P i . A read sub-operation, R (i), corresponds to the read R performing with respect to all processors. Our first use of Collier's work in <ref> [AdH92] </ref> assumed that a write always had n sub-operations. The relaxation that only some sub-operations need be included was first included in [GAG93] to represent more systems; e.g., systems with software-based cache-coherence where a write need not update the memory copies of all processors. Further, the work in [AdH92] implicitly assumes <p> work in <ref> [AdH92] </ref> assumed that a write always had n sub-operations. The relaxation that only some sub-operations need be included was first included in [GAG93] to represent more systems; e.g., systems with software-based cache-coherence where a write need not update the memory copies of all processors. Further, the work in [AdH92] implicitly assumes that in any execution, only a finite number of operations can be ordered before another operation by program order. Our formalism and that in [GAG93] do not make this assumption. <p> Although such implementations are unlikely, the formal system-centric specification needs to ensure that - -- they are prohibited.) hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh Initially X = Y = 0 P0 P1 hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh The formal system-centric specification for data-race-free-0 and the proof of its correctness follow. These are based on the work in <ref> [AdH92] </ref>. In the rest of this chapter, data (respectively synchronization) operation refers to an operation distinguished as data (respectively synchronization). <p> Sections 5.3.2 and 5.3.3 give one system-centric specification and implementation for the synchronization and control requirements. For the sake of simplicity, the synchronization and control specifications given here are not very aggressive. (Parts of this work were originally presented in <ref> [AdH90b, AdH92, AdH93] </ref>.) - -- The proofs that the new low-level system-centric specifications of the data and synchronization requirements specify executions that also satisfy the system-centric specification of Condition 5.10 are straightforward, but the corresponding proof for the control requirement is fairly complex. <p> Then, if remote service is not provided for data writes, each process will stall the data write of the other process indefinitely, leading to deadlock. hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh Method 4: Second Aggressive Implementation For Data-Race-Free-0 The following method is based on the material in <ref> [AdH92, AdH93] </ref>. Low-Level System-Centric Specification for Data Requirement. Condition 5.14: An execution obeys the data requirement if it obeys the following. <p> The third concept involves a loop in a program that does not terminate in any sequentially consistent execution, and instruction instances from the loop. The system-centric specification follows. It is mostly based on the work in <ref> [AdH92] </ref>, and assumes that the high-level data and synchronization requirements are obeyed. Condition 5.16: An execution E of program Prog obeys the control requirement if it obeys the high level data and synchronization requirements of Condition 5.10, and the following. (a) Let read R control an operation X in E. <p> Call the flush write as the end operation of the interval. hhhhhhhhhhhhhhhhhh not present in previous versions of the control condition <ref> [AdH92] </ref> since those versions assumed a stricter model of an execution as discussed in Section 5.1.1. <p> System constraints are less obvious from the above definition, but follow from the earlier discussion in Sections 7.1 and 7.2. The system-centric specifications of this and the next section are generalizations of similar specifications developed for the data-race-free-1 <ref> [AdH92] </ref> and PLpc memory models [AGG93]. The generalization makes explicit the relation between the model, the programmer constraints, and the system constraints. The methodology used to describe the specifications is based on the work in [AdH92] and [GAG93]. <p> of this and the next section are generalizations of similar specifications developed for the data-race-free-1 <ref> [AdH92] </ref> and PLpc memory models [AGG93]. The generalization makes explicit the relation between the model, the programmer constraints, and the system constraints. The methodology used to describe the specifications is based on the work in [AdH92] and [GAG93]. Section 7.6 further explains the relationship between the specifications of this chapter and those in [AdH92, AGG93, GAG93]. <p> The generalization makes explicit the relation between the model, the programmer constraints, and the system constraints. The methodology used to describe the specifications is based on the work in [AdH92] and [GAG93]. Section 7.6 further explains the relationship between the specifications of this chapter and those in <ref> [AdH92, AGG93, GAG93] </ref>. <p> Low-Level System-Centric Specifications and Hardware Implementations The following gives low-level system-centric specifications of the valid path and control requirements and describes corresponding hardware implementations. As mentioned earlier, the specifications of this section and the various concepts they use are related to those in <ref> [AdH92, AGG93, GAG93] </ref>; Section 7.6 discusses this relationship. Valid path requirement. The high-level valid path requirement in Condition 7.16 is analogous to the data requirement of the data-race-free-0 model discussed in Chapter 5. Consequently, analogous low-level specifications are possible. <p> We define the control relation at a high-level that expresses the above properties below; Appendix D gives a low-level and more constructive definition that obeys the following high-level properties. Some of the underlying concepts for the control relation were first developed for the data-race-free-1 model <ref> [AdH92] </ref> and then formalized for the PLpc model [AGG93]. The material in this section and Appendix D is a generalization of those concepts and formalizations for all models within our framework. We first explain some terminology used by the following definitions. <p> Again, the implementations discussed below are based on those in <ref> [AdH92, AGG93, GAG93] </ref>. <p> The model using the signal-await construct in Section 7.3.2 would not be possible without the above extension. The generic system-centric specifications of Sections 7.4 and 7.5 are generalizations of the specifications for data-race-free-1 in <ref> [AdH92] </ref> and PLpc in [AGG93]. Both of these models express their conditions essentially in the form of a valid path requirement and a control requirement. (The control requirement for data-race-free-1 - -- [AdH92] is more conservative than Condition 7.21, while that for PLpc [AGG93] is similar to Condition 7.21.) However, the <p> The generic system-centric specifications of Sections 7.4 and 7.5 are generalizations of the specifications for data-race-free-1 in <ref> [AdH92] </ref> and PLpc in [AGG93]. Both of these models express their conditions essentially in the form of a valid path requirement and a control requirement. (The control requirement for data-race-free-1 - -- [AdH92] is more conservative than Condition 7.21, while that for PLpc [AGG93] is similar to Condition 7.21.) However, the data-race-free-1 and PLpc specifications do not explicitly show the relationship between the two requirements and the information the models derive from the programmer.
Reference: [AGG93] <author> S. V. ADVE, K. GHARACHORLOO, A. GUPTA, J. L. HENNESSY and M. D. HILL, </author> <title> Sufficient System Requirements for Supporting the PLpc Memory Model, </title> <type> Computer Sciences Technical Report #1200, </type> <institution> University of Wisconsin, Madison, </institution> <month> December </month> <year> 1993. </year> <note> Also available as Technical Report #CSL-TR-93-595, </note> <institution> Stanford University, </institution> <month> December </month> <year> 1993. </year>
Reference-contexts: The reasoning used in this section has evolved from other joint work <ref> [AGG93, GAG93] </ref>. We consider compiler optimizations that involve reordering instructions of the program (e.g., loop transformations) and allocating shared-memory locations to registers. The optimization of register allocation does not have a direct analog in the runtime system, since this optimization results in eliminating certain operations from the original input program. <p> Call the flush write as the end operation of the interval. hhhhhhhhhhhhhhhhhh not present in previous versions of the control condition [AdH92] since those versions assumed a stricter model of an execution as discussed in Section 5.1.1. These parts were developed as part of other joint work <ref> [AGG93] </ref>. - -- It follows that the register reads of the first type of interval can be modeled as memory reads that occur (in the execution order) just after the start read of this interval. <p> System constraints are less obvious from the above definition, but follow from the earlier discussion in Sections 7.1 and 7.2. The system-centric specifications of this and the next section are generalizations of similar specifications developed for the data-race-free-1 [AdH92] and PLpc memory models <ref> [AGG93] </ref>. The generalization makes explicit the relation between the model, the programmer constraints, and the system constraints. The methodology used to describe the specifications is based on the work in [AdH92] and [GAG93]. <p> The generalization makes explicit the relation between the model, the programmer constraints, and the system constraints. The methodology used to describe the specifications is based on the work in [AdH92] and [GAG93]. Section 7.6 further explains the relationship between the specifications of this chapter and those in <ref> [AdH92, AGG93, GAG93] </ref>. <p> For processors that do speculative execution, Figure 7.12 illustrates the need for this condition. (Similar examples appear in <ref> [AGG93] </ref>.) In part (a) of the figure, for every sequentially consistent execution, P 1 's while loop will be in an - -- infinite loop and so its write of X will not be executed. Therefore, P 2 will never write an error message on the output. <p> Low-Level System-Centric Specifications and Hardware Implementations The following gives low-level system-centric specifications of the valid path and control requirements and describes corresponding hardware implementations. As mentioned earlier, the specifications of this section and the various concepts they use are related to those in <ref> [AdH92, AGG93, GAG93] </ref>; Section 7.6 discusses this relationship. Valid path requirement. The high-level valid path requirement in Condition 7.16 is analogous to the data requirement of the data-race-free-0 model discussed in Chapter 5. Consequently, analogous low-level specifications are possible. <p> Some of the underlying concepts for the control relation were first developed for the data-race-free-1 model [AdH92] and then formalized for the PLpc model <ref> [AGG93] </ref>. The material in this section and Appendix D is a generalization of those concepts and formalizations for all models within our framework. We first explain some terminology used by the following definitions. <p> The control relation also requires determining if a memory operation in one execution executes in some sequentially consistent execution and vice versa as follows. (Most of the following is taken from <ref> [AGG93] </ref>.) We say an operation O in execution E 1 executes in some execu - -- tion E 2 if its instruction instance executes in E 2 , and the corresponding operation of the instruction instance in E 2 accesses the same address and writes the same value (if O is <p> Again, the implementations discussed below are based on those in <ref> [AdH92, AGG93, GAG93] </ref>. <p> We consider the compiler optimizations of reordering operations of a process and allocating shared-memory locations in registers. We reason about register allocation in the same way as for data-race-free-0; part of the reasoning was done jointly with Kourosh Gharachorloo for the work in <ref> [AGG93, GAG93] </ref>. Thus, we allow two types of intervals over which a memory location can be allocated to a register. The first type allows only read operations to be substituted by register operations and is preceded by a start memory read. <p> The following discusses the constraints on the compiler imposed by the valid path and control requirements. Much of the discussion parallels that for data-race-free-0 in Section 5.4, and evolved from joint work in <ref> [AGG93, GAG93] </ref>. Recall that the compiler must ensure that the relevant constraints are obeyed for all executions (that might be possible on the system the output program will run on). Below, all references to an ordering refer to program order, unless stated otherwise. <p> The model using the signal-await construct in Section 7.3.2 would not be possible without the above extension. The generic system-centric specifications of Sections 7.4 and 7.5 are generalizations of the specifications for data-race-free-1 in [AdH92] and PLpc in <ref> [AGG93] </ref>. Both of these models express their conditions essentially in the form of a valid path requirement and a control requirement. (The control requirement for data-race-free-1 - -- [AdH92] is more conservative than Condition 7.21, while that for PLpc [AGG93] is similar to Condition 7.21.) However, the data-race-free-1 and PLpc specifications <p> generalizations of the specifications for data-race-free-1 in [AdH92] and PLpc in <ref> [AGG93] </ref>. Both of these models express their conditions essentially in the form of a valid path requirement and a control requirement. (The control requirement for data-race-free-1 - -- [AdH92] is more conservative than Condition 7.21, while that for PLpc [AGG93] is similar to Condition 7.21.) However, the data-race-free-1 and PLpc specifications do not explicitly show the relationship between the two requirements and the information the models derive from the programmer. <p> It would be interesting to formalize results that would allow translating - -- specifications from one abstraction to another easily. Theorem H.1 in Appendix H formalizes one type of translation for some systems and is used to prove results for the PLpc models <ref> [AGG93] </ref>. We next discuss the methodology for specifying system constraints in [GAG93]. The methodology is an extension of the data-race-free-1 specifications and has also been used for system-centric specifications of PLpc [AGG93]. The methodology for the system-centric specifications of this Chapter is based on the above work. <p> H formalizes one type of translation for some systems and is used to prove results for the PLpc models <ref> [AGG93] </ref>. We next discuss the methodology for specifying system constraints in [GAG93]. The methodology is an extension of the data-race-free-1 specifications and has also been used for system-centric specifications of PLpc [AGG93]. The methodology for the system-centric specifications of this Chapter is based on the above work. Specifically, [GAG93] proposes specifying systems as constraints on conflicting sub-operations, and shows how such specifications are more aggressive but semantically equivalent to previous specifications of hardware-centric models. <p> Some of the formalization of the control requirement was done jointly with others in the course of developing the PLpc model <ref> [AGG93] </ref>. - -- sequentially consistent executions. We develop a system condition to allows this, and show that currently practical data-race-free systems obey this condition.
Reference: [AdH93] <author> S. V. ADVE and M. D. HILL, </author> <title> A Unified Formalization of Four Shared-Memory Models, </title> <journal> IEEE Transactions on Parallel and Distributed Systems 4, </journal> <month> 6 (June </month> <year> 1993), </year> <pages> 613-624. </pages>
Reference-contexts: There are several ways to formalize data-race-free-0 programs. The following method was used in our original definition of the data-race-free models <ref> [AdH90b, AdH93] </ref>. This method uses a happens-before relation to indicate when two memory operations are ordered by intervening synchronization operations. <p> The following first shows how weak ordering meets this requirement, and then shows how data-race-free-0 allows more aggressive implementations. (This work is based on material in <ref> [AdH90b, AdH93] </ref>.) Weak ordering requires a processor to stall on every synchronization operation until its preceding operations complete. Thus, in figure 4.6 (b), weak ordering would prohibit P 1 from issuing its write of Valid, and all following operations, until the data write on A completes. <p> This could also make P 2 progress faster since the earlier execution of the write of Valid implies that P 2 's read of Valid could succeed earlier than with weak ordering. The final implementation proposal <ref> [AdH93] </ref> is even more aggressive. This proposal does not require P 2 's reads on Valid to wait for P 1 's data operations, thus allowing P 2 's synchronization to succeed earlier than the previous implementation. <p> There are three aspects to the high-level system-centric specification proposed in this section: the data, synchronization, and control requirements. These are first intuitively motivated below (the description below is simi lar to that in <ref> [AdH93] </ref>). A system-centric specification of data-race-free-0 is correct if the result of any execution of a data-race-free-0 program allowed by the specification is the same as that of a sequentially consistent execution of the program. <p> Sections 5.3.2 and 5.3.3 give one system-centric specification and implementation for the synchronization and control requirements. For the sake of simplicity, the synchronization and control specifications given here are not very aggressive. (Parts of this work were originally presented in <ref> [AdH90b, AdH92, AdH93] </ref>.) - -- The proofs that the new low-level system-centric specifications of the data and synchronization requirements specify executions that also satisfy the system-centric specification of Condition 5.10 are straightforward, but the corresponding proof for the control requirement is fairly complex. <p> Then, if remote service is not provided for data writes, each process will stall the data write of the other process indefinitely, leading to deadlock. hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh Method 4: Second Aggressive Implementation For Data-Race-Free-0 The following method is based on the material in <ref> [AdH92, AdH93] </ref>. Low-Level System-Centric Specification for Data Requirement. Condition 5.14: An execution obeys the data requirement if it obeys the following. <p> Therefore, we claim that all implementations of weak ordering obey data-race-free-0. - -- Chapter 6 Three More SCNF Models: Data-Race-Free-1, PLpc1, and PLpc2 This chapter defines three SCNF models data-race-free-1, PLpc1, and PLpc2. The data-race-free-1 model <ref> [AdH93] </ref> extends data-race-free-0 by allowing the programmer to provide more information about the synchronization operations in the program. Data-race-free-1 unifies the hardware-centric model of release consistency (RCsc) and other new implementations along with the systems unified by data-race-free-0. <p> This is an important contribution because in the past, determining when a certain optimization was safe to use involved lengthy and complex proofs that required reasoning about executions on the optimized system and proving that such an execution would appear sequentially consistent <ref> [AdH90b, AdH93, GLL90, GMG91, GiM92] </ref>; our mapping largely eliminates that complexity. The mapping between program information and optimizations leads to a characterization of SCNF memory models that determines their performance and programmability.
Reference: [ABM89] <author> Y. AFEK, G. BROWN and M. MERRITT, </author> <title> A Lazy Cache Algorithm, </title> <booktitle> Proc. 1st ACM Symp. on Parallel Algorithms and Architectures, </booktitle> <month> July </month> <year> 1989, </year> <pages> 209-222. </pages>
Reference-contexts: They show that it is sufficient for a writing processor to wait only until the root node receives its request and sends an acknowledgement. Afek et al. propose a scheme called lazy caching for a cache-coherent bus-based system <ref> [ABM89, ABM93] </ref>. They show how a processor can proceed with a memory operation even if preceding writes are only buffered at the queues of other processors, but not necessarily performed with respect to those processors. <p> The advantage of this formalism is that it captures the write buffer interaction described above; the disadvantage, however, is that it does not adequately model the non-atomicity of writes. Gibbons et al. and Afek et al. <ref> [ABM89, ABM93, GMG91, GiM92] </ref> use the formalism of an I/O automaton developed by Lynch and Tuttle [LyT88]. This formalism expresses the system in terms of a non-deterministic automaton. The formalism is powerful and precise, and does not suffer from the disadvantages of the above formalisms.
Reference: [ABM93] <author> Y. AFEK, G. BROWN and M. MERRITT, </author> <title> Lazy Caching, </title> <journal> ACM Transactions on Programming Languages and Systems 15, </journal> <month> 1 (January </month> <year> 1993), </year> <pages> 182-205. </pages>
Reference-contexts: They show that it is sufficient for a writing processor to wait only until the root node receives its request and sends an acknowledgement. Afek et al. propose a scheme called lazy caching for a cache-coherent bus-based system <ref> [ABM89, ABM93] </ref>. They show how a processor can proceed with a memory operation even if preceding writes are only buffered at the queues of other processors, but not necessarily performed with respect to those processors. <p> The advantage of this formalism is that it captures the write buffer interaction described above; the disadvantage, however, is that it does not adequately model the non-atomicity of writes. Gibbons et al. and Afek et al. <ref> [ABM89, ABM93, GMG91, GiM92] </ref> use the formalism of an I/O automaton developed by Lynch and Tuttle [LyT88]. This formalism expresses the system in terms of a non-deterministic automaton. The formalism is powerful and precise, and does not suffer from the disadvantages of the above formalisms.
Reference: [ASH88] <author> A. AGARWAL, R. SIMONI, M. HOROWITZ and J. HENNESSY, </author> <title> An Evaluation of Directory Schemes for Cache Coherence, </title> <booktitle> 15th Annual International Symposium on Computer Architecture, </booktitle> <address> Honolulu, Hawaii, </address> <month> June </month> <year> 1988, </year> <pages> 280-289. </pages>
Reference-contexts: The shared-memory abstraction can be provided either by hardware, software, or a combination of both. Pure hardware configurations include uniform [GGK83] and non-uniform access machines [ReT86]. Most configurations employ caches to reduce memory latency, and either use snooping [Bel85] or directory-based protocols <ref> [ASH88, Gus92, LLG90] </ref> to keep the caches up-to-date. Runtime software based systems typically use virtual memory hardware to trap on non-local references, and then invoke system software to handle the references [CBZ91, Li88]. <p> The proposals also assume a directory-based, writeback, invalidation, ownership, hardware cache-coherence protocol, similar in most respects to those discussed by Agarwal et al. <ref> [ASH88] </ref>. One significant feature of the protocol is that invalidations sent on a write to a line in read-only or shared state are acknowledged by the invalidated processors. <p> We define a generic memory model and a system-centric specification for the generic model in terms of this characteristic. Our common characterization for memory models is similar to the MOESI [SwS86] and Dir i [B | NB] <ref> [ASH88] </ref> characterizations of cache coherence protocols that unified several seemingly disparate protocols and exposed the design space for more protocols. Our overall approach is to first identify a system-centric specification for sequential consistency, and then use it to characterize when an optimization will not violate sequential consistency. <p> Specifically, we consider a system with a general interconnection network which may or may not have caches. In the presence of caches, we assume some form of a hardware cache coherence protocol <ref> [ASH88] </ref>. In practice, such a system needs to adhere to the following to appear sequentially consistent [ScD87]. A processor does not issue a memory operation until the preceding operations complete. Thus, a processor must wait until a preceding read returns a value and a preceding write reaches memory.
Reference: [ALK90] <author> A. AGARWAL, B. LIM, D. KRANZ and J. KUBIATOWICZ, </author> <month> APRIL: </month> <title> A Processor Architecture for Multiprocessing, </title> <booktitle> Proc. 17th Annual Symposium on Computer Architecture, </booktitle> <month> June </month> <year> 1990, </year> <pages> 104-114. </pages>
Reference-contexts: For example, in systems using software-based cache coherence [BMW85, ChV88, PBG85], hardware provides a globally addressable memory, but the compiler is responsible for ensuring that shared data in a cache is up-to-date when required by its processor. The Alewife system <ref> [ALK90] </ref> and the Cooperative Shared Memory scheme [HLR92] manage cache-able shared-data in hardware for the common cases, but invoke runtime software for the less frequent cases. Building scalable, high-performance shared-memory systems that are also easy to program, however, has remained an elusive goal. <p> We also note that researchers have suggested techniques to tolerate the long memory latencies that are incurred with sequential consistency. These techniques include software or hardware based prefetching [GGH91b, MoG91] and the use of multiple contexts <ref> [ALK90, WeG89] </ref>. While these techniques help to improve the performance of sequentially consistent systems, they can also be used to aid the performance of relaxed systems [GHG91]. Observations similar to the above also apply to the compiler.
Reference: [AHJ90] <author> M. AHAMAD, P. W. HUTTO and R. JOHN, </author> <title> Implementing and Programming Causal Distributed Shared Memory, </title> <institution> College of Computing Technical Report GIT-CC-90-49, Georgia Institute of Technology, </institution> <month> November </month> <year> 1990. </year>
Reference-contexts: Thus, slow memory is weaker than all the models discussed so far. Although the solutions of some problems have been demonstrated on slow memory, programming general problems on such a model seems difficult. Hutto et al. also introduce the model of causal memory <ref> [AHJ90, HuA90] </ref> which ensures that any write that causally precedes a read is observed by the read. Causal precedence is a transitive relation established by program order or due to a read that returns the value of a write. Bitar has proposed the weakest memory access order [Bit92]. <p> In fact, since often interactions occurring due to a race between two writes are unintended, it may be difficult for programmers to provide the necessary information to preserve sequential consistency that require considering such interactions. The model of causality, however, only considers interactions due to write/read pairs <ref> [AHJ90, HuA90] </ref>. Should causality be considered as the base model? Our framework would still be applicable; only now programmers would provide information about the program behavior on causal systems and systems would ensure causality. Correspondingly, in Chapter 7, the program/conflict graph would be replaced by the program/causal-conflict graph.
Reference: [ABJ93] <author> M. AHAMAD, R. BAZZI, R. JOHN, P. KOHLI and G. NEIGER, </author> <title> The Power of Processor Consistency, </title> <booktitle> Proc. Symposium on Parallel Algorithms and Architectures, </booktitle> <year> 1993. </year>
Reference-contexts: Ahamad et al. describe a formalization of Goodman's definition of processor consistency and the pipelined RAM using the formalism of execution histories (see Section 2.3), and discuss certain programming properties of processor consistency <ref> [ABJ93] </ref>. Some of the authors of this work have subsequently developed the work to include other models also, but do not provide formal proofs of equivalence between their formalizations and the original definitions of the models [KNA93]. 2.2.3. <p> Others have used I/O automata, but express the model as constraints on the ``execution histories of processors'' generated by the automaton, rather than as the transition functions of the automaton itself <ref> [ABJ93, AtF92, KNA93] </ref>. The histories in the cited works indicate the state of memory as viewed by a processor, and thus represent systems similar to Collier's abstraction. Consequently, they suffer from the same limitation as Collier's abstraction.
Reference: [ASU86] <author> A. AHO, R. SETHI and J. ULLMAN, </author> <booktitle> Compilers: Principles, Techniques, and Tools, </booktitle> <publisher> Addison-Wesley, </publisher> <year> 1986. </year>
Reference: [AlP87] <author> T. R. ALLEN and D. A. PADUA, </author> <title> Debugging Fortran on a Shared Memory Machine, </title> <booktitle> Proc. Intl. Conf. on Parallel Processing, </booktitle> <month> August </month> <year> 1987, </year> <pages> 721-727. </pages>
Reference-contexts: Dynamic techniques, on the other hand, use a tracing mechanism to detect whether a particular sequentially consistent execution of a program actually exhibited a data race <ref> [AlP87, ChM91, DiS90, HKM90, NeM90, NeM91] </ref>. While dynamic techniques provide precise information about a single execution, they provide little information about other executions, a serious disadvantage. <p> We call executions that obey system-centric specifications of the data-race-free models as data-race-free executions. Existing dynamic data race detection techniques for sequentially consistent systems <ref> [AlP87, ChM91, DiS90, HKM90, NeM90, NeM91] </ref> instrument the program to record information about the memory operations of its execution. This information allows the happens-before relation for the execution to be constructed, thereby allowing the detection of data races. There are two approaches for recording and using this information.
Reference: [ACC90] <author> R. ALVERSON, D. CALLAHAN, D. CUMMINGS, B. KOBLENZ, A. PORTERFIELD and B. SMITH, </author> <title> The Tera Computer System, </title> <booktitle> Proc. International Conference on Supercomputing, </booktitle> <address> Amsterdam, </address> <month> June </month> <year> 1990, </year> <pages> 1-6. </pages>
Reference-contexts: This scheme can be used only for a limited number of sender and receiver categories, and thus puts a practical bound on the amount of information that hardware exploit with a generic model. Alternative schemes are possible; e.g., Tera <ref> [ACC90] </ref> uses a field with every instruction to indicate the number of following instructions that can be overlapped with this in struction. As for data-race-free-0, more aggressive hardware implementations of the high-level valid path requirement are possible.
Reference: [ArB86] <author> J. ARCHIBALD and J. BAER, </author> <title> Cache Coherence Protocols: Evaluation Using a Multiprocessor Simulation Model, </title> <journal> ACM Trans. on Computer Systems 4, </journal> <month> 4 (November </month> <year> 1986), </year> <pages> 273-298. </pages>
Reference-contexts: Consider a cache-based system using an update-based coherence protocol <ref> [ArB86] </ref>. Assume all processors execute their memory operations in program order and one-at-a-time (waiting for acknowledgements as described above).
Reference: [AtF92] <author> H. ATTIYA and R. FRIEDMAN, </author> <title> A Correctness Condition for High-Performance Multiprocessors, </title> <booktitle> Proc. Symp. on Theory of Computing, </booktitle> <year> 1992, </year> <pages> 679-690. </pages>
Reference-contexts: Attiya et al. describe the hybrid consistency model as a formalization of models such as weak ordering <ref> [AtF92] </ref>. They use the formalism of a simplified I/O automaton to define their models [LyT88] (briefly described in Section 2.3). The model classifies memory operations as strong and weak. <p> Informally, the model requires that ``(a) strong operations appear to be executed in some sequential order at all processes, and (b) if two operations are invoked by the same process and one of them is strong, then they appear to be executed in the order they were invoked'' <ref> [AtF92] </ref>. Thus, strong operations are analogous to the synchronization operations of weak ordering, and weak operations are analogous to the data operations of weak ordering. <p> The authors have also proposed implementations of hybrid consistency, assuming each processor has a copy of the entire memory and that all writes can be broadcast atomically <ref> [AtF92, Fri93] </ref>. They also prove lower bounds on response times in hybrid consistent systems [AtF92, Fri93]. 2.2.2. Processor Consistency And Related Models The processor consistency model was originally identified by Goodman [Goo89]. <p> The authors have also proposed implementations of hybrid consistency, assuming each processor has a copy of the entire memory and that all writes can be broadcast atomically <ref> [AtF92, Fri93] </ref>. They also prove lower bounds on response times in hybrid consistent systems [AtF92, Fri93]. 2.2.2. Processor Consistency And Related Models The processor consistency model was originally identified by Goodman [Goo89]. <p> Others have used I/O automata, but express the model as constraints on the ``execution histories of processors'' generated by the automaton, rather than as the transition functions of the automaton itself <ref> [ABJ93, AtF92, KNA93] </ref>. The histories in the cited works indicate the state of memory as viewed by a processor, and thus represent systems similar to Collier's abstraction. Consequently, they suffer from the same limitation as Collier's abstraction.
Reference: [ACF93] <author> H. ATTIYA, S. CHAUDHURI, R. FRIEDMAN and J. WELCH, </author> <title> Shared Memory Consistency Conditions for Non-Sequential Execution: Definitions and Programming Strategies, </title> <booktitle> Proc. Symp. on Parallel Algorithms and Architectures, </booktitle> <year> 1993. </year> - -- 
Reference-contexts: Midkiff et al. have developed the work by Shasha and Snir to determine when a compiler can reorder program-ordered operations or schedule them in parallel without violating sequential consistency. While this work is applicable to general programs, it also depends on a global data dependence analysis. Attiya and Welch <ref> [ACF93] </ref> and Lipton and Sandberg [LiS88] have derived bounds for the response time of memory operations on sequentially consistent implementations. (Attiya and Welch also derive bounds for the model of linearizability discussed in Section 2.5.) Finally, Collier [Col84-92], Shasha and Snir [ShS88], and Landin et al. [LHH91] describe a sufficient system <p> However, as the authors point out in another paper, the formalism used above does not easily express aggressive optimizations such as non-blocking reads, out-of-order issue, and speculative execution <ref> [ACF93] </ref>. The authors have also proposed implementations of hybrid consistency, assuming each processor has a copy of the entire memory and that all writes can be broadcast atomically [AtF92, Fri93]. They also prove lower bounds on response times in hybrid consistent systems [AtF92, Fri93]. 2.2.2. <p> To allow some of the optimizations disallowed by the original formalism of hybrid consistency, Attiya et al. have extended the formalism and redefined hybrid consistency <ref> [ACF93] </ref>. However, their new conditions seem to imply that all operations of a processor must appear to occur in ``almost'' program order, and so seem much stronger than the original definition.
Reference: [BaZ91] <author> J. BAER and R. N. ZUCKER, </author> <title> On Synchronization Patterns in Parallel Programs, </title> <booktitle> Intl. Conference on Parallel Processing, </booktitle> <year> 1991, </year> <month> II60-II67. </month>
Reference-contexts: The following studies examine smaller bus-based systems and/or use probabilistic work-loads. Baer and Zucker have compared sequential consistency and weak ordering on a bus-based system <ref> [BaZ91] </ref> using simulation. Torellas and Hennessy use analytical models to compare the same models on a DASH-like system [ToH90]. Both studies find little improvement with weak ordering.
Reference: [BaK89] <author> V. BALASUNDARAM and K. KENNEDY, </author> <title> Compile-time Detection of Race Conditions in a Parallel Program, </title> <booktitle> 3rd Intl. Conf. on Supercomputing, </booktitle> <month> June </month> <year> 1989, </year> <pages> 175-185. </pages>
Reference-contexts: Static techniques perform a compile-time analysis of the program text to detect a superset of all possible data races that could potentially occur in all possible sequentially consistent executions of the program <ref> [BaK89, Tay83b] </ref>. In general, static analysis must be conservative and slow, because detecting data races is undecidable for arbitrary programs [Ber66] and is NP-complete for even very restricted classes of programs (e.g., those containing no branches) [Tay83a].
Reference: [Bel85] <author> C. G. BELL, Multis: </author> <title> A New Class of Multiprocessor Computers, </title> <booktitle> Science 228(April 1985), </booktitle> <pages> 462-466. </pages>
Reference-contexts: The shared-memory abstraction can be provided either by hardware, software, or a combination of both. Pure hardware configurations include uniform [GGK83] and non-uniform access machines [ReT86]. Most configurations employ caches to reduce memory latency, and either use snooping <ref> [Bel85] </ref> or directory-based protocols [ASH88, Gus92, LLG90] to keep the caches up-to-date. Runtime software based systems typically use virtual memory hardware to trap on non-local references, and then invoke system software to handle the references [CBZ91, Li88].
Reference: [BCZ90] <author> J. K. BENNETT, J. B. CARTER and W. ZWANAEPOEL, </author> <title> Adaptive Software Cache Management for Distributed Shared Memory Architectures, </title> <booktitle> Proc. 17th Annual Intl. Symp. on Computer Architecture, </booktitle> <address> Seattle, </address> <month> May </month> <year> 1990. </year>
Reference-contexts: This scheme alleviates the problem of false sharing; however, it does not strictly implement release consistency since a processor can execute a release before its preceding operations are performed at all other processors. Another form of delayed consistency, referred to as loose coherence for Munin <ref> [BCZ90] </ref> and also discussed in [DWB91], buffers updates of a processor until the following synchronization operation of the same processor. This retains the memory model of release consistency. Zucker proposes implementations for systems using software cache coherence [Zuc92]. <p> Second, reducing the number of messages is critical for performance in such systems [CBZ91, KCZ92]. For these reasons, our more aggressive system-centric specifications (Condition 5.10 and Methods 3 and 4) are less formidable to implement and can provide greater benefit on such systems. Munin <ref> [BCZ90, CBZ91] </ref> employs a release consistency (RCsc) based implementation (Method 2) in which messages for data writes are combined and sent at the following synchronization write (release).
Reference: [Ber66] <author> A. J. BERNSTEIN, </author> <title> Analysis of Programs for Parallel Processing, </title> <journal> IEEE Trans. on Electronic Computers EC-15, </journal> <month> 5 (October </month> <year> 1966), </year> <pages> 757-763. </pages>
Reference-contexts: In general, static analysis must be conservative and slow, because detecting data races is undecidable for arbitrary programs <ref> [Ber66] </ref> and is NP-complete for even very restricted classes of programs (e.g., those containing no branches) [Tay83a]. Dynamic techniques, on the other hand, use a tracing mechanism to detect whether a particular sequentially consistent execution of a program actually exhibited a data race [AlP87, ChM91, DiS90, HKM90, NeM90, NeM91].
Reference: [BeG81] <author> P. A. BERNSTEIN and N. GOODMAN, </author> <title> Concurrency Control in Distributed Systems, </title> <journal> Computing Surveys 13, </journal> <month> 2 (June, </month> <year> 1981), </year> <pages> 185-221. </pages>
Reference-contexts: Their study does not show significant performance gains with buffer consistency. 2.5. Correctness Criteria Stronger Than or Similar to Sequential Consistency This section discusses correctness criteria that are stronger than, or similar to, sequential consistency. A common correctness criteria for concurrent databases is serializability <ref> [BeG81, Pap86] </ref>, which requires that transactions should appear as if they are executed one at a time in some sequential order. This is very similar to sequential consistency.
Reference: [BeZ91] <author> B. N. BERSHAD and M. J. ZEKAUSKAS, Midway: </author> <title> Shared Memory Parallel Programming with Entry Consistency for Distributed Memory Multiprocessors, </title> <type> Technical Report CMU-CS-91-170, </type> <institution> Carnegie Mellon University, </institution> <month> September </month> <year> 1991. </year>
Reference-contexts: Instead, it requires performing the access only at the next acquire to the same location as a following release, and only with respect to the acquiring processor. Petersen and Li have proposed alternative implementations of lazy release consistency using virtual memory support [PeL92a, PeL92b]. Bershad et al. <ref> [BeZ91] </ref> have proposed a relaxation of release consistency for the Midway software-based shared virtual memory system [BeZ91]. This model, called entry consistency, requires programmers to associate each data operation with a lock variable that should protect the operation. <p> Petersen and Li have proposed alternative implementations of lazy release consistency using virtual memory support [PeL92a, PeL92b]. Bershad et al. <ref> [BeZ91] </ref> have proposed a relaxation of release consistency for the Midway software-based shared virtual memory system [BeZ91]. This model, called entry consistency, requires programmers to associate each data operation with a lock variable that should protect the operation. <p> Other researchers have proposed further relaxations of these models; e.g., the entry consistency model requires programmers to associate locks with the data they protect <ref> [BeZ91, BZS92] </ref>, Gibbons and Merritt allow programmers to associate a release with a specific subset of operations that it releases [GiM92], Carlton suggests associating locks with the data they protect and allocating them in the same memory module [Car91]. <p> For example, the write to Flag2 can be executed before the writes to A and B as well as before the write to Flag1. Optimizations similar to those of the above model are suggested by Bershad et al. <ref> [BeZ91, BZS92] </ref> and Gibbons and Merritt [GiM92]; we discuss how they compare to our models at the end of this sub-section. The above mechanism may be difficult for programmers to use and difficult for system designers to implement. <p> The optimizations of allowing the write of Flag2 in Figure 7.4 to be executed before the write of Flag1 and before the writes of A and B are allowed by the definition of the entry consistency model <ref> [BeZ91] </ref> as well. The write of Flag2 is allowed before the write of Flag1 because entry consistency does not impose any ordering constraints on synchronization operations of the same process (the Flag operations qualify as synchronization). <p> In this case, it requires only that P1's writes of A and B be executed only before P2 reads those locations. Previous work has shown how to pipeline P1's writes of A, B, and Flag1 while still ensuring the above <ref> [AdH90, AdH92b, BeZ91, Car91, Col84-92, - -- KCZ92, LHH91] </ref>. However, except for the work by Carlton [Car91], all the previous methods either require considerable hardware or software complexity or are applicable only to highly restricted (acyclic or ring) networks. <p> As discussed earlier, for increased parallelism, the entry consistency model requires that a data location be accessed only between an acquire and release of a lock that has been declared as protecting that data <ref> [BeZ91, BZS92] </ref>. Using our terminology, an unlock in this model only sends the operations following the preceding lock to the same location, and a lock only receives for operations preceding the following unlock to the same location.
Reference: [BZS92] <author> B. N. BERSHAD, M. J. ZEKAUSKAS and W. A. SAWDON, </author> <title> The Midway Distributed Shared Memory System, </title> <booktitle> Compcon, </booktitle> <year> 1992. </year>
Reference-contexts: In addition, entry consistency only guarantees that operations associated with the lock variable will be performed (with respect to the acquiring processor) at the time of the acquire. Bershad et al. have also proposed a combination of release consistency and entry consistency for Midway <ref> [BZS92] </ref>. The specification of entry consistency, however, seems to be incomplete; Chapter 7 discusses this issue further. Gibbons and Merritt have proposed a relaxation of release consistency (RCsc) that is a generalization of entry consistency [GiM92]. <p> Other researchers have proposed further relaxations of these models; e.g., the entry consistency model requires programmers to associate locks with the data they protect <ref> [BeZ91, BZS92] </ref>, Gibbons and Merritt allow programmers to associate a release with a specific subset of operations that it releases [GiM92], Carlton suggests associating locks with the data they protect and allocating them in the same memory module [Car91]. <p> For example, the write to Flag2 can be executed before the writes to A and B as well as before the write to Flag1. Optimizations similar to those of the above model are suggested by Bershad et al. <ref> [BeZ91, BZS92] </ref> and Gibbons and Merritt [GiM92]; we discuss how they compare to our models at the end of this sub-section. The above mechanism may be difficult for programmers to use and difficult for system designers to implement. <p> As discussed earlier, for increased parallelism, the entry consistency model requires that a data location be accessed only between an acquire and release of a lock that has been declared as protecting that data <ref> [BeZ91, BZS92] </ref>. Using our terminology, an unlock in this model only sends the operations following the preceding lock to the same location, and a lock only receives for operations preceding the following unlock to the same location. <p> To alleviate the above problem, the Midway system allows different data locations to be accessed using different consistency protocols <ref> [BZS92] </ref>. <p> The above model combines the notions of entry consistency which essentially allows only partial lock/unlock constructs, and the data-race-free models which essentially allow only the ``regular'' lock/unlock constructs. However, this combination is different from the model provided by Midway that combines entry and release consistency <ref> [BZS92] </ref> since Midway requires the unlock of the dequeue to send data operations from the preceding task processing procedure. <p> To eliminate the restriction on the use of locks by entry consistency, the Midway system allows data to be declared as either release, processor, or entry consistent <ref> [BZS92] </ref>. Our work extends such a model as well by proposing more optimizations (e.g., as in section 7.3.2.3). - -- The model for allowing reordering of operations across barriers (Section 7.3.2.2) is strongly related to the notion of fuzzy barriers [Gup89].
Reference: [BiJ87] <author> K. BIRMAN and T. JOSEPH, </author> <title> Exploiting Virtual Synchrony in Distributed Systems, </title> <booktitle> Proc. 11th ACM Symp. Operating Systems and Principles, </booktitle> <month> November </month> <year> 1987, </year> <pages> 123-138. </pages>
Reference-contexts: However, he does not say how hardware can identify these operations. Chapter 7 discusses his work in more detail. The memory model problem of shared-memory systems has analogues in distributed message passing systems. The approach taken by the ISIS distributed system is particularly relevant to our work <ref> [BiJ87, Mul89] </ref>. The analogous term for sequential consistency in this context is synchronous behavior, which imposes strict constraints on the ordering of message deliveries. Instead of imposing such constraints all the time, the ISIS system provides the programmer with a variety of primitives that guarantee different ordering constraints.
Reference: [BNR89] <author> R. BISIANI, A. NOWATZYK and M. RAVISHANKAR, </author> <title> Coherent Shared Memory on a Distributed Memory Machine, </title> <booktitle> Proc. Intl. Conf. on Parallel Processing, </booktitle> <month> August </month> <year> 1989, </year> <month> I-133-141. </month>
Reference-contexts: The CRAY XMP also provides a similar instruction called the complete memory reference or CMR instruction [CRA82]. Bisiani, Nowatzyk, and Ravishankar propose a slightly relaxed version of weak ordering for the PLUS system <ref> [BNR89, BiR90] </ref>. The system allows a processor to proceed on read-modify-write synchronization operations before the requested value is returned, if it is known that the returned value does not ``affect'' the subsequent operations, or if the subsequent operations can be undone if the returned value necessitates it.
Reference: [BiR90] <author> R. BISIANI and M. RAVISHANKAR, </author> <title> PLUS: A Distributed Shared-Memory System, </title> <booktitle> Proc. 17th Annual Intl. Symp. on Computer Architecture, </booktitle> <month> May </month> <year> 1990, </year> <pages> 115-124. </pages>
Reference-contexts: The CRAY XMP also provides a similar instruction called the complete memory reference or CMR instruction [CRA82]. Bisiani, Nowatzyk, and Ravishankar propose a slightly relaxed version of weak ordering for the PLUS system <ref> [BNR89, BiR90] </ref>. The system allows a processor to proceed on read-modify-write synchronization operations before the requested value is returned, if it is known that the returned value does not ``affect'' the subsequent operations, or if the subsequent operations can be undone if the returned value necessitates it.
Reference: [Bit92] <author> P. BITAR, </author> <title> The Weakest Memory-Access Order, </title> <journal> Journal of Parallel and Distributed Computing 15, </journal> <month> 4 (August </month> <year> 1992), </year> <pages> 305-331. </pages>
Reference-contexts: Causal precedence is a transitive relation established by program order or due to a read that returns the value of a write. Bitar has proposed the weakest memory access order <ref> [Bit92] </ref>. He proposes a weakest order for the processor to issue memory operations and for the memory modules to execute memory operations. However, the order proposed by Bitar cannot be the ``weakest'' order because Chapter 7 shows weaker sufficient constraints. <p> Furthermore, Collier's abstraction for shared-memory systems has been an invaluable aid for reasoning about non-atomic systems, specifying such systems, and proving the correctness of our system-centric specifications. 7.6.3. Relation with Work by Bitar Bitar has proposed the ``weakest memory access order'' <ref> [Bit92] </ref>. He proposes a weakest order for the processor to issue memory operations and for the memory modules to execute memory operations. For the processor, he associates a tree of locks and unlocks with every memory operation.
Reference: [BMW85] <author> W. C. BRANTLEY, K. P. MCAULIFFE and J. WEISS, </author> <title> RP3 Process-Memory Element, </title> <booktitle> International Conference on Parallel Processing, </booktitle> <month> August </month> <year> 1985, </year> <pages> 772-781. </pages>
Reference-contexts: Other software-based systems depend on compilers that detect non-local shared memory accesses in high-level code, and convert them into appropriate messages for the underlying message passing machine [HKT92]. Several systems employ a combination of hardware and software techniques. For example, in systems using software-based cache coherence <ref> [BMW85, ChV88, PBG85] </ref>, hardware provides a globally addressable memory, but the compiler is responsible for ensuring that shared data in a cache is up-to-date when required by its processor. <p> Rudolph and Segall have developed two cache coherence protocols for bus-based systems. They formally prove the protocols obey their correctness criteria mentioned in Section 2.1.2; if we assume they implicitly considered processors that execute operations in program order, the protocols guarantee sequential consistency [RuS84]. The RP3 system <ref> [BMW85, PBG85] </ref> is a cache-based system, where processor memory communication is via an Omega network, but the management of cache coherence for shared writable variables is entrusted to the software. <p> The semantics of a synchronization operation of weak ordering can be simulated by the Alpha by a memory operation that is immediately preceded and immediately followed by MB instructions. The RP3 system (described in Section 2.1.4) <ref> [BMW85, PBG85] </ref> provides a fence instruction similar to the MB instruction of Alpha, and can also be viewed as an implementation of weak ordering. The CRAY XMP also provides a similar instruction called the complete memory reference or CMR instruction [CRA82].
Reference: [CRA82] <author> CRAY, </author> <title> Cray X-MP Series Mainframe Reference Manual, Publication Number HR-0032, </title> <institution> CRAY Research Inc., </institution> <month> November </month> <year> 1982. </year>
Reference-contexts: The RP3 system (described in Section 2.1.4) [BMW85, PBG85] provides a fence instruction similar to the MB instruction of Alpha, and can also be viewed as an implementation of weak ordering. The CRAY XMP also provides a similar instruction called the complete memory reference or CMR instruction <ref> [CRA82] </ref>. Bisiani, Nowatzyk, and Ravishankar propose a slightly relaxed version of weak ordering for the PLUS system [BNR89, BiR90].
Reference: [Car91] <author> M. CARLTON, </author> <title> Implementation Issues for Multiprocessor Consistency Models, Presentation in the Second Workshop on Scalable Shared-Memory Multiprocessors, </title> <institution> Toronto, </institution> <month> May </month> <year> 1991. </year>
Reference-contexts: consistency model requires programmers to associate locks with the data they protect [BeZ91, BZS92], Gibbons and Merritt allow programmers to associate a release with a specific subset of operations that it releases [GiM92], Carlton suggests associating locks with the data they protect and allocating them in the same memory module <ref> [Car91] </ref>. A natural question that arises next is: are there other models that can potentially give higher performance and/or better programmability than the current models? This chapter answers the above question. <p> In this case, it requires only that P1's writes of A and B be executed only before P2 reads those locations. Previous work has shown how to pipeline P1's writes of A, B, and Flag1 while still ensuring the above <ref> [AdH90, AdH92b, BeZ91, Car91, Col84-92, - -- KCZ92, LHH91] </ref>. However, except for the work by Carlton [Car91], all the previous methods either require considerable hardware or software complexity or are applicable only to highly restricted (acyclic or ring) networks. <p> Previous work has shown how to pipeline P1's writes of A, B, and Flag1 while still ensuring the above [AdH90, AdH92b, BeZ91, Car91, Col84-92, - -- KCZ92, LHH91]. However, except for the work by Carlton <ref> [Car91] </ref>, all the previous methods either require considerable hardware or software complexity or are applicable only to highly restricted (acyclic or ring) networks. <p> Carlton proposed (without proof) that if all data is protected by locks, a lock and the data it protects are put in the same memory module, and the network does not reorder accesses along the same path, then all operations can be pipelined <ref> [Car91] </ref>. We use Carlton's approach, but argue formally to show how it can be applied to only a few accesses, eliminating the need for only lock-based synchronization and the need to allocate a lock variable and all data associated with it in the same memory module. <p> Eliminating Acknowledgements Finally, we consider eliminating acknowledgements for some writes with general interconnection networks. 27 Previous work has considered this optimization [Car91,Col84-92,LHH91], but either assumes restricted networks [Col84-92, LHH91], or does not ensure sequential consistency (as in some schemes in [Col84-92, LHH91]), or requires restricted, lock-based synchronization <ref> [Car91] </ref>. Only the work by Carlton [Car91] indicates (without proof) how this optimization might be used for an SCNF model, but requires restricted, lock-based synchronization. Acknowledgements are needed only to indicate the completion of a write operation. <p> we consider eliminating acknowledgements for some writes with general interconnection networks. 27 Previous work has considered this optimization [Car91,Col84-92,LHH91], but either assumes restricted networks [Col84-92, LHH91], or does not ensure sequential consistency (as in some schemes in [Col84-92, LHH91]), or requires restricted, lock-based synchronization <ref> [Car91] </ref>. Only the work by Carlton [Car91] indicates (without proof) how this optimization might be used for an SCNF model, but requires restricted, lock-based synchronization. Acknowledgements are needed only to indicate the completion of a write operation. <p> Our work is a straightforward generalization and formalization of the fuzzy barrier work that allows programmers to give the necessary information for the optimization. For the optimization of pipelining memory operations in Section 7.3.1.2, we have used Carlton's approach <ref> [Car91] </ref>, but remove the restriction of only lock-based synchronization and the restriction of requiring a lock and data associated with it to be in the same memory module for every lock. We also provide the proof of correctness for this optimization. <p> Compared to data-race-free-1 and PLpc, we have shown models that allow more writes to be non-atomic (e.g., the signal constructs of Section 7.3.2.1). The optimization of not requiring acknowledgements on certain accesses has been considered by Collier [Col84-92], Landin et al. [LHH91], and Carlton <ref> [Car91] </ref>. Collier's scheme has the limitation described above for non-atomic writes. Landin et al. also required a restricted (race-free) network. Carlton assumes only lock-based synchronization and restricts locks and the data they protect to be in the same memory module.
Reference: [CBZ91] <author> J. B. CARTER, J. K. BENNETT and W. ZWAENEPOEL, </author> <title> Implementation and Performance of Munin, </title> <booktitle> Proc. Symposium on Operating System Principles, </booktitle> <month> October </month> <year> 1991, </year> <pages> 152-164. </pages>
Reference-contexts: Most configurations employ caches to reduce memory latency, and either use snooping [Bel85] or directory-based protocols [ASH88, Gus92, LLG90] to keep the caches up-to-date. Runtime software based systems typically use virtual memory hardware to trap on non-local references, and then invoke system software to handle the references <ref> [CBZ91, Li88] </ref>. Other software-based systems depend on compilers that detect non-local shared memory accesses in high-level code, and convert them into appropriate messages for the underlying message passing machine [HKT92]. Several systems employ a combination of hardware and software techniques. <p> The system-centric specifications, however, are well-suited for other types of systems and can benefit them in other ways as well. Software-based shared virtual memory systems <ref> [CBZ91, Li89] </ref>, for example, have different tradeoffs compared to hardware cache-coherent systems. First, more complex mechanisms can be used to ensure memory consistency in software-based systems since the implementation is in software. Second, reducing the number of messages is critical for performance in such systems [CBZ91, KCZ92]. <p> First, more complex mechanisms can be used to ensure memory consistency in software-based systems since the implementation is in software. Second, reducing the number of messages is critical for performance in such systems <ref> [CBZ91, KCZ92] </ref>. For these reasons, our more aggressive system-centric specifications (Condition 5.10 and Methods 3 and 4) are less formidable to implement and can provide greater benefit on such systems. <p> Second, reducing the number of messages is critical for performance in such systems [CBZ91, KCZ92]. For these reasons, our more aggressive system-centric specifications (Condition 5.10 and Methods 3 and 4) are less formidable to implement and can provide greater benefit on such systems. Munin <ref> [BCZ90, CBZ91] </ref> employs a release consistency (RCsc) based implementation (Method 2) in which messages for data writes are combined and sent at the following synchronization write (release).
Reference: [CeF78] <author> L. M. CENSIER and P. FEAUTRIER, </author> <title> A New Solution to Coherence Problems in Multicache Systems, </title> <journal> IEEE Trans. on Computers C-27, </journal> <month> 12 (December </month> <year> 1978), </year> <pages> 1112-1118. </pages>
Reference-contexts: Sequential Consistency vs. Cache Coherence Early bus-based systems with caches used cache coherence (or cache consistency) as the notion of correctness <ref> [CeF78, RuS84] </ref>. The definition, given by Censier and Feautrier, requires that a read should return the value deposited by the latest write to the same address [CeF78]. However, in systems with write buffers and general interconnection networks, the latest write to a location is not well-defined [DSB86]. <p> Sequential Consistency vs. Cache Coherence Early bus-based systems with caches used cache coherence (or cache consistency) as the notion of correctness [CeF78, RuS84]. The definition, given by Censier and Feautrier, requires that a read should return the value deposited by the latest write to the same address <ref> [CeF78] </ref>. However, in systems with write buffers and general interconnection networks, the latest write to a location is not well-defined [DSB86].
Reference: [ChV88] <author> J. CHEONG and A. V. VEIDENBAUM, </author> <title> A Cache Coherence Scheme With Fast Selective Invalidation, </title> <booktitle> Proceedings of the 15th Annual International Symposium on Computer Architecture 16, </booktitle> <month> 2 (June </month> <year> 1988), </year> <pages> 299-307. </pages>
Reference-contexts: Other software-based systems depend on compilers that detect non-local shared memory accesses in high-level code, and convert them into appropriate messages for the underlying message passing machine [HKT92]. Several systems employ a combination of hardware and software techniques. For example, in systems using software-based cache coherence <ref> [BMW85, ChV88, PBG85] </ref>, hardware provides a globally addressable memory, but the compiler is responsible for ensuring that shared data in a cache is up-to-date when required by its processor. <p> The specification of Condition 5.10 is also well-suited for software-based cache-coherent systems <ref> [ChV88, Che90, CKM88] </ref>. Zucker explains how the hardware-centric models proposed so far cannot be used on software-based cache-coherent systems but our specifications lead to efficient implementations [Zuc92]. 5.3.2. The Synchronization Requirement The synchronization requirement is met by ensuring that synchronization operations interact like on a sequentially consistent system.
Reference: [Che90] <author> H. CHEONG, </author> <title> Compiler-Directed Cache Coherence Strategies for Large-Scale Shared-Memory Multiprocessor Systems, </title> <type> Ph.D. Thesis, </type> <institution> Dept. of Electrical Engineering, University of Illinois, Urbana-Champaign, </institution> <year> 1990. </year>
Reference-contexts: The specification of Condition 5.10 is also well-suited for software-based cache-coherent systems <ref> [ChV88, Che90, CKM88] </ref>. Zucker explains how the hardware-centric models proposed so far cannot be used on software-based cache-coherent systems but our specifications lead to efficient implementations [Zuc92]. 5.3.2. The Synchronization Requirement The synchronization requirement is met by ensuring that synchronization operations interact like on a sequentially consistent system.
Reference: [ChM91] <author> J. CHOI and S. L. MIN, </author> <title> Race Frontier: Reproducing Data Races in Parallel Program Debugging, </title> <booktitle> Proc. 3rd ACM Symp. on Principles and Practice of Parallel Programming, </booktitle> <month> April </month> <year> 1991. </year>
Reference-contexts: Dynamic techniques, on the other hand, use a tracing mechanism to detect whether a particular sequentially consistent execution of a program actually exhibited a data race <ref> [AlP87, ChM91, DiS90, HKM90, NeM90, NeM91] </ref>. While dynamic techniques provide precise information about a single execution, they provide little information about other executions, a serious disadvantage. <p> We call executions that obey system-centric specifications of the data-race-free models as data-race-free executions. Existing dynamic data race detection techniques for sequentially consistent systems <ref> [AlP87, ChM91, DiS90, HKM90, NeM90, NeM91] </ref> instrument the program to record information about the memory operations of its execution. This information allows the happens-before relation for the execution to be constructed, thereby allowing the detection of data races. There are two approaches for recording and using this information.
Reference: [Col84-92] <author> W. W. COLLIER, </author> <title> Reasoning about Parallel Architectures, </title> <publisher> Prentice-Hall, </publisher> <address> Englewood Cliffs, New Jersey, </address> <year> 1992. </year> <note> Parts of this work originally appeared in the following technical reports by W.W. Collier: ``Architectures for Systems of Parallel Processes'' (27 January 1984, Technical Report 00.3253), ``Write Atomicity in Distributed Systems'' (19 October 1984, Technical Report 00.3304), - -- ``Reasoning about Parallel Architectures'' (1985), </note> <institution> IBM Corp., Poughkeepsie, N.Y. </institution>
Reference-contexts: This notion of coherence does not ensure sequential consistency since (informally) sequential consistency (a) requires all processors to see all writes to all locations in the same order <ref> [Col84-92, DuS90] </ref>, and (b) requires all operations of a single process to be seen in program order. Figure 2.1 shows two program fragments that illustrate the difference between cache coherence and sequential consistency due to (a) above. <p> Collier has proved that a system where all writes are performed with respect to all processors in the same order is (in most cases) equivalent to a system where all writes are executed atomically <ref> [Col84-92] </ref>. Thus, a system where writes obey the above condition and where all operations of a single processor are performed with respect to other processors in program order is sequentially consistent [Col84-92, DuS90]. <p> Thus, a system where writes obey the above condition and where all operations of a single processor are performed with respect to other processors in program order is sequentially consistent <ref> [Col84-92, DuS90] </ref>. Several researchers have proposed implementations that allow multiple outstanding writes of a processor, and essentially exploit Collier's result as follows. Collier himself proposes an implementation using a ring network where one processor in the network is used to serialize all writes [Col84-92]. <p> Several researchers have proposed implementations that allow multiple outstanding writes of a processor, and essentially exploit Collier's result as follows. Collier himself proposes an implementation using a ring network where one processor in the network is used to serialize all writes <ref> [Col84-92] </ref>. In this system, a writing processor sends a write request to the serializing processor which then sends an update message along the ring. <p> All of the above schemes that allow a processor to overlap or reorder its memory accesses without software support, however, either require complex or restricted hardware (e.g., hardware prefetching and rollback for [GGH91b] and restricted networks for <ref> [Col84-92, LHH91] </ref>) or the gains are expected to be small (e.g., [AdH90a, LHH91]). Further, the optimizations of these schemes can be exploited by hardware (or the runtime system software), but cannot be exploited by compilers. <p> Attiya and Welch [ACF93] and Lipton and Sandberg [LiS88] have derived bounds for the response time of memory operations on sequentially consistent implementations. (Attiya and Welch also derive bounds for the model of linearizability discussed in Section 2.5.) Finally, Collier <ref> [Col84-92] </ref>, Shasha and Snir [ShS88], and Landin et al. [LHH91] describe a sufficient system condition for sequential consistency in graph theoretic terms that we will use for the framework of Chapter 7. 2.1.5. <p> Seven instructions are provided to permit interlocked access to a control variable.'' We have adopted the VAX approach for our work, as described in Chapter 3. 2.2.4. Other Relaxed Models Collier has developed a general framework to express a variety of memory consistency models <ref> [Col84-92] </ref>. <p> We adopt this abstraction for specifying system requirements for our work (Chapter 5). Based on the above abstraction, Collier defines architectures as sets of rules, where each rule is a restriction on the order in which certain memory sub-operations will appear to execute <ref> [Col84-92] </ref>. Using graph theoretic results, he has proved equivalences and inequivalences of several of these sets of rules or architectures. Examples of the rules include program order which states that all sub-operations of an operation will appear to execute before any sub-operation of the following operation by program order. <p> Section 5.1.3 gives an assumption made by all our implementations of SCNF models that rely on distinguishing memory operations, and extends some previous definitions to the more general notion of an execution. 5.1.1. A Formalism for Shared-Memory Systems and Executions Based on Collier's work <ref> [Col84-92] </ref>, we extend the definition of a system as follows. <p> Our overall approach is to first identify a system-centric specification for sequential consistency, and then use it to characterize when an optimization will not violate sequential consistency. Section 7.1 gives such a specification based on several previous works <ref> [AdH92a, Col84-92, GAG92, LHH91, ShS88] </ref>. Section 7.2 uses the above specification to deduce the mapping between optimizations and information. Section 7.3 uses the mapping of Section 7.2 to examine several optimizations and common program constructs. <p> A Condition for Sequential Consistency Section 7.1.1 first gives a simple system-centric specification for sequential consistency. Section 7.1.2 makes four observations that modify the specification to reflect certain optimizations. Section 7.1.1 and Observation 1 of Section 7.1.2 follow directly from previous work by Shasha and Snir [ShS88], Collier <ref> [Col84-92] </ref>, and Landin et al. [LHH91]. Observations 2 and 3 of Section 7.1.2 are extensions of similar concepts developed for the PLpc model [GAG92]. 7.1.1. <p> Section 7.5 will alleviate these restrictions. A simple system-centric specification for sequential consistency (with the finite speculation and write termination assumptions) is that the program/conflict graph of the execution should be acyclic (as also observed by others in various forms <ref> [Col84-92, LHH91, ShS88] </ref>). The following first illustrates the above condition with an example, and then gives a formal proof of correctness. Figure 7.1 (a) shows code in which processor P0 writes location x and then location y. <p> In this case, it requires only that P1's writes of A and B be executed only before P2 reads those locations. Previous work has shown how to pipeline P1's writes of A, B, and Flag1 while still ensuring the above <ref> [AdH90, AdH92b, BeZ91, Car91, Col84-92, - -- KCZ92, LHH91] </ref>. However, except for the work by Carlton [Car91], all the previous methods either require considerable hardware or software complexity or are applicable only to highly restricted (acyclic or ring) networks. <p> Eliminating Acknowledgements Finally, we consider eliminating acknowledgements for some writes with general interconnection networks. 27 Previous work has considered this optimization [Car91,Col84-92,LHH91], but either assumes restricted networks <ref> [Col84-92, LHH91] </ref>, or does not ensure sequential consistency (as in some schemes in [Col84-92, LHH91]), or requires restricted, lock-based synchronization [Car91]. Only the work by Carlton [Car91] indicates (without proof) how this optimization might be used for an SCNF model, but requires restricted, lock-based synchronization. <p> Eliminating Acknowledgements Finally, we consider eliminating acknowledgements for some writes with general interconnection networks. 27 Previous work has considered this optimization [Car91,Col84-92,LHH91], but either assumes restricted networks <ref> [Col84-92, LHH91] </ref>, or does not ensure sequential consistency (as in some schemes in [Col84-92, LHH91]), or requires restricted, lock-based synchronization [Car91]. Only the work by Carlton [Car91] indicates (without proof) how this optimization might be used for an SCNF model, but requires restricted, lock-based synchronization. Acknowledgements are needed only to indicate the completion of a write operation. <p> Relation with Work by Collier Collier has developed a general framework to define memory models using his abstraction of a shared-memory system (described in Chapters 2 and 5) <ref> [Col84-92] </ref>. We have adopted Collier's abstraction for system-centric specifications of our models. Collier defines architectures (or memory models) as sets of rules, where each rule is a restriction on the order of execution of certain sub-operations (see Chapter 2 for examples). <p> We also provide the proof of correctness for this optimization. To the best of our knowledge, the optimization of non-atomic writes has been considered only for systems already allowed by data-race-free/PLpc models and by Collier <ref> [Col84-92] </ref>. As discussed above, the only case for which Collier seems to allow non-atomic writes and guarantee sequential consistency is when writes to all locations are seen in the same order by all processors. The implementation described requires a restricted (ring based) network. <p> Our models do not impose the above restrictions. Compared to data-race-free-1 and PLpc, we have shown models that allow more writes to be non-atomic (e.g., the signal constructs of Section 7.3.2.1). The optimization of not requiring acknowledgements on certain accesses has been considered by Collier <ref> [Col84-92] </ref>, Landin et al. [LHH91], and Carlton [Car91]. Collier's scheme has the limitation described above for non-atomic writes. Landin et al. also required a restricted (race-free) network. Carlton assumes only lock-based synchronization and restricts locks and the data they protect to be in the same memory module.
Reference: [CKM88] <author> R. CYTRON, S. KARLOVSKY and K. P. MCAULIFFE, </author> <title> Automatic Management of Programmable Caches, </title> <booktitle> Proc. 1988 Intl. Conf. on Parallel Processing, </booktitle> <address> University Park PA, </address> <month> August </month> <year> 1988, </year> <month> II-229-238. </month>
Reference-contexts: The specification of Condition 5.10 is also well-suited for software-based cache-coherent systems <ref> [ChV88, Che90, CKM88] </ref>. Zucker explains how the hardware-centric models proposed so far cannot be used on software-based cache-coherent systems but our specifications lead to efficient implementations [Zuc92]. 5.3.2. The Synchronization Requirement The synchronization requirement is met by ensuring that synchronization operations interact like on a sequentially consistent system.
Reference: [DeM88] <author> R. DELEONE and O. L. MANGASARIAN, </author> <title> Asynchronous Parallel Successive Overrelaxation for the Symmetric Linear Complementarity Problem, </title> <booktitle> Mathematical Programming 42, </booktitle> <year> 1988, </year> <pages> 347-361. </pages>
Reference-contexts: First, the SCNF specification does not provide a good model for programmers of asynchronous algorithms that do not rely on sequential consistency for correctness <ref> [DeM88] </ref>. This is because the only guarantee given by SCNF models is that of sequential consistency and only when the program obeys the specified characterization. Enforcing sequential consistency for asynchronous algorithms could result in lower performance than is possible with other hardware-centric models. <p> This is difficult and the performance gains, especially compared - -- to the more aggressive implementations of data-race-free-0, are not clear. A disadvantage of data-race-free-0 similar to the above arises for programmers using asynchronous algorithms <ref> [DeM88] </ref> that do not rely on sequential consistency for correctness. Such programs would probably get better performance in the presence of data races. <p> Similarly, operations that form a race (and hence need to be distinguished as synchronization) but occur due to asynchronous accesses to data <ref> [DeM88] </ref> are unpaired synchronization in our terminology. Of the synchronization operations, the system needs to enforce greater restrictions on paired operations since they are used to order data operations. For example, unlike the paired Unset in figure 6.1, the unpaired Set need not await the completion of preceding data operations.
Reference: [Dec81] <author> DEC., </author> <title> VAX Architecture Handbook, </title> <year> 1981. </year>
Reference-contexts: Analogous observations hold for the RP3 system. The VAX memory model differs from the above models by avoiding explicit restrictions on the order of execution of specific memory operations. Instead, the VAX architecture handbook states the following <ref> [Dec81] </ref>: ``Accesses to explicitly shared data that may be written must be synchronized. Before accessing shared writable data, the programmer must acquire control of the data structure. <p> Such a unification is desirable because for many programmers (those that want sequential consistency), these systems are similar. A few other models have used approaches similar to that of SCNF. The closest is the VAX model <ref> [Dec81] </ref>. The VAX model does not explicitly impose any system constraints. Instead, the following restriction is mentioned for programmers [Dec81]. ``Accesses to explicitly shared data that may be written must be synchronized. <p> A few other models have used approaches similar to that of SCNF. The closest is the VAX model <ref> [Dec81] </ref>. The VAX model does not explicitly impose any system constraints. Instead, the following restriction is mentioned for programmers [Dec81]. ``Accesses to explicitly shared data that may be written must be synchronized.
Reference: [DiS90] <author> A. DINNING and E. SCHONBERG, </author> <title> An Empirical Comparison of Monitoring Algorithms for Access Anomaly Detection, </title> <booktitle> Proc. ACM SIGPLAN Notices Symp. on Principles and Practice of Parallel Programming, </booktitle> <month> March </month> <year> 1990, </year> <pages> 1-10. </pages>
Reference-contexts: Dynamic techniques, on the other hand, use a tracing mechanism to detect whether a particular sequentially consistent execution of a program actually exhibited a data race <ref> [AlP87, ChM91, DiS90, HKM90, NeM90, NeM91] </ref>. While dynamic techniques provide precise information about a single execution, they provide little information about other executions, a serious disadvantage. <p> We call executions that obey system-centric specifications of the data-race-free models as data-race-free executions. Existing dynamic data race detection techniques for sequentially consistent systems <ref> [AlP87, ChM91, DiS90, HKM90, NeM90, NeM91] </ref> instrument the program to record information about the memory operations of its execution. This information allows the happens-before relation for the execution to be constructed, thereby allowing the detection of data races. There are two approaches for recording and using this information.
Reference: [DSB86] <author> M. DUBOIS, C. SCHEURICH and F. A. BRIGGS, </author> <title> Memory Access Buffering in Multiprocessors, </title> <booktitle> Proc. 13th Annual Intl. Symp. on Computer Architecture 14, </booktitle> <month> 2 (June </month> <year> 1986), </year> <pages> 434-442. </pages>
Reference-contexts: r2 = 26, and for part (b), sequential consistency ensures that both r1 and r2 cannot be 0. hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh The view of the system provided by sequential consistency retains the simplicity of the uniprocessor model for programmers; however, implementing systems to provide this view often involves serious compromises in performance <ref> [DSB86, Lam79, MPC89] </ref>. Consider the code in part (b) of figure 1.2. Consider a system that uses write buffers to allow a read to bypass a write that precedes it in the program. <p> Similarly, in a system where processors can reorder their instructions, P1 could issue its read of Y before its write of X, again allowing both P1 and P2 to read the old values of Y and X respectively <ref> [DSB86, Lam79] </ref>. Note that neither processor has data dependencies among its instructions (because X and Y are different locations); therefore, simple interlock logic will not preclude either processor from issuing its second instruction before the first. <p> We develop four SCNF models that unify the IBM 370 model [IBM83], the SPARC V8 models of total store ordering and partial store ordering [SUN91], the Alpha model [Sit92], weak ordering <ref> [DSB86] </ref>, processor consistency [GLL90], two flavors of release consistency (RCsc and RCpc) [GLL90], and lazy release consistency [KCZ92]. The four SCNF models data-race-free-0, data-race-free-1, PLpc1, and PLpc2 exploit strictly increasing amounts of information from the programmer regarding the behavior of memory operations in the program. <p> The definition, given by Censier and Feautrier, requires that a read should return the value deposited by the latest write to the same address [CeF78]. However, in systems with write buffers and general interconnection networks, the latest write to a location is not well-defined <ref> [DSB86] </ref>. Sequential consistency makes the notion of the latest write more precise; further, it also explicitly requires that operations of a single process appear to execute in the program order of the process. <p> As discussed in Chapter 1, systems that allow processors to reorder memory operations, or use write buffers to allow reads to bypass preceding writes, or overlap memory requests on the interconnection network can result in either or both of the non-sequentially consistent executions represented in figure 2.2 <ref> [DSB86, Lam79] </ref>. Analogously, compilers that reorder memory operations or allocate shared-memory locations in registers can violate sequential consistency [MPC89]. These violations are possible irrespective of whether systems have caches. In the presence of caches and a general interconnection network, sequential consistency imposes further constraints on hardware as follows. <p> Implementations that Obey Sequential Consistency In their seminal work, Dubois, Scheurich, and Briggs have analyzed the problem of imposing sequential consistency in many different types of hardware systems <ref> [DSB86, DSB88, DuS90, ScD87, Sch89] </ref>. To reason with non-atomic memory operations in a cache-based system, they define the notion of an operation being performed with respect to a processor, performed, and globally performed. <p> Weak Ordering And Related Models The weak ordering model was proposed by Dubois, Scheurich and Briggs <ref> [DSB86, DuS90, Sch89] </ref>. It is based on the intuition that the ordering of memory operations is important only with respect to synchronization (as opposed to data) operations. It requires programmers to distinguish between data and synchronization operations, and requires the system to recognize this distinction. <p> No practical implementations of concurrent consistency that are not sequentially consistent are given. The above sufficient conditions were originally stated as the definition of strong ordering and claimed to be sufficient for sequential consistency <ref> [DSB86] </ref>. However, there are programs for which the conditions violate sequential consistency [AdH90a] and strong ordering was later redefined as the hardware quality of a multiprocessor which guarantees sequential consistency without further software aid [DuS90, Sch89]. Hutto and Ahamad have hierarchically characterized various weak models [HuA90]. <p> Formalisms For Specifying Memory Models This section briefly summarizes the various formalisms used to specify the memory models discussed in the previous section. Dubois et al. developed the notions of ``perform with respect to,'' ``perform,'' and ``globally perform'' to capture the non-atomicity of memory as described in Section 2.1.4 <ref> [DSB86] </ref>. Several memory models have been described using this terminology including weak ordering, processor consistency, and release consistency. One disadvantage of this terminology is that it prescribes constraints in real time and so is more restrictive than the other models described below. <p> The system can then treat the two types of operations differently, typically executing the data operations faster for higher performance. The model of weak - -- hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh A = 100; Valid = 1; while (Valid != 1) -;- ... = B; P2P1 hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh ordering by Dubois, Scheurich and Briggs <ref> [DSB86, DSB88, Sch89] </ref> is such an alternative model, where memory operations of a processor are not guaranteed to execute in program order unless they are separated by synchronization, and writes may appear to execute at different times to different processors. <p> However, one can identify a single instant of time at which the update takes effect such that other sub-operations take effect either before or after this time.'' The notion of sub-operations is similar to that of memory operations performing with respect to a processor defined by Dubois et al. <ref> [DSB86] </ref>. A write sub-operation, W (i), corresponds to the write W performing with respect to processor P i . A read sub-operation, R (i), corresponds to the read R performing with respect to all processors. <p> Similarly, hardware can actually execute more operations than specified by an execution as long as the effects of these extra operations are not seen. This is in contrast to many earlier specifications <ref> [AdH90b, DSB86, GLL90, ScD87] </ref> which impose constraints on the real time ordering of events.
Reference: [DSB88] <author> M. DUBOIS, C. SCHEURICH and F. A. BRIGGS, </author> <title> Synchronization, Coherence, and Event Ordering in Multiprocessors, </title> <booktitle> IEEE Computer 21, </booktitle> <month> 2 (February </month> <year> 1988), </year> <pages> 9-21. </pages>
Reference-contexts: Implementations that Obey Sequential Consistency In their seminal work, Dubois, Scheurich, and Briggs have analyzed the problem of imposing sequential consistency in many different types of hardware systems <ref> [DSB86, DSB88, DuS90, ScD87, Sch89] </ref>. To reason with non-atomic memory operations in a cache-based system, they define the notion of an operation being performed with respect to a processor, performed, and globally performed. <p> The system can then treat the two types of operations differently, typically executing the data operations faster for higher performance. The model of weak - -- hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh A = 100; Valid = 1; while (Valid != 1) -;- ... = B; P2P1 hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh ordering by Dubois, Scheurich and Briggs <ref> [DSB86, DSB88, Sch89] </ref> is such an alternative model, where memory operations of a processor are not guaranteed to execute in program order unless they are separated by synchronization, and writes may appear to execute at different times to different processors.
Reference: [DuS90] <author> M. DUBOIS and C. SCHEURICH, </author> <title> Memory Access Dependencies in Shared-Memory Multiprocessors, </title> <journal> IEEE Trans. on Software Engineering SE-16, </journal> <month> 6 (June </month> <year> 1990), </year> <pages> 660-673. </pages>
Reference-contexts: This notion of coherence does not ensure sequential consistency since (informally) sequential consistency (a) requires all processors to see all writes to all locations in the same order <ref> [Col84-92, DuS90] </ref>, and (b) requires all operations of a single process to be seen in program order. Figure 2.1 shows two program fragments that illustrate the difference between cache coherence and sequential consistency due to (a) above. <p> Implementations that Obey Sequential Consistency In their seminal work, Dubois, Scheurich, and Briggs have analyzed the problem of imposing sequential consistency in many different types of hardware systems <ref> [DSB86, DSB88, DuS90, ScD87, Sch89] </ref>. To reason with non-atomic memory operations in a cache-based system, they define the notion of an operation being performed with respect to a processor, performed, and globally performed. <p> Thus, a system where writes obey the above condition and where all operations of a single processor are performed with respect to other processors in program order is sequentially consistent <ref> [Col84-92, DuS90] </ref>. Several researchers have proposed implementations that allow multiple outstanding writes of a processor, and essentially exploit Collier's result as follows. Collier himself proposes an implementation using a ring network where one processor in the network is used to serialize all writes [Col84-92]. <p> Weak Ordering And Related Models The weak ordering model was proposed by Dubois, Scheurich and Briggs <ref> [DSB86, DuS90, Sch89] </ref>. It is based on the intuition that the ordering of memory operations is important only with respect to synchronization (as opposed to data) operations. It requires programmers to distinguish between data and synchronization operations, and requires the system to recognize this distinction. <p> It is based on the intuition that the ordering of memory operations is important only with respect to synchronization (as opposed to data) operations. It requires programmers to distinguish between data and synchronization operations, and requires the system to recognize this distinction. The model is defined as follows <ref> [DuS90] </ref>. (The definition implicitly assumes that uniprocessor dependences are obeyed.) Definition 2.2: In a multiprocessor system, memory accesses are weakly ordered if (1) accesses to global synchronizing variables are strongly ordered, (2) no access to a synchronizing variable is issued by a processor before all its [preceding] global data accesses have <p> Strong ordering referred to in (1) is defined as the hardware quality that guarantees sequential consistency without software aid <ref> [DuS90] </ref> (see Section 2.2.4). Weak ordering potentially provides higher performance than sequential consistency by allowing memory operations between consecutive synchronization operations of a processor to be executed in parallel and non-atomically. <p> This is in contrast to the processor consistency based models of the next sub-section, where most of the allowed optimizations can be exploited only by the hardware (or runtime system software). The authors of weak ordering also give informal constraints for programmers. For example, in <ref> [DuS90] </ref>, they state ``if a shared variable is modified by one process and is accessed by other processes, then the access to the variable must be protected, and it is the responsibility of the programmer to ensure mutual exclusion of each access to the variable by using high-level language constructs such <p> However, there are programs for which the conditions violate sequential consistency [AdH90a] and strong ordering was later redefined as the hardware quality of a multiprocessor which guarantees sequential consistency without further software aid <ref> [DuS90, Sch89] </ref>. Hutto and Ahamad have hierarchically characterized various weak models [HuA90]. <p> For example, the authors of weak ordering mention that programmers should ``ensure mutual exclusion for each access to [a shared writable] variable by using high-level language constructs such as critical sections,'' where critical sections are implemented using hardware-recognizable synchronization primitives <ref> [DuS90] </ref>. There are potentially higher performance implementations not allowed by weak ordering that would also allow correct execution of such programs. <p> For example, the authors of weak ordering informally cite the programs that use mutual exclusion through constructs such as critical sections <ref> [DuS90] </ref> and the authors of total store ordering and partial store ordering cite programs that use locks as described in Section 2.2.2 [SUN91].
Reference: [DWB91] <author> M. DUBOIS, J. C. WANG, L. A. BARROSO, K. LEE and Y. CHEN, </author> <title> Delayed Consistency and Its Effects on the Miss Rate of Parallel Programs, </title> <booktitle> Supercomputing '91, </booktitle> <month> November </month> <year> 1991, </year> <pages> 197-206. </pages>
Reference-contexts: The specification in this work also seems to be incomplete and is discussed further in Chapter 7. Dubois et al. propose delayed consistency for a release consistent system where an invalidation is buffered at the receiving processor until a subsequent acquire is executed by the processor <ref> [DWB91] </ref>. This scheme alleviates the problem of false sharing; however, it does not strictly implement release consistency since a processor can execute a release before its preceding operations are performed at all other processors. <p> Another form of delayed consistency, referred to as loose coherence for Munin [BCZ90] and also discussed in <ref> [DWB91] </ref>, buffers updates of a processor until the following synchronization operation of the same processor. This retains the memory model of release consistency. Zucker proposes implementations for systems using software cache coherence [Zuc92].
Reference: [DKC93] <author> S. DWARKADAS, P. KELEHER, A. L. COX and W. ZWAENEPOEL, </author> <title> Evaluation of Release Consistent Software Distributed Shared Memory on Emerging Network Technology, </title> <booktitle> Proc. 20th Intl. Symp. on Computer Architecture, </booktitle> <year> 1993, </year> <pages> 144-155. </pages>
Reference-contexts: This thesis also does not provide any quantitative figures on the performance of the proposed memory models. Other researchers have already demonstrated that alternative memory models can give high performance <ref> [DKC93, GGH91a, GGH92, KCZ92, ZuB92] </ref>. This thesis addresses the problem of how to make the available performance potential usable for programmers and how to reason about models in a way that can result in better performance than that of previous models. 1.2. <p> Researchers at Rice University have performed two studies to compare release consistency and lazy release consistency implementations on a software-based shared virtual memory system <ref> [DKC93, KCZ92] </ref>. A key determinant of performance for such systems is the number of messages and amount of data exchanged. The first study is a trace-based simulation that examines the above metrics using invalidation and update based protocols [KCZ92].
Reference: [EmP88] <author> P. A. EMRATH and D. A. PADUA, </author> <title> Automatic Detection of Nondeterminacy in Parallel Programs, </title> <booktitle> Proc. SIGOPS Workshop on Parallel and Distributed Debugging, </booktitle> <month> May </month> <year> 1988, </year> <pages> 89-99. </pages> <note> Also appears in SIGPLAN Notices 24(1) (January 1989). </note>
Reference-contexts: While dynamic techniques provide precise information about a single execution, they provide little information about other executions, a serious disadvantage. For these reasons, the general consensus among researchers investigating data race detection is that tools should support both static and dynamic techniques in a complementary fashion <ref> [EmP88] </ref>. Rather than start from scratch, we seek to apply the data race detection techniques from sequentially consistent systems to data-race-free systems. Static techniques can be applied to programs for data-race-free systems unchanged, because they do not rely on executing the program. Dynamic techniques, however, depend on executing a program.
Reference: [FOW87] <author> J. FERRANTE, K. J. OTTENSTEIN and J. D. WARREN, </author> <title> The Program Dependence Graph and its Use in Optimization, </title> <journal> ACM Transactions on Programming Languages and Systems 9, </journal> <month> 3 (July </month> <year> 1987), </year> <pages> 319-349. </pages>
Reference: [FoW78] <author> S. FORTUNE and J. WYLLIE, </author> <title> Parallelism in Random Access Machines, </title> <booktitle> Proc. Tenth ACM Symposium on Theory of Computing, </booktitle> <year> 1978, </year> <pages> 114-118. </pages>
Reference-contexts: Many theoretical models of shared-memory have been proposed to simplify the design and analysis of parallel algorithms. The parallel random access machine (PRAM) model is one of the most well-known <ref> [FoW78] </ref>. The PRAM model assumes that processors proceed in lockstep, executing a read, compute, and write function of their next instruction in every cycle. A PRAM obeys sequential consistency because all memory operations of a PRAM execution can be serialized consistent with program order.
Reference: [FrS92] <author> M. FRANKLIN and G. S. SOHI, </author> <title> The Expandable Split Window Paradigm for Exploiting Fine-Grain Parallelism, </title> <booktitle> Proc. 19th Annual Intl. Symp. on Computer Architecture, </booktitle> <month> May </month> <year> 1992, </year> <pages> 58-67. </pages>
Reference-contexts: Thus, simply observing uniprocessor control dependences prohibits an anomaly of the above type. tially consistent execution is Write,Y po Write,flag co Read, flag po Read,Y. Assume a memory model where only the above path is valid. In the absence of the control requirement, an aggressive implementation <ref> [FrS92] </ref> could allow P 1 to write flag before its read of X returned a value.
Reference: [Fri93] <author> R. FRIEDMAN, </author> <title> Implementing Hybrid Consistency with High-Level Synchronization Operations, </title> <booktitle> Proc. Principles of Distributed Computing, </booktitle> <month> August </month> <year> 1993. </year>
Reference-contexts: The authors have also proposed implementations of hybrid consistency, assuming each processor has a copy of the entire memory and that all writes can be broadcast atomically <ref> [AtF92, Fri93] </ref>. They also prove lower bounds on response times in hybrid consistent systems [AtF92, Fri93]. 2.2.2. Processor Consistency And Related Models The processor consistency model was originally identified by Goodman [Goo89]. <p> The authors have also proposed implementations of hybrid consistency, assuming each processor has a copy of the entire memory and that all writes can be broadcast atomically <ref> [AtF92, Fri93] </ref>. They also prove lower bounds on response times in hybrid consistent systems [AtF92, Fri93]. 2.2.2. Processor Consistency And Related Models The processor consistency model was originally identified by Goodman [Goo89].
Reference: [GLL90] <author> K. GHARACHORLOO, D. LENOSKI, J. LAUDON, P. GIBBONS, A. GUPTA and J. HENNESSY, </author> <title> Memory Consistency and Event Ordering in Scalable Shared-Memory Multiprocessors, </title> <booktitle> Proc. 17th Annual Intl. Symp. on Computer Architecture, </booktitle> <month> May </month> <year> 1990, </year> <pages> 15-26. </pages>
Reference-contexts: We develop four SCNF models that unify the IBM 370 model [IBM83], the SPARC V8 models of total store ordering and partial store ordering [SUN91], the Alpha model [Sit92], weak ordering [DSB86], processor consistency <ref> [GLL90] </ref>, two flavors of release consistency (RCsc and RCpc) [GLL90], and lazy release consistency [KCZ92]. The four SCNF models data-race-free-0, data-race-free-1, PLpc1, and PLpc2 exploit strictly increasing amounts of information from the programmer regarding the behavior of memory operations in the program. <p> We develop four SCNF models that unify the IBM 370 model [IBM83], the SPARC V8 models of total store ordering and partial store ordering [SUN91], the Alpha model [Sit92], weak ordering [DSB86], processor consistency <ref> [GLL90] </ref>, two flavors of release consistency (RCsc and RCpc) [GLL90], and lazy release consistency [KCZ92]. The four SCNF models data-race-free-0, data-race-free-1, PLpc1, and PLpc2 exploit strictly increasing amounts of information from the programmer regarding the behavior of memory operations in the program. <p> They do not, however, explicitly mention the requirement of program order. For general interconnection networks, an often accepted definition of cache coherence requires that only writes to the same location appear to be seen by all processors in the same order <ref> [GLL90] </ref>. This thesis uses the term cache coherence, or simply coherence, to imply the above notion (a formal definition appears in Section 5.1.3). <p> Goodman also hypothesizes that processor consistency is adequate for most programs. Processor consistency was later refined by Gharachorloo et al. to explicitly specify the constraints on read operations as follows <ref> [GLL90] </ref>. (The definition implicitly assumes uniprocessor dependences and cache coherence.) Definition 2.3: [A system is processor consistent if] (1) before a load is allowed to perform with respect to any other processor, all [preceding] load accesses must be performed, and (2) before a store is allowed to perform with respect to <p> Release Consistency And Related Models The release consistency model combines and extends weak ordering and processor consistency <ref> [GLL90] </ref>. 7 The model exploits differences between different types of synchronization operations to provide higher performance potential than either weak ordering or processor consistency [GLL90]. The model classifies all shared-memory operations into special and ordinary, all special operations into syncs and nsyncs, and all sync operations into releases and acquires. <p> Release Consistency And Related Models The release consistency model combines and extends weak ordering and processor consistency <ref> [GLL90] </ref>. 7 The model exploits differences between different types of synchronization operations to provide higher performance potential than either weak ordering or processor consistency [GLL90]. The model classifies all shared-memory operations into special and ordinary, all special operations into syncs and nsyncs, and all sync operations into releases and acquires. This classification of memory operations will be discussed in more detail in Chapter 6. <p> Compared to processor consistency, release consistency provides higher performance potential mainly by allowing all operations between consecutive synchronization operations to be overlapped. A variant of release consistency where special accesses are sequentially consistent is also proposed <ref> [GLL90] </ref>. This model is abbreviated as RCsc, while the model with processor consistent special accesses is abbreviated as RCpc. Gharachorloo et al. identify a set of software constraints for which a system that obeys RCsc appears sequentially consistent. Programs that obey these constraints are called properly labeled (PL) programs. <p> Finally, the RCsc model, which was developed in parallel with the SCNF approach, is accompanied by a formal characterization of programs, called properly labeled or PL programs, for which RCsc gives sequential consistency <ref> [GLL90] </ref>. Thus, like the SCNF methodology, RCsc does not require programmers to deal with the system constraints of RCsc. However, in contrast to the SCNF methodology, RCsc imposes constraints on the system that are not necessary to guarantee sequential consistency to PL programs (Chapter 6). <p> used in Definition 4.5 to define the same set of programs as data-race-free-0. (The proofs assume a very general set of mechanisms to distinguish operations; the assumptions are given in Section 5.1.3 in connection with implementations of SCNF models.) Alternative 1 is based on the work by Gharachorloo et al. <ref> [GLL90] </ref>. It defines a race in a sequentially consistent execution as two conflicting operations that occur consecutively in the execution order. We recommend this definition for programmers since it only involves the concept of the execution order of a sequentially consistent execution. <p> Similarly, hardware can actually execute more operations than specified by an execution as long as the effects of these extra operations are not seen. This is in contrast to many earlier specifications <ref> [AdH90b, DSB86, GLL90, ScD87] </ref> which impose constraints on the real time ordering of events. <p> A more dynamic solution involves having two counters and providing a mechanism to distinguish operations (and their acknowledgements) preceding a particular synchronization write from those following the write. A simpler implementation, such as the DASH implementation of release consistency <ref> [GLL90] </ref>, is possible by forsaking some of the flexibility of the above specification. The DASH implementation blocks on all read operations; therefore, a synchronization write needs to wait for the completion of only preceding write operations. <p> The characterization of memory operations into pairable and unpairable operations is similar to the sync and nsync operations for properly labeled programs of release consistency <ref> [GLL90] </ref>; Section 6.1.5 discusses the differences. 6.1.2. Definition of Data-Race-Free-1 The previous section informally characterized synchronization operations as paired and unpaired based on the function they perform. This section gives the formal criterion for when the operations are distinguished correctly for data-race-free-1. <p> Comparison of Data-Race-Free-1 with Release Consistency (RCsc) This section compares data-race-free-1 with release consistency on the basis of programmability, portability, and performance. Programmability. For ease-of-programming, release consistency (RCsc) formalizes programs, called properly labeled programs, for which it ensures sequential consistency <ref> [GLL90] </ref>. All data-race-free-1 programs are properly labeled (interpreting data operations as ordinary, pairable synchronizations as syncs, and unpairable synchronizations as nsyncs), but there are some properly labeled programs that are not data-race-free-1 (as defined by Definition 6.4) [GMG91]. <p> This is an important contribution because in the past, determining when a certain optimization was safe to use involved lengthy and complex proofs that required reasoning about executions on the optimized system and proving that such an execution would appear sequentially consistent <ref> [AdH90b, AdH93, GLL90, GMG91, GiM92] </ref>; our mapping largely eliminates that complexity. The mapping between program information and optimizations leads to a characterization of SCNF memory models that determines their performance and programmability. <p> This work proposes additional hardware support for a release consistent system that allows runtime detection of violations of sequential consistency (due to programs not properly labeled <ref> [GLL90] </ref>). The scheme imposes an additional constraint on release consistent (RCsc) hardware requiring acquires to wait for all previous operations. Since release consistent (RCsc) systems obey data-race-free-1 and since data-race-free-1 programs are also properly labeled programs, it follows that the above scheme is applicable to data-race-free systems as well.
Reference: [GGH91a] <author> K. GHARACHORLOO, A. GUPTA and J. HENNESSY, </author> <title> Performance Evaluation of Memory Consistency Models for Shared-Memory Multiprocessors, </title> <booktitle> Proc. 4th Intl. Conf. on Architectural Support for Programming Languages and Operating Systems, </booktitle> <year> 1991, </year> <pages> 245-257. </pages>
Reference-contexts: Many of these models have been motivated by hardware optimizations and are specified in terms of these hardware optimizations. This hardware-centric nature of the models leads to substantial performance increases <ref> [GGH91a, GGH92, ZuB92] </ref>, but at the cost of losing significant advantages of sequential consistency for the programmer. The first disadvantage for programmers of hardware-centric models is that the programmer's view of the system is more complex than with sequential consistency. <p> This thesis also does not provide any quantitative figures on the performance of the proposed memory models. Other researchers have already demonstrated that alternative memory models can give high performance <ref> [DKC93, GGH91a, GGH92, KCZ92, ZuB92] </ref>. This thesis addresses the problem of how to make the available performance potential usable for programmers and how to reason about models in a way that can result in better performance than that of previous models. 1.2. <p> Gharachorloo et al. compared the models of sequential consistency, processor consistency, weak ordering, and release consistency (RCpc) for a DASH-like architecture [LLG90] using an execution-driven instruction-level simulator <ref> [GGH91a] </ref>. The study assumed an invalidation-based cache coherence protocol and blocking reads. It showed that the relaxed models can hide most of the write latency and can perform upto 41% better than sequential consistency. <p> Third, considering performance, earlier detailed hardware simulation studies for release consistency (RCpc) and weak ordering have reported upto 41% speedup in hardware (compared to sequential consistency) <ref> [GGH91a, ZuB92] </ref>. These studies do not fully exploit non-blocking reads; a study that does exploit such reads indicates better performance, but is trace-based [GGH92]. In most cases, weak ordering performs as well as release consistency (RCpc), but there are some cases where it gives significantly worse performance.
Reference: [GGH91b] <author> K. GHARACHORLOO, A. GUPTA and J. HENNESSY, </author> <title> Two Techniques to Enhance the Performance of Memory Consistency Models, </title> <booktitle> Proc. Intl. Conf. on Parallel Processing, </booktitle> <year> 1991, </year> <month> I355-I364. </month>
Reference-contexts: Gharachorloo et al. describe the use of non-binding hardware prefetching and speculative execution for overlapped memory accesses with sequential consistency in cache-based systems <ref> [GGH91b] </ref>. All of the above schemes that allow a processor to overlap or reorder its memory accesses without software support, however, either require complex or restricted hardware (e.g., hardware prefetching and rollback for [GGH91b] and restricted networks for [Col84-92, LHH91]) or the gains are expected to be small (e.g., [AdH90a, LHH91]). <p> of non-binding hardware prefetching and speculative execution for overlapped memory accesses with sequential consistency in cache-based systems <ref> [GGH91b] </ref>. All of the above schemes that allow a processor to overlap or reorder its memory accesses without software support, however, either require complex or restricted hardware (e.g., hardware prefetching and rollback for [GGH91b] and restricted networks for [Col84-92, LHH91]) or the gains are expected to be small (e.g., [AdH90a, LHH91]). Further, the optimizations of these schemes can be exploited by hardware (or the runtime system software), but cannot be exploited by compilers. <p> This approach allows a processor to execute its operations optimistically; the run-time environment detects any consistency violations and rolls back the relevant processes to a consistent state. The optimistic scheduling in [Jef85] has parallels with the speculative execution scheme of <ref> [GGH91b] </ref>; however, rollbacks in [GGH91b] are local while those in [Jef85] may have global effects. Similar ideas have also been used for parallelizing sequential programs written in mostly functional languages [Kni86, TiK88]. <p> This approach allows a processor to execute its operations optimistically; the run-time environment detects any consistency violations and rolls back the relevant processes to a consistent state. The optimistic scheduling in [Jef85] has parallels with the speculative execution scheme of <ref> [GGH91b] </ref>; however, rollbacks in [GGH91b] are local while those in [Jef85] may have global effects. Similar ideas have also been used for parallelizing sequential programs written in mostly functional languages [Kni86, TiK88]. The parallel tasks are executed optimistically, using runtime (hardware or software) support to detect dependence violations and effect rollbacks. <p> We also note that researchers have suggested techniques to tolerate the long memory latencies that are incurred with sequential consistency. These techniques include software or hardware based prefetching <ref> [GGH91b, MoG91] </ref> and the use of multiple contexts [ALK90, WeG89]. While these techniques help to improve the performance of sequentially consistent systems, they can also be used to aid the performance of relaxed systems [GHG91]. Observations similar to the above also apply to the compiler. <p> = data-race-free-0 (b) hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh the completion of preceding data operations, and to overlap part of the execution of data writes with the completion of a previous synchronization read, and (2) a rollback mechanism to let a processor conditionally execute its reads following its synchronization read before the synchronization read completes <ref> [GGH91b] </ref>. <p> Similar observations hold for release consistency (RCsc) since all implementations of release consistency (RCsc) must obey the specification of method 2. 18 As described in Section 4.3, our optimizations also benefit the aggressive implementations of release consistency (RCsc) based on hardware prefetch and rollback <ref> [GGH91b] </ref>. hhhhhhhhhhhhhhhhhh 18. It may be argued that the definition of ``performs with respect to'' on which release consistency (RCsc) is based allows a processor to execute its release as long as the release is not made visible to any other processor.
Reference: [GhG91] <author> K. GHARACHORLOO and P. B. GIBBONS, </author> <title> Detecting Violations of Sequential Consistency, </title> <booktitle> Proc. Symp. on Parallel Algorithms and Architectures, </booktitle> <month> July </month> <year> 1991, </year> <pages> 316-326. </pages>
Reference-contexts: Section 2.4 summarizes performance studies of relaxed models. Section 2.5 describes other correctness criteria that are stronger than or similar to sequential consistency. For our work on debugging on relaxed models (Chapter 8), the only other related work is by Gharachorloo and Gibbons <ref> [GhG91] </ref>. Since this work is related only to Chapter 8, we postpone its discussion to that chapter. This chapter uses the terms preceding and following to indicate program order. 2.1. <p> These techniques include software or hardware based prefetching [GGH91b, MoG91] and the use of multiple contexts [ALK90, WeG89]. While these techniques help to improve the performance of sequentially consistent systems, they can also be used to aid the performance of relaxed systems <ref> [GHG91] </ref>. Observations similar to the above also apply to the compiler. Compilers need to perform a global data dependence analysis to determine when it might be safe to reorder memory operations without violating sequential consistency. <p> Gharachorloo et al. also propose hardware support that informs the programmer of when there might be data races in the program <ref> [GhG91] </ref>; however, further support is required to allow programmers to reason with sequential consistency to eliminate these data races. <p> We argue in [AHM91] how all the limitations of our technique are analogous to the limitations of dynamic techniques for sequentially consistent systems. 8.5. Related Work To the best of our knowledge, the only other work related to debugging on non-sequentially consistent systems is by Gharachorloo and Gibbons <ref> [GhG91] </ref>. This work proposes additional hardware support for a release consistent system that allows runtime detection of violations of sequential consistency (due to programs not properly labeled [GLL90]). The scheme imposes an additional constraint on release consistent (RCsc) hardware requiring acquires to wait for all previous operations. <p> The advantages of the above scheme compared to our work are the following. First, the debugging overhead for the scheme in <ref> [GhG91] </ref> is in the additional hardware support and the additional constraint on acquires. This is far less than that incurred by our tracing mechanism. <p> The disadvantages of the above scheme are the following. First, the above scheme cannot detect the first data races that may be responsible for the bug in the program. Therefore, unlike our scheme, the scheme in <ref> [GhG91] </ref> does not (yet) help programmers to reason with sequential consistency while debugging. The second disadvantage is that the debugging overhead due to the additional hardware constraint is incurred with the above scheme even after the program is debugged. <p> Interactions between relaxed memory models and other latency hiding or reducing techniques are also important to study further <ref> [GHG91] </ref>. Note that most of the optimizations due to memory models are essentially trying to exploit the implicit, fine-grained parallelism in a single thread.
Reference: [GAG92] <author> K. GHARACHORLOO, S. V. ADVE, A. GUPTA, J. L. HENNESSY and M. D. HILL, </author> <title> Programming for Different Memory Consistency Models, </title> <journal> Journal of Parallel and Distributed Computing 15, </journal> <month> 4 (August </month> <year> 1992), </year> <pages> 399-407. </pages>
Reference-contexts: Data-race-free-1 unifies the hardware-centric model of release consistency (RCsc) and other new implementations along with the systems unified by data-race-free-0. The PLpc1 and PLpc2 models are based on our earlier programmer-centric model called PLpc, proposed jointly with others <ref> [GAG92] </ref>. PLpc unifies the models of total store ordering, partial store ordering, processor consistency, and release consistency (RCpc) along with the systems unified by the data-race-free models. <p> The PLpc1 Memory Model As mentioned earlier, the PLpc1 memory model is based on the PLpc memory model <ref> [GAG92] </ref>; therefore, the key concepts used to define PLpc1 below were originally presented in [GAG92]. 6.2.1. Motivation of PLpc1 PLpc1 extends data-race-free-1 by exploiting additional differences between synchronization operations. Consider the program in figure 6.6. <p> The PLpc1 Memory Model As mentioned earlier, the PLpc1 memory model is based on the PLpc memory model <ref> [GAG92] </ref>; therefore, the key concepts used to define PLpc1 below were originally presented in [GAG92]. 6.2.1. Motivation of PLpc1 PLpc1 extends data-race-free-1 by exploiting additional differences between synchronization operations. Consider the program in figure 6.6. <p> The next sub-section formalizes these concepts. Loop reads and loop writes have two important properties that were first observed for the PLpc model <ref> [GAG92] </ref>. First, assuming that a synchronization loop eventually terminates, the number of times the loop executes or the values returned by its unsuccessful reads cannot be detected by the programmer and cannot comprise the result of a run of the program. <p> Since PLpc1 seeks to optimize loop operations, any operation can be distinguished as non-loop; however, the operations distinguished as loop must obey the definition for a loop operation given below. For simplicity, the following formalization of a synchronization loop (from <ref> [GAG92] </ref>) captures only a simple (but common) case in which the loop repeatedly executes a read or a read-modify-write to a specific location until it returns one of certain specific values. A more general definition allowing locks that employ Test&Test&Set [RuS84] and/or backoff techniques [MeS91] appears in [GAG92]. <p> synchronization loop (from <ref> [GAG92] </ref>) captures only a simple (but common) case in which the loop repeatedly executes a read or a read-modify-write to a specific location until it returns one of certain specific values. A more general definition allowing locks that employ Test&Test&Set [RuS84] and/or backoff techniques [MeS91] appears in [GAG92]. Chapter 7 provides a further generalization of this concept. Definition 6.6: Synchronization Loop: A synchronization loop of a program is a sequence of instructions of the program that satisfies the following. (i) The loop specifies the execution of a read or a read-modify-write to a specific location. <p> For distinguishing loop and non-loop operations, a PLpc1 system can provide mechanisms similar to those for distinguishing the operations of data-race-free programs, as shown in Figure 6.8 (the examples of the figure are also used for the PLpc model in <ref> [GAG92] </ref>). Figure 6.8 (a) shows how the program of figure 6.6 can be converted to a PLpc1 program assuming the programming language provides annotations. A sufficient set of annotations for PLpc1 is: data = ON, unpairable = ON, loop = ON, and non-loop = ON. <p> Appendix H formally proves that the above mapping to read-modify-writes is sufficient by showing that with the above mapping, total store ordering systems obey the aggressive system-centric specification of PLpc1 in Appendix G; the proof is very similar to that for the PLpc model <ref> [GAG92] </ref>. The use of read-modify-writes is necessary in general as illustrated by figure 6.9. <p> Based on the discussion of PLpc2 implementations, the mapping to read-modify-writes ensures that the effect of non-atomic writes is not seen. Appendix H gives the formal proof (based on the proof in <ref> [GAG92] </ref>) that these mappings are correct. Figure 6.12 illustrates that this conversion to read-modify-writes is, in general, necessary as follows. Processor P3's read of X forms a partial race with P1's write of X and is an atomic read. <p> Thus, the PLpc2 memory model unifies many commercial and academic models, resulting in advantages of programmability, portability, and performance for many of the programmers of such systems. 6.4. The PLpc Memory Model For completeness, this section first briefly describes the PLpc model <ref> [GAG92] </ref> (the original model on which PLpc1 and PLpc2 are based) and then briefly describes why we choose to use PLpc1 and PLpc2 instead. <p> Our overall approach is to first identify a system-centric specification for sequential consistency, and then use it to characterize when an optimization will not violate sequential consistency. Section 7.1 gives such a specification based on several previous works <ref> [AdH92a, Col84-92, GAG92, LHH91, ShS88] </ref>. Section 7.2 uses the above specification to deduce the mapping between optimizations and information. Section 7.3 uses the mapping of Section 7.2 to examine several optimizations and common program constructs. <p> Section 7.1.1 and Observation 1 of Section 7.1.2 follow directly from previous work by Shasha and Snir [ShS88], Collier [Col84-92], and Landin et al. [LHH91]. Observations 2 and 3 of Section 7.1.2 are extensions of similar concepts developed for the PLpc model <ref> [GAG92] </ref>. 7.1.1. A Simple Condition We use the notions of conflict order ( co ) and program/conflict graph defined earlier for a sequentially consistent execution and reproduced below for a general execution. Definition 7.1: Conflict Order ( co ): Let X and Y be two memory operations in an execution. <p> Observation 2. The second observation, based on work for PLpc <ref> [GAG92] </ref>, is that certain instructions and operations, called unessential operations, can be removed from the execution, leaving behind instructions and operations that also form an execution and with the same result as the original execution. Consequently, the system need not consider ordering paths resulting from unessential operations. <p> The ordering path from the write of d to the read of d results from the unessential read on c; therefore, this path can be ignored and the system need not restrict the operations on this path. A case of unessential operations was identified for the PLpc model <ref> [GAG92] </ref> (and a conservative definition was used for PLpc1 and PLpc2 in Chapter 6). This chapter allows operations from more general types of loops where the termination of the loop may require several reads to different locations to return certain values. <p> Like PLpc, an exit read may be part of a read-modify-write, but only if it is the only read specified by the predicate. The write of the read-modify-write is called an exit write. The other constraints below are the same as for the aggressive definition for PLpc given in <ref> [GAG92] </ref>; they ensure that all but the last iteration of the loop can be ignored in every execution and the only shared-memory operations in the last iteration are the exit reads and writes. <p> Our general approach outlined hhhhhhhhhhhhhhhhhh 36. The data race detection work is joint work with others [AHM91]; PLpc1 and PLpc2 are derived from joint work with others on the PLpc model <ref> [GAG92] </ref>. - -- above, however, is valid with any other base model as well.
Reference: [GGH92] <author> K. GHARACHORLOO, A. GUPTA and J. HENNESSY, </author> <title> Hiding Memory Latency using Dynamic Scheduling in Shared-Memory Multiprocessors, </title> <booktitle> Proc. 19th Intl. Symp. on Computer Architecture, </booktitle> <year> 1992, </year> <pages> 22-33. </pages> - -- 
Reference-contexts: Many of these models have been motivated by hardware optimizations and are specified in terms of these hardware optimizations. This hardware-centric nature of the models leads to substantial performance increases <ref> [GGH91a, GGH92, ZuB92] </ref>, but at the cost of losing significant advantages of sequential consistency for the programmer. The first disadvantage for programmers of hardware-centric models is that the programmer's view of the system is more complex than with sequential consistency. <p> This thesis also does not provide any quantitative figures on the performance of the proposed memory models. Other researchers have already demonstrated that alternative memory models can give high performance <ref> [DKC93, GGH91a, GGH92, KCZ92, ZuB92] </ref>. This thesis addresses the problem of how to make the available performance potential usable for programmers and how to reason about models in a way that can result in better performance than that of previous models. 1.2. <p> It showed gains of upto 35% over SC, which were highly correlated with the hit rate, which in turn was highly correlated with the cache and line size. Neither of the above studies fully exploited the non-blocking reads allowed by weak ordering and release consistency. Gharachorloo et al. <ref> [GGH92] </ref> examine non-blocking reads on a dynamically scheduled processor with a trace-driven simulation. The results indicate that weak ordering and release consistency can hide most of the read latency, but only with large window sizes (from 64 to 256). <p> Third, considering performance, earlier detailed hardware simulation studies for release consistency (RCpc) and weak ordering have reported upto 41% speedup in hardware (compared to sequential consistency) [GGH91a, ZuB92]. These studies do not fully exploit non-blocking reads; a study that does exploit such reads indicates better performance, but is trace-based <ref> [GGH92] </ref>. In most cases, weak ordering performs as well as release consistency (RCpc), but there are some cases where it gives significantly worse performance. However, the above studies analyze programs written for sequential consistency and are not data-race-free-0 or PLpc.
Reference: [GAG93] <author> K. GHARACHORLOO, S. V. ADVE, A. GUPTA, J. L. HENNESSY and M. D. HILL, </author> <title> Specifying System Requirements for Memory Consistency Models, </title> <type> Technical Report #CSL-TR-93-594, </type> <institution> Stanford University, </institution> <month> December </month> <year> 1993. </year> <note> Also available as Computer Sciences Technical Report #1199, </note> <institution> University of Wisconsin, Madison, </institution> <month> December </month> <year> 1993. </year>
Reference-contexts: We have recently, in a joint effort, proposed a uniform methodology for expressing the system constraints of various hardware-centric models <ref> [GAG93] </ref>. <p> For example, release consistency itself combines the optimizations of weak ordering and processor consistency. This section presents the definitions of the various models in their original form. Subsequently, we have (jointly with others) proposed a uniform terminology and framework to express these models <ref> [GAG93] </ref>; this methodology is discussed briefly in Section 2.3 and in more detail in Chapter 7. A goal of this section is to demonstrate the hardware-centric nature of many of the current models and the large variety of interfaces they present to the programmer. <p> Synchronization operations act like local barriers that ensure all operations preceding the synchronization will globally perform before any operation following the synchronization is issued. Analogously, compilers can reorder memory operations between two consecutive synchronization operations, and can also allocate such locations in registers (with a little care <ref> [GAG93] </ref>). This is in contrast to the processor consistency based models of the next sub-section, where most of the allowed optimizations can be exploited only by the hardware (or runtime system software). The authors of weak ordering also give informal constraints for programmers. <p> Landin et al. propose various schemes for race-free interconnection networks that exploit the transaction orderings preserved by the network [LHH91]. One of the schemes is more aggressive than definition 2.3 of processor consistency, but preserves the semantics of definition 2.3 <ref> [GAG93] </ref>. Other schemes are variants of processor consistency. Lipton and Sandberg have proposed the pipelined RAM model [LiS88, San90]. The definition assumes that each processor is associated with a copy of its memory. <p> Hutto et al. describe causal memory using similar partial orders. These methods do not adequately model non-atomicity and can be considered to be special cases of Collier's formalism. - -- We have, jointly with others, proposed a uniform methodology to describe many of the models of Section 2.2 <ref> [GAG93] </ref>. The formalism used is a combination of Collier's formalism and the formalism developed for the SPARC V8 models, and eliminates the disadvantages of the above formalisms. <p> Chapter 5 extends the above formalism for designers of SCNF systems who need to reason with non-sequentially consistent behavior as well. hhhhhhhhhhhhhhhhhh 9. We assume a relatively weak notion of atomic read-modify-writes to be consistent with other work <ref> [GAG93] </ref>. A stronger notion would require all operations of an atomic subset of an instruction to appear together in the execution order. <p> A read sub-operation, R (i), corresponds to the read R performing with respect to all processors. Our first use of Collier's work in [AdH92] assumed that a write always had n sub-operations. The relaxation that only some sub-operations need be included was first included in <ref> [GAG93] </ref> to represent more systems; e.g., systems with software-based cache-coherence where a write need not update the memory copies of all processors. Further, the work in [AdH92] implicitly assumes that in any execution, only a finite number of operations can be ordered before another operation by program order. <p> Further, the work in [AdH92] implicitly assumes that in any execution, only a finite number of operations can be ordered before another operation by program order. Our formalism and that in <ref> [GAG93] </ref> do not make this assumption. Finally, the work in [GAG93] explicitly models features such as write buffers; Section 7.6 of Chapter 7 explains why we chose to exclude that feature. <p> Further, the work in [AdH92] implicitly assumes that in any execution, only a finite number of operations can be ordered before another operation by program order. Our formalism and that in <ref> [GAG93] </ref> do not make this assumption. Finally, the work in [GAG93] explicitly models features such as write buffers; Section 7.6 of Chapter 7 explains why we chose to exclude that feature. With the above formalism for a system, an execution is defined similar to a sequentially consistent execution with four important exceptions. <p> The reasoning used in this section has evolved from other joint work <ref> [AGG93, GAG93] </ref>. We consider compiler optimizations that involve reordering instructions of the program (e.g., loop transformations) and allocating shared-memory locations to registers. The optimization of register allocation does not have a direct analog in the runtime system, since this optimization results in eliminating certain operations from the original input program. <p> The generalization makes explicit the relation between the model, the programmer constraints, and the system constraints. The methodology used to describe the specifications is based on the work in [AdH92] and <ref> [GAG93] </ref>. Section 7.6 further explains the relationship between the specifications of this chapter and those in [AdH92, AGG93, GAG93]. <p> The generalization makes explicit the relation between the model, the programmer constraints, and the system constraints. The methodology used to describe the specifications is based on the work in [AdH92] and [GAG93]. Section 7.6 further explains the relationship between the specifications of this chapter and those in <ref> [AdH92, AGG93, GAG93] </ref>. <p> Low-Level System-Centric Specifications and Hardware Implementations The following gives low-level system-centric specifications of the valid path and control requirements and describes corresponding hardware implementations. As mentioned earlier, the specifications of this section and the various concepts they use are related to those in <ref> [AdH92, AGG93, GAG93] </ref>; Section 7.6 discusses this relationship. Valid path requirement. The high-level valid path requirement in Condition 7.16 is analogous to the data requirement of the data-race-free-0 model discussed in Chapter 5. Consequently, analogous low-level specifications are possible. <p> Again, the implementations discussed below are based on those in <ref> [AdH92, AGG93, GAG93] </ref>. <p> We consider the compiler optimizations of reordering operations of a process and allocating shared-memory locations in registers. We reason about register allocation in the same way as for data-race-free-0; part of the reasoning was done jointly with Kourosh Gharachorloo for the work in <ref> [AGG93, GAG93] </ref>. Thus, we allow two types of intervals over which a memory location can be allocated to a register. The first type allows only read operations to be substituted by register operations and is preceded by a start memory read. <p> The following discusses the constraints on the compiler imposed by the valid path and control requirements. Much of the discussion parallels that for data-race-free-0 in Section 5.4, and evolved from joint work in <ref> [AGG93, GAG93] </ref>. Recall that the compiler must ensure that the relevant constraints are obeyed for all executions (that might be possible on the system the output program will run on). Below, all references to an ordering refer to program order, unless stated otherwise. <p> Relation with a Framework for Specifying System-Centric Requirements Jointly with others, we have proposed a framework for uniformly specifying previous hardware-centric models and system constraints for future memory consistency models <ref> [GAG93] </ref>. This framework uses an extension of Collier's abstraction that also models the equivalent of a write buffer in a processor. The following first discusses how the extension differs from the original abstraction by Collier and why we chose the original abstraction for this work. <p> The following first discusses how the extension differs from the original abstraction by Collier and why we chose the original abstraction for this work. We then discuss how the methodology for specifying system-centric requirements in <ref> [GAG93] </ref> relates to the methodology of this chapter. The extended abstraction explicitly models the equivalent of a write buffer in a processor. A write operation now involves an additional initiation sub-operation that can be viewed as the write being placed in the write buffer of its processor. <p> Similar interactions are possible when allocating shared-memory locations in registers and when a processor writes and subsequently reads from a cache line before getting ownership <ref> [GAG93, GGH93] </ref>. Thus, the extended abstraction allows more direct modeling of certain interactions than the original abstraction by Collier. <p> Theorem H.1 in Appendix H formalizes one type of translation for some systems and is used to prove results for the PLpc models [AGG93]. We next discuss the methodology for specifying system constraints in <ref> [GAG93] </ref>. The methodology is an extension of the data-race-free-1 specifications and has also been used for system-centric specifications of PLpc [AGG93]. The methodology for the system-centric specifications of this Chapter is based on the above work. Specifically, [GAG93] proposes specifying systems as constraints on conflicting sub-operations, and shows how such specifications <p> We next discuss the methodology for specifying system constraints in <ref> [GAG93] </ref>. The methodology is an extension of the data-race-free-1 specifications and has also been used for system-centric specifications of PLpc [AGG93]. The methodology for the system-centric specifications of this Chapter is based on the above work. Specifically, [GAG93] proposes specifying systems as constraints on conflicting sub-operations, and shows how such specifications are more aggressive but semantically equivalent to previous specifications of hardware-centric models. <p> The observation that all system constraints (other than finite speculation) can be expressed in terms of constraints on conflicting sub-operations was made by Kourosh Gharachorloo while working on the PLpc model and led to the work in <ref> [GAG93] </ref>; the data-race-free-1 model made this observation only for part of the system constraints. Effectively, [GAG93] proposes that system specifications should give the ordering paths that are executed safely by the system (along with a write termination requirement, a requirement for the initiation sub-operation of a write, and finite speculation). <p> that all system constraints (other than finite speculation) can be expressed in terms of constraints on conflicting sub-operations was made by Kourosh Gharachorloo while working on the PLpc model and led to the work in <ref> [GAG93] </ref>; the data-race-free-1 model made this observation only for part of the system constraints. Effectively, [GAG93] proposes that system specifications should give the ordering paths that are executed safely by the system (along with a write termination requirement, a requirement for the initiation sub-operation of a write, and finite speculation). However, unlike the work in this chapter, the work in [GAG93] is concerned only with specifying <p> Effectively, <ref> [GAG93] </ref> proposes that system specifications should give the ordering paths that are executed safely by the system (along with a write termination requirement, a requirement for the initiation sub-operation of a write, and finite speculation). However, unlike the work in this chapter, the work in [GAG93] is concerned only with specifying systems in the most aggressive manner. <p> This chapter tries to determine the relationship between the ordering paths executed safely by the system and information from the program that will give sequential consistency to such a system, and to propose new SCNF models based on this relationship. The work in <ref> [GAG93] </ref> does not provide insight for such a relationship, and uses the work of the current chapter to ensure that specifications of previous hardware-centric models appear sequentially consistent to data-race-free/PL/PLpc programs. 7.6.6.
Reference: [GGH93] <author> K. GHARACHORLOO, A. GUPTA and J. HENNESSY, </author> <title> Revision to ``Memory Consistency and Event Ordering in Scalable Shared-Memory Multiprocessors'', </title> <type> Technical Report CSL-TR-93-568, </type> <institution> Stanford University, </institution> <month> April </month> <year> 1993. </year>
Reference-contexts: Two models not connected with arrows cannot be compared in terms of performance or programmability. Chapter 2 discusses these models and their relationships in more detail. The dotted arrows exist only if a recent, subtle revision to processor consistency and release consistency (RCpc) <ref> [GGH93] </ref> is assumed (see Chapter 2), an additional testimony to the complexity of reasoning with hardware-centric memory models. hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh Finally, even from the system designer's viewpoint, the absence of a unifying framework for designing memory models makes it difficult to determine new, useful memory models that might allow new optimizations. <p> It was recently discovered that definition 2.3 does not allow a read to return the value of its own processor's preceding write from the write buffer until the write is serialized at memory, and a revision that eliminates the above constraint has been published <ref> [GGH93] </ref>. This revision involves modifying the definition of ``perform,'' and allows certain executions not allowed by definition 2.3. The relevant results of the rest of this thesis are applicable to both the above definition and the revised version of the processor consistency model. <p> requires that a write by a processor P i should appear to be performed with respect to all other processors in the system at the same time (this is implied by (1) above); this makes total store ordering more strict than definition 2.3 (and the revised definition of processor consistency <ref> [GGH93] </ref>). Total store ordering allows a processor to read the value of its own write from its write buffer before the write is sent out to the memory system (implied by (4)). <p> This makes total store ordering less strict than processor consistency when compared to definition 2.3, but not when compared to the revised definition <ref> [GGH93] </ref>. The partial store ordering model is similar to total store ordering except that it ensures two writes of the same processor to different locations will appear in program order only if they are separated by a special STBAR (store barrier) instruction [SUN91]. <p> The second disadvantage of this terminology is that it seems inadequate to capture a subtle interaction between reads returning values from writes in their write buffers and cache coherence. This interaction prompted the revisions in processor consistency and release consistency (Section 2.2) <ref> [GGH93] </ref>, and is discussed in detail in Chapter 7. The formalism used by Collier is related to that of Dubois et al., but is not time-based. <p> Similar interactions are possible when allocating shared-memory locations in registers and when a processor writes and subsequently reads from a cache line before getting ownership <ref> [GAG93, GGH93] </ref>. Thus, the extended abstraction allows more direct modeling of certain interactions than the original abstraction by Collier.
Reference: [GMG91] <author> P. B. GIBBONS, M. MERRITT and K. GHARACHORLOO, </author> <title> Proving Sequential Consistency of High-Performance Shared Memories, </title> <booktitle> Proc. Symp. on Parallel Algorithms and Architectures, </booktitle> <month> July </month> <year> 1991, </year> <pages> 292-303. </pages>
Reference-contexts: The advantage of this formalism is that it captures the write buffer interaction described above; the disadvantage, however, is that it does not adequately model the non-atomicity of writes. Gibbons et al. and Afek et al. <ref> [ABM89, ABM93, GMG91, GiM92] </ref> use the formalism of an I/O automaton developed by Lynch and Tuttle [LyT88]. This formalism expresses the system in terms of a non-deterministic automaton. The formalism is powerful and precise, and does not suffer from the disadvantages of the above formalisms. <p> All data-race-free-1 programs are properly labeled (interpreting data operations as ordinary, pairable synchronizations as syncs, and unpairable synchronizations as nsyncs), but there are some properly labeled programs that are not data-race-free-1 (as defined by Definition 6.4) <ref> [GMG91] </ref>. The difference is minor and arises because properly labeled programs have a less explicit notion of pairing. They allow conflicting data operations to be ordered by operations (nsyncs) that correspond to the unpair-able synchronization operations of data-race-free-1. <p> However, the program is not data-race-free-1 and so the implementations of data-race-free-1 corresponding to methods 3 and 4 of Section 5.3.1 allow executions where the reads return 0. hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh Performance. With respect to performance, data-race-free-1 allows all implementations of release consistency (RCsc) because all data-race-free-1 programs are properly labeled <ref> [GMG91] </ref> (interpreting data operations as ordinary, pair-able synchronizations as syncs, and unpairable synchronizations as nsyncs), and so all implementations of release consistency (RCsc) ensure sequential consistency to data-race-free-1 programs. <p> This is an important contribution because in the past, determining when a certain optimization was safe to use involved lengthy and complex proofs that required reasoning about executions on the optimized system and proving that such an execution would appear sequentially consistent <ref> [AdH90b, AdH93, GLL90, GMG91, GiM92] </ref>; our mapping largely eliminates that complexity. The mapping between program information and optimizations leads to a characterization of SCNF memory models that determines their performance and programmability.
Reference: [GiM92] <author> P. B. GIBBONS and M. MERRITT, </author> <title> Specifying Nonblocking Shared Memories, </title> <booktitle> Proc. Symp. on Parallel Algorithms and Architectures, </booktitle> <year> 1992, </year> <pages> 306-315. </pages>
Reference-contexts: The specification of entry consistency, however, seems to be incomplete; Chapter 7 discusses this issue further. Gibbons and Merritt have proposed a relaxation of release consistency (RCsc) that is a generalization of entry consistency <ref> [GiM92] </ref>. This relaxation allows a programmer to associate a release with a subset of the preceding operations. A release now needs to wait for only its associated data operations (and all preceding synchronization operations) to be performed. <p> The advantage of this formalism is that it captures the write buffer interaction described above; the disadvantage, however, is that it does not adequately model the non-atomicity of writes. Gibbons et al. and Afek et al. <ref> [ABM89, ABM93, GMG91, GiM92] </ref> use the formalism of an I/O automaton developed by Lynch and Tuttle [LyT88]. This formalism expresses the system in terms of a non-deterministic automaton. The formalism is powerful and precise, and does not suffer from the disadvantages of the above formalisms. <p> The partial order on the instances (and the corresponding order on the shared-memory operations) is called the program order (denoted po adopt the approach of Gibbons and Merritt <ref> [GiM92] </ref> and allow the program order to be partial even over instruction instances of the same process. We will use the terms preceding and following to indicate program order. <p> Other researchers have proposed further relaxations of these models; e.g., the entry consistency model requires programmers to associate locks with the data they protect [BeZ91, BZS92], Gibbons and Merritt allow programmers to associate a release with a specific subset of operations that it releases <ref> [GiM92] </ref>, Carlton suggests associating locks with the data they protect and allocating them in the same memory module [Car91]. A natural question that arises next is: are there other models that can potentially give higher performance and/or better programmability than the current models? This chapter answers the above question. <p> This is an important contribution because in the past, determining when a certain optimization was safe to use involved lengthy and complex proofs that required reasoning about executions on the optimized system and proving that such an execution would appear sequentially consistent <ref> [AdH90b, AdH93, GLL90, GMG91, GiM92] </ref>; our mapping largely eliminates that complexity. The mapping between program information and optimizations leads to a characterization of SCNF memory models that determines their performance and programmability. <p> For example, the write to Flag2 can be executed before the writes to A and B as well as before the write to Flag1. Optimizations similar to those of the above model are suggested by Bershad et al. [BeZ91, BZS92] and Gibbons and Merritt <ref> [GiM92] </ref>; we discuss how they compare to our models at the end of this sub-section. The above mechanism may be difficult for programmers to use and difficult for system designers to implement. <p> Gibbons and Merritt have proposed an aggressive version of release consistency where the programmer can associate a release (or sender) operation with data operations that it must release <ref> [GiM92] </ref>. However, the system they propose does not allow overlapping two releases (or senders). Thus, just by associating releases with appropriate operations, the system they propose would not execute the writes of Flag1 and Flag2 in parallel. Their framework, however, allows program order to be a partial order per process. <p> The work by Gibbons and Merritt <ref> [GiM92] </ref> discussed in Section 7.3.1.1 is related to the model of this section as well since it allows associating a sender barrier with all operations it sends. <p> Intuitively, this serialization is not necessary. How can we extend the key idea of entry consistency without the above limitation? Gibbons and Merritt proposed a general scheme to associate releases with operations they release <ref> [GiM92] </ref>; i.e., an unlock could be associated with the operations it sends. However, such a general mechanism is difficult to implement and use, and not necessary for figure 7.8. Can we identify easier-to-use mechanisms? unlocks that send only the operations that follow the last preceding lock. <p> Also note that although from the programmer's perspective, the above model can be viewed as a special case of the relaxed release consistency model in <ref> [GiM92] </ref>, the above optimization is not allowed by the systems considered in [GiM92] since they do not reorder unlocks; further, the special case we cite is easier to use than the general mechanism of [GiM92]. 7.3.2.4. Constructs to Decrease Lock Contention Consider figure 7.9 (a). <p> Also note that although from the programmer's perspective, the above model can be viewed as a special case of the relaxed release consistency model in <ref> [GiM92] </ref>, the above optimization is not allowed by the systems considered in [GiM92] since they do not reorder unlocks; further, the special case we cite is easier to use than the general mechanism of [GiM92]. 7.3.2.4. Constructs to Decrease Lock Contention Consider figure 7.9 (a). It shows a processor that accesses some shared variables protected by a lock. <p> above model can be viewed as a special case of the relaxed release consistency model in <ref> [GiM92] </ref>, the above optimization is not allowed by the systems considered in [GiM92] since they do not reorder unlocks; further, the special case we cite is easier to use than the general mechanism of [GiM92]. 7.3.2.4. Constructs to Decrease Lock Contention Consider figure 7.9 (a). It shows a processor that accesses some shared variables protected by a lock. <p> The following discusses models related to the optimization of executing memory operations out of program order, pipelining operations, executing writes non-atomically, and eliminating acknowledgements in the above order. Gibbons and Merritt have relaxed release consistency by allowing a release to be associated with a subset of the preceding operations <ref> [GiM92] </ref>. <p> Section 7.3.1.1 shows how to generalize the above observation by allowing a receiver (an acquire in the terminology of <ref> [GiM92] </ref>) to be associated with operations it receives for as well, and allow more optimizations with acquires. Further, we also allow a sender to not wait for the preceding synchronization operations that it is not associated with. <p> This is expressed in our work by making the program order of each process partial, as recommended by Gibbons and Merritt <ref> [GiM92] </ref>. Finally, from a more theoretical standpoint, it would also be interesting to make the critical set and control requirement more aggressive, and the proof of correctness of the control requirement simpler.
Reference: [Goo89] <author> J. R. GOODMAN, </author> <title> Cache Consistency and Sequential Consistency, </title> <type> Technical Report #61, </type> <institution> SCI Committee, </institution> <month> March </month> <year> 1989. </year> <note> Also available as Computer Sciences Technical Report #1006, </note> <institution> University of Wisconsin, Madison, </institution> <month> February </month> <year> 1991. </year>
Reference-contexts: They also prove lower bounds on response times in hybrid consistent systems [AtF92, Fri93]. 2.2.2. Processor Consistency And Related Models The processor consistency model was originally identified by Goodman <ref> [Goo89] </ref>. The model ensures that write operations of a given processor are observed in the same order by all processors in the system; however, writes by different processors may be seen in different orders by different processors.
Reference: [Goo93] <author> J. R. GOODMAN, </author> <title> Private Communication, </title> <year> 1993. </year>
Reference-contexts: The relaxed memory order model for SPARC V9 is an example of such a model. Further, with more powerful hardware primitives such as full/empty bits [Smi82] that enforce implicit synchronization, a system with no model at all may be viable <ref> [Goo93] </ref>. Finally, we could continue to use SCNF, but also provide an alternative, high-level system-centric specification.
Reference: [GGK83] <author> A. GOTTLIEB, R. GRISHMAN, C. P. KRUSKAL, K. P. MCAULIFFE, L. RUDOLPH and M. SNIR, </author> <title> The NYU Ultracomputer Designing an MIMD Shared Memory Parallel Computer, </title> <journal> IEEE Trans. on Computers, </journal> <month> February </month> <year> 1983, </year> <pages> 175-189. </pages>
Reference-contexts: Shared-memory also permits decoupling the correctness of a program from its performance, allowing incremental tuning of programs for higher performance. The shared-memory abstraction can be provided either by hardware, software, or a combination of both. Pure hardware configurations include uniform <ref> [GGK83] </ref> and non-uniform access machines [ReT86]. Most configurations employ caches to reduce memory latency, and either use snooping [Bel85] or directory-based protocols [ASH88, Gus92, LLG90] to keep the caches up-to-date.
Reference: [Gup89] <author> R. GUPTA, </author> <title> The Fuzzy Barrier: A Mechanism for High Speed Synchronization of Processors, </title> <booktitle> Proc. Third Intl. Conf. on Architectural Support for Programming Languages and Operating Systems, </booktitle> <month> April </month> <year> 1989, </year> <pages> 54-63. </pages>
Reference-contexts: From (iii) and (iv), neither is possible, and so a signal write can be executed non-atomically. 7.3.2.2. Barriers This section examines overlapping the latency of barrier operations with operations that precede and follow the barrier. The resulting model is similar to the notion of fuzzy barriers proposed by Gupta <ref> [Gup89] </ref>; we discuss the relationship with Gupta's work at the end of the section. As before, assume programs that are divided into phases where adjacent phases are separated by a barrier and locations accessed by barrier operations are not accessed by non-barrier operations. <p> The model described above is related to the notion of fuzzy barriers proposed by Gupta <ref> [Gup89] </ref>. The work assumes a barrier implementation using a hardware broadcast mechanism without involving memory. A processor can execute a barrier in parallel with its other memory operations. <p> Our work extends such a model as well by proposing more optimizations (e.g., as in section 7.3.2.3). - -- The model for allowing reordering of operations across barriers (Section 7.3.2.2) is strongly related to the notion of fuzzy barriers <ref> [Gup89] </ref>. Our work is a straightforward generalization and formalization of the fuzzy barrier work that allows programmers to give the necessary information for the optimization.
Reference: [GHG91] <author> A. GUPTA, J. HENNESSY, K. GHARACHORLOO, T. MOWRY and W. WEBER, </author> <title> Comparative Evaluation of Latency Reducing and Tolerating Techniques, </title> <booktitle> Proc. 18th Ann. Intl. Symp. on Computer Architecture, </booktitle> <month> May </month> <year> 1991, </year> <pages> 254-263. </pages>
Reference-contexts: Section 2.4 summarizes performance studies of relaxed models. Section 2.5 describes other correctness criteria that are stronger than or similar to sequential consistency. For our work on debugging on relaxed models (Chapter 8), the only other related work is by Gharachorloo and Gibbons <ref> [GhG91] </ref>. Since this work is related only to Chapter 8, we postpone its discussion to that chapter. This chapter uses the terms preceding and following to indicate program order. 2.1. <p> These techniques include software or hardware based prefetching [GGH91b, MoG91] and the use of multiple contexts [ALK90, WeG89]. While these techniques help to improve the performance of sequentially consistent systems, they can also be used to aid the performance of relaxed systems <ref> [GHG91] </ref>. Observations similar to the above also apply to the compiler. Compilers need to perform a global data dependence analysis to determine when it might be safe to reorder memory operations without violating sequential consistency. <p> Gharachorloo et al. also propose hardware support that informs the programmer of when there might be data races in the program <ref> [GhG91] </ref>; however, further support is required to allow programmers to reason with sequential consistency to eliminate these data races. <p> We argue in [AHM91] how all the limitations of our technique are analogous to the limitations of dynamic techniques for sequentially consistent systems. 8.5. Related Work To the best of our knowledge, the only other work related to debugging on non-sequentially consistent systems is by Gharachorloo and Gibbons <ref> [GhG91] </ref>. This work proposes additional hardware support for a release consistent system that allows runtime detection of violations of sequential consistency (due to programs not properly labeled [GLL90]). The scheme imposes an additional constraint on release consistent (RCsc) hardware requiring acquires to wait for all previous operations. <p> The advantages of the above scheme compared to our work are the following. First, the debugging overhead for the scheme in <ref> [GhG91] </ref> is in the additional hardware support and the additional constraint on acquires. This is far less than that incurred by our tracing mechanism. <p> The disadvantages of the above scheme are the following. First, the above scheme cannot detect the first data races that may be responsible for the bug in the program. Therefore, unlike our scheme, the scheme in <ref> [GhG91] </ref> does not (yet) help programmers to reason with sequential consistency while debugging. The second disadvantage is that the debugging overhead due to the additional hardware constraint is incurred with the above scheme even after the program is debugged. <p> Interactions between relaxed memory models and other latency hiding or reducing techniques are also important to study further <ref> [GHG91] </ref>. Note that most of the optimizations due to memory models are essentially trying to exploit the implicit, fine-grained parallelism in a single thread.
Reference: [Gus92] <author> D. B. GUSTAVSON, </author> <title> The Scalable Coherent Interface and Related Standards Projects, </title> <booktitle> IEEE Micro 12, </booktitle> <month> 2 (February </month> <year> 1992), </year> <pages> 10-22. </pages>
Reference-contexts: The shared-memory abstraction can be provided either by hardware, software, or a combination of both. Pure hardware configurations include uniform [GGK83] and non-uniform access machines [ReT86]. Most configurations employ caches to reduce memory latency, and either use snooping [Bel85] or directory-based protocols <ref> [ASH88, Gus92, LLG90] </ref> to keep the caches up-to-date. Runtime software based systems typically use virtual memory hardware to trap on non-local references, and then invoke system software to handle the references [CBZ91, Li88].
Reference: [HeW90] <author> M. P. HERLIHY and J. M. WING, </author> <title> Linearizability: A Correctness Condition for Concurrent Objects, </title> <journal> ACM Trans. on Programming Languages and Systems 12, </journal> <month> 3 (July </month> <year> 1990), </year> <pages> 463-492. </pages>
Reference-contexts: In particular, since database transactions may involve multiple disk accesses, and hence take much longer than simple memory operations, database systems can afford to incur a much larger overhead for concurrency control. Herlihy and Wing propose a model of correctness, called linearizability, for systems with general concurrent objects <ref> [HeW90] </ref>. This model is proposed using the formalism of execution histories, where a history consists of invocation and response events for each operation (which may be a high-level operation involving several reads and writes of a concurrent object).
Reference: [HLR92] <author> M. D. HILL, J. R. LARUS, S. K. REINHARDT and D. A. WOOD, </author> <title> Cooperative Shared Memory: Software and Hardware Support for Scalable Multiprocessors, </title> <booktitle> Proc. 5th Intl. Conf. on Architectural Support for Programming Languages and Operating Systems, </booktitle> <month> October </month> <year> 1992, </year> <pages> 262-273. </pages>
Reference-contexts: For example, in systems using software-based cache coherence [BMW85, ChV88, PBG85], hardware provides a globally addressable memory, but the compiler is responsible for ensuring that shared data in a cache is up-to-date when required by its processor. The Alewife system [ALK90] and the Cooperative Shared Memory scheme <ref> [HLR92] </ref> manage cache-able shared-data in hardware for the common cases, but invoke runtime software for the less frequent cases. Building scalable, high-performance shared-memory systems that are also easy to program, however, has remained an elusive goal.
Reference: [HKT92] <author> S. HIRANANDANI, K. KENNEDY and C. TSENG, </author> <title> Compiling Fortran D for MIMD Distributed-Memory Machines, </title> <journal> Communications of the ACM 35, </journal> <month> 8 (August </month> <year> 1992), </year> <pages> 66-80. </pages>
Reference-contexts: Other software-based systems depend on compilers that detect non-local shared memory accesses in high-level code, and convert them into appropriate messages for the underlying message passing machine <ref> [HKT92] </ref>. Several systems employ a combination of hardware and software techniques. For example, in systems using software-based cache coherence [BMW85, ChV88, PBG85], hardware provides a globally addressable memory, but the compiler is responsible for ensuring that shared data in a cache is up-to-date when required by its processor.
Reference: [HKM90] <author> R. HOOD, K. KENNEDY and J. MELLOR-CRUMMEY, </author> <title> Parallel Program Debugging with On-the-fly Anomaly Detection, </title> <booktitle> Supercomputing '90, </booktitle> <month> November </month> <year> 1990, </year> <pages> 74-81. </pages>
Reference-contexts: Dynamic techniques, on the other hand, use a tracing mechanism to detect whether a particular sequentially consistent execution of a program actually exhibited a data race <ref> [AlP87, ChM91, DiS90, HKM90, NeM90, NeM91] </ref>. While dynamic techniques provide precise information about a single execution, they provide little information about other executions, a serious disadvantage. <p> We call executions that obey system-centric specifications of the data-race-free models as data-race-free executions. Existing dynamic data race detection techniques for sequentially consistent systems <ref> [AlP87, ChM91, DiS90, HKM90, NeM90, NeM91] </ref> instrument the program to record information about the memory operations of its execution. This information allows the happens-before relation for the execution to be constructed, thereby allowing the detection of data races. There are two approaches for recording and using this information.
Reference: [Hor92] <author> J. HORNING, </author> <type> Private Communication, </type> <institution> Digital Systems Research Center, </institution> <month> April </month> <year> 1992. </year>
Reference-contexts: In the absence of some form of a control requirement, however, the informal protocols (including the above) are not sufficient to guarantee correct communication in all cases <ref> [Hor92] </ref>. The specified protocols are similar to those for PLpc1 and PLpc2 programs, if we interpret certain uses of MB instructions as certain operation types.
Reference: [HuA90] <author> P. W. HUTTO and M. AHAMAD, </author> <title> Slow Memory: Weakening Consistency to Enhance Concurrency in Distributed Shared Memories, </title> <booktitle> Proc. 10th Intl. Conf. on Distributed Computing Systems, </booktitle> <year> 1990, </year> <pages> 302-311. </pages>
Reference-contexts: However, there are programs for which the conditions violate sequential consistency [AdH90a] and strong ordering was later redefined as the hardware quality of a multiprocessor which guarantees sequential consistency without further software aid [DuS90, Sch89]. Hutto and Ahamad have hierarchically characterized various weak models <ref> [HuA90] </ref>. <p> Thus, slow memory is weaker than all the models discussed so far. Although the solutions of some problems have been demonstrated on slow memory, programming general problems on such a model seems difficult. Hutto et al. also introduce the model of causal memory <ref> [AHJ90, HuA90] </ref> which ensures that any write that causally precedes a read is observed by the read. Causal precedence is a transitive relation established by program order or due to a read that returns the value of a write. Bitar has proposed the weakest memory access order [Bit92]. <p> In fact, since often interactions occurring due to a race between two writes are unintended, it may be difficult for programmers to provide the necessary information to preserve sequential consistency that require considering such interactions. The model of causality, however, only considers interactions due to write/read pairs <ref> [AHJ90, HuA90] </ref>. Should causality be considered as the base model? Our framework would still be applicable; only now programmers would provide information about the program behavior on causal systems and systems would ensure causality. Correspondingly, in Chapter 7, the program/conflict graph would be replaced by the program/causal-conflict graph.
Reference: [IBM83] <author> IBM, </author> <title> IBM System/370 Principles of Operation, Publication Number GA22-7000-9, File Number S370-01, </title> <month> May </month> <year> 1983. </year>
Reference-contexts: The second contribution of this thesis demonstrates the effectiveness of the SCNF approach by applying it to the optimizations of several commercially implemented and several academic models. We develop four SCNF models that unify the IBM 370 model <ref> [IBM83] </ref>, the SPARC V8 models of total store ordering and partial store ordering [SUN91], the Alpha model [Sit92], weak ordering [DSB86], processor consistency [GLL90], two flavors of release consistency (RCsc and RCpc) [GLL90], and lazy release consistency [KCZ92]. <p> The IBM 370 memory model <ref> [IBM83] </ref> can be viewed as a combination of a restricted total store ordering and the weak ordering models, although it was defined before either.
Reference: [Jef85] <author> D. R. JEFFERSON, </author> <title> Virtual Time, </title> <journal> ACM Trans. on Programming Languages and Systems 7, </journal> <month> 3 (July </month> <year> 1985), </year> <pages> 404-425. </pages>
Reference-contexts: Further, the optimizations of these schemes can be exploited by hardware (or the runtime system software), but cannot be exploited by compilers. A relevant scheme from distributed message passing systems is Jefferson's virtual time scheme using the time warp mechanism <ref> [Jef85] </ref>. This approach allows a processor to execute its operations optimistically; the run-time environment detects any consistency violations and rolls back the relevant processes to a consistent state. The optimistic scheduling in [Jef85] has parallels with the speculative execution scheme of [GGH91b]; however, rollbacks in [GGH91b] are local while those in <p> A relevant scheme from distributed message passing systems is Jefferson's virtual time scheme using the time warp mechanism <ref> [Jef85] </ref>. This approach allows a processor to execute its operations optimistically; the run-time environment detects any consistency violations and rolls back the relevant processes to a consistent state. The optimistic scheduling in [Jef85] has parallels with the speculative execution scheme of [GGH91b]; however, rollbacks in [GGH91b] are local while those in [Jef85] may have global effects. Similar ideas have also been used for parallelizing sequential programs written in mostly functional languages [Kni86, TiK88]. <p> The optimistic scheduling in <ref> [Jef85] </ref> has parallels with the speculative execution scheme of [GGH91b]; however, rollbacks in [GGH91b] are local while those in [Jef85] may have global effects. Similar ideas have also been used for parallelizing sequential programs written in mostly functional languages [Kni86, TiK88]. The parallel tasks are executed optimistically, using runtime (hardware or software) support to detect dependence violations and effect rollbacks.
Reference: [KCZ92] <author> P. KELEHER, A. L. COX and W. ZWAENEPOEL, </author> <title> Lazy Consistency for Software Distributed Shared Memory, </title> <booktitle> Proc. 19th Intl. Symp. on Computer Architecture, </booktitle> <year> 1992, </year> <pages> 13-21. </pages> - -- 
Reference-contexts: This thesis also does not provide any quantitative figures on the performance of the proposed memory models. Other researchers have already demonstrated that alternative memory models can give high performance <ref> [DKC93, GGH91a, GGH92, KCZ92, ZuB92] </ref>. This thesis addresses the problem of how to make the available performance potential usable for programmers and how to reason about models in a way that can result in better performance than that of previous models. 1.2. <p> We develop four SCNF models that unify the IBM 370 model [IBM83], the SPARC V8 models of total store ordering and partial store ordering [SUN91], the Alpha model [Sit92], weak ordering [DSB86], processor consistency [GLL90], two flavors of release consistency (RCsc and RCpc) [GLL90], and lazy release consistency <ref> [KCZ92] </ref>. The four SCNF models data-race-free-0, data-race-free-1, PLpc1, and PLpc2 exploit strictly increasing amounts of information from the programmer regarding the behavior of memory operations in the program. They use this information to allow increasingly many optimizations, including those of the above hardware-centric models, without violating sequential consistency. <p> Programs that obey these constraints are called properly labeled (PL) programs. Keleher et al. have proposed an implementation of release consistency, called lazy release consistency, for software-based shared virtual memory systems <ref> [KCZ92] </ref>. However, the implementation is a relaxation of release consistency since it does not require performing a memory access before the following release. Instead, it requires performing the access only at the next acquire to the same location as a following release, and only with respect to the acquiring processor. <p> Researchers at Rice University have performed two studies to compare release consistency and lazy release consistency implementations on a software-based shared virtual memory system <ref> [DKC93, KCZ92] </ref>. A key determinant of performance for such systems is the number of messages and amount of data exchanged. The first study is a trace-based simulation that examines the above metrics using invalidation and update based protocols [KCZ92]. <p> A key determinant of performance for such systems is the number of messages and amount of data exchanged. The first study is a trace-based simulation that examines the above metrics using invalidation and update based protocols <ref> [KCZ92] </ref>. The study shows that the number of messages and data transferred in the lazy release consistency schemes is consistently lower than for release consistency, especially for programs that show false sharing and frequent synchronization. The comparison between update and invalidate protocols is dependent on the type of sharing. <p> Another aggressive implementation called lazy release consistency has been proposed for software-based shared virtual memory systems <ref> [KCZ92] </ref>. This implementation does not obey weak ordering or release consistency, but does obey our sufficient conditions for data-race-free-0 described in the next chapter (assuming all read synchronizations are acquires and all write synchronizations are releases). - -- 4.4. <p> However, this complexity is manageable for software-based shared virtual memory systems, as demonstrated by recent implementations based on similar ideas <ref> [KCZ92] </ref>. (These are further discussed at the end of this section.) The following method is based on the proposal in [AdH90b]. Low-Level System-Centric Specification for Data Requirement. <p> First, more complex mechanisms can be used to ensure memory consistency in software-based systems since the implementation is in software. Second, reducing the number of messages is critical for performance in such systems <ref> [CBZ91, KCZ92] </ref>. For these reasons, our more aggressive system-centric specifications (Condition 5.10 and Methods 3 and 4) are less formidable to implement and can provide greater benefit on such systems. <p> A more aggressive implementation not allowed by previous hardware-centric models, but allowed by the specification of Condition 5.10 is the lazy release consistency implementation <ref> [KCZ92] </ref>. <p> As with lazy release consistency, software based shared virtual memory systems may also implement the high-level valid path requirement directly <ref> [KCZ92] </ref>. Control requirement. Our work so far does not make any assumptions on how the programmer can make the valid paths explicit to the system.
Reference: [Kni86] <author> T. KNIGHT, </author> <title> An Architecture for Mostly Functional Languages, </title> <booktitle> Proc. ACM Conf. on LISP and Functional Programming, </booktitle> <year> 1986, </year> <pages> 105-112. </pages>
Reference-contexts: The optimistic scheduling in [Jef85] has parallels with the speculative execution scheme of [GGH91b]; however, rollbacks in [GGH91b] are local while those in [Jef85] may have global effects. Similar ideas have also been used for parallelizing sequential programs written in mostly functional languages <ref> [Kni86, TiK88] </ref>. The parallel tasks are executed optimistically, using runtime (hardware or software) support to detect dependence violations and effect rollbacks. Work related to compiler optimizations includes that by Shasha and Snir [ShS88] and Midkiff et al. [MPC89].
Reference: [KNA93] <author> P. KOHLI, G. NEIGER and M. AHAMAD, </author> <title> A Characterization of Scalable Shared Memories, </title> <institution> GIT-CC-93/04, Georgia Institute of Technology, </institution> <month> January </month> <year> 1993. </year>
Reference-contexts: Some of the authors of this work have subsequently developed the work to include other models also, but do not provide formal proofs of equivalence between their formalizations and the original definitions of the models <ref> [KNA93] </ref>. 2.2.3. Release Consistency And Related Models The release consistency model combines and extends weak ordering and processor consistency [GLL90]. 7 The model exploits differences between different types of synchronization operations to provide higher performance potential than either weak ordering or processor consistency [GLL90]. <p> Others have used I/O automata, but express the model as constraints on the ``execution histories of processors'' generated by the automaton, rather than as the transition functions of the automaton itself <ref> [ABJ93, AtF92, KNA93] </ref>. The histories in the cited works indicate the state of memory as viewed by a processor, and thus represent systems similar to Collier's abstraction. Consequently, they suffer from the same limitation as Collier's abstraction.
Reference: [Kro81] <author> D. KROFT, </author> <title> Lockup-Free Instruction Fetch/Prefetch Cache Organization, </title> <booktitle> Proc. Eighth Symp. on Computer Architecture, </booktitle> <month> May </month> <year> 1981, </year> <pages> 81-87. </pages>
Reference-contexts: This prohibits the use of many performance enhancing hardware features commonly used in uniprocessors such as write buffers, out-of-order issue, pipelining (or overlapping) memory operations, and lockup-free caches <ref> [Kro81] </ref>. Analogously, in the absence of extensive data dependence analysis, compilers cannot reorder instructions that generate shared-memory operations and cannot allocate shared-memory locations to registers, again sacrificing performance enhancing techniques from uniprocessors.
Reference: [Lam78] <author> L. LAMPORT, </author> <title> Time, Clocks, and the Ordering of Events in a Distributed System, </title> <journal> Communications of the ACM 21, </journal> <month> 7 (July </month> <year> 1978), </year> <pages> 558-565. </pages>
Reference-contexts: The following method was used in our original definition of the data-race-free models [AdH90b, AdH93]. This method uses a happens-before relation to indicate when two memory operations are ordered by intervening synchronization operations. The relation is strongly related to the happened-before relation defined by Lamport for message passing systems <ref> [Lam78] </ref>, and - -- the approximate temporal order relation defined by Netzer and Miller for detecting races in shared-memory paral lel programs [NeM90]. Definition 4.1: Two memory operations conflict if they access the same location and at least one of them is a write [ShS88].
Reference: [Lam79] <author> L. LAMPORT, </author> <title> How to Make a Multiprocessor Computer That Correctly Executes Multiprocess Programs, </title> <journal> IEEE Trans. on Computers C-28, </journal> <month> 9 (September </month> <year> 1979), </year> <pages> 690-691. </pages>
Reference-contexts: The most commonly (and often implicitly) assumed programmer's model of memory for shared-memory systems is sequential consistency. This model was first formalized by Lamport <ref> [Lam79] </ref> and is a natural extension of the uniprocessor model. <p> r2 = 26, and for part (b), sequential consistency ensures that both r1 and r2 cannot be 0. hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh The view of the system provided by sequential consistency retains the simplicity of the uniprocessor model for programmers; however, implementing systems to provide this view often involves serious compromises in performance <ref> [DSB86, Lam79, MPC89] </ref>. Consider the code in part (b) of figure 1.2. Consider a system that uses write buffers to allow a read to bypass a write that precedes it in the program. <p> Similarly, in a system where processors can reorder their instructions, P1 could issue its read of Y before its write of X, again allowing both P1 and P2 to read the old values of Y and X respectively <ref> [DSB86, Lam79] </ref>. Note that neither processor has data dependencies among its instructions (because X and Y are different locations); therefore, simple interlock logic will not preclude either processor from issuing its second instruction before the first. <p> Thus, it is possible for P 2 to return the new value of flag but the old values of X and Y, violating sequential consistency <ref> [Lam79] </ref>. Analogously, if the compiler reorders memory operations or allocates memory locations in registers (e.g., allocates Y in a register in P 2 in figure 1.2 (b)), then again the non-sequentially consistent executions described above can occur [MPC89]. <p> Definition Sequential consistency was first defined by Lamport <ref> [Lam79] </ref> as follows. <p> As discussed in Chapter 1, systems that allow processors to reorder memory operations, or use write buffers to allow reads to bypass preceding writes, or overlap memory requests on the interconnection network can result in either or both of the non-sequentially consistent executions represented in figure 2.2 <ref> [DSB86, Lam79] </ref>. Analogously, compilers that reorder memory operations or allocate shared-memory locations in registers can violate sequential consistency [MPC89]. These violations are possible irrespective of whether systems have caches. In the presence of caches and a general interconnection network, sequential consistency imposes further constraints on hardware as follows. <p> Thus, this condition requires a processor to execute its memory operations one-at-a-time, and writes to appear atomic. Other researchers have proposed conditions for more restricted systems, which also obey the condition by Scheurich and Dubois. Lamport gives conditions for shared-memory systems with general interconnection networks, but no caches <ref> [Lam79] </ref>. Rudolph and Segall have developed two cache coherence protocols for bus-based systems. They formally prove the protocols obey their correctness criteria mentioned in Section 2.1.2; if we assume they implicitly considered processors that execute operations in program order, the protocols guarantee sequential consistency [RuS84].
Reference: [LHH91] <author> A. LANDIN, E. HAGERSTEN and S. HARIDI, </author> <title> Race-Free Interconnection Networks and Multiprocessor Consistency, </title> <booktitle> Proc. 18th Annual Intl. Symp. on Computer Architecture, </booktitle> <month> May </month> <year> 1991, </year> <pages> 106-115. </pages>
Reference-contexts: Such a network preserves the ordering of certain transactions, much like the ring network in Collier's work. Landin et al. identify a root node for every data item such that the sub-tree at the node contains all the copies of the data <ref> [LHH91] </ref>. They show that it is sufficient for a writing processor to wait only until the root node receives its request and sends an acknowledgement. Afek et al. propose a scheme called lazy caching for a cache-coherent bus-based system [ABM89, ABM93]. <p> All of the above schemes that allow a processor to overlap or reorder its memory accesses without software support, however, either require complex or restricted hardware (e.g., hardware prefetching and rollback for [GGH91b] and restricted networks for <ref> [Col84-92, LHH91] </ref>) or the gains are expected to be small (e.g., [AdH90a, LHH91]). Further, the optimizations of these schemes can be exploited by hardware (or the runtime system software), but cannot be exploited by compilers. <p> All of the above schemes that allow a processor to overlap or reorder its memory accesses without software support, however, either require complex or restricted hardware (e.g., hardware prefetching and rollback for [GGH91b] and restricted networks for [Col84-92, LHH91]) or the gains are expected to be small (e.g., <ref> [AdH90a, LHH91] </ref>). Further, the optimizations of these schemes can be exploited by hardware (or the runtime system software), but cannot be exploited by compilers. A relevant scheme from distributed message passing systems is Jefferson's virtual time scheme using the time warp mechanism [Jef85]. <p> Attiya and Welch [ACF93] and Lipton and Sandberg [LiS88] have derived bounds for the response time of memory operations on sequentially consistent implementations. (Attiya and Welch also derive bounds for the model of linearizability discussed in Section 2.5.) Finally, Collier [Col84-92], Shasha and Snir [ShS88], and Landin et al. <ref> [LHH91] </ref> describe a sufficient system condition for sequential consistency in graph theoretic terms that we will use for the framework of Chapter 7. 2.1.5. Why Relaxed Memory Models? We have seen that using uniprocessor optimizations such as write buffers and overlapped execution can violate sequential consistency. <p> Some of the serialization instructions also access memory; therefore, memory operations from such instructions are equivalent to synchronization operations of weak ordering. Landin et al. propose various schemes for race-free interconnection networks that exploit the transaction orderings preserved by the network <ref> [LHH91] </ref>. One of the schemes is more aggressive than definition 2.3 of processor consistency, but preserves the semantics of definition 2.3 [GAG93]. Other schemes are variants of processor consistency. Lipton and Sandberg have proposed the pipelined RAM model [LiS88, San90]. <p> This seems to obey the aggressive version of processor consistency proposed by Landin et al. <ref> [LHH91] </ref>, but is more aggressive than definition 2.3 since it allows writes of a single processor to be pipelined. <p> Our overall approach is to first identify a system-centric specification for sequential consistency, and then use it to characterize when an optimization will not violate sequential consistency. Section 7.1 gives such a specification based on several previous works <ref> [AdH92a, Col84-92, GAG92, LHH91, ShS88] </ref>. Section 7.2 uses the above specification to deduce the mapping between optimizations and information. Section 7.3 uses the mapping of Section 7.2 to examine several optimizations and common program constructs. <p> Section 7.1.2 makes four observations that modify the specification to reflect certain optimizations. Section 7.1.1 and Observation 1 of Section 7.1.2 follow directly from previous work by Shasha and Snir [ShS88], Collier [Col84-92], and Landin et al. <ref> [LHH91] </ref>. Observations 2 and 3 of Section 7.1.2 are extensions of similar concepts developed for the PLpc model [GAG92]. 7.1.1. A Simple Condition We use the notions of conflict order ( co ) and program/conflict graph defined earlier for a sequentially consistent execution and reproduced below for a general execution. <p> Section 7.5 will alleviate these restrictions. A simple system-centric specification for sequential consistency (with the finite speculation and write termination assumptions) is that the program/conflict graph of the execution should be acyclic (as also observed by others in various forms <ref> [Col84-92, LHH91, ShS88] </ref>). The following first illustrates the above condition with an example, and then gives a formal proof of correctness. Figure 7.1 (a) shows code in which processor P0 writes location x and then location y. <p> In this case, it requires only that P1's writes of A and B be executed only before P2 reads those locations. Previous work has shown how to pipeline P1's writes of A, B, and Flag1 while still ensuring the above <ref> [AdH90, AdH92b, BeZ91, Car91, Col84-92, - -- KCZ92, LHH91] </ref>. However, except for the work by Carlton [Car91], all the previous methods either require considerable hardware or software complexity or are applicable only to highly restricted (acyclic or ring) networks. <p> Eliminating Acknowledgements Finally, we consider eliminating acknowledgements for some writes with general interconnection networks. 27 Previous work has considered this optimization [Car91,Col84-92,LHH91], but either assumes restricted networks <ref> [Col84-92, LHH91] </ref>, or does not ensure sequential consistency (as in some schemes in [Col84-92, LHH91]), or requires restricted, lock-based synchronization [Car91]. Only the work by Carlton [Car91] indicates (without proof) how this optimization might be used for an SCNF model, but requires restricted, lock-based synchronization. <p> Eliminating Acknowledgements Finally, we consider eliminating acknowledgements for some writes with general interconnection networks. 27 Previous work has considered this optimization [Car91,Col84-92,LHH91], but either assumes restricted networks <ref> [Col84-92, LHH91] </ref>, or does not ensure sequential consistency (as in some schemes in [Col84-92, LHH91]), or requires restricted, lock-based synchronization [Car91]. Only the work by Carlton [Car91] indicates (without proof) how this optimization might be used for an SCNF model, but requires restricted, lock-based synchronization. Acknowledgements are needed only to indicate the completion of a write operation. <p> Our models do not impose the above restrictions. Compared to data-race-free-1 and PLpc, we have shown models that allow more writes to be non-atomic (e.g., the signal constructs of Section 7.3.2.1). The optimization of not requiring acknowledgements on certain accesses has been considered by Collier [Col84-92], Landin et al. <ref> [LHH91] </ref>, and Carlton [Car91]. Collier's scheme has the limitation described above for non-atomic writes. Landin et al. also required a restricted (race-free) network. Carlton assumes only lock-based synchronization and restricts locks and the data they protect to be in the same memory module.
Reference: [LCW93] <author> J. R. LARUS, S. CHANDRA and D. A. WOOD, CICO: </author> <title> A Practical Shared-Memory Programming Performance Model, </title> <type> Computer Sciences Technical Report #1171, </type> <institution> University of Wisconsin-Madison, </institution> <month> August </month> <year> 1993. </year> <title> Presented at the Workshop on Portability and Performance for Parallel Processing, </title> <month> July, </month> <year> 1993. </year> <note> To appear: </note> <author> Ferrante & Hey eds., </author> <title> Portability and Performance for Parallel Processors, </title> <publisher> John Wiley & Sons, Ltd.. </publisher>
Reference-contexts: For example, the shared-memory abstraction facilitates load balancing through processor independent data structures, allows pointer-based data structures, and allows the effective use of the entire system memory <ref> [LCW93, LeM92] </ref>. Shared-memory also permits decoupling the correctness of a program from its performance, allowing incremental tuning of programs for higher performance. The shared-memory abstraction can be provided either by hardware, software, or a combination of both. Pure hardware configurations include uniform [GGK83] and non-uniform access machines [ReT86].
Reference: [LeM92] <author> T. J. LEBLANC and E. P. MARKATOS, </author> <title> Shared Memory vs. Message Passing in Shared-Memory Multiprocessors, </title> <booktitle> 4th IEEE Symp. Parallel and Distributed Processing, </booktitle> <month> December </month> <year> 1992. </year>
Reference-contexts: For example, the shared-memory abstraction facilitates load balancing through processor independent data structures, allows pointer-based data structures, and allows the effective use of the entire system memory <ref> [LCW93, LeM92] </ref>. Shared-memory also permits decoupling the correctness of a program from its performance, allowing incremental tuning of programs for higher performance. The shared-memory abstraction can be provided either by hardware, software, or a combination of both. Pure hardware configurations include uniform [GGK83] and non-uniform access machines [ReT86].
Reference: [LeR91] <author> J. LEE and U. RAMACHANDRAN, </author> <title> Architectural Primitives for a Scalable Shared-Memory Multiprocessor, </title> <booktitle> Proc. 3rd Annual ACM Symp. Parallel Algorithms and Architectures, </booktitle> <month> July </month> <year> 1991, </year> <pages> 103-114. </pages>
Reference-contexts: implementations do not obey the release consistency model, but obey the related programmer-centric model of data-race-free-1 that will be developed later in this thesis. - -- The buffer consistency model for the Beehive system differs from release consistency mainly by not requiring reads to be performed before the following release <ref> [LeR91, ShR91] </ref>. Note that the Alpha simulates the different types of operations of release consistency (RCpc) more efficiently than those of weak ordering. A write immediately preceded by an MB is a release, a read immediately followed by an MB is an acquire, and all other operations are data operations. <p> They show that the lazy release consistency scheme is competitive with the snooping scheme in terms of performance, and recommend it because of its implementation simplicity and flexibility. Lee and Ramachandran use a probabilistic workload to compare buffer consistency with sequential consistency <ref> [LeR91] </ref>. Their study does not show significant performance gains with buffer consistency. 2.5. Correctness Criteria Stronger Than or Similar to Sequential Consistency This section discusses correctness criteria that are stronger than, or similar to, sequential consistency.
Reference: [LLG90] <author> D. LENOSKI, J. LAUDON, K. GHARACHORLOO, A. GUPTA and J. HENNESSY, </author> <title> The Directory-Based Cache Coherence Protocol for the DASH Multiprocessor, </title> <booktitle> Proc. 17th Annual Symposium on Computer Architecture, Computer Architecture News 18, </booktitle> <month> 2 (June </month> <year> 1990), </year> <pages> 148-159, </pages> <publisher> ACM. </publisher>
Reference-contexts: The shared-memory abstraction can be provided either by hardware, software, or a combination of both. Pure hardware configurations include uniform [GGK83] and non-uniform access machines [ReT86]. Most configurations employ caches to reduce memory latency, and either use snooping [Bel85] or directory-based protocols <ref> [ASH88, Gus92, LLG90] </ref> to keep the caches up-to-date. Runtime software based systems typically use virtual memory hardware to trap on non-local references, and then invoke system software to handle the references [CBZ91, Li88]. <p> To the best of our knowledge, there have not been any studies evaluating performance gains for the relaxed models due to compiler optimizations. Gharachorloo et al. compared the models of sequential consistency, processor consistency, weak ordering, and release consistency (RCpc) for a DASH-like architecture <ref> [LLG90] </ref> using an execution-driven instruction-level simulator [GGH91a]. The study assumed an invalidation-based cache coherence protocol and blocking reads. It showed that the relaxed models can hide most of the write latency and can perform upto 41% better than sequential consistency.
Reference: [Li88] <author> K. LI, IVY: </author> <title> A Shared Virtual Memory System for Parallel Computing, </title> <booktitle> Proc. Intl. Conf. on Parallel Processing, </booktitle> <year> 1988, </year> <pages> 94-101. </pages>
Reference-contexts: Most configurations employ caches to reduce memory latency, and either use snooping [Bel85] or directory-based protocols [ASH88, Gus92, LLG90] to keep the caches up-to-date. Runtime software based systems typically use virtual memory hardware to trap on non-local references, and then invoke system software to handle the references <ref> [CBZ91, Li88] </ref>. Other software-based systems depend on compilers that detect non-local shared memory accesses in high-level code, and convert them into appropriate messages for the underlying message passing machine [HKT92]. Several systems employ a combination of hardware and software techniques.
Reference: [Li89] <author> K. LI and P. HUDAK, </author> <title> Memory Coherence in Shared Virtual Memory Systems, </title> <journal> ACM Trans. on Computer Systems 7, </journal> <month> 4 (November </month> <year> 1989), </year> <pages> 321-359. </pages>
Reference-contexts: The system-centric specifications, however, are well-suited for other types of systems and can benefit them in other ways as well. Software-based shared virtual memory systems <ref> [CBZ91, Li89] </ref>, for example, have different tradeoffs compared to hardware cache-coherent systems. First, more complex mechanisms can be used to ensure memory consistency in software-based systems since the implementation is in software. Second, reducing the number of messages is critical for performance in such systems [CBZ91, KCZ92].
Reference: [LiS88] <author> R. J. LIPTON and J. S. SANDBERG, </author> <title> PRAM: A Scalable Shared Memory, </title> <type> Technical Report CS-Tech. </type> <institution> Rep.-180-88, Princeton University, </institution> <month> September </month> <year> 1988. </year>
Reference-contexts: While this work is applicable to general programs, it also depends on a global data dependence analysis. Attiya and Welch [ACF93] and Lipton and Sandberg <ref> [LiS88] </ref> have derived bounds for the response time of memory operations on sequentially consistent implementations. (Attiya and Welch also derive bounds for the model of linearizability discussed in Section 2.5.) Finally, Collier [Col84-92], Shasha and Snir [ShS88], and Landin et al. [LHH91] describe a sufficient system condition for sequential consistency in <p> One of the schemes is more aggressive than definition 2.3 of processor consistency, but preserves the semantics of definition 2.3 [GAG93]. Other schemes are variants of processor consistency. Lipton and Sandberg have proposed the pipelined RAM model <ref> [LiS88, San90] </ref>. The definition assumes that each processor is associated with a copy of its memory.
Reference: [LyT88] <author> N. A. LYNCH and M. R. TUTTLE, </author> <title> An Introduction to Input/Output Automata, </title> <type> Technical Report MIT/LCS/TM-373, </type> <institution> MIT Laboratory for Computer Science, </institution> <month> November </month> <year> 1988. </year>
Reference-contexts: Attiya et al. describe the hybrid consistency model as a formalization of models such as weak ordering [AtF92]. They use the formalism of a simplified I/O automaton to define their models <ref> [LyT88] </ref> (briefly described in Section 2.3). The model classifies memory operations as strong and weak. <p> Gibbons et al. and Afek et al. [ABM89, ABM93, GMG91, GiM92] use the formalism of an I/O automaton developed by Lynch and Tuttle <ref> [LyT88] </ref>. This formalism expresses the system in terms of a non-deterministic automaton. The formalism is powerful and precise, and does not suffer from the disadvantages of the above formalisms.
Reference: [MeS91] <author> J. M. MELLOR-CRUMMEY and M. L. SCOTT, </author> <title> Algorithms for Scalable Synchronization on Shared-Memory Multiprocessors, </title> <journal> ACM Transactions on Computer Systems, </journal> <month> February </month> <year> 1991, </year> <pages> 21-65. </pages>
Reference-contexts: Therefore, a program consisting of N ( 2) processors where each processor is executing the critical section code is data-race-free-1. useful for implementing a critical section, based on ticket locks <ref> [MeS91, ReK79] </ref>. The Fetch&Inc instruction is a read-modify-write that atomically reads and increments a memory location. The Dec instruction atomically decrements a memory location. The SyncRead is a read that is distinguished by hardware as synchronization. <p> The figure shows code for implementing a critical section and a sequentially consistent execution with two processors executing the code. This implementation of critical sections (and others based on similar mechanisms) have more desirable properties than the implementation in figure 6.3 (a) in terms of reduced contention and fairness <ref> [MeS91] </ref>. In the execution shown, all pairs of conflicting operations, such that at least one is a data operation, are ordered by the happens-before-1 relation; this is true of all sequentially consistent executions of the code. <p> A write due to a Fetch&Inc is pairable with a read due to another Fetch&Inc and a write due to a SyncWrite is pairable with a read due to a SyncRead. Also shown is code where N processors synchronize on a barrier <ref> [MeS91] </ref>, and its execution for N = 2. The variable local_flag is implemented in a local register of the processor and operations on it are not shown in the execution. <p> A more general definition allowing locks that employ Test&Test&Set [RuS84] and/or backoff techniques <ref> [MeS91] </ref> appears in [GAG92]. Chapter 7 provides a further generalization of this concept.
Reference: [MPC89] <author> S. MIDKIFF, D. PADUA and R. CYTRON, </author> <title> Compiling Programs with User Parallelism, </title> <booktitle> Proceedings of the Second Workshop on Languages and Compilers for Parallel Computing, </booktitle> <month> August </month> <year> 1989. </year>
Reference-contexts: r2 = 26, and for part (b), sequential consistency ensures that both r1 and r2 cannot be 0. hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh The view of the system provided by sequential consistency retains the simplicity of the uniprocessor model for programmers; however, implementing systems to provide this view often involves serious compromises in performance <ref> [DSB86, Lam79, MPC89] </ref>. Consider the code in part (b) of figure 1.2. Consider a system that uses write buffers to allow a read to bypass a write that precedes it in the program. <p> Analogously, if the compiler reorders memory operations or allocates memory locations in registers (e.g., allocates Y in a register in P 2 in figure 1.2 (b)), then again the non-sequentially consistent executions described above can occur <ref> [MPC89] </ref>. Chapter 2 will show that to implement sequential consistency on a general system and with a reasonable level of complexity, a processor must often execute its shared-memory operations one at a time and in the order specified by the program. <p> Analogously, compilers that reorder memory operations or allocate shared-memory locations in registers can violate sequential consistency <ref> [MPC89] </ref>. These violations are possible irrespective of whether systems have caches. In the presence of caches and a general interconnection network, sequential consistency imposes further constraints on hardware as follows. <p> The parallel tasks are executed optimistically, using runtime (hardware or software) support to detect dependence violations and effect rollbacks. Work related to compiler optimizations includes that by Shasha and Snir [ShS88] and Midkiff et al. <ref> [MPC89] </ref>. Although Shasha and Snir motivate their work for hardware optimizations (as discussed above), they suggest that compilers can apply optimizations that reorder memory operations to non-critical pairs of memory operations. The original algorithm given in [ShS88], however, is for straightline code that does not include branch operations. <p> Subsequently, Midkiff et al. have shown a detailed and more practical algorithm to apply the above work to programs with branches and loops <ref> [MPC89] </ref>, but this also requires a global data dependence analysis. <p> We believe it would be difficult for programmers to use the work in [ShS88] (and <ref> [MPC89] </ref>) because it would require programmers to look at all possible execution paths of a process, including those not possible with sequentially consistent executions. We overcame this problem by imposing the control requirement on implementations. <p> We exploit this knowledge through the notions of synchronization loops and self-ordered loops. These allow more optimizations by using more information about the program than just the conflict relation used in [ShS88]. (The work in <ref> [MPC89] </ref> partly exploits the above type of knowledge by fixing the orientation of conflict order arcs between synchronization operations whose order of execution is known.) Our work also leads to aggressive im - -- plementations (such as for data-race-free-0 in Chapter 5) that do not necessarily require the system to impose
Reference: [MoG91] <author> T. MOWRY and A. GUPTA, </author> <title> Tolerating Latency Through Software-Controlled Prefetching, </title> <journal> Journal of Parallel and Distributed Computing, </journal> <month> June </month> <year> 1991, </year> <pages> 87-106. </pages>
Reference-contexts: We also note that researchers have suggested techniques to tolerate the long memory latencies that are incurred with sequential consistency. These techniques include software or hardware based prefetching <ref> [GGH91b, MoG91] </ref> and the use of multiple contexts [ALK90, WeG89]. While these techniques help to improve the performance of sequentially consistent systems, they can also be used to aid the performance of relaxed systems [GHG91]. Observations similar to the above also apply to the compiler.
Reference: [Mul89] <author> S. MULLENDER, </author> <title> Distributed Systems, Chapter 15, </title> <publisher> Addison-Wesley, ACM Press, </publisher> <year> 1989. </year>
Reference-contexts: However, he does not say how hardware can identify these operations. Chapter 7 discusses his work in more detail. The memory model problem of shared-memory systems has analogues in distributed message passing systems. The approach taken by the ISIS distributed system is particularly relevant to our work <ref> [BiJ87, Mul89] </ref>. The analogous term for sequential consistency in this context is synchronous behavior, which imposes strict constraints on the ordering of message deliveries. Instead of imposing such constraints all the time, the ISIS system provides the programmer with a variety of primitives that guarantee different ordering constraints.
Reference: [NaT92] <author> B. NARENDRAN and P. TIWARI, </author> <title> Polynomial Root-Finding: Analysis and Computational Investigation of a Parallel Algorithm, </title> <booktitle> Proceedings of the 4th Annual Symposium on Parallel Algorithms and Architectures, </booktitle> <year> 1992, </year> <pages> 178-187. </pages>
Reference-contexts: To test our arguments above, we examined a set of programs from the SPLASH benchmark suite (Bar-nesHut, MP3D, and LocusRoute) [SWG92], and a program (Polyroots) for computing the roots of a large degree polynomial with integer coefficients of arbitrary precision <ref> [NaT92] </ref>. The programs are written in C and use the Argonne National Laboratory macro package to provide most synchronization and sharing primitives.
Reference: [NeM90] <author> R. H. B. NETZER and B. P. MILLER, </author> <title> Detecting Data Races in Parallel Program Executions, </title> <booktitle> Research Monographs in Parallel and Distributed Computing, </booktitle> <publisher> MIT Press, </publisher> <year> 1991. </year> <note> Also available as Proc. 3rd Workshop on Programming Languages and Compilers for Parallel Computing, </note> <month> August </month> <year> 1990. </year> - -- 
Reference-contexts: The relation is strongly related to the happened-before relation defined by Lamport for message passing systems [Lam78], and - -- the approximate temporal order relation defined by Netzer and Miller for detecting races in shared-memory paral lel programs <ref> [NeM90] </ref>. Definition 4.1: Two memory operations conflict if they access the same location and at least one of them is a write [ShS88]. Definition 4.2: Synchronization-order-0 ( so0 ): Let X and Y be two memory operations in a sequentially consistent execution. <p> Dynamic techniques, on the other hand, use a tracing mechanism to detect whether a particular sequentially consistent execution of a program actually exhibited a data race <ref> [AlP87, ChM91, DiS90, HKM90, NeM90, NeM91] </ref>. While dynamic techniques provide precise information about a single execution, they provide little information about other executions, a serious disadvantage. <p> We call executions that obey system-centric specifications of the data-race-free models as data-race-free executions. Existing dynamic data race detection techniques for sequentially consistent systems <ref> [AlP87, ChM91, DiS90, HKM90, NeM90, NeM91] </ref> instrument the program to record information about the memory operations of its execution. This information allows the happens-before relation for the execution to be constructed, thereby allowing the detection of data races. There are two approaches for recording and using this information.
Reference: [NeM91] <author> R. H. B. NETZER and B. P. MILLER, </author> <title> Improving the Accuracy of Data Race Detection, </title> <booktitle> Proc. 3rd ACM Symp. on Principles and Practice of Parallel Programming, </booktitle> <month> April </month> <year> 1991. </year>
Reference-contexts: Dynamic techniques, on the other hand, use a tracing mechanism to detect whether a particular sequentially consistent execution of a program actually exhibited a data race <ref> [AlP87, ChM91, DiS90, HKM90, NeM90, NeM91] </ref>. While dynamic techniques provide precise information about a single execution, they provide little information about other executions, a serious disadvantage. <p> We call executions that obey system-centric specifications of the data-race-free models as data-race-free executions. Existing dynamic data race detection techniques for sequentially consistent systems <ref> [AlP87, ChM91, DiS90, HKM90, NeM90, NeM91] </ref> instrument the program to record information about the memory operations of its execution. This information allows the happens-before relation for the execution to be constructed, thereby allowing the detection of data races. There are two approaches for recording and using this information.
Reference: [Pap86] <author> C. PAPADIMITRIOU, </author> <title> The Theory of Database Concurrency Control, </title> <publisher> Computer Science Press, </publisher> <address> Rockville, Maryland 20850, </address> <year> 1986. </year>
Reference-contexts: Their study does not show significant performance gains with buffer consistency. 2.5. Correctness Criteria Stronger Than or Similar to Sequential Consistency This section discusses correctness criteria that are stronger than, or similar to, sequential consistency. A common correctness criteria for concurrent databases is serializability <ref> [BeG81, Pap86] </ref>, which requires that transactions should appear as if they are executed one at a time in some sequential order. This is very similar to sequential consistency.
Reference: [Pat83-86] <author> J. PATEL, </author> <title> Mulitprocessor Cache Memories, </title> <institution> Seminar at Texas Instruments Research Labs (Dallas, </institution> <month> Dec. </month> <year> 1983), </year> <institution> Intel (Aloha, Oregon, </institution> <month> April </month> <year> 1984), </year> <institution> Digital Equipment (Hudson, </institution> <address> Mass., </address> <month> June </month> <year> 1984), </year> <institution> IBM (Yorktown, </institution> <month> Oct. </month> <year> 1984), </year> <institution> IBM (Poughkeepsie, </institution> <month> Aug. </month> <year> 1986). </year>
Reference-contexts: This definition allows the programmer to view the system as in an arbitrary number of time steps, the switch connects the memory to an arbitrary processor and the processor executes the next memory operation specified by the program. Alternatively, the system appears like a multipro-grammed uniprocessor <ref> [Pat83-86] </ref>. hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh P1 P2 P3 Pn MEMORY hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh a producer-consumer interaction where processor P 1 writes X and Y, and then sets a flag to indicate that it has completed its writes.
Reference: [PeL92a] <author> K. PETERSEN and K. LI, </author> <title> An Evaluation of Multiprocessor Cache Coherence Based On Virtual Memory Support, </title> <type> Technical Report Tech. </type> <institution> Rep.-401-92, Princeton University, </institution> <month> December </month> <year> 1992. </year>
Reference-contexts: Instead, it requires performing the access only at the next acquire to the same location as a following release, and only with respect to the acquiring processor. Petersen and Li have proposed alternative implementations of lazy release consistency using virtual memory support <ref> [PeL92a, PeL92b] </ref>. Bershad et al. [BeZ91] have proposed a relaxation of release consistency for the Midway software-based shared virtual memory system [BeZ91]. This model, called entry consistency, requires programmers to associate each data operation with a lock variable that should protect the operation. <p> Both studies find little improvement with weak ordering. Petersen and Li use trace-based simulation to study sequential consistency and lazy release consistency implemented using virtual memory hardware support and compare it with a snooping protocol on a bus-based system (they also give a few results for a crossbar system) <ref> [PeL92a, PeL92b] </ref>. They show that the lazy release consistency scheme is competitive with the snooping scheme in terms of performance, and recommend it because of its implementation simplicity and flexibility. Lee and Ramachandran use a probabilistic workload to compare buffer consistency with sequential consistency [LeR91].
Reference: [PeL92b] <author> K. PETERSEN and K. LI, </author> <title> Cache Coherence for Shared Memory Multiprocessors Based On Virtual Memory Support, </title> <type> Technical Report Tech. </type> <institution> Rep.-400-92, Princeton University, </institution> <month> December </month> <year> 1992. </year>
Reference-contexts: Instead, it requires performing the access only at the next acquire to the same location as a following release, and only with respect to the acquiring processor. Petersen and Li have proposed alternative implementations of lazy release consistency using virtual memory support <ref> [PeL92a, PeL92b] </ref>. Bershad et al. [BeZ91] have proposed a relaxation of release consistency for the Midway software-based shared virtual memory system [BeZ91]. This model, called entry consistency, requires programmers to associate each data operation with a lock variable that should protect the operation. <p> Both studies find little improvement with weak ordering. Petersen and Li use trace-based simulation to study sequential consistency and lazy release consistency implemented using virtual memory hardware support and compare it with a snooping protocol on a bus-based system (they also give a few results for a crossbar system) <ref> [PeL92a, PeL92b] </ref>. They show that the lazy release consistency scheme is competitive with the snooping scheme in terms of performance, and recommend it because of its implementation simplicity and flexibility. Lee and Ramachandran use a probabilistic workload to compare buffer consistency with sequential consistency [LeR91].
Reference: [PBG85] <author> G. F. PFISTER, W. C. BRANTLEY, D. A. GEORGE, S. L. HARVEY, W. J. KLEINFELDER, K. P. MCAULIFFE, E. A. MELTON, V. A. NORTON and J. WEISS, </author> <title> The IBM Research Parallel Processor Prototype (RP3): Introduction and Architecture, </title> <booktitle> International Conference on Parallel Processing, </booktitle> <month> August </month> <year> 1985, </year> <pages> 764-771. </pages>
Reference-contexts: Other software-based systems depend on compilers that detect non-local shared memory accesses in high-level code, and convert them into appropriate messages for the underlying message passing machine [HKT92]. Several systems employ a combination of hardware and software techniques. For example, in systems using software-based cache coherence <ref> [BMW85, ChV88, PBG85] </ref>, hardware provides a globally addressable memory, but the compiler is responsible for ensuring that shared data in a cache is up-to-date when required by its processor. <p> Rudolph and Segall have developed two cache coherence protocols for bus-based systems. They formally prove the protocols obey their correctness criteria mentioned in Section 2.1.2; if we assume they implicitly considered processors that execute operations in program order, the protocols guarantee sequential consistency [RuS84]. The RP3 system <ref> [BMW85, PBG85] </ref> is a cache-based system, where processor memory communication is via an Omega network, but the management of cache coherence for shared writable variables is entrusted to the software. <p> The semantics of a synchronization operation of weak ordering can be simulated by the Alpha by a memory operation that is immediately preceded and immediately followed by MB instructions. The RP3 system (described in Section 2.1.4) <ref> [BMW85, PBG85] </ref> provides a fence instruction similar to the MB instruction of Alpha, and can also be viewed as an implementation of weak ordering. The CRAY XMP also provides a similar instruction called the complete memory reference or CMR instruction [CRA82].
Reference: [ReK79] <author> D. P. REED and R. K. KANODIA, </author> <title> Synchronization with Eventcounts and Sequencers, </title> <journal> Communications of the ACM 22, </journal> <month> 2 (February </month> <year> 1979), </year> <pages> 115-123. </pages>
Reference-contexts: Therefore, a program consisting of N ( 2) processors where each processor is executing the critical section code is data-race-free-1. useful for implementing a critical section, based on ticket locks <ref> [MeS91, ReK79] </ref>. The Fetch&Inc instruction is a read-modify-write that atomically reads and increments a memory location. The Dec instruction atomically decrements a memory location. The SyncRead is a read that is distinguished by hardware as synchronization.
Reference: [ReT86] <author> R. RETTBERG and R. THOMAS, </author> <title> Contention is No Obstacle to Shared-memory Multiprocessors, </title> <journal> Communications of the ACM 29, </journal> <month> 12 (December </month> <year> 1986), </year> . 
Reference-contexts: Shared-memory also permits decoupling the correctness of a program from its performance, allowing incremental tuning of programs for higher performance. The shared-memory abstraction can be provided either by hardware, software, or a combination of both. Pure hardware configurations include uniform [GGK83] and non-uniform access machines <ref> [ReT86] </ref>. Most configurations employ caches to reduce memory latency, and either use snooping [Bel85] or directory-based protocols [ASH88, Gus92, LLG90] to keep the caches up-to-date. Runtime software based systems typically use virtual memory hardware to trap on non-local references, and then invoke system software to handle the references [CBZ91, Li88].
Reference: [RuS84] <author> L. RUDOLPH and Z. SEGALL, </author> <title> Dynamic Decentralized Cache Schemes for MIMD Parallel Processors, </title> <booktitle> Proc. Eleventh International Symposium on Computer Architecture, </booktitle> <month> June </month> <year> 1984, </year> <pages> 340-347. </pages>
Reference-contexts: Sequential Consistency vs. Cache Coherence Early bus-based systems with caches used cache coherence (or cache consistency) as the notion of correctness <ref> [CeF78, RuS84] </ref>. The definition, given by Censier and Feautrier, requires that a read should return the value deposited by the latest write to the same address [CeF78]. However, in systems with write buffers and general interconnection networks, the latest write to a location is not well-defined [DSB86]. <p> To model the final state of memory, a program can be considered to include reads of all memory locations after the rest of its execution. - -- operations where a read returns the value of the last write to the same location ordered before it by this total order <ref> [RuS84] </ref>. They do not, however, explicitly mention the requirement of program order. For general interconnection networks, an often accepted definition of cache coherence requires that only writes to the same location appear to be seen by all processors in the same order [GLL90]. <p> Rudolph and Segall have developed two cache coherence protocols for bus-based systems. They formally prove the protocols obey their correctness criteria mentioned in Section 2.1.2; if we assume they implicitly considered processors that execute operations in program order, the protocols guarantee sequential consistency <ref> [RuS84] </ref>. The RP3 system [BMW85, PBG85] is a cache-based system, where processor memory communication is via an Omega network, but the management of cache coherence for shared writable variables is entrusted to the software. <p> A more general definition allowing locks that employ Test&Test&Set <ref> [RuS84] </ref> and/or backoff techniques [MeS91] appears in [GAG92]. Chapter 7 provides a further generalization of this concept.
Reference: [SUN91] <author> SUN, </author> <title> The SPARC Architecture Manual, </title> <institution> Sun Microsystems Inc., </institution> <note> No. 800-199-12, Version 8, </note> <month> January </month> <year> 1991. </year>
Reference-contexts: We develop four SCNF models that unify the IBM 370 model [IBM83], the SPARC V8 models of total store ordering and partial store ordering <ref> [SUN91] </ref>, the Alpha model [Sit92], weak ordering [DSB86], processor consistency [GLL90], two flavors of release consistency (RCsc and RCpc) [GLL90], and lazy release consistency [KCZ92]. <p> The relevant results of the rest of this thesis are applicable to both the above definition and the revised version of the processor consistency model. The SPARC V8 architecture defined the total store ordering and partial store ordering models using a partial-order based formalism <ref> [SUN91] </ref>. These models are best understood when the system is viewed as a set of processors with a common memory module (as in figure 1.1), but where each processor has a write buffer. <p> The partial store ordering model is similar to total store ordering except that it ensures two writes of the same processor to different locations will appear in program order only if they are separated by a special STBAR (store barrier) instruction <ref> [SUN91] </ref>. Thus, partial store ordering allows writes of a processor to be executed in parallel. <p> The formalism developed for the SPARC V8 architecture was illustrated in the description of the total store ordering model in the previous section. This formalism requires that there be a partial order on all memory operations, where the order obeys certain axioms <ref> [SUN91, SFC91] </ref>. The advantage of this formalism is that it captures the write buffer interaction described above; the disadvantage, however, is that it does not adequately model the non-atomicity of writes. <p> For example, the authors of weak ordering informally cite the programs that use mutual exclusion through constructs such as critical sections [DuS90] and the authors of total store ordering and partial store ordering cite programs that use locks as described in Section 2.2.2 <ref> [SUN91] </ref>. In general, however, to formally ensure that a program is correct, further interpretation of the above informal statements may be required and the programmer has to reason with the specific optimizations of the model. <p> For example, memory operations can be distinguished based on the locations accessed. Alternately, analogous to the annotations of high-level languages, all operations could be assumed to be data by default unless their instructions were preceded by special prefix instructions; e.g., memory barrier instructions <ref> [SUN91, Sit92] </ref>. Another general mechanism that does not require adding extra instructions is to provide an extra bit for the op codes of all instructions that access memory to indicate whether the operations of the instructions are synchronization or data. <p> Examples that illustrate this also appear in <ref> [SUN91, SFC91] </ref>. The hardware-centric specifications of the data-race-free models allow the first optimization above if at least one of the read or write is a data operation and allow the second optimization for all data operations.
Reference: [SUN93] <author> SUN, </author> <title> The SPARC Architecture Manual, </title> <institution> Sun Microsystems Inc., </institution> <note> Version 9, </note> <year> 1993. </year>
Reference-contexts: protect all accesses to writable shared data will be portable across systems that obey total store ordering, partial store ordering, and sequential consistency (the lock routines need to be written to run correctly for partial store ordering). 5 The SPARC V9 architecture introduces an additional model called relaxed memory order <ref> [SUN93] </ref>. This model guarantees uniprocessor dependencies. To enforce any other program order, it provides a range of barrier instructions (like STBAR) that order various pairs of memory operations. The model ensures that two memory hhhhhhhhhhhhhhhhhh 5.
Reference: [San90] <author> J. SANDBERG, </author> <title> Design of the PRAM Network, </title> <type> Technical Report CS-Tech. </type> <institution> Rep.-254-90, Princeton University, </institution> <month> April </month> <year> 1990. </year>
Reference-contexts: One of the schemes is more aggressive than definition 2.3 of processor consistency, but preserves the semantics of definition 2.3 [GAG93]. Other schemes are variants of processor consistency. Lipton and Sandberg have proposed the pipelined RAM model <ref> [LiS88, San90] </ref>. The definition assumes that each processor is associated with a copy of its memory. <p> However, discussions of the pipelined RAM system seem to imply that writes by the same processor are intended to be seen in the same order by all other processors, and each processor must execute its operations in program order <ref> [San90] </ref>. This seems to obey the aggressive version of processor consistency proposed by Landin et al. [LHH91], but is more aggressive than definition 2.3 since it allows writes of a single processor to be pipelined.
Reference: [ScD87] <author> C. SCHEURICH and M. DUBOIS, </author> <title> Correct Memory Operation of Cache-Based Multiprocessors, </title> <booktitle> Proc. Fourteenth Annual Intl. Symp. on Computer Architecture, </booktitle> <address> Pittsburgh, PA, </address> <month> June </month> <year> 1987, </year> <pages> 234-243. </pages>
Reference-contexts: Implementations that Obey Sequential Consistency In their seminal work, Dubois, Scheurich, and Briggs have analyzed the problem of imposing sequential consistency in many different types of hardware systems <ref> [DSB86, DSB88, DuS90, ScD87, Sch89] </ref>. To reason with non-atomic memory operations in a cache-based system, they define the notion of an operation being performed with respect to a processor, performed, and globally performed. <p> A read is globally performed when it is performed and when the write whose value it reads is performed. Thus, a write or a read is globally performed when the value written or read is observed by all processors. Scheurich and Dubois state a sufficient condition for sequential consistency <ref> [ScD87, Sch89] </ref> for general systems (including systems with caches and systems with general interconnection networks that do not guarantee the ordering of any message pairs). <p> Similarly, hardware can actually execute more operations than specified by an execution as long as the effects of these extra operations are not seen. This is in contrast to many earlier specifications <ref> [AdH90b, DSB86, GLL90, ScD87] </ref> which impose constraints on the real time ordering of events. <p> The Synchronization Requirement The synchronization requirement is met by ensuring that synchronization operations interact like on a sequentially consistent system. The conditions for sequential consistency given by Dubois et al. <ref> [ScD87] </ref> motivate the following system-centric specification to meet the synchronization requirement. <p> Specifically, we consider a system with a general interconnection network which may or may not have caches. In the presence of caches, we assume some form of a hardware cache coherence protocol [ASH88]. In practice, such a system needs to adhere to the following to appear sequentially consistent <ref> [ScD87] </ref>. A processor does not issue a memory operation until the preceding operations complete. Thus, a processor must wait until a preceding read returns a value and a preceding write reaches memory.
Reference: [ScD88] <author> C. SCHEURICH and M. DUBOIS, </author> <title> Concurrent Miss Resolution in Multiprocessor Caches, </title> <booktitle> Proc. of the 1988 Intl. Conf. on Parallel Processing, </booktitle> <address> University Park PA, </address> <month> August, </month> <year> 1988, </year> <month> I-118-125. </month>
Reference: [Sch89] <author> C. E. SCHEURICH, </author> <title> Access Ordering and Coherence in Shared Memory Multiprocessors, </title> <type> Ph.D. Thesis, </type> <institution> Department of Computer Engineering, </institution> <type> Technical Report CENG 89-19, </type> <institution> University of Southern California, </institution> <month> May </month> <year> 1989. </year>
Reference-contexts: Implementations that Obey Sequential Consistency In their seminal work, Dubois, Scheurich, and Briggs have analyzed the problem of imposing sequential consistency in many different types of hardware systems <ref> [DSB86, DSB88, DuS90, ScD87, Sch89] </ref>. To reason with non-atomic memory operations in a cache-based system, they define the notion of an operation being performed with respect to a processor, performed, and globally performed. <p> A read is globally performed when it is performed and when the write whose value it reads is performed. Thus, a write or a read is globally performed when the value written or read is observed by all processors. Scheurich and Dubois state a sufficient condition for sequential consistency <ref> [ScD87, Sch89] </ref> for general systems (including systems with caches and systems with general interconnection networks that do not guarantee the ordering of any message pairs). <p> Again, this scheme preserves sequential consistency by - -- ensuring that all writes are seen in the same order by all processors. Scheurich proposes a similar scheme, but more informally, in his thesis <ref> [Sch89] </ref>. Other schemes that allow overlapping or reordering a processor's memory operations, but do not depend on specific network properties are as follows. <p> Weak Ordering And Related Models The weak ordering model was proposed by Dubois, Scheurich and Briggs <ref> [DSB86, DuS90, Sch89] </ref>. It is based on the intuition that the ordering of memory operations is important only with respect to synchronization (as opposed to data) operations. It requires programmers to distinguish between data and synchronization operations, and requires the system to recognize this distinction. <p> Different combinations of the various rules represent different memory models, many of which are weaker than sequential consistency. The issue of programming most of these models, however, has not been addressed. Scheurich proposes a model called concurrent consistency <ref> [Sch89] </ref>, which is defined to behave like a sequentially consistent system for all programs except those that explicitly test for sequential consistency or take access timings into consideration. The programs have not been further characterized. Scheurich states sufficient conditions for obeying concurrent consistency. <p> However, there are programs for which the conditions violate sequential consistency [AdH90a] and strong ordering was later redefined as the hardware quality of a multiprocessor which guarantees sequential consistency without further software aid <ref> [DuS90, Sch89] </ref>. Hutto and Ahamad have hierarchically characterized various weak models [HuA90]. <p> The concurrent consistency model <ref> [Sch89] </ref> ensures sequential consistency to all programs except those ``which explicitly test for sequential consistency or take access timings into consideration.'' The SCNF method also generalizes the concurrent consistency approach, but requires that the constraints on programs be formally verifiable by reasoning about the behavior of the program on sequentially consistent <p> The system can then treat the two types of operations differently, typically executing the data operations faster for higher performance. The model of weak - -- hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh A = 100; Valid = 1; while (Valid != 1) -;- ... = B; P2P1 hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh ordering by Dubois, Scheurich and Briggs <ref> [DSB86, DSB88, Sch89] </ref> is such an alternative model, where memory operations of a processor are not guaranteed to execute in program order unless they are separated by synchronization, and writes may appear to execute at different times to different processors.
Reference: [ShR91] <author> G. SHAH and U. RAMACHANDRAN, </author> <title> Towards Exploiting the Architectural Features of Beehive, </title> <type> Technical Report GIT-CC-91/51, </type> <institution> College of Computing, Georgia Institute of Technology, </institution> <month> November </month> <year> 1991. </year>
Reference-contexts: implementations do not obey the release consistency model, but obey the related programmer-centric model of data-race-free-1 that will be developed later in this thesis. - -- The buffer consistency model for the Beehive system differs from release consistency mainly by not requiring reads to be performed before the following release <ref> [LeR91, ShR91] </ref>. Note that the Alpha simulates the different types of operations of release consistency (RCpc) more efficiently than those of weak ordering. A write immediately preceded by an MB is a release, a read immediately followed by an MB is an acquire, and all other operations are data operations.
Reference: [ShS88] <author> D. SHASHA and M. SNIR, </author> <title> Efficient and Correct Execution of Parallel Programs that Share Memory, </title> <journal> ACM Trans. on Programming Languages and Systems 10, </journal> <month> 2 (April </month> <year> 1988), </year> <pages> 282-312. </pages>
Reference-contexts: Several conditions for implementing sequential consistency that allow a processor to issue a shared-memory operation without waiting for its preceding operation to be globally performed have also been proposed. Shasha and Snir have proposed a scheme that uses static software support <ref> [ShS88] </ref>. Their scheme statically identifies a minimal set of pairs of memory operations within a process (called critical pairs), such that the only delays required are for the second element in each pair to wait for the first to complete. <p> Similar ideas have also been used for parallelizing sequential programs written in mostly functional languages [Kni86, TiK88]. The parallel tasks are executed optimistically, using runtime (hardware or software) support to detect dependence violations and effect rollbacks. Work related to compiler optimizations includes that by Shasha and Snir <ref> [ShS88] </ref> and Midkiff et al. [MPC89]. Although Shasha and Snir motivate their work for hardware optimizations (as discussed above), they suggest that compilers can apply optimizations that reorder memory operations to non-critical pairs of memory operations. The original algorithm given in [ShS88], however, is for straightline code that does not include <p> to compiler optimizations includes that by Shasha and Snir <ref> [ShS88] </ref> and Midkiff et al. [MPC89]. Although Shasha and Snir motivate their work for hardware optimizations (as discussed above), they suggest that compilers can apply optimizations that reorder memory operations to non-critical pairs of memory operations. The original algorithm given in [ShS88], however, is for straightline code that does not include branch operations. The paper suggests extensions, but no detailed algorithm is given for more general programs. <p> Attiya and Welch [ACF93] and Lipton and Sandberg [LiS88] have derived bounds for the response time of memory operations on sequentially consistent implementations. (Attiya and Welch also derive bounds for the model of linearizability discussed in Section 2.5.) Finally, Collier [Col84-92], Shasha and Snir <ref> [ShS88] </ref>, and Landin et al. [LHH91] describe a sufficient system condition for sequential consistency in graph theoretic terms that we will use for the framework of Chapter 7. 2.1.5. <p> Intuitively, the distinguishing characteristic between data and synchronization operations is that in the execution order of any sequentially consistent execution, conflicting data operations are separated by conflicting synchronization operations. (Two operations conflict if they access the same location and at least one is a write <ref> [ShS88] </ref>.) Referring back to figure 4.1, consider the write and read of B. In every sequentially consistent execution of this program, the write and read of B will always be separated by the write and read of Valid in the execution order. <p> Definition 4.1: Two memory operations conflict if they access the same location and at least one of them is a write <ref> [ShS88] </ref>. Definition 4.2: Synchronization-order-0 ( so0 ): Let X and Y be two memory operations in a sequentially consistent execution. X so0 Y iff X and Y conflict, X and Y are distinguished as syn chronization operations to the system, and X xo Y in the execution. <p> Our overall approach is to first identify a system-centric specification for sequential consistency, and then use it to characterize when an optimization will not violate sequential consistency. Section 7.1 gives such a specification based on several previous works <ref> [AdH92a, Col84-92, GAG92, LHH91, ShS88] </ref>. Section 7.2 uses the above specification to deduce the mapping between optimizations and information. Section 7.3 uses the mapping of Section 7.2 to examine several optimizations and common program constructs. <p> A Condition for Sequential Consistency Section 7.1.1 first gives a simple system-centric specification for sequential consistency. Section 7.1.2 makes four observations that modify the specification to reflect certain optimizations. Section 7.1.1 and Observation 1 of Section 7.1.2 follow directly from previous work by Shasha and Snir <ref> [ShS88] </ref>, Collier [Col84-92], and Landin et al. [LHH91]. Observations 2 and 3 of Section 7.1.2 are extensions of similar concepts developed for the PLpc model [GAG92]. 7.1.1. <p> Section 7.5 will alleviate these restrictions. A simple system-centric specification for sequential consistency (with the finite speculation and write termination assumptions) is that the program/conflict graph of the execution should be acyclic (as also observed by others in various forms <ref> [Col84-92, LHH91, ShS88] </ref>). The following first illustrates the above condition with an example, and then gives a formal proof of correctness. Figure 7.1 (a) shows code in which processor P0 writes location x and then location y. <p> Thus, the system can exploit more parallelism and execute more writes non-atomically, because it need not constrain operations on the ordering paths identified by the following observations. Observation 1. The first observation (also made in <ref> [ShS88] </ref> in a different form) is that if there are multiple ordering paths from X to Y, then the system needs to ``be careful'' when executing the operations of only one of those ordering paths. <p> Observation 3 is that self-ordered reads will always execute after the correct write as long as they execute after the necessary previous writes that do not write an exit value. Thus, even if there hhhhhhhhhhhhhhhhhh 25. The term ``critical'' is inspired from the work by Shasha and Snir <ref> [ShS88] </ref>. 26. We will later use information from only sequentially consistent executions to identify the critical paths of any execution (see Section 7.2). Part (2) of definition 7.11 then ensures that a synchronization loop does not terminate too early as follows. Consider the operations W and Y in part (2). <p> Relation with Previous Work This section discusses the relationship of our work with several previous studies. 7.6.1. Relation with Work by Shasha and Snir Shasha and Snir have proposed a compile time algorithm that allows parallel and out of program order execution of memory operations <ref> [ShS88] </ref>. Their scheme aims to statically identify a minimal set of pairs of operations within a process, such that delaying the issue of one of the elements of each pair until the completion of the other is sufficient for sequential consistency. <p> We believe it would be difficult for programmers to use the work in <ref> [ShS88] </ref> (and [MPC89]) because it would require programmers to look at all possible execution paths of a process, including those not possible with sequentially consistent executions. We overcame this problem by imposing the control requirement on implementations. <p> We exploit this knowledge through the notions of synchronization loops and self-ordered loops. These allow more optimizations by using more information about the program than just the conflict relation used in <ref> [ShS88] </ref>. (The work in [MPC89] partly exploits the above type of knowledge by fixing the orientation of conflict order arcs between synchronization operations whose order of execution is known.) Our work also leads to aggressive im - -- plementations (such as for data-race-free-0 in Chapter 5) that do not necessarily require <p> Finally, we also examine the optimizations of executing writes non-atomically and eliminating acknowledgements in cache-based systems; the work in <ref> [ShS88] </ref> implicitly considers systems with only one copy of a line (i.e., writes are atomic). 7.6.2. Relation with Work by Collier Collier has developed a general framework to define memory models using his abstraction of a shared-memory system (described in Chapters 2 and 5) [Col84-92].
Reference: [SFC91] <author> P. S. SINDHU, J. FRAILONG and M. CEKLEOV, </author> <title> Formal Specification of Memory Models, </title> <type> Technical Report CSL-91-11 [P91-00112], </type> <institution> Xerox Corporation, </institution> <month> December </month> <year> 1991. </year>
Reference-contexts: The formalism developed for the SPARC V8 architecture was illustrated in the description of the total store ordering model in the previous section. This formalism requires that there be a partial order on all memory operations, where the order obeys certain axioms <ref> [SUN91, SFC91] </ref>. The advantage of this formalism is that it captures the write buffer interaction described above; the disadvantage, however, is that it does not adequately model the non-atomicity of writes. <p> Examples that illustrate this also appear in <ref> [SUN91, SFC91] </ref>. The hardware-centric specifications of the data-race-free models allow the first optimization above if at least one of the read or write is a data operation and allow the second optimization for all data operations.
Reference: [SWG92] <author> J. P. SINGH, W. WEBER and A. GUPTA, </author> <title> SPLASH: Stanford Parallel Applications for Shared-Memory, Computer Architecture News 20, </title> <month> 1 (March </month> <year> 1992), </year> <pages> 5-44. </pages>
Reference-contexts: To test our arguments above, we examined a set of programs from the SPLASH benchmark suite (Bar-nesHut, MP3D, and LocusRoute) <ref> [SWG92] </ref>, and a program (Polyroots) for computing the roots of a large degree polynomial with integer coefficients of arbitrary precision [NaT92]. The programs are written in C and use the Argonne National Laboratory macro package to provide most synchronization and sharing primitives.
Reference: [Sit92] <author> R. </author> <title> SITES, EDITOR, Alpha Architecture Reference Manual, </title> <year> 1992. </year>
Reference-contexts: We develop four SCNF models that unify the IBM 370 model [IBM83], the SPARC V8 models of total store ordering and partial store ordering [SUN91], the Alpha model <ref> [Sit92] </ref>, weak ordering [DSB86], processor consistency [GLL90], two flavors of release consistency (RCsc and RCpc) [GLL90], and lazy release consistency [KCZ92]. The four SCNF models data-race-free-0, data-race-free-1, PLpc1, and PLpc2 exploit strictly increasing amounts of information from the programmer regarding the behavior of memory operations in the program. <p> The Alpha memory model is specified in terms of partial orders on memory operations and constraints on those partial orders <ref> [Sit92] </ref>. Although the specification methodology for the Alpha model is very different from that of weak ordering, the two models are semantically similar. <p> For example, memory operations can be distinguished based on the locations accessed. Alternately, analogous to the annotations of high-level languages, all operations could be assumed to be data by default unless their instructions were preceded by special prefix instructions; e.g., memory barrier instructions <ref> [SUN91, Sit92] </ref>. Another general mechanism that does not require adding extra instructions is to provide an extra bit for the op codes of all instructions that access memory to indicate whether the operations of the instructions are synchronization or data.
Reference: [Smi82] <author> B. SMITH, </author> <title> Architecture and Applications of the HEP Multiprocessor Computer System, </title> <booktitle> Proc. of the Int. </booktitle> <institution> Soc. for Opt. Engr., </institution> <year> 1982, </year> <pages> 241-248. </pages>
Reference-contexts: This would specifically be useful for users of asynchronous algorithms which need very weak guarantees to converge on the correct answer. The relaxed memory order model for SPARC V9 is an example of such a model. Further, with more powerful hardware primitives such as full/empty bits <ref> [Smi82] </ref> that enforce implicit synchronization, a system with no model at all may be viable [Goo93]. Finally, we could continue to use SCNF, but also provide an alternative, high-level system-centric specification.
Reference: [SwS86] <author> P. SWEAZEY and A. J. SMITH, </author> <title> A Class of Compatible Cache Consistency Protocols and their Support by the IEEE Futurebus, </title> <booktitle> Proc. Thirteenth International Symposium on Computer Architecture, </booktitle> <address> Tokyo, - -- Japan, </address> <month> June </month> <year> 1986, </year> <pages> 414-423. </pages>
Reference-contexts: We define a generic memory model and a system-centric specification for the generic model in terms of this characteristic. Our common characterization for memory models is similar to the MOESI <ref> [SwS86] </ref> and Dir i [B | NB] [ASH88] characterizations of cache coherence protocols that unified several seemingly disparate protocols and exposed the design space for more protocols.
Reference: [Tay83a] <author> R. N. TAYLOR, </author> <title> Complexity of Analyzing the Synchronization Structure of Concurrent Programs, </title> <journal> Acta Informatica 19(1983), </journal> <pages> 57-84. </pages>
Reference-contexts: In general, static analysis must be conservative and slow, because detecting data races is undecidable for arbitrary programs [Ber66] and is NP-complete for even very restricted classes of programs (e.g., those containing no branches) <ref> [Tay83a] </ref>. Dynamic techniques, on the other hand, use a tracing mechanism to detect whether a particular sequentially consistent execution of a program actually exhibited a data race [AlP87, ChM91, DiS90, HKM90, NeM90, NeM91].
Reference: [Tay83b] <author> R. N. TAYLOR, </author> <title> A General-Purpose Algorithm for Analyzing Concurrent Programs, </title> <journal> Communications of the ACM 26, </journal> <month> 5 (May </month> <year> 1983), </year> <pages> 362-376. </pages>
Reference-contexts: Static techniques perform a compile-time analysis of the program text to detect a superset of all possible data races that could potentially occur in all possible sequentially consistent executions of the program <ref> [BaK89, Tay83b] </ref>. In general, static analysis must be conservative and slow, because detecting data races is undecidable for arbitrary programs [Ber66] and is NP-complete for even very restricted classes of programs (e.g., those containing no branches) [Tay83a].
Reference: [TiK88] <author> P. TINKER and M. KATZ, </author> <title> Parallel Execution of Sequential Scheme with ParaTran, </title> <booktitle> Proc. ACM Conf. on LISP and Functional Programming, </booktitle> <month> July </month> <year> 1988, </year> <pages> 28-39. </pages>
Reference-contexts: The optimistic scheduling in [Jef85] has parallels with the speculative execution scheme of [GGH91b]; however, rollbacks in [GGH91b] are local while those in [Jef85] may have global effects. Similar ideas have also been used for parallelizing sequential programs written in mostly functional languages <ref> [Kni86, TiK88] </ref>. The parallel tasks are executed optimistically, using runtime (hardware or software) support to detect dependence violations and effect rollbacks. Work related to compiler optimizations includes that by Shasha and Snir [ShS88] and Midkiff et al. [MPC89].
Reference: [ToH90] <author> J. TORELLAS and J. HENNESSY, </author> <title> Estimating the Performance Advantages of Relaxing Consistency in a Shared-Memory Multiprocessor, </title> <booktitle> Intl. Conference on Parallel Processing, </booktitle> <year> 1990. </year>
Reference-contexts: The following studies examine smaller bus-based systems and/or use probabilistic work-loads. Baer and Zucker have compared sequential consistency and weak ordering on a bus-based system [BaZ91] using simulation. Torellas and Hennessy use analytical models to compare the same models on a DASH-like system <ref> [ToH90] </ref>. Both studies find little improvement with weak ordering.
Reference: [WeG89] <author> W. WEBER and A. GUPTA, </author> <title> Exploring the Benefits of Multiple Hardware Contexts in a Multiprocessor Architecture: Preliminary Results, </title> <booktitle> The 16th. Annual Intl. Symp. on Computer Architecture, </booktitle> <month> May </month> <year> 1989, </year> <pages> 273-280. </pages>
Reference-contexts: We also note that researchers have suggested techniques to tolerate the long memory latencies that are incurred with sequential consistency. These techniques include software or hardware based prefetching [GGH91b, MoG91] and the use of multiple contexts <ref> [ALK90, WeG89] </ref>. While these techniques help to improve the performance of sequentially consistent systems, they can also be used to aid the performance of relaxed systems [GHG91]. Observations similar to the above also apply to the compiler.
Reference: [WiL92] <author> A. W. WILSON and R. P. LAROWE, </author> <title> Hiding Shared Memory Reference Latency on the Galactica Net Distributed Shared Memory Architecture, </title> <journal> Journal of Parallel and Distributed Computing 15, </journal> <month> 4 (August </month> <year> 1992), </year> <pages> 351-367. </pages>
Reference-contexts: A processor can use the updated value from its cache only after receiving an acknowledgement of the second phase. (For a system employing such a protocol, see <ref> [WiL92] </ref>.) Thus, sequential consistency, in general, restricts the use of several optimizations that can hide and tolerate memory latency and involves additional network traffic. 2.1.4.
Reference: [Zuc91] <author> R. N. ZUCKER, </author> <title> A Study of Weak Consistency Models, Dissertation Proposal, </title> <institution> University of Washington, </institution> <year> 1991. </year>
Reference-contexts: The delay on the write of Valid imposed by the two techniques described above, however, is not necessary to maintain sequential consistency (as also observed by Zucker <ref> [Zuc91, Zuc92] </ref>). We discuss two aggressive implementation proposals for data-race-free-0 that do not impose the above delay, and are not allowed by weak ordering (and release consistency).
Reference: [Zuc92] <author> R. N. ZUCKER, </author> <title> Relaxed Consistency And Synchronization in Parallel Processors, </title> <type> Ph.D. Thesis, Technical Report 92-12-05, </type> <institution> University of Washington, </institution> <month> December </month> <year> 1992. </year>
Reference-contexts: Another form of delayed consistency, referred to as loose coherence for Munin [BCZ90] and also discussed in [DWB91], buffers updates of a processor until the following synchronization operation of the same processor. This retains the memory model of release consistency. Zucker proposes implementations for systems using software cache coherence <ref> [Zuc92] </ref>. <p> Zucker and Baer have studied sequential consistency, weak ordering, and release consistency (RCpc) with non-blocking reads, but where a processor is stalled when the value of an outstanding read is needed <ref> [Zuc92, ZuB92] </ref>. Again an execution-driven instruction-level simulator was used. The architecture studied was a dance-hall system with processors connected to memory through an omega network. The study involved examining the benefits of the relaxed models with varying cache and line sizes. <p> The delay on the write of Valid imposed by the two techniques described above, however, is not necessary to maintain sequential consistency (as also observed by Zucker <ref> [Zuc91, Zuc92] </ref>). We discuss two aggressive implementation proposals for data-race-free-0 that do not impose the above delay, and are not allowed by weak ordering (and release consistency). <p> The specification of Condition 5.10 is also well-suited for software-based cache-coherent systems [ChV88, Che90, CKM88]. Zucker explains how the hardware-centric models proposed so far cannot be used on software-based cache-coherent systems but our specifications lead to efficient implementations <ref> [Zuc92] </ref>. 5.3.2. The Synchronization Requirement The synchronization requirement is met by ensuring that synchronization operations interact like on a sequentially consistent system. The conditions for sequential consistency given by Dubois et al. [ScD87] motivate the following system-centric specification to meet the synchronization requirement.
Reference: [ZuB92] <author> R. N. ZUCKER and J. BAER, </author> <title> A Performance Study of Memory Consistency Models, </title> <booktitle> 19th Intl. Symp. on Computer Architecture, </booktitle> <year> 1992, </year> <pages> 2-12. </pages> - -- 
Reference-contexts: Many of these models have been motivated by hardware optimizations and are specified in terms of these hardware optimizations. This hardware-centric nature of the models leads to substantial performance increases <ref> [GGH91a, GGH92, ZuB92] </ref>, but at the cost of losing significant advantages of sequential consistency for the programmer. The first disadvantage for programmers of hardware-centric models is that the programmer's view of the system is more complex than with sequential consistency. <p> This thesis also does not provide any quantitative figures on the performance of the proposed memory models. Other researchers have already demonstrated that alternative memory models can give high performance <ref> [DKC93, GGH91a, GGH92, KCZ92, ZuB92] </ref>. This thesis addresses the problem of how to make the available performance potential usable for programmers and how to reason about models in a way that can result in better performance than that of previous models. 1.2. <p> Zucker and Baer have studied sequential consistency, weak ordering, and release consistency (RCpc) with non-blocking reads, but where a processor is stalled when the value of an outstanding read is needed <ref> [Zuc92, ZuB92] </ref>. Again an execution-driven instruction-level simulator was used. The architecture studied was a dance-hall system with processors connected to memory through an omega network. The study involved examining the benefits of the relaxed models with varying cache and line sizes. <p> Third, considering performance, earlier detailed hardware simulation studies for release consistency (RCpc) and weak ordering have reported upto 41% speedup in hardware (compared to sequential consistency) <ref> [GGH91a, ZuB92] </ref>. These studies do not fully exploit non-blocking reads; a study that does exploit such reads indicates better performance, but is trace-based [GGH92]. In most cases, weak ordering performs as well as release consistency (RCpc), but there are some cases where it gives significantly worse performance.
References-found: 130

