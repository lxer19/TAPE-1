URL: http://www.iro.umontreal.ca/labs/neuro/pointeurs/rnn_review93.ps
Refering-URL: http://www.iro.umontreal.ca/labs/neuro/proc.html
Root-URL: http://www.iro.umontreal.ca
Title: Recurrent Neural Networks for Adaptive Temporal Processing  
Author: Yoshua Bengio Paolo Frasconi Marco Gori 
Date: July 5, 1993  
Note: Paper in Progress  
Abstract: Comparing to other existing approaches to deal with temporal data, the interest in recurrent networks is mostly due to their capability of implementing adaptive long-term memories. In spite of these potentialities, optimal training of parametric dynamical systems is not an easy task. In this paper we focus on sequence processing tasks such as production and classification. After reviewing some approaches proposed in the literature we describe the difficulties that are encountered in training recurrent networks and alternatives that can be pursued to circumvent these problems. In particular we consider alternative optimization techiques that can be better suited to deal with long-term dependencies, and prior knowledge injection techniques that may simplify the learning task in such situations. We finally discuss the implications that achievements in recurrent networks research might have for the technology of adaptive systems and artificial neural networks. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> J. </author> <title> Hopfield, "Neural networks and physical systems with emergent collective computational abilities," </title> <booktitle> Proceedings of the National Academy of Sciences, USA, </booktitle> <volume> vol. 79, </volume> <year> 1982. </year> <note> Reprinted in [59]. </note>
Reference-contexts: A recurrent neural network is characterized by cycles in its graph. This property introduces in its behavior the temporal dimension. Two basic types of dynamics are distinguished: autonomous converging dynamics (fixed input), and non-autonomous non-converging dynamics (time-varying input). Hopfield networks <ref> [1] </ref>, Boltzmann machines [2] and fixed point dynamics back-propagation networks [3] are found in the first category. We propose in section 2 a brief overview of this kind of recurrent network. <p> Hopfield networks <ref> [1] </ref> were the first remarkable example of this class of architecture.
Reference: [2] <author> G. Hinton, T. Sejnowski, and D. Ackley, </author> <title> "Boltzmann machines: Constraint satisfaction networks that learn," </title> <type> Tech. Rep. </type> <institution> TR-CMU-CS-84-119, Carnegie-Mellon University, Dept. of Computer Science, </institution> <year> 1984. </year>
Reference-contexts: A recurrent neural network is characterized by cycles in its graph. This property introduces in its behavior the temporal dimension. Two basic types of dynamics are distinguished: autonomous converging dynamics (fixed input), and non-autonomous non-converging dynamics (time-varying input). Hopfield networks [1], Boltzmann machines <ref> [2] </ref> and fixed point dynamics back-propagation networks [3] are found in the first category. We propose in section 2 a brief overview of this kind of recurrent network. Networks of the second category have time-varying inputs and they can thus be used to map input sequences to output sequences.
Reference: [3] <author> F. Pineda, </author> <title> "Recurrent back-propagation and the dynamical approach to adaptive neural computation," </title> <journal> Neural Computation, </journal> <volume> vol. 1, </volume> <pages> pp. 161-172, </pages> <year> 1989. </year>
Reference-contexts: This property introduces in its behavior the temporal dimension. Two basic types of dynamics are distinguished: autonomous converging dynamics (fixed input), and non-autonomous non-converging dynamics (time-varying input). Hopfield networks [1], Boltzmann machines [2] and fixed point dynamics back-propagation networks <ref> [3] </ref> are found in the first category. We propose in section 2 a brief overview of this kind of recurrent network. Networks of the second category have time-varying inputs and they can thus be used to map input sequences to output sequences.
Reference: [4] <author> T. Sejnowski and C. Rosenberg, </author> <title> "Parallel networks that learn to pronounce english text," </title> <journal> Complex Systems, </journal> <volume> vol. 1, </volume> <pages> pp. 145-168, </pages> <year> 1987. </year>
Reference: [5] <author> A. Waibel, T. Hanazawa, G. Hinton, K. Shikano, and K. Lang, </author> <title> "Phoneme recognition using time-delay neural networks," </title> <journal> IEEE Transactions on Acoustics, Speech, and Signal Processing, </journal> <volume> vol. 37, </volume> <pages> pp. 328-339, </pages> <year> 1989. </year>
Reference: [6] <author> Y. Le Cun, B. Boser, J. Denker, D. Henderson, R. Howard, W. Hubbard, and L. Jackel, </author> <title> "Backpropagation applied to handwritten zip code recognition," </title> <journal> Neural Computation, </journal> <volume> vol. 1, </volume> <pages> pp. 541-551, </pages> <year> 1989. </year>
Reference-contexts: This strategy was applied successfully in handwritten character recognition <ref> [6] </ref>. A simple application of this idea yielded Time-Delay Neural Networks (TDNN) [13, 14], in which hidden units perform one-dimensional convolutions (with finite support) in the temporal direction.
Reference: [7] <author> H. Bourlard and C. Wellekens, </author> <title> "Speech pattern discrimination and multi-layered perceptrons," </title> <booktitle> Computer Speech and Language, </booktitle> <volume> vol. 3, </volume> <pages> pp. 1-19, </pages> <year> 1989. </year>
Reference: [8] <author> A. Corana, M. Marchesi, C. Martini, and S. Ridella, </author> <title> "Minimizing multimodal functions of continuous variables with the simulated annealing algorithm," </title> <journal> ACM Transactions on Mathematical Software, </journal> <volume> vol. 13, </volume> <pages> pp. 262-280, </pages> <month> Sept. </month> <year> 1987. </year>
Reference-contexts: Hopfield networks turn out to be very useful also for solving constraint satisfaction problems. One problem that typically arises is that of discovering sub-optimal solutions. In order to cope with this serious limitation, techniques like simulating annealing <ref> [8] </ref> turns out to be useful. A parallel implementation of the simulating annealing process can also take place directly in a stochastic neural network.
Reference: [9] <author> D. Ackley, G. Hinton, and T. Sejnowski, </author> <title> "A learning algorithm for boltzmann machines," </title> <journal> Cognitive Science, </journal> <volume> vol. 9, </volume> <year> 1985. </year> <note> Reprinted in [59]. </note>
Reference-contexts: Stochastic networks of this kind are referred to as Bolzmann machines <ref> [9] </ref> 2.2 Recurrent Backpropagation After the diffusion of Backpropagation algorithm for feedforward networks by Rumelhart et al. [10] many researchers have explored the possibility of extending Backpropagation to the case of networks with recurrent connections.
Reference: [10] <author> D. Rumelhart, G. Hinton, and R. Williams, </author> <title> "Learning representations by back-propagating errors," </title> <journal> Nature, </journal> <volume> vol. 323, </volume> <pages> pp. 533-536, </pages> <year> 1986. </year> <note> Reprinted in [59]. </note>
Reference-contexts: Stochastic networks of this kind are referred to as Bolzmann machines [9] 2.2 Recurrent Backpropagation After the diffusion of Backpropagation algorithm for feedforward networks by Rumelhart et al. <ref> [10] </ref> many researchers have explored the possibility of extending Backpropagation to the case of networks with recurrent connections.
Reference: [11] <author> F. Pineda, </author> <title> "Generalization of back-propagation to recurrent neural networks," </title> <journal> Physical Review Letters, </journal> <volume> vol. 59, </volume> <pages> pp. 2229-2232, </pages> <year> 1987. </year>
Reference-contexts: Quite a straightforward generalization of Backpropagation can be attained for recurrent networks with relaxation computing style where each neuron follows the equation: _y i (t) = o i y i (t) + f (x i (t)) + I i (t) (5) where x i (t) = j Pineda <ref> [11] </ref> discovered that the Backpropagation algorithm can be applied directly also to this kind of recurrent networks provided that a fixed point is reached asymptotically. Independently, Almeida [12] 4 found a similar extension for the discrete time case.
Reference: [12] <author> L. Almeida, </author> <title> "A learning rule for asynchronous perceptrons with feedback in a combinatorial environment," </title> <booktitle> in IEEE International Conference on Neural Networks (M. </booktitle> <editor> Caudill and C. Butler, eds.), </editor> <volume> vol. </volume> <pages> 2, </pages> <address> (San Diego 1987), </address> <pages> pp. 609-618, </pages> <publisher> IEEE, </publisher> <address> New York, </address> <year> 1987. </year> <month> 26 </month>
Reference-contexts: Independently, Almeida <ref> [12] </ref> 4 found a similar extension for the discrete time case. Recurrent networks relaxing to fixed points extend, to some extent Hopfield networks in that hidden units can be also included. Their main application concerns the implementation of associative memories and the solution of constraint satisfaction problems.
Reference: [13] <author> K. Lang and G. Hinton, </author> <title> "The development of the time-delay neural network architecture for speech recognition," </title> <type> Tech. Rep. </type> <institution> CMU-CS-88-152, Carnegie-Mellon University, </institution> <year> 1988. </year>
Reference-contexts: This strategy was applied successfully in handwritten character recognition [6]. A simple application of this idea yielded Time-Delay Neural Networks (TDNN) <ref> [13, 14] </ref>, in which hidden units perform one-dimensional convolutions (with finite support) in the temporal direction.
Reference: [14] <author> A. Waibel, H. Sawai, and K. Shikano, </author> <title> "Modularity and scaling in large phonemic neural networks," </title> <journal> IEEE Transactions on Acoustics, Speech, and Signal Processing, </journal> <volume> vol. 37, </volume> <pages> pp. 1888-1898, </pages> <year> 1989. </year>
Reference-contexts: This strategy was applied successfully in handwritten character recognition [6]. A simple application of this idea yielded Time-Delay Neural Networks (TDNN) <ref> [13, 14] </ref>, in which hidden units perform one-dimensional convolutions (with finite support) in the temporal direction.
Reference: [15] <author> B. de Vries and J. Principe, </author> <title> "The gamma model anew neural net model for temporal processing," </title> <booktitle> Neural Networks, </booktitle> <volume> vol. 5, </volume> <pages> pp. 565-576, </pages> <year> 1992. </year>
Reference-contexts: This strategy was applied successfully in handwritten character recognition [6]. A simple application of this idea yielded Time-Delay Neural Networks (TDNN) [13, 14], in which hidden units perform one-dimensional convolutions (with finite support) in the temporal direction. A related architecture is based on the Gamma memory <ref> [15] </ref>, in which the hidden units perform a computation equivalent to a temporal convolution of their input with a kernel that is a gamma density function. This convolution is computed incrementally. Different forms of short-term memory are reviewed in [16].
Reference: [16] <author> M. Mozer, </author> <title> "Neural net architectures for temporal sequence processing," in Predicting the future and understanding the past (A. </title> <editor> Weigend and N. Gershenfeld, eds.), </editor> <address> Redwood City, CA: </address> <publisher> Addison-Wesley, </publisher> <year> 1993. </year>
Reference-contexts: A related architecture is based on the Gamma memory [15], in which the hidden units perform a computation equivalent to a temporal convolution of their input with a kernel that is a gamma density function. This convolution is computed incrementally. Different forms of short-term memory are reviewed in <ref> [16] </ref>. The main disadvantage of the above two schemes (TDNNs and Gamma memory) is that they do not naturally capture temporal structures which are elastic, i.e., where the delay between two events may vary widely. <p> However, for different sequences, different delays may be appropriate. According to the terminology introduced in <ref> [16] </ref>, the memory is static in the case of TDNNs but it is adaptive in the case of recurrent networks. A simple example of that problem is illustrated with the minimal task of section 5.1. <p> Although recurrent networks can in many instances outperform static networks [42], they appear more difficult to train optimally. Earlier experiments indicated that their parameters settle in sub-optimal solutions which take into account short-term dependencies but not long-term dependencies [43]. Similar results were obtained by Mozer <ref> [16] </ref>. It was found that back-propagation was not sufficiently powerful to discover contingencies spanning long temporal intervals. 11 Assuming that the network stores information using hyperbolic attractors we show that the contributions to the gradient due to past events decrease exponentially with time, thus preventing efficient learning.
Reference: [17] <author> M. Jordan, </author> <title> "Serial order: a parallel distributed processing approach," </title> <type> Tech. Rep. 8604, </type> <institution> ICS (Institute for Cognitive Science, University of California), </institution> <year> 1986. </year>
Reference-contexts: In other cases one may be interested in modeling the dynamic behavior of a plant and an adaptive network may be trained to emulate the input/output temporal response. As a last example a recurrent network may be forced to act as an oscillator <ref> [17] </ref>. Architectures and training strategies may be substantially different depending on the particular applicative field. <p> They go bejond the scope of this paper and the reader may refer to [19],[20], [21], [22]. A recurrent network can generate trajectories also without applying an external input. In such cases the network behaves like an autonomous oscillator. The Jordan net <ref> [17] </ref> is a first example of a network with feedback connections trained to oscillate on a periodic attractor. In that case feedback weights were fixed 7 and feedforward weights were trained by backpropagation.
Reference: [18] <author> D. Kirk, </author> <title> Optimal Control Theory: an Introduction. </title> <address> Englewood Cliffs NJ: </address> <publisher> Prentice Hall, </publisher> <year> 1970. </year>
Reference-contexts: One possible solution is that of setting up a complete model of the system to be controlled and to compute temporal derivatives of the total cost function with respect to the adaptive parameters of the controller. Such approach is well known in control theory as calculus of variations <ref> [18] </ref>. If the controller is a neural network then backpropagation through time (see section 4) can be used to efficiently compute such derivatives. Despite the fact that recurrent network may exhibit a very good representational power to deal with motor control tasks, their practical use is indeed very limited.
Reference: [19] <author> R. Sutton, </author> <title> Temporal Credit Assignment in Reinforcement Learning. </title> <type> PhD thesis, </type> <institution> University of Mas-sachusetts, Amherst, </institution> <year> 1984. </year>
Reference: [20] <author> R. Sutton, </author> <title> "Learning to predict by the methods of temporal differences," </title> <journal> Machine Learning, </journal> <volume> vol. 3, </volume> <pages> pp. 9-44, </pages> <year> 1988. </year>
Reference: [21] <author> C. Watkins, </author> <title> Learning from Delayed Rewards. </title> <type> PhD thesis, </type> <institution> Cambridge University, </institution> <address> Cambridge, England, </address> <year> 1989. </year>
Reference-contexts: More effective techniques are in the framework of learning to approximate the optimal solution that would be generated by dynamic programming. They go bejond the scope of this paper and the reader may refer to [19],[20], <ref> [21] </ref>, [22]. A recurrent network can generate trajectories also without applying an external input. In such cases the network behaves like an autonomous oscillator. The Jordan net [17] is a first example of a network with feedback connections trained to oscillate on a periodic attractor.
Reference: [22] <author> A. Barto, R. Sutton, and C. Watkins, </author> <title> "Learning and sequential decision making," in Learning and Computational Neuroscience (M. </title> <editor> Gabriel and J. Moore, eds.), </editor> <publisher> Cambridge: MIT Press, </publisher> <year> 1991. </year>
Reference-contexts: More effective techniques are in the framework of learning to approximate the optimal solution that would be generated by dynamic programming. They go bejond the scope of this paper and the reader may refer to [19],[20], [21], <ref> [22] </ref>. A recurrent network can generate trajectories also without applying an external input. In such cases the network behaves like an autonomous oscillator. The Jordan net [17] is a first example of a network with feedback connections trained to oscillate on a periodic attractor.
Reference: [23] <author> J. Elman, </author> <title> "Finding structure in time," </title> <journal> Cognitive Science, </journal> <volume> vol. 14, </volume> <pages> pp. 179-211, </pages> <year> 1990. </year>
Reference-contexts: In that case feedback weights were fixed 7 and feedforward weights were trained by backpropagation. Of course the approach is not optimal and a gradient can be computed also for the feedback weights. Elman <ref> [23] </ref> proposed a rough approximation for estimating feedback weights, in which the past activations of "state" units are copied in a subset of the hidden layer to represent context. To compute the true gradient however, temporal dependencies have to be correctly taken into account.
Reference: [24] <author> R. Williams and D. Zipser, </author> <title> "A learning algorithm for continually running fully recurrent neural networks," </title> <journal> Neural Computation, </journal> <volume> vol. 1, </volume> <pages> pp. 270-280, </pages> <year> 1989. </year>
Reference-contexts: To compute the true gradient however, temporal dependencies have to be correctly taken into account. If it is desired to adapt the parameters of a recurent network in real time (i.e., after each time step), the Williams and Zipser <ref> [24] </ref> can be used (see also section 4) to compute the correct gradient. However, a serious drawback exists in real-time training of a system to produce oscillations. <p> This problem can be solved with a technique similar to the one used by Jordan and named teacher forcing by Williams and Zipser <ref> [24] </ref>. The simple idea is that of replacing the activity of each output neuron with the value supplied by the supervision. Instead, the activities of the hidden neurons are updated according to the iterated map 7. <p> not local in time, since we have to store all past activations of the network units and run the algorithm backward in time. 9 4.2 Real-time gradient computation Another algorithm for training recurrent networks with no restriction on the architecture was independently proposed in various forms in [35, 36] and <ref> [24] </ref>. This algorithm avoids the back-propagation in time, which requires storing the complete sequence of network activations. It achieves this by computing recursively and keeping in memory during the regular forward pass partial derivatives which indicate how each weight of the network influences each unit activation.
Reference: [25] <author> P. Simard, </author> <title> Learning State Space Dynamics in Recurrent Networks. </title> <type> PhD thesis, </type> <institution> University of Rochester, Rochester, </institution> <address> NY, </address> <year> 1991. </year> <type> Tech. Rep. 383. </type>
Reference-contexts: In principle, the same recurrent network may generate multiple autonomous trajectories depending on a triggering input signal (or a fixed input pattern). This is possible by developping multiple periodic attractors and having the input signal to push the initial state into the correct basin of attraction. However, Simard <ref> [25] </ref> reports negative results for a recurrent network trained to emulate hand-drawing of character glyphs. A different approach, that can be considered as an extension of teacher forcing is that of feeding a feedforward network with the input-state pair and training it to predict the correct next-state.
Reference: [26] <author> M. Yirayama, E. Vatikiotis-Bateson, M. Kawato, and M. Jordan, </author> <title> "Forward dynamics modeling of speech motor control using physiological data," </title> <booktitle> in Advances in Neural Information Processing Systems 4 (J. </booktitle> <editor> Moody, S. Hanson, and R. Lipmann, eds.), </editor> <booktitle> (Denver 1991), </booktitle> <pages> pp. 191-198, </pages> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, </address> <year> 1992. </year>
Reference-contexts: A different approach, that can be considered as an extension of teacher forcing is that of feeding a feedforward network with the input-state pair and training it to predict the correct next-state. Once the network has learned, loop connections may be added, obtaining a dynamic system. Hirayama et. al. <ref> [26] </ref> found this approach successful in modeling the motor control underlying the human speech production system. The task of sequence classification can be described as a mapping from the input sequence space to the boolean values 0 and 1.
Reference: [27] <author> A. Cleeremans, D. Servan-Schreiber, and J. McClelland, </author> <title> "Finite state automata and simple recurrent networks," </title> <journal> Neural Computation, </journal> <volume> vol. 1, </volume> <pages> pp. 372-381, </pages> <year> 1989. </year>
Reference: [28] <author> R. Watrous and G. Kuhn, </author> <title> "Induction of finite-state languages using second-order recurrent networks," </title> <journal> Neural Computation, </journal> <volume> vol. 4, no. 3, </volume> <pages> pp. 406-414, </pages> <year> 1992. </year>
Reference: [29] <author> C. Giles, C. Miller, D. Chen, G. Sun, H. Chen, and Y. Lee, </author> <title> "Learning and extracting finite state automata with second-order recurrent neural networks," </title> <journal> Neural Computation, </journal> <volume> vol. 4, no. 3, </volume> <pages> pp. 393-405, </pages> <year> 1992. </year>
Reference-contexts: In this way the vectors of activities of the neurons can be converted to a finite set of discrete states. A simple quantization procedure is based on partitioning the continuous state space. Such approach is described in <ref> [29] </ref> for second-order recurrent networks. By dividing the range of the n hidden neuron's outputs in k intervals, k n partition states are obtained. <p> The inherent problem of this approach is that the complexity would grown exponentially if most of the partition states were actually visited. Experiments in <ref> [29] </ref> show that this is not the case for small network trained to learn relatively simple regular grammars. A different extraction approach can be used in conjunction with KBANNs [58]. In that case it is assumed that each hidden node preserves a boolean interpretation also after learning.
Reference: [30] <author> R. Rohwer, </author> <title> "The "moving targets" training algorithm," </title> <booktitle> in Advances in Neural Information Processing Systems (D. </booktitle> <editor> Touretzky, ed.), </editor> <volume> vol. </volume> <pages> 2, </pages> <address> (Denver 1989), </address> <pages> pp. 558-565, </pages> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, </address> <year> 1990. </year>
Reference-contexts: Techniques like teacher forcing cannot be used for sequence classification because neither the state nor the output trajectories are known during training. Rohwer <ref> [30] </ref> proposed a nice approach for figuring which the state trajectory should be. The activities of the non supervised neuron that partecipate in the network state are considered learnable parameters ("moving targets"). Optimization is based on gradient descent in the space of all the free parameters (weights and moving targets).
Reference: [31] <author> D. Rumelhart, G. Hinton, and R. Williams, </author> <title> "Learning internal representations by error propagation," in Parallel Distributed Processing (D. </title> <editor> Rumelhart and J. McClelland, eds.), </editor> <volume> vol. 1, ch. 8, </volume> <pages> pp. 318-362, </pages> <address> Cambridge: </address> <publisher> MIT Press, </publisher> <year> 1986. </year> <note> Reprinted in [59]. 27 </note>
Reference-contexts: Backpropagation through time, the Williams and Zipser algorithm and algorithms for local feedback networks are briefly derived in the following. 4.1 Backpropagation through time The time-unfolding algorithm was proposed in <ref> [31] </ref> and another version of it (from discretized differential equations) can be found in [32]. This algorithm computes the error gradient of an unconstrained discrete recurrent ANN.
Reference: [32] <author> B. Pearlmutter, </author> <title> "Learning state space trajectories in recurrent neural networks," </title> <journal> Neural Computation, </journal> <volume> vol. 1, </volume> <pages> pp. 263-269, </pages> <year> 1989. </year>
Reference-contexts: Backpropagation through time, the Williams and Zipser algorithm and algorithms for local feedback networks are briefly derived in the following. 4.1 Backpropagation through time The time-unfolding algorithm was proposed in [31] and another version of it (from discretized differential equations) can be found in <ref> [32] </ref>. This algorithm computes the error gradient of an unconstrained discrete recurrent ANN.
Reference: [33] <author> P. Werbos, </author> <title> "Generalization of backpropagation with application to a recurrent gas market model," </title> <booktitle> Neural Networks, </booktitle> <volume> vol. 1, </volume> <pages> pp. 339-356, </pages> <year> 1988. </year>
Reference-contexts: To clarify our use of partial derivatives, let us define two types of partial derivatives, as in <ref> [33] </ref>: * The conventional partial derivative @u @v is calculated by differentiating the function u as it would normally be written as a function of its direct arguments, without any substitutions. * The ordered derivative @ + u @v refers to the total causal impact, including direct and indirect effects of
Reference: [34] <author> P. Werbos, </author> <title> Beyond Regression: New Tools for Prediction and Analysis in the Behavioral Sciences. </title> <type> PhD thesis, </type> <institution> Harvard University, </institution> <year> 1974. </year>
Reference-contexts: A proof of the validity of the application of the chain rule as above can be found in <ref> [34] </ref>.
Reference: [35] <author> G. Kuhn, </author> <title> "A first look at phonetic discrimanation using connectionist models with recurrent links." </title> <institution> CCRP IDA SCIMP working paper No.4/87, Institute for Defense Analysis, Princeton, NJ, </institution> <year> 1987. </year>
Reference-contexts: in space but not local in time, since we have to store all past activations of the network units and run the algorithm backward in time. 9 4.2 Real-time gradient computation Another algorithm for training recurrent networks with no restriction on the architecture was independently proposed in various forms in <ref> [35, 36] </ref> and [24]. This algorithm avoids the back-propagation in time, which requires storing the complete sequence of network activations. It achieves this by computing recursively and keeping in memory during the regular forward pass partial derivatives which indicate how each weight of the network influences each unit activation.
Reference: [36] <author> G. Kuhn, R. Watrous, and B. Ladendorf, </author> <title> "Connected recognition with a recurrent network," </title> <journal> Speech Communication, </journal> <volume> vol. 9, </volume> <pages> pp. 41-49, </pages> <year> 1990. </year>
Reference-contexts: in space but not local in time, since we have to store all past activations of the network units and run the algorithm backward in time. 9 4.2 Real-time gradient computation Another algorithm for training recurrent networks with no restriction on the architecture was independently proposed in various forms in <ref> [35, 36] </ref> and [24]. This algorithm avoids the back-propagation in time, which requires storing the complete sequence of network activations. It achieves this by computing recursively and keeping in memory during the regular forward pass partial derivatives which indicate how each weight of the network influences each unit activation.
Reference: [37] <author> G. Kuhn and N. Herzberg, </author> <title> "Variations on training of recurrent networks," </title> <booktitle> in Proc. 24th Conference on Information Sciences and Systems, </booktitle> <address> (NJ), Princeton University, </address> <year> 1990. </year>
Reference-contexts: However, the complexity of the forward propagation algorithm can be limited as in the experiments described in <ref> [37] </ref>, for example by using only self-loops. This is similar to the architectural constraint of the local algorithms introduced in the next subsection, which rely on combined forward and backward propagation of partial derivatives. 4.3 Local algorithms The two above described gradient computation techniques trade space vs. time locality.
Reference: [38] <author> M. Mozer, </author> <title> "A focused back-propagation algorithm for temporal pattern recognition," </title> <journal> Complex Systems, </journal> <volume> vol. 3, </volume> <pages> pp. 349-381, </pages> <year> 1989. </year>
Reference-contexts: Interestingly, if the the recurrent connections are restricted to self-loops for some neurons, both space and time locality can be achieved. Local gradient computation algorithms for such networks were independently derived by Mozer <ref> [38] </ref> and Gori, Bengio and De Mori [39]. <p> For example y i (t 1) may be replaced by a i (t 1), obtaining an activation-feedback connection that yields a linear dynamic behavior (such approach was pursued in <ref> [38] </ref>.) Another variant is to introduce multiple delays, feeding back the past output [activation] of the unit at time t 2; t 3 and so on. This corresponds to a higher than first order nonlinear [linear] autoregressive model for the unit.
Reference: [39] <author> M. Gori, Y. Bengio, and R. D. Mori, </author> <title> "Bps: A learning algorithm for capturing the dynamical nature of speech," </title> <booktitle> in Proceedings of the International Joint Conference on Neural Networks, </booktitle> <address> (Washington D.C.), </address> <pages> pp. 643-644, </pages> <publisher> IEEE, </publisher> <address> New York, </address> <year> 1989. </year>
Reference-contexts: Interestingly, if the the recurrent connections are restricted to self-loops for some neurons, both space and time locality can be achieved. Local gradient computation algorithms for such networks were independently derived by Mozer [38] and Gori, Bengio and De Mori <ref> [39] </ref>. In a local feedback architecture (see Fig. 1,) neurons are connected in layers like in feedforward nets, eventually using shortcut links (i.e., skipping one or more layers.) Units directly connected to the external inputs may have a self-loop link, thus introducing dynamic in the computation. <p> Equation 20 can be computed while processing the input sequence, thus providing real time gradient computation. Local feedback recurrent networks have been successfully employed in tasks requiring short-term memories, such as continuous speech phoneme recognition <ref> [39] </ref>. However, they have intrinsic limits in their representation capabilities that make them inadequate to deal with problems requiring long-term memorization [41]. 5 Theoretical problems with gradient descent training In this section we present theoretical and experimental arguments to better understand some limitations of learning based on gradient descent.
Reference: [40] <author> Y. Bengio, M. Gori, and R. D. Mori, </author> <title> "Learning the dynamic nature of speech with back-propagation for sequences," </title> <journal> Pattern Recognition Letters, </journal> <volume> vol. 13, no. 5, </volume> <pages> pp. 375-386, </pages> <year> 1992. </year> <journal> (Special issue on Artificial Neural Networks). </journal>
Reference-contexts: This corresponds to a higher than first order nonlinear [linear] autoregressive model for the unit. A more general case is to use an autoregressive moving-average transfer function for the dynamic neurons. This case is considered in <ref> [40] </ref>.
Reference: [41] <author> P. Frasconi, M. Gori, and G. </author> <title> Soda, "Local feedback multi-layered networks," </title> <journal> Neural Computation, </journal> <volume> vol. 4, no. 1, </volume> <pages> pp. 120-130, </pages> <year> 1992. </year>
Reference-contexts: Local feedback recurrent networks have been successfully employed in tasks requiring short-term memories, such as continuous speech phoneme recognition [39]. However, they have intrinsic limits in their representation capabilities that make them inadequate to deal with problems requiring long-term memorization <ref> [41] </ref>. 5 Theoretical problems with gradient descent training In this section we present theoretical and experimental arguments to better understand some limitations of learning based on gradient descent. Although recurrent networks can in many instances outperform static networks [42], they appear more difficult to train optimally.
Reference: [42] <author> Y. Bengio, </author> <title> Artificial Neural Networks and their Application to Sequence Recognition. </title> <type> PhD thesis, </type> <institution> McGill University, (Computer Science), Montreal, Qc., Canada, </institution> <year> 1991. </year>
Reference-contexts: Although recurrent networks can in many instances outperform static networks <ref> [42] </ref>, they appear more difficult to train optimally. Earlier experiments indicated that their parameters settle in sub-optimal solutions which take into account short-term dependencies but not long-term dependencies [43]. Similar results were obtained by Mozer [16].
Reference: [43] <author> Y. Bengio, R. D. Mori, G. Flammia, and R. Kompe, </author> <title> "Global optimization of a neural network-hidden markov model hybrid," </title> <journal> IEEE Transactions on Neural Networks, </journal> <volume> vol. 3, no. 2, </volume> <pages> pp. 252-259, </pages> <year> 1992. </year>
Reference-contexts: Although recurrent networks can in many instances outperform static networks [42], they appear more difficult to train optimally. Earlier experiments indicated that their parameters settle in sub-optimal solutions which take into account short-term dependencies but not long-term dependencies <ref> [43] </ref>. Similar results were obtained by Mozer [16].
Reference: [44] <author> P. Frasconi, M. Gori, M. Maggini, and G. </author> <title> Soda, "Unified integration of explicit rules and learning by example in recurrent networks," </title> <journal> IEEE Transactions on Knowledge and Data Engineering, </journal> <note> 1993. (in press). </note>
Reference-contexts: We performed experiments on this minimal task with a single recurrent neuron, as shown in Fig. 2a. This recurrent neuron can latch information if w &gt; 1=f 0 (0) = 1 as shown in <ref> [44] </ref>. In Fig. 2b we show two sample input sequences. A set of simulations were carried out to evaluate the effectiveness of back-propagation (through time) on this simple task. <p> j 3. a t escapes from the basin of attraction and reaches the basin of the other stable attractor 7 in a finite number of steps if I t;i &gt; I fl i (low to high switching) I t;i &lt; I fl i (high to low switching.) (35) Proof: See <ref> [44] </ref>. 2 The mapping of the automaton rules into the weight values is then accomplished as follows. <p> This guarantees that the injected rules are preserved during learning. Constraints can be enforced using classic numerical techniques such as reduced gradient. A different approach is used in <ref> [44] </ref>, where weights are constrained to lie inside the largest hypersphere contained in the feasible region. 7.2.2 K-L networks In many cases it may be difficult or inconvenient to rely only on a description of what is known and what is unknown, even in terms of nondeterministic rules. <p> In these cases, modular approaches can be exploited in which prior knowledge is only injected into a subnetwork of the complete architecture. For example in <ref> [57, 44] </ref> a randomly initialized network (L) is integrated with a knowledge based structure (K). The resulting K-L architecture offers the advantages of rule injection, allowing at the same time to extract unpredictable information from the training examples. <p> The weights of the first module are quickly initialized thanks to the method shown in 7.2.1. Learning the uncertain information is mainly accomplished by the second module, a full-connected randomly initialized recurrent network. In <ref> [44] </ref> experiments are reported for speaker independent small dictionary isolated word recognition.
Reference: [45] <author> Y. Bengio, P. Frasconi, and P. Simard, </author> <title> "The problem of learning long-term dependencies in recurrent networks," </title> <booktitle> in IEEE International Conference on Neural Networks, </booktitle> <address> (San Francisco), </address> <pages> pp. 1183-1195, </pages> <publisher> IEEE Press, </publisher> <year> 1993. </year> <type> (invited paper). </type>
Reference-contexts: In a first experiment we investigated the effect of the noise variance s and of different initial values w 0 for the self loop weight. It was found that convergence becomes very unlikely for large noise variance or small initial values of w (see <ref> [45] </ref>). In Fig. 3, we show the effect of varying the sequence length T , keeping fixed s = 0:2 and w 0 = 1:25. In this case the task consists in learning only the input parameters u t . <p> The "pseudo gradient" along this loop doesn't vanish with time which is the essential reason for using discrete units. This approach is in no way optimal and many other discrete error propagation algorithms are possible. Another very promising approach for instance is the trainable discrete flip-flop unit <ref> [45] </ref> which also preserves error information in time. Our only claim here is that discrete propagation of error offers interesting solutions to the vanishing gradient pro blem in recurrent network. Our preliminary results on toy problems (see next subsection and [45]) confirm this hypothesis. 6.3 Experimental Results Experiments were performed to <p> promising approach for instance is the trainable discrete flip-flop unit <ref> [45] </ref> which also preserves error information in time. Our only claim here is that discrete propagation of error offers interesting solutions to the vanishing gradient pro blem in recurrent network. Our preliminary results on toy problems (see next subsection and [45]) confirm this hypothesis. 6.3 Experimental Results Experiments were performed to evaluate various alternative optimization approaches on problems on which one can increase the temporal span of input/output dependencies. Experiments were performed with and without input noise (uniformly distributed in [-0.2,0.2]) and varying the length of the input/output sequences.
Reference: [46] <author> C. Marcus, F. Waugh, and R. Westervelt, </author> <title> "Nonlinear dynamics and stability of analog neural networks," </title> <journal> Physica D, </journal> <volume> vol. 51, </volume> <editor> p. </editor> <year> 1991, 1991. </year> <note> (special issue). </note>
Reference-contexts: In particular, for a network defined by a t = W tanh (a t1 ) + u t , if W is symmetric and its minimum eigenvalue is greater than -1, then the attractors are all fixed points <ref> [46] </ref>. On the other hand, if jW j &lt; 1 or if the system is linear and stable, the system has a single fixed point attractor at the origin.
Reference: [47] <author> J. Ortega and W. Rheinboldt, </author> <title> Iterative Solution of Non-linear Equations in Several Variables and Systems. </title> <address> New York: </address> <publisher> Academic Press, </publisher> <year> 1970. </year>
Reference-contexts: Theorem 2 Let M be a differentiable mapping on a convex set D. If 8x 2 D; jM 0 (x)j &lt; 1 , then M is contracting on D. Proof: See <ref> [47] </ref>. <p> By Lagrange's mean value theorem and convexity of t , 9z 2 t s.t. kM (x) M (y)k jM 0 (z)jkx yk, but jM 0 (z)j &lt; by hypothesis. Then by the contraction theorem <ref> [47] </ref> we have ae 0 = 0, ae 1 = b, ae 2 = b + b, ae t = ae t1 + b. Hence for t &gt; 0, 15 ae t = b i=0 i = 1 , thus lim t!1 ae t = b 1 .
Reference: [48] <author> Y. Bengio, , P. Simard, and P. Frasconi, </author> <title> "Learning long-term dependencies with gradient descent is difficult," </title> <journal> IEEE Transactions on Neural Networks, 1993. Special Issue on Recurrent Neural Networks, </journal> <note> to appear. </note>
Reference-contexts: More details about this analysis can be found in <ref> [48] </ref>. 6 New Directions for Training Recurrent Networks The theoretical results above helped us understand better why training a recurrent network to learn long range input/output dependencies is a hard problem. Gradient-based methods appear inadequate for this kind of problem. <p> The time-weighted pseudo-Newton algorithm appears to perform better than back-propagation but its performance also appears to worsen when increasing sequence length. A more detailed set of experiments and comparisons with other algorithms can be found in <ref> [48] </ref>. 7 Integrating symbolic processing As mentioned in the introduction, besides exploiting novel optimization techniques one may try to reduce the hardness of training recurrent networks by starting the learning process from a point sufficiently close to the solution. Such alternative approach can be exploited under certain conditions.
Reference: [49] <author> S. Becker and Y. L. Cun, </author> <title> "Improving the convergence of back-propagation learning with second order methods," </title> <booktitle> in Proceedings of the 1988 Connectionist Models Summer School (D. </booktitle> <editor> Touretzky, G. Hinton, and T. Sejnowski, eds.), </editor> <booktitle> (Pittsburg 1988), </booktitle> <pages> pp. 29-37, </pages> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, </address> <year> 1989. </year> <month> 28 </month>
Reference-contexts: However, there are many instances where many long-term input/output dependencies are unknown and have to be learned from examples. 6.1 Time-Weighted Pseudo-Newton Optimization The pseudo-Newton algorithm <ref> [49] </ref> for neural networks has the advantage of re-scaling the learning rate of each weight dynamically to match the curvature of the energy function with respect to that weight. This is of interest because adjusting the learning rate could potentially circumvent the problem of vanishing gradient. <p> Consequently, updating w according to equation 25 does not actually follow the gradient (but neither would following equation 24). Instead, several gradient contributions are weighted using second derivatives, in order to make faster moves in the flatter directions. Like for the pseudo-Newton algorithm of <ref> [49] </ref>, we prefer using a diagonal approximation of the Hessian which is cheap to compute and guaranteed to be positive. j is a global learning rate (0.01 in our experiments).
Reference: [50] <author> M. Goudreau, C. Giles, S. Chakradhar, and D. Chen, </author> <title> "First-order vs. second-order single layer recurrent neural networks," </title> <journal> IEEE Transactions on Neural Networks, </journal> <note> 1993. (in press). </note>
Reference-contexts: Given the intrinsically symbolic nature of a DFA, whichever representation is used for states and inputs the state-transition function gets a boolean-like form. This typically involves the computation of nonlinearly-separable binary valued functions that in general cannot be implemented using a single layer of first-order neurons <ref> [50] </ref>. More powerful static mappings can be obtained in two ways. A first solution is to modify the computation of each unit.
Reference: [51] <author> C. Miller and C. Giles, </author> <title> "Experimental comparison of the effect of order in recurrent neural networks," </title> <journal> Int. Journal of Pattern Recognition and Artificial Intelligence, </journal> <year> 1993. </year> <title> Special Issue on Applications of Neural Networks to Pattern Recognition (I. </title> <publisher> Guyon Ed.). </publisher>
Reference: [52] <author> C. Omlin and C. Giles, </author> <title> "Training second-order recurrent neural networks using hints," </title> <booktitle> in Machine Learning: Proc. of the Ninth Int. Conference (D. </booktitle> <editor> Sleeman and P. Edwards, eds.), </editor> <address> (San Mateo CA), </address> <publisher> Morgan Kaufmann, </publisher> <year> 1992. </year>
Reference-contexts: This may prevent further learning (as to refine a previous solution) if gradient descent techniques are used for optimization | see also section 5.) 7.1.1 Rules insertion using second-order networks In this approach, originally proposed by Omlin and Giles <ref> [52] </ref>, the state-transition function of a DFA is supposed to be partially known. <p> Rules inserted in this way are defined as hints and the value H is referred to as hint strength [53]. As shown in <ref> [52] </ref>, the hint strength H has a significant influence on the training time. Wrong choices of H may lead to bad training performances, comparable to those obtained without prior knowledge. In particular, experiments in [52] reveal that too large values of H are likely to slow down training. <p> As shown in <ref> [52] </ref>, the hint strength H has a significant influence on the training time. Wrong choices of H may lead to bad training performances, comparable to those obtained without prior knowledge. In particular, experiments in [52] reveal that too large values of H are likely to slow down training. We can interpret this experimental finding in the light of the theory shown in section 5. Higher values for the hint strength lead to more saturated neurons, corresponding to attractors with a larger reduced attracting set.
Reference: [53] <author> C. Giles and C. Omlin, </author> <title> "Inserting rules into recurrent neural networks," in Neural Networks for Signal Processing II, </title> <booktitle> Proceedings of the 1992 IEEE workshop (Kung, Fallside, Sorenson, and Kamm, eds.), </booktitle> <pages> pp. 13-22, </pages> <publisher> IEEE Press, </publisher> <year> 1992. </year>
Reference-contexts: Rules inserted in this way are defined as hints and the value H is referred to as hint strength <ref> [53] </ref>. As shown in [52], the hint strength H has a significant influence on the training time. Wrong choices of H may lead to bad training performances, comparable to those obtained without prior knowledge.
Reference: [54] <author> G. Towell, J. Shawlick, and M. Noordewier, </author> <title> "Refinement of approximate domain theories by knowledge-based neural networks," </title> <booktitle> in Proceedings of the Eighth National Conference on Artificial Intelligence (AAAI-90), </booktitle> <pages> pp. 861-866, </pages> <year> 1990. </year>
Reference-contexts: However, increasing the order in higher-order networks quickly constitutes a practical obstacle inasmuch as the number of parameters to be learned explodes. 7.1.2 Recurrent KBANNs The Knowledge-Based Artificial Neural Networks (KBANN) learning system was introduced as a method for mapping hierarchically-structured rules into feedforward networks <ref> [54] </ref>.
Reference: [55] <author> R. Maclin and J. Shawlik, </author> <title> "Refining domain theories expressed as finite-state automata," </title> <booktitle> in Machine Learning: Proceedings of the Eighth International Workshop (L. </booktitle> <editor> Birnbaum and G. Collins, eds.), </editor> <address> (San Mateo CA), </address> <publisher> Morgan Kaufmann, </publisher> <year> 1991. </year>
Reference: [56] <author> R. Maclin and J. Shawlik, </author> <title> "Using knowledge-based neural networks to improve algorithms: Refining the chou-fasman algorithm for protein folding," </title> <journal> Machine Learning, </journal> . <note> (to appear). </note>
Reference: [57] <author> P. Frasconi, M. Gori, M. Maggini, and G. </author> <title> Soda, "A unified approach for integrating explicit knowledge and learning by example in recurrent networks," </title> <booktitle> in International Joint Conference on Neural Networks, </booktitle> <pages> pp. 811-816, </pages> <year> 1991. </year>
Reference-contexts: In these cases, modular approaches can be exploited in which prior knowledge is only injected into a subnetwork of the complete architecture. For example in <ref> [57, 44] </ref> a randomly initialized network (L) is integrated with a knowledge based structure (K). The resulting K-L architecture offers the advantages of rule injection, allowing at the same time to extract unpredictable information from the training examples.
Reference: [58] <author> G. Towell and J. Shawlik, </author> <title> "Interpretation of artificial neural networks: Mapping knowledge-based neural networks into rules," </title> <booktitle> in Advances in Neural Information Processing Systems 4 (J. </booktitle> <editor> Moody, S. Hanson, and R. Lipmann, eds.), </editor> <address> (San Mateo CA), </address> <publisher> Morgan Kaufmann, </publisher> <year> 1992. </year>
Reference-contexts: Experiments in [29] show that this is not the case for small network trained to learn relatively simple regular grammars. A different extraction approach can be used in conjunction with KBANNs <ref> [58] </ref>. In that case it is assumed that each hidden node preserves a boolean interpretation also after learning. An exhaustive procedure scans for all possible subsets of connection weights that make the clauses verified. However, the prohibitive complexity of the procedure demands for some approximation technique. <p> An exhaustive procedure scans for all possible subsets of connection weights that make the clauses verified. However, the prohibitive complexity of the procedure demands for some approximation technique. Some interesting heuristics are proposed in <ref> [58] </ref>. 25 8 Conclusions The conclusions of this paper will follow the major elements covered in the paper: * The main advantage of recurrent networks is their expressive power (i.e., the size and usefulness of the class of machines that a recurrent network can emulate with a certain number of free
Reference: [59] <editor> J. Anderson and E. Rosenfeld, eds., Neurocomputing: </editor> <booktitle> Foundations of Research. </booktitle> <address> Cambridge: </address> <publisher> MIT Press, </publisher> <year> 1988. </year> <month> 29 </month>
References-found: 59

