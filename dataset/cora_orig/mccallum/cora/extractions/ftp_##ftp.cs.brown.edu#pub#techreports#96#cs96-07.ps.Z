URL: ftp://ftp.cs.brown.edu/pub/techreports/96/cs96-07.ps.Z
Refering-URL: http://www.cs.brown.edu/publications/techreports/reports/CS-96-07.html
Root-URL: http://www.cs.brown.edu/
Abstract-found: 0
Intro-found: 1
Reference: [1] <author> F. Alizadeh. </author> <title> Interior point methods in semidefi-nite programming with applications to combinatorial optimization. </title> <journal> SIAM Journal on Optimization, </journal> <volume> 5(1) </volume> <pages> 13-51, </pages> <year> 1995. </year>
Reference-contexts: This subroutine, combined with the ellipsoid method, provided a polynomial-time algorithm for semidefinite programming. However, the complexity of this algorithm is quite high. Nesterov and Nemirovsky showed [17] how to use the interior-point method to solve semidefinite programs. Alizadeh <ref> [1] </ref> showed how an interior-point algorithm for linear programming could be directly generalized to handle semidefinite programming. Since the work of Alizadeh, there has been a great deal of research into such algorithms [8, 24, 20, 25, 6, 12, 3].
Reference: [2] <author> F. Chatelin. </author> <title> Eigenvalues of Matrices. </title> <publisher> John Wiley & Sons, </publisher> <year> 1993. </year>
Reference-contexts: The eigenvector problem can be approximately solved by a well-known numerical algorithm called the power method (e.g. see x5.3 of <ref> [2] </ref>), which is one of the oldest methods for computing the dominant eigen-pair of a matrix. Let L 0 be a matrix defined as L 0 p y i y j . One can easily verify that L 0 - 0.
Reference: [3] <author> R. M. Freund. </author> <title> Complexity of an Algorithm for Finding an Approximate Solution of a Semi-Definite Program with no Regularity Assumption. </title> <type> Technical Report OR-302-94, </type> <institution> Operations Research Center, M.I.T., </institution> <year> 1994. </year>
Reference-contexts: Nesterov and Nemirovsky showed [17] how to use the interior-point method to solve semidefinite programs. Alizadeh [1] showed how an interior-point algorithm for linear programming could be directly generalized to handle semidefinite programming. Since the work of Alizadeh, there has been a great deal of research into such algorithms <ref> [8, 24, 20, 25, 6, 12, 3] </ref>. Essentially, the number of iterations is roughly ~ O ( p and each iteration involves either factoring a matrix or solving a least-squares problem. The size of either problem is sfin 2 where s is the number of constraints defining the semidefinite program.
Reference: [4] <author> M. X. Goemans and D. P. Williamson. </author> <title> .878-approximation algorithms for MAX CUT and MAX 2SAT. </title> <booktitle> In Proceedings of the Twenty-Sixth Annual ACM Symposium on Theory of Computing, </booktitle> <pages> pages 422-431, </pages> <address> Montreal, Quebec, Canada, </address> <month> 23-25 May </month> <year> 1994. </year>
Reference-contexts: Lovasz and Schri-jver [16] described a way to use semidefinite programming to estimate the value of integer programs. In an important recent breakthrough, Goemans and Williamson <ref> [4] </ref> discovered an approximation algorithm for graph MAX CUT whose accuracy is significantly better than that of the previously known algorithms. Their algorithm is based on obtaining a near-optimum solution to a semidefinite program. Building on this work, Karger, Motwani, and Su-dan [9] discovered an approximation algorithm for graph coloring. <p> Let L be the Laplacian of G, defined as L ij = C ij i 6= j P For convenience we refer to the MAX CUT semidef-inite program as the VECTOR MAX CUT problem. The VECTOR MAX CUT problem is the following semidefinite program as shown in <ref> [4] </ref>. max 1 s.t. X ii = 1 for every 1 i n X - 0: Lemma 1 ensures that the above semidefinite program can be rewritten as follows. max 1 L * X s.t. <p> We then give an upper bound on the feasible values, which also holds for the optimal value. Lemma 2 The optimum value is at least 0:17. Proof Let X fl be an optimal solution to (3). The results in <ref> [4] </ref> ensure that 0:878 4 L * X fl is at most the sum of edge costs. Since we assume that the sum of edge costs of G is one, it follows that L * X fl 4 0:878 .
Reference: [5] <author> M. Grotschel, L. Lovasz, and A. Schrijver. </author> <title> Geometric Algorithms and Combinatorial Optimization. </title> <publisher> Springer Verlag, </publisher> <year> 1988. </year>
Reference-contexts: Plotkin, Shmoys, and Tardos then took a final step and generalized this multicom-modity flow algorithm, obtaining not a single algorithm but a whole framework in which algorithms for a variety of problems could be formulated. This framework, like that of the ellipsoid algorithm <ref> [5] </ref> and the method of Vaidya [23], casts algorithms in terms of a subroutine, an oracle. For the ellipsoid algorithm, the subroutine is called a separation oracle. For Plotkin, Shmoys, and Tardos, the subroutine must find an optimum (or near-optimum) solution to a simpler optimization problem.
Reference: [6] <author> C. Helmberg, F. Rendl, R. J. Vanderbei, and H. Wolkowicz. </author> <title> An interior-point method for semidefinite programming. </title> <note> SIAM J. Optim., 1994. (To appear). </note>
Reference-contexts: Nesterov and Nemirovsky showed [17] how to use the interior-point method to solve semidefinite programs. Alizadeh [1] showed how an interior-point algorithm for linear programming could be directly generalized to handle semidefinite programming. Since the work of Alizadeh, there has been a great deal of research into such algorithms <ref> [8, 24, 20, 25, 6, 12, 3] </ref>. Essentially, the number of iterations is roughly ~ O ( p and each iteration involves either factoring a matrix or solving a least-squares problem. The size of either problem is sfin 2 where s is the number of constraints defining the semidefinite program.
Reference: [7] <author> R. A. Horn and C. R. Johnson. </author> <title> Matrix Analysis. </title> <publisher> Cambridge University Press, </publisher> <year> 1985. </year>
Reference-contexts: It follows that (y) = minfv v : L 0 * (vv T ) = 1g: Therefore 1=(y) = maxfL 0 * (ww T ) : w w = 1g: By Rayleigh-Ritz's theorem (e.g. see Theorem 4.2.2 of <ref> [7] </ref>) we know 1=(y) is the maximum eigenvalue of L 0 . A normalized eigenvector w in the eigenspace of L 0 corresponding to 1=(y) is a vector that maximizes L 0 *(ww T ) over all normalized vectors.
Reference: [8] <author> F. Jarre. </author> <title> An interior-point method for minimizing the maximum eigenvalue of a linear combination of matrices. </title> <journal> SIAM Journal on Control and Optimization, </journal> <volume> 31(5) </volume> <pages> 1360-1377, </pages> <year> 1993. </year>
Reference-contexts: Nesterov and Nemirovsky showed [17] how to use the interior-point method to solve semidefinite programs. Alizadeh [1] showed how an interior-point algorithm for linear programming could be directly generalized to handle semidefinite programming. Since the work of Alizadeh, there has been a great deal of research into such algorithms <ref> [8, 24, 20, 25, 6, 12, 3] </ref>. Essentially, the number of iterations is roughly ~ O ( p and each iteration involves either factoring a matrix or solving a least-squares problem. The size of either problem is sfin 2 where s is the number of constraints defining the semidefinite program.
Reference: [9] <author> D. Karger, R. Motwani, and M. Sudan. </author> <title> Approximate graph coloring by semidefinite programming. </title> <booktitle> In 35th Annual Symposium on Foundations of Computer Science, </booktitle> <pages> pages 2-13, </pages> <address> Santa Fe, New Mexico, 20-22 Nov. 1994. </address> <publisher> IEEE. </publisher>
Reference-contexts: Their algorithm is based on obtaining a near-optimum solution to a semidefinite program. Building on this work, Karger, Motwani, and Su-dan <ref> [9] </ref> discovered an approximation algorithm for graph coloring. If the input graph is 3-colorable, their algorithm obtains a coloring that uses O (n 1=4 log n) colors. More generally, if the input graph is k colorable, the coloring obtained by the algorithm uses ~ O (n 11=(k+1) ) colors. <p> Therefore L * X = D * X C * X = C * J C * X: The VECTOR COLORING problem is the follow ing semidefinite program as shown in <ref> [9] </ref>. min s.t. X ii = 1 for every 1 i n X ij for every edge ij of G X - 0: For a mathematical program, e.g. (3), an assignment to the variables is said to be a feasible solution if it satisfies the constraints. <p> Then ~ X is an *-optimal minimizer for (11). Proof Let (X; ) be an optimal solution to the VECTOR COLORING problem (10). Since G is k-colorable, it follows from the results in <ref> [9] </ref> that 1 k . By the feasibility of (X; ) we know that Y * X k Let X fl be an optimal solution to the VECTOR MAXCUT problem (3) corresponding to the cost matrix Y . Clearly (Y ) = Y * X fl .
Reference: [10] <author> P. Klein, S. Plotkin, C. Stein, and E. Tardos. </author> <title> Faster approximation algorithms for the unit capacity concurrent flow problem with applications to routing and finding sparse cuts. </title> <journal> SIAM Journal on Computing, </journal> <volume> 23(3) </volume> <pages> 466-487, </pages> <month> June </month> <year> 1994. </year>
Reference-contexts: Shahrokhi and Matula proved a polynomial but rather high bound on the running time of their algorithm. A much faster algorithm for this problem was developed by Klein, Plotkin, Stein, and Tardos <ref> [10] </ref>. One important ingredient in the improvement is a formulation of approximate optimality (relaxed complementary slackness conditions) that works well in this setting. This multicommodity flow algorithm was generalized by Leighton, Makedon, Plotkin, Stein, Tardos, and Tragoudas [14].
Reference: [11] <author> D. E. Knuth. </author> <booktitle> The Art of Computer Programming, </booktitle> <volume> volume 2. </volume> <publisher> Addison-Wesley, </publisher> <year> 1973. </year>
Reference-contexts: 1 : A uniformly distributed random vector x on the surface of the n-dimensional unit sphere can be obtained by randomly generating n values x 1 ; : : : ; x n independently from the standard normal distribution, and then normalizing the resulting vector (e.g. see page 130 of <ref> [11] </ref>.) The standard normal distribution can be simulated using the uniform distribution be tween 0 and 1 (e.g. see page 117 of [11].) Let x = x (k) , where k = d 1 * ln ( 2n c * )e. <p> n values x 1 ; : : : ; x n independently from the standard normal distribution, and then normalizing the resulting vector (e.g. see page 130 of <ref> [11] </ref>.) The standard normal distribution can be simulated using the uniform distribution be tween 0 and 1 (e.g. see page 117 of [11].) Let x = x (k) , where k = d 1 * ln ( 2n c * )e. It follows from Lemma 5 and Lemma 6 that A * (xx T ) (1 + *) 1 1 holds with probability at least 1 n c .
Reference: [12] <author> M. Kojima, S. Shindoh, and S. Hara. </author> <title> Interior-Point Methods for the Monotone Linear Complementarity Problem in Symmetric Matrices. </title> <type> Technical Report B-282, </type> <institution> Department of Information Sciences, Tokyo Inst. of Technology, </institution> <year> 1994. </year>
Reference-contexts: Nesterov and Nemirovsky showed [17] how to use the interior-point method to solve semidefinite programs. Alizadeh [1] showed how an interior-point algorithm for linear programming could be directly generalized to handle semidefinite programming. Since the work of Alizadeh, there has been a great deal of research into such algorithms <ref> [8, 24, 20, 25, 6, 12, 3] </ref>. Essentially, the number of iterations is roughly ~ O ( p and each iteration involves either factoring a matrix or solving a least-squares problem. The size of either problem is sfin 2 where s is the number of constraints defining the semidefinite program.
Reference: [13] <author> P. Lancaster and M. Tismenetsky. </author> <title> The Theory of Matrices. </title> <publisher> Academic Press, </publisher> <year> 1985. </year>
Reference-contexts: Proof Since L = L + I=n, it follows that I * X = n (1 L * X). Note that L - 0 by Gersgorin's theorem (e.g. see x10.6 of <ref> [13] </ref>). Since also X - 0, we obtain L * X 0, and thus X 11 + + X nn = I * X n. Since X - 0, each X ii is nonnegative. It follows that X ii n for every 1 i n. <p> The procedure InitialC (G) obtains an initial so lution (X; ) as follows: X ij = &lt; 1 if i = j n if ij is an edge of G 0 otherwise: Clearly X - 0 (e.g. see the Gersgorin's theorem in x10.6 of <ref> [13] </ref>), and thus the initial solution (X; 1 n ) is feasible. At the beginning of the procedure ImproveC (X; ; *), we assume that fl = = 1 + O (*).
Reference: [14] <author> T. Leighton, F. Makedon, S. Plotkin, C. Stein, E. Tardos, and S. Tragoudas. </author> <title> Fast approximation algorithms for multicommodity flow problems. </title> <booktitle> In Proceedings of the Twenty Third Annual ACM Symposium on Theory of Computing, </booktitle> <pages> pages 101-111, </pages> <address> New Orleans, Louisiana, </address> <month> 6-8 May </month> <year> 1991. </year>
Reference-contexts: One important ingredient in the improvement is a formulation of approximate optimality (relaxed complementary slackness conditions) that works well in this setting. This multicommodity flow algorithm was generalized by Leighton, Makedon, Plotkin, Stein, Tardos, and Tragoudas <ref> [14] </ref>. Plotkin, Shmoys, and Tardos then took a final step and generalized this multicom-modity flow algorithm, obtaining not a single algorithm but a whole framework in which algorithms for a variety of problems could be formulated.
Reference: [15] <author> L. Lovasz. </author> <title> On the Shannon Capacity of a graph. </title> <journal> IEEE Transactions on Information Theory, </journal> <volume> IT-25:1-7, </volume> <year> 1979. </year>
Reference-contexts: 1 Introduction It is well-established that linear programs can be useful in (1) estimating the value of an integer program, and (2) obtaining approximately optimum solutions to an integer program. Similar use of semidefinite programming is emerging as an important technique. Lovasz <ref> [15] </ref> showed semidefinite programming could be used to compute the Shannon capacity of a graph, often referred to as the theta function; this is a number that lies between the size of the maximum clique and the minimum number of colors.
Reference: [16] <author> L. Lovasz and A. Schrijver. </author> <title> Cones of Matrices and Setfunctions, </title> <journal> and 0-1 Optimization. SIAM Journal on Optimization, </journal> <volume> 1(2) </volume> <pages> 166-190, </pages> <year> 1991. </year>
Reference-contexts: Lovasz [15] showed semidefinite programming could be used to compute the Shannon capacity of a graph, often referred to as the theta function; this is a number that lies between the size of the maximum clique and the minimum number of colors. Lovasz and Schri-jver <ref> [16] </ref> described a way to use semidefinite programming to estimate the value of integer programs. In an important recent breakthrough, Goemans and Williamson [4] discovered an approximation algorithm for graph MAX CUT whose accuracy is significantly better than that of the previously known algorithms.
Reference: [17] <author> Y. Nesterov and A. Nemirovskii. </author> <title> Interior Point Polynomial Methods in Convex Programming: Theory and Applications. </title> <institution> Society for Industrial and Applied Mathematics, </institution> <address> Philadelphia, </address> <year> 1994. </year>
Reference-contexts: This subroutine, combined with the ellipsoid method, provided a polynomial-time algorithm for semidefinite programming. However, the complexity of this algorithm is quite high. Nesterov and Nemirovsky showed <ref> [17] </ref> how to use the interior-point method to solve semidefinite programs. Alizadeh [1] showed how an interior-point algorithm for linear programming could be directly generalized to handle semidefinite programming.
Reference: [18] <author> S. A. Plotkin, D. B. Shmoys, and E. Tardos. </author> <title> Fast approximation algorithms for fractional packing and covering problems. </title> <booktitle> In 32nd Annual Symposium on Foundations of Computer Science, </booktitle> <pages> pages 495-504, </pages> <address> San Juan, Puerto Rico, 1-4 Oct. 1991. </address> <publisher> IEEE. </publisher>
Reference-contexts: The time required as is at least that of the algorithm of Goemans and Williamson. In this paper, we propose an alternative approach. We apply the method of Plotkin, Shmoys, and Tar-dos <ref> [18] </ref> to find an approximate solution to each of these semidefinite programs. As the core of these algorithms, we use the power method to solve an eigenvalue problem. The power method can exploit the sparsity of the matrix, which in turn reflects the sparsity of the graph. <p> Since X - 0, each X ii is nonnegative. It follows that X ii n for every 1 i n. We now apply the framework of Plotkin, Shmoys, and Tardos <ref> [18] </ref>. We view (5) as min s.t. X ii for every 1 i n X 2 P; where P = fX : L * X = 1; X - 0g. The algorithm depends on a parameter that Plotkin, Shmoys, and Tardos call the width of the formulation. <p> X ii = 1 for every 1 i n X ij for every edge ij of G X - 0: In order to put the problem into the framework of Plotkin, Shmoys, and Tardos <ref> [18] </ref>, we can reformulate this program as follows. max fl ij fl for every edge ij of G X 2 P 0 ; where P 0 = fX 0 : X 0 ii = 1; X 0 - 0g. Let P = P 0 . <p> Let P = P 0 . Like P , P 0 is a convex body. The framework of <ref> [18] </ref> requires that we optimize over P 0 . Equivalently, we can optimize over P . 7 5.1 Definitions In this section, we say a matrix X is feasible if X 2 P . <p> By Lemma 7 we know the width of (10) is one. It follows from the framework of <ref> [18] </ref> that the number of iterations is O (* 2 log n). Suppose G is k-colorable. It follows from Lemma 9 that DirectionC can be implemented as VectorMaxCut (Y; * 6k ), which takes time ~ O (* 3 k 3 n log n).
Reference: [19] <author> G. B. Price. </author> <title> Multivariable Analysis. </title> <publisher> Springer-Verlag, </publisher> <year> 1984. </year>
Reference-contexts: Clearly Pr ((x x 0 ) 2 a 2 ) = Pr (jx x 0 j a). We show Pr (jx x 0 j a) n c . Let ^ = arccos a. It is known by multivariable anal ysis (e.g. see x60 of <ref> [19] </ref>) that Pr (jx yj a) = S ^ S ^ ); where S = Z =2 sin n2 d. Note that 2 sin , for any 2 . It follows that 2 ^ 2 sin ( 2 cos ^ = 2a = n c1 . <p> Note that 2 sin , for any 2 . It follows that 2 ^ 2 sin ( 2 cos ^ = 2a = n c1 . Therefore it suffices to show that S 1 n . It is known (e.g. see (131) of x60 in <ref> [19] </ref>) that S = 2 + 1)n n=2 2 + 1) for any n 2.
Reference: [20] <author> F. Rendl, R. Vanderbei, and H. Wolkowicz. </author> <title> A primal-dual interior-point method for the max-min eigenvalue problem. </title> <type> Technical report, </type> <institution> University of Waterloo, Dept. of Combinatorics and Optimization, </institution> <year> 1993. </year>
Reference-contexts: Nesterov and Nemirovsky showed [17] how to use the interior-point method to solve semidefinite programs. Alizadeh [1] showed how an interior-point algorithm for linear programming could be directly generalized to handle semidefinite programming. Since the work of Alizadeh, there has been a great deal of research into such algorithms <ref> [8, 24, 20, 25, 6, 12, 3] </ref>. Essentially, the number of iterations is roughly ~ O ( p and each iteration involves either factoring a matrix or solving a least-squares problem. The size of either problem is sfin 2 where s is the number of constraints defining the semidefinite program.
Reference: [21] <author> S. Sahni and T. Gonzalez. </author> <title> P-complete approximation problems. </title> <journal> Journal of the ACM, </journal> <volume> 23(3) </volume> <pages> 555-565, </pages> <month> July </month> <year> 1976. </year>
Reference-contexts: We also need to show how to obtain a good initial solution to the semidefinite program. For this, we use the previously known approximation algorithm for MAX CUT, due to Sahni and Gonzalez <ref> [21] </ref>. Finally, we show that after only only O (* 2 n log n) iterations of optimizing over P , we obtain an *-optimal solution X to the semidefinite program. The time required is thus O (* 3 nm log 2 n). <p> The procedure Initial (C) obtains an initial solution (X; ) from a greedy solution to the MAX CUT problem. Using the greedy method by Sahni and Gonzalez <ref> [21] </ref>, one can obtain a 1-optimal solution to the MAX CUT problem in time linear in the number of edges. Moreover such a greedy cut has value at least one half of the sum of edge costs.
Reference: [22] <author> F. Shahrokhi and D. W. Matula. </author> <title> The maximum concurrent flow problem. </title> <journal> Journal of the ACM, </journal> <volume> 37(2) </volume> <pages> 318-334, </pages> <month> Apr. </month> <year> 1990. </year>
Reference-contexts: These penalties are used to select a direction to move from the current candidate solution to obtain the next candidate solution, and the process is repeated. Some of the particulars of their approach originated in the work of Shahrokhi and Matula <ref> [22] </ref> on approximate solution of a multicommodity flow problem. Shahrokhi and Matula proved a polynomial but rather high bound on the running time of their algorithm. A much faster algorithm for this problem was developed by Klein, Plotkin, Stein, and Tardos [10].
Reference: [23] <author> P. M. Vaidya. </author> <title> Speeding-up linear programming using fast matrix multiplication (extended abstract). </title> <booktitle> In 30th Annual Symposium on Foundations of Computer Science, </booktitle> <pages> pages 332-337, </pages> <address> Research Triangle Park, North Carolina, 30 Oct.-1 Nov. 1989. </address> <publisher> IEEE. </publisher>
Reference-contexts: Plotkin, Shmoys, and Tardos then took a final step and generalized this multicom-modity flow algorithm, obtaining not a single algorithm but a whole framework in which algorithms for a variety of problems could be formulated. This framework, like that of the ellipsoid algorithm [5] and the method of Vaidya <ref> [23] </ref>, casts algorithms in terms of a subroutine, an oracle. For the ellipsoid algorithm, the subroutine is called a separation oracle. For Plotkin, Shmoys, and Tardos, the subroutine must find an optimum (or near-optimum) solution to a simpler optimization problem.
Reference: [24] <author> L. Vandenberghe and S. Boyd. </author> <title> A primal-dual potential reduction method for problems involving matrix inequalities. </title> <type> Technical report, </type> <institution> Information Systems Laboratory, Department of Electrical Engineering, Stanford University, Stanford, </institution> <address> CA, </address> <month> January </month> <year> 1993. </year> <note> Math. Prog. (to appear). </note>
Reference-contexts: Nesterov and Nemirovsky showed [17] how to use the interior-point method to solve semidefinite programs. Alizadeh [1] showed how an interior-point algorithm for linear programming could be directly generalized to handle semidefinite programming. Since the work of Alizadeh, there has been a great deal of research into such algorithms <ref> [8, 24, 20, 25, 6, 12, 3] </ref>. Essentially, the number of iterations is roughly ~ O ( p and each iteration involves either factoring a matrix or solving a least-squares problem. The size of either problem is sfin 2 where s is the number of constraints defining the semidefinite program.
Reference: [25] <author> A. Yoshise. </author> <title> An optimization method for convex programs-interior-point method and analytical center. </title> <journal> Systems, Control and Information, </journal> <volume> 38(3) </volume> <pages> 155-160, </pages> <month> Mar. </month> <year> 1994. </year> <month> 10 </month>
Reference-contexts: Nesterov and Nemirovsky showed [17] how to use the interior-point method to solve semidefinite programs. Alizadeh [1] showed how an interior-point algorithm for linear programming could be directly generalized to handle semidefinite programming. Since the work of Alizadeh, there has been a great deal of research into such algorithms <ref> [8, 24, 20, 25, 6, 12, 3] </ref>. Essentially, the number of iterations is roughly ~ O ( p and each iteration involves either factoring a matrix or solving a least-squares problem. The size of either problem is sfin 2 where s is the number of constraints defining the semidefinite program.
References-found: 25

