URL: ftp://ftp.ai.univie.ac.at/papers/oefai-tr-96-23.ps.Z
Refering-URL: http://www.ph.tn.tudelft.nl/PRInfo/reports/msg00163.html
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Email: arthur@ai.univie.ac.at  
Title: Limitations of self-organizing maps for vector quantization and multidimensional scaling  
Author: Arthur Flexer and 
Date: oefai-tr-96-23  
Address: Schottengasse 3, A-1010 Vienna, Austria  Liebiggasse 5, A-1010 Vienna, Austria  
Affiliation: The Austrian Research Institute for Artificial Intelligence  Department of Psychology, University of Vienna  
Pubnum: Technical Report  
Abstract: The limitations of using self-organizing maps (SOM) for either clustering/vector quantization (VQ) or multidimensional scaling (MDS) are being discussed by reviewing recent empirical findings and the relevant theory. SOM's remaining ability of doing both VQ and MDS at the same time is challenged by a new combined technique of online K-means clustering plus Sammon mapping of the cluster centroids. SOM are shown to perform significantly worse in terms of quantization error, in recovering the structure of the clusters and in preserving the topology in a comprehensive empirical study using a series of multivariate normal clustering problems.
Abstract-found: 1
Intro-found: 1
Reference: [Balakrishnan et al. 94] <author> Balakrishnan P.V., Cooper M.C., </author> <title> Jacob V.S., Lewis P.A.: A study of the classification capabilities of neural networks using unsupervised learning: a comparison with k-means clustering, </title> <journal> Psychometrika, </journal> <volume> Vol. 59, No. 4, </volume> <pages> 509-525, </pages> <year> 1994. </year>
Reference-contexts: has to be answered: Should SOM be used for doing VQ, MDS, both at the same time or none of them? Two recent comprehensive studies comparing SOM either to traditional VQ or MDS techniques separately seem to indicate that SOM is not competitive when used for either VQ or MDS: <ref> [Balakrishnan et al. 94] </ref> compare SOM to traditional K-means clustering on 108 multivariate normal clustering problems with known clustering solutions and show that SOM performs significantly worse in terms of data points misclassified 1 , especially with higher numbers of clusters in the data sets. [Bezdek & Nikhil 95] compare SOM <p> The multivariate normal distributions were generated using the procedure by [Milligan & Cooper 85], which since has been used for several comparisons of cluster algorithms (see e.g. <ref> [Balakrishnan et al. 94] </ref>). The marginal normal distributions gave internal cohesion of the clusters by warranting that more than 99% of the data lie within 3 standard deviations ().
Reference: [Bezdek & Nikhil 95] <author> Bezdek J.C., Nikhil R.P.: </author> <title> An index of topological preservation for feature extraction, </title> <journal> Pattern Recognition, </journal> <volume> Vol. 28, No. 3, pp.381-391, </volume> <year> 1995. </year>
Reference-contexts: for either VQ or MDS: [Balakrishnan et al. 94] compare SOM to traditional K-means clustering on 108 multivariate normal clustering problems with known clustering solutions and show that SOM performs significantly worse in terms of data points misclassified 1 , especially with higher numbers of clusters in the data sets. <ref> [Bezdek & Nikhil 95] </ref> compare SOM to principal component analysis and the MDS-technique Sammon mapping on seven artificial data sets with different numbers of points and dimensionality and different shapes of input distributions. <p> Note that for SOM the coordinates of the code book vectors on the planar grid were used to compute the ^ d. Of course, an algorithm that perfectly preserves topology would preserve such distances and hence yield a value of 1:0 (see <ref> [Bezdek & Nikhil 95] </ref> for a discussion of measures of topolgy preservation). For each cell in the full-factorial 2 fi 2 fi 3 design 3 data sets with 25 points for each cluster were generated resulting in a total of 36 data sets. <p> In this work we examined the vague concept of using SOM as a "data visualization tool" both from a theoretical and empirical point of view. SOM cannot outperform traditional VQ techniques in terms of quantization error and that SOM should therefore not be used for doing VQ. From <ref> [Bezdek & Nikhil 95] </ref> as well as from our discussion of SOM's restriction to planar grids in the output space which allows only a restricted number of different distances to be represented, it should be evident that SOM is also a rather crude way of doing MDS.
Reference: [Bottou & Bengio 95] <author> Bottou L., Bengio Y.: </author> <title> Convergence Properties of the K-Means Algorithms, </title> <editor> in [Tesauro et al. </editor> <volume> 95]. </volume>
Reference-contexts: If the degree of neighbourhood is decreased to zero, the SOM-algorithm becomes equal to the oKMC-algorithm. Whereas local convergence is guaranteed for oKMC (at least for decreasing ff, <ref> [Bottou & Bengio 95] </ref>), no general proof for the convergence of SOM with nonzero neighbourhood is known. [Kohonen 95, p.128] notes that the last steps of the SOM algorithm should be computed with zero neighbourhood in order to guarantee "the most accurate density approximation of the input samples". 3 SOM and
Reference: [Erwin et al. 92] <author> Erwin E., Obermayer K., Schulten K.: </author> <title> Self-organizing maps: ordering, convergence properties and energy functions, </title> <journal> Biological Cybernetics, </journal> <volume> 67, 47- 55, </volume> <year> 1992. </year>
Reference-contexts: The data was normalized to zero mean and unit variance in all dimensions. 2 Note that for MDS not the actual coordinates of the points in the input space but only their distances or the ordering of the latter are needed. 3 <ref> [Erwin et al. 92] </ref> even showed that generally no energy function that SOM can be considered to minimize exists. algorithm no. clusters dimension msqe Rand corr.
Reference: [Hofmann & Buhmann 95] <author> Hofmann T., Buhmann J.: </author> <title> Multidimensional Scaling and Data Clustering, </title> <editor> in [Tesauro et al. </editor> <volume> 95]. </volume>
Reference-contexts: Whether it is a good idea to combine clustering or vector quantization and multidimensional scaling at all and whether more principled approaches towards such a combination (see e.g. <ref> [Hofmann & Buhmann 95] </ref>) can yield even better results than our oKMC+ and last but not least what self-organizing maps should be used for under this new light remain questions to be answered by future investigations. Acknowledgements: Thanks are due to James Pardey, University of Oxford, for the Sammon code.
Reference: [Hubert & Arabie 85] <author> Hubert L.J., Arabie P.: </author> <title> Comparing partitions, </title> <journal> J. of Classification, </journal> <volume> 2, </volume> <pages> 63-76, </pages> <year> 1985. </year>
Reference-contexts: Factor 2, Number of clusters was set to 4 and 9. Factor 3, Number of dimensions was set to 4; 6; or8. Dependent variable 1: mean squared error was computed using formula (1). Dependent variable 2, Rand index (see <ref> [Hubert & Arabie 85] </ref>) is a measure of agreement between the true, known partition structure and the obtained clusters. Both the numerator and the denominator of the index reflect frequency counts.
Reference: [Jolliffe 86] <author> Jolliffe I.T.: </author> <title> Principal Component Analysis, </title> <publisher> Springer, </publisher> <year> 1986. </year>
Reference-contexts: Techniques for finding such transformations are, among others, various forms of multidimensional scaling 2 (MDS) like metric MDS [Torgerson 52], nonmetric MDS [Shepard 62] or Sammon mapping [Sammon 69], but also principal component analysis (PCA) (see e.g. <ref> [Jolliffe 86] </ref>) or SOM.
Reference: [Kohonen 84] <author> Kohonen T.: </author> <title> Self-Organization and Associative Memory, </title> <publisher> Springer, </publisher> <year> 1984. </year>
Reference-contexts: 1 Introduction Self-organizing maps (SOM) introduced by <ref> [Kohonen 84] </ref> are a very popular tool used for visualization of high dimensional data spaces.
Reference: [Kohonen 95] <author> Kohonen T.: </author> <title> Self-organizing maps, </title> <publisher> Springer, </publisher> <address> Berlin, </address> <year> 1995. </year>
Reference-contexts: Although the level of activity and research around the SOM algorithm is quite large (a recent overview of the literature by <ref> [Kohonen 95] </ref> contains more than 1000 citations), only little comparison among the numerous existing variants of the basic approach and also to more traditional statistical techniques of the larger frameworks of VQ and MDS is available. <p> If the degree of neighbourhood is decreased to zero, the SOM-algorithm becomes equal to the oKMC-algorithm. Whereas local convergence is guaranteed for oKMC (at least for decreasing ff, [Bottou & Bengio 95]), no general proof for the convergence of SOM with nonzero neighbourhood is known. <ref> [Kohonen 95, p.128] </ref> notes that the last steps of the SOM algorithm should be computed with zero neighbourhood in order to guarantee "the most accurate density approximation of the input samples". 3 SOM and multidimensional scaling Formally, a topology preserving algorithm is a transformation : R k 7! R p , <p> During the second phase (10000 code book updates) ff was set to 0:02 and the radius of the neighbourhood to 0 to guarantee the most accurate vector quantization <ref> [Kohonen 95, p.128] </ref>. The oKMC+ algorithm had the parameter ff fixed to 0:02 and was trained using each data set 20 times, the minimization of formula (5) was stopped after 100 iterations. <p> In fact it seems to be common practice to apply SOM with numbers of code book vectors that are a multiple of the input vectors available for training (see e.g. <ref> [Kohonen 95, pp.113] </ref>).
Reference: [Linde et al. 80] <author> Linde Y., Buzo A., Gray R.M.: </author> <title> An Algorithm for Vector Quantizer Design, </title> <journal> IEEE Transactions on Communications, </journal> <volume> Vol. COM-28, No. 1, </volume> <month> January, </month> <year> 1980. </year>
Reference-contexts: A together with the partition S = fS i ; i = 1; : : :; N g, of the input vector space into the sets S i = fx : q (x) = ^x i g of input vectors mapping into the i th reproduction vector (or code word) <ref> [Linde et al. 80] </ref>. <p> wrong cluster center is an appropriate and commonly used performance measure for cluster procedures if the true cluster structure is known. distortion d: D = n i=0 k1 X j x i ^x i j 2 (2) The classical vector quantization technique to achieve such a mapping is the LBG-algorithm <ref> [Linde et al. 80] </ref>, where a given quantizer is iteratively improved. Already [Linde et al. 80] noted that their proposed algorithm is almost similar to the k-means approach developed in the cluster analysis literature starting from [MacQueen 67]. <p> cluster procedures if the true cluster structure is known. distortion d: D = n i=0 k1 X j x i ^x i j 2 (2) The classical vector quantization technique to achieve such a mapping is the LBG-algorithm <ref> [Linde et al. 80] </ref>, where a given quantizer is iteratively improved. Already [Linde et al. 80] noted that their proposed algorithm is almost similar to the k-means approach developed in the cluster analysis literature starting from [MacQueen 67]. Closely related to SOM is online K-means clustering (oKMC) consisting of the following steps: 1.
Reference: [MacQueen 67] <author> MacQueen J.: </author> <title> Some Methods for Classification and Analysis of Multivari-ate Observations, </title> <booktitle> Proc. of the Fifth Berkeley Symposium on Math., Stat. and Prob., </booktitle> <volume> Vol. 1, </volume> <pages> pp. 281-296, </pages> <year> 1967. </year>
Reference-contexts: Already [Linde et al. 80] noted that their proposed algorithm is almost similar to the k-means approach developed in the cluster analysis literature starting from <ref> [MacQueen 67] </ref>. Closely related to SOM is online K-means clustering (oKMC) consisting of the following steps: 1.
Reference: [Milligan & Cooper 85] <author> Milligan G.W., Cooper M.C.: </author> <title> An examination of procedures for determining the number of clusters in a data set, </title> <type> Psychometrika 50(2), </type> <pages> 159-179, </pages> <year> 1985. </year>
Reference-contexts: This combined algorithm is abbreviated oKMC+. 5 Empirical comparison The empirical comparison was done using a 3 factorial experimental design with 3 dependent variables. The multivariate normal distributions were generated using the procedure by <ref> [Milligan & Cooper 85] </ref>, which since has been used for several comparisons of cluster algorithms (see e.g. [Balakrishnan et al. 94]). The marginal normal distributions gave internal cohesion of the clusters by warranting that more than 99% of the data lie within 3 standard deviations ().
Reference: [Sammon 69] <author> Sammon J.W.: </author> <title> A Nonlinear Mapping for Data Structure Analysis, </title> <journal> IEEE Transactions on Comp., </journal> <volume> Vol. C-18, No. 5, p.401-409, </volume> <year> 1969. </year>
Reference-contexts: Techniques for finding such transformations are, among others, various forms of multidimensional scaling 2 (MDS) like metric MDS [Torgerson 52], nonmetric MDS [Shepard 62] or Sammon mapping <ref> [Sammon 69] </ref>, but also principal component analysis (PCA) (see e.g. [Jolliffe 86]) or SOM.
Reference: [Shepard 62] <author> Shepard R.N.: </author> <title> The analysis of proximities: multidimensional scaling with an unknown distance function. I., </title> <journal> Psychometrika, </journal> <volume> Vol. 27, No. 2, p.125-140, </volume> <year> 1962. </year>
Reference-contexts: Techniques for finding such transformations are, among others, various forms of multidimensional scaling 2 (MDS) like metric MDS [Torgerson 52], nonmetric MDS <ref> [Shepard 62] </ref> or Sammon mapping [Sammon 69], but also principal component analysis (PCA) (see e.g. [Jolliffe 86]) or SOM.
Reference: [Tesauro et al. 95] <editor> Tesauro G., Touretzky D., Leen T.K.(eds.): </editor> <booktitle> Advances in Neural Infor--mation Processing System 7, </booktitle> <publisher> MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1995. </year>
Reference: [Torgerson 52] <author> Torgerson W.S.: </author> <title> Multidimensional Scaling, I: theory and method, </title> <journal> Psy-chometrika, </journal> <volume> 17, </volume> <pages> 401-419, </pages> <year> 1952. </year>
Reference-contexts: Techniques for finding such transformations are, among others, various forms of multidimensional scaling 2 (MDS) like metric MDS <ref> [Torgerson 52] </ref>, nonmetric MDS [Shepard 62] or Sammon mapping [Sammon 69], but also principal component analysis (PCA) (see e.g. [Jolliffe 86]) or SOM.
References-found: 16

