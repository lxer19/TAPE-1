URL: http://www.speech.sri.com/people/konig/papers/nips95-herve.ps
Refering-URL: http://www.speech.sri.com/people/konig/resume.html
Root-URL: 
Email: fkonig,bourlard,morgang@icsi.berkeley.edu  
Title: REMAP: Recursive Estimation and Maximization of A Posteriori Probabilities Application to Transition-Based Connectionist Speech Recognition  
Author: Yochai Konig, Herve Bourlard and Nelson Morgan 
Address: 1947 Center Street Berkeley, CA 94704, USA.  
Affiliation: International Computer Science Institute  
Abstract: In this paper, we introduce REMAP, an approach for the training and estimation of posterior probabilities using a recursive algorithm that is reminiscent of the EM-based Forward-Backward (Liporace 1982) algorithm for the estimation of sequence likelihoods. Although very general, the method is developed in the context of a statistical model for transition-based speech recognition using Artificial Neural Networks (ANN) to generate probabilities for Hidden Markov Models (HMMs). In the new approach, we use local conditional posterior probabilities of transitions to estimate global posterior probabilities of word sequences. Although we still use ANNs to estimate posterior probabilities, the network is trained with targets that are themselves estimates of local posterior probabilities. An initial experimental result shows a significant decrease in error-rate in comparison to a baseline system.
Abstract-found: 1
Intro-found: 1
Reference: <author> Bengio, Y., & P. Frasconi. </author> <year> 1995. </year> <title> An input output HMM architecture. </title> <booktitle> In Advances in Neural Information Processing Systems, </booktitle> <editor> ed. by G. Tesauro, D. Touretzky, & T. Leen, </editor> <volume> volume 7. </volume> <publisher> Cambridge: MIT press. </publisher> ||, <editor> R. De Mori, G. Flammia, & R. Kompe. </editor> <year> 1992. </year> <title> Global optimization of a neural network-hidden Markov model hybrid. </title> <journal> IEEE trans. on Neural Networks 3.252-258. </journal>
Reference-contexts: Similarly, REMAP increases the global posterior function during the M step (in the direction of targets that actually maximize that global function), rather than actually maximizing it. Recently, a similar approach was suggested for mapping input sequences to output sequences <ref> (Bengio & Frasconi 1995) </ref>. 3 Note here that one "iteration" does not stand for one iteration of the MLP training but for one estimation-maximization iteration for which a complete MLP training will be required. 4 This can be done, for instance, by training up such a net from a hand-labeled database
Reference: <author> Bourlard, H., Y. Konig, & N. Morgan. </author> <year> 1994. </year> <title> REMAP: Recursive estimation and maximization of a posteriori probabilities, application to transition-based connectionist speech recognition. </title> <type> Technical Report TR-94-064, </type> <institution> International Computer Science Institute, Berkeley, </institution> <address> CA. ||, & N. </address> <publisher> Morgan. </publisher> <year> 1994. </year> <title> Connectionist Speech Recognition A Hybrid Approach. </title> <publisher> Kluwer Academic Publishers. </publisher> ||, & <editor> C. J. Wellekens. </editor> <year> 1989. </year> <title> Links between Markov models and multilayer perceptrons. </title> <booktitle> In Advances in Neural Information Processing Systems 1 , ed. by D.J. Touretzky, </booktitle> <pages> 502-510, </pages> <address> San Mateo. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: In <ref> (Bourlard & Morgan 1994) </ref>, summarizing earlier work (such as (Bourlard & Wellekens 1989)), we showed that it was possible to compute the global a posteriori probability P (M jX) of a discriminant form of Hidden Markov Model (Discriminant HMM), M , given a sequence of acoustic vectors X. <p> i ; q 1 ; q 2 ; : : : ; q N jX) = P (q 1 ; q 2 ; : : :; q N jX)P (M i jq 1 ; q 2 ; : : : ; q N ; X) Under the assumptions stated in <ref> (Bourlard & Morgan 1994) </ref> we can compute P (q 1 ; q 2 ; : : : ; q N jX) = n=1 The Discriminant HMM is thus described in terms of conditional transition proba bilities p (q ` n jq k n1 ; x n ), in which q ` <p> 2 [1; K] and 8n 2 [1; N ], 2. when training the MLP for iteration t+1, will lead to new estimates of fi t+1 and P (q ` n1 ; fi t+1 ) that are guaranteed to incrementally increase the global posterior probability P (M i jX; fi)? In <ref> (Bourlard et al. 1994) </ref>, we prove that a re-estimate of MLP targets that guarantee convergence to a local maximum of (1) is given by 1 : P fl (q ` n1 ; M ) = P (q ` n1 ; fi t ; M ) (2) where we have estimated the <p> In <ref> (Bourlard et al. 1994) </ref>, we further prove that alternating MLP target estimation (the "estimation" step) and MLP training (the "maximization" step) is guaranteed to incrementally increase (1) over t. 3 The remaining problem is to find an efficient algorithm to express P (q ` n jX; q k n1 ; M <p> We have developed several approaches to this esti mation, some of which are described in <ref> (Bourlard et al. 1994) </ref>. Currently, we are implementing this with an efficient recursion that estimates the sum of all possible paths in a model, for every possible transition at each possible time. <p> For every x n in the training database, train the MLP to minimize the relative entropy between the outputs and targets. See <ref> (Bourlard et al. 1994) </ref> for more details. This provides us with a new set of parameters fi t , for t = t + 1. 4. Iterate from 2 until convergence.
Reference: <author> Cole, R.A., M. Fanty, & T. Lander. </author> <year> 1994. </year> <title> Telephone speech corpus development at CSLU. </title> <booktitle> In Proceedings Int'l Conference on Spoken Language Processing, </booktitle> <address> Yokohama, Japan. </address>
Reference-contexts: It is a continuous speech database collected by CSLU at the Oregon Graduate Institute. It consists of numbers spoken naturally over telephone lines on the public-switched network <ref> (Cole et al. 1994) </ref>. The Numbers'93 database consists of 2167 speech files of spoken numbers produced by 1132 callers. We used 877 of these utterances for training and 657 for cross-validation and testing (200 for cross-validation) saving the remaining utterances for final testing purposes.
Reference: <author> Dempster, A. P., N. M. Laird, & D. B. Rubin. </author> <year> 1977. </year> <title> Maximum likelihood from incomplete data via the EM algorithm. </title> <journal> Journal of the Royal Statistical Society, Series B 34.1-38. </journal>
Reference-contexts: Iterate from 2 until convergence. This procedure is thus composed of two steps: an Estimation (E) step, corresponding to step 2 above, and a Maximization (M) step, corresponding to step 3 above. In this regards, it is reminiscent of the Estimation-Maximization (EM) algorithm as discussed in <ref> (Dempster et al. 1977) </ref>. However, in the standard EM algorithm, the M step involves the actual maximization of the likelihood function.
Reference: <author> Glass, J. R., </author> <year> 1988. </year> <title> Finding Acoustic Regularities in Speech Applications to Phonetic Recognition. </title> <publisher> M.I.T dissertation. </publisher>
Reference-contexts: However, it is well known that estimating transitions accurately is a difficult problem <ref> (Glass 1988) </ref>. Due to the inertia of the articulators, the boundaries between phones are blurred and overlapped in continuous speech. In our previous hybrid HMM/MLP system, targets were typically obtained by using a standard forced Viterbi alignment (segmentation).
Reference: <author> Katagiri, S., C.H. Lee, & Juang B.H. </author> <year> 1991. </year> <title> New discriminative training algorithms based on the generalized probabilistic decent method. </title> <booktitle> In Proc. of the IEEE Workshop on Neural Netwroks for Signal Processing , ed. by B.H. </booktitle>
Reference: <author> Juang, S.Y. Kung, </author> & <title> C.A. </title> <booktitle> Kamm, </booktitle> <pages> 299-308. </pages>
Reference: <author> Konig, Y., & N. Morgan. </author> <year> 1994. </year> <title> Modeling dynamics in connectionist speech recognition the time index model. </title> <booktitle> In Proceedings Int'l Conference on Spoken Language Processing , 1523-1526, </booktitle> <address> Yokohama, Japan. </address>
Reference: <author> Liporace, L. A. </author> <year> 1982. </year> <title> Maximum likelihood estimation for multivariate observations of markov sources. </title> <journal> IEEE Trans. on Information Theory IT-28.729-734. </journal>
Reference-contexts: One possible solution to these problems is to use a full MAP algorithm to find transition probabilities at each frame for all possible transitions by a forward-backward-like algorithm <ref> (Liporace 1982) </ref>, taking all possible paths into account. 2.2 PROBLEM FORMULATION As described above, global maximum a posteriori training of HMMs should find the optimal parameter set fi maximizing J Y P (M j jX j ; fi) (1) in which M j represents the Markov model associated with each training
Reference: <author> Morgan, N., H. Bourlard, S. Greenberg, & H. Hermansky. </author> <year> 1994. </year> <title> Stochastic perceptual auditory-event-based models for speech recognition. </title> <booktitle> In Proceedings Int'l Conference on Spoken Language Processing, </booktitle> <pages> 1943-1946, </pages> <address> Yokohama, Japan. </address>
Reference-contexts: In <ref> (Bourlard & Morgan 1994) </ref>, summarizing earlier work (such as (Bourlard & Wellekens 1989)), we showed that it was possible to compute the global a posteriori probability P (M jX) of a discriminant form of Hidden Markov Model (Discriminant HMM), M , given a sequence of acoustic vectors X. <p> i ; q 1 ; q 2 ; : : : ; q N jX) = P (q 1 ; q 2 ; : : :; q N jX)P (M i jq 1 ; q 2 ; : : : ; q N ; X) Under the assumptions stated in <ref> (Bourlard & Morgan 1994) </ref> we can compute P (q 1 ; q 2 ; : : : ; q N jX) = n=1 The Discriminant HMM is thus described in terms of conditional transition proba bilities p (q ` n jq k n1 ; x n ), in which q `
References-found: 10

