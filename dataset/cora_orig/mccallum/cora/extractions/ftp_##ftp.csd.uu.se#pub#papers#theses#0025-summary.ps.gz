URL: ftp://ftp.csd.uu.se/pub/papers/theses/0025-summary.ps.gz
Refering-URL: http://www.csd.uu.se/papers/long-theses.html
Root-URL: 
Title: Data-parallel Implementation of Prolog  
Author: Johan Bevemyr 
Degree: Thesis for the Degree of Doctor of Philosophy  
Date: UPPSALA 1996  
Address: Uppsala University  
Affiliation: Computing Science Department  
Note: UPPSALA THESES IN COMPUTING SCIENCE 25  
Abstract-found: 0
Intro-found: 0
Reference: <institution> The numbers inside braces indicate on which pages each citation occured. </institution>
Reference: 1. <author> H. At-Kaci, </author> <title> The WAM: A (Real)Tutorial, </title> <publisher> MIT Press, </publisher> <year> 1991. </year> <month> f4g </month>
Reference: 2. <author> K. Ali, </author> <title> Incremental Garbage Collection for OR-Parallel Prolog Based on WAM, </title> <booktitle> Gigalips Workshop, </booktitle> <year> 1989. </year> <month> f22g </month>
Reference-contexts: OR-parallel systems such as Muse [4] and Aurora [34, 105] use more or less sequential mark-sweep collectors <ref> [2, 56, 57] </ref>. The Aurora collector [185] designed by Weemeeuw and Demoen is slightly more complicated since Aurora uses the SRI-model [180] for OR-parallel execution with its more complicated data structures. The closest we get to a parallel collector for an AND-parallel Prolog is Cram-mond's [48] mark-sweep collector for Parlog.
Reference: 3. <author> K. Ali, </author> <title> A Parallel Copying Garbage Collection Scheme for Shared-Memory Multiprocessors, </title> <journal> New Generation Computing, </journal> <volume> 14(1) </volume> <pages> 53-77, </pages> <year> 1996. </year> <note> f12, 22g </note>
Reference-contexts: The algorithm uses a mark and copy technique for handling internal pointers. The algorithm uses ideas from our sequential Prolog collector, and from a general parallel copying collection scheme described by Ali <ref> [3] </ref>. We show how the resulting collector can be made generational. An improved strategy for load balancing for Prolog is presented. 1.6. <p> Its execution time is proportional to the size of the largest heap instead of the live data. The space requirements are, at worst, as large as for a copying collector since the import stacks may be as large as the live data. Ali <ref> [3] </ref> describes a general copying collector for shared memory. It is a two space collector (to- and from-space) where each subspace consists of a number of segments. Processing elements (PEs) allocate data in the segments in from-space. Collection takes place when from-space fills up.
Reference: 4. <author> K. Ali, R. Karlsson, </author> <title> The Muse OR-Parallel Prolog Model and its Performance, </title> <booktitle> Proc. North American Conf. Logic Programming, </booktitle> <publisher> MIT Press, </publisher> <year> 1990. </year> <month> f22g </month>
Reference-contexts: There is also a survey of collection schemes for sequential logic programming languages by Bekkers, Ridoux and Ungaro [25]. 22 Summary Parallel Collection Most parallel Prolog implementations do not include a garbage collector. OR-parallel systems such as Muse <ref> [4] </ref> and Aurora [34, 105] use more or less sequential mark-sweep collectors [2, 56, 57]. The Aurora collector [185] designed by Weemeeuw and Demoen is slightly more complicated since Aurora uses the SRI-model [180] for OR-parallel execution with its more complicated data structures.
Reference: 5. <author> S. Anderson, P. Hudak, </author> <title> Compilation of Haskell Array Comprehensions for Scientific Computing, </title> <booktitle> Proc. SIGPLAN'90 Conf. on Programming Language Design and Implementation, </booktitle> <publisher> ACM Press, </publisher> <year> 1990. </year> <month> f20g </month>
Reference-contexts: Meier also considers "backtracking" iteration, a subject we have not discussed here (compilation of bounded existential quantifications). It seems to us that the instruction set we use would be appropriate as a base also for Meier's methods. 20 Summary Array comprehensions in Haskell <ref> [5] </ref> can be used for expressing array operations, similar to our array extended bounded quantifications. The Common LISP language [154] (and some earlier LISP dialects), as well as Standard ML [72], contain iteration, mapping and reduction constructs that in some cases resemble ours.
Reference: 6. <author> A. W. Appel, </author> <title> A Runtime System, </title> <journal> Lisp and Symbolic Computation, </journal> <volume> 3(4) </volume> <pages> 343-380, </pages> <year> 1990. </year> <month> f21g </month>
Reference-contexts: We want to avoid this situation. Their algorithm does preserve the segment-structure of the heap (but not the ordering within a segment). Hence, they can reclaim all memory by backtracking. In contrast, our algorithm only supports partial reclamation of memory by backtracking. Appel <ref> [6, 7] </ref> describes a simple generational garbage collector for Standard ML. The collector uses Cheney's [39] garbage collection algorithm, which is the basis of our algorithm as well. However, Appel's collector relies on assignments being infrequent. In Prolog, variable binding is assignment in this sense.
Reference: 7. <author> A. W. Appel, </author> <title> Simple Generational Garbage Collection and Fast Allocation, </title> <journal> Software|Practice and Experience, </journal> <volume> 19(2) </volume> <pages> 171-183, </pages> <year> 1989. </year> <note> f20, 21g </note>
Reference-contexts: Garbage Collection of Prolog Sequential Collection It has been observed in other languages that most data tend to be short lived [54, 172]. This insight led to the invention of generational garbage collection <ref> [7, 98] </ref> where young objects are garbage collected more often than old. Copying collectors [60, 112] and generational copying collectors have been considered unsuitable for Prolog until recently. Prolog implementations such as SICStus Prolog [35] use a mark-sweep algorithm [106] that first marks the live data, then compacts the heap. <p> We want to avoid this situation. Their algorithm does preserve the segment-structure of the heap (but not the ordering within a segment). Hence, they can reclaim all memory by backtracking. In contrast, our algorithm only supports partial reclamation of memory by backtracking. Appel <ref> [6, 7] </ref> describes a simple generational garbage collector for Standard ML. The collector uses Cheney's [39] garbage collection algorithm, which is the basis of our algorithm as well. However, Appel's collector relies on assignments being infrequent. In Prolog, variable binding is assignment in this sense.
Reference: 8. <author> A. W. Appel, </author> <title> Garbage Collection, Advanced Language Implementations, </title> <publisher> MIT Press, </publisher> <year> 1991. </year> <month> f21g </month>
Reference-contexts: It was designed after our collector and use our ideas for handling internal pointers. It is not clear how their algorithm can be made generational. Cohen [45], Appel <ref> [8] </ref>, Jones and Lins [88], and Wilson [186] have written comprehensive surveys on general-purpose garbage collection algorithms. There is also a survey of collection schemes for sequential logic programming languages by Bekkers, Ridoux and Ungaro [25]. 22 Summary Parallel Collection Most parallel Prolog implementations do not include a garbage collector.
Reference: 9. <author> K. Appleby, M. Carlsson, S. Haridi, D. Sahlin, </author> <title> Garbage Collection for Prolog Based on WAM, </title> <journal> Communications of the ACM, </journal> <volume> 31(6) </volume> <pages> 719-741, </pages> <month> June </month> <year> 1988. </year> <note> f20, 21g 25 26 BIBLIOGRAPHY </note>
Reference-contexts: Copying collectors [60, 112] and generational copying collectors have been considered unsuitable for Prolog until recently. Prolog implementations such as SICStus Prolog [35] use a mark-sweep algorithm [106] that first marks the live data, then compacts the heap. We take the implementation of Appleby et al. <ref> [9] </ref> as typical. It is based on the Deutsch-Schorr-Waite [45, 139] algorithm for marking and on Morris' algorithm [45, 117] for compacting. Touati and Hama [163] developed a generational copying garbage collector for Prolog. The heap is split into an old and a new generation. <p> However, Appel's collector relies on assignments being infrequent. In Prolog, variable binding is assignment in this sense. Our algorithm handles frequent assignments efficiently. Sahlin [132] has developed a method that makes the execution time of the Appleby et al. <ref> [9] </ref> algorithm proportional to the size of the live data. The main drawback of Sahlin's algorithm is that implementing the mark-sweep algorithm becomes more difficult, not to mention guaranteeing that there are no programming errors in its implementation. To our knowledge it has never been implemented.
Reference: 10. <author> K. R. Apt, </author> <title> Arrays, Bounded Quantifications and Iteration in Logic and Constraint Logic Programming, </title> <booktitle> Science of Computer Programming, </booktitle> <address> 26(1-3):133-148, </address> <year> 1996. </year> <month> f19g </month>
Reference-contexts: Some authors have studied the use of bounded quantifiers with sets, for example in SETL [140] and flogg [55]. Barklund & Hill [21] have studied how to incorporate restricted quantifications and arrays in Godel [81], while Apt <ref> [10] </ref> has studied how bounded quantifications and arrays could be used also in constraint based languages. Lloyd & Topor [104] have studied transformation methods for running more general quantifier expressions, although their method will `flounder' for some examples that can be run using our method (Lloyd, personal communication).
Reference: 11. <author> H. Arro, J. Barklund, J. Bevemyr, </author> <title> Parallel Bounded Quantifications | Preliminary Results, </title> <journal> ACM SIGPLAN Notices, </journal> <volume> 28(5) </volume> <pages> 117-124, </pages> <year> 1993. </year> <month> f20g </month>
Reference-contexts: Hermenegildo and Carro [78] discuss how parallel execution of bounded quantifications relates to more traditional AND-parallel execution of logic programs. Arro and Barklund <ref> [11] </ref> have investigated how bounded quantifiers can be executed on the Connection Machine, a SIMD multiprocessor that directly supports data parallel computation. Finally, we should mention that transforming recursive programs to iterative programs is an activity that has been studied extensively in computing science.
Reference: 12. <author> Arvind, K. P. Gostelow, </author> <title> The U-Interpreter, </title> <journal> IEEE Computer, </journal> <volume> 15(2) </volume> <pages> 42-49, </pages> <year> 1982. </year> <month> f18g </month>
Reference-contexts: This in turn was inspired by other data flow models for parallel execution of Prolog proposed by Moto-oka and Fuchi [116] and Umeyama and Tamura [171]. These execution models were motivated from the research on data flow parallelism which emerged in the early 1980's <ref> [12, 53] </ref>. Nilsson and Tanaka [121, 122] designed a scheme for compiling Flat GHC into Fleng. Fleng is a primitive process-oriented language which has been implemented using a data-parallel interpreter.
Reference: 13. <author> L. Augustsson, T. Johnsson, </author> <title> Parallal Graph Reduction with the hv; Gi-Machine, </title> <booktitle> Proc. Workshop on Implementation of Lazy Functional Languages, </booktitle> <year> 1988. </year> <month> f23g </month>
Reference-contexts: Ellis, 1.6. Related Work 23 Li and Appel [58] propose a similar design with several mutators and one collector. Rojemo [131] extended the collector by Ellis et al. for the hv; Gi machine <ref> [13] </ref> (a parallel version of the G-machine [14]).
Reference: 14. <author> L. Augustsson, T. Johnsson, </author> <title> The Chalmers Lazy-ML Compiler, </title> <journal> The Computer Journal, </journal> <volume> 32(2) 127-141,1989. </volume> <month> f23g </month>
Reference-contexts: Ellis, 1.6. Related Work 23 Li and Appel [58] propose a similar design with several mutators and one collector. Rojemo [131] extended the collector by Ellis et al. for the hv; Gi machine [13] (a parallel version of the G-machine <ref> [14] </ref>).
Reference: 15. <author> H. G. Baker, </author> <title> List Processing in Real Time on a Serial Computer, </title> <journal> Communications of the ACM, </journal> <volume> 21(4) </volume> <pages> 280-294, </pages> <year> 1978. </year> <month> f22g </month>
Reference-contexts: Imai and Tick [84] describe a parallel stop-and-copy collector based on Ch-eney's [39] algorithm. It provides dynamic load balancing through a global work stack. Object of equal size are allocated in the same memory block. The later makes it unsuitable for Prolog. Baker <ref> [15] </ref> describes a concurrent collection scheme divided into two processes, a mutator which creates data and a collector which performs garbage collection. Execution of the two processes are interleaved. Halstead [69] describe a parallelisation of Baker's algorithm. The heap is statically divides into separate areas collected by distinct PEs.
Reference: 16. <author> J. Barklund, </author> <title> Parallel Unification, </title> <type> PhD thesis, </type> <institution> Comp. Sci. Dept., Upp-sala Univ., Uppsala, </institution> <year> 1990. </year> <month> f18g </month>
Reference-contexts: Fleng is a primitive process-oriented language which has been implemented using a data-parallel interpreter. Barklund, Hagner and Wafin [19, 20] translated a flat committed choice language into condition graphs, and proposed a data-parallel inference mechanism. Barklund <ref> [16] </ref> also proposed a data-parallel unification algorithm suitable for data-parallel logic programming implementations, e.g., Reform Prolog. Yet another approach to data-parallelism is to add parallel data structures on which certain operations can be performed in parallel.
Reference: 17. <author> J. Barklund, </author> <title> Bounded Quantifications for Iteration and Concurrency in Logic Programming, </title> <journal> New Generation Computing, </journal> <volume> 12(2) </volume> <pages> 161-182, </pages> <publisher> Springer-Verlag, </publisher> <year> 1994. </year> <month> f9g </month>
Reference-contexts: Most imperative languages provide such iteration over finite sets of integers, e.g., for loops in Pascal and DO loops in Fortran, combining the results through serialisation. Previously Barklund & Millroth <ref> [17, 23] </ref> and Voronkov [176, 177] have presented bounded quantifications as a concise way of providing definite iteration in logic programs. In paper VI we propose an extension of Prolog with bounded quantifications and argue that it then naturally follows to introduce arrays.
Reference: 18. <author> J. Barklund, </author> <title> Tabulation of Functions in Definite Clause Programs, </title> <type> UPMAIL Tech. Rep. 82, </type> <institution> Comp. Sci. Dept., Uppsala Univ.,1994. f20g </institution>
Reference-contexts: Finally, we should mention that transforming recursive programs to iterative programs is an activity that has been studied extensively in computing science. This often involves tabulation techniques [26, 28, 62] and has also been applied in logic programming <ref> [18, 183] </ref>. Garbage Collection of Prolog Sequential Collection It has been observed in other languages that most data tend to be short lived [54, 172]. This insight led to the invention of generational garbage collection [7, 98] where young objects are garbage collected more often than old.
Reference: 19. <author> J. Barklund, N. Hagner, M. Wafin, </author> <title> KL1 in Condition Graphs on a Connection Machine, </title> <booktitle> Proc. Intl. Conf. Fifth Generation Computer Systems, </booktitle> <publisher> Ohmsha, </publisher> <year> 1988. </year> <month> f18g </month>
Reference-contexts: Nilsson and Tanaka [121, 122] designed a scheme for compiling Flat GHC into Fleng. Fleng is a primitive process-oriented language which has been implemented using a data-parallel interpreter. Barklund, Hagner and Wafin <ref> [19, 20] </ref> translated a flat committed choice language into condition graphs, and proposed a data-parallel inference mechanism. Barklund [16] also proposed a data-parallel unification algorithm suitable for data-parallel logic programming implementations, e.g., Reform Prolog.
Reference: 20. <author> J. Barklund, N. Hagner, M. Wafin, </author> <title> Connection Graphs, </title> <type> UPMAIL Tech. Rep. 48, </type> <institution> Comp. Sci. Dept., Uppsala Univ., Uppsala, </institution> <year> 1988. </year> <month> f18g </month>
Reference-contexts: Nilsson and Tanaka [121, 122] designed a scheme for compiling Flat GHC into Fleng. Fleng is a primitive process-oriented language which has been implemented using a data-parallel interpreter. Barklund, Hagner and Wafin <ref> [19, 20] </ref> translated a flat committed choice language into condition graphs, and proposed a data-parallel inference mechanism. Barklund [16] also proposed a data-parallel unification algorithm suitable for data-parallel logic programming implementations, e.g., Reform Prolog.
Reference: 21. <author> J. Barklund, P. M. Hill, </author> <title> Extending Godel for Expressing Restricted Quantifications and Arrays, </title> <type> UPMAIL Tech. Rep. 102, </type> <institution> Comp. Sci. Dept., Uppsala Univ., Uppsala, </institution> <year> 1995. </year> <month> f19g </month>
Reference-contexts: However, the quantifications are translated essentially as by Lloyd & Topor (cf. below). Some authors have studied the use of bounded quantifiers with sets, for example in SETL [140] and flogg [55]. Barklund & Hill <ref> [21] </ref> have studied how to incorporate restricted quantifications and arrays in Godel [81], while Apt [10] has studied how bounded quantifications and arrays could be used also in constraint based languages.
Reference: 22. <author> J. Barklund, H. Millroth, </author> <title> Nova Prolog, </title> <type> UPMAIL Tech. Rep. 52, </type> <institution> Comp. Sci. Dept., Uppsala Univ., </institution> <year> 1988. </year> <month> f18g </month>
Reference-contexts: Yet another approach to data-parallelism is to add parallel data structures on which certain operations can be performed in parallel. This is the approach taken by Kacsuk in DAP Prolog [91], Barklund and Millroth in Nova Prolog <ref> [22] </ref>, and by Fagin [59]. The idea of having explicitly parallel data structures have previously been used in other languages such as APL [85], CM Lisp [82, 155], ?Lisp [160], NESL [29] among others. These languages are often designed to exploit the architecture of a specific machine, i.e.
Reference: 23. <author> J. Barklund, H. Millroth, </author> <title> Providing Iteration and Concurrency in Logic Programs Through Bounded Quantifications, </title> <booktitle> Intl. Conf. on Fifth Generation Computer Systems, </booktitle> <year> 1992. </year> <note> f9g 1.6. BIBLIOGRAPHY 27 </note>
Reference-contexts: Most imperative languages provide such iteration over finite sets of integers, e.g., for loops in Pascal and DO loops in Fortran, combining the results through serialisation. Previously Barklund & Millroth <ref> [17, 23] </ref> and Voronkov [176, 177] have presented bounded quantifications as a concise way of providing definite iteration in logic programs. In paper VI we propose an extension of Prolog with bounded quantifications and argue that it then naturally follows to introduce arrays.
Reference: 24. <author> J. Barklund, H. Millroth, </author> <title> Garbage Cut for Garbage Collection of Iterative Prolog Programs, </title> <booktitle> 3rd Symp. on Logic Programming, IEEE, </booktitle> <year> 1986. </year> <month> f20g </month>
Reference-contexts: For the older generation they use a mark-sweep algorithm. The technique is similar to that described by Barklund and Millroth <ref> [24] </ref> and later by Older and Rummell [123]. Bekkers, Ridoux and Ungaro [25] describe an algorithm for copying garbage collection of Prolog. They observe that it is possible to reclaim garbage 1.6. Related Work 21 collected data on backtracking if copying starts at the oldest choice point (bottom-to-top).
Reference: 25. <author> Y. Bekkers, O. Ridoux and L. Ungaro, </author> <title> Dynamic Memory Management for Sequential Logic Programming Languages, </title> <booktitle> Proc. Intl. Workshop on Memory Management 92, </booktitle> <publisher> LNCS 637, Springer-Verlag, </publisher> <year> 1992. </year> <note> f13, 20, 21g </note>
Reference-contexts: Before this only mark-compact collectors and partially copying collectors were used. Our scheme showed how internal pointers were handled and how generational collection could be provided. Demoen, Engels and Tarau [52] have improved the scheme further using ideas from Bekkers, Ridoux and Ungaro <ref> [25] </ref>. This work also showed that instant reclaiming on backtracking can be sacrificed for data which survive garbage collection, without significant efficiency penalties. This is crucial for the parallel collector. <p> For the older generation they use a mark-sweep algorithm. The technique is similar to that described by Barklund and Millroth [24] and later by Older and Rummell [123]. Bekkers, Ridoux and Ungaro <ref> [25] </ref> describe an algorithm for copying garbage collection of Prolog. They observe that it is possible to reclaim garbage 1.6. Related Work 21 collected data on backtracking if copying starts at the oldest choice point (bottom-to-top). <p> It is not clear how their algorithm can be made generational. Cohen [45], Appel [8], Jones and Lins [88], and Wilson [186] have written comprehensive surveys on general-purpose garbage collection algorithms. There is also a survey of collection schemes for sequential logic programming languages by Bekkers, Ridoux and Ungaro <ref> [25] </ref>. 22 Summary Parallel Collection Most parallel Prolog implementations do not include a garbage collector. OR-parallel systems such as Muse [4] and Aurora [34, 105] use more or less sequential mark-sweep collectors [2, 56, 57].
Reference: 26. <author> R. E. Bellman, </author> <title> Dynamic Programming, </title> <publisher> Princeton Univ. Press, </publisher> <address> Prince-ton, N.J., </address> <year> 1957. </year> <month> f20g </month>
Reference-contexts: Finally, we should mention that transforming recursive programs to iterative programs is an activity that has been studied extensively in computing science. This often involves tabulation techniques <ref> [26, 28, 62] </ref> and has also been applied in logic programming [18, 183]. Garbage Collection of Prolog Sequential Collection It has been observed in other languages that most data tend to be short lived [54, 172].
Reference: 27. <author> J. Bevemyr, T. Lindgren, H. Millroth, </author> <title> Reform Prolog: The Language and its Implementation, </title> <booktitle> Logic Programming: Proc. Tenth Intl. Conf., </booktitle> <publisher> MIT Press, </publisher> <year> 1993. </year> <month> f8g </month>
Reference-contexts: Hence, sequential compiler technology should be largely applicable to our system. 8 Summary Nested Execution (paper IV) In paper IV we propose a scheme for executing nested recursion parallelism. The scheme requires only minimal extensions to the flat execution model of Reform Prolog <ref> [27] </ref>. It is possible to transform some nested recursions into a single recursive loop. However, it is not always feasible to flatten a nested recursion in Reform Prolog, e.g., when the size of the nested recursion cannot be statically determined.
Reference: 28. <author> R. S. Bird, </author> <title> Tabulation Techniques for Recursive Programs, </title> <journal> ACM Computing Surveys, </journal> <volume> 12(4) </volume> <pages> 403-417, </pages> <year> 1980. </year> <month> f20g </month>
Reference-contexts: Finally, we should mention that transforming recursive programs to iterative programs is an activity that has been studied extensively in computing science. This often involves tabulation techniques <ref> [26, 28, 62] </ref> and has also been applied in logic programming [18, 183]. Garbage Collection of Prolog Sequential Collection It has been observed in other languages that most data tend to be short lived [54, 172].
Reference: 29. <author> G. Blelloch, </author> <title> Programming Parallel Algorithms, </title> <journal> Communications of the ACM, </journal> <volume> 39(3), </volume> <year> 1996. </year> <month> f18g </month>
Reference-contexts: This is the approach taken by Kacsuk in DAP Prolog [91], Barklund and Millroth in Nova Prolog [22], and by Fagin [59]. The idea of having explicitly parallel data structures have previously been used in other languages such as APL [85], CM Lisp [82, 155], ?Lisp [160], NESL <ref> [29] </ref> among others. These languages are often designed to exploit the architecture of a specific machine, i.e. CM Lisp was designed for the Connection Machine and DAP Prolog for the Distributed Array Processor.
Reference: 30. <author> P. Borgwardt, </author> <title> Parallel Prolog Using Stack Segments on Shared-Memory Multiprocessors, </title> <booktitle> IEEE Symp. Logic Programming, </booktitle> <year> 1984. </year> <month> f14g </month>
Reference-contexts: System such as ACE [65, 66] and DDAS [148, 146, 147] are largely based on &-Prolog. Chang, Despain and DeGroot [37, 38] were first to propose that scheduling of parallel execution could be done through compile time analysis. Borgwardt <ref> [30, 31] </ref> proposed a stack-based implementation of the execution model developed by Chang et al. [38]. 1.6. Related Work 15 Hermenegildo and Rossi [80] classified independent and parallelism as strict independence and non-strict independence.
Reference: 31. <author> P. Borgwardt, D. </author> <title> Rea, Distributed Semi-Intelligent Backtracking for a Stack Based AND-Parallel Prolog, </title> <booktitle> Proc. Symp. Logic Programming, IEEE Computer Society, </booktitle> <year> 1986. </year> <month> f14g </month>
Reference-contexts: System such as ACE [65, 66] and DDAS [148, 146, 147] are largely based on &-Prolog. Chang, Despain and DeGroot [37, 38] were first to propose that scheduling of parallel execution could be done through compile time analysis. Borgwardt <ref> [30, 31] </ref> proposed a stack-based implementation of the execution model developed by Chang et al. [38]. 1.6. Related Work 15 Hermenegildo and Rossi [80] classified independent and parallelism as strict independence and non-strict independence.
Reference: 32. <author> H. Burkhardt, S. Frank, B. Knobe, J. Rothnie, </author> <title> Overview of the KSR1 Computer System, </title> <type> Tech. Rep. </type> <institution> KSR-TR-9202001, Kendall Square Research, </institution> <year> 1992. </year> <month> f7g </month>
Reference-contexts: We find that the ineffectiveness of locking elimination is due to the relative rarity of locking writes, and the execution model of Reform Prolog, which results in few invalidations of shared cache lines when such writes occur. The benchmarks are evaluated on KSR-1 <ref> [32] </ref>, a cache-coherent multiprocessor with physically distributed memory, using up to 48 processors.
Reference: 33. <author> M. Burke, R. Cytron, </author> <title> Interprocedural Dependence Analysis and Par-allelization, </title> <booktitle> Proc. SIGPLAN'86 Symp. Compiler Construction, </booktitle> <year> 1986. </year> <month> f18g </month>
Reference-contexts: Loop Parallelisation in Imperative Languages Parallelising iteration in imperative languages have received much attention since it potentially may increase the speed of large sets of existing applications. Most of the research have focussed on parallelising DO-loops in Fortran. Kuck, Kuhn, Leasure, Wolfe [96], Kennedy [92], Burke and Cytron <ref> [33] </ref> among others have looked into this. The problem has proven to be difficult to solve for general loops. The best results have been obtained for loops performing simple operations on arrays.
Reference: 34. <author> M. Carlsson, </author> <title> Design and Implementation of an Or-Parallel Prolog Engine, </title> <type> PhD thesis, </type> <institution> SICS-RITA/02, </institution> <year> 1990. </year> <month> f22g </month>
Reference-contexts: There is also a survey of collection schemes for sequential logic programming languages by Bekkers, Ridoux and Ungaro [25]. 22 Summary Parallel Collection Most parallel Prolog implementations do not include a garbage collector. OR-parallel systems such as Muse [4] and Aurora <ref> [34, 105] </ref> use more or less sequential mark-sweep collectors [2, 56, 57]. The Aurora collector [185] designed by Weemeeuw and Demoen is slightly more complicated since Aurora uses the SRI-model [180] for OR-parallel execution with its more complicated data structures.
Reference: 35. <author> M. Carlsson, </author> <title> SICStus Prolog Internals Manual, </title> <type> Internal Report, </type> <institution> Swedish Institute of Computer Science, </institution> <year> 1989. </year> <month> f20g </month>
Reference-contexts: This insight led to the invention of generational garbage collection [7, 98] where young objects are garbage collected more often than old. Copying collectors [60, 112] and generational copying collectors have been considered unsuitable for Prolog until recently. Prolog implementations such as SICStus Prolog <ref> [35] </ref> use a mark-sweep algorithm [106] that first marks the live data, then compacts the heap. We take the implementation of Appleby et al. [9] as typical. It is based on the Deutsch-Schorr-Waite [45, 139] algorithm for marking and on Morris' algorithm [45, 117] for compacting.
Reference: 36. <author> M. Carlsson, </author> <title> Variable Shunting for the WAM, </title> <type> Tech. Rep. </type> <institution> R91-07, Swedish Institute of Computer Science, </institution> <year> 1989. </year> <month> f21g </month>
Reference-contexts: We think our approach leads to better locality of reference. However, we have not found any published measurements of the effi ciency of the Bekkers-Ridoux-Ungaro algorithm. * Variable shunting <ref> [36, 97] </ref>, i.e. collapsing variable-variable chains, is used to avoid duplication of variables inside structures. However, this technique may introduce variable chains in new places. We want to avoid this situation. Their algorithm does preserve the segment-structure of the heap (but not the ordering within a segment).
Reference: 37. <author> J.-H. Chang, </author> <title> High Performance Execution of Prolog Programs Based on a Static Dependency Analysis, </title> <type> PhD thesis, </type> <institution> UCB/CSD 86/263, Univ. Calif. Berkeley, </institution> <year> 1986. </year> <note> f14g 28 BIBLIOGRAPHY </note>
Reference-contexts: Hermenegildo [75, 79, 80, 118] defined and implemented the first independent AND-parallel system: &-Prolog. &-Prolog has played an important role for AND-parallel implementations. System such as ACE [65, 66] and DDAS [148, 146, 147] are largely based on &-Prolog. Chang, Despain and DeGroot <ref> [37, 38] </ref> were first to propose that scheduling of parallel execution could be done through compile time analysis. Borgwardt [30, 31] proposed a stack-based implementation of the execution model developed by Chang et al. [38]. 1.6.
Reference: 38. <author> J.-H. Chang, A. M. Despain, D. </author> <title> DeGroot, AND-Parallelism of Logic Programs Based on Static Data Dependency Analysis, </title> <booktitle> Proc. IEEE Symp. Logic Programming, </booktitle> <year> 1985. </year> <month> f14g </month>
Reference-contexts: Hermenegildo [75, 79, 80, 118] defined and implemented the first independent AND-parallel system: &-Prolog. &-Prolog has played an important role for AND-parallel implementations. System such as ACE [65, 66] and DDAS [148, 146, 147] are largely based on &-Prolog. Chang, Despain and DeGroot <ref> [37, 38] </ref> were first to propose that scheduling of parallel execution could be done through compile time analysis. Borgwardt [30, 31] proposed a stack-based implementation of the execution model developed by Chang et al. [38]. 1.6. <p> Chang, Despain and DeGroot [37, 38] were first to propose that scheduling of parallel execution could be done through compile time analysis. Borgwardt [30, 31] proposed a stack-based implementation of the execution model developed by Chang et al. <ref> [38] </ref>. 1.6. Related Work 15 Hermenegildo and Rossi [80] classified independent and parallelism as strict independence and non-strict independence.
Reference: 39. <author> C. J. </author> <title> Cheney, A Nonrecursive List Compacting Algorithm, </title> <journal> Communications of the ACM, </journal> <volume> 13(11) </volume> <pages> 677-678, </pages> <month> November </month> <year> 1970. </year> <note> f21, 22g </note>
Reference-contexts: Hence, they can reclaim all memory by backtracking. In contrast, our algorithm only supports partial reclamation of memory by backtracking. Appel [6, 7] describes a simple generational garbage collector for Standard ML. The collector uses Cheney's <ref> [39] </ref> garbage collection algorithm, which is the basis of our algorithm as well. However, Appel's collector relies on assignments being infrequent. In Prolog, variable binding is assignment in this sense. Our algorithm handles frequent assignments efficiently. <p> Load balancing is provided for the coping phase. Our parallel algorithm use ideas from Ali's collector and adds support for handling internal pointers and improves on the load balancing machinery. Imai and Tick [84] describe a parallel stop-and-copy collector based on Ch-eney's <ref> [39] </ref> algorithm. It provides dynamic load balancing through a global work stack. Object of equal size are allocated in the same memory block. The later makes it unsuitable for Prolog.
Reference: 40. <author> T. Chikayama, Y. Kimura, </author> <title> Multiple Reference Management in Flat GHC, </title> <booktitle> Logic Programming|Proc. Fourth Intl. Conf., </booktitle> <publisher> MIT Press, </publisher> <year> 1987. </year> <month> f16g </month>
Reference-contexts: These ideas were adopted by others and resulted in the concurrent logic languages: Concurrent Prolog [143], FCP [145], Parlog [43, 49], Strand [61], Guarded Horn Clauses (GHC) [166, 167], and Flat GHC <ref> [40, 170] </ref>. These languages are also referred to as de-evolutioned by Tick [162] since they have been significantly restricted to allow efficient implementation. Most of the AND-parallel systems described above are implemented as extensions of WAM.
Reference: 41. <author> K. L. Clark, F. McCabe, </author> <title> The Control Facilities of IC-Prolog, Expert Systems in the Micro-Electronic World (ed. </title> <editor> D. Michie), </editor> <publisher> Edinburgh University Press, </publisher> <year> 1979. </year> <note> f14, 16g </note>
Reference-contexts: In this system only one goal, the producer, is allowed to bind a variable. All other goals are consumers. A consumer is not allowed to bind a variable, it suspends until the variable becomes bound. This form of dependent parallelism has its origin in IC-Prolog <ref> [41] </ref>. DeGroot [51] proposed a scheme for restricted AND-parallelism (RAP), i.e. only independent goals were allowed to execute in parallel. Runtime tests were added for determining variable groundness and independence. The goals were executed in parallel when the tests succeeded. <p> The current implementation of DDAS does not provide automated variable annotation, but Shen [150] mentions work in progress. 16 Summary Another approach to parallelism is to introduce explicit concurrency. This was done in IC-Prolog by Clark and McCabe <ref> [41] </ref> and in Relational Language by Clark and Gregory [42]. These ideas were adopted by others and resulted in the concurrent logic languages: Concurrent Prolog [143], FCP [145], Parlog [43, 49], Strand [61], Guarded Horn Clauses (GHC) [166, 167], and Flat GHC [40, 170].
Reference: 42. <author> K. L. Clark, S. Gregory, </author> <title> A Relational Language for Parallel Programming, </title> <booktitle> Proc. ACM Symp. Functional Programming and Computer Architecture, </booktitle> <year> 1981. </year> <month> f16g </month>
Reference-contexts: The current implementation of DDAS does not provide automated variable annotation, but Shen [150] mentions work in progress. 16 Summary Another approach to parallelism is to introduce explicit concurrency. This was done in IC-Prolog by Clark and McCabe [41] and in Relational Language by Clark and Gregory <ref> [42] </ref>. These ideas were adopted by others and resulted in the concurrent logic languages: Concurrent Prolog [143], FCP [145], Parlog [43, 49], Strand [61], Guarded Horn Clauses (GHC) [166, 167], and Flat GHC [40, 170].
Reference: 43. <author> K. L. Clark, S. Gregory, </author> <title> PARLOG: A Parallel Logic Programming Language, </title> <type> Rep. DOC 83/5, </type> <institution> Dept. Computing, Imperial College, </institution> <address> London, </address> <year> 1983. </year> <note> f5, 16g </note>
Reference-contexts: Instead of output unification, variables were allowed only one producer and, in some languages, only one consumer. 1.3. Reform Prolog 5 Examples of languages which exhibit one or more of these simplifications are: Concurrent Prolog [143, 144], FCP [145, 191], Parlog <ref> [43, 44] </ref>, (F)GHC [168, 169], Strand [61] and Janus [138]. These simplifications made it harder to write programs in these languages, the introduction of explicit concurrency being one of the main reasons. The following quote is from Tick's book on parallel logic programming [161, p. 410]. <p> This was done in IC-Prolog by Clark and McCabe [41] and in Relational Language by Clark and Gregory [42]. These ideas were adopted by others and resulted in the concurrent logic languages: Concurrent Prolog [143], FCP [145], Parlog <ref> [43, 49] </ref>, Strand [61], Guarded Horn Clauses (GHC) [166, 167], and Flat GHC [40, 170]. These languages are also referred to as de-evolutioned by Tick [162] since they have been significantly restricted to allow efficient implementation. Most of the AND-parallel systems described above are implemented as extensions of WAM.
Reference: 44. <author> K. L. Clark, S. Gregory, </author> <title> Notes on the Implementation of PARLOG, </title> <journal> Journal of Logic Programming, </journal> <volume> 2(1) </volume> <pages> 17-42, </pages> <year> 1985. </year> <month> f5g </month>
Reference-contexts: Instead of output unification, variables were allowed only one producer and, in some languages, only one consumer. 1.3. Reform Prolog 5 Examples of languages which exhibit one or more of these simplifications are: Concurrent Prolog [143, 144], FCP [145, 191], Parlog <ref> [43, 44] </ref>, (F)GHC [168, 169], Strand [61] and Janus [138]. These simplifications made it harder to write programs in these languages, the introduction of explicit concurrency being one of the main reasons. The following quote is from Tick's book on parallel logic programming [161, p. 410].
Reference: 45. <author> J. Cohen, </author> <title> Garbage Collection of Linked Data Structures, </title> <journal> Computing Surveys, </journal> <volume> 13(3) </volume> <pages> 341-367, </pages> <month> September </month> <year> 1981. </year> <note> f20, 21g </note>
Reference-contexts: Prolog implementations such as SICStus Prolog [35] use a mark-sweep algorithm [106] that first marks the live data, then compacts the heap. We take the implementation of Appleby et al. [9] as typical. It is based on the Deutsch-Schorr-Waite <ref> [45, 139] </ref> algorithm for marking and on Morris' algorithm [45, 117] for compacting. Touati and Hama [163] developed a generational copying garbage collector for Prolog. The heap is split into an old and a new generation. <p> Prolog implementations such as SICStus Prolog [35] use a mark-sweep algorithm [106] that first marks the live data, then compacts the heap. We take the implementation of Appleby et al. [9] as typical. It is based on the Deutsch-Schorr-Waite [45, 139] algorithm for marking and on Morris' algorithm <ref> [45, 117] </ref> for compacting. Touati and Hama [163] developed a generational copying garbage collector for Prolog. The heap is split into an old and a new generation. <p> It was designed after our collector and use our ideas for handling internal pointers. It is not clear how their algorithm can be made generational. Cohen <ref> [45] </ref>, Appel [8], Jones and Lins [88], and Wilson [186] have written comprehensive surveys on general-purpose garbage collection algorithms.
Reference: 46. <author> A. Colmerauer, H. Kanoui, R. Pasero, P. Roussel, </author> <title> Un Systeme de Communication Homme-Machine en Fran~cais, </title> <institution> Groupe de Recherche en Intelligence Artificielle, Univ. de Aix-Marseille, </institution> <address> Luminy, </address> <year> 1972. </year> <month> f3g </month>
Reference: 47. <author> J. S. Conery, D. F. Kibler, </author> <title> Parallel Interpretation of Logic Programs, </title> <booktitle> Proc. ACM Symp. Functional Programming and Computer Architecture, </booktitle> <year> 1981. </year> <month> f14g </month>
Reference-contexts: The possibility of AND-parallel execution was noted by Kowalski [94] in 1974. Parallel Prolog can be divided into two categories: OR-parallel and AND-parallel. Our research falls into the AND-parallel category and we will not elaborate on the OR-parallel field. Conery and Kibler <ref> [47] </ref> proposed the first scheme for OR- and AND-parallel execution, where dependencies for the AND-parallel execution were determined using an ordering algorithm. The dependencies were calculated when entering a clause and updated when each body goal succeeded. The scheme has not been considered practical due to substantial overheads.
Reference: 48. <author> J. Crammond, </author> <title> A Garbage Collection Algorithm for Shared Memory Parallel Processors, </title> <journal> Intl. Journal of Parallel Programming, </journal> <volume> 17(6) </volume> <pages> 497-522, </pages> <year> 1988. </year> <month> f22g </month>
Reference-contexts: The Aurora collector [185] designed by Weemeeuw and Demoen is slightly more complicated since Aurora uses the SRI-model [180] for OR-parallel execution with its more complicated data structures. The closest we get to a parallel collector for an AND-parallel Prolog is Cram-mond's <ref> [48] </ref> mark-sweep collector for Parlog. It is parallelised by pushing all external references to each heap on a heap specific import stack. Each heap can then be collected using an almost sequential mark-sweep collector.
Reference: 49. <author> J. Crammond, </author> <title> The Abstract Machine and Implementation of Parallel Parlog, </title> <journal> New Generation Computing, </journal> <volume> 10(4) </volume> <pages> 385-422, </pages> <publisher> Springer-Verlag, </publisher> <year> 1992. </year> <month> f16g </month>
Reference-contexts: This was done in IC-Prolog by Clark and McCabe [41] and in Relational Language by Clark and Gregory [42]. These ideas were adopted by others and resulted in the concurrent logic languages: Concurrent Prolog [143], FCP [145], Parlog <ref> [43, 49] </ref>, Strand [61], Guarded Horn Clauses (GHC) [166, 167], and Flat GHC [40, 170]. These languages are also referred to as de-evolutioned by Tick [162] since they have been significantly restricted to allow efficient implementation. Most of the AND-parallel systems described above are implemented as extensions of WAM.
Reference: 50. <author> S. K. Debray, M. Jain, </author> <title> A Simple Program Transformation for Parallelism, </title> <booktitle> Intl. Symp. Logic Programming, </booktitle> <publisher> MIT Press, </publisher> <year> 1994. </year> <month> f17g </month>
Reference-contexts: Only inner loops are parallelised, similarly to Fortran. The consequence is that less work can be parallelised and the exploited parallelism is fine-grained. Also, no compiler nor implementation exists. A compilation technique is outlined but never implemented. Hermenegildo and Carro [78] and Debray and Jain <ref> [50] </ref> discuss how goals can be spawned more efficiently using program transformation techniques. These have some similarities to the transformations described by Tarnlund [164] in his thesis. Hermenegildo and Carro also discuss how the &-Prolog implementation can be extended with low level primitives to support data-parallelism.
Reference: 51. <author> D. </author> <title> DeGroot, Restricted AND-parallelism, </title> <booktitle> Proc. Intl. Conf. Fifth Generation Computer Systems, </booktitle> <publisher> North-Holland, </publisher> <address> Amsterdam, </address> <year> 1984. </year> <note> f14g 1.6. BIBLIOGRAPHY 29 </note>
Reference-contexts: In this system only one goal, the producer, is allowed to bind a variable. All other goals are consumers. A consumer is not allowed to bind a variable, it suspends until the variable becomes bound. This form of dependent parallelism has its origin in IC-Prolog [41]. DeGroot <ref> [51] </ref> proposed a scheme for restricted AND-parallelism (RAP), i.e. only independent goals were allowed to execute in parallel. Runtime tests were added for determining variable groundness and independence. The goals were executed in parallel when the tests succeeded.
Reference: 52. <author> B. Demoen, G. Engels, P. Tarau, </author> <title> Segment Preserving Copying Garbage Collection for WAM based Prolog, </title> <booktitle> Proc. 1996 ACM Symp. on Applied Computing, </booktitle> <publisher> ACM Press, </publisher> <address> 1996 f13, 21g </address>
Reference-contexts: Scientific Contributions The sequential collection scheme showed how a generational copying collector could be applied to Prolog. Before this only mark-compact collectors and partially copying collectors were used. Our scheme showed how internal pointers were handled and how generational collection could be provided. Demoen, Engels and Tarau <ref> [52] </ref> have improved the scheme further using ideas from Bekkers, Ridoux and Ungaro [25]. This work also showed that instant reclaiming on backtracking can be sacrificed for data which survive garbage collection, without significant efficiency penalties. This is crucial for the parallel collector. <p> The main drawback of Sahlin's algorithm is that implementing the mark-sweep algorithm becomes more difficult, not to mention guaranteeing that there are no programming errors in its implementation. To our knowledge it has never been implemented. The collector described by Demoen et al. <ref> [52] </ref> maintains heap segments across garbage collections, and even increases the amount of memory that can be deallocated on backtracking. It was designed after our collector and use our ideas for handling internal pointers. It is not clear how their algorithm can be made generational.
Reference: 53. <author> J. B. Dennis, </author> <title> Data Flow Supercomputers, </title> <journal> IEEE Computer, </journal> <volume> 13(11) </volume> <pages> 48-56, </pages> <year> 1980. </year> <month> f18g </month>
Reference-contexts: This in turn was inspired by other data flow models for parallel execution of Prolog proposed by Moto-oka and Fuchi [116] and Umeyama and Tamura [171]. These execution models were motivated from the research on data flow parallelism which emerged in the early 1980's <ref> [12, 53] </ref>. Nilsson and Tanaka [121, 122] designed a scheme for compiling Flat GHC into Fleng. Fleng is a primitive process-oriented language which has been implemented using a data-parallel interpreter.
Reference: 54. <author> L. P. Deutsch, D. G. Bobrow, </author> <title> An Efficient Incremental Automatic Garbage Collector, </title> <journal> Communications of the ACM, </journal> <volume> 19(9) </volume> <pages> 522-526, </pages> <year> 1976. </year> <note> f11, 20g </note>
Reference-contexts: This procedure must be fast to achieve high system performance since Prolog programs usually create large amounts of data. It has been observed in other languages, with similar allocation patters as Prolog, that most data tend to be short lived <ref> [54, 172] </ref>. This insight has led to the invention of generational garbage collection [98] where young objects are garbage collected more often than old. <p> This often involves tabulation techniques [26, 28, 62] and has also been applied in logic programming [18, 183]. Garbage Collection of Prolog Sequential Collection It has been observed in other languages that most data tend to be short lived <ref> [54, 172] </ref>. This insight led to the invention of generational garbage collection [7, 98] where young objects are garbage collected more often than old. Copying collectors [60, 112] and generational copying collectors have been considered unsuitable for Prolog until recently.
Reference: 55. <author> A. Dovier, E. G. Omodeo, E. Pontelli, G. Rossi, flogg: </author> <title> a Logic Programming Language with Finite Sets, </title> <booktitle> Proc. Intl. Conf. Logic Programming, </booktitle> <publisher> MIT Press, </publisher> <year> 1991. </year> <month> f19g </month>
Reference-contexts: Specifications written in SPILL are executable by translating them to Pro-log, according to a set of translation rules. However, the quantifications are translated essentially as by Lloyd & Topor (cf. below). Some authors have studied the use of bounded quantifiers with sets, for example in SETL [140] and flogg <ref> [55] </ref>. Barklund & Hill [21] have studied how to incorporate restricted quantifications and arrays in Godel [81], while Apt [10] has studied how bounded quantifications and arrays could be used also in constraint based languages.
Reference: 56. <author> M. Doprochevsky, </author> <title> Garbage Collection in the OR-Parallel Logic Programming System ElipSys, </title> <type> ECRC Tech. Rep. </type> <institution> DPS-85, </institution> <year> 1991. </year> <month> f22g </month>
Reference-contexts: OR-parallel systems such as Muse [4] and Aurora [34, 105] use more or less sequential mark-sweep collectors <ref> [2, 56, 57] </ref>. The Aurora collector [185] designed by Weemeeuw and Demoen is slightly more complicated since Aurora uses the SRI-model [180] for OR-parallel execution with its more complicated data structures. The closest we get to a parallel collector for an AND-parallel Prolog is Cram-mond's [48] mark-sweep collector for Parlog.
Reference: 57. <author> M. Dorochevsky, K. Schuerman, A. Veron, and J. Xu, </author> <title> Constraint Handling, Garbage Collection and Execution Model Issues in ElipSys, Parallel Execution of Logic Programs, </title> <booktitle> Proc. ICLP'91 Pre-Conf. Workshop, LNCS 569, </booktitle> <year> 1991. </year> <month> f22g </month>
Reference-contexts: OR-parallel systems such as Muse [4] and Aurora [34, 105] use more or less sequential mark-sweep collectors <ref> [2, 56, 57] </ref>. The Aurora collector [185] designed by Weemeeuw and Demoen is slightly more complicated since Aurora uses the SRI-model [180] for OR-parallel execution with its more complicated data structures. The closest we get to a parallel collector for an AND-parallel Prolog is Cram-mond's [48] mark-sweep collector for Parlog.
Reference: 58. <author> J. R. Ellis, K. Li, A. W. Appel, </author> <title> Real-time Concurrent Collection on Stock Multiprocessors, </title> <type> Tech. Rep. 25, </type> <institution> Digital Systems Research Center, Palo Alto, </institution> <year> 1988. </year> <month> f23g </month>
Reference-contexts: Our implementation instead uses the trail for pointing out references from the old to the new generation. Huelsbergen and Larus' [83] developed a concurrent copying garbage collector for shared memory with two processes; collector and mutator. Ellis, 1.6. Related Work 23 Li and Appel <ref> [58] </ref> propose a similar design with several mutators and one collector. Rojemo [131] extended the collector by Ellis et al. for the hv; Gi machine [13] (a parallel version of the G-machine [14]).
Reference: 59. <author> B. Fagin, </author> <title> Data-Parallel Logic Programming Systems, </title> <type> Tech. Rep., </type> <institution> Thayer School of Engineering, Dartmouth Collage, </institution> <address> New Hampshire, </address> <year> 1990. </year> <month> f18g </month>
Reference-contexts: Yet another approach to data-parallelism is to add parallel data structures on which certain operations can be performed in parallel. This is the approach taken by Kacsuk in DAP Prolog [91], Barklund and Millroth in Nova Prolog [22], and by Fagin <ref> [59] </ref>. The idea of having explicitly parallel data structures have previously been used in other languages such as APL [85], CM Lisp [82, 155], ?Lisp [160], NESL [29] among others. These languages are often designed to exploit the architecture of a specific machine, i.e.
Reference: 60. <author> R. Fenichel, J. Yochelson, </author> <title> A LISP Garbage-collector for Virtual memory Computer Systems, </title> <journal> Communications of the ACM, </journal> <volume> 12(11) </volume> <pages> 611-612, </pages> <year> 1969. </year>
Reference-contexts: Garbage Collection of Prolog Sequential Collection It has been observed in other languages that most data tend to be short lived [54, 172]. This insight led to the invention of generational garbage collection [7, 98] where young objects are garbage collected more often than old. Copying collectors <ref> [60, 112] </ref> and generational copying collectors have been considered unsuitable for Prolog until recently. Prolog implementations such as SICStus Prolog [35] use a mark-sweep algorithm [106] that first marks the live data, then compacts the heap. We take the implementation of Appleby et al. [9] as typical.
Reference: 61. <author> I. Foster, S. Taylor, Strand: </author> <title> A Practical Parallel Programming Language, </title> <booktitle> North American Conf. Logic Programming, </booktitle> <publisher> MIT Press, </publisher> <year> 1989. </year> <note> f5, 16g </note>
Reference-contexts: Instead of output unification, variables were allowed only one producer and, in some languages, only one consumer. 1.3. Reform Prolog 5 Examples of languages which exhibit one or more of these simplifications are: Concurrent Prolog [143, 144], FCP [145, 191], Parlog [43, 44], (F)GHC [168, 169], Strand <ref> [61] </ref> and Janus [138]. These simplifications made it harder to write programs in these languages, the introduction of explicit concurrency being one of the main reasons. The following quote is from Tick's book on parallel logic programming [161, p. 410]. <p> This was done in IC-Prolog by Clark and McCabe [41] and in Relational Language by Clark and Gregory [42]. These ideas were adopted by others and resulted in the concurrent logic languages: Concurrent Prolog [143], FCP [145], Parlog [43, 49], Strand <ref> [61] </ref>, Guarded Horn Clauses (GHC) [166, 167], and Flat GHC [40, 170]. These languages are also referred to as de-evolutioned by Tick [162] since they have been significantly restricted to allow efficient implementation. Most of the AND-parallel systems described above are implemented as extensions of WAM.
Reference: 62. <author> D. P. Friedman, D. S. Wise, M. Wand, </author> <title> Recursive Programming through Table Look-Up, </title> <booktitle> Symp. on Symbolic and Algebraic Computation, ACM, </booktitle> <year> 1976. </year> <month> f20g </month>
Reference-contexts: Finally, we should mention that transforming recursive programs to iterative programs is an activity that has been studied extensively in computing science. This often involves tabulation techniques <ref> [26, 28, 62] </ref> and has also been applied in logic programming [18, 183]. Garbage Collection of Prolog Sequential Collection It has been observed in other languages that most data tend to be short lived [54, 172].
Reference: 63. <editor> D. Gries, </editor> <booktitle> The Science of Programming, </booktitle> <publisher> Springer-Verlag, </publisher> <year> 1981. </year> <month> f20g </month>
Reference-contexts: The idea for using bounded quantification in logic programming was inspired by Tennent's proposed use of them in ALGOL-like languages [158, 159], and also by an exposition by Gries <ref> [63] </ref>. Hermenegildo and Carro [78] discuss how parallel execution of bounded quantifications relates to more traditional AND-parallel execution of logic programs. Arro and Barklund [11] have investigated how bounded quantifiers can be executed on the Connection Machine, a SIMD multiprocessor that directly supports data parallel computation.
Reference: 64. <author> G. Gupta, V. Santos Costa, R. Yang, M. V. Hermenegildo, IDIOM: </author> <title> Intergrating Dependent and-, Independent And- and Or-parallelism, </title> <booktitle> Proc. Intl. Logic Programming Symp., </booktitle> <publisher> MIT Press, </publisher> <year> 1991. </year> <note> f15g 30 BIBLIOGRAPHY </note>
Reference-contexts: In the Extended Andorra Model [71, 133], a copy of the computation is created for each possible binding of a variable when all deterministic goals have been executed. The extended model is implemented in AKL [86, 87, 113]. Gupta, Santos Costa, Yang, and Hermenegildo <ref> [64] </ref> combine independent AND-parallelism, deterministic dependent AND-parallelism and OR-parallelism in IDIOM. This built on ideas from AO-WAM [67], which is an extension of RAP-WAM [76] with OR-parallelism. Another approach to solving the problem with conflicting bindings is to execute different branches separately and join the bindings when the branches terminate.
Reference: 65. <author> G. Gupta, M. V. Hermenegildo, </author> <title> ACE: And/Or-Parallel Copying-Based Execution of Logic Programs, Parallel Execution of Logic Programs, </title> <publisher> LNCS 569, Springer-Verlag, </publisher> <year> 1991. </year> <month> f14g </month>
Reference-contexts: Hermenegildo [76] refined DeGroot's scheme and added a backtracking semantics. Hermenegildo [75, 79, 80, 118] defined and implemented the first independent AND-parallel system: &-Prolog. &-Prolog has played an important role for AND-parallel implementations. System such as ACE <ref> [65, 66] </ref> and DDAS [148, 146, 147] are largely based on &-Prolog. Chang, Despain and DeGroot [37, 38] were first to propose that scheduling of parallel execution could be done through compile time analysis.
Reference: 66. <author> G. Gupta, M. V. Hermenegildo, E. Pontelli, V. Santos Costa, </author> <title> ACE: And/Or-parallel Copying-based Execution of Logic Programs, </title> <booktitle> Proc. Intl. Conf. Logic Programming, </booktitle> <publisher> MIT press, </publisher> <year> 1994. </year> <month> f14g </month>
Reference-contexts: Hermenegildo [76] refined DeGroot's scheme and added a backtracking semantics. Hermenegildo [75, 79, 80, 118] defined and implemented the first independent AND-parallel system: &-Prolog. &-Prolog has played an important role for AND-parallel implementations. System such as ACE <ref> [65, 66] </ref> and DDAS [148, 146, 147] are largely based on &-Prolog. Chang, Despain and DeGroot [37, 38] were first to propose that scheduling of parallel execution could be done through compile time analysis.
Reference: 67. <author> G. Gupta, B. Jayaraman, </author> <title> Combined And-Or Parallelism on Shared Memory Multiprocessors, </title> <booktitle> North American Conf. Logic Programming, </booktitle> <publisher> MIT Press, </publisher> <year> 1989. </year> <month> f15g </month>
Reference-contexts: The extended model is implemented in AKL [86, 87, 113]. Gupta, Santos Costa, Yang, and Hermenegildo [64] combine independent AND-parallelism, deterministic dependent AND-parallelism and OR-parallelism in IDIOM. This built on ideas from AO-WAM <ref> [67] </ref>, which is an extension of RAP-WAM [76] with OR-parallelism. Another approach to solving the problem with conflicting bindings is to execute different branches separately and join the bindings when the branches terminate. This is often combined with OR-parallelism.
Reference: 68. <author> G. Gupta, E. Pontelli, </author> <title> Last Alternative Optimization, </title> <booktitle> 8th IEEE Symp. on Parallel and Distributed Processing, IEEE Computer Society, </booktitle> <year> 1996. </year> <month> f17g </month>
Reference-contexts: These have some similarities to the transformations described by Tarnlund [164] in his thesis. Hermenegildo and Carro also discuss how the &-Prolog implementation can be extended with low level primitives to support data-parallelism. Pontelli and Gupta <ref> [68, 126, 127, 128, 129] </ref> present a number of similar techniques for minimising the overheads for creating processes in &ACE. They then argue that data-parallelism can be efficiently exploited. However, there are still some significant differences compared to Reform Prolog. They can only exploit independent AND-parallelism.
Reference: 69. <author> R. H. Halstead, </author> <title> Implementation of Multilisp: Lisp on a Multiprocessor, </title> <booktitle> ACM Symp. LISP and Functional Programming, </booktitle> <year> 1984. </year> <month> f22g </month>
Reference-contexts: Object of equal size are allocated in the same memory block. The later makes it unsuitable for Prolog. Baker [15] describes a concurrent collection scheme divided into two processes, a mutator which creates data and a collector which performs garbage collection. Execution of the two processes are interleaved. Halstead <ref> [69] </ref> describe a parallelisation of Baker's algorithm. The heap is statically divides into separate areas collected by distinct PEs. No support for load balancing is provided. Herlihy and Moss [74] developed a concurrent lock-free version of Halstead's algorithm.
Reference: 70. <author> S. Haridi, </author> <title> A Logic Programming Language Based on the Andorra Model, New Generation Computing, </title> <address> 7(2-3):109-125, </address> <year> 1990. </year> <month> f15g </month>
Reference-contexts: Non-deterministic goals are allowed as long as no non-determinate bindings are created. Conflicting bindings result in global failure. Naish's ideas on deterministic binding and computation have been one of the major influences on Reform Prolog. Warren used ideas similar to PNU-Prolog when he formulated the Basic Andorra Model <ref> [70, 181] </ref> where deterministic goals are executed in parallel, and non-deterministic goals are suspended. The Basic Andorra Model has been implemented in Andorra-I [134, 135, 136, 190].
Reference: 71. <author> S. Haridi, S. Janson, </author> <title> Kernel Andorra Prolog and its Computation Model, </title> <booktitle> Intl. Conf. Logic Programming, </booktitle> <publisher> MIT Press, </publisher> <year> 1990. </year> <month> f15g </month>
Reference-contexts: Warren used ideas similar to PNU-Prolog when he formulated the Basic Andorra Model [70, 181] where deterministic goals are executed in parallel, and non-deterministic goals are suspended. The Basic Andorra Model has been implemented in Andorra-I [134, 135, 136, 190]. In the Extended Andorra Model <ref> [71, 133] </ref>, a copy of the computation is created for each possible binding of a variable when all deterministic goals have been executed. The extended model is implemented in AKL [86, 87, 113]. Gupta, Santos Costa, Yang, and Hermenegildo [64] combine independent AND-parallelism, deterministic dependent AND-parallelism and OR-parallelism in IDIOM.
Reference: 72. <author> R. Harper, D. MacQueen, R. Milner, </author> <title> Standard ML, </title> <type> Technical Report ECS-LFCS-86-2, </type> <institution> Dept. Computer Science, Edinburgh Univ., </institution> <year> 1986. </year> <month> f20g </month>
Reference-contexts: The Common LISP language [154] (and some earlier LISP dialects), as well as Standard ML <ref> [72] </ref>, contain iteration, mapping and reduction constructs that in some cases resemble ours. The idea for using bounded quantification in logic programming was inspired by Tennent's proposed use of them in ALGOL-like languages [158, 159], and also by an exposition by Gries [63].
Reference: 73. <author> W. L. Harrison III, </author> <title> The Interprocedural Analysis and Parallelization of Scheme Programs, Lisp and Symbolic Computation, </title> <address> 2(3-4):176-396, </address> <year> 1989. </year> <month> f17g </month>
Reference-contexts: Warren and Hermenegildo [184] performed some early experiments with what they called MAP-parallelism. This was a limited form of independent data AND-parallelism where a procedure was applied over a set of data, similarly to mapcar in Lisp and DOALL in Fortran. Harrison <ref> [73] </ref> has developed a scheme for exploiting recursion parallelism in Scheme which is similar to Reform Prolog. The main difference is that Harrison only handles DOALL loops and recurrences which do not require synchronisation between different recursion levels. Reform Prolog handles general DO-ACROSS loops.
Reference: 74. <author> M. P. Herlihy, J. E. B. Moss, </author> <title> Lock-Free Garbage Collection for Multiprocessors, </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 3(3) </volume> <pages> 304-311, </pages> <year> 1992. </year> <month> f22g </month>
Reference-contexts: Execution of the two processes are interleaved. Halstead [69] describe a parallelisation of Baker's algorithm. The heap is statically divides into separate areas collected by distinct PEs. No support for load balancing is provided. Herlihy and Moss <ref> [74] </ref> developed a concurrent lock-free version of Halstead's algorithm. Lieberman and Hewitt [98] describe a real-time generational collector in which all pointers from older to newer generations pass through an indirection table. Our implementation instead uses the trail for pointing out references from the old to the new generation.
Reference: 75. <author> M. V. Hermenegildo, </author> <title> An Abstract Machine for Restricted AND-Parallel Execution of Logic Programs, </title> <booktitle> Third Intl. Conf. Logic Programming, </booktitle> <publisher> LNCS 225, Springer-Verlag, </publisher> <year> 1986. </year> <month> f14g </month>
Reference-contexts: Runtime tests were added for determining variable groundness and independence. The goals were executed in parallel when the tests succeeded. This scheme exploited less parallelism than Conery and Kibler's but was expected to have substantially smaller runtime overheads. Hermenegildo [76] refined DeGroot's scheme and added a backtracking semantics. Hermenegildo <ref> [75, 79, 80, 118] </ref> defined and implemented the first independent AND-parallel system: &-Prolog. &-Prolog has played an important role for AND-parallel implementations. System such as ACE [65, 66] and DDAS [148, 146, 147] are largely based on &-Prolog.
Reference: 76. <author> M. V. Hermenegildo, </author> <title> An Abstract Machine Based Execution Model for Computer Architecture Design and Efficient Implementation of Logic Programs in Parallel, </title> <type> PhD thesis, </type> <institution> University of Texas At Austin, </institution> <year> 1986. </year> <note> f14, 15g </note>
Reference-contexts: Runtime tests were added for determining variable groundness and independence. The goals were executed in parallel when the tests succeeded. This scheme exploited less parallelism than Conery and Kibler's but was expected to have substantially smaller runtime overheads. Hermenegildo <ref> [76] </ref> refined DeGroot's scheme and added a backtracking semantics. Hermenegildo [75, 79, 80, 118] defined and implemented the first independent AND-parallel system: &-Prolog. &-Prolog has played an important role for AND-parallel implementations. System such as ACE [65, 66] and DDAS [148, 146, 147] are largely based on &-Prolog. <p> The extended model is implemented in AKL [86, 87, 113]. Gupta, Santos Costa, Yang, and Hermenegildo [64] combine independent AND-parallelism, deterministic dependent AND-parallelism and OR-parallelism in IDIOM. This built on ideas from AO-WAM [67], which is an extension of RAP-WAM <ref> [76] </ref> with OR-parallelism. Another approach to solving the problem with conflicting bindings is to execute different branches separately and join the bindings when the branches terminate. This is often combined with OR-parallelism. Examples of such systems are Wise's EPILOG [187] and Wrench's APPNet [188].
Reference: 77. <author> M. V. Hermenegildo, D. Cabeza, M. Carro, </author> <title> Using Attributed Variables in the Implementation of Concurrent and Parallel Logic Programming Systems, </title> <booktitle> Proc. Intl. Conf. Logic Programming, </booktitle> <publisher> MIT Press, </publisher> <year> 1995. </year> <note> f16g 1.6. BIBLIOGRAPHY 31 </note>
Reference-contexts: These languages are also referred to as de-evolutioned by Tick [162] since they have been significantly restricted to allow efficient implementation. Most of the AND-parallel systems described above are implemented as extensions of WAM. Hermenegildo, Cabeza and Carro <ref> [77] </ref> recently proposed an elegant scheme where attributed variables [120, 97] are used for handling most of the support for parallelism. This technique makes it possible to model different forms of parallelism using the same runtime machinery, with some decrease in efficiency.
Reference: 78. <author> M. V. Hermenegildo, M. Carro, </author> <title> Relating Data-Parallelism and (And-) Parallelism in Logic Programs, </title> <journal> Journal of Computer Languages, </journal> <pages> 22(2-3), </pages> <year> 1996. </year> <note> f17, 20g </note>
Reference-contexts: However, their ideas are much less developed. Only inner loops are parallelised, similarly to Fortran. The consequence is that less work can be parallelised and the exploited parallelism is fine-grained. Also, no compiler nor implementation exists. A compilation technique is outlined but never implemented. Hermenegildo and Carro <ref> [78] </ref> and Debray and Jain [50] discuss how goals can be spawned more efficiently using program transformation techniques. These have some similarities to the transformations described by Tarnlund [164] in his thesis. <p> The idea for using bounded quantification in logic programming was inspired by Tennent's proposed use of them in ALGOL-like languages [158, 159], and also by an exposition by Gries [63]. Hermenegildo and Carro <ref> [78] </ref> discuss how parallel execution of bounded quantifications relates to more traditional AND-parallel execution of logic programs. Arro and Barklund [11] have investigated how bounded quantifiers can be executed on the Connection Machine, a SIMD multiprocessor that directly supports data parallel computation.
Reference: 79. <author> M. V. Hermenegildo, K. J. Greene, </author> <title> &-Prolog and its Performance: Exploiting Independent And-Parallelism, </title> <booktitle> Proc. Intl. Conf. Logic Programming, </booktitle> <publisher> MIT Press, </publisher> <year> 1990. </year> <month> f14g </month>
Reference-contexts: Runtime tests were added for determining variable groundness and independence. The goals were executed in parallel when the tests succeeded. This scheme exploited less parallelism than Conery and Kibler's but was expected to have substantially smaller runtime overheads. Hermenegildo [76] refined DeGroot's scheme and added a backtracking semantics. Hermenegildo <ref> [75, 79, 80, 118] </ref> defined and implemented the first independent AND-parallel system: &-Prolog. &-Prolog has played an important role for AND-parallel implementations. System such as ACE [65, 66] and DDAS [148, 146, 147] are largely based on &-Prolog.
Reference: 80. <author> M. V. Hermenegildo, F. Rossi, </author> <title> Non-Strict Independent And-Parallelism, </title> <booktitle> Proc. Intl. Conf. Logic Programming, </booktitle> <publisher> MIT Press, </publisher> <year> 1990. </year> <note> f14, 15g </note>
Reference-contexts: Runtime tests were added for determining variable groundness and independence. The goals were executed in parallel when the tests succeeded. This scheme exploited less parallelism than Conery and Kibler's but was expected to have substantially smaller runtime overheads. Hermenegildo [76] refined DeGroot's scheme and added a backtracking semantics. Hermenegildo <ref> [75, 79, 80, 118] </ref> defined and implemented the first independent AND-parallel system: &-Prolog. &-Prolog has played an important role for AND-parallel implementations. System such as ACE [65, 66] and DDAS [148, 146, 147] are largely based on &-Prolog. <p> Chang, Despain and DeGroot [37, 38] were first to propose that scheduling of parallel execution could be done through compile time analysis. Borgwardt [30, 31] proposed a stack-based implementation of the execution model developed by Chang et al. [38]. 1.6. Related Work 15 Hermenegildo and Rossi <ref> [80] </ref> classified independent and parallelism as strict independence and non-strict independence. No free variables were allowed to be shared in strict independence, while non-strict independence allowed unbound variables to be passed around as long as they were only used, i.e., bound to a non-variable value, by at most one goal.
Reference: 81. <author> P. M. Hill, J. W. Lloyd, </author> <title> The Godel Programming Language, </title> <publisher> MIT Press, </publisher> <year> 1993. </year> <month> f19g </month>
Reference-contexts: However, the quantifications are translated essentially as by Lloyd & Topor (cf. below). Some authors have studied the use of bounded quantifiers with sets, for example in SETL [140] and flogg [55]. Barklund & Hill [21] have studied how to incorporate restricted quantifications and arrays in Godel <ref> [81] </ref>, while Apt [10] has studied how bounded quantifications and arrays could be used also in constraint based languages.
Reference: 82. <author> W. D. Hillis, G. L. Steele Jr., </author> <title> Data Parallel Algorithms, </title> <journal> Communications of the ACM, </journal> <volume> 29(12) </volume> <pages> 1170-1183, </pages> <year> 1986. </year> <month> f18g </month>
Reference-contexts: This is the approach taken by Kacsuk in DAP Prolog [91], Barklund and Millroth in Nova Prolog [22], and by Fagin [59]. The idea of having explicitly parallel data structures have previously been used in other languages such as APL [85], CM Lisp <ref> [82, 155] </ref>, ?Lisp [160], NESL [29] among others. These languages are often designed to exploit the architecture of a specific machine, i.e. CM Lisp was designed for the Connection Machine and DAP Prolog for the Distributed Array Processor.
Reference: 83. <author> L. F. Huelsbergen, J. R. Larus, </author> <title> A Concurrent Copying Garbage Collector for Languages that Distinguish (Im)mutable Data, </title> <booktitle> Principles and Practice of Parallel Programming, ACM, </booktitle> <year> 1993. </year> <month> f22g </month>
Reference-contexts: Lieberman and Hewitt [98] describe a real-time generational collector in which all pointers from older to newer generations pass through an indirection table. Our implementation instead uses the trail for pointing out references from the old to the new generation. Huelsbergen and Larus' <ref> [83] </ref> developed a concurrent copying garbage collector for shared memory with two processes; collector and mutator. Ellis, 1.6. Related Work 23 Li and Appel [58] propose a similar design with several mutators and one collector.
Reference: 84. <author> A. Imai, E. Tick, </author> <title> Evaluation of Parallel Copying Garbage Collection on Shared-Memory Multiprocessors, </title> <journal> IEEE Transactions on Parallel and Distributed Computing, </journal> <volume> 4(9) </volume> <pages> 1030-1040, </pages> <year> 1993. </year> <month> f22g </month>
Reference-contexts: During collection each PE copies its reachable data into to-space. Load balancing is provided for the coping phase. Our parallel algorithm use ideas from Ali's collector and adds support for handling internal pointers and improves on the load balancing machinery. Imai and Tick <ref> [84] </ref> describe a parallel stop-and-copy collector based on Ch-eney's [39] algorithm. It provides dynamic load balancing through a global work stack. Object of equal size are allocated in the same memory block. The later makes it unsuitable for Prolog.
Reference: 85. <author> K. E. Iverson, </author> <title> A Programming Language, </title> <publisher> Wiley, </publisher> <year> 1962. </year> <month> f18g </month>
Reference-contexts: This is the approach taken by Kacsuk in DAP Prolog [91], Barklund and Millroth in Nova Prolog [22], and by Fagin [59]. The idea of having explicitly parallel data structures have previously been used in other languages such as APL <ref> [85] </ref>, CM Lisp [82, 155], ?Lisp [160], NESL [29] among others. These languages are often designed to exploit the architecture of a specific machine, i.e. CM Lisp was designed for the Connection Machine and DAP Prolog for the Distributed Array Processor.
Reference: 86. <author> S. Janson, </author> <title> AKL A Multiparadigm Programming Language, </title> <type> PhD thesis, </type> <institution> Comp. Sci. Dept., Uppsala Univ., Uppsala, </institution> <year> 1994. </year> <month> f15g </month>
Reference-contexts: The Basic Andorra Model has been implemented in Andorra-I [134, 135, 136, 190]. In the Extended Andorra Model [71, 133], a copy of the computation is created for each possible binding of a variable when all deterministic goals have been executed. The extended model is implemented in AKL <ref> [86, 87, 113] </ref>. Gupta, Santos Costa, Yang, and Hermenegildo [64] combine independent AND-parallelism, deterministic dependent AND-parallelism and OR-parallelism in IDIOM. This built on ideas from AO-WAM [67], which is an extension of RAP-WAM [76] with OR-parallelism.
Reference: 87. <author> S. Janson, J. Montelius, </author> <title> The Design of the AKL/PS 0.0 Prototype Implementation of the Andorra Kernel Language, ESPRIT Deliverable, </title> <type> EP 2471 (PEPMA), </type> <institution> Swedish Institute of Computer Science, </institution> <year> 1992. </year> <month> f15g </month>
Reference-contexts: The Basic Andorra Model has been implemented in Andorra-I [134, 135, 136, 190]. In the Extended Andorra Model [71, 133], a copy of the computation is created for each possible binding of a variable when all deterministic goals have been executed. The extended model is implemented in AKL <ref> [86, 87, 113] </ref>. Gupta, Santos Costa, Yang, and Hermenegildo [64] combine independent AND-parallelism, deterministic dependent AND-parallelism and OR-parallelism in IDIOM. This built on ideas from AO-WAM [67], which is an extension of RAP-WAM [76] with OR-parallelism.
Reference: 88. <author> R. Jones, R. Lins, </author> <title> Garbage Collection: Algorithms for Automatic Dynamic Memory Management, </title> <publisher> John Wiley & Sons, </publisher> <year> 1996. </year> <month> f21g </month>
Reference-contexts: It was designed after our collector and use our ideas for handling internal pointers. It is not clear how their algorithm can be made generational. Cohen [45], Appel [8], Jones and Lins <ref> [88] </ref>, and Wilson [186] have written comprehensive surveys on general-purpose garbage collection algorithms. There is also a survey of collection schemes for sequential logic programming languages by Bekkers, Ridoux and Ungaro [25]. 22 Summary Parallel Collection Most parallel Prolog implementations do not include a garbage collector.
Reference: 89. <author> P. Kacsuk, </author> <title> A Highly Parallel Prolog Interpreter Based on the Generalized Data Flow Model, </title> <booktitle> Proc. Intl. Logic Programming Conf., </booktitle> <institution> Uppsala University, Uppsala, </institution> <year> 1984. </year> <month> f18g </month>
Reference-contexts: Reductions and inferences may be performed in parallel. This technique results in significant overheads compared with compiled implementations. It has therefore mostly been aimed at massively parallel machines. 18 Summary Kacsuk designed a parallel interpreter for Prolog based on his generalised data flow model <ref> [89, 90, 91] </ref>. This in turn was inspired by other data flow models for parallel execution of Prolog proposed by Moto-oka and Fuchi [116] and Umeyama and Tamura [171]. These execution models were motivated from the research on data flow parallelism which emerged in the early 1980's [12, 53].
Reference: 90. <author> P. Kacsuk, </author> <title> Generalized Data Flow Model for Programming Multiple Microprocessor Systems, </title> <booktitle> Proc. of the 3rd Symp. on Microcomp. and Microproc. Appl., </booktitle> <year> 1983. </year> <month> f18g </month>
Reference-contexts: Reductions and inferences may be performed in parallel. This technique results in significant overheads compared with compiled implementations. It has therefore mostly been aimed at massively parallel machines. 18 Summary Kacsuk designed a parallel interpreter for Prolog based on his generalised data flow model <ref> [89, 90, 91] </ref>. This in turn was inspired by other data flow models for parallel execution of Prolog proposed by Moto-oka and Fuchi [116] and Umeyama and Tamura [171]. These execution models were motivated from the research on data flow parallelism which emerged in the early 1980's [12, 53].
Reference: 91. <author> P. Kacsuk, </author> <title> Execution Models of Prolog for Parallel Computers, </title> <publisher> Pitman, </publisher> <address> London, </address> <year> 1990. </year> <note> f18g 32 BIBLIOGRAPHY </note>
Reference-contexts: Reductions and inferences may be performed in parallel. This technique results in significant overheads compared with compiled implementations. It has therefore mostly been aimed at massively parallel machines. 18 Summary Kacsuk designed a parallel interpreter for Prolog based on his generalised data flow model <ref> [89, 90, 91] </ref>. This in turn was inspired by other data flow models for parallel execution of Prolog proposed by Moto-oka and Fuchi [116] and Umeyama and Tamura [171]. These execution models were motivated from the research on data flow parallelism which emerged in the early 1980's [12, 53]. <p> Barklund [16] also proposed a data-parallel unification algorithm suitable for data-parallel logic programming implementations, e.g., Reform Prolog. Yet another approach to data-parallelism is to add parallel data structures on which certain operations can be performed in parallel. This is the approach taken by Kacsuk in DAP Prolog <ref> [91] </ref>, Barklund and Millroth in Nova Prolog [22], and by Fagin [59]. The idea of having explicitly parallel data structures have previously been used in other languages such as APL [85], CM Lisp [82, 155], ?Lisp [160], NESL [29] among others.
Reference: 92. <author> K. Kennedy, </author> <title> Automatic Translation of Fortran Programs to Vector Form, </title> <type> Tech. Rep. </type> <institution> 467-029-4, Rice Univ., </institution> <year> 1980. </year> <month> f18g </month>
Reference-contexts: Loop Parallelisation in Imperative Languages Parallelising iteration in imperative languages have received much attention since it potentially may increase the speed of large sets of existing applications. Most of the research have focussed on parallelising DO-loops in Fortran. Kuck, Kuhn, Leasure, Wolfe [96], Kennedy <ref> [92] </ref>, Burke and Cytron [33] among others have looked into this. The problem has proven to be difficult to solve for general loops. The best results have been obtained for loops performing simple operations on arrays.
Reference: 93. <author> F. Kluzniak, </author> <title> SPILL: A Specification Language Base on Logic Programming, </title> <institution> LOGPRO Research Rep. LITH-IDA-R-91-28, Dept. of Comp. and Inf. Sci., Linkoping Univ., Linkoping, </institution> <year> 1991. </year> <month> f19g </month>
Reference-contexts: We view bounded quantifications as an elegant way of expressing iteration and data-(AND-)parallelism. We present Prolog extensions for certain bounded quantifications, and describe WAM based compilation schemes for quantifications ranging over integers and lists. We evaluate their implementations for sequential and parallel execution. Kluzniak has (also independently) proposed SPILL <ref> [93] </ref>, a specification language which includes certain bounded quantifications with integer ranges. Specifications written in SPILL are executable by translating them to Pro-log, according to a set of translation rules. However, the quantifications are translated essentially as by Lloyd & Topor (cf. below).
Reference: 94. <author> R. A. Kowalski, </author> <title> Predicate Logic as a Computer Language, </title> <booktitle> Information Processing 74, </booktitle> <pages> pp. 569-574, </pages> <publisher> North-Holland, </publisher> <address> Amsterdam, 1974. f3, 14g </address>
Reference-contexts: I implemented and designed the parallel collector. 14 Summary 1.6 RELATED WORK Parallel Prolog The earliest published work on parallel Prolog is Pollards PhD thesis [125] where execution of pure OR-parallelism is discussed. The possibility of AND-parallel execution was noted by Kowalski <ref> [94] </ref> in 1974. Parallel Prolog can be divided into two categories: OR-parallel and AND-parallel. Our research falls into the AND-parallel category and we will not elaborate on the OR-parallel field.
Reference: 95. <author> R. A. Kowalski, </author> <title> Logic for Problem Solving, </title> <publisher> North-Holland, </publisher> <address> Amster-dam, </address> <year> 1979. </year> <month> f3g </month>
Reference: 96. <author> D. Kuck, R. Kuhn, B. Leasure, M. Wolfe, </author> <title> The Structure of an Advanced Retargetable Vectorizer, Tutorial on Supercomputers: Designs and Applications, </title> <publisher> IEEE Computer Society Press, </publisher> <year> 1984. </year> <month> f18g </month>
Reference-contexts: Loop Parallelisation in Imperative Languages Parallelising iteration in imperative languages have received much attention since it potentially may increase the speed of large sets of existing applications. Most of the research have focussed on parallelising DO-loops in Fortran. Kuck, Kuhn, Leasure, Wolfe <ref> [96] </ref>, Kennedy [92], Burke and Cytron [33] among others have looked into this. The problem has proven to be difficult to solve for general loops. The best results have been obtained for loops performing simple operations on arrays.
Reference: 97. <author> S. Le Houitouze, </author> <title> A New Data Structure for Implementing Extensions to Prolog, </title> <booktitle> Proc. Programming Language Implementation and Logic Programming, </booktitle> <publisher> LNCS 456, Springer-Verlag, </publisher> <year> 1990. </year> <note> f16, 21g </note>
Reference-contexts: These languages are also referred to as de-evolutioned by Tick [162] since they have been significantly restricted to allow efficient implementation. Most of the AND-parallel systems described above are implemented as extensions of WAM. Hermenegildo, Cabeza and Carro [77] recently proposed an elegant scheme where attributed variables <ref> [120, 97] </ref> are used for handling most of the support for parallelism. This technique makes it possible to model different forms of parallelism using the same runtime machinery, with some decrease in efficiency. Data OR-parallelism Smith [152] described Multilog, a system utilising data OR-parallelism. <p> We think our approach leads to better locality of reference. However, we have not found any published measurements of the effi ciency of the Bekkers-Ridoux-Ungaro algorithm. * Variable shunting <ref> [36, 97] </ref>, i.e. collapsing variable-variable chains, is used to avoid duplication of variables inside structures. However, this technique may introduce variable chains in new places. We want to avoid this situation. Their algorithm does preserve the segment-structure of the heap (but not the ordering within a segment).
Reference: 98. <author> H. Lieberman, C. Hewitt, </author> <title> A Real-Time Garbage Collector Based on the Lifetimes of Objects, </title> <journal> Communications of the ACM, </journal> <volume> 26(6) </volume> <pages> 419-429, </pages> <month> June </month> <year> 1983. </year> <note> f11, 20, 22g </note>
Reference-contexts: It has been observed in other languages, with similar allocation patters as Prolog, that most data tend to be short lived [54, 172]. This insight has led to the invention of generational garbage collection <ref> [98] </ref> where young objects are garbage collected more often than old. Sequential Copying Collection (paper VII) In paper VII we describe a technique for adapting conventional copying garbage collection to Prolog and how to extend the new scheme for generational collection. <p> Garbage Collection of Prolog Sequential Collection It has been observed in other languages that most data tend to be short lived [54, 172]. This insight led to the invention of generational garbage collection <ref> [7, 98] </ref> where young objects are garbage collected more often than old. Copying collectors [60, 112] and generational copying collectors have been considered unsuitable for Prolog until recently. Prolog implementations such as SICStus Prolog [35] use a mark-sweep algorithm [106] that first marks the live data, then compacts the heap. <p> Execution of the two processes are interleaved. Halstead [69] describe a parallelisation of Baker's algorithm. The heap is statically divides into separate areas collected by distinct PEs. No support for load balancing is provided. Herlihy and Moss [74] developed a concurrent lock-free version of Halstead's algorithm. Lieberman and Hewitt <ref> [98] </ref> describe a real-time generational collector in which all pointers from older to newer generations pass through an indirection table. Our implementation instead uses the trail for pointing out references from the old to the new generation.

References-found: 99

