URL: http://www.cs.berkeley.edu/~dandre/papers/Parallel-TR-SU-1542.ps
Refering-URL: http://www.cs.berkeley.edu/~dandre/cs267/assignment1.html
Root-URL: http://www.cs.berkeley.edu
Abstract-found: 0
Intro-found: 1
Reference: <author> Abramson, David, Mills, Graham, and Perkins, Sonya. </author> <year> 1994. </year> <title> Parallelisation of a genetic algorithm for the computation of efficient train schedules. </title> <editor> In Arnold, David, Christie, Ruth, Day, John, and Roe, Paul (editors). </editor> <booktitle> Parallel Computing and Transputers. </booktitle> <address> Amsterdam: </address> <publisher> IOS Press. </publisher> <pages> Pages 139149. </pages>
Reference: <author> Bianchini, Ricardo and Brown, Christopher. </author> <year> 1993. </year> <title> Parallel genetic algorithms on distributed-memory architectures. </title> <editor> In Atkins, S. and Wagner, A. S. (editors). </editor> <booktitle> Transputer Research and Applications 6. </booktitle> <address> Amsterdam: </address> <publisher> IOS Press. </publisher> <pages> Pages 6782. </pages>
Reference: <author> Cui, J. and Fogarty, T. C. </author> <year> 1992. </year> <title> Optimization by using a parallel genetic algorithm on a transputer computing surface. </title> <editor> In Valero, M, Onate, E., Jane, M., Larriba, J. L., Suarez, B. (editors). </editor> <booktitle> Parallel Computing and Transputer Applications. </booktitle> <address> Amsterdam: </address> <publisher> IOS Press. </publisher> <pages> Pages 246 254. </pages>
Reference: <author> Dietz, H. G. </author> <year> 1992. </year> <title> Common subexpression induction. </title> <note> Parallel Processing Laboratory Technical Report TREE-92-5. </note> <institution> School of Electrical Engineering. Purdue University. </institution>
Reference-contexts: It should also be noted that it may, in fact, be possible to efficiently use a fine-grained SIMD machine for an application (such as genetic programming) that seemingly requires a MIMD machine <ref> (Dietz and Cohen 1992, Dietz 1992) </ref>. However, successful practical implementation of this approach appeared to require substantial expertise in areas outside our existing capabilities. It is not clear whether this concept would deliver any net benefit on any existing machine for programs containing a large number of primitive functions.
Reference: <author> Dietz, H. G. and Cohen, W. E. </author> <year> 1992. </year> <title> A Massively parallel MIND implemented by SIMD hardware. </title> <note> Parallel Processing Laboratory Technical Report TREE-92-4. </note> <institution> School of Electrical Engineering. Purdue University. </institution>
Reference-contexts: It should also be noted that it may, in fact, be possible to efficiently use a fine-grained SIMD machine for an application (such as genetic programming) that seemingly requires a MIMD machine <ref> (Dietz and Cohen 1992, Dietz 1992) </ref>. However, successful practical implementation of this approach appeared to require substantial expertise in areas outside our existing capabilities. It is not clear whether this concept would deliver any net benefit on any existing machine for programs containing a large number of primitive functions.
Reference: <author> Goldberg, David E. l989a. </author> <title> Genetic Algorithms in Search, Optimization, </title> <booktitle> and Machine Learning. </booktitle> <address> Reading, MA: </address> <publisher> Addison-Wesley. </publisher>
Reference: <author> Goldberg, David E. </author> <year> 1989b. </year> <title> Sizing populations for serial and parallel genetic algorithms. </title> <editor> In Schaffer, J. D. (editor). </editor> <booktitle> Proceedings of the Third International Conference on Genetic Algorithms. </booktitle> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann Publishers Inc. </publisher> <pages> Pages 70-79. </pages> <publisher> Holland, John. H. </publisher> <year> 1975. </year> <title> Adaptation in Natural and Artificial Systems: An Introductory Analysis with Applications to Biology, Control, </title> <booktitle> and Artificial Intelligence. </booktitle> <address> Ann Arbor, MI: </address> <publisher> University of Michigan Press. </publisher> <address> Also Cambridge, MA: </address> <publisher> The MIT Press 1992. </publisher>
Reference: <author> Juric, M., Potter, W. D., and Plaksin, M. </author> <year> 1995. </year> <title> Using the parallel virtual machine for hunting snake-in-the-box codes. </title> <editor> In Arabnia, Hamid R. </editor> <booktitle> (editor) Transputer Research and Applications 7. </booktitle> <address> Amsterdam: </address> <publisher> IOS Press. </publisher>
Reference: <editor> Kinnear, Kenneth. E. Jr. (editor). </editor> <booktitle> 1994. Advances in Genetic Programming. </booktitle> <address> Cambridge, MA: </address> <publisher> The MIT Press. </publisher>
Reference: <author> Koza, John. R. </author> <year> 1992. </year> <title> Genetic Programming: On the Programming of Computers by Means of Natural Selection. </title> <address> Cambridge, MA: </address> <publisher> The MIT Press. </publisher>
Reference-contexts: Runs may also stop when a specified targeted maximum number of generations have been run. The distributed genetic algorithm is well suited to loosely coupled, low bandwidth, parallel computation. 2.3 Design Considerations for Parallel Genetic Programming The largest of the programs evolved in Genetic Programming <ref> (Koza 1992) </ref> and Genetic Programming II (Koza 1994a) contained only a few hundred points (i.e., the number of functions and terminals actually appearing in the work-performing bodies of the various branches of the overall program).
Reference: <author> Koza, John. R. </author> <year> 1994a. </year> <title> Genetic Programming II: Automatic Discovery of Reusable Programs. </title> <address> Cambridge, MA: </address> <publisher> The MIT Press. </publisher>
Reference-contexts: The distributed genetic algorithm is well suited to loosely coupled, low bandwidth, parallel computation. 2.3 Design Considerations for Parallel Genetic Programming The largest of the programs evolved in Genetic Programming (Koza 1992) and Genetic Programming II <ref> (Koza 1994a) </ref> contained only a few hundred points (i.e., the number of functions and terminals actually appearing in the work-performing bodies of the various branches of the overall program). <p> Thus, a population of 1,000 2,500-point programs can be accommodated in 2.5 megabytes of storage with this one-byte representation. In the event that the population is architecturally diverse <ref> (as described in Koza 1994a, 1994b) </ref>, the actual memory requirements approach 3 megabytes. We further hypothesized, for purposes of design, a measurement of fitness requiring one second of computer time for each 2,500-point individual in the population. <p> when one automatically defined function can hierarchically refer to another, the effective size (and the execution time) of the program may be an exponential function of the visible number of points actually appearing in the overall program. 1 3 In addition, if the programs in the population are architecturally diverse <ref> (Koza 1994a) </ref>, additional variation is introduced. This variation is further magnified if the architecture of a program changes during the run as a result of architecture-altering operations (Koza 1994b). <p> Since some runs may have infinite (or unmeasurably large) duration, it is not possible to use a simple arithmetic average to compute the expected number of fitness evaluations required to solve a given problem. One way to measure the performance of a probabilistic algorithm is to use performance curves <ref> (as described in Koza 1994a) </ref>. Performance curves provide a measure of the computational effort required to solve the problem with a certain specified probability (say, 99%).
Reference: <author> Koza, John R. </author> <year> 1994b. </year> <title> Architecture-altering operations for evolving the architecture of a multipart program in genetic programming. </title> <institution> Stanford University Computer Science Department technical report STAN-CS-TR-94-1528. </institution> <month> October 21, </month> <year> 1994. </year>
Reference-contexts: This variation is further magnified if the architecture of a program changes during the run as a result of architecture-altering operations <ref> (Koza 1994b) </ref>. For problems involving a simulation of behavior over many time steps, many separate experiments, or many probabilistic scenarios, some programs may finish the simulation considerably earlier or later than others.
Reference: <author> Kroger, Berthold, Schwenderling, Peter, and Vornberger, Oliver. </author> <year> 1992. </year> <title> Massive parallel genetic packing. </title> <editor> In Reijns, G. L. and Luo, J. (editors). </editor> <booktitle> Transputing in Numerical and Neural Network Applications. </booktitle> <address> Amsterdam: </address> <publisher> IOS Press. </publisher> <pages> Pages 214-230. </pages>
Reference: <author> Min, Shermann L. </author> <year> 1994. </year> <title> Feasibility of evolving self-learned pattern recognition applied toward the solution of a constrained system using genetic programming. </title> <editor> In Koza, John R. (editor). </editor> <booktitle> Genetic Algorithms at Stanford 1994. </booktitle> <address> Stanford, CA: </address> <publisher> Stanford University Bookstore. </publisher> <address> ISBN 0 18-187263-3. </address>
Reference-contexts: Current extremely fast serial supercomputers (e.g., Cray machines) attain their very high speeds by vectorization or pipelining. We believed <ref> (and Min 1994 verified) </ref> that these machines would not prove to deliver particularly good performance on genetic programming applications because of the disorderly sequence of conditional and other operations in the program trees of the individuals in the population.
Reference: <author> Robertson, George. l987. </author> <title> Parallel implementation of genetic algorithms in a classifier system. </title> <editor> In Davis, L. (editor). </editor> <title> Genetic Algorithms and Simulated Annealing. </title> <publisher> London: Pittman. </publisher>
Reference: <author> Schwehm, M. </author> <title> Implementation of genetic algorithms on various interconneciton networks. </title> <editor> In Valero, M, Onate, E., Jane, M., Larriba, J. L., Suarez, B. (editors). </editor> <booktitle> 1992. Parallel Computing and Transputer Applications. </booktitle> <address> Amsterdam: </address> <publisher> IOS Press. </publisher> <pages> Pages 195-203. </pages> <note> 1 8 Singleton, </note> <author> Andrew. </author> <year> 1994. </year> <title> Personal communication. </title> <editor> Stender, Joachim (editor). </editor> <booktitle> 1993. Parallel Genetic Algorithms. </booktitle> <address> Amsterdam: </address> <publisher> IOS Publishing. Tanese, Reiko. </publisher> <year> 1989. </year> <title> Distributed Genetic Algorithm for Function Optimization. </title> <type> PhD. dissertation. </type> <institution> Department of Electrical Engineering and Computer Science. University of Michigan. </institution>
Reference: <author> Tout, K. Ribeiro-Filho, B, Mignot, B, and Idlebi, N. A. </author> <year> 1994. </year> <title> cross-platform parallel genetic algorithms programming environment. Transputer Applications and Systems '94. </title> <publisher> Amsterdam: IOS Press. </publisher> <year> 1994. </year> <pages> Pages 7990. </pages>

References-found: 17

