URL: ftp://ftp.cs.swarthmore.edu/pub/meeden/meeden.smc.ps.Z
Refering-URL: http://www.cs.swarthmore.edu/~meeden/
Root-URL: 
Email: meeden@cs.swarthmore.edu  
Title: An incremental approach to developing intelligent neural network controllers for robots  
Author: Lisa A. Meeden 
Address: Swarthmore College 500 College Ave Swarthmore, PA 19081 USA  
Affiliation: Computer Science Program  
Abstract: By beginning with simple reactive behaviors and gradually building up to more memory-dependent behaviors, it may be possible for connectionist systems to eventually achieve the level of planning. This paper focuses on an intermediate step in this incremental process, where the appropriate means of providing guidance to adapting controllers is explored. A local and a global method of reinforcement learning are contrasted|a special form of back-propagation and an evolutionary algorithm. These methods are applied to a neural network controller for a simple robot. A number of experiments are described where the presence of explicit goals and the immediacy of reinforcement are varied. These experiments reveal how various types of guidance can affect the final control behavior. The results show that the respective advantages and disadvantages of these two adaptation methods are complementary, suggesting that some hybrid of the two may be the most effective method. Concluding remarks discuss the next incremental steps towards more complex control behaviors. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> D. H. Ackley and M. L. Littman, </author> <title> "Generalization and scaling in reinforcement learning," </title> <booktitle> in 27 Advances in Neural Information Processing Systems 2 (D. </booktitle> <editor> S. Touretsky, </editor> <publisher> ed.), </publisher> <pages> pp. 550-557, </pages> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, CA, </address> <year> 1990. </year>
Reference-contexts: From this test it obtains a global fitness measure that is then used to bias the subsequent adaptation. A. Local Method: Back-Propagation In half of the experiments, the control networks were trained with a modified version of the complementary reinforcement back-propagation (CRBP) learning algorithm <ref> [1] </ref>. Back-propagation learning requires precise error measures for each output produced by a network so that gradient descent on the error can be performed. CRBP provides these exact error measures from the abstract reward and punishment signals as follows. The output is determined by a two-step process.
Reference: [2] <author> S. Baluja, </author> <title> "Evolution of an artificial neural network based autonomous land vehicle controller," </title> <note> this issue. </note>
Reference-contexts: Baluja found evidence to support Fitpatrick's and 3 Other fitness options were also explored, such as averaging the results of the random starts or using the minimum of the random starts. Neither of these options improved performance. 13 Grefenstette's claim <ref> [2] </ref>.
Reference: [3] <author> R. D. Beer and J. C. Gallagher, </author> <title> "Evolving dynamical neural networks for adaptive behavior," </title> <booktitle> Adaptive Behavior, </booktitle> <volume> vol. 1, no. 1, </volume> <pages> pp. 91-122, </pages> <year> 1992. </year>
Reference: [4] <author> R. K. Belew, J. McInerney, and N. N. Schraudolph, </author> <title> "Evolving networks: Using genetic algorithms with connectionist learning," in Artificial Life II (C. </title> <editor> G. Langton, C. Taylor, J. D. Farmer, and S. Rasmussen, </editor> <booktitle> eds.), </booktitle> <pages> pp. 511-547, </pages> <address> Redwood City, CA, </address> <year> 1992, </year> <month> Addison-Welsley. </month>
Reference-contexts: Combining Genetic Algorithms and Neural Networks There are many interesting possibilities for applying genetic algorithms to neural networks. GAs have been used to find good initial network weights, to tune network learning parameters, to determine network structure, to evolve network learning algorithms, and to learn network weights <ref> [4, 18, 20, 8, 32] </ref>. It is the last option|learning weights|that will be used here: for the carbot problem, the network architecture is fixed (as shown in Figure 2) and the GA works to adapt an appropriate set of weights. <p> Therefore due to the problematic nature of crossover for network representations, this form of recombination was not used in the experiments described here (although crossover has been used in other GA applications to networks|for a good discussion of this topic see <ref> [4] </ref>). Instead, new individuals were created solely by mutation. Harvey notes that there has been "surprising success (in some circumstances) of what has come to be called naive evolution; i.e. mutation only, contrary to normal GA folklore which emphasizes the significance of crossover" [19]. <p> Belew, McInerney, and Schraudolph did a number of experiments to test the feasibility of using a GA as a source of initial weights for gradient descent learning and found that this 25 technique is effective <ref> [4] </ref>. To return to the incremental program described in the introduction, combining global and local adaptation methods such as the GA and CRBP is a promising answer to the questions raised about how to properly guide the adaptation process of a network controller.
Reference: [5] <author> L. Booker, D. Goldberg, and J. Holland, </author> <title> "Classifier systems and genetic algorithms," </title> <journal> Artificial Intelligence, </journal> <volume> vol. 40, no. </volume> <pages> 1-3, pp. 235-282, </pages> <year> 1989. </year>
Reference-contexts: B. Global Method: Genetic Algorithm Genetic algorithms (GAs) were originally used for function optimization. However with the arrival of classifier systems, which use GAs for rule discovery, they became linked with a reinforcement procedure <ref> [21, 5, 17, 11, 10] </ref>. But as will be seen below, a GA was, in a sense, already a reinforcement procedure prior to the advent of classifier systems because it operates on information about the relative performance of potential solutions [32].
Reference: [6] <author> V. </author> <title> Braitenberg, Vehicles: Experiments in Synthetic Psychology. </title> <publisher> MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1984. </year>
Reference: [7] <author> R. A. Brooks, </author> <title> "A robust layered control system for a mobile robot," </title> <journal> IEEE Journal of Robotics and Automation, </journal> <volume> vol. 2, no. 1, </volume> <pages> pp. 14-23, </pages> <month> March </month> <year> 1986. </year>
Reference: [8] <author> D. J. Chalmers, </author> <title> "The evolution of learning: An experiment in genetic connectionism," </title> <booktitle> in Proceedings of the 1990 Connectionist Summer School, </booktitle> <pages> pp. 81-90, </pages> <address> Palo Alto, CA, 1990, </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Combining Genetic Algorithms and Neural Networks There are many interesting possibilities for applying genetic algorithms to neural networks. GAs have been used to find good initial network weights, to tune network learning parameters, to determine network structure, to evolve network learning algorithms, and to learn network weights <ref> [4, 18, 20, 8, 32] </ref>. It is the last option|learning weights|that will be used here: for the carbot problem, the network architecture is fixed (as shown in Figure 2) and the GA works to adapt an appropriate set of weights.
Reference: [9] <author> A. Clark, </author> <title> Associative Engines: Conectionism, Concepts, and Representational Change. </title> <publisher> MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1993. </year>
Reference-contexts: In terms of the first three principles, it has already been well demonstrated that connectionist networks are fundamentally associative engines that naturally perform generalizations <ref> [24, 9] </ref>. However it is not yet clear, in terms of the fourth principle, what are the best ways to incorporate goals or to provide adaptation evaluation for robotics domains.
Reference: [10] <author> M. Dorigo and H. Bersini, </author> <title> "A comparison of Q-learning and classifier systems," in From animals to animats 3 (D. </title> <editor> Cliff, P. Husbands, J. Meyer, and S. Wilson, </editor> <booktitle> eds.), </booktitle> <pages> pp. 248-255, </pages> <address> Cambridge, MA, 1994, </address> <publisher> MIT Press. </publisher>
Reference-contexts: B. Global Method: Genetic Algorithm Genetic algorithms (GAs) were originally used for function optimization. However with the arrival of classifier systems, which use GAs for rule discovery, they became linked with a reinforcement procedure <ref> [21, 5, 17, 11, 10] </ref>. But as will be seen below, a GA was, in a sense, already a reinforcement procedure prior to the advent of classifier systems because it operates on information about the relative performance of potential solutions [32].
Reference: [11] <author> M. Dorigo and U. Schnepf, </author> <title> "Genetics-based machine learning and behavior-based robotics: A new synthesis," </title> <journal> IEEE Transactions on Systems, Man, and Cybernetics, </journal> <volume> vol. 23, no. 1, </volume> <pages> pp. 141-154, </pages> <year> 1993. </year>
Reference-contexts: B. Global Method: Genetic Algorithm Genetic algorithms (GAs) were originally used for function optimization. However with the arrival of classifier systems, which use GAs for rule discovery, they became linked with a reinforcement procedure <ref> [21, 5, 17, 11, 10] </ref>. But as will be seen below, a GA was, in a sense, already a reinforcement procedure prior to the advent of classifier systems because it operates on information about the relative performance of potential solutions [32].
Reference: [12] <author> J. L. Elman, </author> <title> "Finding structure in time," </title> <journal> Cognitive Science, </journal> <volume> vol. 14, </volume> <pages> pp. 179-212, </pages> <year> 1990. </year>
Reference-contexts: One form of simple recurrent network (these are termed simple because the location of recurrent connections is restricted to a single layer) was developed by Elman <ref> [12] </ref>. In this architecture (shown in Figure 2), each unit in the hidden layer is connected to all the other hidden units including itself.
Reference: [13] <author> M. J. Fitzpatrick and J. J. Grefenstette, </author> <title> "Genetic algorithms in noisy environments," </title> <journal> Machine Learning, </journal> <volume> vol. 3, no. 2/3, </volume> <pages> pp. 101-120, </pages> <year> 1988. </year> <month> 28 </month>
Reference-contexts: Yet, according to Fitzpatrick and Grefenstette, it is better to obtain quick, rough estimates and to allow the GA to consider many candidate solutions than it is to attempt to obtain highly accurate evaluations with a smaller population size <ref> [13] </ref>. Baluja found evidence to support Fitpatrick's and 3 Other fitness options were also explored, such as averaging the results of the random starts or using the minimum of the random starts. Neither of these options improved performance. 13 Grefenstette's claim [2].
Reference: [14] <author> D. Floreano and F. Mondada, </author> <title> "Automatic creation of an autonomous agent: Genetic evolution of a neural-network driven robot," in From animals to animats 3 (D. </title> <editor> Cliff, P. Husbands, J. Meyer, and S. Wilson, </editor> <booktitle> eds.), </booktitle> <pages> pp. 421-430, </pages> <address> Cambridge, MA, 1994, </address> <publisher> MIT Press. </publisher>
Reference: [15] <author> D. B. Fogel, L. J. Fogel, and P. V. W., </author> <title> "Evolving neural networks," </title> <journal> Biological Cybernetics, </journal> <volume> vol. 63, </volume> <pages> pp. 487-493, </pages> <year> 1990. </year>
Reference-contexts: Weights in connectionist networks are real-valued, and converting them into a binary encoding would entail arbitrarily discretizing them to a particular precision. So for many GA applications to networks, individuals are represented as real-coded vectors of weights (for example see <ref> [15, 32] </ref>), and this was done for the carbot domain as well. Another tradition in GAs is that the crossover operator is used much more frequently than the mutation operator. <p> In some instances, even a much more brute-force form of mutation has been successful for GA applications to networks. Rather than just altering a small number of the weights, every weight on the entire string is modified by a random amount <ref> [15] </ref> from a range as large as -10 to +10 [32]. This form of mutation essentially creates a new random point located within a specific radius from the parent.
Reference: [16] <author> D. E. Goldberg, </author> <title> Genetic Algorithms in Search, Optimization, and Machine Learning. </title> <publisher> Addison-Wesley, </publisher> <address> Reading, MA, </address> <year> 1989. </year>
Reference-contexts: Goldberg provides a thorough introduction to both the optimization and reinforcement applications of GAs <ref> [16] </ref>. Genetic algorithms are based on the theory of natural selection in evolution. GAs work on a population of individuals, where each individual represents a possible solution to the given problem.
Reference: [17] <author> J. J. Grefenstette, </author> <title> "The evolution of strategies for multiagent environments," </title> <booktitle> Adaptive Behavior, </booktitle> <volume> vol. 1, no. 1, </volume> <pages> pp. 65-90, </pages> <year> 1992. </year>
Reference-contexts: B. Global Method: Genetic Algorithm Genetic algorithms (GAs) were originally used for function optimization. However with the arrival of classifier systems, which use GAs for rule discovery, they became linked with a reinforcement procedure <ref> [21, 5, 17, 11, 10] </ref>. But as will be seen below, a GA was, in a sense, already a reinforcement procedure prior to the advent of classifier systems because it operates on information about the relative performance of potential solutions [32].
Reference: [18] <author> S. A. Harp, T. Samad, and A. Guha, </author> <title> "Towards the genetic synthesis of neural networks," </title> <booktitle> in Proceedings of the Third International Conference on Genetic Algorithms (J. </booktitle> <editor> D. Schaffer, </editor> <publisher> ed.), </publisher> <pages> pp. 360-369, </pages> <address> Palo Alto, CA, 1989, </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Combining Genetic Algorithms and Neural Networks There are many interesting possibilities for applying genetic algorithms to neural networks. GAs have been used to find good initial network weights, to tune network learning parameters, to determine network structure, to evolve network learning algorithms, and to learn network weights <ref> [4, 18, 20, 8, 32] </ref>. It is the last option|learning weights|that will be used here: for the carbot problem, the network architecture is fixed (as shown in Figure 2) and the GA works to adapt an appropriate set of weights.
Reference: [19] <author> I. Harvey, </author> <title> "Evolutionary robotics and SAGA: The case for hill crawling and tournament selection," </title> <booktitle> in Artificial Life III (C. </booktitle> <editor> G. Langton, </editor> <publisher> ed.), </publisher> <pages> pp. 299-326, </pages> <address> Redwood City, CA, </address> <year> 1993, </year> <month> Addison-Welsley. </month>
Reference-contexts: Instead, new individuals were created solely by mutation. Harvey notes that there has been "surprising success (in some circumstances) of what has come to be called naive evolution; i.e. mutation only, contrary to normal GA folklore which emphasizes the significance of crossover" <ref> [19] </ref>. Further, he found that the optimal mutation rates were between one and two mutations per individual and this was nearly invariant over the length of the individual's representation. In some instances, even a much more brute-force form of mutation has been successful for GA applications to networks. <p> The one with the higher fitness was declared the winner. By forcing both robots to attempt the same tasks, the evaluation of the robots is reduced to the simple question: which performed the best across these tasks <ref> [19] </ref>? Each GA run consisted of at most 2500 generations. After each generation, the current best individual was determined.
Reference: [20] <author> I. Harvey, P. Husbands, and D. Cliff, </author> <title> "Issues in evolutionary robotics," in From animals to animats 2 (J.-A. </title> <editor> Meyer, H. Roitblat, and S. Wilson, </editor> <booktitle> eds.), </booktitle> <pages> pp. 364-373, </pages> <address> Cambridge, MA, 1993, </address> <publisher> MIT Press. </publisher>
Reference-contexts: To alleviate this time limitation, a software simulation was implemented. Although simulators are usually not very much like reality, when the simulator is based on an actual robot, the simulator's behavior can be programmed to closely correspond with the real-world behavior. Harvey, Husbands, and Cliff <ref> [20] </ref> suggest some ways to ensure that a simulator stays in close step with reality: 1. Simulations of the inputs to sensors and the outputs to actuators should be based on carefully collected empirical data. 7 2. Noise must be taken into account at all levels. 3. <p> Combining Genetic Algorithms and Neural Networks There are many interesting possibilities for applying genetic algorithms to neural networks. GAs have been used to find good initial network weights, to tune network learning parameters, to determine network structure, to evolve network learning algorithms, and to learn network weights <ref> [4, 18, 20, 8, 32] </ref>. It is the last option|learning weights|that will be used here: for the carbot problem, the network architecture is fixed (as shown in Figure 2) and the GA works to adapt an appropriate set of weights.
Reference: [21] <author> J. Holland, </author> <title> "Escaping brittleness: The possibilities of general purpose learning algorithms applied to parallel rule-based systems," in Machine Learning: An Artificial Intelligence Approach (R. </title> <editor> Michalski, J. Carbonell, and M. T., eds.), </editor> <volume> vol. 2, </volume> <pages> pp. 593-623, </pages> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, CA, </address> <year> 1986. </year>
Reference-contexts: B. Global Method: Genetic Algorithm Genetic algorithms (GAs) were originally used for function optimization. However with the arrival of classifier systems, which use GAs for rule discovery, they became linked with a reinforcement procedure <ref> [21, 5, 17, 11, 10] </ref>. But as will be seen below, a GA was, in a sense, already a reinforcement procedure prior to the advent of classifier systems because it operates on information about the relative performance of potential solutions [32].
Reference: [22] <author> P. Husbands, I. Harvey, and D. Cliff, </author> <title> "Analysing recurrent dynamical networks evolved for robot control," </title> <booktitle> in Proceedings of the Third IEE International Conference on Artificial Neural Networks, </booktitle> <pages> pp. 158-162. </pages> <publisher> IEE Press, </publisher> <year> 1993. </year>
Reference: [23] <author> F. Martin, </author> <title> "Mini board 2.0 technical reference," </title> <publisher> MIT Media Lab, </publisher> <address> Cambridge MA, </address> <month> July </month> <year> 1992. </year>
Reference-contexts: II. THE ROBOT Carbot is a modified toy car (approximately 15 cm wide, 23 cm long, and 10 cm high) controlled by a programmable mini-board (designed at MIT <ref> [23] </ref>). During operation, carbot was tethered to a PC and was controlled by a remote connectionist network that communicated with the mini-board through the serial port. Carbot was inexpensive to build, primarily because it makes use of primitive sensors|no lasers or video.
Reference: [24] <editor> J. McClelland and D. Rumelhart, eds., </editor> <booktitle> Parallel Distributed Processing, </booktitle> <volume> vol. 1. </volume> <publisher> MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1986. </year>
Reference-contexts: In terms of the first three principles, it has already been well demonstrated that connectionist networks are fundamentally associative engines that naturally perform generalizations <ref> [24, 9] </ref>. However it is not yet clear, in terms of the fourth principle, what are the best ways to incorporate goals or to provide adaptation evaluation for robotics domains.
Reference: [25] <author> L. A. Meeden, </author> <title> Towards planning: Incremental investigations into adaptive robot control. </title> <type> PhD dissertation, </type> <institution> Indiana University, </institution> <year> 1994. </year> <month> 29 </month>
Reference-contexts: This hypothesis has begun to be tested in subsequent work <ref> [25] </ref>. The hidden layer representations at the time of goal achievement were termed protoplans. To investigate whether protoplans could actually help to guide behavior, a transfer of learning experiment was done.
Reference: [26] <author> L. A. Meeden, G. McGraw, and D. Blank, </author> <title> "Emergence of control and planning in an au-tonomous vehicle," </title> <booktitle> in Proceedings of the Fifteenth Annual Meeting of the Cognitive Science Society, </booktitle> <pages> pp. 735-740, </pages> <address> Hillsdale, NJ, 1993, </address> <publisher> Lawrence Erlbaum Associates. </publisher>
Reference-contexts: At the reactive level, carbot receives negative 4 reinforcement any time it contacts one of the walls or is not moving. Conceptually this could be considered rudimentary navigation|carbot must learn to continually keep moving while avoiding the boundaries of its environment. See <ref> [26] </ref> for a detailed description of factors that affect a network's performance when learning this type of simple reactive task. At the goal level, carbot must either seek or avoid the light depending on the current goal.
Reference: [27] <author> D. Montana and L. Davis, </author> <title> "Training feedforward neural networks using genetic algorithms," </title> <booktitle> in Proceedings of the 1989 International Joint Conference on Artificial Intelligence, </booktitle> <pages> pp. 762-767, </pages> <year> 1989. </year>
Reference-contexts: Although using real-encoded strings and depending on mutation for the creation of new genetic material are not standard GA choices, some research has shown that they are crucial to obtaining positive results for evolving neural networks <ref> [27, 32] </ref>. Implementation Details Each individual in the GA's population is a possible control network for carbot. Therefore the fitness function should measure the effectiveness of a network for controlling carbot in its task of doing rudimentary navigation and periodically seeking and avoiding the light.
Reference: [28] <author> R. Pfeifer and P. F. Verschure, </author> <title> "Designing efficiently navigating non-goal-directed robots," </title> <note> in From Animals to Animats 2 (J. </note> <editor> Meyer, H. Roitblat, and W. S., </editor> <booktitle> eds.), </booktitle> <pages> pp. 31-39, </pages> <address> Cambridge, MA, 1993, </address> <publisher> MIT Press. </publisher>
Reference: [29] <author> D. A. Pomerlearu, </author> <title> Neural Network Perception for Mobile Robot Guidance. </title> <publisher> Kluwer Academic Publishing, </publisher> <year> 1993. </year>
Reference-contexts: phenomenon, Pomerleau found that in training a neural network to drive a large vehicle, it was beneficial to train the network not only on good data provided by a human driver, but also on shifted versions of this data which forced the controller to learn how to recover from errors <ref> [29] </ref>. 8 is not usually available. Reinforcement procedures are well suited for this type of learning because they only require scalar values to encode the relative desirability of particular states. The magnitude of this value can be used to reflect the degree of the state's goodness.
Reference: [30] <author> U. Schnepf, </author> <title> "Robot ethology: A proposal for the research into intelligent autonomous systems," in From Animals to Animats (J.-A. </title> <editor> Meyer and S. W. Wilson, </editor> <booktitle> eds.), </booktitle> <pages> pp. 465-474, </pages> <address> Cambridge, MA, 1991, </address> <publisher> MIT Press. </publisher>
Reference: [31] <author> D. L. Waltz, </author> <title> "Eight principles for building an intelligent robot," in From Animals to Animats (J.-A. </title> <editor> Meyer and S. W. Wilson, </editor> <booktitle> eds.), </booktitle> <pages> pp. 462-464, </pages> <address> Cambridge, MA, 1991, </address> <publisher> MIT Press. </publisher>
Reference: [32] <author> C. Whitley, S. Dominic, R. Das, and C. W. Anderson, </author> <title> "Genetic reinforcement learning for neurocontrol problems," </title> <journal> Machine Learning, </journal> <volume> vol. 13, no. 2/3, </volume> <pages> pp. 259-284, </pages> <year> 1993. </year>
Reference-contexts: But as will be seen below, a GA was, in a sense, already a reinforcement procedure prior to the advent of classifier systems because it operates on information about the relative performance of potential solutions <ref> [32] </ref>. Goldberg provides a thorough introduction to both the optimization and reinforcement applications of GAs [16]. Genetic algorithms are based on the theory of natural selection in evolution. GAs work on a population of individuals, where each individual represents a possible solution to the given problem. <p> Therefore "genetic algorithms are capable of performing a global search of a space because they can rely on hyperplane sampling to guide the search instead of searching along the gradient of a function" as back-propagation does <ref> [32] </ref>. Combining Genetic Algorithms and Neural Networks There are many interesting possibilities for applying genetic algorithms to neural networks. <p> Combining Genetic Algorithms and Neural Networks There are many interesting possibilities for applying genetic algorithms to neural networks. GAs have been used to find good initial network weights, to tune network learning parameters, to determine network structure, to evolve network learning algorithms, and to learn network weights <ref> [4, 18, 20, 8, 32] </ref>. It is the last option|learning weights|that will be used here: for the carbot problem, the network architecture is fixed (as shown in Figure 2) and the GA works to adapt an appropriate set of weights. <p> Weights in connectionist networks are real-valued, and converting them into a binary encoding would entail arbitrarily discretizing them to a particular precision. So for many GA applications to networks, individuals are represented as real-coded vectors of weights (for example see <ref> [15, 32] </ref>), and this was done for the carbot domain as well. Another tradition in GAs is that the crossover operator is used much more frequently than the mutation operator. <p> Rather than just altering a small number of the weights, every weight on the entire string is modified by a random amount [15] from a range as large as -10 to +10 <ref> [32] </ref>. This form of mutation essentially creates a new random point located within a specific radius from the parent. <p> Although using real-encoded strings and depending on mutation for the creation of new genetic material are not standard GA choices, some research has shown that they are crucial to obtaining positive results for evolving neural networks <ref> [27, 32] </ref>. Implementation Details Each individual in the GA's population is a possible control network for carbot. Therefore the fitness function should measure the effectiveness of a network for controlling carbot in its task of doing rudimentary navigation and periodically seeking and avoiding the light.
Reference: [33] <author> S. W. Wilson, </author> <title> "The animat path to AI," in From Animals to Animats (J.-A. </title> <editor> Meyer and S. W. Wilson, </editor> <booktitle> eds.), </booktitle> <pages> pp. 15-21, </pages> <address> Cambridge, MA, 1991, </address> <publisher> MIT Press. </publisher> <pages> 30 </pages>
References-found: 33

