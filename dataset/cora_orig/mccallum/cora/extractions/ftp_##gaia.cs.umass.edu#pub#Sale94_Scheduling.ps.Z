URL: ftp://gaia.cs.umass.edu/pub/Sale94:Scheduling.ps.Z
Refering-URL: http://www-net.cs.umass.edu/papers/papers.html
Root-URL: 
Title: Scheduling for cache affinity in parallelized communication protocols  
Author: James D. Salehi James F. Kurose Don Towsley 
Note: This work was supported by NSF under grant NCR-9206908 and by ARPA under ESD/AVS contract F-19628-92-C-0089. The authors can be contacted at [salehi,kurose,towsley]@cs.umass.edu.  
Date: Oct. 1994  
Address: Amherst, MA 01003  
Affiliation: Department of Computer Science University of Massachusetts  University of Massachusetts  
Pubnum: Technical Report UM-CS-1994-075,  
Abstract: In this paper, we explore the benefits of processor-cache affinity scheduling of parallel network protocol processing, in a setting in which protocol processing executes on the multiprocessor host concurrently with a general workload of non-protocol activity. We implement two parallelization approaches, Locking and Independent Protocol Stacks (IPS), in the UDP/IP/FDDI protocol stack in the x-kernel on an SGI Challenge XL multiprocessor. We then conduct a set of multiprocessor experiments designed to measure packet processing times under specific conditions of cache state. These measurements are then used to parameterize the analytic component of a simulation model of multiprocessor protocol processing, designed to reflect the impact on protocol execution time of the non-protocol workload. In this manner, we are able to explore the benefits of affinity scheduling of parallel network processing (while accounting for the impact of general non-protocol activity) in a carefully-controlled experimental setting. Our results show that affinity scheduling can significantly reduce the communication delay associated with protocol processing, providingsignificant reduction in end-to-end delay even when the fixed-overhead components of communication are large with respect to message processing time. Affinity scheduling thus enables the host to support a greater number of concurrent streams, and to provide a higher maximum throughput to individual streams. We compare the performance of Locking with IPS, and find that IPS (which maximizes cache affinity) delivers much lower message latency and significantly higher message throughput capacity. Yet IPS exhibits limited intra-stream scalability and less robust response to intra-stream burstiness. We thus propose a hybrid approach for a specific class of streams, and show it offers the best overall performanceyielding high message throughput, high intra-stream scalability, and robustness in the presence of bursty arrivals. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> A. Agarwal, M. Horowitz, and J. Hennessy. </author> <title> An analytical cache model. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 7(2) </volume> <pages> 184-215, </pages> <month> May </month> <year> 1989. </year>
Reference-contexts: (R (x i ); L) is a power function of R (x i ) for fixed L was observed independently by Thiebaut [31, 32], and Kobayashi and MacDougall [16].) In [28], the authors show equation (1) to be consistent with data given by Smith [29], and Agarwal, Horowitz and Hennessy <ref> [1] </ref>. They also demonstrate its accuracy through detailed validation on segments of a 200-million-reference trace of a multiprogrammed IBM/370 MVS workload, consisting of a representative workload of user applications and operating system activity.
Reference: [2] <author> Thomas E. Anderson, Edward D. Lazowska, and Henry M. Levy. </author> <title> The performance implications of thread management alternatives for shared-memory multiprocessors. </title> <journal> IEEE Transactions on Computers, </journal> <volume> 38(12) </volume> <pages> 1631-1644, </pages> <month> December </month> <year> 1989. </year>
Reference-contexts: Under IPS, independent stacks should be wired to processorsexcept under low arrival rate, when MRU processor scheduling performs better. 10 Note that while the load-balancing and synchronization benefits of per-processor thread pools has been explored <ref> [2] </ref>, the cache affinity benefits of such organization have not been previously evaluated. 24 There are several possible extensions to the work presented in this paper. First, we plan to examine the performance of affinity scheduling for send-side protocol processing.
Reference: [3] <author> Mats Bjorkman and Per Gunningberg. </author> <title> Locking effects in multiprocessor implementations of protocols. </title> <booktitle> In Proceedings of the ACM SIGCOMM Conference on Communications, Architectures, Protocols and Applications, </booktitle> <pages> pages 74-83, </pages> <address> San Francisco, CA, </address> <month> September </month> <year> 1993. </year>
Reference-contexts: In layer parallelism, packets visit multiple processors in a pipelined fashion [7, 24, 26]. Packet-level <ref> [3, 6, 9, 12, 13, 20, 23, 25, 26, 27] </ref> and connection-level [6, 9, 23, 25, 27] parallelisms enable concurrency at higher levels of granularity. 3 We use the terms thread and process interchangeably. 1 [3, 20], the STREAMS implementation in Plan 9 [23], and the ASX framework [26, 27]. <p> Packet-level [3, 6, 9, 12, 13, 20, 23, 25, 26, 27] and connection-level [6, 9, 23, 25, 27] parallelisms enable concurrency at higher levels of granularity. 3 We use the terms thread and process interchangeably. 1 <ref> [3, 20] </ref>, the STREAMS implementation in Plan 9 [23], and the ASX framework [26, 27]. A related form of parallelism is found in the STREAMS implementations in many commercial operating systems [25, 6, 9]. <p> Our results suggest that affinity scheduling can provide significant performance gains to parallel applications requiring low-latency communication, such as those performing multiprocessor IPC or RPC in a distributed environment. In the context of parallel network processing, in which it is well-known that software synchronization can impose a large overhead <ref> [3, 20, 25] </ref>, our work establishes the importance of accounting for caching effects as well. Second, we implement and compare the performance of two approaches toward enabling protocol concurrency. <p> The model does not reflect the impact of contention for software locks, which would inflate the protocol execution time (e.g. [25]). However, UDP/IP has been shown to scale up nearly linearly (to about 20 processors) in multiprocessor parallelizations of the x-kernel <ref> [3, 20] </ref>.
Reference: [4] <author> Torsten Braun and Claudia Schmidt. </author> <title> Implementation of a parallel transport subsystem on a multiprocessor architecture. </title> <booktitle> In Second International Symposium on High-Performance Distributed Computing, </booktitle> <address> Spokane, Washington, </address> <month> July </month> <year> 1993. </year> <month> 25 </month>
Reference-contexts: This captures the packet- and connection-level parallelism found in several multiprocessor protocol implementations, including parallelizations of the x-kernel 1 Depending on the coherence state of the referenced cache line. 2 In functional parallelism, an individual packet concurrently visits multiple processors <ref> [4, 15, 17] </ref>. In layer parallelism, packets visit multiple processors in a pipelined fashion [7, 24, 26].
Reference: [5] <author> Murthy Devarakonda and Arup Mukherjee. </author> <title> Issues in implementation of cache-affinity scheduling. </title> <booktitle> In Proceedings of the Winter 1992 USENIX Conference, </booktitle> <pages> pages 345-357, </pages> <address> San Francicso, CA, </address> <month> January </month> <year> 1992. </year>
Reference-contexts: A number of previous studies of have explored the benefits of affinity-based scheduling in the context of general parallel programs (i.e., non-network-related application processing) <ref> [5, 8, 19, 30, 34] </ref>, with somewhat conflicting results. Although a primarily analytic study has demonstrated significant potential benefits of the technique [30], most implementation studies of process-level affinity scheduling have found only a marginal improvement for common applications [5, 8, 34]. <p> Although a primarily analytic study has demonstrated significant potential benefits of the technique [30], most implementation studies of process-level affinity scheduling have found only a marginal improvement for common applications <ref> [5, 8, 34] </ref>. This is because the time between rescheduling of the affinity-managed entity (in these cases, the process) is much larger than the time required to entirely reload the referenced memory locations into the cache. <p> No prior work has examined the technique in the context of parallel network processing. Four studies consider scheduling at the process level <ref> [5, 8, 30, 34] </ref>, while one considers the finer granularity of loop scheduling [19]. Vaswani and Zajorian [34] show experimentally that affinity scheduling within kernel-level processor space-sharing scheduling policies provides little benefit. Their workload is a mix of three types of parallel applications executing on a Sequent Symmetry multiprocessor. <p> The authors use a variety of queueing-theoretic techniques and employ the analytic cache model developed by Thiebaut and Stone [33]. In <ref> [5] </ref>, Devarakonda and Mukherjee explore implementation issues in affinity scheduling, both in-kernel and within a user-level thread scheduler, on an 8-processor Encore Multimax running Mach 2.5. A schedulable task is defined to have affinity strictly for the processor it most recently visited. <p> A simple heuristic may perform well, e.g., given N executing protocol threads, to unblock an idle protocol thread when the length of the packet queue exceeds N t cold; cold =t hot; hot . Our conclusions contrast with those of several earlier studies <ref> [5, 8, 34] </ref> and support the contention [30, 19] that there are platforms and common workloads for which affinity scheduling is worthwhile. We hope to have demonstrated techniques and a methodology which will facilitate further research in this area.
Reference: [6] <author> Arun Garg. </author> <title> Parallel STREAMS: A multi-processor implementation. </title> <booktitle> In Proceedings of the Winter 1990 USENIX Conference, </booktitle> <pages> pages 163-176, </pages> <address> Washington, D.C., </address> <month> January </month> <year> 1990. </year>
Reference-contexts: In layer parallelism, packets visit multiple processors in a pipelined fashion [7, 24, 26]. Packet-level <ref> [3, 6, 9, 12, 13, 20, 23, 25, 26, 27] </ref> and connection-level [6, 9, 23, 25, 27] parallelisms enable concurrency at higher levels of granularity. 3 We use the terms thread and process interchangeably. 1 [3, 20], the STREAMS implementation in Plan 9 [23], and the ASX framework [26, 27]. <p> In layer parallelism, packets visit multiple processors in a pipelined fashion [7, 24, 26]. Packet-level [3, 6, 9, 12, 13, 20, 23, 25, 26, 27] and connection-level <ref> [6, 9, 23, 25, 27] </ref> parallelisms enable concurrency at higher levels of granularity. 3 We use the terms thread and process interchangeably. 1 [3, 20], the STREAMS implementation in Plan 9 [23], and the ASX framework [26, 27]. <p> A related form of parallelism is found in the STREAMS implementations in many commercial operating systems <ref> [25, 6, 9] </ref>. Although much work has been done in the area, we do not consider functional or layer parallelism, since the synchronization overheads they incur on RISC-based shared-memory machines can be quite high [26]. We present two sets of results.
Reference: [7] <author> Dario Giarrizzo, Matthias Kaiserswerth, Thomas Wicki, and Robin C. Williamson. </author> <title> High-speed parallel protocol implementation. </title> <booktitle> First IFIP WG6.1/WG6.4 International Workshop on Protocols for High-Speed Networks, </booktitle> <pages> pages 165-180, </pages> <month> May </month> <year> 1989. </year>
Reference-contexts: In layer parallelism, packets visit multiple processors in a pipelined fashion <ref> [7, 24, 26] </ref>.
Reference: [8] <author> Anoop Gupta, Andrew Tucker, and Shigeru Urushibara. </author> <title> The impact of operating system scheduling policies and synchronization methods on the performance of parallel applications. </title> <booktitle> In Proceedings of the ACM SIGMETRICS Conference on Measurement and Modeling of Computer Systems, </booktitle> <pages> pages 120-132, </pages> <month> May </month> <year> 1991. </year>
Reference-contexts: A number of previous studies of have explored the benefits of affinity-based scheduling in the context of general parallel programs (i.e., non-network-related application processing) <ref> [5, 8, 19, 30, 34] </ref>, with somewhat conflicting results. Although a primarily analytic study has demonstrated significant potential benefits of the technique [30], most implementation studies of process-level affinity scheduling have found only a marginal improvement for common applications [5, 8, 34]. <p> Although a primarily analytic study has demonstrated significant potential benefits of the technique [30], most implementation studies of process-level affinity scheduling have found only a marginal improvement for common applications <ref> [5, 8, 34] </ref>. This is because the time between rescheduling of the affinity-managed entity (in these cases, the process) is much larger than the time required to entirely reload the referenced memory locations into the cache. <p> No prior work has examined the technique in the context of parallel network processing. Four studies consider scheduling at the process level <ref> [5, 8, 30, 34] </ref>, while one considers the finer granularity of loop scheduling [19]. Vaswani and Zajorian [34] show experimentally that affinity scheduling within kernel-level processor space-sharing scheduling policies provides little benefit. Their workload is a mix of three types of parallel applications executing on a Sequent Symmetry multiprocessor. <p> Although experimental measurements do not support advocacy of in-kernel affinity scheduling, within the user-level thread scheduler the authors find affinity scheduling yields a 12% reduction in execution time for one of the two real applications. In a trace-driven simulation study, Gupta, Tucker and Urushibara <ref> [8] </ref> consider in-kernel affinity scheduling of parallel applications on a shared memory platform. Their approach is to simulate a multiprocessor system by interleaving the process execution traces obtained from a set of parallel scientific applications. <p> A simple heuristic may perform well, e.g., given N executing protocol threads, to unblock an idle protocol thread when the length of the packet queue exceeds N t cold; cold =t hot; hot . Our conclusions contrast with those of several earlier studies <ref> [5, 8, 34] </ref> and support the contention [30, 19] that there are platforms and common workloads for which affinity scheduling is worthwhile. We hope to have demonstrated techniques and a methodology which will facilitate further research in this area.
Reference: [9] <author> Ian Heavens. </author> <title> Experiences in parallelisation of streams-based communications drivers. </title> <booktitle> OpenForum Conference on Distributed Systems, </booktitle> <month> November </month> <year> 1992. </year>
Reference-contexts: In layer parallelism, packets visit multiple processors in a pipelined fashion [7, 24, 26]. Packet-level <ref> [3, 6, 9, 12, 13, 20, 23, 25, 26, 27] </ref> and connection-level [6, 9, 23, 25, 27] parallelisms enable concurrency at higher levels of granularity. 3 We use the terms thread and process interchangeably. 1 [3, 20], the STREAMS implementation in Plan 9 [23], and the ASX framework [26, 27]. <p> In layer parallelism, packets visit multiple processors in a pipelined fashion [7, 24, 26]. Packet-level [3, 6, 9, 12, 13, 20, 23, 25, 26, 27] and connection-level <ref> [6, 9, 23, 25, 27] </ref> parallelisms enable concurrency at higher levels of granularity. 3 We use the terms thread and process interchangeably. 1 [3, 20], the STREAMS implementation in Plan 9 [23], and the ASX framework [26, 27]. <p> A related form of parallelism is found in the STREAMS implementations in many commercial operating systems <ref> [25, 6, 9] </ref>. Although much work has been done in the area, we do not consider functional or layer parallelism, since the synchronization overheads they incur on RISC-based shared-memory machines can be quite high [26]. We present two sets of results.
Reference: [10] <author> Mark D. Hill and Alan J. Smith. </author> <title> Evaluating associativity in CPU caches. </title> <journal> IEEE Transactions on Computers, </journal> <volume> 38(12) </volume> <pages> 1612-1630, </pages> <month> December </month> <year> 1989. </year>
Reference-contexts: This results in only a small change to F (x i ) under the assumption that the reference stream is split approximately between the two caches. Experimental measurements support this assumption (see for example Table 1 of <ref> [10] </ref>). We compute u (R (x i ); L 1 ) and u (R (x i ); L 2 ), where L 1 and L 2 are the line sizes of the L1 and L2 caches, respectively.
Reference: [11] <author> Norman C. Hutchinson and Larry L. Peterson. </author> <title> The x-Kernel: An architecture for implementing network protocols. </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> 17(1) </volume> <pages> 64-76, </pages> <month> January </month> <year> 1991. </year>
Reference-contexts: We establish our results using experimental measurements in conjunction with simulation and analytic techniques. We begin with an unparallelized version of the x-kernel protocol framework <ref> [11, 21] </ref> running in user-space on an 8-processor MIPS R4400-based SGI Challenge XL. We parallelize the receive-side fast-path of the x-kernel's UDP/IP/FDDI protocol stack, and conduct a set of multiprocessor experiments designed to measure packet execution times under specific conditions of cache affinity.
Reference: [12] <author> Mabo Ito, Len Takeuchi, and Gerald Neufeld. </author> <title> A multiprocessing approach for meeting the processing requirements for OSI. </title> <journal> IEEE Journal on Selected Areas in Communications, </journal> <volume> 11(2) </volume> <pages> 220-227, </pages> <month> February </month> <year> 1993. </year>
Reference-contexts: In layer parallelism, packets visit multiple processors in a pipelined fashion [7, 24, 26]. Packet-level <ref> [3, 6, 9, 12, 13, 20, 23, 25, 26, 27] </ref> and connection-level [6, 9, 23, 25, 27] parallelisms enable concurrency at higher levels of granularity. 3 We use the terms thread and process interchangeably. 1 [3, 20], the STREAMS implementation in Plan 9 [23], and the ASX framework [26, 27].
Reference: [13] <author> Niraj Jain, Mischa Schwartz, and Theordore R. Bashkow. </author> <title> Transport protocol processing at Gbps rates. </title> <booktitle> In Proceedings of the ACM SIGCOMM Conference on Communications, Architectures, Protocols and Applications, </booktitle> <pages> pages 188-199, </pages> <address> Philadelphia, PA, </address> <month> September </month> <year> 1990. </year> <note> ACM. </note>
Reference-contexts: In layer parallelism, packets visit multiple processors in a pipelined fashion [7, 24, 26]. Packet-level <ref> [3, 6, 9, 12, 13, 20, 23, 25, 26, 27] </ref> and connection-level [6, 9, 23, 25, 27] parallelisms enable concurrency at higher levels of granularity. 3 We use the terms thread and process interchangeably. 1 [3, 20], the STREAMS implementation in Plan 9 [23], and the ASX framework [26, 27].
Reference: [14] <author> Raj Jain and Shawn Routhier. </author> <title> Packet trains: Measurements and a new model for computer network traffic. </title> <journal> IEEE Journal on Selected Areas in Communications, </journal> <volume> 4(6) </volume> <pages> 986-995, </pages> <month> September </month> <year> 1986. </year>
Reference-contexts: We conjecture a higher performance gain since the send-side execution path is shorter (e.g., there is no packet demultiplexing). Second, recent work indicates that while some classes of network traffic are well-modeled as Poisson, others are not [22]. We plan to investigate how alternative models of packet arrivals <ref> [14, 22] </ref> impact our results. Third, it will be interesting to assess the performance of affinity scheduling as a function of recent architectural trends.
Reference: [15] <author> Matthias Kaiserswerth. </author> <title> The parallel protocol engine. </title> <journal> IEEE/ACM Transactions on Networking, </journal> <volume> 1(6) </volume> <pages> 650-663, </pages> <month> December </month> <year> 1993. </year>
Reference-contexts: This captures the packet- and connection-level parallelism found in several multiprocessor protocol implementations, including parallelizations of the x-kernel 1 Depending on the coherence state of the referenced cache line. 2 In functional parallelism, an individual packet concurrently visits multiple processors <ref> [4, 15, 17] </ref>. In layer parallelism, packets visit multiple processors in a pipelined fashion [7, 24, 26].
Reference: [16] <author> M. Kobayashi and M. MacDougall. </author> <title> The stack growth function: Cache line reference models. </title> <journal> IEEE Transactions on Computers, </journal> <volume> 38(6) </volume> <pages> 798-805, </pages> <month> June </month> <year> 1989. </year>
Reference-contexts: set size, spatial locality, temporal locality, and interactions between spatial and temporal locality, respectively, of the intervening processing. (The fact that u (R (x i ); L) is a power function of R (x i ) for fixed L was observed independently by Thiebaut [31, 32], and Kobayashi and MacDougall <ref> [16] </ref>.) In [28], the authors show equation (1) to be consistent with data given by Smith [29], and Agarwal, Horowitz and Hennessy [1].
Reference: [17] <author> Thomas F. LaPorta and Mischa Schwartz. </author> <title> Performance analysis of MSP: Feature-rich high-speed transport protocol. </title> <journal> IEEE/ACM Transactions on Networking, </journal> <volume> 1(6) </volume> <pages> 740-753, </pages> <month> December </month> <year> 1993. </year>
Reference-contexts: This captures the packet- and connection-level parallelism found in several multiprocessor protocol implementations, including parallelizations of the x-kernel 1 Depending on the coherence state of the referenced cache line. 2 In functional parallelism, an individual packet concurrently visits multiple processors <ref> [4, 15, 17] </ref>. In layer parallelism, packets visit multiple processors in a pipelined fashion [7, 24, 26].
Reference: [18] <author> B. Lindgren, B. Krupczak, M. Ammar, and K. Schwan. </author> <title> Parallel and configurable protocols: Experience with a prototype and an architectural framework. </title> <booktitle> In International Conference on Network Protocols, </booktitle> <pages> pages 234-242, </pages> <address> San Francisco, CA, </address> <month> October </month> <year> 1993. </year>
Reference: [19] <author> Evangelos P. Markatos and Thomas J. LeBlanc. </author> <title> Using processor affinity in loop scheduling on shared-memory multiprocessors. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 5(4) </volume> <pages> 379-400, </pages> <month> April </month> <year> 1994. </year>
Reference-contexts: A number of previous studies of have explored the benefits of affinity-based scheduling in the context of general parallel programs (i.e., non-network-related application processing) <ref> [5, 8, 19, 30, 34] </ref>, with somewhat conflicting results. Although a primarily analytic study has demonstrated significant potential benefits of the technique [30], most implementation studies of process-level affinity scheduling have found only a marginal improvement for common applications [5, 8, 34]. <p> No prior work has examined the technique in the context of parallel network processing. Four studies consider scheduling at the process level [5, 8, 30, 34], while one considers the finer granularity of loop scheduling <ref> [19] </ref>. Vaswani and Zajorian [34] show experimentally that affinity scheduling within kernel-level processor space-sharing scheduling policies provides little benefit. Their workload is a mix of three types of parallel applications executing on a Sequent Symmetry multiprocessor. <p> Simulation results show that affinity scheduling yields a small but consistently positive impact across all applications, increasing processor utilization by an overall average of about 3%. Markatos and LeBlanc consider affinity scheduling of non-nested, completely parallelizable loops on several modern shared-memory multiprocessor platforms <ref> [19] </ref>. Previous work in loop scheduling has focused on load balancing of loops among processors and the overheads introduced by processor synchronization. <p> Our conclusions contrast with those of several earlier studies [5, 8, 34] and support the contention <ref> [30, 19] </ref> that there are platforms and common workloads for which affinity scheduling is worthwhile. We hope to have demonstrated techniques and a methodology which will facilitate further research in this area.
Reference: [20] <author> Erich M. Nahum, David J. Yates, James F. Kurose, and Don Towsley. </author> <title> Performance issues in parallelized network protocols. </title> <booktitle> In Proceedings of the First USENIX Symposium on Operating Systems Design and Implementation (OSDI), </booktitle> <pages> pages 125-137, </pages> <address> Monterey, CA, </address> <month> November </month> <year> 1994. </year>
Reference-contexts: In layer parallelism, packets visit multiple processors in a pipelined fashion [7, 24, 26]. Packet-level <ref> [3, 6, 9, 12, 13, 20, 23, 25, 26, 27] </ref> and connection-level [6, 9, 23, 25, 27] parallelisms enable concurrency at higher levels of granularity. 3 We use the terms thread and process interchangeably. 1 [3, 20], the STREAMS implementation in Plan 9 [23], and the ASX framework [26, 27]. <p> Packet-level [3, 6, 9, 12, 13, 20, 23, 25, 26, 27] and connection-level [6, 9, 23, 25, 27] parallelisms enable concurrency at higher levels of granularity. 3 We use the terms thread and process interchangeably. 1 <ref> [3, 20] </ref>, the STREAMS implementation in Plan 9 [23], and the ASX framework [26, 27]. A related form of parallelism is found in the STREAMS implementations in many commercial operating systems [25, 6, 9]. <p> Our results suggest that affinity scheduling can provide significant performance gains to parallel applications requiring low-latency communication, such as those performing multiprocessor IPC or RPC in a distributed environment. In the context of parallel network processing, in which it is well-known that software synchronization can impose a large overhead <ref> [3, 20, 25] </ref>, our work establishes the importance of accounting for caching effects as well. Second, we implement and compare the performance of two approaches toward enabling protocol concurrency. <p> The model does not reflect the impact of contention for software locks, which would inflate the protocol execution time (e.g. [25]). However, UDP/IP has been shown to scale up nearly linearly (to about 20 processors) in multiprocessor parallelizations of the x-kernel <ref> [3, 20] </ref>. <p> These measurements reflect protocol processing without software checksumming of the packet data. Checksumming can be performed in the x-kernel on our SGI Challenge at the rate of 32 bytes per microsecond <ref> [20] </ref>. We have verified experimentally that our measurements can be extended to reflect per-packet execution time with software checksumming by adding the appropriate fixed overhead (i.e., the per-byte checksumming overhead weighted by the number of bytes of data carried by the packet).
Reference: [21] <author> Sean W. O'Malley and Larry L. Peterson. </author> <title> A dynamic network architecture. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 10(2) </volume> <pages> 110-143, </pages> <month> May </month> <year> 1992. </year>
Reference-contexts: We establish our results using experimental measurements in conjunction with simulation and analytic techniques. We begin with an unparallelized version of the x-kernel protocol framework <ref> [11, 21] </ref> running in user-space on an 8-processor MIPS R4400-based SGI Challenge XL. We parallelize the receive-side fast-path of the x-kernel's UDP/IP/FDDI protocol stack, and conduct a set of multiprocessor experiments designed to measure packet execution times under specific conditions of cache affinity.
Reference: [22] <author> Vern Paxson and Sally Floyd. </author> <title> Wide area traffic: The failure of Poisson modeling. </title> <booktitle> In Proceedings of the ACM SIGCOMM Conference on Communications, Architectures, Protocols and Applications, </booktitle> <pages> pages 257-268, </pages> <address> London, England UK, </address> <month> August </month> <year> 1994. </year>
Reference-contexts: We conjecture a higher performance gain since the send-side execution path is shorter (e.g., there is no packet demultiplexing). Second, recent work indicates that while some classes of network traffic are well-modeled as Poisson, others are not <ref> [22] </ref>. We plan to investigate how alternative models of packet arrivals [14, 22] impact our results. Third, it will be interesting to assess the performance of affinity scheduling as a function of recent architectural trends. <p> We conjecture a higher performance gain since the send-side execution path is shorter (e.g., there is no packet demultiplexing). Second, recent work indicates that while some classes of network traffic are well-modeled as Poisson, others are not [22]. We plan to investigate how alternative models of packet arrivals <ref> [14, 22] </ref> impact our results. Third, it will be interesting to assess the performance of affinity scheduling as a function of recent architectural trends.
Reference: [23] <author> David Presotto. </author> <title> Multiprocessor STREAMS for Plan 9. In United Kingdom UNIX User's Group, </title> <month> January </month> <year> 1993. </year> <month> 26 </month>
Reference-contexts: In layer parallelism, packets visit multiple processors in a pipelined fashion [7, 24, 26]. Packet-level <ref> [3, 6, 9, 12, 13, 20, 23, 25, 26, 27] </ref> and connection-level [6, 9, 23, 25, 27] parallelisms enable concurrency at higher levels of granularity. 3 We use the terms thread and process interchangeably. 1 [3, 20], the STREAMS implementation in Plan 9 [23], and the ASX framework [26, 27]. <p> In layer parallelism, packets visit multiple processors in a pipelined fashion [7, 24, 26]. Packet-level [3, 6, 9, 12, 13, 20, 23, 25, 26, 27] and connection-level <ref> [6, 9, 23, 25, 27] </ref> parallelisms enable concurrency at higher levels of granularity. 3 We use the terms thread and process interchangeably. 1 [3, 20], the STREAMS implementation in Plan 9 [23], and the ASX framework [26, 27]. <p> Packet-level [3, 6, 9, 12, 13, 20, 23, 25, 26, 27] and connection-level [6, 9, 23, 25, 27] parallelisms enable concurrency at higher levels of granularity. 3 We use the terms thread and process interchangeably. 1 [3, 20], the STREAMS implementation in Plan 9 <ref> [23] </ref>, and the ASX framework [26, 27]. A related form of parallelism is found in the STREAMS implementations in many commercial operating systems [25, 6, 9].
Reference: [24] <author> Erich Rutsche and Mattias Kaiserswerth. </author> <title> TCP/IP on the parallel protocol engine. </title> <booktitle> Fourth IFIP TC6.1/WG6.4 International Conference on High Performance Networking, </booktitle> <pages> pages 119-134, </pages> <month> December </month> <year> 1992. </year>
Reference-contexts: In layer parallelism, packets visit multiple processors in a pipelined fashion <ref> [7, 24, 26] </ref>.
Reference: [25] <author> Sunil Saxena, J. Kent Peacock, Fred Yang, Vijaya Verma, and Mohan Krishnan. </author> <title> Pitfalls in multithreading SVR4 STREAMS and other weightless processes. </title> <booktitle> In Proceedings of the Winter 1993 USENIX Conference, </booktitle> <pages> pages 85-96, </pages> <address> San Diego, CA, </address> <month> January </month> <year> 1993. </year>
Reference-contexts: In layer parallelism, packets visit multiple processors in a pipelined fashion [7, 24, 26]. Packet-level <ref> [3, 6, 9, 12, 13, 20, 23, 25, 26, 27] </ref> and connection-level [6, 9, 23, 25, 27] parallelisms enable concurrency at higher levels of granularity. 3 We use the terms thread and process interchangeably. 1 [3, 20], the STREAMS implementation in Plan 9 [23], and the ASX framework [26, 27]. <p> In layer parallelism, packets visit multiple processors in a pipelined fashion [7, 24, 26]. Packet-level [3, 6, 9, 12, 13, 20, 23, 25, 26, 27] and connection-level <ref> [6, 9, 23, 25, 27] </ref> parallelisms enable concurrency at higher levels of granularity. 3 We use the terms thread and process interchangeably. 1 [3, 20], the STREAMS implementation in Plan 9 [23], and the ASX framework [26, 27]. <p> A related form of parallelism is found in the STREAMS implementations in many commercial operating systems <ref> [25, 6, 9] </ref>. Although much work has been done in the area, we do not consider functional or layer parallelism, since the synchronization overheads they incur on RISC-based shared-memory machines can be quite high [26]. We present two sets of results. <p> Our results suggest that affinity scheduling can provide significant performance gains to parallel applications requiring low-latency communication, such as those performing multiprocessor IPC or RPC in a distributed environment. In the context of parallel network processing, in which it is well-known that software synchronization can impose a large overhead <ref> [3, 20, 25] </ref>, our work establishes the importance of accounting for caching effects as well. Second, we implement and compare the performance of two approaches toward enabling protocol concurrency. <p> The approach thus avoids the difficulties inherent in capturing memory references traces from large parallel applications. The model does not reflect the impact of contention for software locks, which would inflate the protocol execution time (e.g. <ref> [25] </ref>). However, UDP/IP has been shown to scale up nearly linearly (to about 20 processors) in multiprocessor parallelizations of the x-kernel [3, 20].
Reference: [26] <author> Douglas C. Schmidt and Tatsuya Suda. </author> <title> Measuring the impact of alternative parallel process architectures on communication subsystem performance. </title> <booktitle> In Proceedings of the 4 th International Workshop on Protocols for High-Speed Networks, </booktitle> <address> Vancouver, British Columbia, </address> <month> August </month> <year> 1994. </year> <pages> IFIP. </pages>
Reference-contexts: In layer parallelism, packets visit multiple processors in a pipelined fashion <ref> [7, 24, 26] </ref>. <p> In layer parallelism, packets visit multiple processors in a pipelined fashion [7, 24, 26]. Packet-level <ref> [3, 6, 9, 12, 13, 20, 23, 25, 26, 27] </ref> and connection-level [6, 9, 23, 25, 27] parallelisms enable concurrency at higher levels of granularity. 3 We use the terms thread and process interchangeably. 1 [3, 20], the STREAMS implementation in Plan 9 [23], and the ASX framework [26, 27]. <p> Packet-level [3, 6, 9, 12, 13, 20, 23, 25, 26, 27] and connection-level [6, 9, 23, 25, 27] parallelisms enable concurrency at higher levels of granularity. 3 We use the terms thread and process interchangeably. 1 [3, 20], the STREAMS implementation in Plan 9 [23], and the ASX framework <ref> [26, 27] </ref>. A related form of parallelism is found in the STREAMS implementations in many commercial operating systems [25, 6, 9]. <p> Although much work has been done in the area, we do not consider functional or layer parallelism, since the synchronization overheads they incur on RISC-based shared-memory machines can be quite high <ref> [26] </ref>. We present two sets of results. First, we evaluate several alternative affinity-based scheduling policies for the various resources involved in parallel network processing.
Reference: [27] <author> Douglas C. Schmidt and Tatsuya Suda. </author> <title> Measuring the performance of parallel message-based process architectures. </title> <booktitle> In Proceedings of the Conference on Computer Communications (IEEE Infocom), </booktitle> <pages> pages 624-633, </pages> <address> Boston, MA, </address> <month> April </month> <year> 1995. </year> <note> IEEE. </note>
Reference-contexts: In layer parallelism, packets visit multiple processors in a pipelined fashion [7, 24, 26]. Packet-level <ref> [3, 6, 9, 12, 13, 20, 23, 25, 26, 27] </ref> and connection-level [6, 9, 23, 25, 27] parallelisms enable concurrency at higher levels of granularity. 3 We use the terms thread and process interchangeably. 1 [3, 20], the STREAMS implementation in Plan 9 [23], and the ASX framework [26, 27]. <p> In layer parallelism, packets visit multiple processors in a pipelined fashion [7, 24, 26]. Packet-level [3, 6, 9, 12, 13, 20, 23, 25, 26, 27] and connection-level <ref> [6, 9, 23, 25, 27] </ref> parallelisms enable concurrency at higher levels of granularity. 3 We use the terms thread and process interchangeably. 1 [3, 20], the STREAMS implementation in Plan 9 [23], and the ASX framework [26, 27]. <p> Packet-level [3, 6, 9, 12, 13, 20, 23, 25, 26, 27] and connection-level [6, 9, 23, 25, 27] parallelisms enable concurrency at higher levels of granularity. 3 We use the terms thread and process interchangeably. 1 [3, 20], the STREAMS implementation in Plan 9 [23], and the ASX framework <ref> [26, 27] </ref>. A related form of parallelism is found in the STREAMS implementations in many commercial operating systems [25, 6, 9].
Reference: [28] <author> Jaswinder Pal Singh, Harold S. Stone, and Dominique F. Thiebaut. </author> <title> A model of workloads and its use in miss-rate prediction for fully associative caches. </title> <journal> IEEE Transactions on Computers, </journal> <volume> 41(7) </volume> <pages> 811-825, </pages> <month> July </month> <year> 1992. </year>
Reference-contexts: These measurements are then used to parameterize the analytic component of a simulation model of multiprocessor protocol processing, under various affinity-based scheduling policies. The analytic model, which combines well-established analytic results from other researchers <ref> [28, 33] </ref>, accounts for the impact of general non-protocol activity on packet execution time as well as the specific cache architecture and organization of our experimental platform. <p> This is the minimum execution time. We formulate the packet execution time t i as a monotonically increasing function of x i , bounded below by t hot and above by t cold . To do so, we rely on the work of Singh, Stone and Thiebaut <ref> [28] </ref>. Let m denote the average memory reference rate of the intervening processing. We assume m is stationary. R (x i ) = x i =m is the average number of references issued during x i . <p> Let u (R (x i ); L) denote the footprint function (the number of unique memory lines referenced at the processor in R references) for a cache line size L bytes. In <ref> [28] </ref>, the authors show that this function is closely modeled by an expression of the form u (R (x i ); L) = W L a R b d logL log R (x i ) (1) where the constants W , a, b, and d relate to working set size, spatial <p> spatial locality, temporal locality, and interactions between spatial and temporal locality, respectively, of the intervening processing. (The fact that u (R (x i ); L) is a power function of R (x i ) for fixed L was observed independently by Thiebaut [31, 32], and Kobayashi and MacDougall [16].) In <ref> [28] </ref>, the authors show equation (1) to be consistent with data given by Smith [29], and Agarwal, Horowitz and Hennessy [1].
Reference: [29] <author> A. J. Smith. </author> <title> Line (block) size choice for CPU cache memories. </title> <journal> IEEE Transactions on Computers, </journal> <volume> C-36(9):1063-1075, </volume> <month> September </month> <year> 1987. </year>
Reference-contexts: intervening processing. (The fact that u (R (x i ); L) is a power function of R (x i ) for fixed L was observed independently by Thiebaut [31, 32], and Kobayashi and MacDougall [16].) In [28], the authors show equation (1) to be consistent with data given by Smith <ref> [29] </ref>, and Agarwal, Horowitz and Hennessy [1]. They also demonstrate its accuracy through detailed validation on segments of a 200-million-reference trace of a multiprogrammed IBM/370 MVS workload, consisting of a representative workload of user applications and operating system activity.
Reference: [30] <author> Mark S. Squillante and Edward D. Lazowska. </author> <title> Using processor cache affinity information in shared-memory multiprocessor scheduling. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 4(2) </volume> <pages> 131-143, </pages> <month> February </month> <year> 1993. </year>
Reference-contexts: A number of previous studies of have explored the benefits of affinity-based scheduling in the context of general parallel programs (i.e., non-network-related application processing) <ref> [5, 8, 19, 30, 34] </ref>, with somewhat conflicting results. Although a primarily analytic study has demonstrated significant potential benefits of the technique [30], most implementation studies of process-level affinity scheduling have found only a marginal improvement for common applications [5, 8, 34]. <p> A number of previous studies of have explored the benefits of affinity-based scheduling in the context of general parallel programs (i.e., non-network-related application processing) [5, 8, 19, 30, 34], with somewhat conflicting results. Although a primarily analytic study has demonstrated significant potential benefits of the technique <ref> [30] </ref>, most implementation studies of process-level affinity scheduling have found only a marginal improvement for common applications [5, 8, 34]. <p> This allows us to evaluate the marginal contributions of the individual policies, and assess their relative performance gains. Second, in our modeling of non-protocol activity, we present extensions to an existing analytic model of the execution time of an affinity-scheduled task in a multitasking environment <ref> [33, 30] </ref>. The primary contribution here is to relax the requirement of having identified the task's footprintthe set of cache lines currently referenced by the executing task. In practice, it can be hard to determine a task's footprint, especially for large, multi-threaded, multiprocessor applications. <p> Let F (x i ) denote the fraction of the cache which has been flushed by these u (R (x i ); L) references. We compute F (x i ) by assuming the references map independently into cache sets, an assumption also made in <ref> [30] </ref> and [33] 7 The model can be easily extended to a set-associative cache. 11 in similar context. Let the random variable X denote the number of the u (R (x i ); L) references that map to a randomly chosen set. <p> Although each processor on the Challenge has separate 16K instruction and data caches at L1, we make the 8 Task execution time as the linear interpolation of the maximum reload transient is also the approach taken in <ref> [30] </ref>. In the authors' formulation, the inherent computing demand of a task is denoted D, the average time to reload the entire footprint is C, and the fraction of the footprint displaced is R. <p> Note that our approach, which is based on direct timing measurements, does not require identifying the footprint of the task being affinity scheduled (as is the case, e.g., in <ref> [30] </ref>). The approach thus avoids the difficulties inherent in capturing memory references traces from large parallel applications. The model does not reflect the impact of contention for software locks, which would inflate the protocol execution time (e.g. [25]). <p> No prior work has examined the technique in the context of parallel network processing. Four studies consider scheduling at the process level <ref> [5, 8, 30, 34] </ref>, while one considers the finer granularity of loop scheduling [19]. Vaswani and Zajorian [34] show experimentally that affinity scheduling within kernel-level processor space-sharing scheduling policies provides little benefit. Their workload is a mix of three types of parallel applications executing on a Sequent Symmetry multiprocessor. <p> The explanation lies in the fact that among these applications, the upper bound on the time to completely reload the processor cache (about 1-2ms) is small in comparison to the processor reallocation interval (about 200-500ms). Squillante and Lazowska <ref> [30] </ref> conduct a modeling study designed to gain insight into the general class of scheduling policies which consider the state of processor caches. The study examines the performance of a range of in-kernel affinity scheduling policies on a multiprocessor system running multiple independent single-threaded processes. <p> Our conclusions contrast with those of several earlier studies [5, 8, 34] and support the contention <ref> [30, 19] </ref> that there are platforms and common workloads for which affinity scheduling is worthwhile. We hope to have demonstrated techniques and a methodology which will facilitate further research in this area.
Reference: [31] <author> Dominique F. Thiebaut. </author> <title> Influence of program transients in computer cache-memories. </title> <type> PhD thesis, </type> <institution> University of Massachusetts, Amherst, </institution> <year> 1989. </year>
Reference-contexts: b, and d relate to working set size, spatial locality, temporal locality, and interactions between spatial and temporal locality, respectively, of the intervening processing. (The fact that u (R (x i ); L) is a power function of R (x i ) for fixed L was observed independently by Thiebaut <ref> [31, 32] </ref>, and Kobayashi and MacDougall [16].) In [28], the authors show equation (1) to be consistent with data given by Smith [29], and Agarwal, Horowitz and Hennessy [1].
Reference: [32] <author> Dominique F. Thiebaut. </author> <title> On the fractal dimension of computer programs and its application to the prediction of the cache miss ratio. </title> <journal> IEEE Transactions on Computers, </journal> <volume> 38(7) </volume> <pages> 1012-1026, </pages> <month> July </month> <year> 1989. </year>
Reference-contexts: b, and d relate to working set size, spatial locality, temporal locality, and interactions between spatial and temporal locality, respectively, of the intervening processing. (The fact that u (R (x i ); L) is a power function of R (x i ) for fixed L was observed independently by Thiebaut <ref> [31, 32] </ref>, and Kobayashi and MacDougall [16].) In [28], the authors show equation (1) to be consistent with data given by Smith [29], and Agarwal, Horowitz and Hennessy [1].
Reference: [33] <author> Dominique F. Thiebaut and Harold S. Stone. </author> <title> Footprints in the cache. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 5(4) </volume> <pages> 305-329, </pages> <month> November </month> <year> 1987. </year>
Reference-contexts: These measurements are then used to parameterize the analytic component of a simulation model of multiprocessor protocol processing, under various affinity-based scheduling policies. The analytic model, which combines well-established analytic results from other researchers <ref> [28, 33] </ref>, accounts for the impact of general non-protocol activity on packet execution time as well as the specific cache architecture and organization of our experimental platform. <p> This allows us to evaluate the marginal contributions of the individual policies, and assess their relative performance gains. Second, in our modeling of non-protocol activity, we present extensions to an existing analytic model of the execution time of an affinity-scheduled task in a multitasking environment <ref> [33, 30] </ref>. The primary contribution here is to relax the requirement of having identified the task's footprintthe set of cache lines currently referenced by the executing task. In practice, it can be hard to determine a task's footprint, especially for large, multi-threaded, multiprocessor applications. <p> Let F (x i ) denote the fraction of the cache which has been flushed by these u (R (x i ); L) references. We compute F (x i ) by assuming the references map independently into cache sets, an assumption also made in [30] and <ref> [33] </ref> 7 The model can be easily extended to a set-associative cache. 11 in similar context. Let the random variable X denote the number of the u (R (x i ); L) references that map to a randomly chosen set. <p> This work motivated our own by demonstrating that when the cache reload time is large with respect to the task's inherent computing demands, affinity scheduling can have a significant impact. The authors use a variety of queueing-theoretic techniques and employ the analytic cache model developed by Thiebaut and Stone <ref> [33] </ref>. In [5], Devarakonda and Mukherjee explore implementation issues in affinity scheduling, both in-kernel and within a user-level thread scheduler, on an 8-processor Encore Multimax running Mach 2.5. A schedulable task is defined to have affinity strictly for the processor it most recently visited.
Reference: [34] <author> Raj Vaswani and John Zahorjan. </author> <title> The implications of cache affinity on processor scheduling for multiprogrammed, shared memory multiprocessors. </title> <booktitle> In Proceedings of the Thirteenth Symposium on Operating Systems Principles, </booktitle> <pages> pages 26-40, </pages> <address> Pacific Grove, CA, </address> <month> October </month> <year> 1991. </year> <note> ACM. </note>
Reference-contexts: A number of previous studies of have explored the benefits of affinity-based scheduling in the context of general parallel programs (i.e., non-network-related application processing) <ref> [5, 8, 19, 30, 34] </ref>, with somewhat conflicting results. Although a primarily analytic study has demonstrated significant potential benefits of the technique [30], most implementation studies of process-level affinity scheduling have found only a marginal improvement for common applications [5, 8, 34]. <p> Although a primarily analytic study has demonstrated significant potential benefits of the technique [30], most implementation studies of process-level affinity scheduling have found only a marginal improvement for common applications <ref> [5, 8, 34] </ref>. This is because the time between rescheduling of the affinity-managed entity (in these cases, the process) is much larger than the time required to entirely reload the referenced memory locations into the cache. <p> No prior work has examined the technique in the context of parallel network processing. Four studies consider scheduling at the process level <ref> [5, 8, 30, 34] </ref>, while one considers the finer granularity of loop scheduling [19]. Vaswani and Zajorian [34] show experimentally that affinity scheduling within kernel-level processor space-sharing scheduling policies provides little benefit. Their workload is a mix of three types of parallel applications executing on a Sequent Symmetry multiprocessor. <p> No prior work has examined the technique in the context of parallel network processing. Four studies consider scheduling at the process level [5, 8, 30, 34], while one considers the finer granularity of loop scheduling [19]. Vaswani and Zajorian <ref> [34] </ref> show experimentally that affinity scheduling within kernel-level processor space-sharing scheduling policies provides little benefit. Their workload is a mix of three types of parallel applications executing on a Sequent Symmetry multiprocessor. The measured reduction in task response time enabled by affinity scheduling does not exceed 1%. <p> A simple heuristic may perform well, e.g., given N executing protocol threads, to unblock an idle protocol thread when the length of the packet queue exceeds N t cold; cold =t hot; hot . Our conclusions contrast with those of several earlier studies <ref> [5, 8, 34] </ref> and support the contention [30, 19] that there are platforms and common workloads for which affinity scheduling is worthwhile. We hope to have demonstrated techniques and a methodology which will facilitate further research in this area.

References-found: 34

