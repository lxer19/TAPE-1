URL: http://www.isi.edu/~crago/hidisc/tr-96-06.ps
Refering-URL: http://www.isi.edu/~crago/hidisc/papers.html
Root-URL: http://www.isi.edu
Email: crago@isi.edu  apoorv@isi.edu  obenland@isi.edu  despain@isi.edu  
Phone: (310) 822-1511 ext. 713  (310) 822-1511 ext. 102  (310) 822-1511 ext. 713  (310) 822-1511 ext. 377  
Author: Stephen P. Crago Apoorv Srivastava Kevin Obenland Alvin M. Despain 
Date: June 14, 1996  
Address: 4676 Admiralty Way Marina del Rey, CA 90292-6695  
Affiliation: University of Southern California Information Sciences Institute  
Abstract: A Hierarchical Decoupled Architecture: Preliminary Results 
Abstract-found: 1
Intro-found: 1
Reference: [AgCo87] <author> T.Y. Agawala and J. Cocke. </author> <title> High Performance Reduced Instruction Set Processors. </title> <institution> IBM T.J. Watson Research Center, </institution> <type> Technical Report #55845, </type> <month> March </month> <year> 1987. </year>
Reference-contexts: As transistors continue to shrink and die sizes get larger, computer architects must determine how to best use the larger number of available transistors. Processors have recently used transistors to provide multiple functional units, using the VLIW [Fish83] or superscalar <ref> [AgCo87] </ref> paradigm, and are deeply pipelined to increase the clock speed. While superscalar, VLIW, and deep pipelining can all provide additional microparallelism, processor performance is limited by memory performance, diminishing the returns of additional micro-parallelism within uniprocessors.
Reference: [BaCh91] <author> J.-L. Baer and T.-F. Chen. </author> <title> An effective on-chip preloading scheme to reduce data access penalty. </title> <booktitle> Proceedings of Supercomputing 91, </booktitle> <year> 1991. </year>
Reference-contexts: Hardware schemes, such as hardware prefetching and multithreading, adapt dynamically to run-time behavior. However, hardware schemes are hardwired and cannot adapt to individual programs. Hardware prefetching <ref> [BaCh91] </ref> tries to reduce the miss ratio of the cache by prefetch-ing additional data when a miss occurs. Stride-based accesses can be accommodated by hardware prefetching because the memory access pattern is static over the course of a program.
Reference: [Brig74] <author> E.O. Brigham. </author> <title> The Fast Fourier Transform, </title> <publisher> Prentice-Hall, Inc., </publisher> <year> 1974, </year> <note> p. 165. </note>
Reference-contexts: The right-hand side of Figure 5 shows the performance of the bit-reverse algorithm, shown in Figure 7,used in the Fast Fourier Transform algorithm <ref> [Brig74] </ref>. The bit_reverse function, which is not shown, is a function that bit-reverses a binary number and is executed on the AP and the CMP. The results shown are for a 1024-element array of 32-bit integers.
Reference: [ChSB86] <author> D.R. Cheriton, G.A. Slavenburg, and P.D. Boyle. </author> <title> Software-controlled caches in the VMP multiprocessor. </title> <institution> CS-TR-86-1105, Stanford University, </institution> <year> 1986. </year>
Reference-contexts: However, prefetches should not be inserted unless cache misses can be predicted because the instruction overhead will reduce performance when cache hit rates are high. Software cache controlling <ref> [ChSB86] </ref> has been proposed, but it has formerly been used at the time of a cache miss and uses the main processor to execute the software. Compiler techniques are static in nature and therefore work only in a limited domain.
Reference: [Denn74] <author> R. Dennard et al. </author> <title> Design of Ion-Implanted MOSFETs with very small physical dimensions. </title> <journal> IEEE Journal of Solid-State Circuits, v. CS-9, </journal> <volume> No. 5, </volume> <month> Oct. </month> <year> 1974. </year>
Reference-contexts: 1. Introduction The gap between processor speeds and DRAM speeds is increasing at an exponential rate [WuMc94]. Transistor sizes have been shrinking and die sizes have been growing steadily for many years [Sano94]. These trends have allowed processors to get faster <ref> [Denn74] </ref> and more complex. Processors have used these additional transistors by adding multiple functional units and highly pipelined modules. As transistors continue to shrink and die sizes get larger, computer architects must determine how to best use the larger number of available transistors.
Reference: [Fish83] <author> Fisher, J.A. </author> <title> Very long instruction word architectures and ELSI-512. </title> <booktitle> Proceedings of the Tenth Symposium on Computer Architecture, </booktitle> <year> 1993, </year> <pages> pp. 140-150. </pages>
Reference-contexts: As transistors continue to shrink and die sizes get larger, computer architects must determine how to best use the larger number of available transistors. Processors have recently used transistors to provide multiple functional units, using the VLIW <ref> [Fish83] </ref> or superscalar [AgCo87] paradigm, and are deeply pipelined to increase the clock speed. While superscalar, VLIW, and deep pipelining can all provide additional microparallelism, processor performance is limited by memory performance, diminishing the returns of additional micro-parallelism within uniprocessors.
Reference: [GHLP85] <author> J.R. Goodman, J.T. Hsieh, K. Liou, A.R. Pleszkun, P.B. Schechter, and H.C. Young. </author> <title> PIPE: a VLSI decoupled architecture. </title> <booktitle> Proceedings of the 16th Annual International Symposium on Computer Architecture, </booktitle> <year> 1985, </year> <pages> pp. 20-27. </pages>
Reference-contexts: Our decoupled architecture also overlaps computation but is not limited to statically determined instructions in loops as it is in software pipelining. Many decoupled architectures have been proposed to exploit instruction-level parallelism and to hide memory latency. The PIPE <ref> [GHLP85] </ref>, DEAP [KuHC94], and ZS-1 [Smit89] are similar to a 2-level HiDISC without a data cache. The WM [Wulf92] exploits microparallelism by decoupling the fetch unit, the integer processing unit, and the floating point processing and provides streaming.
Reference: [Joup90] <author> N.P. Jouppi. </author> <title> Improving direct-mapped cache performance by the addition of a small fully-associative cache and prefetch buffers. </title> <type> Technical Note TN-14, </type> <institution> Digital Western Research Laboratory, </institution> <month> March </month> <year> 1990. </year>
Reference-contexts: 8 12 16 0 50 100 150 200 250 Execution Time (Millions of Cycles) Convolution (1 KB Cache) 2-Level Decoupled HiDISC Uniprocessor w/o Prefetching 2-Level Decoupled HiDISC 2-Level Decoupled HiDISC June 14, 1996 14 effect of prefetches conflicting with useful data, which could also be reduced using a victim cache <ref> [Joup90] </ref>. The cache sizes used in the results of Figure 5, which are shown in bytes, are small because they allow us to simulate the effects of large data sets while keeping the simulation time low.
Reference: [KuHC94] <author> L. Kurian, P.T. Hulina, and L.D. Caraor. </author> <title> Memory latency effects in decoupled architectures. </title> <journal> IEEE Transactions on Computers, v. </journal> <volume> 43, no. </volume> <month> 10 (October </month> <year> 1994), </year> <pages> pp. 1129-1139. </pages>
Reference-contexts: Many applications do not have enough locality to reap the benefit of a cache. Many signal processing applications, for example, have been shown to perform worse with a cache than without one because there is a penalty associated with cache misses <ref> [KuHC94] </ref>. When each datum is used only once, the penalty is often not offset by a sufficient benefit. The HiDISC system, shown in Figure 1, adds a processor between each of the three levels of the memory hierarchy. <p> Our decoupled architecture also overlaps computation but is not limited to statically determined instructions in loops as it is in software pipelining. Many decoupled architectures have been proposed to exploit instruction-level parallelism and to hide memory latency. The PIPE [GHLP85], DEAP <ref> [KuHC94] </ref>, and ZS-1 [Smit89] are similar to a 2-level HiDISC without a data cache. The WM [Wulf92] exploits microparallelism by decoupling the fetch unit, the integer processing unit, and the floating point processing and provides streaming.
Reference: [Lam88] <author> M.S. Lam. </author> <title> Software pipelining: an effective scheduling technique for VLIW machines. </title> <booktitle> Proceedings of the SIGPLAN Conference on Programming Language Design and Implementation, </booktitle> <year> 1988, </year> <pages> pp. 318-328. </pages>
Reference-contexts: Techniques such as tiling [Wolf92] try to increase locality by making compile time transformations. These techniques are limited to scientific programs which exhibit simple access behavior. In our architecture we can use these techniques as well as others which would not be possible by complier analysis alone. Software pipelining <ref> [Lam88] </ref> is a technique to expose more parallelism so that computation can be overlapped. Our decoupled architecture also overlaps computation but is not limited to statically determined instructions in loops as it is in software pipelining.
Reference: [MoLG92] <author> T.C. Mowry, M.S. Lam, and A. Gupta. </author> <title> Design and evaluation of a compiler algorithm for prefetching. </title> <booktitle> Proceedings of the Fifth International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <month> June 14, </month> <year> 1996 </year> <month> 19 </month> <year> 1992, </year> <pages> pp. 62-73. </pages>
Reference-contexts: Software schemes, such as software prefetching and software controlled caches, expose the memory hierarchy to the compiler. The compiler can adapt the memory hierarchy behavior to the individual program. However, software schemes incur an instruction overhead and cannot adapt to dynamic run-time behavior. Software prefetching <ref> [MoLG92] </ref> allows prefetch instructions to be inserted into the instruction stream and hides memory latency by loading data into the cache before it is needed. Prefetching initiates data accesses a fixed number of instructions before the data is needed and works well only when memory behavior is predictable.
Reference: [Papo80] <author> A. Papoulis. </author> <title> Circuits and Systems, </title> <publisher> Holt, Rinehart and Winston, Inc., </publisher> <year> 1980, </year> <note> p. 146. </note>
Reference-contexts: The CMP also has a GET_SYNC instruction which stalls until a token has been put on the Synchronization Queue by the AP. Table 3 shows the proposed instructions of the CMP. 3.4 Sample Program In this section, we use the inner loop of the discrete convolution algorithm <ref> [Papo80] </ref> to illustrate how a program runs on the HiDISC architecture. <p> The results on the left-hand side of Figure 5 show the performance of the discrete convolution algorithm, which is shown in Figure 6 <ref> [Papo80] </ref>. The algorithm is implemented with the straightfor ward method, without using techniques for improving cache locality, and the results shown are for arrays of 640 32-bit integers.
Reference: [Sano94] <author> B.J. Sano. </author> <title> Microparallel Processors. </title> <type> Ph.D. Thesis, </type> <institution> University of Southern Cali-fornia, </institution> <year> 1994. </year>
Reference-contexts: 1. Introduction The gap between processor speeds and DRAM speeds is increasing at an exponential rate [WuMc94]. Transistor sizes have been shrinking and die sizes have been growing steadily for many years <ref> [Sano94] </ref>. These trends have allowed processors to get faster [Denn74] and more complex. Processors have used these additional transistors by adding multiple functional units and highly pipelined modules.
Reference: [Smit89] <author> J. Smith. </author> <title> Dynamic Instruction Scheduling and the Astronautics ZS-1. </title> <journal> Computer, v.22, </journal> <volume> no. </volume> <month> 2 (July </month> <year> 1989), </year> <pages> pp. 21-35. </pages>
Reference-contexts: Our decoupled architecture also overlaps computation but is not limited to statically determined instructions in loops as it is in software pipelining. Many decoupled architectures have been proposed to exploit instruction-level parallelism and to hide memory latency. The PIPE [GHLP85], DEAP [KuHC94], and ZS-1 <ref> [Smit89] </ref> are similar to a 2-level HiDISC without a data cache. The WM [Wulf92] exploits microparallelism by decoupling the fetch unit, the integer processing unit, and the floating point processing and provides streaming.
Reference: [TyFP92] <author> G. Tyson, M. Farrens, and A.R. Pleszkun. MISC: </author> <title> a multiple instruction stream computer. </title> <booktitle> Proceedings of the 25th Annual International Symposium on Microar-chitecture, </booktitle> <month> December </month> <year> 1992, </year> <pages> pp. 193-196. </pages>
Reference-contexts: The absence of a data cache in these decoupled architectures prevents them from exploiting locality when it does exist. When dependencies prevent the access processor from running far enough ahead of the computation processor, these decoupled architectures will not be able to hide memory latency. The MISC <ref> [TyFP92] </ref> architecture includes a data cache, but, like the WM does not address the performance of the memory hierarchy. Each processor in the MISC architecture is the same and has the same access to other processors and the memory hierarchy, unlike the HiDISC, in which each processor is specialized.
Reference: [WeGu89] <author> W.D. Weber and A. Gupta. </author> <title> Exploring the benefits of multiple hardware contexts in a multiprocessor architecture: preliminary results. </title> <booktitle> Proceedings of the 16th Annual International Symposium on Computer Architecture, </booktitle> <year> 1989, </year> <pages> pp. 273-280. </pages>
Reference-contexts: Hardware prefetching [BaCh91] tries to reduce the miss ratio of the cache by prefetch-ing additional data when a miss occurs. Stride-based accesses can be accommodated by hardware prefetching because the memory access pattern is static over the course of a program. Multithread-ing <ref> [WeGu89] </ref> has also been proposed to hide memory latency from the processor by switching threads when cache misses occur. Multithreading places a heavy burden on the compiler because the compiler has to find enough independent threads to hide the memory latency.
Reference: [Wolf92] <author> M.E. Wolf. </author> <title> Improving Locality and Parallelism in Nested Loops. </title> <type> Ph.D. Thesis, </type> <institution> Stanford University, </institution> <year> 1992. </year>
Reference-contexts: These techniques do not extend well to programs with irregular data structures such as dynamically allocated structures in symbolic programs. Techniques such as tiling <ref> [Wolf92] </ref> try to increase locality by making compile time transformations. These techniques are limited to scientific programs which exhibit simple access behavior. In our architecture we can use these techniques as well as others which would not be possible by complier analysis alone.
Reference: [WuMc94] <author> W.A. Wulf. </author> <title> Hitting the Memory Wall: Implications of the Obvious. Computer Architecture News, </title> <editor> v. </editor> <volume> 23, no. </volume> <month> 1 (December </month> <year> 1994), </year> <pages> pp. 20-24. </pages>
Reference-contexts: 1. Introduction The gap between processor speeds and DRAM speeds is increasing at an exponential rate <ref> [WuMc94] </ref>. Transistor sizes have been shrinking and die sizes have been growing steadily for many years [Sano94]. These trends have allowed processors to get faster [Denn74] and more complex. Processors have used these additional transistors by adding multiple functional units and highly pipelined modules.
Reference: [Wulf92] <author> W.A. Wulf. </author> <title> Evaluation of the WM Architecture. </title> <booktitle> Proceedings of the 19th Annual International Symposium on Computer Architecture, </booktitle> <year> 1992, </year> <pages> pp. 382-390. </pages>
Reference-contexts: Many decoupled architectures have been proposed to exploit instruction-level parallelism and to hide memory latency. The PIPE [GHLP85], DEAP [KuHC94], and ZS-1 [Smit89] are similar to a 2-level HiDISC without a data cache. The WM <ref> [Wulf92] </ref> exploits microparallelism by decoupling the fetch unit, the integer processing unit, and the floating point processing and provides streaming. The absence of a data cache in these decoupled architectures prevents them from exploiting locality when it does exist.
References-found: 19

