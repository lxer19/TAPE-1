URL: ftp://ftp.cs.umass.edu/pub/mckinley/topds.ps.gz
Refering-URL: http://spa-www.cs.umass.edu/bibliography.html
Root-URL: 
Title: A Compiler Optimization Algorithm for Shared-Memory Multiprocessors  
Author: Kathryn S. M c Kinley 
Keyword: Index TermsProgram parallelization, parallelization techniques, program optimization, data locality, restructuring compilers, performance evaluation.  
Address: Amherst, MA 01003-4610.  
Affiliation: Department of Computer Science, University of Massachusetts,  
Note: K. M c Kinley is with the  
Email: E-mail: mckinley@cs.umass.edu.  
Date: July 1, 1998  
Abstract: This paper presents a new compiler optimization algorithm that parallelizes applications for symmetric, shared-memory multiprocessors. The algorithm considers data locality, parallelism, and the granularity of parallelism. It uses dependence analysis and a simple cache model to drive its optimizations. It also optimizes across procedures by using interprocedural analysis and transformations. We validate the algorithm by hand-applying it to sequential versions of parallel, Fortran programs operating over dense matrices. The programs initially were hand-coded to target a variety of parallel machines using loop parallelism. We ignore the user's parallel loop directives, and use known and implemented dependence and interprocedural analysis to find parallelism. We then apply our new optimization algorithm to the resulting program. We compare the original parallel program to the hand-optimized program, and show that our algorithm improves 3 programs, matches 4 programs, and degrades 1 program in our test suite on a shared-memory, bus-based parallel machine with local caches. This experiment suggests existing dependence and interprocedural array analysis can automatically detect user parallelism, and demonstrates that user parallelized codes often benefit from our compiler optimizations, providing evidence that we need both parallel algorithms and compiler optimizations to effectively utilize parallel machines. In IEEE Transactions on Parallel and Distributed Systems, VOL. 9, NO. 8, August, 1998. Personal use of this material is permitted. However, perrmission to reprint/republish this material for advertising or promotional puroses or for creating new collective works for resale or redistribution to servers or lists, or to reuse any copyrighted component of this work in other works must be obtained from the IEEE. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> F. Allen, M. Burke, P. Charles, J. Ferrante, W. Hsieh, and V. Sarkar. </author> <title> A framework for detecting useful parallelism. </title> <booktitle> In Proceedings of the Second International Conference on Supercomputing, </booktitle> <address> St. Malo, France, </address> <month> July </month> <year> 1988. </year>
Reference-contexts: Many commercial parallelizing compilers do not reveal their optimization strategies to maintain a market advantage. The IBM PTRAN project, an industrial research compiler, has published parallelization algorithms that use control and data dependence, and a wide selection of transformations, but without results <ref> [1, 40, 41] </ref>. Below, we compare this study with those of parallelizing compilers from Illinois and Stanford [10, 17, 19, 43]. The Illinois studies are traditional; they evaluate their techniques on dusty deck programs [10, 16, 28, 17].
Reference: [2] <author> J. R. Allen, D. Callahan, and K. Kennedy. </author> <title> Automatic decomposition of scientific programs for parallel execution. </title> <booktitle> In Proceedings of the Fourteenth Annual ACM Symposium on the Principles of Programming Languages, </booktitle> <address> Munich, Germany, </address> <month> January </month> <year> 1987. </year>
Reference-contexts: Each scr is then placed in a loop by itself which divides the statements up into the finest granularity possible. In the style of Allen et al. <ref> [2] </ref>, the process is repeated for the next outermost loop, until some loop cannot be distributed over the statements (this loop may of course be l n ). If new nests are created as in Figure 7, these become candidates for parallelization by Optimize.
Reference: [3] <author> J. R. Allen and K. Kennedy. </author> <title> Automatic loop interchange. </title> <booktitle> In Proceedings of the SIGPLAN '84 Symposium on Compiler Construction, </booktitle> <address> Montreal, Canada, </address> <month> June </month> <year> 1984. </year>
Reference-contexts: Given an original set of legal direction vectors, each step of the for is guaranteed to find a loop which results in a legal (lexicographically positive) direction vector, otherwise the original was not legal <ref> [3, 9] </ref>. In addition, if any loop 1 through n1 may be legally positioned prior to n , it will be. NearbyPermutation therefore places the loops carrying the most reuse as innermost as possible.
Reference: [4] <author> J. R. Allen and K. Kennedy. </author> <title> Automatic translation of Fortran programs to vector form. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 9(4):491542, </volume> <month> October </month> <year> 1987. </year>
Reference-contexts: If new nests are created as in Figure 7, these become candidates for parallelization by Optimize. This algorithm is not optimal because combining distribution with loop permutation may uncover deeper distributions that in turn may be more effectively parallelized <ref> [4, 33] </ref>. This flexibility was not required in our experiments and is not explored further here. After distribution and parallelization, there may be a sequence of parallel and sequential nests, some of which may be fused back together.
Reference: [5] <author> J. Anderson, S. P. Amarasinghe, and M. Lam. </author> <title> Data and computation transformations for multiprocessors. </title> <booktitle> In Proceedings of the Fifth ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming, </booktitle> <address> Santa Barbara, CA, </address> <month> July </month> <year> 1995. </year>
Reference-contexts: Their work includes very few experimental results for the parallelization algorithm, and they do not perform fusion, distribution, or any interprocedural analysis and transformations. More recent work on the SUIF system includes extensive interprocedural analysis and data and control restructuring between nests to further improve locality and parallelism <ref> [5, 19] </ref>. This work demonstrates important improvements over our approach, and some success on dusty deck programs. However, they do not perform fusion or distribution, and our approach is effective in many cases. <p> We instead focus on restructuring the program control flow. We also transform the program into a sequential equivalent and therefore do not analyze parallel programs. Data transformations [23] and combining data transformations with control transformations <ref> [5] </ref> can successfully parallelize programs that the techniques presented in this paper can not. 8 Conclusions This paper presents a new parallelization algorithm that balances parallelism and data locality. We use an effective strategy to introduce locality, exploit parallelism, and maximize the granularity of parallelism.
Reference: [6] <author> B. Appelbe, S. Doddapaneni, and C. Hardnett. </author> <title> A new algorithm for global optimization for parallelism and locality. </title> <booktitle> In Proceedings of the Sixth Workshop on Languages and Compilers for Parallel Computing, </booktitle> <address> Portland, OR, </address> <month> August </month> <year> 1993. </year>
Reference-contexts: However, they do not perform fusion or distribution, and our approach is effective in many cases. Instead of using fusion to eliminate barrier synchronization, recent work has focused on replacing barrier synchronization between nests with explicit data placement and finer grain communication <ref> [6, 7, 38, 46] </ref>. The data placement yields locality on the processor and the finer grain communication enables the processors to overlap more computation and communication rather than all waiting at a barrier.
Reference: [7] <author> B. Appelbe, C. Hardnett, and S. Doddapaneni. </author> <title> Program transformation for locality using affinity regions. </title> <booktitle> In Proceedings of the Sixth Workshop on Languages and Compilers for Parallel Computing, </booktitle> <address> Portland, OR, </address> <month> August </month> <year> 1993. </year>
Reference-contexts: However, they do not perform fusion or distribution, and our approach is effective in many cases. Instead of using fusion to eliminate barrier synchronization, recent work has focused on replacing barrier synchronization between nests with explicit data placement and finer grain communication <ref> [6, 7, 38, 46] </ref>. The data placement yields locality on the processor and the finer grain communication enables the processors to overlap more computation and communication rather than all waiting at a barrier.
Reference: [8] <author> V. Balasundaram and K. Kennedy. </author> <title> A technique for summarizing data access and its use in parallelism enhancing transformations. </title> <booktitle> In Proceedings of the SIGPLAN '89 Conference on Programming Language Design and Implementation, </booktitle> <pages> pages 4153, </pages> <address> Portland, OR, </address> <month> June </month> <year> 1989. </year>
Reference-contexts: This analysis is part of dependence testing in ParaScope, and is computed before optimization [22]. We include this description as technical background. We use section analysis to analyze interprocedural side effects to arrays <ref> [8, 20, 21, 22] </ref>. Sections represent the most commonly occurring array access patterns; single elements, rows, columns, grids, and their higher dimensional analogs. The various approaches to interprocedural array side-effect analysis must make tradeoffs between precision and efficiency [8, 11, 22, 30, 45]. <p> We use section analysis to analyze interprocedural side effects to arrays [8, 20, 21, 22]. Sections represent the most commonly occurring array access patterns; single elements, rows, columns, grids, and their higher dimensional analogs. The various approaches to interprocedural array side-effect analysis must make tradeoffs between precision and efficiency <ref> [8, 11, 22, 30, 45] </ref>. Section analysis loses precision because it only represents a selection of array structures and it merges sections for all references to a variable in a procedure into a single section. However, these properties make it efficient. <p> (f,i,j) subroutine Q (f,i,j) real f (n,n) real f (n,n) real f (n,n) integer i,j integer i,j integer i,j do j = 1,100 enddo test for loop interchange and fusion between loops in the caller and the call, we require sections that are slightly more precise than data access descriptors <ref> [8] </ref>. We need to know if the sections are precise. For example, when section analysis merges information for two read accesses it may lose precision. At a section merge, we record whether the new section is still precise or if it becomes imprecise.
Reference: [9] <author> U. Banerjee. </author> <title> A theory of loop permutations. </title> <editor> In D. Gelernter, A. Nicolau, and D. Padua, editors, </editor> <booktitle> Languages and Compilers for Parallel Computing. </booktitle> <publisher> The MIT Press, </publisher> <year> 1990. </year>
Reference-contexts: Given an original set of legal direction vectors, each step of the for is guaranteed to find a loop which results in a legal (lexicographically positive) direction vector, otherwise the original was not legal <ref> [3, 9] </ref>. In addition, if any loop 1 through n1 may be legally positioned prior to n , it will be. NearbyPermutation therefore places the loops carrying the most reuse as innermost as possible.
Reference: [10] <author> W. Blume, R. Eigenmann, K. Faigin, J. Grout, J. Hoeflinger, D. Padua, P. Petersen, W. Pottenger, L. Rauch-werger, P. Tu, and S. Weatherford. </author> <title> Effective automatic parallelization with Polaris. </title> <journal> International Journal of Parallel Programming, </journal> <month> May </month> <year> 1995. </year>
Reference-contexts: We believe that just as automatic vectorization was not successful for dusty deck programs, automatic parallelization of dusty decks is unlikely to yield complete success. Finding medium to large grain parallelism is more difficult than single statement parallelism and compilers have had limited success on dusty deck programs <ref> [10, 19, 42, 43, 17] </ref>. We believe that only a combination of algorithmic parallelization and compiler optimization will enable the effective use of parallel hardware. This paper provides evidence that this approach is viable. <p> The IBM PTRAN project, an industrial research compiler, has published parallelization algorithms that use control and data dependence, and a wide selection of transformations, but without results [1, 40, 41]. Below, we compare this study with those of parallelizing compilers from Illinois and Stanford <ref> [10, 17, 19, 43] </ref>. The Illinois studies are traditional; they evaluate their techniques on dusty deck programs [10, 16, 28, 17]. They extend Kap, an automatic parallelizer, and then use it to parallelize the Perfect Benchmarks. <p> Below, we compare this study with those of parallelizing compilers from Illinois and Stanford [10, 17, 19, 43]. The Illinois studies are traditional; they evaluate their techniques on dusty deck programs <ref> [10, 16, 28, 17] </ref>. They extend Kap, an automatic parallelizer, and then use it to parallelize the Perfect Benchmarks. Their target architecture is Cedar, a shared-memory parallel machine with cluster memory and vector processors. Their work focuses on detecting parallelism via array and scalar analysis, rather than improving locality.
Reference: [11] <author> M. Burke and R. Cytron. </author> <title> Interprocedural dependence analysis and parallelization. </title> <booktitle> In Proceedings of the SIGPLAN '86 Symposium on Compiler Construction, </booktitle> <pages> pages 162175, </pages> <address> Palo Alto, CA, </address> <month> June </month> <year> 1986. </year>
Reference-contexts: We use section analysis to analyze interprocedural side effects to arrays [8, 20, 21, 22]. Sections represent the most commonly occurring array access patterns; single elements, rows, columns, grids, and their higher dimensional analogs. The various approaches to interprocedural array side-effect analysis must make tradeoffs between precision and efficiency <ref> [8, 11, 22, 30, 45] </ref>. Section analysis loses precision because it only represents a selection of array structures and it merges sections for all references to a variable in a procedure into a single section. However, these properties make it efficient.
Reference: [12] <author> K. Cooper, M. W. Hall, R. T. Hood, K. Kennedy, K. S. M c Kinley, J. M. Mellor-Crummey, L. Torczon, and S. K. Warren. </author> <title> The ParaScope parallel programming environment. </title> <booktitle> Proceedings of the IEEE, </booktitle> <volume> 81(2):244 263, </volume> <month> February </month> <year> 1993. </year>
Reference-contexts: The algorithm then optimized this version. We do not recommend that users convert their programs to serial versions before handing it to the compiler, but we use this methodology to assess the ability of the compiler to find, exploit, and optimize known parallelism. We used ParaScope <ref> [12, 27] </ref>, an interactive parallelization tool, to systematically apply the transformations in the algorithm to the sequential programs. ParaScope implements dependence analysis, interprocedural analysis, and safe application of the loop transformations (tiling, interchange, fusion, and distribution), but not the interprocedural optimizations, nor the optimization algorithm itself. <p> We created the sequential version of each program simply by ignoring all the parallel directives. We optimize the sequential version using the advanced analysis and transformations available in our interactive parallel programming tool, the ParaScope Editor (PED) <ref> [12, 27] </ref>, and also use PED to hand-apply our parallelization 18 algorithm. PED is a source-to-source transformation tool which provides dependence and section analysis, loop parallelization, and loop transformations.
Reference: [13] <author> K. Cooper, M. W. Hall, and K. Kennedy. </author> <title> Procedure cloning. </title> <booktitle> In Proceedings of the 1992 IEEE International Conference on Computer Language, </booktitle> <address> Oakland, CA, </address> <month> April </month> <year> 1992. </year>
Reference-contexts: This method separates mechanisms from policy and is consistent with good software engineering practices. 3.5.4 Procedure Cloning Procedure cloning generates multiple copies of a procedure each tailored to its calling environment <ref> [13] </ref>. Even without embedding or extraction, cloning is necessary for interprocedural parallelization because multiple versions of a procedure are required if a procedure is called in two or more settings that require different paralleliz-ing optimizations.
Reference: [14] <author> J. E. Dennis, Jr. and V. Torczon. </author> <title> Direct search methods on parallel machines. </title> <journal> SIAM Journal of Optimization, </journal> <volume> 1(4):448474, </volume> <month> November </month> <year> 1991. </year>
Reference-contexts: Erlebacher ADI Integration 615 Thomas Eidson ICASE 4. Interior Interior Point Method 3555 Guangye Li, Irv Lustig [31] Cray Research, Princeton 5. Control Optimal Control 1878 Stephen Wright Argonne 6. Direct Direct Search Methods 344 Virginia Torczon <ref> [14] </ref> Rice 7. ODE Two-Point Boundary Problems 3614 Stephen Wright [51] Argonne 8. Multi Multidirectional Search Methods 1025 Virginia Torczon [14] Rice 9. Banded Banded Linear Systems 1281 Stephen Wright [50] Argonne 10. Linpackd Linpackd benchmark 772 Jack Dongarra [15] Tennessee attention was paid to their performance. <p> Interior Interior Point Method 3555 Guangye Li, Irv Lustig [31] Cray Research, Princeton 5. Control Optimal Control 1878 Stephen Wright Argonne 6. Direct Direct Search Methods 344 Virginia Torczon <ref> [14] </ref> Rice 7. ODE Two-Point Boundary Problems 3614 Stephen Wright [51] Argonne 8. Multi Multidirectional Search Methods 1025 Virginia Torczon [14] Rice 9. Banded Banded Linear Systems 1281 Stephen Wright [50] Argonne 10. Linpackd Linpackd benchmark 772 Jack Dongarra [15] Tennessee attention was paid to their performance. It is therefore unlikely that large amounts of additional parallelism are available without significant algorithm restructuring.
Reference: [15] <author> J. Dongarra, J. Bunch, C. Moler, and G. Stewart. </author> <title> LINPACK User's Guide. </title> <publisher> SIAM Publications, </publisher> <address> Philadel-phia, PA, </address> <year> 1979. </year>
Reference-contexts: Direct Direct Search Methods 344 Virginia Torczon [14] Rice 7. ODE Two-Point Boundary Problems 3614 Stephen Wright [51] Argonne 8. Multi Multidirectional Search Methods 1025 Virginia Torczon [14] Rice 9. Banded Banded Linear Systems 1281 Stephen Wright [50] Argonne 10. Linpackd Linpackd benchmark 772 Jack Dongarra <ref> [15] </ref> Tennessee attention was paid to their performance. It is therefore unlikely that large amounts of additional parallelism are available without significant algorithm restructuring. The programs are described in more detail elsewhere [33]. The discussion will focus on the first 8 programs.
Reference: [16] <author> R. Eigenmann, J. Hoeflinger, G. Jaxon, Z. Li, and D. Padua. </author> <title> Restructuring Fortran programs for Cedar. </title> <journal> Concurrency: Practice and Experience, </journal> <volume> 5(7):553574, </volume> <month> October </month> <year> 1993. </year>
Reference-contexts: Below, we compare this study with those of parallelizing compilers from Illinois and Stanford [10, 17, 19, 43]. The Illinois studies are traditional; they evaluate their techniques on dusty deck programs <ref> [10, 16, 28, 17] </ref>. They extend Kap, an automatic parallelizer, and then use it to parallelize the Perfect Benchmarks. Their target architecture is Cedar, a shared-memory parallel machine with cluster memory and vector processors. Their work focuses on detecting parallelism via array and scalar analysis, rather than improving locality.
Reference: [17] <author> R. Eigenmann, J. Hoeflinger, and D. Padua. </author> <title> On the automatic parallelization of the Perfect Benchmarks. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 9(1):523, </volume> <month> January </month> <year> 1998. </year> <month> 26 </month>
Reference-contexts: We believe that just as automatic vectorization was not successful for dusty deck programs, automatic parallelization of dusty decks is unlikely to yield complete success. Finding medium to large grain parallelism is more difficult than single statement parallelism and compilers have had limited success on dusty deck programs <ref> [10, 19, 42, 43, 17] </ref>. We believe that only a combination of algorithmic parallelization and compiler optimization will enable the effective use of parallel hardware. This paper provides evidence that this approach is viable. <p> The IBM PTRAN project, an industrial research compiler, has published parallelization algorithms that use control and data dependence, and a wide selection of transformations, but without results [1, 40, 41]. Below, we compare this study with those of parallelizing compilers from Illinois and Stanford <ref> [10, 17, 19, 43] </ref>. The Illinois studies are traditional; they evaluate their techniques on dusty deck programs [10, 16, 28, 17]. They extend Kap, an automatic parallelizer, and then use it to parallelize the Perfect Benchmarks. <p> Below, we compare this study with those of parallelizing compilers from Illinois and Stanford [10, 17, 19, 43]. The Illinois studies are traditional; they evaluate their techniques on dusty deck programs <ref> [10, 16, 28, 17] </ref>. They extend Kap, an automatic parallelizer, and then use it to parallelize the Perfect Benchmarks. Their target architecture is Cedar, a shared-memory parallel machine with cluster memory and vector processors. Their work focuses on detecting parallelism via array and scalar analysis, rather than improving locality. <p> The resulting programs were then further improved manually by `automatable' transformations. It is not clear that even if each individual transformation they propose is automatable, that a practical decision procedure exists that could correctly apply them. The most recent work on Polaris <ref> [17] </ref> demonstrates that they have come closer to finding such a decision procedure, but they still do not specify it. In contrast, our study uses a more clearly defined algorithm. Both studies however would benefit greatly from complete implementations.
Reference: [18] <author> G. Goff, K. Kennedy, and C. Tseng. </author> <title> Practical dependence testing. </title> <booktitle> In Proceedings of the SIGPLAN '91 Conference on Programming Language Design and Implementation, </booktitle> <pages> pages 1529, </pages> <address> Toronto, Canada, </address> <month> June </month> <year> 1991. </year>
Reference-contexts: Data Dependence. We assume the reader is familiar with data dependence <ref> [18, 29] </ref>. Throughout the paper, ~ ffi = fffi 1 : : : ffi k g represents a hybrid direction/distance vector for a data dependence between two array references. Each entry in the vector describes the distance or direction in loop iterations between references to the same location. <p> PED uses a range of dependence tests that start with simple, quick tests and then, if necessary, uses more powerful and expensive tests <ref> [18] </ref>. If a dependence cannot be disproved, PED produces distance and direction vectors. It also performs analysis to determine scalar variables that can be made private in parallel loops.
Reference: [19] <author> M. W. Hall, S. P. Amarasinghe, B. R. Murphy, S. Liao, and M. Lam. </author> <title> Detecting coarse-grain parallelism using an interprocedural parallelizing compiler. </title> <booktitle> In Proceedings of Supercomputing '95, </booktitle> <address> San Diego, CA, </address> <month> December </month> <year> 1995. </year>
Reference-contexts: We believe that just as automatic vectorization was not successful for dusty deck programs, automatic parallelization of dusty decks is unlikely to yield complete success. Finding medium to large grain parallelism is more difficult than single statement parallelism and compilers have had limited success on dusty deck programs <ref> [10, 19, 42, 43, 17] </ref>. We believe that only a combination of algorithmic parallelization and compiler optimization will enable the effective use of parallel hardware. This paper provides evidence that this approach is viable. <p> The IBM PTRAN project, an industrial research compiler, has published parallelization algorithms that use control and data dependence, and a wide selection of transformations, but without results [1, 40, 41]. Below, we compare this study with those of parallelizing compilers from Illinois and Stanford <ref> [10, 17, 19, 43] </ref>. The Illinois studies are traditional; they evaluate their techniques on dusty deck programs [10, 16, 28, 17]. They extend Kap, an automatic parallelizer, and then use it to parallelize the Perfect Benchmarks. <p> Their work includes very few experimental results for the parallelization algorithm, and they do not perform fusion, distribution, or any interprocedural analysis and transformations. More recent work on the SUIF system includes extensive interprocedural analysis and data and control restructuring between nests to further improve locality and parallelism <ref> [5, 19] </ref>. This work demonstrates important improvements over our approach, and some success on dusty deck programs. However, they do not perform fusion or distribution, and our approach is effective in many cases.
Reference: [20] <author> M. W. Hall, K. Kennedy, and K. S. M c Kinley. </author> <title> Interprocedural transformations for parallel code generation. </title> <booktitle> In Proceedings of Supercomputing '91, </booktitle> <pages> pages 424434, </pages> <address> Albuquerque, NM, </address> <month> November </month> <year> 1991. </year>
Reference-contexts: Without loss of generality, we assume Fortran's column-major storage. Augmented Call Graph. We use an augmented call graph G ac to describe the calling relationships among procedures and loop nest structures in the program <ref> [20] </ref>. This flow-insensitive call graph contains procedure nodes and call nodes. For each procedure p that makes a procedure call at site s, an edge connects node p to node s. For each call site s to procedure q, an edge connects node s to node q. <p> This analysis is part of dependence testing in ParaScope, and is computed before optimization [22]. We include this description as technical background. We use section analysis to analyze interprocedural side effects to arrays <ref> [8, 20, 21, 22] </ref>. Sections represent the most commonly occurring array access patterns; single elements, rows, columns, grids, and their higher dimensional analogs. The various approaches to interprocedural array side-effect analysis must make tradeoffs between precision and efficiency [8, 11, 22, 30, 45].
Reference: [21] <author> M. W. Hall, B. R. Murphy, S. P. Amarasinghe, S. Liao, and M. Lam. </author> <title> Data and computation transformations for multiprocessors. </title> <booktitle> In Proceedings of the Eighth Workshop on Languages and Compilers for Parallel Computing, </booktitle> <address> Columbus, OH, </address> <month> August </month> <year> 1995. </year>
Reference-contexts: This analysis is part of dependence testing in ParaScope, and is computed before optimization [22]. We include this description as technical background. We use section analysis to analyze interprocedural side effects to arrays <ref> [8, 20, 21, 22] </ref>. Sections represent the most commonly occurring array access patterns; single elements, rows, columns, grids, and their higher dimensional analogs. The various approaches to interprocedural array side-effect analysis must make tradeoffs between precision and efficiency [8, 11, 22, 30, 45].
Reference: [22] <author> P. Havlak and K. Kennedy. </author> <title> An implementation of interprocedural bounded regular section analysis. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 2(3):350360, </volume> <month> July </month> <year> 1991. </year>
Reference-contexts: This analysis is part of dependence testing in ParaScope, and is computed before optimization <ref> [22] </ref>. We include this description as technical background. We use section analysis to analyze interprocedural side effects to arrays [8, 20, 21, 22]. Sections represent the most commonly occurring array access patterns; single elements, rows, columns, grids, and their higher dimensional analogs. <p> This analysis is part of dependence testing in ParaScope, and is computed before optimization [22]. We include this description as technical background. We use section analysis to analyze interprocedural side effects to arrays <ref> [8, 20, 21, 22] </ref>. Sections represent the most commonly occurring array access patterns; single elements, rows, columns, grids, and their higher dimensional analogs. The various approaches to interprocedural array side-effect analysis must make tradeoffs between precision and efficiency [8, 11, 22, 30, 45]. <p> We use section analysis to analyze interprocedural side effects to arrays [8, 20, 21, 22]. Sections represent the most commonly occurring array access patterns; single elements, rows, columns, grids, and their higher dimensional analogs. The various approaches to interprocedural array side-effect analysis must make tradeoffs between precision and efficiency <ref> [8, 11, 22, 30, 45] </ref>. Section analysis loses precision because it only represents a selection of array structures and it merges sections for all references to a variable in a procedure into a single section. However, these properties make it efficient. <p> Section analysis loses precision because it only represents a selection of array structures and it merges sections for all references to a variable in a procedure into a single section. However, these properties make it efficient. It often works as well as more precise techniques <ref> [22, 30] </ref>. Sections reduce the dependence problem on loops containing procedure calls to the problem on ordinary statements. For example, Figure 10 illustrates the two sections for array a that result from each of the two calls to Q. <p> The sections are in terms of constants and parameters passed at the call. The superscript indicates a Read and/or Write access and their relative order. The sections contain all the information necessary to perform dependence testing in the calling procedure without further inspecting the called procedure <ref> [22] </ref>. <p> It also performs analysis to determine scalar variables that can be made private in parallel loops. To improve the precision of dependence testing, it uses advanced symbolic dependence tests, interprocedural constants, interprocedural symbolics, and interprocedural MOD and REF array sections <ref> [22] </ref>. All of this testing is implemented. Transformations. PED's selection of source-to-source transformations includes loop parallelization with private variable declarations, loop interchange, fusion, distribution, and tiling. It does not include loop embedding, extraction, or procedure cloning.
Reference: [23] <author> T. E. Jeremiassen and S. J. Eggers. </author> <title> Reducing false sharing on shared memory multiprocessors through compile time data transformations. </title> <booktitle> In Proceedings of the Fifth ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming, </booktitle> <pages> pages 179188, </pages> <address> Santa Barbara, CA, </address> <month> July </month> <year> 1995. </year>
Reference-contexts: For example, consider two adjacent loops that access the same array. If the working set of the first loop exceeds the cache, fusion yields reuse. However when fusion is not legal, these techniques can be used instead to improve performance. Jeremiassen and Eggers <ref> [23] </ref> improve locality in explicitly parallel programs by restructuring the data layout. We instead focus on restructuring the program control flow. We also transform the program into a sequential equivalent and therefore do not analyze parallel programs. Data transformations [23] and combining data transformations with control transformations [5] can successfully parallelize <p> Jeremiassen and Eggers <ref> [23] </ref> improve locality in explicitly parallel programs by restructuring the data layout. We instead focus on restructuring the program control flow. We also transform the program into a sequential equivalent and therefore do not analyze parallel programs. Data transformations [23] and combining data transformations with control transformations [5] can successfully parallelize programs that the techniques presented in this paper can not. 8 Conclusions This paper presents a new parallelization algorithm that balances parallelism and data locality.
Reference: [24] <author> K. Kennedy and K. S. M c Kinley. </author> <title> Optimizing for parallelism and data locality. </title> <booktitle> In Proceedings of the 1992 ACM International Conference on Supercomputing, </booktitle> <pages> pages 323334, </pages> <address> Washington, DC, </address> <month> July </month> <year> 1992. </year>
Reference-contexts: This effect is called false sharing, since each processor is actually only using and updating independent elements in each cache line. This effect can dramatically degrade performance <ref> [24] </ref>. 2. Even if the runtime system assigns adjacent elements of y (i) to the same processor, each pair of processors may share cache lines. <p> Also, because the reuses of y (i) are further apart in time than with an inner i loop, it is more likely that m (i; j) or x (j) will map to the same cache line causing additional cache misses. This effect also degrades performance <ref> [24] </ref>. To achieve locality and parallelism, we thus strip-mine and interchange the i loop as shown in Figure 1 (c). The outer ii loop is parallel, and the inner i loop still attains spatial and temporal locality given a large enough tile. <p> This transformation eliminates barrier synchronization and can also improve locality. The next section describes each of these steps in detail. 3.2 Optimize: Data Locality and Parallelism The most effective and essential component of our parallelization algorithm uses a simple memory model to drive optimizations for data locality and parallelism <ref> [34, 24] </ref>. We employ loop permutation to improve data locality and tiling to introduce parallelism. Using a memory model and loop transformations, our algorithm places the loops with the most reuse innermost and parallel loops outermost, where each is most effective. <p> Because this loop structure maximizes data locality, it reduces communication of data between iterations and therefore between processors. In experiments on the Sequent, this version of dmxpy results in speed-ups of up to 16.4 on 19 processors. This algorithm also attains linear speed-ups for kernels such as matrix multiply <ref> [24] </ref>. Previous work also experiments with different versions of dmxpy, and shows the version in Figure 1 (c) is the best [24]. <p> This algorithm also attains linear speed-ups for kernels such as matrix multiply <ref> [24] </ref>. Previous work also experiments with different versions of dmxpy, and shows the version in Figure 1 (c) is the best [24]. For example, parallelizing i in the outermost position and assigning adjacent i iterations to different processors instead causes the cache line for y (i) to be shared among multiple processors. <p> Previous work demonstrates that these costs degrade performance significantly <ref> [24] </ref>. Constants and Tile Sizes. In the experiments reported in Section 4, the optimizer gives the parallel iterator one iteration for each processor, producing strips as large as possible. If the parallel loop carries spatial reuse and has enough iterations, reuse is attained and this strategy works well [24]. <p> performance significantly <ref> [24] </ref>. Constants and Tile Sizes. In the experiments reported in Section 4, the optimizer gives the parallel iterator one iteration for each processor, producing strips as large as possible. If the parallel loop carries spatial reuse and has enough iterations, reuse is attained and this strategy works well [24]. <p> loop has spatial locality and fewer iterations than the number of processors times the number of items on a cache line, then previous work shows that to achieve the best performance, we should actually reduce the number of parallel iterations such that the cache lines are not shared between processors <ref> [24] </ref>. A runtime test could differentiate these cases. In this work for unknown loop bounds of parallel loops, we assume the number of iterations is greater than the number processors times the cache line size. 4 3.2.6 Summary and Discussion To review, the complete Optimize algorithm appears in Figure 3. <p> The parallelization step of the algorithm is linear. We have previously shown the data locality algorithm effective for uniprocessors [34]. We have also demonstrated that the parallelization algorithm effective for kernels <ref> [24] </ref>, and in Section 4 we show that this algorithm is effective for application programs on shared-memory multiprocessors. 3.3 Fuser: Improving the Granularity of Parallelism This section describes an approach for incorporating fusion and distribution into the Optimize algorithm. Loop fusion and distribution have several purposes in our algorithm.
Reference: [25] <author> K. Kennedy and K. S. M c Kinley. </author> <title> Maximizing loop parallelism and improving data locality via loop fusion and distribution. </title> <editor> In U. Banerjee, D. Gelernter, A. Nicolau, and D. Padua, editors, </editor> <booktitle> Languages and Compilers for Parallel Computing, </booktitle> <pages> pages 301321, </pages> <address> Portland, OR, </address> <month> August </month> <year> 1993. </year>
Reference-contexts: After distribution and parallelization, there may be a sequence of parallel and sequential nests, some of which may be fused back together. We showed that the problem of fusing a set of loops is the same, regardless if they resulted from distribution or were written that way <ref> [25] </ref>. Fusion is desirable between parallel loops because it may reduce communication of shared data and it reduces the number of barrier synchronization. Barrier synchronization is often costly on multiprocessors. 3.3.2 Loop Fusion Loop fusion merges multiple loops with conformable headers into a single loop. <p> Fusion algorithm. When there is a group of adjacent loop nests with conformable headers which are differentiated only by their parallel and sequential status and n candidate nests, we have an O (n 2 ) time and space algorithm that minimizes the number of parallel loops <ref> [25] </ref>. (More general fusion problems are NP-hard [25, 26].) 12 (a) original program do k = 1, kmaxd do j = 1, jmaxd (1) f (1,j,k) = f (1,j,k) fl b (1) do k = 1, kmaxd do j = 1, jmaxd do i = 2, n-1 do k = 1, <p> there is a group of adjacent loop nests with conformable headers which are differentiated only by their parallel and sequential status and n candidate nests, we have an O (n 2 ) time and space algorithm that minimizes the number of parallel loops [25]. (More general fusion problems are NP-hard <ref> [25, 26] </ref>.) 12 (a) original program do k = 1, kmaxd do j = 1, jmaxd (1) f (1,j,k) = f (1,j,k) fl b (1) do k = 1, kmaxd do j = 1, jmaxd do i = 2, n-1 do k = 1, kmaxd do j = 1, jmaxd (3) <p> It then greedily merges sequential nodes. This merge respects the original constraints, any constraints introduced by the fusion of parallel nests, and insures that a DAG results <ref> [25] </ref>. The original order of the nests may change as long as no dependence constraints are violated. Example. Figure 8 (c) illustrates the DAG for the code in Figure 8 (a). <p> The resulting code appears in Figure 8 (b). Notice that statement reordering would enable an additional fusion of the inner loops and thus further improve locality. This step is beyond the scope of this paper <ref> [34, 25] </ref>. 3.4 Intra and Inter-Nest Parallelization We combine Optimize and Fuser in Figure 9 to optimize loop nests within a single procedure. We call this algorithm Parallelize. It combines fusion and distribution with Optimize to introduce effective parallelism and to improve the granularity of parallelism achieved.
Reference: [26] <author> K. Kennedy and K. S. M c Kinley. </author> <title> Typed fusion with applications to parallel and sequential code generation. </title> <type> Technical Report TR93-208, </type> <institution> Dept. of Computer Science, Rice University, </institution> <month> August </month> <year> 1993. </year>
Reference-contexts: there is a group of adjacent loop nests with conformable headers which are differentiated only by their parallel and sequential status and n candidate nests, we have an O (n 2 ) time and space algorithm that minimizes the number of parallel loops [25]. (More general fusion problems are NP-hard <ref> [25, 26] </ref>.) 12 (a) original program do k = 1, kmaxd do j = 1, jmaxd (1) f (1,j,k) = f (1,j,k) fl b (1) do k = 1, kmaxd do j = 1, jmaxd do i = 2, n-1 do k = 1, kmaxd do j = 1, jmaxd (3)
Reference: [27] <author> K. Kennedy, K. S. M c Kinley, and C. Tseng. </author> <title> Analysis and transformation in an interactive parallel programming tool. </title> <journal> Concurrency: Practice and Experience, </journal> <volume> 5(7):575602, </volume> <month> October </month> <year> 1993. </year>
Reference-contexts: The algorithm then optimized this version. We do not recommend that users convert their programs to serial versions before handing it to the compiler, but we use this methodology to assess the ability of the compiler to find, exploit, and optimize known parallelism. We used ParaScope <ref> [12, 27] </ref>, an interactive parallelization tool, to systematically apply the transformations in the algorithm to the sequential programs. ParaScope implements dependence analysis, interprocedural analysis, and safe application of the loop transformations (tiling, interchange, fusion, and distribution), but not the interprocedural optimizations, nor the optimization algorithm itself. <p> We created the sequential version of each program simply by ignoring all the parallel directives. We optimize the sequential version using the advanced analysis and transformations available in our interactive parallel programming tool, the ParaScope Editor (PED) <ref> [12, 27] </ref>, and also use PED to hand-apply our parallelization 18 algorithm. PED is a source-to-source transformation tool which provides dependence and section analysis, loop parallelization, and loop transformations.
Reference: [28] <author> D. Kuck, E. Davidson, D. Lawrie, A. Sameh, C.-Q. Zhu, A. Veidenbaum, J. Konicek, P. Yew, K. Gallivan, W. Jalby, H. Wijshoff, R. Bramley, U.M. Yang, P. Emrath, D. Padua, R. Eigenmann, J. Hoeflinger, G. Jaxon, Z. Li, T. Murphy, J. Andrews, and S. Turner. </author> <title> The Cedar system and an initial performance study. </title> <booktitle> In Proceedings of the 20th International Symposium on Computer Architecture, </booktitle> <address> San Diego, CA, </address> <month> May </month> <year> 1993. </year>
Reference-contexts: Below, we compare this study with those of parallelizing compilers from Illinois and Stanford [10, 17, 19, 43]. The Illinois studies are traditional; they evaluate their techniques on dusty deck programs <ref> [10, 16, 28, 17] </ref>. They extend Kap, an automatic parallelizer, and then use it to parallelize the Perfect Benchmarks. Their target architecture is Cedar, a shared-memory parallel machine with cluster memory and vector processors. Their work focuses on detecting parallelism via array and scalar analysis, rather than improving locality.
Reference: [29] <author> D. Kuck, R. Kuhn, D. Padua, B. Leasure, and M. J. Wolfe. </author> <title> Dependence graphs and compiler optimizations. </title> <booktitle> In Conference Record of the Eighth Annual ACM Symposium on the Principles of Programming Languages, </booktitle> <address> Williamsburg, VA, </address> <month> January </month> <year> 1981. </year>
Reference-contexts: Data Dependence. We assume the reader is familiar with data dependence <ref> [18, 29] </ref>. Throughout the paper, ~ ffi = fffi 1 : : : ffi k g represents a hybrid direction/distance vector for a data dependence between two array references. Each entry in the vector describes the distance or direction in loop iterations between references to the same location.
Reference: [30] <author> Z. Li and P. Yew. </author> <title> Efficient interprocedural analysis for program restructuring for parallel programs. </title> <booktitle> In Proceedings of the ACM SIGPLAN Symposium on Parallel Programming: Experience with Applications, Languages, and Systems (PPEALS), </booktitle> <address> New Haven, CT, </address> <month> July </month> <year> 1988. </year>
Reference-contexts: We use section analysis to analyze interprocedural side effects to arrays [8, 20, 21, 22]. Sections represent the most commonly occurring array access patterns; single elements, rows, columns, grids, and their higher dimensional analogs. The various approaches to interprocedural array side-effect analysis must make tradeoffs between precision and efficiency <ref> [8, 11, 22, 30, 45] </ref>. Section analysis loses precision because it only represents a selection of array structures and it merges sections for all references to a variable in a procedure into a single section. However, these properties make it efficient. <p> Section analysis loses precision because it only represents a selection of array structures and it merges sections for all references to a variable in a procedure into a single section. However, these properties make it efficient. It often works as well as more precise techniques <ref> [22, 30] </ref>. Sections reduce the dependence problem on loops containing procedure calls to the problem on ordinary statements. For example, Figure 10 illustrates the two sections for array a that result from each of the two calls to Q.
Reference: [31] <author> I. J. Lustig and G. Li. </author> <title> An implementation of a parallel primal-dual interior point method for multicom-mondity flow problems. </title> <type> Technical Report CRPC-TR92194, </type> <institution> Center for Research on Parallel Computation, Rice University, </institution> <month> January </month> <year> 1992. </year>
Reference-contexts: Seismic 1-D Seismic Inversion 606 Michael Lewis Rice 2. BTN BTN Unconstrained Optimization 1506 Stephen Nash, Ariela Sofer [36, 37] George Mason 3. Erlebacher ADI Integration 615 Thomas Eidson ICASE 4. Interior Interior Point Method 3555 Guangye Li, Irv Lustig <ref> [31] </ref> Cray Research, Princeton 5. Control Optimal Control 1878 Stephen Wright Argonne 6. Direct Direct Search Methods 344 Virginia Torczon [14] Rice 7. ODE Two-Point Boundary Problems 3614 Stephen Wright [51] Argonne 8. Multi Multidirectional Search Methods 1025 Virginia Torczon [14] Rice 9.
Reference: [32] <author> K. S. McKinley. </author> <title> Dependence analysis of arrays subscripted by index arrays. </title> <type> Technical Report TR91-162, </type> <institution> Dept. of Computer Science, Rice University, </institution> <month> December </month> <year> 1990. </year>
Reference-contexts: Several of these are monotonic non-decreasing with a regular, well defined pattern. In Interior, Control, and Direct, paralleliza-tion would not have been possible without using user assertions and the testing techniques developed in our earlier research <ref> [32] </ref>. The other two programs used them in a way that did not affect parallelization. When the user asserts that an index variable used in a subscript is a monotonic non-decreasing permutation array, dependence testing can then eliminate dependences and detect parallel loops.
Reference: [33] <author> K. S. M c Kinley. </author> <title> Automatic and Interactive Parallelization. </title> <type> PhD thesis, </type> <institution> Dept. of Computer Science, Rice University, </institution> <month> April </month> <year> 1992. </year>
Reference-contexts: If new nests are created as in Figure 7, these become candidates for parallelization by Optimize. This algorithm is not optimal because combining distribution with loop permutation may uncover deeper distributions that in turn may be more effectively parallelized <ref> [4, 33] </ref>. This flexibility was not required in our experiments and is not explored further here. After distribution and parallelization, there may be a sequence of parallel and sequential nests, some of which may be fused back together. <p> Our strategy separates legality and profitability tests from the mechanics of the transformations <ref> [33] </ref>. The safety tests depend on the precision of the dependence information and section analysis. For permutation, the dependences must be precise enough in the caller to determine if they would be reversed after permutation. Since fusion requires additional dependence testing, the sections must be precise. <p> We did not implement our algorithm in PED. We instead performed the transformations as specified by the algorithm in PED, and applied them only when PED assured their correctness. We kept optimization diaries for each program <ref> [33] </ref>. 5.3 Execution Environment We ran and compared all three versions on a Sequent Symmetry S81 with 20 processors. We validated each program using its output. For each of the programs, all the versions produced the same correct output. <p> Banded Banded Linear Systems 1281 Stephen Wright [50] Argonne 10. Linpackd Linpackd benchmark 772 Jack Dongarra [15] Tennessee attention was paid to their performance. It is therefore unlikely that large amounts of additional parallelism are available without significant algorithm restructuring. The programs are described in more detail elsewhere <ref> [33] </ref>. The discussion will focus on the first 8 programs. We included Linpackd since it is well known and it contains parallelism, but we did not use a hand-parallelized version. The ninth code, Banded, did not execute correctly on the Sequent.
Reference: [34] <author> K. S. M c Kinley, S. Carr, and C. Tseng. </author> <title> Improving data locality with loop transformations. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 18(4):424453, </volume> <month> July </month> <year> 1996. </year> <month> 27 </month>
Reference-contexts: This transformation eliminates barrier synchronization and can also improve locality. The next section describes each of these steps in detail. 3.2 Optimize: Data Locality and Parallelism The most effective and essential component of our parallelization algorithm uses a simple memory model to drive optimizations for data locality and parallelism <ref> [34, 24] </ref>. We employ loop permutation to improve data locality and tiling to introduce parallelism. Using a memory model and loop transformations, our algorithm places the loops with the most reuse innermost and parallel loops outermost, where each is most effective. <p> The tiling step strip-mines and if necessary, permutes the nest to position an outermost parallel loop. 1 McKinley and Temam support this assumption [35], and McKinley et al. demonstrate that this memory model works well for uniprocessor caches <ref> [34] </ref>. 6 INPUT: A loop nest L = fl 1 ; : : : ; l k g OUTPUT: An optimized loop nest P ALGORITHM: procedure Optimize (L) compute RefGroup for all references in L MO = MemoryOrder (L) P = NearbyPermutation (L; MO) for j = 1; m f outermost <p> We call this permutation of the nest with the least cost memory order. Although contrived counterexamples exist to the above observation for 3 or more levels of loop nesting, previous work demonstrates that in practice, the model is extremely accurate and always gets the inner loop right <ref> [34] </ref>. We assume that each of the loop bounds is greater than 1, which is the only interesting case. If the constants are comparable with the number of cache items on a line, this model loses accuracy. <p> To determine if the order is a legal one for a perfect nest, 2 we permute the corresponding entries in the distance/direction vector. If the result is lexicographically positive (the majority of the time it is <ref> [34] </ref>), the permutation is legal and we transform the nest. If not, we use the algorithm NearbyPermutation in Figure 6. <p> Next we find memory order, and apply NearbyPermutation to achieve memory order when possible. The final step parallelizes the outermost parallel loop, and if necessary permutes it to the outermost position. In our experiments, memory order is usually a legal permutation of the nest <ref> [34] </ref>. The complexity of the entire algorithm in this case is dominated by the time to sort the loops in the nest and the corresponding dependence vectors. <p> In the worst case, when the desired outermost loop must be innermost, NearbyPermutation's complexity dominates, O (n 2 ) time. The parallelization step of the algorithm is linear. We have previously shown the data locality algorithm effective for uniprocessors <ref> [34] </ref>. <p> The resulting code appears in Figure 8 (b). Notice that statement reordering would enable an additional fusion of the inner loops and thus further improve locality. This step is beyond the scope of this paper <ref> [34, 25] </ref>. 3.4 Intra and Inter-Nest Parallelization We combine Optimize and Fuser in Figure 9 to optimize loop nests within a single procedure. We call this algorithm Parallelize. It combines fusion and distribution with Optimize to introduce effective parallelism and to improve the granularity of parallelism achieved.
Reference: [35] <author> K. S. M c Kinley and O. Temam. </author> <title> A quantitative analysis of loop nest locality. </title> <booktitle> In Proceedings of the Seventh International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pages 94104, </pages> <address> Boston, MA, </address> <month> October </month> <year> 1996. </year>
Reference-contexts: It introduces outer loop parallelism by tiling the nest to to maintain locality on individual processors as the computation is divided among multiple processors. The tiling step strip-mines and if necessary, permutes the nest to position an outermost parallel loop. 1 McKinley and Temam support this assumption <ref> [35] </ref>, and McKinley et al. demonstrate that this memory model works well for uniprocessor caches [34]. 6 INPUT: A loop nest L = fl 1 ; : : : ; l k g OUTPUT: An optimized loop nest P ALGORITHM: procedure Optimize (L) compute RefGroup for all references in L MO
Reference: [36] <author> S. G. Nash and A. Sofer. </author> <title> A general-purpose parallel algorithm for unconstrained optimization. </title> <journal> SIAM Journal of Optimization, </journal> <volume> 1(4):530547, </volume> <month> November </month> <year> 1991. </year>
Reference-contexts: Seismic 1-D Seismic Inversion 606 Michael Lewis Rice 2. BTN BTN Unconstrained Optimization 1506 Stephen Nash, Ariela Sofer <ref> [36, 37] </ref> George Mason 3. Erlebacher ADI Integration 615 Thomas Eidson ICASE 4. Interior Interior Point Method 3555 Guangye Li, Irv Lustig [31] Cray Research, Princeton 5. Control Optimal Control 1878 Stephen Wright Argonne 6. Direct Direct Search Methods 344 Virginia Torczon [14] Rice 7.
Reference: [37] <author> S. G. Nash and A. Sofer. </author> <title> BTN: software for parallel unconstrained optimization. </title> <journal> ACM TOMS, </journal> <year> 1992. </year>
Reference-contexts: Seismic 1-D Seismic Inversion 606 Michael Lewis Rice 2. BTN BTN Unconstrained Optimization 1506 Stephen Nash, Ariela Sofer <ref> [36, 37] </ref> George Mason 3. Erlebacher ADI Integration 615 Thomas Eidson ICASE 4. Interior Interior Point Method 3555 Guangye Li, Irv Lustig [31] Cray Research, Princeton 5. Control Optimal Control 1878 Stephen Wright Argonne 6. Direct Direct Search Methods 344 Virginia Torczon [14] Rice 7.
Reference: [38] <author> M. O'Boyle and F. Bodin. </author> <title> Compiler reduction of synchronization in shared memory virtual memory systems. </title> <booktitle> In Proceedings of the 1995 ACM International Conference on Supercomputing, </booktitle> <pages> pages 318327, </pages> <address> Barcelona, Spain, </address> <month> July </month> <year> 1995. </year>
Reference-contexts: However, they do not perform fusion or distribution, and our approach is effective in many cases. Instead of using fusion to eliminate barrier synchronization, recent work has focused on replacing barrier synchronization between nests with explicit data placement and finer grain communication <ref> [6, 7, 38, 46] </ref>. The data placement yields locality on the processor and the finer grain communication enables the processors to overlap more computation and communication rather than all waiting at a barrier.
Reference: [39] <author> A. Osterhaug, </author> <title> editor. Guide to Parallel Programming on Sequent Computer Systems. </title> <publisher> Sequent Technical Publications, </publisher> <address> San Diego, CA, </address> <year> 1989. </year>
Reference-contexts: Each processor has its own 64Kbyte two-way set-associative cache with a cache line size of 4 words. Each processor and one shared, main memory is connected to the bus. The Sequent compiler introduces parallelism based on parallel loop and fork directives <ref> [39] </ref>. We used the parallel loop directives with private variable declarations to introduce parallelism. We compiled with version 2.1 of Sequent's Fortran ATS compiler using the compiler options that specify multiprocessing, the Weitek 1167 floating-point accelerator, and optimization at its highest level (O3).
Reference: [40] <author> V. Sarkar. </author> <title> Automatic partitioning of a program dependence graph into parallel tasks. </title> <journal> IBM Journal of Research and Development, </journal> <volume> 35(6):779804, </volume> <month> November </month> <year> 1991. </year>
Reference-contexts: Many commercial parallelizing compilers do not reveal their optimization strategies to maintain a market advantage. The IBM PTRAN project, an industrial research compiler, has published parallelization algorithms that use control and data dependence, and a wide selection of transformations, but without results <ref> [1, 40, 41] </ref>. Below, we compare this study with those of parallelizing compilers from Illinois and Stanford [10, 17, 19, 43]. The Illinois studies are traditional; they evaluate their techniques on dusty deck programs [10, 16, 28, 17].
Reference: [41] <author> V. Sarkar and R. Thekkath. </author> <title> A general framework for iteration-reordering loop transformations (technical summary). </title> <booktitle> In Proceedings of the SIGPLAN '92 Conference on Programming Language Design and Implementation, </booktitle> <pages> pages 175187, </pages> <address> San Francisco, CA, </address> <month> June </month> <year> 1992. </year>
Reference-contexts: Many commercial parallelizing compilers do not reveal their optimization strategies to maintain a market advantage. The IBM PTRAN project, an industrial research compiler, has published parallelization algorithms that use control and data dependence, and a wide selection of transformations, but without results <ref> [1, 40, 41] </ref>. Below, we compare this study with those of parallelizing compilers from Illinois and Stanford [10, 17, 19, 43]. The Illinois studies are traditional; they evaluate their techniques on dusty deck programs [10, 16, 28, 17].
Reference: [42] <author> J. Singh and J. Hennessy. </author> <title> An empirical investigation of the effectiveness of and limitations of automatic parallelization. </title> <booktitle> In Proceedings of the International Symposium on Shared Memory Multiprocessors, </booktitle> <address> Tokyo, Japan, </address> <month> April </month> <year> 1991. </year>
Reference-contexts: We believe that just as automatic vectorization was not successful for dusty deck programs, automatic parallelization of dusty decks is unlikely to yield complete success. Finding medium to large grain parallelism is more difficult than single statement parallelism and compilers have had limited success on dusty deck programs <ref> [10, 19, 42, 43, 17] </ref>. We believe that only a combination of algorithmic parallelization and compiler optimization will enable the effective use of parallel hardware. This paper provides evidence that this approach is viable. <p> In contrast, our study uses a more clearly defined algorithm. Both studies however would benefit greatly from complete implementations. Singh & Hennessy used the Alliant FX/8, the Encore Multimax, and their Fortran compilers <ref> [42, 43] </ref>. The compiler algorithms are again unpublished. The FX/8 has cluster memory instead of local caches, which means all data accesses are slow. Since caches are not available to improve performance, the parallelization algorithm is simplified.
Reference: [43] <author> J. Singh and J. Hennessy. </author> <title> Finding and exploiting parallelism in an ocean simulation program: Experiences, results, and implications. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 15(1):2748, </volume> <month> May </month> <year> 1992. </year>
Reference-contexts: We believe that just as automatic vectorization was not successful for dusty deck programs, automatic parallelization of dusty decks is unlikely to yield complete success. Finding medium to large grain parallelism is more difficult than single statement parallelism and compilers have had limited success on dusty deck programs <ref> [10, 19, 42, 43, 17] </ref>. We believe that only a combination of algorithmic parallelization and compiler optimization will enable the effective use of parallel hardware. This paper provides evidence that this approach is viable. <p> The IBM PTRAN project, an industrial research compiler, has published parallelization algorithms that use control and data dependence, and a wide selection of transformations, but without results [1, 40, 41]. Below, we compare this study with those of parallelizing compilers from Illinois and Stanford <ref> [10, 17, 19, 43] </ref>. The Illinois studies are traditional; they evaluate their techniques on dusty deck programs [10, 16, 28, 17]. They extend Kap, an automatic parallelizer, and then use it to parallelize the Perfect Benchmarks. <p> In contrast, our study uses a more clearly defined algorithm. Both studies however would benefit greatly from complete implementations. Singh & Hennessy used the Alliant FX/8, the Encore Multimax, and their Fortran compilers <ref> [42, 43] </ref>. The compiler algorithms are again unpublished. The FX/8 has cluster memory instead of local caches, which means all data accesses are slow. Since caches are not available to improve performance, the parallelization algorithm is simplified.
Reference: [44] <author> J. Subhlok. </author> <title> Analysis of Synchronization in a Parallel Programming Environment. </title> <type> PhD thesis, </type> <institution> Dept. of Computer Science, Rice University, </institution> <month> August </month> <year> 1990. </year>
Reference-contexts: Each of BTN and Multi contain parallel loops with critical sections that update shared variables. Analysis techniques exist that can properly identify the parallelism <ref> [44] </ref>, but since it was not part of our algorithm, we did not use them. In BTN, the benefit of parallelism was actually overwhelmed by the overhead of the critical section, resulting in better performance when the loop executed sequentially.
Reference: [45] <author> R. Triolet, F. Irigoin, and P. Feautrier. </author> <title> Direct parallelization of CALL statements. </title> <booktitle> In Proceedings of the SIGPLAN '86 Symposium on Compiler Construction, </booktitle> <pages> pages 176185, </pages> <address> Palo Alto, CA, </address> <month> June </month> <year> 1986. </year>
Reference-contexts: We use section analysis to analyze interprocedural side effects to arrays [8, 20, 21, 22]. Sections represent the most commonly occurring array access patterns; single elements, rows, columns, grids, and their higher dimensional analogs. The various approaches to interprocedural array side-effect analysis must make tradeoffs between precision and efficiency <ref> [8, 11, 22, 30, 45] </ref>. Section analysis loses precision because it only represents a selection of array structures and it merges sections for all references to a variable in a procedure into a single section. However, these properties make it efficient.
Reference: [46] <author> C. Tseng. </author> <title> Compiler optimizations for eliminating barrier synchronization. </title> <booktitle> In Proceedings of the Fifth ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming, </booktitle> <pages> pages 144155, </pages> <address> Santa Barbara, CA, </address> <month> July </month> <year> 1995. </year>
Reference-contexts: However, they do not perform fusion or distribution, and our approach is effective in many cases. Instead of using fusion to eliminate barrier synchronization, recent work has focused on replacing barrier synchronization between nests with explicit data placement and finer grain communication <ref> [6, 7, 38, 46] </ref>. The data placement yields locality on the processor and the finer grain communication enables the processors to overlap more computation and communication rather than all waiting at a barrier.
Reference: [47] <author> M. E. Wolf. </author> <title> Improving Locality and Parallelism in Nested Loops. </title> <type> PhD thesis, </type> <institution> Dept. of Computer Science, Stanford University, </institution> <month> August </month> <year> 1992. </year>
Reference-contexts: We start with this premise. However, the ability of our techniques to find further improvements reveals that even after users perform algorithm restructuring for parallelism, there is performance to be gained. Our core technique, Optimize, bears the most similarity to Wolf & Lam's research <ref> [47, 48] </ref>. Their algorithm is potentially more precise and uses skewing and reversal. Our algorithm can take advantage of known loop bounds to more precisely compute locality and granularity of parallelism, and is more efficient.
Reference: [48] <author> M. E. Wolf and M. Lam. </author> <title> A loop transformation theory and an algorithm to maximize parallelism. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 2(4):452471, </volume> <month> October </month> <year> 1991. </year>
Reference-contexts: We start with this premise. However, the ability of our techniques to find further improvements reveals that even after users perform algorithm restructuring for parallelism, there is performance to be gained. Our core technique, Optimize, bears the most similarity to Wolf & Lam's research <ref> [47, 48] </ref>. Their algorithm is potentially more precise and uses skewing and reversal. Our algorithm can take advantage of known loop bounds to more precisely compute locality and granularity of parallelism, and is more efficient.
Reference: [49] <author> M. J. Wolfe. </author> <title> Advanced loop interchanging. </title> <booktitle> In Proceedings of the 1986 International Conference on Parallel Processing, </booktitle> <pages> pages 536543, </pages> <address> St. Charles, IL, </address> <month> August </month> <year> 1986. </year>
Reference-contexts: The following theorem holds for the NearbyPermutation algorithm. Theorem: If there exists a legal permutation where n is the innermost loop, then NearbyPermuta tion will find a permutation where n is innermost. 2 Determining memory order does not depend on a perfect nest. Methods exist for permuting imperfect nests <ref> [49] </ref>, but we only permute perfect nests or nests that fusion or distribution make perfect (see Section 3.3.1). 9 INPUT: O = fi 1 ; i 2 ; :::; i n g, the original loop ordering L = fi 1 ; i 2 ; : : : ; i n g
Reference: [50] <author> S. J. Wright. </author> <title> Parallel algorithms for banded linear systems. </title> <journal> SIAM Journal of Scientific and Statistical Computation, </journal> <volume> 12(4):824842, </volume> <month> July </month> <year> 1991. </year>
Reference-contexts: Control Optimal Control 1878 Stephen Wright Argonne 6. Direct Direct Search Methods 344 Virginia Torczon [14] Rice 7. ODE Two-Point Boundary Problems 3614 Stephen Wright [51] Argonne 8. Multi Multidirectional Search Methods 1025 Virginia Torczon [14] Rice 9. Banded Banded Linear Systems 1281 Stephen Wright <ref> [50] </ref> Argonne 10. Linpackd Linpackd benchmark 772 Jack Dongarra [15] Tennessee attention was paid to their performance. It is therefore unlikely that large amounts of additional parallelism are available without significant algorithm restructuring. The programs are described in more detail elsewhere [33].
Reference: [51] <author> S. J. Wright. </author> <title> Stable parallel algorithms for two-point boundary value problems. </title> <journal> SIAM Journal of Scientific and Statistical Computation, </journal> <year> 1992. </year> <month> 28 </month>
Reference-contexts: Erlebacher ADI Integration 615 Thomas Eidson ICASE 4. Interior Interior Point Method 3555 Guangye Li, Irv Lustig [31] Cray Research, Princeton 5. Control Optimal Control 1878 Stephen Wright Argonne 6. Direct Direct Search Methods 344 Virginia Torczon [14] Rice 7. ODE Two-Point Boundary Problems 3614 Stephen Wright <ref> [51] </ref> Argonne 8. Multi Multidirectional Search Methods 1025 Virginia Torczon [14] Rice 9. Banded Banded Linear Systems 1281 Stephen Wright [50] Argonne 10. Linpackd Linpackd benchmark 772 Jack Dongarra [15] Tennessee attention was paid to their performance.
References-found: 51

