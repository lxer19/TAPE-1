URL: http://http.cs.berkeley.edu/~rywang/papers/sosp97.ps
Refering-URL: http://http.cs.berkeley.edu/~rywang/papers/sosp97.html
Root-URL: 
Title: Abstract  
Abstract: File system designers today face a dilemma. A log-structured file system (LFS) can offer superior performance for many common workloads such as those with frequent small writes, read traffic that is predominantly absorbed by the cache, and sufficient idle time to clean the log. However, an LFS has poor performance for other workloads, such as random updates to a full disk with little idle time to clean. In this paper, we show how adaptive algorithms can be used to enable LFS to provide high performance across a wider range of workloads. First, we show how to improve LFS write performance in three ways: by choosing the segment size to match disk and workload characteristics, by modifying the LFS cleaning policy to adapt to changes in disk utilization, and by using cached data to lower cleaning costs. Second, we show how to improve LFS read performance by reorganizing data to match read patterns. Using trace-driven simulations on a combination of synthetic and measured workloads, we demonstrate that these extensions to LFS can significantly improve its performance. 
Abstract-found: 1
Intro-found: 1
Reference: [Aky95] <author> S. Akyrek and K. Salem. </author> <title> Adaptive Block Rearrangement. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 13(2):89 121, </volume> <month> May </month> <year> 1995. </year>
Reference-contexts: In this case, the live blocks from that single PEG are appended to the end of the RAID-5 write log [Wilk96]. There is a significant amount of research into altering disk layout to improve read performance. <ref> [Wong83, Vong90, Ruem91, Aky95] </ref> discuss the benefits of placing the most frequently accessed data in the middle of the disk where it is most likely to be close to the disk head. These systems do restructuring at either the cylinder or block level. <p> These systems do restructuring at either the cylinder or block level. They use the disk controller or the device driver, not the file system, to monitor access frequencies and the move data. <ref> [Aky95] </ref> limits the number of blocks for which information must be maintained much like we do; however we are maintaining information about the relationships between blocks where they are maintaining access frequencies. [Ruem91] evaluates the benefits for an update-in-place file system and conjectures that the benefits would be even greater for
Reference: [Ande96] <author> T. Anderson, M. Dahlin, J. Neefe, D. Patterson, D. Roselli, and R. Wang. </author> <title> Serverless Network File Systems. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 14(1):4179, </volume> <month> Feb. </month> <year> 1996. </year>
Reference: [Bake91] <author> M. Baker, J. Hartman, M. Kupfer, K. Shirriff, and J. Ousterhout. </author> <title> Measurements of a Distributed File System. </title> <booktitle> Proc. Thirteenth ACM Symposium on Operating Systems Principles, </booktitle> <pages> pp. </pages> <address> 198212, </address> <month> Oct. </month> <year> 1991. </year>
Reference-contexts: First, data is often read as it is written. In that case, the temporal locality of LFS will be as effective as the semantic locality of update-in-place systems. Studies of traditional UNIX workloads <ref> [Bake91] </ref> show that most files are written and read sequentially. However, there are workloads for which reads patterns do not match write patterns. Even for a UNIX workload, LFS may often do a poor job of keeping the contents of a directory together.
Reference: [Barn93] <author> S. Barnard and H. Simon. </author> <title> A Fast Multilevel Implementation of Recursive Spectral Bisection for Partitioning Unstructured Problems. </title> <booktitle> Proc. Sixth SIAM Conference on Parallel Processing for Scientific Computing, </booktitle> <pages> pp. 711718, </pages> <year> 1993. </year>
Reference-contexts: We observe that partitioning the access graph used to capture file system activity is an application of the more general irregular graph partitioning problem. Although the general problem is NP-complete, there exist many heuristic solutions in the literature <ref> [Barn93, Hend93] </ref>. There is a recent trend towards incorporating LFS techniques into other file system architectures. Network Appliances file system, WAFL, improves write performance for their RAID array by writing multiple blocks in a stripe [Hitz95].
Reference: [Birr93] <author> A. Birrell, A. Hisgen, C. Jerian, T. Mann, and G. Swart. </author> <title> The Echo Distributed File System. </title> <type> Technical Report 111, </type> <institution> Digital Equipment Corp. Systems Research Center, </institution> <month> Sep. </month> <year> 1993. </year>
Reference-contexts: Perhaps more importantly, crash recovery requires scanning the entire disk; for example, it can take over 10 minutes to recover a modern 9 GB FFS disk after a crash. Write-ahead logging file systems were designed to simplify crash recovery <ref> [Hagm87, Chut92, Birr93, Cust94, Veri95, Swee96] </ref>. Write-ahead logging batches metadata updates into a log. After the log is safely on disk, the updates are copied into fixed disk locations, placed as in an update-in-place system.
Reference: [Blac95] <author> T. Blackwell, J. Harris, and M. Seltzer. </author> <title> Heuristic Cleaning Algorithms in Log-Structured File Systems. </title> <booktitle> Proc. 1995 Winter USENIX Conference, </booktitle> <pages> pp. 277-288, </pages> <month> Jan. </month> <year> 1995. </year>
Reference-contexts: For many workloads, there is sufficient idle time in which the LFS cleaner can run without interfering with normal file system accesses <ref> [Blac95] </ref>. However, when the disk fills up, disk updates are scattered randomly across the disk, or long-term sustained disk performance is required (leaving little idle time to clean), then LFS cleaning can significantly degrade file system performance [Selt93, Selt95]. <p> In many environments, traditional cleaning performs very well. Idle time can often be exploited to hide cleaning costs from users; for the workloads examined in <ref> [Blac95] </ref>, 97% of cleaning could be done in the background. [McNu94] shows FIGURE 2. Effect of disk characteristics on overall write cost for the Auspex workload. Disk utilization is 85%. <p> Despite the criticism of the results, they do describe some real problems with LFS that should not be ignored. In this paper, we investigate ways to enable LFS to provide reasonable performance even for these problematic workloads. <ref> [McNu94, Blac95, Lome95] </ref> all explore enhancements to LFS. [McNu94] presents a mathematical model of garbage collection and concludes that disk utilization must be kept below 80% for LFS to provide good performance. <p> In this paper, we explore solutions that do not require leaving 20% of the disk unused. <ref> [Blac95] </ref> considers how much of the garbage collection costs can be shifted into the background. [Lome95] argues for the use of LFS for databases and suggests detecting expensive read patterns and simply rewriting the data in the same order it is read. [Dahl95] makes an initial evaluation of using cached data
Reference: [Chen94] <author> P. Chen, E. Lee, G. Gibson, R. Katz, and D. Patterson. </author> <title> RAID: High-Performance, Reliable Secondary Storage. </title> <journal> ACM Computing Surveys, </journal> <volume> 26(2):145188, </volume> <month> Jun. </month> <year> 1994. </year>
Reference: [Chut92] <author> S. Chutani, O. Anderson, M. Kazar, B. Leverett, W. Mason, and R. Siedbotham. </author> <title> The Episode File System. </title> <booktitle> Proc. 1992 Winter USENIX Conference, </booktitle> <pages> pp. 4360, </pages> <month> Jan. </month> <year> 1992. </year> <month> 15 </month>
Reference-contexts: Perhaps more importantly, crash recovery requires scanning the entire disk; for example, it can take over 10 minutes to recover a modern 9 GB FFS disk after a crash. Write-ahead logging file systems were designed to simplify crash recovery <ref> [Hagm87, Chut92, Birr93, Cust94, Veri95, Swee96] </ref>. Write-ahead logging batches metadata updates into a log. After the log is safely on disk, the updates are copied into fixed disk locations, placed as in an update-in-place system.
Reference: [Cust94] <author> H. Custer. </author> <title> Inside the Windows NT File System. </title> <publisher> Microsoft Press, </publisher> <year> 1994. </year>
Reference-contexts: Perhaps more importantly, crash recovery requires scanning the entire disk; for example, it can take over 10 minutes to recover a modern 9 GB FFS disk after a crash. Write-ahead logging file systems were designed to simplify crash recovery <ref> [Hagm87, Chut92, Birr93, Cust94, Veri95, Swee96] </ref>. Write-ahead logging batches metadata updates into a log. After the log is safely on disk, the updates are copied into fixed disk locations, placed as in an update-in-place system.
Reference: [Dahl94] <author> M. Dahlin, C. Mather, R. Wang, T. Anderson, and D. Patterson. </author> <title> A Quantitative Analysis of Cache Policies for Scalable Network File Systems. </title> <booktitle> Proc. SIGMETRICS Conference on Measurement and Modeling of Computer Sys tems, </booktitle> <pages> pp. 150160, </pages> <month> May </month> <year> 1994. </year>
Reference-contexts: 1 Introduction File system designs have long been driven by changes in the cost and performance of the underlying hardware. A designer must consider the relative cost per byte of memory versus disk [Rose92a], the relative performance of the CPU versus a network access versus a disk access <ref> [Dahl94] </ref>, the relative magnitudes of seek time, rotational delay, and disk bandwidth [Selt90], not to mention changes in the workload placed on the file system. As a concrete example, the management of free blocks on disk has evolved over the past two decades to reflect hardware technology changes. <p> Our simulator is descended from Mendel Rosenblums LFS simulator [Rose92b]. Mike Dahlin modified this simulator to accept input from a trace file and to track cache information [Dahl95] and used it to evaluate cooperative caching in xFS <ref> [Dahl94] </ref>. We have added a write buffer and implemented the modifications being evaluated here. The benefit of this history is that the simulator has already been used in several significant LFS evaluations. <p> This is necessary in order to demonstrate that our self-tuning algorithms achieve robust performance across a wide range of workloads. We use the real traces to verify that we are helping, or at least not harming, average case performance. For a measured trace, we use the Berkeley Auspex Trace <ref> [Dahl94] </ref>. This trace follows the NFS activity of 236 clients serviced by an Auspex file server over the period of 1 week during late 1993. It was gathered by snooping Ethernet packets on four subnets.
Reference: [Dahl95] <author> M. Dahlin. </author> <title> Serverless Network File Systems. </title> <type> PhD Thesis. </type> <institution> University of California, Berkeley, </institution> <month> Dec. </month> <year> 1995. </year>
Reference-contexts: Our simulator is descended from Mendel Rosenblums LFS simulator [Rose92b]. Mike Dahlin modified this simulator to accept input from a trace file and to track cache information <ref> [Dahl95] </ref> and used it to evaluate cooperative caching in xFS [Dahl94]. We have added a write buffer and implemented the modifications being evaluated here. The benefit of this history is that the simulator has already been used in several significant LFS evaluations. <p> More importantly, the large and widening gap between CPU and disk performance has meant that file system read response times are dominated by disk accesses, even for very high cache hit rates <ref> [Dahl95] </ref>. Third, Ousterhout has argued that while fragmentation in an FFS degrades performance for both reads and writes, LFS cleaning has no ill effects on read cost [Oust95c]. <p> not require leaving 20% of the disk unused. [Blac95] considers how much of the garbage collection costs can be shifted into the background. [Lome95] argues for the use of LFS for databases and suggests detecting expensive read patterns and simply rewriting the data in the same order it is read. <ref> [Dahl95] </ref> makes an initial evaluation of using cached data to lower cleaning costs. We continue this evaluation by considering the effects of additional parameters and workloads. Our section on adaptive cleaning combines traditional LFS cleaning with another garbage collection mechanism, hole-plugging, that was used in HP AutoRAID [Wilk96].
Reference: [Gang94] <author> G. Ganger and Y. Patt. </author> <title> Metadata Update Performance in File Systems. </title> <booktitle> Proc. First Symposium on Operating Systems Design and Implementation, </booktitle> <pages> pp. 49-60, </pages> <month> Nov. </month> <year> 1994. </year>
Reference-contexts: These synchronous updates can severely limit the effective disk bandwidth (although the ordering constraints can be loosened in some circumstances <ref> [Gang94] </ref>). Perhaps more importantly, crash recovery requires scanning the entire disk; for example, it can take over 10 minutes to recover a modern 9 GB FFS disk after a crash. Write-ahead logging file systems were designed to simplify crash recovery [Hagm87, Chut92, Birr93, Cust94, Veri95, Swee96].
Reference: [Gold95] <author> R. Golding, P. Bosch, C. Staelin, T. Sullivan, and J. Wilkes. </author> <title> Idleness is Not Sloth. </title> <booktitle> Proc. 1995 Winter US ENIX Conference, </booktitle> <pages> pp. </pages> <address> 201202, </address> <month> Jan. </month> <year> 1995. </year>
Reference: [Grif94] <author> J. Griffioen and R. Appleton. </author> <title> Reducing File System Latency Using A Predictive Approach. </title> <booktitle> Proc. 1994 Summer US ENIX Conference, </booktitle> <pages> pp. 197-207, </pages> <month> Jun. </month> <year> 1994. </year>
Reference-contexts: Third, it must analyze the difference between the current layout and the desired layout and if necessary issue I/O requests to correct the difference. 5.2.1 Tracking File Access History To capture the past access pattern, we build a block access graph, similar to the file access graphs proposed by <ref> [Grif94] </ref>, for use in prefetching file data into memory. Intuitively, reorganizing data for reads is complementary to prefetching. Prefetching must identify blocks or files that are used together to know what to pull into memory. <p> Update-in-place systems such as FFS [McKu84] reduce average disk access times by collecting statically related data in cylinder groups. The problem of organizing data for reads is very similar to prefetching. In order to facilitate prefetching, <ref> [Grif94] </ref> proposes a file access graph similar to the one we use to reorganize data for read accesses. [Kroe96] uses a data compression technique, prediction by partial match, to predict file system activity and thus improve the effectiveness of prefetching.
Reference: [Hagm87] <author> R. Hagmann. </author> <title> Reimplementing the Cedar File System Using Logging and Group Commit. </title> <booktitle> Proc. Eleventh ACM Symposium on Operating Systems Principles, </booktitle> <pages> pp. 155162, </pages> <month> Oct. </month> <year> 1987. </year>
Reference-contexts: Perhaps more importantly, crash recovery requires scanning the entire disk; for example, it can take over 10 minutes to recover a modern 9 GB FFS disk after a crash. Write-ahead logging file systems were designed to simplify crash recovery <ref> [Hagm87, Chut92, Birr93, Cust94, Veri95, Swee96] </ref>. Write-ahead logging batches metadata updates into a log. After the log is safely on disk, the updates are copied into fixed disk locations, placed as in an update-in-place system.
Reference: [Hart93] <author> J. Hartman and J. Ousterhout. </author> <title> The Zebra Striped Network File System. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 13(3) </volume> <pages> 274-310, </pages> <month> Aug. </month> <year> 1995. </year>
Reference: [Hend93] <author> B. Hendrickson and R. Leland. </author> <title> A Multilevel Algorithm for Partitioning Graphs. </title> <type> Technical Report SAND93-1301, </type> <institution> Sandia National Laboratories, </institution> <year> 1993. </year>
Reference-contexts: We observe that partitioning the access graph used to capture file system activity is an application of the more general irregular graph partitioning problem. Although the general problem is NP-complete, there exist many heuristic solutions in the literature <ref> [Barn93, Hend93] </ref>. There is a recent trend towards incorporating LFS techniques into other file system architectures. Network Appliances file system, WAFL, improves write performance for their RAID array by writing multiple blocks in a stripe [Hitz95].
Reference: [Hitz94] <author> D. Hitz and J. Lau and M. Malcolm. </author> <title> File System Design for an NFS Server Appliance. </title> <booktitle> Proc. 1994 Winter USENIX Conference, </booktitle> <pages> pp. 235-246, </pages> <year> 1994. </year>
Reference: [Jaco88] <author> V. Jacobson and M. Karels. </author> <title> Congestion Avoidance and Control. </title> <booktitle> Proc. SIGCOMM Conference on Data Communi cation. </booktitle> <month> Nov. </month> <year> 1988. </year>
Reference-contexts: Many other systems have applied self-tuning principles at some level; our work was initially inspired by these efforts. For example, TCP implementations have long measured round-trip delays to determine appropriate time-out values <ref> [Jaco88] </ref>. More recently these implementations have begun to adapt to patterns in how packets are dropped by the network under congestion [Math96]. In the file system arena, AutoRAID [Wilk96] adapts the amount of disk space devoted to mirroring vs.
Reference: [Jaco91] <author> D. Jacobson and J. Wilkes. </author> <title> Disk Scheduling Algorithms Based on Rotational Position. </title> <type> Technical Report HPL-CSP-91-7rev1, </type> <institution> Hewlett-Packard Laboratories, </institution> <address> Palo Alto, CA, </address> <month> Mar. </month> <year> 1991. </year>
Reference: [Kary95] <author> G. Karypis and V. Kumar. </author> <title> A Fast and High Quality Multilevel Scheme for Partitioning Irregular Graphs. </title> <type> Technical Report TR 95-035, </type> <institution> University of Minnesota, </institution> <year> 1995. </year>
Reference-contexts: We have validated our dynamic algorithm by comparing the partition qualities of our algorithm with that of a well known off-line graph partitioning package <ref> [Kary95] </ref>. For file access graphs based on the Berkeley Auspex traces, the partitions produced by our data reorganizer were better than or equivalent to those generated by the off-line algorithm, even with only 4 neighbors recorded per block.
Reference: [Kotz94] <author> D. Kotz, S. Toh, and S. Radhakrishnan. </author> <title> A Detailed Simulation Model of the HP 97560 Disk Drive. </title> <type> Technical Re port PCS-TR94-220, </type> <institution> Dartmouth, </institution> <month> Jul. </month> <year> 1994. </year>
Reference-contexts: However, a more sophisticated disk model is required to study read performance. To evaluate reorganizing data for reads, we hooked our simulator to the HP97560 disk simulator from Dartmouth <ref> [Kotz94] </ref>. Our simulator can be configured to run with or without data reorganization. Separate client and server caches can be simulated. Unless specified, the client caches are 16 MB and the server cache is 128 MB. Data is channeled into the log through one write buffer. <p> Finally, we flush the cache to simulate the passage of time. This cycle is repeated many times. In order to get more accurate estimates of disk read performance, we hooked our simulator to the HP97560 disk simulator from Dartmouth <ref> [Kotz94] </ref>. We assume that reorganization could be accomplished in the background since we are modeling a workload for which a significant amount of time passes between the edit-compile cycles. kB block from disk as a function of the number of times we repeated the edit-compile cycle. <p> With reorganization turned on, the response time is kept stable around 6.5 ms. orig LFS LFS with reorganizer compile iteration disk time per kB read (ms) 13 performance difference. The HP97560 disk controller model <ref> [Kotz94] </ref> includes a 64 kB read-ahead buffer cache. The graph shows the average time a read request spends in the disk cache. As a result of periodically optimizing layout for read performance, we are able to raise the read-ahead cache hit rate from 37.9% to 66.8%.
Reference: [Kowa78] <author> T. Kowalski. </author> <title> FSCK: The UNIX System Check Program. </title> <type> Technical report, </type> <institution> Bell Laboratory, </institution> <address> Murray Hill, NJ, </address> <month> Mar. </month> <year> 1978. </year> <note> [Kroe96]. </note> <author> T. Kroeger and D. </author> <title> Long. Predicting Future File-System Actions From Prior Events. </title> <booktitle> Proc. 1996 USENIX Confer ence, </booktitle> <pages> pp. 319-328, </pages> <month> Jan. </month> <year> 1996. </year>
Reference-contexts: In FFS, this is accomplished by applying each update 3 synchronously to disk in a consistent order, so that the crash recovery procedure can detect logical operations that were in progress at the time of the crash <ref> [Kowa78] </ref>. These synchronous updates can severely limit the effective disk bandwidth (although the ordering constraints can be loosened in some circumstances [Gang94]).
Reference: [Lome95] <author> D. Lomet. </author> <title> The Case for Log Structuring in Database Systems. </title> <booktitle> Intl Workshop on High Performance Transac tion Systems, </booktitle> <month> Sep. </month> <year> 1995. </year>
Reference-contexts: Despite the criticism of the results, they do describe some real problems with LFS that should not be ignored. In this paper, we investigate ways to enable LFS to provide reasonable performance even for these problematic workloads. <ref> [McNu94, Blac95, Lome95] </ref> all explore enhancements to LFS. [McNu94] presents a mathematical model of garbage collection and concludes that disk utilization must be kept below 80% for LFS to provide good performance. <p> In this paper, we explore solutions that do not require leaving 20% of the disk unused. [Blac95] considers how much of the garbage collection costs can be shifted into the background. <ref> [Lome95] </ref> argues for the use of LFS for databases and suggests detecting expensive read patterns and simply rewriting the data in the same order it is read. [Dahl95] makes an initial evaluation of using cached data to lower cleaning costs.
Reference: [McDo89] <author> M. McDonald and R. Bunt. </author> <title> Improving File System Performance by Dynamically Restructuring Disk Space. </title> <booktitle> Proc. Phoenix Conference on Computers and Communication (Scottsdale, </booktitle> <address> AZ), </address> <pages> pp. 264-269, </pages> <month> Mar. </month> <year> 1989. </year>
Reference-contexts: however we are maintaining information about the relationships between blocks where they are maintaining access frequencies. [Ruem91] evaluates the benefits for an update-in-place file system and conjectures that the benefits would be even greater for a file system that did not do such a good job of initial data placement. <ref> [McDo89, Stae91] </ref> explore file system directed reorganization at the granularity of whole files. Update-in-place systems such as FFS [McKu84] reduce average disk access times by collecting statically related data in cylinder groups. The problem of organizing data for reads is very similar to prefetching.
Reference: [McNu94] <author> B. McNutt. </author> <title> Background Data Movement in a Log-Structured File System. </title> <journal> IBM Journal of Research and De velopment, </journal> <volume> 38(1) </volume> <pages> 47-58, </pages> <year> 1994. </year>
Reference-contexts: In many environments, traditional cleaning performs very well. Idle time can often be exploited to hide cleaning costs from users; for the workloads examined in [Blac95], 97% of cleaning could be done in the background. <ref> [McNu94] </ref> shows FIGURE 2. Effect of disk characteristics on overall write cost for the Auspex workload. Disk utilization is 85%. <p> Despite the criticism of the results, they do describe some real problems with LFS that should not be ignored. In this paper, we investigate ways to enable LFS to provide reasonable performance even for these problematic workloads. <ref> [McNu94, Blac95, Lome95] </ref> all explore enhancements to LFS. [McNu94] presents a mathematical model of garbage collection and concludes that disk utilization must be kept below 80% for LFS to provide good performance. <p> Despite the criticism of the results, they do describe some real problems with LFS that should not be ignored. In this paper, we investigate ways to enable LFS to provide reasonable performance even for these problematic workloads. [McNu94, Blac95, Lome95] all explore enhancements to LFS. <ref> [McNu94] </ref> presents a mathematical model of garbage collection and concludes that disk utilization must be kept below 80% for LFS to provide good performance.
Reference: [McKu84] <author> M. McKusick, W. Joy, S. Leffler, and R. Fabry. </author> <title> A Fast File System for UNIX. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 2(3):181197, </volume> <month> Aug. </month> <year> 1984. </year>
Reference-contexts: Early file systems, such as the original UNIX file system [Ritc74], used a simple on-disk linked list to track free blocks. Later, systems such as the BSD Fast File System (FFS) <ref> [McKu84] </ref>, replaced the on-disk linked list with an in-core bitmap, allowing the file system to optimize block allocation to keep related data, such as blocks within a file, as adjacent as possible on disk. <p> The traditional approach to building file systems has been to place most of the burden of minimizing seeks and rotational delays on the disk block allocation policy. For example, FFS <ref> [McKu84] </ref> places new data and metadata blocks on disk near other semantically related blocks (e.g., blocks within the same file or within the same directory). <p> Update-in-place systems such as FFS <ref> [McKu84] </ref> reduce average disk access times by collecting statically related data in cylinder groups. The problem of organizing data for reads is very similar to prefetching.
Reference: [McVo91] <author> L. McVoy and S. Kleiman. </author> <title> Extent-like Performance from a UNIX File System. </title> <booktitle> Proc. 1991 Winter USENIX Conference, </booktitle> <pages> pp. 3343, </pages> <month> Jan. </month> <year> 1991. </year>
Reference-contexts: Particularly when dynamic access patterns follow semantic relationships (e.g., when large files are read or written in large chunks), this can offer good performance <ref> [McVo91, Selt95, Smit96] </ref>. However, performance can decrease over time as the disk becomes fragmented, particularly as the disk fills up, making it harder for the allocation policy to find appropriate slots for new blocks [Smit97].
Reference: [Math96] <author> M. Mathis and J. Mahdavi. </author> <title> Forward Acknowledgment: Refining TCP Congestion Control. </title> <booktitle> Proc. SIGCOMM Con ference on Data Communication. </booktitle> <month> Aug. </month> <year> 1996. </year>
Reference-contexts: For example, TCP implementations have long measured round-trip delays to determine appropriate time-out values [Jaco88]. More recently these implementations have begun to adapt to patterns in how packets are dropped by the network under congestion <ref> [Math96] </ref>. In the file system arena, AutoRAID [Wilk96] adapts the amount of disk space devoted to mirroring vs. RAID-5 based on the percentage of free space available; AutoRAID also moves data between the mirrored region and the RAID-5 based on the pattern of writes to the data.
Reference: [Oust95a] <author> J. Ousterhout. </author> <booktitle> A Critique of Seltzers 1993 USENIX Pa per.http://www.sunlabs.com/people/john.ousterhout/ seltzer93.html, </booktitle> <year> 1995. </year>
Reference-contexts: However, it has dramatically lower performance for some other workloads, such as those dominated by random updates to a full disk with little idle time to clean [Selt93, Selt95a]. This dichotomy in LFS performance has led to a debate among LFS researchers <ref> [Selt93, Oust95a, Selt95a, Oust95b, Selt95b, Oust95c] </ref>, and has led many to conclude that LFS is an interesting, but impractical, idea. <p> In addition, it is pointed out that certain modifications to an update-in-place system such as FFS can allow it to achieve some of the same benefits as LFS. A debate ensued <ref> [Oust95a, Oust95b, Selt95b, Oust95c] </ref> concerning whether the results presented in [Selt93] and [Selt95a] were legitimate and representative. Despite the criticism of the results, they do describe some real problems with LFS that should not be ignored.
Reference: [Oust95b] <author> J. Ousterhout. </author> <title> A Critique of Seltzers LFS Measurements. </title> <address> http://www.sunlabs.com/people/john.ousterhout/ seltzer.html, </address> <year> 1995. </year>
Reference-contexts: However, it has dramatically lower performance for some other workloads, such as those dominated by random updates to a full disk with little idle time to clean [Selt93, Selt95a]. This dichotomy in LFS performance has led to a debate among LFS researchers <ref> [Selt93, Oust95a, Selt95a, Oust95b, Selt95b, Oust95c] </ref>, and has led many to conclude that LFS is an interesting, but impractical, idea. <p> In addition, it is pointed out that certain modifications to an update-in-place system such as FFS can allow it to achieve some of the same benefits as LFS. A debate ensued <ref> [Oust95a, Oust95b, Selt95b, Oust95c] </ref> concerning whether the results presented in [Selt93] and [Selt95a] were legitimate and representative. Despite the criticism of the results, they do describe some real problems with LFS that should not be ignored.
Reference: [Oust95c] <author> J. Ousterhout. </author> <title> A Response to Seltzers Response. </title> <note> http:// www.sunlabs.com/people/john.ousterhout/seltzer2.html, 1995. </note>
Reference-contexts: However, it has dramatically lower performance for some other workloads, such as those dominated by random updates to a full disk with little idle time to clean [Selt93, Selt95a]. This dichotomy in LFS performance has led to a debate among LFS researchers <ref> [Selt93, Oust95a, Selt95a, Oust95b, Selt95b, Oust95c] </ref>, and has led many to conclude that LFS is an interesting, but impractical, idea. <p> Third, Ousterhout has argued that while fragmentation in an FFS degrades performance for both reads and writes, LFS cleaning has no ill effects on read cost <ref> [Oust95c] </ref>. However, this is not obvious since cleaning coalesces blocks from different segments together even though the contents of these segments are unrelated both semantically and temporally. In this section, we explore one approach to reorganizing data for readsa dynamic algorithm that operates at the granularity of blocks. <p> In addition, it is pointed out that certain modifications to an update-in-place system such as FFS can allow it to achieve some of the same benefits as LFS. A debate ensued <ref> [Oust95a, Oust95b, Selt95b, Oust95c] </ref> concerning whether the results presented in [Selt93] and [Selt95a] were legitimate and representative. Despite the criticism of the results, they do describe some real problems with LFS that should not be ignored.
Reference: [Ritc74] <author> D. Ritchie and K. Thompson. </author> <title> The UNIX Timesharing System. </title> <journal> Communications of the ACM. </journal> <volume> 17(7), </volume> <pages> pp. 365-375, </pages> <month> Jul. </month> <year> 1974. </year>
Reference-contexts: As a concrete example, the management of free blocks on disk has evolved over the past two decades to reflect hardware technology changes. Early file systems, such as the original UNIX file system <ref> [Ritc74] </ref>, used a simple on-disk linked list to track free blocks.
Reference: [Rose92a] <author> M. Rosenblum and J. Ousterhout. </author> <title> The Design and Implementation of a Log-Structured File System. </title> <journal> ACM Trans actions on Computer Systems, </journal> <volume> 10(1):2652, </volume> <month> Feb. </month> <year> 1992. </year>
Reference-contexts: 1 Introduction File system designs have long been driven by changes in the cost and performance of the underlying hardware. A designer must consider the relative cost per byte of memory versus disk <ref> [Rose92a] </ref>, the relative performance of the CPU versus a network access versus a disk access [Dahl94], the relative magnitudes of seek time, rotational delay, and disk bandwidth [Selt90], not to mention changes in the workload placed on the file system. <p> A plethora of knobs could be added, but they are as likely to be mistuned as well-tuned. We explore self-tuning by means of a set of four enhancements to the design of a log-structured file system (LFS) <ref> [Rose92a] </ref>. LFS research has been a good case study of the need for adaptive methods because it has shown the difficulty of designing a file system to have good performance across a wide spectrum of workloads, even for a fixed technology point. <p> [Wilk96] and an adaptive combination of cleaning and hole-plugging. (Each of these methods will be discussed in 4 more detail in Section 4.2.) A variety of policies for choosing segments to garbage collect are also implemented, including greedy, which simply chooses the least utilized segment at each opportunity, and cost-benefit <ref> [Rose92a] </ref>. The cost-benefit policy chooses the segment which minimizes the formula , where u is the utilization of the segment and a is the age of the segment. <p> In <ref> [Rose92a, Rose92b] </ref>, segment size is chosen to be large enough that the access time becomes insignificant when amortized over the segment transfer. In Sprite LFS, a relatively large 1 MB segment is used. <p> In Figure 6, we show cleaning, hole-plugging and the adaptive policy for the random update workload. We include greedy cleaning as well as cost-age since greedy has been shown to have slightly better performance than cost-age on a random workload <ref> [Rose92a, Selt95b] </ref>. The adaptive policy correctly shifts from cleaning to hole-plugging at the appropriate point. We are indeed able to retain the good common case performance of traditional cleaning while avoiding its dramatic performance degradation at high disk utilizations. <p> The hit and miss rates are specified in each column. The column heights are hit rate times hit time and miss rate times miss time, respectively. 37.9% 33.2% 20.6% 58.8% 6 Related Work When LFS was originally introduced <ref> [Rose92a, Rose92b] </ref>, the design space of this radically new file system organization was explored with simulations using write cost as a metric. A full implementation of LFS was incorporated into the Sprite operating system and was used in a production environment.
Reference: [Rose92b] <author> M. Rosenblum. </author> <title> The Design and Implementation of a Log-structured File System. </title> <type> PhD Thesis. </type> <institution> University of Cal ifornia, Berkeley, </institution> <month> Jun. </month> <year> 1992. </year>
Reference-contexts: Throughout the rest of the paper, we refer to this policy as cost-age to avoid confusion with other cost-benefit formulas presented in section 4.2.2. Our simulator is descended from Mendel Rosenblums LFS simulator <ref> [Rose92b] </ref>. Mike Dahlin modified this simulator to accept input from a trace file and to track cache information [Dahl95] and used it to evaluate cooperative caching in xFS [Dahl94]. We have added a write buffer and implemented the modifications being evaluated here. <p> In our evaluation, we examine the effect of our optimizations on write cost. Write cost is the metric traditionally used in evaluating LFS write performance <ref> [Rose92b] </ref>. The original write cost model can be expressed with the following formula: The write cost is the ratio of total work to the work necessary to initially write the new data to disk. <p> The cleaner reads an entire segment even if only a few live blocks remain. It may be beneficial to allow the cleaner to read only the live blocks when that would take less time <ref> [Rose92b] </ref>, but following the original LFS, we did not include this optimization. Ideally, the data would be written once and never moved by the cleaner; this happens if all data in the segment is overwritten before the segment is reclaimed. <p> In <ref> [Rose92a, Rose92b] </ref>, segment size is chosen to be large enough that the access time becomes insignificant when amortized over the segment transfer. In Sprite LFS, a relatively large 1 MB segment is used. <p> In [Rose92a, Rose92b], segment size is chosen to be large enough that the access time becomes insignificant when amortized over the segment transfer. In Sprite LFS, a relatively large 1 MB segment is used. On the other hand, there is a countervailing benefit to choosing a smaller segment size. <ref> [Rose92b] </ref> observes that at smaller segment sizes the variance in segment utilizations is larger; allowing the cleaner to choose less utilized segments. In particular, smaller segments are more likely to empty completely before cleaning. Empty segments can simply be declared clean without requiring any disk transfers by the cleaner. <p> It is minimized at an intermediate segment size. Note that when the transfer inefficiency is 1, the overall write cost is equal to the original write cost. This is consistent with the assumption made in <ref> [Rose92b] </ref> that the segment size is large enough that access time becomes insignificant. In Figure 1, the difference between EQ 3 and EQ 4 is due to the impact of partial segments. Changes in disk characteristics affect the trade-off between cleaner overhead and transfer inefficiency. <p> The hit and miss rates are specified in each column. The column heights are hit rate times hit time and miss rate times miss time, respectively. 37.9% 33.2% 20.6% 58.8% 6 Related Work When LFS was originally introduced <ref> [Rose92a, Rose92b] </ref>, the design space of this radically new file system organization was explored with simulations using write cost as a metric. A full implementation of LFS was incorporated into the Sprite operating system and was used in a production environment.
Reference: [Ruem91] <author> C. Ruemmler and J. Wilkes. </author> <title> Disk Shuffling. </title> <type> Technical Report HPL-91-156. </type> <institution> Hewlett-Packard Laboratories, </institution> <address> Palo Alto, CA, </address> <month> Oct. </month> <year> 1991. </year>
Reference-contexts: In this case, the live blocks from that single PEG are appended to the end of the RAID-5 write log [Wilk96]. There is a significant amount of research into altering disk layout to improve read performance. <ref> [Wong83, Vong90, Ruem91, Aky95] </ref> discuss the benefits of placing the most frequently accessed data in the middle of the disk where it is most likely to be close to the disk head. These systems do restructuring at either the cylinder or block level. <p> disk controller or the device driver, not the file system, to monitor access frequencies and the move data. [Aky95] limits the number of blocks for which information must be maintained much like we do; however we are maintaining information about the relationships between blocks where they are maintaining access frequencies. <ref> [Ruem91] </ref> evaluates the benefits for an update-in-place file system and conjectures that the benefits would be even greater for a file system that did not do such a good job of initial data placement. [McDo89, Stae91] explore file system directed reorganization at the granularity of whole files.
Reference: [Ruem93] <author> C. Ruemmler and J. Wilkes. </author> <title> A Trace-driven Analysis of Disk Working Set Sizes. </title> <type> Technical Report HPL-OSR-93-23. </type> <institution> Hewlett-Packard Laboratories, </institution> <address> Palo Alto, CA, </address> <month> Apr. 4, </month> <year> 1993. </year>
Reference-contexts: After initialization, over 60% of the data on disk is not rewritten; it is a common characteristic of real systems that only a small portion of the disk is actively written <ref> [Ruem93] </ref>.
Reference: [Sea97a] <institution> Seagate Technology, Inc. Hawk 2XL Family 3.5-inch Driives, </institution> <address> http://www.seagate.com/disc/hawk/ hawk2xlscsi3.shtml, </address> <year> 1997. </year>
Reference-contexts: A disk request is modelled as taking the access time plus the request size over the disk bandwidth. Unless otherwise specified, we simulate a 15 ms access time and 5 MB/s bandwidth, typical of a mid-range disk <ref> [Sea97a] </ref>. Although simple, this model reflects the fact that most LFS implementations make no effort to opportunistically choose which segments to write or clean based on the current disk head location. However, a more sophisticated disk model is required to study read performance.
Reference: [Sea97b] <author> Seagate Technology, Inc. </author> <title> Cheetah Family 3.5-inch Form Factor. </title> <note> http://www.seagate.com/disc/cheetah/cheetah.sht ml, </note> <year> 1997. </year>
Reference-contexts: The middle curve represents the baseline disk simulated in this paper, and the top curve represents the highest performance disk available from Seagate as this paper goes to press <ref> [Sea97b] </ref>. Note that the curve is the same for different disks with the same access time bandwidth product. For all curves, overall write cost is minimized for a segment size of roughly four times bandwidth times access time.
Reference: [Selt90] <author> M. Seltzer, P. Chen, and J. Ousterhout. </author> <title> Disk Scheduling Revisted. </title> <booktitle> Proc. 1990 Winter USENIX Conference, </booktitle> <pages> pp. 313324, </pages> <month> Jan. </month> <year> 1990. </year>
Reference-contexts: A designer must consider the relative cost per byte of memory versus disk [Rose92a], the relative performance of the CPU versus a network access versus a disk access [Dahl94], the relative magnitudes of seek time, rotational delay, and disk bandwidth <ref> [Selt90] </ref>, not to mention changes in the workload placed on the file system. As a concrete example, the management of free blocks on disk has evolved over the past two decades to reflect hardware technology changes.
Reference: [Selt92] <author> M. Seltzer. </author> <title> File System Performance and Transaction Support. </title> <type> PhD Thesis. </type> <institution> University of California, Berkeley, </institution> <month> Dec. </month> <year> 1992. </year>
Reference-contexts: A full implementation of LFS was incorporated into the Sprite operating system and was used in a production environment. Many possible avenues of improvement to LFS were proposed, but not all of them were fully pursued, including reorganizing data for reads and the impact of different segment sizes. <ref> [Selt92, Selt93, Selt95a] </ref> further explored the LFS paradigm. [Selt93] and [Selt95a] describe an implementation of LFS for BSD and compare it with the Berkeley Fast File System (FFS) and an enhanced extent-based FFS.
Reference: [Selt93] <author> M. Seltzer, K. Bostic, M. McKusick, and C. Staelin. </author> <title> An Implementation of a Log-Structured File System for UNIX. </title> <booktitle> Proc. 1993 Winter USENIX Conference, </booktitle> <pages> pp. 307326, </pages> <month> Jan. </month> <year> 1993. </year>
Reference-contexts: However, it has dramatically lower performance for some other workloads, such as those dominated by random updates to a full disk with little idle time to clean <ref> [Selt93, Selt95a] </ref>. This dichotomy in LFS performance has led to a debate among LFS researchers [Selt93, Oust95a, Selt95a, Oust95b, Selt95b, Oust95c], and has led many to conclude that LFS is an interesting, but impractical, idea. <p> However, it has dramatically lower performance for some other workloads, such as those dominated by random updates to a full disk with little idle time to clean [Selt93, Selt95a]. This dichotomy in LFS performance has led to a debate among LFS researchers <ref> [Selt93, Oust95a, Selt95a, Oust95b, Selt95b, Oust95c] </ref>, and has led many to conclude that LFS is an interesting, but impractical, idea. <p> However, when the disk fills up, disk updates are scattered randomly across the disk, or long-term sustained disk performance is required (leaving little idle time to clean), then LFS cleaning can significantly degrade file system performance <ref> [Selt93, Selt95] </ref>. Update-in-place file systems handle these situations more gracefully because they simply pay the initial cost to place each write on top of its previous location. Although most LFS performance evaluations have focused on write cost, read cost is also an important metric. <p> The trace lacks pathname information which limited our ability to evaluate semantic reorganization. A worst-case workload for LFS is one with random updates, no idle time, and high disk utilization. The TPC-B database benchmark is an example of such a workload and was examined in <ref> [Selt93, Selt95] </ref>. We approximate this workload with a synthetically generated random update workload that 1 u+ - has similar worst-case characteristics for LFS. To initialize the disk, we first write enough blocks sequentially to fill the disk to the desired utilization. <p> However, additional research demonstrated that cleaning overhead can result in dramatically lower write performance for some workloads <ref> [Selt93, Selt95a] </ref>. In this section, we show how self-tuning principles can be applied to LFS to provide high write performance across a broader range of workloads, even those that were previously problematic. In our evaluation, we examine the effect of our optimizations on write cost. <p> If segment updates show a high degree of locality, then some segments will be emptier than others and will yield more free space when cleaned. The problem with cleaning appears at high disk utilizations, especially for workloads with many random updates and insufficient idle time <ref> [Selt93, Selt95a] </ref>. Because segments do not have a chance to empty before they must be cleaned, the cost of cleaning can skyrocket. In order to coalesce one free segments worth of space, the cleaner must process many nearly full segments. <p> A full implementation of LFS was incorporated into the Sprite operating system and was used in a production environment. Many possible avenues of improvement to LFS were proposed, but not all of them were fully pursued, including reorganizing data for reads and the impact of different segment sizes. <ref> [Selt92, Selt93, Selt95a] </ref> further explored the LFS paradigm. [Selt93] and [Selt95a] describe an implementation of LFS for BSD and compare it with the Berkeley Fast File System (FFS) and an enhanced extent-based FFS. <p> Many possible avenues of improvement to LFS were proposed, but not all of them were fully pursued, including reorganizing data for reads and the impact of different segment sizes. [Selt92, Selt93, Selt95a] further explored the LFS paradigm. <ref> [Selt93] </ref> and [Selt95a] describe an implementation of LFS for BSD and compare it with the Berkeley Fast File System (FFS) and an enhanced extent-based FFS. This work demonstrated that cleaning costs can seriously degrade LFS performance on workloads with random updates at high disk utilization, such as the TPC-B benchmark. <p> In addition, it is pointed out that certain modifications to an update-in-place system such as FFS can allow it to achieve some of the same benefits as LFS. A debate ensued [Oust95a, Oust95b, Selt95b, Oust95c] concerning whether the results presented in <ref> [Selt93] </ref> and [Selt95a] were legitimate and representative. Despite the criticism of the results, they do describe some real problems with LFS that should not be ignored.
Reference: [Selt95a] <author> M. Seltzer, K. Smith, H. Balakrishnan, J. Chang, S. McMains, and V. Padmanabhan. </author> <title> File System Logging Versus Clustering: A Performance Comparison. </title> <booktitle> Proc. 1995 Winter USENIX Conference, </booktitle> <pages> pp. 249264, </pages> <month> Jan. </month> <year> 1995. </year> <month> 16 </month>
Reference-contexts: However, it has dramatically lower performance for some other workloads, such as those dominated by random updates to a full disk with little idle time to clean <ref> [Selt93, Selt95a] </ref>. This dichotomy in LFS performance has led to a debate among LFS researchers [Selt93, Oust95a, Selt95a, Oust95b, Selt95b, Oust95c], and has led many to conclude that LFS is an interesting, but impractical, idea. <p> However, it has dramatically lower performance for some other workloads, such as those dominated by random updates to a full disk with little idle time to clean [Selt93, Selt95a]. This dichotomy in LFS performance has led to a debate among LFS researchers <ref> [Selt93, Oust95a, Selt95a, Oust95b, Selt95b, Oust95c] </ref>, and has led many to conclude that LFS is an interesting, but impractical, idea. <p> However, additional research demonstrated that cleaning overhead can result in dramatically lower write performance for some workloads <ref> [Selt93, Selt95a] </ref>. In this section, we show how self-tuning principles can be applied to LFS to provide high write performance across a broader range of workloads, even those that were previously problematic. In our evaluation, we examine the effect of our optimizations on write cost. <p> If segment updates show a high degree of locality, then some segments will be emptier than others and will yield more free space when cleaned. The problem with cleaning appears at high disk utilizations, especially for workloads with many random updates and insufficient idle time <ref> [Selt93, Selt95a] </ref>. Because segments do not have a chance to empty before they must be cleaned, the cost of cleaning can skyrocket. In order to coalesce one free segments worth of space, the cleaner must process many nearly full segments. <p> A full implementation of LFS was incorporated into the Sprite operating system and was used in a production environment. Many possible avenues of improvement to LFS were proposed, but not all of them were fully pursued, including reorganizing data for reads and the impact of different segment sizes. <ref> [Selt92, Selt93, Selt95a] </ref> further explored the LFS paradigm. [Selt93] and [Selt95a] describe an implementation of LFS for BSD and compare it with the Berkeley Fast File System (FFS) and an enhanced extent-based FFS. <p> Many possible avenues of improvement to LFS were proposed, but not all of them were fully pursued, including reorganizing data for reads and the impact of different segment sizes. [Selt92, Selt93, Selt95a] further explored the LFS paradigm. [Selt93] and <ref> [Selt95a] </ref> describe an implementation of LFS for BSD and compare it with the Berkeley Fast File System (FFS) and an enhanced extent-based FFS. This work demonstrated that cleaning costs can seriously degrade LFS performance on workloads with random updates at high disk utilization, such as the TPC-B benchmark. <p> In addition, it is pointed out that certain modifications to an update-in-place system such as FFS can allow it to achieve some of the same benefits as LFS. A debate ensued [Oust95a, Oust95b, Selt95b, Oust95c] concerning whether the results presented in [Selt93] and <ref> [Selt95a] </ref> were legitimate and representative. Despite the criticism of the results, they do describe some real problems with LFS that should not be ignored.
Reference: [Selt95b] <author> M. Seltzer and K. Smith. </author> <title> A Response to Ousterhouts Critique of LFS Measurements. </title> <address> http://www.eecs.harvard.edu/ ~margo/usenix.195/ouster.html, </address> <year> 1995. </year>
Reference-contexts: However, it has dramatically lower performance for some other workloads, such as those dominated by random updates to a full disk with little idle time to clean [Selt93, Selt95a]. This dichotomy in LFS performance has led to a debate among LFS researchers <ref> [Selt93, Oust95a, Selt95a, Oust95b, Selt95b, Oust95c] </ref>, and has led many to conclude that LFS is an interesting, but impractical, idea. <p> In Figure 6, we show cleaning, hole-plugging and the adaptive policy for the random update workload. We include greedy cleaning as well as cost-age since greedy has been shown to have slightly better performance than cost-age on a random workload <ref> [Rose92a, Selt95b] </ref>. The adaptive policy correctly shifts from cleaning to hole-plugging at the appropriate point. We are indeed able to retain the good common case performance of traditional cleaning while avoiding its dramatic performance degradation at high disk utilizations. <p> In addition, it is pointed out that certain modifications to an update-in-place system such as FFS can allow it to achieve some of the same benefits as LFS. A debate ensued <ref> [Oust95a, Oust95b, Selt95b, Oust95c] </ref> concerning whether the results presented in [Selt93] and [Selt95a] were legitimate and representative. Despite the criticism of the results, they do describe some real problems with LFS that should not be ignored.
Reference: [Smit96] <author> K. Smith and M. Seltzer. </author> <title> A Comparison of FFS Disk Allocation Polices. </title> <booktitle> Proc. 1996 USENIX Conference, </booktitle> <pages> pp. 15 26, </pages> <month> Jan. </month> <year> 1996. </year>
Reference-contexts: Particularly when dynamic access patterns follow semantic relationships (e.g., when large files are read or written in large chunks), this can offer good performance <ref> [McVo91, Selt95, Smit96] </ref>. However, performance can decrease over time as the disk becomes fragmented, particularly as the disk fills up, making it harder for the allocation policy to find appropriate slots for new blocks [Smit97].
Reference: [Smit97] <author> K.Smith and M. Seltzer. </author> <title> File System Aging - Increasing the Relevance of File System Benchmarks. </title> <booktitle> Proc. SIGMET-RICS Conference on Measurement and Modeling of Com puter Systems, </booktitle> <month> Jun. </month> <year> 1997. </year>
Reference-contexts: However, performance can decrease over time as the disk becomes fragmented, particularly as the disk fills up, making it harder for the allocation policy to find appropriate slots for new blocks <ref> [Smit97] </ref>. Worse, this performance penalty persists; without a disk reorganizer, once the disk fills up, performance can be negatively impacted from then on. An update-in-place approach also has significant performance costs associated with crash recovery, both during recovery itself and during normal operation.
Reference: [Stae91] <author> C. Staelin and H. Garcia-Molina. </author> <title> Smart Filesystems. </title> <booktitle> Proc. 1991 Winter USENIX Conference, </booktitle> <pages> pp. 4551, </pages> <month> Jan. </month> <year> 1991. </year>
Reference-contexts: however we are maintaining information about the relationships between blocks where they are maintaining access frequencies. [Ruem91] evaluates the benefits for an update-in-place file system and conjectures that the benefits would be even greater for a file system that did not do such a good job of initial data placement. <ref> [McDo89, Stae91] </ref> explore file system directed reorganization at the granularity of whole files. Update-in-place systems such as FFS [McKu84] reduce average disk access times by collecting statically related data in cylinder groups. The problem of organizing data for reads is very similar to prefetching.
Reference: [Stae96] <author> C. </author> <type> Staelin. </type> <institution> Discussion at UC Berkeley. Personal Commu nication. </institution> <month> Nov. </month> <year> 1996. </year>
Reference-contexts: The remaining update stream reaching the RAID-5 applies 14 to cold data and shows very little locality in practice. Thus, the AutoRAID environment is the worst possible case for traditional cleaning (high utilized segments that are updated randomly) <ref> [Stae96] </ref>. AutoRAID does PEG-cleaning only in the special case that there are no holes to be plugged; that is when all PEGs but one are full or empty. In this case, the live blocks from that single PEG are appended to the end of the RAID-5 write log [Wilk96].
Reference: [Swee96] <author> A. Sweeney, D. Doucette, W. Hu, C. Anderson, M. Nish-imoto, and G. Peck. </author> <title> Scalability in the XFS File System. </title> <booktitle> Proc. 1996 USENIX Conference, </booktitle> <pages> pp. 1-14, </pages> <month> Jan. </month> <year> 1996. </year>
Reference-contexts: More recently, the increasing capacity of disks combined with the increasing use of RAIDs has driven some to abandon bit maps for B-trees to reduce the CPU overhead of searching for a free disk block <ref> [Swee96] </ref>; a 100 disk system today could require over 10 MB of bitmap. These changes pose a tremendous challenge to file system designers. <p> Perhaps more importantly, crash recovery requires scanning the entire disk; for example, it can take over 10 minutes to recover a modern 9 GB FFS disk after a crash. Write-ahead logging file systems were designed to simplify crash recovery <ref> [Hagm87, Chut92, Birr93, Cust94, Veri95, Swee96] </ref>. Write-ahead logging batches metadata updates into a log. After the log is safely on disk, the updates are copied into fixed disk locations, placed as in an update-in-place system. <p> There is a recent trend towards incorporating LFS techniques into other file system architectures. Network Appliances file system, WAFL, improves write performance for their RAID array by writing multiple blocks in a stripe [Hitz95]. Sweeney et al. recently incorporated location-independent inodes, an idea from LFS, into XFS <ref> [Swee96] </ref>, a write-ahead logging file system. As in LFS, location-independent inodes would make it easier to incorporate a disk reorganizer.
Reference: [Tran90a] <author> Transaction Processing Performance Council. </author> <title> TPC Benchmark B Standard Specification. Waterside Associ ates, </title> <address> Fremont, CA, </address> <month> Aug. </month> <year> 1990. </year>
Reference: [Tran90b] <author> Transaction Processing Performance Council. </author> <title> TPC Benchmark C Standard Specification. Waterside Associ ates, </title> <address> Fremont, CA, </address> <month> Jul. </month> <year> 1990. </year>
Reference: [Tran95] <author> Transaction Processing Performance Council. </author> <title> TPC Benchmark D Standard Specification. Waterside Associ ates, </title> <address> Fremont, CA, </address> <month> Apr. </month> <year> 1995. </year>
Reference-contexts: Another problematic workload is random writes followed by sequential reads. Decision support database workloads can exhibit this pattern. Random updates are applied to the active portion of the database and then sometime later large sweeping queries read relations sequentially <ref> [Tran95] </ref>. Second, it was initially argued that almost all reads would be satisfied from large caches and therefore would be unaffected by disk layout. However, the rapid fluctuation in the relative cost per byte of disk and DRAM make such predictions uncertain at best.
Reference: [Veri95] <institution> Veritas Software. </institution> <note> The VERITAS File System (VxFS). http://www.veritas.com/products.html, 1995. </note>
Reference-contexts: Perhaps more importantly, crash recovery requires scanning the entire disk; for example, it can take over 10 minutes to recover a modern 9 GB FFS disk after a crash. Write-ahead logging file systems were designed to simplify crash recovery <ref> [Hagm87, Chut92, Birr93, Cust94, Veri95, Swee96] </ref>. Write-ahead logging batches metadata updates into a log. After the log is safely on disk, the updates are copied into fixed disk locations, placed as in an update-in-place system.
Reference: [Vong90] <author> P. Vongsathorn and S. Carson. </author> <title> A System for Adaptive Disk Rearrangement. </title> <journal> Software: Practice and Experience. </journal> <volume> 20(3) </volume> <pages> 225-242, </pages> <month> Mar. </month> <year> 1990. </year>
Reference-contexts: In this case, the live blocks from that single PEG are appended to the end of the RAID-5 write log [Wilk96]. There is a significant amount of research into altering disk layout to improve read performance. <ref> [Wong83, Vong90, Ruem91, Aky95] </ref> discuss the benefits of placing the most frequently accessed data in the middle of the disk where it is most likely to be close to the disk head. These systems do restructuring at either the cylinder or block level.
Reference: [Wilk96] <author> J. Wilkes, R. Golding, C. Staelin, and T. Sullivan. </author> <title> The HP AutoRAID Hierarchical Storage System. </title> <journal> ACM Transac tions on Computer Systems, </journal> <volume> 14(1) </volume> <pages> 108-136, </pages> <month> Feb. </month> <year> 1996. </year>
Reference-contexts: Three of our optimizations concern improving LFS write performance. First, we show how to choose the LFS segment size by trading transfer efficiency against cleaning efficiency. Second, we show how to combine traditional LFS cleaning with an alternate garbage collection mechanism called hole-plugging <ref> [Wilk96] </ref>. Our approach adapts to changes in disk utilization and workload to avoid the traditional LFS performance cliff at high disk utilizations for random updates, while still preserving the advantage LFS has at lower disk utilizations. <p> For example, TCP implementations have long measured round-trip delays to determine appropriate time-out values [Jaco88]. More recently these implementations have begun to adapt to patterns in how packets are dropped by the network under congestion [Math96]. In the file system arena, AutoRAID <ref> [Wilk96] </ref> adapts the amount of disk space devoted to mirroring vs. RAID-5 based on the percentage of free space available; AutoRAID also moves data between the mirrored region and the RAID-5 based on the pattern of writes to the data. <p> The amount of data that the cleaner may process at one time can be varied. For the experiments presented in this paper, we allowed the cleaner to process up to 20 MB at a time. Several garbage collection methods can be chosen, including traditional LFS cleaning, hole-plugging <ref> [Wilk96] </ref> and an adaptive combination of cleaning and hole-plugging. (Each of these methods will be discussed in 4 more detail in Section 4.2.) A variety of policies for choosing segments to garbage collect are also implemented, including greedy, which simply chooses the least utilized segment at each opportunity, and cost-benefit [Rose92a]. <p> It does this by dynamically choosing between two mechanisms: traditional LFS cleaning and hole-plugging <ref> [Wilk96] </ref>. Our adaptive method successfully chooses the lowest cost mechanism based on the observed usage patterns. 4.2.1 Comparing Traditional Cleaning With Hole-plugging In traditional cleaning, the live blocks in several partially empty segments are combined to produce a new full segment, freeing the old partially empty segments for reuse. <p> We continue this evaluation by considering the effects of additional parameters and workloads. Our section on adaptive cleaning combines traditional LFS cleaning with another garbage collection mechanism, hole-plugging, that was used in HP AutoRAID <ref> [Wilk96] </ref>. We observe that hole-plugging is especially beneficial at high disk utilizations, while cleaning is better at lower disk utilizations (below 8085%). In AutoRAID, hole-plugging, is used rather than traditional LFS cleaning or an adaptive combination of the two, because AutoRAID is structured such that hole-plugging always performs better. <p> AutoRAID does PEG-cleaning only in the special case that there are no holes to be plugged; that is when all PEGs but one are full or empty. In this case, the live blocks from that single PEG are appended to the end of the RAID-5 write log <ref> [Wilk96] </ref>. There is a significant amount of research into altering disk layout to improve read performance. [Wong83, Vong90, Ruem91, Aky95] discuss the benefits of placing the most frequently accessed data in the middle of the disk where it is most likely to be close to the disk head.
Reference: [Wong83] <author> C. Wong. </author> <title> Algorithmic Studies in Mass Storage Systems. </title> <publisher> Computer Science Press, </publisher> <year> 1983. </year>
Reference-contexts: In this case, the live blocks from that single PEG are appended to the end of the RAID-5 write log [Wilk96]. There is a significant amount of research into altering disk layout to improve read performance. <ref> [Wong83, Vong90, Ruem91, Aky95] </ref> discuss the benefits of placing the most frequently accessed data in the middle of the disk where it is most likely to be close to the disk head. These systems do restructuring at either the cylinder or block level.
Reference: [Wort95] <author> B. Worthington, G. Ganger, W. Patt and J. Wilkes. </author> <title> Online Extraction of SCSI Disk Drive Parameters. </title> <booktitle> Proc. SIG-METRICS Conference on Measurement and Modeling of Computer Systems, </booktitle> <pages> pp. 146-156, </pages> <month> May </month> <year> 1995. </year>
Reference-contexts: The optimal segment size has been increasing since then. This suggests that to be able to scale with disk technology improvements, an LFS file system should measure and adapt to its underlying disk performance; <ref> [Wort95] </ref> outlines a set of techniques for extracting disk parameters on-line. workload.
References-found: 57

