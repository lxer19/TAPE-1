URL: http://c.gp.cs.cmu.edu:5103/afs/cs.cmu.edu/project/scandal/public/papers/CMU-CS-94-200.ps.gz
Refering-URL: http://c.gp.cs.cmu.edu:5103/afs/cs.cmu.edu/project/scandal/public/papers/CMU-CS-94-200.html
Root-URL: http://www.cs.cmu.edu
Title: Porting a Vector Library: a Comparison of MPI, Paris, CMMD and PVM (or, "I'll never
Author: Jonathan C. Hardwick 
Address: Pittsburgh, PA 15213  
Affiliation: School of Computer Science Carnegie Mellon University  
Date: November 1994  
Pubnum: CMU-CS-94-200  
Abstract: An earlier version of this paper appeared in "Proceedings of the 2nd Scalable Parallel Libraries Conference", Mississippi State University, Mississippi, October 1994. Abstract This paper describes the design and implementation in MPI of the parallel vector library CVL, which is used as the basis for implementing nested data-parallel languages such as Nesl and Proteus. We compare the ease of writing and debugging the portable MPI implementation of CVL with our experiences writing previous versions in CM-2 Paris, CM-5 CMMD, and PVM, and give initial performance results for MPI CVL running on an IBM SP-1, Intel Paragon, and TMC CM-5. This research was sponsored in part by the Wright Laboratory, Aeronautical Systems Center, Air Force Materiel Command, USAF, and the Advanced Research Projects Agency (ARPA) under grant number F33615-93-1-1330. It was also supported in part by the Pittsburgh Supercomputing Center (Grant ASC890018P), who provided TMC CM-2 time, and in part by the National Center for Supercomputing Applications (Grant TRA930102N), who provided TMC CM-5 time, and in part by the Argonne National Laboratory, who provided IBM SP-1 time. The views and conclusions contained in this document are those of the authors and should not be interpreted as necessarily representing the official policies or endorsements, either expressed or implied, of Wright Laboratory or the United States Government. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> David H. Bailey. </author> <title> RISC microprocessors and scientific computing. </title> <booktitle> In Proceedings of Supercomputing '93, </booktitle> <pages> pages 645-654, </pages> <month> November </month> <year> 1993. </year>
Reference-contexts: Since elementwise CVL functions are essentially single loops over the data, they get no benefit from any cache for large problem sizes, and become limited by main-memory bandwidth. This problem is becoming progressively worse as RISC CPU speeds continue to outrun DRAM bandwidth <ref> [1] </ref>. One obvious solution is to abandon CVL and the Vcode interpreter, and instead compile Nesl down to a low-level language such as C or assembler, which allows us to perform loop fusion and other optimizations [8].
Reference: [2] <author> Guy E. Blelloch. </author> <title> Vector Models for Data-Parallel Computing. </title> <publisher> MIT Press, </publisher> <year> 1990. </year>
Reference-contexts: This ability to operate efficiently on irregular data structures is one of Nesl's main strengths [6]. Nesl is compiled into Vcode [4], a stack-based intermediate language. Nesl's nested data structures are "flattened" into segmented vectors <ref> [2] </ref>, allowing a single function call to operate on the entire data structure at once. The resulting Vcode is then interpreted, with the interpreter using CVL to achieve portability and efficiency.
Reference: [3] <author> Guy E. Blelloch. NESL: </author> <title> A nested data-parallel language (version 2.6). </title> <type> Technical Report CMU-CS-93-129, </type> <institution> School of Computer Science, Carnegie Mellon University, </institution> <month> April </month> <year> 1993. </year>
Reference-contexts: CVL was developed primarily as the lowest-level component of a three-tier system that implements the portable nested data-parallel language Nesl <ref> [3] </ref>, although it is also used as a stand-alone C vector library, and as a back end by the Proteus language [15]. Figure 1 shows an overview of the current Nesl system. The shaded components (CVL and its parallel implementations) are the main focus of this paper.
Reference: [4] <author> Guy E. Blelloch and Siddhartha Chatterjee. </author> <title> VCODE: A data-parallel intermediate language. </title> <booktitle> In Proceedings of Frontiers of Massively Parallel Computation, </booktitle> <pages> pages 471-480, </pages> <month> October </month> <year> 1990. </year>
Reference-contexts: A parallel function that sums the elements of a vector can then be applied in parallel to sum each row of this sparse array. This ability to operate efficiently on irregular data structures is one of Nesl's main strengths [6]. Nesl is compiled into Vcode <ref> [4] </ref>, a stack-based intermediate language. Nesl's nested data structures are "flattened" into segmented vectors [2], allowing a single function call to operate on the entire data structure at once. The resulting Vcode is then interpreted, with the interpreter using CVL to achieve portability and efficiency.
Reference: [5] <author> Guy E. Blelloch, Siddhartha Chatterjee, Jonathan C. Hardwick, Margaret Reid-Miller, Jay Sipelstein, and Marco Zagha. CVL: </author> <title> A C vector library. </title> <type> Technical Report CMU-CS-93-114, </type> <institution> School of Computer Science, Carnegie Mellon University, </institution> <month> February </month> <year> 1993. </year>
Reference-contexts: 1 CVL overview CVL (C Vector Library <ref> [5] </ref>) is a library of vector functions callable from C. It provides an abstract vector memory model that is independent of the underlying architecture, and is designed so that efficient CVL implementations can be developed for a wide variety of parallel machines.
Reference: [6] <author> Guy E. Blelloch, Jonathan C. Hardwick, Jay Sipelstein, Marco Zagha, and Siddhartha Chat-terjee. </author> <title> Implementation of a portable nested data-parallel language. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 21(1) </volume> <pages> 4-14, </pages> <month> April </month> <year> 1994. </year>
Reference-contexts: A parallel function that sums the elements of a vector can then be applied in parallel to sum each row of this sparse array. This ability to operate efficiently on irregular data structures is one of Nesl's main strengths <ref> [6] </ref>. Nesl is compiled into Vcode [4], a stack-based intermediate language. Nesl's nested data structures are "flattened" into segmented vectors [2], allowing a single function call to operate on the entire data structure at once. <p> The resulting Vcode is then interpreted, with the interpreter using CVL to achieve portability and efficiency. Since each Vcode instruction typically translates to a single CVL function call operating on vectors, the overhead of interpreting each instruction is amortized over the length of its vector operands <ref> [6] </ref>. To support high-level languages, CVL supplies a rich variety of vector functions, including elementwise function application, global communication functions such as scans and permutations, and ancillary functions such as timing and memory allocation. <p> In a comparison on the CM-2 between CM Fortran and Nesl, CM Fortran wins on benchmarks that emphasize floating-point operations, whilst Nesl wins on benchmarks that utilize nested parallelism and emphasize communication <ref> [6] </ref>. 3.2 CM-5 CVL The implementation of CVL for the TMC CM-5 is written in C and the message-passing library CMMD [21]. Again, CM Fortran could not be used because at the time it lacked array aliasing capabilities.
Reference: [7] <author> Patrick Bridges, Nathan Doss, William Gropp, Edward Karrels, Ewing Lusk, and Anthony Skjellum. </author> <title> Users' Guide to mpich, a Portable Implementation of MPI, </title> <month> November </month> <year> 1994. </year>
Reference-contexts: The ANL/MSU model MPI implementation mpich <ref> [7] </ref> (version 1.0.5) was used for the MPI benchmarks. This is implemented on top of an abstract device interface, easing the task of porting it to new machines, but adding some overhead since it is layered on top of the manufacturer's own message passing system.
Reference: [8] <author> Siddhartha Chatterjee. </author> <title> Compiling Data-Parallel Programs for Efficient Execution on Shared-Memory Multiprocessors. </title> <type> PhD thesis, </type> <institution> School of Computer Science, Carnegie Mellon University, </institution> <month> October </month> <year> 1991. </year>
Reference-contexts: One obvious solution is to abandon CVL and the Vcode interpreter, and instead compile Nesl down to a low-level language such as C or assembler, which allows us to perform loop fusion and other optimizations <ref> [8] </ref>. This can be combined with the communication optimizations mentioned above, and with new models for the control and partitioning of nested data parallel programs on MIMD machines. We are now actively working on a system that will achieve this.
Reference: [9] <author> Siddhartha Chatterjee, Guy E. Blelloch, and Marco Zagha. </author> <title> Scan primitives for vector computers. </title> <booktitle> In Proceedings Supercomputing '90, </booktitle> <pages> pages 666-675, </pages> <month> November </month> <year> 1990. </year>
Reference-contexts: This specialization of functionality allows CVL to be tuned for each machine to which it is ported. For example, the segmented scan and reduction operations on the Cray have been vectorized by hand in Cray assembler language using a novel algorithm <ref> [9] </ref>, and are much faster than could be easily achieved using Fortran or C.
Reference: [10] <author> Cray Research, Inc. </author> <title> PVM and HeNCE Programmer's Manual, </title> <month> May </month> <year> 1993. </year> <note> SR-2501 2.0. </note>
Reference-contexts: Unfortunately, PVM is an evolving research project rather than an agreed-upon standard. Thus, manufacturers' extensions to PVM that improve its performance (such as the Cray T3D's pvm fastsend () function <ref> [10] </ref>) are not supported by other PVM implementations. A portable PVM version of CVL would therefore either have to sacrifice performance on such platforms, or would require constant updating as new machine-specific PVM implementations appear.
Reference: [11] <author> Rickard E. Faith, Doug L. Hoffman, and David G. Stahl. UnCvl: </author> <title> The University of North Carolina C vector library. </title> <note> Version 1.1, </note> <month> May </month> <year> 1993. </year>
Reference-contexts: Machine-specific versions currently exist for the Thinking Machines CM-2 and CM-5, the Cray Y-MP and Y-MP/C90, and the MasPar MP-1 and MP-2 <ref> [11] </ref>, as well as a portable serial version written in ANSI C.
Reference: [12] <author> Message Passing Interface Forum. </author> <title> MPI: A message-passing interface standard. </title> <type> Technical Report MCS-P342-1193, </type> <institution> Computer Science Department, University of Tennesse, </institution> <year> 1994. </year>
Reference-contexts: This results in needless synchronization and memory traffic. 3.4 MPI CVL: portability and performance? Development of MPI CVL was started in the hopes that MPI <ref> [12] </ref> would combine the advantages of PVM (portability, ease of development) with performance approaching that of existing vendor communication libraries for MPPs. It also promised a more stable specification than that provided by CMMD or PVM.
Reference: [13] <author> Al Geist, Adam Beguelin, Jack Dongarra, Weicheng Jiang, Robert Manchek, and Vaidy Sun-deram. </author> <title> PVM 3.0 User's Guide and Reference Manual, </title> <month> February </month> <year> 1993. </year> <month> 15 </month>
Reference-contexts: To explore the possibilities of a more portable CVL, a proof-of-concept implementation [14] of CVL was written in C and PVM 3.0 <ref> [13] </ref>. A subset of the CVL functions were implemented in order to get an idea of the relative speed and usability of PVM for our application. To speed development, the design of PVM CVL was mostly copied from that of CM-5 CVL.
Reference: [14] <author> Jonathan C. Hardwick. </author> <title> A proof-of-concept for CVL on top of PVM. </title> <type> Internal report, </type> <month> May </month> <year> 1993. </year>
Reference-contexts: Note that a new CVL implementation using the current aliasing capabilities of CM Fortran would be able to fully realize the floating-point performance of both the CM-2 and the CM-5. To explore the possibilities of a more portable CVL, a proof-of-concept implementation <ref> [14] </ref> of CVL was written in C and PVM 3.0 [13]. A subset of the CVL functions were implemented in order to get an idea of the relative speed and usability of PVM for our application.
Reference: [15] <author> Peter Mills, Lars Nyland, Jan Prins, John Reif, and Robert Wagner. </author> <title> Prototyping parallel and distributed programs in Proteus. </title> <booktitle> In Proceedings of the Third IEEE Symposium on Parallel and Distributed Processing, </booktitle> <pages> pages 10-19, </pages> <address> Dallas, Texas, </address> <month> December </month> <year> 1991. </year> <note> IEEE. </note>
Reference-contexts: CVL was developed primarily as the lowest-level component of a three-tier system that implements the portable nested data-parallel language Nesl [3], although it is also used as a stand-alone C vector library, and as a back end by the Proteus language <ref> [15] </ref>. Figure 1 shows an overview of the current Nesl system. The shaded components (CVL and its parallel implementations) are the main focus of this paper. Nesl is a parallel functional language with an ML-like syntax, and is designed to be used interactively.
Reference: [16] <author> Gary W. Sabot. </author> <title> Optimizing CM Fortran compiler for Connection Machine computers. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 23(2) </volume> <pages> 224-238, </pages> <month> November </month> <year> 1994. </year>
Reference-contexts: Paris performance: There are several performance penalties involved in using Paris: * Paris uses the older "fieldwise" representation of data, requiring extra transpose operations to use the CM-2's floating-point accelerators. The newer and more efficient "slicewise" rep resentation that is now available in CM Fortran <ref> [16] </ref> is not supported by Paris. * Paris communication functions use one virtual processor per element, which requires that the user make "aliases" of dynamically-sized fields before passing them to a communication function.
Reference: [17] <author> William Saphir. </author> <title> A comparison of communication libraries: NX, CMMD, MPI, PVM. NASA Ames User Seminar, http://lovelace.nas.nasa.gov/Parallel/People/wcs/talks/ message passing comparison.ps, </title> <month> November </month> <year> 1993. </year>
Reference-contexts: There may be an additional performance benefit to be gained by aggregrating elements in fixed-size buffers in user space, rather than relying on system buffering <ref> [17] </ref>. However, this introduces the problem of how nodes determine when all messages have been sent and received. It is not enough for the nodes to use a barrier to agree that all messages have been sent, since messages may still be in transit. <p> This imposes a serial bottleneck on PVM's collective functions. * Even in PVM 3.3, support for asynchronous communication to overlap computation and communication and to reduce system buffering remains incomplete <ref> [17] </ref>. This results in needless synchronization and memory traffic. 3.4 MPI CVL: portability and performance? Development of MPI CVL was started in the hopes that MPI [12] would combine the advantages of PVM (portability, ease of development) with performance approaching that of existing vendor communication libraries for MPPs.
Reference: [18] <institution> Thinking Machines Corporation, </institution> <address> Cambridge, MA. </address> <note> Paris Reference Manual, February 1991. Version 6.0. </note>
Reference-contexts: For each implementation, we discuss the particular solutions chosen to the CVL porting decisions outlined in Section 2, and their impact on development time and final performance. 3.1 CM-2 CVL The implementation of CVL for the TMC CM-2 is written in C and Paris <ref> [18] </ref>, a parallel instruction set for the CM-2's SIMD processing array.
Reference: [19] <institution> Thinking Machines Corporation, </institution> <address> Cambridge, MA. </address> <month> CDPEAC: </month> <title> Using GCC to program in DPEAC, </title> <month> December </month> <year> 1992. </year>
Reference-contexts: When the CM-5's 6 vector units were introduced, offering memory-to-memory floating-point performance an order of magnitude better than that achievable with the node SPARC processors, only CM Fortran was upgraded to support them. Using the vector units from C is possible via a set of assembler macros <ref> [19] </ref>, but with the limited resources available, updating CMMD CVL to use the vector units has not been attempted. Note that a new CVL implementation using the current aliasing capabilities of CM Fortran would be able to fully realize the floating-point performance of both the CM-2 and the CM-5.
Reference: [20] <institution> Thinking Machines Corporation, </institution> <address> Cambridge, MA. </address> <note> CM Fortran Reference Manual, December 1992. Version 2.0. </note>
Reference-contexts: CM Fortran <ref> [20] </ref> was not used because at the time it did not have the ability to alias subsections of arrays (for example, storing a vector of integers into the middle of where a vector of floating-point numbers used to be), and therefore did not meet CVL's vector reuse requirements.
Reference: [21] <institution> Thinking Machines Corporation, </institution> <address> Cambridge, MA. </address> <note> CMMD Reference Manual, May 1993. Version 3.0. </note>
Reference-contexts: on the CM-2 between CM Fortran and Nesl, CM Fortran wins on benchmarks that emphasize floating-point operations, whilst Nesl wins on benchmarks that utilize nested parallelism and emphasize communication [6]. 3.2 CM-5 CVL The implementation of CVL for the TMC CM-5 is written in C and the message-passing library CMMD <ref> [21] </ref>. Again, CM Fortran could not be used because at the time it lacked array aliasing capabilities. To avoid possible confusion with the portable MPI CVL implementation running on a CM-5, we shall henceforth refer to CM-5 CVL as CMMD CVL.
Reference: [22] <author> Thorsten von Eicken, David E. Culler, Seth Copen Goldstein, and Klaus Erik Schauser. </author> <title> Active messages: a mechanism for integrated communication and computation. </title> <booktitle> In Proceedings 19th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 256-266, </pages> <month> May </month> <year> 1992. </year> <month> 16 </month>
Reference-contexts: CMMD ease of development: CMMD went through three major versions during the writing of CMMD CVL, necessitating constant rewriting and considerably lengthening the development process. Active messages <ref> [22] </ref> were introduced as a third-party extension to the first major version of CMMD, were incompatible with the second version, and were finally adopted by Thinking Machines in the third version. CMMD supplies scan and reduction functions in both unsegmented and segmented versions, simplifying the task of implementing CVL.
References-found: 22

