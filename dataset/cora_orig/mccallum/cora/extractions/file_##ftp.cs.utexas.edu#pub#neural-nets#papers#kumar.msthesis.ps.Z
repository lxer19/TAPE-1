URL: file://ftp.cs.utexas.edu/pub/neural-nets/papers/kumar.msthesis.ps.Z
Refering-URL: http://www.cs.utexas.edu/users/nn/pages/publications/abstracts.html
Root-URL: http://www.cs.utexas.edu
Title: Confidence based Dual Reinforcement Q-Routing: an On-line Adaptive Network Routing Algorithm  
Author: Shailesh Kumar 
Address: Austin, TX 78712  
Affiliation: Artificial Intelligence Laboratory The University of Texas at Austin  
Web: http://www.cs.utexas.edu/users/skumar/  
Note: skumar@cs.utexas.edu  
Date: May 1998  
Pubnum: Report AI98-267  
Abstract-found: 0
Intro-found: 1
Reference: <author> A. G. Barto, R. S. S., and Anderson, C. W. </author> <year> (1983). </year> <title> Neuronlike adaptive elements that can solve difficult learning control problems. </title> <journal> IEEE Transactions on Systems, Man, and Cybernetics, SMC-13:834-846. </journal>
Reference: <author> Barto, A. G. </author> <year> (1992). </year> <title> Reinforcement learning and adaptive critic methods. In White, </title> <editor> D. A., and Sofge, D. A., editors, </editor> <booktitle> Handbook of Intelligent Control, </booktitle> <pages> 469-491. </pages> <address> New York: </address> <publisher> Van Nostrand-Reinhold. </publisher>
Reference: <author> Bellman, R. E. </author> <year> (1957). </year> <title> Dynamic Programming. </title> <publisher> Princeton, </publisher> <address> NJ: </address> <publisher> Princeton University Press. </publisher>
Reference: <author> Bellman, R. E. </author> <year> (1958). </year> <title> On a routing problem. </title> <journal> Quarterly of Applied Mathematics, </journal> <volume> 16 </volume> <pages> 87-90. </pages>
Reference-contexts: This approach has lead to adaptive distance vector routing algorithms. Distributed Bellman-Ford Routing <ref> (Bellman 1958) </ref>, described in chapter 2, is the state of the art and most widely used and cited distance vector routing algorithm.
Reference: <author> Boyan, J. A., and Littman, M. L. </author> <year> (1994). </year> <title> Packet routing in dynamically changing networks: A reinforcement learning approach. </title> <editor> In Cowan, J., Tesauro, G., and Alspector, J., editors, </editor> <booktitle> Advances in Neural Information Processing Systems 6. </booktitle> <address> Cambridge, MA: </address> <publisher> MIT Press. </publisher>
Reference-contexts: Since CDRQ-Routing is superior than Q-Routing, Bellman-Ford is not compared with CDRQ-Routing, only Q-Routing is compared with CDRQ-Routing in chapter 6. 42 Chapter 5 Confidence-Based Dual Reinforcement Q-Routing In chapter 3, the Q-learning framework was applied to the problem of adaptive network routing, yielding the Q-Routing algorithm <ref> (Boyan and Littman 1994) </ref>. The ability of Q-Routing to learn an effective routing policy starting from a random policy was also demonstrated. In this chapter, a new routing algorithm based on Q-Routing is developed. <p> The parameters are the weights in the links. Error back propagation can be used to update these parameters, equivalent to Q value updates in CDRQ-Routing. Boyan 75 and Littman observed that using neural networks instead of Q tables for network routing lead to poor performance <ref> (Boyan and Littman 1994) </ref>. Thus if the search space is discrete, the tables approach is useful and neural network should be used only for continuous spaces. * Heterogeneous systems: The different components of a system might have different capacities.
Reference: <author> C. Cheng, R. Ripley, S. K., and Garcia-Luna-Aceves, J. </author> <year> (1989). </year> <title> A loop-free extended bellman-ford routing protocol without boincing effect. </title> <booktitle> In ACM Sigcomm '89 Symposium. </booktitle>
Reference: <author> Choi, S. P. M., and Yeung, D.-Y. </author> <year> (1996). </year> <title> Predictive Q-routing: A memory-based reinforcement learning approach to adaptive traffic control. </title> <editor> In Touretzky, D. S., Mozer, M. C., and Hasselmo, M. E., editors, </editor> <booktitle> Advances in Neural Information Processing Systems 8, </booktitle> <pages> 945-951. </pages> <address> Cambridge, MA: </address> <publisher> MIT Press. 92 Dijkstra, E. </publisher> <year> (1959). </year> <title> A note on two problems in connexion with graphs. </title> <journal> Numer. Math., </journal> <volume> 1:269 - 271. </volume>
Reference-contexts: As the network load levels, traffic patterns, and topology change, so should the routing policies. As an example, consider the network shown in figure 1.1 <ref> (Choi and Yeung 1996) </ref>. If nodes 12, 13, and 14 are sending packets to node 15, then the routes (12 ! 1 ! 4 ! 15), (13 ! 2 ! 4 ! 15) and (14 ! 3 ! 4 ! 15) are the optimal routes for small loads. <p> Next subsection extends PQ-Routing to incorporate backward exploration into PQ-Routing to yield Dual reinforcement predictive Q-routing. Since both prediction mechanism in PQ-Routing <ref> (Choi and Yeung 1996) </ref> and backward exploration in DRQ-Routing (Kumar and Miikkulainen 1997) have been shown to improve Q-Routing separately, bringing them together into one algorithm is expected to further improve Q-Routing. 87 7.5.2 Backward Exploration in PQ-Routing PQ-Routing can be extended to include backward exploration.
Reference: <author> Floyd, R. W. </author> <year> (1962). </year> <title> Algorithm 97 (shortest path). </title> <journal> In Communications of the ACM, </journal> <volume> vol. </volume> <pages> 5(6). </pages>
Reference-contexts: These issues call for an approximate greedy solution to the problem where the routing policy adapts as routing takes place and overhead due to exploration is minimum. 1.3 Motivation for Adaptive Routing As a solution to the routing problem, first consider the simplest possible routing algorithm, the Shortest Path Algorithm <ref> (Floyd 1962) </ref>. This solution assumes that the network topology never changes, and that the best route from any source to any destination is the 6 source nodes, and node 15 is destination node. shortest path in terms of the number of hops or length (l) of the route. <p> Features that make adaptive routing a difficult problem were highlighted and different conventional non-adaptive and adaptive routing algorithms were outlined. This chapter presents the details of the conventional routing algorithms. Section 2.1 describes the non-adaptive Shortest Path Routing <ref> (Floyd 1962) </ref>. Section 2.2 discusses the Weighted Shortest Path Routing (Dijk-stra 1959) which is a generalization of shortest path routing and provides a benchmark for the algorithm developed in this thesis. It is a theoretical bound on the best possible routing one can do.
Reference: <author> Ford, L. R., and Fulkerson, D. R. </author> <year> (1962). </year> <title> Flows in Networks. </title> <publisher> Princeton, </publisher> <address> NJ: </address> <publisher> Princeton University Press. </publisher>
Reference: <author> Goetz, P., Kumar, S., and Miikkulainen, R. </author> <year> (1996). </year> <title> On-line adaptation of a signal predis-torter through dual reinforcement learning. </title> <booktitle> In Machine Learning: Proceedings of the 13th Annual Conference (Bari, </booktitle> <address> Italy). </address>
Reference-contexts: As a result, with each packet hop, two Q-value updates take place, one due to forward exploration and the other due to backward exploration. Essentially, DRQ-Routing combines Q-routing with dual reinforcement learning, which was first applied to a satellite communication problem <ref> (Goetz et al. 1996) </ref>. Section 5.2 describes this component of CDRQ-Routing in detail. As shown in figure 1.4, CDRQ-Routing combines the two components into a single adaptive routing algorithm. Experiments over several network topologies show that CQ-Routing and DRQ-routing both outperform Q-Routing.
Reference: <author> Gouda, M. </author> <year> (1998). </year> <title> Elements of Network Protocols. </title> <address> New York: </address> <publisher> John Wiley & Sons. </publisher>
Reference: <author> Huitema, C. </author> <year> (1995). </year> <title> Routing in The Internet. </title> <publisher> Prentice Hall PTR. </publisher>
Reference: <author> Kumar, S., and Miikkulainen, R. </author> <year> (1997). </year> <title> Dual reinforcement Q-routing: An on-line adaptive routing algorithm. </title> <booktitle> In Proceedings of the Artificial Neural Networks in Engineering Conference (St. </booktitle> <address> Loius, USA). </address>
Reference-contexts: The first component improves the quality of exploration and is called the Confidence-based Q-Routing. It uses confidence measures to represent reliability of the routing information (i.e. the Q-values) in the network. The second component increases the quantity of exploration, and is called the Dual Reinforcement Q-routing (DRQ-Routing) <ref> (Kumar and Miikkulai 1 nen 1997) </ref>. It uses backward exploration, an additional direction of exploration. These two components result in significant improvements in speed and quality of adaptation. The organization of the rest of this chapter is as follows. <p> With every hop of the packet P (s; d), one Q-value is updated. The properties of forward exploration are highlighted next, before the experimental results on the effectiveness of this idea are presented. In the Dual Reinforcement Q-routing <ref> (Kumar and Miikkulainen 1997) </ref> algorithm discussed in chapter 5, the other possible direction of exploration, backward exploration, is also utilized. 24 3.5 Properties of Forward Exploration Two aspects of a Q-value update rule (like that in equation 3.6) are characterized in this section. First, the update rule should be admissible. <p> Next subsection extends PQ-Routing to incorporate backward exploration into PQ-Routing to yield Dual reinforcement predictive Q-routing. Since both prediction mechanism in PQ-Routing (Choi and Yeung 1996) and backward exploration in DRQ-Routing <ref> (Kumar and Miikkulainen 1997) </ref> have been shown to improve Q-Routing separately, bringing them together into one algorithm is expected to further improve Q-Routing. 87 7.5.2 Backward Exploration in PQ-Routing PQ-Routing can be extended to include backward exploration.
Reference: <author> Littman, M., and Boyan, J. </author> <year> (1993a). </year> <title> A distributed reinforcement learning scheme for network routing. </title> <type> Technical Report CMU-CS-93-165, </type> <institution> Computer Science Department, Carnegie Mellon Universit y, </institution> <address> Pittsburgh, PA. </address>
Reference: <author> Littman, M., and Boyan, J. A. </author> <year> (1993b). </year> <title> A distributed reinforcement learning scheme for network routing. </title> <booktitle> In Proceedings of the First Internation Workshop on Applications of Neural Networks to Telecommunications, </booktitle> <pages> 45-51. </pages> <address> Hillside, New Jersy: </address> <publisher> Erlbaum. </publisher>
Reference: <author> Rajagopalan, B., and Faiman, M. </author> <year> (1989). </year> <title> A new responsive distributed shortest-path routing algorithm. </title> <booktitle> In ACM Sigcomm '89 Symposium. </booktitle>
Reference: <author> Ravindra K. Ahuja, Kurt Mehlhorn, J. B. O., and Tarjan, R. E. </author> <year> (1988). </year> <title> Fast algorithms for shortest path problem. </title> <type> Technical Report 193, </type> <institution> MIT-Operations research center. </institution> <note> 93 Sutton, </note> <author> R. </author> <year> (1988). </year> <title> Learning to predict by the method of temporal differences. </title> <journal> Machine Learning, </journal> <volume> 3(1) </volume> <pages> 9-44. </pages>
Reference: <author> Sutton, R. </author> <year> (1996). </year> <title> Generalization in reinforcement learning: Successful examples using sparse coarse coding. </title> <booktitle> In Advances in Neural Information Processing Systems 8. </booktitle> <address> Cam-bridge, MA: </address> <publisher> MIT Press. </publisher>
Reference: <author> Sutton, R. S. </author> <year> (1984). </year> <title> Temporal Credit Assignment in Reinforcement Learning. </title> <type> PhD thesis, </type> <institution> University of Massechusetts, </institution> <address> Amherst, MA. </address>
Reference: <author> Tanenbaum, A. </author> <year> (1989). </year> <title> Computer Networks. </title> <publisher> Prentice Hall. Second edition. </publisher>
Reference-contexts: The total time a packet spends in the network between its introduction at the source node and consumption at the destination node is called packet delivery time (T D ), and depends on two major factors <ref> (Tanenbaum 1989) </ref>: 1. <p> Instead a local view of the network can be maintained by each node and it can be updated as the network changes. One version of this approach is called the Distance Vector Routing <ref> (Tanenbaum 1989) </ref> that is used in modern communication network 15 systems. Bellman-Ford routing (Bellman 1957; Bellman 1958; Ford and Fulkerson 1962; C. Cheng and Garcia-Luna-Aceves 1989; Rajagopalan and Faiman 1989) is one of the most commonly used and cited adaptive distance vector routing algorithms.
Reference: <author> Thomas H. Cormen, C. E. L., and Rivest, R. L. </author> <year> (1990). </year> <title> Introduction to Algorithms. </title> <address> Cam-bridge, MA: </address> <publisher> MIT Press. </publisher>
Reference: <author> Thrun, S. B. </author> <year> (1992). </year> <title> The role of exploration in learning control. In White, </title> <editor> D. A., and Sofge, D. A., editors, </editor> <booktitle> Handbook of Intelligent Control: Neural, Fuzzy, and Adaptive Approaches. </booktitle> <address> New York: </address> <publisher> Van Nostrand Reinhold. </publisher>
Reference-contexts: In adaptive routing, however, the routing decisions of a node for a particular destination might be different at different times. The adaptation is based on exploration where routing information at each node is spread to other nodes over the network as the packets are transmitted (see <ref> (Thrun 1992) </ref> for the role of exploration 11 in learning control). Individual nodes exploit this information by updating their routing policies. 2.1 Shortest Path Routing In Shortest Path Routing (Floyd 1962; Ravindra K. Ahuja and Tarjan 1988; Thomas H.
Reference: <author> W. H. Press, S. A. Teukolsky, W. T. V. . B. P. F. </author> <year> (1995). </year> <title> Numerical Recipies in C. </title> <address> Cambridge, UK: </address> <publisher> Cambridge University Press. </publisher>
Reference-contexts: Averages over 50 test runs are shown at all loads. Statistical significance is calculated to 99% confidence using standard t-test <ref> (W. H. Press 1995) </ref>. At low loads (figure 4.2), shortest path routing is the best routing algorithm. The average packet delivery time remains at the lowest level for shortest path throughout the simulation. <p> Results averaged over 50 simulation runs, each starting with random initializations of Q values for both network topologies are reported in figures 6.1 through 6.12. Statistical significance is computed at 99% confidence using the standard student's t-test <ref> (W. H. Press 1995) </ref>.
Reference: <author> Watkins, C. J. C. H., and Dayan, P. </author> <year> (1989). </year> <title> Q-learning. </title> <journal> Machine Learning, </journal> <volume> 8 </volume> <pages> 279-292. 94 </pages>
Reference-contexts: This thesis improves the Q-Routing algorithm further by improving its quality and quantity of exploration. 8 1.4 Contributions of This Work The Q learning framework <ref> (Watkins and Dayan 1989) </ref> was used by Boyan and Littman (1994) to develop an adaptive routing algorithm called Q-Routing. Q-learning is well suited for adaptive routing as discussed above. The Q estimates are used to make decisions and these estimates are updated to reflect changes in the network. <p> The AHC converts the primary reinforcement signal into heuristic reinforcement signal which is used to change the parameters of the RL. The work of the two components of adaptive heuristic critic can be accomplished by a single component in Watkins' Q-learning algorithm <ref> (Watkins and Dayan 1989) </ref>. Q-learning is typically easier to implement. The states and the possible actions in a given state are discrete and finite in number. The model of the system is learned in terms of Q-values.
References-found: 24

