URL: ftp://ftp.cs.colorado.edu/users/baveja/Papers/Nips95.ps.gz
Refering-URL: http://www.cs.colorado.edu/~baveja/papers.html
Root-URL: 
Title: Improving Policies without Measuring Merits  
Author: Peter Dayan Satinder P Singh 
Address: Cambridge, MA 02139  
Affiliation: MIT  
Pubnum: CBCL E25-201,  
Abstract: Performing policy iteration in dynamic programming should only require knowledge of relative rather than absolute measures of the utility of actions what Baird (1993) calls the advantages of actions at states. Nevertheless, existing methods in dynamic programming (including Baird's) compute some form of absolute utility function. For smooth problems, advantages satisfy two differential consistency conditions (including the requirement that they be free of curl), and we show that enforcing these can lead to appropriate policy improvement solely in terms of advantages.
Abstract-found: 1
Intro-found: 1
Reference: <author> Athans, M & Falb, </author> <title> PL (1966). Optimal Control. </title> <address> New York, NY: </address> <publisher> McGraw-Hill. Atkeson, </publisher> <month> CG </month> <year> (1994). </year> <title> Using Local Trajectory Optimizers To Speed Up Global Optimization in Dynamic Programming. </title> <booktitle> In NIPS 6. Baird, LC, </booktitle> <month> IIIrd </month> <year> (1993). </year> <title> Advantage Updating. </title> <type> Technical report, </type> <institution> Wright Laboratory, Wright-Patterson Air Force Base. </institution>
Reference-contexts: It is well known <ref> (eg Athans & Falb, 1966) </ref> that the solution to this problem is that V fl (x) = k fl x 2 =2 where k fl = (ff + fi (u fl ) 2 )=((a + u fl )) and u (t) = (a + p Policy Iteration Using Equations 3, 4
Reference: <author> Barto, AG, Bradtke, SJ & Singh, </author> <title> SP (1991). Real-Time Learning and Control using Asynchronous Dynamic Programming. </title> <type> TR 91-57, </type> <institution> Department of Computer Science, University of Amherst, MA. Barto, AG, Sutton, RS & Watkins, </institution> <month> CJCH </month> <year> (1990). </year> <title> Learning and sequential decision making. </title> <editor> In M Gabriel & J Moore, editors, </editor> <booktitle> Learning and Computational Neuroscience: Foundations of Adaptive Networks. </booktitle> <address> Cambridge, MA: </address> <publisher> MIT Press, Bradford Books. </publisher> <editor> Bellman, </editor> <booktitle> RE (1957). Dynamic Programming. </booktitle> <address> Princeton, NJ: </address> <publisher> Princeton University Press. </publisher> <editor> Broomhead, </editor> <title> DS & Lowe, D (1988). Multivariable functional interpolation and adaptive networks. </title> <journal> Complex Systems, </journal> <volume> 2, </volume> <pages> 321-55. </pages>
Reference: <author> Dreyfus, </author> <title> SE (1965). Dynamic Programming and the Calculus of Variations. </title> <address> New York, NY: </address> <publisher> Academic Press. </publisher>
Reference-contexts: We do not treat the cases in which the infinite integrals do not converge comfortably and we will also assume adequate continuity and differentiability. The solution by advantages: This problem can be solved by writing down the Hamilton-Jacobi-Bellman (HJB) equation <ref> (see Dreyfus, 1965) </ref> which V fl (x) satisfies: 0 = min fi fl It is the continuous space/time analogue of the conventional Bellman equation (Bellman, 1957) for discrete, non-discounted, deterministic decision problems, which says that for the optimal value function V fl , 0 = min [r (x; a) + V
Reference: <author> Howard, </author> <title> RA (1960). Dynamic Programming and Markov Processes. </title> <address> New York, NY: </address> <publisher> Technology Press & Wiley. </publisher>
Reference-contexts: Equation 1 describes what the optimal value function must satisfy. Discrete dynamic programming also comes with a method called value iteration which starts with any function V 0 (x), improves it sequentially, and converges to the optimum. The alternative method, policy iteration <ref> (Howard, 1960) </ref>, operates in the space of policies, ie functions w (x).
Reference: <author> Mitchell, </author> <title> TM & Thrun, SB (1993). Explanation-based neural network learning for robot control. </title> <booktitle> In NIPS 5. </booktitle>
Reference: <author> Mussa-Ivaldi, </author> <title> FA (1992). From basis functions to basis fields: Vector field approximation from sparse data. </title> <journal> Biological Cybernetics, </journal> <volume> 67, </volume> <pages> 479-489. </pages>
Reference: <author> Peterson, </author> <month> JK </month> <year> (1993). </year> <title> On-Line estimation of optimal value functions. </title> <booktitle> In NIPS 5. </booktitle>
Reference: <author> Poggio, </author> <title> T & Girosi, F (1990). A theory of networks for learning. </title> <journal> Science, </journal> <volume> 247, </volume> <pages> 978-982. </pages> <editor> Sutton, </editor> <title> RS (1988). Learning to predict by the methods of temporal difference. </title> <journal> Machine Learning, </journal> <volume> 3, </volume> <pages> pp 9-44. Watkins, </pages> <month> CJCH </month> <year> (1989). </year> <title> Learning from Delayed Rewards. </title> <type> PhD Thesis. </type> <institution> University of Cambridge, </institution> <address> England. </address>
References-found: 8

