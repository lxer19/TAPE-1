URL: http://www.gia.ist.utl.pt/~pedrod/kdd97.ps.gz
Refering-URL: http://www.gia.ist.utl.pt/~pedrod/
Root-URL: 
Email: pedrod@ics.uci.edu  
Title: Why Does Bagging Work? A Bayesian Account and its Implications bagging's success, both in a
Author: Pedro Domingos ; 
Keyword: Bagging  
Note: Two related explanations have been proposed for  
Web: http://www.ics.uci.edu/~pedrod  
Address: Irvine, California 92697, U.S.A.  
Affiliation: Department of Information and Computer Science University of California, Irvine  
Abstract: The error rate of decision-tree and other classification learners can often be much reduced by bagging: learning multiple models from bootstrap samples of the database, and combining them by uniform voting. In this paper we empirically test two alternative explanations for this, both based on Bayesian learning theory: (1) bagging works because it is an approximation to the optimal procedure of Bayesian model averaging, with an appropriate implicit prior; (2) bagging works because it effectively shifts the prior to a more appropriate region of model space. All the experimental evidence contradicts the first hypothesis, and confirms the second. Bagging (Breiman 1996a) is a simple and effective way to reduce the error rate of many classification learning algorithms. For example, in the empirical study described below, it reduces the error of a decision-tree learner in 19 of 26 databases, by 4% on average. In the bagging procedure, given a training set of size s, a "bootstrap" replicate of it is constructed by taking s samples with replacement from the training set. Thus a new training set of the same size is produced, where each of the original examples may appear once, more than once, or not. On average, 63% of the original examples will appear in the bootstrap sample. The learning algorithm is then applied to this training set. This procedure is repeated m times, and the resulting m models are aggregated by uniform voting. Bagging is one of several "multiple model" approaches that have recently received much attention (see, for example, (Chan, Stolfo, & Wolpert 1996)). Other procedures of this type include boosting (Freund & Schapire 1996) and stacking (Wolpert 1992). 
Abstract-found: 1
Intro-found: 1
Reference: <author> Bernardo, J. M., and Smith, A. F. L. </author> <year> 1994. </year> <title> Bayesian Theory. </title> <address> New York, NY: </address> <publisher> Wiley. </publisher>
Reference: <author> Breiman, L. </author> <year> 1996a. </year> <title> Bagging predictors. </title> <booktitle> Machine Learning 24 </booktitle> <pages> 123-140. </pages>
Reference: <author> Breiman, L. </author> <year> 1996b. </year> <title> Bias, variance and arcing classifiers. </title> <type> Technical Report 460, </type> <institution> Statistics Department, University of California at Berkeley, Berkeley, </institution> <address> CA. </address>
Reference: <author> Buntine, W. L. </author> <year> 1990. </year> <title> A Theory of Learning Classification Rules. </title> <type> Ph.D. Dissertation, </type> <institution> School of Computing Science, University of Technology, </institution> <address> Sydney, Australia. </address>
Reference-contexts: Equation 1 then becomes: P r (hj~x; ~c) / P r (h) (1 *) s * ns (2) where s is the number of examples correctly classified by h. An alternative approach <ref> (Buntine 1990) </ref> relies on the fact that, implicitly or explicitly, a classification model divides the instance space into regions, and labels each region with a class. For example, if the model is a decision tree (Quinlan 1993), each leaf corresponds to a region.
Reference: <author> Chan, P.; Stolfo, S.; and Wolpert, D., eds. </author> <year> 1996. </year> <booktitle> Proc. AAAI-96 Workshop on Integrating Multiple Learned Models. </booktitle> <address> Portland, OR: </address> <publisher> AAAI Press. </publisher>
Reference: <author> Cheeseman, P. </author> <year> 1990. </year> <title> On finding the most probable model. </title> <editor> In Shrager, J., and Langley, P., eds., </editor> <title> Computational Models of Scientific Discovery and Theory Formation. </title> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: This use of training-set information in the prior is not strictly allowed by Bayesian theory, but is nevertheless common <ref> (Cheeseman 1990) </ref>. Although counter-intuitive, penalizing models that have lower error on the training data simply corresponds to an assumption that the models overfit the data, or more precisely, that the models that have lower error on the training data will in fact haver higher error on test data.
Reference: <author> Domingos, P. </author> <year> 1997. </year> <title> Knowledge acquisition from examples via multiple models. </title> <booktitle> In Proc. Fourteenth International Conference on Machine Learning. </booktitle> <address> Nashville, TN: </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: This "meta-learning" procedure was carried out for the 26 databases previously mentioned, using C4.5 as before. Details and full results are given elsewhere <ref> (Domingos 1997) </ref>. In all but four of the 22 databases where bagging improves on the single rule set, meta-learning also produces a rule set with lower error, with over 99% confidence according to sign and Wilcoxon tests.
Reference: <author> Freund, Y., and Schapire, R. E. </author> <year> 1996. </year> <title> Experiments with a new boosting algorithm. </title> <booktitle> In Proc. Thirteenth International Conference on Machine Learning, </booktitle> <pages> 148-156. </pages> <address> Bari, Italy: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Friedman, J. H. </author> <year> 1996. </year> <title> On bias, variance, 0/1 loss, and the curse-of-dimensionality. </title> <type> Technical report, </type> <institution> Department of Statistics and Stanford Linear Accelerator Center, Stanford University, Stanford, </institution> <address> CA. </address>
Reference: <author> Hyafil, L., and Rivest, R. L. </author> <year> 1976. </year> <title> Constructing optimal binary decision trees is NP-complete. </title> <journal> Information Processing Letters 5 </journal> <pages> 15-17. </pages>
Reference-contexts: Rather, we would like to find the simplest decision tree extensionally representing the same model as a bagged ensemble, and compare its complexity with that of the single tree induced from the whole training set. Although this is likely to be an NP-complete problem <ref> (Hyafil & Rivest 1976) </ref>, an approximation to this approach can be obtained by simply applying the base learner to a training set composed of a large number of examples generated at random, and classified according to the bagged ensemble.
Reference: <author> Kohavi, R., and Wolpert, D. H. </author> <year> 1996. </year> <title> Bias plus variance decomposition for zero-one loss functions. </title> <booktitle> In Proc. Thirteenth International Conference on Machine Learning, </booktitle> <pages> 275-283. </pages> <address> Bari, Italy: </address> <publisher> Morgan Kauf-mann. </publisher>
Reference: <author> Kong, E. B., and Dietterich, T. G. </author> <year> 1995. </year> <title> Error-correcting output coding corrects bias and variance. </title> <booktitle> In Proc. Twelfth International Conference on Machine Learning, </booktitle> <pages> 313-321. </pages> <address> Tahoe City, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Madigan, D.; Raftery, A. E.; Volinsky, C. T.; and Hoeting, J. A. </author> <year> 1996. </year> <title> Bayesian model averaging. </title> <booktitle> In Proc. AAAI-96 Workshop on Integrating Multiple Learned Models, </booktitle> <pages> 77-83. </pages> <address> Portland, OR: </address> <publisher> AAAI Press. </publisher>
Reference-contexts: Since P r (hj~x; ~c) is often very peaked, using only the model with highest posterior can be an acceptable approximation. Alternatively, a sampling scheme (e.g., Markov chain Monte Carlo <ref> (Madigan et al. 1996) </ref>) can be used. Empirical Tests of the First Hypothesis This section empirically tests the following hypothesis: 1. Bagging reduces a classification learner's error rate because it more closely approximates Equation 4 than the single model output by the learner.
Reference: <author> Merz, C. J.; Murphy, P. M.; and Aha, D. W. </author> <year> 1997. </year> <title> UCI repository of machine learning databases. </title> <institution> Department of Information and Computer Science, University of California at Irvine, </institution> <address> Irvine, CA. </address>
Reference-contexts: Both these variants of Hypothesis 1 were tested. A decision-tree learner, C4.5 release 8 (Quinlan 1993), was used in all experiments. 3 Twenty-six databases from the UCI repository were used 4 <ref> (Merz, Murphy, & Aha 1997) </ref>. Hypothesis 1a was tested by comparing bagging's error with that obtained by weighing the models according to Equation 1, using both a uniform class noise model (Equation 2) and Equation 3. Equation 4 was used in both the "pure classification" and "class probability" forms described.
Reference: <author> Quinlan, J. R. </author> <year> 1993. </year> <title> C4.5: Programs for Machine Learning. </title> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: An alternative approach (Buntine 1990) relies on the fact that, implicitly or explicitly, a classification model divides the instance space into regions, and labels each region with a class. For example, if the model is a decision tree <ref> (Quinlan 1993) </ref>, each leaf corresponds to a region. <p> Alternatively, bagging may be regarded as assuming a prior probability distribution that approximately cancels the likelihood for these models (Hypothesis 1b). Both these variants of Hypothesis 1 were tested. A decision-tree learner, C4.5 release 8 <ref> (Quinlan 1993) </ref>, was used in all experiments. 3 Twenty-six databases from the UCI repository were used 4 (Merz, Murphy, & Aha 1997).
Reference: <author> Wolpert, D. </author> <year> 1992. </year> <title> Stacked generalization. </title> <booktitle> Neural Networks 5 </booktitle> <pages> 241-259. </pages>
References-found: 16

