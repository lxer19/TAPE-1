URL: http://www-ai.cs.uni-dortmund.de/DOKUMENTE/Joachims_97b.ps.gz
Refering-URL: http://www-ai.informatik.uni-dortmund.de/PERSONAL/joachims.html
Root-URL: 
Title: K unstliche Intelligenz Text Categorization with Support Vector Machines: Learning with Many Relevant Features  
Author: Thorsten Joachims 
Date: Dortmund, 27. November, 1997 Revised: 19. April, 1998  
Note: VIII  
Affiliation: UNIVERSIT AT DORTMUND Fachbereich Informatik Lehrstuhl  Universitat Dortmund Fachbereich Informatik University of Dortmund Computer Science Department  
Pubnum: LS-8 Report 23  
Abstract-found: 0
Intro-found: 1
Reference: <author> 7 Acknowledgements Many thanks to my advisor Prof. Morik, Ralf Klinkenberg, </author> <title> and Marc Craven for comments on this paper. Thanks also to Ken Lang for providing some of the code. References </title>
Reference: [Balabanovic and Shoham, 1995] <author> Balabanovic, M. and Shoham, Y. </author> <year> (1995). </year> <title> Learning information retrieval agents: Experiments with automated web browsing. </title> <booktitle> In Working Notes of the AAAI Spring Symposium Series on Information Gathering from Distributed, Heterogeneous Environments. </booktitle> <address> AAAI-Press. </address>
Reference-contexts: 1 Introduction With the rapid growth of online information, text categorization has become one of the key techniques for handling and organizing text data. Text categorization is used to classify news stories [Hayes and Weinstein, 1990] [Masand et al., 1992], to find interesting information on the WWW [Lang, 1995] <ref> [Balabanovic and Shoham, 1995] </ref>, and to guide a users search through hypertext [Joachims et al., 1997]. Since building text classifiers by hand is difficult and time consuming, it is desirable to learn classifiers from examples.
Reference: [Boser et al., 1992] <author> Boser, B., Guyon, M., and Vapnik, V. </author> <year> (1992). </year> <title> A training algorithm for optimal margin classifiers. </title> <booktitle> In Conference on Computational Learning Theory (COLT), </booktitle> <pages> pages 144-152. </pages>
Reference-contexts: In this paper I will explore and identify the benefits of Support Vector Machines (SVMs) for text categorization. SVMs are a new learning method introduced by V. Vap-nik [Vapnik, 1995] [Cortes and Vapnik, 1995] <ref> [Boser et al., 1992] </ref>. They are well founded in terms of computational learning theory and very open to theoretical understanding and analysis. After reviewing the standard feature vector representation of text (section 2.1), I will identify the particular properties of text in this representation in section 2.3.
Reference: [Burges and Scholkopf, 1997] <author> Burges, C. and Scholkopf, B. </author> <year> (1997). </year> <title> Improving the accuracy and speed of support vector machines. </title> <booktitle> In Neural Information Processing Systems, </booktitle> <volume> volume 9. </volume>
Reference-contexts: Comparing training time, SVMs are roughly comparable to C4.5, but they are more expensive than naive Bayes, Rocchio, and k-NN. Nevertheless, current research is likely to improve efficiency of SVM-type quadratic programming problems. SVMs are faster than k-NN at classification time, especially when using the reduced set <ref> [Burges and Scholkopf, 1997] </ref> method. 6 Conclusions This paper introduces support vector machines for text categorization. It provides both theoretical and empirical evidence that SVMs are very well suited for text categorization.
Reference: [Cortes and Vapnik, 1995] <author> Cortes, C. and Vapnik, V. </author> <year> (1995). </year> <title> Support-vector networks. </title> <journal> Machine Learning, </journal> <volume> 20 </volume> <pages> 273-297. </pages>
Reference-contexts: Since building text classifiers by hand is difficult and time consuming, it is desirable to learn classifiers from examples. In this paper I will explore and identify the benefits of Support Vector Machines (SVMs) for text categorization. SVMs are a new learning method introduced by V. Vap-nik [Vapnik, 1995] <ref> [Cortes and Vapnik, 1995] </ref> [Boser et al., 1992]. They are well founded in terms of computational learning theory and very open to theoretical understanding and analysis. <p> What if this is not possible for the chosen hypothesis space? Cortes and Vapnik <ref> [Cortes and Vapnik, 1995] </ref> suggest the introduction of slack variables. In this paper a simpler approach is taken. During the optimization of (9) the values of the coefficients ff i are monitored. Training examples with high ff i "contribute a lot to the inseparability" of the data.
Reference: [Hayes and Weinstein, 1990] <author> Hayes, P. and Weinstein, S. </author> <year> (1990). </year> <title> Construe/tis: a system for content-based indexing of a database of news stories. </title> <booktitle> In Annual Conference on Innovative Applications of AI. </booktitle>
Reference-contexts: 1 Introduction With the rapid growth of online information, text categorization has become one of the key techniques for handling and organizing text data. Text categorization is used to classify news stories <ref> [Hayes and Weinstein, 1990] </ref> [Masand et al., 1992], to find interesting information on the WWW [Lang, 1995] [Balabanovic and Shoham, 1995], and to guide a users search through hypertext [Joachims et al., 1997].
Reference: [Joachims, 1997] <author> Joachims, T. </author> <year> (1997). </year> <title> A probabilistic analysis of the rocchio algorithm with tfidf for text categorization. </title> <booktitle> In International Conference on Machine Learning (ICML). </booktitle>
Reference-contexts: With perfect knowledge of Pr (+jd 0 ) the optimum performance is achieved when d 0 is assigned to class + iff Pr (+jd 0 ) 0:5 (Bayes' rule). Using a unigram model of text leads to the following estimate of Pr (+jd 0 ) (see <ref> [Joachims, 1997] </ref>): Pr (+jd 0 ) = Q Pr (+) i Pr (w i j+) T F (w i ;d 0 ) + Pr () i Pr (w i j) T F (w i ;d 0 ) (18) 8 5 EXPERIMENTS The probabilities P (+) and P () can be estimated <p> For Pr (w i j+) and Pr (w i j) the so called Laplace estimator is used <ref> [Joachims, 1997] </ref>. 4.2 Rocchio Algorithm This type of classifier is based on the relevance feedback algorithm originally proposed by Rocchio [Rocchio, 1971] for the vector space retrieval model [Salton, 1991]. It has been extensively used for text classification.
Reference: [Joachims et al., 1997] <author> Joachims, T., Freitag, D., and Mitchell, T. </author> <year> (1997). </year> <title> Webwatcher: A tour guide for the world wide web. </title> <booktitle> In International Joint Conference on Artificial Intelligence (IJCAI). </booktitle>
Reference-contexts: Text categorization is used to classify news stories [Hayes and Weinstein, 1990] [Masand et al., 1992], to find interesting information on the WWW [Lang, 1995] [Balabanovic and Shoham, 1995], and to guide a users search through hypertext <ref> [Joachims et al., 1997] </ref>. Since building text classifiers by hand is difficult and time consuming, it is desirable to learn classifiers from examples. In this paper I will explore and identify the benefits of Support Vector Machines (SVMs) for text categorization. SVMs are a new learning method introduced by V.
Reference: [Kivinen et al., 1995] <author> Kivinen, J., Warmuth, M., and Auer, P. </author> <year> (1995). </year> <title> The perceptron algorithm vs. winnow: Linear vs. logarithmic mistake bounds when few input variables are relevant. </title> <booktitle> In Conference on Computational Learning Theory. </booktitle>
Reference-contexts: Document vectors are sparse: For each document d i , the corresponding document vector ~ d i contains only few entries which are not zero. Kivinen et al. <ref> [Kivinen et al., 1995] </ref> give both theoretical and empirical evidence for the mistake bound model that "additive" algorithms, which have a similar inductive bias like SVMs, are well suited for problems with dense concepts and sparse instances.
Reference: [Lang, 1995] <author> Lang, K. </author> <year> (1995). </year> <title> Newsweeder: Learning to filter netnews. </title> <booktitle> In International Conference on Machine Learning (ICML). </booktitle>
Reference-contexts: 1 Introduction With the rapid growth of online information, text categorization has become one of the key techniques for handling and organizing text data. Text categorization is used to classify news stories [Hayes and Weinstein, 1990] [Masand et al., 1992], to find interesting information on the WWW <ref> [Lang, 1995] </ref> [Balabanovic and Shoham, 1995], and to guide a users search through hypertext [Joachims et al., 1997]. Since building text classifiers by hand is difficult and time consuming, it is desirable to learn classifiers from examples.
Reference: [Lewis and Ringuette, 1994] <author> Lewis, D. and Ringuette, M. </author> <year> (1994). </year> <title> A comparison of two learning algorithms for text classification. </title> <booktitle> In Third Annual Symposium on Document Analysis and Information Retrieval, </booktitle> <pages> pages 81-93. </pages>
Reference-contexts: It is used with the default parameter settings and with rule post-pruning turned on. C4.5 outputs a confidence value when classifying new examples. This value is used to compute precision/recall tables (see section 5.2). Previous results with decision tree or rule learning algorithms are reported in <ref> [Lewis and Ringuette, 1994] </ref> [Moulinier et al., 1996]. 5 Experiments The following experiments compare the performance of SVMs using polynomial and RBF convolution operators with the four conventional learning methods. 5.1 Test Collections 9 5.1 Test Collections The empirical evaluation is done on two test collection.
Reference: [Masand et al., 1992] <author> Masand, B., Linoff, G., and Waltz, D. </author> <year> (1992). </year> <title> Classifying news stories using memory based reasoning. </title> <booktitle> In International ACM SIGIR Conference on Research and Development in Information Retrieval, </booktitle> <pages> pages 59-65. REFERENCES 13 </pages>
Reference-contexts: 1 Introduction With the rapid growth of online information, text categorization has become one of the key techniques for handling and organizing text data. Text categorization is used to classify news stories [Hayes and Weinstein, 1990] <ref> [Masand et al., 1992] </ref>, to find interesting information on the WWW [Lang, 1995] [Balabanovic and Shoham, 1995], and to guide a users search through hypertext [Joachims et al., 1997]. Since building text classifiers by hand is difficult and time consuming, it is desirable to learn classifiers from examples. <p> Using an appropriate threshold on the cosine leads to a binary classification rule. 4.3 k-Nearest Neighbors k-nearest neighbor (k-NN) classifiers were found to show very good performance on text categorization tasks [Yang, 1997] <ref> [Masand et al., 1992] </ref>. This paper follows the setup in [Yang, 1997]. The cosine is used as a similarity metric. knn (d 0 ) denotes the indexes of the k documents which have the highest cosine with the document to classify d 0 .
Reference: [Mitchell, 1997] <author> Mitchell, T. </author> <year> (1997). </year> <title> Machine Learning. </title> <publisher> McGraw-Hill. </publisher>
Reference-contexts: H knn (d 0 ) = sign ( P y i cos (d 0 ; d i ) i2knn (d 0 ) ) (20) Further details can be found in <ref> [Mitchell, 1997] </ref>. 4.4 Decision Tree Classifier The C4.5 [Quinlan, 1993] decision tree algorithm is used for the experiments in this paper. It is the most popular decision tree algorithm and has shown good results on a variety of problem.
Reference: [Moulinier et al., 1996] <author> Moulinier, I., Raskinis, G., and Ganascia, J. </author> <year> (1996). </year> <title> Text categorization: A symbolic approach. </title> <booktitle> In Annual Symposium on Document Analysis and Information Retrieval (SDAIR). </booktitle>
Reference-contexts: C4.5 outputs a confidence value when classifying new examples. This value is used to compute precision/recall tables (see section 5.2). Previous results with decision tree or rule learning algorithms are reported in [Lewis and Ringuette, 1994] <ref> [Moulinier et al., 1996] </ref>. 5 Experiments The following experiments compare the performance of SVMs using polynomial and RBF convolution operators with the four conventional learning methods. 5.1 Test Collections 9 5.1 Test Collections The empirical evaluation is done on two test collection.
Reference: [Osuna et al., 1997] <author> Osuna, E., Freund, R., and Girosi, F. </author> <year> (1997). </year> <title> An improved training algorithm for support vector machines. </title> <booktitle> In IEEE Workshop on Neural Networks for Signal Processing (NNSP). </booktitle>
Reference-contexts: For the polynomial convolution this is the degree d, for RBFs it is the variance fl, etc. How can we pick appropriate values for these parameters automatically? The following procedure [Vapnik, 2 For the experiments in this paper a refined version of the algorithm in <ref> [Osuna et al., 1997] </ref> is used. It can efficiently handle problems with many thousand support vectors, converges fast, and has minimal memory requirements. 3.3 Non-Separable Problems 7 1995] can be used, which is again inspired by bound (2).
Reference: [Porter, 1980] <author> Porter, M. </author> <year> (1980). </year> <title> An algorithm for suffix stripping. Program (Automated Library and Information Systems), </title> <booktitle> 14(3) </booktitle> <pages> 130-137. </pages>
Reference-contexts: IR research suggests that word stems work well as representation units and that their ordering in a document is of minor importance for many tasks. The word stem is derived from the occurrence form of a word by removing case and flection information <ref> [Porter, 1980] </ref>. For example "computes", "computing", and "computer" are all mapped to the same stem "comput". The terms "word" and "word stem" will be used synonymously in the following. This leads to an attribute-value representation of text.
Reference: [Quinlan, 1993] <author> Quinlan, J. R. </author> <year> (1993). </year> <title> C4.5: Programs for Machine Learning. </title> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: H knn (d 0 ) = sign ( P y i cos (d 0 ; d i ) i2knn (d 0 ) ) (20) Further details can be found in [Mitchell, 1997]. 4.4 Decision Tree Classifier The C4.5 <ref> [Quinlan, 1993] </ref> decision tree algorithm is used for the experiments in this paper. It is the most popular decision tree algorithm and has shown good results on a variety of problem. It is used with the default parameter settings and with rule post-pruning turned on.
Reference: [Raghavan et al., 1989] <author> Raghavan, V., Bollmann, P., and Jung, G. </author> <year> (1989). </year> <title> A critical investigation of recall and precision as measures of retrieval system performance. </title> <journal> ACM Transactions on Information Systems, </journal> <volume> 7(3) </volume> <pages> 205-229. </pages>
Reference-contexts: Between high recall and high precision exists a trade-off. All methods examined in this paper make category assignments by thresholding a "confidence value". By adjusting this threshold we can achieve different levels of recall and precision. The PRR method <ref> [Raghavan et al., 1989] </ref> is used for interpolation. Since precision and recall are defined only for binary classification tasks, the results of multiple binary tasks need to be averaged to get to a single performance value for multiple class problems. This will be done using microaveraging [Yang, 1997].
Reference: [Rocchio, 1971] <author> Rocchio, J. </author> <year> (1971). </year> <title> Relevance feedback in information retrieval. </title> <editor> In Salton, G., editor, </editor> <booktitle> The SMART Retrieval System: Experiments in Automatic Document Processing, </booktitle> <pages> pages 313-323. </pages> <publisher> Prentice-Hall Inc. </publisher>
Reference-contexts: For Pr (w i j+) and Pr (w i j) the so called Laplace estimator is used [Joachims, 1997]. 4.2 Rocchio Algorithm This type of classifier is based on the relevance feedback algorithm originally proposed by Rocchio <ref> [Rocchio, 1971] </ref> for the vector space retrieval model [Salton, 1991]. It has been extensively used for text classification. First, both the normalized document vectors of the positive examples as well as those of the negative examples are summed up.
Reference: [Salton, 1991] <author> Salton, G. </author> <year> (1991). </year> <title> Developments in automatic text retrieval. </title> <journal> Science, </journal> <volume> 253 </volume> <pages> 974-979. </pages>
Reference-contexts: For Pr (w i j+) and Pr (w i j) the so called Laplace estimator is used [Joachims, 1997]. 4.2 Rocchio Algorithm This type of classifier is based on the relevance feedback algorithm originally proposed by Rocchio [Rocchio, 1971] for the vector space retrieval model <ref> [Salton, 1991] </ref>. It has been extensively used for text classification. First, both the normalized document vectors of the positive examples as well as those of the negative examples are summed up.
Reference: [Salton and Buckley, 1988] <author> Salton, G. and Buckley, C. </author> <year> (1988). </year> <title> Term weighting approaches in automatic text retrieval. </title> <booktitle> Information Processing and Management, </booktitle> <volume> 24(5) </volume> <pages> 513-523. </pages>
Reference-contexts: Based on this basic representation it is known that scaling the dimensions of the feature vector with their inverse document frequency IDF (w i ) <ref> [Salton and Buckley, 1988] </ref> leads to an improved performance. IDF (w i ) can be calculated from the document frequency DF (w i ), which is the number of documents the word w i occurs in.
Reference: [Sch-utze et al., 1995] <author> Sch-utze, H., Hull, D., and Pedersen, J. </author> <year> (1995). </year> <title> A comparison of classifiers and document representations for the routing problem. </title> <booktitle> In International ACM SIGIR Conference on Research and Development in Information Retrieval. </booktitle>
Reference-contexts: The most popular approach to feature selection is to select a subset of the available features using methods like DF-thresholding [Yang and Pedersen, 1997], the 2 -test <ref> [Sch-utze et al., 1995] </ref>, or the term strength criterion [Yang and Wilbur, 1996]. The most commonly used and often most effective [Yang and Pedersen, 1997] method for selecting features is the information gain criterion. It will be used in this paper following the setup in [Yang and Pedersen, 1997].
Reference: [Shawe-Taylor et al., 1996] <author> Shawe-Taylor, J., Bartlett, P., Williamson, R., and Anthony, M. </author> <year> (1996). </year> <title> Structural risk minimization over data-dependent hierarchies. </title> <type> Technical Report NC-TR-96-053, </type> <institution> NeuroCOLT. </institution>
Reference-contexts: The constraints (8) require that all training examples are classified correctly. We can use the lemma from above to draw conclusions about the VCdim of the structure element that the separating hyperplane comes from. A bound similar to (2) <ref> [Shawe-Taylor et al., 1996] </ref> gives us a bound on the true error of this hyperplane on our classification task. Since the optimization problem from above is difficult to handle numerically, Lagrange multipliers are used to translate the problem into an equivalent quadratic optimization problem [Vapnik, 1995].
Reference: [Vapnik, 1982] <author> Vapnik, V. </author> <year> (1982). </year> <title> Estimation of Dependencies Based on Empirical Data. </title> <booktitle> Springer Series in Statistics. </booktitle> <publisher> Springer-Verlag. </publisher>
Reference-contexts: The examples closest to the hyperplane are called Support Vectors (marked with circles). Lemma 1. <ref> [Vapnik, 1982] </ref> Consider hyperplanes h ( ~ d) = signf ~w ~ d + bg as hypotheses.
Reference: [Vapnik, 1995] <author> Vapnik, V. </author> <year> (1995). </year> <title> The Nature of Statistical Learning Theory. </title> <publisher> Springer, </publisher> <address> New York. </address>
Reference-contexts: Since building text classifiers by hand is difficult and time consuming, it is desirable to learn classifiers from examples. In this paper I will explore and identify the benefits of Support Vector Machines (SVMs) for text categorization. SVMs are a new learning method introduced by V. Vap-nik <ref> [Vapnik, 1995] </ref> [Cortes and Vapnik, 1995] [Boser et al., 1992]. They are well founded in terms of computational learning theory and very open to theoretical understanding and analysis. <p> The idea of SVMs is to find such linear (or polynomial, RBF, etc.) separators. These arguments give evidence that SVMs should perform well for text categorization. 3 Support Vector Machines Support vector machines are based on the Structural Risk Minimization principle <ref> [Vapnik, 1995] </ref> from computational learning theory. The idea of structural risk minimization is to find a hypothesis h for which we can guarantee the lowest true error. The true error of h is the probability that h will make an error on an unseen and randomly selected test example. <p> The true error of h is the probability that h will make an error on an unseen and randomly selected test example. The following upper bound connects the true error of a hypothesis h with the error of h on the training set and the complexity of h <ref> [Vapnik, 1995] </ref>. P (error (h)) train error (h) + 2 s d + 1) ln n The bound holds with probability at least 1 . n denotes the number of training examples and d is the VC-Dimension (VCdim) [Vapnik, 1995], which is a property of the hypothesis space and indicates its <p> error of h on the training set and the complexity of h <ref> [Vapnik, 1995] </ref>. P (error (h)) train error (h) + 2 s d + 1) ln n The bound holds with probability at least 1 . n denotes the number of training examples and d is the VC-Dimension (VCdim) [Vapnik, 1995], which is a property of the hypothesis space and indicates its expressiveness. Equation (2) reflects the well known trade-off between the complexity of the hypothesis space and the training error. <p> A bound similar to (2) [Shawe-Taylor et al., 1996] gives us a bound on the true error of this hyperplane on our classification task. Since the optimization problem from above is difficult to handle numerically, Lagrange multipliers are used to translate the problem into an equivalent quadratic optimization problem <ref> [Vapnik, 1995] </ref>. <p> ~ d 2 ) = exp (fl ( ~ d 1 ~ d 2 ) 2 ) (13) K sigmoid ( ~ d 1 ; ~ d 2 ) = tanh (s ( ~ d 1 ~ d 2 ) + c) (14) These convolution functions satisfy Mercer's Theorem (see <ref> [Vapnik, 1995] </ref>).
Reference: [Wiener et al., 1995] <author> Wiener, E., Pedersen, J., and Weigend, A. </author> <year> (1995). </year> <title> A neural network approach to topic spotting. </title> <booktitle> In Annual Symposium on Document Analysis and Information Retrieval (SDAIR). </booktitle>
Reference-contexts: Note that there may be multiple breakeven points or none at all. In the case of multiple breakeven points, the lowest one is selected. In case of no breakeven 3 Since cosine similarities are not comparable across classes, the method of proportional assignment <ref> [Wiener et al., 1995] </ref> is used for the Rocchio algorithm to come up with improved confidence values. 10 5 EXPERIMENTS SVM (poly) SVM (rbf) d = fl = Bayes Rocchio C4.5 k-NN 1 2 3 4 5 0.6 0.8 1.0 1.2 earn 95.9 96.1 96.1 97.3 98.2 98.4 98.5 98.4 98.3
Reference: [Yang, 1997] <author> Yang, Y. </author> <year> (1997). </year> <title> An evaluation of statistical approaches to text categorization. </title> <type> Technical Report CMU-CS-97-127, </type> <institution> Carnegie Mellon University. </institution> <address> 14 REFERENCES </address>
Reference-contexts: To classify a new document d 0 , the cosine between ~w and ~ d 0 is computed. Using an appropriate threshold on the cosine leads to a binary classification rule. 4.3 k-Nearest Neighbors k-nearest neighbor (k-NN) classifiers were found to show very good performance on text categorization tasks <ref> [Yang, 1997] </ref> [Masand et al., 1992]. This paper follows the setup in [Yang, 1997]. The cosine is used as a similarity metric. knn (d 0 ) denotes the indexes of the k documents which have the highest cosine with the document to classify d 0 . <p> Using an appropriate threshold on the cosine leads to a binary classification rule. 4.3 k-Nearest Neighbors k-nearest neighbor (k-NN) classifiers were found to show very good performance on text categorization tasks <ref> [Yang, 1997] </ref> [Masand et al., 1992]. This paper follows the setup in [Yang, 1997]. The cosine is used as a similarity metric. knn (d 0 ) denotes the indexes of the k documents which have the highest cosine with the document to classify d 0 . <p> The PRR method [Raghavan et al., 1989] is used for interpolation. Since precision and recall are defined only for binary classification tasks, the results of multiple binary tasks need to be averaged to get to a single performance value for multiple class problems. This will be done using microaveraging <ref> [Yang, 1997] </ref>. In our setting this results in the following procedure. The classification threshold fi is lowered simultaneously over all binary tasks 3 . At each value of fi the microaveraged precision and recall are computed based on the merged contingency table. <p> The results for the parameters with the best performance on the test set are reported. On the Reuters data the k-NN classifier performs best among the conventional methods (see figure 4). This replicates the findings of <ref> [Yang, 1997] </ref>. Slightly worse perform the decision tree method and the Rocchio algorithm. The naive Bayes classifier shows the worst results. Compared to the conventional methods all SVMs perform better independent of the choice of parameters.
Reference: [Yang and Pedersen, 1997] <author> Yang, Y. and Pedersen, J. </author> <year> (1997). </year> <title> A comparative study on feature selection in text categorization. </title> <booktitle> In International Conference on Machine Learning (ICML). </booktitle>
Reference-contexts: Many have noted the need for feature selection to make the use of conventional learning methods possible, to improve generalization accuracy, and to avoid "overfitting" (e.g. <ref> [Yang and Pedersen, 1997] </ref>[Moulinier et al., 1996]). The most popular approach to feature selection is to select a subset of the available features using methods like DF-thresholding [Yang and Pedersen, 1997], the 2 -test [Sch-utze et al., 1995], or the term strength criterion [Yang and Wilbur, 1996]. <p> the need for feature selection to make the use of conventional learning methods possible, to improve generalization accuracy, and to avoid "overfitting" (e.g. <ref> [Yang and Pedersen, 1997] </ref>[Moulinier et al., 1996]). The most popular approach to feature selection is to select a subset of the available features using methods like DF-thresholding [Yang and Pedersen, 1997], the 2 -test [Sch-utze et al., 1995], or the term strength criterion [Yang and Wilbur, 1996]. The most commonly used and often most effective [Yang and Pedersen, 1997] method for selecting features is the information gain criterion. <p> The most popular approach to feature selection is to select a subset of the available features using methods like DF-thresholding <ref> [Yang and Pedersen, 1997] </ref>, the 2 -test [Sch-utze et al., 1995], or the term strength criterion [Yang and Wilbur, 1996]. The most commonly used and often most effective [Yang and Pedersen, 1997] method for selecting features is the information gain criterion. It will be used in this paper following the setup in [Yang and Pedersen, 1997]. All words are ranked according to their information gain. <p> The most commonly used and often most effective <ref> [Yang and Pedersen, 1997] </ref> method for selecting features is the information gain criterion. It will be used in this paper following the setup in [Yang and Pedersen, 1997]. All words are ranked according to their information gain. To select a subset of f features, the f words with the highest mutual information are chosen.
Reference: [Yang and Wilbur, 1996] <author> Yang, Y. and Wilbur, J. </author> <year> (1996). </year> <title> Using corpus statistics to remove redundant words in text categorization. </title> <journal> Journal of the American Society for Information Science, </journal> <volume> 47(5) </volume> <pages> 357-369. </pages>
Reference-contexts: The most popular approach to feature selection is to select a subset of the available features using methods like DF-thresholding [Yang and Pedersen, 1997], the 2 -test [Sch-utze et al., 1995], or the term strength criterion <ref> [Yang and Wilbur, 1996] </ref>. The most commonly used and often most effective [Yang and Pedersen, 1997] method for selecting features is the information gain criterion. It will be used in this paper following the setup in [Yang and Pedersen, 1997]. All words are ranked according to their information gain.
References-found: 29

