URL: http://charm.cs.uiuc.edu/version2/papers/SanjeevThesis.ps
Refering-URL: http://charm.cs.uiuc.edu/version2/papers/SanjeevThesis.html
Root-URL: http://www.cs.uiuc.edu
Title: c  
Author: flCopyright by Sanjeev Krishnan 
Date: 1996  
Abstract-found: 0
Intro-found: 1
Reference: [1] <author> Sanjeev Krishnan and L. V. Kale. </author> <title> Automating Runtime Optimizations Using Post-Mortem Analysis. </title> <booktitle> In Proceedings of the 10th ACM International Conference on Supercomputing, </booktitle> <address> Philadelphia, </address> <month> May </month> <year> 1996. </year>
Reference-contexts: Even in cases where completely automatic techniques are not possible, it is beneficial to automate the optimization steps to the extent possible. In this thesis we have designed and implemented the Paradise (PARallel programming ADvISEr) postmortem analysis tool <ref> [1] </ref> which can be used for automating run-time optimizations without programmer intervention (Figure 1.1b). It feeds optimization hints directly into the Charm++ run-time system, thus resulting in improved performance for the parallel program. <p> with other language modules in a single application due to its imple mentation on the Converse runtime system. 38 Chapter 3 A framework for automating runtime optimizations This chapter describes the basic concepts in the framework for automating runtime optimizations, which has been embodied in the Paradise post-mortem analysis tool <ref> [1] </ref>. We first motivate runtime optimizations and the use of post-mortem analysis to automate them. We describe the framework for performance optimization, discuss issues in the program representation and present strategies for inferring optimizations and generating concise hints. Finally we present a detailed comparison with related work.
Reference: [2] <author> L. V. Kale and Sanjeev Krishnan. Charm++. In Gregory V. Wilson and Paul Lu, </author> <title> editors, Parallel Programming Using C++, chapter 5. </title> <publisher> MIT Press, </publisher> <year> 1996. </year>
Reference-contexts: In Chapter 4 we discuss strategies and mechanisms for run-time optimizations, and 9 specific techniques for automating them. Chapter 5 presents the application programs used to evaluate Charm++ and Paradise. Finally, in Chapter 6 we summarize the thesis and present directions for future work. 10 Chapter 2 Charm++ Charm++ <ref> [2] </ref> is a parallel object-oriented language based on C++. It was one of the first few C++-based parallel languages when developed in 1992-93. Charm++ enables the application of object orientation to the problems of parallel programming. <p> These issues, and empirical studies of the performance advantages of message-driven execution, are discussed in [15]. A comparison of message-driven execution with other concepts such as threads and active messages is also presented in <ref> [2] </ref>. 2.2.2 Dynamic object creation: chares and messages In order to support irregular computations in which the amount of work on a processor changes dynamically and unpredictably, Charm++ allows dynamic, asynchronous creation of parallel objects (chares), which can then be mapped to different processors to balance loads.
Reference: [3] <author> L.V. Kale. </author> <title> The Chare Kernel Parallel Programming Language and System. </title> <booktitle> In Proceedings of the 19th International Conference on Parallel Processing, </booktitle> <pages> pages II-17-II-25, </pages> <month> August </month> <year> 1990. </year>
Reference-contexts: It was one of the first few C++-based parallel languages when developed in 1992-93. Charm++ enables the application of object orientation to the problems of parallel programming. Most of its features are based on those in the C-based parallel programming language Charm <ref> [3] </ref>. Charm++ programs can run today on most MIMD computers, including shared- and distributed-memory machines such as the Intel Paragon, TMC CM-5, IBM SP-2, nCUBE/2, Convex Exemplar, Sequent Symmetry, Encore Multimax, and workstation networks. <p> Although Charm++ automates mapping and scheduling, it allows programmers to override mapping explicitly and also lets them influence scheduling via prioritization. 2.1.2 Evolution Several of the parallel programming concepts in Charm++ were first developed in the Charm language and the Chare Kernel runtime system between 1987 and 1991 <ref> [3, 8, 9, 10, 11] </ref>. Charm is an extension of C which supports parallel objects called chares. It was one of the first parallel languages to support a message-driven style of programming.
Reference: [4] <author> V.S. Sunderam. </author> <title> PVM: A Framework for Parallel Distributed Computing. </title> <journal> Concurrency: Practice and Experience, </journal> <volume> 2(4), </volume> <month> December </month> <year> 1990. </year>
Reference-contexts: Various approaches to parallel programming can be characterized by the extent to which these tasks are automated by the programming system and by the generality of their approaches. A low-level portable layer such as PVM <ref> [4] </ref> only provides machine-independent implementation (thereby automating machine dependent implementation), but is sufficiently general to be useful in most problem domains. A programming system such as HPF [5] automates scheduling, but allows the programmer to specify the mapping of computations to processors and directly supports only data-parallel applications.
Reference: [5] <author> C.H. Koelbel, D.B. Loveman, R.S. Schreiber, G.L. Steele Jr., and M.E. Zosel. </author> <title> The High Performance Fortran Handbook. </title> <publisher> MIT Press, </publisher> <year> 1994. </year>
Reference-contexts: A low-level portable layer such as PVM [4] only provides machine-independent implementation (thereby automating machine dependent implementation), but is sufficiently general to be useful in most problem domains. A programming system such as HPF <ref> [5] </ref> automates scheduling, but allows the programmer to specify the mapping of computations to processors and directly supports only data-parallel applications. Parallelizing compilers [6] attempt to automate all four tasks, but have proved effective only for regular problems having loop-based parallelism. <p> Each dimension of each array is represented as the node of a dimension-graph, and edges represent communication between dimensions. A graph partitioning heuristic partitions nodes in this dimension-graph so that inter-partition communication is minimized. Each resulting partition represents a dimension of a template grid (as in HPF <ref> [5] </ref>), and each object in each array is assigned to a grid point. Finally the template grid is partitioned across processors as described in the previous paragraph. Chapter 5 gives an example of automatic array partitioning for regular programs.
Reference: [6] <author> D.A. Padua and M.J. Wolfe. </author> <title> Advanced Compiler Optimizations for Supercomputers. </title> <journal> Communications of the ACM, </journal> <volume> 29(12) </volume> <pages> 829-842, </pages> <month> dec </month> <year> 1986. </year>
Reference-contexts: A programming system such as HPF [5] automates scheduling, but allows the programmer to specify the mapping of computations to processors and directly supports only data-parallel applications. Parallelizing compilers <ref> [6] </ref> attempt to automate all four tasks, but have proved effective only for regular problems having loop-based parallelism. Charm++ is a general-purpose language that tries to automate the details of mapping, scheduling and portability.
Reference: [7] <author> Message Passing Interface Forum. </author> <title> MPI: A Message-Passing Interface Standard, </title> <month> May </month> <year> 1994. </year> <month> 134 </month>
Reference-contexts: In a programming system with little automation (e.g. PVM or MPI <ref> [7] </ref>), the programmer explicitly 13 specifies decomposition, mapping and scheduling; programming tools do not have access to in-formation about those tasks and cannot influence them without help from the programmer. On the other hand, Charm++ is a language that is designed for automation.
Reference: [8] <author> W. Fenton, B. Ramkumar, V. Saletore, A.B. Sinha, and L.V. Kale. </author> <title> Supporting Machine--Independent Parallel Programming on Diverse Architectures. </title> <booktitle> In Proceedings of the 1991 International Conference on Parallel Processing, </booktitle> <year> 1991. </year>
Reference-contexts: Although Charm++ automates mapping and scheduling, it allows programmers to override mapping explicitly and also lets them influence scheduling via prioritization. 2.1.2 Evolution Several of the parallel programming concepts in Charm++ were first developed in the Charm language and the Chare Kernel runtime system between 1987 and 1991 <ref> [3, 8, 9, 10, 11] </ref>. Charm is an extension of C which supports parallel objects called chares. It was one of the first parallel languages to support a message-driven style of programming. <p> The translator converts Charm++ constructs into C++ constructs and calls to the runtime system, and also generates stubs for messaging. The runtime system consists of a language-independent portable layer called Converse [13], on top of which is the Chare Kernel layer <ref> [8, 11] </ref>. Figure 2.3 shows the structure of the runtime system. 2.3.1 Converse: portability and interoperability The Converse layer provides a portable machine interface which supports essential parallel operations on MIMD machines.
Reference: [9] <author> L.V. Kale. </author> <title> A Tutorial Introduction to CHARM, </title> <month> December </month> <year> 1992. </year>
Reference-contexts: Although Charm++ automates mapping and scheduling, it allows programmers to override mapping explicitly and also lets them influence scheduling via prioritization. 2.1.2 Evolution Several of the parallel programming concepts in Charm++ were first developed in the Charm language and the Chare Kernel runtime system between 1987 and 1991 <ref> [3, 8, 9, 10, 11] </ref>. Charm is an extension of C which supports parallel objects called chares. It was one of the first parallel languages to support a message-driven style of programming.
Reference: [10] <author> L.V. Kale, B. Ramkumar, A. Sinha, and A. Gursoy. </author> <title> The Charm Parallel Programming Language and System: Part I Description of Language Features. </title> <type> Technical Report 95-2, </type> <institution> Parallel Programming Laboratory, Department of Computer Science, University of Illinois, Urbana-Champaign, </institution> <year> 1995. </year>
Reference-contexts: Although Charm++ automates mapping and scheduling, it allows programmers to override mapping explicitly and also lets them influence scheduling via prioritization. 2.1.2 Evolution Several of the parallel programming concepts in Charm++ were first developed in the Charm language and the Chare Kernel runtime system between 1987 and 1991 <ref> [3, 8, 9, 10, 11] </ref>. Charm is an extension of C which supports parallel objects called chares. It was one of the first parallel languages to support a message-driven style of programming.
Reference: [11] <author> B. Ramkumar, A. Sinha, V. Saletore, and L.V. Kale. </author> <title> The Charm Parallel Programming Language and System: Part II The Runtime System. </title> <type> Technical Report 95-3, </type> <institution> Parallel Programming Laboratory, Department of Computer Science, University of Illinois, Urbana-Champaign, </institution> <year> 1995. </year>
Reference-contexts: Although Charm++ automates mapping and scheduling, it allows programmers to override mapping explicitly and also lets them influence scheduling via prioritization. 2.1.2 Evolution Several of the parallel programming concepts in Charm++ were first developed in the Charm language and the Chare Kernel runtime system between 1987 and 1991 <ref> [3, 8, 9, 10, 11] </ref>. Charm is an extension of C which supports parallel objects called chares. It was one of the first parallel languages to support a message-driven style of programming. <p> The translator converts Charm++ constructs into C++ constructs and calls to the runtime system, and also generates stubs for messaging. The runtime system consists of a language-independent portable layer called Converse [13], on top of which is the Chare Kernel layer <ref> [8, 11] </ref>. Figure 2.3 shows the structure of the runtime system. 2.3.1 Converse: portability and interoperability The Converse layer provides a portable machine interface which supports essential parallel operations on MIMD machines.
Reference: [12] <author> L.V. Kale and Sanjeev Krishnan. </author> <title> Charm++ : A portable concurrent object oriented system based on C++. </title> <booktitle> In Proceedings of the Conference on Object Oriented Programmi ng Systems, Languages and Applications, </booktitle> <month> September </month> <year> 1993. </year> <note> (Also: Technical Report UIUCDCS-R-93-1796, </note> <month> March </month> <year> 1993, </year> <institution> University of Illinois, Urbana, IL). </institution>
Reference-contexts: It was one of the first parallel languages to support a message-driven style of programming. The Chare Kernel runtime system supported dynamic object creation, dynamic load balancing, and prioritized message-driven scheduling in a portable manner. Charm++, which was developed in 1992-93 <ref> [12] </ref>, fully incorporates object-oriented features such as inheritance and polymorphism into the Charm model and adds other abstractions such as parallel object arrays.
Reference: [13] <author> L.V. Kale, M. Bhandarkar, N. Jagathesan, and S. Krishnan. </author> <title> Converse: An Interoperable Framework for Parallel Programming. </title> <type> Technical Report 95-12, </type> <institution> Parallel Programming Laboratory, Department of Computer Science, University of Illinois, Urbana-Champaign, </institution> <month> March </month> <year> 1995. </year>
Reference-contexts: Charm++, which was developed in 1992-93 [12], fully incorporates object-oriented features such as inheritance and polymorphism into the Charm model and adds other abstractions such as parallel object arrays. Recently, the Chare Kernel was modified to operate on top of Converse <ref> [13] </ref>, a machine-independent layer (described in section 2.3.1) which supports interoperability by allowing modules from multiple languages to co-exist in a single application. 14 2.2 The Charm++ language In this section we discuss the essential features of Charm++. <p> The translator converts Charm++ constructs into C++ constructs and calls to the runtime system, and also generates stubs for messaging. The runtime system consists of a language-independent portable layer called Converse <ref> [13] </ref>, on top of which is the Chare Kernel layer [8, 11]. Figure 2.3 shows the structure of the runtime system. 2.3.1 Converse: portability and interoperability The Converse layer provides a portable machine interface which supports essential parallel operations on MIMD machines.
Reference: [14] <author> G. Agha. </author> <title> Actors: A Model of Concurrent Computation in Distributed Systems. </title> <publisher> MIT Press, </publisher> <year> 1986. </year>
Reference-contexts: The first two features|message-driven execution and dynamic object creation|had been defined and explored in work on actors <ref> [14] </ref> done prior to Charm++.
Reference: [15] <author> A. Gursoy. </author> <title> Simplified Expression of Message-Driven Programs and Quantification of Their Impact on Performance. </title> <type> PhD thesis, </type> <institution> Department of Computer Science, University of Illinois, Urbana-Champaign, </institution> <month> June </month> <year> 1994. </year>
Reference-contexts: Message-driven execution thus has advantages over communication based on blocking receives and yields better performance by adaptively scheduling computations. These issues, and empirical studies of the performance advantages of message-driven execution, are discussed in <ref> [15] </ref>.
Reference: [16] <author> A. Sinha and L.V. Kale. </author> <title> A Load Balancing Strategy for Prioritized Execution of Tasks. </title> <booktitle> In Proceedings of the 7th International Parallel Processing Symposium, </booktitle> <pages> pages 230-237, </pages> <month> April </month> <year> 1993. </year>
Reference-contexts: Several load balancing strategies are provided <ref> [16] </ref>, which may be selected at link time by the user depending on the demands of the application. In Chapter 4, we describe how the Paradise optimization tool automatically selects the load balancing strategy. Prioritized Execution: Charm++ provides many user-selectable strategies for managing queues of messages waiting to be processed.
Reference: [17] <author> Gregory V. Wilson and Paul Lu, </author> <title> editors. Parallel Programming Using C++. </title> <publisher> MIT Press, </publisher> <year> 1996. </year>
Reference-contexts: These are called by the system just before sending and after receiving a message, respectively. Thus, only messages that are actually sent to other processors are packed. 2.2.2.2 Related work in concurrent objects Dynamic object creation is provided in ICC++ <ref> [17] </ref>, Mentat [18], C++ [19], C++// [17], ABCL [20], and COOL [21], among others. Mentat supports coarse-grained objects, while 19 ICC++ and languages such as Cantor [22], HAL [23], and CA [24] support fine-grained objects. <p> These are called by the system just before sending and after receiving a message, respectively. Thus, only messages that are actually sent to other processors are packed. 2.2.2.2 Related work in concurrent objects Dynamic object creation is provided in ICC++ <ref> [17] </ref>, Mentat [18], C++ [19], C++// [17], ABCL [20], and COOL [21], among others. Mentat supports coarse-grained objects, while 19 ICC++ and languages such as Cantor [22], HAL [23], and CA [24] support fine-grained objects. <p> In data-parallel languages such as HPF [26] programmers have a shared-memory model in which arrays in the program are distributed over processors using compiler directives, and computations are assigned to processors using the "owner-computes" rule. Some parallel C++ systems such as pC++, LPARX, Amelia, CHAOS++ and POOMA <ref> [27, 17] </ref> support parallel arrays or collections of objects (LPARX and CHAOS++ support irregular arrays too) for scientific applications, also in the context of a loosely-synchronous data-parallel model. <p> Parallel object arrays in Charm++ are closest in spirit to the collections construct in ICC++ <ref> [17] </ref>, which was developed around the same time. ICC++ does not support multicasts and remapping, though. <p> The Charm runtime system itself [51] provides several dynamic load balancing strategies for optimizing object placement, prioritized message-queueing strategies for optimizing scheduling, and optimization of remote object creation using "virtual object handles". CHAOS++ <ref> [17] </ref> optimizes irregular communication between distributed arrays of objects with pointer-based referencing. It also provides user-selectable partitioners for distributing irregular arrays across processors. CHAOS++ depends on a loosely-synchronous (separate phases of computation and communication) model, allowing communication to be performed by inspector and executor phases. <p> ICC++ and the Concert system <ref> [17, 53] </ref> support grainsize optimizations through compiler analysis: the compiler increases the grainsize of objects by inlining method invocations statically, or by cloning methods and generating hints to the runtime via annotations. <p> The actor-based language HAL [54, 55] optimizes request-reply communication by transforming them into asynchronous sends and extracting continuations, converts local messaging into function calls, and optimizes remote actor creation using alias addresses for actors. Mentat <ref> [17] </ref> supports dynamic placement through round-robin, random and sender-initiated strategies. However, the selection of strategy must be done by the user. <p> Since the development of Charm++ in 1992-93, parallel object-oriented languages have become very popular in the parallel computing community. Today there are more than fifteen different dialects of parallel C++ <ref> [17] </ref>, with a wide variety of programming models and approaches to combining object-orientation and parallelism. Thus it is hoped that the field of parallel object-oriented programming is maturing, as experience with various features grows.
Reference: [18] <author> A.S. Grimshaw. </author> <title> Easy to Use Object-Oriented Parallel Programming with Mentat. </title> <booktitle> IEEE Computer, </booktitle> <pages> pages 39-51, </pages> <month> May </month> <year> 1993. </year>
Reference-contexts: These are called by the system just before sending and after receiving a message, respectively. Thus, only messages that are actually sent to other processors are packed. 2.2.2.2 Related work in concurrent objects Dynamic object creation is provided in ICC++ [17], Mentat <ref> [18] </ref>, C++ [19], C++// [17], ABCL [20], and COOL [21], among others. Mentat supports coarse-grained objects, while 19 ICC++ and languages such as Cantor [22], HAL [23], and CA [24] support fine-grained objects.
Reference: [19] <author> P.A. Buhr, G. Ditchfield, R.A. Stroobosscher, B.M. Younger, and C.R. Zarnke. </author> <title> uC++: Concurrency in the Object-Oriented Language C++. </title> <journal> Software Practice and Experience, </journal> <volume> 22(2) </volume> <pages> 137-172, </pages> <year> 1992. </year>
Reference-contexts: These are called by the system just before sending and after receiving a message, respectively. Thus, only messages that are actually sent to other processors are packed. 2.2.2.2 Related work in concurrent objects Dynamic object creation is provided in ICC++ [17], Mentat [18], C++ <ref> [19] </ref>, C++// [17], ABCL [20], and COOL [21], among others. Mentat supports coarse-grained objects, while 19 ICC++ and languages such as Cantor [22], HAL [23], and CA [24] support fine-grained objects.
Reference: [20] <author> A. Yonezawa, </author> <title> editor. ABCL: An Object-Oriented Concurrent System. </title> <publisher> MIT Press, </publisher> <year> 1990. </year>
Reference-contexts: These are called by the system just before sending and after receiving a message, respectively. Thus, only messages that are actually sent to other processors are packed. 2.2.2.2 Related work in concurrent objects Dynamic object creation is provided in ICC++ [17], Mentat [18], C++ [19], C++// [17], ABCL <ref> [20] </ref>, and COOL [21], among others. Mentat supports coarse-grained objects, while 19 ICC++ and languages such as Cantor [22], HAL [23], and CA [24] support fine-grained objects. Compilers for fine-grained languages typically coalesce multiple objects and operations into coarser ones in order to achieve acceptable performance on available parallel machines.
Reference: [21] <author> R. Chandra, A. Gupta, and J.L. Hennessy. </author> <title> COOL: An Object-Based Language for Parallel Programming. </title> <journal> IEEE Computer, </journal> <volume> 27(8) </volume> <pages> 14-26, </pages> <month> August </month> <year> 1994. </year>
Reference-contexts: Thus, only messages that are actually sent to other processors are packed. 2.2.2.2 Related work in concurrent objects Dynamic object creation is provided in ICC++ [17], Mentat [18], C++ [19], C++// [17], ABCL [20], and COOL <ref> [21] </ref>, among others. Mentat supports coarse-grained objects, while 19 ICC++ and languages such as Cantor [22], HAL [23], and CA [24] support fine-grained objects. Compilers for fine-grained languages typically coalesce multiple objects and operations into coarser ones in order to achieve acceptable performance on available parallel machines.
Reference: [22] <author> W. Athas and N. Boden. Cantor: </author> <title> An Actor Programming System for Scientific Computing. </title> <booktitle> In Proceedings of the ACM SIGPLAN Workshop on Object Based Concurrent Programming, ACM SIGPLAN Notices, </booktitle> <pages> pages 66-68, </pages> <month> April </month> <year> 1989. </year>
Reference-contexts: Mentat supports coarse-grained objects, while 19 ICC++ and languages such as Cantor <ref> [22] </ref>, HAL [23], and CA [24] support fine-grained objects. Compilers for fine-grained languages typically coalesce multiple objects and operations into coarser ones in order to achieve acceptable performance on available parallel machines.
Reference: [23] <author> C. Houck and G. Agha. HAL: </author> <title> A High Level Actor Language and its Distributed Implementation. </title> <booktitle> In Proceedings of the International Conference on Parallel Processing, </booktitle> <month> August </month> <year> 1992. </year>
Reference-contexts: Mentat supports coarse-grained objects, while 19 ICC++ and languages such as Cantor [22], HAL <ref> [23] </ref>, and CA [24] support fine-grained objects. Compilers for fine-grained languages typically coalesce multiple objects and operations into coarser ones in order to achieve acceptable performance on available parallel machines.
Reference: [24] <author> A.A. Chien. </author> <title> Concurrent Aggregates: Supporting Modularity in Massively-Parallel Pro--grams. </title> <publisher> MIT Press, </publisher> <year> 1993. </year>
Reference-contexts: Mentat supports coarse-grained objects, while 19 ICC++ and languages such as Cantor [22], HAL [23], and CA <ref> [24] </ref> support fine-grained objects. Compilers for fine-grained languages typically coalesce multiple objects and operations into coarser ones in order to achieve acceptable performance on available parallel machines.
Reference: [25] <author> Sanjeev Krishnan and L. V. Kale. </author> <title> A parallel array abstraction for data-driven objects. </title> <booktitle> In Proceedings of the Parallel Object-Oriented Methods and Applications Conference, </booktitle> <month> February </month> <year> 1996. </year>
Reference-contexts: Message prioritization is also very useful when quick response times are needed for high priority queries, or to prevent large batch queries from delaying smaller interactive ones. 2.2.3 Parallel object arrays A parallel object array is actually a group of chares with a single global name <ref> [25] </ref>. Parallel object arrays in Charm++ are a generalization of the branch-office chares of Charm, which allowed one group member per processor. A parallel object array is a multidimensional array of chares, with arbitrary mapping of array elements to processors (a default mapping is provided when mapping is not significant).
Reference: [26] <author> High Performance Fortran Forum. </author> <title> High Performance Fortran Language Specification (Draft), </title> <address> 1.0 edition, </address> <month> January </month> <year> 1993. </year>
Reference-contexts: The ability of parallel arrays to link together a group of objects created via a single creation call is crucial for such applications involving multiple clients. 2.2.3.8 Related concepts Parallel arrays have been used in various forms in several parallel programming systems. In data-parallel languages such as HPF <ref> [26] </ref> programmers have a shared-memory model in which arrays in the program are distributed over processors using compiler directives, and computations are assigned to processors using the "owner-computes" rule.
Reference: [27] <author> S. R. Kohn and S. B. Baden. </author> <title> Irregular coarse-grain data parallelism under lparx. </title> <journal> Scientific Programming, </journal> <volume> 5(3), </volume> <year> 1996. </year> <note> To appear. </note>
Reference-contexts: In data-parallel languages such as HPF [26] programmers have a shared-memory model in which arrays in the program are distributed over processors using compiler directives, and computations are assigned to processors using the "owner-computes" rule. Some parallel C++ systems such as pC++, LPARX, Amelia, CHAOS++ and POOMA <ref> [27, 17] </ref> support parallel arrays or collections of objects (LPARX and CHAOS++ support irregular arrays too) for scientific applications, also in the context of a loosely-synchronous data-parallel model.
Reference: [28] <author> L.V. Kale and Amitabh Sinha. </author> <title> Projections : A scalable performance tool. In Parallel Systems Fair, </title> <booktitle> International Parallel Processing Sympo sium, </booktitle> <month> April </month> <year> 1993. </year>
Reference-contexts: The basic version of the event graph was developed for the Projections <ref> [28] </ref> performance visualization and analysis tool. Issues in collecting trace data, reducing perturbation, and constructing the basic event graph are discussed in [29] and are beyond the scope of this thesis. <p> The picture 124 processors of the CM-5. was obtained using the Projections performance analysis tool <ref> [28] </ref>. The picture shows two iterations of the AFMA (the vertical dashed lines separate the iterations). In the first iteration (stage 30 through stage 550) the load partitioning (ORB) algorithm assumes that all particles have the same load, so that all processors are assigned equal numbers of particles. <p> deriv 132 ing expressions for predicting the execution time of the program, or using trace-driven simulation. * Integration with other tools : Paradise is one of several tools developed in the author's research group including: Visual Dagger, a GUI-based editor for depicting macro-dataflow and dependences within and across objects; Projections <ref> [28] </ref>, a performance visualization and expert analysis tool; and a trace-driven simulator [91].
Reference: [29] <author> Amitabh B. Sinha. </author> <title> Performance Analysis of Object Based and Message Driven Programs. </title> <type> PhD thesis, </type> <institution> Department of Computer Science, University of Illinois, Urbana-Champaign, </institution> <month> January </month> <year> 1995. </year>
Reference-contexts: The basic version of the event graph was developed for the Projections [28] performance visualization and analysis tool. Issues in collecting trace data, reducing perturbation, and constructing the basic event graph are discussed in <ref> [29] </ref> and are beyond the scope of this thesis. The basic event graph consists of vertices representing method executions within chare objects, and edges representing messages sent by a method and causing the execution of another method. <p> Use heuristic rules to match the characteristics to optimizations. This process is similar to the heuristic classification scheme used in Poirot [31], and the search tree based analysis in Projections <ref> [29] </ref>. 50 * Backward reasoning: For each type of optimization, determine if the program would benefit by its application, by extracting the relevant characteristics from the event graph. <p> A few of the above performance tools also go further than merely reporting performance data. These include MPP-Apprentice [43] and P 3 T [39] for data-parallel programming models; Poirot [31] and Paradyn for message-passing / data-parallel programs [34]; and Projections for object-based message-driven programs <ref> [29] </ref>. These tools analyze the performance data to give the user insights into performance problems and diagnose their causes. <p> Some of the tools also give a quantitative estimate of performance improvements when each performance problem is corrected, and prioritize the problems depending on their severity. In particular, the Projections performance analysis tool <ref> [29] </ref> developed earlier in the author's research group 53 has an expert analysis component that incorporates several of the above features. However, all the above tools leave the task of interpreting the performance problem and choosing and incorporating optimizations to the programmer. <p> Several techniques for reducing trace overhead and perturbation, reconstructing actual program execution from traces and dynamic instrumentation have been explored in the literature <ref> [29, 34] </ref>. In particular, the Projections performance analysis tool developed earlier in the author's research group incorporates several such techniques, which are used in Paradise too. These issues are beyond the scope of this thesis. <p> We describe some important characteristics, and discuss how they are inferred from the event graph. 63 4.2.1 Phase structure of program This characteristic tells us if there are repeatedly occurring phases in the program. Separating the program into independent phases helps to focus analysis <ref> [29] </ref>. <p> This control point is provided in the form of the function "GetPipelineDegree ()" which is called by the user method and returns the proper degree of pipelining for the method. The analysis performed for automating pipelining is similar to the one described in <ref> [29] </ref>. Paradise looks along the critical path of the program for methods which execute after a significant idle period on their processor. <p> Paradise finds a predecessor method which may be pipelined, and generates a hint to pipeline the chosen methods, specifying the optimal degree of pipelining. The optimal degree of pipelining is computed using the formula described in <ref> [29] </ref> (Figure 4.3), presented here for completeness: k = fil+g s , where k is the degree of pipelining, fi is the time to transfer one byte across the network (i.e. fi = 1=bandwidth), l is the length of the message, g is the grainsize of the computation to be pipelined, <p> Hence it decided to pipeline overlay methods and their predecessor "start" method, which started the pipe. The degree of pipelining was determined using the formula from <ref> [29] </ref>, and a hint to that effect was generated.
Reference: [30] <author> A. Gursoy and L. V. Kale. Dagger: </author> <title> Combining the benefits of synchronous and asynchronous communication styles. </title> <type> Technical Report 93-3, </type> <institution> Parallel Programming Laboratory, Department of Computer Science , University of Illinois, Urbana-Champaign, </institution> <month> March </month> <year> 1993. </year>
Reference-contexts: We have enhanced the basic event graph with precise information about intra-object synchronizations (internal dependences between methods), such as specified in the Dagger notation <ref> [30] </ref>. Dagger allows programmers to specify synchronizations between methods and messages: e.g. "method X can execute only if method Y has executed and message Z has arrived". We incorporate this information into the basic event graph.
Reference: [31] <author> B. Robert Helm and Allen Malony. </author> <title> Automating Performance Diagnosis : A Theory and Architecture. </title> <booktitle> In Proceedings of the International Workshop on Computer Performance Measurement and Analysis, </booktitle> <address> Beppu, Japan, </address> <month> August </month> <year> 1995. </year>
Reference-contexts: Use heuristic rules to match the characteristics to optimizations. This process is similar to the heuristic classification scheme used in Poirot <ref> [31] </ref>, and the search tree based analysis in Projections [29]. 50 * Backward reasoning: For each type of optimization, determine if the program would benefit by its application, by extracting the relevant characteristics from the event graph. <p> A few of the above performance tools also go further than merely reporting performance data. These include MPP-Apprentice [43] and P 3 T [39] for data-parallel programming models; Poirot <ref> [31] </ref> and Paradyn for message-passing / data-parallel programs [34]; and Projections for object-based message-driven programs [29]. These tools analyze the performance data to give the user insights into performance problems and diagnose their causes.
Reference: [32] <author> D. A. Reed et al. </author> <title> Scalable Performance Analysis : The Pablo Performance Analysis Environment. </title> <booktitle> In Proceedings of the Scalable Parallel Libraries Conference, </booktitle> <pages> pages 104-113. </pages> <publisher> IEEE Computer Society, </publisher> <year> 1993. </year> <month> 137 </month>
Reference: [33] <author> V. Adve, J. Mellor-Crummey, M. Anderson, K. Kennedy, J. Wang, and D. A. Reed. </author> <title> An integrated compilation and performance analysis environment for data-parallel programs. </title> <booktitle> In Proceedings of Supercomputing 1995, </booktitle> <month> December </month> <year> 1995. </year>
Reference: [34] <author> Barton P. Miller et al. </author> <title> The Paradyn Parallel Performance Measurement Tools. </title> <journal> IEEE Computer, </journal> <volume> 28(11), </volume> <month> November </month> <year> 1995. </year>
Reference-contexts: A few of the above performance tools also go further than merely reporting performance data. These include MPP-Apprentice [43] and P 3 T [39] for data-parallel programming models; Poirot [31] and Paradyn for message-passing / data-parallel programs <ref> [34] </ref>; and Projections for object-based message-driven programs [29]. These tools analyze the performance data to give the user insights into performance problems and diagnose their causes. <p> Several techniques for reducing trace overhead and perturbation, reconstructing actual program execution from traces and dynamic instrumentation have been explored in the literature <ref> [29, 34] </ref>. In particular, the Projections performance analysis tool developed earlier in the author's research group incorporates several such techniques, which are used in Paradise too. These issues are beyond the scope of this thesis.
Reference: [35] <author> R. Bruce Irvin and Barton P. Miller. </author> <title> Mapping Performance Data for High-Level and Data Views of Parallel Program Performance. </title> <booktitle> In Proceedings of the ACM International Conference on Supercomputing, </booktitle> <month> May </month> <year> 1996. </year>
Reference: [36] <author> J. C. Yan, S. Sarukkai, and P. Mehra. </author> <title> Performance Measurement, Visualization and Modeling of Parallel and Distributed Programs using the AIMS Toolkit. </title> <journal> Software Practice and Experience, </journal> <month> April </month> <year> 1995. </year>
Reference: [37] <author> B. Mohr, D. Brown, and A. Malony. </author> <title> TAU: A Portable Parallel Program Analysis Environment for pC++. </title> <booktitle> In Proceedings of the 3rd Joint Conference on Parallel Processing: CONPAR 94 - VAPP VI, </booktitle> <month> September </month> <year> 1994. </year>
Reference: [38] <author> M. Crovella and T. J. LeBlanc. </author> <title> The Search for Lost Cycles: A New Approach to Performance Tuning of Parallel Programs. </title> <booktitle> In Proceedings of Supercomputing 1994, </booktitle> <month> November </month> <year> 1994. </year>
Reference: [39] <author> Thomas Fahringer. </author> <title> Estimating and Optimizing Performance for Parallel Programs. </title> <journal> IEEE Computer, </journal> <volume> 28(11), </volume> <month> November </month> <year> 1995. </year>
Reference-contexts: A few of the above performance tools also go further than merely reporting performance data. These include MPP-Apprentice [43] and P 3 T <ref> [39] </ref> for data-parallel programming models; Poirot [31] and Paradyn for message-passing / data-parallel programs [34]; and Projections for object-based message-driven programs [29]. These tools analyze the performance data to give the user insights into performance problems and diagnose their causes. <p> The use of profile information for compiler optimizations is well-known. Several sequential compilers use profile information to predict branch probabilities (e.g. [46]). Some compilers for data-parallel languages such as HPF <ref> [39] </ref> use profile information to accurately find the cost of various computation and communication operations.
Reference: [40] <author> M. Heath and J. Etheridge. </author> <title> Visualizing the performance of parallel programs. </title> <journal> IEEE Software, </journal> <month> September </month> <year> 1991. </year>
Reference: [41] <author> V. Herrarte and E. W. Lusk. </author> <title> Studying parallel program behavior with upshot. User Manual for Upshot. </title> <type> 138 </type>
Reference: [42] <author> J. Kohn and W. Williams. </author> <title> ATExpert. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 18 </volume> <pages> 205-222, </pages> <year> 1993. </year>
Reference: [43] <author> W. Williams, T. Hoel, and D. Pase. </author> <title> The MPP Apprentice Performance Tool: Delivering the Performance of the Cray T3D. </title> <editor> In K. M. Decker and R. M. Rehmann, editors, </editor> <title> Programming Environments for Massively Parallel Distributed Systems. </title> <publisher> Birkaeuser Verlag, </publisher> <address> Basel, Switzerland, </address> <year> 1994. </year>
Reference-contexts: A few of the above performance tools also go further than merely reporting performance data. These include MPP-Apprentice <ref> [43] </ref> and P 3 T [39] for data-parallel programming models; Poirot [31] and Paradyn for message-passing / data-parallel programs [34]; and Projections for object-based message-driven programs [29]. These tools analyze the performance data to give the user insights into performance problems and diagnose their causes.
Reference: [44] <author> P. Banerjee et al. </author> <title> The PARADIGM Compiler for Distributed-Memory Multicomputers. </title> <booktitle> IEEE Computer, </booktitle> <month> October </month> <year> 1995. </year>
Reference-contexts: Paradise can be profitably used in conjunction with a performance visualization tool, and complements the functionalities provided in the tools listed above. 3.9.2 Compiler and runtime optimizations Automatic compiler optimizations have achieved success for automatic data partitioning and communication schedule generation in array-based Fortran programs <ref> [44, 45] </ref>. The use of profile information for compiler optimizations is well-known. Several sequential compilers use profile information to predict branch probabilities (e.g. [46]). Some compilers for data-parallel languages such as HPF [39] use profile information to accurately find the cost of various computation and communication operations.
Reference: [45] <author> S. Sharma, R. Ponnusamy, B. Moon, Y. Hwang, R. Das, and J. Saltz. </author> <title> Run-time and compile-time support for adaptive irregular problems. </title> <booktitle> In Proceedings of Supercomputing 1994, </booktitle> <month> November </month> <year> 1994. </year>
Reference-contexts: Paradise can be profitably used in conjunction with a performance visualization tool, and complements the functionalities provided in the tools listed above. 3.9.2 Compiler and runtime optimizations Automatic compiler optimizations have achieved success for automatic data partitioning and communication schedule generation in array-based Fortran programs <ref> [44, 45] </ref>. The use of profile information for compiler optimizations is well-known. Several sequential compilers use profile information to predict branch probabilities (e.g. [46]). Some compilers for data-parallel languages such as HPF [39] use profile information to accurately find the cost of various computation and communication operations. <p> Section 4.7.2. 4.7 Communication optimizations Communication optimizations are useful in many parallel programs to reduce communication volume and overheads, as well as reduce the latency of remote data access. 4.7.1 Mechanisms for optimizing communication There has been a lot of research in optimizing communication for data-parallel applications written in HPF/Fortran90D <ref> [69, 45] </ref>, in which the compiler generates a good communication schedule or generates calls to run-time communication libraries. The run-time optimizations described below have been commonly observed to be useful for diverse applications, in a concurrent object-oriented context.
Reference: [46] <author> P. P. Chang, S. Mahlke, W. Chen, N. Warter, and W. Hwu. </author> <title> IMPACT : An Architectural Framework for Multiple-Instruction Issue Processors. </title> <booktitle> In Proceedings of the 18th International Symposium on Computer Architecture, </booktitle> <month> May </month> <year> 1991. </year>
Reference-contexts: The use of profile information for compiler optimizations is well-known. Several sequential compilers use profile information to predict branch probabilities (e.g. <ref> [46] </ref>). Some compilers for data-parallel languages such as HPF [39] use profile information to accurately find the cost of various computation and communication operations.
Reference: [47] <author> L. Rauchwerger, N. Amato, and D. Padua. </author> <title> Run-time methods for parallelizing partially parallel loops. </title> <booktitle> In Proceedings of the 9th ACM International Conference on Supercomputing, </booktitle> <month> July </month> <year> 1995. </year>
Reference-contexts: Some compilers for data-parallel languages such as HPF [39] use profile information to accurately find the cost of various computation and communication operations. Parallelizing compilers for loop-based programs have explored the use of runtime dependence analysis and parallelism detection (e.g. using inspector and executor code) and speculative execution <ref> [47, 48, 49, 50] </ref>, especially for irregular programs with input-dependent data access patterns. However, such work has been restricted to loop-based scientific programs, usually running on shared memory multiprocessors.
Reference: [48] <author> J. Wu, J. Saltz, S. Hiranandani, and J. Berryman. </author> <title> Runtime compilation methods for multicomputers. </title> <booktitle> In Proceedings of the International Conference on Parallel Processing, </booktitle> <month> August </month> <year> 1991. </year>
Reference-contexts: Some compilers for data-parallel languages such as HPF [39] use profile information to accurately find the cost of various computation and communication operations. Parallelizing compilers for loop-based programs have explored the use of runtime dependence analysis and parallelism detection (e.g. using inspector and executor code) and speculative execution <ref> [47, 48, 49, 50] </ref>, especially for irregular programs with input-dependent data access patterns. However, such work has been restricted to loop-based scientific programs, usually running on shared memory multiprocessors.
Reference: [49] <author> D. K. Chen, J. Torrellas, and P. C. Yew. </author> <title> An efficient algorithm for the run-time paral-lelization of doacross loops. </title> <booktitle> In Proceedings of Supercomputing 1994, </booktitle> <month> November </month> <year> 1994. </year> <month> 139 </month>
Reference-contexts: Some compilers for data-parallel languages such as HPF [39] use profile information to accurately find the cost of various computation and communication operations. Parallelizing compilers for loop-based programs have explored the use of runtime dependence analysis and parallelism detection (e.g. using inspector and executor code) and speculative execution <ref> [47, 48, 49, 50] </ref>, especially for irregular programs with input-dependent data access patterns. However, such work has been restricted to loop-based scientific programs, usually running on shared memory multiprocessors.
Reference: [50] <author> A. Lain and P. Banerjee. </author> <title> Exploiting spatial regularity in irregular iterative applications. </title> <booktitle> In Proceedings of the International Parallel Processing Symposium, </booktitle> <month> April </month> <year> 1995. </year>
Reference-contexts: Some compilers for data-parallel languages such as HPF [39] use profile information to accurately find the cost of various computation and communication operations. Parallelizing compilers for loop-based programs have explored the use of runtime dependence analysis and parallelism detection (e.g. using inspector and executor code) and speculative execution <ref> [47, 48, 49, 50] </ref>, especially for irregular programs with input-dependent data access patterns. However, such work has been restricted to loop-based scientific programs, usually running on shared memory multiprocessors.
Reference: [51] <author> W. Fenton, B. Ramkumar, V.A. Saletore, A.B. Sinha, and L.V. Kale. </author> <title> Supporting machine independent programming on diverse parallel architectures. </title> <booktitle> In Proceedings of the International Conference on Parallel Processing, </booktitle> <month> August </month> <year> 1991. </year>
Reference-contexts: The scope of our optimizations is much broader than in traditional compiler optimizations, and moreover applies to dynamic and irregular applications as well as regular ones. 54 Runtime systems for some parallel object-oriented languages incorporate several runtime optimizations. The Charm runtime system itself <ref> [51] </ref> provides several dynamic load balancing strategies for optimizing object placement, prioritized message-queueing strategies for optimizing scheduling, and optimization of remote object creation using "virtual object handles". CHAOS++ [17] optimizes irregular communication between distributed arrays of objects with pointer-based referencing.
Reference: [52] <author> R. Chandra, A. Gupta, and J. Hennessy. </author> <title> Integrating concurrency and data abstraction in the COOL parallel programming language. </title> <booktitle> IEEE Computer, </booktitle> <month> February </month> <year> 1994. </year>
Reference-contexts: CHAOS++ [17] optimizes irregular communication between distributed arrays of objects with pointer-based referencing. It also provides user-selectable partitioners for distributing irregular arrays across processors. CHAOS++ depends on a loosely-synchronous (separate phases of computation and communication) model, allowing communication to be performed by inspector and executor phases. COOL <ref> [52] </ref> is a shared-memory, thread-based parallel language which uses hints from the programmer to optimize scheduling and data locality. <p> This load information can be specified by the programmer as object-local variables or as parameters while creating an object. * Information about interactions between objects: this is necessary for maintaining locality. This information can be specified by the programmer in the form of object-affinity hints <ref> [52] </ref>, before creating the object or after it has started execution. 4.3.3 Heuristics for automating dynamic object placement For programs which create objects dynamically throughout the execution of a program, Paradise chooses a load balancing scheme depending on the program's characteristics. <p> iterative programs it may be possible for the runtime system to automatically store the load of an object in its "load" variable during one iteration, and use the load information for optimizing mapping in subsequent iterations. 6 Other aims include cache-locality enhancement by scheduling methods of the same object back-to-back <ref> [52] </ref>. 7 A path through a parallel program corresponds to a path in the event graph for the program. The length of a path includes the computations and communication delays on that path. 82 some computational paths are longer than others, there is likely to be a critical (longest) path.
Reference: [53] <author> A. Chien, V. Karamcheti, and J. Plevyak. </author> <title> The concert system: Compiler and runtime support for fine-grained concurrent object-oriented languages. </title> <type> Technical Report R-93-1815, </type> <institution> Department of Computer Science, University of Illinois, Urbana-Champaign, </institution> <month> June </month> <year> 1993. </year>
Reference-contexts: ICC++ and the Concert system <ref> [17, 53] </ref> support grainsize optimizations through compiler analysis: the compiler increases the grainsize of objects by inlining method invocations statically, or by cloning methods and generating hints to the runtime via annotations. <p> Requiring the programmer to explicitly specify grainsize is cumbersome, although it usually provides acceptable performance [66]. There has been research into compiler techniques for automatically determining the grainsize of computations (e.g. by merging fine-grained pieces of work <ref> [53] </ref>), however, the grainsize decision often needs to be made differently depending on run-time load conditions.
Reference: [54] <author> C. Houck and G. Agha. Hal: </author> <title> A high level actor language and its distributed implementation. </title> <booktitle> In Proceedings of the International Conference on Parallel Processing, </booktitle> <month> August </month> <year> 1992. </year>
Reference-contexts: ICC++ and the Concert system [17, 53] support grainsize optimizations through compiler analysis: the compiler increases the grainsize of objects by inlining method invocations statically, or by cloning methods and generating hints to the runtime via annotations. The actor-based language HAL <ref> [54, 55] </ref> optimizes request-reply communication by transforming them into asynchronous sends and extracting continuations, converts local messaging into function calls, and optimizes remote actor creation using alias addresses for actors. Mentat [17] supports dynamic placement through round-robin, random and sender-initiated strategies.
Reference: [55] <author> W. Kim and G. Agha. </author> <title> Efficient support of location transparency in concurrent object-oriented programming languages. </title> <booktitle> In Proceedings of Supercomputing 1995, </booktitle> <month> December </month> <year> 1995. </year>
Reference-contexts: ICC++ and the Concert system [17, 53] support grainsize optimizations through compiler analysis: the compiler increases the grainsize of objects by inlining method invocations statically, or by cloning methods and generating hints to the runtime via annotations. The actor-based language HAL <ref> [54, 55] </ref> optimizes request-reply communication by transforming them into asynchronous sends and extracting continuations, converts local messaging into function calls, and optimizes remote actor creation using alias addresses for actors. Mentat [17] supports dynamic placement through round-robin, random and sender-initiated strategies.
Reference: [56] <author> A. B. Sinha and L.V. Kale. </author> <title> A load balancing strategy for prioritized execution of tasks. </title> <booktitle> In Proceedings of the International Parallel Processing Symposium, </booktitle> <month> April </month> <year> 1993. </year>
Reference-contexts: objects have the same grainsize ) Choose the round-robin scheme. else Choose the neighbor-averaging scheme (large number of objects with varying grainsize: none of the other two will work). endif endif These rules embody some of the expertise we have accumulated while optimizing several applications requiring dynamic load balancing schemes <ref> [56, 57] </ref>.
Reference: [57] <author> L. V. Kale, Ben Richards, and Terry Allen. </author> <title> Efficient parallel graph coloring with prioritization. </title> <booktitle> Lecture Notes in Computer Science, </booktitle> <year> 1996. </year> <note> To be published. </note>
Reference-contexts: objects have the same grainsize ) Choose the round-robin scheme. else Choose the neighbor-averaging scheme (large number of objects with varying grainsize: none of the other two will work). endif endif These rules embody some of the expertise we have accumulated while optimizing several applications requiring dynamic load balancing schemes <ref> [56, 57] </ref>. <p> If some processors become idle later, this leaves some load for redistribution 10 . Parameterized code-blocks can be used for both iterative and recursive computations. Adaptive grainsize control has been manually programmed before with good results <ref> [57, 68] </ref>. 4.6.2 Automating grainsize optimization Paradise automates the two optimizations for grainsize control described in the previous subsection.
Reference: [58] <author> M. Gupta and P. Banerjee. </author> <title> Paradigm : A compiler for automatic data distribution on multicomputers. </title> <booktitle> In ACM International Conference on Supercomputing, </booktitle> <month> July </month> <year> 1993. </year> <month> 140 </month>
Reference-contexts: The control point for allowing the run-time to determine static object placement can be provided by a function call to a partitioning or placement library, at the beginning of the program. Work on compiler techniques for automatic data partitioning in array-based Fortran and HPF programs has achieved considerable success <ref> [58, 59, 60] </ref> ; block and cyclic mappings of regular arrays can be generated by compilers. However, there are many other types of irregular/dynamic applications for which static placement cannot be done by only compile-time analysis.
Reference: [59] <author> B. Chapman, T. Fahringer, and H. Zima. </author> <title> Automatic support for data distribution on dis-tributed memory multiprocessor systems. </title> <booktitle> In 6th Conference on Languages and Compilers for Parallel Computing, </booktitle> <month> August </month> <year> 1993. </year>
Reference-contexts: The control point for allowing the run-time to determine static object placement can be provided by a function call to a partitioning or placement library, at the beginning of the program. Work on compiler techniques for automatic data partitioning in array-based Fortran and HPF programs has achieved considerable success <ref> [58, 59, 60] </ref> ; block and cyclic mappings of regular arrays can be generated by compilers. However, there are many other types of irregular/dynamic applications for which static placement cannot be done by only compile-time analysis.
Reference: [60] <author> U. Kremer and K. Kennedy. </author> <title> Automatic data layout for high performance fortran. </title> <booktitle> In Proceedings of Supercomputing 1995, </booktitle> <month> December </month> <year> 1995. </year>
Reference-contexts: The control point for allowing the run-time to determine static object placement can be provided by a function call to a partitioning or placement library, at the beginning of the program. Work on compiler techniques for automatic data partitioning in array-based Fortran and HPF programs has achieved considerable success <ref> [58, 59, 60] </ref> ; block and cyclic mappings of regular arrays can be generated by compilers. However, there are many other types of irregular/dynamic applications for which static placement cannot be done by only compile-time analysis.
Reference: [61] <author> N. H. Naik, V. K. Naik, and M. Nicoules. </author> <title> Parallelization of a class of implicit finite difference schemes in computational fluid dynamics. </title> <journal> International Journal of High Speed Computing, </journal> <volume> 5(1), </volume> <year> 1993. </year>
Reference-contexts: For array-based programs (e.g. programs written using the parallel object array construct in Charm++), with regular patterns of communication, the static object placement strategy maps array elements to processors. The most popular schemes are block and cyclic-k. Other interesting schemes include the multi-partition schemes <ref> [61] </ref> for multi-dimensional arrays. 75 2. Applications having a large number of small grained objects with fixed positions in space: E.g. each object is a grid-point in a mesh, or each object corresponds to a particle in space. <p> E.g. for a 2-D array, this has the form M ap (i; j) = j c M OD d) * the multi-partition (Bruno-Capello) mapping scheme <ref> [61] </ref>. <p> Thus there are a total of three transpose operations needed per iteration. The advantage of this method is that computations within each sweep are completely local to a processor. However, the transpose operations between sweeps can result in significant overhead on bandwidth-limited machines. * The multi-partition or Bruno-Capello decomposition <ref> [61, 74] </ref>: this is a static decomposition where the computational mesh is divided into cubes, and each cube is assigned to a processor such that all processors are active at all stages in each of the three sweeps.
Reference: [62] <author> B. Hendrickson and R. Leland. </author> <title> A multilevel algorithm for partitioning graphs. </title> <booktitle> In Proceedings of Supercomputing 95, </booktitle> <month> December </month> <year> 1995. </year>
Reference-contexts: Since each object interacts with neighboring objects, communication between regions can be minimized by minimizing the length / area of the boundary between regions. Several techniques have been researched in the past, based on orthogonal bisection, spectral bisection, index-based mapping, genetic algorithms, linear programming, and multi-level methods <ref> [62, 63, 64] </ref>. The information required for these algorithms is the load per object and the positions of objects in space, both of which can be specified as local variables of the objects.
Reference: [63] <author> B. Hendrickson and R. Leland. </author> <title> The Chaco user's guide. </title> <type> Technical Report SAND 93-2339, </type> <institution> Sandia National Laboratories, </institution> <address> Albuquerque, NM, </address> <month> October </month> <year> 1993. </year>
Reference-contexts: Since each object interacts with neighboring objects, communication between regions can be minimized by minimizing the length / area of the boundary between regions. Several techniques have been researched in the past, based on orthogonal bisection, spectral bisection, index-based mapping, genetic algorithms, linear programming, and multi-level methods <ref> [62, 63, 64] </ref>. The information required for these algorithms is the load per object and the positions of objects in space, both of which can be specified as local variables of the objects.
Reference: [64] <author> Chao-Wei Ou and Sanjay Ranka. </author> <title> Parallel incremental graph partitioning using linear programming. </title> <booktitle> In Proceedings of the Supercomputing 94, </booktitle> <month> November </month> <year> 1994. </year>
Reference-contexts: Since each object interacts with neighboring objects, communication between regions can be minimized by minimizing the length / area of the boundary between regions. Several techniques have been researched in the past, based on orthogonal bisection, spectral bisection, index-based mapping, genetic algorithms, linear programming, and multi-level methods <ref> [62, 63, 64] </ref>. The information required for these algorithms is the load per object and the positions of objects in space, both of which can be specified as local variables of the objects.
Reference: [65] <author> J. Li and M. Chen. </author> <title> Index domain alignment : Minimizing the cost of cross-referencing between distributed arrays. </title> <booktitle> In 3rd Symposium on the Frontiers of Massively Parallel Computation, </booktitle> <month> October </month> <year> 1990. </year>
Reference-contexts: When there is more than one object array, it may be necessary to align the arrays such that objects from different arrays which communicate heavily are mapped to the same processor. We use the strategy developed by Li & Chen <ref> [65] </ref> for compilers. Each dimension of each array is represented as the node of a dimension-graph, and edges represent communication between dimensions. A graph partitioning heuristic partitions nodes in this dimension-graph so that inter-partition communication is minimized.
Reference: [66] <author> L.V. Kale and Sanjeev Krishnan. </author> <title> Medium grained execution in concurrent object-oriented systems. In Workshop on Efficient Implementation of Concurrent Object Oriented Languages, </title> <booktitle> at OOPSLA 1993, </booktitle> <month> September </month> <year> 1993. </year>
Reference-contexts: Requiring the programmer to explicitly specify grainsize is cumbersome, although it usually provides acceptable performance <ref> [66] </ref>. There has been research into compiler techniques for automatically determining the grainsize of computations (e.g. by merging fine-grained pieces of work [53]), however, the grainsize decision often needs to be made differently depending on run-time load conditions.
Reference: [67] <author> M. Wu and W. Shu. </author> <title> A dynamic program partitioning strategy on distributed memory systems. </title> <booktitle> In Proceedings of the International Conference on Parallel Processing, </booktitle> <month> August </month> <year> 1990. </year>
Reference-contexts: ( ShouldCreateObject () ) newchare ChareType (m) ; // create a new parallel object else new ChareType (m) ; // do work sequentially g Note that for tree structured computations, it is better to send nodes at shallower depths in the tree to other processors, because they have larger grainsizes <ref> [67, 68] </ref>. However, in order to do this in the most general and efficient manner, the runtime should be able to dynamically switch from sequential work through local function calls (on stack) to parallel work via remote objects. Currently the Charm++ runtime does not support this functionality.
Reference: [68] <author> E. Mohr, D. Kranz, and R. Halstead. </author> <title> Lazy task creation : A technique for increasing the granularity of parallel programs. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <month> July </month> <year> 1991. </year>
Reference-contexts: ( ShouldCreateObject () ) newchare ChareType (m) ; // create a new parallel object else new ChareType (m) ; // do work sequentially g Note that for tree structured computations, it is better to send nodes at shallower depths in the tree to other processors, because they have larger grainsizes <ref> [67, 68] </ref>. However, in order to do this in the most general and efficient manner, the runtime should be able to dynamically switch from sequential work through local function calls (on stack) to parallel work via remote objects. Currently the Charm++ runtime does not support this functionality. <p> If some processors become idle later, this leaves some load for redistribution 10 . Parameterized code-blocks can be used for both iterative and recursive computations. Adaptive grainsize control has been manually programmed before with good results <ref> [57, 68] </ref>. 4.6.2 Automating grainsize optimization Paradise automates the two optimizations for grainsize control described in the previous subsection.
Reference: [69] <author> S. Hiranandani, Ken Kennedy, and C-W Tseng. </author> <title> Compiler Optimizations for Fortran-D on MIMD Distributed memory machines. </title> <booktitle> In Proceedings of Supercomputing 1991, </booktitle> <month> November </month> <year> 1991. </year>
Reference-contexts: Section 4.7.2. 4.7 Communication optimizations Communication optimizations are useful in many parallel programs to reduce communication volume and overheads, as well as reduce the latency of remote data access. 4.7.1 Mechanisms for optimizing communication There has been a lot of research in optimizing communication for data-parallel applications written in HPF/Fortran90D <ref> [69, 45] </ref>, in which the compiler generates a good communication schedule or generates calls to run-time communication libraries. The run-time optimizations described below have been commonly observed to be useful for diverse applications, in a concurrent object-oriented context.
Reference: [70] <author> L. V. Kale and Sanjeev Krishnan. </author> <title> A comparison based parallel sorting algorithm. </title> <booktitle> In Proceedings of the International Conference on Parallel Processing, </booktitle> <month> August </month> <year> 1993. </year>
Reference: [71] <author> D.H. Bailey et al. </author> <title> The NAS Parallel Benchmarks. </title> <journal> Intl. Journal of Supercomputer Applications, </journal> <volume> 5(3), </volume> <year> 1996. </year>
Reference-contexts: computed the overlay of two lists of 15,000 polygons each. 103 Strategy No pipelining Automatic pipelining Best (manual) pipelining Time (sec) 526.4 15.37 15.09 Table 5.10: Time for polygon overlay program without pipelining, with automatic pipelining and with manual pipelining. 5.2 NAS scalar-pentadiagonal benchmark The NAS Scalar Pentadiagonal (SP) benchmark <ref> [71] </ref> is one of three Computational Fluid Dynamics benchmarks in the NAS benchmark suite. It is intended to represent the principal computation and communication requirements of CFD applications in use today. The SP benchmark involves the solution of multiple independent systems of scalar penta-diagonal equations which are not diagonally dominant. <p> The results are for problem size A as specified in the NAS benchmark documents <ref> [71] </ref>: the computational space is a 3-dimensional array of 64 x 64 x 64 grid points. Sync-Transpose is the block mapping which does a transpose using the parallel object array synchronous remap operation. Async-Transpose is the block mapping with asynchronous transpose (migration) of parallel objects for moving data between sweeps.
Reference: [72] <author> D.H. Bailey et al. </author> <title> The NAS Parallel Benchmarks. </title> <type> Technical Report NASA Technical Memorandum 103863, </type> <institution> NASA Ames Research Center, </institution> <month> July </month> <year> 1993. </year>
Reference-contexts: We also wished to leverage the object-orientation provided by Charm++ to develop reusable abstractions that would simplify the process of developing parallel applications. 5.2.1 Parallelization schemes The steps in the the numerical algorithm <ref> [72] </ref> which are significant for parallelization are: 104 * Computation of the RHS vector of the partial differential equation. Each grid point in the cubical mesh needs values of the U matrix from two neighboring grid points on either side, in each of the three dimensions.
Reference: [73] <author> R. Van der Wijngaart. </author> <title> Efficient implementation of a 3-dimensional adi method on the ipsc/860. </title> <booktitle> In Proceedings of Supercomputing 1993, </booktitle> <month> November </month> <year> 1993. </year>
Reference-contexts: Parallelizing these steps requires decomposition of the three-dimensional computational array among processors. This decomposition must be done so as to balance computational load across processors as well as reduce inter-processor data communication. Three of the most common methods used to parallelize ADI methods are <ref> [73] </ref>: * Pipelined static block decomposition: each processor is statically allocated a contiguous three-dimensional block of grid points for the entire length of the computation. The block is made as close to cubical as possible to minimize the amount of communication (which is proportional to surface-area of the block). <p> The results show that the Multipartition mapping is the best overall, with the Async-Transpose and Sync-Transpose decompositions being successively worse. The important point here is that Paradise chose the best (Multipartition) mapping, which has also been established as the best scheme in the literature <ref> [73] </ref>. 109 Processors 4 16 64 256 Sync-Transpose - 8.08 3.01 1.98 Async-Transpose - 7.81 2.54 1.40 Multipartition 24.63 7.60 1.98 1.00 Table 5.11: Time (in milliseconds) for different decompositions for the NAS SP benchmark (size A) on the Intel Paragon. 5.3 Adaptive fast multipole algorithm In this section we describe
Reference: [74] <author> J. Bruno and P. Capello. </author> <title> Implementing the Beam and Warming method on the hypercube. </title> <booktitle> In Proceedings of the 3rd Conference on Hypercube Concurrent Computers and Applications, </booktitle> <month> January </month> <year> 1988. </year>
Reference-contexts: Thus there are a total of three transpose operations needed per iteration. The advantage of this method is that computations within each sweep are completely local to a processor. However, the transpose operations between sweeps can result in significant overhead on bandwidth-limited machines. * The multi-partition or Bruno-Capello decomposition <ref> [61, 74] </ref>: this is a static decomposition where the computational mesh is divided into cubes, and each cube is assigned to a processor such that all processors are active at all stages in each of the three sweeps.
Reference: [75] <author> Sanjeev Krishnan and L. V. Kale. </author> <title> A parallel adaptive fast multipole algorithm for n-body problems. </title> <booktitle> In Proceedings of the International Conference on Parallel Processing, </booktitle> <month> August </month> <year> 1995. </year>
Reference-contexts: 1.40 Multipartition 24.63 7.60 1.98 1.00 Table 5.11: Time (in milliseconds) for different decompositions for the NAS SP benchmark (size A) on the Intel Paragon. 5.3 Adaptive fast multipole algorithm In this section we describe the design and implementation of a parallel adaptive fast multipole algorithm (AFMA) for N-body problems <ref> [75] </ref>. Our work is one of the first parallel implementations of the AFMA on distributed memory computers. This work on this AFMA application provided the motivation for many of the optimization ideas in this thesis, and preceded the development of Paradise. <p> Second, instead of fine-grained receiver initiated communication for expanding the tree one level at a time [87], each processor sends to its neighbors the exact part of its own local tree that they will require, resulting in just one message per neighbor (details of this step are described in <ref> [75] </ref>). Thus we use large grained, sender initiated communication to reduce data transfer overheads. <p> Each cell C has four types of interaction lists (details are described in <ref> [75] </ref>) : * U list : this contains cells which are adjacent to C or are "close enough" that the inter action needs to be computed between every pair of their particles. * V list : this contains cells which are sufficiently "well separated" from C such that their multipole expansion
Reference: [76] <author> L. Greengard and V. I. Rokhlin. </author> <title> A fast algorithm for particle simulations. </title> <journal> Journal of Computational Physics, </journal> <volume> 73, </volume> <year> 1987. </year> <month> 142 </month>
Reference-contexts: For N particles, the Fast Multipole Algorithm due to Greengard and Rokhlin <ref> [76] </ref> claims to have an O (N ) time complexity with a rigorous bound on error. Other methods include the O (N logN ) algorithms due to Appel [77] and Barnes-Hut [78]), the particle-in-cell methods [79], and the simple O (N 2 ) all-pairs algorithm.
Reference: [77] <author> A. W. Appel. </author> <title> An efficient program for many-body simulation. </title> <journal> SIAM Journal of Scientific and Statistical Computing, </journal> <volume> 6, </volume> <month> January </month> <year> 1985. </year>
Reference-contexts: For N particles, the Fast Multipole Algorithm due to Greengard and Rokhlin [76] claims to have an O (N ) time complexity with a rigorous bound on error. Other methods include the O (N logN ) algorithms due to Appel <ref> [77] </ref> and Barnes-Hut [78]), the particle-in-cell methods [79], and the simple O (N 2 ) all-pairs algorithm. Some implementations also use a Distance Class method [80] which has larger time steps for computing interactions between atoms at greater distances.
Reference: [78] <author> J. E. Barnes and P. Hut. </author> <title> A hierarchical O(NlogN) force calculation algorithm. </title> <journal> Nature, </journal> <volume> 324, </volume> <year> 1986. </year>
Reference-contexts: For N particles, the Fast Multipole Algorithm due to Greengard and Rokhlin [76] claims to have an O (N ) time complexity with a rigorous bound on error. Other methods include the O (N logN ) algorithms due to Appel [77] and Barnes-Hut <ref> [78] </ref>), the particle-in-cell methods [79], and the simple O (N 2 ) all-pairs algorithm. Some implementations also use a Distance Class method [80] which has larger time steps for computing interactions between atoms at greater distances.
Reference: [79] <author> R. W. Hockney and J. W. Eastwood. </author> <title> Computer Simulation Using Particles. </title> <publisher> McGraw Hill International, </publisher> <year> 1981. </year>
Reference-contexts: For N particles, the Fast Multipole Algorithm due to Greengard and Rokhlin [76] claims to have an O (N ) time complexity with a rigorous bound on error. Other methods include the O (N logN ) algorithms due to Appel [77] and Barnes-Hut [78]), the particle-in-cell methods <ref> [79] </ref>, and the simple O (N 2 ) all-pairs algorithm. Some implementations also use a Distance Class method [80] which has larger time steps for computing interactions between atoms at greater distances.
Reference: [80] <author> H. Heller, H. GrubMuller, and K. Schulten. </author> <title> Molecular dynamics simulation on a parallel computer. Molecular Simulation, </title> <type> 5, </type> <year> 1990. </year>
Reference-contexts: Other methods include the O (N logN ) algorithms due to Appel [77] and Barnes-Hut [78]), the particle-in-cell methods [79], and the simple O (N 2 ) all-pairs algorithm. Some implementations also use a Distance Class method <ref> [80] </ref> which has larger time steps for computing interactions between atoms at greater distances.
Reference: [81] <author> Leslie Greengard. </author> <title> The Rapid Evaluation of Potential Fields in Particle Systems. </title> <publisher> MIT Press, </publisher> <year> 1988. </year>
Reference-contexts: Multipole method would outperform the Barnes-Hut method is not clear, there has recently been a lot of interest in implementing both algorithms on parallel machines. 111 The FMA exploits the "center-of-mass" concept by approximating the effect due to a well--separated (sufficiently far away) group of particles by a multipole expansion <ref> [81] </ref>, which is a refined formalization of the center-of-mass, leading to provable error bounds. Interactions are computed between particles and groups of particles, as well as between different groups of particles.
Reference: [82] <author> J. Carrier, L. Greengard, and V. Rokhlin. </author> <title> A fast adaptive multipole algorithm for particle simulations. </title> <journal> SIAM Journal of Scientific and Statistical Computing, </journal> <volume> 9, </volume> <month> July </month> <year> 1988. </year>
Reference-contexts: For the three dimensional problem an octtree is generated. In the non-adaptive FMA, a uniform grid is imposed on the computational space, resulting in a complete tree whose leaves all have the same depth. However, this is unsuitable for non-uniform particle distributions. Hence the adaptive FMA (AFMA) <ref> [82] </ref> divides cells until the number of particles in a leaf cell is less than some specified grain-size, leading to an irregular tree which is deeper in regions corresponding to greater particle densities.
Reference: [83] <author> John A. Board et al. </author> <title> Scalable implementations of multipole accelerated algorithms for molecular dynamics. </title> <booktitle> In Proceedings of the Scalable High Performance Computing Conference, </booktitle> <month> May </month> <year> 1994. </year>
Reference-contexts: There are parallel implementations of the non-adaptive FMA on shared and distributed memory computers, including the work by Board and others <ref> [83, 84] </ref>, and of the adaptive FMA on shared memory computers [85]. Our work is one of the first parallel implementations of the AFMA on message passing computers. Our design for the parallel AFMA program consists of the following stages : 112 1. <p> The number of multipole expansion terms is 8, corresponding to the high-accuracy simulations in the work by Board <ref> [83] </ref>. The results in Tables 5.12 and 5.13 do not include the parallel tree construction step. 1 Each coordinate value in this distribution was generated by doing a bitwise AND of two random integers, and then normalizing it within the computational box.
Reference: [84] <author> L. Greengard and W. D. Gropp. </author> <title> A parallel version of the fast multipole method. </title> <journal> Computers Math Applications, </journal> <volume> 20(7), </volume> <year> 1990. </year>
Reference-contexts: There are parallel implementations of the non-adaptive FMA on shared and distributed memory computers, including the work by Board and others <ref> [83, 84] </ref>, and of the adaptive FMA on shared memory computers [85]. Our work is one of the first parallel implementations of the AFMA on message passing computers. Our design for the parallel AFMA program consists of the following stages : 112 1.
Reference: [85] <author> J. Singh, C. Holt, J. Hennessy, and A. Gupta. </author> <title> A parallel adaptive fast multipole method. </title> <booktitle> In Proceedings of Supercomputing 93, </booktitle> <month> November </month> <year> 1993. </year>
Reference-contexts: There are parallel implementations of the non-adaptive FMA on shared and distributed memory computers, including the work by Board and others [83, 84], and of the adaptive FMA on shared memory computers <ref> [85] </ref>. Our work is one of the first parallel implementations of the AFMA on message passing computers. Our design for the parallel AFMA program consists of the following stages : 112 1. Partitioning and exchange of particles among processors. 2. <p> We optimize the partitioning by obtaining the computational load for each particle from the previous iteration of the FMA since it is likely that the load does not change significantly from one iteration to the next. The technique used is similar to <ref> [85] </ref> where the measured load of a cell is distributed among its particles. This provides a very effective means of adaptively redistributing load, and results in must better load balance (Section 5.3.6 presents results for this optimization).
Reference: [86] <author> M. S. Warren and J. K. Salmon. </author> <title> Astrophysical n-body simulations using hierarchical tree data structures. </title> <booktitle> In Proceedings of Supercomputing 92, </booktitle> <month> November </month> <year> 1992. </year>
Reference-contexts: In our implementation, we have used a fast implementation of the Orthogonal Recursive Bisection (ORB) algorithm. Orthogonal recursive bisection is a well known technique which recursively partitions the computational space by planes parallel to the coordinate axes <ref> [86] </ref>. The partitions resulting from ORB are rectangular and thus convex, resulting in less communication with neighboring partitions. The direction of bisection is chosen such that every cell is bisected along its longest 113 dimension, ensuring that the ratio of surface area to volume is as low as possible.
Reference: [87] <author> M. S. Warren and J. K. Salmon. </author> <title> A parallel hashed oct-tree n-body algorithm. </title> <booktitle> In Proceedings of Supercomputing 93, </booktitle> <month> November </month> <year> 1993. </year> <month> 143 </month>
Reference-contexts: After each processor builds its local tree, it needs to get parts of other processors' trees in order to compute the remote members of the cell-cell interactions required for the AFMA. The 114 purpose of this step is similar to the Locally Essential Tree (LET) construction step in <ref> [87] </ref>, however, we avoid the overheads associated with their implementation by two optimizations. <p> Second, instead of fine-grained receiver initiated communication for expanding the tree one level at a time <ref> [87] </ref>, each processor sends to its neighbors the exact part of its own local tree that they will require, resulting in just one message per neighbor (details of this step are described in [75]). Thus we use large grained, sender initiated communication to reduce data transfer overheads.
Reference: [88] <author> A. Grama, V. Kumar, and A. Sameh. </author> <title> Scalable parallel formulations of the Barnes-Hut method for n-body simulations. </title> <booktitle> In Proceedings of Supercomputing 94, </booktitle> <month> November </month> <year> 1994. </year>
Reference-contexts: Although there is much parallelism within each stage of the AFMA, executing them sequentially (as has been done in almost all previous implementations) can lead to serious imbalances and processor idling. This is because it is difficult to balance the load in each stage by itself. In <ref> [88] </ref> an attempt has been made to explicitly overlap two stages (in the context of the Barnes-Hut method) using a "non-synchronizing" global communication protocol. <p> Thus it is not possible to get good performance with fine-grained, receiver initiated communication, as is possible for shared memory machines [89]. We have extensively used a sender-initiated advance-send protocol to reduce communication overhead in our implementation. In <ref> [88, 90] </ref> a form of advance-send is used in the context of the Barnes-Hut method by sending particle data to remote nodes instead of requesting for their particles.
Reference: [89] <author> J. P. Singh, J. L. Hennessy, and A. Gupta. </author> <title> Implications of hierarchical n-body methods for multiprocessor arc hitecture. </title> <journal> ACM Transactions on Computer Systems, </journal> <month> May </month> <year> 1995. </year>
Reference-contexts: This is especially a problem for distributed memory computers which have high message latencies. Thus it is not possible to get good performance with fine-grained, receiver initiated communication, as is possible for shared memory machines <ref> [89] </ref>. We have extensively used a sender-initiated advance-send protocol to reduce communication overhead in our implementation. In [88, 90] a form of advance-send is used in the context of the Barnes-Hut method by sending particle data to remote nodes instead of requesting for their particles.
Reference: [90] <author> P. Liu and S. Bhatt. </author> <title> Experiences with parallel n-body simulation. </title> <booktitle> In 6th Annual ACM Symposium on Parallel Algorithms and Architectu res, </booktitle> <year> 1994. </year>
Reference-contexts: Thus it is not possible to get good performance with fine-grained, receiver initiated communication, as is possible for shared memory machines [89]. We have extensively used a sender-initiated advance-send protocol to reduce communication overhead in our implementation. In <ref> [88, 90] </ref> a form of advance-send is used in the context of the Barnes-Hut method by sending particle data to remote nodes instead of requesting for their particles.
Reference: [91] <author> Attila Gursoy and L. V. Kale. </author> <title> Simulating message-driven programs. </title> <booktitle> In Proceedings of the International Conference on Parallel Processing, </booktitle> <month> August </month> <year> 1996. </year> <note> To Appear. 144 </note>
Reference-contexts: or using trace-driven simulation. * Integration with other tools : Paradise is one of several tools developed in the author's research group including: Visual Dagger, a GUI-based editor for depicting macro-dataflow and dependences within and across objects; Projections [28], a performance visualization and expert analysis tool; and a trace-driven simulator <ref> [91] </ref>. Integrating these tools into a full program development environment will greatly simplify Charm and Charm++ program development. * More optimizations and heuristics : An expert system is only as good as the amount of expert knowledge given to it.
References-found: 91

