URL: http://www.neci.nj.nec.com/homepages/giles/papers/overview.semiotic.workshop.1995.ps.Z
Refering-URL: http://www.neci.nj.nec.com/homepages/giles/papers/
Root-URL: 
Email: E-mail: fgiles,omlincg@research.nj.nec.com  
Title: Learning, Representation, and Synthesis of Discrete Dynamical Systems in Continuous Recurrent Neural Networks  
Author: C. Lee Giles a;b Christian W. Omlin a 
Address: 4 Independence Way, Princeton, NJ 08540  College Park, MD 20742  
Affiliation: a NEC Research Institute,  b Institute for Advanced Computer Studies, University of Maryland,  
Abstract: This paper gives an overview on learning and representation of discrete-time, discrete-space dynamical systems in discrete-time, continuous-space recurrent neural networks. We limit our discussion to dynamical systems (recurrent neural networks) which can be represented as finite-state machines (e.g. discrete event systems [53]). In particular, we discuss how a symbolic representation of the learned states and dynamics can be extracted from trained neural networks, and how (partially) known deterministic finite-state automata (DFAs) can be encoded in recurrent networks. While the DFAs that can be learned exactly with recurrent neural networks are generally small (on the order of 20 states), there exist subclasses of DFAs with on the order of 1000 states that can be learned by small recurrent networks. However, recent work in natural language processing implies that recurrent networks can possibly learn larger state systems [35]. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Y. Abu-Mostafa, </author> <title> "Learning from hints in neural networks," </title> <journal> Journal of Complexity, </journal> <volume> vol. 6, </volume> <editor> p. </editor> <volume> 192, </volume> <year> 1990. </year>
Reference: [2] <author> R. Alquezar and A. Sanfeliu, </author> <title> "An algebraic framework to represent finite state automata in single-layer recurrent neural networks," </title> <booktitle> Neural Computation, </booktitle> <year> 1995. </year> <note> In press. </note>
Reference: [3] <author> J. Alspector and R. Allen, </author> <title> "A neuromorphic VLSI learning system," </title> <booktitle> in Advanced Research in VLSI: Proceedings of the 1987 Stanford Conference (P. </booktitle> <editor> Losleben, ed.), </editor> <publisher> (Cambridge), </publisher> <pages> pp. 313-349, </pages> <publisher> MIT Press, </publisher> <year> 1987. </year>
Reference: [4] <author> P. Ashar, S. Devadas, and A. </author> <title> Newton, Sequen tial Logic Synthesis. </title> <publisher> Norwell, </publisher> <address> MA: </address> <publisher> Kluwer Academic Publishers, </publisher> <year> 1992. </year>
Reference: [5] <author> J. Bajer and P. Lisonek, </author> <title> "Symbolic computa tion approach to nonlinear dynamics," </title> <journal> Journal of Modern Optics, </journal> <volume> vol. 38, no. 4, </volume> <editor> p. </editor> <volume> 719, </volume> <year> 1991. </year>
Reference: [6] <author> Y. Bengio, P. Simard, and P. Frasconi, </author> <title> "Learn ing long-term dependencies with gradient descent is difficult," </title> <journal> IEEE Transactions on Neural Networks, </journal> <volume> vol. 5, no. 2, </volume> <pages> pp. 157-166, </pages> <year> 1994. </year>
Reference: [7] <author> M. Casey, </author> <title> Computation in discrete-time dynam ical systems. </title> <type> PhD thesis, </type> <institution> Department of Mathematics, University of California at San Diego, La Jolla, </institution> <address> CA, </address> <year> 1995. </year>
Reference: [8] <author> A. Cleeremans, D. Servan-Schreiber, and J. Mc Clelland, </author> <title> "Finite state automata and simple recurrent recurrent networks," </title> <journal> Neural Computation, </journal> <volume> vol. 1, no. 3, </volume> <pages> pp. 372-381, </pages> <year> 1989. </year>
Reference: [9] <author> D. Clouse, C. Giles, B. Horne, and G. Cottrell, </author> <title> "Learning large debruijn automata with feed-forward neural networks," </title> <type> Tech. Rep. </type> <institution> CS94-398, Computer Science and Engineering, University of California at San Diego, La Jolla, </institution> <address> CA, </address> <year> 1994. </year>
Reference: [10] <author> J. Crutchfield and K. Young, </author> <title> "Computation at the onset of chaos," </title> <booktitle> in Proceedings of the 1988 Workshop on Complexity, Entropy and the Physics of Information (W. </booktitle> <editor> Zurek, ed.), </editor> <address> (Red-wood City, CA), </address> <pages> pp. 223-269, </pages> <publisher> Addison-Wesley, </publisher> <year> 1991. </year>
Reference: [11] <author> S. Das, C. Giles, and G. Sun, </author> <title> "Learning context free grammars: Limitations of a recurrent neural network with an external stack memory," </title> <booktitle> in Proceedings of The Fourteenth Annual Conference of the Cognitive Science Society, </booktitle> <address> (San Ma-teo, CA), </address> <pages> pp. 791-795, </pages> <publisher> Morgan Kaufmann Publishers, </publisher> <year> 1992. </year>
Reference: [12] <author> S. Das and M. Mozer, </author> <title> "A unified gradient descent/clustering architecture for finite state machine induction," </title> <booktitle> in Advances in Neural Information Processing Systems 6 (J. </booktitle> <editor> Cowan, G. Tesauro, and J. Alspector, </editor> <booktitle> eds.), </booktitle> <pages> pp. 19-26, </pages> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann, </publisher> <year> 1994. </year>
Reference: [13] <author> K. Doya, </author> <title> "Universality of fully-connected recur rent neural networks," </title> <type> tech. rep., </type> <institution> Department of Biology, University of California, </institution> <address> San Diego, La Jolla, CA 92093, </address> <year> 1993. </year>
Reference: [14] <author> J. Elman, </author> <title> "Distributed representations, sim ple recurrent networks, and grammatical structure," </title> <journal> Machine Learning, </journal> <volume> vol. 7, no. 2/3, </volume> <pages> pp. 195-226, </pages> <year> 1991. </year>
Reference: [15] <author> J. Elman, </author> <title> "Finding structure in time," </title> <journal> Cogni tive Science, </journal> <volume> vol. 14, </volume> <pages> pp. 179-211, </pages> <year> 1990. </year>
Reference: [16] <author> P. Frasconi, M. Gori, M. Maggini, and G. </author> <title> Soda, "Representation of finite state automata in recurrent radial basis function networks," </title> <booktitle> Machine Learning, </booktitle> <year> 1995. </year> <note> In press. </note>
Reference: [17] <author> P. Frasconi, M. Gori, M. Maggini, and G. </author> <title> Soda, "Unified integration of explicit rules and learning by example in recurrent networks," </title> <journal> IEEE Transactions on Knowledge and Data Engineering, </journal> <volume> vol. 7, no. 2, </volume> <pages> pp. 340-346, </pages> <year> 1995. </year>
Reference: [18] <author> P. Frasconi, M. Gori, and G. </author> <title> Soda, "Injecting nondeterministic finite state automata into recurrent networks," </title> <type> tech. rep., </type> <institution> Dipartimento di Sistemi e Informatica, Universita di Firenze, Italy, Florence, Italy, </institution> <year> 1993. </year>
Reference: [19] <author> S. Geman, E. Bienenstock, and R. </author> <title> Dour stat, "Neural networks and the bias/variance dilemma," </title> <journal> Neural Computation, </journal> <volume> vol. 4, no. 1, </volume> <pages> pp. 1-58, </pages> <year> 1992. </year>
Reference: [20] <author> C. Giles, B. Horne, and T. Lin, </author> <title> "Learning a class of large finite state machines with a recurrent neural network," </title> <booktitle> Neural Networks, </booktitle> <year> 1995. </year> <note> In press. </note>
Reference: [21] <author> C. Giles, G. Kuhn, and R. Williams, </author> <title> "Dynamic recurrent neural networks: </title> <journal> Theory and applications," IEEE Transactions on Neural Networks, </journal> <volume> vol. 5, no. 2, </volume> <pages> pp. 153-156, </pages> <year> 1994. </year>
Reference: [22] <author> C. Giles, C. Miller, D. Chen, H. Chen, G. Sun, and Y. Lee, </author> <title> "Learning and extracting finite state automata with second-order recurrent neural networks," </title> <journal> Neural Computation, </journal> <volume> vol. 4, no. 3, </volume> <pages> pp. 393-405, </pages> <year> 1992. </year>
Reference: [23] <author> C. Giles and C. Omlin, </author> <title> "Inserting rules into re current neural networks," in Neural Networks for Signal Processing II, </title> <booktitle> Proceedings of The 1992 IEEE Workshop (S. </booktitle> <editor> Kung, F. Fallside, J. A. Sorenson, and C. Kamm, eds.), (Piscat-away, </editor> <address> NJ), </address> <pages> pp. 13-22, </pages> <publisher> IEEE Press, </publisher> <year> 1992. </year>
Reference: [24] <author> C. Giles, G. Sun, H. Chen, Y. Lee, and D. Chen, </author> <title> "Higher order recurrent networks & grammatical inference," </title> <booktitle> in Advances in Neural Information Processing Systems 2 (D. </booktitle> <editor> Touretzky, ed.), </editor> <address> (San Mateo, CA), </address> <pages> pp. 380-387, </pages> <publisher> Morgan Kauf-mann Publishers, </publisher> <year> 1990. </year>
Reference: [25] <author> M. Goudreau and C. Giles, </author> <title> "Using recurrent neural networks to learn the structure of interconnection networks," </title> <booktitle> Neural Networks, </booktitle> <year> 1995. </year> <note> In press. </note>
Reference: [26] <author> H. Graf, L. Jackel, and W. Hubbard, </author> <title> "VLSI im plementation of a neural network model," </title> <journal> Computer, </journal> <volume> vol. 21, no. 3, </volume> <pages> pp. 41-49, </pages> <year> 1988. </year>
Reference: [27] <author> Y.-C. Ho, ed., </author> <title> Discrete Event Dynamic Systems, Analyzing Complexity and Performance in the Modern World. </title> <address> Piscataway, NJ: </address> <publisher> IEEE Press, </publisher> <year> 1992. </year>
Reference: [28] <author> J. Hopcroft and J. Ullman, </author> <title> Introduction to Au tomata Theory, </title> <booktitle> Languages, and Computation. </booktitle> <address> Reading, MA: </address> <publisher> Addison-Wesley Publishing Company, Inc., </publisher> <year> 1979. </year>
Reference: [29] <author> B. Horne and C. Giles, </author> <title> "An experimental com parison of recurrent neural networks," </title> <booktitle> in Advances in Neural Information Processing Systems 7 (G. </booktitle> <editor> Tesauro, D. Touretzky, and T. Leen, eds.), </editor> <publisher> MIT Press, </publisher> <year> 1995. </year> <note> To appear. </note>
Reference: [30] <author> M. Jabri and B. Flower, </author> <title> "Weight perturbation: An optimal architecture and learning technique for analog VLSI feedforward and recurrent multilayer networks," </title> <journal> Neural Computation, </journal> <volume> vol. 3, no. 4, </volume> <pages> pp. 546-565, </pages> <year> 1991. </year>
Reference: [31] <author> S. Kirkpatrick, C. Gelatt Jr., and M. Vecchi, </author> <title> "Optimization by simulated annealing," </title> <journal> Science, </journal> <volume> vol. 220, </volume> <pages> pp. 671-680, </pages> <year> 1983. </year>
Reference: [32] <author> C. Koch, </author> <title> "Seeing chips: Analog VLSI cir cuits for computer vision," </title> <journal> Neural Computation, </journal> <volume> vol. 1, no. 2, </volume> <pages> pp. 184-200, </pages> <year> 1989. </year>
Reference: [33] <author> Z. Kohavi, </author> <title> Switching and Finite Automata The ory. </title> <address> New York, NY: </address> <publisher> McGraw-Hill, Inc., </publisher> <address> seconded., </address> <year> 1978. </year>
Reference: [34] <author> J. Kolen, </author> <title> "Recurrent networks: State machines or iterated function systems?," </title> <booktitle> in Proceedings of the 1993 Connectionist Models Summer School (M. </booktitle> <editor> Mozer, P. Smolensky, D. Touretzky, J. El-man, and A. Weigend, eds.), </editor> <address> (Hillsdale, NJ), </address> <publisher> Lawrence Erlbaum, </publisher> <year> 1994. </year>
Reference: [35] <author> S. Lawrence, C. Giles, and S. Fong, </author> <title> "On the ap plicability of neural network and machine learning methodologies to natural language processing," </title> <type> Tech. Rep. </type> <institution> UMIACS-TR-95-64, University of Maryland, College Park, MD 20742, </institution> <year> 1995. </year>
Reference: [36] <author> R. Maclin and J. Shavlik, </author> <title> "Refining algorithms with knowledge-based neural networks: Improving the chou-fasman algorithm for protein folding," in Computational Learning Theory and Natural Learning Systems (S. </title> <editor> Hanson, G. Drastal, and R. Rivest, eds.), </editor> <publisher> MIT Press, </publisher> <year> 1992. </year>
Reference: [37] <author> R. Maclin and J. Shavlik, </author> <title> "Refining domain theories expressed as finite-state automata," </title> <booktitle> in Proceedings of the Eighth International Workshop on Machine Learning (ML'91) (L. </booktitle> <editor> Birn-baum and G. Collins, eds.), </editor> <address> (San Mateo, CA), </address> <publisher> Morgan Kaufmann, </publisher> <year> 1991. </year>
Reference: [38] <author> N. May and D. Hammerstrom, </author> <title> "Fault simula tion of a wafer-scale integrated neural network," </title> <type> Technical Report: </type> <institution> CS/E-88-020, </institution> <year> 1988. </year>
Reference: [39] <author> C. Mead, </author> <title> Analog VLSI and Neural Systems. </title> <address> Reading: </address> <publisher> Addison Wesley, </publisher> <year> 1989. </year>
Reference: [40] <author> A. </author> <title> Meystel, "Multiscale models and con trollers," </title> <booktitle> in Proceedings of IEEE/IFAC Joint Symposium on Computer-Aided Control System Design, </booktitle> <pages> pp. 13-26, </pages> <year> 1994. </year>
Reference: [41] <author> K. Narendra and K. Parthasarathy, </author> <title> "Identifi cation and control of dynamical systems using neural networks," </title> <journal> IEEE Trans. on Neural Networks, </journal> <volume> vol. 1, no. 1, </volume> <editor> p. </editor> <volume> 4, </volume> <year> 1990. </year>
Reference: [42] <author> A. Nerode and W. Kohn, </author> <title> "Models for hybrid systems: Automata, topologies, controllability, observability," </title> <type> Tech. Rep. TR 93-28, </type> <institution> Mathematical Sciences Institute, Cornell University, </institution> <address> Ithaca, New York 14850, </address> <month> June </month> <year> 1993. </year>
Reference: [43] <author> O. Nerrand, P. Roussel-Ragot, G. D. L. Person naz, and S. Marcos, </author> <title> "Neural networks and nonlinear adaptive filtering: Unifying concepts and new algorithms," </title> <journal> Neural Computation, </journal> <volume> vol. 5, </volume> <pages> pp. 165-197, </pages> <year> 1993. </year>
Reference: [44] <author> P. T. no, B. Horne, and C.L.Giles, </author> <title> "Finite state machines and recurrent neural networks automata and dynamical systems approaches," </title> <type> Tech. Rep. </type> <institution> UMIACS-TR-95-1, Institute for Advance Computer Studies, University of Mary-land, College Park, MD 20742, </institution> <year> 1995. </year>
Reference: [45] <author> C. Omlin and C. Giles, </author> <title> "Constructing determin istic finite-state automata in recurrent neural networks," </title> <type> Tech. Rep. </type> <institution> UMIACS-TR-95-50 and CS-TR-3460, University of Maryland, College Park, MD 20742, </institution> <year> 1995. </year>
Reference: [46] <author> C. Omlin and C. Giles, </author> <title> "Extraction of rules from discrete-time recurrent neural networks," </title> <booktitle> Neural Networks, </booktitle> <year> 1995. </year> <note> Accepted for publication. </note>
Reference: [47] <author> C. Omlin and C. Giles, </author> <title> "Fault-tolerant imple mentation of finite-state automata in recurrent neural networks," </title> <type> Tech. Rep. 95-3, </type> <institution> Computer Science Department, Rensselaer Polytechnic Institute, </institution> <address> Troy, NY 12180, </address> <year> 1995. </year>
Reference: [48] <author> C. Omlin and C. Giles, </author> <title> "Rule checking with recurrent neural networks," </title> <journal> IEEE Transactions on Knowledge and Data Engineering, </journal> <note> 1995. in press. </note>
Reference: [49] <author> C. Omlin and C. Giles, </author> <title> "Stable encoding of large finite-state automata in recurrent neural networks with sigmoid discriminants," </title> <booktitle> Neural Computation, </booktitle> <year> 1995. </year> <note> Accepted for publication. </note>
Reference: [50] <author> C. Omlin, C. Giles, and C. Miller, </author> <title> "Heuristics for the extraction of rules from discrete-time recurrent neural networks," </title> <booktitle> in Proceedings International Joint Conference on Neural Networks 1992, </booktitle> <volume> vol. I, </volume> <pages> pp. 33-38, </pages> <month> June </month> <year> 1992. </year>
Reference: [51] <author> P. Peleties and R. DeCarlo, </author> <title> "Analysis of a hy brid system using symbolic dynamics and Petri nets," </title> <journal> Automatica, </journal> <volume> vol. 30, no. 9, </volume> <editor> p. </editor> <volume> 1421, </volume> <year> 1991. </year>
Reference: [52] <author> J. Pollack, </author> <title> "The induction of dynamical rec ognizers," </title> <journal> Machine Learning, </journal> <volume> vol. 7, no. 2/3, </volume> <pages> pp. 227-252, </pages> <year> 1991. </year>
Reference: [53] <author> P. Ramadge and W. Wonham, </author> <title> "The control of discrete event systems," </title> <booktitle> Proceedings of the IEEE, </booktitle> <volume> vol. 77, no. 1, </volume> <pages> pp. 81-98, </pages> <year> 1989. </year>
Reference: [54] <author> A. Rao, M. Walker, L. Clark, L. Akers, and R. Grondin, </author> <title> "VLSI implementation of neural classifiers," </title> <journal> Neural Computation, </journal> <volume> vol. 2, no. 1, </volume> <pages> pp. 35-43, </pages> <year> 1990. </year>
Reference: [55] <author> R.J.Williams and D. Zipser, </author> <title> "Gradient-based learning algorithms for recurrent networks and their computational complexity," in Backpropagation: Theory, Architectures and Applications (Y. </title> <editor> Chauvin and D. E. Rumelhart, eds.), ch. </editor> <volume> 13, </volume> <pages> pp. 433-486, </pages> <address> Hillsdale, N.J.: </address> <publisher> Lawrence Erlbaum Publishers, </publisher> <year> 1995. </year>
Reference: [56] <author> S. Satyanarayana, Y. Tsividis, and H. P. Graf, </author> <title> "A reconfigurable analog VLSI neural network chip," </title> <booktitle> in Proceedings of NIPS-89, </booktitle> <pages> pp. 758-768, </pages> <year> 1989. </year>
Reference: [57] <author> J. W. Shavlik, </author> <title> "Combining symbolic and neu ral learning," </title> <journal> Machine Learning, </journal> <volume> vol. 14, no. 3, </volume> <pages> pp. 321-331, </pages> <year> 1994. </year>
Reference: [58] <author> B. J. Sheu, </author> <booktitle> Neural Information Processing and VLSI. </booktitle> <address> Boston, MA: </address> <publisher> Kluwer Academic Publishers, </publisher> <year> 1995. </year>
Reference: [59] <author> H. Siegelmann, B. Horne, and C. Giles, </author> <title> "Com putational capabilities of recurrent narx neural networks," </title> <journal> IEEE Trans. on Systems, Man and Cybernetics, </journal> <note> 1996. accepted. </note>
Reference: [60] <author> H. Siegelmann and E. Sontag, </author> <title> "On the com putational power of neural nets," </title> <journal> Journal of Computer and System Sciences, </journal> <volume> vol. 50, no. 1, </volume> <pages> pp. 132-150, </pages> <year> 1995. </year>
Reference: [61] <author> G. Sun, C. Giles, H. Chen, and Y. Lee, </author> <title> "The neural network pushdown automaton: Model, stack and learning simulations," </title> <type> Tech. Rep. </type> <institution> UMIACS-TR-93-77, Institute for Advanced Computer Studies, University of Mary-land, College Park, MD, </institution> <year> 1993. </year>
Reference: [62] <author> P. Tino and J. Sajda, </author> <title> "Learning and extract ing initial mealy automata with a modular neural network model," </title> <journal> Neural Computation, </journal> <volume> vol. 7, </volume> <pages> pp. 822-844, </pages> <year> 1995. </year>
Reference: [63] <author> R. Watrous and G. Kuhn, </author> <title> "Induction of finite state languages using second-order recurrent networks," </title> <journal> Neural Computation, </journal> <volume> vol. 4, no. 3, </volume> <editor> p. </editor> <volume> 406, </volume> <year> 1992. </year>
Reference: [64] <author> K. Yip, </author> <title> "Understanding complex dynamics by visual and symbolic reasoning," </title> <journal> Artificial Intelligence, </journal> <volume> vol. 51, no. </volume> <pages> 1-3, </pages> <address> p. 179, </address> <year> 1991. </year>
Reference: [65] <author> Z. Zeng, R. Goodman, and P. Smyth, </author> <title> "Discrete recurrent neural networks for grammatical inference," </title> <journal> IEEE Transactions on Neural Networks, </journal> <volume> vol. 5, no. 2, </volume> <pages> pp. 320-330, </pages> <year> 1994. </year>
Reference: [66] <author> Z. Zeng, R. Goodman, and P. Smyth, </author> <title> "Learn ing finite state machines with self-clustering recurrent networks," </title> <journal> Neural Computation, </journal> <volume> vol. 5, no. 6, </volume> <pages> pp. 976-990, </pages> <year> 1993. </year>
References-found: 66

