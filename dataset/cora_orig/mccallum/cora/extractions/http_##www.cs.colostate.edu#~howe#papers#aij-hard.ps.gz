URL: http://www.cs.colostate.edu/~howe/papers/aij-hard.ps.gz
Refering-URL: http://eksl-www.cs.umass.edu/publications.html
Root-URL: 
Email: howe@cs.colostate.edu  cohen@cs.umass.edu  
Title: Understanding Planner Behavior  
Author: Adele E. Howe -- Paul R. Cohen 
Date: 413-545-3638  
Address: Fort Collins, CO 80524  Amherst, MA 01003  
Affiliation: Computer Science Department Colorado State University  Computer Science Department University of Massachusetts  
Abstract: As planners and their environments become increasingly complex, planner behavior becomes increasingly difficult to understand. We often do not understand what causes them to fail, so that we can debug their failures, and we may not understand what allows them to succeed, so that we can design the next generation. This paper describes a partially automated methodology for understanding planner behavior over long periods of time. The methodology, called Dependency Interpretation, uses statistical dependency detection to identify interesting patterns of behavior in execution traces and interprets the patterns using a weak model of the planner's interaction with its environment to explain how the patterns might be caused by the planner. Dependency Interpretation has been applied to identify possible causes of plan failures in the Phoenix planner. By analyzing four sets of execution traces gathered from about 400 runs of the Phoenix planner, we showed that the statistical dependencies describe patterns of behavior that are sensitive to the version of the planner and to increasing temporal separation between events, and that dependency detection degrades predictably as the number of available execution traces decreases and as noise is introduced in the execution traces. Dependency Interpretation is appropriate when a complete and correct model of the planner and environment is not available, but execution traces are available. fl This research was supported by a DARPA-AFOSR contract F49620-89-C-00113, the National Science Foundation under an Issues in Real-Time Computing grant, CDA-8922572, and a grant from the Office of Naval Research under the University Research Initiative N00014-86-K-0764. The US Government is authorized to reproduce and distribute reprints for governmental purposes notwithstanding any copyright notation hereon. This research was conducted as part of the first author's PhD thesis research at the University of Massachusetts. We wish to thank Cynthia Loiselle for carefully reading and translating portions of the document to latex and David Hart and David Westbrook for helping run experiments with Phoenix. We also wish to thank the reviewers for their insightful comments. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Peter C. Bates and Jack C. Wileden. </author> <title> High-level debugging of distributed systems: The behavioral abstraction approach. </title> <institution> Department of Computer and Information Science 83-29, University of Massachusetts, </institution> <address> Amherst, MA, </address> <month> March </month> <year> 1983. </year>
Reference-contexts: After we have made some modifications, we test whether our explanations were correct by executing plans and seeing whether particular dependencies disappear or are reduced. Our approach has some parallels in software testing and program analysis. For example, Bates and Wileden <ref> [1] </ref> developed a language for describing salient abstractions of a system's behavior; a module monitors the system's behavior and extracts event traces based on the desired abstractions. Gupta describes a knowledge based system for selectively collecting and analyzing traces of interprocess messages [9].
Reference: [2] <author> Scott W. Bennett. </author> <title> Learning approximate plans for use in the real world. </title> <editor> In Alberto Segre, editor, </editor> <booktitle> Proceedings of the Sixth International Workshop on Machine Learning, </booktitle> <pages> pages 224-228, </pages> <address> Palo Alto, Ca., June 26-27 1989. </address> <publisher> Morgan Kaufmann Publishers, Inc. </publisher>
Reference-contexts: Others have exploited the idea of model-based explanation of failures, bugs and errors that arise in execution traces to learn new plans (e.g., <ref> [2] </ref>, [5]). Our position is an amalgam of the structural and simulation/execution approaches focused on debugging the planner.
Reference: [3] <author> Lawrence Birnbaum, Gregg Collins, Michael Freed, and Bruce Krulwich. </author> <title> Model-based diagnosis of planning failures. </title> <booktitle> In Proceedings of the Eight National Conference on Artificial Intelligence, </booktitle> <pages> pages 318-323, </pages> <address> Boston, MA, </address> <month> July </month> <year> 1990. </year>
Reference-contexts: Related efforts, all of which rely on causal models of the planner, include Hudlicka and Lesser's work on diagnosing failures during execution [14], and Birnbaum et al.'s proposal to enhance model-based diagnosis of plans <ref> [3] </ref>. Most of the research addresses debugging plans, rather than debugging the planner. While the two are related (after all, explaining why a plan would fail is a step toward explaining why a planner should not favor such a plan), little has been done on planner debugging.
Reference: [4] <author> Lisa J. Burnell and Scott E. </author> <title> Talbot. Incorporating probabilistic reasoning in a reactive program debugging system. </title> <booktitle> In Proceedings of the Ninth Conference on Artificial Intelligence for Applications, pages 321 -327, </booktitle> <address> Orlando, FL, </address> <month> March 1-5 </month> <year> 1993. </year>
Reference-contexts: As in our knowledge-based approach, the DAACS (Dump Analysis And Consulting System) takes a snapshot of a particular type of fatal program error (the contents of a minidump) and matches information from the dump to a a belief network of canonical diagnoses <ref> [4] </ref>. The result is a set of hypotheses about the source of the failure. 3 Most research in AI debugging explains particular failures in order to debug a plan; most research from software testing addresses finding patterns of behavior in order to debug a program.
Reference: [5] <author> Steve A. Chien. </author> <title> Learning by analyzing fortuitous occurences. </title> <editor> In Alberto Segre, editor, </editor> <booktitle> Proceedings of the Sixth International Workshop on Machine Learning, </booktitle> <pages> pages 249-251, </pages> <address> Palo Alto, Ca., June 26-27 1989. </address> <publisher> Morgan Kaufmann Publishers, Inc. </publisher>
Reference-contexts: Others have exploited the idea of model-based explanation of failures, bugs and errors that arise in execution traces to learn new plans (e.g., [2], <ref> [5] </ref>). Our position is an amalgam of the structural and simulation/execution approaches focused on debugging the planner.
Reference: [6] <author> Paul R. Cohen. </author> <title> An experiment to test additivity of effects. </title> <type> Technical Report Memo no. 20, </type> <institution> University of Massachusetts, Experimental Knowedge Systems Laboratory, </institution> <month> August </month> <year> 1991. </year>
Reference-contexts: A new experiment design would selectively eliminate recovery methods or plan actions to test whether each precipitates or avoids particular failures <ref> [6] </ref>. For the analysis, we would remove an action or recovery method from consideration, which results in execution traces free from interactions with the missing action.
Reference: [7] <author> Paul R. Cohen. </author> <title> Empirical Methods for Artificial Intelligence. </title> <note> In Preparation, </note> <year> 1993. </year>
Reference-contexts: Statistical tests of contingency tables tell us the probability that apparent relationships, such as the dependency between A and B, are due to chance. (We will not describe the underlying probabilistic justification for these tests, here, but see <ref> [7, chapter 2] </ref>.) The most common test of contingency tables is the chi-square test [18], but we will use the closely-related G test because it is additive, as described later.
Reference: [8] <author> Paul R. Cohen, Michael Greenberg, David M. Hart, and Adele E. Howe. </author> <title> Trial by fire: Understanding the design requirements for agents in complex environments. </title> <journal> AI Magazine, </journal> <volume> 10(3), </volume> <month> Fall </month> <year> 1989. </year>
Reference-contexts: its suggestive structures and explanations, it can localize a broad range of bugs and should be appropriate for many planners, but it cannot guarantee that it will find all bugs or even find the actual cause of failure. 4.1 Phoenix: The Target Planner and its Environment The Phoenix system 3 <ref> [8] </ref> serves as the laboratory for this application of dependency interpretation. As shown in Figure 7, Phoenix provides the simulated environment, an agent architecture with a set of plan knowledge bases for each type of agent, and an experimental interface for automatically controlling experiments and collecting data.
Reference: [9] <author> N.K. Gupta and R.E. Seviora. </author> <title> An expert system approach to real time system debugging. </title> <booktitle> In Proceedings of the IEEE Computer Society Conference on AI Applications, </booktitle> <pages> pages 336-343, </pages> <address> Denver, CO, </address> <month> December 5-7 </month> <year> 1984. </year>
Reference-contexts: For example, Bates and Wileden [1] developed a language for describing salient abstractions of a system's behavior; a module monitors the system's behavior and extracts event traces based on the desired abstractions. Gupta describes a knowledge based system for selectively collecting and analyzing traces of interprocess messages <ref> [9] </ref>. In both of these systems, a human programmer must examine the resulting traces and localized failures to determine why the software failed and to debug it.
Reference: [10] <author> Kristian J. Hammond. </author> <title> Explaining and repairing plans that fail. </title> <booktitle> In Proceedings of the Tenth International Joint Conference on Artificial Intelligence, </booktitle> <pages> pages 109-114, </pages> <address> Milan, Italy, </address> <year> 1987. </year> <booktitle> International Joint Council on Artificial Intelligence. </booktitle>
Reference-contexts: For several years, researchers built planners to identify and satisfy increasingly complex constraints among actions; again, without actually simulating plan execution [19]. Many subsequent efforts relied on simulation or execution, however. Hammond's chef <ref> [10] </ref> simulates execution to produce an execution trace of the plan that includes relationships between plan actions and resulting states. chef chains backward through the execution trace to determine the steps that caused a failure, classifies the failure cause based on the explanation of what happened and indexes into a set
Reference: [11] <author> David M. Hart, Paul R. Cohen, and Scott D. Anderson. </author> <title> Envelopes as a vehicle for improving the efficiency of plan execution. </title> <editor> In Katia P. Sycara, editor, </editor> <booktitle> Proceedings of the Workshop on Innovative Approaches to Planning, Scheduling and Control, </booktitle> <pages> pages 71-76, </pages> <address> Palo Alto, Ca., November 1990. </address> <publisher> Morgan Kaufmann Publishers, Inc. </publisher>
Reference-contexts: Failure F ip is detected when plan monitoring indicates that progress against the fire has been insufficient and not enough time remains to complete the plan. F ip is detected by an envelope action (a structure for comparing expected to actual progress <ref> [11] </ref>) called indirect-attack-envelope (A env ). The three projection calculation actions and the envelope action appear together in three different indirect attack plans. All three indirect attack plans include the same suggestive structures: Shared Variable and Sequential Ordering.
Reference: [12] <author> Adele E. Howe. </author> <title> Accepting the Inevitable: The Role of Failure Recovery in the Design of Planners. </title> <type> PhD thesis, </type> <institution> University of Massachusetts, Department of Computer Science, </institution> <address> Amherst, MA, </address> <month> February </month> <year> 1993. </year> <month> 42 </month>
Reference-contexts: When we have found all such structures, we use them to index a library of explanations of the dependencies. All these steps are completely automated or semi-automated, and they have been applied to help us debug the failure-recovery component of the Phoenix planner <ref> [12] </ref>. It is straightforward to find statistical dependencies in execution traces, and only slightly more difficult to identify relationships, such as overlap, between dependencies. However, most dependencies signify nothing of interest.
Reference: [13] <author> Adele E. Howe and Paul R. Cohen. </author> <title> Failure recovery: A model and experiments. </title> <booktitle> In Proceedings of the Ninth National Conference on Artificial Intelligence, </booktitle> <pages> pages 801-808, </pages> <address> Anaheim, CA, </address> <month> July </month> <year> 1991. </year>
Reference-contexts: First, failure recovery influences which failures occur. Minor changes in the design of failure recovery produce significant changes in the number and types of failures (as we discovered in a series of experiments with Phoenix <ref> [13] </ref>). Second, failure recovery uses plans in ways not explicitly foreseen by its designers, but not forbidden or prevented by them either. Failure recovery repairs plans by adding or replacing portions of them.
Reference: [14] <author> Eva Hudlicka and Victor Lesser. </author> <title> Modeling and diagnosing problem-solving system behavior. </title> <journal> IEEE Transactions on Systems, Man and Cybernetics, </journal> <volume> SMC-17(3):407-419, </volume> <month> May/June </month> <year> 1987. </year>
Reference-contexts: Related efforts, all of which rely on causal models of the planner, include Hudlicka and Lesser's work on diagnosing failures during execution <ref> [14] </ref>, and Birnbaum et al.'s proposal to enhance model-based diagnosis of plans [3]. Most of the research addresses debugging plans, rather than debugging the planner.
Reference: [15] <author> Daniel E. Koshland. </author> <title> Cancer research: Prevention and therapy. </title> <journal> Science, </journal> <volume> 254(5035), </volume> <month> Novem-ber 22 </month> <year> 1991. </year>
Reference-contexts: From that set, we selected one for interpretation: [R sp ; F ip ]. We selected this dependency because the failure is a frequently occurring failure (F ip ) that is expensive to repair and the precursor includes a failure recovery 4 Daniel Koshland <ref> [15] </ref> summed up the problem of displaced frequency in an editorial concerning cancer research: "Cancer is now one of the major causes of death in the United States, despite the fact that great advances are being made in prevention and therapy.
Reference: [16] <author> L.F. Pau. </author> <title> Failure Diagnosis and Performance Monitoring, volume 11 of Control and Systems Theory. </title> <publisher> Marcel Dekker, Inc., </publisher> <address> New York, </address> <year> 1981. </year>
Reference-contexts: As in our statistical approach, a technique for hardware fault diagnosis, called correspondence analysis, classifies failure modes into "causes" by analyzing contingency tables of system test results <ref> [16] </ref>. As in our knowledge-based approach, the DAACS (Dump Analysis And Consulting System) takes a snapshot of a particular type of fatal program error (the contents of a minidump) and matches information from the dump to a a belief network of canonical diagnoses [4].
Reference: [17] <author> Reid G. Simmons. </author> <title> A theory of debugging plans and interpretations. </title> <booktitle> In Proceedings of the Seventh National Conference on Artificial Intelligence, </booktitle> <pages> pages 94-99, </pages> <address> Minneapolis, Min-nesota, </address> <year> 1988. </year> <journal> American Association for Artificial Intelligence. </journal>
Reference-contexts: Like chef, Simmons' gordius <ref> [17] </ref> debugs plans by simulating them with a causal model. gordius traces plan assumptions by regressing desired outcomes (values of states) through a causal dependency structure (generated through simulation of the plan) and identifying mismatches.
Reference: [18] <author> Robert R. Sokal and F. James Rohlf. Biometry: </author> <booktitle> The Principles and Practice of Statistics in Biological Research. W.H. </booktitle> <publisher> Freeman and Co., </publisher> <address> New York, </address> <note> second edition, </note> <year> 1981. </year>
Reference-contexts: of contingency tables tell us the probability that apparent relationships, such as the dependency between A and B, are due to chance. (We will not describe the underlying probabilistic justification for these tests, here, but see [7, chapter 2].) The most common test of contingency tables is the chi-square test <ref> [18] </ref>, but we will use the closely-related G test because it is additive, as described later.
Reference: [19] <author> Mark Stefik. </author> <title> Planning with constraints (molgen: Part 1). </title> <journal> Artificial Intelligence Journal, </journal> <volume> 16 </volume> <pages> 111-169, </pages> <year> 1981. </year>
Reference-contexts: Instead of simulating execution to discover bugs, 2 hacker could recognize structures that would lead to bugs. For several years, researchers built planners to identify and satisfy increasingly complex constraints among actions; again, without actually simulating plan execution <ref> [19] </ref>. Many subsequent efforts relied on simulation or execution, however.
Reference: [20] <author> Gerald A. Sussman. </author> <title> A computational model of skill acquisition. </title> <type> Technical Report Memo no. </type> <institution> AI-TR-297, MIT AI Lab, </institution> <year> 1973. </year> <month> 43 </month>
Reference-contexts: One is that bugs or failures can be anticipated by looking at the structure of plans, the other approach is to uncover pathologies by simulating plan execution or actually executing plans. Sussman's hacker is the earliest example of the first approach <ref> [20] </ref>: critics look at the structure of a plan at each level of its development and detect potential problems (and opportunities), which inform the next level of plan development. Instead of simulating execution to discover bugs, 2 hacker could recognize structures that would lead to bugs. <p> We believe that many of them would apply to and characterize other planners as aptly as they do Phoenix, and we are in the process of acquiring other planners and analyzing them for whether these structures apply. Following Sussman's lead on plans <ref> [20] </ref>, we envision defining canonical bugs and fixes for many classes of planners. Suggestive Structures Suggestive structures are determined by the plan language.
References-found: 20

