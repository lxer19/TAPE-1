URL: http://c.gp.cs.cmu.edu:5103/afs/cs.cmu.edu/user/lafferty/www/LS/tex/a710.ps
Refering-URL: http://c.gp.cs.cmu.edu:5103/afs/cs.cmu.edu/user/lafferty/www/LS/syllabus.html
Root-URL: http://www.cs.cmu.edu
Title: Cheating with Imperfect Transcripts  
Author: Paul Placeway and John Lafferty 
Address: Pittsburgh, Pennsylvania 15213  
Affiliation: School of Computer Science Carnegie Mellon University  
Note: To appear in Proceedings of ICSLP'96  
Abstract: In this paper, we present a formalization of this problem in terms of the source channel paradigm. We propose a simple translation model for mapping caption sequences to word sequences which updates the language model with the prior information inherent in the captions. We also describe an efficient implementation of the search in a Viterbi decoder, and present results using this system in the broadcast news domain. 
Abstract-found: 1
Intro-found: 1
Reference: 1. <author> A. Acero. </author> <title> Acoustical and Environmental Robustness in Automatic Speech Recognition. </title> <publisher> Kluwer Academic Publishers, </publisher> <address> Boston, </address> <year> 1993. </year>
Reference-contexts: Our initial experiment for the joint search used a recording of The CBS Evening News from May of 1995. We used the Sphinx-3 decoder, running semi-continuous models trained on WSJ data, with CDCN noise modeling <ref> [1] </ref>.
Reference: 2. <author> J. Brousseau, G. Foster, P. Isabelle, R. Kuhn, Y. Nor-mandin, and P. Plamondon. </author> <title> French speech recognition in an automatic dictation system for translators: the TransTalk project. </title> <booktitle> In Proceedings of Eurospeech, </booktitle> <pages> pages 193-196, </pages> <address> Madrid, </address> <year> 1995. </year>
Reference-contexts: Indeed, the framework for closed-captions described above is directly analogous to the use of a translation model to aid a speech decoder for machine-aided translation, as proposed in <ref> [2, 3] </ref>.
Reference: 3. <author> P. Brown, S. Chen, S. Della Pietra, V. Della Pietra, S. Kehler, and R. Mercer. </author> <title> Automatic speech recognition in machine aided translation. </title> <booktitle> Computer Speech and Language, </booktitle> <volume> 8 </volume> <pages> 177-187, </pages> <year> 1994. </year>
Reference-contexts: Indeed, the framework for closed-captions described above is directly analogous to the use of a translation model to aid a speech decoder for machine-aided translation, as proposed in <ref> [2, 3] </ref>. <p> This distribution takes the place of the language model in the decoder. The calculation is similar to, but simpler than, that described in <ref> [3] </ref> using a statistical translation model between French and English. The simplicity comes from the fact that the model proceeds from left to right through both strings, so that there are no crossings in the "alignment" between them. 3.
Reference: 4. <author> L. Chase, R. Rosenfeld, A. Hauptmann, M. Ravishankar, E. Thayer, P. Placeway, R. Weide, and C. Lu. </author> <title> Improvements in language, lexical, and phonetic modeling in Sphinx-II. </title> <booktitle> In Proc. Spoken Language Systems Technology Workshop. </booktitle> <publisher> Morgan Kaufmann Publishers, </publisher> <year> 1995. </year>
Reference-contexts: 1. Introduction The Informedia News on Demand project [5, 6] is concerned with storing and indexing television or radio news broadcasts, so that they may be searched and browsed in a convenient way. As part of the indexing, the Sphinx speech recognition system <ref> [8, 4] </ref> is used to produce a text transcript of the audio. When processing television news broadcasts, we may be fortunate enough to receive closed-caption text along with the video and audio tracks. The closed-captions are not a true word-for-word transcript of the audio, but they are often close.
Reference: 5. <author> M. Christel, T. Kanade, M. Mauldin, R. Reddy, M. Sirbu, S. Stevens, and H Wactlar. </author> <title> Informedia digital video library. </title> <journal> Comm. ACM, </journal> <volume> 38(4) </volume> <pages> 57-58, </pages> <year> 1995. </year>
Reference-contexts: 1. Introduction The Informedia News on Demand project <ref> [5, 6] </ref> is concerned with storing and indexing television or radio news broadcasts, so that they may be searched and browsed in a convenient way. As part of the indexing, the Sphinx speech recognition system [8, 4] is used to produce a text transcript of the audio.
Reference: 6. <author> A. G. Hauptmann, M. J. Witbrock, A. I. Rudnicky, and S. Reed. </author> <title> Speech for multimedia information retrieval. </title> <booktitle> In UIST-95 Proceedings of User Interface Software and Technology, </booktitle> <year> 1995. </year>
Reference-contexts: 1. Introduction The Informedia News on Demand project <ref> [5, 6] </ref> is concerned with storing and indexing television or radio news broadcasts, so that they may be searched and browsed in a convenient way. As part of the indexing, the Sphinx speech recognition system [8, 4] is used to produce a text transcript of the audio.
Reference: 7. <author> F. Jelinek. </author> <title> Self-organized language modeling for speech recognition. In Readings in Speech Recognition. </title> <publisher> Morgan Kaufmann Publishers, </publisher> <year> 1990. </year>
Reference-contexts: The use of an explicit model for generating captions from text is an important ingredient in our approach. A simple alternative would be to interpolate the text of the closed-captions into a large language model <ref> [7] </ref>.
Reference: 8. <author> K. F. Lee. </author> <title> Automatic Speech Recognition: The Development of the SPHINX SYSTEM. </title> <publisher> Kluwer Academic Publishers, </publisher> <address> Boston, </address> <year> 1989. </year>
Reference-contexts: 1. Introduction The Informedia News on Demand project [5, 6] is concerned with storing and indexing television or radio news broadcasts, so that they may be searched and browsed in a convenient way. As part of the indexing, the Sphinx speech recognition system <ref> [8, 4] </ref> is used to produce a text transcript of the audio. When processing television news broadcasts, we may be fortunate enough to receive closed-caption text along with the video and audio tracks. The closed-captions are not a true word-for-word transcript of the audio, but they are often close.
Reference: 9. <author> Mosur Ravishankar. </author> <title> Efficient algorithms for speech recognition. </title> <type> PhD Thesis CMU-CS-96-138, </type> <institution> Carnegie Mel-lon University, </institution> <year> 1996. </year>
Reference-contexts: This optimization works because we are starting a unique word; other methods are required when using a search vocabulary represented as a tree <ref> [10, 9] </ref>.
Reference: 10. <author> H. Ney, R. Haeb-Umbach, and B.-H. Tran. </author> <title> Improvements in beam search for 10000-word continuous speech recognition. </title> <booktitle> In Proc. ICASSP-92, volume I, </booktitle> <address> pages I-9 - I-12, </address> <year> 1992. </year>
Reference-contexts: This optimization works because we are starting a unique word; other methods are required when using a search vocabulary represented as a tree <ref> [10, 9] </ref>.
References-found: 10

