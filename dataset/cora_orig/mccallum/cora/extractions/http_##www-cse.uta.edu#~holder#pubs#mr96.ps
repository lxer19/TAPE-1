URL: http://www-cse.uta.edu/~holder/pubs/mr96.ps
Refering-URL: http://www-cse.uta.edu/~holder/pubs.html
Root-URL: 
Email: Email: chaudhry@cse.uta.edu, holder@cse.uta.edu  
Title: AN EMPIRICAL APPROACH TO SOLVING THE GENERAL UTILITY PROBLEM IN SPEEDUP LEARNING  
Author: Anurag Chaudhry and Lawrence B. Holder 
Address: Box 19015, Arlington, TX 76019-0015  
Affiliation: Learning and Planning Laboratory, Department of Computer Science Engineering University of Texas at Arlington,  
Abstract: The utility problem in speedup learning describes a common behavior of machine learning methods: the eventual degradation of performance due to increasing amounts of learned knowledge. The shape of the learning curve (cost of using a learning method vs. number of training examples) over several domains suggests a parameterized model relating performance to the amount of learned knowledge and a mechanism to limit the amount of learned knowledge for optimal performance. Many recent approaches to avoiding the utility problem in speedup learning rely on sophisticated utility measures and significant numbers of training data to accurately estimate the utility of control knowledge. Empirical results presented here and elsewhere indicate that a simple selection strategy of retaining all control rules derived from a training problem explanation quickly defines an efficient set of control knowledge from few training problems. This simple selection strategy provides a low-cost alternative to example-intensive approaches for improving the speed of a problem solver. Experimentation illustrates the existence of a minimum (representing least cost) in the learning curve which is reached after a few training examples. Stress is placed on controlling the amount of learned knowledge as opposed to which knowledge. An attempt is also made to relate domain characteristics to the shape of the learning curve.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> G. F. DeJong and R. J. Mooney. </author> <title> Explanation-based learning: An alternative view. </title> <journal> Machine Learning, </journal> <volume> 1(2) </volume> <pages> 145-176, </pages> <month> April </month> <year> 1986. </year>
Reference-contexts: Weights can be used as a measure for choosing a particular operator. 4 * Selection, rejection and preference rules are control rules that are evaluated to decide whether a domain rule is singly-applicable, not applicable, or preferred to another rule for solving the current goal. EBL <ref> [16, 1] </ref> systems learn by analyzing explanations of problem-solving behavior. Search control knowledge can be acquired by explaining why a particular operator, when applied to a state, leads to a successful solution (or alternatively, why it leads to an unsuccessful search path).
Reference: [2] <author> O. Etzioni and S. Minton. </author> <title> Why EBL produces overly-specific knowledge: A critique of the PRODIGY approaches. </title> <booktitle> In Proceedings of the Ninth International Conference on Machine Learning, </booktitle> <pages> pages 137-143, </pages> <year> 1992. </year>
Reference-contexts: This is in contrast to other approaches which apply empirical <ref> [2] </ref> and statistical [4, 5] measures to learn control rules for which there is high certainty of utility. However, these approaches require a large number of training examples to estimate the problem distribution (implying higher learning time) and ensure utile control rules.
Reference: [3] <author> N. S. Flann and T. G. Dietterich. </author> <title> A study of explanation-based methods for inductive learning. </title> <journal> Machine Learning, </journal> <volume> 4(2) </volume> <pages> 187-226, </pages> <year> 1989. </year>
Reference-contexts: Once produced, explanations can be generalized. Some approaches generalize the constants in explanations into constrained variables, but the structure of the graph that represents the explanation can also be generalized <ref> [3] </ref>. EBL systems save generalized versions of the solutions to specific problems under the assumption that this will speed up future problem solving. However, saving generalized solutions may be detrimental due to the cost associated with their use.
Reference: [4] <author> J. Gratch and G. DeJong. COMPOSER: </author> <title> A probabilistic solution to the utility problem in speed-up learning. </title> <booktitle> In Proceedings of the Tenth National Conference on Artificial Intelligence, </booktitle> <pages> pages 235-240, </pages> <year> 1992. </year>
Reference-contexts: This is in contrast to other approaches which apply empirical [2] and statistical <ref> [4, 5] </ref> measures to learn control rules for which there is high certainty of utility. However, these approaches require a large number of training examples to estimate the problem distribution (implying higher learning time) and ensure utile control rules. <p> Minton's Prodigy system [12] utilizes a utility function that evaluates control knowledge based on application cost, frequency of use and average savings. PALO [5] and Composer <ref> [4] </ref> use statistical measures to evaluate control knowledge. <p> This estimation phase eliminates rules that are poor. After a rule is added to the system, Prodigy attempts to empirically validate the utility estimate, in order to discard any remaining rules which have negative utility. 3.2 Composer The Composer <ref> [4] </ref> system embodies a probabilistic solution to the utility problem.
Reference: [5] <author> R. Greiner and I. Jurisica. </author> <title> A statistical approach to solving the EBL utility problem. </title> <booktitle> In Proceedings of the Tenth National Conference on Artificial Intelligence, </booktitle> <pages> pages 241-248, </pages> <year> 1992. </year>
Reference-contexts: This is in contrast to other approaches which apply empirical [2] and statistical <ref> [4, 5] </ref> measures to learn control rules for which there is high certainty of utility. However, these approaches require a large number of training examples to estimate the problem distribution (implying higher learning time) and ensure utile control rules. <p> Minton's Prodigy system [12] utilizes a utility function that evaluates control knowledge based on application cost, frequency of use and average savings. PALO <ref> [5] </ref> and Composer [4] use statistical measures to evaluate control knowledge. <p> Composer's strategy of generating search control knowledge is more expensive than the heuristic approach adopted by Prodigy/EBL. This is because Composer pays the penalty of matching preconditions without acquiring any of the benefits of candidate control rules. 10 3.3 PALO The PALO (Probably Approximately Locally Optimal) <ref> [5] </ref> approach adopts a hill-climbing technique that evaluates transformations 2 to the performance element (as effected by the control knowledge) using a statistical method. PALO incorporates a criterion for when to stop learning. <p> Typically, a large number of training examples are necessary to accurately estimate the problem distribution and the utility of control knowledge. Moreover, the task of finding the optimal element, even knowing the distribution is intractable most of the time <ref> [5] </ref>. On the other end of the spectrum, simply limiting the amount of the learned knowledge (while ignoring utility) may be advantageous in terms of learning time saved. PALO tries to estimate the unknown distribution, but the learning time is extremely high.
Reference: [6] <author> L. B. Holder. </author> <title> The general utility problem in machine learning. </title> <booktitle> In Proceedings of the Seventh International Conference on Machine Learning, </booktitle> <pages> pages 402-410, </pages> <year> 1990. </year>
Reference-contexts: As the amount of experience increases, the cost associated with applying that experience also increases which ultimately exceeds the benefits. This experience has low utility. The above discussion summarizes the utility problem <ref> [6, 12] </ref>: the eventual degradation of performance due to increasing amounts of learned knowledge. 1 Our approach to solving the utility problem in speedup learning requires few training examples (low learning time) and does not use utility measures.
Reference: [7] <author> L. B. Holder. </author> <title> Empirical analysis of the general utility problem in machine learning. </title> <booktitle> In Proceedings of the Tenth National Conference on Artificial Intelligence, </booktitle> <pages> pages 249-254, </pages> <year> 1992. </year>
Reference-contexts: Thus, a typical cost (classification error) curve as a function of the amount of learned knowledge will be similar to that in figure 1. The general utility problem in inductive learning has been analyzed for a number of inductive learning techniques <ref> [7] </ref>. An extensive empirical investigation has also shown that the minimum of the learning curve is frequently better than popular, statistics-based methods for addressing overfit [8].
Reference: [8] <author> L. B. Holder. </author> <title> Intermediate decision trees. </title> <booktitle> In Proceedings of the Fourteenth International Joint Conference on Artificial Intelligence, </booktitle> <pages> pages 1056-1062, </pages> <year> 1995. </year>
Reference-contexts: The general utility problem in inductive learning has been analyzed for a number of inductive learning techniques [7]. An extensive empirical investigation has also shown that the minimum of the learning curve is frequently better than popular, statistics-based methods for addressing overfit <ref> [8] </ref>. The utility problem has been verified in several speedup learning systems [13, 19, 17, 10]; however, the underlying cause is less obvious than in inductive learning.
Reference: [9] <author> L. B. Holder and A. Chaudhry. </author> <title> Simple selection of utile control rules in speedup learning. </title> <booktitle> In Proceedings of the Third International Workshop on Knowledge Compilation and Speedup Learning, </booktitle> <pages> pages 77-82, </pages> <year> 1993. </year>
Reference-contexts: Control rules are generated by solutions to training problems. Our approach is a greedy approach that keeps all control rules (generated by training problems). Duplicate control rules increment corresponding counters. As indicated by results in <ref> [9] </ref> and this work, a global minimum exists in the learning curve. Few training examples (&lt; 5) are required to reach this minimum. <p> of the search space, size of the problem space, recursive versus non-recursive domain theories) to the probability of seeing a majority of training problems that follow a certain, highly-efficient path through the search space that is also followed by a large number of other problems prevalent in the problem distribution <ref> [9] </ref>. This approach as opposed to statistical approaches could require a smaller number of training examples. In our experiments we have demonstrated the ubiquity of the general utility problem in speedup learning. We have shown that a global minimum exists in the learning curve. <p> If these results are indicative of the behavior in other domains, there should be no need for large numbers of training problems, and a set of utile control rules can be learned with less cost <ref> [9] </ref>. If the distribution of queries changes, the control strategy of the system needs to be re-evaluated. This re-evaluation will be cheaper for our approach, which requires fewer training examples to reach the minimum of the learning curve.
Reference: [10] <author> S. Markovitch and P. D. Scott. </author> <title> Utilization filtering: A method for reducing the inherent harmfulness of deductively learned knowledge. </title> <booktitle> In Proceedings of the Eleventh International Joint Conference on Artificial Intelligence, </booktitle> <pages> pages 738-743, </pages> <year> 1989. </year>
Reference-contexts: An extensive empirical investigation has also shown that the minimum of the learning curve is frequently better than popular, statistics-based methods for addressing overfit [8]. The utility problem has been verified in several speedup learning systems <ref> [13, 19, 17, 10] </ref>; however, the underlying cause is less obvious than in inductive learning. If control knowledge is used, the expected cost to solve a representative sample (from which the control knowledge was learned) of the problems is less. This may not necessarily be true for unseen examples.
Reference: [11] <author> S. Minton. </author> <title> Selectively generalizing plans for problem-solving. </title> <booktitle> In Proceedings of the Ninth International Joint Conference on Artificial Intelligence, </booktitle> <pages> pages 596-599, </pages> <year> 1985. </year>
Reference-contexts: However, all of this garnered knowledge may not be useful. There is a cost associated with applying the knowledge which may outweigh the benefits accrued via its use. For example, in a typical speedup learning system that learns macro-operators, performance improvement results from (based on a discussion in <ref> [11] </ref>): 1. Experiential bias or re-ordering effect: The learned knowledge (macro-operators) can change the path traversed to reach the goal. 2. <p> Decreased cost path: Cost of reaching a goal via a macro-operator may be less than the corresponding cost of applying the sequence of primitive operators that make up the macro operator. Performance degradation in such a system is due to (based on a discussion in <ref> [11] </ref>): 1. Duplication of the search space: If macro-operators fail to achieve the goal, the system will resort to primitive operators to solve the problem thus repeating some of the work done when macro-operators were used. 2.
Reference: [12] <author> S. Minton. </author> <title> Learning Search Control Knowledge: An Explanation-Based Approach. </title> <publisher> Kluwer Academic Publishers, </publisher> <year> 1988. </year>
Reference-contexts: As the amount of experience increases, the cost associated with applying that experience also increases which ultimately exceeds the benefits. This experience has low utility. The above discussion summarizes the utility problem <ref> [6, 12] </ref>: the eventual degradation of performance due to increasing amounts of learned knowledge. 1 Our approach to solving the utility problem in speedup learning requires few training examples (low learning time) and does not use utility measures. <p> The other technique for speeding up problem solvers is to learn some form of control knowledge (e.g., knowledge that can be applied to determine which operator to try next). The control knowledge can take many forms including operator selection, rejection and preference rules, evaluation functions and operator strengths <ref> [12] </ref>. * An evaluation function is a function that can be applied to a state to estimate how close the state is to the goal. Best-first search uses an evaluation function to guide the search. * Weights can be associated with each operator. <p> These are subjects of the following section. 3 RELATED WORK Most approaches to avoiding the utility problem in speedup learning rely on training examples to empirically evaluate the utility of learned knowledge. Minton's Prodigy system <ref> [12] </ref> utilizes a utility function that evaluates control knowledge based on application cost, frequency of use and average savings. PALO [5] and Composer [4] use statistical measures to evaluate control knowledge. <p> PALO [5] and Composer [4] use statistical measures to evaluate control knowledge. Several examples are needed to support an explanation with high confidence and adopt the corresponding control rules. 3.1 Prodigy The Prodigy system <ref> [12] </ref> evaluates the utility of problem-solving control knowledge by estimating the application cost, frequency and savings afforded by the control knowledge based on the training problems. Prodigy uses explanation-based specialization to learn from a variety of phenomena including solutions, failures and goal interactions [15, 14, 13].
Reference: [13] <author> S. Minton. </author> <title> Quantitative results concerning the utility of explanation-based learning. </title> <booktitle> In Proceedings of the Seventh National Conference on Artificial Intelligence, </booktitle> <pages> pages 564-569, </pages> <year> 1988. </year>
Reference-contexts: An extensive empirical investigation has also shown that the minimum of the learning curve is frequently better than popular, statistics-based methods for addressing overfit [8]. The utility problem has been verified in several speedup learning systems <ref> [13, 19, 17, 10] </ref>; however, the underlying cause is less obvious than in inductive learning. If control knowledge is used, the expected cost to solve a representative sample (from which the control knowledge was learned) of the problems is less. This may not necessarily be true for unseen examples. <p> EBL systems save generalized versions of the solutions to specific problems under the assumption that this will speed up future problem solving. However, saving generalized solutions may be detrimental due to the cost associated with their use. This slowdown has come to be called the utility problem <ref> [13] </ref>. 2.1.1 Macro rules EBL begins with a high-level target concept and a training example for that concept. Using a set of axioms describing the domain, the system can explain why the training example is an instance of the target concept. <p> The actual purpose of EBL is not 6 to learn more about the target concept but to re-express the target concept in a more operational manner <ref> [13] </ref>. 2.1.2 Control rules Control rules extend traditional problem solving by separating search control knowledge and domain knowledge. Control rules modify the default behavior by specifying that a particular candidate (e.g., goal, operator) should be either selected, rejected or preferred over another candidate. <p> Control rules modify the default behavior by specifying that a particular candidate (e.g., goal, operator) should be either selected, rejected or preferred over another candidate. Examples include (based on Prodigy/EBL <ref> [13, 15] </ref>): * Preference Rules: For the blocks-world domain, if on (X, Y) (to be read as block X on block Y) and on (Y, Z) are both goals at the current node in the search tree, then the latter goal should be achieved first, because achieving on (X, Y) first <p> Prodigy uses explanation-based specialization to learn from a variety of phenomena including solutions, failures and goal interactions <ref> [15, 14, 13] </ref>. Explicit target concepts describe these phenomena, and each target concept is associated with a strategy for dynamically improving the performance of the problem solver. Explanations are formulated using a theory describing the domain and the Prodigy problem solver.
Reference: [14] <author> S. Minton and J. Carbonell. </author> <title> Strategies for learning search control rules: An explanation-based approach. </title> <booktitle> In Proceedings of the Tenth International Joint Conference on Artificial Intelligence, </booktitle> <volume> volume 1, </volume> <pages> pages 228-235, </pages> <year> 1987. </year>
Reference-contexts: Prodigy uses explanation-based specialization to learn from a variety of phenomena including solutions, failures and goal interactions <ref> [15, 14, 13] </ref>. Explicit target concepts describe these phenomena, and each target concept is associated with a strategy for dynamically improving the performance of the problem solver. Explanations are formulated using a theory describing the domain and the Prodigy problem solver.
Reference: [15] <author> S. Minton, J. Carbonell, C. Knoblock, D. Kuokka, O. Etzioni, and Y. Gil. </author> <title> Explanation-based learning: A problem solving perspective. </title> <journal> In Artificial Intelligence, </journal> <volume> volume 40, </volume> <pages> pages 63-118, </pages> <year> 1989. </year>
Reference-contexts: Control rules modify the default behavior by specifying that a particular candidate (e.g., goal, operator) should be either selected, rejected or preferred over another candidate. Examples include (based on Prodigy/EBL <ref> [13, 15] </ref>): * Preference Rules: For the blocks-world domain, if on (X, Y) (to be read as block X on block Y) and on (Y, Z) are both goals at the current node in the search tree, then the latter goal should be achieved first, because achieving on (X, Y) first <p> Prodigy uses explanation-based specialization to learn from a variety of phenomena including solutions, failures and goal interactions <ref> [15, 14, 13] </ref>. Explicit target concepts describe these phenomena, and each target concept is associated with a strategy for dynamically improving the performance of the problem solver. Explanations are formulated using a theory describing the domain and the Prodigy problem solver.
Reference: [16] <author> T. M. Mitchell, R. Keller, and S. Kedar-Cabelli. </author> <title> Explanation-based generalization: A unifying view. </title> <journal> Machine Learning, </journal> <volume> 1(1) </volume> <pages> 47-80, </pages> <month> January </month> <year> 1986. </year>
Reference-contexts: Weights can be used as a measure for choosing a particular operator. 4 * Selection, rejection and preference rules are control rules that are evaluated to decide whether a domain rule is singly-applicable, not applicable, or preferred to another rule for solving the current goal. EBL <ref> [16, 1] </ref> systems learn by analyzing explanations of problem-solving behavior. Search control knowledge can be acquired by explaining why a particular operator, when applied to a state, leads to a successful solution (or alternatively, why it leads to an unsuccessful search path). <p> Using a set of axioms describing the domain, the system can explain why the training example is an instance of the target concept. The explanation is essentially a proof that shows how the training example satisfies the target concept. Specification of EBL <ref> [16] </ref>: Input: * Target Concept: A concept definition describing the concept to be learned. * Training Example: An example of the target concept. * Domain Theory: A set of rules and facts to be used in explaining how the training example is an instance of the target concept. * Operationality Criterion:
Reference: [17] <author> R. J. Mooney. </author> <title> The effect of rule use on the utility of explanation-based learning. </title> <booktitle> In Proceedings of the Eleventh International Joint Conference on Artificial Intelligence, </booktitle> <pages> pages 725-730, </pages> <year> 1989. </year>
Reference-contexts: An extensive empirical investigation has also shown that the minimum of the learning curve is frequently better than popular, statistics-based methods for addressing overfit [8]. The utility problem has been verified in several speedup learning systems <ref> [13, 19, 17, 10] </ref>; however, the underlying cause is less obvious than in inductive learning. If control knowledge is used, the expected cost to solve a representative sample (from which the control knowledge was learned) of the problems is less. This may not necessarily be true for unseen examples.
Reference: [18] <author> J. R. Quinlan. </author> <title> Induction of decision trees. </title> <journal> Machine Learning, </journal> <volume> 1(1) </volume> <pages> 81-106, </pages> <year> 1986. </year>
Reference-contexts: Inductive methods construct a hypothesis to correctly classify not only examples from the training set but other unseen examples as well. Overfit usually occurs when the hypothesis becomes complex, because a complex hypothesis tends to be overly specific 1 and hence explains the training set <ref> [18] </ref>. A complex hypothesis represents trends in the training data which may not occur in unseen examples. The specificity of the hypothesis may increase due to noise in the training examples or inadequate stopping criteria of the method, the latter being more relevant to our discussion.
Reference: [19] <author> M. Tambe and P. Rosenbloom. </author> <title> Eliminating expensive chunks by restricting expressiveness. </title> <booktitle> In Proceedings of the Eleventh International Joint Conference on Artificial Intelligence, </booktitle> <pages> pages 731-737, </pages> <year> 1989. </year>
Reference-contexts: An extensive empirical investigation has also shown that the minimum of the learning curve is frequently better than popular, statistics-based methods for addressing overfit [8]. The utility problem has been verified in several speedup learning systems <ref> [13, 19, 17, 10] </ref>; however, the underlying cause is less obvious than in inductive learning. If control knowledge is used, the expected cost to solve a representative sample (from which the control knowledge was learned) of the problems is less. This may not necessarily be true for unseen examples.
References-found: 19

