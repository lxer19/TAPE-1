URL: http://http.cs.berkeley.edu/~tzuyi/cs267/project.ps
Refering-URL: http://http.cs.berkeley.edu/~tzuyi/cs267/project.html
Root-URL: 
Title: Parallel Solvers for Almost-Tridiagonal Linear Systems  
Author: Tzu-Yi Chen 
Date: May 16, 1996  
Abstract: The problem of solving tridiagonal systems on parallel machines has been studied extensively. This paper examines an existing parallel solvers for tridiagonal systems and extends this divide-and-conquer algorithm to solving almost-tridiagonal systems, systems consisting of a tridiagonal matrix with non-zeros elements in the upper right and lower left corners. In addition to a sketch of a solver already used in global climate modeling code, where almost-tridiagonal systems arise because the problem domain wraps around a sphere, two new algorithms are proposed, one using a pipeline to send many small messages, and another sending a few large messages. In-depth descriptions are provided with pseudocode where considered helpful. The algorithms are generalized to the case where there are multiple matrices and multiple right hand side vectors to be solved for. The proposed algorithm which minimizes messages has been implemented in CMMD and a discussion of preliminary timing results is given. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> J. Demmel and S. L. Smith. </author> <title> Parallelizing a global atmospheric chemical tracer model. </title> <booktitle> In Proceedings of the Scalable High Performance Computing Conference, </booktitle> <pages> pages 718-725, </pages> <address> Knoxville, TN., </address> <month> May </month> <year> 1994. </year> <note> url: http://http.cs.berkeley.edu/~ssmith/shpcc.ps. </note>
Reference-contexts: Overall, this section gives a high-level overview of the algorithm currently in use in order to help the reader understand what a parallel algorithm must improve upon. For further discussion of the current algorithm see <ref> [1, 2] </ref>. 2.1 Basic Algorithm Each A matrix is factored into a lower triangular matrix L and an upper triangular matrix U and then backwards and forward substitution are used to solve for x. This algorithm uses a simple recurrence to factor A. <p> It is also possible to subdivide the work into more than p stages so that the pipeline is deeper at the cost of more interprocessor messages. This is described in <ref> [1] </ref>. However, the algorithm implemented in the current GATOR code remains primarily a serial one. <p> a previously developed performance models for the currently implemented parallel almost-tridiagonal system solve in GATOR, and a performance model for the minimal message algorithm presented previously in this section. 5.2.1 Current Solver Details concerning already developed performance models of the algorithm currently in place in GATOR can be found in <ref> [1, 2] </ref>. Their final result breaks down the time for the transport phase into a setup phase and a solve phase.
Reference: [2] <author> J. Demmel and S. L. Smith. </author> <title> Performance of a parallel global atmospheric chemical tracer model. </title> <note> 1995. url: http://http.cs.berkeley.edu/~ssmith/SC95/SC95.ps. 21 </note>
Reference-contexts: Overall, this section gives a high-level overview of the algorithm currently in use in order to help the reader understand what a parallel algorithm must improve upon. For further discussion of the current algorithm see <ref> [1, 2] </ref>. 2.1 Basic Algorithm Each A matrix is factored into a lower triangular matrix L and an upper triangular matrix U and then backwards and forward substitution are used to solve for x. This algorithm uses a simple recurrence to factor A. <p> a previously developed performance models for the currently implemented parallel almost-tridiagonal system solve in GATOR, and a performance model for the minimal message algorithm presented previously in this section. 5.2.1 Current Solver Details concerning already developed performance models of the algorithm currently in place in GATOR can be found in <ref> [1, 2] </ref>. Their final result breaks down the time for the transport phase into a setup phase and a solve phase. <p> The time for the latter if the direction of advection is East-West is as follows: T EW = 2N B l NLAT PLAT m fi NVS fi NBYTES nr fi fi BLONG fi l NLAT PLAT m nr trans mflops 1 A This previous equation, taken directly from <ref> [2] </ref>, rewritten in terms of NA and NRHS: T = 2N ff + NA fi NBYTES nr fi fi BLONG fi NA fi NRHS nr trans mflops ! The trans in the equation refers to the number of floating point operations needed to factor and solve for one column of the
Reference: [3] <author> D. H. Lawrie and A. H. Sameh. </author> <title> The computation and communication complexity of a parallel banded system solver. </title> <journal> ACM Transactions on Mathematical Software, </journal> <volume> 10(2) </volume> <pages> 185-195, </pages> <month> Jun </month> <year> 1984. </year>
Reference-contexts: Section 4 describes how one might solve an almost-tridiagonal system on a parallel machine by extending an algorithm for solving a purely tridiagonal system on a parallel machine. This algorithm for solving a tridiagonal system, due to Lawrie and Sameh <ref> [3] </ref>, is described in Section 3. A similar algorithm, described in [4], is part of IBM's PESSL (Parallel Engineering and Scientific Subrouting Library) package. However, in at least one application where almost-tridiagonal systems arise, many systems need to be solved numerous times. <p> Now, this paper begins by describing the current almost-tridiagonal system solver used in GATOR in Section 2. Next, the paper describes in detail the parallel banded solver presented in <ref> [3] </ref> for the specific case of a tridiagonal matrix in Section 3. Section 4 describes two variations on an algorithm which is a modification of the tridiagonal solver tailored to work on the almost-tridiagonal matrices which are the focus of this paper. <p> This algorithm is due to Lawrie and Sameh <ref> [3] </ref>. Although their paper discusses solving general banded systems, we are specifically interested in the tridiagonal case, so all the figures and pseudocode in this section assume the system is tridiagonal. Figure 2 shows the form of the tridiagonal matrix A, defining some of the submatrices used in the algorithm. <p> Note that the matrix has been decoupled into two systems. The system S consists of the elements in the rows that lie on interprocessor boundaries. The 2 (p 1) fi 2 (p 1) system S can now be factored. Lawrie and Sameh <ref> [3] </ref> shows how to do this by doing a local LU factorization on processor 1, passing a single number on to processor 2 which then does a local LU factorization and passes a single number on to processor 3, and so on. <p> Again, <ref> [3] </ref> shows how to do this by doing a local solve and then passing a single number on to the next processor. To solve LU z = h both a forward pass through the processors and a backward pass are needed. <p> This takes approximately time 12p + (2ff + 2fi)(p 1) where, again, ff is the time needed to start a message, and fi is the amount of time it takes to send one floating point number. In <ref> [3] </ref> this step is broken down into two steps, one for the forward solve and one for the backward solve. (6): Finally use the z computed in step 5 and the g i calculated in step 4 to compute the final x i on each processor. <p> Pseudocode for the forward and backward solves are given in Figure 11. This solve takes approximately time 19p, which is broken down into 9p for the backward solve and 10p for the forward solve. 4.2.2 Variation 1 In this first variation, we attempt to modify the original algorithm in <ref> [3] </ref> as little as possible. Hence, the factorization still begins with processor 1 doing a small calculation before passing along a few floating point numbers to both processor 2 and to processor p. Although the algorithm in [3] only requires each processor to pass data to the processors immediate preceding and <p> 1 In this first variation, we attempt to modify the original algorithm in <ref> [3] </ref> as little as possible. Hence, the factorization still begins with processor 1 doing a small calculation before passing along a few floating point numbers to both processor 2 and to processor p. Although the algorithm in [3] only requires each processor to pass data to the processors immediate preceding and following, the extra elements in the corners of A and S requires more communication. <p> We go through the algorithms again, explaining how they would work for multiple A matrices and multiple f vectors. Note that because the algorithm merges steps (3) and (5) from the original algorithm described in <ref> [3] </ref>, the step numbers in the algorithm following do not correspond exactly to the step numbers from Section 3. 15 5.1 Algorithm (1): Each processor i computes A i = L i U i for all NA A matrices. Each processor stores all NLAT NVERT LU decompositions. <p> To study this problem we proposed two algorithms, looked at basic performance models, and implemented one of the proposed algorithms. To develop code for solving an almost-tridiagonal system on a parallel machine, we began by studying an algorithm developed to solve symmetric positive definite banded systems in parallel <ref> [3] </ref>. This algorithm was generalized to the almost-tridiagonal case, with two algorithms proposed for solving the independent system that arises.
Reference: [4] <author> X. H. Sun, H. Zhang, and L. M. Ni. </author> <title> Efficient tridiagonal solvers on multicomputers. </title> <journal> IEEE Transactions on Computers, </journal> <volume> 41(3) </volume> <pages> 286-296, </pages> <month> Mar </month> <year> 1992. </year>
Reference-contexts: This algorithm for solving a tridiagonal system, due to Lawrie and Sameh [3], is described in Section 3. A similar algorithm, described in <ref> [4] </ref>, is part of IBM's PESSL (Parallel Engineering and Scientific Subrouting Library) package. However, in at least one application where almost-tridiagonal systems arise, many systems need to be solved numerous times. The particular application in mind is global climate modeling, where the domain is the entire globe. <p> PESSL contains two routines for solving tridiagonal systems. One routine is for general nonsymmetric matrices, and the other is for nonsymmetric matrices which do not require pivoting for numerical stability (e.g. diagonally dominant matrices). The algorithm they use is based on one described in <ref> [4] </ref>. <p> This combines step (3) from the factorization and step (5) from the solve. This also means that step (4) from the solve needs to be done before this new combined step. In fact, if only one system Sz = h needs to be solved, <ref> [4] </ref> suggests solving the same system on all p processors.
References-found: 4

