URL: http://eksl-www.cs.umass.edu/~gregory/sea/theory/spdp-draft.ps
Refering-URL: http://eksl-www.cs.umass.edu/~gregory/sea/theory/
Root-URL: 
Email: fgao,gregory,rsnbrg,coheng@cs.umass.edu  
Title: Efficient Scheduling of Branching Computations on Rings of Processors: An Empirical Study  
Author: Lixin Gao Dawn E. Gregory Arnold L. Rosenberg Paul R. Cohen 
Address: Amherst, Mass. 01003, USA  
Affiliation: Department of Computer Science University of Massachusetts  
Abstract: We empirically analyze and compare two simple, deterministic policies for scheduling dynamically evolving tree-structured computations on parallel architectures that are rings of identical processing elements (PEs). Our computations have each task either halt or spawn two independent children; they abstract, for instance, computations generated by multigrid methods. Our simpler policy, called koso, has each PE keep one child of a spawning task and pass the other to its clockwise neighbor in the ring; our more sophisticated policy, called koso ? , operates similarly, but allows child-passing only from a more heavily loaded PE to a more lightly loaded one. Both policies execute waiting tasks in increasing order of their depths in the evolving task-tree. Based on partial (mathematical) analyses of our policies' behaviors, we conjectured that both yield good parallel speedup on large classes of the computations we study, but that policy koso ? outperforms policy koso in many important situations. Not having been able to verify these conjectures analytically, we study them in this paper via a suite of carefully designed and analyzed experiments. Our experiments largely substantiate both of our conjectures. We find that both policies give close to optimal parallel speedup on large classes of computations, and that koso ? outperforms koso on these computations, except on very small processor rings. We believe that our methodology of experimental design and analysis will prove useful in other such studies.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> R.P. </author> <title> Brent (1974): The parallel evaluation of general arithmetic expressions. </title> <editor> J. </editor> <booktitle> ACM 21, </booktitle> <pages> 201-206. </pages>
Reference-contexts: algorithm designer's ability to keep all (or most) of a computer's processors fruitfully occupied all (or most) of the time. 1 The problem of balancing computational loads so as to approach this goal has received considerable attention since the advent of (even the promise of ) parallel computers; see, e.g., <ref> [1] </ref>. In this paper, we study the problem of efficiently scheduling dynamically evolving tree-structured computations on parallel computers whose underlying structure is a ring of identical processing elements (PEs).
Reference: [2] <author> R.M. </author> <title> Chamberlain (1988): Gray codes, Fast Fourier Transforms and hypercubes. </title> <booktitle> Parallel Computing 6, </booktitle> <pages> 225-233. </pages>
Reference-contexts: The benefits of randomization and of PRAM-like capabilities are evidenced in [10, 11, 12, 13]. * The low communication bandwidths of rings preclude algorithms which depend on massive amounts of carefully orchestrated communication, such as one finds in <ref> [2, 9] </ref>. * We actually orchestrate the execution of eligible tasks, rather than just balancing loads among the PEs of the ring (as in, say, [11, 13]).
Reference: [3] <author> Paul R. </author> <title> Cohen (1995): Empirical Methods for Artificial Intelligence. </title> <publisher> MIT Press. </publisher>
Reference: [4] <author> P. Fizzano, D. Karger, C. Stein, J. </author> <title> Wein (1994): Job scheduling in rings. </title> <booktitle> 6th ACM Symp. on Parallel Algorithms and Architectures, </booktitle> <pages> 210-219. </pages>
Reference-contexts: on the one hand, and just balancing computational loads, on the other, is that the former activity precludes pathological situations wherein PEs do equally much, but with little concurrency, so that one achieves no speedup over a sequential computer. (b) Our computation-scheduling problem differs substantively from job scheduling (as in <ref> [4] </ref>) in that the latter activity deals with mutually unrelated tasks having independent arrival times and durations while our problem deals with tasks whose arrival times are highly interrelated|by the dependency structure of the computation|and whose durations are assumed to be uniform. (The simplifying assumption of uniform task durations is consistent
Reference: [5] <author> L.-X. Gao and A.L. </author> <title> Rosenberg (1994): On balancing computational load on rings of processors. </title> <booktitle> 6th IEEE Symp. on Parallel and Distr. Processing, </booktitle> <pages> 478-483. </pages>
Reference-contexts: We do, however, have two analytical results which are consistent with the beliefs and may lend the reader some intuition about their origins. The first piece of analytical evidence, which concerns only policy koso, comes from <ref> [5] </ref>. It asserts that koso schedules trees that ultimately grow into complete binary trees asymptotically optimally. Theorem 1 [5] Under policy koso, the ring R p executes each input tree that grows into the height-n complete binary tree T n in time not exceeding 1 (2 n 1) + ff n <p> The first piece of analytical evidence, which concerns only policy koso, comes from <ref> [5] </ref>. It asserts that koso schedules trees that ultimately grow into complete binary trees asymptotically optimally. Theorem 1 [5] Under policy koso, the ring R p executes each input tree that grows into the height-n complete binary tree T n in time not exceeding 1 (2 n 1) + ff n where ff p &lt; 2 is a constant that depends only on p.
Reference: [6] <author> L.-X. Gao, A.L. Rosenberg, R.K. </author> <title> Sitaraman (1995): Optimal architecture-independent scheduling of fine-grain tree-sweep computations. </title> <booktitle> 7th IEEE Symp. on Parallel and Distr. Processing, </booktitle> <pages> 620-629. </pages>
Reference-contexts: Such global knowledge is well known to be very helpful in devising efficient schedules <ref> [6, 7, 8] </ref>. * The large diameters of rings render certain balancing-via-randomizing strategies that depend on low diameters unacceptably time-consuming; they also make PRAM algorithms which ignore communication latency less useful as algorithmic tools.
Reference: [7] <author> A. Gerasoulis and T. </author> <title> Yang (1992): A comparison of clustering heuristics for scheduling dags on multiprocessors. </title> <journal> J. Parallel Distr. Comput. </journal> <volume> 16, </volume> <pages> 276-291. </pages>
Reference-contexts: Such global knowledge is well known to be very helpful in devising efficient schedules <ref> [6, 7, 8] </ref>. * The large diameters of rings render certain balancing-via-randomizing strategies that depend on low diameters unacceptably time-consuming; they also make PRAM algorithms which ignore communication latency less useful as algorithmic tools.
Reference: [8] <author> A. Gerasoulis and T. </author> <title> Yang (1992): Static scheduling of parallel programs for message passing architectures. </title> <booktitle> Parallel Processing: CONPAR 92 - VAPP V. Lecture Notes in Computer Science 634, </booktitle> <publisher> Springer-Verlag, Berlin, </publisher> <pages> 601-612. </pages>
Reference-contexts: Such global knowledge is well known to be very helpful in devising efficient schedules <ref> [6, 7, 8] </ref>. * The large diameters of rings render certain balancing-via-randomizing strategies that depend on low diameters unacceptably time-consuming; they also make PRAM algorithms which ignore communication latency less useful as algorithmic tools.
Reference: [9] <author> S.L. </author> <title> Johnsson (1987): Communication efficient basic linear algebra computations on hypercube architectures. </title> <journal> J. Parallel Distr. Comput. </journal> <volume> 4, </volume> <pages> 133-172. </pages>
Reference-contexts: The benefits of randomization and of PRAM-like capabilities are evidenced in [10, 11, 12, 13]. * The low communication bandwidths of rings preclude algorithms which depend on massive amounts of carefully orchestrated communication, such as one finds in <ref> [2, 9] </ref>. * We actually orchestrate the execution of eligible tasks, rather than just balancing loads among the PEs of the ring (as in, say, [11, 13]).
Reference: [10] <author> R.M. Karp and Y. </author> <title> Zhang (1988): Randomized parallel algorithms for backtrack search and branch-and-bound computation. </title> <editor> J. </editor> <booktitle> ACM 40, </booktitle> <pages> 765-789. </pages>
Reference-contexts: The benefits of randomization and of PRAM-like capabilities are evidenced in <ref> [10, 11, 12, 13] </ref>. * The low communication bandwidths of rings preclude algorithms which depend on massive amounts of carefully orchestrated communication, such as one finds in [2, 9]. * We actually orchestrate the execution of eligible tasks, rather than just balancing loads among the PEs of the ring (as in,
Reference: [11] <author> R. Luling and B. </author> <title> Monien (1993): A dynamic, distributed load-balancing algorithm with provable good performance. </title> <booktitle> 5th ACM Symp. on Parallel Algorithms and Architectures, </booktitle> <pages> 164-172. </pages>
Reference-contexts: The benefits of randomization and of PRAM-like capabilities are evidenced in <ref> [10, 11, 12, 13] </ref>. * The low communication bandwidths of rings preclude algorithms which depend on massive amounts of carefully orchestrated communication, such as one finds in [2, 9]. * We actually orchestrate the execution of eligible tasks, rather than just balancing loads among the PEs of the ring (as in, <p> 12, 13]. * The low communication bandwidths of rings preclude algorithms which depend on massive amounts of carefully orchestrated communication, such as one finds in [2, 9]. * We actually orchestrate the execution of eligible tasks, rather than just balancing loads among the PEs of the ring (as in, say, <ref> [11, 13] </ref>).
Reference: [12] <author> A.G. </author> <title> Ranade (1994): Optimal speedup for backtrack search on a butterfly network. </title> <journal> Math. Syst. Theory 27, </journal> <pages> 85-101. </pages>
Reference-contexts: The benefits of randomization and of PRAM-like capabilities are evidenced in <ref> [10, 11, 12, 13] </ref>. * The low communication bandwidths of rings preclude algorithms which depend on massive amounts of carefully orchestrated communication, such as one finds in [2, 9]. * We actually orchestrate the execution of eligible tasks, rather than just balancing loads among the PEs of the ring (as in,
Reference: [13] <author> L. Rudolph, M. Slivkin, E. </author> <title> Upfal (1991): A simple load balancing scheme for task allocation in parallel machines. </title> <booktitle> 3rd ACM Symp. on Parallel Algorithms and Architectures, </booktitle> <pages> 237-244. 12 </pages>
Reference-contexts: The benefits of randomization and of PRAM-like capabilities are evidenced in <ref> [10, 11, 12, 13] </ref>. * The low communication bandwidths of rings preclude algorithms which depend on massive amounts of carefully orchestrated communication, such as one finds in [2, 9]. * We actually orchestrate the execution of eligible tasks, rather than just balancing loads among the PEs of the ring (as in, <p> 12, 13]. * The low communication bandwidths of rings preclude algorithms which depend on massive amounts of carefully orchestrated communication, such as one finds in [2, 9]. * We actually orchestrate the execution of eligible tasks, rather than just balancing loads among the PEs of the ring (as in, say, <ref> [11, 13] </ref>).
References-found: 13

