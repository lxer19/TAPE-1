URL: ftp://olympos.cs.umd.edu/pub/TechReports/vldb97.ps
Refering-URL: http://www.cs.toronto.edu/~mendel/dwbib.html
Root-URL: 
Email: christos@cs.umd.edu  jag@research.att.com  nikos@glue.umd.edu  
Title: Recovering Information from Summary Data  
Author: Christos Faloutsos H. V. Jagadish N. D. Sidiropoulos 
Address: College Park MD 20742  Florham Park, NJ 07932  College Park MD 20742  
Affiliation: Dept. of Computer Science and Inst. of Systems Research Univ. of Maryland  AT&T Laboratories  Inst. for Systems Research University of Maryland  
Abstract: Data is often stored in summarized form, as a histogram of aggregates (COUNTs, SUMs, or AVeraGes) over specified ranges. We study how to estimate the original detail data from the stored summary. We formulate this task as an inverse problem, specifying a well-defined cost function that has to be optimized under constraints. We show that our formulation includes the uniformity and independence assumptions as a special case, and that it can achieve better reconstruction results if we maximize the smoothness as opposed to the uniformity. In our experiments on real and synthetic datasets, the proposed method almost consistently outperforms its competitor, improving the root-mean-square error by up to 20 per cent for stock price data, and up to 90 per cent for smoother data sets. Finally, we show how to apply this theory to a variety of database problems that involve partial information, such as OLAP, data warehousing and histograms in query optimization.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Richard A. Becker, John M. Chambers, and Allan R. Wilks. </author> <title> The New S Language. Wadsworth & Brooks/Cole Advanced Books & Software, </title> <address> Pacific Grove, CA, </address> <year> 1988. </year>
Reference-contexts: See * `LYNX' dataset (real): Canadian lynx trappings data per year, 1821-1934, for a total of N =114 samples. This is a well known dataset in population biology it can be found in any time-sequence book (e.g., [2]), as well as on-line through the "S" statistical package <ref> [1] </ref>. Notice that it has a periodicity of 9-10 years. However, it is not very smooth: it has abrupt population explosions, with significantly different peak values each time. See Figure 4. The experiments were designed to answer the following questions: 1.
Reference: [2] <author> George E.P. Box, Gwilym M. Jenkins, and Gregory C. Reinsel. </author> <title> Time Series Analysis: Forecasting and Control. </title> <publisher> Prentice Hall, </publisher> <address> Englewood Cliffs, NJ, </address> <year> 1994. </year> <note> 3rd Edition. </note>
Reference-contexts: See * `LYNX' dataset (real): Canadian lynx trappings data per year, 1821-1934, for a total of N =114 samples. This is a well known dataset in population biology it can be found in any time-sequence book (e.g., <ref> [2] </ref>), as well as on-line through the "S" statistical package [1]. Notice that it has a periodicity of 9-10 years. However, it is not very smooth: it has abrupt population explosions, with significantly different peak values each time. See Figure 4. <p> We have focused mainly on 1-d signals (= time sequences), for two reasons: (a) they lead to a more clear description of the approach and (b) they are very interesting in their own right (sales patterns, stock prices, etc.) <ref> [2, 29] </ref>. However, the reduction in data size becomes particularly emphatic as the number of dimensions is increased, and the techniques presented in this paper become even more important. Here we show how we could handle higher dimensionalities.
Reference: [3] <author> Chungmin M. Chen and Nick Roussopoulos. </author> <title> Adaptive selectivity estimation using query feedback. </title> <booktitle> Proc. of the ACM-SIGMOD, </booktitle> <pages> pages 161-172, </pages> <month> May </month> <year> 1994. </year>
Reference-contexts: Ioannidis and Poosala [15] studied the trade-off between high prediction accuracy and ease of maintenance. Their recommendation was that histograms should maintain perfect information about selected attribute values, and assume the uniform distribution for the rest. A recent, adaptive method, has been suggested by Chen and Roussopoulos <ref> [3] </ref>. The idea is to approximate the unknown value distribution with a polynomial, and to use query feedback to adjust the coefficients of the polynomial.
Reference: [4] <author> S. Christodoulakis. </author> <title> Implication of certain assumptions in data base performance evaluation. </title> <journal> ACM TODS, </journal> <month> June </month> <year> 1984. </year>
Reference-contexts: Early query optimizers used the uniformity assumption [25], which provably leads to pessimistic results <ref> [4] </ref>. Modern query optimizers typically use histograms [15]. The histogram of an attribute gives the count of records that fall into each pre-determined sub-range ("bucket") 3 of the attribute range. DeWitt and Muralikrishna [21] examined combined histograms for multiple attributes.
Reference: [5] <author> Shaul Dar, H.V. Jagadish, Alon Y. Levy, and Divesh Srivastava. </author> <title> Answering SQL queries with aggregation using views. </title> <type> Technical report, </type> <institution> AT&T, </institution> <year> 1995. </year>
Reference-contexts: All of these efforts have focussed on the logical nature of partial or missing information. In our paper, there is little qualitative reasoning and the logical analysis is trivial: the emphasis is on effective numerical estimation. Finally, there is much work on views with aggregates. For instance, <ref> [5] </ref> and [11] consider how to answer queries using aggregate views, and [12] shows how to maintain such views incrementally.
Reference: [6] <author> D. E. Dudgeon and R. M. Mersereau. </author> <title> Multi-Dimensional Digital Signal Processing. </title> <publisher> Prentice-Hall, </publisher> <year> 1984. </year>
Reference-contexts: In image processing, a local smoothing function is typically used to improve the zoomed image. Linear Regularization can be used to obtain exactly the same effect <ref> [6, 28] </ref>. 7.2 Data Warehousing It should be clear that the summation constraints C k (~x) may be arbitrary. Our proposed approach can also handle overlapping intervals, as well as intervals of variable length.
Reference: [7] <author> Heinz W. Engl, Martin Hanke, and Andreas Neubaue. </author> <title> Regularization of Inverse Problems. </title> <publisher> Kluwer, </publisher> <address> Dordrecht, </address> <year> 1996. </year>
Reference-contexts: Since data has been aggregated over incompatible ranges in the two base relations, such a union cannot easily be created. In this paper, we show how to attack this reconstruction problem formally. We formulate this as an inverse problem (cf. <ref> [7] </ref>) so that we can draw upon the vast array of literature on this topic in the field of signal processing. In particular, we focus on two ways to reconstruct the missing information.
Reference: [8] <author> C. Faloutsos, H. V. Jagadish, and N. D. Sidiropoulos. </author> <title> Recovering information from summary data. </title> <type> TR 97-7, </type> <institution> Inst. for Systems Research, University of Maryland, </institution> <month> January </month> <year> 1997. </year>
Reference-contexts: Proof: Omitted, for brevity (see <ref> [8] </ref>) QED 4.2 Linear Regularization In many situations, it is expected that there will only be a small difference between successive elements of the vector. Most population distributions, for large enough populations, would follow this principle. <p> See <ref> [8] </ref> for details. QED However, if the 2-d joint distribution is smooth, we should do better with Linear Regularization.
Reference: [9] <author> Georg Gottlob and Roberto Zicari. </author> <title> Closed world database opened through null values. </title> <booktitle> In Proc. 14th Int'l Conf. on Very Large Databases, </booktitle> <pages> pages 50-61, </pages> <year> 1988. </year>
Reference-contexts: Ng and Ravishankar [22] also consider multiple summary tables, and propose a matrix-algebra criterion to choose the best combination of summary tables to answer a query. Incomplete information has been studied extensively. For example, see [14] or <ref> [9] </ref>. The use of class structure, and other aggregation mechanisms, to store partial information has been presented in [16], and to respond to queries has been studied in [26]. All of these efforts have focussed on the logical nature of partial or missing information.
Reference: [10] <author> J. Gray, A. Bosworth, A. Layman, and H. Pirahesh. </author> <title> Data cube: a relational aggregation operator generalizing group-by, cross-tab, and sub-totals. </title> <type> Technical Report No. </type> <institution> MSR-TR-95-22, Microsoft, </institution> <year> 1995. </year>
Reference-contexts: This sort of technique is at the heart of the proposal in [17]. Managing such data well is a necessary pre-requisite for effective data mining and decision support. * Statistical databases [19], particularly in conjunction with the DataCube operator <ref> [10, 13] </ref>: For example, consider Census data with income levels, given as summary tables (=histograms), with one histogram for each of several attributes (age, years in school, years in present job, geographic location etc.).
Reference: [11] <author> Ashish Gupta, Venkatesh Harinarayan, and Dallan Quass. </author> <title> Generalized projections: A powerful approach to aggregation. </title> <booktitle> In Proc. 21st International Conference on VLDB, </booktitle> <address> Zurich, Switzerland, </address> <month> September </month> <year> 1995. </year>
Reference-contexts: All of these efforts have focussed on the logical nature of partial or missing information. In our paper, there is little qualitative reasoning and the logical analysis is trivial: the emphasis is on effective numerical estimation. Finally, there is much work on views with aggregates. For instance, [5] and <ref> [11] </ref> consider how to answer queries using aggregate views, and [12] shows how to maintain such views incrementally.
Reference: [12] <author> Ashish Gupta, Inderpal Singh Mumick, and V. S. Subrahmanian. </author> <title> Maintaining views incrementally. </title> <booktitle> In Proc. of ACM SIGMOD, </booktitle> <address> Washington, D.C., </address> <month> May </month> <year> 1993. </year>
Reference-contexts: In our paper, there is little qualitative reasoning and the logical analysis is trivial: the emphasis is on effective numerical estimation. Finally, there is much work on views with aggregates. For instance, [5] and [11] consider how to answer queries using aggregate views, and <ref> [12] </ref> shows how to maintain such views incrementally.
Reference: [13] <author> Venky Harinarayan, Anand Rajaraman, and Jeffrey D. Ullman. </author> <title> Implementing data cubes efficiently. </title> <booktitle> In Proc. ACM SIGMOD, </booktitle> <pages> pages 205-216, </pages> <address> Montreal, Canada, </address> <month> May </month> <year> 1996. </year>
Reference-contexts: This sort of technique is at the heart of the proposal in [17]. Managing such data well is a necessary pre-requisite for effective data mining and decision support. * Statistical databases [19], particularly in conjunction with the DataCube operator <ref> [10, 13] </ref>: For example, consider Census data with income levels, given as summary tables (=histograms), with one histogram for each of several attributes (age, years in school, years in present job, geographic location etc.).
Reference: [14] <author> T. Imielinski and W. Lipski. </author> <title> Incomplete information in relational databases. </title> <journal> JACM, </journal> <volume> 31(4), </volume> <month> October </month> <year> 1984. </year> <month> 16 </month>
Reference-contexts: Ng and Ravishankar [22] also consider multiple summary tables, and propose a matrix-algebra criterion to choose the best combination of summary tables to answer a query. Incomplete information has been studied extensively. For example, see <ref> [14] </ref> or [9]. The use of class structure, and other aggregation mechanisms, to store partial information has been presented in [16], and to respond to queries has been studied in [26]. All of these efforts have focussed on the logical nature of partial or missing information.
Reference: [15] <author> Yannis E. Ioannidis and Viswanath Poosala. </author> <title> Balancing histogram optimality and practi-cality for query result size estimation. </title> <booktitle> ACM SIGMOD, </booktitle> <pages> pages 233-244, </pages> <month> June </month> <year> 1995. </year>
Reference-contexts: The task then is to reconstruct as good an estimate of the original base data as possible. Applications of such a generic reconstruction method abound: * Query optimization: DBMSs typically maintain histograms <ref> [15] </ref> reporting the number of tuples for selected attribute-value ranges. Queries may select only specific values, or select ranges that only partially overlap with the value ranges used in the histogram. Cost estimation for such queries will benefit from an accurate reconstruction of attribute-value occurrences for the queried value (-range). <p> Early query optimizers used the uniformity assumption [25], which provably leads to pessimistic results [4]. Modern query optimizers typically use histograms <ref> [15] </ref>. The histogram of an attribute gives the count of records that fall into each pre-determined sub-range ("bucket") 3 of the attribute range. DeWitt and Muralikrishna [21] examined combined histograms for multiple attributes. Ioannidis and Poosala [15] studied the trade-off between high prediction accuracy and ease of maintenance. <p> Modern query optimizers typically use histograms <ref> [15] </ref>. The histogram of an attribute gives the count of records that fall into each pre-determined sub-range ("bucket") 3 of the attribute range. DeWitt and Muralikrishna [21] examined combined histograms for multiple attributes. Ioannidis and Poosala [15] studied the trade-off between high prediction accuracy and ease of maintenance. Their recommendation was that histograms should maintain perfect information about selected attribute values, and assume the uniform distribution for the rest. A recent, adaptive method, has been suggested by Chen and Roussopoulos [3].
Reference: [16] <author> H. V. Jagadish. </author> <title> The incinerate data model. </title> <journal> ACM TODS, </journal> <volume> 20(1) </volume> <pages> 71-110, </pages> <month> March </month> <year> 1995. </year>
Reference-contexts: Incomplete information has been studied extensively. For example, see [14] or [9]. The use of class structure, and other aggregation mechanisms, to store partial information has been presented in <ref> [16] </ref>, and to respond to queries has been studied in [26]. All of these efforts have focussed on the logical nature of partial or missing information. In our paper, there is little qualitative reasoning and the logical analysis is trivial: the emphasis is on effective numerical estimation.
Reference: [17] <author> H. V. Jagadish, Inderpal Singh Mumick, and Avi Silberschatz. </author> <title> View maintenance issues in the chronicle data model. </title> <booktitle> In Proc. ACM PODS, </booktitle> <pages> pages 113-124, </pages> <year> 1995. </year>
Reference-contexts: Thus, older records are either stored in tertiary storage, or discarded altogether. Saving summary data on-line, and providing a reconstruction algorithm, is an attractive alternative. This sort of technique is at the heart of the proposal in <ref> [17] </ref>.
Reference: [18] <author> D. G. Luenberger. </author> <title> Introduction to Linear and Nonlinear Programming. </title> <publisher> Addison-Wesley, </publisher> <address> Reading, Massachusetts, </address> <year> 1973. </year>
Reference-contexts: Estimate ~x to minimize (maximize) F (~x) under the constraints C k (~x) = 0 k = 1; : : : ; n Under appropriate convexity and continuity conditions, the textbook method for solving both the minimization and the maximization version of such problems is the method of La-grange multipliers <ref> [18] </ref>. See Appendix A for the details. The main question is how to choose the functional F (). The objective is to minimize the expected value of the root-mean-squared error, given what we know a-priori about the distribution of values in the vector.
Reference: [19] <author> Francesco M. Malvestuto. </author> <title> A universal-scheme approach to statistical databases containing homgeneous summary tables. </title> <journal> ACM TODS, </journal> <volume> 18(4) </volume> <pages> 678-708, </pages> <month> December </month> <year> 1993. </year>
Reference-contexts: Saving summary data on-line, and providing a reconstruction algorithm, is an attractive alternative. This sort of technique is at the heart of the proposal in [17]. Managing such data well is a necessary pre-requisite for effective data mining and decision support. * Statistical databases <ref> [19] </ref>, particularly in conjunction with the DataCube operator [10, 13]: For example, consider Census data with income levels, given as summary tables (=histograms), with one histogram for each of several attributes (age, years in school, years in present job, geographic location etc.). <p> Related work appeared in statistical databases: Malvestuto <ref> [19] </ref> examined the case of multiple summary tables, and developed algorithms to determine whether a given query can be evaluated to a single number, a range, or not at all.
Reference: [20] <author> B. Mandelbrot. </author> <title> Fractal Geometry of Nature. W.H. </title> <publisher> Freeman, </publisher> <address> New York, </address> <year> 1977. </year>
Reference-contexts: It is a pleasant surprise that the Linear Regularization does better even for the `IBM' dataset, which is not smooth at all. In fact, being a stock price movement, it is expected to be a "random walk", which is known to be a "fractal", with fractal dimension 1.5 <ref> [20] </ref>. That is, it is nowhere close to being smooth.
Reference: [21] <author> M. Muralikrishna and David J. DeWitt. </author> <title> Equi-depth histograms for estimating selectivity factors for multi-dimensional queries. </title> <booktitle> Proc. ACM SIGMOD, </booktitle> <pages> pages 28-36, </pages> <month> June </month> <year> 1988. </year>
Reference-contexts: Early query optimizers used the uniformity assumption [25], which provably leads to pessimistic results [4]. Modern query optimizers typically use histograms [15]. The histogram of an attribute gives the count of records that fall into each pre-determined sub-range ("bucket") 3 of the attribute range. DeWitt and Muralikrishna <ref> [21] </ref> examined combined histograms for multiple attributes. Ioannidis and Poosala [15] studied the trade-off between high prediction accuracy and ease of maintenance. Their recommendation was that histograms should maintain perfect information about selected attribute values, and assume the uniform distribution for the rest.
Reference: [22] <author> Wee-Keong Ng and Chinya V. Ravishankar. </author> <title> Information synthesis in statistical databases. </title> <booktitle> Proc. CIKM, </booktitle> <pages> pages 355-361, </pages> <month> November </month> <year> 1995. </year>
Reference-contexts: Related work appeared in statistical databases: Malvestuto [19] examined the case of multiple summary tables, and developed algorithms to determine whether a given query can be evaluated to a single number, a range, or not at all. Ng and Ravishankar <ref> [22] </ref> also consider multiple summary tables, and propose a matrix-algebra criterion to choose the best combination of summary tables to answer a query. Incomplete information has been studied extensively. For example, see [14] or [9].
Reference: [23] <author> Alan Victor Oppenheim and Ronald W. Schafer. </author> <title> Discrete-Time Signal Processing. </title> <publisher> Prentice-Hall, </publisher> <address> Englewood Cliffs, N.J., </address> <year> 1989. </year>
Reference: [24] <author> William H. Press, Saul A. Teukolsky, William T. Vetterling, and Brian P. Flannery. </author> <title> Numerical Recipes in C. </title> <publisher> Cambridge University Press, </publisher> <year> 1992. </year> <note> 2nd Edition. </note>
Reference-contexts: While the specific error metric used is not likely to be critical, for the sake of specificity we focus on the root-mean-squared error. The theory of inverse problems <ref> [24] </ref> is applicable to the question at hand. Our specific case is typically under-constrained and thus ill-posed. Since the original vector is not known, we cannot use the root-mean-squared error as the objective function. <p> The objective is to minimize the expected value of the root-mean-squared error, given what we know a-priori about the distribution of values in the vector. In the following subsections we describe two popular criteria, namely, Maximum Entropy and Linear Regularization. 4.1 Maximum Entropy (ME) Maximum Entropy (e.g., <ref> [24, sec. 18.7] </ref>) will introduce no additional constraints on the nature of the signal to be estimated. <p> Therefore, the problem becomes: Minimize Eq. 6, subject to the conditions of Eq. 5. The functional of Eq. 6 results in an instance of so-called Linear Regularization (or `Phillips-Twomey method', or `constrained linear inversion method' or `Tikhonov-Miller regularization' <ref> [24] </ref>). <p> This may be prohibitive, particularly since N may often be large. However, in our case, the matrix is of a special form: it is almost tri-diagonal, and specifically, it is singly-bordered tri-diagonal, and the border itself is block-diagonal. In this case, matrix inversion has complexity O (M ) <ref> [24, p. 72] </ref>, that is linear on the matrix side. Since the length of the unknown distribution, N , is significantly greater than the number of batches/constraints n, for all practical purposes we can think of the inversion effort as O (N ). <p> How to do this is shown in Appendix A. The resulting matrix equation 20 can now be solved using any standard linear algebra package. In particular, we recommend the technique that makes use of the tri-diagonal nature of the matrix <ref> [24, p. 72] </ref>, as discussed earlier. 7 Extensions Discussion We have presented the theory of inverse problems, and we have shown how its special case, the Linear Regularization, can give better reconstruction from (one-dimensional) summary data.
Reference: [25] <author> P.G. Selinger, D.D. Astrahan, R.A. Chamberlain, R.A. Lorie, and T.G. Price. </author> <title> Access path selection in a relational database management system. </title> <booktitle> Proc. ACM-SIGMOD, </booktitle> <pages> pages 23-34, </pages> <year> 1979. </year>
Reference-contexts: Early query optimizers used the uniformity assumption <ref> [25] </ref>, which provably leads to pessimistic results [4]. Modern query optimizers typically use histograms [15]. The histogram of an attribute gives the count of records that fall into each pre-determined sub-range ("bucket") 3 of the attribute range. DeWitt and Muralikrishna [21] examined combined histograms for multiple attributes.
Reference: [26] <author> Chung-Dak Shum and Richard Muntz. </author> <title> An information-theoretic study on aggregate responses. </title> <booktitle> Proc. of VLDB, </booktitle> <pages> pages 479-490, </pages> <month> August </month> <year> 1988. </year>
Reference-contexts: Incomplete information has been studied extensively. For example, see [14] or [9]. The use of class structure, and other aggregation mechanisms, to store partial information has been presented in [16], and to respond to queries has been studied in <ref> [26] </ref>. All of these efforts have focussed on the logical nature of partial or missing information. In our paper, there is little qualitative reasoning and the logical analysis is trivial: the emphasis is on effective numerical estimation. Finally, there is much work on views with aggregates.
Reference: [27] <author> Yannis Theodoridis and Timos Sellis. </author> <title> A model for the prediction of r-tree performance. </title> <booktitle> Proc. of ACM PODS, </booktitle> <year> 1996. </year>
Reference-contexts: A recent, adaptive method, has been suggested by Chen and Roussopoulos [3]. The idea is to approximate the unknown value distribution with a polynomial, and to use query feedback to adjust the coefficients of the polynomial. Similar approaches have been used for spatial databases: Theodoridis and Sellis <ref> [27] </ref> suggest a coarse discretization of the address space; for each grid cell, they use the average data density, and, making the uniformity assumption for each individual grid-cell, they estimate the performance of an R-tree.
Reference: [28] <author> P. P. Vaidyanathan. </author> <title> Multirate Systems and Filter Banks. </title> <publisher> Prentice-Hall, </publisher> <year> 1993. </year>
Reference-contexts: In image processing, a local smoothing function is typically used to improve the zoomed image. Linear Regularization can be used to obtain exactly the same effect <ref> [6, 28] </ref>. 7.2 Data Warehousing It should be clear that the summation constraints C k (~x) may be arbitrary. Our proposed approach can also handle overlapping intervals, as well as intervals of variable length.
Reference: [29] <author> Andreas S. Weigend and Neil A. Gerschenfeld. </author> <title> Time Series Prediction: Forecasting the Future and Understanding the Past. </title> <publisher> Addison Wesley, </publisher> <year> 1994. </year>
Reference-contexts: We used both the Maximum Entropy method and the Linear Regularization method. The measure of success was the normalized root-mean-square error (RMS), which is a typical measure for forecasting in time series <ref> [29] </ref>. Specifically, we define: RM S = 1=N i=1 (x i x actual;i ) 2 ! 1=2 where x i is the reconstructed value and x actual;i is the actual value at time i. We ran our experiments on a number of real and synthetic datasets. These are discussed below. <p> We have focused mainly on 1-d signals (= time sequences), for two reasons: (a) they lead to a more clear description of the approach and (b) they are very interesting in their own right (sales patterns, stock prices, etc.) <ref> [2, 29] </ref>. However, the reduction in data size becomes particularly emphatic as the number of dimensions is increased, and the techniques presented in this paper become even more important. Here we show how we could handle higher dimensionalities.
Reference: [30] <author> Jennifer Widom. </author> <title> Research problems in data warehousing. </title> <address> CIKM, </address> <month> November </month> <year> 1995. </year> <type> Invited paper. 17 </type>
Reference-contexts: Cost estimation for such queries will benefit from an accurate reconstruction of attribute-value occurrences for the queried value (-range). Similarly, range queries on multiple attributes will benefit from an accurate synthesis and extrapolation from the histograms of value distributions for individual attributes. * Data warehousing <ref> [30] </ref>: The idea is that the central site will have meta-data, and condensed information (e.g., summary data) from each participating site, which has detailed information. <p> Our proposed approach can also handle overlapping intervals, as well as intervals of variable length. This is especially suitable in the case that we have to merge information from several sources, as in multi-databases and data warehousing <ref> [30] </ref>. For example, suppose that one source provides weekly (non-overlapping) sums, a second source provides the exact values for some selected days, and a third source provides monthly sums (where month boundaries do not usually coincide with week boundaries).
References-found: 30

