URL: http://www.cs.cmu.edu/afs/cs.cmu.edu/project/scandal/public/papers/treaps-spaa98.ps.gz
Refering-URL: http://www.cs.cmu.edu/afs/cs.cmu.edu/project/scandal/public/papers/treaps-spaa98.html
Root-URL: 
Email: fblelloch,mrmillerg@cs.cmu.edu  
Title: Fast Set Operations Using Treaps  
Author: Guy E. Blelloch Margaret Reid-Miller 
Address: Pittsburgh, PA 15213-3890  
Affiliation: School of Computer Science Carnegie Mellon University  
Abstract: We present parallel algorithms for union, intersection and difference on ordered sets using random balanced binary trees (treaps [26]). For two sets of size n and m (m n) the algorithms run in expected O(m lg(n=m)) work and O(lg n) depth (parallel time) on an EREW PRAM with scan operations (implying O(lg 2 n) depth on a plain EREW PRAM). As with the sequential algorithms on treaps for insertion and deletion, the main advantage of our algorithms are their simplicity. In fact, our algorithms for set operations seem simpler than previous sequential algorithms with the same work bounds, and might therefore also be useful in a sequential context. To analyze the effectiveness of the algorithms we implemented both sequential and parallel versions of the algorithms and ran several experiments on them. Our parallel implementation uses the Cilk [5] shared memory run-time system on a 16 processor SGI Power Challenge and a 6 processor Sun Ultra Enterprise 3000. It shows reasonable speedup: 6.3 to 6.8 speedup on 8 processors of the SGI, and 4.1 to 4.4 speedup on 5 processors of the Sun. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> R. J. Anderson, E. W. Meyer, and M. K. Warmuth. </author> <title> Parallel approximation algorithms for bin packing. </title> <journal> Information and Computation, </journal> <volume> 82 </volume> <pages> 262-277, </pages> <year> 1989. </year>
Reference-contexts: Anderson et al. <ref> [1] </ref>, Dekel and Ozsvath [9], Hagerup and Rub [12], and Varman et al. [29] provide O (n=p + lg n) time EREW PRAM algorithms for merging.
Reference: [2] <author> A. Baumker and W. Dittrich. </author> <title> Fully dynamic search trees for an extension of the BSP model. </title> <booktitle> In Proceedings of the 8th Annual ACM Symposium on Parallel Algorithms and Architectures, </booktitle> <pages> pages 233-242, </pages> <year> 1996. </year>
Reference-contexts: Paul et al. provide EREW PRAM search, insertion, and deletion algorithms for 2-3 trees [20], and Highan and Schenk have extended these results to B-trees [13]. Ranade [23] gives algorithms for processing least-upper-bound queries and insertion on distributed memory networks. Baumker and Dit-trich <ref> [2] </ref> give algorithms for search, insertion, and deletion into BB*(a) trees for the BSP* model that are 1-optimal and 2-optimal. All these algorithms require O (m lg n) work and appear to have large constants.
Reference: [3] <author> G. Blelloch and M. Reid-Miller. </author> <title> Pipelining with futures. </title> <booktitle> In Proceedings of the 9th Annual ACM Symposium on Parallel Algorithms and Architectures, </booktitle> <pages> pages 249-259, </pages> <month> June </month> <year> 1997. </year>
Reference-contexts: Thus, after O (h m h n ) steps the algorithms complete. Since the expected heights of t m and t n are O (lg m) and O (lg n), the expected depth of the operations are O (lg m lg n). In <ref> [3] </ref> the authors show that the depths of the operations can be reduced to O (lg m + lg n) using pipelining. Furthermore since the recursive calls in the algorithms are independent (never access the same parts of the trees) the algorithms run with exclusive reads and writes.
Reference: [4] <author> G. E. Blelloch. </author> <title> Scans as primitive parallel operations. </title> <journal> IEEE Transactions on Computers, </journal> <volume> C-38(11):1526-1538, </volume> <month> Nov. </month> <year> 1989. </year>
Reference-contexts: Such a compaction can easily be implemented with a parallel prefix (scan) operation, giving T c = lg p on a plain EREW PRAM or T c = 1 in a PRAM with scan operations <ref> [4] </ref>. 2.5 Analysis of Union with Fast Splits Here we prove bounds on the work for Union using fast splits when using the block metric.
Reference: [5] <author> R. D. Blumofe, C. F. Joerg, B. C. Kuszmaul, C. E. Leiserson, K. H. Randall, and Y. Zhou. Cilk: </author> <title> An efficient multithreaded runtime system. </title> <booktitle> In Proceedings of the Fifth ACM SIGPLAN on Principles and Practice of Parallel Programming, </booktitle> <pages> pages 207-216, </pages> <address> Santa Barbara, CA, </address> <month> July </month> <year> 1995. </year>
Reference-contexts: The paper includes the full C code for our algorithms. * The parallelism in our algorithms comes purely from their divide-and-conquer nature. The algorithms are therefore easy to implement in parallel and well suited for asynchronous execution. We have implemented the parallel versions of the algorithms in Cilk <ref> [5] </ref> with only minor changes to the sequential C code. * Our algorithms are fully persistent|they create the resulting set while leaving the input sets untouched. 1 Such persistence is important in database and indexing applications. * Our algorithms are efficient when the results are from interleaving blocks of the ordered <p> The serial experiments show that treaps are competitive with splay trees, skip lists, and red-black trees. They also show that the algorithms perform well with small overlap in the keys. The parallel implementation was coded in Cilk <ref> [5] </ref>, a parallel extension of C, and run on a Sun Ultra Enterprise 3000 and a SGI Power Challenge. The algorithms achieve speedups of between 4.1 and 4.4 on 5 processors of the Sun and between 6.3 and 6.8 on 8 processors of the SGI. <p> The constant work of any calls to union with an empty set (base of the recursion) are counted against their parent's split or join. 3 Implementation To evaluate the performance of these set-based operations, we implemented the serial treap algorithms in Gnu C and the parallel ones in Cilk 5.1 <ref> [5] </ref>. Cilk is a language for mul-tithreaded parallel programs based on ANSI C, and is designed for computations with dynamic, highly asynchronous parallelism, such as divide-and-conquer algorithms. It includes a runtime system that schedules the multithreaded computation using work-stealing and provides dag-consistent distributed shared memory.
Reference: [6] <author> M. R. Brown and R. E. Tarjan. </author> <title> A fast merging algo rithm. </title> <journal> Journal of the Association for Computing Machinery, </journal> <volume> 26(2) </volume> <pages> 211-226, </pages> <month> Apr. </month> <year> 1979. </year>
Reference-contexts: The set union operation is also closely related to merging|it is simply a merge with the duplicates removed. It is well known that for two sets of size m and n (m n) merging, union, intersection and difference can be implemented sequentially in O (m lg (n=m)) time <ref> [6] </ref>. <p> Previous sequential algorithms that achieve these bounds, however, are reasonably messy <ref> [6, 7] </ref>. The only parallel version we know of that claims to achieve these work bounds [15] is much more complicated. <p> To rearrange the data into a single ordered output array requires an additional O (n + m) steps. Brown and Tarjan gave the first (m lg (n=m)) time algorithm that outputs the result in the same format as the inputs <ref> [6] </ref>. Their algorithm was based on AVL-trees and they later showed a variant based on finger searching in 2-3 trees [7]. The same bounds can also be achieved with skip-lists [21]. The lower bound given above makes no assumptions about the input.
Reference: [7] <author> M. R. Brown and R. E. Tarjan. </author> <title> Design and analysis of a data structure for representing sorted lists. </title> <journal> SIAM Journal of Computing, </journal> <volume> 9(3) </volume> <pages> 594-614, </pages> <month> Aug. </month> <year> 1980. </year>
Reference-contexts: Previous sequential algorithms that achieve these bounds, however, are reasonably messy <ref> [6, 7] </ref>. The only parallel version we know of that claims to achieve these work bounds [15] is much more complicated. <p> Brown and Tarjan gave the first (m lg (n=m)) time algorithm that outputs the result in the same format as the inputs [6]. Their algorithm was based on AVL-trees and they later showed a variant based on finger searching in 2-3 trees <ref> [7] </ref>. The same bounds can also be achieved with skip-lists [21]. The lower bound given above makes no assumptions about the input. When using a measure of the "easiness" of the input the bounds can be improved.
Reference: [8] <author> S. Carlsson, C. Levcopoulos, and O. Petersson. </author> <title> Sublin ear merging and natural merge sort. </title> <booktitle> In Proceedings of the International Symposium on Algorithms SIGAL'90, </booktitle> <pages> pages 251-260, </pages> <address> Tokyo, Japan, </address> <month> Aug. </month> <year> 1990. </year>
Reference-contexts: This is optimal with respect to the measure <ref> [8] </ref>. To analyze the effectiveness of the algorithms in practice we ran several experiments. <p> Such inputs can be merged in O (k lg ((n+m)=k)) time <ref> [8, 21] </ref>. In fact any data structure that takes O (lg k) time for a split on the k th element of an ordered set, or to append (join) k elements to an ordered set can be used to achieve these bounds.
Reference: [9] <author> E. Dekel and I. Azsvath. </author> <title> Parallel external merging. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 6 </volume> <pages> 623-635, </pages> <year> 1989. </year>
Reference-contexts: Anderson et al. [1], Dekel and Ozsvath <ref> [9] </ref>, Hagerup and Rub [12], and Varman et al. [29] provide O (n=p + lg n) time EREW PRAM algorithms for merging. Guan and Langston [11] give the first time-space optimal algorithm that takes O (n=p + lg n) time and O (1) extra space on an EREW PRAM.
Reference: [10] <author> J. R. Driscoll, N. Sarnak, D. D. Sleator, and R. E. Tarjan. </author> <title> Making data structures persistent. </title> <journal> Journal of Computer and System Sciences, </journal> <volume> 38(1) </volume> <pages> 86-124, </pages> <month> Feb. </month> <year> 1989. </year>
Reference-contexts: The relative performance of the persistent and nonper-sistent versions are discussed in Section 3.2. We note that our persistent code is not as space efficient as the more sophisticated method of Driscoll et al. <ref> [10] </ref>, but their solution is much more complex and can only be applied to a tree of changes. 2.4 Analysis In this section we first analyze the expected work to find the union of two treaps t n and t n of size n and m, respectively and m n.
Reference: [11] <author> X. Guan and M. A. Langston. </author> <title> Time-space optimal par allel merging and sorting. </title> <journal> IEEE Transactions on Computers, </journal> <volume> 40 </volume> <pages> 592-602, </pages> <year> 1991. </year>
Reference-contexts: Anderson et al. [1], Dekel and Ozsvath [9], Hagerup and Rub [12], and Varman et al. [29] provide O (n=p + lg n) time EREW PRAM algorithms for merging. Guan and Langston <ref> [11] </ref> give the first time-space optimal algorithm that takes O (n=p + lg n) time and O (1) extra space on an EREW PRAM.
Reference: [12] <author> T. Hagerup and C. Rub. </author> <title> Optimal merging and sorting on the EREW PRAM. </title> <journal> Information Processing Letters, </journal> <volume> 33 </volume> <pages> 181-185, </pages> <year> 1989. </year>
Reference-contexts: Anderson et al. [1], Dekel and Ozsvath [9], Hagerup and Rub <ref> [12] </ref>, and Varman et al. [29] provide O (n=p + lg n) time EREW PRAM algorithms for merging. Guan and Langston [11] give the first time-space optimal algorithm that takes O (n=p + lg n) time and O (1) extra space on an EREW PRAM.
Reference: [13] <author> L. Highan and E. Schenk. </author> <title> Maintaining B-tree on an EREW PRAM. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 22 </volume> <pages> 329-335, </pages> <year> 1994. </year>
Reference-contexts: Paul et al. provide EREW PRAM search, insertion, and deletion algorithms for 2-3 trees [20], and Highan and Schenk have extended these results to B-trees <ref> [13] </ref>. Ranade [23] gives algorithms for processing least-upper-bound queries and insertion on distributed memory networks. Baumker and Dit-trich [2] give algorithms for search, insertion, and deletion into BB*(a) trees for the BSP* model that are 1-optimal and 2-optimal.
Reference: [14] <author> F. K. Hwang and S. Lin. </author> <title> A simple algorithm for merging two disjoint linearly ordered sets. </title> <journal> SIAM Journal of Computing, </journal> <volume> 1 </volume> <pages> 31-39, </pages> <month> Mar. </month> <year> 1972. </year>
Reference-contexts: Without loss of generality we will henceforth assume m n, in which case dlg n e = (m lg (n=m)). Hwang and Lin <ref> [14, 18] </ref> described an algorithm that matches this lower bound, but the algorithm assumes the input sets are in arrays and only returns cross pointers between the arrays. To rearrange the data into a single ordered output array requires an additional O (n + m) steps.
Reference: [15] <author> J. Katajainen. </author> <title> Efficient parallel algorithms for manipu lating sorted sets. </title> <booktitle> Proceedings of the 17th Annual Computer Science Conference, Australian Computer Science Communications, </booktitle> <volume> 16(1) </volume> <pages> 281-288, </pages> <year> 1994. </year>
Reference-contexts: Previous sequential algorithms that achieve these bounds, however, are reasonably messy [6, 7]. The only parallel version we know of that claims to achieve these work bounds <ref> [15] </ref> is much more complicated. In regards to treaps, Seidel and Aragon did not directly consider merging or set operations, but it is straightforward to use their finger search to implement these operations within the optimal time bounds, expected case. <p> However, as with Hwang and Lin's serial algorithm, it requires an additional O (n + m) operations to return the sorted merged results in an array. Katajainen describes EREW PRAM algorithms for union, intersection and difference that use the same input and output representations <ref> [15] </ref>. The algorithms are an extension of Paul et al.'s 2-3 tree algorithms and he claims they run in optimal O (lg n + lg m) depth and O (m lg (n=m)) work.
Reference: [16] <author> J. Katajainen, C. Levcopoulos, and O. Petersson. </author> <title> Space-efficient parallel merging. </title> <booktitle> In Proceedings of the 4th International PARLE Conference (Parallel Architectures and Languages Europe), volume 605 of Lecture Notes in Computer Science, </booktitle> <pages> pages 37-49, </pages> <year> 1992. </year>
Reference-contexts: Guan and Langston [11] give the first time-space optimal algorithm that takes O (n=p + lg n) time and O (1) extra space on an EREW PRAM. Katajainen et al. <ref> [16] </ref> gives a simpler algorithm with the same time and space bounds for the EREW and an optimal space-efficient O (n=p + lg lg m) time and O (1) space algorithm for the CREW PRAM.
Reference: [17] <author> D. E. Knuth. </author> <title> Fundamental Algorithms, </title> <booktitle> volume 1 of The Art of Computer Programming. </booktitle> <publisher> Addison-Wesley Publishing Company, </publisher> <address> Reading, MA, </address> <year> 1968. </year>
Reference-contexts: Then lg i n ^p i lg nm i n ~p i : When we use fl + (n + 1=2) lg n n lg e lg (n + 1) fl + (n + 1=2) lg n n lg e + lg e=12n, where fl = lg p real (see <ref> [17] </ref> exercise 1.2.11.2-6) to substitute for the choose expressions we get Equation 3.
Reference: [18] <author> D. E. Knuth. </author> <title> Sorting and Searching, </title> <booktitle> volume 3 of The Art of Computer Programming. </booktitle> <publisher> Addison-Wesley Publishing Company, </publisher> <address> Reading, MA, </address> <year> 1973. </year>
Reference-contexts: Without loss of generality we will henceforth assume m n, in which case dlg n e = (m lg (n=m)). Hwang and Lin <ref> [14, 18] </ref> described an algorithm that matches this lower bound, but the algorithm assumes the input sets are in arrays and only returns cross pointers between the arrays. To rearrange the data into a single ordered output array requires an additional O (n + m) steps.
Reference: [19] <author> M. L. Mauldin. Lycos: </author> <title> Design choices in an Inter net search service. </title> <journal> IEEE Expert, </journal> <volume> 12(1), </volume> <month> Jan. </month> <year> 1997. </year> <note> (www.computer.org/pubs/expert/1997/trends/x1008/ mauldin.htm). </note>
Reference-contexts: We focus on the aggregate set operations intersection, union, and difference since, among other applications, these operations play an important role in databases queries and index searching <ref> [30, 19] </ref>. The techniques we describe can also be applied to searching, inserting and deleting multiple elements from a dictionary in parallel. Although we use two of the same building blocks used in the sequential algorithms (split and join), the design and analysis of our algorithms is quite different. <p> Set operations are used extensively for index searching|each term (word) can be represented as a set of the "documents" it appears in and searches on logical conjunctions of terms are implemented as set operations (intersection for and, union for or, and difference for and-not) <ref> [30, 19] </ref>. Most web search engines use this technique. The set union operation is also closely related to merging|it is simply a merge with the duplicates removed.
Reference: [20] <author> W. Paul, U. Vishkin, and H. Wagener. </author> <title> Parallel dic tionaries on 2-3 trees. </title> <booktitle> In Lecture Notes in Computer Science 143: Proceedings of the 10th Colloquium on Automata, Languages and Programming, Barcelona, Spain, </booktitle> <pages> pages 597-609, </pages> <address> Berlin/New York, July 1983. </address> <publisher> Springer-Verlag. </publisher>
Reference-contexts: Paul et al. provide EREW PRAM search, insertion, and deletion algorithms for 2-3 trees <ref> [20] </ref>, and Highan and Schenk have extended these results to B-trees [13]. Ranade [23] gives algorithms for processing least-upper-bound queries and insertion on distributed memory networks. Baumker and Dit-trich [2] give algorithms for search, insertion, and deletion into BB*(a) trees for the BSP* model that are 1-optimal and 2-optimal.
Reference: [21] <author> W. Pugh. </author> <title> A skip list cookbook. </title> <type> Technical Report CS TR-2286.1, </type> <institution> University of Maryland Institute for Advanced Computer Studies Dept. of Computer Science, University of Maryland, </institution> <month> June </month> <year> 1990. </year>
Reference-contexts: Their algorithm was based on AVL-trees and they later showed a variant based on finger searching in 2-3 trees [7]. The same bounds can also be achieved with skip-lists <ref> [21] </ref>. The lower bound given above makes no assumptions about the input. When using a measure of the "easiness" of the input the bounds can be improved. <p> Such inputs can be merged in O (k lg ((n+m)=k)) time <ref> [8, 21] </ref>. In fact any data structure that takes O (lg k) time for a split on the k th element of an ordered set, or to append (join) k elements to an ordered set can be used to achieve these bounds. <p> In fact any data structure that takes O (lg k) time for a split on the k th element of an ordered set, or to append (join) k elements to an ordered set can be used to achieve these bounds. Pugh used this approach for skip lists <ref> [21] </ref> and although not discussed directly, the approach can also be applied to treaps when using the "fast" versions of splits and joins. However, as with finger searches, these "fast" versions require parent pointers. <p> Our first step is to verify that sequential treaps compare reasonably well with other good sequential balanced tree algorithms. We implemented and compared the performance of red/black trees [25], splay trees [27] (Sleator's code), skip lists <ref> [21] </ref> (Pugh's code) and treaps on an SGI Power Challenge and Sun Ultra Enterprise 3000. To evaluate the performance of the algorithms we performed a series of tests that create a tree of size n, and insert, delete and search for k keys in a tree of size n.
Reference: [22] <author> W. Pugh. </author> <title> Skip lists: A probilistic alternative to bal anced trees. </title> <journal> Communications of the ACM, </journal> <volume> 33(6) </volume> <pages> 668-676, </pages> <month> June </month> <year> 1990. </year>
Reference-contexts: The simplicity of the algorithms led them to conjecture that the algorithms should be fast in practice. Although they did not present experimental results, we have run experiments that compared treaps to splay trees [27], red-black trees, and skip-lists <ref> [22] </ref> and the results show that treaps have around the same performance as these other fast data structures. These results are briefly presented in Section 3.1. Our interest is in developing fast parallel algorithms for set operations.
Reference: [23] <author> A. Ranade. </author> <title> Maintaining dynamic ordered sets on pro cessor networks. </title> <booktitle> In Proceedings of the 4th Annual ACM Symposium on Parallel Algorithms and Architectures, </booktitle> <pages> pages 127-137, </pages> <address> San Diego, CA, </address> <month> June-July </month> <year> 1992. </year>
Reference-contexts: Paul et al. provide EREW PRAM search, insertion, and deletion algorithms for 2-3 trees [20], and Highan and Schenk have extended these results to B-trees [13]. Ranade <ref> [23] </ref> gives algorithms for processing least-upper-bound queries and insertion on distributed memory networks. Baumker and Dit-trich [2] give algorithms for search, insertion, and deletion into BB*(a) trees for the BSP* model that are 1-optimal and 2-optimal.
Reference: [24] <author> M. Reid-Miller. </author> <title> Experiments with Parallel Pointer Based Algoriths. </title> <type> PhD thesis, </type> <institution> School of Computer Science, Carnegie Mellon University, </institution> <address> Pittsburgh, PA, </address> <year> 1998. </year> <note> To appear. </note>
Reference-contexts: The expected work to find intersection and difference of two treaps follows. Then we analyze the expected depth of these operations. The proof of some of the lemmas in this section and a more detailed discussion is given in <ref> [24] </ref>. Without loss of generality, assume that the key values for the two trees are 1; 2; 3; : : : ; n + m. <p> From [26] we know that E [T split (N; i)] = O (lg n). These lead to the following lemma, proven in <ref> [24] </ref>.
Reference: [25] <author> R. Sedgewick. </author> <title> Algorithms in C. </title> <publisher> Addison-Wesley, Read ing, </publisher> <address> MA, </address> <year> 1990. </year>
Reference-contexts: Our first step is to verify that sequential treaps compare reasonably well with other good sequential balanced tree algorithms. We implemented and compared the performance of red/black trees <ref> [25] </ref>, splay trees [27] (Sleator's code), skip lists [21] (Pugh's code) and treaps on an SGI Power Challenge and Sun Ultra Enterprise 3000.
Reference: [26] <author> R. Seidel and C. R. Aragon. </author> <title> Randomized search trees. </title> <journal> Algorithmica, </journal> <volume> 16 </volume> <pages> 464-497, </pages> <year> 1996. </year>
Reference-contexts: 1 Introduction Balanced trees provide a wide variety of low-cost operations on ordered sets and dynamic dictionaries. Of the many types of balanced trees that have been developed over the years, treaps <ref> [26] </ref> have the advantage of both being simple and general|in addition to insertion and deletion they easily support efficient finger searching, joining, and splitting. Furthermore, by using appropriate hash functions they require no balance information to be stored at the nodes. <p> inputs have only a constant number of blocks. 1 Note that just copying the inputs does not work since this would violate the O (m lg (n=m)) work bounds|it would require O (n + m) work. 1 * The algorithms use the same representation of treaps as Seidel and Aragon <ref> [26] </ref> so that all their algorithms can be used on the same data without modification. Treaps are randomized search trees in which each node in the tree has a key and an associated random priority. <p> This function fills a new node with the key and priority data given in its arguments. All code in this section along with nonpersistent and parallel versions are available at http://www.cs.cmu.edu/~scandal/treaps.html. 2.1 Sequential algorithms Seidel and Aragon showed how to perform many operations on treaps <ref> [26] </ref>. We quickly review the operations split and join, which we use to manipulating pairs of treaps. (L, x, G) = split (T , key) Split T into two trees, L with key values less than key and G with key values greater than key. <p> Below we give the recursive top-down C code for persistent split and join. The split and join we use in our experiments are the slightly more efficient iterative versions. In addition, there are bottom-up algorithms that use rotations for these operations <ref> [26] </ref>. Split To split a tree rooted at r by key value a split follows the in-order access path with respect to the key value a until either it reaches a node with key value a or a leaf node. <p> Figure 1 shows the result of a split on a treap. The expected time to split two treaps into treaps of size n and m is O (lg n + lg m) <ref> [26] </ref>. The following code returns the less and greater results by side effecting the first two argument. <p> To maintain the heap order join interleaves pieces of the spines so that the priorities descend all the way to a leaf. The expected time to join two treaps of size n and m is O (lg n + lg m) <ref> [26] </ref>. (key,priority) values. When splitting by the key v, the unshaded region becomes the less-than tree and the shaded region becomes the greater-than tree. The double links show the path split follows. <p> From <ref> [26] </ref> we know that E [T split (N; i)] = O (lg n). These lead to the following lemma, proven in [24].
Reference: [27] <author> D. D. Sleator and R. E. Tarjan. </author> <title> Self-adjusting binary trees. </title> <journal> Journal of the Association for Computing Machinery, </journal> <volume> 32(3) </volume> <pages> 652-686, </pages> <year> 1985. </year>
Reference-contexts: The simplicity of the algorithms led them to conjecture that the algorithms should be fast in practice. Although they did not present experimental results, we have run experiments that compared treaps to splay trees <ref> [27] </ref>, red-black trees, and skip-lists [22] and the results show that treaps have around the same performance as these other fast data structures. These results are briefly presented in Section 3.1. Our interest is in developing fast parallel algorithms for set operations. <p> Our first step is to verify that sequential treaps compare reasonably well with other good sequential balanced tree algorithms. We implemented and compared the performance of red/black trees [25], splay trees <ref> [27] </ref> (Sleator's code), skip lists [21] (Pugh's code) and treaps on an SGI Power Challenge and Sun Ultra Enterprise 3000.
Reference: [28] <institution> Supercomputing Technologies Group, MIT Laboratory for Computer Science. </institution> <note> Cilk-5.0 (Beta 1) Reference Manual, </note> <month> Mar. </month> <year> 1997. </year>
Reference-contexts: The C procedure was exactly like the Cilk one except it did not use Spawn and Sync. In this way, in the C procedure we avoid the overhead of the Cilk function call, which is about three times the overhead of a C function call <ref> [28] </ref>. In Figure 7 we show the times for union when we vary the depth at which we revert to C. Notice that as the depth decreases the times improve and then get worse.
Reference: [29] <author> P. J. Varman, B. R. Iyer, D. J. Haderle, and S. M. Dunn. </author> <title> Parallel merging: Algorithm and implementation results. </title> <journal> Parallel Computing, </journal> <volume> 15 </volume> <pages> 165-177, </pages> <year> 1990. </year>
Reference-contexts: Anderson et al. [1], Dekel and Ozsvath [9], Hagerup and Rub [12], and Varman et al. <ref> [29] </ref> provide O (n=p + lg n) time EREW PRAM algorithms for merging. Guan and Langston [11] give the first time-space optimal algorithm that takes O (n=p + lg n) time and O (1) extra space on an EREW PRAM.
Reference: [30] <author> I. H. Witten, A. Moffat, and T. C. Bell. </author> <title> Managing Gigabytes: Compressing and Indexing Documents and Images. </title> <publisher> Van Nostrand Reinhold, </publisher> <year> 1994. </year> <month> 11 </month>
Reference-contexts: We focus on the aggregate set operations intersection, union, and difference since, among other applications, these operations play an important role in databases queries and index searching <ref> [30, 19] </ref>. The techniques we describe can also be applied to searching, inserting and deleting multiple elements from a dictionary in parallel. Although we use two of the same building blocks used in the sequential algorithms (split and join), the design and analysis of our algorithms is quite different. <p> Set operations are used extensively for index searching|each term (word) can be represented as a set of the "documents" it appears in and searches on logical conjunctions of terms are implemented as set operations (intersection for and, union for or, and difference for and-not) <ref> [30, 19] </ref>. Most web search engines use this technique. The set union operation is also closely related to merging|it is simply a merge with the duplicates removed.
References-found: 30

