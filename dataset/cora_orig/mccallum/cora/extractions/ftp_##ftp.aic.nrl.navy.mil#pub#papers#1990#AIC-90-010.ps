URL: ftp://ftp.aic.nrl.navy.mil/pub/papers/1990/AIC-90-010.ps
Refering-URL: http://www.aic.nrl.navy.mil/~schultz/papers.html
Root-URL: 
Email: (GREF@AIC.NRL.NAVY.MIL)  (RAMSEY@AIC.NRL.NAVY.MIL)  (SCHULTZ@AIC.NRL.NAVY.MIL)  
Title: Learning Sequential Decision Rules Using Simulation Models and Competition  
Author: JOHN J. GREFENSTETTE CONNIE LOGGIA RAMSEY ALAN C. SCHULTZ 
Keyword: Key words: sequential decision rules, competition-based learning, genetic algorithms Running Head: Learning Sequential Decision Rules  
Note: Machine Learning 5(4), 355-381.  
Address: DC 20375-5000, U.S.A.  
Affiliation: Navy Center for Applied Research in Artificial Intelligence, Naval Research Laboratory, Washington,  
Abstract: The problem of learning decision rules for sequential tasks is addressed, focusing on the problem of learning tactical decision rules from a simple flight simulator. The learning method relies on the notion of competition and employs genetic algorithms to search the space of decision policies. Several experiments are presented that address issues arising from differences between the simulation model on which learning occurs and the target environment on which the decision rules are ultimately tested. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Agre, P. E. and D. </author> <title> Chapman (1987). Pengi: An implementation of a theory of activity. </title> <booktitle> Proceedings Sixth National Conference on Artificial Intelligence. </booktitle> <pages> (pp 268-272). </pages>
Reference-contexts: This approach is especially designed for sequential decision tasks involving a rapidly changing state and other agents, and therefore best suited to reactive rather than projective planning <ref> (Agre & Chapman, 1987) </ref>. The approach described here reflects a particular methodology for learning via a simulation model. The motivation behind the methodology is that making mistakes on real systems may be costly or dangerous.
Reference: <author> Antonisse H. J. and K. S. Keller, </author> <year> (1987). </year> <title> Genetic operators for high-level knowledge representations. </title> <booktitle> Proceedings of the Second International Conference Genetic Algorithms and Their Applications (pp. </booktitle> <pages> 69-76). </pages> <address> Cambridge, MA: </address> <publisher> Erlbaum. </publisher>
Reference: <author> Barto, A. G., R. S. Sutton and C. J. C. H. </author> <title> Watkins (1989). Learning and sequential decision making. </title> <type> COINS Technical Report, </type> <institution> University of Massachusetts, Amherst. </institution> - <note> 24 - Bickel A. </note> <author> S. and R. W. Bickel, </author> <year> (1987). </year> <title> Tree structured rules in genetic algorithms. </title> <booktitle> Proceedings of the Second International Conference Genetic Algorithms and Their Applications (pp. </booktitle> <pages> 77-81). </pages> <address> Cambridge, MA: </address> <publisher> Erlbaum. </publisher>
Reference-contexts: While the tasks we consider here have a naturally graduated payoff function, it should be noted that any problem solving task may be cast into sequential decision paradigm, by defining the payoff to be a positive constant for any goal state and null for non-goal states <ref> (Barto et. al, 1989) </ref>. Several laboratory-scale sequential decision tasks have been investigated in the machine learning literature, including pole balancing (Selfridge, Sutton & Barto, 1985), gas pipeline control (Goldberg, 1983), and the animat problem (Wilson, 1985; Wilson, 1987). <p> The Temporal Difference (TD) method (Sutton, 1988) addresses learning control rules through incremental experience. Like dynamic programming, the TD method requires sufficient memory (perhaps distributed among the units of a neural net) to store information about the individual states of the dynamical system <ref> (Barto et al., 1989) </ref>. For very large state spaces, genetic algorithms offer the chance to learn decision rules without partitioning of the state space a priori. <p> Classifier systems (Holland, 1986; Goldberg, 1983) use genetic algorithms at the level of individual rules, or classifiers, to derive decision rules for sequential tasks. ________________ 1 If payoff is accumulated over an infinite period, the total payoff is usually defined to be a (finite) time-weighted sum <ref> (Barto, Sutton & Watkins, 1989) </ref>. - 3 - The system described in this paper adopts a distinctly different approach, applying genetic algorithms at the level of the tactical plan, rather than the individual rule, with each tactical plan comprising an entire set of decision rules for the given task. <p> Given the many-to-one mapping performed by the sensors, the agent does not have sufficient information in this formulation of the problem to apply techniques such as dynamic programming or the TD method since these methods require unambiguous state information <ref> (Barto et al., 1989) </ref>. A large state space. While the number of states in the underlying dynamical system is of course infinite, even the number of observable feature vectors defined by the sensors is quite large.
Reference: <author> Booker, L. B. </author> <year> (1982). </year> <title> Intelligent behavior as as adaptation to the task environment. </title> <type> Doctoral dissertation, </type> <institution> Department of Computer and Communications Sciences, University of Michigan, </institution> <address> Ann Arbor. </address>
Reference: <author> Booker, L. B. </author> <year> (1985). </year> <title> Improving the performance of genetic algorithms in classifier systems. </title> <booktitle> Proceedings of the International Conference Genetic Algorithms and Their Applications (pp. </booktitle> <pages> 80-92). </pages> <address> Pittsburgh, PA. </address>
Reference-contexts: However, since there is no guarantee that the current set of rules is in any sense complete, it is important to provide a mechanism for gracefully handling cases in which no rule matches <ref> (Booker, 1985) </ref>. In CPS this is accomplished by assigning each rule a match score equal to the number of conditions it matches. The match set consists of all the rules with the highest current match score.
Reference: <author> Booker, L. B. </author> <year> (1988). </year> <title> Classi fier systems that learn internal world models Machine Learning, </title> <booktitle> 3(2/3), </booktitle> <pages> (pp. 161-192). </pages>
Reference: <author> Buchanan, B. G., J. Sullivan, T. P. Cheng and S. H. </author> <month> Clearwater </month> <year> (1988). </year> <title> Simulation-assisted inductive learning. </title> <booktitle> Proceedings Seventh National Conference on Artificial Intelligence. </booktitle> <pages> (pp. 552-557). </pages>
Reference-contexts: We are investigating alternative methods for generating plausible rules using available background knowledge. For example, in the EM domain, we might want to use a Half-Order Theory <ref> (Buchanan et al., 1988) </ref> to generate symmetric variants of rules. That is, if SPECIALIZE creates a rule that says, "If the missile is on the left, turn hard left", we may also want to create a rule that says, "If the missile is on the right, turn hard right". 5.3.
Reference: <author> Cramer, N. L. </author> <year> (1985). </year> <title> A representation for the adaptive generation of simple sequential programs. </title> <booktitle> Proceedings of the International Conference Genetic Algorithms and Their Applications (pp. </booktitle> <pages> 183-187). </pages> <address> Pittsburgh, PA. </address>
Reference: <author> Davis, L. </author> <year> (1989). </year> <title> Adapting operator probabilities in genetic algorithms. </title> <booktitle> Proceedings of the Third International Conference on Genetic Algorithms. </booktitle> <address> Fairfax, VA: </address> <publisher> Morgan Kaufmann. </publisher> <pages> (pp. 61-69). </pages>
Reference-contexts: We expect that the competitive mechanisms of SAMUEL will enable the effective use of these operators without making unreasonably strong assumptions about the critic's ability to assign credit and blame. We also plan to investigate adaptive methods <ref> (Davis, 1989) </ref> for selecting which operator to apply. 5.4. More complex problems The current statement of the EM problem assumes a two-dimensional world. Future experiments will adopt a three-dimensional model and will address problems with multiple control variables, such as controlling both the direction and the speed of the plane.
Reference: <author> De Jong, K. A. </author> <year> (1975). </year> <title> Analysis of the behavior of a class of genetic adaptive systems. </title> <type> Doctoral dissertation, </type> <institution> Department of Computer and Communications Sciences, University of Michigan, </institution> <address> Ann Arbor. </address>
Reference: <author> Erickson, M. D. & J. M. </author> <title> Zytkow (1988). Utilizing experience for improving the tactical manager. </title> <booktitle> Proceedings of the Fifth International Conference on Machine Learning. </booktitle> <address> Ann Arbor, MI. </address> <pages> (pp. 444-450). </pages>
Reference: <author> Fitzpatrick, M. J. and J. J. </author> <title> Grefenstette (1988). Genetic algorithms in noisy environments, </title> <journal> Machine Learning, </journal> <volume> 3(2/3), </volume> <pages> (pp. 101-120). </pages>
Reference: <author> Fujiki C. and J. Dickinson, </author> <year> (1987). </year> <title> Using the genetic algorithm to generate LISP source code to solve the prisoner's dilemma. </title> <booktitle> Proceedings of the Second International Conference Genetic Algorithms and Their Applications (pp. </booktitle> <pages> 236-240). </pages> <address> Cambridge, MA: </address> <publisher> Erlbaum. </publisher>
Reference: <author> Forbus, K. D. </author> <year> (1984). </year> <title> Qualitative process theory. </title> <booktitle> Artificial Intelligence 24, </booktitle> <pages> (pp. 85-168). </pages>
Reference-contexts: We are attempting to adapt Qualitative Process Theory <ref> (Forbus, 1984) </ref>, which provides a language well-suited to describing processes, for the interpretation of the empirical learned rules. The interpretations are expected to be helpful in creating new rules for SAMUEL. - 23 - 5.2.
Reference: <author> Goldberg, D. E. </author> <year> (1983). </year> <title> Computer-aided gas pipeline operation using genetic algorithms and machine learning, </title> <type> Doctoral dissertation, </type> <institution> Department Civil Engineering, University of Michigan, </institution> <address> Ann Arbor. </address>
Reference-contexts: Several laboratory-scale sequential decision tasks have been investigated in the machine learning literature, including pole balancing (Selfridge, Sutton & Barto, 1985), gas pipeline control <ref> (Goldberg, 1983) </ref>, and the animat problem (Wilson, 1985; Wilson, 1987). In addition, sequential decision problems include many important practical problems, and much work has been devoted to their solution.
Reference: <author> Goldberg, D. E. </author> <year> (1988). </year> <title> Probability matching, the magnitude of reinforcement, and classifier system bidding (Technical Report TCGA-88002). </title> <institution> Tuscaloosa, AL: University of Alabama, Department of Engineering Mechanics. </institution>
Reference: <author> Goldberg, D. E. </author> <year> (1989). </year> <title> Genetic algorithms in search, optimization, </title> <booktitle> and machine learning. </booktitle> <address> Reading: </address> <publisher> Addison-Wesley. </publisher>
Reference: <author> Gordon, D. F. and J. J. </author> <title> Grefenstette (1990). Explanations of empirically derived reactive plans. </title> <booktitle> Proceedings of the Seventh International Conference on Machine Learning. </booktitle> <address> Austin, TX: </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: it is possible to combine several forms of learning in a single system: Empirical methods such as genetic algorithms may discover a set of high performance rules, and then an analytic learning method may attempt to explain the success of those rules and thereby enhance an existing partial domain theory <ref> (Gordon & Grefenstette, 1990) </ref>. The following subsections describe the major modules in more detail, using EM as a concrete example problem. It should be noted that the current version of SAMUEL represents just one way to implement a rule learning system based on genetic algorithms.
Reference: <author> Grefenstette, J. J. </author> <year> (1986). </year> <title> Optimization of control parameters for genetic algorithms. </title> <journal> IEEE Transactions on Systems, Man, and Cybernetics, </journal> <volume> SMC-16(1), </volume> <pages> 122-128. </pages> - <note> 25 - Grefenstette, </note> <author> J. J. </author> <year> (1987). </year> <title> Incorporating problem speci fic knowledge into genetic algorithms. In Genetic algorithms and simulated annealing. </title> <editor> D. Davis (ed.), </editor> <publisher> London: Pitman Press. </publisher>
Reference-contexts: Goldberg (1989) provides a detailed discussion of genetic algorithms. The learning level of SAMUEL is a specialized version of a standard genetic algorithm, GENESIS <ref> (Grefenstette, 1986) </ref>. The remainder of this section outlines the differences between GENESIS and the genetic algorithm in SAMUEL. 3.2.2.1. Adaptive Initialization Two approaches to initializing the knowledge structures of a genetic algorithm have been reported. By far, random initialization of the first population is the most common method. <p> The topic of reproductive selection in genetic algorithms is discussed in the literature (Goldberg, 1989; Grefenstette & Baker, 1989). One new aspect to SAMUEL's selection algorithm concerns the issue of scaling that is, how to maintain selective pressure as the overall performance rises within the population <ref> (Grefenstette, 1986) </ref>. In SAMUEL, the fitness of each plan is defined as the difference between the average payoff received by the plan and some baseline performance measure. The baseline is adjusted to track the mean payoff received by the population, minus one standard deviation.
Reference: <author> Grefenstette, J. J. </author> <year> (1988). </year> <title> Credit assignment in rule discovery system based on genetic algorithms. </title> <journal> Machine Learning, </journal> <volume> 3(2/3), </volume> <pages> (pp. 225-245). </pages>
Reference-contexts: In general, a given rule may specify conditions for any subset of the sensors and actions for any subset of the control variables. Each rule also has a numeric strength, that serves as a prediction of the rule's utility <ref> (Grefenstette, 1988) </ref>. The methods used to update the rule strengths is described in the section on credit assignment below. A sample rule in the EM system follows: if (and (last-turn 0 45) (time 4 14) (range 500 1400) (heading 330 90) (speed 50 850)) then (turn 90) strength 750 3.1.2. <p> As a result, the settings for different control variables may be recommended by distinct rules. This situation does not arise in EM, since there is only one control variable. - 10 - a time-weighted estimate of the expected external payoff, and that this estimate is useful for conflict resolution <ref> (Grefenstette, 1988) </ref>. However, conflict resolution should take into account not only the expected payoff associated with each rule, but also some measure of our confidence in that estimate. <p> This is consistent with previous experiments in which clustering contributed to faster learning <ref> (Grefenstette, 1988) </ref>. The runs with (population = 20, - 16 - episodes = 100), on the other hand, exhibit rapid improvement early, but taper off when the smaller population converges.
Reference: <author> Grefenstette, J. J. </author> <year> (1989). </year> <title> A system for learning control plans with genetic algorithms. </title> <booktitle> Proceedings of the Third International Conference on Genetic Algorithms. </booktitle> <address> Fairfax, VA: </address> <publisher> Morgan Kaufmann. </publisher> <pages> (pp. 183-190). </pages>
Reference-contexts: Many genetic algorithms permit recombination within individual rules as a way of creating new rules (Smith, 1980; Schaffer, 1984; Holland, 1986). While such operators are easily defined for SAMUEL's rule language <ref> (Grefenstette, 1989) </ref>, we prefer to use CROSSOVER solely to explore the space of rule combinations, and leave rule modification to other operators (i.e., SPECIALIZE and MUTATION). In SAMUEL, CROSSOVER assigns each rule in two selected parent plans to one of two offspring plans.
Reference: <author> Grefenstette, J. J. and J. E. </author> <title> Baker (1989). How genetic algorithms work: A critical look at implicit parallelism. </title> <booktitle> Proceedings of the Third International Conference on Genetic Algorithms. </booktitle> <address> Fairfax, VA: </address> <publisher> Morgan Kaufmann. </publisher> <pages> (pp. 20-27). </pages>
Reference-contexts: Many genetic algorithms permit recombination within individual rules as a way of creating new rules (Smith, 1980; Schaffer, 1984; Holland, 1986). While such operators are easily defined for SAMUEL's rule language <ref> (Grefenstette, 1989) </ref>, we prefer to use CROSSOVER solely to explore the space of rule combinations, and leave rule modification to other operators (i.e., SPECIALIZE and MUTATION). In SAMUEL, CROSSOVER assigns each rule in two selected parent plans to one of two offspring plans.
Reference: <author> Holland, J. H. </author> <year> (1975). </year> <booktitle> Adaptation in natural and artificial systems. </booktitle> <address> Ann Arbor: </address> <publisher> University Michigan Press. </publisher>
Reference-contexts: A genetic algorithm is used to perform the search. Genetic algorithms are motivated by standard models of heredity and evolution in the field of population genetics, and embody abstractions of the mechanisms of adaptation present in natural systems <ref> (Holland, 1975) </ref>. Briefly, a genetic algorithm simulates the dynamics of population genetics by maintaining a knowledge base of knowledge structures that evolves over time in response to the observed performance of its knowledge structures in their training environment.
Reference: <author> Holland, J. H. </author> <year> (1986). </year> <title> Escaping brittleness: The possibilities of general-purpose learning algorithms applied to parallel rule-based systems. In R.S. </title> <editor> Michalski, J. G. Carbonell, </editor> & <booktitle> T. </booktitle>
Reference-contexts: A condition for the weather sensor might be (weather is [cloudy wet]) This would match if weather had value cloudy, wet, rain, or snow. Finally, pattern sensors can take on binary string values. Conditions on pattern sensors specify patterns over the alphabet -0, 1, #-, as in classifier systems <ref> (Holland, 1986) </ref>. For example, the sensor detector1 might be defined as a eight bit string, and a condition for this sensor might be (detector1 00##10#1) This condition matches if the string assigned to detector1 agrees with all the positions occupied by 0 or 1 in the condition's pattern. <p> In order to introduce plausible new rules, a plan modification operator called SPECIALIZE is applied after each evaluation of a plan. SPECIALIZE is similar in spirit to Holland's triggered operators <ref> (Holland, 1986) </ref>. The trigger in this case is the conjunction of the following conditions: (1) There is room in the plan for at least one more rule. (2) A maximally general rule fired during an episode that ended with a successful evasion.
Reference: <editor> M. Mitchell (Eds.), </editor> <booktitle> Machine learning: An artificial intelligence approach (Vol. 2). </booktitle> <address> Los Altos, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Koza, J. R. </author> <year> (1989). </year> <title> Hierarchical genetic algorithms operating on populations of computer programs. </title> <booktitle> Proceedings of the Eleventh International Joint Conference on Artificial Intelligence (pp. </booktitle> <pages> 768-774). </pages> <address> Detroit: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Langley, P. </author> <year> (1983). </year> <title> Learning effective search heuristics. </title> <booktitle> Proceedings of the Eighth International Joint Conference on Artificial Intelligence (pp. </booktitle> <pages> 419-421). </pages> <address> Karlsruhe, Germany: </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Traditional Rule Learning Operators SAMUEL provides a testbed for investigating several rule modification operators besides the genetic operators, such as specialization, generalization, merging, and discrimination <ref> (Langley, 1983) </ref>. We expect that the competitive mechanisms of SAMUEL will enable the effective use of these operators without making unreasonably strong assumptions about the critic's ability to assign credit and blame. We also plan to investigate adaptive methods (Davis, 1989) for selecting which operator to apply. 5.4.
Reference: <author> Michalski, R. S. </author> <year> (1983). </year> <title> A theory and methodology for inductive learning. </title> <journal> Artificial Intelligence, </journal> <volume> 20(2), </volume> <pages> (pp. 111-161). </pages>
Reference-contexts: The choice of an appropriate learning technique depends on the nature of the performance task and the form of available knowledge. If the performance task is classification, and a large number of training examples are available, then inductive learning techniques <ref> (Michalski, 1983) </ref> can be used to learn classification rules. If there exists an extensive domain theory and a source of expert behavior, then explanation-based methods may be applied (Mitchell, Mahadevan & Steinberg, 1985).
Reference: <author> Mitchell, T. M., S. Mahadevan and L. </author> <title> Steinberg (1985). LEAP: A learning apprentice for VLSI design. </title> <booktitle> Proc. Ninth IJCAI, </booktitle> <pages> (pp. 573-580). </pages> <address> Los Angeles: </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: If the performance task is classification, and a large number of training examples are available, then inductive learning techniques (Michalski, 1983) can be used to learn classification rules. If there exists an extensive domain theory and a source of expert behavior, then explanation-based methods may be applied <ref> (Mitchell, Mahadevan & Steinberg, 1985) </ref>. Many interesting practical problems that may be amenable to automated learning do not fit either of these models. One such class of problems is the class of sequential decision tasks.
Reference: <author> Riolo, R. L. </author> <year> (1988). </year> <title> Empirical studies of default hierarchies and sequences of rules in learning classifier systems, </title> <type> Doctoral dissertation, Doctoral dissertation, </type> <institution> Department of Electrical Engineering and Computer Science, University of Michigan, </institution> <address> Ann Arbor. </address>
Reference-contexts: Each possible action receives a bid equal to the strength of the strongest rule in the match set that specifies that action in its right-hand side. Unlike classifier systems in which all members of the match set vote on which action to perform <ref> (Riolo, 1988) </ref>, CPS selects an action using the probability distribution defined by the strength of the (single) bidder for each action. This prevents a large number of low strength rules from combining to suggest an action that is actually associated with low payoff.
Reference: <author> Samuel, A. L. </author> <year> (1963). </year> <title> Some studies in machine learning using the game of checkers. </title> <editor> In E. </editor> <publisher> A. </publisher>
Reference: <editor> Feigenbaum & J. Feldman, (Eds), </editor> <booktitle> Computer and Thought. </booktitle> <publisher> McGraw-Hill. </publisher>
Reference: <author> Schaffer, J. D. </author> <year> (1984). </year> <title> Some experiments in machine learning using vector evaluated genetic algorithms, </title> <type> Doctoral dissertation, </type> <institution> Department of Electrical and Biomedical Engineering, Vanderbilt University, Nashville. </institution>
Reference: <author> Schaffer, J. D., R. A. Caruana, L. J. Eshelman and R. </author> <title> Das (1989). A study of control parameters affecting online performance of genetic algorithms for function optimization. </title> <booktitle> Proceedings of the Third International Conference on Genetic Algorithms. </booktitle> <address> Fairfax, VA: </address> <publisher> Morgan Kaufmann. </publisher> <pages> (pp. 51-60). </pages>
Reference: <author> Selfridge, O., R. S. Sutton and A. G. </author> <title> Barto (1985). Training and tracking in robotics. </title> <booktitle> Proceedings of the Ninth International Conference on Artificial Intelligence. </booktitle> <address> Los Angeles, CA. </address> <month> August, </month> <year> 1985. </year>
Reference-contexts: Several laboratory-scale sequential decision tasks have been investigated in the machine learning literature, including pole balancing <ref> (Selfridge, Sutton & Barto, 1985) </ref>, gas pipeline control (Goldberg, 1983), and the animat problem (Wilson, 1985; Wilson, 1987). In addition, sequential decision problems include many important practical problems, and much work has been devoted to their solution.
Reference: <author> Smith, S. F. </author> <year> (1980). </year> <title> A learning system based on genetic adaptive algorithms, </title> <type> Doctoral dissertation, </type> <institution> Department of Computer Science, University of Pittsburgh. </institution>
Reference-contexts: SAMUEL: A Learning System for Tactical Plans plans are selected for replication and modification. Genetic operators, such as crossover and mutation, produce plausible new plans from high performance precursors. The design of SAMUEL owes much to Smith's LS-1 system <ref> (Smith, 1980) </ref>, and draws on some ideas from classifier systems (Holland, 1986; Wilson, 1985; Riolo 1988). In a departure from several previous genetic learning systems, SAMUEL learns rules expressed in a high level rule language.
Reference: <author> Sutton, R. S. </author> <year> (1988). </year> <title> Learning to predict by the method of temporal differences. </title> <journal> Machine Learning, </journal> <volume> 3, </volume> <pages> (pp. 9-44). </pages> - <note> 26 - Wilson, </note> <author> S. W. </author> <year> (1985). </year> <title> Knowledge growth in an artificial animal. </title> <booktitle> Proceedings of the International Conference Genetic Algorithms and Their Applications (pp. </booktitle> <pages> 16-23). </pages> <address> Pittsburgh, PA. </address>
Reference-contexts: For problems lacking a complete mathematical model of the dynamical system, dynamic programming methods can produce optimal decision rules, as long as the number of states is fairly small. The Temporal Difference (TD) method <ref> (Sutton, 1988) </ref> addresses learning control rules through incremental experience. Like dynamic programming, the TD method requires sufficient memory (perhaps distributed among the units of a neural net) to store information about the individual states of the dynamical system (Barto et al., 1989).
Reference: <author> Wilson, S. W. </author> <year> (1987). </year> <title> Classi fier systems and the animat problem. </title> <journal> Machine Learning, </journal> <volume> 2(3), </volume> <pages> (pp. 199-228). </pages>
Reference-contexts: This prevents a large number of low strength rules from combining to suggest an action that is actually associated with low payoff. All rules in the match set that agree with the selected action are said to be active <ref> (Wilson, 1987) </ref>, and will have their strength adjusted according to the credit assignment algorithm described in the next section. After conflict resolution, the control variables are set to the values indicated by the selected actions. 3 The world model is then advanced by one simulation step. <p> This is consistent with a number of previous studies that show that a genetic algorithm with recombination outperforms a genetic algorithm with mutation alone <ref> (Wilson, 1987) </ref>. Given the many differences between the genetic algorithm in SAMUEL and many previous genetic algorithms, confirmation of this conclusion strengthens our confidence that recombination, even broadly defined, is a powerful search operator. - 17 - 4.4.
References-found: 38

