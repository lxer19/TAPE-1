URL: http://c.gp.cs.cmu.edu:5103/afs/cs/project/cmcl/archive/Nectar-papers/92aroma.ps
Refering-URL: http://c.gp.cs.cmu.edu:5103/afs/cs.cmu.edu/project/nectar/WWW/appl_papers.html
Root-URL: http://www.cs.cmu.edu
Title: Aroma: Language Support for Distributed Objects synchronization and data consistency with them, making it possible
Author: Hiroshi Nishikawa and Peter Steenkiste 
Note: Aroma combines the benefits of both categories: it Abstract [8] if the  
Address: Pittsburgh, Pennsylvania 15213  
Affiliation: School of Computer Science Carnegie Mellon University  
Abstract: supports general distributed objects, but allows efficient accesses in the case of regular data structures. This is Aroma simplifies the task of parallelizing large achieved through aggregate objects that are distributed applications on multicomputers by providing applications over multiple computing nodes using directives similar to with a shared object space. Aroma supports both those used by parallelizing compilers. For data parallel traditional monolithic objects and aggregate object that computations, aggregate objects support the distribution can be partitioned across multiple nodes. Aggregate of data and associated computation, resulting in mostly objects support data parallelism efficiently. An Aroma local data accesses and good load balancing. Aroma data program consists of tasks that operate on shared objects. objects can also have synchronization properties Tasks typically execute on the node on which their input associated with them. This makes it possible to write data is located, thus minimizing communication. Shared applications without explicit locks or condition variables data objects have synchronization properties associated the synchronization and access properties of objects also variables. In this paper, we present and justify the Aroma allows an efficient implementation. language features, and give examples of Aroma programs. Aroma has been implemented on the Nectar An Aroma program is organized as a set of tasks. Tasks multicomputer and we give performance results for are segments of sequential code that can read shared data several applications. when they start and write shared data when they finish. Tasks are scheduled based on the availability of their 1. Introduction input data and they never block once they started 
Abstract-found: 1
Intro-found: 1
Reference: 12. <author> H.T. Kung, Peter Steenkiste, Marco Gubitoso, and Manpreet Khaira. </author> <title> Parallelizing a New Class of Large </title>
Reference-contexts: The main routine distributed version using send and receive primitives is combines the results when all tasks are finished. The described in <ref> [12] </ref>. Computing the particle traces is a two speedup of the prime sieve program is close to linear. phase process. Phase I consists of computing wind velocity at each point of a regular grid on the concerned 3. Henri Bal, Andrew Tanenbaum, and M.
Reference: 1. <author> Alfred V. Aho, Ravi Sethi and Jeffery D. Ullman. </author> <title> Applications over High-Speed Networks. </title> <booktitle> Third ACM Compilers Principles, Techniques, and Tools. Addison SIGPLAN Symposium on Principles and Practice of Wesley, 1986. Parallel Programming, ACM, </booktitle> <month> April, </month> <year> 1991, </year> <pages> pp. 167-177. </pages>
Reference-contexts: task queue and runs a schedule-execute loop. format, the same task structures are reused, thus reducing Scheduling is based on data availability: if all the inputs the task overhead; this optimization is similar to tail of a task are available as specified by the synchronization recursion elimination in sequential programs <ref> [1] </ref>. properties of the data, the task is executed, otherwise it remains blocked.
Reference: 2. <author> Emmanuel Arnould, Francois Bitz, </author> <title> Eric Cooper, </title>
Reference-contexts: Sections 2 and 3 describe the Aroma object and tasks concepts. We present several examples in Section 4 A second group provides more general support for and we give performance results for an implementation of distributed objects. Linda [6], Orca [3] and the Chare Aroma on the Nectar multicomputer <ref> [2] </ref>. kernel [10] all support some form of shared object space. Amber [7] and Munin [4] support a thread-based 2. Shared Data programming model similar to that found on a shared-memory processors. <p> We also present performance results for places them in the replicated array i_primes of type the applications for a prototype Aroma implementation on write-once, so all tasks can access them locally. The the Nectar multicomputer <ref> [2] </ref>. The execution time is the interval 0..LIMIT is broken up in segments of size elapsed time on a set of dedicated Sun4/330 workstations RANGE. Each task calculates the prime numbers in one connected by Nectar.
Reference: 13. <author> Matthew Rosing and Robert P. Weaver. Mapping H. T. Kung, Robert Sansom and Peter Steenkiste. </author> <title> The Data to Processors in Distributed Memory Computations. Design of Nectar: A Network Backplane for Proceedings of the Fifth Distributed Memory Computing Heterogeneous Multicomputers. </title> <booktitle> Proceedings of the Third Conference, IEEE, </booktitle> <month> April, </month> <year> 1990, </year> <pages> pp. 884-893. </pages> <booktitle> International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <month> 14. </month> <title> Ping-Sheng Tseng. Compiling Programs for a Linear ACM/IEEE, </title> <address> Boston, </address> <month> April, </month> <year> 1989, </year> <pages> pp. </pages> <month> 205-216. </month> <title> Systolic Array. </title> <booktitle> Proceedings of the SIGPLAN '90 Conference on Programming Language Design and Implementation, ACM, </booktitle> <month> June, </month> <year> 1990, </year> <pages> pp. 311-321. </pages>
Reference-contexts: One of the compiles accesses to data on remote nodes into message motivations for organizing the program as a set of tasks is passing and distributes loop iterations across processors dynamic load balancing; this work is in progress. <ref> [14, 11, 13] </ref>. These systems have proven to be effective for data parallel applications, typically in the form of This paper concentrates on the definition of shared data in FORTRAN DO loops, on tightly-coupled systems. Aroma. Sections 2 and 3 describe the Aroma object and tasks concepts. <p> The distribution patterns available in necessary to provide mechanisms to control access to the Aroma are similar to those supported by parallelizing shared data [8]. The distributed memory environment compilers: BLOCK, CYCLIC <ref> [11, 13] </ref> and RANDOM; also requires us to worry about maintaining data RANDOM leaves the distribution up to the Aroma consistency. In Aroma, read operations are implemented runtime system.
References-found: 4

