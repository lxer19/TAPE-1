URL: ftp://ftp.cs.wisc.edu/machine-learning/shavlik-group/opitz.ijcai93.ps
Refering-URL: http://www.cs.wisc.edu/~shavlik/abstracts/opitz.ijcai93.ps.abstract.html
Root-URL: 
Email: opitz@cs.wisc.edu  
Title: Heuristically Expanding Knowledge-Based Neural Networks  
Author: David W. Opitz and Jude W. Shavlik 
Address: Madison, WI 53706, U.S.A.  
Affiliation: Computer Sciences Department University of Wisconsin Madison  
Note: Appears in the Proceedings of the Thirteenth International Joint Conference on Artificial Intelligence (IJCAI-93)  
Abstract: Knowledge-based neural networks are networks whose topology is determined by mapping the dependencies of a domain-specific rulebase into a neural network. However, existing network training methods lack the ability to add new rules to the (reformulated) rulebases. Thus, on domain theories that are lacking rules, generalization is poor, and training can corrupt the original rules, even those that were initially correct. We present TopGen, an extension to the Kbann algorithm, that heuristically searches for possible expansions of a knowledge-based neural network, guided by the domain theory, the network, and the training data. It does this by dynamically adding hidden nodes to the neural representation of the domain theory, in a manner analogous to adding rules and con-juncts to the symbolic rulebase. Experiments indicate that our method is able to heuristically find effective places to add nodes to the knowledge-base network and verify that new nodes must be added in an intelligent manner.
Abstract-found: 1
Intro-found: 1
Reference: [ Craven and Shavlik, 1993 ] <author> M. Craven and J. Shavlik. </author> <title> Machine learning approaches to gene recognition. </title> <note> Machine Learning Research Group Working Paper 93-1, </note> <institution> Univ. of Wisconsin - Madison, </institution> <year> 1993. </year>
Reference: [ Fahlman and Lebiere, 1989 ] <author> S. Fahlman and C. Lebiere. </author> <booktitle> The cascade-correlation learning architecture. In Adv. in Neural Info. Processing Systems 2, </booktitle> <pages> pages 524-532, </pages> <address> Denver, CO, 1989. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: This paper presents evidence for these claims. TopGen differs from other network-growing algorithms <ref> [ Fahlman and Lebiere, 1989; Frean, 1990 ] </ref> in that it is designed for knowledge-based networks. TopGen uses a symbolic interpretation of the trained network to help 1 We use generalization to mean classification accuracy on examples not seen during training. decide where the primary errors are in the network.
Reference: [ Frean, 1990 ] <author> M. Frean. </author> <title> The upstart algorithm: A method for constructing and training feedforward neural networks. </title> <journal> Neural Computation, </journal> <volume> 2 </volume> <pages> 198-209, </pages> <year> 1990. </year>
Reference-contexts: This paper presents evidence for these claims. TopGen differs from other network-growing algorithms <ref> [ Fahlman and Lebiere, 1989; Frean, 1990 ] </ref> in that it is designed for knowledge-based networks. TopGen uses a symbolic interpretation of the trained network to help 1 We use generalization to mean classification accuracy on examples not seen during training. decide where the primary errors are in the network.
Reference: [ Ginsberg, 1990 ] <author> A. Ginsberg. </author> <title> Theory reduction, theory revision, </title> <booktitle> and retranslation. Proc. of the 8th Nat. Conf. on Artificial Intelligence, </booktitle> <pages> pages 777-782, </pages> <year> 1990. </year>
Reference-contexts: DAID helps train a Kbann-net by trying to locate low-level links with errors, while TopGen expands a Kbann-net by searching for nodes with errors. Propositional theory-refinement systems, such as EITHER [ Ourston and Mooney, 1990 ] and RTLS <ref> [ Ginsberg, 1990 ] </ref> , are also related to Top-Gen. These systems differ from TopGen, in that their approaches are purely symbolic.
Reference: [ Hinton, 1986 ] <author> G. Hinton. </author> <title> Learning distributed representations of concepts. </title> <booktitle> In Proc. of the 8th Annual Conf. of the Cognitive Science Soc., </booktitle> <pages> pages 1-12, </pages> <year> 1986. </year>
Reference-contexts: To help address the trade-off between changing the domain theory and disregarding the misclassified training examples as noise, TopGen uses a variant of weight decay <ref> [ Hinton, 1986 ] </ref> . Weights that are part of the original domain theory decay toward their initial value, while other weights decay toward zero.
Reference: [ Ourston and Mooney, 1990 ] <author> D. Ourston and R. Mooney. </author> <title> Changing the rules: A comprehensive approach to theory refinement. </title> <booktitle> In Proc. of the 8th Nat. Conf. on Artificial Intelligence, </booktitle> <pages> pages 815-820, </pages> <year> 1990. </year>
Reference-contexts: Another extension to Kbann is the DAID algorithm [ Towell and Shavlik, 1992 ] . DAID helps train a Kbann-net by trying to locate low-level links with errors, while TopGen expands a Kbann-net by searching for nodes with errors. Propositional theory-refinement systems, such as EITHER <ref> [ Ourston and Mooney, 1990 ] </ref> and RTLS [ Ginsberg, 1990 ] , are also related to Top-Gen. These systems differ from TopGen, in that their approaches are purely symbolic.
Reference: [ Rumelhart et al., 1986 ] <author> D. Rumelhart, G. Hinton, and R. Williams. </author> <title> Learning internal representations by error propagation. </title> <editor> In D. Rumelhart and J. McClel-land, editors, </editor> <booktitle> Parallel Distributed Processing, </booktitle> <volume> Volume 1. </volume> <publisher> MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1986. </year>
Reference-contexts: TopGen uses Kbann's rule-to-network translation algorithm to define an initial guess for the network's topology. TopGen trains this network using backpropagation <ref> [ Rumelhart et al., 1986 ] </ref> and places it on a search queue. In each cycle, TopGen takes the best network from the search queue (as measured by tuning-set-2), decides possible ways to add new nodes, trains these new networks, and places them on the search queue.
Reference: [ Towell and Shavlik, 1992 ] <author> G. Towell and J. Shavlik. </author> <title> Using symbolic learning to improve knowledge-based neural networks. </title> <booktitle> In Proc. of the 10th Nat. Conf. on Artificial Intelligence, </booktitle> <pages> pages 177-182, </pages> <year> 1992. </year>
Reference-contexts: Another extension to Kbann is the DAID algorithm <ref> [ Towell and Shavlik, 1992 ] </ref> . DAID helps train a Kbann-net by trying to locate low-level links with errors, while TopGen expands a Kbann-net by searching for nodes with errors.
Reference: [ Towell et al., 1990 ] <author> G. Towell, J. Shavlik, and M. No-ordewier. </author> <title> Refinement of approximate domain theories by knowledge-based neural networks. </title> <booktitle> In Proc. of the 8th Nat. Conf. on Artificial Intelligence, </booktitle> <pages> pages 861-866, </pages> <address> Boston, MA, </address> <year> 1990. </year>
Reference-contexts: The relevant part of Kbann for this paper is the rules-to-network translation algorithm, which translates a set of propositional rules, representing what is initially known about the topic, into a neural network. This translation defines the network's topology and connection weights. Details of this translation process appear in <ref> [ Towell et al., 1990; Towell, 1992 ] </ref> . An example of this process is shown in Figure 1. Figure 1a shows a Prolog-like rule set that defines membership in category A.
Reference: [ Towell, 1992 ] <author> G. Towell. </author> <title> Symbolic Knowledge and Neural Networks: Insertion, Refinement, and Extraction. </title> <type> PhD thesis, </type> <institution> Univ. of Wisconsin, Madison, WI, </institution> <year> 1992. </year>
Reference-contexts: A learning system should make repairs that minimize the changes to the initial domain theory, while making it consistent with the data. We present a connectionist approach to theory refinement, particularly focusing on the task of expanding impoverished domain theories. Our system builds on the Kbann system <ref> [ Towell, 1992 ] </ref> . Kbann translates the initial theory into a neural network, thereby determining the network's topology and initial weights. <p> Kbann has been shown to be fl This work was partially supported by DOE Grant DE-FG02-91ER61129, NSF Grant IRI-9002413, and ONR Grant N00014-90-J-1941. more effective at classifying previously-unseen examples than a wide variety of machine learning algorithms <ref> [ Tow-ell et al., 1990; Towell, 1992 ] </ref> . A large part of the reason for Kbann's superiority over other symbolic systems has been attributed to both its underlying learning algorithm (i.e., backpropagation) and its effective use of domain-specific knowledge [ Towell, 1992 ] . <p> A large part of the reason for Kbann's superiority over other symbolic systems has been attributed to both its underlying learning algorithm (i.e., backpropagation) and its effective use of domain-specific knowledge <ref> [ Towell, 1992 ] </ref> . However, Kbann suffers from the fact that, since it does not alter the initial network's topology, it can only add and subtract antecedents of existing rules. Thus it is unable to add new symbolic rules to an impoverished rule set. <p> The relevant part of Kbann for this paper is the rules-to-network translation algorithm, which translates a set of propositional rules, representing what is initially known about the topic, into a neural network. This translation defines the network's topology and connection weights. Details of this translation process appear in <ref> [ Towell et al., 1990; Towell, 1992 ] </ref> . An example of this process is shown in Figure 1. Figure 1a shows a Prolog-like rule set that defines membership in category A. <p> To decrease false positives in a symbolic rulebase, we can either add antecedents to existing rules or remove rules from the rulebase. While Kbann can effectively remove rules <ref> [ Towell, 1992 ] </ref> , it is less effective at adding antecedents to rules and is unable to invent (constructively induce) new terms as antecedents. Figures 2b,d show the ways (analogous to Figures 2a,c explained above) of adding constructively-induced antecedents. <p> Trained Kbann networks are interpretable because (a) the meaning of its nodes does not significantly shift during training and (b) almost all the nodes are either fully active or inactive <ref> [ Towell, 1992 ] </ref> . TopGen adds nodes in a fashion that does not violate these two assumptions. Other future work includes testing new ways of adding nodes. Nodes are currently added so that they are fully connected to all input nodes.
References-found: 10

