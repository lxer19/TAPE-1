URL: http://www.cs.cmu.edu/afs/cs/user/aberger/www/ps/punc.ps
Refering-URL: http://www.cs.cmu.edu/afs/cs.cmu.edu/user/aberger/www/lm.html
Root-URL: 
Email: dougb,aberger,lafferty@cs.cmu.edu  
Title: CYBERPUNC: A LIGHTWEIGHT PUNCTUATION ANNOTATION SYSTEM FOR SPEECH  
Author: Doug Beeferman Adam Berger John Lafferty 
Address: Pittsburgh PA 15213  
Affiliation: School of Computer Science Carnegie Mellon University  
Abstract: This paper describes a lightweight method for the automatic insertion of intra-sentence punctuation into text. Despite the intuition that pauses in an acoustic stream are a positive indicator for some types of punctuation, this work will demonstrate the feasibility of a system which relies solely on lexical information. Besides its potential role in a speech recognition system, such a system could serve equally well in non-speech applications such as automatic grammar correction in a word processor and parsing of spoken text. After describing the design of a punctuation-restoration system, which relies on a trigram language model and a straightforward application of the Viterbi algorithm, we summarize results, both quantitative and subjective, of the performance and behavior of a prototype system. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> D. Beeferman, A. Berger, and J. </author> <title> Lafferty (1997) A model of lexical attraction and repulsion. </title> <booktitle> Proceedings of the ACL-EACL Joint Conference </booktitle>
Reference-contexts: Resolving such ambiguities lies beyond the scope of a second-order Markov model, and requires a model with richer conditioning information such as a trigger language model, described in <ref> [1] </ref>. Higher-order information|such as the parts of speech assigned by a tagger or even the structure of a parse tree hypothesized by a statistical parser| could also provide such long-range information, but at the expense of a less lightweight system.
Reference: [2] <author> A. Black and P. </author> <title> Taylor (1997) Assigning phrase breaks from part-of-speech sequences. </title> <booktitle> Eurospeech '97 </booktitle>
Reference-contexts: Though we are aware of no prior work on predicting punctuation in the lexical stream, variations of the phrase breaking task have received considerable attention. Noun phrase bracketers and "chunkers" <ref> [4, 2] </ref> are useful in applications that require some level of phrase-level segmentation but cannot afford the resources required by a full-blown parser.
Reference: [3] <author> N. Campbell, T. Herbert, </author> <title> and Ezra Black (1997) Parsers, prominence, and pauses. </title> <booktitle> Eurospeech '97 </booktitle>
Reference-contexts: The intelligibility of a text-to-speech system, for example, depends partly upon its ability to assign convincing intonation to regions of the input sentence and to insert pauses where appropriate <ref> [3] </ref>. Phrase breaking algorithms typically use part-of-speech n-gram models or the Viterbi algorithm to predict the bracketing defined in a corpus of parse trees.
Reference: [4] <author> K.H. Chen and H-H. </author> <title> Chen (1994) Extracting Noun Phrases from Large-Scale Texts: A Hybrid Approach and its Automatic Evaluation. </title> <booktitle> Proc. ACL </booktitle>
Reference-contexts: Though we are aware of no prior work on predicting punctuation in the lexical stream, variations of the phrase breaking task have received considerable attention. Noun phrase bracketers and "chunkers" <ref> [4, 2] </ref> are useful in applications that require some level of phrase-level segmentation but cannot afford the resources required by a full-blown parser.
Reference: [5] <author> M. Christel, T. Kanade, M. Mauldin, R. Reddy, M. Sirbu, S. Stevens, H. </author> <month> Wactlar </month> <year> (1995). </year> <title> Informedia Digital Video Library. </title> <journal> Comm. of the ACM 38:4, </journal> <pages> 57-58. </pages>
Reference-contexts: For instance, a broadcast-news transcription system (In-formedia <ref> [5] </ref>, for example) cannot rely on the speakers to pronounce "comma" or "semicolon" wherever necessary in the transcribed text. Telephony and other remote speech applications (such as the answering system described in [6]) share the injunction against requiring the user to speak unnaturally.
Reference: [6] <author> B. M. Lobanov, S. V. Brickle, A. V. Kubashin, T. V. </author> <month> Levovskaja </month> <year> (1997). </year> <title> An intelligent telephone answering system using speech recognition Eurospeech '97 </title>
Reference-contexts: For instance, a broadcast-news transcription system (In-formedia [5], for example) cannot rely on the speakers to pronounce "comma" or "semicolon" wherever necessary in the transcribed text. Telephony and other remote speech applications (such as the answering system described in <ref> [6] </ref>) share the injunction against requiring the user to speak unnaturally. Ultimately, we imagine that all speech recognition systems will automatically insert punctuation as accurately and efficiently as a professional transcriptionist.
Reference: [7] <author> M. </author> <title> McAllister (1989). The problems of punctuation ambiguity in fully automatic text-to-speech conversion Eurospeech '89 </title>
Reference-contexts: For the remainder of this paper we shall focus on the comma, the most frequent and unpredictable punctuation mark (Table 1). The comma exhibits a wide range of applications in text (Table 2), and disambiguating a comma's role in a sentence is a separate issue <ref> [7] </ref> from identification, the focus of this paper. Other intra-sentence punctuation marks may be amenable to an approach similar to the one we present, although it can be argued that sentence-terminating punctuation such as question marks and exclamation marks require substantially more semantic insight [10]. 3.
Reference: [8] <author> M. Marcus and B. Santorini and M. </author> <month> Marcinkiewicz </month> <year> (1993). </year> <title> Building a large annotated corpus of English: the Penn Treebank, </title> <note> Computational Linguistics 19:1. </note>
Reference-contexts: Within places ...a vaccine and bioresearch firm based in Lyon, France, is... Table 2: The most frequent roles of the comma in the Penn Treebank corpus. (a) for a sample sentence, with the Viterbi path highlighted. 4. EVALUATION To evaluate our systems empirically, we selected the Penn Treebank corpus <ref> [8] </ref> of sentences from the Wall Street Journal. We used the standard 2317-sentence test suite stripped of all punctuation. We ran the decoding algorithms described above to produce punctuated output that was then compared with the original test set.
Reference: [9] <author> G. </author> <booktitle> Nunberg (1990) The linguistics of punctuation. CSLI Lecture Notes No. </booktitle> <volume> 18, </volume> <publisher> University of Chicago Press </publisher>
Reference: [10] <author> J. Reynar and A. </author> <month> Ratnaparkhi </month> <year> (1997). </year> <title> A maximum-entropy approach to identifying sentence boundaries. </title> <booktitle> Proceedings of the Fifth Conference on Applied Natural Language Processing. </booktitle>
Reference-contexts: Other intra-sentence punctuation marks may be amenable to an approach similar to the one we present, although it can be argued that sentence-terminating punctuation such as question marks and exclamation marks require substantially more semantic insight <ref> [10] </ref>. 3. DECODING The comma-restoration algorithm relies on a punctuation-aware trigram language model Q. Below we describe how such a model can be constructed from a trigram model P without punctuation, using minimal space.

References-found: 10

