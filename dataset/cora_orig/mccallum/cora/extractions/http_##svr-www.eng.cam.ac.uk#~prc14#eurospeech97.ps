URL: http://svr-www.eng.cam.ac.uk/~prc14/eurospeech97.ps
Refering-URL: http://gs213.sp.cs.cmu.edu/prog/findhome/?query=Roni.Rosenfeld
Root-URL: 
Email: prc14@eng.cam.ac.uk  roni@cmu.edu  
Title: STATISTICAL LANGUAGE MODELING USING THE CMU-CAMBRIDGE TOOLKIT  
Author: Philip Clarkson Ronald Rosenfeld 
Address: Trumpington Street, Cambridge, CB2 1PZ, UK.  5000 Forbes Avenue, Pittsburgh, PA 15213, USA.  
Affiliation: Cambridge University Engineering Department,  School of Computer Science, Carnegie Mellon University  
Abstract: The CMU Statistical Language Modeling toolkit was released in 1994 in order to facilitate the construction and testing of bigram and trigram language models. It is currently in use in over 40 academic, government and industrial laboratories in over 12 countries. This paper presents a new version of the toolkit. We outline the conventional language modeling technology, as implemented in the toolkit, and describe the extra efficiency and functionality that the new toolkit provides as compared to previous software for this task. Finally, we give an example of the use of the toolkit in constructing and testing a simple language model. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> L.R. Bahl, P.F. Brown, P.V. de Souza, and R.L. Mercer. </author> <title> A Tree-Based Statistical Language Model for Natural Language Speech Recognition. </title> <journal> IEEE Transactions on Pattern Analysis and Machine Intelligence, </journal> <volume> 37(7), </volume> <year> 1989. </year>
Reference-contexts: Even when improvements have been made over the traditional models, they have normally come about by combining a new model with a conventional trigram [2] model <ref> [1, 10, 11, 12, 3] </ref>. The CMU Statistical Language Modeling (CMU-SLM) toolkit [16] is a set of Unix software tools facilitating the construction and testing of conventional bigram and trigram language models.
Reference: [2] <author> L.R. Bahl, F. Jelinek, and R.L. Mercer. </author> <title> A Maximum Likelihood Approach to Continuous Speech Recognition. </title> <journal> IEEE Transactions on Pattern Analysis and Machine Intelligence, </journal> <volume> 5(2), </volume> <year> 1983. </year>
Reference-contexts: Even when improvements have been made over the traditional models, they have normally come about by combining a new model with a conventional trigram <ref> [2] </ref> model [1, 10, 11, 12, 3]. The CMU Statistical Language Modeling (CMU-SLM) toolkit [16] is a set of Unix software tools facilitating the construction and testing of conventional bigram and trigram language models. <p> Cutoffs In order to reduce the size of a language model, infrequent N -grams are often removed from the model. The counts below which the N -grams are discarded are referred to as cutoffs. Table 1 shows how the bigram and trigram cutoffs affect the size and perplexity <ref> [2] </ref> of a trigram language model. 2.3. Context Cues Language data is viewed by the toolkit as a stream of words interspersed with context cues. These are markers which indicate events such as sentence, paragraph and article boundaries.
Reference: [3] <author> P.R. Clarkson and A.J. Robinson. </author> <title> Language Model Adaptation using Mixtures and and Exponentially Decaying Cache. </title> <booktitle> In Proceedings IEEE ICASSP, </booktitle> <year> 1997. </year>
Reference-contexts: Even when improvements have been made over the traditional models, they have normally come about by combining a new model with a conventional trigram [2] model <ref> [1, 10, 11, 12, 3] </ref>. The CMU Statistical Language Modeling (CMU-SLM) toolkit [16] is a set of Unix software tools facilitating the construction and testing of conventional bigram and trigram language models.
Reference: [4] <author> A.P. Dempster, N.M. Laird, and D.B. Rubin. </author> <title> Maximum Likelihood from Incomplete Data Using the EM Algorithm. </title> <journal> Journal of the Royal Society of Statistics, </journal> <volume> 39(1) </volume> <pages> 1-38, </pages> <year> 1977. </year>
Reference-contexts: Linear Interpolation of Models In order to facilitate the combination of language models, the toolkit also includes a program [15, Appendix B] for calculating maximum likelihood weights for a set of models by use of the expectation maximisation (EM) algorithm <ref> [4] </ref>. The models are described by their output on a common set of items, and it is these probability streams which the program receives as input. These may have been generated via models created by the toolkit, or by other software.
Reference: [5] <author> I.J. </author> <title> Good. The Population Frequencies of Species and the Estimation of Population Parameters. </title> <journal> Biometrika, </journal> <volume> 40, </volume> <year> 1953. </year>
Reference-contexts: Frequently, backing-off and discounting are combined according to the scheme devised by Katz [8], which combines Good-Turing discounting <ref> [5] </ref> with backing-off. This is the approach to data smoothing which was implemented in version 1 of the toolkit. 2.2. Cutoffs In order to reduce the size of a language model, infrequent N -grams are often removed from the model.
Reference: [6] <author> F. Jelinek. </author> <title> Self-Organized Language Models for Speech Recognition. </title> <editor> In A. Waibel and K.-F Lee, edi-tors, </editor> <booktitle> Readings in Speech Recognition, </booktitle> <pages> pages 450-506. </pages> <publisher> Morgan Kaufman Publishers, </publisher> <year> 1990. </year>
Reference-contexts: FUTURE DEVELOPMENT Future development of the toolkit may include the follow <br>- ing: 1. Support for class-based N -grams <ref> [6] </ref>. Currently classes can be supported implicitly by mapping the training set into a given set of classes fC i g, and building a standard model in the class vocabulary.
Reference: [7] <author> F. Jelinek. </author> <title> Up From Trigrams! The Struggle for Improved Language Models. </title> <booktitle> In Proceedings Eurospeech, </booktitle> <year> 1991. </year>
Reference-contexts: 1. INTRODUCTION While language modeling continues to be a fruitful research topic, the standard language modeling techniques which are used in most large vocabulary recognition systems have changed little over the past few years <ref> [7] </ref>. Even when improvements have been made over the traditional models, they have normally come about by combining a new model with a conventional trigram [2] model [1, 10, 11, 12, 3].
Reference: [8] <author> S.M. Katz. </author> <title> Estimation of Probabilities from Sparse Data for the Language Model Component of a Speech Recognizer. </title> <journal> IEEE Transactions on Acoustics, Speech and Signal Processing, </journal> <volume> 35(3) </volume> <pages> 400-401, </pages> <year> 1987. </year>
Reference-contexts: the training data, P (w N j w 1 ; w 2 ; : : : ; w N1 ) can be estimated from the lower order model, namely from P (w N j w 2 ; : : : ; w N1 ), in a process known as backing-off <ref> [8] </ref>. Frequently, backing-off and discounting are combined according to the scheme devised by Katz [8], which combines Good-Turing discounting [5] with backing-off. This is the approach to data smoothing which was implemented in version 1 of the toolkit. 2.2. <p> : : ; w N1 ) can be estimated from the lower order model, namely from P (w N j w 2 ; : : : ; w N1 ), in a process known as backing-off <ref> [8] </ref>. Frequently, backing-off and discounting are combined according to the scheme devised by Katz [8], which combines Good-Turing discounting [5] with backing-off. This is the approach to data smoothing which was implemented in version 1 of the toolkit. 2.2. Cutoffs In order to reduce the size of a language model, infrequent N -grams are often removed from the model. <p> define n r as the number of events which occur r times, then the Good-Turing-based discounting scheme which was implemented in version 1 defines d r = rn r n 1 (k+1)n k+1 (2) for r &lt; k (where typically k 7) and d r = 1 for higher counts <ref> [8] </ref>, the belief being that events occurring more than 7 times are well estimated by maximum likelihood. This approach does have some disadvantages, however.
Reference: [9] <author> R. Kneser and H. Ney. </author> <title> Improved Backing-Off for M-gram Language Modeling. </title> <booktitle> In Proceedings IEEE ICASSP, </booktitle> <year> 1995. </year>
Reference-contexts: The most general model will allow each word position to have its own distinct class vocabulary. 2. Porting to run under Microsoft Windows 95, to make the toolkit accessible to a larger community of users. 3. In <ref> [9] </ref> a smoothing strategy is described in which backing-off distributions which have been optimized for the back-off model are used to give an improvement over conventional methods. This technique will be implemented in a future version of the toolkit. 4. More flexible backing-off.
Reference: [10] <author> R. Kuhn and R. De Mori. </author> <title> A Cache-Based Natural Language Model for Speech Reproduction. </title> <journal> IEEE Transactions on Pattern Analysis and Machine Intelligence, </journal> <volume> 12(6) </volume> <pages> 570-583, </pages> <year> 1990. </year>
Reference-contexts: Even when improvements have been made over the traditional models, they have normally come about by combining a new model with a conventional trigram [2] model <ref> [1, 10, 11, 12, 3] </ref>. The CMU Statistical Language Modeling (CMU-SLM) toolkit [16] is a set of Unix software tools facilitating the construction and testing of conventional bigram and trigram language models. <p> The models are described by their output on a common set of items, and it is these probability streams which the program receives as input. These may have been generated via models created by the toolkit, or by other software. For example, one may have generated cache-based probabilities <ref> [10] </ref>, and these could be interpolated with probabilities from a trigram model created by the toolkit. 3.9. More Efficient Pre-processing Tools Many of the tools in version 1 were simple filters which would convert a data stream into a slightly different format.
Reference: [11] <author> R. Kuhn and R. De Mori. </author> <title> Corrections to `A CacheBased Natural Language Model for Speech Reproduction'. </title> <journal> IEEE Transactions on Pattern Analysis and Machine Intelligence, </journal> <volume> 14 </volume> <pages> 691-692, </pages> <year> 1992. </year>
Reference-contexts: Even when improvements have been made over the traditional models, they have normally come about by combining a new model with a conventional trigram [2] model <ref> [1, 10, 11, 12, 3] </ref>. The CMU Statistical Language Modeling (CMU-SLM) toolkit [16] is a set of Unix software tools facilitating the construction and testing of conventional bigram and trigram language models.
Reference: [12] <author> R. Lau, R. Rosenfeld, and S. Roukos. </author> <title> Trigger-Based Language Models: A Maximum Entropy Approach. </title> <booktitle> In Proceedings IEEE ICASSP, </booktitle> <year> 1993. </year>
Reference-contexts: Even when improvements have been made over the traditional models, they have normally come about by combining a new model with a conventional trigram [2] model <ref> [1, 10, 11, 12, 3] </ref>. The CMU Statistical Language Modeling (CMU-SLM) toolkit [16] is a set of Unix software tools facilitating the construction and testing of conventional bigram and trigram language models.
Reference: [13] <author> H. Ney, U. Essen, and R. Kneser. </author> <title> On Structuring Probabilistic Dependencies in Stochastic Language Modelling. </title> <booktitle> Computer Speech and Language, </booktitle> <address> 8(1):138, </address> <year> 1994. </year>
Reference-contexts: Good-Turing discounting remains the default in version 2, but other discounting schemes have been implemented which do not suffer from this problem, and which sometimes produce superior results. These are presented below. 3.1.2. Linear discounting In linear discounting <ref> [13] </ref> a quantity proportional to each count is subtracted from the count itself. That is, we set d r = 1 ff. <p> Absolute discounting Absolute discounting involves subtracting a constant b from each of the counts. d r = r It can be shown <ref> [13] </ref> that setting b = n 1 n 1 +2n 2 is approximately optimal in terms of maximising the log likelihood function using leaving-one-out. 3.1.4.
Reference: [14] <author> P. Placeway, R. Schwartz, P. Fung, and L. Nguyen. </author> <title> The Estimation of Powerful Language Models from Small and Large Corpora. </title> <booktitle> In Proceedings IEEE ICASSP, </booktitle> <year> 1993. </year>
Reference-contexts: Witten-Bell discounting The discounting scheme which we refer to as Witten- Bell discounting is that which is referred to as type C in [17], and which was first applied to language modeling by <ref> [14] </ref>. The discounting ratio is not dependent on the event's count, but on t, the number of distinct events which followed the particular context.
Reference: [15] <author> R. Rosenfeld. </author> <title> Adaptive Statistical Language Modeling: A Maximum Entropy Approach. </title> <type> PhD thesis, </type> <institution> School of Computer Science, Carnegie Mellon University, </institution> <month> April </month> <year> 1994. </year> <note> Published as Technical Report CMU-CS-94-138. </note>
Reference-contexts: Version 2 supports both types of forced back-off from arbitrary vocabulary items. 3.8. Linear Interpolation of Models In order to facilitate the combination of language models, the toolkit also includes a program <ref> [15, Appendix B] </ref> for calculating maximum likelihood weights for a set of models by use of the expectation maximisation (EM) algorithm [4]. The models are described by their output on a common set of items, and it is these probability streams which the program receives as input.
Reference: [16] <author> R. Rosenfeld. </author> <title> The CMU Statistical Language Modeling Toolkit, and its use in the 1994 ARPA CSR Evaluation. </title> <booktitle> In ARPA Spoken Language Technology Workshop, </booktitle> <address> Austin, TX, </address> <month> January </month> <year> 1995. </year>
Reference-contexts: Even when improvements have been made over the traditional models, they have normally come about by combining a new model with a conventional trigram [2] model [1, 10, 11, 12, 3]. The CMU Statistical Language Modeling (CMU-SLM) toolkit <ref> [16] </ref> is a set of Unix software tools facilitating the construction and testing of conventional bigram and trigram language models. Version 1 was released in 1994, and is currently in use in over 40 academic, government and industrial laboratories in 12 countries.
Reference: [17] <author> I.T. Witten and T.C. Bell. </author> <title> The Zero-Frequency Problem: Estimating the Probabilities of Novel Events in Adaptive Text Compression. </title> <journal> IEEE Transactions on Information Theory, </journal> <volume> 37(4) </volume> <pages> 1085-1094, </pages> <month> July </month> <year> 1991. </year>
Reference-contexts: Witten-Bell discounting The discounting scheme which we refer to as Witten- Bell discounting is that which is referred to as type C in <ref> [17] </ref>, and which was first applied to language modeling by [14]. The discounting ratio is not dependent on the event's count, but on t, the number of distinct events which followed the particular context.
References-found: 17

