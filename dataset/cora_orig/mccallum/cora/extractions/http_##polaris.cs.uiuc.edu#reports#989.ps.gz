URL: http://polaris.cs.uiuc.edu/reports/989.ps.gz
Refering-URL: http://polaris.cs.uiuc.edu/tech_reports.html
Root-URL: http://www.cs.uiuc.edu
Title: The Impact of Parallel Loop Scheduling Strategies on Prefetching in a Shared-Memory Multiprocessor  
Author: David J. Lilja 
Keyword: cache coherence; cache pollution; false sharing; guided self-scheduling; prefetching; scheduling; shared memory multiprocessor.  
Note: (to appear in) IEEE Transactions on Parallel and Distributed Systems  
Address: 200 Union Street S.E. Minneapolis, MN 55455  
Affiliation: Department of Electrical Engineering University of Minnesota  
Email: E-mail: lilja@ee.umn.edu  
Phone: Phone: (612) 625-5007 FAX: (612) 625-4583  
Date: April 28, 1992 Revised: December 8, 1992 Revised: May 4, 1993  
Abstract: Trace-driven simulations of numerical Fortran programs are used to study the impact of the parallel loop scheduling strategy on data prefetching in a shared memory multiprocessor with private data caches. The simulations indicate that to maximize memory performance it is important to schedule blocks of consecutive iterations to execute on each processor, and then to adaptively prefetch single-word cache blocks to match the number of iterations scheduled. Prefetching multiple single-word cache blocks on a miss reduces the miss ratio by approximately 5 to 30 percent compared to a system with no prefetching. In addition, the proposed adaptive prefetching scheme further reduces the miss ratio while significantly reducing the false sharing among cache blocks compared to nonadaptive prefetching strategies. Reducing the false sharing causes fewer coherence invalidations to be generated, and thereby reduces the total network traffic. The impact of the prefetching and scheduling strategies on the temporal distribution of coherence invalidations also is examined. It is found that invalidations tend to be evenly distributed throughout the execution of parallel loops, but that they tend to be clustered when executing sequential program sections. The distribution of invalidations in both types of program sections is relatively insensitive to the prefetching and scheduling strategy. 
Abstract-found: 1
Intro-found: 1
Reference: 1. <author> Anant Agarwal and Anoop Gupta, </author> <title> ``Memory-Reference Characteristics of Multiprocessor Applications Under MACH,'' </title> <booktitle> ACM SIGMETRICS Conference on Measurement and Modeling of Computer Systems, </booktitle> <pages> pp. 215-225, </pages> <year> 1988. </year>
Reference-contexts: 1. Introduction Several studies <ref> [1, 6, 8, 15] </ref> have suggested that small cache blocks should be used in shared memory multiprocessors with private data caches to reduce the performance impact of false sharing. <p> The write-back strategy used by the cache coherence mechanism writes a single b-word block back from the cache to memory, regardless of the fetch size. Since previous studies have suggested the use of small cache blocks <ref> [1, 6, 8, 15] </ref>, these simulations use b=1 word per block. The number of blocks fetched on a miss, f , is a free parameter when using the nonadaptive prefetching strategies.
Reference: 2. <author> Anant Agarwal, Richard Simoni, John Hennessy, and Mark Horowitz, </author> <title> ``An Evaluation of Directory Schemes for Cache Coherence,'' </title> <booktitle> International Symposium on Computer Architecture, </booktitle> <pages> pp. 280-289, </pages> <year> 1988. </year>
Reference-contexts: An invalidation is not needed in this case, but since the processor does not know if it has the only cached copy, it must first request exclusive access from the directory. Similar histogram results have been reported elsewhere for a variety of programs <ref> [2, 27] </ref>. Table 2: Memory referencing statistics for the test programs. Static scheduling, c =1; p=8 processors; b=1 word per block; f =1 block per fetch.
Reference: 3. <author> David Callahan, Ken Kennedy, and Allan Porterfield, </author> <title> ``Software Prefetching,'' </title> <booktitle> International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pp. 40-52, </pages> <year> 1991. </year>
Reference-contexts: Another technique is to use explicit software prefetching where the compiler inserts special memory prefetch instructions in loop iteration i to begin fetching the data needed in iteration i+j <ref> [3, 12] </ref>. The prefetch distance, j, is determined by the compiler, and may be different for different loops. 2.2.1.
Reference: 4. <author> Lucien M. Censier and Paul Feautrier, </author> <title> ``A New Solution to Coherence Problems in Multicache Systems,'' </title> <journal> IEEE Transactions on Computers, </journal> <volume> Vol. C-27, No. 12, </volume> <pages> pp. 1112-1118, </pages> <month> December </month> <year> 1978. </year>
Reference-contexts: This sharing of cache blocks is not necessarily harmful as long as the processors only read the array. With an invalidation-based coherence protocol <ref> [4] </ref>, however, a processor must request exclusive access to an entire block before it can write to any word within the block. <p> This bus also is used to distribute the next available iteration values to the processors when the loop iterations are dynamically scheduled. A p+1-bit distributed full directory scheme <ref> [4] </ref> is used to maintain cache coherence in this system. This coherence mechanism associates p valid bits and one exclusive bit with each block in memory.
Reference: 5. <author> Michel Dubois, Christoph Scheurich, and Faye A. Briggs, </author> <title> ``Synchronization, Coherence, and Event Ordering in Multiprocessors,'' </title> <journal> Computer, </journal> <volume> Vol. 21, No. 2, </volume> <pages> pp. 9-21, </pages> <month> February </month> <year> 1988. </year>
Reference-contexts: All messages between the processors and the memories are acknowledged to ensure consistency using a weak-ordering consistency model <ref> [5] </ref>. The steps required for each type of memory reference, along with the corresponding network traffic, are shown in Table 1. Table 1: Memory operations and resulting network traffic. b= number of words per block; f = number of blocks per fetch; word size = 4 bytes.
Reference: 6. <author> Susan J. Eggers and Randy H. Katz, </author> <title> ``The Effect of Sharing on the Cache and Bus Performance of Parallel Programs,'' </title> <booktitle> International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pp. 257-270, </pages> <year> 1989. </year>
Reference-contexts: 1. Introduction Several studies <ref> [1, 6, 8, 15] </ref> have suggested that small cache blocks should be used in shared memory multiprocessors with private data caches to reduce the performance impact of false sharing. <p> The write-back strategy used by the cache coherence mechanism writes a single b-word block back from the cache to memory, regardless of the fetch size. Since previous studies have suggested the use of small cache blocks <ref> [1, 6, 8, 15] </ref>, these simulations use b=1 word per block. The number of blocks fetched on a miss, f , is a free parameter when using the nonadaptive prefetching strategies.
Reference: 7. <author> John W. C. Fu and Janak H. Patel, </author> <title> ``Data Prefetching in Multiprocessor Vector Cache Memories,'' </title> <booktitle> International Symposium on Computer Architecture, </booktitle> <pages> pp. 54-63, </pages> <year> 1991. </year>
Reference-contexts: For instance, the cache controller can be designed to fetch a fixed number of consecutive blocks on every miss [25], or to prefetch extra blocks only on a miss for a long vector operand <ref> [7] </ref>. While this latter approach improves the miss ratio for these vectors, it ignores the potential improvement from prefetching scalars and arrays that do not vectorize.
Reference: 8. <author> James R. Goodman, </author> <title> ``Using Cache Memory to Reduce Processor-Memory Traffic,'' </title> <booktitle> International Symposium on Computer Architecture, </booktitle> <pages> pp. 124-131, </pages> <year> 1983. </year> <month> 21 </month>
Reference-contexts: 1. Introduction Several studies <ref> [1, 6, 8, 15] </ref> have suggested that small cache blocks should be used in shared memory multiprocessors with private data caches to reduce the performance impact of false sharing. <p> The write-back strategy used by the cache coherence mechanism writes a single b-word block back from the cache to memory, regardless of the fetch size. Since previous studies have suggested the use of small cache blocks <ref> [1, 6, 8, 15] </ref>, these simulations use b=1 word per block. The number of blocks fetched on a miss, f , is a free parameter when using the nonadaptive prefetching strategies.
Reference: 9. <author> Allan Gottlieb, Ralph Grishman, Clyde P. Kruskal, Kevin P. McAuliffe, Larry Rudolph, and Marc Snir, </author> <title> ``The NYU Ultracomputer -- Designing a MIMD, Shared-Memory Parallel Machine,'' </title> <booktitle> International Symposium on Computer Architecture, </booktitle> <pages> pp. 27-42, </pages> <year> 1982. </year>
Reference-contexts: To extend this previous work, this paper examines how data prefetching can be used to improve memory performance in a shared memory multiprocessor with a multistage interconnection network <ref> [9, 14, 19] </ref>, taking into consideration how the parallel task scheduling strategy interacts with the prefetching strategy. This study concentrates on numerical Fortran programs since these large-scale parallel systems frequently are used to execute this type of program.
Reference: 10. <author> Manish Gupta and David A. Padua, </author> <title> ``Effects of Program Parallelization and Stripmining Transformation on Cache Performance in a Multiprocessor,'' </title> <booktitle> International Conference on Parallel Processing, </booktitle> <volume> Vol I: Architecture, </volume> <pages> pp. 301-304, </pages> <year> 1991. </year>
Reference-contexts: This type of sharing occurs when processors are not actually sharing data, but are sharing multiword cache blocks due to the placement of data within the memory modules. Another study <ref> [10] </ref> has suggested that false sharing can be reduced by scheduling several consecutive parallel loop iterations to execute on each processor using the stripmining program transformation.
Reference: 11. <author> S. F. Hummel, E. Schonberg, and L. E. Flynn, </author> <title> ``Factoring: A Practical and Robust Method for Scheduling Parallel Loops,'' </title> <booktitle> Supercomputing '91, </booktitle> <pages> pp. 610-619, </pages> <year> 1991. </year>
Reference-contexts: A variation of this scheduling strategy, called factoring, assigns smaller blocks of iterations to the processors at the start of the loop to provide potentially better load balancing <ref> [11] </ref>. A barrier synchronization operation is required at the end of each parallel loop to ensure that no processor begins executing tasks from after the parallel loop until all of the iterations have been 3 completed from the current loop.
Reference: 12. <author> Alexander C. Klaiber and Henry M. Levy, </author> <title> ``An Architecture for Software-Controlled Data Prefetching,'' </title> <booktitle> International Symposium on Computer Architecture, </booktitle> <pages> pp. 43-54, </pages> <year> 1991. </year>
Reference-contexts: Another technique is to use explicit software prefetching where the compiler inserts special memory prefetch instructions in loop iteration i to begin fetching the data needed in iteration i+j <ref> [3, 12] </ref>. The prefetch distance, j, is determined by the compiler, and may be different for different loops. 2.2.1.
Reference: 13. <author> David Kroft, </author> <title> ``Lockup-Free Instruction Fetch/Prefetch Cache Organization,'' </title> <booktitle> International Symposium on Computer Architecture, </booktitle> <pages> pp. 81-87, </pages> <year> 1981. </year>
Reference-contexts: The performance impact of the larger blocks can be reduced by using lockup-free caches <ref> [13, 24] </ref>, although the temporal clustering of misses may reduce the effectiveness of these complex fetch strategies [23]. An alternative to the implicit hardware prefetching of multiword cache blocks is to have the fetch size, f , greater than one block so that the hardware can explicitly prefetch multiple blocks.
Reference: 14. <author> David J. Kuck, Edward S. Davidson, Duncan J. Lawrie, and Ahmed H. Sameh, </author> <title> ``Parallel Supercomputing Today and the Cedar Approach,'' </title> <journal> Science, </journal> <volume> Vol. 231, </volume> <pages> pp. 967-974, </pages> <month> 28 February </month> <year> 1986. </year>
Reference-contexts: To extend this previous work, this paper examines how data prefetching can be used to improve memory performance in a shared memory multiprocessor with a multistage interconnection network <ref> [9, 14, 19] </ref>, taking into consideration how the parallel task scheduling strategy interacts with the prefetching strategy. This study concentrates on numerical Fortran programs since these large-scale parallel systems frequently are used to execute this type of program.
Reference: 15. <author> Roland L. Lee, Pen-Chung Yew, and Duncan J. Lawrie, </author> <title> ``Multiprocessor Cache Design Considerations,'' </title> <booktitle> International Symposium on Computer Architecture, </booktitle> <pages> pp. 253-262, </pages> <year> 1987. </year>
Reference-contexts: 1. Introduction Several studies <ref> [1, 6, 8, 15] </ref> have suggested that small cache blocks should be used in shared memory multiprocessors with private data caches to reduce the performance impact of false sharing. <p> The write-back strategy used by the cache coherence mechanism writes a single b-word block back from the cache to memory, regardless of the fetch size. Since previous studies have suggested the use of small cache blocks <ref> [1, 6, 8, 15] </ref>, these simulations use b=1 word per block. The number of blocks fetched on a miss, f , is a free parameter when using the nonadaptive prefetching strategies.
Reference: 16. <author> David J. Lilja, </author> <title> ``Prefetching and Scheduling Interactions in Shared Memory Multiprocessors,'' </title> <booktitle> Midwest Electrotechnology Conference, </booktitle> <pages> pp. 84-87, </pages> <month> April </month> <year> 1992. </year>
Reference-contexts: This observation suggests that an adaptive prefetching strategy may allow the processors to more consistently take advantage of this alignment effect. In particular, it may be possible to improve the memory performance by matching the loop blocking factor, or chunk size, to the cache fetch size <ref> [16] </ref>. However, fixing the loop blocking factor to match the fetch size may unbalance the distribution of iterations to processors, which would reduce the overall parallelism and thereby increase the execution time. <p> Acknowledgements A preliminary version of this work was presented at the First Midwest Electrotechnology Conference in April 1992 <ref> [16] </ref>. The comments and suggestions provided by the anonymous reviewers helped to improve the quality of this paper, and their efforts are appreciated. Support for this work was provided in part by the National Science Foundation under grants CCR-9209458 and MIP-9221900.
Reference: 17. <author> David J. Lilja and Pen-Chung Yew, </author> <title> ``Improving Memory Utilization in Cache Coherence Directories,'' </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <note> (to appear). </note>
Reference-contexts: This increasing number of invalidations can significantly increase the average memory delay, however, since the directory must wait for each processor with a cached copy of the block being invalidated to acknowledge the invalidation <ref> [17] </ref>. The adaptive prefetching scheme tends to produce the smallest number of invalidations, and thus will reduce the average memory delay when compared to the nonadaptive schemes.
Reference: 18. <author> Robert Perron and Craig Mundie, </author> <booktitle> ``The Architecture of the Alliant FX/8 Computer,'' IEEE COMPCON, </booktitle> <pages> pp. 390-393, </pages> <year> 1986. </year>
Reference-contexts: Parallel loops within application programs written in Fortran are found automatically using the Alliant compiler <ref> [18] </ref>. The compiler produces parallel assembly code which is executed by a multiprocessor emulator using static scheduling with c =1, single-chunk scheduling with c= R N/p H , or GSS. The emulator produces a trace of all addresses generated by each of the p processors.
Reference: 19. <author> G. F. Pfister, W. C. Brantley, D. A. George, S. L. Harvey, W. J. Kleinfelder, K. P. McAuliffe, E. A. Melton, V. A. Norton, and J. Weiss, </author> <title> ``The IBM Research Parallel Processor Prototype (RP3): Introduction and Architecture,'' </title> <booktitle> International Conference on Parallel Processing, </booktitle> <pages> pp. 764-771, </pages> <year> 1985. </year>
Reference-contexts: To extend this previous work, this paper examines how data prefetching can be used to improve memory performance in a shared memory multiprocessor with a multistage interconnection network <ref> [9, 14, 19] </ref>, taking into consideration how the parallel task scheduling strategy interacts with the prefetching strategy. This study concentrates on numerical Fortran programs since these large-scale parallel systems frequently are used to execute this type of program.
Reference: 20. <author> C. Polychronopoulos and D. Kuck, </author> <title> ``Guided Self-Scheduling: A Practical Scheduling Scheme for Parallel Supercomputers,'' </title> <journal> IEEE Transactions on Computers, </journal> <volume> Vol. C-36, No. 12, </volume> <pages> pp. 1425-1439, </pages> <month> December </month> <year> 1987. </year>
Reference-contexts: Larger values of c decrease the scheduling overhead at the expense of poorer load balancing. Guided self-scheduling <ref> [20] </ref> combines a dynamic scheduling strategy with a variable loop blocking factor in an attempt to compromise between the low scheduling overhead of large values of c , and the good load balancing of smaller values of c .
Reference: 21. <author> Constantine D. Polychronopoulos, </author> <title> ``Toward Auto-scheduling Compilers,'' </title> <journal> Journal of Supercomputing, </journal> <volume> No. 2, </volume> <pages> pp. 297-330, </pages> <year> 1988. </year>
Reference-contexts: Parallel Loop Scheduling Strategies Since the body of a loop is executed many times, a large portion of the parallelism available in a program may be exploited by simultaneously executing independent loop iterations on different processors <ref> [21] </ref>. For example, a Fortran doall loop is one in which each iteration of the loop is independent of the other iterations; that is, in this type of loop, there are no dependences between iterations.
Reference: 22. <author> Steven Przybylski, Mark Horowitz, and John Hennessy, </author> <title> ``Performance Tradeoffs in Cache Design,'' </title> <booktitle> International Symposium on Computer Architecture, </booktitle> <pages> pp. 290-296, </pages> <year> 1988. </year>
Reference-contexts: It has been pointed out that due to the time required to transfer large cache blocks, the block size that minimizes the average memory delay typically is smaller than the block size that minimizes the miss ratio <ref> [22, 26] </ref>. The performance impact of the larger blocks can be reduced by using lockup-free caches [13, 24], although the temporal clustering of misses may reduce the effectiveness of these complex fetch strategies [23].
Reference: 23. <author> Steven Przybylski, </author> <title> ``The Performance Impact of Block Sizes and Fetch Strategies,'' </title> <booktitle> International Symposium on Computer Architecture, </booktitle> <pages> pp. 160-169, </pages> <year> 1990. </year>
Reference-contexts: The performance impact of the larger blocks can be reduced by using lockup-free caches [13, 24], although the temporal clustering of misses may reduce the effectiveness of these complex fetch strategies <ref> [23] </ref>. An alternative to the implicit hardware prefetching of multiword cache blocks is to have the fetch size, f , greater than one block so that the hardware can explicitly prefetch multiple blocks.
Reference: 24. <author> C. Scheurich and M. Dubois, </author> <title> ``The Design of a Lockup-Free Cache for High-Performance Multiprocessors,'' </title> <booktitle> Proceedings of Supercomputing '88, </booktitle> <pages> pp. 352-359, </pages> <year> 1988. </year>
Reference-contexts: The performance impact of the larger blocks can be reduced by using lockup-free caches <ref> [13, 24] </ref>, although the temporal clustering of misses may reduce the effectiveness of these complex fetch strategies [23]. An alternative to the implicit hardware prefetching of multiword cache blocks is to have the fetch size, f , greater than one block so that the hardware can explicitly prefetch multiple blocks.
Reference: 25. <author> Alan Jay Smith, </author> <title> ``Cache Memories,'' </title> <journal> ACM Computing Surveys, </journal> <volume> Vol. 14, No. 3, </volume> <pages> pp. 473-530, </pages> <month> September </month> <year> 1982. </year> <month> 22 </month>
Reference-contexts: This reduction occurs because many programs exhibit the property of spatial locality in which memory addresses physically near each other in the memory space are likely to be referenced within a short span of time <ref> [25] </ref>. Assuming that the total number of words that can be stored in a cache is fixed, there is an optimum block size that minimizes the miss ratio. <p> For instance, the cache controller can be designed to fetch a fixed number of consecutive blocks on every miss <ref> [25] </ref>, or to prefetch extra blocks only on a miss for a long vector operand [7]. While this latter approach improves the miss ratio for these vectors, it ignores the potential improvement from prefetching scalars and arrays that do not vectorize.
Reference: 26. <author> Alan Jay Smith, </author> <title> ``Line (Block) Size Choice for CPU Cache Memories,'' </title> <journal> IEEE Transactions on Computers, </journal> <volume> Vol. C-36, No. 9, </volume> <pages> pp. 1063-1075, </pages> <month> September </month> <year> 1987. </year>
Reference-contexts: It has been pointed out that due to the time required to transfer large cache blocks, the block size that minimizes the average memory delay typically is smaller than the block size that minimizes the miss ratio <ref> [22, 26] </ref>. The performance impact of the larger blocks can be reduced by using lockup-free caches [13, 24], although the temporal clustering of misses may reduce the effectiveness of these complex fetch strategies [23].

References-found: 26

