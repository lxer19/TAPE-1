URL: ftp://cns.brown.edu/nin/papers/hybrid_preprint.ps.Z
Refering-URL: http://www.math.tau.ac.il/~nin/research.html
Root-URL: 
Phone: 2  
Title: PHONETIC CLASSIFICATION OF TIMIT SEGMENTS PREPROCESSED WITH LYON'S COCHLEAR MODEL USING A SUPERVISED/UNSUPERVISED HYBRID NEURAL NETWORK  
Author: Gary N. Tajchman Nathan Intrator 
Address: Box 1978, Providence, RI 02912  Box 1843, Providence, RI 02912  
Affiliation: 1 Dept. of Cognitive and Linguistic Sciences, Brown University,  Center for Neural Science, Brown University,  
Abstract: We report results on vowel and stop consonant recognition with tokens extracted from the TIMIT database. Our current system differs from others doing similar tasks in that we do not use any specific time normalization techniques. We use a very detailed biologically motivated input representation of the speech tokens - Lyon's cochlear model as implemented by Slaney [20]. This detailed, high dimensional representation, known as a cochleagram, is classified by either a back-propagation or by a hybrid supervised/unsupervised neural network classifier. The hybrid network is composed of a biologically motivated unsupervised network and a supervised back-propagation network. This approach produces results comparable to those obtained by others without the addition of time normalization. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> A. R. Barron and R. L. Barron. </author> <title> Statistical learning networks: A unifying view. </title> <editor> In Ed Wegman, editor, </editor> <booktitle> Computing Science and Statistics: Proc. 20th Symp. Interface, </booktitle> <pages> pages 192-203. </pages> <publisher> American Statistical Association, </publisher> <address> Washington, DC., </address> <year> 1988. </year>
Reference-contexts: Artificial neural network architectures provide a means for feature extraction in the form of low dimensional projections. These methods were found to be less sensitive to the dimensionality <ref> [1, 15] </ref> than many conventional non-parametric techniques such as kernel or nearest neighbor classifiers. However, in very large dimensional representations such as the ones used here, a hybrid method that combines unsupervised constraints with supervised training [7] improved classification results.
Reference: [2] <author> R. E. Bellman. </author> <title> Adaptive Control Processes. </title> <publisher> Princeton University Press, </publisher> <address> Princeton, NJ, </address> <year> 1961. </year>
Reference-contexts: We use the cochleagram outputs of this model with as many as 6300 dimensions per segmental token as input to our networks. When using such detailed high-dimensional representation of the speech data, many existing classification techniques fail to work, mainly because of the curse of dimensionality <ref> [2] </ref>. This problem is related to the sparsity of high dimensional spaces, and implies that the amount of training data has to grow exponentially with the dimensionality.
Reference: [3] <author> E. L. Bienenstock, L. N. Cooper, and P. W. Munro. </author> <title> Theory for the development of neuron selectivity: orientation specificity and binocular interaction in visual cortex. </title> <journal> Journal Neuroscience, </journal> <volume> 2 </volume> <pages> 32-48, </pages> <year> 1982. </year>
Reference-contexts: THE FEATURE EXTRACTION - CLASSIFICATION In addition to a standard single hidden layer network trained using the back-propagation algorithm, we applied a hybrid method that combines unsupervised feature extraction and classification [7]. The unsupervised feature extraction is based on the biologically motived BCM neuron <ref> [3] </ref> and was shown [6] to be related to a statistical technique called exploratory projection pursuit [4] The hybrid 2 method incorporates a single hidden layer ANN trained to minimize MSE using error back-propagation in addition to minimizing a projection pursuit index [8] that favors multi-modality in the projected distribution of
Reference: [4] <author> J. H. Friedman. </author> <title> Exploratory projection pursuit. </title> <journal> Journal of the American Statistical Association, </journal> <volume> 82 </volume> <pages> 249-266, </pages> <year> 1987. </year>
Reference-contexts: The unsupervised feature extraction is based on the biologically motived BCM neuron [3] and was shown [6] to be related to a statistical technique called exploratory projection pursuit <ref> [4] </ref> The hybrid 2 method incorporates a single hidden layer ANN trained to minimize MSE using error back-propagation in addition to minimizing a projection pursuit index [8] that favors multi-modality in the projected distribution of the hidden units.
Reference: [5] <author> N. Hataoka and A. H. Waibel. </author> <title> Speaker-independent phoneme recognition on TIMIT database using integrated time-dely neural networks(TDNNs). </title> <type> Technical report, </type> <institution> Carnegie Mellon University, </institution> <year> 1989. </year> <month> CMU-CS-89-190. </month>
Reference-contexts: Using a variant of this approach in which three networks with different durations of temporal input (100ms, 150ms, and 200ms) Hataoka and Waibel obtained 59.3% correct for the set of stressed vowels extracted from TIMIT <ref> [5] </ref>. A third approach is to use recurrent neural networks. This technique allows the individual units to learn something about the temporal structure of the input.
Reference: [6] <author> N. Intrator. </author> <title> A neural network for feature extraction. </title> <editor> In D. S. Touretzky and R. P. Lippmann, editors, </editor> <booktitle> Advances in Neural Information Processing Systems, </booktitle> <volume> volume 2, </volume> <pages> pages 719-726. </pages> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, CA, </address> <year> 1990. </year>
Reference-contexts: THE FEATURE EXTRACTION - CLASSIFICATION In addition to a standard single hidden layer network trained using the back-propagation algorithm, we applied a hybrid method that combines unsupervised feature extraction and classification [7]. The unsupervised feature extraction is based on the biologically motived BCM neuron [3] and was shown <ref> [6] </ref> to be related to a statistical technique called exploratory projection pursuit [4] The hybrid 2 method incorporates a single hidden layer ANN trained to minimize MSE using error back-propagation in addition to minimizing a projection pursuit index [8] that favors multi-modality in the projected distribution of the hidden units.
Reference: [7] <author> N. Intrator. </author> <title> Combining exploratory projection pursuit and projection pursuit regression with application to neural networks, 1991. </title> <type> Preprint. </type>
Reference-contexts: These methods were found to be less sensitive to the dimensionality [1, 15] than many conventional non-parametric techniques such as kernel or nearest neighbor classifiers. However, in very large dimensional representations such as the ones used here, a hybrid method that combines unsupervised constraints with supervised training <ref> [7] </ref> improved classification results. INPUT REPRESENTATION - LYON'S COCHLEAR MODEL Many models of the mammalian peripheral auditory system are linear approximations of the biomechanical information processing of real biological systems. <p> Figures 3 and Figure 4 show examples of the cochleagram representation. THE FEATURE EXTRACTION - CLASSIFICATION In addition to a standard single hidden layer network trained using the back-propagation algorithm, we applied a hybrid method that combines unsupervised feature extraction and classification <ref> [7] </ref>.
Reference: [8] <author> N. Intrator and L. N. Cooper. </author> <title> Objective function formulation of the BCM theory of visual cortical plasticity: Statistical connections, stability conditions. </title> <booktitle> Neural Networks, </booktitle> <volume> 5 </volume> <pages> 3-17, </pages> <year> 1992. </year>
Reference-contexts: is based on the biologically motived BCM neuron [3] and was shown [6] to be related to a statistical technique called exploratory projection pursuit [4] The hybrid 2 method incorporates a single hidden layer ANN trained to minimize MSE using error back-propagation in addition to minimizing a projection pursuit index <ref> [8] </ref> that favors multi-modality in the projected distribution of the hidden units. This hybrid method (Figure 5) has already been success fully used for feature extraction and classification in earlier speech recognition tasks [10], and in face recognition [9].
Reference: [9] <author> N. Intrator, D. Reisfeld, and Y. Yeshurun. </author> <title> Face recognition using a hybrid supervised/unsupervised neural network, 1992. </title> <type> Preprint. </type>
Reference-contexts: This hybrid method (Figure 5) has already been success fully used for feature extraction and classification in earlier speech recognition tasks [10], and in face recognition <ref> [9] </ref>. RESULTS In previous work with the hybrid network [10] we found a reliable difference between backprop and the hybrid technique for a voiceless stop consonant identification when presented with vowel context.
Reference: [10] <author> N. Intrator and G. Tajchman. </author> <title> Unsupervised feature extraction from a cochlear model for speech recognition. </title> <editor> In B. H. Juang, S. Y. Kung, and C. A. Kamm, editors, </editor> <booktitle> Neural Networks for Signal Processing: Proceedings of 1991 IEEE-SP Workshop. IEEE, </booktitle> <year> 1991. </year>
Reference-contexts: This hybrid method (Figure 5) has already been success fully used for feature extraction and classification in earlier speech recognition tasks <ref> [10] </ref>, and in face recognition [9]. RESULTS In previous work with the hybrid network [10] we found a reliable difference between backprop and the hybrid technique for a voiceless stop consonant identification when presented with vowel context. <p> This hybrid method (Figure 5) has already been success fully used for feature extraction and classification in earlier speech recognition tasks <ref> [10] </ref>, and in face recognition [9]. RESULTS In previous work with the hybrid network [10] we found a reliable difference between backprop and the hybrid technique for a voiceless stop consonant identification when presented with vowel context. In the set of experiments reported here, we moved on to a more traditional task of recognizing phonemes presented in isolation.
Reference: [11] <author> F. Itakura. </author> <title> Minimum prediction residual principle applied to spech recognition. </title> <journal> IEEE-ASSP, </journal> <volume> 23 </volume> <pages> 67-72, </pages> <month> February </month> <year> 1975. </year>
Reference-contexts: INTRODUCTION One of the problems in the classification of phonetic segments in a speaker-independent continuous task is the variability in the duration of the segments. The conventional solution to this problem is dynamic-time-warping <ref> [11] </ref>. Within the domain of neural networks for speech recognition, several other solutions have been offered. One is to first normalize the length of the segment across all tokens using prior knowledge of the segment boundaries.
Reference: [12] <author> D. O. Kim. </author> <title> Functional roles of the inner- and outer-hair-cell subsystems in the cochlea and brainstem. </title> <editor> In C. I. Berlin, editor, </editor> <booktitle> Hearing Science: Recent Advances, </booktitle> <pages> page 241. </pages> <publisher> Colleg-Hill Press, </publisher> <address> San Diego, CA, </address> <year> 1984. </year>
Reference-contexts: Whereas the behavioral data in the form of discrimination performance for awake animals indicate much sharper tuning curves. This has lead to such theories as the illusive second filter and lateral inhibition in later stages of processing [18]. However, citing findings by auditory neurophysiologist Kim <ref> [12] </ref>, Lyon asserts that the explanation comes instead from the introduction of active nonlinearities at the level of the outer hair cells in the cochlea.
Reference: [13] <author> K. F. Lee and H. W. Hon. </author> <title> Speaker-independent phone recognition using hidden markov models. </title> <type> Technical Report CMU-CS-88-121, </type> <institution> Carnegie-Mellon University, </institution> <year> 1988. </year>
Reference-contexts: In general, these error rates compare favorably to others reported in the literature (eg. about 40% error with the stops <ref> [13] </ref>). CONCLUSIONS We have shown that a cochlear model producing a large dimensional, detailed output can be used to produce good results in speaker independent phonetic recognition tasks. In addition, the hybrid unsupervised/supervised weight modification technique outperformed straight backpropagation in most cases.
Reference: [14] <author> H. C. Leung and V. W. Zue. </author> <title> Phonetic classification using multi-layer perceptrons. </title> <booktitle> In Proceedings IEEE International Conference on Acoustics, Speech, and Signal Processing, </booktitle> <pages> pages 525-528, </pages> <year> 1990. </year>
Reference-contexts: One is to first normalize the length of the segment across all tokens using prior knowledge of the segment boundaries. This approach has provided good results with the set of stressed vowels extracted from TIMIT (60% <ref> [14] </ref>). However, it does require segment boundaries be known prior to recognition so that normalization can take place. Another approach that has been proposed is to use a time-delay neural network (TDNN) [22]. This technique essentially replicates the network input window to cover an entire, possibly unaligned token.
Reference: [15] <author> R. P. Lippmann. </author> <title> Review of neural networks for speech recognition. </title> <journal> Neural Computation, </journal> <volume> 1 </volume> <pages> 1-38, </pages> <year> 1989. </year>
Reference-contexts: Artificial neural network architectures provide a means for feature extraction in the form of low dimensional projections. These methods were found to be less sensitive to the dimensionality <ref> [1, 15] </ref> than many conventional non-parametric techniques such as kernel or nearest neighbor classifiers. However, in very large dimensional representations such as the ones used here, a hybrid method that combines unsupervised constraints with supervised training [7] improved classification results.
Reference: [16] <author> R. Lyon and C. Mead. </author> <title> An analog electronic cochlea. </title> <journal> IEEE Transactions on Acoustics, Speech, and Signal Processing, </journal> <volume> 36 </volume> <pages> 1119-1134, </pages> <year> 1988. </year>
Reference-contexts: INPUT REPRESENTATION - LYON'S COCHLEAR MODEL Many models of the mammalian peripheral auditory system are linear approximations of the biomechanical information processing of real biological systems. Lyon makes the point that doing so can lead to serious problems when interpreting the body of physiological data now in existence <ref> [16] </ref>. For example, one of the most vexing problems in the auditory physiology literature is determining the sharpness of the auditory nerve fiber tuning curves. Measured from the responses of single auditory nerve fibers in anesthetized cats, the tuning curves are relatively broad.
Reference: [17] <author> R. F. Lyon. </author> <title> A computational model of filtering, detection, and compression in the cochlea. </title> <booktitle> In Proceedings IEEE International Conference on Acoustics, Speech, and Signal Processing, </booktitle> <year> 1982. </year>
Reference-contexts: Even though the approach we currently use does not use any mechanism that explicitly deals with temporal variability we obtain comparable results to those reported in the literature. Specifically, we use the biologically plausible preprocessing provided by Slaney's implementation of Lyon's cochlear model <ref> [20, 17] </ref>. We use the cochleagram outputs of this model with as many as 6300 dimensions per segmental token as input to our networks. When using such detailed high-dimensional representation of the speech data, many existing classification techniques fail to work, mainly because of the curse of dimensionality [2].
Reference: [18] <author> B. C. J. Moore. </author> <title> An Introduction to the Psychology of Hearing. </title> <publisher> Academic Press, </publisher> <year> 1988. </year>
Reference-contexts: Whereas the behavioral data in the form of discrimination performance for awake animals indicate much sharper tuning curves. This has lead to such theories as the illusive second filter and lateral inhibition in later stages of processing <ref> [18] </ref>. However, citing findings by auditory neurophysiologist Kim [12], Lyon asserts that the explanation comes instead from the introduction of active nonlinearities at the level of the outer hair cells in the cochlea.
Reference: [19] <author> T. Robinson and F. Fallside. </author> <title> A recurrent error propagation network speech recognition system. </title> <booktitle> Computer Speech and Language, </booktitle> <volume> 5(3), </volume> <month> July </month> <year> 1991. </year>
Reference-contexts: This technique allows the individual units to learn something about the temporal structure of the input. Fall-side, Robinson and colleagues have implemented a complete system within this framework and report results of 70.0% correct with the entire set of 61 TIMIT phons <ref> [19] </ref>. Even though the approach we currently use does not use any mechanism that explicitly deals with temporal variability we obtain comparable results to those reported in the literature. Specifically, we use the biologically plausible preprocessing provided by Slaney's implementation of Lyon's cochlear model [20, 17].
Reference: [20] <author> M. Slaney. </author> <title> Lyon's cochlear model. </title> <type> Technical report, </type> <institution> Apple Corporate Library, Cupertino, </institution> <address> CA 95014, </address> <year> 1988. </year>
Reference-contexts: Even though the approach we currently use does not use any mechanism that explicitly deals with temporal variability we obtain comparable results to those reported in the literature. Specifically, we use the biologically plausible preprocessing provided by Slaney's implementation of Lyon's cochlear model <ref> [20, 17] </ref>. We use the cochleagram outputs of this model with as many as 6300 dimensions per segmental token as input to our networks. When using such detailed high-dimensional representation of the speech data, many existing classification techniques fail to work, mainly because of the curse of dimensionality [2].
Reference: [21] <institution> TIMIT Acoustic-Phonetic Continuous Speech Corpus. National Institute of Standards and Technology Speech Disc 1-1.1, </institution> <month> October </month> <year> 1990. </year> <title> NTIS Order No. </title> <publisher> PB91-505065. </publisher>
Reference-contexts: We focused on the set of sixteen stressed vowels in one subset of experiments, and on the six stop consonants in the other. For these experiments, training stimuli were extracted from the training portion of TIMIT <ref> [21] </ref>, and the test stimuli extracted from the test portion of TIMIT, always using the segment boundaries provided in the database. Vowels Various temporal resolutions (2ms to 16ms frame rates) have been tried with the sixteen stressed vowels from TIMIT.
Reference: [22] <author> A. Waibel, T. Hanazawa, G. Hinton, K. Shikano, and K. Lang. </author> <title> Phoneme recognition using time-delay neural networks. </title> <journal> IEEE Trans. on ASSP, </journal> <volume> 37 </volume> <pages> 328-339, </pages> <year> 1989. </year> <month> 4 </month>
Reference-contexts: This approach has provided good results with the set of stressed vowels extracted from TIMIT (60% [14]). However, it does require segment boundaries be known prior to recognition so that normalization can take place. Another approach that has been proposed is to use a time-delay neural network (TDNN) <ref> [22] </ref>. This technique essentially replicates the network input window to cover an entire, possibly unaligned token.
References-found: 22

