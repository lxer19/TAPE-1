URL: ftp://synapse.cs.byu.edu/pub/papers/wilson.icml97.prune.ps
Refering-URL: ftp://synapse.cs.byu.edu/pub/papers/details.html
Root-URL: 
Email: randy@axon.cs.byu.edu, martinez@cs.byu.edu  
Title: Instance Pruning Techniques  
Author: D. Randall Wilson, Tony R. Martinez 
Address: Provo, UT 84058  
Affiliation: Computer Science Department Brigham Young University  
Note: In Fisher, D., ed., Machine Learning: Proceedings of the Fourteenth International Conference, Morgan Kaufmann Publishers, San Francisco, CA, pp. 404-411, 1997.  
Abstract: The nearest neighbor algorithm and its derivatives are often quite successful at learning a concept from a training set and providing good generalization on subsequent input vectors. However, these techniques often retain the entire training set in memory, resulting in large memory requirements and slow execution speed, as well as a sensitivity to noise. This paper provides a discussion of issues related to reducing the number of instances retained in memory while maintaining (and sometimes improving) generalization accuracy, and mentions algorithms other researchers have used to address this problem. It presents three intuitive noise-tolerant algorithms that can be used to prune instances from the training set. In experiments on 29 applications, the algorithm that achieves the highest reduction in storage also results in the highest generalization accuracy of the three methods.
Abstract-found: 1
Intro-found: 1
Reference: <author> Aha, David W., Dennis Kibler, Marc K. Albert, </author> <year> (1991). </year> <title> Instance-Based Learning Algorithms, </title> <journal> Machine Learning, </journal> <volume> vol. 6, </volume> <pages> pp. 37-66. </pages>
Reference-contexts: INSTANCE-BASED LEARNING AL - GORITHMS Aha et. al. (1991) presented a series of instance-based learning algorithms that reduce storage. IB2 is quite similar to the Condensed Nearest Neighbor (CNN) rule (Hart, 1968), and suffers from the same sensitivity to noise. IB3 <ref> (Aha et al. 1991) </ref> addresses IB2s problem of keeping noisy instances by using a statistical test to retain only acceptable misclassified instances. <p> other hand, many models seek to retain a subset of the original instances, including the Condensed NN rule (CNN) (Hart, 1968), the Reduced NN rule (RNN) (Gates 1972), the Selective NN rule (SNN) (Ritter et. al., 1975), Wilsons rule (Wilson, 1972), the all k-NN method (Tomek, 1976), Instance-Based (IBL) Algorithms <ref> (Aha et. al. 1991) </ref>, and the Typical Instance Based Learning (TIBL) algorithm (Zhang, 1992). Another decision that affects the concept description for many algorithms is the choice of k, which is the number of neighbors used to decide the output class of an input vector. <p> 7.81 2.56 6.86 39.18 10.33 8.88 13.97 8.03 14.79 10.96 24.32 13.22 15.22 29.17 13.42 Results were also obtained for several of the methods discussed in Section 2, including CNN (Hart, 1968), SNN (Ritter et al., 1975), Wilsons Rule (Wilson, 1972), the All k- NN method (Tomek, 1976), IB2, IB3 <ref> (Aha, Kibler & Albert, 1991) </ref>, and the E x p l o r e method (Cameron-Jones, 1995).
Reference: <author> Cameron-Jones, R. M., </author> <year> (1995). </year> <title> Instance Selection by Encoding Length Heuristic with Random Mutation Hill Climbing. </title> <booktitle> In Proceedings of the Eighth Australian Joint Conference on Artificial Intelligence, </booktitle> <pages> pp. 99-106. </pages>
Reference: <author> Chang, Chin-Liang, </author> <year> (1974). </year> <title> Finding Prototypes for Nearest Neighbor Classifiers, </title> <journal> IEEE Transactions on Computers, </journal> <volume> vol. 23, no. 11, </volume> <month> November </month> <year> 1974, </year> <pages> pp. 1179-1184. </pages>
Reference-contexts: For example, NGE (Salzberg, 1991) and its derivatives (Wettschereck & Dietterich, 1995) use hyperrectangles to represent collections of instances; RISE (Domingos, 1995) generalizes instances into rules; and prototypes <ref> (Chang 1974) </ref> can be used to represent a cluster of instances, even if no original instance occurred at the point where the prototype is located.
Reference: <author> Cover, T. M., and P. E. Hart, </author> <year> (1967). </year> <title> Nearest Neighbor Pattern Classification, </title> <journal> Institute of Electrical and Electronics Engineers Transactions on Information Theory, </journal> <volume> vol. 13, no. 1, </volume> <month> January </month> <year> 1967, </year> <pages> pp. 21-27. </pages>
Reference: <author> Dasarathy, Belur V., </author> <year> (1991). </year> <title> Nearest Neighbor (NN) Norms: NN Pattern Classification Techniques . Los Alamitos, </title> <address> CA: </address> <publisher> IEEE Computer Society Press. </publisher>
Reference: <author> Domingos, Pedro, </author> <year> (1995). </year> <title> Rule Induction and Instance - Based Learning: A Unified Approach, </title> <booktitle> to appear in The 1995 International Joint Conference on Artificial Intelligence (IJCAI-95). </booktitle>
Reference-contexts: For example, NGE (Salzberg, 1991) and its derivatives (Wettschereck & Dietterich, 1995) use hyperrectangles to represent collections of instances; RISE <ref> (Domingos, 1995) </ref> generalizes instances into rules; and prototypes (Chang 1974) can be used to represent a cluster of instances, even if no original instance occurred at the point where the prototype is located.
Reference: <author> Gates, G. W. </author> <year> (1972). </year> <title> The Reduced Nearest Neighbor Rule, </title> <journal> IEEE Transactions on Information Theory , vol. IT-18, </journal> <volume> no. 3, </volume> <pages> pp. 431-433. </pages>
Reference-contexts: Section 4 presents four new instance reduction techniques that were hypothesized to provide substantial instance reduction while continuing to generalize accurately, even in the presence of noise. The first is similar to the Reduced Nearest Neighbor (RNN) algorithm <ref> (Gates 1972) </ref>. The second changes the order in which instances are considered for removal, and the 2 third adds a noise-reduction step similar to that done by Wilson (1972) before proceeding with the main reduction algorithm. <p> On the other hand, many models seek to retain a subset of the original instances, including the Condensed NN rule (CNN) (Hart, 1968), the Reduced NN rule (RNN) <ref> (Gates 1972) </ref>, the Selective NN rule (SNN) (Ritter et. al., 1975), Wilsons rule (Wilson, 1972), the all k-NN method (Tomek, 1976), Instance-Based (IBL) Algorithms (Aha et. al. 1991), and the Typical Instance Based Learning (TIBL) algorithm (Zhang, 1992).
Reference: <author> Hart, P. E., </author> <year> (1968). </year> <title> The Condensed Nearest Neighbor Rule, </title> <journal> Institute of Electrical and Electronics Engineers Transactions on Information Theory, </journal> <volume> vol. 14, </volume> <pages> pp. 515-516. </pages>
Reference-contexts: INSTANCE-BASED LEARNING AL - GORITHMS Aha et. al. (1991) presented a series of instance-based learning algorithms that reduce storage. IB2 is quite similar to the Condensed Nearest Neighbor (CNN) rule <ref> (Hart, 1968) </ref>, and suffers from the same sensitivity to noise. IB3 (Aha et al. 1991) addresses IB2s problem of keeping noisy instances by using a statistical test to retain only acceptable misclassified instances. <p> On the other hand, many models seek to retain a subset of the original instances, including the Condensed NN rule (CNN) <ref> (Hart, 1968) </ref>, the Reduced NN rule (RNN) (Gates 1972), the Selective NN rule (SNN) (Ritter et. al., 1975), Wilsons rule (Wilson, 1972), the all k-NN method (Tomek, 1976), Instance-Based (IBL) Algorithms (Aha et. al. 1991), and the Typical Instance Based Learning (TIBL) algorithm (Zhang, 1992). <p> 26.87 23.00 16.11 H-IB3 85.99 59.37 93.39 67.77 74.23 69.50 84.62 61.82 88.32 52.60 55.64 88.59 90.23 94.70 77.41 (size) 7.81 2.56 6.86 39.18 10.33 8.88 13.97 8.03 14.79 10.96 24.32 13.22 15.22 29.17 13.42 Results were also obtained for several of the methods discussed in Section 2, including CNN <ref> (Hart, 1968) </ref>, SNN (Ritter et al., 1975), Wilsons Rule (Wilson, 1972), the All k- NN method (Tomek, 1976), IB2, IB3 (Aha, Kibler & Albert, 1991), and the E x p l o r e method (Cameron-Jones, 1995).
Reference: <author> Merz, C. J., and P. M. Murphy, </author> <year> (1996). </year> <title> UCI Repository of Machine Learning Databases. </title> <address> Irvine, CA: </address> <institution> University of California Irvine, Department of Information and Computer Science. Internet: </institution> <address> http://www.ics.uci.edu/ ~mlearn/ MLRepository.html. </address>
Reference-contexts: EXPERIMENTAL RESULTS The reduction algorithms RT1, RT2 and RT3 were implemented using k =3, and using the HVDM distance function described in Section 3.4. These algorithms were tested on 29 data sets from the University of California, Irvine Machine Learning Database Repository <ref> (Merz & Murphy, 1996) </ref> and compared to a k-nearest neighbor classifier that was identical to RT1 except that it does not remove any instances from the instance set (i.e., S=T).
Reference: <author> Papadimitriou, Christos H., and Jon Louis Bentley, </author> <year> (1980). </year> <title> A Worst-Case Analysis of Nearest Neighbor Searching by Projection. </title> <booktitle> Lecture Notes in Computer Science, </booktitle> <volume> Vol. 85, </volume> <booktitle> Automata Languages and Programming, </booktitle> <pages> pp. 470-482. </pages>
Reference-contexts: Since it stores every instance in the training set, noisy instances (i.e., those with errors in the input vector or output class, or those not representative of typical cases) are stored as well, which can degrade generalization accuracy. Techniques such as k-d trees (Sproull, 1991) and projection <ref> (Papadimitriou & Bentley, 1980) </ref> can reduce the time required to find the nearest neighbor (s) of an input vector, but they do not reduce storage requirements, do not address the problem of noise, and often become much less effective as the dimensionality of the problem (i.e., the number of input attributes)
Reference: <author> Ritter, G. L., H. B. Woodruff, S. R. Lowry, and T. L. Isenhour, </author> <year> (1975). </year> <title> An Algorithm for a Selective Nearest Neighbor Decision Rule, </title> <journal> IEEE Transactions on Information Theory, </journal> <volume> vol. 21, no. 6, </volume> <month> November </month> <year> 1975, </year> <pages> pp. 665-669. </pages>
Reference-contexts: On the other hand, many models seek to retain a subset of the original instances, including the Condensed NN rule (CNN) (Hart, 1968), the Reduced NN rule (RNN) (Gates 1972), the Selective NN rule (SNN) <ref> (Ritter et. al., 1975) </ref>, Wilsons rule (Wilson, 1972), the all k-NN method (Tomek, 1976), Instance-Based (IBL) Algorithms (Aha et. al. 1991), and the Typical Instance Based Learning (TIBL) algorithm (Zhang, 1992). <p> H-IB3 85.99 59.37 93.39 67.77 74.23 69.50 84.62 61.82 88.32 52.60 55.64 88.59 90.23 94.70 77.41 (size) 7.81 2.56 6.86 39.18 10.33 8.88 13.97 8.03 14.79 10.96 24.32 13.22 15.22 29.17 13.42 Results were also obtained for several of the methods discussed in Section 2, including CNN (Hart, 1968), SNN <ref> (Ritter et al., 1975) </ref>, Wilsons Rule (Wilson, 1972), the All k- NN method (Tomek, 1976), IB2, IB3 (Aha, Kibler & Albert, 1991), and the E x p l o r e method (Cameron-Jones, 1995).
Reference: <author> Salzberg, Steven, </author> <year> (1991). </year> <title> A Nearest Hyperrectangle Learning Method, </title> <journal> Machine Learning, </journal> <volume> vol. 6, </volume> <pages> pp. 277 - 309. </pages>
Reference-contexts: REPRESENTATION One choice in designing a training set reduction algorithm is to decide whether to retain a subset of the original instances or whether to modify the instances using a new representation. For example, NGE <ref> (Salzberg, 1991) </ref> and its derivatives (Wettschereck & Dietterich, 1995) use hyperrectangles to represent collections of instances; RISE (Domingos, 1995) generalizes instances into rules; and prototypes (Chang 1974) can be used to represent a cluster of instances, even if no original instance occurred at the point where the prototype is located.
Reference: <author> Sproull, Robert F., </author> <year> (1991). </year> <title> Refinements to Nearest - Neighbor Searching in k Dimensional Trees. </title> <journal> Algorithmica, </journal> <volume> Vol.6, </volume> <pages> pp. 579-589. </pages>
Reference-contexts: Since it stores every instance in the training set, noisy instances (i.e., those with errors in the input vector or output class, or those not representative of typical cases) are stored as well, which can degrade generalization accuracy. Techniques such as k-d trees <ref> (Sproull, 1991) </ref> and projection (Papadimitriou & Bentley, 1980) can reduce the time required to find the nearest neighbor (s) of an input vector, but they do not reduce storage requirements, do not address the problem of noise, and often become much less effective as the dimensionality of the problem (i.e., the
Reference: <author> Stanfill, C., and D. Waltz, </author> <year> (1986). </year> <title> Toward memory-based reasoning, </title> <journal> Communications of the ACM, </journal> <volume> vol. 29, </volume> <month> December </month> <year> 1986, </year> <pages> pp. 1213-1228. </pages>
Reference-contexts: When nominal (discrete, unordered) attributes are included in an application, a distance metric is needed that can handle them. We use a distance function based upon the Value Difference Metric (VDM) <ref> (Stanfill & Waltz, 1986) </ref> for nominal attributes.
Reference: <author> Tomek, Ivan, </author> <year> (1976). </year> <title> An Experiment with the Edited Nearest-Neighbor Rule, </title> <journal> IEEE Transactions on Systems, Man, and Cybernetics, </journal> <volume> vol. 6, no. 6, </volume> <month> June </month> <year> 1976, </year> <pages> pp. 448 - 452. </pages>
Reference-contexts: On the other hand, many models seek to retain a subset of the original instances, including the Condensed NN rule (CNN) (Hart, 1968), the Reduced NN rule (RNN) (Gates 1972), the Selective NN rule (SNN) (Ritter et. al., 1975), Wilsons rule (Wilson, 1972), the all k-NN method <ref> (Tomek, 1976) </ref>, Instance-Based (IBL) Algorithms (Aha et. al. 1991), and the Typical Instance Based Learning (TIBL) algorithm (Zhang, 1992). Another decision that affects the concept description for many algorithms is the choice of k, which is the number of neighbors used to decide the output class of an input vector. <p> 90.23 94.70 77.41 (size) 7.81 2.56 6.86 39.18 10.33 8.88 13.97 8.03 14.79 10.96 24.32 13.22 15.22 29.17 13.42 Results were also obtained for several of the methods discussed in Section 2, including CNN (Hart, 1968), SNN (Ritter et al., 1975), Wilsons Rule (Wilson, 1972), the All k- NN method <ref> (Tomek, 1976) </ref>, IB2, IB3 (Aha, Kibler & Albert, 1991), and the E x p l o r e method (Cameron-Jones, 1995).
Reference: <author> Wettschereck, Dietrich, </author> <year> (1994). </year> <title> A Hybrid Nearest - Neighbor and Nearest-Hyperrectangle Algorithm, </title> <booktitle> To appear in the Proceedings of the 7th European Conference on Machine Learning. </booktitle>
Reference: <author> Wettschereck, Dietrich, and Thomas G. Dietterich, </author> <year> (1995). </year> <title> An Experimental Comparison of Nearest - Neighbor and Nearest-Hyperrectangle Algorithms, </title> <journal> Machine Learning, </journal> <volume> vol. 19, no. 1, </volume> <pages> pp. 5-28. </pages>
Reference-contexts: REPRESENTATION One choice in designing a training set reduction algorithm is to decide whether to retain a subset of the original instances or whether to modify the instances using a new representation. For example, NGE (Salzberg, 1991) and its derivatives <ref> (Wettschereck & Dietterich, 1995) </ref> use hyperrectangles to represent collections of instances; RISE (Domingos, 1995) generalizes instances into rules; and prototypes (Chang 1974) can be used to represent a cluster of instances, even if no original instance occurred at the point where the prototype is located.
Reference: <author> Wilson, D. Randall, and Tony R. Martinez, </author> <year> (1997). </year> <title> Improved Heterogeneous Distance Functions, </title> <journal> Journal of Artificial Intelligence Research (JAIR), </journal> <volume> vol. 6, no. 1, </volume> <pages> pp. 1-34. </pages>
Reference-contexts: Using this distance measure, two values are considered to be closer if they have more similar classifications, regardless of the order of the values. In order to handle heterogeneous applicationsthose with both numeric and nominal attributeswe use the heterogeneous distance function HV DM <ref> (Wilson & Martinez, 1997) </ref>, which is defined as: HVDM (x, y) = d a 2 ( x a , y a ) m where the function d a (x,y) is the distance for attribute a and is defined as: d a ( x, y) = vdm a ( x, y), if <p> These reduction algorithms were also among the first to use heterogeneous distance functions that are appropriate for applications with both nominal and continuous attributes <ref> (Wilson & Martinez, 1997) </ref>. Future research will focus on determining the conditions under which these algorithms are not appropriate, and will seek to overcome weaknesses in such areas.
Reference: <author> Wilson, Dennis L., </author> <year> (1972). </year> <title> Asymptotic Properties of Nearest Neighbor Rules Using Edited Data, </title> <journal> IEEE Transactions on Systems, Man, and Cybernetics , vol. </journal> <volume> 2, no. 3, </volume> <pages> pp. 408-421. </pages>
Reference-contexts: On the other hand, many models seek to retain a subset of the original instances, including the Condensed NN rule (CNN) (Hart, 1968), the Reduced NN rule (RNN) (Gates 1972), the Selective NN rule (SNN) (Ritter et. al., 1975), Wilsons rule <ref> (Wilson, 1972) </ref>, the all k-NN method (Tomek, 1976), Instance-Based (IBL) Algorithms (Aha et. al. 1991), and the Typical Instance Based Learning (TIBL) algorithm (Zhang, 1992). <p> RT3 therefore uses a noise-filtering pass before sorting the instances in S. This is done using a rule similar to Wilsons Rule <ref> (Wilson, 1972) </ref>: Any instance misclassified by its k nearest neighbors is removed. This removes noisy instances, as well as close border points, which can in turn smooth the decision boundary slightly. <p> 69.50 84.62 61.82 88.32 52.60 55.64 88.59 90.23 94.70 77.41 (size) 7.81 2.56 6.86 39.18 10.33 8.88 13.97 8.03 14.79 10.96 24.32 13.22 15.22 29.17 13.42 Results were also obtained for several of the methods discussed in Section 2, including CNN (Hart, 1968), SNN (Ritter et al., 1975), Wilsons Rule <ref> (Wilson, 1972) </ref>, the All k- NN method (Tomek, 1976), IB2, IB3 (Aha, Kibler & Albert, 1991), and the E x p l o r e method (Cameron-Jones, 1995).
Reference: <author> Zhang, Jianping, </author> <year> (1992). </year> <title> Selecting Typical Instances in Instance-Based Learning, </title> <booktitle> Proceedings of the Ninth International Conference on Machine Learning. </booktitle>
Reference-contexts: instances, including the Condensed NN rule (CNN) (Hart, 1968), the Reduced NN rule (RNN) (Gates 1972), the Selective NN rule (SNN) (Ritter et. al., 1975), Wilsons rule (Wilson, 1972), the all k-NN method (Tomek, 1976), Instance-Based (IBL) Algorithms (Aha et. al. 1991), and the Typical Instance Based Learning (TIBL) algorithm <ref> (Zhang, 1992) </ref>. Another decision that affects the concept description for many algorithms is the choice of k, which is the number of neighbors used to decide the output class of an input vector.
References-found: 20

