URL: http://www.cis.udel.edu/~jochen/passages/papers/iccl.ps
Refering-URL: http://www.cis.udel.edu/~jochen/passages/pubs.htm
Root-URL: http://www.cis.udel.edu
Email: carroll@udel.edu  pollock@udel.edu  
Title: Composites: Trees for Data Parallel Programming  
Author: Mark C. Carroll Lori Pollock 
Keyword: data parallel programming, object oriented programming, parallel data structures, distributed computing, parallel programming languages.  
Note: January 20, 1994  
Address: 19716  
Affiliation: Department of Computer Science University of Delaware, Newark, DE  
Abstract: Data parallel programming languages offer ease of programming and debugging and scalability of parallel programs to increasing numbers of processors. Unfortunately, the usefulness of these languages for non-scientific programmers and loosely coupled parallel machines is currently limited. In this paper, we present the composite tree model which seeks to provide greater flexibility via parallel data types, support for more general, hierachical parallelism, parallel control flow, and efficient execution on loosely coupled, coarse grained parallel machines such as workstation networks. The composite tree model is a new model of parallel programming based on merging data parallelism with object oriented programming languages, and can be implemented as a small set of extensions to any pure, static typed, object oriented programming language. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> High Performance Fortran, </author> <title> Draft Language Specification, </title> <address> 1.0 edition, </address> <year> 1993. </year>
Reference-contexts: Debugging is much easier because race conditions and deadlocks are avoided through single threaded execution. These advantages have led to numerous implementations of data parallel languages, such as C* [27, 30], Dataparallel C [16], Fortran D [20], and High Performance Fortran <ref> [1] </ref>. Unfortunately, the usefulness of data parallel programming for non-scientific applications is limited in many ways by its basic assumptions. <p> Data parallelism as a separate model of parallel programming originated with CmLisp [18] and C*[27, 30], the first data parallel languages from Thinking Machines. The success of C* led to the design and implementation of a large number of other data parallel languages, including High Performance Fortran <ref> [1] </ref>, Fortran-D [20], DINO [28], and Dataparallel C [16]. As data parallel programming came into wider use, a number of researchers started to investigate ways of applying the data parallel model to a broader class of architectures. <p> To do this, we took the basic assumptions of the data parallel programming model, and either weakened or generalized them in some way. We have produced a new model, which should allow casual applications programmers to write parallel programs on available hardware while Selection: <ref> [1, 2, 3, 4, 5] </ref>selectfx : x mod 2 6= 0g = [1, 3, 5] Mapping: [1, 2, 3, 4] + [5, 6, 7, 8] = [6, 8, 10, 12] Reduction: reduce + [1, 2, 3, 4] = 10 still retaining many of the advantages of data parallelism. <p> We have produced a new model, which should allow casual applications programmers to write parallel programs on available hardware while Selection: [1, 2, 3, 4, 5]selectfx : x mod 2 6= 0g = <ref> [1, 3, 5] </ref> Mapping: [1, 2, 3, 4] + [5, 6, 7, 8] = [6, 8, 10, 12] Reduction: reduce + [1, 2, 3, 4] = 10 still retaining many of the advantages of data parallelism. <p> We have produced a new model, which should allow casual applications programmers to write parallel programs on available hardware while Selection: [1, 2, 3, 4, 5]selectfx : x mod 2 6= 0g = [1, 3, 5] Mapping: <ref> [1, 2, 3, 4] </ref> + [5, 6, 7, 8] = [6, 8, 10, 12] Reduction: reduce + [1, 2, 3, 4] = 10 still retaining many of the advantages of data parallelism. <p> new model, which should allow casual applications programmers to write parallel programs on available hardware while Selection: [1, 2, 3, 4, 5]selectfx : x mod 2 6= 0g = [1, 3, 5] Mapping: <ref> [1, 2, 3, 4] </ref> + [5, 6, 7, 8] = [6, 8, 10, 12] Reduction: reduce + [1, 2, 3, 4] = 10 still retaining many of the advantages of data parallelism.
Reference: [2] <author> G. Agha and C. Houck. HAL: </author> <title> A high level actor language and its distributed implementation. </title> <booktitle> In Proceedings 21st International Conference on Parallel Processing, </booktitle> <pages> pages 158--165, </pages> <year> 1992. </year>
Reference-contexts: A group at the University of Illinois, let by Gul Agha, has designed a family of actor based languages including Charm [15] and HAL <ref> [2] </ref>, which allow actors programs to be run on workstation networks. These systems allow programmers to implement arbitrary parallel data and control structures, but like data parallel, require architectural support to work effectively. <p> To do this, we took the basic assumptions of the data parallel programming model, and either weakened or generalized them in some way. We have produced a new model, which should allow casual applications programmers to write parallel programs on available hardware while Selection: <ref> [1, 2, 3, 4, 5] </ref>selectfx : x mod 2 6= 0g = [1, 3, 5] Mapping: [1, 2, 3, 4] + [5, 6, 7, 8] = [6, 8, 10, 12] Reduction: reduce + [1, 2, 3, 4] = 10 still retaining many of the advantages of data parallelism. <p> We have produced a new model, which should allow casual applications programmers to write parallel programs on available hardware while Selection: [1, 2, 3, 4, 5]selectfx : x mod 2 6= 0g = [1, 3, 5] Mapping: <ref> [1, 2, 3, 4] </ref> + [5, 6, 7, 8] = [6, 8, 10, 12] Reduction: reduce + [1, 2, 3, 4] = 10 still retaining many of the advantages of data parallelism. <p> new model, which should allow casual applications programmers to write parallel programs on available hardware while Selection: [1, 2, 3, 4, 5]selectfx : x mod 2 6= 0g = [1, 3, 5] Mapping: <ref> [1, 2, 3, 4] </ref> + [5, 6, 7, 8] = [6, 8, 10, 12] Reduction: reduce + [1, 2, 3, 4] = 10 still retaining many of the advantages of data parallelism.
Reference: [3] <author> Gul Agha. </author> <title> Actors: a model of concurrent computation in distributed systems. </title> <publisher> MIT Press, </publisher> <address> Cambridge, Mass, </address> <year> 1986. </year>
Reference-contexts: The NUMA features of dpSather, when used properly, allow the compiler to generate better code than a pure shared memory language like C**. 2.2 Actors The Actors model <ref> [17, 3] </ref> is a model of parallel programming based on highly parallel systems of cooperating agents, called actors, which communicate through asynchronous message passing. <p> To do this, we took the basic assumptions of the data parallel programming model, and either weakened or generalized them in some way. We have produced a new model, which should allow casual applications programmers to write parallel programs on available hardware while Selection: <ref> [1, 2, 3, 4, 5] </ref>selectfx : x mod 2 6= 0g = [1, 3, 5] Mapping: [1, 2, 3, 4] + [5, 6, 7, 8] = [6, 8, 10, 12] Reduction: reduce + [1, 2, 3, 4] = 10 still retaining many of the advantages of data parallelism. <p> We have produced a new model, which should allow casual applications programmers to write parallel programs on available hardware while Selection: [1, 2, 3, 4, 5]selectfx : x mod 2 6= 0g = <ref> [1, 3, 5] </ref> Mapping: [1, 2, 3, 4] + [5, 6, 7, 8] = [6, 8, 10, 12] Reduction: reduce + [1, 2, 3, 4] = 10 still retaining many of the advantages of data parallelism. <p> We have produced a new model, which should allow casual applications programmers to write parallel programs on available hardware while Selection: [1, 2, 3, 4, 5]selectfx : x mod 2 6= 0g = [1, 3, 5] Mapping: <ref> [1, 2, 3, 4] </ref> + [5, 6, 7, 8] = [6, 8, 10, 12] Reduction: reduce + [1, 2, 3, 4] = 10 still retaining many of the advantages of data parallelism. <p> new model, which should allow casual applications programmers to write parallel programs on available hardware while Selection: [1, 2, 3, 4, 5]selectfx : x mod 2 6= 0g = [1, 3, 5] Mapping: <ref> [1, 2, 3, 4] </ref> + [5, 6, 7, 8] = [6, 8, 10, 12] Reduction: reduce + [1, 2, 3, 4] = 10 still retaining many of the advantages of data parallelism.
Reference: [4] <author> ANSI Committee X3J3. </author> <title> ANSI Programming Language Fortran-90 Draft. </title> <institution> American National Standards Institute. </institution>
Reference-contexts: In this section, we discuss related work in each of these areas. 2.1 Data Parallelism The data parallel programming model was inspired by sequence oriented operations in languages such as SETL [12], APL [26], and Fortran-90 <ref> [4] </ref>. Data parallelism as a separate model of parallel programming originated with CmLisp [18] and C*[27, 30], the first data parallel languages from Thinking Machines. <p> To do this, we took the basic assumptions of the data parallel programming model, and either weakened or generalized them in some way. We have produced a new model, which should allow casual applications programmers to write parallel programs on available hardware while Selection: <ref> [1, 2, 3, 4, 5] </ref>selectfx : x mod 2 6= 0g = [1, 3, 5] Mapping: [1, 2, 3, 4] + [5, 6, 7, 8] = [6, 8, 10, 12] Reduction: reduce + [1, 2, 3, 4] = 10 still retaining many of the advantages of data parallelism. <p> We have produced a new model, which should allow casual applications programmers to write parallel programs on available hardware while Selection: [1, 2, 3, 4, 5]selectfx : x mod 2 6= 0g = [1, 3, 5] Mapping: <ref> [1, 2, 3, 4] </ref> + [5, 6, 7, 8] = [6, 8, 10, 12] Reduction: reduce + [1, 2, 3, 4] = 10 still retaining many of the advantages of data parallelism. <p> new model, which should allow casual applications programmers to write parallel programs on available hardware while Selection: [1, 2, 3, 4, 5]selectfx : x mod 2 6= 0g = [1, 3, 5] Mapping: <ref> [1, 2, 3, 4] </ref> + [5, 6, 7, 8] = [6, 8, 10, 12] Reduction: reduce + [1, 2, 3, 4] = 10 still retaining many of the advantages of data parallelism.
Reference: [5] <author> Joshua Auerbach, Arthur Goldberg, German Gold-szmidt, Ajei Gopal, Mark Kennedy, and James Rus-sel. </author> <title> Concert/C Manual: A Programmers Guide to a Language for Distributed C Programming. </title> <institution> IBM T.J. Watson Research Center, </institution> <year> 1993. </year>
Reference-contexts: To do this, we took the basic assumptions of the data parallel programming model, and either weakened or generalized them in some way. We have produced a new model, which should allow casual applications programmers to write parallel programs on available hardware while Selection: <ref> [1, 2, 3, 4, 5] </ref>selectfx : x mod 2 6= 0g = [1, 3, 5] Mapping: [1, 2, 3, 4] + [5, 6, 7, 8] = [6, 8, 10, 12] Reduction: reduce + [1, 2, 3, 4] = 10 still retaining many of the advantages of data parallelism. <p> We have produced a new model, which should allow casual applications programmers to write parallel programs on available hardware while Selection: [1, 2, 3, 4, 5]selectfx : x mod 2 6= 0g = <ref> [1, 3, 5] </ref> Mapping: [1, 2, 3, 4] + [5, 6, 7, 8] = [6, 8, 10, 12] Reduction: reduce + [1, 2, 3, 4] = 10 still retaining many of the advantages of data parallelism. <p> We have produced a new model, which should allow casual applications programmers to write parallel programs on available hardware while Selection: [1, 2, 3, 4, 5]selectfx : x mod 2 6= 0g = [1, 3, 5] Mapping: [1, 2, 3, 4] + <ref> [5, 6, 7, 8] </ref> = [6, 8, 10, 12] Reduction: reduce + [1, 2, 3, 4] = 10 still retaining many of the advantages of data parallelism. <p> Languages like C++, that include no run time type information need to be augmented with annotations similar to those in Concert/C <ref> [5] </ref>, to make this information available at run time. Global variables Since our model is intended for distributed memory architectures, and tries to make all message passing explicit in the form of composite operations, global variables should be illegal in composite tree operations.
Reference: [6] <author> Geoff Barrett. </author> <title> Occam 3 Reference Manual. </title> <type> INMOS, </type> <year> 1992. </year>
Reference-contexts: These systems include complete programming languages such as LCS [7] and Occam <ref> [6] </ref>, and libraries such as PVM [14] and P4 [8]. Synchronous message passing systems are the simplest form of parallel programming for workstations. <p> We have produced a new model, which should allow casual applications programmers to write parallel programs on available hardware while Selection: [1, 2, 3, 4, 5]selectfx : x mod 2 6= 0g = [1, 3, 5] Mapping: [1, 2, 3, 4] + <ref> [5, 6, 7, 8] </ref> = [6, 8, 10, 12] Reduction: reduce + [1, 2, 3, 4] = 10 still retaining many of the advantages of data parallelism. <p> We have produced a new model, which should allow casual applications programmers to write parallel programs on available hardware while Selection: [1, 2, 3, 4, 5]selectfx : x mod 2 6= 0g = [1, 3, 5] Mapping: [1, 2, 3, 4] + [5, 6, 7, 8] = <ref> [6, 8, 10, 12] </ref> Reduction: reduce + [1, 2, 3, 4] = 10 still retaining many of the advantages of data parallelism.
Reference: [7] <author> Bernard Berthomieu. </author> <title> LCS users manual. </title> <type> Technical Report 91226, </type> <institution> LAAS, Toulouse, France, </institution> <month> June </month> <year> 1991. </year>
Reference-contexts: These systems include complete programming languages such as LCS <ref> [7] </ref> and Occam [6], and libraries such as PVM [14] and P4 [8]. Synchronous message passing systems are the simplest form of parallel programming for workstations. <p> We have produced a new model, which should allow casual applications programmers to write parallel programs on available hardware while Selection: [1, 2, 3, 4, 5]selectfx : x mod 2 6= 0g = [1, 3, 5] Mapping: [1, 2, 3, 4] + <ref> [5, 6, 7, 8] </ref> = [6, 8, 10, 12] Reduction: reduce + [1, 2, 3, 4] = 10 still retaining many of the advantages of data parallelism.
Reference: [8] <author> Ralph Butler and Ewing Lusk. </author> <title> User's Guide to the P4 parallel programming system. </title> <institution> Argonne National Laboratory, </institution> <year> 1992. </year>
Reference-contexts: These systems include complete programming languages such as LCS [7] and Occam [6], and libraries such as PVM [14] and P4 <ref> [8] </ref>. Synchronous message passing systems are the simplest form of parallel programming for workstations. When implemented as libraries, they require no changes to the sequential languages that use them, and when implemented as languages, they require very few constructs to support parallel programming. <p> We have produced a new model, which should allow casual applications programmers to write parallel programs on available hardware while Selection: [1, 2, 3, 4, 5]selectfx : x mod 2 6= 0g = [1, 3, 5] Mapping: [1, 2, 3, 4] + <ref> [5, 6, 7, 8] </ref> = [6, 8, 10, 12] Reduction: reduce + [1, 2, 3, 4] = 10 still retaining many of the advantages of data parallelism. <p> We have produced a new model, which should allow casual applications programmers to write parallel programs on available hardware while Selection: [1, 2, 3, 4, 5]selectfx : x mod 2 6= 0g = [1, 3, 5] Mapping: [1, 2, 3, 4] + [5, 6, 7, 8] = <ref> [6, 8, 10, 12] </ref> Reduction: reduce + [1, 2, 3, 4] = 10 still retaining many of the advantages of data parallelism.
Reference: [9] <author> K. Mani Chandy and Carl Kesselman. </author> <title> The CC++ language definition. </title> <type> Technical Report CS-TR-92-02, </type> <institution> California Insitute of Technology, </institution> <year> 1992. </year>
Reference-contexts: They cannot represent hierarchical structures or embedded parallelism. These assumptions essentially preclude the use of data parallelism for non-scientific applications. Many researchers have attempted to design languages for more general parallel structures or coarser grained architectures by relaxing these assumptions while preserving the basic concepts of data parallel programming <ref> [22, 29, 28, 9, 16] </ref> or by providing constructs which will allow programmers to implement data parallel programs, while not explicitly supporting data parallelism [10, 24, 13] .
Reference: [10] <author> Andrew Chien. </author> <title> Concurrent Aggregates: Supporting Modularity in Massively Parallel Programs. </title> <publisher> MIT Press, </publisher> <address> Cambridge, Mass, </address> <year> 1993. </year>
Reference-contexts: attempted to design languages for more general parallel structures or coarser grained architectures by relaxing these assumptions while preserving the basic concepts of data parallel programming [22, 29, 28, 9, 16] or by providing constructs which will allow programmers to implement data parallel programs, while not explicitly supporting data parallelism <ref> [10, 24, 13] </ref> . For example, a more loosely synchronous data parallelism can be achieved through the SPMD [21] approach in which the data is distributed over the memories of the individual processors and each processor executes the same program. <p> Due to the high degree of communication in Actor systems, these systems do not perform well on workstation networks. Chien at MIT has extended the Actors model with a construct called an aggregate, to design a language called Concurrent Aggregates <ref> [10] </ref>. An aggregate consists of a collection of objects, called representatives, which work cooperatively to perform a computation. The representatives of an aggregate can cooperate in a variety of ways, including in a synchronous data parallel style. <p> We have produced a new model, which should allow casual applications programmers to write parallel programs on available hardware while Selection: [1, 2, 3, 4, 5]selectfx : x mod 2 6= 0g = [1, 3, 5] Mapping: [1, 2, 3, 4] + [5, 6, 7, 8] = <ref> [6, 8, 10, 12] </ref> Reduction: reduce + [1, 2, 3, 4] = 10 still retaining many of the advantages of data parallelism.
Reference: [11] <author> Margaret Ellis and Bjarne Stroustrup. </author> <title> The Annotated C++ Reference Manual. </title> <publisher> Addison-Wesley, </publisher> <year> 1990. </year>
Reference-contexts: We permit this through the use of rendezvous communication statements to synchronize and exchange data between nodes of a composite tree. The resulting model is composite tree parallelism. Composite tree parallelism augments a strongly typed object oriented language (like C++ <ref> [11] </ref>, Eiffel [23], or Sather [25]), by adding a new object type called a composite. A composite type describes a parallel tree structure where data parallel operations can be performed on the children of any node in the structure.
Reference: [12] <author> J. T. Schwartz et al. </author> <title> Programming with sets: an introduction to SETL. </title> <publisher> Springer Verlag, </publisher> <year> 1986. </year>
Reference-contexts: In this section, we discuss related work in each of these areas. 2.1 Data Parallelism The data parallel programming model was inspired by sequence oriented operations in languages such as SETL <ref> [12] </ref>, APL [26], and Fortran-90 [4]. Data parallelism as a separate model of parallel programming originated with CmLisp [18] and C*[27, 30], the first data parallel languages from Thinking Machines. <p> We have produced a new model, which should allow casual applications programmers to write parallel programs on available hardware while Selection: [1, 2, 3, 4, 5]selectfx : x mod 2 6= 0g = [1, 3, 5] Mapping: [1, 2, 3, 4] + [5, 6, 7, 8] = <ref> [6, 8, 10, 12] </ref> Reduction: reduce + [1, 2, 3, 4] = 10 still retaining many of the advantages of data parallelism.
Reference: [13] <author> Dennis Gannon and Jenq Kuen Lee. </author> <title> Object oriented parallel programming: Experiments and results. </title> <booktitle> In Proceedings of Supercomputing '91, pages 273--282. IEEE, </booktitle> <year> 1991. </year>
Reference-contexts: attempted to design languages for more general parallel structures or coarser grained architectures by relaxing these assumptions while preserving the basic concepts of data parallel programming [22, 29, 28, 9, 16] or by providing constructs which will allow programmers to implement data parallel programs, while not explicitly supporting data parallelism <ref> [10, 24, 13] </ref> . For example, a more loosely synchronous data parallelism can be achieved through the SPMD [21] approach in which the data is distributed over the memories of the individual processors and each processor executes the same program. <p> Concurrent aggregate pro grams are difficult to write because the language provides no explicit support for any particular parallel programming technique, and they are difficult to compile because the language provides no explicit information about the parallel control structure or communication structure of the program. The pC++ project <ref> [13] </ref> from the University of Illinois is based on an extension to C++ for the concurrent aggregates model. This results in a language with most of the flexibility of Concurrent Aggregates, but makes programs far easier to understand and to compile.
Reference: [14] <author> Al Geist, Adam Beguelin, Jack Dongarra, Weicheng Jiang, Robert Manchek, and Vaidy Sunderam. </author> <title> PVM 3 Users Guide and Reference Manual. </title> <institution> Oak Ridge National Laboratory, </institution> <year> 1993. </year>
Reference-contexts: These systems include complete programming languages such as LCS [7] and Occam [6], and libraries such as PVM <ref> [14] </ref> and P4 [8]. Synchronous message passing systems are the simplest form of parallel programming for workstations. When implemented as libraries, they require no changes to the sequential languages that use them, and when implemented as languages, they require very few constructs to support parallel programming.
Reference: [15] <author> Attila Gursoy, Amitabh Sinha, and Laxmikant Kale. </author> <title> The CHARM(3.2) Programming Language Manual. </title> <institution> University of Illinois, </institution> <month> December </month> <year> 1992. </year>
Reference-contexts: A group at the University of Illinois, let by Gul Agha, has designed a family of actor based languages including Charm <ref> [15] </ref> and HAL [2], which allow actors programs to be run on workstation networks. These systems allow programmers to implement arbitrary parallel data and control structures, but like data parallel, require architectural support to work effectively.
Reference: [16] <author> Philip Hatcher and Michael Quinn. </author> <title> Data-parallel programming on MIMD computers. </title> <publisher> MIT Press, </publisher> <year> 1991. </year>
Reference-contexts: Debugging is much easier because race conditions and deadlocks are avoided through single threaded execution. These advantages have led to numerous implementations of data parallel languages, such as C* [27, 30], Dataparallel C <ref> [16] </ref>, Fortran D [20], and High Performance Fortran [1]. Unfortunately, the usefulness of data parallel programming for non-scientific applications is limited in many ways by its basic assumptions. <p> They cannot represent hierarchical structures or embedded parallelism. These assumptions essentially preclude the use of data parallelism for non-scientific applications. Many researchers have attempted to design languages for more general parallel structures or coarser grained architectures by relaxing these assumptions while preserving the basic concepts of data parallel programming <ref> [22, 29, 28, 9, 16] </ref> or by providing constructs which will allow programmers to implement data parallel programs, while not explicitly supporting data parallelism [10, 24, 13] . <p> The success of C* led to the design and implementation of a large number of other data parallel languages, including High Performance Fortran [1], Fortran-D [20], DINO [28], and Dataparallel C <ref> [16] </ref>. As data parallel programming came into wider use, a number of researchers started to investigate ways of applying the data parallel model to a broader class of architectures.
Reference: [17] <author> C. E. Hewitt and H. Baker. </author> <title> Actors and continuous functionals. </title> <editor> In Erich J. Neuhold, editor, </editor> <booktitle> Proceedings IFIP Working Conference on Formal Description of Programming Concepts, pages 367--387. IFIP, </booktitle> <year> 1977. </year>
Reference-contexts: The NUMA features of dpSather, when used properly, allow the compiler to generate better code than a pure shared memory language like C**. 2.2 Actors The Actors model <ref> [17, 3] </ref> is a model of parallel programming based on highly parallel systems of cooperating agents, called actors, which communicate through asynchronous message passing.
Reference: [18] <author> W. Daniel Hillis. </author> <title> The Connection Machine. </title> <publisher> MIT Press, </publisher> <year> 1985. </year>
Reference-contexts: In this section, we discuss related work in each of these areas. 2.1 Data Parallelism The data parallel programming model was inspired by sequence oriented operations in languages such as SETL [12], APL [26], and Fortran-90 [4]. Data parallelism as a separate model of parallel programming originated with CmLisp <ref> [18] </ref> and C*[27, 30], the first data parallel languages from Thinking Machines. The success of C* led to the design and implementation of a large number of other data parallel languages, including High Performance Fortran [1], Fortran-D [20], DINO [28], and Dataparallel C [16].
Reference: [19] <author> W. Daniel Hillis and Guy L. Steele Jr. </author> <title> Data parallel algorithms. </title> <journal> Communications of the ACM, 29(12):1170 -1183, </journal> <year> 1986. </year>
Reference-contexts: 1 Introduction Data parallel programming achieves parallelism through the simultaneous execution of the same operation across a set of data <ref> [19] </ref>. In addition to being acknowledged as the favored paradigm for parallelizing array-based scientific codes and for exploiting massively parallel machines, data parallel programming offers many advantages from the parallel programmer's point of view.
Reference: [20] <author> Seema Hiranandani, Ken Kennedy, and Chau-Wen Tseng. </author> <title> An overview of the Fortran-D programming system. </title> <booktitle> In Proceedings of the Fourth Workshop on Languages and Compilers for Parallel Computing, </booktitle> <year> 1991. </year>
Reference-contexts: Debugging is much easier because race conditions and deadlocks are avoided through single threaded execution. These advantages have led to numerous implementations of data parallel languages, such as C* [27, 30], Dataparallel C [16], Fortran D <ref> [20] </ref>, and High Performance Fortran [1]. Unfortunately, the usefulness of data parallel programming for non-scientific applications is limited in many ways by its basic assumptions. <p> Data parallelism as a separate model of parallel programming originated with CmLisp [18] and C*[27, 30], the first data parallel languages from Thinking Machines. The success of C* led to the design and implementation of a large number of other data parallel languages, including High Performance Fortran [1], Fortran-D <ref> [20] </ref>, DINO [28], and Dataparallel C [16]. As data parallel programming came into wider use, a number of researchers started to investigate ways of applying the data parallel model to a broader class of architectures.
Reference: [21] <author> A. H. Karp and R. G. Babb. </author> <title> Programming for parallelism. </title> <booktitle> IEEE Computer, </booktitle> <pages> pages 43--57, </pages> <year> 1987. </year>
Reference-contexts: For example, a more loosely synchronous data parallelism can be achieved through the SPMD <ref> [21] </ref> approach in which the data is distributed over the memories of the individual processors and each processor executes the same program.
Reference: [22] <author> James Larus, Brad Richards, and Guhan Vi-wsanathan. </author> <title> C**: A large grain, object-oriented, data-parallel programming language. </title> <type> Technical Report 1125, </type> <institution> University of Wisconsin-Madison, </institution> <month> November </month> <year> 1992. </year>
Reference-contexts: They cannot represent hierarchical structures or embedded parallelism. These assumptions essentially preclude the use of data parallelism for non-scientific applications. Many researchers have attempted to design languages for more general parallel structures or coarser grained architectures by relaxing these assumptions while preserving the basic concepts of data parallel programming <ref> [22, 29, 28, 9, 16] </ref> or by providing constructs which will allow programmers to implement data parallel programs, while not explicitly supporting data parallelism [10, 24, 13] .
Reference: [23] <author> Bertrand Meyer. </author> <title> Object-oriented Software Construction. </title> <publisher> Prentice Hall, </publisher> <year> 1988. </year>
Reference-contexts: We permit this through the use of rendezvous communication statements to synchronize and exchange data between nodes of a composite tree. The resulting model is composite tree parallelism. Composite tree parallelism augments a strongly typed object oriented language (like C++ [11], Eiffel <ref> [23] </ref>, or Sather [25]), by adding a new object type called a composite. A composite type describes a parallel tree structure where data parallel operations can be performed on the children of any node in the structure. Composite programs combine message passing and control flow into a single framework. <p> Static typed languages like C++ and Eiffel are preferable to latent typed languages like Smalltalk for our purposes, and strong type systems like Eiffel's are preferable to weaker type systems like C++. On the basis of these criteria, we believe that pure, static typed object oriented languages like Eiffel <ref> [23] </ref> and 1 process CompositeNode is 2 loop 3 WaitForMessage 4 rep := target (message) 5 arguments := unpack (message) 6 case MessageTag of 7 when routine1 then 8 retval := rep.routine1 (arguments) 9 message := pack (retval) 10 reply (message) 11 when routine2 then 12 ... 13 endcase 14 endloop
Reference: [24] <author> Stephan Murer, Jerome Feldman, and Chu-Cheow Lim. pSather: </author> <title> Layered extensions to an object-oriented language for efficient parallel computation. </title> <type> Technical Report TR-93-029, </type> <institution> ICSI/CSD, University of California at Berkeley, </institution> <year> 1993. </year>
Reference-contexts: attempted to design languages for more general parallel structures or coarser grained architectures by relaxing these assumptions while preserving the basic concepts of data parallel programming [22, 29, 28, 9, 16] or by providing constructs which will allow programmers to implement data parallel programs, while not explicitly supporting data parallelism <ref> [10, 24, 13] </ref> . For example, a more loosely synchronous data parallelism can be achieved through the SPMD [21] approach in which the data is distributed over the memories of the individual processors and each processor executes the same program.
Reference: [25] <author> Stephen Omohundro. </author> <title> The Sather 1.0 Specification. </title> <booktitle> The International Computer Science Institute, </booktitle> <address> Berke-ley, CA, </address> <month> June </month> <year> 1993. </year>
Reference-contexts: Because of this shared memory assumption, C** is difficult to compile for distributed memory architectures. C** also does nothing to allow programmers to implement parallel data structures other than rectangular arrays. dpSather [29], a variation on the Sather language from ICSI <ref> [25] </ref>, adds array-based data parallelism to a static typed object oriented language. dpSather, like C**, assumes that parallel aggregates exist within a shared memory, but it includes a number of features for dealing with non-uniform memory access (NUMA) and data communication. <p> We permit this through the use of rendezvous communication statements to synchronize and exchange data between nodes of a composite tree. The resulting model is composite tree parallelism. Composite tree parallelism augments a strongly typed object oriented language (like C++ [11], Eiffel [23], or Sather <ref> [25] </ref>), by adding a new object type called a composite. A composite type describes a parallel tree structure where data parallel operations can be performed on the children of any node in the structure. Composite programs combine message passing and control flow into a single framework. <p> is 2 loop 3 WaitForMessage 4 rep := target (message) 5 arguments := unpack (message) 6 case MessageTag of 7 when routine1 then 8 retval := rep.routine1 (arguments) 9 message := pack (retval) 10 reply (message) 11 when routine2 then 12 ... 13 endcase 14 endloop 15 end process Sather <ref> [25] </ref> are ideal for implementing composite based extensions. 5.2 Basic Code Generation A composite compiler is a front end preprocessor which takes programs written in an object oriented language extended by composites, and translates this into an SPMD program written in the base language, with calls to a parallel message passing
Reference: [26] <author> S. Pommier. </author> <title> An Introduction to APL. </title> <publisher> Cambridge University Press, </publisher> <year> 1983. </year>
Reference-contexts: In this section, we discuss related work in each of these areas. 2.1 Data Parallelism The data parallel programming model was inspired by sequence oriented operations in languages such as SETL [12], APL <ref> [26] </ref>, and Fortran-90 [4]. Data parallelism as a separate model of parallel programming originated with CmLisp [18] and C*[27, 30], the first data parallel languages from Thinking Machines.
Reference: [27] <author> J. R. Rose and Guy L. Steele Jr. </author> <title> C*: An extended C language for data parallel programming. </title> <type> Technical Report PL 87-5, </type> <institution> Thinking Machines Corporation, </institution> <year> 1987. </year>
Reference-contexts: Debugging is much easier because race conditions and deadlocks are avoided through single threaded execution. These advantages have led to numerous implementations of data parallel languages, such as C* <ref> [27, 30] </ref>, Dataparallel C [16], Fortran D [20], and High Performance Fortran [1]. Unfortunately, the usefulness of data parallel programming for non-scientific applications is limited in many ways by its basic assumptions.
Reference: [28] <author> Matthew Rosing, Robert Schnabel, and Robert Weaver. </author> <title> The DINO parallel programming language. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 13(1):30--42, </volume> <month> Sept </month> <year> 1991. </year>
Reference-contexts: They cannot represent hierarchical structures or embedded parallelism. These assumptions essentially preclude the use of data parallelism for non-scientific applications. Many researchers have attempted to design languages for more general parallel structures or coarser grained architectures by relaxing these assumptions while preserving the basic concepts of data parallel programming <ref> [22, 29, 28, 9, 16] </ref> or by providing constructs which will allow programmers to implement data parallel programs, while not explicitly supporting data parallelism [10, 24, 13] . <p> The success of C* led to the design and implementation of a large number of other data parallel languages, including High Performance Fortran [1], Fortran-D [20], DINO <ref> [28] </ref>, and Dataparallel C [16]. As data parallel programming came into wider use, a number of researchers started to investigate ways of applying the data parallel model to a broader class of architectures.
Reference: [29] <author> Heinz Schmidt. </author> <title> Data-parallel object-oriented programming. </title> <booktitle> In Proceedings Fifth Australian Supercomputing Conference. </booktitle> <institution> CSIRO, Division of Information Technology, </institution> <year> 1992. </year>
Reference-contexts: They cannot represent hierarchical structures or embedded parallelism. These assumptions essentially preclude the use of data parallelism for non-scientific applications. Many researchers have attempted to design languages for more general parallel structures or coarser grained architectures by relaxing these assumptions while preserving the basic concepts of data parallel programming <ref> [22, 29, 28, 9, 16] </ref> or by providing constructs which will allow programmers to implement data parallel programs, while not explicitly supporting data parallelism [10, 24, 13] . <p> Because of this shared memory assumption, C** is difficult to compile for distributed memory architectures. C** also does nothing to allow programmers to implement parallel data structures other than rectangular arrays. dpSather <ref> [29] </ref>, a variation on the Sather language from ICSI [25], adds array-based data parallelism to a static typed object oriented language. dpSather, like C**, assumes that parallel aggregates exist within a shared memory, but it includes a number of features for dealing with non-uniform memory access (NUMA) and data communication.
Reference: [30] <institution> Thinking Machines Corporation. </institution> <note> C* Reference Manual, </note> <year> 1990. </year>
Reference-contexts: Debugging is much easier because race conditions and deadlocks are avoided through single threaded execution. These advantages have led to numerous implementations of data parallel languages, such as C* <ref> [27, 30] </ref>, Dataparallel C [16], Fortran D [20], and High Performance Fortran [1]. Unfortunately, the usefulness of data parallel programming for non-scientific applications is limited in many ways by its basic assumptions.
References-found: 30

