URL: http://www.ai.mit.edu/people/cgdemarc/postscript/thesis.ps
Refering-URL: http://www.ai.mit.edu/people/cgdemarc/cgdemarc.html
Root-URL: 
Title: Unsupervised Language Acquisition  
Author: by Carl G. de Marcken Robert C. Berwick 
Degree: S.B. Electrical Engineering and Computer Science (1990) Massachussetts Institute of Technology S.B. Mathematics (1991) Massachussetts Institute of Technology S.M. Electrical Engineering and Computer Science (1993) Massachussetts Institute of Technology Submitted to the Department of Electrical Engineering and Computer Science in partial fulfillment of the requirements for the degree of Doctor of Philosophy at the  c fl1996 Massachusetts Intstitute of Technology. All Rights Reserved. Signature of Author  Certified by  Professor of Computer Science and Engineering Thesis Supervisor Accepted by F. R. Morgenthaler Chair, Department Committee on Graduate Students  
Date: September, 1996  September 6, 1996  
Affiliation: Massachusetts Institute of Technology  Department of Electrical Engineering and Computer Science  
Abstract-found: 0
Intro-found: 1
Reference: [1] <author> J. R. Anderson. </author> <title> Induction of augmented transition networks. </title> <journal> Cognitive Science, </journal> <volume> 1 </volume> <pages> 125-47, </pages> <year> 1977. </year>
Reference-contexts: These requirements are far beyond the capabilities of any known mechanisms. Finally, all of these theories assume that other modules can function without feedback and can be learned independently. 2.2. THEORIES OF LANGUAGE ACQUISITION 23 Paper Assumes Input Output Anderson 1977 <ref> [1] </ref> I,NN,NH MI,WM,SM G,WS Anderson 1981 [2] I,NN,NH MI,WM,SM G,WS,MPHR Berwick 1985 [16] I,NN,OCWS WI,WM,TR G,WS Brent 1993 [22] WW MPH,MPHR Gibson & Wexler 1994 [61] NN P,TR G Kazman 1994 [75] I,NH WI,WM,WS G,MPH,MPHR Rayner et al. 1988 [109] I,FG,NN,NH WI WS Selfridge 1981 [121] I,NN,NH WI,SM WM Siklossy 1972
Reference: [2] <author> J. R. Anderson. </author> <title> A theory of language acquisition based on general learning principles. </title> <booktitle> In Proc. of the 7th International Joint Conference on Artificial Intelligence, </booktitle> <pages> pages 97-103, </pages> <address> Vancouver, B.C., Canada, </address> <year> 1981. </year>
Reference-contexts: These requirements are far beyond the capabilities of any known mechanisms. Finally, all of these theories assume that other modules can function without feedback and can be learned independently. 2.2. THEORIES OF LANGUAGE ACQUISITION 23 Paper Assumes Input Output Anderson 1977 [1] I,NN,NH MI,WM,SM G,WS Anderson 1981 <ref> [2] </ref> I,NN,NH MI,WM,SM G,WS,MPHR Berwick 1985 [16] I,NN,OCWS WI,WM,TR G,WS Brent 1993 [22] WW MPH,MPHR Gibson & Wexler 1994 [61] NN P,TR G Kazman 1994 [75] I,NH WI,WM,WS G,MPH,MPHR Rayner et al. 1988 [109] I,FG,NN,NH WI WS Selfridge 1981 [121] I,NN,NH WI,SM WM Siklossy 1972 [125] I,NN,NH WI,SM WM Siskind 1992
Reference: [3] <author> S. R. Anderson. </author> <title> Morphology as a parsing problem. </title> <journal> Linguistics, </journal> <volume> 26 </volume> <pages> 521-544, </pages> <year> 1988. </year>
Reference-contexts: There are many additional ways that information about the derivational process is lost. For example, many phonological processes destroy or hide information about underlying memorized forms <ref> [3] </ref>. For these reasons and many more, the raw speech signal offers few direct insights into the parameter settings of the process that generated it. <p> During language production the underlying forms of words are transformed by various corrupting and distorting morphological, phonological and phonetic processes. Standard computational models of recognition attempt to invert these processes <ref> [3, 81, 79] </ref> during recognition; since the forward processes are many-to-one, inversion seems to require (expensive) search. And yet speech recognition is quick and easy for people. The representational framework offers a partial explanation. Many of the corrupting processes are non-deterministic, but not entirely random.
Reference: [4] <author> D. Angluin. </author> <title> On the complexity of minimum inference of regular sets. </title> <journal> Information and Control, </journal> <volume> 39 </volume> <pages> 337-350, </pages> <year> 1978. </year>
Reference-contexts: Because this problem taken at face value is trivial (merely encode the positive examples directly into the model), various optimization criteria have been imposed; for example, Angluin <ref> [4] </ref> and Gold [64] show that identification of the minimum-size automaton consistent with a finite set of examples is NP-complete. This literature has not generally considered linguistic applications (though see Berwick and Pilato [17], who use Angluin's [6] notion of k-reversibility to acquire automata for the English auxiliary system).
Reference: [5] <author> D. Angluin. </author> <title> Inductive inference of formal languages from positive data. </title> <journal> Information and Control, </journal> <volume> 45(2) </volume> <pages> 117-135, </pages> <month> May </month> <year> 1980. </year>
Reference-contexts: It is possible to place strong bounds on what classes of languages are identifiable in the limit from positive examples alone <ref> [5] </ref>, even assuming a preference ordering on languages like Chomsky's m function. Gold proved that many linguistically relevant classes of languages, such as the regular and context-free languages, are identifiable from both positive and negative examples but not from positive examples alone. <p> In learning frameworks that do not allow for such graded judgments of "grammaticality", heuristics (such as the Subset Principle <ref> [5, 16] </ref>) must be introduced to favor Grammar 1 over Grammar 2. Generative grammars with probabilistic interpretations (in other words, grammars that implicitly or explicitly define p (U jG)) are commonly called stochastic language models. The discriminatory power of stochastic language models comes at a steep price.
Reference: [6] <author> D. Angluin. </author> <title> Inference of reversible langauges. </title> <journal> Journal of the Association for Computing Machinery, </journal> <volume> 29 </volume> <pages> 741-765, </pages> <year> 1982. </year>
Reference-contexts: This literature has not generally considered linguistic applications (though see Berwick and Pilato [17], who use Angluin's <ref> [6] </ref> notion of k-reversibility to acquire automata for the English auxiliary system). Since the classes of automata that are generally used allow for arbitrary states and arbitrary transitions, it is often difficult to imagine how these automata could be given linguistic interpretations.
Reference: [7] <author> H. Baayen and R. Sproat. </author> <title> Estimating lexical priors for low-frequency syncretic forms. </title> <journal> Computational Linguistics, </journal> <volume> 22(2) </volume> <pages> 155-166, </pages> <year> 1996. </year>
Reference-contexts: If these sound changes are not compiled into the lexicon but handled on-line by the processing mechanism, then they should be predicted with frequency proportional to the frequency of the words in which they occur. But Baayen and Sproat <ref> [7] </ref> have determined that the best indicator of the frequency of such phenomena in new or unknown words is the frequency of the phenomenon in the lexicon, unweighted by word frequency. This is exactly as would be expected if the changes are compiled into the lexicon.
Reference: [8] <author> J. K. Baker. </author> <title> Trainable grammars for speech recognition. </title> <booktitle> In Proceedings of the 97th Meeting of the Acoustical Society of America, </booktitle> <pages> pages 547-550, </pages> <year> 1979. </year>
Reference-contexts: This fact can be captured naturally in the Bayesian framework, if grammars are given a probabilistic interpretation. In particular, compare the following two stochastic context-free grammars (SCFGs <ref> [8, 70] </ref>), where the choice of nonterminal expansion is governed by probabilities: Grammar 1 S ) aBa (1) 2 ) 2 ) Grammar 2 S ) SS ( 1 ) a ( 1 ) b ( 1 The probability of the sentence aba under Grammar 1 is 1 2 . <p> This section briefly explains how this can be partially remedied by basing the composition operator on stochastic context-free grammars (SCFGs <ref> [8, 70] </ref>). Although we have performed successful experiments with the type of model described here, a number of fundamental deficiencies remain, and it is presented only as an illustration. <p> The reasons behind the failures harken back to the discussions of chapter 3: extralinguistic patterns are learned at the expense of linguistic ones and words are made long in an effort to improve stochastic models. Much recent work has focused on the induction of context-free grammars or variations thereof <ref> [8, 25, 31, 30, 35, 36, 43, 83, 104, 137] </ref>. The hierarchical nature of these grammars would seem on the surface to be quite similar to our hierarchical, concatenative representation. However, algorithms designed for the induction of context-free grammars have not performed well in practice. <p> However, algorithms designed for the induction of context-free grammars have not performed well in practice. Pereira and Schabes [104] attempt to learn an English grammar by applying the inside-outside algorithm <ref> [8] </ref> (the EM-algorithm for stochastic context-free grammars) to a grammar that contains all possible binary rules over a fixed set of nonterminals and terminal parts-of-speech. Although the end grammars model the input moderately well from a predictive viewpoint, the derivations assigned to sentences do not agree with human judgments.
Reference: [9] <author> G. E. Barton, R. C. Berwick, and E. S. Ristad. </author> <title> Computational Complexity and Natural Language. </title> <publisher> MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1987. </year>
Reference-contexts: In a similar vein, Ristad, Barton and Berwick <ref> [9, 115] </ref> have argued that many theories can be dismissed on the basis of their computational complexity. Such deficiencies usually become apparent immediately upon implementation. * In the course of applying algorithms and representations to real input, the most significant "problems" of language learning are identified.
Reference: [10] <author> E. B. Baum and D. Haussler. </author> <title> What size net gives valid generalization? Neural Computation, </title> <booktitle> 1(1) </booktitle> <pages> 151-160, </pages> <year> 1989. </year>
Reference-contexts: that it is merely a heuristic, but point out three important ways in which it mimics the philosophy of the better-justified structural risk minimization: * In very many cases the VC-dimension of a parameterized class of functions is linear or near-linear in the number of free parameters in the class <ref> [10, 76, 142] </ref>. Given an efficient coding scheme, the length of a description of a set of (independent) parameters is linear in the number of parameters.
Reference: [11] <author> L. E. Baum. </author> <title> An inequality and associated maximization technique in statistical estimation for probabilistic functions of Markov processes. </title> <journal> Inequalities, </journal> <volume> 3 </volume> <pages> 1-8, </pages> <year> 1972. </year>
Reference-contexts: Stochastic optimization via EM is accomplished by the Baum-Welch procedure <ref> [11] </ref>, and fairly simple estimation procedures are used to predict the effects of adding and deleting parameters. <p> Other advantages will be discussed in section 5.4. 5.2.1 Optimization of Stochastic Parameters As discussed in section 4.4.1, multigrams are a special form of hidden Markov models. The EM procedure for HMMs is known as the Baum-Welch algorithm <ref> [11] </ref>, and is rederived below in a simpler form more appropriate for multigrams. As with all EM procedures, the Baum-Welch procedure alternates E-steps and M-steps. This procedure rapidly converges. <p> This would appear from equation 5.3 to involve a sum over all possible representations for each utterance. Since the number of representations can be exponential in the length of an utterance, this might appear intractable. However, a dynamic programming 3 technique known as the forward-backward algorithm <ref> [11] </ref> enables this computation to be performed efficiently. Here the forward-backward algorithm is presented in a somewhat simplified form appropriate for the multigram model. The algorithm consists of two steps for each utterance. First, forward and backward probabilities are computed for each location (character index) in the utterance. <p> The classical solution to the problem of estimating the transition probabilities of a hidden Markov model is the Baum-Welch algorithm <ref> [11] </ref>. <p> 25.761 769.83 [work] [[wor]k] 107 10.292 15.832 764.56 [hewas] [[he][was]] 108 10.295 25.078 762.46 [after] [[aft][er]] 1000 13.043 23.794 113.57 [drive] [[dr][ive]] 1001 13.043 24.480 113.56 [didnt] [[did][nt]] 1002 13.045 27.501 113.39 [performance] [[perform][ance]] 1003 13.046 15.442 113.33 [afterthe] [[after][the]] 1004 13.047 23.689 113.26 [mission] [[miss][ion]] 1005 13.047 21.170 113.25 <ref> [11] </ref> [11] 1006 13.048 27.852 113.17 [project] [[pro][ject]] 1007 13.048 22.046 113.15 [lie] [l [ie]] 1008 13.049 16.026 113.06 [outofthe] [[outof][the]] 10000 16.063 27.062 13.99 [transmission] [[trans][mission]] 10001 16.063 27.063 13.99 [corruption] [[corrupt][ion]] 10002 16.063 29.858 13.99 [forthebenefitof] [[forthe][benefit][of]] 10003 16.063 19.948 13.99 [stillhad] [[still][had]] 10004 16.064 24.526 13.99 [tak] [tak] <p> 769.83 [work] [[wor]k] 107 10.292 15.832 764.56 [hewas] [[he][was]] 108 10.295 25.078 762.46 [after] [[aft][er]] 1000 13.043 23.794 113.57 [drive] [[dr][ive]] 1001 13.043 24.480 113.56 [didnt] [[did][nt]] 1002 13.045 27.501 113.39 [performance] [[perform][ance]] 1003 13.046 15.442 113.33 [afterthe] [[after][the]] 1004 13.047 23.689 113.26 [mission] [[miss][ion]] 1005 13.047 21.170 113.25 <ref> [11] </ref> [11] 1006 13.048 27.852 113.17 [project] [[pro][ject]] 1007 13.048 22.046 113.15 [lie] [l [ie]] 1008 13.049 16.026 113.06 [outofthe] [[outof][the]] 10000 16.063 27.062 13.99 [transmission] [[trans][mission]] 10001 16.063 27.063 13.99 [corruption] [[corrupt][ion]] 10002 16.063 29.858 13.99 [forthebenefitof] [[forthe][benefit][of]] 10003 16.063 19.948 13.99 [stillhad] [[still][had]] 10004 16.064 24.526 13.99 [tak] [tak] 10005
Reference: [12] <author> L. E. Baum, T. Petrie, G. Soules, and N. Weiss. </author> <title> A maximization technique occuring in the statistical analysis of probabilistic functions in Markov chains. </title> <journal> Annals of Mathematical Statistics, </journal> <volume> 41 </volume> <pages> 164-171, </pages> <year> 1970. </year>
Reference-contexts: Hence, the optimization procedure seems cyclic: the optimal stochastic model is a function of representations, which are in turn a function of the stochastic model. The expectation-maximization (EM) procedure of Dempster et al. <ref> [12, 45, 54] </ref> is a standard method for 5.1. GENERAL ARCHITECTURE 75 solving optimization problems involving hidden representations.
Reference: [13] <author> T. C. Bell, J. G. Cleary, and I. H. Witten. </author> <title> Text Compression. </title> <publisher> Prentice-Hall, </publisher> <address> Englewood Cliffs, New Jersey, </address> <year> 1990. </year>
Reference-contexts: This memory extends at most the length of the longest parameter. As a finite-state model, it has obvious similarities to other finite-state models such as Markov models (MMs) and hidden Markov models (HMMs) <ref> [13, 107] </ref>. Both Markov models and hidden Markov models define a stochastic model on top of a finite state machine, where the state q i of the system at time i is drawn from a finite set Q. <p> In Markov models the state q i is defined by recent outputs, q i = o im : : : o i1 , where m is the order of the Markov model (more general context models <ref> [13, 112] </ref> select the context o im : : : o i1 from among a set of variable-length suffixes). <p> Text compression techniques are in general more relevant to language acquisition than language modeling techniques, because little prior knowledge tends to be encoded into compression algorithms, and thus they usually incorporate structural induction mechanisms. Bell et al. <ref> [13] </ref> provide an excellent introduction to the problems and methods of compression, and in particular, text compression. <p> Popular text compression schemes can be divided into four classes: those based on adaptive frequency techniques like Huffman codes; those based on context models [112] (such as the PPM algorithm [42, 96, 139], probably the most effective widely-used method for text compression); those based on hidden Markov models <ref> [13] </ref> (these are less common); and those based on dictionary methods. Only the dictionary methods, exemplified by the LZ78 [156] and LZW [147] algorithms, have underlying models that can easily be assigned linguistic interpretations. Dictionary-based text compression techniques are variable-length block coding schemes, very similar to the multigram model. <p> the size of this city". the jury said it did find that many of georgia's registration and election laws "are outmod ed or inadequate and often ambiguous". 6.1.1 Input The concatenative algorithm of chapter 5 was run on two bodies of text, the Brown corpus [59] and the Calgary corpus <ref> [13] </ref>. The Brown corpus is a diverse million-word (approximately 40,000 sentence) corpus of English text, divided into 15 sections by document type and further into 500 documents of about 2000 words each. The text ranges from romance novels to political commentary to music reviews, and dates from 1961.
Reference: [14] <author> R. Bellman. </author> <title> Dynamic Programming. </title> <publisher> Princeton University Press, </publisher> <address> Princeton, New Jersey, </address> <year> 1957. </year> <note> 125 126 BIBLIOGRAPHY </note>
Reference-contexts: = p G (u) if w = u a+1 : : : u b . p G (a ! bju) 0 otherwise. 3 Here dynamic programming is used in the algorithmic sense, though the backward portion of the forward-backward algorithm is also a dynamic programming algorithm in the optimization sense <ref> [14] </ref>! 4 The fact that fi l (u) 1 follows from the simplification made in equation 4.3, that ignores in the stochastic model the number of parameters in a representation. The use of a special terminating parameter would eliminate the need for this approximation. 5.2.
Reference: [15] <author> A. L. Berger, S. A. Della Pietra, and V. J. Della Pietra. </author> <title> A maximum entropy approach to natural language processing. </title> <journal> Computational Linguistics, </journal> <volume> 22(1) </volume> <pages> 39-71, </pages> <year> 1996. </year>
Reference-contexts: when the learner's grammar is chosen to minimize the Kullback-Leibler distance D (p T k p L ) between the distributions p T and p L , where the Kullback-Leibler distance is defined by D (p T k p L ) = u p L (u) : It is possible <ref> [15, 53] </ref> (and indeed effective) to construct stochastic language models by defining p L to be the least-committal (maximum-entropy) distribution consistent with known properties of the target language distribution p T . <p> More sophisticated models might represent statistical properties differently, perhaps in a manner better suited for combining multiple pieces of information (see, for example, the maximum-entropy language modeling scheme described by Della Pietra et al. <ref> [15, 53] </ref>). The more parameters are in a lexicon, the more control points the stochastic model has, and the better it will be able to model the target language. Hence, any lexical parameter should reduce (or at least not increase) the description length of the input and existing parameters. <p> Language models that can combine multiple knowledge sources are described by Della Pietra et al. <ref> [15, 53] </ref>, but result in complex and computationally burdensome learning algorithms. 4.4.2 Composition by Substitution One of the most significant deficiencies with the concatenative model is that it can not capture relations that hold of broad classes of objects without multiplying the number of parameters in the grammar. <p> This work includes studies of language acquisition (see Selfridge [121], Siklossy [125] and Siskind [126, 127, 128]); parameter estimation schemes for machine translation, where sentences in a second language substitute for semantic input (see Brown et al. [26] and Berger et al. <ref> [15] </ref>); and parameter estimation schemes for systems that classify utterances (see Tishby and Gorin [140]). The learning algorithm presented in this chapter for the concatenative model extended with the meaning perturbation operator advances previous work in many ways.
Reference: [16] <author> R. C. Berwick. </author> <title> The Acquisition of Syntactic Knowledge. </title> <publisher> MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1985. </year>
Reference-contexts: Finally, all of these theories assume that other modules can function without feedback and can be learned independently. 2.2. THEORIES OF LANGUAGE ACQUISITION 23 Paper Assumes Input Output Anderson 1977 [1] I,NN,NH MI,WM,SM G,WS Anderson 1981 [2] I,NN,NH MI,WM,SM G,WS,MPHR Berwick 1985 <ref> [16] </ref> I,NN,OCWS WI,WM,TR G,WS Brent 1993 [22] WW MPH,MPHR Gibson & Wexler 1994 [61] NN P,TR G Kazman 1994 [75] I,NH WI,WM,WS G,MPH,MPHR Rayner et al. 1988 [109] I,FG,NN,NH WI WS Selfridge 1981 [121] I,NN,NH WI,SM WM Siklossy 1972 [125] I,NN,NH WI,SM WM Siskind 1992 [126] I,NN,NH WI,SM G,WS,WM Siskind 1994 <p> This is the case, for instance, if every language contains a sentence that is unique to that language. Some have proposed that the class of grammars that children consider is highly restricted, with particular properties that render it identifiable (see Berwick <ref> [16] </ref> and Wexler and Culicover [148] for discussion). This possibility has generally been raised in the context of syntax. Regardless of whether it holds, other parts of language, such as the lexicon, are not so limited. <p> In learning frameworks that do not allow for such graded judgments of "grammaticality", heuristics (such as the Subset Principle <ref> [5, 16] </ref>) must be introduced to favor Grammar 1 over Grammar 2. Generative grammars with probabilistic interpretations (in other words, grammars that implicitly or explicitly define p (U jG)) are commonly called stochastic language models. The discriminatory power of stochastic language models comes at a steep price. <p> Other relevant arguments for simplicity as measured by description length include <ref> [16, 20, 37, 66, 133] </ref>. 40 CHAPTER 3. STOCHASTIC GRAMMARS, MODEL SELECTION AND LANGUAGE ACQ. generalization. From a Bayesian perspective this is not surprising: the 2 jGj prior very heavily biases towards grammars that are improbably simple from the linguistic perspective.
Reference: [17] <author> R. C. Berwick and S. Pilato. </author> <title> Learning syntax by automata induction. </title> <journal> Machine Learning, </journal> <volume> 2 </volume> <pages> 9-38, </pages> <year> 1987. </year>
Reference-contexts: This literature has not generally considered linguistic applications (though see Berwick and Pilato <ref> [17] </ref>, who use Angluin's [6] notion of k-reversibility to acquire automata for the English auxiliary system). Since the classes of automata that are generally used allow for arbitrary states and arbitrary transitions, it is often difficult to imagine how these automata could be given linguistic interpretations.
Reference: [18] <author> A. W. Biermann and J. W. Feldman. </author> <title> A survey of results in grammatical inference. </title> <editor> In S. Watanabe, editor, </editor> <booktitle> Frontiers of Pattern Recognition. </booktitle> <publisher> Academic Press, </publisher> <year> 1972. </year>
Reference-contexts: most fundamentally, our algorithm lumps 6 Historically (in line with Gold's view of (E-)language [63]) the term grammatical inference has referred to the learning of a classification procedure from positive and negative examples that can predict whether a sentence is or is not in a language; see Biermann and Feldman <ref> [18] </ref> for a review. More recently, researchers interested in building mechanisms that acquire the specific generative grammar believed to underly some input have also adopted the term to refer to their work; it is this (I-language) sense of grammatical inference that is used here. 5.4.
Reference: [19] <author> L. Bloomfield. </author> <title> Language. </title> <publisher> Holt, </publisher> <address> New York, </address> <year> 1933. </year>
Reference-contexts: To understand the goals of this thesis, it is necessary to define "knowledge of language" more precisely. In one sense, languages are sets of sentences, or alternatively mappings between sound and meaning; this is the traditional view of the structuralist and descriptivist schools (see, for example, Bloomfield <ref> [19] </ref> and Lewis [85]). Chomsky [41] uses the term E-language (externalized language) to refer to this notion. Viewing languages this way, learning language means to acquire knowledge sufficient to generate and interpret new utterances in the same manner as the rest of the speech community.
Reference: [20] <author> A. Blumer, A. Ehrenfeucht, D. Haussler, and M. K. Warmuth. </author> <title> Occam's Razor. </title> <journal> Information Processing Letters, </journal> <volume> 24 </volume> <pages> 377-380, </pages> <year> 1987. </year>
Reference-contexts: Other relevant arguments for simplicity as measured by description length include <ref> [16, 20, 37, 66, 133] </ref>. 40 CHAPTER 3. STOCHASTIC GRAMMARS, MODEL SELECTION AND LANGUAGE ACQ. generalization. From a Bayesian perspective this is not surprising: the 2 jGj prior very heavily biases towards grammars that are improbably simple from the linguistic perspective.
Reference: [21] <author> L. Breiman, J. Friedman, R. Olshen, and C. Stone. </author> <title> Classification and Regression Trees. Probability Series. </title> <booktitle> Wadsworth International Group, </booktitle> <address> Belmont, California, </address> <year> 1984. </year>
Reference-contexts: Niyogi [100] explores this idea in more mathematical detail, also with respect to language acquisition; see also literature on the bias-variance tradeoff <ref> [21, 60] </ref>. At face value structural risk minimization seems to be irrelevant to the language acquisition problem. After all, the learner does not get to choose what the class of human grammars is; that is defined externally to learning altogether.
Reference: [22] <author> M. R. Brent. </author> <title> Untitled. </title> <journal> Unpublished ms., </journal> <year> 1993. </year>
Reference-contexts: Finally, all of these theories assume that other modules can function without feedback and can be learned independently. 2.2. THEORIES OF LANGUAGE ACQUISITION 23 Paper Assumes Input Output Anderson 1977 [1] I,NN,NH MI,WM,SM G,WS Anderson 1981 [2] I,NN,NH MI,WM,SM G,WS,MPHR Berwick 1985 [16] I,NN,OCWS WI,WM,TR G,WS Brent 1993 <ref> [22] </ref> WW MPH,MPHR Gibson & Wexler 1994 [61] NN P,TR G Kazman 1994 [75] I,NH WI,WM,WS G,MPH,MPHR Rayner et al. 1988 [109] I,FG,NN,NH WI WS Selfridge 1981 [121] I,NN,NH WI,SM WM Siklossy 1972 [125] I,NN,NH WI,SM WM Siskind 1992 [126] I,NN,NH WI,SM G,WS,WM Siskind 1994 [128] I WI,SM WM Assumptions FG
Reference: [23] <author> M. R. Brent and T. A. Cartwright. </author> <title> Distributional regularity and phonotactic constraints are useful for segmentation. </title> <journal> Cognition, </journal> <note> 61, to appear. </note>
Reference-contexts: This line of research has lead to many approaches that are similar to ours. For example, Olivier [102], Wolff [149, 150, 151], Brent et al. [24], and Cartwright and Brent <ref> [23, 32] </ref> all present algorithms for the induction of word-like linguistic units from character and phoneme sequences; these algorithms all rely on dictionary-based representations similar to our multigram model (though usually no stochastic interpretation is assigned). <p> They report in Brent and Cartwright <ref> [23] </ref> substantially lower recall rates (40%-70%) for similar algorithms tested on slightly different data.
Reference: [24] <author> M. R. Brent, A. Lundberg, and S. Murthy. </author> <title> Discovering morphemic suffixes: A case study in minimum description length induction. </title> <booktitle> In Fifth International Workshop on AI and Statistics, </booktitle> <address> Ft. Lauderdale, Florida, </address> <year> 1995. </year>
Reference-contexts: This line of research has lead to many approaches that are similar to ours. For example, Olivier [102], Wolff [149, 150, 151], Brent et al. <ref> [24] </ref>, and Cartwright and Brent [23, 32] all present algorithms for the induction of word-like linguistic units from character and phoneme sequences; these algorithms all rely on dictionary-based representations similar to our multigram model (though usually no stochastic interpretation is assigned). <p> However, our methodology is unique in that the evaluation function used (based on equation 5.8) is a very close approximation of the description length actually achieved by versions of our algorithms that generate a complete description (for text compression). In contrast, Brent et al. <ref> [24] </ref>, Cartwright and Brent [32], Chen [35, 36], Ristad and Thomas [116] and others that invoke the MDL principle all compute ad hoc estimates of description length (often based on symbol counts) that do not closely reflect the best possible encodings of their grammars (Stolcke [137] is more careful).
Reference: [25] <author> T. Briscoe and N. Waegner. </author> <title> Robust stochastic parsing using the inside-outside algorithm. </title> <booktitle> In Proc. of the AAAI Workshop on Probabilistic-Based Natural Language Processing Techniques, </booktitle> <pages> pages 39-52, </pages> <year> 1992. </year>
Reference-contexts: This is an advantage of algorithms that manipulate the structure of the grammar over algorithms that start with complete structures (for example, all possible grammatical rules) and attempt to learn solely by manipulating stochastic properties <ref> [25, 104] </ref>. Only rarely will the EM-algorithm converge to the exact (local) optimum in a finite number of iterations. However, for the implementations considered below, after two or three iterations improvements to the description length tend to be so small as to be irrelevant. <p> The reasons behind the failures harken back to the discussions of chapter 3: extralinguistic patterns are learned at the expense of linguistic ones and words are made long in an effort to improve stochastic models. Much recent work has focused on the induction of context-free grammars or variations thereof <ref> [8, 25, 31, 30, 35, 36, 43, 83, 104, 137] </ref>. The hierarchical nature of these grammars would seem on the surface to be quite similar to our hierarchical, concatenative representation. However, algorithms designed for the induction of context-free grammars have not performed well in practice. <p> update the underlying structure of the grammar. (Some, like our algorithms, start with the most general grammar possible while others, like Stolcke's, start with the most specific grammar possible.) Members of the second class, exemplified by the work of Pereira and Schabes [104], Deligne and Bimbot [51], Briscoe and Waegner <ref> [25] </ref> and Lari and Young [83], pick an extremely general structural backbone for a stochastic model, and proceed by optimizing its stochastic properties, usually through the EM procedure. For example, Pereira and Schabes train a giant stochastic context-free grammar containing all possible rules of a certain form.
Reference: [26] <author> P. F. Brown, S. A. Della Pietra, V. J. Della Pietra, and R. L. Mercer. </author> <title> The mathematics of machine translation: Parameter estimation. </title> <journal> Computational Linguistics, </journal> <volume> 19(2) </volume> <pages> 253-312, </pages> <year> 1993. </year>
Reference-contexts: This work includes studies of language acquisition (see Selfridge [121], Siklossy [125] and Siskind [126, 127, 128]); parameter estimation schemes for machine translation, where sentences in a second language substitute for semantic input (see Brown et al. <ref> [26] </ref> and Berger et al. [15]); and parameter estimation schemes for systems that classify utterances (see Tishby and Gorin [140]). The learning algorithm presented in this chapter for the concatenative model extended with the meaning perturbation operator advances previous work in many ways.
Reference: [27] <author> P. L. Brown, J. Cocke, S. A. Della Pietra, V. J. Della Pietra, F. Jelinek, J. D. Lafferty, R. L. Mercer, and P. S. Rossin. </author> <title> A statistical approach to machine translation. </title> <journal> Computational Linguistics, </journal> <volume> 16(2) </volume> <pages> 79-86, </pages> <year> 1990. </year>
Reference-contexts: Then x = argmax x 0 p (x 0 jy) = argmax x 0 Thus, an important part of a recognition system is a prior probability over word or character sequences, p (x 0 ). This same noisy-channel methodology has been applied to problems of language translation <ref> [27] </ref>. 96 CHAPTER 5.
Reference: [28] <author> P. L. Brown, S. A. Della Pietra, V. J. Della Pietra, J. C. Lai, and R. L. Mercer. </author> <title> An estimate of an upper bound for the entropy of English. </title> <journal> Computational Linguistics, </journal> <volume> 18(1) </volume> <pages> 31-40, </pages> <year> 1992. </year>
Reference-contexts: At this point most children have attained nearly all the fluency and linguistic expertise of adults. Though 11 million utterances may seem like a lot, it is far less data than is commonly used to train computer models of language <ref> [28] </ref>, and allows for precious few examples for each of the tens of thousands of words that must be learned. However, the paucity of data is not nearly so troublesome for acquisition as the opaque relation between the grammar and the input signal. <p> This same noisy-channel methodology has been applied to problems of language translation [27]. 96 CHAPTER 5. LEARNING ALGORITHMS language model reported to date, with an entropy rate of 1.75 bits per character over the Brown corpus, was achieved by the IBM Language Modeling Group <ref> [28] </ref> using a Markov model over words with a two-word context (a trigram); as with most work in language modeling, their algorithms had access to a predefined lexicon. <p> The Brown corpus was converted to lower case so that the learning algorithm does not introduce additional parameters to model capitalized words at the start of sentences; Brown et al. <ref> [28] </ref> demonstrate that case distinctions contribute at most 0.04 bits per character to the entropy rate of the Brown corpus. <p> This happens to be the same rate achieved by Ristad and Thomas [116] using a context model on the same data. The best rate over the entire Brown corpus, achieved by Brown et al. <ref> [28] </ref> with a trigram Markov model over words, is 1.75 bits/char.
Reference: [29] <author> R. Brown and C. Hanlon. </author> <title> Derivational complexity and order of acquisition in child speech. </title> <editor> In J. R. Hayes, editor, </editor> <booktitle> Cognition and the Development of Language. </booktitle> <publisher> Wiley, </publisher> <address> New York, </address> <year> 1970. </year>
Reference-contexts: In this framework Chomsky is implicitly assuming that learning takes place from positive examples-sample sentences from the target language. This is consistent with Brown and Hanlon's <ref> [29] </ref> assessment (see also Marcus [92]) that children receive no negative evidence, a term that refers to both feedback from the teacher to the learner and negative examples- sentences labeled as outside of the target language.
Reference: [30] <author> G. Carroll. </author> <title> Learning Probabilistic Grammars for Language Modeling. </title> <type> PhD thesis, </type> <institution> Brown University, </institution> <address> Providence, Rhode Island, </address> <year> 1995. </year>
Reference-contexts: The reasons behind the failures harken back to the discussions of chapter 3: extralinguistic patterns are learned at the expense of linguistic ones and words are made long in an effort to improve stochastic models. Much recent work has focused on the induction of context-free grammars or variations thereof <ref> [8, 25, 31, 30, 35, 36, 43, 83, 104, 137] </ref>. The hierarchical nature of these grammars would seem on the surface to be quite similar to our hierarchical, concatenative representation. However, algorithms designed for the induction of context-free grammars have not performed well in practice. <p> Although the end grammars model the input moderately well from a predictive viewpoint, the derivations assigned to sentences do not agree with human judgments. Follow up work by Carroll and Charniak <ref> [30, 31] </ref> achieves similar results. Stolcke [137] and Chen [35, 36], by emphasizing structural induction to a greater extent, achieve better results on artificial languages but again are unable to learn natural-language grammars that reflect human judgments from real data.
Reference: [31] <author> G. Carroll and E. Charniak. </author> <title> Learning probabilistic dependency grammars from labeled text. </title> <booktitle> In Working Notes, Fall Symposium Series, AAAI, </booktitle> <pages> pages 25-31, </pages> <year> 1992. </year>
Reference-contexts: Efficiency, convergence, robustness and other properties of learning mechanisms all indirectly bear on other parts of the learning framework. For example, there is significant evidence that the known induction algorithms for certain classes of grammars (such as stochastic context-free grammars <ref> [31, 48, 104] </ref>) are systematically incapable of learning linguistically relevant languages; this reflects back on the appropriateness of the grammar class as a model of human language. <p> A natural way to estimate stochastic parameters for a language model is to find the parameters that maximize the likelihood of the observed evidence; this puts each grammar in its best possible light with respect to equation 3.1. Empirical tests <ref> [31, 48, 104] </ref> using various naive classes of stochastic grammars indicate that the stochastic grammars that maximize the probability of linguistic evidence do not in general have "linguistically plausible" structure. <p> The reasons behind the failures harken back to the discussions of chapter 3: extralinguistic patterns are learned at the expense of linguistic ones and words are made long in an effort to improve stochastic models. Much recent work has focused on the induction of context-free grammars or variations thereof <ref> [8, 25, 31, 30, 35, 36, 43, 83, 104, 137] </ref>. The hierarchical nature of these grammars would seem on the surface to be quite similar to our hierarchical, concatenative representation. However, algorithms designed for the induction of context-free grammars have not performed well in practice. <p> Although the end grammars model the input moderately well from a predictive viewpoint, the derivations assigned to sentences do not agree with human judgments. Follow up work by Carroll and Charniak <ref> [30, 31] </ref> achieves similar results. Stolcke [137] and Chen [35, 36], by emphasizing structural induction to a greater extent, achieve better results on artificial languages but again are unable to learn natural-language grammars that reflect human judgments from real data.
Reference: [32] <author> T. A. Cartwright and M. R. Brent. </author> <title> Segmenting speech without a lexicon: Evidence for a bootstrapping model of lexical acquisition. </title> <booktitle> In Proc. of the 16th Annual Meeting of the Cognitive Science Society, </booktitle> <address> Hillsdale, New Jersey, </address> <year> 1994. </year> <note> BIBLIOGRAPHY 127 </note>
Reference-contexts: Extralinguistic patterns have been the downfall of many computational theories of language acquisition, that have modeled them at the expense of linguistic ones (see for example Olivier [102] and Cartwright and Brent <ref> [32] </ref>). In chapter 4 a representation for language is presented that does not prevent extralin-guistic patterns from making their way into the grammar, but does ensure that they do not preclude desired parameters. 3.2. <p> As an extreme example, consider a case in which the learner's evidence U is a sequence of utterances U 0 6 Many other statistical induction schemes used for language inference have suffered from this problem; for examples see the results of Olivier [102] and Cartwright and Brent <ref> [32] </ref>. Their schemes, like the one presented in section 4.1.1, increase the number and length of the parameters learned as the size of the input increases, in an effort to model the statistics of the input as closely as possible (with a block code, in effect). <p> This line of research has lead to many approaches that are similar to ours. For example, Olivier [102], Wolff [149, 150, 151], Brent et al. [24], and Cartwright and Brent <ref> [23, 32] </ref> all present algorithms for the induction of word-like linguistic units from character and phoneme sequences; these algorithms all rely on dictionary-based representations similar to our multigram model (though usually no stochastic interpretation is assigned). <p> [136] and Chang et al. [33] discuss methods for learning Chinese names that are based on their idiosyncratic properties, and Chang et al. [34] judge new Chinese words by their similarity to existing words) or very similar to the more general lexical induction schemes of Olivier [102], Cartwright and Brent <ref> [32] </ref>, etc. Thus, if applied to the task of learning words from scratch, most of these algorithms would either be inappropriate or suffer from many of the same problems as the algorithms already discussed in the section on grammatical inference. <p> Members of the first class, found here and in the work of Olivier [102], Cook et al. [43] Wolff [150, 151], Ellison [57, 58], Nevill-Manning [99], Cartwright and Brent <ref> [32] </ref>, Chen [35, 36], Stol-cke [137] and others, iteratively update the underlying structure of the grammar. (Some, like our algorithms, start with the most general grammar possible while others, like Stolcke's, start with the most specific grammar possible.) Members of the second class, exemplified by the work of Pereira and Schabes <p> However, our methodology is unique in that the evaluation function used (based on equation 5.8) is a very close approximation of the description length actually achieved by versions of our algorithms that generate a complete description (for text compression). In contrast, Brent et al. [24], Cartwright and Brent <ref> [32] </ref>, Chen [35, 36], Ristad and Thomas [116] and others that invoke the MDL principle all compute ad hoc estimates of description length (often based on symbol counts) that do not closely reflect the best possible encodings of their grammars (Stolcke [137] is more careful). <p> The algorithm achieves a peak word recall rate of about 80%. This result is the most directly comparable to our Brown corpus tests. The poorer recall rate reflects the lengthy parameters learned to model regularities above the word level. Cartwright and Brent <ref> [32] </ref>, testing several word-learning algorithms on a very small (4000 phoneme) corpus of phonemified English text, 7 report a peak recall rate of 95.6%, but this drops dramatically if the algorithm is given more evidence, as the algorithm adds extralinguistic patterns to the lexicon.
Reference: [33] <author> J.-S. Chang, S.-D. Chen, Y. Zheng, Z.-Z. Liu, and S.-J. Ke. </author> <title> Large-corpus-based methods for Chinese personal name recognition. </title> <journal> Journal of Chinese Information Processing, </journal> <volume> 6(3) </volume> <pages> 7-15, </pages> <year> 1992. </year>
Reference-contexts: The only difference between this problem and ours is that we start with no prior knowledge of the lexicon. However, most of the techniques commonly used to discover new words for segmentation tasks are either application specific (Sproat et al. [136] and Chang et al. <ref> [33] </ref> discuss methods for learning Chinese names that are based on their idiosyncratic properties, and Chang et al. [34] judge new Chinese words by their similarity to existing words) or very similar to the more general lexical induction schemes of Olivier [102], Cartwright and Brent [32], etc.
Reference: [34] <author> J.-S. Chang, Y.-C. Lin, and K.-Y. Su. </author> <title> Automatic construction of a Chinese electronic dictionary. </title> <booktitle> In Third Workshop on Very Large Corpora, </booktitle> <address> Cambridge, Massachusetts, </address> <year> 1995. </year>
Reference-contexts: However, most of the techniques commonly used to discover new words for segmentation tasks are either application specific (Sproat et al. [136] and Chang et al. [33] discuss methods for learning Chinese names that are based on their idiosyncratic properties, and Chang et al. <ref> [34] </ref> judge new Chinese words by their similarity to existing words) or very similar to the more general lexical induction schemes of Olivier [102], Cartwright and Brent [32], etc.
Reference: [35] <author> S. F. Chen. </author> <title> Bayesian grammar induction for language modeling. </title> <booktitle> In Proc. 33rd Annual Meeting of the Association for Computational Linguistics, </booktitle> <pages> pages 228-235, </pages> <address> Cambridge, Massachusetts, </address> <year> 1995. </year>
Reference-contexts: This general procedure of alternating between structural and parametric (in the traditional sense) updates to Bayesian models is a popular strategy for structural induction problems; see for example the stochastic grammar induction schemes of Stolcke [137] and Chen <ref> [35] </ref>, and the extended literature on structural induction of neural networks and Bayes' nets. 74 CHAPTER 5. LEARNING ALGORITHMS Let G be the simplest lexicon. Iterate until convergence: Let U 0 = U + G. Optimize stochastic properties of G over U 0 . <p> The reasons behind the failures harken back to the discussions of chapter 3: extralinguistic patterns are learned at the expense of linguistic ones and words are made long in an effort to improve stochastic models. Much recent work has focused on the induction of context-free grammars or variations thereof <ref> [8, 25, 31, 30, 35, 36, 43, 83, 104, 137] </ref>. The hierarchical nature of these grammars would seem on the surface to be quite similar to our hierarchical, concatenative representation. However, algorithms designed for the induction of context-free grammars have not performed well in practice. <p> Although the end grammars model the input moderately well from a predictive viewpoint, the derivations assigned to sentences do not agree with human judgments. Follow up work by Carroll and Charniak [30, 31] achieves similar results. Stolcke [137] and Chen <ref> [35, 36] </ref>, by emphasizing structural induction to a greater extent, achieve better results on artificial languages but again are unable to learn natural-language grammars that reflect human judgments from real data. <p> Members of the first class, found here and in the work of Olivier [102], Cook et al. [43] Wolff [150, 151], Ellison [57, 58], Nevill-Manning [99], Cartwright and Brent [32], Chen <ref> [35, 36] </ref>, Stol-cke [137] and others, iteratively update the underlying structure of the grammar. (Some, like our algorithms, start with the most general grammar possible while others, like Stolcke's, start with the most specific grammar possible.) Members of the second class, exemplified by the work of Pereira and Schabes [104], Deligne <p> In contrast, Brent et al. [24], Cartwright and Brent [32], Chen <ref> [35, 36] </ref>, Ristad and Thomas [116] and others that invoke the MDL principle all compute ad hoc estimates of description length (often based on symbol counts) that do not closely reflect the best possible encodings of their grammars (Stolcke [137] is more careful).
Reference: [36] <author> S. F. Chen. </author> <title> Building Probabalistic Models for Natural Language. </title> <type> PhD thesis, </type> <institution> Harvard University, Cambridge, Massachusetts, </institution> <year> 1996. </year>
Reference-contexts: The reasons behind the failures harken back to the discussions of chapter 3: extralinguistic patterns are learned at the expense of linguistic ones and words are made long in an effort to improve stochastic models. Much recent work has focused on the induction of context-free grammars or variations thereof <ref> [8, 25, 31, 30, 35, 36, 43, 83, 104, 137] </ref>. The hierarchical nature of these grammars would seem on the surface to be quite similar to our hierarchical, concatenative representation. However, algorithms designed for the induction of context-free grammars have not performed well in practice. <p> Although the end grammars model the input moderately well from a predictive viewpoint, the derivations assigned to sentences do not agree with human judgments. Follow up work by Carroll and Charniak [30, 31] achieves similar results. Stolcke [137] and Chen <ref> [35, 36] </ref>, by emphasizing structural induction to a greater extent, achieve better results on artificial languages but again are unable to learn natural-language grammars that reflect human judgments from real data. <p> Members of the first class, found here and in the work of Olivier [102], Cook et al. [43] Wolff [150, 151], Ellison [57, 58], Nevill-Manning [99], Cartwright and Brent [32], Chen <ref> [35, 36] </ref>, Stol-cke [137] and others, iteratively update the underlying structure of the grammar. (Some, like our algorithms, start with the most general grammar possible while others, like Stolcke's, start with the most specific grammar possible.) Members of the second class, exemplified by the work of Pereira and Schabes [104], Deligne <p> In contrast, Brent et al. [24], Cartwright and Brent [32], Chen <ref> [35, 36] </ref>, Ristad and Thomas [116] and others that invoke the MDL principle all compute ad hoc estimates of description length (often based on symbol counts) that do not closely reflect the best possible encodings of their grammars (Stolcke [137] is more careful).
Reference: [37] <author> N. A. Chomsky. </author> <title> Morphophonemics of modern Hebrew. </title> <type> Master's thesis, </type> <institution> University of Pennsylvania, </institution> <year> 1951. </year>
Reference-contexts: Other relevant arguments for simplicity as measured by description length include <ref> [16, 20, 37, 66, 133] </ref>. 40 CHAPTER 3. STOCHASTIC GRAMMARS, MODEL SELECTION AND LANGUAGE ACQ. generalization. From a Bayesian perspective this is not surprising: the 2 jGj prior very heavily biases towards grammars that are improbably simple from the linguistic perspective.
Reference: [38] <author> N. A. Chomsky. </author> <title> The Logical Structure of Linguistic Theory. </title> <publisher> Plenum Press, </publisher> <address> New York, </address> <year> 1955. </year>
Reference-contexts: In his language work the importance of penalizing complexity is already emphasized. As far back as 1955 Chomsky wrote in The Logical Structure of Linguistic Theory <ref> [38] </ref> In applying this theory to actual linguistic material, we must construct a grammar of the proper form: : : Among all grammars meeting this condition, we select the simplest.
Reference: [39] <author> N. A. Chomsky. </author> <title> On the notion "rule of grammar". </title> <editor> In R. Jakobson, editor, </editor> <booktitle> Proceedings of the Symposia in Applied Mathematics, Volume VII, </booktitle> <address> New York, </address> <year> 1961. </year>
Reference-contexts: The second is the extent to which extralinguistic input serves to directly transmit parameter values. These are both discussed here in the context of one particular framework for theories of acquisition. Chomsky writes <ref> [39, 40] </ref> that any theory of language must provide * (i) an enumeration of the class s 1 ; s 2 ; : : : of possible sentences; * (ii) an enumeration of the class SD 1 ; SD 2 ; : : : of possible structural descriptions; * (iii) an
Reference: [40] <author> N. A. Chomsky. </author> <title> Aspects of The Theory of Syntax. </title> <publisher> MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1965. </year>
Reference-contexts: The second is the extent to which extralinguistic input serves to directly transmit parameter values. These are both discussed here in the context of one particular framework for theories of acquisition. Chomsky writes <ref> [39, 40] </ref> that any theory of language must provide * (i) an enumeration of the class s 1 ; s 2 ; : : : of possible sentences; * (ii) an enumeration of the class SD 1 ; SD 2 ; : : : of possible structural descriptions; * (iii) an <p> THE REPRESENTATIONAL FRAMEWORK 45 4.1 The Representational Framework A central tenet of modern linguistic theory is that language makes "infinite use of finite means" <ref> [40, 144] </ref>, or in plainer terms, that language combines a finite set of lexical parameters to produce an infinite variety of sentences. This chapter argues that these lexical parameters, the primitive units of sentence processing, are themselves built by composing parts, inside the lexicon.
Reference: [41] <author> N. A. Chomsky. </author> <title> Knowledge of Language: Its Nature, Origin, and Use. Praeger, </title> <address> New York, </address> <year> 1986. </year>
Reference-contexts: In one sense, languages are sets of sentences, or alternatively mappings between sound and meaning; this is the traditional view of the structuralist and descriptivist schools (see, for example, Bloomfield [19] and Lewis [85]). Chomsky <ref> [41] </ref> uses the term E-language (externalized language) to refer to this notion. Viewing languages this way, learning language means to acquire knowledge sufficient to generate and interpret new utterances in the same manner as the rest of the speech community.
Reference: [42] <author> J. G. Cleary and I. H. Witten. </author> <title> Data compression using adaptive coding and partial string matching. </title> <journal> IEEE Transactions on Communications, </journal> <volume> 32(4) </volume> <pages> 396-402, </pages> <year> 1984. </year>
Reference-contexts: Popular text compression schemes can be divided into four classes: those based on adaptive frequency techniques like Huffman codes; those based on context models [112] (such as the PPM algorithm <ref> [42, 96, 139] </ref>, probably the most effective widely-used method for text compression); those based on hidden Markov models [13] (these are less common); and those based on dictionary methods. <p> Figure 6.4 presents results over the corpus, compared with the LZ78-based [147, 156] compress program, the LZ77-based [155] gzip program, Sequitur [99] and a PPM-based program <ref> [42, 96] </ref>. The performance figures for other programs are taken from Nevill-Manning [99]. 3 This is not exactly true. Were the parameter not in the lexicon, each of its components would have higher counts, and thus slightly shorter codewords. 106 CHAPTER 6.
Reference: [43] <author> C. M. Cook, A. Rosenfeld, and A. R. Aronson. </author> <title> Grammatical inference by hill climbing. </title> <journal> Information Sciences, </journal> <volume> 10 </volume> <pages> 59-80, </pages> <year> 1976. </year>
Reference-contexts: The reasons behind the failures harken back to the discussions of chapter 3: extralinguistic patterns are learned at the expense of linguistic ones and words are made long in an effort to improve stochastic models. Much recent work has focused on the induction of context-free grammars or variations thereof <ref> [8, 25, 31, 30, 35, 36, 43, 83, 104, 137] </ref>. The hierarchical nature of these grammars would seem on the surface to be quite similar to our hierarchical, concatenative representation. However, algorithms designed for the induction of context-free grammars have not performed well in practice. <p> Members of the first class, found here and in the work of Olivier [102], Cook et al. <ref> [43] </ref> Wolff [150, 151], Ellison [57, 58], Nevill-Manning [99], Cartwright and Brent [32], Chen [35, 36], Stol-cke [137] and others, iteratively update the underlying structure of the grammar. (Some, like our algorithms, start with the most general grammar possible while others, like Stolcke's, start with the most specific grammar possible.) Members
Reference: [44] <author> T. M. Cover and R. C. King. </author> <title> A convergent gambling estimate of the entropy of English. </title> <journal> IEEE Transactions on Information Theory, </journal> <volume> 24(4) </volume> <pages> 413-421, </pages> <year> 1950. </year>
Reference-contexts: This upper bound on the "true" entropy of English (or at least of the Brown corpus) is significantly closer to the rates of 1.3 and 1.25 bits/char achieved by human subjects as tested by Shannon [123] and Cover and King <ref> [44] </ref>. 4 However, that result came after training on almost 600 million words of text, starting with substantial knowledge of language. The resulting model would have dwarfed the Brown corpus in size, and hence is difficult to compare with a compression algorithm.
Reference: [45] <author> I. Csiszar and G. Tusnady. </author> <title> Information geometry and alternating minimization procedures. </title> <journal> Statistics and Decisions, Supplemental Issue 1 </journal> <pages> 205-237, </pages> <year> 1984. </year>
Reference-contexts: Hence, the optimization procedure seems cyclic: the optimal stochastic model is a function of representations, which are in turn a function of the stochastic model. The expectation-maximization (EM) procedure of Dempster et al. <ref> [12, 45, 54] </ref> is a standard method for 5.1. GENERAL ARCHITECTURE 75 solving optimization problems involving hidden representations.
Reference: [46] <author> A. Cutler. </author> <title> Segmentation problems, rhythmic solutions. </title> <booktitle> Lingua, </booktitle> <pages> 92(1-4), </pages> <year> 1994. </year>
Reference-contexts: so at least the early stages of learning must be performed on the basis of the speech signal alone, which has been argued to contain few explicit clues about the 3 Some have argued that the acquisition of the phonological lexicon is dependent on knowledge of stress and intonational patterns <ref> [46, 72, 73] </ref>. 30 CHAPTER 2. THE PROBLEM OF LANGUAGE ACQUISITION source grammar. These stages must therefore produce parameter values that are consistent with the mapping even though they have no access to it.
Reference: [47] <author> C. de Marcken. </author> <title> The acquisition of a lexicon from paired phoneme sequences and semantic representations. </title> <booktitle> In International Colloquium on Grammatical Inference, </booktitle> <pages> pages 66-77, </pages> <address> Alicante, Spain, </address> <year> 1994. </year>
Reference-contexts: Similarly, various algorithms have been constructed that use artificial semantic representations to aid the acquisition of syntax [126, 128] and the phonological lexicon <ref> [47] </ref>. Indeed, much work on syntactic acquisition has assumed that the thematic roles of noun phrases are known to the learner [61]. Finally, it has been shown that children do not learn much, if anything, from sound patterns in isolation [117, 130]; some environmental clues are probably necessary for learning.
Reference: [48] <author> C. de Marcken. </author> <title> Lexical heads, phrase structure and the induction of grammar. </title> <booktitle> In Third Workshop on Very Large Corpora, </booktitle> <address> Cambridge, Massachusetts, </address> <year> 1995. </year>
Reference-contexts: Efficiency, convergence, robustness and other properties of learning mechanisms all indirectly bear on other parts of the learning framework. For example, there is significant evidence that the known induction algorithms for certain classes of grammars (such as stochastic context-free grammars <ref> [31, 48, 104] </ref>) are systematically incapable of learning linguistically relevant languages; this reflects back on the appropriateness of the grammar class as a model of human language. <p> Thus, the important question is: given evidence U produced from a (non-stochastic, teacher's) grammar G, does the stochastic grammar that maximizes the likelihood of U have the same core (non-stochastic) structure as G? 1 The answer, discussed at length in de Marcken <ref> [48] </ref>, depends crucially on the way that the stochastic properties of language models are tied to linguistic structure. <p> A natural way to estimate stochastic parameters for a language model is to find the parameters that maximize the likelihood of the observed evidence; this puts each grammar in its best possible light with respect to equation 3.1. Empirical tests <ref> [31, 48, 104] </ref> using various naive classes of stochastic grammars indicate that the stochastic grammars that maximize the probability of linguistic evidence do not in general have "linguistically plausible" structure. <p> affect the representations of other parameters and utterances. 7 In contrast, many other representational frameworks that merge the representation of patterns with on-line processing, such as neural networks, have difficulty changing internal representations, because in intermediate stages their performance is degraded (the pervasive local-minima problem; see for example de Marcken <ref> [48] </ref>). 4.2.2 Language All of the above arguments in favor of the representational framework center around learning issues and are independent of "linguistics". It remains important that the compositional prior favor linguistically plausible parameters. Here several linguistic arguments in favor of the representational framework are presented. <p> global optimum is a function of the complexities of the search space as well as the starting point for the algorithm; in many cases the procedure is woefully incapable of finding a global optimum, and this can have significant effects on learning strategies based on the EM algorithm alone (see <ref> [48] </ref> for discussion). In the context of the algorithms discussed here, the other optimization step, which modifies the linguistic structure of the lexicon, often provides a means of escaping from local optima. <p> Stolcke [137] and Chen [35, 36], by emphasizing structural induction to a greater extent, achieve better results on artificial languages but again are unable to learn natural-language grammars that reflect human judgments from real data. Some of the reasons for these failures are given in de Marcken <ref> [48] </ref>, and motivate the compositional representation we use. They can be divided into two categories. First, search in the space of context-free grammars is fraught with local optima. This is discussed at greater length below. <p> In general, the second class of learning algorithms has fared more poorly than the first. The reason, as discussed by Pereira and Schabes [104] and de Marcken <ref> [48] </ref>, is that the hill-climbing inside-outside algorithm is incapable of making the complex moves in grammar-space necessary to escape local optima. As a consequence, these learning algorithms quickly get stuck near their starting point, with little learning having taken place. <p> LEARNING ALGORITHMS stochastic methods of the second type. This is because these algorithms maintain a single grammar, stored and manipulated in terms of its representation. As pointed out in de Marcken <ref> [48] </ref>, moves that are relatively simple to express at a conceptual level may involve quite substantial changes to the representation of a grammar.
Reference: [49] <author> C. de Marcken. </author> <title> The unsupervised acquisition of a lexicon from continuous speech. </title> <type> Memo 1558, </type> <institution> MIT Artificial Intelligence Lab., Cambridge, Massachusetts, </institution> <year> 1995. </year>
Reference-contexts: freq where in the first case the sound change is captured by an assimilation of nasality from the /n/ to the /t/ and a reduction of the vowel in to, and in the second case also an assimilation of the nasality of the /n/ to the /d/ (see de Marcken <ref> [49] </ref> for a more detailed definition of a phonological perturbation scheme that can account for such phenomena). These parameters inherit their syntax and meaning from underlying (uncorrupted) words, and yet contain statistical information that indicates that the sound changes are to be expected. <p> The description length of a sound change from a sequence to a sequence fi is determined by a stochastic model p (fij). p (fij) is constructed to reflect a simple theory of phonetics. This is described in more detail in de Marcken <ref> [49] </ref>. Figure 4.7 gives a flavor for how this model works. The final output of the first two stages of our model is still a sequence of phonemes. The third and final stage of the stochastic production process maps from phoneme sequence to acoustic signals. <p> In section 4.4.4 (see also de Marcken <ref> [49] </ref>) we presented a more sophisticated composition and perturbation model that incorporates significantly greater knowledge of phonology and phonetics, and allows for sound changes.
Reference: [50] <author> C. de Marcken. </author> <title> Linguistic structure as composition and perturbation. </title> <booktitle> In Proc. 34th Annual Meeting of the Association for Computational Linguistics, </booktitle> <pages> pages 335-341, </pages> <address> Santa Cruz, California, </address> <year> 1996. </year>
Reference-contexts: Trained with the three ambiguous meanings, sememe accuracy was 96.5%, sememe recall was 70.2%. For very similar results on a slightly different data set, see de Marcken <ref> [50] </ref>. Although it would have been valuable to do so, no experiments were performed in which the algorithm was tested on different utterances than it was trained on.
Reference: [51] <author> S. Deligne and F. Bimbot. </author> <title> Language modeling by variable length sequences: Theoretical formulation and evaluation of multigrams. </title> <booktitle> In Proceedings of the International Conference on Speech and Signal Processing, </booktitle> <volume> volume 1, </volume> <pages> pages 169-172, </pages> <year> 1995. </year> <note> 128 BIBLIOGRAPHY </note>
Reference-contexts: This stochastic language model has been called a multigram <ref> [51] </ref> and used for a variety of language modeling applications. Multigrams account for statistical dependencies by assigning probabilities p (w) to lengthy character sequences: they are essentially variable-length block codes. For example, the fact that p (the) 4.2. <p> The only cases that have met with significant success (on language modeling grounds) have not demonstrated that they actually produce derivations that agree with linguistic intuitions. The most pointed example of this is the multigram model, discussed in the context of language modeling by Deligne and Bimbot <ref> [51] </ref>. Although this is the same model that is used here (and that has been studied by others; see below), the model is used by them in a different way (without the compositional representation) and does not produce linguistically plausible segmentations of the input. <p> [137] and others, iteratively update the underlying structure of the grammar. (Some, like our algorithms, start with the most general grammar possible while others, like Stolcke's, start with the most specific grammar possible.) Members of the second class, exemplified by the work of Pereira and Schabes [104], Deligne and Bimbot <ref> [51] </ref>, Briscoe and Waegner [25] and Lari and Young [83], pick an extremely general structural backbone for a stochastic model, and proceed by optimizing its stochastic properties, usually through the EM procedure.
Reference: [52] <author> S. Della Pietra, V. Della Pietra, J. Gillett, J. Lafferty, H. Printz, and L. Ures. </author> <title> Inference and estimation of a long-range trigram model. </title> <booktitle> In International Colloquium on Grammatical Inference, </booktitle> <pages> pages 78-92, </pages> <address> Alicante, Spain, </address> <year> 1994. </year>
Reference-contexts: Nevertheless, there have been some experiments in language modeling that used underlying structures with natural linguistic interpretations, such as the long-range trigram model of Della Pietra et al. <ref> [52] </ref>, based loosely on the link grammars of Sleator and Temperley [129]. The only cases that have met with significant success (on language modeling grounds) have not demonstrated that they actually produce derivations that agree with linguistic intuitions.
Reference: [53] <author> S. Della Pietra, V. Della Pietra, and J. Lafferty. </author> <title> Inducing features of random fields. </title> <type> Technical Report CMU-CS-95-144, </type> <institution> Carnegie Mellon University, Pittsburgh, Pennsylvania, </institution> <month> May </month> <year> 1995. </year>
Reference-contexts: when the learner's grammar is chosen to minimize the Kullback-Leibler distance D (p T k p L ) between the distributions p T and p L , where the Kullback-Leibler distance is defined by D (p T k p L ) = u p L (u) : It is possible <ref> [15, 53] </ref> (and indeed effective) to construct stochastic language models by defining p L to be the least-committal (maximum-entropy) distribution consistent with known properties of the target language distribution p T . <p> More sophisticated models might represent statistical properties differently, perhaps in a manner better suited for combining multiple pieces of information (see, for example, the maximum-entropy language modeling scheme described by Della Pietra et al. <ref> [15, 53] </ref>). The more parameters are in a lexicon, the more control points the stochastic model has, and the better it will be able to model the target language. Hence, any lexical parameter should reduce (or at least not increase) the description length of the input and existing parameters. <p> Language models that can combine multiple knowledge sources are described by Della Pietra et al. <ref> [15, 53] </ref>, but result in complex and computationally burdensome learning algorithms. 4.4.2 Composition by Substitution One of the most significant deficiencies with the concatenative model is that it can not capture relations that hold of broad classes of objects without multiplying the number of parameters in the grammar.
Reference: [54] <author> A. P. Dempster, N. M. Liard, and D. B. Rubin. </author> <title> Maximum liklihood from incomplete data via the EM algorithm. </title> <journal> Journal of the Royal Statistical Society, </journal> <volume> B(39):1-38, </volume> <year> 1977. </year>
Reference-contexts: Hence, the optimization procedure seems cyclic: the optimal stochastic model is a function of representations, which are in turn a function of the stochastic model. The expectation-maximization (EM) procedure of Dempster et al. <ref> [12, 45, 54] </ref> is a standard method for 5.1. GENERAL ARCHITECTURE 75 solving optimization problems involving hidden representations.
Reference: [55] <author> B. E. Dresher and J. D. Kaye. </author> <title> A computational learning model for metrical phonology. </title> <journal> Cognition, </journal> <volume> 34 </volume> <pages> 137-195, </pages> <year> 1990. </year>
Reference-contexts: It is for similar reasons that optimization researchers think in terms of an objective function, even though their algorithms may only consider its derivative when searching. An example serves to illustrate the problematic nature of complex learning algorithms. Dresher and Kaye <ref> [55] </ref>, arguing that brute-force enumeration strategies are unsuitable models of human language acquisition, propose a cue-based learning algorithm for the parameters of a metrical stress system. In cue-based strategies, the learner is aware of the relationship between various sentences and parameter values.
Reference: [56] <author> A. Ehrenfeucht, D. Haussler, M. Kearns, and L. Valiant. </author> <title> A general lower bound on the number of examples needed for learning. </title> <journal> Information and Control, </journal> <volume> 82(3) </volume> <pages> 247-251, </pages> <year> 1989. </year>
Reference-contexts: formalization it can be shown that for this to be possible, the class of hypothesis languages must be heavily constrained; for example, in the PAC learning framework [141] it can be shown that the VC-dimension of the hypothesis class is bounded by the number of samples available to the learner <ref> [56] </ref>, up to a factor that depends on the allowable error rate. 3 This means that the complexity 4 of the class of grammars that can be entertained by the learner is inherently constrained by the amount of data available for parameter estimation.
Reference: [57] <author> T. M. Ellison. </author> <title> The Machine Learning of Phonological Structure. </title> <type> PhD thesis, </type> <institution> University of Western Australia, </institution> <year> 1992. </year>
Reference-contexts: Very often the complexity of grammars is measured using coding schemes that treat the grammar as a sequence of symbols to be written out on a piece of paper for viewing. Exceptions include the work of Ellison <ref> [57, 58] </ref> (where linguistically interesting representations for grammars are explored) and Stolcke [137] (where statistically principled means are used to estimate description length). 4.5. <p> Members of the first class, found here and in the work of Olivier [102], Cook et al. [43] Wolff [150, 151], Ellison <ref> [57, 58] </ref>, Nevill-Manning [99], Cartwright and Brent [32], Chen [35, 36], Stol-cke [137] and others, iteratively update the underlying structure of the grammar. (Some, like our algorithms, start with the most general grammar possible while others, like Stolcke's, start with the most specific grammar possible.) Members of the second class, exemplified <p> Of course some of these algorithms (like ours) also use the EM algorithm to optimize stochastic properties along the way. Almost all algorithms of the first type define a set of candidate changes, and an evaluation function. Some, like Ellison's <ref> [57] </ref>, use a simulated-annealing approach where a change may be accepted even if it results in a poorer score from the evaluation function. Stolcke [137] uses incremental count-change techniques very similar to ours to estimate changes in description length.
Reference: [58] <author> T. M. Ellison. </author> <title> The iterative learning of phonological rules. </title> <journal> Computational Linguistics, </journal> <volume> 20(3), </volume> <year> 1994. </year>
Reference-contexts: Very often the complexity of grammars is measured using coding schemes that treat the grammar as a sequence of symbols to be written out on a piece of paper for viewing. Exceptions include the work of Ellison <ref> [57, 58] </ref> (where linguistically interesting representations for grammars are explored) and Stolcke [137] (where statistically principled means are used to estimate description length). 4.5. <p> Members of the first class, found here and in the work of Olivier [102], Cook et al. [43] Wolff [150, 151], Ellison <ref> [57, 58] </ref>, Nevill-Manning [99], Cartwright and Brent [32], Chen [35, 36], Stol-cke [137] and others, iteratively update the underlying structure of the grammar. (Some, like our algorithms, start with the most general grammar possible while others, like Stolcke's, start with the most specific grammar possible.) Members of the second class, exemplified
Reference: [59] <author> W. N. Francis and H. Kucera. </author> <title> Frequency Analysis of English usage: Lexicon and Grammar. </title> <address> Houghton-Mi*in, Boston, </address> <year> 1982. </year>
Reference-contexts: The first assumes that words and sentences are character sequences, represented by concatenating words. Thus, every word and sentence is hierarchically decomposed. Figure 1.1 presents an example of how one word learned from the Brown corpus <ref> [59] </ref> using this model is represented. A second instantiation of the framework extends this concatenative model with an operator that adds meanings to words. This model can be used to learn word meanings from pairs of sentences and representations of meanings. <p> Terminals have no representation. For example, when tested on an unsegmented (spaceless) version of the Brown corpus <ref> [59] </ref>, one of the parameters learned is nationalfootballleague. The representation of this phrase is national ffi football ffi league. A larger portion of the recursive decomposition of the phrase in the lexicon is presented in figure 1.1. <p> As a simple example, in the Brown corpus of English text <ref> [59] </ref>, the phrase kicking the bucket is used five times. That is surprisingly high, given the relative infrequency of the words kicking and bucket. <p> the number of voters and the size of this city". the jury said it did find that many of georgia's registration and election laws "are outmod ed or inadequate and often ambiguous". 6.1.1 Input The concatenative algorithm of chapter 5 was run on two bodies of text, the Brown corpus <ref> [59] </ref> and the Calgary corpus [13]. The Brown corpus is a diverse million-word (approximately 40,000 sentence) corpus of English text, divided into 15 sections by document type and further into 500 documents of about 2000 words each.
Reference: [60] <author> S. Geman, E. Bienenstock, and R. Doursat. </author> <title> Neural networks and the bias/variance dilemma. </title> <journal> Neural Computation, </journal> <volume> 4 </volume> <pages> 1-58, </pages> <year> 1992. </year>
Reference-contexts: Niyogi [100] explores this idea in more mathematical detail, also with respect to language acquisition; see also literature on the bias-variance tradeoff <ref> [21, 60] </ref>. At face value structural risk minimization seems to be irrelevant to the language acquisition problem. After all, the learner does not get to choose what the class of human grammars is; that is defined externally to learning altogether.
Reference: [61] <author> E. Gibson and K. </author> <title> Wexler. Triggers. </title> <journal> Linguistic Inquiry, </journal> <volume> 25 </volume> <pages> 407-454, </pages> <year> 1994. </year>
Reference-contexts: THEORIES OF LANGUAGE ACQUISITION 23 Paper Assumes Input Output Anderson 1977 [1] I,NN,NH MI,WM,SM G,WS Anderson 1981 [2] I,NN,NH MI,WM,SM G,WS,MPHR Berwick 1985 [16] I,NN,OCWS WI,WM,TR G,WS Brent 1993 [22] WW MPH,MPHR Gibson & Wexler 1994 <ref> [61] </ref> NN P,TR G Kazman 1994 [75] I,NH WI,WM,WS G,MPH,MPHR Rayner et al. 1988 [109] I,FG,NN,NH WI WS Selfridge 1981 [121] I,NN,NH WI,SM WM Siklossy 1972 [125] I,NN,NH WI,SM WM Siskind 1992 [126] I,NN,NH WI,SM G,WS,WM Siskind 1994 [128] I WI,SM WM Assumptions FG Grammar fixed in program. <p> Little can be said about the nature of the cues without reference to the details of the parameter system; for any change in the model of stress the feasibility of a cue-based strategy must be re-justified. In contrast, Gibson and Wexler's <ref> [61] </ref> simpler "TLA" parameter-setting algorithm is easily analyzed [101], though its success is similarly dependent on the structure of the parameter system. 2.3 The Nature of the Input In section 2.2.3 it was argued that theories of language acquisition should be built up from the evidence that is available to the <p> Similarly, various algorithms have been constructed that use artificial semantic representations to aid the acquisition of syntax [126, 128] and the phonological lexicon [47]. Indeed, much work on syntactic acquisition has assumed that the thematic roles of noun phrases are known to the learner <ref> [61] </ref>. Finally, it has been shown that children do not learn much, if anything, from sound patterns in isolation [117, 130]; some environmental clues are probably necessary for learning.
Reference: [62] <author> L. Gleitman. </author> <title> The structural sources of verb meanings. </title> <journal> Language Acquisition, </journal> <volume> 1(1) </volume> <pages> 3-55, </pages> <year> 1990. </year>
Reference-contexts: More generally, it is possible that side information extracted from beyond the speech stream or derived independently from the speech stream could be used to disambiguate between grammars, if the side information reflects properties of the derivation of input sentences under the target grammar. For example, Gleitman <ref> [62] </ref> suggests that syntactic parse trees can be reconstructed from prosodic information alone. Perhaps more plausibly, the actions taking place around a child may suggest various possible "meanings" for the sentences the child is hearing.
Reference: [63] <author> E. M. Gold. </author> <title> Language identification in the limit. </title> <journal> Information and Control, </journal> <volume> 10 </volume> <pages> 447-474, </pages> <year> 1967. </year>
Reference-contexts: This is consistent with what is known, but from a scientific standpoint it is important to explore the possibility that such side information plays a limited role in the learning process. 2.3.1 Positive and Negative Examples and Restricted Language Classes Gold <ref> [63] </ref> presents a framework for the study of the induction of formal languages that is very similar to Chomsky's, but allows for negative examples. <p> At the algorithmic level, there are three major differences between our approach and the range of algorithms explored by the above researchers. First and most fundamentally, our algorithm lumps 6 Historically (in line with Gold's view of (E-)language <ref> [63] </ref>) the term grammatical inference has referred to the learning of a classification procedure from positive and negative examples that can predict whether a sentence is or is not in a language; see Biermann and Feldman [18] for a review.
Reference: [64] <author> E. M. Gold. </author> <title> Complexity of automaton identification from given data. </title> <journal> Information and Control, </journal> <volume> 37 </volume> <pages> 302-320, </pages> <year> 1978. </year>
Reference-contexts: Because this problem taken at face value is trivial (merely encode the positive examples directly into the model), various optimization criteria have been imposed; for example, Angluin [4] and Gold <ref> [64] </ref> show that identification of the minimum-size automaton consistent with a finite set of examples is NP-complete. This literature has not generally considered linguistic applications (though see Berwick and Pilato [17], who use Angluin's [6] notion of k-reversibility to acquire automata for the English auxiliary system).
Reference: [65] <author> H. Goodluck. </author> <title> Language Acquisition. </title> <publisher> Blackwell Publishers, </publisher> <address> Cambridge, MA, </address> <year> 1991. </year>
Reference-contexts: Roughly speaking, phonological distinctions, syllable structure and other information concerning sound patterns are learned early [72, 73], followed later by words and syntax <ref> [65, 105] </ref>. But such facts shed little insight into the character of the process that maps evidence to grammar.
Reference: [66] <author> M. Halle. </author> <title> On the role of simplicity in linguistic descriptions. </title> <editor> In R. Jakobson, editor, </editor> <booktitle> Proceedings of the Symposia in Applied Mathematics, Volume VII, </booktitle> <address> New York, </address> <year> 1961. </year>
Reference-contexts: Other relevant arguments for simplicity as measured by description length include <ref> [16, 20, 37, 66, 133] </ref>. 40 CHAPTER 3. STOCHASTIC GRAMMARS, MODEL SELECTION AND LANGUAGE ACQ. generalization. From a Bayesian perspective this is not surprising: the 2 jGj prior very heavily biases towards grammars that are improbably simple from the linguistic perspective.
Reference: [67] <author> M. Halle. </author> <title> On distinctive features and their articulatory implementation. </title> <booktitle> Natural Language and Linguistic Theory, </booktitle> <volume> 1 </volume> <pages> 91-105, </pages> <year> 1983. </year>
Reference-contexts: To take just one example, the phonological representations that are the basis for speech production are rooted in coarse articulatory gestures <ref> [67] </ref> like tongue movements that have complex and sometimes subtle affects on the acoustic signal [108]. For this reason, it is extremely difficult to determine the motor commands that produced a signal. <p> Suppose that language production is modeled as a three-stage process. The first stage encompasses most of the mechanisms commonly associated with higher-level linguistic processing and terminates in a sequence of phonemes. A phoneme is a primitive object used to represent sound in the lexicon <ref> [67] </ref>. Each one defines a set of desired positions for various vocal articulators. For example, the /m/ phoneme specifies that the lips should be closed, that the velum should be lowered so that air flows through the nose, that the vocal cords should be vibrating, and so on.
Reference: [68] <author> J. W. Harris. </author> <title> Integrity of prosodic constituents and the domain of syllabification rules in Spanish and Catalan. </title> <editor> In K. Hale and S. J. Keyser, editors, </editor> <title> The View from Building 20: </title> <booktitle> Essays in Linguistics in Honor of Sylvain Bromberger. </booktitle> <publisher> MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1993. </year>
Reference-contexts: This is not an issue if the parent parameter requires the information, since then the representation is constrained. But if the parameter has the equivalent of "don't cares", then some extra complexities are introduced. 8 For example, in languages like Spanish syllables can cross word boundaries <ref> [68] </ref>. See the literature on bracketing paradoxes [78, 135]. 4.2. MOTIVATIONS 57 Irregular Forms and Compilation One of the great mysteries of language is how the processing mechanism rapidly reconstructs word sequences from speech.
Reference: [69] <author> D. A. Huffman. </author> <title> A method for the construction of minimum-redundancy codes. </title> <booktitle> Proceedings of the IRE, </booktitle> <volume> 40(9) </volume> <pages> 1098-1101, </pages> <year> 1952. </year>
Reference-contexts: The count of how many times parameters are referenced in the complete description of both thecatinthehat and the lexicon determines the length of the codeword for each parameter (here a Huff-man code <ref> [69] </ref> was used). The description length of each parameter is the sum of the lengths of its components' codewords (since the cost of perturbations and terminators is negligible). <p> In the concatenative representation, the input and each parameter are described by the composition of a sequence of parameters. 11 The coding scheme described here references each parameter by a codeword. Codewords are determined by a Huffman code <ref> [69] </ref> that is constructed in accordance with parameter frequencies. In practice, Huffman codes very closely approach the theoretical optimum efficiency that would result from non-integral length codewords; this is true both because the number of parameters is usually large and because parameter frequencies follow a smooth inverse-frequency distribution [154].
Reference: [70] <author> F. Jelinek, J. D. Lafferty, and R. L. Mercer. </author> <title> Basic methods of probabilistic context free grammars. </title> <editor> In P. Laface and R. DeMori, editors, </editor> <booktitle> Speech Recognition and Understanding, </booktitle> <pages> pages 345-360. </pages> <publisher> Springer-Verlag, </publisher> <address> New York, NY, </address> <year> 1990. </year>
Reference-contexts: This fact can be captured naturally in the Bayesian framework, if grammars are given a probabilistic interpretation. In particular, compare the following two stochastic context-free grammars (SCFGs <ref> [8, 70] </ref>), where the choice of nonterminal expansion is governed by probabilities: Grammar 1 S ) aBa (1) 2 ) 2 ) Grammar 2 S ) SS ( 1 ) a ( 1 ) b ( 1 The probability of the sentence aba under Grammar 1 is 1 2 . <p> This section briefly explains how this can be partially remedied by basing the composition operator on stochastic context-free grammars (SCFGs <ref> [8, 70] </ref>). Although we have performed successful experiments with the type of model described here, a number of fundamental deficiencies remain, and it is presented only as an illustration.
Reference: [71] <author> A. K. Joshi. </author> <title> Tree adjunct grammars. </title> <journal> Journal of Computer and System Sciences, </journal> <volume> 10 </volume> <pages> 136-163, </pages> <year> 1975. </year> <note> BIBLIOGRAPHY 129 </note>
Reference-contexts: In the compositional framework the phrase will be represented in the grammar in terms of other parameters, implicitly defining a tree structure over the words. There are many similarities between this type of model and tree-grammars <ref> [71] </ref>. However, the substitution model as defined above is not pursued further in this thesis, because it has significant linguistic and statistical shortcomings, and is not a sufficient improvement over the concate-native model to warrent extensive investigation.
Reference: [72] <author> P. W. Jusczyk. </author> <title> Discovering sound patterns in the native language. </title> <booktitle> In Proc. of the 15th Annual Meeting of the Cognitive Science Society, </booktitle> <pages> pages 49-60, </pages> <year> 1993. </year>
Reference-contexts: Roughly speaking, phonological distinctions, syllable structure and other information concerning sound patterns are learned early <ref> [72, 73] </ref>, followed later by words and syntax [65, 105]. But such facts shed little insight into the character of the process that maps evidence to grammar. <p> so at least the early stages of learning must be performed on the basis of the speech signal alone, which has been argued to contain few explicit clues about the 3 Some have argued that the acquisition of the phonological lexicon is dependent on knowledge of stress and intonational patterns <ref> [46, 72, 73] </ref>. 30 CHAPTER 2. THE PROBLEM OF LANGUAGE ACQUISITION source grammar. These stages must therefore produce parameter values that are consistent with the mapping even though they have no access to it.
Reference: [73] <author> P. W. Jusczyk. </author> <title> Infants speech perception and the development of the mental lexicon. </title> <editor> In J. C. Goodman and H. C. Nusbaum, editors, </editor> <booktitle> The Development of Speech Perception. </booktitle> <publisher> MIT Press, </publisher> <address> Cam-bridge, MA, </address> <year> 1994. </year>
Reference-contexts: Roughly speaking, phonological distinctions, syllable structure and other information concerning sound patterns are learned early <ref> [72, 73] </ref>, followed later by words and syntax [65, 105]. But such facts shed little insight into the character of the process that maps evidence to grammar. <p> so at least the early stages of learning must be performed on the basis of the speech signal alone, which has been argued to contain few explicit clues about the 3 Some have argued that the acquisition of the phonological lexicon is dependent on knowledge of stress and intonational patterns <ref> [46, 72, 73] </ref>. 30 CHAPTER 2. THE PROBLEM OF LANGUAGE ACQUISITION source grammar. These stages must therefore produce parameter values that are consistent with the mapping even though they have no access to it.
Reference: [74] <author> M. </author> <title> Kanazawa. Learnability Classes of Categorial Grammars. </title> <type> PhD thesis, </type> <institution> Stanford University, Stanford, California, </institution> <year> 1994. </year>
Reference-contexts: Providing the learner with linguistically structured information like syntactic trees or semantic formulae can trivialize the learning process, by making the grammar explicit in the input. Some recent papers argue that there are powerful classes of languages identifiable from positive data alone <ref> [74, 118, 124] </ref>; these learnability proofs assume access to structural descriptions in the input. Sakakibara [118], for example, has shown that a significant subset of context-free languages (those generated by reversible context-free grammars) are identifiable from positive data, if the example sentences come structured into unlabeled derivation trees.
Reference: [75] <author> R. Kazman. </author> <title> Simulating the child's acquisition of the lexicon and syntax- experiences with Babel. </title> <journal> Machine Learning, </journal> <volume> 16 </volume> <pages> 87-120, </pages> <year> 1994. </year>
Reference-contexts: THEORIES OF LANGUAGE ACQUISITION 23 Paper Assumes Input Output Anderson 1977 [1] I,NN,NH MI,WM,SM G,WS Anderson 1981 [2] I,NN,NH MI,WM,SM G,WS,MPHR Berwick 1985 [16] I,NN,OCWS WI,WM,TR G,WS Brent 1993 [22] WW MPH,MPHR Gibson & Wexler 1994 [61] NN P,TR G Kazman 1994 <ref> [75] </ref> I,NH WI,WM,WS G,MPH,MPHR Rayner et al. 1988 [109] I,FG,NN,NH WI WS Selfridge 1981 [121] I,NN,NH WI,SM WM Siklossy 1972 [125] I,NN,NH WI,SM WM Siskind 1992 [126] I,NN,NH WI,SM G,WS,WM Siskind 1994 [128] I WI,SM WM Assumptions FG Grammar fixed in program. NN No noise or inconsistent input.
Reference: [76] <author> M. J. Kearns and U. V. Vazirani. </author> <title> An Introduction to Computational Learning Theory. </title> <publisher> MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1994. </year>
Reference-contexts: the higher the ratio of the VC-dimension of the hypothesis class to the amount of evidence, the more likely the learner is to select a grammar that generalizes more poorly than is necessary [142] (given sufficient evidence for a given VC-dimension, any function consistent with the evidence will generalize well <ref> [76] </ref>). 3 The VC-dimension of a set of functions is, roughly speaking, a measure of the effective coverage of the set [143]. <p> that it is merely a heuristic, but point out three important ways in which it mimics the philosophy of the better-justified structural risk minimization: * In very many cases the VC-dimension of a parameterized class of functions is linear or near-linear in the number of free parameters in the class <ref> [10, 76, 142] </ref>. Given an efficient coding scheme, the length of a description of a set of (independent) parameters is linear in the number of parameters.
Reference: [77] <author> E. S. </author> <title> Keeping. Introduction to Statistical Inference. </title> <publisher> Van Nostrand, </publisher> <address> Princeton, New Jersey, </address> <year> 1962. </year>
Reference-contexts: Then (see an introductory statistics textbook such as Keeping <ref> [77] </ref>) various assumptions and approximations lead to the condition to add x 12 if 1:96 p (x 12 )(1 ^p (x 2 jx 1 )) N This condition says that x 12 is justified if its empirical probability exceeds the model's prediction by more than a certain threshold that depends inversely
Reference: [78] <author> M. Kenstowicz. </author> <title> Phonology in Generative Grammar. </title> <publisher> Blackwell Publishers, </publisher> <address> Cambridge, MA, </address> <year> 1994. </year>
Reference-contexts: But if the parameter has the equivalent of "don't cares", then some extra complexities are introduced. 8 For example, in languages like Spanish syllables can cross word boundaries [68]. See the literature on bracketing paradoxes <ref> [78, 135] </ref>. 4.2. MOTIVATIONS 57 Irregular Forms and Compilation One of the great mysteries of language is how the processing mechanism rapidly reconstructs word sequences from speech. During language production the underlying forms of words are transformed by various corrupting and distorting morphological, phonological and phonetic processes.
Reference: [79] <author> D. H. Klatt. </author> <title> Review of selected models of speech perception. </title> <editor> In W. Marslen-Wilson, editor, </editor> <title> Lexical Representation and Process. </title> <publisher> MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1992. </year>
Reference-contexts: Even if they were known, this would not uniquely determine the control sequence that caused them, because in the process of speaking gestures are routinely (but not necessarily predictably) omitted and otherwise corrupted in an attempt to minimize muscular effort <ref> [79] </ref>. Furthermore, unlike in the English writing system, neither phonemes (primitive bundles of articulatory gestures) nor words nor other units in speech are routinely separated by delimiters. <p> During language production the underlying forms of words are transformed by various corrupting and distorting morphological, phonological and phonetic processes. Standard computational models of recognition attempt to invert these processes <ref> [3, 81, 79] </ref> during recognition; since the forward processes are many-to-one, inversion seems to require (expensive) search. And yet speech recognition is quick and easy for people. The representational framework offers a partial explanation. Many of the corrupting processes are non-deterministic, but not entirely random.
Reference: [80] <author> A. N. </author> <title> Kolmogorov. Three approaches to the quantitative definition of information. </title> <journal> Problems of Information Transmission, </journal> <volume> 1(1) </volume> <pages> 1-7, </pages> <year> 1965. </year>
Reference-contexts: optimal way, and in no way guarantees that generalization performance is maximized: although results vary by application [97], as is to be expected, practical experience indicates (see [98, 116, 146] and section 6.1.3) that MDL as commonly used tends to underestimate the number of parameters necessary for optimum 5 See <ref> [80, 86, 87, 111, 113, 114] </ref> for attempted justifications of MDL and the closely related Kolmogorov complexity. Other relevant arguments for simplicity as measured by description length include [16, 20, 37, 66, 133]. 40 CHAPTER 3. STOCHASTIC GRAMMARS, MODEL SELECTION AND LANGUAGE ACQ. generalization.
Reference: [81] <author> K. Koskenniemi. </author> <title> Two-Level Morphology: A General Computational Model for Word-Form Recognition and Production. </title> <type> PhD thesis, </type> <institution> University of Helsinki, Helsinki, Finland, </institution> <year> 1983. </year>
Reference-contexts: During language production the underlying forms of words are transformed by various corrupting and distorting morphological, phonological and phonetic processes. Standard computational models of recognition attempt to invert these processes <ref> [3, 81, 79] </ref> during recognition; since the forward processes are many-to-one, inversion seems to require (expensive) search. And yet speech recognition is quick and easy for people. The representational framework offers a partial explanation. Many of the corrupting processes are non-deterministic, but not entirely random.
Reference: [82] <author> J. Kupiec. </author> <title> Robust part-of-speech tagging using a hidden Markov model. </title> <booktitle> Computer Speech and Language, </booktitle> <volume> 6 </volume> <pages> 225-242, </pages> <year> 1992. </year>
Reference-contexts: usually with an eye to using such models as the prior probability in speech and handwriting recognition applications. 7 Markov models have generally been the tool of choice, because there are no hidden aspects to the derivation of a sequence, and therefore the stochastic optimization process is trivial (see Kupiec <ref> [82] </ref> for a notable exception). The most impressive stochastic 7 Speech or handwriting production is modeled as a two stage process: an underlying sequence x (a word or character sequence) is generated and then an observable signal y (speech or handwriting) is generated from x.
Reference: [83] <author> K. Lari and S. J. Young. </author> <title> The estimation of stochastic context-free grammars using the inside-outside algorithm. </title> <booktitle> Computer Speech and Language, </booktitle> <volume> 4 </volume> <pages> 35-56, </pages> <year> 1990. </year>
Reference-contexts: The reasons behind the failures harken back to the discussions of chapter 3: extralinguistic patterns are learned at the expense of linguistic ones and words are made long in an effort to improve stochastic models. Much recent work has focused on the induction of context-free grammars or variations thereof <ref> [8, 25, 31, 30, 35, 36, 43, 83, 104, 137] </ref>. The hierarchical nature of these grammars would seem on the surface to be quite similar to our hierarchical, concatenative representation. However, algorithms designed for the induction of context-free grammars have not performed well in practice. <p> the grammar. (Some, like our algorithms, start with the most general grammar possible while others, like Stolcke's, start with the most specific grammar possible.) Members of the second class, exemplified by the work of Pereira and Schabes [104], Deligne and Bimbot [51], Briscoe and Waegner [25] and Lari and Young <ref> [83] </ref>, pick an extremely general structural backbone for a stochastic model, and proceed by optimizing its stochastic properties, usually through the EM procedure. For example, Pereira and Schabes train a giant stochastic context-free grammar containing all possible rules of a certain form.
Reference: [84] <author> W. J. M. Levelt. </author> <title> Speaking. </title> <publisher> MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1991. </year>
Reference-contexts: Here each of these are briefly reviewed to provide a general background for further discussion. 2.1.1 The Parameters Speakers express thoughts by causing rapid changes in air pressure. The production of this speech signal does not happen in one step but through a complex derivational process <ref> [84] </ref> that involves many intermediate representations, each generated in a manner that depends on information the speaker has learned.
Reference: [85] <author> D. Lewis. </author> <title> Languages and language. </title> <editor> In K. Gunderson, editor, </editor> <title> Language, Mind and Knowledge. </title> <publisher> University of Minnesota Press, </publisher> <address> Minneapolis, </address> <year> 1975. </year>
Reference-contexts: In one sense, languages are sets of sentences, or alternatively mappings between sound and meaning; this is the traditional view of the structuralist and descriptivist schools (see, for example, Bloomfield [19] and Lewis <ref> [85] </ref>). Chomsky [41] uses the term E-language (externalized language) to refer to this notion. Viewing languages this way, learning language means to acquire knowledge sufficient to generate and interpret new utterances in the same manner as the rest of the speech community.
Reference: [86] <author> M. Li and P. Vitanyi. </author> <title> Inductive reasoning. </title> <editor> In E. S. Ristad, editor, </editor> <booktitle> Language Computations: DIMACS Series vol. </booktitle> <volume> 17. </volume> <publisher> American Mathematical Society, </publisher> <year> 1991. </year>
Reference-contexts: optimal way, and in no way guarantees that generalization performance is maximized: although results vary by application [97], as is to be expected, practical experience indicates (see [98, 116, 146] and section 6.1.3) that MDL as commonly used tends to underestimate the number of parameters necessary for optimum 5 See <ref> [80, 86, 87, 111, 113, 114] </ref> for attempted justifications of MDL and the closely related Kolmogorov complexity. Other relevant arguments for simplicity as measured by description length include [16, 20, 37, 66, 133]. 40 CHAPTER 3. STOCHASTIC GRAMMARS, MODEL SELECTION AND LANGUAGE ACQ. generalization.
Reference: [87] <author> M. Li and P. Vitanyi. </author> <title> An introduction to Kolmogorov complexity and its applications. </title> <publisher> Springer-Verlag, </publisher> <address> New York, NY, </address> <year> 1993. </year>
Reference-contexts: optimal way, and in no way guarantees that generalization performance is maximized: although results vary by application [97], as is to be expected, practical experience indicates (see [98, 116, 146] and section 6.1.3) that MDL as commonly used tends to underestimate the number of parameters necessary for optimum 5 See <ref> [80, 86, 87, 111, 113, 114] </ref> for attempted justifications of MDL and the closely related Kolmogorov complexity. Other relevant arguments for simplicity as measured by description length include [16, 20, 37, 66, 133]. 40 CHAPTER 3. STOCHASTIC GRAMMARS, MODEL SELECTION AND LANGUAGE ACQ. generalization.
Reference: [88] <author> E. V. M. Lieven. </author> <title> Crosslinguistic and crosscultural aspects of language addressed to children. </title> <editor> In C. Gallaway and B. J. Richards, editors, </editor> <booktitle> Input and Interaction in Language Acquisition, </booktitle> <pages> pages 56-73. </pages> <publisher> Cambridge University Press, </publisher> <address> New York, NY, </address> <year> 1994. </year>
Reference-contexts: Unfortunately, evidence for significant amounts of feedback is tenuous (it is not clear how much is present, or of what sort) and there is little evidence that children rely on it; some cultures do not even direct speech at pre-linguistic infants <ref> [88] </ref>. For this reason, although it is possible that children make use of negative evidence, it appears more promising to look for alternative explanations of learnability. 2.3.2 Side Information Chomsky allows that the learner may have access to structural descriptions as well as sentences.
Reference: [89] <author> X. Luo and S. Roukos. </author> <title> An iterative algorithm to build Chinese language models. </title> <booktitle> In Proc. 34th Annual Meeting of the Association for Computational Linguistics, </booktitle> <pages> pages 139-143, </pages> <address> Santa Cruz, California, </address> <year> 1996. </year>
Reference-contexts: Thus, if applied to the task of learning words from scratch, most of these algorithms would either be inappropriate or suffer from many of the same problems as the algorithms already discussed in the section on grammatical inference. One potential exception to this is Luo and Roukos <ref> [89] </ref>, who learn words in Chinese starting from scratch and use a cross-validation technique to keep from building too-large words. 5.4.6 Search Procedures The search procedures used for grammatical inference and language modeling generally fall into one of two classes. <p> Most of these algorithms attack a slightly differently problem, starting with a lexicon defined by hand-segmentations of text or man-made dictionaries. However, it is interesting to compare results. Sproat et al. [136] make the point (see also Luo and Roukos <ref> [89] </ref>) that it is difficult define "true" segmentations in Chinese: when people are asked to segment sentences into words, their segmentations very often disagree. This reflects the fact that there are many levels of linguistic structure in a sentence, and it can be difficult to define what a "word" is.
Reference: [90] <author> B. MacWhinney and C. Snow. </author> <title> The child language data exchange system. </title> <journal> Journal of Child Language, </journal> <volume> 12 </volume> <pages> 271-296, </pages> <year> 1985. </year> <note> 130 BIBLIOGRAPHY </note>
Reference-contexts: Accuracy It is reasonable to ask whether our algorithm is in some sense cheating by producing a hierarchical structure for each sentence. Does this not make it easy to achieve high recall rates? Is there any 7 Transcriptions of mothers' speech to children taken from the CHILDES database <ref> [90] </ref> and converted to phonemes in a manner that ensures each word is given a consistent transcription. Spaces between words in the original text are used to define word boundaries in the phoneme sequences, but are removed in the evidence presented to the algorithm. 114 CHAPTER 6. <p> The tests are completely artificial in the sense that the meanings presented to the learning algorithm are constructed from the orthography of sentences rather than dervied from real situations. 6.4.1 Input Evidence is constructed from the Nina portion of the CHILDES database <ref> [90, 138] </ref>. This is a set of transcriptions of interactions between a mother and a young child (Nina) over a multi-year period. Only the transcriptions of the mother's speech are used; these amount to approximately 34,000 sentences of English text.
Reference: [91] <author> B. MacWhinney. </author> <title> Conditions on acquisition models. </title> <booktitle> In Proceedings of the Association for Computing Machinery, </booktitle> <pages> pages 421-426, </pages> <year> 1978. </year>
Reference-contexts: One condition not listed above is that a theory should predict learning in the same manner as human beings <ref> [91] </ref>. This is omitted for several reasons. First, in any scientific endeavor some simplifications must be made and relaxing the manner condition is unlikely to alter the fundamental character of the learning problem. Secondly, it is important to understand how language can be learned, irrespective of mechanism.
Reference: [92] <author> G. F. Marcus. </author> <title> Negative evidence in language acquisition. </title> <journal> Cognition, </journal> <volume> 46 </volume> <pages> 53-85, </pages> <year> 1993. </year>
Reference-contexts: In this framework Chomsky is implicitly assuming that learning takes place from positive examples-sample sentences from the target language. This is consistent with Brown and Hanlon's [29] assessment (see also Marcus <ref> [92] </ref>) that children receive no negative evidence, a term that refers to both feedback from the teacher to the learner and negative examples- sentences labeled as outside of the target language.
Reference: [93] <author> A. A. </author> <title> Markov. Example of a statistical investigation of the text of "Eugene Onegin" illustrating the dependence between samples in chain. </title> <journal> Bulletin de l'Academie Imperiale des Sciences de St. </journal> <volume> Petersbourg, </volume> <pages> pages 153-162, </pages> <year> 1913. </year>
Reference-contexts: Stochastic methods have also been applied from very early on. One of the first demonstrations of Markov models <ref> [93] </ref> was an elucidation of the dependencies between adjacent characters in the text of Pushkin's Eugene Onegin. Olivier [102] uses stochastic models in an early computational study of language acquisition.
Reference: [94] <author> D. Marr. </author> <title> Vision. </title> <editor> W. H. </editor> <publisher> Freeman and Company, </publisher> <address> San Francisco, </address> <year> 1982. </year>
Reference-contexts: An understanding of language acquisition must therefore be founded on an understanding of the nature of the input, the form and interpretation of grammars, and the mapping itself. These can each be understood at different levels. For example, Marr <ref> [94] </ref> distinguishes between the broad goals of a computation, the particular representations and algorithms employed, and their hardware implementation. Given our limited understanding of language, computation and cognition, a complete theory of language acquisition at all three levels is presently beyond reach. <p> Or an evolutionary biologist might wish for an abstract characterization that makes plain what classes of language are learnable by any mechanism. Naturally, the ideal situation would be to understand language acquisition at all levels from neural implementation to computational theory <ref> [94] </ref>. As a practical matter such an understanding is beyond current reach. To understand the goals of this thesis, it is necessary to define "knowledge of language" more precisely.
Reference: [95] <author> V. S. Miller and M. N. Wegman. </author> <title> Variations on a theme by Ziv and Lempel. </title> <editor> In A. Apostolico and Z. Galil, editors, </editor> <booktitle> Combinatorial Algorithms on Words, </booktitle> <pages> pages 131-140. </pages> <publisher> Springer-Verlag, </publisher> <address> New York, NY, </address> <year> 1984. </year>
Reference-contexts: This compression technique has been proven to asymptotically approach the entropy of any Markov source [156]. Through the derivational history of words, algorithms like LZW implicitly define a hierarchical structure in the lexicon (in the case of LZW, a left-branching tree). The LZMW algorithm <ref> [95] </ref>, which is like LZW except that the dictionary is built by concatenating two words rather than one word and a character, constructs a hierarchy that is very similar in spirit to our compositional representation.
Reference: [96] <author> A. Moffat. </author> <title> Implementing the PPM data compression scheme. </title> <journal> IEEE Transactions on Communications, </journal> <volume> 38(11) </volume> <pages> 1917-1921, </pages> <year> 1990. </year>
Reference-contexts: Popular text compression schemes can be divided into four classes: those based on adaptive frequency techniques like Huffman codes; those based on context models [112] (such as the PPM algorithm <ref> [42, 96, 139] </ref>, probably the most effective widely-used method for text compression); those based on hidden Markov models [13] (these are less common); and those based on dictionary methods. <p> Figure 6.4 presents results over the corpus, compared with the LZ78-based [147, 156] compress program, the LZ77-based [155] gzip program, Sequitur [99] and a PPM-based program <ref> [42, 96] </ref>. The performance figures for other programs are taken from Nevill-Manning [99]. 3 This is not exactly true. Were the parameter not in the lexicon, each of its components would have higher counts, and thus slightly shorter codewords. 106 CHAPTER 6.
Reference: [97] <author> P. M. Murphy. </author> <title> An empirical analysis of the benefit of decision tree size biases as a function of concept distribution. </title> <type> Technical Report 95-29, </type> <institution> Department of Information and Computer Science, University of California, Irvine, </institution> <year> 1995. </year>
Reference-contexts: Despite its motivations, it does not trade VC-dimension against evidence in the theoretically optimal way, and in no way guarantees that generalization performance is maximized: although results vary by application <ref> [97] </ref>, as is to be expected, practical experience indicates (see [98, 116, 146] and section 6.1.3) that MDL as commonly used tends to underestimate the number of parameters necessary for optimum 5 See [80, 86, 87, 111, 113, 114] for attempted justifications of MDL and the closely related Kolmogorov complexity.
Reference: [98] <author> P. M. Murphy and M. J. Pazzani. </author> <title> Exploring the decision forest: an empirical investigation of Occam's razor in decision-tree induction. </title> <journal> Journal of Artificial Intelligence Research, </journal> <volume> 1 </volume> <pages> 257-275, </pages> <year> 1994. </year>
Reference-contexts: Despite its motivations, it does not trade VC-dimension against evidence in the theoretically optimal way, and in no way guarantees that generalization performance is maximized: although results vary by application [97], as is to be expected, practical experience indicates (see <ref> [98, 116, 146] </ref> and section 6.1.3) that MDL as commonly used tends to underestimate the number of parameters necessary for optimum 5 See [80, 86, 87, 111, 113, 114] for attempted justifications of MDL and the closely related Kolmogorov complexity.
Reference: [99] <author> C. G. Nevill-Manning. </author> <title> Inferring Sequential Structure. </title> <type> PhD thesis, </type> <institution> Department of Computer Science, University of Waikato, </institution> <address> New Zealand, </address> <year> 1996. </year>
Reference-contexts: In fact, in one of the earliest empirical works in natural language grammar induction, Olivier [102] built an algorithm very similar to LZMW, and its failings were a principal motivation for this thesis. Nevill-Manning's Sequitur Recently Nevill-Manning <ref> [99] </ref> has described Sequitur, a text compression algorithm with remarkable similarities to our concatenative algorithm, also motivated in part by arguments related to language acquisition. Sequitur constructs a deterministic context-free grammar that generates the input. <p> Figure 5.4 presents a trace of Sequitur's execution on the input abcdbcabcd, taken from Nevill-Manning <ref> [99] </ref>. As should be clear, the end result is a grammar that is similar to the representations our algorithm would produce. There are several key differences between Sequitur and our algorithm. <p> Members of the first class, found here and in the work of Olivier [102], Cook et al. [43] Wolff [150, 151], Ellison [57, 58], Nevill-Manning <ref> [99] </ref>, Cartwright and Brent [32], Chen [35, 36], Stol-cke [137] and others, iteratively update the underlying structure of the grammar. (Some, like our algorithms, start with the most general grammar possible while others, like Stolcke's, start with the most specific grammar possible.) Members of the second class, exemplified by the work <p> Figure 6.4 presents results over the corpus, compared with the LZ78-based [147, 156] compress program, the LZ77-based [155] gzip program, Sequitur <ref> [99] </ref> and a PPM-based program [42, 96]. The performance figures for other programs are taken from Nevill-Manning [99]. 3 This is not exactly true. Were the parameter not in the lexicon, each of its components would have higher counts, and thus slightly shorter codewords. 106 CHAPTER 6. <p> Figure 6.4 presents results over the corpus, compared with the LZ78-based [147, 156] compress program, the LZ77-based [155] gzip program, Sequitur <ref> [99] </ref> and a PPM-based program [42, 96]. The performance figures for other programs are taken from Nevill-Manning [99]. 3 This is not exactly true. Were the parameter not in the lexicon, each of its components would have higher counts, and thus slightly shorter codewords. 106 CHAPTER 6. <p> It seems therefore that our algorithm performs substantially better. Wolff [149, 150] presents a word-learning algorithm and applies it to English and pseudo-English text, but does not provide results in a manner suitable for comparison; however, experiments performed by Nevill-Manning <ref> [99] </ref> indicate that Wolff's algorithms are not competitive. Finally, Nevill-Manning [99] applies his Sequitur algorithm to English text (with spaces) but reports results in a manner incomparable with those presented here. <p> Wolff [149, 150] presents a word-learning algorithm and applies it to English and pseudo-English text, but does not provide results in a manner suitable for comparison; however, experiments performed by Nevill-Manning <ref> [99] </ref> indicate that Wolff's algorithms are not competitive. Finally, Nevill-Manning [99] applies his Sequitur algorithm to English text (with spaces) but reports results in a manner incomparable with those presented here. From the sample hierarchical structures he provides it appears that his algorithm performs well, but has a lower recall rate and a substantially higher rate of crossing-brackets.
Reference: [100] <author> P. Niyogi. </author> <title> The Informational Complexity of Learning from Examples. </title> <type> PhD thesis, </type> <institution> Massachusetts Institute of Technology, Cambridge, Massachusetts, </institution> <year> 1995. </year>
Reference-contexts: Niyogi <ref> [100] </ref> explores this idea in more mathematical detail, also with respect to language acquisition; see also literature on the bias-variance tradeoff [21, 60]. At face value structural risk minimization seems to be irrelevant to the language acquisition problem.
Reference: [101] <author> P. Niyogi and R. C. Berwick. </author> <title> A Markov language learning model for finite parameter spaces. </title> <booktitle> In Proc. 32nd Annual Meeting of the Association for Computational Linguistics, </booktitle> <pages> pages 171-180, </pages> <address> Las Cruces, New Mexico, </address> <year> 1994. </year>
Reference-contexts: Little can be said about the nature of the cues without reference to the details of the parameter system; for any change in the model of stress the feasibility of a cue-based strategy must be re-justified. In contrast, Gibson and Wexler's [61] simpler "TLA" parameter-setting algorithm is easily analyzed <ref> [101] </ref>, though its success is similarly dependent on the structure of the parameter system. 2.3 The Nature of the Input In section 2.2.3 it was argued that theories of language acquisition should be built up from the evidence that is available to the learner.
Reference: [102] <author> D. C. Olivier. </author> <title> Stochastic Grammars and Language Acquisition Mechanisms. </title> <type> PhD thesis, </type> <institution> Harvard University, Cambridge, Massachusetts, </institution> <year> 1968. </year>
Reference-contexts: This fact is one of the primary reasons that unsupervised learning schemes can be successful at elucidating linguistic structure. Extralinguistic patterns have been the downfall of many computational theories of language acquisition, that have modeled them at the expense of linguistic ones (see for example Olivier <ref> [102] </ref> and Cartwright and Brent [32]). In chapter 4 a representation for language is presented that does not prevent extralin-guistic patterns from making their way into the grammar, but does ensure that they do not preclude desired parameters. 3.2. <p> Stochastic methods have also been applied from very early on. One of the first demonstrations of Markov models [93] was an elucidation of the dependencies between adjacent characters in the text of Pushkin's Eugene Onegin. Olivier <ref> [102] </ref> uses stochastic models in an early computational study of language acquisition. <p> As an extreme example, consider a case in which the learner's evidence U is a sequence of utterances U 0 6 Many other statistical induction schemes used for language inference have suffered from this problem; for examples see the results of Olivier <ref> [102] </ref> and Cartwright and Brent [32]. <p> Thus, researchers in grammatical inference often directly evaluate grammars (or grammatical derivations) rather than the languages generated by a grammar or a grammar's predictive ability. This line of research has lead to many approaches that are similar to ours. For example, Olivier <ref> [102] </ref>, Wolff [149, 150, 151], Brent et al. [24], and Cartwright and Brent [23, 32] all present algorithms for the induction of word-like linguistic units from character and phoneme sequences; these algorithms all rely on dictionary-based representations similar to our multigram model (though usually no stochastic interpretation is assigned). <p> However, because these algorithms do not iteratively restructure the dictionary and rely on greedy on-line parsing strategies, the lexical hierarchies they generate do not agree very well with linguistic intuitions. In fact, in one of the earliest empirical works in natural language grammar induction, Olivier <ref> [102] </ref> built an algorithm very similar to LZMW, and its failings were a principal motivation for this thesis. Nevill-Manning's Sequitur Recently Nevill-Manning [99] has described Sequitur, a text compression algorithm with remarkable similarities to our concatenative algorithm, also motivated in part by arguments related to language acquisition. <p> specific (Sproat et al. [136] and Chang et al. [33] discuss methods for learning Chinese names that are based on their idiosyncratic properties, and Chang et al. [34] judge new Chinese words by their similarity to existing words) or very similar to the more general lexical induction schemes of Olivier <ref> [102] </ref>, Cartwright and Brent [32], etc. Thus, if applied to the task of learning words from scratch, most of these algorithms would either be inappropriate or suffer from many of the same problems as the algorithms already discussed in the section on grammatical inference. <p> Members of the first class, found here and in the work of Olivier <ref> [102] </ref>, Cook et al. [43] Wolff [150, 151], Ellison [57, 58], Nevill-Manning [99], Cartwright and Brent [32], Chen [35, 36], Stol-cke [137] and others, iteratively update the underlying structure of the grammar. (Some, like our algorithms, start with the most general grammar possible while others, like Stolcke's, start with the most <p> Furthermore, direct comparison is impossible given that our algorithm produces hierarchical segmentations whereas most other algorithms produce only a single level of structure. Olivier <ref> [102] </ref> presents an on-line word-learning algorithm and applies it to 288,000 characters of unsegmented (spaceless), lower-case English text taken from the nomination speeches of major-party presidential nominees between 1928 (Al Smith) and 1960 (Richard Nixon). The algorithm achieves a peak word recall rate of about 80%.
Reference: [103] <author> R. C. </author> <title> Pasco. Source Coding Algorithms for Fast Data Compression. </title> <type> PhD thesis, </type> <institution> Stanford University, Stanford, California, </institution> <year> 1976. </year>
Reference-contexts: But in the ideal situations codewords are chosen according to the the equation l (w) = log p (w), which does not in general imply integral length codewords. In fact codes can be designed that circumvent this problem (arithmetic codes <ref> [103, 110] </ref> are a practical solution). But again, since the principal purpose of codes here is to compute description lengths for use in the already heuristic MDL criterion, it is much more convenient to simply ignore details of code construction and use log p (w) directly in description length computations.
Reference: [104] <author> F. Pereira and Y. Schabes. </author> <title> Inside-outside reestimation from partially bracketed corpora. </title> <booktitle> In Proc. 30th Annual Meeting of the Association for Computational Linguistics, </booktitle> <pages> pages 128-135, </pages> <address> Berkeley, California, </address> <year> 1992. </year>
Reference-contexts: Efficiency, convergence, robustness and other properties of learning mechanisms all indirectly bear on other parts of the learning framework. For example, there is significant evidence that the known induction algorithms for certain classes of grammars (such as stochastic context-free grammars <ref> [31, 48, 104] </ref>) are systematically incapable of learning linguistically relevant languages; this reflects back on the appropriateness of the grammar class as a model of human language. <p> A natural way to estimate stochastic parameters for a language model is to find the parameters that maximize the likelihood of the observed evidence; this puts each grammar in its best possible light with respect to equation 3.1. Empirical tests <ref> [31, 48, 104] </ref> using various naive classes of stochastic grammars indicate that the stochastic grammars that maximize the probability of linguistic evidence do not in general have "linguistically plausible" structure. <p> This is an advantage of algorithms that manipulate the structure of the grammar over algorithms that start with complete structures (for example, all possible grammatical rules) and attempt to learn solely by manipulating stochastic properties <ref> [25, 104] </ref>. Only rarely will the EM-algorithm converge to the exact (local) optimum in a finite number of iterations. However, for the implementations considered below, after two or three iterations improvements to the description length tend to be so small as to be irrelevant. <p> The reasons behind the failures harken back to the discussions of chapter 3: extralinguistic patterns are learned at the expense of linguistic ones and words are made long in an effort to improve stochastic models. Much recent work has focused on the induction of context-free grammars or variations thereof <ref> [8, 25, 31, 30, 35, 36, 43, 83, 104, 137] </ref>. The hierarchical nature of these grammars would seem on the surface to be quite similar to our hierarchical, concatenative representation. However, algorithms designed for the induction of context-free grammars have not performed well in practice. <p> The hierarchical nature of these grammars would seem on the surface to be quite similar to our hierarchical, concatenative representation. However, algorithms designed for the induction of context-free grammars have not performed well in practice. Pereira and Schabes <ref> [104] </ref> attempt to learn an English grammar by applying the inside-outside algorithm [8] (the EM-algorithm for stochastic context-free grammars) to a grammar that contains all possible binary rules over a fixed set of nonterminals and terminal parts-of-speech. <p> Chen [35, 36], Stol-cke [137] and others, iteratively update the underlying structure of the grammar. (Some, like our algorithms, start with the most general grammar possible while others, like Stolcke's, start with the most specific grammar possible.) Members of the second class, exemplified by the work of Pereira and Schabes <ref> [104] </ref>, Deligne and Bimbot [51], Briscoe and Waegner [25] and Lari and Young [83], pick an extremely general structural backbone for a stochastic model, and proceed by optimizing its stochastic properties, usually through the EM procedure. <p> In general, the second class of learning algorithms has fared more poorly than the first. The reason, as discussed by Pereira and Schabes <ref> [104] </ref> and de Marcken [48], is that the hill-climbing inside-outside algorithm is incapable of making the complex moves in grammar-space necessary to escape local optima. As a consequence, these learning algorithms quickly get stuck near their starting point, with little learning having taken place.
Reference: [105] <author> S. Pinker. </author> <title> The Language Instinct. </title> <publisher> William Morrow and Company, </publisher> <address> New York, </address> <year> 1994. </year>
Reference-contexts: Roughly speaking, phonological distinctions, syllable structure and other information concerning sound patterns are learned early [72, 73], followed later by words and syntax <ref> [65, 105] </ref>. But such facts shed little insight into the character of the process that maps evidence to grammar.
Reference: [106] <author> L. Pitt. </author> <title> Inductive inference, DFAs, and computational complexity. </title> <type> Technical Report UIUCDCS-R-89-1530, </type> <institution> University of Illinois at Urbana-Champaign, Urbana, Illinois, </institution> <year> 1989. </year>
Reference-contexts: The literature on the induction of finite-state automata has traditionally been divided. On the one hand there has been a great deal of study put into the induction of non-stochastic finite-state automata from examples; see Pitt <ref> [106] </ref> for a survey.
Reference: [107] <author> L. R. Rabiner. </author> <title> A tutorial on hidden markov models and selected applications in speech recognition. </title> <booktitle> Proceedings of the IEEE, </booktitle> <volume> 77(2) </volume> <pages> 257-286, </pages> <year> 1989. </year> <note> BIBLIOGRAPHY 131 </note>
Reference-contexts: This memory extends at most the length of the longest parameter. As a finite-state model, it has obvious similarities to other finite-state models such as Markov models (MMs) and hidden Markov models (HMMs) <ref> [13, 107] </ref>. Both Markov models and hidden Markov models define a stochastic model on top of a finite state machine, where the state q i of the system at time i is drawn from a finite set Q.
Reference: [108] <author> L. R. Rabiner and B.-H. Juang. </author> <title> Fundamentals of Speech Recognition. </title> <publisher> Prentice Hall, </publisher> <address> Englewood Cliffs, NJ, </address> <year> 1993. </year>
Reference-contexts: To take just one example, the phonological representations that are the basis for speech production are rooted in coarse articulatory gestures [67] like tongue movements that have complex and sometimes subtle affects on the acoustic signal <ref> [108] </ref>. For this reason, it is extremely difficult to determine the motor commands that produced a signal. <p> Indeed, one of the principal components of any automatic speech recognition device is a highly restrictive language model that attempts to filter possible word sequences on the basis of language-specific usage patterns <ref> [108] </ref>. This leaves open the possibility that parameter values can be easily determined from extralinguistic input, such as the way a mother wiggles her eyebrows, or (more plausibly) the manner in which she emphasizes different parts of the speech signal. <p> Tests of the transcriber on the TIMIT test data put phoneme recall at 55.5% and phoneme accuracy at 68.7%. These numbers were computed by 8 See Rabiner and Huang <ref> [108] </ref> for an introduction to the methods of automatic speech recognition. In a triphone-based speech recognizer, speech production is modeled as a three-stage process. First a phoneme sequence is generated. In our model phonemes are generated independently under a uniform distribution.
Reference: [109] <author> M. Rayner, A. sa Hugosson, and G. Hagert. </author> <title> Using a logic grammar to learn a lexicon. </title> <type> Technical Report R88001, </type> <institution> Swedish Institute of Computer Science, </institution> <year> 1988. </year>
Reference-contexts: THEORIES OF LANGUAGE ACQUISITION 23 Paper Assumes Input Output Anderson 1977 [1] I,NN,NH MI,WM,SM G,WS Anderson 1981 [2] I,NN,NH MI,WM,SM G,WS,MPHR Berwick 1985 [16] I,NN,OCWS WI,WM,TR G,WS Brent 1993 [22] WW MPH,MPHR Gibson & Wexler 1994 [61] NN P,TR G Kazman 1994 [75] I,NH WI,WM,WS G,MPH,MPHR Rayner et al. 1988 <ref> [109] </ref> I,FG,NN,NH WI WS Selfridge 1981 [121] I,NN,NH WI,SM WM Siklossy 1972 [125] I,NN,NH WI,SM WM Siskind 1992 [126] I,NN,NH WI,SM G,WS,WM Siskind 1994 [128] I WI,SM WM Assumptions FG Grammar fixed in program. NN No noise or inconsistent input. NH No homonymy: each identifier has a single interpretation.
Reference: [110] <author> J. Rissanen. </author> <title> Generalized Kraft inequality and arithmetic coding. </title> <journal> IBM Journal of Research and Development, </journal> <volume> 20, </volume> <year> 1976. </year>
Reference-contexts: But in the ideal situations codewords are chosen according to the the equation l (w) = log p (w), which does not in general imply integral length codewords. In fact codes can be designed that circumvent this problem (arithmetic codes <ref> [103, 110] </ref> are a practical solution). But again, since the principal purpose of codes here is to compute description lengths for use in the already heuristic MDL criterion, it is much more convenient to simply ignore details of code construction and use log p (w) directly in description length computations.
Reference: [111] <author> J. Rissanen. </author> <title> Modeling by shortest data description. </title> <journal> Automatica, </journal> <volume> 14 </volume> <pages> 465-471, </pages> <year> 1978. </year>
Reference-contexts: Both of these quantities are extremely difficult if not impossible to compute in practice. For this reason, heuristic approximations must be used in place of structural risk minimization. One effective heuristic is Rissanen's minimum description length (MDL) principle <ref> [111, 113, 114] </ref>, in which description length is used as a substitute for informational complexity measures like the VC-dimension. The minimum description length principle, as applied to stochastic grammars, says that the best grammar G minimizes the combined description length of the grammar and the evidence. <p> optimal way, and in no way guarantees that generalization performance is maximized: although results vary by application [97], as is to be expected, practical experience indicates (see [98, 116, 146] and section 6.1.3) that MDL as commonly used tends to underestimate the number of parameters necessary for optimum 5 See <ref> [80, 86, 87, 111, 113, 114] </ref> for attempted justifications of MDL and the closely related Kolmogorov complexity. Other relevant arguments for simplicity as measured by description length include [16, 20, 37, 66, 133]. 40 CHAPTER 3. STOCHASTIC GRAMMARS, MODEL SELECTION AND LANGUAGE ACQ. generalization.
Reference: [112] <author> J. Rissanen. </author> <title> A universal data compression system. </title> <journal> IEEE Transactions on Information Theory, </journal> <volume> 29(5) </volume> <pages> 656-664, </pages> <year> 1983. </year>
Reference-contexts: In Markov models the state q i is defined by recent outputs, q i = o im : : : o i1 , where m is the order of the Markov model (more general context models <ref> [13, 112] </ref> select the context o im : : : o i1 from among a set of variable-length suffixes). <p> Bell et al. [13] provide an excellent introduction to the problems and methods of compression, and in particular, text compression. Popular text compression schemes can be divided into four classes: those based on adaptive frequency techniques like Huffman codes; those based on context models <ref> [112] </ref> (such as the PPM algorithm [42, 96, 139], probably the most effective widely-used method for text compression); those based on hidden Markov models [13] (these are less common); and those based on dictionary methods.
Reference: [113] <author> J. Rissanen. </author> <title> Stochastic Complexity in Statistical Inquiry. </title> <publisher> World Scientific, </publisher> <address> Singapore, </address> <year> 1989. </year>
Reference-contexts: Both of these quantities are extremely difficult if not impossible to compute in practice. For this reason, heuristic approximations must be used in place of structural risk minimization. One effective heuristic is Rissanen's minimum description length (MDL) principle <ref> [111, 113, 114] </ref>, in which description length is used as a substitute for informational complexity measures like the VC-dimension. The minimum description length principle, as applied to stochastic grammars, says that the best grammar G minimizes the combined description length of the grammar and the evidence. <p> optimal way, and in no way guarantees that generalization performance is maximized: although results vary by application [97], as is to be expected, practical experience indicates (see [98, 116, 146] and section 6.1.3) that MDL as commonly used tends to underestimate the number of parameters necessary for optimum 5 See <ref> [80, 86, 87, 111, 113, 114] </ref> for attempted justifications of MDL and the closely related Kolmogorov complexity. Other relevant arguments for simplicity as measured by description length include [16, 20, 37, 66, 133]. 40 CHAPTER 3. STOCHASTIC GRAMMARS, MODEL SELECTION AND LANGUAGE ACQ. generalization.
Reference: [114] <author> J. Rissanen and E. S. Ristad. </author> <title> Language acquisition in the MDL framework. </title> <editor> In E. S. Ristad, editor, </editor> <booktitle> Language Computations: DIMACS Series vol. </booktitle> <volume> 17. </volume> <publisher> American Mathematical Society, </publisher> <year> 1991. </year>
Reference-contexts: Both of these quantities are extremely difficult if not impossible to compute in practice. For this reason, heuristic approximations must be used in place of structural risk minimization. One effective heuristic is Rissanen's minimum description length (MDL) principle <ref> [111, 113, 114] </ref>, in which description length is used as a substitute for informational complexity measures like the VC-dimension. The minimum description length principle, as applied to stochastic grammars, says that the best grammar G minimizes the combined description length of the grammar and the evidence. <p> optimal way, and in no way guarantees that generalization performance is maximized: although results vary by application [97], as is to be expected, practical experience indicates (see [98, 116, 146] and section 6.1.3) that MDL as commonly used tends to underestimate the number of parameters necessary for optimum 5 See <ref> [80, 86, 87, 111, 113, 114] </ref> for attempted justifications of MDL and the closely related Kolmogorov complexity. Other relevant arguments for simplicity as measured by description length include [16, 20, 37, 66, 133]. 40 CHAPTER 3. STOCHASTIC GRAMMARS, MODEL SELECTION AND LANGUAGE ACQ. generalization.
Reference: [115] <author> E. S. Ristad. </author> <title> The Language Complexity Game. </title> <publisher> MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1994. </year>
Reference-contexts: In a similar vein, Ristad, Barton and Berwick <ref> [9, 115] </ref> have argued that many theories can be dismissed on the basis of their computational complexity. Such deficiencies usually become apparent immediately upon implementation. * In the course of applying algorithms and representations to real input, the most significant "problems" of language learning are identified.
Reference: [116] <author> E. S. Ristad and R. G. Thomas. </author> <title> New techniques for context modeling. </title> <booktitle> In Proc. 33rd Annual Meeting of the Association for Computational Linguistics, </booktitle> <address> Cambridge, Massachusetts, </address> <year> 1995. </year>
Reference-contexts: Despite its motivations, it does not trade VC-dimension against evidence in the theoretically optimal way, and in no way guarantees that generalization performance is maximized: although results vary by application [97], as is to be expected, practical experience indicates (see <ref> [98, 116, 146] </ref> and section 6.1.3) that MDL as commonly used tends to underestimate the number of parameters necessary for optimum 5 See [80, 86, 87, 111, 113, 114] for attempted justifications of MDL and the closely related Kolmogorov complexity. <p> In Markov models and hidden Markov models each state is given equal status; potential linguistic boundaries can only be defined by ad-hoc functions applied to transition probabilities. Furthermore, for language modeling applications multigrams often have much smaller representations than equivalent MMs and HMMs. For example, Ristad and Thomas <ref> [116] </ref> use the MDL criterion to learn a context model. The equivalent information found in the single multigram parameter Mississippi is in their context model captured by many parameters: 4.4. <p> In contrast, Brent et al. [24], Cartwright and Brent [32], Chen [35, 36], Ristad and Thomas <ref> [116] </ref> and others that invoke the MDL principle all compute ad hoc estimates of description length (often based on symbol counts) that do not closely reflect the best possible encodings of their grammars (Stolcke [137] is more careful). <p> The algorithm achieves a cross-entropy rate of 1.97 bits/char on a portion of the Brown corpus not used for training (though a portion fairly similar to the training data). This happens to be the same rate achieved by Ristad and Thomas <ref> [116] </ref> using a context model on the same data. The best rate over the entire Brown corpus, achieved by Brown et al. [28] with a trigram Markov model over words, is 1.75 bits/char.
Reference: [117] <author> J. Sachs and M. Johnson. </author> <title> Language development in a hearing child of deaf parents. </title> <booktitle> In International Symposium on First Language Acquisition, </booktitle> <address> Florence, Italy, </address> <year> 1972. </year>
Reference-contexts: Indeed, much work on syntactic acquisition has assumed that the thematic roles of noun phrases are known to the learner [61]. Finally, it has been shown that children do not learn much, if anything, from sound patterns in isolation <ref> [117, 130] </ref>; some environmental clues are probably necessary for learning.
Reference: [118] <author> Y. Sakakibara. </author> <title> Efficient learning of context-free grammars from positive structural examples. </title> <journal> Information and Control, </journal> <volume> 97(1) </volume> <pages> 23-60, </pages> <month> Mar. </month> <year> 1992. </year>
Reference-contexts: Providing the learner with linguistically structured information like syntactic trees or semantic formulae can trivialize the learning process, by making the grammar explicit in the input. Some recent papers argue that there are powerful classes of languages identifiable from positive data alone <ref> [74, 118, 124] </ref>; these learnability proofs assume access to structural descriptions in the input. Sakakibara [118], for example, has shown that a significant subset of context-free languages (those generated by reversible context-free grammars) are identifiable from positive data, if the example sentences come structured into unlabeled derivation trees. <p> Some recent papers argue that there are powerful classes of languages identifiable from positive data alone [74, 118, 124]; these learnability proofs assume access to structural descriptions in the input. Sakakibara <ref> [118] </ref>, for example, has shown that a significant subset of context-free languages (those generated by reversible context-free grammars) are identifiable from positive data, if the example sentences come structured into unlabeled derivation trees.
Reference: [119] <author> C. Schaffer. </author> <title> A conservation law for generalization performance. </title> <booktitle> In Proc. of the 1994 International Conference on Machine Learning, </booktitle> <year> 1994. </year>
Reference-contexts: Thus, the prior can be used to manipulate generalization performance. However, Wolpert and others <ref> [119, 152] </ref> have shown that unless assumptions are made about the learning problem, no generalization strategy (and hence no prior) performs better than any other.
Reference: [120] <author> A.-M. D. Sciullo and E. Williams. </author> <title> On the Definition of Word. </title> <publisher> MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1987. </year>
Reference-contexts: As a matter of convenience the word word will often be used to refer to any lexical parameter, though a more proper term would be listeme (defined by Di Sciullo and Williams <ref> [120] </ref> as an item that must be memorized). Listemes include morphemes, many syntactic words, idioms, and perhaps syllables. Here even syntactic rules are treated as part of the lexicon, if there is reason to believe that they are memorized.
Reference: [121] <author> M. Selfridge. </author> <title> A computer model of child language acquisition. </title> <booktitle> In Proc. of the 7th International Joint Conference on Artificial Intelligence, </booktitle> <pages> pages 92-96, </pages> <address> Vancouver, B.C., Canada, </address> <year> 1981. </year>
Reference-contexts: Assumes Input Output Anderson 1977 [1] I,NN,NH MI,WM,SM G,WS Anderson 1981 [2] I,NN,NH MI,WM,SM G,WS,MPHR Berwick 1985 [16] I,NN,OCWS WI,WM,TR G,WS Brent 1993 [22] WW MPH,MPHR Gibson & Wexler 1994 [61] NN P,TR G Kazman 1994 [75] I,NH WI,WM,WS G,MPH,MPHR Rayner et al. 1988 [109] I,FG,NN,NH WI WS Selfridge 1981 <ref> [121] </ref> I,NN,NH WI,SM WM Siklossy 1972 [125] I,NN,NH WI,SM WM Siskind 1992 [126] I,NN,NH WI,SM G,WS,WM Siskind 1994 [128] I WI,SM WM Assumptions FG Grammar fixed in program. NN No noise or inconsistent input. NH No homonymy: each identifier has a single interpretation. <p> This work includes studies of language acquisition (see Selfridge <ref> [121] </ref>, Siklossy [125] and Siskind [126, 127, 128]); parameter estimation schemes for machine translation, where sentences in a second language substitute for semantic input (see Brown et al. [26] and Berger et al. [15]); and parameter estimation schemes for systems that classify utterances (see Tishby and Gorin [140]).
Reference: [122] <author> C. E. Shannon. </author> <title> A mathematical theory of communication. </title> <journal> Bell System Technical Journal, </journal> <volume> 27 </volume> <pages> 623-656, </pages> <year> 1948. </year>
Reference-contexts: GENERALIZATION, MODEL SELECTION AND THE PRIOR 39 ing of U given knowledge of the grammar G 0 . Using near-optimal coding schemes, Shannon's source coding theorem <ref> [122] </ref> implies that jU j G 0 can be made to closely approach log p (U jG 0 ), and therefore equation 3.3 can be rewritten G = argmin G 0 2G a more intuitive formulation from the standpoint of stochastic grammars. <p> In this way, parameters are favored if they can be explained in terms of expected historical processes. 4.3 Coding As has been mentioned many times, codes and probabilities are fundamentally and simply related by Shannon's source coding theorem <ref> [122] </ref>, which says that a code can be designed for a distribution such that the expected description length under the code is almost exactly the entropy of the distribution; as a practical matter this implies that a code can be designed such that the length of a description of u almost
Reference: [123] <author> C. E. Shannon. </author> <title> Prediction and entropy of printed English. </title> <journal> Bell System Technical Journal, </journal> <volume> 30 </volume> <pages> 50-64, </pages> <year> 1951. </year>
Reference-contexts: This upper bound on the "true" entropy of English (or at least of the Brown corpus) is significantly closer to the rates of 1.3 and 1.25 bits/char achieved by human subjects as tested by Shannon <ref> [123] </ref> and Cover and King [44]. 4 However, that result came after training on almost 600 million words of text, starting with substantial knowledge of language. The resulting model would have dwarfed the Brown corpus in size, and hence is difficult to compare with a compression algorithm.
Reference: [124] <author> T. Shinohara. </author> <title> Inductive inference from positive data is powerful. </title> <booktitle> In 1990 Workshop on Computational Learning Theory, </booktitle> <pages> pages 97-110, </pages> <address> Los Altos, CA, 1990. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Providing the learner with linguistically structured information like syntactic trees or semantic formulae can trivialize the learning process, by making the grammar explicit in the input. Some recent papers argue that there are powerful classes of languages identifiable from positive data alone <ref> [74, 118, 124] </ref>; these learnability proofs assume access to structural descriptions in the input. Sakakibara [118], for example, has shown that a significant subset of context-free languages (those generated by reversible context-free grammars) are identifiable from positive data, if the example sentences come structured into unlabeled derivation trees.
Reference: [125] <author> L. Siklossy. </author> <title> Natural language learning by computer. </title> <editor> In H. A. Simon and L. Siklossy, editors, </editor> <booktitle> Representation and Meaning, </booktitle> <pages> pages 289-327. </pages> <publisher> Prentice Hall, </publisher> <address> Englewood Cliffs, NJ, </address> <year> 1972. </year>
Reference-contexts: I,NN,NH MI,WM,SM G,WS Anderson 1981 [2] I,NN,NH MI,WM,SM G,WS,MPHR Berwick 1985 [16] I,NN,OCWS WI,WM,TR G,WS Brent 1993 [22] WW MPH,MPHR Gibson & Wexler 1994 [61] NN P,TR G Kazman 1994 [75] I,NH WI,WM,WS G,MPH,MPHR Rayner et al. 1988 [109] I,FG,NN,NH WI WS Selfridge 1981 [121] I,NN,NH WI,SM WM Siklossy 1972 <ref> [125] </ref> I,NN,NH WI,SM WM Siskind 1992 [126] I,NN,NH WI,SM G,WS,WM Siskind 1994 [128] I WI,SM WM Assumptions FG Grammar fixed in program. NN No noise or inconsistent input. NH No homonymy: each identifier has a single interpretation. OCWS Syntactic roles of open class words are known. <p> This work includes studies of language acquisition (see Selfridge [121], Siklossy <ref> [125] </ref> and Siskind [126, 127, 128]); parameter estimation schemes for machine translation, where sentences in a second language substitute for semantic input (see Brown et al. [26] and Berger et al. [15]); and parameter estimation schemes for systems that classify utterances (see Tishby and Gorin [140]).
Reference: [126] <author> J. M. Siskind. </author> <title> Naive physics, event perception, lexical semantics, and language acquisition. </title> <type> PhD thesis TR-1456, </type> <institution> MIT Artificial Intelligence Lab., </institution> <year> 1992. </year>
Reference-contexts: I,NN,NH MI,WM,SM G,WS,MPHR Berwick 1985 [16] I,NN,OCWS WI,WM,TR G,WS Brent 1993 [22] WW MPH,MPHR Gibson & Wexler 1994 [61] NN P,TR G Kazman 1994 [75] I,NH WI,WM,WS G,MPH,MPHR Rayner et al. 1988 [109] I,FG,NN,NH WI WS Selfridge 1981 [121] I,NN,NH WI,SM WM Siklossy 1972 [125] I,NN,NH WI,SM WM Siskind 1992 <ref> [126] </ref> I,NN,NH WI,SM G,WS,WM Siskind 1994 [128] I WI,SM WM Assumptions FG Grammar fixed in program. NN No noise or inconsistent input. NH No homonymy: each identifier has a single interpretation. OCWS Syntactic roles of open class words are known. I Identity: words or morphemes are given unique identifiers. <p> Similarly, various algorithms have been constructed that use artificial semantic representations to aid the acquisition of syntax <ref> [126, 128] </ref> and the phonological lexicon [47]. Indeed, much work on syntactic acquisition has assumed that the thematic roles of noun phrases are known to the learner [61]. <p> This work includes studies of language acquisition (see Selfridge [121], Siklossy [125] and Siskind <ref> [126, 127, 128] </ref>); parameter estimation schemes for machine translation, where sentences in a second language substitute for semantic input (see Brown et al. [26] and Berger et al. [15]); and parameter estimation schemes for systems that classify utterances (see Tishby and Gorin [140]).
Reference: [127] <author> J. M. Siskind. </author> <title> Lexical acquisition as constraint satisfaction. </title> <type> Technical Report IRCS-93-41, </type> <institution> University of Pennsylvania Institute for Research in Cognitive Science, </institution> <address> Philadelphia, Pennsylvania, </address> <year> 1993. </year> <note> 132 BIBLIOGRAPHY </note>
Reference-contexts: This work includes studies of language acquisition (see Selfridge [121], Siklossy [125] and Siskind <ref> [126, 127, 128] </ref>); parameter estimation schemes for machine translation, where sentences in a second language substitute for semantic input (see Brown et al. [26] and Berger et al. [15]); and parameter estimation schemes for systems that classify utterances (see Tishby and Gorin [140]).
Reference: [128] <author> J. M. Siskind. </author> <title> Lexical acquisition in the presence of noise and homonymy. </title> <booktitle> In Proc. of the American Association for Artificial Intelligence, </booktitle> <address> Seattle, Washington, </address> <year> 1994. </year>
Reference-contexts: I,NN,OCWS WI,WM,TR G,WS Brent 1993 [22] WW MPH,MPHR Gibson & Wexler 1994 [61] NN P,TR G Kazman 1994 [75] I,NH WI,WM,WS G,MPH,MPHR Rayner et al. 1988 [109] I,FG,NN,NH WI WS Selfridge 1981 [121] I,NN,NH WI,SM WM Siklossy 1972 [125] I,NN,NH WI,SM WM Siskind 1992 [126] I,NN,NH WI,SM G,WS,WM Siskind 1994 <ref> [128] </ref> I WI,SM WM Assumptions FG Grammar fixed in program. NN No noise or inconsistent input. NH No homonymy: each identifier has a single interpretation. OCWS Syntactic roles of open class words are known. I Identity: words or morphemes are given unique identifiers. Inputs WW Sequence of separated written words. <p> Similarly, various algorithms have been constructed that use artificial semantic representations to aid the acquisition of syntax <ref> [126, 128] </ref> and the phonological lexicon [47]. Indeed, much work on syntactic acquisition has assumed that the thematic roles of noun phrases are known to the learner [61]. <p> It is generally compositional (as with most theories of semantic representation) yet acknowledges the possibility that the meaning of a structure might not follow from its parts, which many more complicated theories do not. Siskind <ref> [128] </ref> argues that once semantic symbols have been apportioned, it is a relatively trivial matter to learn the relational structures found in more complicated semantic representations based on tree-like functional composition. 15 In reality, the minimization is of the cross-entropy between the learner's model and the true distribution, but it is <p> This work includes studies of language acquisition (see Selfridge [121], Siklossy [125] and Siskind <ref> [126, 127, 128] </ref>); parameter estimation schemes for machine translation, where sentences in a second language substitute for semantic input (see Brown et al. [26] and Berger et al. [15]); and parameter estimation schemes for systems that classify utterances (see Tishby and Gorin [140]).
Reference: [129] <author> D. D. Sleator and D. Temperley. </author> <title> Parsing English with a link grammar. </title> <type> Technical Report CMU-CS-91-196, </type> <institution> Carnegie Mellon University, Pittsburgh, Pennsylvania, </institution> <year> 1991. </year>
Reference-contexts: Nevertheless, there have been some experiments in language modeling that used underlying structures with natural linguistic interpretations, such as the long-range trigram model of Della Pietra et al. [52], based loosely on the link grammars of Sleator and Temperley <ref> [129] </ref>. The only cases that have met with significant success (on language modeling grounds) have not demonstrated that they actually produce derivations that agree with linguistic intuitions. The most pointed example of this is the multigram model, discussed in the context of language modeling by Deligne and Bimbot [51].
Reference: [130] <author> C. E. Snow, A. Arlmann-Rupp, Y. Hassin, J. Jobse, J. Joosten, and J. Vorster. </author> <title> Mothers' speech in three social classes. </title> <journal> Journal of Psycholinguistic Research, </journal> <volume> 5 </volume> <pages> 1-20, </pages> <year> 1976. </year>
Reference-contexts: Indeed, much work on syntactic acquisition has assumed that the thematic roles of noun phrases are known to the learner [61]. Finally, it has been shown that children do not learn much, if anything, from sound patterns in isolation <ref> [117, 130] </ref>; some environmental clues are probably necessary for learning.
Reference: [131] <author> J. L. Sokolov and C. E. Snow. </author> <title> The changing role of negative evidence in theories of language development. </title> <editor> In C. Gallaway and B. J. Richards, editors, </editor> <booktitle> Input and Interaction in Language Acquisition, </booktitle> <pages> pages 38-55. </pages> <publisher> Cambridge University Press, </publisher> <address> New York, NY, </address> <year> 1994. </year>
Reference-contexts: One is that the child has access to a generous source of negative examples. Many have contested Brown and Hanlon (see Sokolov and Snow <ref> [131] </ref> for review), and suggested that in fact implicit and explicit negative evidence does appear in the input children receive.
Reference: [132] <author> R. J. Solomonoff. </author> <title> A new method for discovering the grammars of phrase structure languages. </title> <booktitle> Information Processing, </booktitle> <pages> pages 258-290, </pages> <year> 1959. </year>
Reference-contexts: Some of the earliest work on the inductive inference of language was performed by Solomonoff <ref> [132, 133] </ref>, who would later play a major role in defining the theory that motivates MDL [134]. In his language work the importance of penalizing complexity is already emphasized.
Reference: [133] <author> R. J. </author> <title> Solomonoff. </title> <booktitle> The mechanization of linguistic learning. In Proceedings of the 2nd International Conference on Cybernetics, </booktitle> <pages> pages 180-193, </pages> <year> 1960. </year>
Reference-contexts: Other relevant arguments for simplicity as measured by description length include <ref> [16, 20, 37, 66, 133] </ref>. 40 CHAPTER 3. STOCHASTIC GRAMMARS, MODEL SELECTION AND LANGUAGE ACQ. generalization. From a Bayesian perspective this is not surprising: the 2 jGj prior very heavily biases towards grammars that are improbably simple from the linguistic perspective. <p> Some of the earliest work on the inductive inference of language was performed by Solomonoff <ref> [132, 133] </ref>, who would later play a major role in defining the theory that motivates MDL [134]. In his language work the importance of penalizing complexity is already emphasized.
Reference: [134] <author> R. J. Solomonoff. </author> <title> A formal theory of inductive inference. </title> <journal> Information and Control, </journal> <volume> 7 </volume> <pages> 224-254, </pages> <year> 1964. </year>
Reference-contexts: Some of the earliest work on the inductive inference of language was performed by Solomonoff [132, 133], who would later play a major role in defining the theory that motivates MDL <ref> [134] </ref>. In his language work the importance of penalizing complexity is already emphasized.
Reference: [135] <author> A. Spencer. </author> <title> Morphological Theory. </title> <publisher> Blackwell Publishers, </publisher> <address> Cambridge, MA, </address> <year> 1991. </year>
Reference-contexts: Here even syntactic rules are treated as part of the lexicon, if there is reason to believe that they are memorized. Under these definitions the lexicon does not include objects that can be derived using completely regular processes, even if they are words in the traditional sense; see Spencer <ref> [135] </ref> for further discussion. 44 4.1. <p> This agrees with the fact that it is extremely difficult to find language-independent definitions that agree with our intuition of what a word is <ref> [135] </ref>. In contrast, if multiple composition operators are used, then parameters can be classified according to the compositional process that they are built with. 4.1. <p> But if the parameter has the equivalent of "don't cares", then some extra complexities are introduced. 8 For example, in languages like Spanish syllables can cross word boundaries [68]. See the literature on bracketing paradoxes <ref> [78, 135] </ref>. 4.2. MOTIVATIONS 57 Irregular Forms and Compilation One of the great mysteries of language is how the processing mechanism rapidly reconstructs word sequences from speech. During language production the underlying forms of words are transformed by various corrupting and distorting morphological, phonological and phonetic processes.
Reference: [136] <author> R. Sproat, N. Chang, C. Shih, and W. Gale. </author> <title> A stochastic finite-state word-segmentation algorithm for Chinese. </title> <booktitle> In Proc. 32nd Annual Meeting of the Association for Computational Linguistics, </booktitle> <pages> pages 66-73, </pages> <address> Las Cruces, New Mexico, </address> <year> 1994. </year>
Reference-contexts: The standard approach is to build a stochastic finite-state model of sentences based on words (perhaps a multigram) and then find the maximum-likelihood segmentation of a sentence using the forward-backward algorithm. The greatest challenge to this problem comes from unknown words and proper names that are not in dictionaries <ref> [136, 145] </ref>. Thus, an important 5.4. RELATED WORK 99 problem in processing text in languages like Chinese is the discovery of words in an environment where word boundaries are uncertain. The only difference between this problem and ours is that we start with no prior knowledge of the lexicon. <p> The only difference between this problem and ours is that we start with no prior knowledge of the lexicon. However, most of the techniques commonly used to discover new words for segmentation tasks are either application specific (Sproat et al. <ref> [136] </ref> and Chang et al. [33] discuss methods for learning Chinese names that are based on their idiosyncratic properties, and Chang et al. [34] judge new Chinese words by their similarity to existing words) or very similar to the more general lexical induction schemes of Olivier [102], Cartwright and Brent [32], <p> Most of these algorithms attack a slightly differently problem, starting with a lexicon defined by hand-segmentations of text or man-made dictionaries. However, it is interesting to compare results. Sproat et al. <ref> [136] </ref> make the point (see also Luo and Roukos [89]) that it is difficult define "true" segmentations in Chinese: when people are asked to segment sentences into words, their segmentations very often disagree.
Reference: [137] <author> A. Stolcke. </author> <title> Bayesian Learning of Probabilistic Language Models. </title> <type> PhD thesis, </type> <institution> University of Cali-fornia at Berkeley, Berkeley, </institution> <address> CA, </address> <year> 1994. </year>
Reference-contexts: Very often the complexity of grammars is measured using coding schemes that treat the grammar as a sequence of symbols to be written out on a piece of paper for viewing. Exceptions include the work of Ellison [57, 58] (where linguistically interesting representations for grammars are explored) and Stolcke <ref> [137] </ref> (where statistically principled means are used to estimate description length). 4.5. RELATED WORK 71 The data compression community has put more emphasis on efficient coding of parameters, and has produced several representations for parameters that are similar to ours; these are described in more detail in section 5.4.4. <p> This general procedure of alternating between structural and parametric (in the traditional sense) updates to Bayesian models is a popular strategy for structural induction problems; see for example the stochastic grammar induction schemes of Stolcke <ref> [137] </ref> and Chen [35], and the extended literature on structural induction of neural networks and Bayes' nets. 74 CHAPTER 5. LEARNING ALGORITHMS Let G be the simplest lexicon. Iterate until convergence: Let U 0 = U + G. Optimize stochastic properties of G over U 0 . <p> The reasons behind the failures harken back to the discussions of chapter 3: extralinguistic patterns are learned at the expense of linguistic ones and words are made long in an effort to improve stochastic models. Much recent work has focused on the induction of context-free grammars or variations thereof <ref> [8, 25, 31, 30, 35, 36, 43, 83, 104, 137] </ref>. The hierarchical nature of these grammars would seem on the surface to be quite similar to our hierarchical, concatenative representation. However, algorithms designed for the induction of context-free grammars have not performed well in practice. <p> Although the end grammars model the input moderately well from a predictive viewpoint, the derivations assigned to sentences do not agree with human judgments. Follow up work by Carroll and Charniak [30, 31] achieves similar results. Stolcke <ref> [137] </ref> and Chen [35, 36], by emphasizing structural induction to a greater extent, achieve better results on artificial languages but again are unable to learn natural-language grammars that reflect human judgments from real data. <p> Members of the first class, found here and in the work of Olivier [102], Cook et al. [43] Wolff [150, 151], Ellison [57, 58], Nevill-Manning [99], Cartwright and Brent [32], Chen [35, 36], Stol-cke <ref> [137] </ref> and others, iteratively update the underlying structure of the grammar. (Some, like our algorithms, start with the most general grammar possible while others, like Stolcke's, start with the most specific grammar possible.) Members of the second class, exemplified by the work of Pereira and Schabes [104], Deligne and Bimbot [51], <p> Almost all algorithms of the first type define a set of candidate changes, and an evaluation function. Some, like Ellison's [57], use a simulated-annealing approach where a change may be accepted even if it results in a poorer score from the evaluation function. Stolcke <ref> [137] </ref> uses incremental count-change techniques very similar to ours to estimate changes in description length. Others define a simpler evaluation function and do not need to utilize formulas like equation 5.8. <p> In contrast, Brent et al. [24], Cartwright and Brent [32], Chen [35, 36], Ristad and Thomas [116] and others that invoke the MDL principle all compute ad hoc estimates of description length (often based on symbol counts) that do not closely reflect the best possible encodings of their grammars (Stolcke <ref> [137] </ref> is more careful). Although it is not clear exactly how much this affects performance, it is worth noting that by assuming naive, nonadaptive encoding schemes for parameters, these researchers are unnecessarily penalizing parameters.
Reference: [138] <author> P. Suppes. </author> <title> The semantics of children's language. </title> <publisher> American Psychologist, </publisher> <year> 1973. </year>
Reference-contexts: The tests are completely artificial in the sense that the meanings presented to the learning algorithm are constructed from the orthography of sentences rather than dervied from real situations. 6.4.1 Input Evidence is constructed from the Nina portion of the CHILDES database <ref> [90, 138] </ref>. This is a set of transcriptions of interactions between a mother and a young child (Nina) over a multi-year period. Only the transcriptions of the mother's speech are used; these amount to approximately 34,000 sentences of English text.
Reference: [139] <author> W. J. Teahan and J. G. Cleary. </author> <title> The entropy of English using PPM-based techniques. </title> <booktitle> In Proceedings of the 1996 Data Compression Conference, </booktitle> <address> Salt Lake City, Utah, </address> <year> 1996. </year>
Reference-contexts: Popular text compression schemes can be divided into four classes: those based on adaptive frequency techniques like Huffman codes; those based on context models [112] (such as the PPM algorithm <ref> [42, 96, 139] </ref>, probably the most effective widely-used method for text compression); those based on hidden Markov models [13] (these are less common); and those based on dictionary methods.
Reference: [140] <author> N. Tishby and A. Gorin. </author> <title> Algebraic learning of statistical associations for language acquisition. </title> <booktitle> Computer Speech and Language, </booktitle> <volume> 8 </volume> <pages> 51-78, </pages> <year> 1994. </year>
Reference-contexts: acquisition (see Selfridge [121], Siklossy [125] and Siskind [126, 127, 128]); parameter estimation schemes for machine translation, where sentences in a second language substitute for semantic input (see Brown et al. [26] and Berger et al. [15]); and parameter estimation schemes for systems that classify utterances (see Tishby and Gorin <ref> [140] </ref>). The learning algorithm presented in this chapter for the concatenative model extended with the meaning perturbation operator advances previous work in many ways. First, unlike all of the other work cited, it does not assume presegmented input. This is a very substantial difference.
Reference: [141] <author> L. G. Valiant. </author> <title> A theory of the learnable. </title> <journal> Comm. of the ACM, </journal> <volume> 27(11) </volume> <pages> 1134-1142, </pages> <year> 1984. </year>
Reference-contexts: With suitable formalization it can be shown that for this to be possible, the class of hypothesis languages must be heavily constrained; for example, in the PAC learning framework <ref> [141] </ref> it can be shown that the VC-dimension of the hypothesis class is bounded by the number of samples available to the learner [56], up to a factor that depends on the allowable error rate. 3 This means that the complexity 4 of the class of grammars that can be entertained
Reference: [142] <author> V. N. Vapnik. </author> <title> Estimation of Dependences Based on Empirical Data. </title> <publisher> Springer-Verlag, </publisher> <address> New York, NY, </address> <year> 1982. </year>
Reference-contexts: is the possibility that the learner will choose incorrectly from among the members of the hypothesis class; the higher the ratio of the VC-dimension of the hypothesis class to the amount of evidence, the more likely the learner is to select a grammar that generalizes more poorly than is necessary <ref> [142] </ref> (given sufficient evidence for a given VC-dimension, any function consistent with the evidence will generalize well [76]). 3 The VC-dimension of a set of functions is, roughly speaking, a measure of the effective coverage of the set [143]. <p> This definition can be extended to measure the VC-dimension of functions with arbitrary ranges, such as probability distributions like p (U jG). 4 Here the word complexity is used with no special technical connotations. 38 CHAPTER 3. STOCHASTIC GRAMMARS, MODEL SELECTION AND LANGUAGE ACQ. Vapnik <ref> [142] </ref> advocates the structural risk minimization framework in which the learner selects a hypothesis class (from among a structural hierarchy of classes) with VC-dimension that minimizes the sum of these two contributions to the generalization error. <p> that it is merely a heuristic, but point out three important ways in which it mimics the philosophy of the better-justified structural risk minimization: * In very many cases the VC-dimension of a parameterized class of functions is linear or near-linear in the number of free parameters in the class <ref> [10, 76, 142] </ref>. Given an efficient coding scheme, the length of a description of a set of (independent) parameters is linear in the number of parameters.
Reference: [143] <author> V. N. Vapnik and A. Y. Chervonenkis. </author> <title> On the uniform convergence of relative frequencies of events to their probabilities. </title> <journal> Theory of Probability and its Applications, </journal> <volume> 16(2) </volume> <pages> 264-280, </pages> <year> 1971. </year>
Reference-contexts: is to select a grammar that generalizes more poorly than is necessary [142] (given sufficient evidence for a given VC-dimension, any function consistent with the evidence will generalize well [76]). 3 The VC-dimension of a set of functions is, roughly speaking, a measure of the effective coverage of the set <ref> [143] </ref>. For a set of indicator functions F it is defined to be the size of the largest set of elements that can be labeled in all possible ways by functions in F. <p> By penalizing grammars with high description length jGj, MDL therefore weighs against classes that have too high VC-dimension for good generalization performance. * With sufficient evidence, for a class of a given VC-dimension good generalization performance can be achieved by selecting the function that models the evidence best <ref> [143] </ref>; for stochastic grammars, this is the one that maximizes p (U jG).
Reference: [144] <author> W. von Humboldt. </author> <title> Uber die Verschiedenheit des menschlichen Sprachbaues. </title> <address> Berlin, </address> <month> 1836. </month>
Reference-contexts: THE REPRESENTATIONAL FRAMEWORK 45 4.1 The Representational Framework A central tenet of modern linguistic theory is that language makes "infinite use of finite means" <ref> [40, 144] </ref>, or in plainer terms, that language combines a finite set of lexical parameters to produce an infinite variety of sentences. This chapter argues that these lexical parameters, the primitive units of sentence processing, are themselves built by composing parts, inside the lexicon.
Reference: [145] <author> L.-J. Wang, W.-C. Li, and C.-H. Chang. </author> <title> Recognizing unregistered names for Mandarin word identification. </title> <booktitle> In COLING 92: International Conference on Computational Linguistics, </booktitle> <pages> pages 1239-1243, </pages> <address> Nantes, France, </address> <year> 1992. </year>
Reference-contexts: The standard approach is to build a stochastic finite-state model of sentences based on words (perhaps a multigram) and then find the maximum-likelihood segmentation of a sentence using the forward-backward algorithm. The greatest challenge to this problem comes from unknown words and proper names that are not in dictionaries <ref> [136, 145] </ref>. Thus, an important 5.4. RELATED WORK 99 problem in processing text in languages like Chinese is the discovery of words in an environment where word boundaries are uncertain. The only difference between this problem and ours is that we start with no prior knowledge of the lexicon.
Reference: [146] <author> G. I. Webb. </author> <title> Further experimental evidence against the utility of Occam's Razor. </title> <journal> Journal of Artificial Intelligence Research, </journal> <volume> 4 </volume> <pages> 397-417, </pages> <year> 1996. </year> <note> BIBLIOGRAPHY 133 </note>
Reference-contexts: Despite its motivations, it does not trade VC-dimension against evidence in the theoretically optimal way, and in no way guarantees that generalization performance is maximized: although results vary by application [97], as is to be expected, practical experience indicates (see <ref> [98, 116, 146] </ref> and section 6.1.3) that MDL as commonly used tends to underestimate the number of parameters necessary for optimum 5 See [80, 86, 87, 111, 113, 114] for attempted justifications of MDL and the closely related Kolmogorov complexity.
Reference: [147] <author> T. Welch. </author> <title> A technique for high-performance data compression. </title> <booktitle> IEEE COMPUTER, </booktitle> <pages> pages 8-19, </pages> <month> June </month> <year> 1984. </year>
Reference-contexts: Only the dictionary methods, exemplified by the LZ78 [156] and LZW <ref> [147] </ref> algorithms, have underlying models that can easily be assigned linguistic interpretations. Dictionary-based text compression techniques are variable-length block coding schemes, very similar to the multigram model. They compress text by building a dictionary of words, each word a character string. Words are referenced via codewords. <p> scheme. 6.1.3 Brown Corpus Compression Results When run on the Brown corpus, our algorithm compresses the input from 48,032,256 bits (each character stored as an 8-bit byte) to 12,530,415 bits, a ratio of 3.83:1 and a compression rate of 2.09 bits/char. 2 Compare to 3.40 bits/char (2.35:1) for the LZ78-based <ref> [147, 156] </ref> UNIX compress program and 3.02 bits/char (2.65:1) for the LZ77-based [155] UNIX gzip program. Figure 6.2 presents learning curves for the algorithm across the 15 iterations. <p> Figure 6.4 presents results over the corpus, compared with the LZ78-based <ref> [147, 156] </ref> compress program, the LZ77-based [155] gzip program, Sequitur [99] and a PPM-based program [42, 96]. The performance figures for other programs are taken from Nevill-Manning [99]. 3 This is not exactly true.
Reference: [148] <author> K. Wexler and P. Culicover. </author> <title> Formal Principles of Language Acquisition. </title> <publisher> Cambridge University Press, </publisher> <address> New York, NY, </address> <year> 1980. </year>
Reference-contexts: This is the case, for instance, if every language contains a sentence that is unique to that language. Some have proposed that the class of grammars that children consider is highly restricted, with particular properties that render it identifiable (see Berwick [16] and Wexler and Culicover <ref> [148] </ref> for discussion). This possibility has generally been raised in the context of syntax. Regardless of whether it holds, other parts of language, such as the lexicon, are not so limited.
Reference: [149] <author> J. G. Wolff. </author> <title> The discovery of segments in natural language. </title> <journal> British Journal of Psychology, </journal> <volume> 68 </volume> <pages> 97-106, </pages> <year> 1977. </year>
Reference-contexts: Thus, researchers in grammatical inference often directly evaluate grammars (or grammatical derivations) rather than the languages generated by a grammar or a grammar's predictive ability. This line of research has lead to many approaches that are similar to ours. For example, Olivier [102], Wolff <ref> [149, 150, 151] </ref>, Brent et al. [24], and Cartwright and Brent [23, 32] all present algorithms for the induction of word-like linguistic units from character and phoneme sequences; these algorithms all rely on dictionary-based representations similar to our multigram model (though usually no stochastic interpretation is assigned). <p> It seems therefore that our algorithm performs substantially better. Wolff <ref> [149, 150] </ref> presents a word-learning algorithm and applies it to English and pseudo-English text, but does not provide results in a manner suitable for comparison; however, experiments performed by Nevill-Manning [99] indicate that Wolff's algorithms are not competitive.
Reference: [150] <author> J. G. Wolff. </author> <title> Language acquisition and the discovery of phrase structure. </title> <journal> Language and Speech, </journal> <volume> 23(3) </volume> <pages> 255-269, </pages> <year> 1980. </year>
Reference-contexts: Thus, researchers in grammatical inference often directly evaluate grammars (or grammatical derivations) rather than the languages generated by a grammar or a grammar's predictive ability. This line of research has lead to many approaches that are similar to ours. For example, Olivier [102], Wolff <ref> [149, 150, 151] </ref>, Brent et al. [24], and Cartwright and Brent [23, 32] all present algorithms for the induction of word-like linguistic units from character and phoneme sequences; these algorithms all rely on dictionary-based representations similar to our multigram model (though usually no stochastic interpretation is assigned). <p> Members of the first class, found here and in the work of Olivier [102], Cook et al. [43] Wolff <ref> [150, 151] </ref>, Ellison [57, 58], Nevill-Manning [99], Cartwright and Brent [32], Chen [35, 36], Stol-cke [137] and others, iteratively update the underlying structure of the grammar. (Some, like our algorithms, start with the most general grammar possible while others, like Stolcke's, start with the most specific grammar possible.) Members of the <p> It seems therefore that our algorithm performs substantially better. Wolff <ref> [149, 150] </ref> presents a word-learning algorithm and applies it to English and pseudo-English text, but does not provide results in a manner suitable for comparison; however, experiments performed by Nevill-Manning [99] indicate that Wolff's algorithms are not competitive.
Reference: [151] <author> J. G. Wolff. </author> <title> Language acquisition, data compression and generalization. </title> <journal> Language and Communication, </journal> <volume> 2(1) </volume> <pages> 57-89, </pages> <year> 1982. </year>
Reference-contexts: Thus, researchers in grammatical inference often directly evaluate grammars (or grammatical derivations) rather than the languages generated by a grammar or a grammar's predictive ability. This line of research has lead to many approaches that are similar to ours. For example, Olivier [102], Wolff <ref> [149, 150, 151] </ref>, Brent et al. [24], and Cartwright and Brent [23, 32] all present algorithms for the induction of word-like linguistic units from character and phoneme sequences; these algorithms all rely on dictionary-based representations similar to our multigram model (though usually no stochastic interpretation is assigned). <p> Members of the first class, found here and in the work of Olivier [102], Cook et al. [43] Wolff <ref> [150, 151] </ref>, Ellison [57, 58], Nevill-Manning [99], Cartwright and Brent [32], Chen [35, 36], Stol-cke [137] and others, iteratively update the underlying structure of the grammar. (Some, like our algorithms, start with the most general grammar possible while others, like Stolcke's, start with the most specific grammar possible.) Members of the
Reference: [152] <author> D. H. Wolpert. </author> <title> Off-training set error and a priori distinctions between learning algorithms. </title> <type> Technical Report 95-01-003, </type> <institution> Santa Fe Institute, </institution> <year> 1995. </year>
Reference-contexts: Thus, the prior can be used to manipulate generalization performance. However, Wolpert and others <ref> [119, 152] </ref> have shown that unless assumptions are made about the learning problem, no generalization strategy (and hence no prior) performs better than any other.
Reference: [153] <author> Z. Wu and G. Tseng. </author> <title> Chinese text segmentation for text retrieval: achievements and problems. </title> <journal> Journal of the American Society for Information Science, </journal> <volume> 44(9) </volume> <pages> 532-542, </pages> <year> 1993. </year>
Reference-contexts: Since Chinese words are of variable length, most sentences are ambiguous with respect to word boundaries, even given knowledge of a dictionary. As a consequence, even the most rudimentary language processing tasks require a complex segmentation process (see for review Wu and Tseng <ref> [153] </ref>). Most researchers attacking the segmentation problem have assumed access to a dictionary. The standard approach is to build a stochastic finite-state model of sentences based on words (perhaps a multigram) and then find the maximum-likelihood segmentation of a sentence using the forward-backward algorithm.
Reference: [154] <author> G. Zipf. </author> <title> Human Behavior and the Principle of Least Effort. </title> <publisher> Addison-Wesley, </publisher> <address> Reading, MA, </address> <year> 1949. </year>
Reference-contexts: In practice, Huffman codes very closely approach the theoretical optimum efficiency that would result from non-integral length codewords; this is true both because the number of parameters is usually large and because parameter frequencies follow a smooth inverse-frequency distribution <ref> [154] </ref>. The number of parameters in each parameter representation is also coded via a Huffman code. The two Huffman codes must themselves be specified; fortunately Huffman codes can be specified quite efficiently so long as the objects they reference are ordered by frequency.
Reference: [155] <author> J. Ziv and A. Lempel. </author> <title> A universal algorithm for sequential data compression. </title> <journal> IEEE Transactions on Information Theory, </journal> <volume> 23(3) </volume> <pages> 337-343, </pages> <year> 1977. </year>
Reference-contexts: our algorithm compresses the input from 48,032,256 bits (each character stored as an 8-bit byte) to 12,530,415 bits, a ratio of 3.83:1 and a compression rate of 2.09 bits/char. 2 Compare to 3.40 bits/char (2.35:1) for the LZ78-based [147, 156] UNIX compress program and 3.02 bits/char (2.65:1) for the LZ77-based <ref> [155] </ref> UNIX gzip program. Figure 6.2 presents learning curves for the algorithm across the 15 iterations. <p> Figure 6.4 presents results over the corpus, compared with the LZ78-based [147, 156] compress program, the LZ77-based <ref> [155] </ref> gzip program, Sequitur [99] and a PPM-based program [42, 96]. The performance figures for other programs are taken from Nevill-Manning [99]. 3 This is not exactly true.
Reference: [156] <author> J. Ziv and A. Lempel. </author> <title> Compression of individual sequences by variable rate coding. </title> <journal> IEEE Transactions on Information Theory, </journal> <volume> 24(5) </volume> <pages> 530-536, </pages> <year> 1978. </year>
Reference-contexts: Only the dictionary methods, exemplified by the LZ78 <ref> [156] </ref> and LZW [147] algorithms, have underlying models that can easily be assigned linguistic interpretations. Dictionary-based text compression techniques are variable-length block coding schemes, very similar to the multigram model. They compress text by building a dictionary of words, each word a character string. Words are referenced via codewords. <p> Both the encoder and the decoder then add the new word wc to their dictionary. Thus, for every codeword that is written a new word is also created. This compression technique has been proven to asymptotically approach the entropy of any Markov source <ref> [156] </ref>. Through the derivational history of words, algorithms like LZW implicitly define a hierarchical structure in the lexicon (in the case of LZW, a left-branching tree). <p> scheme. 6.1.3 Brown Corpus Compression Results When run on the Brown corpus, our algorithm compresses the input from 48,032,256 bits (each character stored as an 8-bit byte) to 12,530,415 bits, a ratio of 3.83:1 and a compression rate of 2.09 bits/char. 2 Compare to 3.40 bits/char (2.35:1) for the LZ78-based <ref> [147, 156] </ref> UNIX compress program and 3.02 bits/char (2.65:1) for the LZ77-based [155] UNIX gzip program. Figure 6.2 presents learning curves for the algorithm across the 15 iterations. <p> Figure 6.4 presents results over the corpus, compared with the LZ78-based <ref> [147, 156] </ref> compress program, the LZ77-based [155] gzip program, Sequitur [99] and a PPM-based program [42, 96]. The performance figures for other programs are taken from Nevill-Manning [99]. 3 This is not exactly true.
References-found: 156

