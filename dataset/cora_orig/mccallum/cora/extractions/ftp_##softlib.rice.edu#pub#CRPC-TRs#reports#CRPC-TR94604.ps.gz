URL: ftp://softlib.rice.edu/pub/CRPC-TRs/reports/CRPC-TR94604.ps.gz
Refering-URL: http://www.crpc.rice.edu/CRPC/softlib/TRs_online.html
Root-URL: 
Title: SCHWARZ METHODS: TO SYMMETRIZE OR NOT TO SYMMETRIZE  
Author: MICHAEL HOLST AND STEFAN VANDEWALLE 
Keyword: Key words. multigrid, domain decomposition, Krylov methods, Schwarz methods, conjugate gradients, Bi-CGstab.  
Web: 65F10, 65N22, 65N55  
Note: AMS subject classifications.  
Abstract: A preconditioning theory for Schwarz methods is presented. The theory establishes sufficient conditions for multiplicative and additive Schwarz algorithms to yield self-adjoint positive definite preconditioners. It allows for the analysis and use of non-variational and non-convergent linear methods as preconditioners for conjugate gradient methods, and it is applied to domain decomposition and multigrid. It is illustrated why symmetrizing may be a bad idea for linear methods. It is conjectured that enforcing minimal symmetry achieves the best results when combined with conjugate gradient acceleration. Also, it is shown that absence of symmetry in the linear preconditioner is advantageous when the linear method is accelerated by using the Bi-CGstab method. Numerical examples are presented for two test problems which illustrate the theory and conjectures. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> S. Ashby, M. Holst, T. Manteuffel, and P. </author> <title> Saylor. The role of the inner product in stopping criteria for conjugate gradient iterations. </title> <type> Technical Report UCRL-JC-112586, </type> <institution> Lawrence Livermore National Laboratory, </institution> <year> 1992. </year>
Reference-contexts: A well-known property is that if M is self-adjoint, then the spectral radius of M , denoted as (M ), satisfies (M ) = kM k. This property can also be shown to hold in the A-norm for A-self-adjoint operators (or, more generally, for A-normal operators <ref> [1] </ref>). Lemma 2.1. If A is SPD and M is A-self-adjoint, then (M ) = kM k A . 2.2. Linear methods. Given the equation Au = f; where A 2 L (H; H) is SPD, consider the preconditioned equation BAu = Bf , with B 2 L (H; H). <p> This ratio is often mistakenly called the (spectral) condition number (BA); in fact, since BA is not self-adjoint, this ratio is not in general equal to the usual condition number (this point is 4 discussed in great detail in <ref> [1] </ref>). However, the ratio does yield a condition number in the A-norm. The following lemma is a special case of Corollary 4.2 in [1]. Lemma 2.12. If A and B are SPD, then A (BA) = kBAk A k (BA) 1 k A = max (BA) :(6) Remark 2.1. <p> fact, since BA is not self-adjoint, this ratio is not in general equal to the usual condition number (this point is 4 discussed in great detail in <ref> [1] </ref>). However, the ratio does yield a condition number in the A-norm. The following lemma is a special case of Corollary 4.2 in [1]. Lemma 2.12. If A and B are SPD, then A (BA) = kBAk A k (BA) 1 k A = max (BA) :(6) Remark 2.1.
Reference: [2] <author> S. F. Ashby, T. A. Manteuffel, and P. E. </author> <title> Saylor. A taxonomy for conjugate gradient methods. </title> <journal> SIAM J. Numer. Anal., </journal> <volume> 27(6) </volume> <pages> 1542-1568, </pages> <year> 1990. </year>
Reference-contexts: Proof. An abbreviated proof appears in [25], a more detailed proof in [13]. 2.4. Krylov acceleration of nonsymmetric linear methods. The convergence theory of the conjugate gradient iteration requires that the preconditioned operator BA be A-self-adjoint (see <ref> [2] </ref> for more general conditions), which from Lemma 2.4 requires that B be self-adjoint. <p> If a Schwarz method is employed which produces a nonsymmetric operator B, then although A is SPD, the theory of the previous section does not apply, and a nonsymmetric solver such as conjugate gradients on the normal equations <ref> [2] </ref>, GMRES [19], CGS [21], or Bi-CGstab [23] must be used for the now non-A-SPD preconditioned system, BAu = Bf .
Reference: [3] <author> J. H. Bramble, J. E. Pasciak, J. Wang, and J. Xu. </author> <title> Convergence estimates for multigrid algorithms without regularity assumptions. </title> <journal> Math. Comp., </journal> <volume> 57 </volume> <pages> 23-45, </pages> <year> 1991. </year>

Reference: [5] <author> A. Brandt. </author> <title> Multi-level adaptive solutions to boundary-value problems. </title> <journal> Math. Comp., </journal> <volume> 31 </volume> <pages> 333-390, </pages> <year> 1977. </year>
Reference-contexts: More recently, DD methods have been reexamined for use as practical computational tools in the (parallel) solution of general elliptic equations on complex domains [8]. MG methods were discovered much more recently [9]. They have been extensively developed both theoretically and practically since the late seventies <ref> [5, 10] </ref>, and they have proven to be extremely efficient for solving very broad classes of partial differential equations. Recent insights in the product nature of certain MG methods have led to a unified theory of MG and DD methods, collectively referred to as Schwarz methods [4, 25].
Reference: [6] <author> J. M. Briggs and J. A. </author> <title> McCammon. </title> <journal> Computation unravels mysteries of molecular biophysics. Computers in Physics, </journal> <volume> 6(3) </volume> <pages> 238-243, </pages> <year> 1990. </year>
Reference-contexts: An example of this occurs with the following test problem. The Poisson-Boltzmann equation describes the electrostatic potential of a biomolecule lying in an ionic solvent (see, e.g., <ref> [6] </ref> for an overview).
Reference: [7] <author> P. Concus, G. H. Golub, and D. P. O'Leary. </author> <title> A generalized conjugate gradient method for the numerical solution of elliptic partial differential equations. </title> <editor> In J. R. Bunch and D. J. Rose, editors, </editor> <booktitle> Sparse Matrix Computations, </booktitle> <pages> pages 309-332. </pages> <publisher> Academic Press, </publisher> <address> New York, NY, </address> <year> 1976. </year>
Reference: [8] <author> M. Dryja and O. B. Widlund. </author> <title> Towards a unified theory of domain decomposition algorithms for elliptic problems. </title> <editor> In T. F. Chan, R. Glowinski, J. Periaux, and O. B. Widlund, editors, </editor> <booktitle> Third International Symposium on Domain Decomposition Methods for Partial Differential Equations, </booktitle> <pages> pages 3-21, </pages> <address> Philadelphia, PA, </address> <year> 1989. </year> <note> SIAM. </note>
Reference-contexts: DD methods were first proposed in 1869 by H. A. Schwarz as a theoretical tool in the study of elliptic problems on non-rectangular domains [20]. More recently, DD methods have been reexamined for use as practical computational tools in the (parallel) solution of general elliptic equations on complex domains <ref> [8] </ref>. MG methods were discovered much more recently [9]. They have been extensively developed both theoretically and practically since the late seventies [5, 10], and they have proven to be extremely efficient for solving very broad classes of partial differential equations. <p> The error propagator of a multiplicative DD method on the space H employing the subspaces I k H k has the general form <ref> [8] </ref>: E = I BA = (I I J R J I J A) (I I 0 R 0 I 0 A) (I I J R J I J A) ;(10) where R k and R k , k = 1; : : : ; J , are linear operators on <p> Convergence theories for DD methods can be quite technical and depend on such things as the discretization, the subdomain number, shape, and size, and the regularity of the solution <ref> [4, 8, 25] </ref>. However, since variational conditions hold naturally between the fine space and each subdomain space for nearly any formulation of a DD method, very general convergence theorems can be derived, if one is not concerned about the actual rate of convergence. <p> However, since variational conditions hold naturally between the fine space and each subdomain space for nearly any formulation of a DD method, very general convergence theorems can be derived, if one is not concerned about the actual rate of convergence. Using the Schwarz theory framework in any of <ref> [4, 8, 25] </ref>, it can be shown that Condition 4 in Theorem 3.3 (convergence of multiplicative DD without a coarse space) hold if the variational conditions (11) holds, and if the subdomain solvers R k are SPD. A proof of this result may be found for example in [13].
Reference: [9] <author> R. P. Fedorenko. </author> <title> A relaxation method for solving elliptic difference equations. </title> <journal> USSR Comput. Math. and Math. Phys., </journal> <volume> 1(5) </volume> <pages> 1092-1096, </pages> <year> 1961. </year>
Reference-contexts: A. Schwarz as a theoretical tool in the study of elliptic problems on non-rectangular domains [20]. More recently, DD methods have been reexamined for use as practical computational tools in the (parallel) solution of general elliptic equations on complex domains [8]. MG methods were discovered much more recently <ref> [9] </ref>. They have been extensively developed both theoretically and practically since the late seventies [5, 10], and they have proven to be extremely efficient for solving very broad classes of partial differential equations.
Reference: [10] <author> W. Hackbusch. </author> <title> Multi-grid Methods and Applications. </title> <publisher> Springer-Verlag, </publisher> <address> Berlin, Germany, </address> <year> 1985. </year>
Reference-contexts: More recently, DD methods have been reexamined for use as practical computational tools in the (parallel) solution of general elliptic equations on complex domains [8]. MG methods were discovered much more recently [9]. They have been extensively developed both theoretically and practically since the late seventies <ref> [5, 10] </ref>, and they have proven to be extremely efficient for solving very broad classes of partial differential equations. Recent insights in the product nature of certain MG methods have led to a unified theory of MG and DD methods, collectively referred to as Schwarz methods [4, 25].
Reference: [11] <author> W. Hackbusch. </author> <title> Iterative Solution of Large Sparse Systems of Equations. </title> <publisher> Springer-Verlag, </publisher> <address> Berlin, Germany, </address> <year> 1994. </year>
Reference-contexts: Krylov acceleration of linear iterative methods. In this section, we review some background material on self-adjoint linear operators, linear methods, and conjugate gradient acceleration. A more thorough reviews can be found in <ref> [11, 16] </ref>. 2.1. Background material, terminology and notation. Let H be a real finite-dimensional Hilbert space equipped with the inner-product (; ) inducing the norm k k = (; ) 1=2 . <p> Proof. By Lemma 2.5, E is A-self-adjoint. By Lemma 2.1 the result follows. Lemma 2.8. If E fl is the A-adjoint of E, then kEk 2 A = kEE fl k A . Proof. The proof follows that of a familiar result for the Euclidean 2-norm <ref> [11] </ref>. Lemma 2.9. If A and B are SPD, and E is A-non-negative, then kEk A &lt; 1. Proof. By Lemma 2.5, E is A-self-adjoint. As E is A-non-negative, it holds that (Eu; u) A 0, or (BAu; u) A (u; u) A .
Reference: [12] <author> M. R. Hestenes and E. </author> <title> Stiefel. Methods of conjugate gradients for solving linear systems. </title> <journal> J. Research of NBS, </journal> <volume> 49 </volume> <pages> 409-435, </pages> <year> 1952. </year>
Reference-contexts: Since C 2 in Lemma 2.10 bounds the largest positive eigenvalue of E, we have that C 2 &lt; 1. 2.3. Krylov acceleration of SPD linear methods. The conjugate gradient method was developed by Hestenes and Stiefel <ref> [12] </ref> as a method for solving linear systems Au = f in a space H, with SPD operators A.
Reference: [13] <author> M. Holst. </author> <title> An Algebraic Schwarz Theory. </title> <type> Technical Report CRPC-94-12/CRPC-TR94460, </type> <institution> Ap plied Mathematics and CRPC, California Institute of Technology, </institution> <year> 1994. </year>
Reference-contexts: Lemma 2.15. If A and B are SPD, and kI BAk A ffi &lt; 1, then ffi cg &lt; ffi. Proof. An abbreviated proof appears in [25], a more detailed proof in <ref> [13] </ref>. 2.4. Krylov acceleration of nonsymmetric linear methods. The convergence theory of the conjugate gradient iteration requires that the preconditioned operator BA be A-self-adjoint (see [2] for more general conditions), which from Lemma 2.4 requires that B be self-adjoint. <p> A proof of this result may be found for example in <ref> [13] </ref>. Remark 3.5. Note that the theorem does not require that the overall multiplicative DD method be convergent. In particular, the conditions on the coarse problem and coarse problem solver are very relaxed. 3.3. Multiplicative multigrid.
Reference: [14] <author> M. Holst and F. Saied. </author> <title> Multigrid and domain decomposition methods for electrostatics problems. </title> <editor> In D. E. Keyes and J. Xu, editors, </editor> <booktitle> Domain Decomposition Methods in Science and Engineering (Proceedings of the Seventh International Conference on Domain Decomposition, </booktitle> <month> October 27-30, </month> <year> 1993, </year> <institution> The Pennsylvania State University). American Mathematical Society, Providence, </institution> <note> 1995 (to appear). </note>
Reference-contexts: On the other hand, multiplicative DD is somewhat more costly than additive DD, due to the need to update boundary information after the solution of each subdomain problem. Table 1 should not be used to compare MG and DD methods for efficiency. Similar experiments <ref> [14] </ref> with more carefully optimized DD and MG methods show DD to be often competitive with MG for difficult elliptic equations such as those with discontinuous coefficients, although there may be some debate as to which approach is more effective on parallel computers [22]. Multiplicative multigrid.
Reference: [15] <author> R. Kress. </author> <title> Linear Integral Equations. </title> <publisher> Springer-Verlag, </publisher> <address> Berlin, Germany, </address> <year> 1989. </year>
Reference-contexts: We recall two well-known lemmas; see for example <ref> [15] </ref> or [18]. Lemma 2.2. For arbitrary f and u 0 , the condition (E) &lt; 1 is necessary and sufficient for convergence of the linear method (3). Lemma 2.3.
Reference: [16] <author> E. </author> <title> Kreyszig. Introductory Functional Analysis with Applications. </title> <publisher> John Wiley & Sons, Inc., </publisher> <address> New York, NY, </address> <year> 1990. </year>
Reference-contexts: Krylov acceleration of linear iterative methods. In this section, we review some background material on self-adjoint linear operators, linear methods, and conjugate gradient acceleration. A more thorough reviews can be found in <ref> [11, 16] </ref>. 2.1. Background material, terminology and notation. Let H be a real finite-dimensional Hilbert space equipped with the inner-product (; ) inducing the norm k k = (; ) 1=2 .
Reference: [17] <author> S. F. McCormick and J. W. Ruge. </author> <title> Unigrid for multigrid simulation. </title> <journal> Math. Comp., </journal> <volume> 41(163) </volume> <pages> 43-62, </pages> <year> 1983. </year>
Reference: [18] <author> J. M. Ortega. </author> <title> Numerical Analysis: A Second Course. </title> <publisher> Academic Press, </publisher> <address> New York, NY, </address> <year> 1972. </year>
Reference-contexts: We recall two well-known lemmas; see for example [15] or <ref> [18] </ref>. Lemma 2.2. For arbitrary f and u 0 , the condition (E) &lt; 1 is necessary and sufficient for convergence of the linear method (3). Lemma 2.3. The condition kEk &lt; 1, or the condition kEk A &lt; 1, is sufficient for convergence of the linear method (3).
Reference: [19] <author> Y. Saad and M. H. Schultz. </author> <title> GMRES: A generalized minimal residual algorithm for solving nonsymmetric linear systems. </title> <journal> SIAM J. Sci. Statist. Comput., </journal> <volume> 7(3) </volume> <pages> 856-869, </pages> <year> 1986. </year>
Reference-contexts: If a Schwarz method is employed which produces a nonsymmetric operator B, then although A is SPD, the theory of the previous section does not apply, and a nonsymmetric solver such as conjugate gradients on the normal equations [2], GMRES <ref> [19] </ref>, CGS [21], or Bi-CGstab [23] must be used for the now non-A-SPD preconditioned system, BAu = Bf .
Reference: [20] <author> H. A. Schwarz. </author> <title> Uber einige Abbildungsaufgaben. </title> <journal> Ges. Math. Abh., </journal> <volume> 11 </volume> <pages> 65-83, 1869. </pages>
Reference-contexts: 1. Introduction. Domain decomposition (DD) and multigrid (MG) methods have been studied extensively in recent years, both from a theoretical and numerical point of view. DD methods were first proposed in 1869 by H. A. Schwarz as a theoretical tool in the study of elliptic problems on non-rectangular domains <ref> [20] </ref>. More recently, DD methods have been reexamined for use as practical computational tools in the (parallel) solution of general elliptic equations on complex domains [8]. MG methods were discovered much more recently [9].
Reference: [21] <author> P. Sonneveld. </author> <title> CGS: A fast Lanczos-type solver for nonsymmetric linear systems. </title> <journal> SIAM J. Sci. Statist. Comput., </journal> <volume> 10 </volume> <pages> 36-52, </pages> <year> 1989. </year>
Reference-contexts: If a Schwarz method is employed which produces a nonsymmetric operator B, then although A is SPD, the theory of the previous section does not apply, and a nonsymmetric solver such as conjugate gradients on the normal equations [2], GMRES [19], CGS <ref> [21] </ref>, or Bi-CGstab [23] must be used for the now non-A-SPD preconditioned system, BAu = Bf .
Reference: [22] <author> E. F. Van de Velde. </author> <title> Domain decomposition vs. concurrent multigrid. </title> <type> Technical Report CRPC 94-11, </type> <institution> Applied Mathematics and CRPC, California Institute of Technology, </institution> <year> 1994. </year>
Reference-contexts: Similar experiments [14] with more carefully optimized DD and MG methods show DD to be often competitive with MG for difficult elliptic equations such as those with discontinuous coefficients, although there may be some debate as to which approach is more effective on parallel computers <ref> [22] </ref>. Multiplicative multigrid. The results for multiplicative V-cycle MG are presented in Table 2. Each row corresponds to a different smoothing strategy, and is annotated by (-1 ; -2 ), with -1 : pre-smoothing strategy, and -2 : post-smoothing strategy.
Reference: [23] <author> H. A. van der Vorst. </author> <title> BI-CGSTAB: A fast and smoothly converging variant of BI-CG for the solution of nonsymmetric linear systems. </title> <journal> SIAM J. Sci. Statist. Comput., </journal> <volume> 13(2) </volume> <pages> 631-644, </pages> <year> 1992. </year>
Reference-contexts: If a Schwarz method is employed which produces a nonsymmetric operator B, then although A is SPD, the theory of the previous section does not apply, and a nonsymmetric solver such as conjugate gradients on the normal equations [2], GMRES [19], CGS [21], or Bi-CGstab <ref> [23] </ref> must be used for the now non-A-SPD preconditioned system, BAu = Bf . <p> For these reasons, methods such as CGS and Bi-CGstab have become the methods of choice in many applications that give rise to nonsymmetric problems. Bi-CGstab has been shown to be more attractive than CGS in many situations due to the more regular convergence behavior <ref> [23] </ref>. In x6, we shall use the preconditioned Bi-CGstab algorithm to accelerate nonsymmetric Schwarz methods. In a sequence of numerical experiments, we shall compare the effectiveness of this approach with unaccelerated symmetric and nonsymmetric Schwarz methods, and with symmetric Schwarz methods accelerated with conjugate gradients. 3. Multiplicative Schwarz methods.
Reference: [24] <author> J. Xu. </author> <title> Theory of Multilevel Methods. </title> <type> PhD thesis, </type> <institution> Department of Mathematics, Penn State University, University Park, </institution> <address> PA, </address> <month> July </month> <year> 1989. </year> <type> Technical Report AM 48. </type>
Reference-contexts: Thus, (E) = 1 min i i &lt; 1: Finally, by Lemma 2.7, we have kEk A = (E). We will have use for the following two simple lemmas, appearing previously in <ref> [24] </ref>. Lemma 2.10. <p> The following result gives a bound on the condition number of the operator BA in terms of the extreme eigenvalues of the error propagator E = I BA; such bounds are often used in the analysis of linear preconditioners (cf. Proposition 5.1 in <ref> [24] </ref>). We give a short proof of this result for completeness. Lemma 2.13. <p> This implication of Lemma 2.13 was first noticed in <ref> [24] </ref>. If only a bound on the norm of the error propagator E = I BA is available, then the following result can be used to bound the condition number of BA. This result is used for example in [25]. Corollary 2.14.
Reference: [25] <author> J. Xu. </author> <title> Iterative methods by space decomposition and subspace correction. </title> <journal> SIAM Review, </journal> <volume> 34(4) </volume> <pages> 581-613, </pages> <year> 1992. </year>
Reference-contexts: Recent insights in the product nature of certain MG methods have led to a unified theory of MG and DD methods, collectively referred to as Schwarz methods <ref> [4, 25] </ref>. In this paper, we consider additive and multiplicative Schwarz methods and their acceleration with Krylov methods, for the numerical solution of self-adjoint positive definite (SPD) operator equations arising from the discretization of elliptic partial differential equations. <p> This implication of Lemma 2.13 was first noticed in [24]. If only a bound on the norm of the error propagator E = I BA is available, then the following result can be used to bound the condition number of BA. This result is used for example in <ref> [25] </ref>. Corollary 2.14. If A and B are SPD, and kI BAk A ffi &lt; 1, then A (BA) 1 ffi 5 Proof. This follows immediately from Lemma 2.13 with ffi = maxfC 1 ; C 2 g. <p> It shows that the conjugate gradient method always accelerates a linear method (if the conditions of the lemma hold). Lemma 2.15. If A and B are SPD, and kI BAk A ffi &lt; 1, then ffi cg &lt; ffi. Proof. An abbreviated proof appears in <ref> [25] </ref>, a more detailed proof in [13]. 2.4. Krylov acceleration of nonsymmetric linear methods. The convergence theory of the conjugate gradient iteration requires that the preconditioned operator BA be A-self-adjoint (see [2] for more general conditions), which from Lemma 2.4 requires that B be self-adjoint. <p> Convergence theories for DD methods can be quite technical and depend on such things as the discretization, the subdomain number, shape, and size, and the regularity of the solution <ref> [4, 8, 25] </ref>. However, since variational conditions hold naturally between the fine space and each subdomain space for nearly any formulation of a DD method, very general convergence theorems can be derived, if one is not concerned about the actual rate of convergence. <p> However, since variational conditions hold naturally between the fine space and each subdomain space for nearly any formulation of a DD method, very general convergence theorems can be derived, if one is not concerned about the actual rate of convergence. Using the Schwarz theory framework in any of <ref> [4, 8, 25] </ref>, it can be shown that Condition 4 in Theorem 3.3 (convergence of multiplicative DD without a coarse space) hold if the variational conditions (11) holds, and if the subdomain solvers R k are SPD. A proof of this result may be found for example in [13]. <p> Again, we allow for the existence of a "coarse" subspace I 0 H 0 H. The error propagator of an additive DD method on the space H employing the subspaces I k H k has the general form (see <ref> [25] </ref>): E = I BA = I !(I 0 R 0 I 0 + I 1 R 1 I 1 + + I J R J I J )A:(18) The operators R k are linear operators on H k , constructed in such a way that R k A 1 k
References-found: 24

