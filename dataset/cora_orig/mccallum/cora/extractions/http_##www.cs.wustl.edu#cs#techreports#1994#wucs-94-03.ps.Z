URL: http://www.cs.wustl.edu/cs/techreports/1994/wucs-94-03.ps.Z
Refering-URL: http://www.cs.wustl.edu/cs/cs/publications.html
Root-URL: 
Email: vasu@wuccrc.wustl.edu jbf@random.wustl.edu  
Title: Speculative Computation: Overcoming Communication Delays in Parallel Algorithms  
Author: Vasudha Govindan Mark A. Franklin 
Keyword: Speculative computation, communication latency masking, performance model, synchronous iterative algorithms, N-body simulation.  
Address: Campus Box 1115, One Brookings Drive St. Louis, Missouri 63130  
Affiliation: Computer Communications Research Center Washington University  
Abstract: Communication latencies and delays are a major source of performance degradation in parallel computing systems. It is important to "mask" these communication delays by overlapping them with useful computation in order to obtain good parallel performance. This paper proposes speculative computation as a technique to mask communication latencies. Speculative computation is discussed in the context of synchronous iterative algorithms. Processors speculate the contents of messages that are not yet received and perform computation based on the speculated values. When the messages are received, they are compared with the speculated values and, if the error is unacceptable, the resulting computation is corrected or recomputed. If the error is small, the speculated value is accepted and the processor has masked the communication delay. The technique is discussed in detail and is incorporated on to a N-body simulation example; the techniques result in a performance improvement of up to 34%. An empirical performance model is developed to estimate the performance of speculative computing. Model and measured values are compared and shown to be in good agreement. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Dimitri P. Bertsekas and John N. Tsitsiklis. </author> <title> Parallel and Distributed Computation: Numerical Methods. </title> <publisher> Prentics Hall, </publisher> <address> Englewood Cliffs, New Jersey 07632, </address> <year> 1989. </year>
Reference-contexts: They propose partitioning strategies to reduce communication traffic, efficient convergence check mechanisms and concurrent execution of multiple time-steps. While their techniques are shown to be effective, they are limited to hypercube machines and to a small class of algorithms. Techniques such as asynchronous algorithms <ref> [1, 11] </ref> and rollback synchronization [1] mask communication delays but are applicable only to a very small set of applications. Overcoming communication delays in message passing systems is analogous to the problem of overcoming memory access delays in shared memory systems. <p> They propose partitioning strategies to reduce communication traffic, efficient convergence check mechanisms and concurrent execution of multiple time-steps. While their techniques are shown to be effective, they are limited to hypercube machines and to a small class of algorithms. Techniques such as asynchronous algorithms [1, 11] and rollback synchronization <ref> [1] </ref> mask communication delays but are applicable only to a very small set of applications. Overcoming communication delays in message passing systems is analogous to the problem of overcoming memory access delays in shared memory systems. <p> The concept of speculative computation to overcome communication delays can be applied to a host of parallel algorithms. However, in this paper, we focus on applying the technique to a 2 class of synchronous iterative algorithms <ref> [1, 3] </ref>. Section 2 describes the synchronous iterative al-gorithm model employed. Section 3 discusses the speculative computing technique and associated performance benefits. In section 4, we develop an empirical performance model to estimate the performance of speculative computation. <p> Our example, implemented on a network of SUN/Sparc workstations, showed a significant performance improvement (up to 34% on 16 processors) with speculative computation. The model and measured performance are then compared. 2 Synchronous Iterative Algorithms Synchronous iterative algorithms <ref> [1, 3] </ref> include most of the compute intensive numerical methods used in science and engineering applications. Iterative techniques to solve linear and non-linear equations, solution of partial differential equations, numerical integration, particle simulation, etc., are some examples. <p> The processors are synchronized either by means of a barrier operation or by message exchanges at the end of each iteration. The next iteration begins only after all the processors complete the current iteration. In this study, a general and widely applicable model for synchronous iterative algorithms <ref> [1] </ref> is used. The model is used for ease of illustration and does not limit the applicability of the principal ideas which apply broadly to a host of other algorithms.
Reference: [2] <author> F.W. Burton. </author> <title> Speculative computation, parallelism, and functional programming. </title> <journal> IEEE Trans. Comput., </journal> <volume> C(34):1190-1193, </volume> <month> Dec </month> <year> 1985. </year>
Reference-contexts: If the error in speculation is large, the resulting computation is corrected or recomputed. If the error is "small", the resulting computation is accepted, and P 1 has effectively "masked" the communication delay. Speculative computation has been proposed <ref> [2] </ref> and implemented [10] to incorporate parallelism into inherently serial algorithms. Work is performed before it is known whether it is needed or not. If the work is needed, it has already been performed, thus speeding up the computation time. Some work may never be needed and thus is wasted. <p> If the work is needed, it has already been performed, thus speeding up the computation time. Some work may never be needed and thus is wasted. To gain from speculative computation it is critical to identify future work which is likely to be needed. Burton <ref> [2] </ref> proposes a simple functional language to incorporate speculative computation in inherently serial algorithms. Witte et.al [10] apply speculative computing to the simulated annealing algorithm. The resultant parallel implementation, based on a tree of speculative computations, yields a speedup of about log p on p processors.
Reference: [3] <author> M. Dubios and F.A. Briggs. </author> <title> Performance of synchronized iterative processes in multiprocessor systems. </title> <journal> IEEE Trans. Software Eng., </journal> <volume> SE-8(4), </volume> <month> July </month> <year> 1982. </year>
Reference-contexts: The concept of speculative computation to overcome communication delays can be applied to a host of parallel algorithms. However, in this paper, we focus on applying the technique to a 2 class of synchronous iterative algorithms <ref> [1, 3] </ref>. Section 2 describes the synchronous iterative al-gorithm model employed. Section 3 discusses the speculative computing technique and associated performance benefits. In section 4, we develop an empirical performance model to estimate the performance of speculative computation. <p> Our example, implemented on a network of SUN/Sparc workstations, showed a significant performance improvement (up to 34% on 16 processors) with speculative computation. The model and measured performance are then compared. 2 Synchronous Iterative Algorithms Synchronous iterative algorithms <ref> [1, 3] </ref> include most of the compute intensive numerical methods used in science and engineering applications. Iterative techniques to solve linear and non-linear equations, solution of partial differential equations, numerical integration, particle simulation, etc., are some examples.
Reference: [4] <author> Mark Franklin and Vasudha Govindan. </author> <title> The N-body Problem: Distributed System Load Balancing and Performance Evaluation. </title> <booktitle> In Proceedings of the 6th International Conference on Parallel and Distributed Computing Systems. ISCA, </booktitle> <month> October </month> <year> 1993. </year> <institution> Louisville, Kentucky, USA. </institution>
Reference-contexts: The force between any two particles is given by Newton's universal law of gravitation. The resultant force on a particle is the vector sum of all its pairwise forces. The 1 A more efficient O (N log N) is possible and has been implemented in the past <ref> [4] </ref>.
Reference: [5] <author> Anoop Gupta, John Hennessey, Kourosh Gharachorloo, Todd Mowry, and Wolf-Dietrich Weber. </author> <title> Comparitive evaluation of latency reducing and tolerating techniques. </title> <booktitle> In Proceedings of International Symposium on Computer Architecture, </booktitle> <pages> pages 254-263, </pages> <year> 1991. </year>
Reference-contexts: Overcoming communication delays in message passing systems is analogous to the problem of overcoming memory access delays in shared memory systems. There has been considerable work on developing and analyzing techniques for overlapping remote memory access delays in medium to large scale shared memory systems <ref> [5] </ref>. Coherent caches, data prefetching and multithreading have been proposed and implemented in some systems. Speculative computing overcomes communication delays by effectively overlapping the communication time with useful computation. While waiting for a message, the processor speculates the contents of the message and uses the speculated values in its computation.
Reference: [6] <author> Gary M. Johnson. </author> <title> Exploiting parallelism in computational science. </title> <journal> Future Generation computer Systems, </journal> <volume> 5 </volume> <pages> 319-337, </pages> <year> 1989. </year>
Reference-contexts: 1 Introduction Several compute intensive problems in science and engineering have found in parallel processing an attractive solution to their high computing needs <ref> [6] </ref>. The problem is appropriately partitioned and mapped on to the available processors and the processors cooperate to solve the problem. One of the major sources of performance degradation in parallel computing is the communication delay.
Reference: [7] <author> Joel Saltz and Vijay Naik. </author> <title> Towards developing robust algorithms for solving partial differential equations on mimd machines. </title> <journal> Parallel Computing, </journal> <volume> 6 </volume> <pages> 19-44, </pages> <year> 1988. </year>
Reference-contexts: Overcoming communication delays in parallel algorithms has become increasingly important in such distributed computing systems. A number of algorithmic improvements have been proposed to overcome communication delays in parallel algorithms. Saltz et.al <ref> [7, 8] </ref> suggest several techniques to develop robust algorithms for solving partial differential equations on hypercube machines. They propose partitioning strategies to reduce communication traffic, efficient convergence check mechanisms and concurrent execution of multiple time-steps.
Reference: [8] <author> Joel Saltz, Vijay Naik, and David Nicol. </author> <title> Reduction of the effects of communication delays in scientific algorithms on message passing mimd architectures. </title> <journal> SIAM J. SCI. STAT. COMPUT., </journal> <volume> 8(1), </volume> <month> Jan </month> <year> 1987. </year>
Reference-contexts: Overcoming communication delays in parallel algorithms has become increasingly important in such distributed computing systems. A number of algorithmic improvements have been proposed to overcome communication delays in parallel algorithms. Saltz et.al <ref> [7, 8] </ref> suggest several techniques to develop robust algorithms for solving partial differential equations on hypercube machines. They propose partitioning strategies to reduce communication traffic, efficient convergence check mechanisms and concurrent execution of multiple time-steps.
Reference: [9] <author> Vaidy S. Sunderam. </author> <title> PVM: A Framework for Parallel Distributed Computing. </title> <journal> Concurrency: Practice and Experience, </journal> <volume> 2, </volume> <month> december </month> <year> 1990. </year>
Reference: [10] <author> Ellen Witte, Roger Chamberlian, and Mark Franklin. </author> <title> Parallel simulated annealing using spec ulative computation. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 2(4) </volume> <pages> 483-494, </pages> <month> october </month> <year> 1991. </year>
Reference-contexts: If the error in speculation is large, the resulting computation is corrected or recomputed. If the error is "small", the resulting computation is accepted, and P 1 has effectively "masked" the communication delay. Speculative computation has been proposed [2] and implemented <ref> [10] </ref> to incorporate parallelism into inherently serial algorithms. Work is performed before it is known whether it is needed or not. If the work is needed, it has already been performed, thus speeding up the computation time. Some work may never be needed and thus is wasted. <p> Some work may never be needed and thus is wasted. To gain from speculative computation it is critical to identify future work which is likely to be needed. Burton [2] proposes a simple functional language to incorporate speculative computation in inherently serial algorithms. Witte et.al <ref> [10] </ref> apply speculative computing to the simulated annealing algorithm. The resultant parallel implementation, based on a tree of speculative computations, yields a speedup of about log p on p processors. The concept of speculative computation to overcome communication delays can be applied to a host of parallel algorithms.
Reference: [11] <author> David E. Womble. </author> <title> The performance of asynchronous algorithms on hypercubes. </title> <type> Technical Report SAND88-2714, </type> <institution> Sandia National Laboratories, </institution> <address> Albuquerque, New Mexico, </address> <month> Dec </month> <year> 1988. </year> <month> 16 </month>
Reference-contexts: They propose partitioning strategies to reduce communication traffic, efficient convergence check mechanisms and concurrent execution of multiple time-steps. While their techniques are shown to be effective, they are limited to hypercube machines and to a small class of algorithms. Techniques such as asynchronous algorithms <ref> [1, 11] </ref> and rollback synchronization [1] mask communication delays but are applicable only to a very small set of applications. Overcoming communication delays in message passing systems is analogous to the problem of overcoming memory access delays in shared memory systems.
References-found: 11

