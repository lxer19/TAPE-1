URL: http://www.cs.cmu.edu/afs/cs.cmu.edu/project/scandal/public/papers/CMU-CS-97-129.ps.gz
Refering-URL: http://www.cs.cmu.edu/afs/cs.cmu.edu/project/scandal/public/papers/CMU-CS-97-129.html
Root-URL: 
Title: Implementation and Evaluation of an Efficient 2D Parallel Delaunay Triangulation Algorithm  
Author: Jonathan C. Hardwick 
Note: The view and conclusions contained in this document are those of the author and should not be interpreted as necessarily representing the official policies or endorsements, either expressed or implied, of ARPA, the Phillips Laboratory, or the U.S. Government.  
Address: Pittsburgh, PA 15213  
Affiliation: School of Computer Science Carnegie Mellon University  
Date: April 1997  
Pubnum: CMU-CS-97-129  
Abstract: An earlier version of this paper appears in "Proceedings of the 9th Annual ACM Symposium on Parallel Algorithms and Architectures", June 1997 [23]. This work was supported in part by ARPA Contract Number DABT-63-96-C-0071, "Hierarchical, Automated Formal Hardware Verification: From Transistors to Abstract Protocols", and in part by the National Center for Supercomputing Applications, who provided the use of an SGI Power Challenge under grant number ACS930003N, and in part by the Pittsburgh Supercomputer Center, who provided the use of a Cray T3D and a DEC AlphaCluster under grant number SCTJ8LP, and in part by the Phillips Laboratory, Air Force Material Command, USAF, through the use of an IBM SP2 at the Maui High Performance Computing Center under cooperative agreement number F29601-93-2-0001. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> A. Aggarwal, B. Chazelle, L. Guibas, C. O Dunlaing, and C. Yap. </author> <title> Parallel computational geometry. </title> <journal> Algorithmica, </journal> <volume> 3(3) </volume> <pages> 293-327, </pages> <year> 1988. </year>
Reference-contexts: Since these algorithms are time and memory intensive, parallel implementations are important both for improved performance and to allow the solution of problems that are too large for serial machines. However, although several parallel algorithms for Delaunay triangulation have been described <ref> [1, 32, 13, 27, 20] </ref>, practical implementations have been slower to appear. One reason is that the dynamic nature of the problem can result in significant inter-processor communication.
Reference: [2] <author> Franz Aurenhammer. </author> <title> Voronoi diagrams|a survey of a fundamental geometric data structure. </title> <journal> ACM Computing Surveys, </journal> <volume> 23(3) </volume> <pages> 345-405, </pages> <month> September </month> <year> 1991. </year>
Reference-contexts: Delaunay triangulations and their duals, Voronoi diagrams, are among the most widely-studied structures in computational geometry. Voronoi diagrams have also appeared in many other fields under different names <ref> [2] </ref>; domains of action in crystallography, Wigner-Seitz zones in metallurgy, Thiessen polygons in geography, and Blum's transforms in biology.
Reference: [3] <author> Tom H. Axford. </author> <title> The divide-and-conquer paradigm as a basis for parallel language design. </title> <booktitle> In Advances in Parallel Algorithms, chapter 2. </booktitle> <publisher> Blackwell, </publisher> <year> 1992. </year>
Reference-contexts: The subproblems that are generated can typically be solved independently, and hence divide-and-conquer algorithms have long been recognized as presenting a potential source of parallelism. This has resulted in many architectures and parallel programming languages being designed specifically for the implementation of divide-and-conquer algorithms (see <ref> [3] </ref> for a survey). However, previous parallel divide-and-conquer models have typically been limited to regular algorithms, in which the subproblems are of equal size. This excludes a very useful class of algorithms; 2 for example, quicksort, selection, and many computational geometry algorithms all have an irreg-ular divide-and-conquer structure.
Reference: [4] <author> C. Bradford Barber, David P. Dobkin, and Hannu Huhdanpaa. </author> <title> The quickhull algorithm for convex hulls. </title> <journal> ACM Transactions on Mathematical Software, </journal> <volume> 22(4) </volume> <pages> 469-483, </pages> <month> December </month> <year> 1996. </year>
Reference-contexts: There are many well-known serial algorithms for Delaunay triangulation. The best have been extensively analyzed [17, 36], and implemented as general-purpose libraries <ref> [4, 33] </ref>. Since these algorithms are time and memory intensive, parallel implementations are important both for improved performance and to allow the solution of problems that are too large for serial machines.
Reference: [5] <author> Stephen T. Barnard. PMRSB: </author> <title> Parallel multilevel recursive spectral bisection. </title> <booktitle> In Proceedings of Supercomputing '95. ACM, </booktitle> <month> December </month> <year> 1995. </year>
Reference-contexts: To achieve the first goal, Machiavelli uses recursive subdivision of asynchronous teams of processors running SPMD code to directly implement the behavior of a divide-and-conquer algorithm (this can be seen a generalized version of the technique used by Barnard's spectral bisection algorithm on the Cray T3D <ref> [5] </ref>). To achieve the second goal, Machiavelli is implemented using C and MPI (the standard Message Passing Interface [18]).
Reference: [6] <author> Guy E. Blelloch. </author> <title> Vector Models for Data-Parallel Computing. </title> <publisher> MIT Press, </publisher> <year> 1990. </year>
Reference-contexts: For example, although nested data-parallel languages such as Nesl [7] and Proteus [31] are well-suited for expressing irregular divide-and-conquer algorithms, their current implementation layer assumes a vector PRAM model <ref> [6] </ref>. This can be efficiently implemented on vector processors with high memory bandwidth, but it is harder to do so on current RISC-based NUMA multiprocessor architectures, due to the higher relative costs of communication and poor data locality [21].
Reference: [7] <author> Guy E. Blelloch, Jonathan C. Hardwick, Jay Sipelstein, Marco Zagha, and Siddhartha Chat-terjee. </author> <title> Implementation of a portable nested data-parallel language. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 21(1) </volume> <pages> 4-14, </pages> <month> April </month> <year> 1994. </year>
Reference-contexts: It is divide-and-conquer in style but uses a "marriage before conquest" approach to eliminate the expensive merge step that has hindered previous parallel algorithms. Additionally, when prototyped in the nested data-parallel language Nesl <ref> [7] </ref>, the algorithm was found to perform only twice as many floating-point operations as a good serial algorithm. 1 level of the outer divide-and-conquer triangulation algorithm, which has a perfect split, uses as a substep a divide-and-conquer convex hull algorithm which may have a much more irregular structure. <p> The alternative approach of using a more general language model to handle irregular algorithms runs the risk of hiding divide-and-conquer parallelism that would otherwise be easy to exploit. For example, although nested data-parallel languages such as Nesl <ref> [7] </ref> and Proteus [31] are well-suited for expressing irregular divide-and-conquer algorithms, their current implementation layer assumes a vector PRAM model [6].
Reference: [8] <author> Guy E. Blelloch, Gary L. Miller, and Dafna Talmor. </author> <title> Developing a practical projection-based parallel Delaunay algorithm. </title> <booktitle> In Proceedings of the 12th Annual Symposium on Computational Geometry. ACM, </booktitle> <month> May </month> <year> 1996. </year>
Reference-contexts: Blelloch, Miller and Talmor recently developed a CREW PRAM algorithm that does not rely on bucketing and hence can efficiently handle non-uniform datasets <ref> [8] </ref>. It is divide-and-conquer in style but uses a "marriage before conquest" approach to eliminate the expensive merge step that has hindered previous parallel algorithms. <p> The resulting algorithm is divide-and-conquer in nature but uses a "marriage before conquest" approach, similar to the DeWall triangulation algorithm [12], which enables it to avoid an expensive merge step. See <ref> [8] </ref> for more details, and http://web.scandal.cs.cmu.edu/cgi-bin/demo for an interactive demonstration. 4 Algorithm: ParDel (P; B; T ) Input: P , a set of points in R 2 , B, a set of Delaunay edges of P which is the border of a region in R 2 containing P , and T <p> Return ParDel (P L ; B L ; T L ) [ ParDel (P R ; B R ; T R ). algorithm by Blelloch et al <ref> [8] </ref> as a coarse partitioner (a correction has been made to step 5). Although the algorithm uses alternating x and y cuts, for simplicity only the x cut is shown. The three subroutines Serial, Lower Convex Hull, and Border Merge are described in the text. <p> To test parallel efficiency, we compared timings to those on one processor, when the program immediately switches to the serial Triangle package [33]. To test the ability to handle non-uniform datasets we used four different distributions taken from <ref> [8] </ref>: Uniform distribution: The coordinates x and y are chosen at random within the unit square. Normal distribution: The coordinates x and y are chosen as independent samples from the normal distribution.
Reference: [9] <author> Timothy M. Y. Chan, Jack Snoeyink, and Chee-Keng Yap. </author> <title> Output-sensitive construction of polytopes in four dimensions and clipped Voronoi diagrams in three. </title> <booktitle> In Proceedings of the 6th Annual ACM-SIAM Symposium on Discrete Algorithms, </booktitle> <pages> pages 282-291, </pages> <year> 1995. </year>
Reference-contexts: However, Blelloch et al found experimentally that a simple quickhull [30] was faster than a more complicated convex hull algorithm that was guaranteed to take linear time. Furthermore, using a point-pruning version of quickhull that limits possible imbalances between recursive calls <ref> [9] </ref> reduces its sensitivity to non-uniform datasets. With these changes, the parallel Delaunay triangulation algorithm was found to perform about twice as many floating-point operations as Dwyer's algorithm [16]. <p> The basic quickhull algorithm tends to pick extreme "pivot" points when operating on nonuniform point distributions, resulting in a poor division of data and a consequent lack of progress. Chan et al <ref> [9] </ref> describe a variant that tests the slope between pairs of points and uses pruning to guarantee that recursive calls have at most 3/4 of the original points. However, pairing all n points and finding the median of their slopes is a significant addition to the basic cost of quickhull. <p> The final p n-pair pruning quickhull was benchmarked against both a basic quickhull and the original n-pair pruning quickhull by Chan et al <ref> [9] </ref>. Results for an extreme case are shown in Figure 9. As can be seen, the n-pair algorithm is more than twice as fast as the basic quickhull on the non-uniform Kuzmin dataset (over all the datasets and machine sizes tested it was a factor of 1.03-2.83 faster).
Reference: [10] <author> L. Paul Chew, Nikos Chrisochoides, and Florian Sukup. </author> <title> Parallel constrained Delaunay meshing. </title> <booktitle> In Proceedings of the Joint ASME/ASCE/SES Summer Meeting Special Symposium on Trends in Unstructured Mesh Generation, </booktitle> <month> June </month> <year> 1997. </year> <note> To appear. </note>
Reference-contexts: Both of these results were for uniform datasets. The 2D algorithm by Chew et al <ref> [10] </ref> (which solves the more general problem of constrained Delaunay triangulation in a meshing algorithm) achieves speedup factors of 3 on an 8-processor SP2, but currently requires that the boundaries between processors be created by hand.
Reference: [11] <author> P. Cignoni, D. Laforenza, C. Montani, R. Perego, and R. Scopigno. </author> <title> Evaluation of parallelization strategies for an incremental Delaunay triangulator in E 3 . Technical Report C93-17, </title> <institution> Consiglio Nazionale delle Ricerche, </institution> <month> November </month> <year> 1993. </year>
Reference-contexts: The use of decomposition techniques such as bucketing <ref> [28, 11, 37, 35] </ref>, or striping [14] can also reduce communication, but relies on the input dataset having a uniform spatial distribution of points in order to avoid load imbalances between processors. <p> Of those that do, the 3D algorithm by Teng et al [37] was up to 5 times slower on non-uniform datasets than on uniform ones (on a 32-processor CM-5), while the 3D algorithm by Cignoni et al <ref> [11] </ref> was up to 10 times slower on non-uniform datasets than on uniform ones (on a 128-processor nCUBE). The 2D algorithm by Ding and Densham [15] is designed to be able to handle non-uniform datasets, but has only been demonstrated to scale to 2 processors. <p> Since Machiavelli uses MPI [18] as a communication mechanism, the resulting Delaunay triangulation program is more portable than most previous implementations, which have used vendor-specific message-passing libraries <ref> [14, 28, 11, 37] </ref> or shared memory directives [35]. We present results for a loosely-coupled DEC AlphaCluster, a distributed-memory IBM SP2, a distributed-memory Cray T3D, and a shared-memory SGI Power Challenge.
Reference: [12] <author> P. Cignoni, C. Montani, R. Perego, and R. Scopigno. </author> <title> Parallel 3D Delaunay triangulation. </title> <booktitle> In Proceedings of the Computer Graphics Forum (Eurographics '93), </booktitle> <pages> pages 129-142, </pages> <year> 1993. </year>
Reference-contexts: The algorithm uses the well-known reduction of two-dimensional Delaunay triangulation to the problem of finding the three-dimensional convex hull of points on a sphere or paraboloid. The resulting algorithm is divide-and-conquer in nature but uses a "marriage before conquest" approach, similar to the DeWall triangulation algorithm <ref> [12] </ref>, which enables it to avoid an expensive merge step.
Reference: [13] <author> Richard Cole, Michael T. Goodrich, and Colm O Dunlaing. </author> <title> Merging free trees in parallel for efficient Voronoi diagram construction. </title> <booktitle> In Proceedings of the 17th International Colloquium on Automata, Languages and Programming, </booktitle> <pages> pages 32-45, </pages> <month> July </month> <year> 1990. </year>
Reference-contexts: Since these algorithms are time and memory intensive, parallel implementations are important both for improved performance and to allow the solution of problems that are too large for serial machines. However, although several parallel algorithms for Delaunay triangulation have been described <ref> [1, 32, 13, 27, 20] </ref>, practical implementations have been slower to appear. One reason is that the dynamic nature of the problem can result in significant inter-processor communication.
Reference: [14] <author> J.R. Davy and P.M. Dew. </author> <title> A note on improving the performance of Delaunay triangulation. </title> <booktitle> In Proceedings of Computer Graphics International '89, </booktitle> <pages> pages 209-226, </pages> <year> 1989. </year>
Reference-contexts: The use of decomposition techniques such as bucketing [28, 11, 37, 35], or striping <ref> [14] </ref> can also reduce communication, but relies on the input dataset having a uniform spatial distribution of points in order to avoid load imbalances between processors. Unfortunately, while most real-world problems are not this uniform, few authors report the performance of their implementations on non-uniform datasets. <p> Since Machiavelli uses MPI [18] as a communication mechanism, the resulting Delaunay triangulation program is more portable than most previous implementations, which have used vendor-specific message-passing libraries <ref> [14, 28, 11, 37] </ref> or shared memory directives [35]. We present results for a loosely-coupled DEC AlphaCluster, a distributed-memory IBM SP2, a distributed-memory Cray T3D, and a shared-memory SGI Power Challenge.
Reference: [15] <author> Yuemin Ding and Paul J. Densham. </author> <title> Dynamic and recursive parallel algorithms for constructing Delaunay triangulations. </title> <booktitle> In Proceedings of the Sixth International Symposium on Spatial Data Handling, </booktitle> <pages> pages 682-696. </pages> <publisher> Taylor & Francis, </publisher> <year> 1994. </year> <month> 15 </month>
Reference-contexts: The 2D algorithm by Ding and Densham <ref> [15] </ref> is designed to be able to handle non-uniform datasets, but has only been demonstrated to scale to 2 processors. A second problem is that the parallel algorithms are typically much more complex than their serial counterparts.
Reference: [16] <author> R.A. Dwyer. </author> <title> A simple divide-and-conquer algorithm for constructing Delaunay triangulations in O(n log log n) expected time. </title> <booktitle> In Proceedings of the 2nd Annual Symposium on Computational Geometry, </booktitle> <pages> pages 276-284. </pages> <publisher> ACM, </publisher> <month> May </month> <year> 1986. </year>
Reference-contexts: The three subroutines Serial, Lower Convex Hull, and Border Merge are described in the text. Pseudocode for the algorithm is shown in Figure 3. It has three important substeps: Serial Delaunay: Although any serial Delaunay triangulation algorithm can be used for the base case, Dwyer's <ref> [16] </ref> is recommended since it has been shown experimentally to be the fastest [36, 33]. Lower convex hull: The lower half of the convex hull of the projected points is used to find a new path H that divides the problem into two smaller problems. <p> Furthermore, using a point-pruning version of quickhull that limits possible imbalances between recursive calls [9] reduces its sensitivity to non-uniform datasets. With these changes, the parallel Delaunay triangulation algorithm was found to perform about twice as many floating-point operations as Dwyer's algorithm <ref> [16] </ref>. Furthermore, the cumulative floating-point operation count was found to increase uniformly with recursion depth, indicating that the algorithm should be usable as a partitioner without loss of efficiency. However, as implemented in Nesl, the algorithm was an order of magnitude slower on one processor than a good serial algorithm.
Reference: [17] <author> Steven Fortune. </author> <title> Voronoi diagrams and Delaunay triangulations. </title> <editor> In Ding-Zhu Du and Frank Hwang, editors, </editor> <booktitle> Computing in Euclidean Geometry, </booktitle> <pages> pages 193-233. </pages> <publisher> World Scientific, </publisher> <year> 1992. </year>
Reference-contexts: There are many well-known serial algorithms for Delaunay triangulation. The best have been extensively analyzed <ref> [17, 36] </ref>, and implemented as general-purpose libraries [4, 33]. Since these algorithms are time and memory intensive, parallel implementations are important both for improved performance and to allow the solution of problems that are too large for serial machines.
Reference: [18] <author> Message Passing Interface Forum. </author> <title> MPI: A message-passing interface standard. </title> <journal> International Journal of Supercomputing Applications and High Performance Computing, </journal> <volume> 8(3/4), </volume> <year> 1994. </year>
Reference-contexts: It is particularly well-suited to exploiting the nested divide-and-conquer nature of the algorithm by Blelloch et al, which uses an inner quickhull algorithm as a substep, as shown in Figure 1. Since Machiavelli uses MPI <ref> [18] </ref> as a communication mechanism, the resulting Delaunay triangulation program is more portable than most previous implementations, which have used vendor-specific message-passing libraries [14, 28, 11, 37] or shared memory directives [35]. <p> To achieve the second goal, Machiavelli is implemented using C and MPI (the standard Message Passing Interface <ref> [18] </ref>). To achieve the third goal, Machiavelli obtains parallelism from both data-parallel operations within teams and from the task-parallel invocation of recursive functions on different teams, and uses good serial code where possible. The Machiavelli toolkit currently consists of a library of vector primitives and a small run-time system. <p> (T_new-&gt;nproc == 1) - /* Then run serial Delaunay triangulation */ result = serial_DT (P_new, B_new); else /* Else recurse in parallel code in new team */ result = parallel_DT (T_new, P_new, B_new); - /* Teams rejoin here */ return result; - code with calls to Machiavelli library. derived datatypes <ref> [18] </ref>. A purely data-parallel operation (such as an elementwise mathematical function on a vector) is implemented as a loop over the appropriate section of data on each processor.
Reference: [19] <author> Sanjay Goil, Srinivas Aluru, and Sanjay Ranka. </author> <title> Concatenated parallelism: A technique for efficient parallel divide and conquer. </title> <booktitle> In Proceedings of the 8th IEEE Symposium on Parallel and Distributed Processing '96, </booktitle> <pages> pages 488-495, </pages> <year> 1996. </year>
Reference-contexts: Concatenated parallelism is a notable recent exception that can handle irregular divide-and-conquer algorithms, but can only outperform a task-parallel approach when the communication cost of redistributing the data is significant compared to the computational cost of subdividing the task <ref> [19] </ref>. The alternative approach of using a more general language model to handle irregular algorithms runs the risk of hiding divide-and-conquer parallelism that would otherwise be easy to exploit.
Reference: [20] <author> Sumanta Guha. </author> <title> An optimal mesh computer algorithm for constrained Delaunay triangulation. </title> <booktitle> In Proceedings of the 8th International Parallel Processing Symposium, </booktitle> <pages> pages 102-109. </pages> <publisher> IEEE, </publisher> <month> April </month> <year> 1994. </year>
Reference-contexts: Since these algorithms are time and memory intensive, parallel implementations are important both for improved performance and to allow the solution of problems that are too large for serial machines. However, although several parallel algorithms for Delaunay triangulation have been described <ref> [1, 32, 13, 27, 20] </ref>, practical implementations have been slower to appear. One reason is that the dynamic nature of the problem can result in significant inter-processor communication.
Reference: [21] <author> Jonathan C. Hardwick. </author> <title> Porting a vector library: a comparison of MPI, Paris, CMMD and PVM. </title> <booktitle> In Proceedings of the 1994 Scalable Parallel Libraries Conference, </booktitle> <pages> pages 68-77, </pages> <month> October </month> <year> 1994. </year>
Reference-contexts: This can be efficiently implemented on vector processors with high memory bandwidth, but it is harder to do so on current RISC-based NUMA multiprocessor architectures, due to the higher relative costs of communication and poor data locality <ref> [21] </ref>. Machiavelli [24] is a new parallel toolkit for divide-and-conquer algorithms that is intended to alleviate some of these problems. It is designed to be usable both as an implementation layer for languages such as Nesl, and as a programmer's toolkit for the direct implementation of efficient parallel programs.
Reference: [22] <author> Jonathan C. Hardwick. </author> <title> An efficient implementation of nested data parallelism for irregular divide-and-conquer algorithms. </title> <booktitle> In Proceedings of the First International Workshop on High-Level Programming Models and Supportive Environments, </booktitle> <month> April </month> <year> 1996. </year>
Reference-contexts: The run-time system adds the ability to perform dynamic load-balancing for irregular algorithms. Specifically, it can ship a recursive serial function call to an idle processor in order to redistribute computation <ref> [22] </ref>. A Machiavellian divide-and-conquer program consists of both serial and SPMD parallel code. The parallel code operates at the upper levels of recursion, and uses calls to the library to redistribute data and subdivide the problem. Initially, data is distributed in block fashion across all the processors.
Reference: [23] <author> Jonathan C. Hardwick. </author> <title> Implementation and evaluation of an efficient parallel Delaunay triangulation algorithm. </title> <booktitle> In Proceedings of the 9th Annual ACM Symposium on Parallel Algorithms and Architectures, </booktitle> <month> June </month> <year> 1997. </year> <note> To appear. </note>
Reference: [24] <author> Jonathan C. Hardwick. </author> <title> Practical Parallel Divide-and-Conquer Algorithms. </title> <type> PhD thesis, </type> <institution> School of Computer Science, Carnegie Mellon University, </institution> <year> 1997. </year> <note> To appear. </note>
Reference-contexts: The program was parallelized using the Machiavelli toolkit <ref> [24] </ref>, which has been designed both for the direct implementation of parallel divide-and-conquer algorithms (as in this case), and as an implementation layer for nested data-parallel languages. <p> This can be efficiently implemented on vector processors with high memory bandwidth, but it is harder to do so on current RISC-based NUMA multiprocessor architectures, due to the higher relative costs of communication and poor data locality [21]. Machiavelli <ref> [24] </ref> is a new parallel toolkit for divide-and-conquer algorithms that is intended to alleviate some of these problems. It is designed to be usable both as an implementation layer for languages such as Nesl, and as a programmer's toolkit for the direct implementation of efficient parallel programs.
Reference: [25] <author> C.A.R. Hoare. </author> <title> Algorithm 63 (partition) and algorithm 65 (find). </title> <journal> Communications of the ACM, </journal> <volume> 4(7) </volume> <pages> 321-322, </pages> <year> 1961. </year>
Reference-contexts: These translate between the pointer-based format of Triangle, which is optimized for serial code, and the indexed format with replication used by the parallel code. No changes are necessary to the source code of Triangle. 6 Finding the median Initially a parallel version of quickmedian <ref> [25] </ref> was used to find the median internal point along the x or y axis. Quickmedian redistributes data amongst the processors on each recursive step, resulting in high communication overhead.
Reference: [26] <author> C.A.R. Hoare. </author> <title> Quicksort. </title> <journal> The Computer Journal, </journal> <volume> 5(1) </volume> <pages> 10-15, </pages> <year> 1962. </year>
Reference-contexts: Section 5 presents and analyzes performance results for a range of input distributions. Finally, Section 6 concludes the paper. 2 The Machiavelli Toolkit Many of the most efficient and widely used computer algorithms use a divide-and-conquer approach to solving a problem. Examples include binary search, quicksort <ref> [26] </ref>, and fast matrix multiplication [34]. The subproblems that are generated can typically be solved independently, and hence divide-and-conquer algorithms have long been recognized as presenting a potential source of parallelism.
Reference: [27] <author> J. Andrew Holey and Oscar H. Ibarra. </author> <title> Triangulation, Voronoi diagram, and convex hull in k-space on mesh-connected arrays and hypercubes. </title> <booktitle> In Proceedings of the 1991 International Conference on Parallel Processing, volume III, Algorithms & Applications. </booktitle> <publisher> CRC Press, </publisher> <month> August </month> <year> 1991. </year>
Reference-contexts: Since these algorithms are time and memory intensive, parallel implementations are important both for improved performance and to allow the solution of problems that are too large for serial machines. However, although several parallel algorithms for Delaunay triangulation have been described <ref> [1, 32, 13, 27, 20] </ref>, practical implementations have been slower to appear. One reason is that the dynamic nature of the problem can result in significant inter-processor communication.
Reference: [28] <author> Marshal L. Merriam. </author> <title> Parallel implementation of an algorithm for Delaunay triangulation. </title> <booktitle> In Proceedings of Computational Fluid Dynamics, </booktitle> <volume> volume 2, </volume> <pages> pages 907-912, </pages> <month> September </month> <year> 1992. </year>
Reference-contexts: The use of decomposition techniques such as bucketing <ref> [28, 11, 37, 35] </ref>, or striping [14] can also reduce communication, but relies on the input dataset having a uniform spatial distribution of points in order to avoid load imbalances between processors. <p> Again, direct comparison is difficult because few authors quote speedups over good serial code. Of those that do, the 2D algorithm by Su achieved speedup factors of 3.5-5.5 on a 32-processor KSR-1 [35], for a parallel efficiency of 11-17%, while the 3D algorithm <ref> [28] </ref> by Merriam achieved speedup factors of 6-20 on a 128-processor Intel Gamma, for a parallel efficiency of 5-16%. Both of these results were for uniform datasets. <p> Since Machiavelli uses MPI [18] as a communication mechanism, the resulting Delaunay triangulation program is more portable than most previous implementations, which have used vendor-specific message-passing libraries <ref> [14, 28, 11, 37] </ref> or shared memory directives [35]. We present results for a loosely-coupled DEC AlphaCluster, a distributed-memory IBM SP2, a distributed-memory Cray T3D, and a shared-memory SGI Power Challenge.
Reference: [29] <author> Mark H. Overmars and Jan van Leeuwen. </author> <title> Maintenance of configurations in the plane. </title> <journal> Journal of Computer and System Sciences, </journal> <volume> 23 </volume> <pages> 166-204, </pages> <year> 1981. </year>
Reference-contexts: The work complexity is optimal for Delaunay triangulation, and the depth complexity is practical for parallelization purposes. Note that these complexities assume that the lower convex hull substep is solved using a linear-work algorithm, which is possible since we can store the points in sorted order <ref> [29] </ref>. However, Blelloch et al found experimentally that a simple quickhull [30] was faster than a more complicated convex hull algorithm that was guaranteed to take linear time. Furthermore, using a point-pruning version of quickhull that limits possible imbalances between recursive calls [9] reduces its sensitivity to non-uniform datasets.
Reference: [30] <author> Franco P. Preparata and Michael Ian Shamos. </author> <title> Computational Geometry: An Introduction. Texts and Monographs in Computer Science. </title> <publisher> Springer-Verlag, </publisher> <year> 1985. </year> <month> 16 </month>
Reference-contexts: Note that these complexities assume that the lower convex hull substep is solved using a linear-work algorithm, which is possible since we can store the points in sorted order [29]. However, Blelloch et al found experimentally that a simple quickhull <ref> [30] </ref> was faster than a more complicated convex hull algorithm that was guaranteed to take linear time. Furthermore, using a point-pruning version of quickhull that limits possible imbalances between recursive calls [9] reduces its sensitivity to non-uniform datasets. <p> Finding the lower convex hull As in the original algorithm, a variant of quickhull <ref> [30] </ref> is used to find the convex hull, resulting in two nested recursive algorithms as shown in Figure 1. However, in contrast to using a different serial Delaunay triangulation algorithm, the serial code of quickhull implements the same algorithm as the parallel code.
Reference: [31] <author> Jan Prins and Daniel Palmer. </author> <title> Transforming high-level data-parallel programs into vector operations. </title> <booktitle> In Proceedings of the 4th ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming, </booktitle> <pages> pages 119-128. </pages> <publisher> ACM, </publisher> <month> May </month> <year> 1993. </year>
Reference-contexts: The alternative approach of using a more general language model to handle irregular algorithms runs the risk of hiding divide-and-conquer parallelism that would otherwise be easy to exploit. For example, although nested data-parallel languages such as Nesl [7] and Proteus <ref> [31] </ref> are well-suited for expressing irregular divide-and-conquer algorithms, their current implementation layer assumes a vector PRAM model [6].
Reference: [32] <author> John H. Reif and Sandeep Sen. </author> <title> Polling: A new randomized sampling technique for computational geometry. </title> <booktitle> In Proceedings of the 21st Annual ACM Symposium on Theory of Computing, </booktitle> <pages> pages 394-404, </pages> <year> 1989. </year>
Reference-contexts: Since these algorithms are time and memory intensive, parallel implementations are important both for improved performance and to allow the solution of problems that are too large for serial machines. However, although several parallel algorithms for Delaunay triangulation have been described <ref> [1, 32, 13, 27, 20] </ref>, practical implementations have been slower to appear. One reason is that the dynamic nature of the problem can result in significant inter-processor communication.
Reference: [33] <author> Jonathan Richard Shewchuk. </author> <title> Triangle: Engineering a 2D quality mesh generator and delaunay triangulator. </title> <editor> In Ming C. Lin and Dinesh Manocha, editors, </editor> <booktitle> Applied Computational Geometry: Towards Geometric Engineering, volume 1148 of Lecture Notes in Computer Science, </booktitle> <pages> pages 203-222. </pages> <publisher> Springer-Verlag, </publisher> <month> May </month> <year> 1996. </year>
Reference-contexts: There are many well-known serial algorithms for Delaunay triangulation. The best have been extensively analyzed [17, 36], and implemented as general-purpose libraries <ref> [4, 33] </ref>. Since these algorithms are time and memory intensive, parallel implementations are important both for improved performance and to allow the solution of problems that are too large for serial machines. <p> This paper describes a practical parallel Delaunay triangulation program which uses the algorithm by Blelloch et al as a coarse parallel partitioner, switching to an efficient implementation of Dwyer's serial algorithm provided by the Triangle package <ref> [33] </ref> at the leaves of the recursion tree. The program was parallelized using the Machiavelli toolkit [24], which has been designed both for the direct implementation of parallel divide-and-conquer algorithms (as in this case), and as an implementation layer for nested data-parallel languages. <p> Pseudocode for the algorithm is shown in Figure 3. It has three important substeps: Serial Delaunay: Although any serial Delaunay triangulation algorithm can be used for the base case, Dwyer's [16] is recommended since it has been shown experimentally to be the fastest <ref> [36, 33] </ref>. Lower convex hull: The lower half of the convex hull of the projected points is used to find a new path H that divides the problem into two smaller problems. Since the projection is based on the median point, the division is perfect, as shown in Figure 1. <p> Given these data structures, the operations of finding internal points, and projecting points onto a parabola (see Figure 3), both reduce to simple local loops. Serial triangulation For the serial base case we use the optimized version of Dwyer's algorithm that is provided by the Triangle mesh generation package <ref> [33] </ref>. Since the input format for Triangle differs from that used by the parallel program, conversion steps are necessary before and after calling it. These translate between the pointer-based format of Triangle, which is optimized for serial code, and the indexed format with replication used by the parallel code. <p> To test parallel efficiency, we compared timings to those on one processor, when the program immediately switches to the serial Triangle package <ref> [33] </ref>. To test the ability to handle non-uniform datasets we used four different distributions taken from [8]: Uniform distribution: The coordinates x and y are chosen at random within the unit square. Normal distribution: The coordinates x and y are chosen as independent samples from the normal distribution. <p> There are several important effects that can be seen here. First, the 9 architectures. The graphs show the time to triangulate a total of 128k points as the number of processors is varied. Single processor results are for good serial code (Triangle <ref> [33] </ref>). Increasing the number of processors results in more levels of recursion being spent in slower parallel code rather than faster serial code, and hence the speedup is not linear.
Reference: [34] <author> Volker Strassen. </author> <title> Gaussian elimination is not optimal. </title> <journal> Numerische Mathematik, </journal> <volume> 13(3) </volume> <pages> 354-356, </pages> <year> 1969. </year>
Reference-contexts: Finally, Section 6 concludes the paper. 2 The Machiavelli Toolkit Many of the most efficient and widely used computer algorithms use a divide-and-conquer approach to solving a problem. Examples include binary search, quicksort [26], and fast matrix multiplication <ref> [34] </ref>. The subproblems that are generated can typically be solved independently, and hence divide-and-conquer algorithms have long been recognized as presenting a potential source of parallelism. This has resulted in many architectures and parallel programming languages being designed specifically for the implementation of divide-and-conquer algorithms (see [3] for a survey).
Reference: [35] <author> Peter Su. </author> <title> Efficient parallel algorithms for closest point problems. </title> <type> PhD thesis, </type> <institution> Dartmouth College, </institution> <year> 1994. </year> <month> PCS-TR94-238. </month>
Reference-contexts: The use of decomposition techniques such as bucketing <ref> [28, 11, 37, 35] </ref>, or striping [14] can also reduce communication, but relies on the input dataset having a uniform spatial distribution of points in order to avoid load imbalances between processors. <p> Again, direct comparison is difficult because few authors quote speedups over good serial code. Of those that do, the 2D algorithm by Su achieved speedup factors of 3.5-5.5 on a 32-processor KSR-1 <ref> [35] </ref>, for a parallel efficiency of 11-17%, while the 3D algorithm [28] by Merriam achieved speedup factors of 6-20 on a 128-processor Intel Gamma, for a parallel efficiency of 5-16%. Both of these results were for uniform datasets. <p> Since Machiavelli uses MPI [18] as a communication mechanism, the resulting Delaunay triangulation program is more portable than most previous implementations, which have used vendor-specific message-passing libraries [14, 28, 11, 37] or shared memory directives <ref> [35] </ref>. We present results for a loosely-coupled DEC AlphaCluster, a distributed-memory IBM SP2, a distributed-memory Cray T3D, and a shared-memory SGI Power Challenge. Experimentally, the program achieves three times the parallel efficiency of previous implementations, and is at most 1.5 times slower on non-uniform datasets than on uniform ones.
Reference: [36] <author> Peter Su and Robert L. Drysdale. </author> <title> A comparison of sequential Delaunay triangulation algorithms. </title> <booktitle> In Proceedings of the 11th Annual Symposium on Computational Geometry, </booktitle> <pages> pages 61-70. </pages> <publisher> ACM, </publisher> <month> June </month> <year> 1995. </year>
Reference-contexts: There are many well-known serial algorithms for Delaunay triangulation. The best have been extensively analyzed <ref> [17, 36] </ref>, and implemented as general-purpose libraries [4, 33]. Since these algorithms are time and memory intensive, parallel implementations are important both for improved performance and to allow the solution of problems that are too large for serial machines. <p> Pseudocode for the algorithm is shown in Figure 3. It has three important substeps: Serial Delaunay: Although any serial Delaunay triangulation algorithm can be used for the base case, Dwyer's [16] is recommended since it has been shown experimentally to be the fastest <ref> [36, 33] </ref>. Lower convex hull: The lower half of the convex hull of the projected points is used to find a new path H that divides the problem into two smaller problems. Since the projection is based on the median point, the division is perfect, as shown in Figure 1.
Reference: [37] <author> Y. Ansel Teng, Francis Sullivan, Isabel Beichl, and Enrico Puppo. </author> <title> A data-parallel algorithm for three-dimensional Delaunay triangulation and its implementation. </title> <booktitle> In Proceedings of Supercomputing '93, </booktitle> <pages> pages 112-121. </pages> <publisher> ACM, </publisher> <month> November </month> <year> 1993. </year>
Reference-contexts: The use of decomposition techniques such as bucketing <ref> [28, 11, 37, 35] </ref>, or striping [14] can also reduce communication, but relies on the input dataset having a uniform spatial distribution of points in order to avoid load imbalances between processors. <p> Unfortunately, while most real-world problems are not this uniform, few authors report the performance of their implementations on non-uniform datasets. Of those that do, the 3D algorithm by Teng et al <ref> [37] </ref> was up to 5 times slower on non-uniform datasets than on uniform ones (on a 32-processor CM-5), while the 3D algorithm by Cignoni et al [11] was up to 10 times slower on non-uniform datasets than on uniform ones (on a 128-processor nCUBE). <p> Since Machiavelli uses MPI [18] as a communication mechanism, the resulting Delaunay triangulation program is more portable than most previous implementations, which have used vendor-specific message-passing libraries <ref> [14, 28, 11, 37] </ref> or shared memory directives [35]. We present results for a loosely-coupled DEC AlphaCluster, a distributed-memory IBM SP2, a distributed-memory Cray T3D, and a shared-memory SGI Power Challenge.
Reference: [38] <author> N.A. Verhoeven, N.P. Weatherill, and K. Morgan. </author> <title> Dynamic load balancing in a 2D parallel Delaunay mesh generator. </title> <booktitle> In Parallel Computational Fluid Dynamics, </booktitle> <pages> pages 641-648. </pages> <publisher> Elsevier Science Publishers B.V. (North-Holland), </publisher> <month> June </month> <year> 1995. </year> <month> 17 </month>
Reference-contexts: One reason is that the dynamic nature of the problem can result in significant inter-processor communication. Performing key phases of the algorithm on a single processor (for example, serializing the merge step of a divide-and-conquer algorithm, as in <ref> [38] </ref>) reduces this communication, but introduces a serial bottleneck that severely limits scalability in terms of both parallel speedup and achievable problem size.
References-found: 38

