URL: http://www.eecs.uic.edu/~sloan/birk-circulate.ps
Refering-URL: http://www.eecs.uic.edu/~sloan/papers.html
Root-URL: 
Email: Email: sloan@eecs.uic.edu  
Title: Pac Learning, Noise, and Geometry  
Author: Robert H. Sloan 
Address: 851 S. Morgan St. Room 1120  Chicago, IL 60607-7053 USA  
Affiliation: Department of EE Computer Science  University of Illinois at Chicago  
Note: To appear in Learning and Geometry, David Kueker and Carl Smith, eds., Birkhauser.  Supported by NSF grant CCR-9108753.  
Abstract: This paper describes the probably approximately correct model of concept learning, paying special attention to the case where instances are points in Euclidean n-space. The problem of learning from noisy training data is also studied. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Dana Angluin. </author> <title> Queries and concept learning. </title> <journal> Machine Learning, </journal> <volume> 2 </volume> <pages> 319-342, </pages> <year> 1987. </year>
Reference-contexts: Thus our instance space X = [ 1 n=1 X n . For the sake of concreteness, in the rest of this paper we will always assume that our instance space X n is one of E n (Euclidean n-space), <ref> [0; 1] </ref> n , or f0; 1g n where C n 2 X n . We will use the term concept class to refer both to C and a particular C n . <p> This approach to pac learning is explored by Linial, Mansour and Rivest, and by Blumer et al. [8, 19]. 3 Pac learning In this section we define Valiant's model of concept learning [26]. Angluin dubbed this model "probably approximately correct learning," <ref> [1] </ref> and it is thus referred to as pac learning. In short, an algorithm pac learns from examples if it can, in a feasible amount of time, find (with high probability), a rule that is highly accurate. <p> However, this is not strictly true. The limitations imposed by Theorem 3 can be overcome by algorithms with slightly different behavior. For instance, the class of all finite unions of closed intervals in <ref> [0; 1] </ref> has infinite VC dimension, but does have a pac learning algorithm [19]. 5 The algorithm behaves roughly as follows.
Reference: [2] <author> Dana Angluin and Philip Laird. </author> <title> Learning from noisy examples. </title> <journal> Machine Learning, </journal> <volume> 2(4) </volume> <pages> 343-370, </pages> <year> 1988. </year>
Reference-contexts: The "desired," noiseless output of each oracle is a correctly labeled example (x; s), where x is drawn according to P. We now now give a precise description of the actual outputs from the following noise oracles: MAL - [27], RMC - <ref> [2] </ref>, MMC - [24, 25]. * When MAL is called, with probability 1-, it does indeed return a correctly labeled (x; s) where x is drawn according to P. With probability it returns an example (x; s) about which no assumptions whatsoever may be made. <p> One final set of examples is drawn, and the meta-algorithm tests each hypothesis of the regular algorithm on this set, choosing the hypothesis with the lowest disagreement rate. 10.2 Labeling noise Angluin and Laird studied the case where the only source of noise is random mislabeling <ref> [2] </ref>. They showed that in this case, for any noise rate strictly less than one half, it is sufficient to pick out the concept with minimum disagreement rate, or even any concept with low disagreement rate. This solves the problem of statistical pac learning.
Reference: [3] <author> Dana Angluin and Carl H. Smith. </author> <title> Inductive inference: Theory and methods. </title> <journal> Computing Surveys, </journal> <volume> 15(3) </volume> <pages> 237-269, </pages> <month> September </month> <year> 1983. </year>
Reference-contexts: More recently, the theoretical computer science community has taken an interest in this field. The work by this community began with Gold [11] back in the 1960s. (See Angluin and Smith <ref> [3] </ref> for an excellent survey of work through the early 1980s.) A seminal paper by Valiant [26] in 1984 prompted intensified effort.
Reference: [4] <author> Patrick Assouad. </author> <title> Densite et dimension. </title> <journal> Ann. Inst. Fourier, Grenoble, </journal> <volume> 33(3) </volume> <pages> 233-282, </pages> <year> 1983. </year>
Reference-contexts: The class of all spheres in E n has dimension n + 1. For any finite concept class C, we have V Cdim (C) log (jCj). x There are now a number of papers which calculate the VC dimension of different classes <ref> [4, 9, 14, 16] </ref>. The VC dimension was first studied in connection with statistical pattern recognition. Pollard and Vapnik have both written good books discussing it from that point of view [22, 28].
Reference: [5] <author> S. Ben-David, A. Itai, and E. Kushilevitz. </author> <title> Learning by distances. </title> <booktitle> In Proc. 3rd Annu. Workshop on Comput. Learning Theory, </booktitle> <pages> pages 232-245, </pages> <address> San Mateo, CA, 1990. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: For instance, we certainly require that every concept is a Borel set. A fair amount x Unless explicitly stated otherwise, all logarithms in this work are base 2. 12 has been written about this issue <ref> [5, 6, 8] </ref>. In practice, this is a nonissue. As computer scientists, we are really interested only in exceedingly well-behaved concept classes.
Reference: [6] <author> Shai Ben-David and Gyora M. Benedek. </author> <title> Measurability constraints on pac learnability. </title> <type> Technical report, </type> <institution> Technion, Haifa, </institution> <year> 1991. </year>
Reference-contexts: For instance, we certainly require that every concept is a Borel set. A fair amount x Unless explicitly stated otherwise, all logarithms in this work are base 2. 12 has been written about this issue <ref> [5, 6, 8] </ref>. In practice, this is a nonissue. As computer scientists, we are really interested only in exceedingly well-behaved concept classes.
Reference: [7] <author> Anselm Blumer, Andrzej Ehrenfeucht, David Haussler, and Manfred K. Warmuth. </author> <title> Occam's razor. </title> <journal> Information Processing Letters, </journal> <volume> 24 </volume> <pages> 377-380, </pages> <month> April </month> <year> 1987. </year>
Reference-contexts: We can extend this line of reasoning to considerably larger (finite) concept classes. Our treatment follows Blumer et al. <ref> [7] </ref>. Consider an algorithm that draws a sample of size m (jCj ; *; ffi), and then outputs any concept c 2 C that is consistent with the sample.
Reference: [8] <author> Anselm Blumer, Andrzej Ehrenfeucht, David Haussler, and Manfred K. Warmuth. </author> <title> Learnability and the Vapnik-Chervonenkis dimension. </title> <journal> Journal of the ACM, </journal> <volume> 36(4) </volume> <pages> 929-965, </pages> <year> 1989. </year>
Reference-contexts: We could then make C n the set of convex n-gons, or alternatively, the set of convex polygons with at most n edges. This approach to pac learning is explored by Linial, Mansour and Rivest, and by Blumer et al. <ref> [8, 19] </ref>. 3 Pac learning In this section we define Valiant's model of concept learning [26]. Angluin dubbed this model "probably approximately correct learning," [1] and it is thus referred to as pac learning. <p> Blumer et al. and Valiant both contain additional material on the motivation for the pac learning model <ref> [8, 26] </ref>. 5 Variations on the definition of pac learn ing The pac learning model can be altered in many ways. <p> In particular, Blumer et al. showed <ref> [8] </ref> Theorem 2 For any instance space X and "well-behaved" C 2 X , any learning algorithm which obtains a sample of size m max 4 log 2 8vcd (C) log 13 ! and returns any concept consistent with all m examples meets the learning criterion of the pac learning definition. <p> For instance, we certainly require that every concept is a Borel set. A fair amount x Unless explicitly stated otherwise, all logarithms in this work are base 2. 12 has been written about this issue <ref> [5, 6, 8] </ref>. In practice, this is a nonissue. As computer scientists, we are really interested only in exceedingly well-behaved concept classes. <p> Of course, it would be nicer to discover a good algorithm than to discover a hardness proof. Another important open class involves unions of boxes. Algorithms exist for learning arbitrary unions of axis-parallel rectangles in E 2 , and arbitrary nested differences of axis-parallel boxes in E n <ref> [8, 15] </ref>. Arbitrary unions of boxes (regardless of whether they are axis-parallel) in E n cannot be pac learnable since the VC dimension is not polynomial in n.
Reference: [9] <author> R. M. Dudley. </author> <title> A course on empirical processes. </title> <booktitle> In Lecture Notes in Mathematics No. </booktitle> <volume> 1097. </volume> <publisher> Springer-Verlag, </publisher> <year> 1984. </year>
Reference-contexts: The class of all spheres in E n has dimension n + 1. For any finite concept class C, we have V Cdim (C) log (jCj). x There are now a number of papers which calculate the VC dimension of different classes <ref> [4, 9, 14, 16] </ref>. The VC dimension was first studied in connection with statistical pattern recognition. Pollard and Vapnik have both written good books discussing it from that point of view [22, 28].
Reference: [10] <author> Andrzej Ehrenfeucht, David Haussler, Michael Kearns, and Leslie Valiant. </author> <title> A general lower bound on the number of examples needed for learning. </title> <journal> Information and Computation, </journal> <volume> 82 </volume> <pages> 247-261, </pages> <year> 1989. </year>
Reference-contexts: sometimes makes the problem of finding an acceptable concept much easier computationally, although in practice developing algorithms to find almost consistent concepts is typically still very difficult. 9.3 Lower bounds on sample complexity The VC dimension also gives us a sort of lower bound on sample complexity for pac learning <ref> [10] </ref>. Theorem 3 Fix a concept class C = f (X n ; C n )g n1 , such that jC n j 3 for sufficiently large n.
Reference: [11] <author> E. Mark Gold. </author> <title> Language identification in the limit. </title> <journal> Information and Control, </journal> <volume> 10 </volume> <pages> 447-474, </pages> <year> 1967. </year> <month> 18 </month>
Reference-contexts: This problem has been studied by the Artificial Intelligence (AI) community since the very beginning of that field|primarily in an empirical fashion. More recently, the theoretical computer science community has taken an interest in this field. The work by this community began with Gold <ref> [11] </ref> back in the 1960s. (See Angluin and Smith [3] for an excellent survey of work through the early 1980s.) A seminal paper by Valiant [26] in 1984 prompted intensified effort.
Reference: [12] <author> Sally A. Goldman and Robert H. Sloan. </author> <title> Can PAC learning algorithms tolerate random attribute noise? Algorithmica, </title> <note> 1995. To appear. Preliminary version available as "The Difficulty of Random Attribute Noise," </note> <institution> Washington University Dept. of Computer Science Tech. Report WUCS-91-92, </institution> <year> 1991. </year>
Reference-contexts: See Haussler et al. [13] for discussion about what difference this small perturbation in the definition of pac learning might make. 14 Three oracles have been studied so far that are relevant to geometric instance spaces (as have several others that apply only to the instance space f0; 1g <ref> [12, 24] </ref>). The "desired," noiseless output of each oracle is a correctly labeled example (x; s), where x is drawn according to P.
Reference: [13] <author> David Haussler, Michael Kearns, Nick Littlestone, and Manfred K. War-muth. </author> <title> Equivalence of models for polynomial learnability. </title> <journal> Information and Computation, </journal> <volume> 95(2) </volume> <pages> 129-161, </pages> <month> December </month> <year> 1991. </year>
Reference-contexts: This model is called the two-button model; the model we are using is called the one-button model. There are in fact numerous different minor variations on pac learnabil-ity. Haussler et al. compare many of these variations, and show them to be substantially equivalent <ref> [13] </ref>. 6 An example of pac learning We now exhibit an algorithm that pac learns monomials [26]. The instance space X n is f0; 1g n for some positive n. <p> It does, however, have halt in expected polynomial time. See Haussler et al. <ref> [13] </ref> for discussion about what difference this small perturbation in the definition of pac learning might make. 14 Three oracles have been studied so far that are relevant to geometric instance spaces (as have several others that apply only to the instance space f0; 1g [12, 24]).
Reference: [14] <author> David Haussler and Elmo Welzl. </author> <title> Epsilon-nets and simplex range queries. </title> <journal> Discrete Computational Geometry, </journal> <volume> 2 </volume> <pages> 127-151, </pages> <year> 1987. </year>
Reference-contexts: The class of all spheres in E n has dimension n + 1. For any finite concept class C, we have V Cdim (C) log (jCj). x There are now a number of papers which calculate the VC dimension of different classes <ref> [4, 9, 14, 16] </ref>. The VC dimension was first studied in connection with statistical pattern recognition. Pollard and Vapnik have both written good books discussing it from that point of view [22, 28].
Reference: [15] <author> David Helmbold, Robert Sloan, and Manfred K. Warmuth. </author> <title> Learning nested differences of intersection-closed concept classes. </title> <journal> Machine Learning, </journal> <volume> 5(2) </volume> <pages> 165-196, </pages> <year> 1990. </year>
Reference-contexts: Of course, it would be nicer to discover a good algorithm than to discover a hardness proof. Another important open class involves unions of boxes. Algorithms exist for learning arbitrary unions of axis-parallel rectangles in E 2 , and arbitrary nested differences of axis-parallel boxes in E n <ref> [8, 15] </ref>. Arbitrary unions of boxes (regardless of whether they are axis-parallel) in E n cannot be pac learnable since the VC dimension is not polynomial in n.
Reference: [16] <author> David Helmbold, Robert Sloan, and Manfred K. Warmuth. </author> <title> Learning integer lattices. </title> <journal> SIAM Journal on Computing, </journal> <volume> 21(2) </volume> <pages> 240-266, </pages> <year> 1992. </year>
Reference-contexts: The class of all spheres in E n has dimension n + 1. For any finite concept class C, we have V Cdim (C) log (jCj). x There are now a number of papers which calculate the VC dimension of different classes <ref> [4, 9, 14, 16] </ref>. The VC dimension was first studied in connection with statistical pattern recognition. Pollard and Vapnik have both written good books discussing it from that point of view [22, 28].
Reference: [17] <author> M. Kearns and M. Li. </author> <title> Learning in the presence of malicious errors. </title> <journal> SIAM J. Comput., </journal> <volume> 22 </volume> <pages> 807-837, </pages> <year> 1993. </year>
Reference-contexts: Kearns and Li <ref> [17] </ref> showed that, given certain very minimal assumptions, it is impossible to pac learn 15 with examples from MAL unless- &lt; (*=1 + *). The proof is information--theoretic, meaning that it is impossible even to statistically pac learn under such circumstances. <p> Any pac learning algorithm for a concept class C can be converted to be robust against a malicious noise with a noise rate up to O * log vcd (C) !! The key idea of this meta-algorithm <ref> [17] </ref> is as follows. The meta-algorithm obtains many, many examples from MAL -. These are broken up into blocks of length s, where s is the number of noise-free examples the regular pac learning algorithm requires for C.
Reference: [18] <author> Michael Kearns, Ming Li, Leonard Pitt, and Leslie Valiant. </author> <title> Recent results on boolean concept learning. </title> <booktitle> In Proceedings of the Fourth International Workshop on Machine Learning, </booktitle> <pages> pages 337-352, </pages> <institution> University of California, Irvine, </institution> <month> June </month> <year> 1987. </year>
Reference-contexts: In general, it is often easier to prove theorems about statistical pac learning than about pac learning. The excellent survey article of Kearns et al. gives a more lengthy discussion of the entire pac learning model, and also provides an overview of early 6 results obtained using this model <ref> [18] </ref>. Blumer et al. and Valiant both contain additional material on the motivation for the pac learning model [8, 26]. 5 Variations on the definition of pac learn ing The pac learning model can be altered in many ways.
Reference: [19] <author> Nathan Linial, Yishay Mansour, and Ronald L. Rivest. </author> <title> Results on learn-ability and the Vapnik-Chervonenkis dimension. </title> <journal> Information and Computation, </journal> <volume> 90(1) </volume> <pages> 33-49, </pages> <month> January </month> <year> 1991. </year>
Reference-contexts: We could then make C n the set of convex n-gons, or alternatively, the set of convex polygons with at most n edges. This approach to pac learning is explored by Linial, Mansour and Rivest, and by Blumer et al. <ref> [8, 19] </ref>. 3 Pac learning In this section we define Valiant's model of concept learning [26]. Angluin dubbed this model "probably approximately correct learning," [1] and it is thus referred to as pac learning. <p> However, this is not strictly true. The limitations imposed by Theorem 3 can be overcome by algorithms with slightly different behavior. For instance, the class of all finite unions of closed intervals in [0; 1] has infinite VC dimension, but does have a pac learning algorithm <ref> [19] </ref>. 5 The algorithm behaves roughly as follows. It first draws a small sample from EXAMPLES, and makes as its first hypothesis some "small" concept (one that is a union of as few intervals as possible) consistent with that sample.
Reference: [20] <author> Judea Pearl. </author> <title> On the connection between the complexity and credibility of inferred models. </title> <journal> Journal of General Systems, </journal> <volume> 4 </volume> <pages> 255-264, </pages> <year> 1978. </year>
Reference-contexts: Pollard and Vapnik have both written good books discussing it from that point of view [22, 28]. The first source that I am aware of to point out that it has some connection to efficient concept learning is Pearl <ref> [20] </ref>. 9.2 Algorithms For our purposes, the key fact about the VC dimension is that a fixed concept class has polynomial sample complexity if and only if it has finite VC dimension, or a sequence of concept classes C n has polynomial sample complexity if and only if vcd (C n
Reference: [21] <author> Leonard Pitt and Leslie G. Valiant. </author> <title> Computational limitations on learning from examples. </title> <journal> Journal of the ACM, </journal> <volume> 35(4) </volume> <pages> 965-984, </pages> <year> 1988. </year>
Reference-contexts: A union of n boxes in E n would generalize learning n-term DNF in f0; 1g n . This problem is known to be NP-complete if we demand that the output be a union of a minimum number of boxes <ref> [21] </ref> 17
Reference: [22] <author> David Pollard. </author> <title> Convergence of Stochastic Processes. </title> <publisher> Springer-Verlag, </publisher> <year> 1984. </year>
Reference-contexts: How many boxes of cereal must one buy to insure getting at least one of each coupon?" 11 Remark: The term shatter is now well established. However, as Pollard points out <ref> [22] </ref>, the right picture to keep in mind is not really S being broken into lots of tiny pieces by C. Rather, one should imagine a diligent C picking out all the different subsets of S. Definition. <p> The VC dimension was first studied in connection with statistical pattern recognition. Pollard and Vapnik have both written good books discussing it from that point of view <ref> [22, 28] </ref>.
Reference: [23] <author> Steven Salzberg. </author> <title> A nearest hyperrectangle learning method. </title> <journal> Machine Learning, </journal> <volume> 6(3) </volume> <pages> 251-276, </pages> <year> 1991. </year>
Reference-contexts: It is open whether unions of up to O (n) axis-parallel boxes (or even of O (log n)) in E n can be pac learned. 7 This concept class is of particular interest because it is also sometimes used in artificial intelligence approaches to data classification (e.g., <ref> [23] </ref>). I believe that in the end, the number of good polynomial-time pac learning algorithms found over the intermediate-term future will determine whether this whole line of research continues to flourish, or becomes just one more interesting little side area studied for some period of time by theoretical computer science.
Reference: [24] <author> Robert H. Sloan. </author> <title> Four types of noise in data for PAC learning. </title> <journal> Information Processing Letters. </journal> <note> To appear. </note>
Reference-contexts: See Haussler et al. [13] for discussion about what difference this small perturbation in the definition of pac learning might make. 14 Three oracles have been studied so far that are relevant to geometric instance spaces (as have several others that apply only to the instance space f0; 1g <ref> [12, 24] </ref>). The "desired," noiseless output of each oracle is a correctly labeled example (x; s), where x is drawn according to P. <p> The "desired," noiseless output of each oracle is a correctly labeled example (x; s), where x is drawn according to P. We now now give a precise description of the actual outputs from the following noise oracles: MAL - [27], RMC - [2], MMC - <ref> [24, 25] </ref>. * When MAL is called, with probability 1-, it does indeed return a correctly labeled (x; s) where x is drawn according to P. With probability it returns an example (x; s) about which no assumptions whatsoever may be made. <p> This solves the problem of statistical pac learning. These results extend to the case where the label is maliciously chosen by a powerful adversary (i.e., where RMC is replaced by MMC -) <ref> [24, 25] </ref>. Unfortunately, in this case no meta-algorithm for converting any pac learning algorithm into an algorithm robust against large amounts of mis-labeling noise is known.
Reference: [25] <author> Robert H. Sloan. </author> <title> Types of noise in data for concept learning. </title> <booktitle> In First Workshop on Computational Learning Theory, </booktitle> <pages> pages 91-96. </pages> <publisher> Morgan Kaufmann, </publisher> <year> 1988. </year>
Reference-contexts: The "desired," noiseless output of each oracle is a correctly labeled example (x; s), where x is drawn according to P. We now now give a precise description of the actual outputs from the following noise oracles: MAL - [27], RMC - [2], MMC - <ref> [24, 25] </ref>. * When MAL is called, with probability 1-, it does indeed return a correctly labeled (x; s) where x is drawn according to P. With probability it returns an example (x; s) about which no assumptions whatsoever may be made. <p> This solves the problem of statistical pac learning. These results extend to the case where the label is maliciously chosen by a powerful adversary (i.e., where RMC is replaced by MMC -) <ref> [24, 25] </ref>. Unfortunately, in this case no meta-algorithm for converting any pac learning algorithm into an algorithm robust against large amounts of mis-labeling noise is known.
Reference: [26] <author> Leslie G. Valiant. </author> <title> A theory of the learnable. </title> <journal> Communications of the ACM, </journal> <volume> 27(11) </volume> <pages> 1134-1142, </pages> <month> November </month> <year> 1984. </year>
Reference-contexts: More recently, the theoretical computer science community has taken an interest in this field. The work by this community began with Gold [11] back in the 1960s. (See Angluin and Smith [3] for an excellent survey of work through the early 1980s.) A seminal paper by Valiant <ref> [26] </ref> in 1984 prompted intensified effort. The goal of the work by the theory community, not surprisingly, is to provide precise models of the problem, and good algorithms to solve the problems so modeled (or proofs of the hardness of those problems). <p> This approach to pac learning is explored by Linial, Mansour and Rivest, and by Blumer et al. [8, 19]. 3 Pac learning In this section we define Valiant's model of concept learning <ref> [26] </ref>. Angluin dubbed this model "probably approximately correct learning," [1] and it is thus referred to as pac learning. In short, an algorithm pac learns from examples if it can, in a feasible amount of time, find (with high probability), a rule that is highly accurate. <p> Blumer et al. and Valiant both contain additional material on the motivation for the pac learning model <ref> [8, 26] </ref>. 5 Variations on the definition of pac learn ing The pac learning model can be altered in many ways. <p> There are in fact numerous different minor variations on pac learnabil-ity. Haussler et al. compare many of these variations, and show them to be substantially equivalent [13]. 6 An example of pac learning We now exhibit an algorithm that pac learns monomials <ref> [26] </ref>. The instance space X n is f0; 1g n for some positive n. The concept class is the set of all boolean expressions in n variables that can be represented as monomials (simple conjunctions of literals).
Reference: [27] <author> Leslie G. Valiant. </author> <title> Learning disjunctions of conjunctions. </title> <booktitle> In Proceedings IJCAI-85, </booktitle> <pages> pages 560-566. </pages> <booktitle> International Joint Committee for Artificial Intelligence, </booktitle> <publisher> Morgan Kaufmann, </publisher> <month> August </month> <year> 1985. </year>
Reference-contexts: The "desired," noiseless output of each oracle is a correctly labeled example (x; s), where x is drawn according to P. We now now give a precise description of the actual outputs from the following noise oracles: MAL - <ref> [27] </ref>, RMC - [2], MMC - [24, 25]. * When MAL is called, with probability 1-, it does indeed return a correctly labeled (x; s) where x is drawn according to P. With probability it returns an example (x; s) about which no assumptions whatsoever may be made.
Reference: [28] <author> V. N. Vapnik. </author> <title> Estimation of Dependences Based on Empirical Data. </title> <publisher> Springer-Verlag, </publisher> <address> New York, </address> <year> 1982. </year>
Reference-contexts: The VC dimension was first studied in connection with statistical pattern recognition. Pollard and Vapnik have both written good books discussing it from that point of view <ref> [22, 28] </ref>.
Reference: [29] <author> V. N. Vapnik and A. Ya. Chervonenkis. </author> <title> On the uniform convergence of relative frequencies of events to their probabilities. Theory of Probability and its Applications, </title> <address> XVI(2):264-280, </address> <year> 1971. </year> <month> 20 </month>
Reference-contexts: It turns out, however, that a combinatorial parameter of set systems can take the place of the cardinality of the concept class. In fact, this parameter, the Vapnik-Chervonenkis dimension <ref> [29] </ref>, hereinafter VC dimension (or in symbols vcd ()), provides both a necessary and sufficient condition for the existence of an algorithm that can statistically pac learn just by drawing a fixed sample size.
References-found: 29

