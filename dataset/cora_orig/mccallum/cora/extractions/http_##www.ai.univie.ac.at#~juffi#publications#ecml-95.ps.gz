URL: http://www.ai.univie.ac.at/~juffi/publications/ecml-95.ps.gz
Refering-URL: http://www.ai.univie.ac.at/~juffi/publications/publications.html
Root-URL: 
Email: E-mail: juffi@ai.univie.ac.at  
Title: A Tight Integration of Pruning and Learning (Extended Abstract)  
Author: Johannes Furnkranz 
Address: Schottengasse 3, A-1010 Vienna  
Affiliation: Austrian Research Institute for Artificial Intelligence  
Abstract: This paper outlines some problems that may occur with Reduced Error Pruning in rule learning algorithms. In particular we show that pruning complete theories is incompatible with the separate-and-conquer learning strategy that is commonly used in propositional and relational rule learning systems. As a solution we propose to integrate pruning into learning and examine two algorithms, one that prunes at the clause level and one that prunes at the literal level. Experiments show that these methods are not only much more efficient, but also able to achieve small gains in accuracy by solving the outlined problem.
Abstract-found: 1
Intro-found: 1
Reference: [Brunk and Pazzani, 1991] <author> Clifford A. Brunk and Michael J. Pazzani. </author> <title> An investigation of noise-tolerant relational concept learning algorithms. </title> <booktitle> In Proceedings of the 8th International Workshop on Machine Learning, </booktitle> <pages> pages 389-393, </pages> <address> Evanston, Illinois, </address> <year> 1991. </year>
Reference-contexts: dealing with noise | post-pruning | is to first learn a theory that overfits the data and then prune this theory to an appropriate level of generality. 2 REP The most common post-pruning algorithm, Reduced Error Pruning (REP), has been adopted from propositional decision tree learning to relational rule learning <ref> [Brunk and Pazzani, 1991] </ref>. <p> Most of the efficiency of the I-REP algorithm comes from the integration of pre-pruning and post-pruning by this definition of a stopping criterion based on the accuracy of the pruned clause on the pruning set. Thus I-REP does not need REP's delete-clause operator <ref> [Brunk and Pazzani, 1991] </ref>, because the clauses of the final theory are constructed directly and learning stops when no more useful clauses can be found.
Reference: [Cohen, 1993] <author> William W. Cohen. </author> <title> Efficient pruning methods for separate-and-conquer rule learning systems. </title> <booktitle> In Proceedings of the 13th International Joint Conference on Artificial Intelligence, </booktitle> <pages> pages 988-994, </pages> <address> Chambery, France, </address> <year> 1993. </year>
Reference-contexts: A lot of work is wasted in learning and subsequently pruning superfluous literals and clauses. This argument has been formalized in <ref> [Cohen, 1993] </ref>, where it was shown that the growing phase of REP has a time complexity of (n 2 log n) and that its pruning phase has a time complexity of (n 4 ) (where n is the size of the training set). [Furnkranz and Widmer, 1994] point out another problem
Reference: [Furnkranz and Widmer, 1994] <author> Johannes Furnkranz and Gerhard Widmer. </author> <title> Incremental Reduced Error Pruning. </title> <booktitle> In Proceedings of the 11th International Conference on Machine Learning, </booktitle> <pages> pages 70-77, </pages> <address> New Brunswick, NJ, </address> <year> 1994. </year>
Reference-contexts: This argument has been formalized in [Cohen, 1993], where it was shown that the growing phase of REP has a time complexity of (n 2 log n) and that its pruning phase has a time complexity of (n 4 ) (where n is the size of the training set). <ref> [Furnkranz and Widmer, 1994] </ref> point out another problem with REP that is caused by the differences between the divide-and-conquer approach used for decision tree learning and the separate-and-conquer strategy commonly used for rule learning. <p> However, as the test is changed at pruning time (after learning), REP has to keep the subtree that has been previously learned from a different set of examples, although there might be a better subtree to explain this new set of examples. 3 I-REP Incremental Reduced Error Pruning (I-REP) <ref> [Furnkranz and Widmer, 1994] </ref> was motivated by the observation that REP is incompatible with the separate-and-conquer learning strategy as we have discussed in Sect. 2.
Reference: [Furnkranz, 1995] <author> Johannes Furnkranz. </author> <title> A tight integration of pruning and learning. </title> <type> Technical Report OEFAI-TR-95-03, </type> <institution> Austrian Research Institute for Artificial Intelligence, </institution> <year> 1995. </year>
Reference-contexts: The results can be found in <ref> [Furnkranz, 1995] </ref> which is available via anonymous ftp from ftp.ai.univie.ac.at. I-REP and I 2 -REP are both significantly faster than REP. In the KRK domain they also learn significantly better theories, in particular at high training set sizes.
Reference: [Quinlan, 1990] <author> John Ross Quinlan. </author> <title> Learning logical definitions from relations. </title> <journal> Machine Learning, </journal> <volume> 5 </volume> <pages> 239-266, </pages> <year> 1990. </year>
Reference-contexts: 1 Introduction Most rule learning algorithms deal with noise in the data during learning, i.e. they employ pre-pruning . In relational learning systems like Foil <ref> [Quinlan, 1990] </ref> pre-pruning is commonly used in the form of so-called stopping criteria.
Reference: [Weiss and Indurkhya, 1994] <author> Sholom M. Weiss and Nitin Indurkhya. </author> <title> Small sample decision tree pruning. </title> <booktitle> In Proceedings of the 11th Conference on Machine Learning, </booktitle> <pages> pages 335-342, </pages> <institution> Rutgers University, </institution> <address> New Brunswick, NJ, </address> <year> 1994. </year>
Reference-contexts: Besides, I 2 -REP's procedure for selecting a literal is very similar to 2-fold cross-validation which has recently been shown to be a reliable procedure for comparing classifiers, in particular at low training set sizes <ref> [Weiss and Indurkhya, 1994] </ref>.
References-found: 6

