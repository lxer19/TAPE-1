URL: ftp://info.mcs.anl.gov/pub/tech_reports/reports/P404.ps.Z
Refering-URL: http://www.mcs.anl.gov/publications/abstracts/abstracts93.htm
Root-URL: http://www.mcs.anl.gov
Title: A LIMITED-MEMORY ALGORITHM FOR BOUND-CONSTRAINED OPTIMIZATION  
Author: by Richard H. Byrd, Peihuang Lu, and Jorge Nocedal yz 
Keyword: Key words: bound-constrained optimization, limited-memory method, nonlinear optimization, quasi-Newton method, large-scale optimization.  
Affiliation: Mathematics and Computer Science Division, Argonne National Laboratory, Argonne, Illinois 60439  
Note: Preprint MCS-P404-1293,  
Abstract: An algorithm for solving large nonlinear optimization problems with simple bounds is described. It is based on the gradient projection method and uses a limited-memory BFGS matrix to approximate the Hessian of the objective function. We show how to take advantage of the form of the limited-memory approximation to implement the algorithm efficiently. The results of numerical tests on a set of large problems are reported. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> A. V. Aho, J. E. Hopcroft, and J. D. Ullman, </author> <title> The Design and Analysis of Computer Algorithms. </title> <address> Reading, Mass: </address> <publisher> Addison-Wesley Pub. Co., </publisher> <year> 1974. </year>
Reference-contexts: Note that there are no O (n) computations inside the loop. If n int denotes the total number of segments explored, then the total cost of Algorithm CP is (2m+2)n+O (m 2 )fin int operations plus n log n operations which is the approximate cost of the heapsort algorithm <ref> [1] </ref>. 5 Methods for Subspace Minimization Once the Cauchy point x c has been found, we proceed to approximately minimize the quadratic model m k over the space of free variables and impose the bounds on the problem. <p> at the solution (n a = 0), some bounds may become active during the iteration. 17 Table 1: Test Problems Problem Variant n n a Reference Additional Bounds EDENSCH 1 2000 0 CUTE [4] none EDENSCH 2 2000 1 " [0; 1:5] 8 odd i EDENSCH 3 2000 667 " <ref> [1; 0:5] </ref> i = 4; 7; 10; ::: EDENSCH 4 2000 999 " [0; 0:99] 8 odd i EDENSCH 5 2000 100 " [0; 0:5] 8 odd i LMINSURF 1 1024 124 " none LMINSURF 2 1024 147 " [2; 10] 8 odd i LMINSURF 3 1024 172 " [5; 10] <p> 8 odd i LMINSURF 1 1024 124 " none LMINSURF 2 1024 147 " [2; 10] 8 odd i LMINSURF 3 1024 172 " [5; 10] 8 odd i LMINSURF 4 1024 227 " [5:5; 6] 8i PENALTY 1 1 1000 0 " none PENALTY 1 2 1000 0 " <ref> [0; 1] </ref> 8 odd i PENALTY 1 3 1000 334 " [0:1; 1] i = 4; 7; 10; ::: PENALTY 1 4 1000 500 " [0:1; 1] 8 odd i RAYBENDL 1 44 4 " none RAYBENDL 2 44 6 " [2; 95] 8 i ORTHREG 1 133 0 " none <p> 1024 147 " [2; 10] 8 odd i LMINSURF 3 1024 172 " [5; 10] 8 odd i LMINSURF 4 1024 227 " [5:5; 6] 8i PENALTY 1 1 1000 0 " none PENALTY 1 2 1000 0 " [0; 1] 8 odd i PENALTY 1 3 1000 334 " <ref> [0:1; 1] </ref> i = 4; 7; 10; ::: PENALTY 1 4 1000 500 " [0:1; 1] 8 odd i RAYBENDL 1 44 4 " none RAYBENDL 2 44 6 " [2; 95] 8 i ORTHREG 1 133 0 " none TORSION 1 1024 320 MINPACK-2 [2] none JOURNAL 1 1024 330 <p> 10] 8 odd i LMINSURF 4 1024 227 " [5:5; 6] 8i PENALTY 1 1 1000 0 " none PENALTY 1 2 1000 0 " [0; 1] 8 odd i PENALTY 1 3 1000 334 " <ref> [0:1; 1] </ref> i = 4; 7; 10; ::: PENALTY 1 4 1000 500 " [0:1; 1] 8 odd i RAYBENDL 1 44 4 " none RAYBENDL 2 44 6 " [2; 95] 8 i ORTHREG 1 133 0 " none TORSION 1 1024 320 MINPACK-2 [2] none JOURNAL 1 1024 330 MINPACK-2 [2] none The results of our numerical tests are given in Table 2.
Reference: [2] <author> B. M. Averick and J. J. </author> <title> More, "User guide for the MINPACK-2 test problem collection," </title> <institution> Ar-gonne National Laboratory, Mathematics and Computer Science Division Report ANL/MCS-TM-157, Argonne, Ill., </institution> <year> 1991. </year>
Reference-contexts: For more details on how to update the limited-memory matrices in Step 7, see [6]. When testing the routine SUBMIN of LANCELOT [10] we used the default options and BFGS updating. We selected seven problems, two bound-constrained quadratic optimization problems from the MINPACK-2 collection <ref> [2] </ref>, and five nonlinear problems from the CUTE collection [4], to test the algorithms. To study a variety of cases, we tightened the bounds on several problems, resulting in more active bounds at the solutions of these problems. <p> " [0; 1:5] 8 odd i EDENSCH 3 2000 667 " [1; 0:5] i = 4; 7; 10; ::: EDENSCH 4 2000 999 " [0; 0:99] 8 odd i EDENSCH 5 2000 100 " [0; 0:5] 8 odd i LMINSURF 1 1024 124 " none LMINSURF 2 1024 147 " <ref> [2; 10] </ref> 8 odd i LMINSURF 3 1024 172 " [5; 10] 8 odd i LMINSURF 4 1024 227 " [5:5; 6] 8i PENALTY 1 1 1000 0 " none PENALTY 1 2 1000 0 " [0; 1] 8 odd i PENALTY 1 3 1000 334 " [0:1; 1] i = <p> 0 " none PENALTY 1 2 1000 0 " [0; 1] 8 odd i PENALTY 1 3 1000 334 " [0:1; 1] i = 4; 7; 10; ::: PENALTY 1 4 1000 500 " [0:1; 1] 8 odd i RAYBENDL 1 44 4 " none RAYBENDL 2 44 6 " <ref> [2; 95] </ref> 8 i ORTHREG 1 133 0 " none TORSION 1 1024 320 MINPACK-2 [2] none JOURNAL 1 1024 330 MINPACK-2 [2] none The results of our numerical tests are given in Table 2. <p> 1 3 1000 334 " [0:1; 1] i = 4; 7; 10; ::: PENALTY 1 4 1000 500 " [0:1; 1] 8 odd i RAYBENDL 1 44 4 " none RAYBENDL 2 44 6 " [2; 95] 8 i ORTHREG 1 133 0 " none TORSION 1 1024 320 MINPACK-2 <ref> [2] </ref> none JOURNAL 1 1024 330 MINPACK-2 [2] none The results of our numerical tests are given in Table 2. All computations were performed on a Sun SPARCstation 2 with a 40-MHz CPU and 32-MB memory. <p> i = 4; 7; 10; ::: PENALTY 1 4 1000 500 " [0:1; 1] 8 odd i RAYBENDL 1 44 4 " none RAYBENDL 2 44 6 " [2; 95] 8 i ORTHREG 1 133 0 " none TORSION 1 1024 320 MINPACK-2 <ref> [2] </ref> none JOURNAL 1 1024 330 MINPACK-2 [2] none The results of our numerical tests are given in Table 2. All computations were performed on a Sun SPARCstation 2 with a 40-MHz CPU and 32-MB memory.
Reference: [3] <author> D. P. Bertsekas, </author> <title> "Projected Newton methods for optimization problems with simple constraints", </title> <journal> SIAM J. Control and Optimization 20 (1982): </journal> <pages> 221-246. </pages>
Reference-contexts: We find that by making use of the compact representations of limited-memory matrices described by Byrd, Nocedal, and Schnabel [6], the computational cost of one iteration of the algorithm can be kept to be of order n. We used the gradient projection approach [16], [18], <ref> [3] </ref> to determine the active set, because recent studies [7], [5] indicate that it possesses good theoretical properties, and because it also appears to be efficient on many large problems [8], [20].
Reference: [4] <author> I. Bongartz, A. R. Conn, N. I. M. Gould, and Ph. L. Toint, "CUTE: </author> <title> Constrained and unconstrained testing environment," </title> <type> Research Report, </type> <institution> IBM T.J. Watson Research Center, </institution> <address> Yorktown, New York, </address> <year> 1993. </year>
Reference-contexts: When testing the routine SUBMIN of LANCELOT [10] we used the default options and BFGS updating. We selected seven problems, two bound-constrained quadratic optimization problems from the MINPACK-2 collection [2], and five nonlinear problems from the CUTE collection <ref> [4] </ref>, to test the algorithms. To study a variety of cases, we tightened the bounds on several problems, resulting in more active bounds at the solutions of these problems. Table 1 lists the test problems and the bounds added to those already given in the specification of the problem. <p> We note that in those problems without active bounds at the solution (n a = 0), some bounds may become active during the iteration. 17 Table 1: Test Problems Problem Variant n n a Reference Additional Bounds EDENSCH 1 2000 0 CUTE <ref> [4] </ref> none EDENSCH 2 2000 1 " [0; 1:5] 8 odd i EDENSCH 3 2000 667 " [1; 0:5] i = 4; 7; 10; ::: EDENSCH 4 2000 999 " [0; 0:99] 8 odd i EDENSCH 5 2000 100 " [0; 0:5] 8 odd i LMINSURF 1 1024 124 " none
Reference: [5] <author> J. V. Burke, and J. J. </author> <title> More, "On the identification of active constraints," </title> <journal> SIAM J. Numer. Anal. </journal> <volume> 25, no. 5 (1988): </volume> <pages> 1197-1211. </pages>
Reference-contexts: We used the gradient projection approach [16], [18], [3] to determine the active set, because recent studies [7], <ref> [5] </ref> indicate that it possesses good theoretical properties, and because it also appears to be efficient on many large problems [8], [20]. <p> " [1; 0:5] i = 4; 7; 10; ::: EDENSCH 4 2000 999 " [0; 0:99] 8 odd i EDENSCH 5 2000 100 " [0; 0:5] 8 odd i LMINSURF 1 1024 124 " none LMINSURF 2 1024 147 " [2; 10] 8 odd i LMINSURF 3 1024 172 " <ref> [5; 10] </ref> 8 odd i LMINSURF 4 1024 227 " [5:5; 6] 8i PENALTY 1 1 1000 0 " none PENALTY 1 2 1000 0 " [0; 1] 8 odd i PENALTY 1 3 1000 334 " [0:1; 1] i = 4; 7; 10; ::: PENALTY 1 4 1000 500 "
Reference: [6] <author> R. H. Byrd, J. Nocedal, and R. B. Schnabel, </author> <title> "Representation of quasi-Newton matrices and their use in limited memory methods," </title> <type> Technical report, </type> <institution> EECS Department, Northwestern University, </institution> <year> 1991, </year> <note> to appear in Mathematical Programming. </note>
Reference-contexts: The properties of these limited-memory matrices have far-reaching consequences in the implementation of the method, as will be discussed later on. We find that by making use of the compact representations of limited-memory matrices described by Byrd, Nocedal, and Schnabel <ref> [6] </ref>, the computational cost of one iteration of the algorithm can be kept to be of order n. <p> The Hessian approximations B k used in our algorithm are limited-memory BFGS matrices (Nocedal [21] and Byrd, Nocedal, and Schnabel <ref> [6] </ref>). Even though these matrices do not take advantage of the structure of the problem, they require only a small amount of storage and, as we will show, allow the computation of the generalized Cauchy point and the subspace minimization to be performed in O (n) operations. <p> next three sections we describe in detail the limited-memory matrices, the computation of the Cauchy point, and the minimization of the quadratic problem on a subspace. 3 Limited-Memory BFGS Matrices In our algorithm, the limited-memory BFGS matrices are represented in the compact form de scribed by Byrd, Nocedal, and Schnabel <ref> [6] </ref>. <p> The question is how to best represent these matrices without explicitly forming them. In <ref> [6] </ref> it is proposed to use a compact (or outer product) form to define the limited-memory matrix B k in terms of the n fi m correction matrices Y k = [y km ; : : :; y k1 ] ; S k = [s km ; : : : ; <p> to define the limited-memory matrix B k in terms of the n fi m correction matrices Y k = [y km ; : : :; y k1 ] ; S k = [s km ; : : : ; s k1 ] : (3:1) More specifically, it is shown in <ref> [6] </ref> that if is a positive scaling parameter and if the m correction pairs fs i ; y i g km i=k1 , satisfy s T i y i &gt; 0, then the matrix obtained by updating I m-times, using the BFGS formula and the pairs fs i ; y i <p> m matrices (L k ) i;j = (s km1+i ) T (y km1+j ) if i &gt; j 0 otherwise; (3:5) D k = diag h km y km ; : : :; s T i (We should point out that (3.2) is a slight rearrangement of Equation (3.5) in <ref> [6] </ref>.) Note that since M k is a 2m fi 2m matrix, and since m is chosen to be a small integer, the cost of computing the inverse in (3.4) is negligible. It is shown in [6] that by using the compact representation (3.2) various computations involving B k become inexpensive. <p> (We should point out that (3.2) is a slight rearrangement of Equation (3.5) in <ref> [6] </ref>.) Note that since M k is a 2m fi 2m matrix, and since m is chosen to be a small integer, the cost of computing the inverse in (3.4) is negligible. It is shown in [6] that by using the compact representation (3.2) various computations involving B k become inexpensive. In particular, the product of B k times a vector, which occurs often in the algorithm of this paper, can be performed efficiently. <p> k j k ; (3:7) where W k j 1 i M k j 6 0 R 1 R T k (D k + 1 k Y k )R 1 3 5 ; and ( 0 otherwise. (3:8) (We note that (3.7) is a slight rearrangement of equation (3.1) in <ref> [6] </ref>.) Since the algorithm performs a backtracking line search, we cannot guarantee that the condition s T k y k &gt; 0 always holds (cf. Dennis and Schnabel [12]). <p> ff 2 q (t operations) * ae 1 := ae 2 ; ae 2 = ^r T ^r; fi := ae 2 =ae 1 (t operations) * p := ^r + fip (t operations) * go to 3 The matrix-vector multiplication of Step 5 should be performed as described in <ref> [6] </ref>. The total operation count of this conjugate gradient procedure is approximately (2m + 2)t + (4m + 6)t fi citer + O (m 2 ); (5:14) where citer is the number of conjugate gradient iterations. <p> x c ) u i ; i 2 F g (t operations) * Set x = x c + ff fl (x k + d fl x c ). (t operations) Since the vectors S T g k and Y T g k have been computed while updating H k <ref> [6] </ref>, they can be saved so that the product W T g k requires no further computation. The total number of operations of this procedure, when no bounds are active (t a = 0), is (2m + 1)n + O (m 2 ). <p> For the heapsort, during the generalized Cauchy point computation, we use the Harwell routine KB12AD written by Gould [13]. The backtracking line search was performed by the routine LNSRCH of Dennis and Schnabel [12]. For more details on how to update the limited-memory matrices in Step 7, see <ref> [6] </ref>. When testing the routine SUBMIN of LANCELOT [10] we used the default options and BFGS updating. We selected seven problems, two bound-constrained quadratic optimization problems from the MINPACK-2 collection [2], and five nonlinear problems from the CUTE collection [4], to test the algorithms. <p> 4 2000 999 " [0; 0:99] 8 odd i EDENSCH 5 2000 100 " [0; 0:5] 8 odd i LMINSURF 1 1024 124 " none LMINSURF 2 1024 147 " [2; 10] 8 odd i LMINSURF 3 1024 172 " [5; 10] 8 odd i LMINSURF 4 1024 227 " <ref> [5:5; 6] </ref> 8i PENALTY 1 1 1000 0 " none PENALTY 1 2 1000 0 " [0; 1] 8 odd i PENALTY 1 3 1000 334 " [0:1; 1] i = 4; 7; 10; ::: PENALTY 1 4 1000 500 " [0:1; 1] 8 odd i RAYBENDL 1 44 4 "
Reference: [7] <author> P. H. Calamai, and J. J. </author> <title> More, "Projected gradient methods for linearly constrained problems" Mathematical Programming 39 (1987): </title> <type> 93-116 </type>
Reference-contexts: We used the gradient projection approach [16], [18], [3] to determine the active set, because recent studies <ref> [7] </ref>, [5] indicate that it possesses good theoretical properties, and because it also appears to be efficient on many large problems [8], [20].
Reference: [8] <author> A. R. Conn, N. I. M. Gould, and Ph. L. Toint, </author> <title> "Testing a class of methods for solving minimization problems with simple bounds on the variables," </title> <journal> Mathematics of Computation 50, </journal> <volume> no. 182 (1988): </volume> <pages> 399-430. </pages>
Reference-contexts: We used the gradient projection approach [16], [18], [3] to determine the active set, because recent studies [7], [5] indicate that it possesses good theoretical properties, and because it also appears to be efficient on many large problems <ref> [8] </ref>, [20]. <p> Note that the accuracy of the solution controls the rate of convergence of the algorithm, once the correct active set is identified, and should therefore be chosen with care. We follow Conn, Gould, and Toint <ref> [8] </ref> and stop the conjugate gradient iteration when the residual ^r of (5.13) satisfies k^rk &lt; min (0:1; k^r c k)k^r c k: We also stop the iteration at a bound when a conjugate gradient step is about to violate a bound, thus guaranteeing that (5.6) is satisfied.
Reference: [9] <author> A. R. Conn, N. I. M. Gould, and Ph. L. Toint, </author> <title> "Global convergence of a class of trust region algorithms for optimization with simple bounds," </title> <journal> SIAM J. Numer. Anal. </journal> <volume> 25 (1988): </volume> <pages> 433-460. </pages>
Reference-contexts: Department of Energy, under Contract W-31-109-Eng-38, while at the Mathematics and Computer Science Division, Argonne National Laboratory, Argonne, IL 60439. 1 The algorithm described in this paper is similar to the algorithms proposed by Conn, Gould, and Toint <ref> [9] </ref> and More and Toraldo [20], in that the gradient projection method is used to determine a set of active constraints at each iteration. <p> This allows us to form a quadratic model of f at x k , m k (x) = f (x k ) + g T 1 (x x k ) T B k (x x k ): (2:1) Just as in the method studied by Conn, Gould, and Toint <ref> [9] </ref>, the algorithm approximately minimizes m k (x) subject to the bounds given by (1.2). This is done by first using the gradient projection method to find a set of active bounds, followed by a minimization of m k treating those bounds as equality constraints.
Reference: [10] <author> A. R. Conn, N. I. M. Gould, Ph.L. Toint, "LANCELOT: </author> <title> A FORTRAN package for large-scale nonlinear optimization (Release A)," </title> <booktitle> Number 17 in Springer Series in Computational Mathematics, </booktitle> <publisher> Springer-Verlag, </publisher> <address> New York, </address> <year> 1992. </year>
Reference-contexts: W T A k A T k 1 M in (5.22). 6 Numerical Experiments We have tested our limited-memory algorithm using the three options for subspace minimiza tion (the direct primal, primal conjugate gradient, and dual methods) and compared the results with those obtained with the subroutine SUBMIN of LANCELOT <ref> [10] </ref> using partitioned BFGS updating. <p> The backtracking line search was performed by the routine LNSRCH of Dennis and Schnabel [12]. For more details on how to update the limited-memory matrices in Step 7, see [6]. When testing the routine SUBMIN of LANCELOT <ref> [10] </ref> we used the default options and BFGS updating. We selected seven problems, two bound-constrained quadratic optimization problems from the MINPACK-2 collection [2], and five nonlinear problems from the CUTE collection [4], to test the algorithms. <p> " [0; 1:5] 8 odd i EDENSCH 3 2000 667 " [1; 0:5] i = 4; 7; 10; ::: EDENSCH 4 2000 999 " [0; 0:99] 8 odd i EDENSCH 5 2000 100 " [0; 0:5] 8 odd i LMINSURF 1 1024 124 " none LMINSURF 2 1024 147 " <ref> [2; 10] </ref> 8 odd i LMINSURF 3 1024 172 " [5; 10] 8 odd i LMINSURF 4 1024 227 " [5:5; 6] 8i PENALTY 1 1 1000 0 " none PENALTY 1 2 1000 0 " [0; 1] 8 odd i PENALTY 1 3 1000 334 " [0:1; 1] i = <p> " [1; 0:5] i = 4; 7; 10; ::: EDENSCH 4 2000 999 " [0; 0:99] 8 odd i EDENSCH 5 2000 100 " [0; 0:5] 8 odd i LMINSURF 1 1024 124 " none LMINSURF 2 1024 147 " [2; 10] 8 odd i LMINSURF 3 1024 172 " <ref> [5; 10] </ref> 8 odd i LMINSURF 4 1024 227 " [5:5; 6] 8i PENALTY 1 1 1000 0 " none PENALTY 1 2 1000 0 " [0; 1] 8 odd i PENALTY 1 3 1000 334 " [0:1; 1] i = 4; 7; 10; ::: PENALTY 1 4 1000 500 "
Reference: [11] <author> A. R. Conn and J. </author> <title> More, </title> <type> Private communication, </type> <year> 1993. </year>
Reference-contexts: Then the iterates could soon fall outside of the feasible region. This example also illustrates the difficulties that the conjugate gradient approach can have on nearly degenerate problems <ref> [11] </ref>. 13 5.3 A Dual Method for Subspace Minimization Since it often happens that the number of active bounds is small relative to the size of the problem, it should be efficient to handle these bounds explicitly with Lagrange multipliers.
Reference: [12] <author> J. E. Dennis, Jr. and R. B. Schnabel, </author> <title> Numerical Methods for Unconstrained Optimization and Nonlinear Equations, </title> <address> Englewood Cliffs, N.J.: </address> <publisher> Prentice-Hall, </publisher> <year> 1983. </year> <month> 21 </month>
Reference-contexts: Dennis and Schnabel <ref> [12] </ref>). Therefore, to maintain the positive definiteness of the limited-memory BFGS matrix, we discard a correction pair fs k ; y k g if the curvature condition s T is not satisfied for a small positive constant eps. <p> Our code is written in double-precision Fortran 77. For the heapsort, during the generalized Cauchy point computation, we use the Harwell routine KB12AD written by Gould [13]. The backtracking line search was performed by the routine LNSRCH of Dennis and Schnabel <ref> [12] </ref>. For more details on how to update the limited-memory matrices in Step 7, see [6]. When testing the routine SUBMIN of LANCELOT [10] we used the default options and BFGS updating.
Reference: [13] <institution> Harwell Subroutine Library, Release 10, Advanced Computing Department, AEA Industrial Technology, Harwell Laboratory, </institution> <address> Oxfordshire, United Kingdom, </address> <year> 1990. </year>
Reference-contexts: Set k := k + 1 and go to 1. Our code is written in double-precision Fortran 77. For the heapsort, during the generalized Cauchy point computation, we use the Harwell routine KB12AD written by Gould <ref> [13] </ref>. The backtracking line search was performed by the routine LNSRCH of Dennis and Schnabel [12]. For more details on how to update the limited-memory matrices in Step 7, see [6]. When testing the routine SUBMIN of LANCELOT [10] we used the default options and BFGS updating.
Reference: [14] <author> J. C. Gilbert and C. </author> <title> Lemarechal, "Some numerical experiments with variable storage quasi-Newton algorithms," </title> <note> Mathematical Programming 45 (1989) 407-436. </note>
Reference-contexts: The new algorithm therefore has computational demands similar to those of the limited-memory algorithm (L-BFGS) for unconstrained problems described by Liu and Nocedal [19] and Gilbert and Lemarechal <ref> [14] </ref>.
Reference: [15] <author> P. E. Gill, W Murray and M. H. Wright, </author> <title> Practical Optimization, </title> <publisher> London: Academic Press, </publisher> <year> 1981. </year>
Reference-contexts: Such an approach is often referred to as a dual or a range space method (see <ref> [15] </ref>).
Reference: [16] <author> A. A. Goldstein, </author> <title> "Convex programming in Hilbert space," </title> <journal> Bull. Amer. Math. Soc. </journal> <volume> 70 (1964): </volume> <pages> 709-710. </pages>
Reference-contexts: We find that by making use of the compact representations of limited-memory matrices described by Byrd, Nocedal, and Schnabel [6], the computational cost of one iteration of the algorithm can be kept to be of order n. We used the gradient projection approach <ref> [16] </ref>, [18], [3] to determine the active set, because recent studies [7], [5] indicate that it possesses good theoretical properties, and because it also appears to be efficient on many large problems [8], [20].
Reference: [17] <author> J. M. Ortega and W. C. Rheinboldt, </author> <title> Iterative Solution of Nonlinear Equations in Several Variables, </title> <address> New York, </address> <publisher> Academic Press, </publisher> <year> 1970. </year>
Reference-contexts: Since B k is given by (3.2) and Z T k Z k = I, the reduced matrix ^ B is given by ^ B = I (Z T W )(M W T Z); where we have dropped the subscripts for simplicity. Applying the Sherman-Morrison-Woodbury formula (see, for example, <ref> [17] </ref>), we obtain ^ B 1 = 1 Z T W (I 1 1 ; (5:10) so that the unconstrained subspace Newton direction ^ d u is given by ^ d u = 1 1 M W T ZZ T W ) M W T Z ^r c : (5:11) Given
Reference: [18] <author> E. S. Levitin and B. T. Polyak, </author> <title> "Constrained minimization problems," </title> <institution> USSR Comput. Math. and Math. Phys. </institution> <month> 6 </month> <year> (1966): </year> <pages> 1-50. </pages>
Reference-contexts: We find that by making use of the compact representations of limited-memory matrices described by Byrd, Nocedal, and Schnabel [6], the computational cost of one iteration of the algorithm can be kept to be of order n. We used the gradient projection approach [16], <ref> [18] </ref>, [3] to determine the active set, because recent studies [7], [5] indicate that it possesses good theoretical properties, and because it also appears to be efficient on many large problems [8], [20].
Reference: [19] <author> D. C. Liu and J. Nocedal, </author> <title> "On the limited memory BFGS method for large scale optimization methods," </title> <booktitle> Mathematical Programming 45 (1989): </booktitle> <pages> 503-528. </pages>
Reference-contexts: The new algorithm therefore has computational demands similar to those of the limited-memory algorithm (L-BFGS) for unconstrained problems described by Liu and Nocedal <ref> [19] </ref> and Gilbert and Lemarechal [14]. <p> 4 25/301/27.7 PENALTY 1 1 62/776/99.1 PENALTY 1 2 54/524/92.4 PENALTY 1 3 33/0/40.9 PENALTY 1 4 32/0/30.3 RAYBENDL 1 108/109/2.1 RAYBENDL 2 80/95/1.5 ORTHREG 1 27/67/2.2 TORSION 1 11/80/12 JOURNAL 1 7/177/14.8 Taking everything together, the new algorithm has most of the efficiency of the unconstrained limited-memory algorithm (L-BFGS) <ref> [19] </ref> together with the capability of handling bounds, at the cost of a significantly more complex code.
Reference: [20] <author> J. J. More and G. Toraldo, </author> <title> "Algorithms for bound constrained quadratic programming problems," </title> <journal> Numer. Math. </journal> <volume> 55 (1989): </volume> <pages> 377-400. </pages>
Reference-contexts: Department of Energy, under Contract W-31-109-Eng-38, while at the Mathematics and Computer Science Division, Argonne National Laboratory, Argonne, IL 60439. 1 The algorithm described in this paper is similar to the algorithms proposed by Conn, Gould, and Toint [9] and More and Toraldo <ref> [20] </ref>, in that the gradient projection method is used to determine a set of active constraints at each iteration. <p> We used the gradient projection approach [16], [18], [3] to determine the active set, because recent studies [7], [5] indicate that it possesses good theoretical properties, and because it also appears to be efficient on many large problems [8], <ref> [20] </ref>.
Reference: [21] <author> J. Nocedal, </author> <title> "Updating quasi-Newton matrices with limited storage," </title> <booktitle> Mathematics of Computation 35 (1980): </booktitle> <pages> 773-782. 22 </pages>
Reference-contexts: The Hessian approximations B k used in our algorithm are limited-memory BFGS matrices (Nocedal <ref> [21] </ref> and Byrd, Nocedal, and Schnabel [6]).
References-found: 21

