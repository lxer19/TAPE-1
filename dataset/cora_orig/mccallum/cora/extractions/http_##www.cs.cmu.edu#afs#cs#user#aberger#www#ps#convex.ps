URL: http://www.cs.cmu.edu/afs/cs/user/aberger/www/ps/convex.ps
Refering-URL: http://www.cs.cmu.edu/~aberger/maxent.html
Root-URL: 
Email: aberger@cs.cmu.edu  
Title: Convexity, Maximum Likelihood and All That  
Author: Adam Berger 
Affiliation: School of Computer Science Carnegie Mellon University  
Abstract: Where examples are called for, we draw from applications in human language technology. 
Abstract-found: 1
Intro-found: 1
Reference: [EM algorithm] 
Reference: [Ba72] <author> L. Baum. </author> <title> An inequality and associated maximization technique in statistical estima tion of probabilistic functions of a Markov process (1972). </title> <journal> Inequalities, </journal> <volume> 3, </volume> <pages> 1-8. 10 </pages>
Reference: [De77] <author> A. Dempster, N. Laird, and D. </author> <title> Rubin (1977). Maximum likelihood from incomplete data via the EM algorithm. </title> <journal> Journal of the Royal Statistical Society, </journal> <volume> 39(B), </volume> <pages> 1-38. </pages>
Reference: [Ha58] <author> H. </author> <title> Hartley (1958). Maximum likelihood estimation from incomplete data. </title> <type> Biometrics 14, </type> <pages> 174-194. </pages>
Reference-contexts: It's not difficult to imagine that someone might think up this algorithm without having the mathematical equipment (in the EM algorithm) to prove anything about it. In fact, at least two people did <ref> [Ha58] </ref> [Welch]. 7 3 IIS algorithm We now consider a different family of conditional distributions|feature-based exponential models.
Reference: [La93a] <author> J. </author> <title> Lafferty (1993). A derivation of the inside-outside algorithm from the EM algo rithm. </title> <type> IBM Technical Report. </type>
Reference-contexts: But this doesn't rule out the possibility of Getting "stuck" at a local maximum Toggling between two local maxima with very different but identical likelihoods. Under certain assumptions, it can be shown <ref> [La93a] </ref> that lim n n = ? ; that is, eventually the EM algorithm converges to the optimal parameter values. Unfortunately, these assumptions are rather restrictive and aren't typically met in practice. 5 It may very well happen that the space is very "bumpy," with lots of local maxima.
Reference: [Welch] <author> L. Welch, </author> <title> unpublished. [Parameter estimation for exponential models] </title>
Reference-contexts: It's not difficult to imagine that someone might think up this algorithm without having the mathematical equipment (in the EM algorithm) to prove anything about it. In fact, at least two people did [Ha58] <ref> [Welch] </ref>. 7 3 IIS algorithm We now consider a different family of conditional distributions|feature-based exponential models. Unlike the EM algorithm, we now assume nothing is hidden; the task is the pure maximum likelihood problem of discovering the optimal values for the model parameters, given some empirical sample ~p (x; y).
Reference: [Br59] <author> D. </author> <title> Brown (1959). A note on approximations to discrete probability distributions. </title> <journal> Information and Control, </journal> <volume> 2, </volume> <pages> 386-392. </pages>
Reference: [Cs84] <author> I. Csiszar and G. </author> <title> Tusnady (1984). Information geometry and alternating minimization procedures. Statistics and Decisions, </title> <booktitle> Supplemental Issue:1, </booktitle> <pages> 205-237. </pages>
Reference: [De97] <author> S. Della Pietra, V. Della Pietra and J. </author> <title> Lafferty (1997) Inducing features of random fields. </title> <journal> IEEE Transactions on Pattern Analysis and Machine Intelligence, </journal> <note> 19(4). [Also of interest] </note>
Reference-contexts: Associated with the model p is some finite collection of n such functions. How one might select these functions is not the topic of this note. There exist methods for automatically "learning" informative features from a large collection; see <ref> [De97] </ref>. * i is a real-valued weight associated with f i . Technically, i is the Lagrange multiplier corresponding to the feature f i in a certain constrained optimization problem. In this sense, the absolute value of i is a measure of the "importance" of the feature f i .
Reference: [Ba83] <author> L. Bahl, F. Jelinek and R. </author> <title> Mercer (1983). A maximum likelihood approach to con tinuous speech recognition. </title> <journal> IEEE Transactions on Pattern Analysis and Machine Intelligence, </journal> <volume> PAMI-5, </volume> <pages> 179-190. </pages>
Reference-contexts: After all, the sample of observed output which constitutes ~p is only a representative sample of the process being modelled. The justification for maximum likelihood lies in its empirical success in selected applications (such as speech recognition <ref> [Ba83] </ref>) and in the convenient algorithms it gives rise to|such as the EM and IIS algorithms. On the flip side, maximum likelihood's prime liability is the overfitting problem.
Reference: [Be96] <author> A. Berger, S. Della Pietra and V. </author> <title> Della Pietra (1996). A maximum entropy approach to natural language processing. </title> <journal> Computational Linguistics, </journal> <volume> 22(1), </volume> <pages> 39-71. </pages>
Reference-contexts: The language modelling problem, for instance, is to construct a conditional probability distribution function (p.d.f.) p (y j x), where y is identity of the next word in some text, and x is the conditioning information, such as the identity of the preceding words. Machine translation [Br91], word-sense disambiguation <ref> [Be96] </ref>, part-of-speech tagging [Me90] and parsing of natural language [Bl92] are just a few other human language-related domains involving stochastic modelling. 1 We will concern ourselves in this document with deriving the expectation- maximization (EM) and improved iterative scaling (IIS) algorithms.
Reference: [Bl92] <author> E. Black, F. Jelinek, J. Lafferty, D. Magerman, R. Mercer, and S. Roukos, </author> <title> (1992) Towards history-based grammars: using richer models for probabilistic parsing. </title> <booktitle> Proceedings of the DARPA Speech and Natural Language Workshop, </booktitle> <address> Arden House, New York. </address>
Reference-contexts: Machine translation [Br91], word-sense disambiguation [Be96], part-of-speech tagging [Me90] and parsing of natural language <ref> [Bl92] </ref> are just a few other human language-related domains involving stochastic modelling. 1 We will concern ourselves in this document with deriving the expectation- maximization (EM) and improved iterative scaling (IIS) algorithms. Both discover an optimal (locally for EM, globally for IIS) member of a parametric family of models.
Reference: [Br91] <author> P. Brown, R. Mercer, S. Della Pietra, V. </author> <title> Della Pietra (1991). The mathematics of statistical machine translation: parameter estimation. </title> <booktitle> Computational Linguistics 19(2), </booktitle> <pages> 263|311. </pages>
Reference-contexts: The language modelling problem, for instance, is to construct a conditional probability distribution function (p.d.f.) p (y j x), where y is identity of the next word in some text, and x is the conditioning information, such as the identity of the preceding words. Machine translation <ref> [Br91] </ref>, word-sense disambiguation [Be96], part-of-speech tagging [Me90] and parsing of natural language [Bl92] are just a few other human language-related domains involving stochastic modelling. 1 We will concern ourselves in this document with deriving the expectation- maximization (EM) and improved iterative scaling (IIS) algorithms. <p> The EM algorithm arises in other human-language settings as well. In a parsing model, the words are again the observed output, but now the hidden state is the parse of the sentence. 4 In <ref> [Br91] </ref>, a English-French translation model is described in which the alignment between the words in the French sentence and its translation represents the hidden information. We postulate a parametric model p (y; h) of the process, with marginal distribution p (y) = h p (y; h).
Reference: [Br87] <author> P. </author> <title> Brown (1987). The acoustic modelling problem in automatic speech recognition. </title> <type> Ph.D. thesis, </type> <institution> Carnegie Mellon University. </institution>
Reference-contexts: Backing up to some middle ground requires ad hoc techniques such as smoothing the model parameters [Je80] [Ka87] using some data held out from the training. There have been some efforts to introduce alternative parameter-estimation approaches which avoid this difficulty <ref> [Br87] </ref>. 1.2 Convexity Recall that a function f (x) is convex ("concave up") if f (ffx 0 + (1 ff)x 1 ) fff (x 0 ) + (1 ff)f (x 1 ) for all 0 ff 1 (4) That is, if one selects any two points x 0 and x 1
Reference: [Je80] <author> F. Jelinek and R. </author> <title> Mercer (1980). Interpolated estimation of Markov source param eters from sparse data. </title> <booktitle> In Proceedings, Workshop on Pattern Recognition in Practice, </booktitle> <address> Amsterdam, The Netherlands. </address> <month> 11 </month>
Reference-contexts: Over-fitting is what happens when a model does its job too well, becoming so faithful to the data 2 it was trained on that it doesn't generalize well to new data. Backing up to some middle ground requires ad hoc techniques such as smoothing the model parameters <ref> [Je80] </ref> [Ka87] using some data held out from the training.
Reference: [Ka87] <author> S. </author> <title> Katz (1987). Estimation of probabilities from sparse data for the langauge model component of a speech recognizer. </title> <journal> IEEE Transactions on Acoustics, Speech and Signal Processing, </journal> <volume> ASSP-35(3), </volume> <pages> 400-401, </pages> <month> March. </month>
Reference-contexts: Over-fitting is what happens when a model does its job too well, becoming so faithful to the data 2 it was trained on that it doesn't generalize well to new data. Backing up to some middle ground requires ad hoc techniques such as smoothing the model parameters [Je80] <ref> [Ka87] </ref> using some data held out from the training.
Reference: [Me90] <author> B. </author> <month> Merialdo </month> <year> (1990). </year> <title> Tagging text with a probabilistic model. </title> <booktitle> Proceedings of the IBM Natural Language ITL, Paris, France, </booktitle> <pages> 161-172. </pages>
Reference-contexts: Machine translation [Br91], word-sense disambiguation [Be96], part-of-speech tagging <ref> [Me90] </ref> and parsing of natural language [Bl92] are just a few other human language-related domains involving stochastic modelling. 1 We will concern ourselves in this document with deriving the expectation- maximization (EM) and improved iterative scaling (IIS) algorithms.
Reference: [Po88] <author> A. Poritz. </author> <title> Hidden markov models: a guided tour. IEEE Conference on Acoustics, Speech and Signal Processing. These notes were informed by unpublished notes on the EM algorithm, forward-backward algorithm, and linear interpolation, written by Peter Brown, </title> <editor> Robert Mercer, John Lafferty, and Harry Printz. </editor> <volume> 12 </volume>
Reference-contexts: Often some cleverness suffices to sidestep this computational hurdle|usually by relying on some underlying Markov property of the model. Such cleverness is what distinguishes the forward-backward algorithm <ref> [Po88] </ref>. Example: linear interpolation A quite typical problem in statistical modelling is to construct a "hyper-model" which is the linear interpolation of a collection of models.
References-found: 18

