URL: ftp://ftp.idsia.ch/pub/techrep/IDSIA-33-98.ps.gz
Refering-URL: http://www.idsia.ch/techrep.html
Root-URL: http://www.idsia.ch/techrep.html
Email: nic@idsia.ch  
Title: Accelerated Gradient Descent by Factor-Centering Decomposition  
Author: Nicol N. Schraudolph 
Date: May 20, 1998  
Web: http://www.idsia.ch/  
Address: Corso Elvezia 36 6900 Lugano, Switzerland  
Affiliation: IDSIA,  
Pubnum: Technical Report IDSIA-33-98  
Abstract: Gradient factor centering is a new methodology for decomposing neural networks into biased and centered subnets which are then trained in parallel. The decomposition can be applied to any pattern-dependent factor in the network's gradient, and is designed such that the subnets are more amenable to optimization by gradient descent than the original network: biased subnets because of their simplified architecture, centered subnets due to a modified gradient that improves conditioning. The architectural and algorithmic modifications mandated by this approach include both familiar and novel elements, often in prescribed combinations. The framework suggests for instance that shortcut connections | a well-known architectural feature | should work best in conjunction with slope centering, a new technique described herein. Our benchmark experiments bear out this prediction, and show that factor-centering decomposition can speed up learning significantly without adversely affecting the trained network's generalization ability. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Battiti, R. </author> <year> (1992). </year> <title> First- and second-order methods for learning: Between steepest descent and Newton's method. </title> <journal> Neural Computation, </journal> <volume> 4 (2), </volume> <pages> 141-166. </pages>
Reference: <author> Deterding, D. H. </author> <year> (1989). </year> <title> Speaker Normalisation for Automatic Speech Recognition. </title> <type> Ph.D. thesis, </type> <institution> University of Cambridge. </institution>
Reference-contexts: Left: using activity (solid lines) and/or error (filled marks) centering; right: using slope centering (dashed lines) and/or shortcut weights (filled marks). 5.2 Vowel Recognition Problem We also tested our approach on a speaker-independent vowel recognition problem <ref> (Deterding, 1989) </ref> which has been adopted (Robinson, 1989) as a popular neural network benchmark (e.g., Hochreiter & Schmidhuber, 1997; Flake, 1998). The task is to recognize the eleven steady-state vowels of British English, given 10 spectral features of the speech signal.
Reference: <author> Flake, G. W. </author> <year> (1998). </year> <title> Square unit augmented, radially extended, multilayer perceptrons. In Neural Networks: Tricks of the Trade (Orr & Muller, </title> <booktitle> 1998), </booktitle> <pages> pp. 145-163. </pages>
Reference: <author> Hochreiter, S., & Schmidhuber, J. </author> <year> (1997). </year> <title> Unsupervised coding with lococode. </title> <booktitle> In Proceedings of the 7th International Conference on Artificial Neural Networks, </booktitle> <pages> pp. 655-660 Lausanne, </pages> <address> Switzerland. </address> <publisher> Springer Verlag, </publisher> <address> Berlin. </address>
Reference: <author> Lapedes, A., & Farber, R. </author> <year> (1986). </year> <title> A self-optimizing, nonsymmetrical neural net for content addressable memory and pattern recognition. </title> <journal> Physica, </journal> <volume> D 22, </volume> <pages> 247-259. </pages>
Reference: <author> LeCun, Y., Kanter, I., & Solla, S. A. </author> <year> (1991). </year> <title> Eigenvalues of covariance matrices: Application to neural-network learning. </title> <journal> Physical Review Letters, </journal> <volume> 66 (18), </volume> <pages> 2396-2399. </pages>
Reference: <author> Neuneier, R., & Zimmermann, H. G. </author> <year> (1998). </year> <title> How to train neural networks. In Neural Networks: Tricks of the Trade (Orr & Muller, </title> <booktitle> 1998), </booktitle> <pages> pp. 373-423. </pages>
Reference: <editor> Orr, G. B., & Muller, K.-R. (Eds.). </editor> <year> (1998). </year> <title> Neural Networks: Tricks of the Trade, </title> <booktitle> Vol. 1524 of Lecture Notes in Computer Science. </booktitle> <publisher> Springer Verlag, </publisher> <address> Berlin. </address>
Reference: <author> Robinson, A. J. </author> <year> (1989). </year> <title> Dynamic Error Propagation Networks. </title> <type> Ph.D. thesis, </type> <institution> University of Cambridge. </institution>
Reference-contexts: Left: using activity (solid lines) and/or error (filled marks) centering; right: using slope centering (dashed lines) and/or shortcut weights (filled marks). 5.2 Vowel Recognition Problem We also tested our approach on a speaker-independent vowel recognition problem (Deterding, 1989) which has been adopted <ref> (Robinson, 1989) </ref> as a popular neural network benchmark (e.g., Hochreiter & Schmidhuber, 1997; Flake, 1998). The task is to recognize the eleven steady-state vowels of British English, given 10 spectral features of the speech signal.
Reference: <author> Schraudolph, N. N. </author> <year> (1998a). </year> <title> Centering neural network gradient factors. In Neural Networks: Tricks of the Trade (Orr & Muller, </title> <booktitle> 1998), </booktitle> <pages> pp. 207-226. </pages> <address> ftp://ftp.idsia.ch/ pub/nic/center.ps.gz fl. </address>
Reference-contexts: Thus any performance advantage for our approach reported thereafter has been realized on top of a state-of-the-art accelerated gradient method as control. See <ref> (Schraudolph, 1998a) </ref> for further details. 5.1 Symmetry Detection Problem A fully connected 8-8-1 feedforward network was to indicate whether a given binary input was symmetric about its middle axis or not. <p> After each epoch, generalization ability was measured in terms of the maximum likelihood misclassification rate on the test set | again, see <ref> (Schraudolph, 1998a) </ref> for further details.
Reference: <author> Schraudolph, N. N. </author> <year> (1998b). </year> <title> Slope centering: Making shortcut weights effective. </title> <editor> In Niklasson, L., Boden, M., & Ziemke, T. (Eds.), </editor> <booktitle> Proceedings of the 8th International Conference on Artificial Neural Networks, Perspectives in Neural Computing, </booktitle> <pages> pp. 523-528 Skovde, </pages> <address> Sweden. </address> <publisher> Springer Verlag, </publisher> <address> Berlin. ftp://ftp.idsia.ch/pub/nic/ slope.ps.gz fl. </address>
Reference-contexts: We note that while shortcuts render the linear component of the error irrelevant to a hidden node in principle, it is slope centering that actually removes this component from its error signal | see <ref> (Schraudolph, 1998b) </ref> for a more detailed discussion.
Reference: <author> Schraudolph, N. N., & Sejnowski, T. J. </author> <year> (1996). </year> <title> Tempering backpropagation networks: Not all weights are created equal. </title> <editor> In Touretzky, D. S., Mozer, M. C., & Hasselmo, M. E. (Eds.), </editor> <booktitle> Advances in Neural Information Processing Systems, </booktitle> <volume> Vol. 8, </volume> <pages> pp. 563-569. </pages> <publisher> The MIT Press, </publisher> <address> Cambridge, MA. ftp://ftp.idsia.ch/pub/nic/nips95.ps.gz fl. </address>
Reference-contexts: Note that the centered subnet has no bias weights, since bias inputs are rendered ineffective (constant zero) by centering. Our approach thus decomposes the network into bias and non-bias weights, the point being that these two should be trained at different rates <ref> (cf. Schraudolph & Sejnowski, 1996) </ref>. input can reintroduce the large eigenvalue that centering has laboriously eliminated. We have suggested before (Schraudolph & Sejnowski, 1996) that one can also center the node's error signal ffi. <p> Our approach thus decomposes the network into bias and non-bias weights, the point being that these two should be trained at different rates (cf. Schraudolph & Sejnowski, 1996). input can reintroduce the large eigenvalue that centering has laboriously eliminated. We have suggested before <ref> (Schraudolph & Sejnowski, 1996) </ref> that one can also center the node's error signal ffi.
Reference: <author> Widrow, B., McCool, J. M., Larimore, M. G., & Johnson, Jr., C. R. </author> <year> (1976). </year> <title> Stationary and nonstationary learning characteristics of the LMS adaptive filter. </title> <booktitle> Proceedings of the IEEE, </booktitle> <volume> 64 (8), </volume> <pages> 1151-1162. </pages>
Reference-contexts: Clockwise from upper left: uncentered, centered a priori by subtracting 0.5, centered exactly, and centered with a bias column added. is the error signal. The convergence time of this system is proportional to the condition number (the ratio between largest and smallest eigenvalue) of the Hessian <ref> (Widrow et al., 1976) </ref>, which here equals the covariance of the inputs: H = . LeCun et al. (1991) have observed that centering the input vector ~x typically decimates the largest eigenvalue of H, permitting an increase of the learning rate j, and a correspondingly shorter convergence time.
Reference: <author> Zimmermann, H. G. </author> <year> (1994). </year> <editor> Neuronale Netze als Entscheidungskalkul. In Rehkugler, H., & Zimmermann, H. G. (Eds.), Neuronale Netze in der Okonomie: </editor> <booktitle> Grundlagen und finanzwirtschaftliche Anwendungen, </booktitle> <pages> pp. 1-87. </pages> <publisher> Vahlen Verlag, </publisher> <address> Munich. </address> <month> 8 </month>
References-found: 14

