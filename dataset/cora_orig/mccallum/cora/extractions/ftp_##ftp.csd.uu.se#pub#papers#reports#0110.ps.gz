URL: ftp://ftp.csd.uu.se/pub/papers/reports/0110.ps.gz
Refering-URL: http://www.csd.uu.se/papers/reports.html
Root-URL: 
Email: e-mail: fthomasl,jb,hakanmg@csd.uu.se  
Phone: Phone: +481818 25 00 Fax: +461851 19 25  
Title: Compiler optimizations in Reform Prolog: experiments on the KSR-1 multiprocessor  
Author: Thomas Lindgren, Johan Bevemyr and H-akan Millroth 
Address: Box 311 751 05 Uppsala, SWEDEN  Box 311, S-751 05 Uppsala, Sweden  
Affiliation: Computing Science Department, Uppsala University  
Abstract: UPMAIL Technical Report No. 110 June, 1995 ISSN 1100-0686 Abstract We describe the compiler analyses of Reform Prolog and evaluate their effectiveness in eliminating suspension and locking on a range of benchmarks. The results of the analysis may also be used to extract non-strict independent and-parallelism. We find that 90% of the predicate arguments are ground or local, and that 95% of the predicate arguments do not require suspension code. Hence, very few suspension operations need to be generated to maintain sequential semantics. The compiler can also remove unnecessary locking of local data by locking only updates to shared data; however, even though locking writes are reduced to 52% of the unoptimized number for our benchmark set, this has little effect on execution times. We find that the ineffectiveness of locking elimination is due to the relative rarity of locking writes, and the execution model of Reform Prolog, which results in few invalidations of shared cache lines when such writes occur. The benchmarks are evaluated on a cache-coherent KSR-1 multiprocessor with physically distributed memory, using up to 48 processors. Speedups scale from previous results on smaller, bus-based multiprocessors, and previous low parallelization overheads are retained.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> J. Bevemyr, </author> <title> A Recursion Parallel Prolog Engine, </title> <institution> Licentiate of Philosophy Thesis, Uppsala Theses in Computer Science 16/93, Uppsala University, </institution> <year> 1993. </year>
Reference-contexts: The parallel implementation is designed as extension of a sequential Prolog machine <ref> [1] </ref>. The implementation imposes very little overhead for process management, such as scheduling and load balancing. We have previously described the compilation scheme [14, 15], execution model [2, 3], and parallel abstract machine [2, 3] of Reform Prolog.
Reference: [2] <author> J. Bevemyr, T. Lindgren & H. Millroth, </author> <title> Exploiting recursion-parallelism in Prolog, </title> <booktitle> Int. Conf. </booktitle> <editor> PARLE-93 (eds. A. Bode, M. Reeve & G. Wolf), </editor> <publisher> Springer LNCS 694, Springer-Verlag, </publisher> <year> 1993. </year>
Reference-contexts: The parallel implementation is designed as extension of a sequential Prolog machine [1]. The implementation imposes very little overhead for process management, such as scheduling and load balancing. We have previously described the compilation scheme [14, 15], execution model <ref> [2, 3] </ref>, and parallel abstract machine [2, 3] of Reform Prolog. In this paper we describe a set of compiler analyses and optimizations for increasing available parallelism and reducing parallelization overheads. We discuss the effectiveness of the optimizations and the runtime space/time efficiency of the compiler. <p> The parallel implementation is designed as extension of a sequential Prolog machine [1]. The implementation imposes very little overhead for process management, such as scheduling and load balancing. We have previously described the compilation scheme [14, 15], execution model <ref> [2, 3] </ref>, and parallel abstract machine [2, 3] of Reform Prolog. In this paper we describe a set of compiler analyses and optimizations for increasing available parallelism and reducing parallelization overheads. We discuss the effectiveness of the optimizations and the runtime space/time efficiency of the compiler. <p> However, the processes descend through the tree in parallel, temporally suspending when encountering not-yet-created subtrees. 2 The parallel execution model of Reform Prolog restricts the nondeterministic behaviour of parallel programs so that the following properties hold <ref> [2, 3] </ref>: * Parallel programs obey the sequential semantics of Prolog. This implies that time-dependent operations (type tests, etc.) on shared, unbound variables are carried out only when leftmost is the sequential computation order. * Parallel programs do not conditionally bind shared variables.
Reference: [3] <author> J. Bevemyr, T. Lindgren & H. Millroth, </author> <title> Reform Prolog: The language and its implementation, </title> <booktitle> Proc. 10th Int. Conf. Logic Programming, </booktitle> <publisher> MIT Press, </publisher> <year> 1993. </year>
Reference-contexts: The parallel implementation is designed as extension of a sequential Prolog machine [1]. The implementation imposes very little overhead for process management, such as scheduling and load balancing. We have previously described the compilation scheme [14, 15], execution model <ref> [2, 3] </ref>, and parallel abstract machine [2, 3] of Reform Prolog. In this paper we describe a set of compiler analyses and optimizations for increasing available parallelism and reducing parallelization overheads. We discuss the effectiveness of the optimizations and the runtime space/time efficiency of the compiler. <p> The parallel implementation is designed as extension of a sequential Prolog machine [1]. The implementation imposes very little overhead for process management, such as scheduling and load balancing. We have previously described the compilation scheme [14, 15], execution model <ref> [2, 3] </ref>, and parallel abstract machine [2, 3] of Reform Prolog. In this paper we describe a set of compiler analyses and optimizations for increasing available parallelism and reducing parallelization overheads. We discuss the effectiveness of the optimizations and the runtime space/time efficiency of the compiler. <p> However, the processes descend through the tree in parallel, temporally suspending when encountering not-yet-created subtrees. 2 The parallel execution model of Reform Prolog restricts the nondeterministic behaviour of parallel programs so that the following properties hold <ref> [2, 3] </ref>: * Parallel programs obey the sequential semantics of Prolog. This implies that time-dependent operations (type tests, etc.) on shared, unbound variables are carried out only when leftmost is the sequential computation order. * Parallel programs do not conditionally bind shared variables.
Reference: [4] <author> J.-H. Chang, </author> <title> High performance execution of Prolog programs based on a static dependency analysis, </title> <type> Ph.D. Thesis, </type> <institution> UCB/CSD 86/263, Univ. Calif. Berkeley, </institution> <year> 1986. </year>
Reference-contexts: The analysis precision is similar to that of Aquarius Prolog [20]. 3.2 Aliasing and linearity The analyzer derives possible and certain aliases by maintaining equivalence classes of possibly or certainly aliased variables. This is similar to the techniques used by Chang <ref> [4] </ref>. A term is linear if no variable occurs more than once in it. To improve aliasing information, the analyzer tracks whether terms are linear [13]. Three classes of linearity are distinguished: linear, nonlinear, and indlist. The latter denotes lists where elements do not share variables.
Reference: [5] <author> M. Codish, A. Mulkers, M. Bruynooghe, M. Garcia de la Banda & M. Hermenegildo, </author> <title> Improving abstract interpretations by combining domains, </title> <booktitle> Proc. 1993 Symp. Partial Evaluation and Program Manipulation, </booktitle> <publisher> ACM Press, </publisher> <year> 1993. </year>
Reference: [6] <author> P. Cousot & R. Cousot, </author> <title> Abstract interpretation: a unified lattice model for static analysis of programs by construction or approximation of fixpoints, </title> <booktitle> Proc. 4th ACM Symp. Principles of Programming Languages, </booktitle> <publisher> ACM Press, </publisher> <year> 1977. </year>
Reference-contexts: In particular, the compiler can generate precisely the code for a sequential Prolog machine when data are local. 3 Compiler analyses The compiler analyses in the Reform Prolog compiler are based on abstract interpretation <ref> [6] </ref>. The abstract interpreter for Reform Prolog shares most of the characteristics of an abstract interpreter for sequential Prolog. This is natural, since each parallel process executes almost as a sequential Prolog machine. We have modified Debray's dataflow algorithms [7, 8] for analysis of parallel recursive predicates.
Reference: [7] <author> S.K. Debray, </author> <title> Static inference of modes and data dependencies in logic pro-grams, </title> <journal> ACM Trans. Programming Languages and Systems, </journal> <volume> Vol. 11, No. 3, </volume> <pages> pp. 418-450, </pages> <month> July </month> <year> 1989. </year>
Reference-contexts: The abstract interpreter for Reform Prolog shares most of the characteristics of an abstract interpreter for sequential Prolog. This is natural, since each parallel process executes almost as a sequential Prolog machine. We have modified Debray's dataflow algorithms <ref> [7, 8] </ref> for analysis of parallel recursive predicates. These algorithms compute call and success patterns for each procedure in the program. Call and success patterns describe the abstract values of the variables in a procedure call at procedure entry and exit, respectively.
Reference: [8] <author> S.K. Debray, </author> <title> Efficient dataflow analysis of logic programs, </title> <journal> J. ACM , Vol. </journal> <volume> 39, No. 4, </volume> <month> October </month> <year> 1992. </year>
Reference-contexts: The abstract interpreter for Reform Prolog shares most of the characteristics of an abstract interpreter for sequential Prolog. This is natural, since each parallel process executes almost as a sequential Prolog machine. We have modified Debray's dataflow algorithms <ref> [7, 8] </ref> for analysis of parallel recursive predicates. These algorithms compute call and success patterns for each procedure in the program. Call and success patterns describe the abstract values of the variables in a procedure call at procedure entry and exit, respectively.
Reference: [9] <author> S.K. Debray & D.S. Warren, </author> <title> Automatic mode inference for logic programs, </title> <journal> J. Logic Programming, </journal> <volume> Vol. 5, No. 3, </volume> <year> 1988. </year>
Reference-contexts: The compiler carries out four different analyses using the same basic algorithm. The abstract domains of these analyses are described below. 3.1 Types The type domain is similar to that of Debray and Warren <ref> [9] </ref>, extended to handle difference lists [17]. For our present concerns it suffices to note that the type analysis can discover ground and nonvariable terms.
Reference: [10] <author> M.A. Friedman, </author> <title> A characterization of Prolog execution, </title> <type> Ph.D. Thesis, </type> <institution> University of Wisconsin at Madison, </institution> <year> 1992. </year>
Reference-contexts: Two factors contribute to this phenomenon: First, locking instructions are infrequent even in unoptimized code. Locking is spatially infrequent, since assignments to heap variables is a small fraction of the total amount of data written in Prolog implementations <ref> [19, 10] </ref>. Locking is temporally infrequent, since our Prolog implementation is based on byte-code emulation of WAM [21] instructions. In unoptimized code, 2100-3700 machine instructions were executed for each locking operation on the three larger benchmarks.
Reference: [11] <author> T.W. Gentzinger, </author> <title> Abstract interpretation for the compile-time optimization of logic programs, </title> <type> Ph.D. Thesis, </type> <institution> University of South California, </institution> <type> Report 93/09, </type> <year> 1993. </year>
Reference-contexts: Furthermore, the absolute compilation times (0.6 to 14 seconds) are quite reasonable, in particular when considering that the SUN 630/MP is not a particulary fast machine by today's standards. Aquarius Prolog seems to have similar absolute analysis times on similar hardware <ref> [11] </ref>. 7 Analysis results We measured analysis results for arguments in procedures called from parallel predicates. The following table shows the percentages of ground arguments and the locality information of non-ground arguments.
Reference: [12] <author> M.V. Hermenegildo & F. Rossi, </author> <title> Non-strict independent and-parallelism, </title> <booktitle> Proc. 7th Int. Conf. Logic Programming, </booktitle> <publisher> MIT Press, </publisher> <year> 1990. </year>
Reference-contexts: The locality domain is thus: local v robust v wbf v fragile This locality domain can furthermore be used to detect non-strict independent and-parallelism <ref> [12] </ref>. As long as a process does not contain fragile data, it is independent of the results of other processes.
Reference: [13] <author> N. Jones & H. Stndergaard, </author> <title> A semantics-based framework for the abstract interpretation of Prolog, </title> <type> report 86/14, </type> <institution> University of Copenhagen, </institution> <year> 1986. </year>
Reference-contexts: This is similar to the techniques used by Chang [4]. A term is linear if no variable occurs more than once in it. To improve aliasing information, the analyzer tracks whether terms are linear <ref> [13] </ref>. Three classes of linearity are distinguished: linear, nonlinear, and indlist. The latter denotes lists where elements do not share variables.
Reference: [14] <author> H. Millroth, </author> <title> Reforming compilation of logic programs, </title> <booktitle> Proc. 1991 Int. Symp. Logic Programming, </booktitle> <publisher> MIT Press, </publisher> <year> 1991. </year>
Reference-contexts: The parallel implementation is designed as extension of a sequential Prolog machine [1]. The implementation imposes very little overhead for process management, such as scheduling and load balancing. We have previously described the compilation scheme <ref> [14, 15] </ref>, execution model [2, 3], and parallel abstract machine [2, 3] of Reform Prolog. In this paper we describe a set of compiler analyses and optimizations for increasing available parallelism and reducing parallelization overheads. We discuss the effectiveness of the optimizations and the runtime space/time efficiency of the compiler. <p> Each thread runs an instance of the same recursive program in an asynchronous parallel computation. This model is often called SPMD (Single Program Multiple Data). The programming model is realized by a compilation technique that translates a regular form of recursion to a parallelizable form of iteration <ref> [14, 15] </ref>. Example. The following program compares a sequence B with a list of sequences. Each comparison, carried out by match/3, computes a similarity value V that is stored in a sorted tree T for later access.
Reference: [15] <author> H. Millroth, SLDR-resolution: </author> <title> parallelizing structural recursion in logic programs, </title> <journal> J. Logic Programming, </journal> <note> to appear. </note>
Reference-contexts: The parallel implementation is designed as extension of a sequential Prolog machine [1]. The implementation imposes very little overhead for process management, such as scheduling and load balancing. We have previously described the compilation scheme <ref> [14, 15] </ref>, execution model [2, 3], and parallel abstract machine [2, 3] of Reform Prolog. In this paper we describe a set of compiler analyses and optimizations for increasing available parallelism and reducing parallelization overheads. We discuss the effectiveness of the optimizations and the runtime space/time efficiency of the compiler. <p> Each thread runs an instance of the same recursive program in an asynchronous parallel computation. This model is often called SPMD (Single Program Multiple Data). The programming model is realized by a compilation technique that translates a regular form of recursion to a parallelizable form of iteration <ref> [14, 15] </ref>. Example. The following program compares a sequence B with a list of sequences. Each comparison, carried out by match/3, computes a similarity value V that is stored in a sorted tree T for later access.
Reference: [16] <author> L. Naish, </author> <title> Parallelizing NU-Prolog, </title> <booktitle> Proc. 5th Int. Conf. Symp. Logic Programming, </booktitle> <publisher> MIT Press, </publisher> <year> 1988. </year>
Reference-contexts: This implies that time-dependent operations (type tests, etc.) on shared, unbound variables are carried out only when leftmost is the sequential computation order. * Parallel programs do not conditionally bind shared variables. This is similar to binding determinism as defined by Naish <ref> [16] </ref> in that shared variables can only be bound when the process is deterministic. However, in contrast to Naish's binding determinism, nondeterministic bindings to local variables are allowed.
Reference: [17] <author> T. Lindgren, </author> <title> The compilation and execution of recursion-parallel Prolog on shared-memory multiprocessors, </title> <booktitle> Licentiate of Philosophy Thesis, Uppsala Theses in Computer Science 18/93, </booktitle> <month> November </month> <year> 1993. </year>
Reference-contexts: However, in contrast to Naish's binding determinism, nondeterministic bindings to local variables are allowed. In order to ensure these properties, the Reform Prolog compiler performs a global dataflow analysis and generates code that suspends processes and perform atomic updates only when necessary <ref> [17] </ref>. In particular, the compiler can generate precisely the code for a sequential Prolog machine when data are local. 3 Compiler analyses The compiler analyses in the Reform Prolog compiler are based on abstract interpretation [6]. <p> The compiler carries out four different analyses using the same basic algorithm. The abstract domains of these analyses are described below. 3.1 Types The type domain is similar to that of Debray and Warren [9], extended to handle difference lists <ref> [17] </ref>. For our present concerns it suffices to note that the type analysis can discover ground and nonvariable terms.
Reference: [18] <author> D.A. Patterson & J.L. Hennessy, </author> <title> Computer Organization & Design: The Hardware/Software Interface, </title> <publisher> Morgan Kaufmann Publ., </publisher> <year> 1993. </year>
Reference-contexts: It is reasonable to expect that most future single adress-space architectures will have distributed memories and to expect a continuing increase in processor to memory speed ratio <ref> [18] </ref>. The performance measurements on the KSR-1 can be summarized as follows. * Low parallelization overhead (0-17%, with the larger benchmarks in the range of 2-6%). * Good absolute parallel efficiency on 48 processors (82-91%) provided that there is enough parallelism in the program.
Reference: [19] <author> E. Tick, </author> <title> Memory- and buffer-referencing characteristics of a WAM-based Pro-log, </title> <journal> J. Logic Programming, </journal> <volume> Vol. 11, </volume> <pages> pp. 133-162, </pages> <year> 1991. </year>
Reference-contexts: Two factors contribute to this phenomenon: First, locking instructions are infrequent even in unoptimized code. Locking is spatially infrequent, since assignments to heap variables is a small fraction of the total amount of data written in Prolog implementations <ref> [19, 10] </ref>. Locking is temporally infrequent, since our Prolog implementation is based on byte-code emulation of WAM [21] instructions. In unoptimized code, 2100-3700 machine instructions were executed for each locking operation on the three larger benchmarks.
Reference: [20] <author> P. Van Roy, A. Despain, </author> <title> The benefits of global dataflow analysis for an optimizing Prolog compiler, </title> <booktitle> Proc. 1990 North Am. Conf. Logic Programming, </booktitle> <publisher> MIT Press, </publisher> <year> 1990. </year>
Reference-contexts: For our present concerns it suffices to note that the type analysis can discover ground and nonvariable terms. The analysis precision is similar to that of Aquarius Prolog <ref> [20] </ref>. 3.2 Aliasing and linearity The analyzer derives possible and certain aliases by maintaining equivalence classes of possibly or certainly aliased variables. This is similar to the techniques used by Chang [4]. A term is linear if no variable occurs more than once in it.
Reference: [21] <author> D.H.D. Warren, </author> <title> An Abstract Prolog Instruction Set, </title> <type> SRI Tech. Note 309, </type> <institution> SRI International, Menlo Park, Calif., USA, </institution> <year> 1983. </year> <month> 11 </month>
Reference-contexts: To improve precision, the compiler uses abstract indexing to approximate the first-argument indexing that will occur at runtime. This technique selects the possible paths for the inferred types of the first argument, based on standard WAM indexing <ref> [21] </ref>. 3.4 Locality The analyzer maintains a hierarchy of data locality information: * shared variables exposed to time-dependent operations by another process (clause indexing, arithmetic, type tests, etc.) are fragile and cannot be modified out of the sequential order; * shared variables not subjected to time-dependent operations are robust; * robust <p> Locking is spatially infrequent, since assignments to heap variables is a small fraction of the total amount of data written in Prolog implementations [19, 10]. Locking is temporally infrequent, since our Prolog implementation is based on byte-code emulation of WAM <ref> [21] </ref> instructions. In unoptimized code, 2100-3700 machine instructions were executed for each locking operation on the three larger benchmarks. Second, single locking instructions are, on average, fast due to cache organization: cache lines owned by a single processor do not need global invalidations.
References-found: 21

