URL: http://www.cs.jhu.edu/grad/murthy/survey-paper.ps.Z
Refering-URL: http://www.cs.jhu.edu/grad/murthy/home.html
Root-URL: http://www.cs.jhu.edu
Email: murthy@scr.siemens.com  
Title: Automatic Construction of Decision Trees from Data: A Multi-Disciplinary Survey  
Author: SREERAMA K. MURTHY 
Keyword: classification, tree-structured classifiers, data compaction  
Address: Princeton, NJ 08540, USA  
Affiliation: Siemens Corporate Research,  
Note: 1-49 c Kluwer Academic Publishers, Boston. Manufactured in The Netherlands.  
Abstract: Decision trees have proved to be valuable tools for the description, classification and generalization of data. Work on constructing decision trees from data exists in multiple disciplines such as statistics, pattern recognition, decision theory, signal processing, machine learning and artificial neural networks. Researchers in these disciplines, sometimes working on quite different problems, identified similar issues and heuristics for decision tree construction. This paper surveys existing work on decision tree construction, attempting to identify the important issues involved, directions the work has taken and the current state of the art. 
Abstract-found: 1
Intro-found: 1
Reference: 1. <editor> AAAI. </editor> <booktitle> AAAI-92: Proc. of the Tenth National Conf. on Artificial Intelligence, </booktitle> <address> San Jose, CA, 12-16th, July 1992. </address> <publisher> AAAI Press / The MIT Press. </publisher>
Reference: 2. <editor> AAAI. </editor> <booktitle> AAAI-93: Proc. of the Eleventh National Conf. on Artificial Intelligence, </booktitle> <address> Washing-ton, DC, 11-15th, July 1993. </address> <publisher> AAAI Press / The MIT Press. </publisher>
Reference: 3. <editor> AAAI. </editor> <booktitle> AAAI-94: Proc. of the Twelfth National Conf. on Artificial Intelligence, volume 1, </booktitle> <address> Seattle, WA, </address> <booktitle> 31st July 4th August 1994. </booktitle> <publisher> AAAI Press / The MIT Press. </publisher>
Reference: 4. <author> J. Aczel and J. Daroczy. </author> <title> On measures of information and their characterizations. </title> <publisher> Academic Pub., </publisher> <address> New York, </address> <year> 1975. </year>
Reference-contexts: One interesting early patent on decision tree growing was assigned to IBM (US Patent 4,719,571). 7. The desirable properties of a measure of entropy include symmetry, expandability, decisivity, additivity and recursivity. Shannon's entropy [336] possesses all of these properties <ref> [4] </ref>. For an insightful treatment of entropy reduction as a common theme underlying several pattern recognition problems, see [376]. 8.
Reference: 5. <author> David W. Aha and Richard L. Bankert. </author> <title> A comparitive evaluation of sequential feature selection algorithms. </title> <booktitle> In AI&Statistics-95 [7], </booktitle> <pages> pages 1-7. </pages>
Reference-contexts: There has been a recent surge of interest in feature subset selection methods in the machine learning community, resulting in several empirical evaluations. These studies provide interesting insights on how DECISION TREE CONSTRUCTION: SURVEY 19 to increase the efficiency and effectiveness of the heuristic search for good feature subsets <ref> [185, 210, 48, 81, 257, 5] </ref>. 5.1.2. Composite features Sometimes the aim is not to choose a good subset of features, but instead to find a few good "composite" features, which are arithmetic or logical combinations of the atomic features.
Reference: 6. <editor> AI&Stats-93: </editor> <booktitle> Preliminary Papers of the Fourth Int. Workshop on Artificial Intelligence and Statistics, </booktitle> <address> Ft. Lauderdale, FL, 3rd-6th, </address> <month> January </month> <year> 1993. </year> <institution> Society for AI and Statistics. </institution>
Reference: 7. <editor> AI&Stats-95: </editor> <booktitle> Preliminary Papers of the Fifth Int. Workshop on Artificial Intelligence and Statistics, </booktitle> <address> Ft. Lauderdale, FL, 4-7th, </address> <month> January </month> <year> 1995. </year> <institution> Society for AI and Statistics. </institution>
Reference: 8. <author> C. Aldrich, D. W. Moolman, F. S. Gouws, and G. P. J. Schmitz. </author> <title> Machine learning strategies for control of flotation plants. </title> <journal> Control Eng. Practice, </journal> <volume> 5(2) </volume> <pages> 263-269, </pages> <month> February </month> <year> 1997. </year> <title> DECISION TREE CONSTRUCTION: SURVEY 33 </title>
Reference-contexts: images [323], in star-galaxy classification [378], for determining galaxy counts [377] and discovering quasars [180] in the Second Palomar Sky Survey. * Biomedical Engineering: For identifying features to be used in implantable devices [123]. * Control Systems: For control of nonlinear dynamical systems [154] and con trol of flotation plants <ref> [8] </ref>. * Financial analysis: For asserting the attractiveness of buy-writes [242], among many other data mining applications. 28 SREERAMA K.
Reference: 9. <author> Kamal M. Ali and Michael J. Pazzani. </author> <title> On the link between error correlation and error reduction in decision tree ensembles. </title> <type> Technical Report ICS-TR-95-38, </type> <institution> University of California, Irvine, Department of Information and Computer Science, </institution> <month> September </month> <year> 1995. </year>
Reference-contexts: Classification results of the trees have been combined using either simplistic voting methods [146] or using statistical methods for combining evidence [339]. The relationship between the correlation of errors of individual classifiers and the error of the combined classifier has been explored <ref> [9] </ref>. An alternative to multiple trees is a hybrid classifier that uses several small classifiers as parts of a larger classifier. Brodley [34] describes a system that automatically 22 SREERAMA K.
Reference: 10. <author> Hussein Almuallim and Thomas G. Dietterich. </author> <title> Learning boolean concepts in the presence of many irrelevant features. </title> <journal> Artificial Intelligence, </journal> <volume> 69 </volume> <pages> 279-305, </pages> <year> 1994. </year>
Reference-contexts: In tasks where more features than the "optimal" are available, decision tree quality is known to be affected by the redundant and irrelevant attributes <ref> [10, 323] </ref>. To avoid this problem, either a feature subset selection method (Section 5.1.1) or a method to form a small set of composite features (Section 5.1.2) can be used as a preprocessing step to tree induction. An orthogonal step to feature selection is instance selection.
Reference: 11. <author> Peter Argentiero, Roland Chin, and Paul Beaudet. </author> <title> An automated approach to the design of decision tree classifiers. </title> <journal> IEEE Trans.on Pattern Analysis and Machine Intelligence, </journal> <volume> PAMI-4(1):51-57, </volume> <month> January </month> <year> 1982. </year>
Reference-contexts: We concentrate in this paper on decision trees that are constructed from labeled examples. The problem of learning trees from decision rules instead of examples is addressed in [162]. The problem of learning trees solely from prior probability distributions is considered in <ref> [11] </ref>. Learning decision trees from qualitative causal models acquired from domain experts is the topic of [295]. Given a trained network or any other learned model, Craven's algorithm TREPAN [68] uses queries to induce a decision tree that approximates the function represented by the model.
Reference: 12. <author> Les Atlas, Ronald Cole, Yeshwant Muthuswamy, Alan Lipman, Jerome Connor, Dong Park, Muhammed El-Sharkawi, and Robert J. Marks II. </author> <title> A performance comparison of trained multilayer perceptrons and trained classification trees. </title> <journal> Proc. of the IEEE, </journal> <volume> 78(10) </volume> <pages> 1614-1619, </pages> <year> 1990. </year>
Reference-contexts: The performance of decision trees improved in their study when multivariate splits were used, and back-propagation networks did better with feature selection. Comparisons of symbolic and connectionist methods can also be found in [379, 337]. Multi-layer perceptrons and CART [31] with and without linear combinations are compared in <ref> [12] </ref> to find that there is not much difference in accuracy. Similar conclusions were reached in [106] when ID3 [301] and back-propagation were compared. Talmon et al. [352] compared classification trees and neural networks for analyzing electrocardiograms (ECG) and concluded that no technique is superior to the other.
Reference: 13. <author> Haldun Aytug, Siddhartha Bhattacharya, Gary J. Koehler, and Jane L. Snowdon. </author> <title> A review of machine learning in scheduling. </title> <journal> IEEE Trans. on Eng. Management, </journal> <volume> 41(2) </volume> <pages> 165-171, </pages> <month> May </month> <year> 1994. </year>
Reference-contexts: For a recent review of the use of machine learning (decision trees and other techniques) in scheduling, see <ref> [13] </ref>. * Medicine: Medical research and practice have long been important areas of application for decision tree techniques.
Reference: 14. <author> L. Bahl, P.F.Brown, P.V. de Souza, and R. L. Mercer. </author> <title> A tree-based statistical language model for natural language speech recognition. </title> <journal> IEEE Trans. on Accoustics, Speech and Signal Processing, </journal> <volume> 37(7) </volume> <pages> 1001-1008, </pages> <year> 1989. </year>
Reference-contexts: Use of linear regression to find good feature combinations has been explored recently in [28]. Discovery of good combinations of Boolean features to be used as tests at tree nodes is explored in the machine learning literature in [284] as well as in signal processing <ref> [14] </ref>. Ragavan and Rendell [310] describe a method that constructs Boolean features using lookahead, and uses the constructed feature combinations as tests at tree nodes. Lookahead for construction of Boolean feature combinations is also considered in [389]. Linear threshold unit trees for Boolean functions are described in [321]. <p> Averaging improves probability estimates by considering multiple trees.) Smoothing in the context of tree structured vector quantizers is described in <ref> [14] </ref>. An approach, which refines the class probability estimates in a greedily induced decision tree using local kernel density estimates has been suggested in [345]. Assignment of probabilistic goodness to splits in a decision tree is described in [136].
Reference: 15. <author> Eard Baker and A. K. Jain. </author> <title> On feature ordering in practice and some finite sample effects. </title> <booktitle> In Proc. of the Third Int. Joint Conf. on Pattern Recognition, </booktitle> <pages> pages 45-49, </pages> <address> San Diego, CA, </address> <year> 1976. </year>
Reference-contexts: Splitting rules are essentially ad hoc heuristics for evaluating the strength of dependence between attributes and the class. Comparisons of individual methods may still be interesting if they enlighten the reader about which metric should be used in what situations. Baker and Jain <ref> [15] </ref> reported experiments comparing eleven feature evaluation criteria and concluded that the feature rankings induced by various rules are very similar. Several feature evaluation criteria, including Shannon's entropy and divergence measures, are compared using simulated data in [18], on a sequential, multi-class classification problem.
Reference: 16. <author> F. A. Baker, David L. Verbyla, C. S. Hodges Jr., and E. W. Ross. </author> <title> Classification and regression tree analysis for assessing hazard of pine mortality caused by hetero basidion annosum. Plant Disease, </title> <address> 77(2):136, </address> <month> February </month> <year> 1993. </year>
Reference-contexts: decision trees for analyzing amino acid sequences can be found in [338] and [322]. * Pharmacology: Use of tree based classification for drug analysis can be found in [71]. * Physics: For the detection of physical particles [26]. * Plant diseases: To assess the hazard of mortality to pine trees <ref> [16] </ref>. * Power systems: For power system security assessment [144] and power sta bility prediction [317]. * Remote Sensing: Remote sensing has been a strong application area for pattern recognition work on decision trees (see [350, 182] ).
Reference: 17. <author> W. A. Belson. </author> <title> Matching and prediction on the principle of biological classification. </title> <journal> Applied Statistics, </journal> <volume> 8 </volume> <pages> 65-75, </pages> <year> 1959. </year>
Reference-contexts: They suggested a splitting criterion, called mean posterior improvement (MPI), that emphasizes exclusivity between offspring class subsets instead. Bhattacharya distance [218], Kolmogorov-Smirnoff distance [113, 316, 143] and the O 2 statistic <ref> [17, 141, 246, 389, 380] </ref> are some other distance-based measures that have been used for tree induction. Though the Kolmogorov-Smirnoff distance was originally proposed for tree induction in two-class problems [113, 316], it was subsequently extended to multiclass domains [143].
Reference: 18. <author> Moshe Ben-Bassat. </author> <title> Myopic policies in sequential classification. </title> <journal> IEEE Trans. on Computing, </journal> <volume> 27(2) </volume> <pages> 170-174, </pages> <month> February </month> <year> 1978. </year>
Reference-contexts: Baker and Jain [15] reported experiments comparing eleven feature evaluation criteria and concluded that the feature rankings induced by various rules are very similar. Several feature evaluation criteria, including Shannon's entropy and divergence measures, are compared using simulated data in <ref> [18] </ref>, on a sequential, multi-class classification problem. The conclusions are that no feature selection rule is consistently superior to the others, and that no specific strategy for alternating different rules seems to be significantly more effective.
Reference: 19. <author> Moshe Ben-Bassat. </author> <title> Use of distance measures, information measures and error bounds on feature evaluation. </title> <booktitle> In Krishnaiah and Kanal [198], </booktitle> <pages> pages 773-791. </pages>
Reference-contexts: DECISION TREE CONSTRUCTION: SURVEY 9 Methods used for selecting a good subset of features are typically quite different. We will postpone the discussion of feature subset selection methods to Section 5.1.1. Ben Bassat <ref> [19] </ref> divides feature evaluation rules into three categories: rules derived from information theory, rules derived from distance measures and rules derived from dependence measures. These categories are sometimes arbitrary and not distinct. Some measures belonging to different categories can be shown to be equivalent. <p> Rules derived from dependence measures: These measure the statistical dependence between two random variables. All dependence-based measures can be interpreted as belonging to one of the above two categories <ref> [19] </ref>. There exist many attribute selection criteria that do not clearly belong to any category in Ben Bassat's taxonomy. Gleser and Collen [126] and Talmon [351] used a combination of mutual information and O 2 measures. They first measured the 10 SREERAMA K.
Reference: 20. <editor> K.P. Bennett and O.L. Mangasarian. </editor> <title> Robust linear programming discrimination of two linearly inseparable sets. </title> <journal> Optimization Methods and Software, </journal> <volume> 1 </volume> <pages> 23-34, </pages> <year> 1992. </year>
Reference-contexts: More recently, Mangasarian and Bennett used linear and quadratic programming techniques to build machine learning systems in general and decision trees in particular <ref> [232, 22, 20, 230, 21] </ref>. Use of zero-one integer programming for designing vector quantizers can be found in [217]. Brown and Pittard [37] also employed linear programming for finding optimal multivariate splits at classification tree nodes.
Reference: 21. <editor> K.P. Bennett and O.L. Mangasarian. </editor> <title> Multicategory discrimination via linear programming. </title> <journal> Optimization Methods and Software, </journal> <volume> 3 </volume> <pages> 29-39, </pages> <year> 1994. </year>
Reference-contexts: More recently, Mangasarian and Bennett used linear and quadratic programming techniques to build machine learning systems in general and decision trees in particular <ref> [232, 22, 20, 230, 21] </ref>. Use of zero-one integer programming for designing vector quantizers can be found in [217]. Brown and Pittard [37] also employed linear programming for finding optimal multivariate splits at classification tree nodes.
Reference: 22. <author> Kristin P. Bennett. </author> <title> Decision tree construction via linear programming. </title> <booktitle> In Proc. of the 4th Midwest Artificial Intelligence and Cognitive Science Society Conf., </booktitle> <pages> pages 97-101, </pages> <year> 1992. </year>
Reference-contexts: More recently, Mangasarian and Bennett used linear and quadratic programming techniques to build machine learning systems in general and decision trees in particular <ref> [232, 22, 20, 230, 21] </ref>. Use of zero-one integer programming for designing vector quantizers can be found in [217]. Brown and Pittard [37] also employed linear programming for finding optimal multivariate splits at classification tree nodes.
Reference: 23. <author> Kristin P. Bennett. </author> <title> Global tree optimization: A non-greedy decision tree algorithm. </title> <booktitle> In Proc. of Interface 94: The 26th Symposium on the Interface, </booktitle> <institution> Research Triangle, North Carolina, </institution> <year> 1994. </year>
Reference-contexts: In the first stage, a sufficient partitioning is induced using any reasonably good (greedy) method. In the second stage, the tree is refined to be as close to optimal as possible. Refinement techniques attempted include dynamic programming [241], fuzzy logic search [373] and multi-linear programming <ref> [23] </ref>. The build-and-refine strategy can be seen as a search through the space of all possible decision trees, starting at the greedily built suboptimal tree.
Reference: 24. <author> A. Blum and R. Rivest. </author> <title> Training a 3-node neural network is NP-complete. </title> <booktitle> In Proc. of the 1988 Workshop on Computational Learning Theory, </booktitle> <pages> pages 9-18, </pages> <address> Boston, MA, 1988. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: However, one might hope that, by reducing the size of the decision tree, or the dimensionality of the data, it might be possible to make the problem tractable. This does not seem to be the case either . Blum and Rivest <ref> [24] </ref> showed that the problem of constructing an optimal 3-node neural network is NP-complete. Goodrich [130] proved that optimal (smallest) linear decision tree construction is NP-complete even in three dimensions. 6.2.
Reference: 25. <author> Marko Bohanec and Ivan Bratko. </author> <title> Trading accuracy for simplicity in decision trees. </title> <journal> Machine Learning, </journal> <volume> 15 </volume> <pages> 223-250, </pages> <year> 1994. </year>
Reference-contexts: Another pruning method that is based on viewing the decision tree as an encoding for the training data was suggested by Forsyth et al. [112]. Use of dynamic programming to prune trees optimally and efficiently has been explored in <ref> [25] </ref>. A few studies have been done to study the relative effectiveness of pruning methods [247, 62, 91]. Just as in the case of splitting criteria, no single ad hoc pruning method has been adjudged to be superior to the others.
Reference: 26. <author> David Bowser-Chao and Debra L. Dzialo. </author> <title> Comparison of the use of binary decision trees and neural networks in top quark detection. Physical Review D: Particles and Fields, </title> <address> 47(5):1900, </address> <month> March </month> <year> 1993. </year>
Reference-contexts: Recent use of decision trees for analyzing amino acid sequences can be found in [338] and [322]. * Pharmacology: Use of tree based classification for drug analysis can be found in [71]. * Physics: For the detection of physical particles <ref> [26] </ref>. * Plant diseases: To assess the hazard of mortality to pine trees [16]. * Power systems: For power system security assessment [144] and power sta bility prediction [317]. * Remote Sensing: Remote sensing has been a strong application area for pattern recognition work on decision trees (see [350, 182] ).
Reference: 27. <author> D. Boyce, A. Farhi, and R. Weishedel. </author> <title> Optimal Subset Selection. </title> <publisher> Springer-Verlag, </publisher> <year> 1974. </year>
Reference-contexts: If the training sample is too large to allow for efficient classifier induction, a subsample selection method (Section 5.1.3) can be employed. 5.1.1. Feature subset selection There is a large body of work on choosing relevant subsets of features (see the texts <ref> [84, 27, 245] </ref>). Much of this work was not developed in the context of tree induction, but a lot of it has direct applicability. There are two components to any method that attempts to choose the best subset of features.
Reference: 28. <author> Anna Bramanti-Gregor and Henry W. Davis. </author> <title> The statistical learning of accurate heuristics. </title> <booktitle> In IJCAI-93 [160], </booktitle> <pages> pages 1079-1085. </pages> <editor> Editor: </editor> <publisher> Ruzena Bajcsy. </publisher>
Reference-contexts: Friedman's [113] tree induction method could consider with equal ease atomic and composite features. Techniques to search for multivariate splits (Section 3.2) can be seen as ways for constructing composite features. Use of linear regression to find good feature combinations has been explored recently in <ref> [28] </ref>. Discovery of good combinations of Boolean features to be used as tests at tree nodes is explored in the machine learning literature in [284] as well as in signal processing [14].
Reference: 29. <author> Y. Brandman, A. Orlitsky, and J. Hennessy. </author> <title> A spectral lower bound technique for the size of decision trees and two-level AND/OR circuits. </title> <journal> IEEE Trans. on Comp., </journal> <volume> 39(2) </volume> <pages> 282-286, </pages> <month> February </month> <year> 1990. </year>
Reference-contexts: Chou and Gray [58] view decision trees as variable-length encoder-decoder pairs, and show that rate is equivalent to tree depth while distortion is the probability of misclassification. Brandman et al. <ref> [29] </ref> suggested a universal technique to lower bound the size and other characteristics of decision trees for arbitrary Boolean functions. This technique is based on the power spectrum coefficients of the n-dimensional Fourier transform of the function.
Reference: 30. <author> Leo Breiman. </author> <title> Bagging predictors. </title> <type> Technical report, </type> <institution> Department of Statistics, Univ. of California, Berkeley, </institution> <address> CA, </address> <year> 1994. </year>
Reference-contexts: Many authors have suggested using a collection of decision trees, instead of just one, to reduce the variance in classification performance <ref> [207, 339, 340, 44, 146, 30] </ref>. The idea is to build a set of (correlated or uncorrelated) trees for the same training sample, and then combine their results. 16 Multiple trees have been built using randomness [146] or using different subsets of attributes for each tree [339, 340].
Reference: 31. <author> Leo Breiman, Jerome Friedman, Richard Olshen, and Charles Stone. </author> <title> Classification and Regression Trees. </title> <publisher> Wadsworth Int. Group, </publisher> <year> 1984. </year> <note> 34 SREERAMA K. MURTHY </note>
Reference-contexts: A high level comparative perspective on the classification literature in pattern recognition and artificial intelligence can be found in [53]. Tree induction from a statistical perspective, as it is popularly used today, is reviewed in Breiman et al.'s excellent book Classification and Regression Trees <ref> [31] </ref>. For a review of earlier statistical work on hierarchical classification, see [103]. A majority of work on decision trees in machine learning is an offshoot of Breiman et al.'s work and Quinlan's ID3 algorithm [301]. <p> Work on hand-constructed decision trees (common in medicine) is also not considered. We do not discuss regression trees. There is a rich body of literature on this topic which shares many issues with the decision tree literature. For an introduction, see <ref> [31, 55] </ref>. We do not discuss binary decision diagrams and decision graphs [188]. We do not discuss patents. 6 3. Finding splits To build a decision tree, it is necessary to find at each internal node a test for splitting the data into subsets. <p> The feature evaluation criteria in this class measure separability, divergence or discrimination between classes. A popular distance measure is the Gini index of diversity 9 , which has been used for tree construction in statistics <ref> [31] </ref>, pattern recognition [119] and sequential fault diagnosis [291]. Breiman et al. pointed out that the Gini index has difficulty when there are a relatively large number of classes, and suggested the twoing rule [31] as a remedy. <p> the Gini index of diversity 9 , which has been used for tree construction in statistics <ref> [31] </ref>, pattern recognition [119] and sequential fault diagnosis [291]. Breiman et al. pointed out that the Gini index has difficulty when there are a relatively large number of classes, and suggested the twoing rule [31] as a remedy. Taylor and Silverman [355] pointed out that the Gini index emphasizes equal sized offspring and purity of both children. They suggested a splitting criterion, called mean posterior improvement (MPI), that emphasizes exclusivity between offspring class subsets instead. <p> Two examples are Heath's sum minority [147] and Lubinsky's inaccuracy [223, 224]. The CART book <ref> [31] </ref>, among others, discuss why this is not a good measure for tree induction. Additional tricks are needed to make this measure useful [223, 269]. <p> The conclusions are that no feature selection rule is consistently superior to the others, and that no specific strategy for alternating different rules seems to be significantly more effective. Breiman et al. <ref> [31] </ref> conjectured that decision tree design is rather insensitive to any one from a large class of splitting rules, and it is the stopping rule that is crucial. Mingers [248] compared several attribute selection criteria, and concluded that tree quality doesn't seem to depend on the specific criterion used. <p> He showed that Q and O do not chose non-essential variables at tree nodes, and that they produce trees that are 1/4th the size of the trees produced by loss. Fayyad and Irani [98] showed that their measure C-SEP, performs better than Gini index <ref> [31] </ref> and information gain [301] for specific types of problems. Several researchers [141, 301] pointed out that information gain is biased towards attributes with a large number of possible values. Mingers [246] compared information gain and the O 2 statistic for growing the tree as well as for stop-splitting. <p> For moderate sized problems, the critical issues are generalization accuracy, honest error rate estimation and gaining insight into the predictive and generalization structure of the data. For very large tree classifiers, the critical issue is optimizing structural properties such as height and balance [372, 50]. Breiman et al. <ref> [31] </ref> pointed out that tree quality depends more on good stopping rules than on splitting rules. Effects of noise on generalization are discussed in [275, 186]. Overfitting avoidance as a specific bias is studied in [383, 326]. <p> Every irreducible tree is optimal with respect to some expected testing cost criterion, and the tree reduction algorithm has the same worst-case complexity as most greedy tree induction methods. 4.1. Pruning Pruning, the method most widely used for obtaining right sized trees, was proposed by Breiman et al. ( <ref> [31] </ref>, Chapter 3). They suggested the following procedure: build the complete tree (a tree in which splitting no leaf node further will improve the accuracy on the training data) and then remove subtrees that are not contributing significantly towards generalization accuracy. <p> Kim and Koehler [183] analytically investigate the conditions under which pruning is beneficial for accuracy. Their main result states that pruning is more beneficial with increasing skewness in class distribution and/or increasing sample size. Breiman et al.'s pruning method <ref> [31] </ref> cost complexity pruning (a.k.a. weakest link pruning or error complexity pruning) proceeds in two stages. In the first stage, a sequence of increasingly smaller trees are built on the training data. <p> The requirement for an independent pruning set might be problematic especially when small training samples are involved. Several solutions have been suggested to get around this problem. Breiman et al. <ref> [31] </ref> describe a cross validation procedure that avoids reserving part of training data for pruning, but has a large computa DECISION TREE CONSTRUCTION: SURVEY 17 tional complexity. Quinlan's pessimistic pruning [302, 306] does away with the need for a separate pruning set by using a statistical correlation test. <p> Several attempts have been made to make tree construction cost-sensitive. These involve incorporating attribute measurement costs (machine learning: [278, 279, 354, 360], pattern recognition: [77, 261], statistics: [184]) and incorporating misclassification costs <ref> [31, 66, 83, 51, 360] </ref>. Methods to incorporate attribute measurement costs typically include a cost term into the feature evaluation criterion, whereas variable misclassification costs are accounted for by using prior probabilities or cost matrices. 20 SREERAMA K. MURTHY 5.3. <p> The classification of an object with missing attribute values will be the largest represented class in the union of all the leaf nodes at which the object ends up. Breiman et al.'s CART system <ref> [31] </ref> more or less implemented Friedman's suggestions. Quinlan [304] also considered the problem of missing attribute values. 5.4. Improving upon greedy induction Most tree induction systems use a greedy approach | trees are induced top-down, a node at a time. <p> They suggested an information based metric to evaluate a classifier, as a remedy to the above problems. Martin [234] argued that information theoretic measures of classifier complexity are not practically computable except within severely restricted families of classifiers, and suggested a generalized version of CART's <ref> [31] </ref> 1-standard error rule as a means of achieving a tradeoff between classifier complexity and accuracy. <p> A consequence of this result is that top-down tree induction (using mutual information) is necessarily suboptimal in terms of average tree depth. Trees of maximal size generated by the CART algorithm <ref> [31] </ref> have been shown to have an error rate bounded by twice the Bayes error rate, and to be asymptotically Bayes optimal [131]. <p> This conjecture is substantiated empirically in [270], where it is shown that the expected depth of DECISION TREE CONSTRUCTION: SURVEY 27 trees greedily induced using information gain [301] and Gini index <ref> [31] </ref> is very close to that of the optimal, under a variety of experimental conditions. Relationship between feature evaluation by Shannon's entropy and the probability of error is investigated in [196, 312]. 7. <p> The performance of decision trees improved in their study when multivariate splits were used, and back-propagation networks did better with feature selection. Comparisons of symbolic and connectionist methods can also be found in [379, 337]. Multi-layer perceptrons and CART <ref> [31] </ref> with and without linear combinations are compared in [12] to find that there is not much difference in accuracy. Similar conclusions were reached in [106] when ID3 [301] and back-propagation were compared. <p> In contrast, ID3 is adjudged to be slightly better than connectionist and Bayesian methods in [347]. Giplin et al. [125] compared stepwise linear discriminant analysis, stepwise logistic regression and CART <ref> [31] </ref> to three senior cardiologists, for predicting whether a patient would die within a year of being discharged after an acute myocardial in 30 SREERAMA K. MURTHY farction. Their results showed that there was no difference between the physicians and the computers, in terms of the prediction accuracy.
Reference: 32. <author> Richard P. Brent. </author> <title> Fast training algorithms for multilayer neural nets. </title> <journal> IEEE Trans. on Neural Networks, </journal> <volume> 2(3) </volume> <pages> 346-354, </pages> <month> May </month> <year> 1991. </year>
Reference-contexts: Techniques very similar to those used in tree construction, such as information theoretic splitting criteria and pruning, can be found in neural tree construction also. Examples of this work include <ref> [127, 342, 32, 59, 150, 324, 72] </ref>. Sethi [331] described a method for converting a univariate decision tree into a neural net and then retraining it, resulting in tree structured entropy nets with sigmoidal splits.
Reference: 33. <author> Leonard A. Breslow and David W. Aha. </author> <title> Simplifying decision trees: A survey. </title> <type> Technical Report AIC-96-014, </type> <institution> Navy Center for Applied Research in Artificial Intelligence, Naval Research Lab., </institution> <address> Washington DC 20375, </address> <year> 1996. </year> <note> breslow, aha.aic.nrl.navy.mil. </note>
Reference-contexts: Quinlan [308] recently discussed improved ways of using continuous attributes with C4.5. 4. Obtaining the right sized trees See Breslow and Aha's recent survey <ref> [33] </ref> on simplifying decision trees for a detailed account of the motivation for tree simplification and existing solution approaches. One of the main difficulties of inducing a recursive partitioning structure is knowing when to stop.
Reference: 34. <author> Carla E. Brodley. </author> <title> Recursive Automatic Algorithm Selection for Inductive Learning. </title> <type> PhD thesis, </type> <institution> Univ. of Massachusetts, </institution> <address> Amherst, MA, </address> <year> 1994. </year>
Reference-contexts: The relationship between the correlation of errors of individual classifiers and the error of the combined classifier has been explored [9]. An alternative to multiple trees is a hybrid classifier that uses several small classifiers as parts of a larger classifier. Brodley <ref> [34] </ref> describes a system that automatically 22 SREERAMA K. MURTHY selects the most suitable among a univariate decision tree, a linear discriminant and an instance based classifier at each node of a hierarchical, recursive classifier. 5.7.
Reference: 35. <author> Carla E. Brodley and Paul E. Utgoff. </author> <title> Multivariate decision trees. </title> <journal> Machine Learning, </journal> <volume> 19 </volume> <pages> 45-77, </pages> <year> 1995. </year>
Reference-contexts: John [167] recently considered linear discriminant trees in the machine learning literature. An extension of linear discriminants are linear machines [276], which are linear structures that can discriminate between multiple classes. In the machine learning literature, Utgoff et al. explored decision trees that used linear machines at internal nodes <ref> [35, 83] </ref>. Locally Opposed Clusters of Objects: Sklansky and his students developed several piecewise linear discriminants based on the principle of locally opposed clusters of objects. Wassel and Sklansky [374, 344] suggested a procedure to train a linear split to minimize the error probability.
Reference: 36. <author> Donald E. Brown, Vincent Corruble, and Clarence Louis Pittard. </author> <title> A comparison of decision tree classifiers with backpropagation neural networks for multimodal classification problems. </title> <journal> Pattern Recognition, </journal> <volume> 26(6) </volume> <pages> 953-961, </pages> <year> 1993. </year>
Reference-contexts: Trees versus other data analysis methods This section, like Section 7.1 above, is not comprehensive but merely illustrative. We briskly provide pointers to work that has compared decision trees against competing techniques for data analysis in statistics and machine learning. Brown et al. <ref> [36] </ref> compared back-propagation neural networks with decision trees on three problems that are known to be multi-modal. Their analysis indicated that there was not much difference between both methods, and that neither method performed very well in its "vanilla" state.
Reference: 37. <author> Donald E. Brown and Clarence Louis Pittard. </author> <title> Classification trees with optimal multivariate splits. </title> <booktitle> In Proc. of the Int. Conf. on Systems, Man and Cybernetics, </booktitle> <volume> volume 3, </volume> <pages> pages 475-477, </pages> <address> Le Touquet, France, 17-20th, </address> <month> October </month> <year> 1993. </year> <booktitle> IEEE, </booktitle> <address> New York. </address>
Reference-contexts: More recently, Mangasarian and Bennett used linear and quadratic programming techniques to build machine learning systems in general and decision trees in particular [232, 22, 20, 230, 21]. Use of zero-one integer programming for designing vector quantizers can be found in [217]. Brown and Pittard <ref> [37] </ref> also employed linear programming for finding optimal multivariate splits at classification tree nodes. Almost all the above papers attempt to minimize the distance of the misclassified points from the decision boundary.
Reference: 38. <author> R.S. Bucy and R.S. Diesposti. </author> <title> Decision tree design by simulated annealing. </title> <journal> Mathematical Modieling and Numerical Analysis, </journal> <volume> 27(5) </volume> <pages> 515-534, </pages> <year> 1993. </year> <note> A RAIRO J. </note>
Reference-contexts: The build-and-refine strategy can be seen as a search through the space of all possible decision trees, starting at the greedily built suboptimal tree. In order to escape local minima in the search space, randomized search techniques, such as genetic programming [197] and simulated annealing <ref> [38, 228] </ref>, have been attempted. These methods search the space of all decision trees using random perturbations, additions and deletions of the splits. A deterministic hill-climbing search procedure DECISION TREE CONSTRUCTION: SURVEY 21 has also been suggested for searching for optimal trees, in the context of sequential fault diagnosis [349].
Reference: 39. <author> M. E. Bullock, D. L. Wang, Fairchild S. R., and T. J. Patterson. </author> <title> Automated training of 3-D morphology algorithm for object recognition. </title> <booktitle> Proc. of SPIE The Int. Society for Optical Eng., </booktitle> <volume> 2234 </volume> <pages> 238-251, </pages> <year> 1994. </year> <title> Issue title: Automatic Object Recognition IV. </title>
Reference-contexts: MURTHY * Image processing: For the interpretation of digital images in radiology [294], for recognizing 3-D objects <ref> [39] </ref>, for high level vision [187] and outdoor image segmentation [40]. * Language processing: For medical text classification [212], for acquiring a statistical parser from a set of parsed sentences [229]. * Law: For discovering knowledge in international conflict and conflict management databases, for the possible avoidance and termination of crises
Reference: 40. <author> Shashi D. Buluswer and Bruce A. Draper. </author> <title> Non-parametric classification of pixels under varying illumination. SPIE: </title> <journal> The Int. Society for Optical Eng., </journal> <volume> 2353 </volume> <pages> 529-536, </pages> <month> November </month> <year> 1994. </year>
Reference-contexts: MURTHY * Image processing: For the interpretation of digital images in radiology [294], for recognizing 3-D objects [39], for high level vision [187] and outdoor image segmentation <ref> [40] </ref>. * Language processing: For medical text classification [212], for acquiring a statistical parser from a set of parsed sentences [229]. * Law: For discovering knowledge in international conflict and conflict management databases, for the possible avoidance and termination of crises and wars [116]. * Manufacturing and Production: To non-destructively test
Reference: 41. <author> W. Buntine and T. Niblett. </author> <title> A further comparison of splitting rules for decision-tree induction. </title> <journal> Machine Learning, </journal> <volume> 8 </volume> <pages> 75-85, </pages> <year> 1992. </year>
Reference-contexts: Mingers [248] compared several attribute selection criteria, and concluded that tree quality doesn't seem to depend on the specific criterion used. He even claimed that random attribute selection criteria are as good as measures like information gain [301]. This later claim was refuted in <ref> [41, 219] </ref>, where the authors argued that random attribute selection criteria are prone to overfitting, and also fail when there are several noisy attributes. Miyakawa [252] compared three activity-based measures, Q, O and loss, both analytically and empirically.
Reference: 42. <author> W. L. Buntine. </author> <title> Decision tree induction systems: a Bayesian analysis. </title> <editor> In L. N. Kanal, T. S. Levitt, and J. F. Lemmer, editors, </editor> <booktitle> Uncertainty in Artificial Intelligence 3. </booktitle> <publisher> Elsevier Science Publishers, </publisher> <address> Amsterdam, </address> <year> 1989. </year>
Reference-contexts: There are an increasing number of papers on the former topic, although the similarities with tree induction are usually not pointed out. For a good discussion of decision tree induction from a Bayesian networks point of view, see <ref> [42] </ref>. For a good introduction to the literature on learning Bayesian networks, see [45].
Reference: 43. <author> Wray Buntine. </author> <title> A theory of learning classification rules. </title> <type> PhD thesis, </type> <institution> Univ. of Technology, </institution> <address> Sydney, Australia, </address> <year> 1991. </year>
Reference-contexts: Chou [56] considered decision trellises, where trellises are directed acyclic graphs with class probability vectors at the leaves and tests at internal nodes. Option trees, in which every internal node holds several optional tests along with their respective subtrees, are discussed in <ref> [43, 44] </ref>. Oliver [281] suggested a method to build decision graphs, which are similar to Chou's decision trellises, using minimum length encoding principles [370]. Rymon [318] suggested SE-trees, set enumeration structures each of which can embed several decision trees.
Reference: 44. <author> Wray Buntine. </author> <title> Learning classification trees. </title> <journal> Statistics and Computing, </journal> <volume> 2 </volume> <pages> 63-73, </pages> <year> 1992. </year>
Reference-contexts: It is not surprising that most existing splitting rules are functionally equivalent. The author acknowledges a shortcoming of this organization. Papers dealing with more than one topic are either listed multiple times or their mention is omitted from DECISION TREE CONSTRUCTION: SURVEY 5 some places. A good example is <ref> [44] </ref> which has relevance to many of the issues we address, and is referenced repeatedly under Sections 5.4,5.5,5.6 and 5.10. The next section (1.2) introduces briefly the basic concepts involved in decision tree construction. Section 1.3 discusses alternative terminology. <p> For early work using dynamic programming and branch-and-bound techniques to convert decision tables to optimal trees, see [259]. Tree construction using partial or exhaustive lookahead has been considered in statistics [103, 57, 88], in pattern recognition [142], for tree structured vector quan-tizers [315], for Bayesian class probability trees <ref> [44] </ref>, for neural trees [72] and in machine learning [278, 310, 271]. Most of these studies indicate that lookahead does not cause considerable improvements over greedy induction. <p> On the contrary, class probability trees assign a probability distribution for all classes at the terminal nodes. Breiman et al. ([31], Chapter 4) proposed a method for building class probability trees. Quinlan [305] discussed methods of extracting probabilities from decision trees. Buntine <ref> [44] </ref> described Bayesian methods for building, smoothing and averaging class probability trees. (Smoothing is the process of adjusting probabilities at a node in the tree based on the probabilities at other nodes on the same path. <p> Many authors have suggested using a collection of decision trees, instead of just one, to reduce the variance in classification performance <ref> [207, 339, 340, 44, 146, 30] </ref>. The idea is to build a set of (correlated or uncorrelated) trees for the same training sample, and then combine their results. 16 Multiple trees have been built using randomness [146] or using different subsets of attributes for each tree [339, 340]. <p> Chou [56] considered decision trellises, where trellises are directed acyclic graphs with class probability vectors at the leaves and tests at internal nodes. Option trees, in which every internal node holds several optional tests along with their respective subtrees, are discussed in <ref> [43, 44] </ref>. Oliver [281] suggested a method to build decision graphs, which are similar to Chou's decision trellises, using minimum length encoding principles [370]. Rymon [318] suggested SE-trees, set enumeration structures each of which can embed several decision trees.
Reference: 45. <author> Wray Buntine. </author> <title> A guide to the literature on learning probabilistic networks from data. </title> <journal> IEEE Trans. on Knowledge and Data Engineering, </journal> <year> 1996. </year>
Reference-contexts: For a good discussion of decision tree induction from a Bayesian networks point of view, see [42]. For a good introduction to the literature on learning Bayesian networks, see <ref> [45] </ref>. Work on automatic construction of hierarchical structures from data in which the dependent variable is unknown (unsupervised learning), present in fields such as cluster analysis [93], machine learning (e.g., [105, 121]) and vector quantization [122] is not covered.
Reference: 46. <author> Janice D. Callahan and Stephen W. Sorensen. </author> <title> Rule induction for group decisions with statistical data an example. </title> <journal> J. of the Operational Research Society, </journal> <volume> 42(3) </volume> <pages> 227-234, </pages> <month> March </month> <year> 1991. </year>
Reference-contexts: Their comparisons show that decision tree classifiers are more comprehensible and flexible to incorporate or change existing categories. Comparisons of CART to multiple linear regression and discriminant analysis can be found in <ref> [46] </ref> where it is argued that CART is more suitable than the other methods for very noisy domains with lots of missing values.
Reference: 47. <author> Jan M. Van Campenhout. </author> <title> Topics in measurement selection. </title> <booktitle> In Krishnaiah and Kanal [198], </booktitle> <pages> pages 793-803. </pages>
Reference-contexts: Efron [87] showed that, although cross validation closely approximates the true result, bootstrap has much less variance, especially for small samples. However, there exist arguments that cross validation is clearly preferable to bootstrap in practice [190]. 15. Van Campenhout <ref> [47] </ref> argues that increasing the amount of information in a measurement subset through enlarging its size or complexity never worsens the error probability of a truly Bayesian classifier.
Reference: 48. <author> Rich Caruana and Dayne Freitag. </author> <title> Greedy attribute selection. </title> <booktitle> In ML-94 [254], </booktitle> <pages> pages 28-36. </pages> <editor> Editors: William W. </editor> <booktitle> Cohen and Haym Hirsh. </booktitle>
Reference-contexts: In stepwise backward elimination, we start with the full feature set and remove, at each step, the worst feature. When more than one feature is greedily added or removed, beam search is said to have been performed <ref> [341, 48] </ref>. A combination of forward selection and backward elimination, a bidirectional search, was attempted in [341]. <p> There has been a recent surge of interest in feature subset selection methods in the machine learning community, resulting in several empirical evaluations. These studies provide interesting insights on how DECISION TREE CONSTRUCTION: SURVEY 19 to increase the efficiency and effectiveness of the heuristic search for good feature subsets <ref> [185, 210, 48, 81, 257, 5] </ref>. 5.1.2. Composite features Sometimes the aim is not to choose a good subset of features, but instead to find a few good "composite" features, which are arithmetic or logical combinations of the atomic features.
Reference: 49. <author> Richard G. Casey and George Nagy. </author> <title> Decision tree design using a probabilistic model. </title> <journal> IEEE Trans. on Information Theory, </journal> <volume> IT-30(1):93-99, </volume> <month> January </month> <year> 1984. </year>
Reference-contexts: i.e., by expanding tree nodes that contribute to the largest gain in average mutual information of the whole tree, is explored in pattern recognition [126, 333, 351]. 8 Tree construction by locally optimizing information gain, the reduction in entropy due to splitting each individual node, is explored in pattern recognition <ref> [142, 372, 49, 139] </ref>, in sequential fault diagnosis [368] and in machine learning [301]. Mingers [246] suggested the G-statistic, an information theoretic measure that is a close approximation to O 2 distribution, for tree construction as well as for deciding when to stop.
Reference: 50. <author> Jason Catlett. </author> <title> Megainduction. </title> <type> PhD thesis, </type> <institution> Basser Department of Computer Science, Univ. of Sydney, Australia, </institution> <year> 1991. </year>
Reference-contexts: For moderate sized problems, the critical issues are generalization accuracy, honest error rate estimation and gaining insight into the predictive and generalization structure of the data. For very large tree classifiers, the critical issue is optimizing structural properties such as height and balance <ref> [372, 50] </ref>. Breiman et al. [31] pointed out that tree quality depends more on good stopping rules than on splitting rules. Effects of noise on generalization are discussed in [275, 186]. Overfitting avoidance as a specific bias is studied in [383, 326].
Reference: 51. <author> Jason Catlett. </author> <title> Tailoring rulesets to misclassification costs. </title> <booktitle> In AI&Statistics-95 [7], </booktitle> <pages> pages 88-94. </pages>
Reference-contexts: Several attempts have been made to make tree construction cost-sensitive. These involve incorporating attribute measurement costs (machine learning: [278, 279, 354, 360], pattern recognition: [77, 261], statistics: [184]) and incorporating misclassification costs <ref> [31, 66, 83, 51, 360] </ref>. Methods to incorporate attribute measurement costs typically include a cost term into the feature evaluation criterion, whereas variable misclassification costs are accounted for by using prior probabilities or cost matrices. 20 SREERAMA K. MURTHY 5.3.
Reference: 52. <author> Bing-Bing Chai, Xinhua Zhuang, Yunxin Zhao, and Jack Sklansky. </author> <title> Binary linear decision tree with genetic algorithm. </title> <booktitle> In Proc. of the 13th Int. Conf. on Pattern Recognition 4. </booktitle> <publisher> IEEE Computer Society Press, Los Alamitos, </publisher> <address> CA, </address> <year> 1996. </year>
Reference-contexts: In information theory, Gelfand and Ravishanker [118] describe a method to build a tree structured filter that has linear processing elements at internal nodes. Heath et al. [147, 145] used simulated annealing to find the best oblique split at each tree node. Chai et al. <ref> [52] </ref> recently suggested using genetic algorithms to search for linear splits at non-terminal nodes in a tree. Lubinsky [225, 224] attempted bivariate trees, trees in which some functions of two variables can be used as tests at internal nodes.
Reference: 53. <author> B. Chandrasekaran. </author> <title> From numbers to symbols to knowledge structures: </title> <booktitle> Pattern Recognition and Artificial Intelligence perspectives on the classification task. </booktitle> <volume> volume 2, </volume> <pages> pages 547-559. </pages> <publisher> Elsevier Science, </publisher> <address> Amsterdam, The Netherlands, </address> <year> 1986. </year>
Reference-contexts: Treatises and surveys An overview of work on decision trees in the pattern recognition literature can be found in [76]. A high level comparative perspective on the classification literature in pattern recognition and artificial intelligence can be found in <ref> [53] </ref>. Tree induction from a statistical perspective, as it is popularly used today, is reviewed in Breiman et al.'s excellent book Classification and Regression Trees [31]. For a review of earlier statistical work on hierarchical classification, see [103].
Reference: 54. <author> B. Chandrasekaran and A. K. Jain. </author> <title> Quantization complexity and independent measurements. </title> <journal> IEEE Trans. on Comp., </journal> <volume> C-23(1):102-106, </volume> <month> January </month> <year> 1974. </year>
Reference-contexts: This section bundles together several such issues. 5.1. Sample size versus dimensionality The relationship between the size of the training set and the dimensionality of the problem is studied extensively in the pattern recognition literature <ref> [153, 175, 108, 54, 173, 202, 166, 114] </ref>. Researchers considered the problem of how sample size should vary according to dimensionality and vice versa.
Reference: 55. <author> P. Chaudhuri, W. D. Lo, W. Y. Loh, and C. C. Yang. </author> <title> Generalized regression trees. </title> <journal> Statistica Sinica, </journal> <volume> 5(2) </volume> <pages> 641-666, </pages> <year> 1995. </year> <title> DECISION TREE CONSTRUCTION: SURVEY 35 </title>
Reference-contexts: Work on hand-constructed decision trees (common in medicine) is also not considered. We do not discuss regression trees. There is a rich body of literature on this topic which shares many issues with the decision tree literature. For an introduction, see <ref> [31, 55] </ref>. We do not discuss binary decision diagrams and decision graphs [188]. We do not discuss patents. 6 3. Finding splits To build a decision tree, it is necessary to find at each internal node a test for splitting the data into subsets.
Reference: 56. <author> Philip A. Chou. </author> <title> Applications of Information Theory to Pattern Recognition and the Design of Decision Trees and Trellises. </title> <type> PhD thesis, </type> <institution> Stanford Univ., </institution> <year> 1988. </year>
Reference-contexts: Given a trained network or any other learned model, Craven's algorithm TREPAN [68] uses queries to induce a decision tree that approximates the function represented by the model. Several attempts at generalizing the decision tree representation exist. Chou <ref> [56] </ref> considered decision trellises, where trellises are directed acyclic graphs with class probability vectors at the leaves and tests at internal nodes. Option trees, in which every internal node holds several optional tests along with their respective subtrees, are discussed in [43, 44].
Reference: 57. <author> Philip A. Chou. </author> <title> Optimal partitioning for classification and regression trees. </title> <journal> IEEE Trans. on Pattern Analysis and Machine Intelligence, </journal> <volume> 13(4) </volume> <pages> 340-354, </pages> <month> April </month> <year> 1991. </year>
Reference-contexts: This seems natural considering application domains such as spectral analysis and remote sensing [350]. In these fields, special techniques [332] were developed to accommodate discrete attributes into what were primarily algorithms for ordered attributes. Fast methods for splitting multiple valued categorical variables are described in <ref> [57] </ref>. In machine learning, a subfield of Artificial Intelligence, which in turn has been dominated by symbolic processing, many tree induction methods (e.g., [299] were originally developed for categorical attributes. The problem of incorporating continuous attributes into these algorithms is considered subsequently. <p> The problem of inducing globally optimal decision trees has been addressed time and again. For early work using dynamic programming and branch-and-bound techniques to convert decision tables to optimal trees, see [259]. Tree construction using partial or exhaustive lookahead has been considered in statistics <ref> [103, 57, 88] </ref>, in pattern recognition [142], for tree structured vector quan-tizers [315], for Bayesian class probability trees [44], for neural trees [72] and in machine learning [278, 310, 271]. Most of these studies indicate that lookahead does not cause considerable improvements over greedy induction.
Reference: 58. <author> Philip A. Chou and Robert M. Gray. </author> <title> On decision trees for pattern recognition. </title> <booktitle> In Proc. of the IEEE Symposium on Information Theory, </booktitle> <pages> page 69, </pages> <address> Ann Arbor, MI, </address> <year> 1986. </year>
Reference-contexts: Such insights provide valuable tools for analyzing decision trees. Wang and Suen [372] show that entropy-reduction point of view is powerful in theoretically bounding search depth and classification error. Chou and Gray <ref> [58] </ref> view decision trees as variable-length encoder-decoder pairs, and show that rate is equivalent to tree depth while distortion is the probability of misclassification. Brandman et al. [29] suggested a universal technique to lower bound the size and other characteristics of decision trees for arbitrary Boolean functions.
Reference: 59. <author> Krzysztof J. Cios and Ning Liu. </author> <title> A machine learning method for generation of a neural network architecture: A continuous ID3 algorithm. </title> <journal> IEEE Trans. on Neural Networks, </journal> <volume> 3(2) </volume> <pages> 280-291, </pages> <month> March </month> <year> 1992. </year>
Reference-contexts: Techniques very similar to those used in tree construction, such as information theoretic splitting criteria and pruning, can be found in neural tree construction also. Examples of this work include <ref> [127, 342, 32, 59, 150, 324, 72] </ref>. Sethi [331] described a method for converting a univariate decision tree into a neural net and then retraining it, resulting in tree structured entropy nets with sigmoidal splits.
Reference: 60. <author> I. Cleote and H. Theron. CID3: </author> <title> An extension of ID3 for attributes with ordered domains. </title> <journal> South African Computer J., </journal> <volume> 4 </volume> <pages> 10-16, </pages> <month> March </month> <year> 1991. </year>
Reference-contexts: Fast methods for splitting a continuous dimension into more than two ranges is considered in the machine learning literature [100, 115]. 12 An extension to ID3 [301] DECISION TREE CONSTRUCTION: SURVEY 15 that distinguishes between attributes with unordered domains and attributes with linearly ordered domains is suggested in <ref> [60] </ref>. Quinlan [308] recently discussed improved ways of using continuous attributes with C4.5. 4. Obtaining the right sized trees See Breslow and Aha's recent survey [33] on simplifying decision trees for a detailed account of the motivation for tree simplification and existing solution approaches.
Reference: 61. <author> J.R.B. Cockett and J.A. Herrera. </author> <title> Decision tree reduction. </title> <journal> J. of the ACM, </journal> <volume> 37(4) </volume> <pages> 815-842, </pages> <month> October </month> <year> 1990. </year>
Reference-contexts: Simple heuristics to generalize and combine the rules generated from trees can act as a substitute for pruning for Quinlan's univariate trees. * Tree reduction: Cockett and Herrera <ref> [61] </ref> suggested a method to reduce an arbitrary binary decision tree to an "irreducible" form, using discrete decision theory principles. Every irreducible tree is optimal with respect to some expected testing cost criterion, and the tree reduction algorithm has the same worst-case complexity as most greedy tree induction methods. 4.1.
Reference: 62. <author> W.W. Cohen. </author> <title> Efficient pruning methods for separate-and-conquer rule learning systems. </title> <booktitle> In IJCAI-93 [160], </booktitle> <pages> pages 988-994. </pages> <editor> Editor: </editor> <publisher> Ruzena Bajcsy. </publisher>
Reference-contexts: Use of dynamic programming to prune trees optimally and efficiently has been explored in [25]. A few studies have been done to study the relative effectiveness of pruning methods <ref> [247, 62, 91] </ref>. Just as in the case of splitting criteria, no single ad hoc pruning method has been adjudged to be superior to the others.
Reference: 63. <author> Douglas Comer and Ravi Sethi. </author> <title> The complexity of trie index construction. </title> <journal> J. of the ACM, </journal> <volume> 24(3) </volume> <pages> 428-440, </pages> <month> July </month> <year> 1977. </year>
Reference-contexts: All the measures considered by the earlier papers on NP-completeness appear to be a subset of Naumov's measures. The problem of constructing the smallest decision tree which best distinguishes characteristics of multiple distinct groups is shown to be NP-complete in [358]. Comer and Sethi <ref> [63] </ref> studied the asymptotic complexity of trie index construction in the document retrieval literature. Megiddo [240] investigated the problem of polyhedral separability (separating two sets of points using k hyper-planes), and proved that several variants of this problem are NP-complete.
Reference: 64. <author> T.M. Cover and J.M. Van Campenhout. </author> <title> On the possible orderings in the measurement selection problems. </title> <journal> IEEE Trans. on Systems, Man and Cybernetics, </journal> <volume> SMC-7(9), </volume> <year> 1977. </year>
Reference-contexts: A combination of forward selection and backward elimination, a bidirectional search, was attempted in [341]. Comparisons of heuristic feature subset selection methods resound the conclusions of studies comparing feature evaluation criteria and studies comparing pruning methods | no feature subset selection heuristic is far superior to the others. <ref> [64, 366] </ref> showed that heuristic sequential feature selection methods can do arbitrarily worse than the optimal strategy. Mucciardi and Gose [262] compared seven feature subset selection techniques empirically and concluded that no technique was uniformly superior to the others.
Reference: 65. <author> Louis Anthony Cox. </author> <title> Using causal knowledge to learn more useful decision rules from data. </title> <booktitle> In AI&Statistics-95 [7], </booktitle> <pages> pages 151-160. </pages>
Reference-contexts: Oliver [281] suggested a method to build decision graphs, which are similar to Chou's decision trellises, using minimum length encoding principles [370]. Rymon [318] suggested SE-trees, set enumeration structures each of which can embed several decision trees. Cox <ref> [65] </ref> argues that classification tree technology, as implemented in commercially available systems, is often more useful for pattern recognition than it is for 24 SREERAMA K. MURTHY decision support. He suggests several ways of modifying existing methods to be prescriptive rather than descriptive.
Reference: 66. <author> Louis Anthony Cox and Yuping Qiu. </author> <title> Minimizing the expected costs of classifying patterns by sequential costly inspections. </title> <booktitle> In AI&Statistics-93 [6]. </booktitle>
Reference-contexts: Several attempts have been made to make tree construction cost-sensitive. These involve incorporating attribute measurement costs (machine learning: [278, 279, 354, 360], pattern recognition: [77, 261], statistics: [184]) and incorporating misclassification costs <ref> [31, 66, 83, 51, 360] </ref>. Methods to incorporate attribute measurement costs typically include a cost term into the feature evaluation criterion, whereas variable misclassification costs are accounted for by using prior probabilities or cost matrices. 20 SREERAMA K. MURTHY 5.3.
Reference: 67. <author> Louis Anthony Cox, Yuping Qiu, and Warren Kuehner. </author> <title> Heuristic least-cost computation of discrete classification functions with uncertain argument values. </title> <journal> Annals of Operations Research, </journal> <volume> 21(1) </volume> <pages> 1-30, </pages> <year> 1989. </year>
Reference: 68. <author> Mark W. Craven. </author> <title> Extracting comprehensible models from trained neural networks. </title> <type> Technical Report CS-TR-96-1326, </type> <institution> University of Wisconsin, Madison, </institution> <month> September </month> <year> 1996. </year>
Reference-contexts: The problem of learning trees solely from prior probability distributions is considered in [11]. Learning decision trees from qualitative causal models acquired from domain experts is the topic of [295]. Given a trained network or any other learned model, Craven's algorithm TREPAN <ref> [68] </ref> uses queries to induce a decision tree that approximates the function represented by the model. Several attempts at generalizing the decision tree representation exist. Chou [56] considered decision trellises, where trellises are directed acyclic graphs with class probability vectors at the leaves and tests at internal nodes.
Reference: 69. <author> Stuart L. Crawford. </author> <title> Extensions to the CART algorithm. </title> <journal> Int. J. of Man-Machine Studies, </journal> <volume> 31(2) </volume> <pages> 197-217, </pages> <month> August </month> <year> 1989. </year>
Reference-contexts: Quinlan's pessimistic pruning [302, 306] does away with the need for a separate pruning set by using a statistical correlation test. Crawford <ref> [69] </ref> analyzed Breiman et al.'s cross validation procedure, and pointed out that it has a large variance, especially for small training samples. He suggested a .632 bootstrap method 14 as an effective alternative. <p> Crawford <ref> [69] </ref> argues that approaches which attempt to update the tree so that the "best" split according to the updated sample is taken at each node, suffer from repeated restructuring. This occurs because the best split at a node vacillates widely while the sample at the node is still small. <p> This occurs because the best split at a node vacillates widely while the sample at the node is still small. An incremental version of CART that uses significance thresholds to avoid the above problem is described in <ref> [69] </ref>. 5.8.
Reference: 70. <author> Stephen P. Curram and John Mingers. </author> <title> Neural networks, decision tree induction and discriminant analysis: An empirical comparison. </title> <journal> J. of the Operational Research Society, </journal> <volume> 45(4) </volume> <pages> 440-450, </pages> <month> April </month> <year> 1994. </year>
Reference-contexts: Long et al. [221] compared Quinlan's C4 [306] to logistic regression on the problem of diagnosing acute cardiac ischemia, and concluded that both methods came fairly close to the expertise of the physicians. In their experiments, logistic regression outperformed C4. Curram and Mingers <ref> [70] </ref> compare decision trees, neural networks and discriminant analysis on several real world data sets. Their comparisons reveal that linear discriminant analysis is the fastest of the methods, when the underlying assumptions are met, and that decision trees methods overfit in the presence of noise.
Reference: 71. <author> K.T. Dago, R. Luthringer, R. Lengelle, G. Rinaudo, and J. P. </author> <title> Matcher. Statistical decision tree: A tool for studying pharmaco-EEG effects of CNS-active drugs. </title> <journal> Neuropsychobiology, </journal> <volume> 29(2) </volume> <pages> 91-96, </pages> <year> 1994. </year>
Reference-contexts: Recent use of decision trees for analyzing amino acid sequences can be found in [338] and [322]. * Pharmacology: Use of tree based classification for drug analysis can be found in <ref> [71] </ref>. * Physics: For the detection of physical particles [26]. * Plant diseases: To assess the hazard of mortality to pine trees [16]. * Power systems: For power system security assessment [144] and power sta bility prediction [317]. * Remote Sensing: Remote sensing has been a strong application area for pattern
Reference: 72. <author> Florence DAlche-Buc, Didier Zwierski, and Jean-Pierre Nadal. </author> <title> Trio learning: A new strategy for building hybrid neural trees. </title> <journal> Int. J. of Neural Systems, </journal> <volume> 5(4) </volume> <pages> 259-274, </pages> <month> December </month> <year> 1994. </year>
Reference-contexts: Techniques very similar to those used in tree construction, such as information theoretic splitting criteria and pruning, can be found in neural tree construction also. Examples of this work include <ref> [127, 342, 32, 59, 150, 324, 72] </ref>. Sethi [331] described a method for converting a univariate decision tree into a neural net and then retraining it, resulting in tree structured entropy nets with sigmoidal splits. <p> Tree construction using partial or exhaustive lookahead has been considered in statistics [103, 57, 88], in pattern recognition [142], for tree structured vector quan-tizers [315], for Bayesian class probability trees [44], for neural trees <ref> [72] </ref> and in machine learning [278, 310, 271]. Most of these studies indicate that lookahead does not cause considerable improvements over greedy induction. Murthy and Salzberg [271] demonstrate that one-level lookahead does not help build significantly better trees and can actually worsen the quality of trees, causing pathology [273].
Reference: 73. <author> S.K. Das and S. Bhambri. </author> <title> A decision tree approach for selecting between demand based, reorder and JIT/kanban methods for material procurement. Production Planning and Control, </title> <address> 5(4):342, </address> <year> 1994. </year>
Reference-contexts: of parsed sentences [229]. * Law: For discovering knowledge in international conflict and conflict management databases, for the possible avoidance and termination of crises and wars [116]. * Manufacturing and Production: To non-destructively test welding quality [90], for semiconductor manufacturing [163], for increasing productivity [179], for material procurement method selection <ref> [73] </ref>, to accelerate rotogravure printing [92], for process optimization in electro-chemical machining [95], to schedule printed circuit board assembly lines [296], to uncover flaws in a Boeing manufacturing process [313] and for quality control [135].
Reference: 74. <author> Belur V. Dasarathy, </author> <title> editor. Nearest neighbor (NN) norms: NN pattern classification techniques. </title> <publisher> IEEE Computer Society Press, Los Alamitos, </publisher> <address> CA, </address> <year> 1991. </year>
Reference-contexts: Quinlan suggested "windowing", a random training set sampling method, for his programs ID3 and C4.5 [306, 382]. A initially randomly chosen window can be iteratively expanded to include only the "important" training samples. Several ways of choosing representative samples for Nearest Neighbor learning methods exist (see <ref> [74, 75] </ref>, for examples). Some of these techniques may be helpful for inducing decision trees on large samples, provided they are efficient. Oates and Jensen recently analyzed the effect of training set size on decision tree complexity [280]. 5.2.
Reference: 75. <author> Belur V. Dasarathy. </author> <title> Minimal consistent set (MCS) identification for optimal nearest neighbor systems design. </title> <journal> IEEE Trans. on systems, man and cybernetics, </journal> <volume> 24(3) </volume> <pages> 511-517, </pages> <year> 1994. </year>
Reference-contexts: Quinlan suggested "windowing", a random training set sampling method, for his programs ID3 and C4.5 [306, 382]. A initially randomly chosen window can be iteratively expanded to include only the "important" training samples. Several ways of choosing representative samples for Nearest Neighbor learning methods exist (see <ref> [74, 75] </ref>, for examples). Some of these techniques may be helpful for inducing decision trees on large samples, provided they are efficient. Oates and Jensen recently analyzed the effect of training set size on decision tree complexity [280]. 5.2.
Reference: 76. <editor> G. R. Dattatreya and Laveen N. Kanal. </editor> <title> Decision trees in pattern recognition. </title> <editor> In Kanal and Rosenfeld, editors, </editor> <booktitle> Progress in Pattern Recognition, </booktitle> <volume> volume 2, </volume> <pages> pages 189-239. </pages> <publisher> Elsevier Science, </publisher> <year> 1985. </year>
Reference-contexts: The testing algorithms normally take the form of decision trees or AND/OR trees [368, 291]. Many heuristics used to construct decision trees are used for test sequencing also. 2.2. Treatises and surveys An overview of work on decision trees in the pattern recognition literature can be found in <ref> [76] </ref>. A high level comparative perspective on the classification literature in pattern recognition and artificial intelligence can be found in [53]. Tree induction from a statistical perspective, as it is popularly used today, is reviewed in Breiman et al.'s excellent book Classification and Regression Trees [31].
Reference: 77. <author> G. R. Dattatreya and V. V. S. Sarma. </author> <title> Bayesian and decision tree approaches to pattern recognition including feature measurement costs. </title> <journal> IEEE Trans. on Pattern Analysis and Machine Intelligence, </journal> <volume> PAMI-3(3):293-298, </volume> <year> 1981. </year>
Reference-contexts: If the measurement (misclassification) costs are not identical between different attributes (classes), decision tree algorithms may need to explicitly prefer cheaper trees. Several attempts have been made to make tree construction cost-sensitive. These involve incorporating attribute measurement costs (machine learning: [278, 279, 354, 360], pattern recognition: <ref> [77, 261] </ref>, statistics: [184]) and incorporating misclassification costs [31, 66, 83, 51, 360]. Methods to incorporate attribute measurement costs typically include a cost term into the feature evaluation criterion, whereas variable misclassification costs are accounted for by using prior probabilities or cost matrices. 20 SREERAMA K. MURTHY 5.3.
Reference: 78. <author> Thomas G. Dietterich, Hermann Hild, and Ghulum Bakiri. </author> <title> A comparison of ID3 and backpropagation for english text-to-speech mapping. </title> <journal> Machine Learning, </journal> <volume> 18 </volume> <pages> 51-80, </pages> <year> 1995. </year>
Reference-contexts: Their comparisons reveal that linear discriminant analysis is the fastest of the methods, when the underlying assumptions are met, and that decision trees methods overfit in the presence of noise. Dietterich et al. <ref> [78] </ref> argue that the inadequacy of trees for certain domains may be due to the fact that trees are unable to take into account some statistical information that is available to other methods like neural networks.
Reference: 79. <author> Thomas G. Dietterich and Eun Bae Kong. </author> <title> Machine learning bias, statistical bias and statistical variance of decision tree algorithms. </title> <note> In ML-95 [255]. to appear. 36 SREERAMA K. MURTHY </note>
Reference-contexts: Multiple trees A known peril of decision tree construction is its variance, especially when the samples are small and the features are many <ref> [79] </ref>. Variance can be caused by random choice of training and pruning samples, by many equally good attributes only one of which can be chosen at a node, due to cross validation or because of other reasons. <p> A c-regular tree is a tree in which all nodes have c children, and if one child of an internal node is a leaf, then so are all other children. A tree is regular is it is c-regular for any c. 18. It is argued empirically <ref> [79] </ref> that the variance in decision tree methods is more a reason than bias for their poor performance on some domains. 19. For a general description of modern classification problems in astronomy, which prompt the use of pattern recognition and machine learning techniques, see [203]. 20.
Reference: 80. <author> Thomas G. Dietterich and Ryszard S. Michalski. </author> <title> A comparitive view of selected methods for learning from examples. In R.S. </title> <editor> Michalski, J.G. Carbonell, and T.M. Mitchell, editors, </editor> <booktitle> Machine Learning, an Artificial Intelligence Approach, </booktitle> <volume> volume 1, </volume> <pages> pages 41-81. </pages> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, CA, </address> <year> 1983. </year>
Reference-contexts: Palvia and Gordon [287] compared decision tables, decision trees and decision rules, to determine which formalism is best for decision analysis. Many methods for learning from examples are compared in an early study by Dietterich and Michalski <ref> [80] </ref>. 8. Conclusions This paper attempted a multi-disciplinary survey of work in automatically constructing decision trees from data. We gave pointers to work in fields such as pattern DECISION TREE CONSTRUCTION: SURVEY 31 recognition, statistics, decision theory, machine learning, mathematical programming and neural networks.
Reference: 81. <author> Justin Doak. </author> <title> An evaluation of search algorithms for feature selection. </title> <type> Technical report, </type> <institution> Graduate Group in Computer Science, Univ. of California at Davis; and Safeguards Systems Group, Los Alamos National Lab., </institution> <month> January </month> <year> 1994. </year>
Reference-contexts: There has been a recent surge of interest in feature subset selection methods in the machine learning community, resulting in several empirical evaluations. These studies provide interesting insights on how DECISION TREE CONSTRUCTION: SURVEY 19 to increase the efficiency and effectiveness of the heuristic search for good feature subsets <ref> [185, 210, 48, 81, 257, 5] </ref>. 5.1.2. Composite features Sometimes the aim is not to choose a good subset of features, but instead to find a few good "composite" features, which are arithmetic or logical combinations of the atomic features.
Reference: 82. <author> D. L. Dowe and N. Krusel. </author> <title> Decision tree models of bushfire activity. </title> <journal> AI Applications, </journal> <volume> 8(3) </volume> <pages> 71-72, </pages> <year> 1994. </year>
Reference-contexts: Recent uses of tree-based classification in remote sensing can be found in <ref> [319, 82, 208] </ref>. * Software development: To estimate the development effort of a given soft ware module in [199]. * Other: Decision trees have also been used recently for building personal learn ing assistants [250] and for classifying sleep signals [201]. DECISION TREE CONSTRUCTION: SURVEY 29 7.2.
Reference: 83. <author> B. A. Draper, Carla E. Brodley, and Paul E. Utgoff. </author> <title> Goal-directed classification using linear machine decision trees. </title> <journal> IEEE Trans. on Pattern Analysis and Machine Intelligence, </journal> <volume> 16(9):888, </volume> <year> 1994. </year>
Reference-contexts: John [167] recently considered linear discriminant trees in the machine learning literature. An extension of linear discriminants are linear machines [276], which are linear structures that can discriminate between multiple classes. In the machine learning literature, Utgoff et al. explored decision trees that used linear machines at internal nodes <ref> [35, 83] </ref>. Locally Opposed Clusters of Objects: Sklansky and his students developed several piecewise linear discriminants based on the principle of locally opposed clusters of objects. Wassel and Sklansky [374, 344] suggested a procedure to train a linear split to minimize the error probability. <p> Several attempts have been made to make tree construction cost-sensitive. These involve incorporating attribute measurement costs (machine learning: [278, 279, 354, 360], pattern recognition: [77, 261], statistics: [184]) and incorporating misclassification costs <ref> [31, 66, 83, 51, 360] </ref>. Methods to incorporate attribute measurement costs typically include a cost term into the feature evaluation criterion, whereas variable misclassification costs are accounted for by using prior probabilities or cost matrices. 20 SREERAMA K. MURTHY 5.3.
Reference: 84. <author> N. R. Draper and H. Smith. </author> <title> Applied Regression Analysis. </title> <publisher> Wiley, </publisher> <address> New York, </address> <year> 1966. </year> <note> 2nd edition in 1981. </note>
Reference-contexts: If the training sample is too large to allow for efficient classifier induction, a subsample selection method (Section 5.1.3) can be employed. 5.1.1. Feature subset selection There is a large body of work on choosing relevant subsets of features (see the texts <ref> [84, 27, 245] </ref>). Much of this work was not developed in the context of tree induction, but a lot of it has direct applicability. There are two components to any method that attempts to choose the best subset of features.
Reference: 85. <author> R. Duda and P. Hart. </author> <title> Pattern Classification and Scene Analysis. </title> <publisher> Wiley, </publisher> <address> New York, </address> <year> 1973. </year>
Reference-contexts: Methods used in the literature for finding good linear tests include linear discriminant analysis, hill climbing search, linear programming, perceptron training and others. Linear Discriminant Trees: Several authors have considered the problem of constructing tree-structured classifiers that have linear discriminants <ref> [85] </ref> at each node. You and Fu [386] used a linear discriminant at each node in the decision tree, computing the hyper-plane coefficients using the Fletcher-Powell descent method [107]. Their method requires that the best set of features at each node be pre-specified by a human. <p> Decision trees with perceptrons at all internal nodes were described in [365, 334]. Mathematical Programming: Linear programming has been used for building adaptive classifiers since late 1960s [156]. Given two possibly intersecting sets of points, Duda and Hart <ref> [85] </ref> proposed a linear programming formulation for finding the split whose distance from the misclassified points is minimized. More recently, Mangasarian and Bennett used linear and quadratic programming techniques to build machine learning systems in general and decision trees in particular [232, 22, 20, 230, 21].
Reference: 86. <author> Eades and Staples. </author> <title> On optimal trees. </title> <journal> Journal of Algorithms, </journal> <volume> 2(4) </volume> <pages> 369-384, </pages> <year> 1981. </year>
Reference-contexts: Miyakawa [251] considered the problem of converting decision tables to optimal trees, and studied the properties of optimal variables, the class of attributes only members of which can be used at the root of an optimal tree. Eades and Staples <ref> [86] </ref> showed that the optimality in search trees, in terms of worst-case depth, is very closely related to regularity. 17 As irregular trees are not likely to be optimal, splitting rules (Section 3.1) that tend to slice off small corners of the attribute space building highly unbalanced trees are less likely
Reference: 87. <author> Bradley Efron. </author> <title> Estimating the error rate of a prediction rule: improvements on cross-validation. </title> <journal> J. of American Statistical Association, </journal> <volume> 78(382) </volume> <pages> 316-331, </pages> <month> June </month> <year> 1983. </year>
Reference-contexts: In bootstrapping, B independent learning samples, each of size N are created by random sampling with replacement from the original learning sample L. In cross validation, L is divided randomly into B mutually exclusive, equal sized partitions. Efron <ref> [87] </ref> showed that, although cross validation closely approximates the true result, bootstrap has much less variance, especially for small samples. However, there exist arguments that cross validation is clearly preferable to bootstrap in practice [190]. 15.
Reference: 88. <author> John F. Elder, </author> <title> IV. Heuristic search for model structure. </title> <booktitle> In AI&Statistics-95 [7], </booktitle> <pages> pages 199-210. </pages>
Reference-contexts: The problem of inducing globally optimal decision trees has been addressed time and again. For early work using dynamic programming and branch-and-bound techniques to convert decision tables to optimal trees, see [259]. Tree construction using partial or exhaustive lookahead has been considered in statistics <ref> [103, 57, 88] </ref>, in pattern recognition [142], for tree structured vector quan-tizers [315], for Bayesian class probability trees [44], for neural trees [72] and in machine learning [278, 310, 271]. Most of these studies indicate that lookahead does not cause considerable improvements over greedy induction.
Reference: 89. <author> Tapio Elomaa. </author> <title> In defence of C4.5: Notes on learning one-level decision trees. </title> <booktitle> In ML-94 [254], </booktitle> <pages> pages 62-69. </pages> <editor> Editors: William W. </editor> <booktitle> Cohen and Haym Hirsh. </booktitle>
Reference-contexts: He concluded that, on most real world data sets commonly used by the machine learning community [266], decision trees do not perform significantly better than one level rules. These conclusions, however, were refuted by Elomaa <ref> [89] </ref> on several grounds. Elomaa argued that Holte's observations may have been the peculiarities of the data he used, and that the slight differences in accuracy that Holte observed were still significant. Bias: Smaller consistent decision trees have higher generalization accuracy than larger consistent trees (Occam's Razor).
Reference: 90. <author> A. Ercil. </author> <title> Classification trees prove useful in nondestructive testing of spotweld quality. </title> <editor> Welding J., 72(9):59, </editor> <month> September </month> <year> 1993. </year> <note> Issue Title: Special emphasis: Rebuilding America's roads, railways and bridges. </note>
Reference-contexts: processing: For medical text classification [212], for acquiring a statistical parser from a set of parsed sentences [229]. * Law: For discovering knowledge in international conflict and conflict management databases, for the possible avoidance and termination of crises and wars [116]. * Manufacturing and Production: To non-destructively test welding quality <ref> [90] </ref>, for semiconductor manufacturing [163], for increasing productivity [179], for material procurement method selection [73], to accelerate rotogravure printing [92], for process optimization in electro-chemical machining [95], to schedule printed circuit board assembly lines [296], to uncover flaws in a Boeing manufacturing process [313] and for quality control [135].
Reference: 91. <author> Floriana Esposito, Donato Malerba, and Giovanni Semeraro. </author> <title> A further study of pruning methods in decision tree induction. </title> <booktitle> In AI&Statistics-95 [7], </booktitle> <pages> pages 211-218. </pages>
Reference-contexts: Use of dynamic programming to prune trees optimally and efficiently has been explored in [25]. A few studies have been done to study the relative effectiveness of pruning methods <ref> [247, 62, 91] </ref>. Just as in the case of splitting criteria, no single ad hoc pruning method has been adjudged to be superior to the others.
Reference: 92. <author> Bob Evans and Doug Fisher. </author> <title> Overcoming process delays with decision tree induction. </title> <journal> IEEE Expert, </journal> <pages> pages 60-66, </pages> <month> February </month> <year> 1994. </year>
Reference-contexts: Law: For discovering knowledge in international conflict and conflict management databases, for the possible avoidance and termination of crises and wars [116]. * Manufacturing and Production: To non-destructively test welding quality [90], for semiconductor manufacturing [163], for increasing productivity [179], for material procurement method selection [73], to accelerate rotogravure printing <ref> [92] </ref>, for process optimization in electro-chemical machining [95], to schedule printed circuit board assembly lines [296], to uncover flaws in a Boeing manufacturing process [313] and for quality control [135].
Reference: 93. <author> Brian Everitt. </author> <title> Cluster Analysis 3rd Edition. </title> <editor> E. </editor> <publisher> Arnold Press, </publisher> <address> London., </address> <year> 1993. </year>
Reference-contexts: For a good introduction to the literature on learning Bayesian networks, see [45]. Work on automatic construction of hierarchical structures from data in which the dependent variable is unknown (unsupervised learning), present in fields such as cluster analysis <ref> [93] </ref>, machine learning (e.g., [105, 121]) and vector quantization [122] is not covered. Work on hand-constructed decision trees (common in medicine) is also not considered. We do not discuss regression trees. There is a rich body of literature on this topic which shares many issues with the decision tree literature.
Reference: 94. <author> Judith A. Falconer, Bruce J. Naughton, Dorothy D. Dunlop, Elliot J. Roth, and Dale C. Strasser. </author> <title> Predicting stroke inpatient rehabilitation outcome using a classification tree approach. </title> <journal> Archives of Physical Medicine and Rehabilitation, </journal> <volume> 75(6):619, </volume> <month> June </month> <year> 1994. </year>
Reference-contexts: For a recent review of the use of machine learning (decision trees and other techniques) in scheduling, see [13]. * Medicine: Medical research and practice have long been important areas of application for decision tree techniques. Recent uses of automatic induction of decision trees can be found in cardiology <ref> [221, 94, 192] </ref>, study of tooth enamel [277], psychiatry [238], gastroenterology [171], for detecting microcalcifications in mammography [385], to analyze Sudden Infant Death (SID) syndrome [381] and for diagnosing thyroid disorders [104]. * Molecular biology: Initiatives such as the Human Genome Project and the GenBank database offer fascinating opportunities for machine
Reference: 95. <author> A. Famili. </author> <title> Use of decision tree induction for process optimization and knowledge refinement of an industrial process. </title> <journal> Artificial Intelligence for Eng. Design, Analysis and Manufacturing (AI EDAM), </journal> <volume> 8(1) </volume> <pages> 63-75, </pages> <month> Winter </month> <year> 1994. </year>
Reference-contexts: and conflict management databases, for the possible avoidance and termination of crises and wars [116]. * Manufacturing and Production: To non-destructively test welding quality [90], for semiconductor manufacturing [163], for increasing productivity [179], for material procurement method selection [73], to accelerate rotogravure printing [92], for process optimization in electro-chemical machining <ref> [95] </ref>, to schedule printed circuit board assembly lines [296], to uncover flaws in a Boeing manufacturing process [313] and for quality control [135].
Reference: 96. <author> R. M. Fano. </author> <title> Transmission of Information. </title> <publisher> MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1961. </year>
Reference-contexts: Goodrich [130] proved that optimal (smallest) linear decision tree construction is NP-complete even in three dimensions. 6.2. Theoretical Insights Goodman and Smyth [128] showed that greedy top-down induction of decision trees is directly equivalent to a form of Shannon-Fano prefix coding <ref> [96] </ref>. A consequence of this result is that top-down tree induction (using mutual information) is necessarily suboptimal in terms of average tree depth.
Reference: 97. <author> Usama M. Fayyad and Keki B. Irani. </author> <title> What should be minimized in a decision tree? In AAAI-90: </title> <booktitle> Proc. of the National Conf. on Artificial Intelligence, </booktitle> <volume> volume 2, </volume> <pages> pages 749-754. </pages> <publisher> AAAI, </publisher> <year> 1990. </year>
Reference-contexts: He shows that these three measures are pairwise incompatible, which implies that an algorithm mini DECISION TREE CONSTRUCTION: SURVEY 23 mizing one measure is guaranteed not to minimize the others, for some tree. Fayyad and Irani <ref> [97] </ref> argue that, by concentrating on optimizing one measure, number of leaf nodes, one can achieve performance improvement along other measures. Generalization accuracy is a popular measure for quantifying the goodness of learning systems.
Reference: 98. <author> Usama M. Fayyad and Keki B. Irani. </author> <title> The attribute specification problem in decision tree generation. </title> <booktitle> In AAAI-92 [1], </booktitle> <pages> pages 104-110. </pages>
Reference-contexts: Though the Kolmogorov-Smirnoff distance was originally proposed for tree induction in two-class problems [113, 316], it was subsequently extended to multiclass domains [143]. Class separation-based metrics developed in the machine learning literature <ref> [98, 388] </ref> are also distance measures. A relatively simplistic method for estimating class separation, which assumes that the values of each feature follow a Gaussian distribution in each class, was used for tree construction in [227]. Rules derived from dependence measures: These measure the statistical dependence between two random variables. <p> Miyakawa [252] compared three activity-based measures, Q, O and loss, both analytically and empirically. He showed that Q and O do not chose non-essential variables at tree nodes, and that they produce trees that are 1/4th the size of the trees produced by loss. Fayyad and Irani <ref> [98] </ref> showed that their measure C-SEP, performs better than Gini index [31] and information gain [301] for specific types of problems. Several researchers [141, 301] pointed out that information gain is biased towards attributes with a large number of possible values.

References-found: 98

