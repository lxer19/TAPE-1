URL: http://www.cs.wisc.edu/condor/doc/flock.ps
Refering-URL: http://www.cs.wisc.edu/condor/condor-world/0014.html
Root-URL: 
Phone: 356, 2600  
Title: A Worldwide Flock of Condors: Load Sharing among Workstation Clusters  
Author: D.H.J. Epema a M. Livny b R. van Dantzig c X. Evers a,c and J. Pruyne b 
Keyword: distributed processing, batch queueing system, wide-area load sharing, ownership rights, flocking.  
Note: c National Institute for Nuclear  
Address: P.O. Box  Madison, Wisconsin, USA  (NIKHEF), P.O. Box 41882, 1009 DB Amsterdam, The Netherlands  
Affiliation: a Department of Mathematics and Computer Science, Delft University of Technology,  AJ Delft, The Netherlands b Department of Computer Sciences, University of Wisconsin-Madison,  Physics and High-Energy Physics Research  
Abstract: Condor is a distributed batch system for sharing the workload of compute-intensive jobs in a pool of Unix workstations connected by a network. In such a Condor pool, idle machines are spotted by Condor and allocated to queued jobs, thus putting otherwise unutilized capacity to efficient use. When institutions owning Condor pools cooperate, they may wish to exploit the joint capacity of their pools in a similar way. So the need arises to extend the Condor load-sharing and protection mechanisms beyond the boundaries of Condor pools, or in other words, to create a flock of Condors. Such a flock may include Condor pools connected by local-area networks as well as by wide-area networks. In this paper we describe the design and implementation of a distributed, layered Condor flocking mechanism. The main concept in this design is the Gateway Machine that represents in each pool idle machines from other pools in the flock and allows job transfers across pool boundaries. Our flocking design is transparent to the workstation owners, to the users, and to Condor itself. We also discuss our experiences with an intercontinental Condor flock. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> A. Bricker, M.J. Litzkow and M. Livny, </author> <title> Condor Technical Summary, Version 4.1b, </title> <type> Technical Report 1069, </type> <institution> Computer Sciences Department, University of Wisconsin-Madison, Wisconsin, </institution> <address> USA (1992). </address> <month> 16 </month>
Reference-contexts: In this section we summarize those aspects of the Condor system which are relevant for this paper. A detailed description of the system can be found in <ref> [1, 6, 7] </ref>. The following three principles have guided the design of Condor. 1. Condor batch processing should have almost no impact on the availability of and the quality of service provided by workstations to their owners. 2.
Reference: [2] <author> Codine, </author> <title> Computing in Distributed Networked Environments, User's Guide and Refer--ence Manual, </title> <institution> Genias Software GmbH, </institution> <address> Erzgebirgstr. 2B, D-93073 Neutraubling, Germany (1994). </address>
Reference-contexts: It provides owners with means to control the impact batch processing has on the quality of service they experience on their workstation. Other batch systems, such as DQS [3], LSF [17], LoadLeveler [5] (that is based on Condor), and Codine <ref> [2] </ref>, have also recently made such means available. Since Condor became operational in 1988, it has been proven [8] that turning a cluster of workstations into a Condor pool results in a substantial increase in productivity and efficiency.
Reference: [3] <author> D. Duke, T. Green, and J. Pasko, </author> <title> Research towards a heterogeneous networked computing cluster: The distributed queueing system version 3.0, </title> <type> Technical Report, </type> <institution> Supercomputer Computations Research Institute, Florida State University (1994). </institution>
Reference-contexts: Condor [7] is the first batch system for clusters of workstations to address the distributed-ownership problem. It provides owners with means to control the impact batch processing has on the quality of service they experience on their workstation. Other batch systems, such as DQS <ref> [3] </ref>, LSF [17], LoadLeveler [5] (that is based on Condor), and Codine [2], have also recently made such means available. Since Condor became operational in 1988, it has been proven [8] that turning a cluster of workstations into a Condor pool results in a substantial increase in productivity and efficiency.
Reference: [4] <author> X. Evers, </author> <title> Condor Flocking: Load Sharing between Pools of Workstations, </title> <type> Master's Thesis, </type> <institution> Department of Mathematics and Computer Science, Delft University of Technology (1993). </institution>
Reference-contexts: In 1993, a prototype of the Condor flocking mechanism was designed and implemented as the master's thesis project for the Delft University of Technology of one of the authors <ref> [4] </ref>, carried out at NIKHEF, as part of an informal collaboration among the authors' institutions.
Reference: [5] <author> IBM LoadLeveler: </author> <title> User's Guide, Doc. No. </title> <institution> SH26-7226-00, IBM Corporation (1993). </institution>
Reference-contexts: Condor [7] is the first batch system for clusters of workstations to address the distributed-ownership problem. It provides owners with means to control the impact batch processing has on the quality of service they experience on their workstation. Other batch systems, such as DQS [3], LSF [17], LoadLeveler <ref> [5] </ref> (that is based on Condor), and Codine [2], have also recently made such means available. Since Condor became operational in 1988, it has been proven [8] that turning a cluster of workstations into a Condor pool results in a substantial increase in productivity and efficiency.
Reference: [6] <author> M.J. Litzkow, </author> <title> Remote UNIX, turning idle workstations into cycle servers, </title> <booktitle> Proc. of the 1987 Usenix Summer Conference, </booktitle> <address> Phoenix, Arizona, USA (1987) 381-384. </address>
Reference-contexts: In this section we summarize those aspects of the Condor system which are relevant for this paper. A detailed description of the system can be found in <ref> [1, 6, 7] </ref>. The following three principles have guided the design of Condor. 1. Condor batch processing should have almost no impact on the availability of and the quality of service provided by workstations to their owners. 2. <p> This illusion is maintained by the Remote Unix facility <ref> [6] </ref>, through which a number of system calls (which mainly have to do with file I/O) are redirected to 1 Steps 1.(a) and 1.(b) can be executed in any order or in parallel. 5 the submission machine, where they are handled by the Shadow.
Reference: [7] <author> M.J. Litzkow, M. Livny and M.W. </author> <title> Mutka, Condor-A hunter of idle workstations, </title> <booktitle> Proc. of the 8th Int'l Conf. on Distributed Computing Systems, </booktitle> <address> San Jose, CA, USA (1988) 104-111. </address>
Reference-contexts: While the ultimate goal of a batch system is to make essentially the entire computing power of the cluster available for batch processing, it is the owner of the workstation who has the right to decide when and by whom the workstation can be used for batch processing. Condor <ref> [7] </ref> is the first batch system for clusters of workstations to address the distributed-ownership problem. It provides owners with means to control the impact batch processing has on the quality of service they experience on their workstation. <p> In this section we summarize those aspects of the Condor system which are relevant for this paper. A detailed description of the system can be found in <ref> [1, 6, 7] </ref>. The following three principles have guided the design of Condor. 1. Condor batch processing should have almost no impact on the availability of and the quality of service provided by workstations to their owners. 2.
Reference: [8] <author> M.J. Litzkow and M. Livny, </author> <title> Experience with the Condor distributed batch system, </title> <booktitle> Proc. of the IEEE Workshop on Experimental Distributed Systems, </booktitle> <address> Huntsville, AL, USA (1990). </address>
Reference-contexts: Other batch systems, such as DQS [3], LSF [17], LoadLeveler [5] (that is based on Condor), and Codine [2], have also recently made such means available. Since Condor became operational in 1988, it has been proven <ref> [8] </ref> that turning a cluster of workstations into a Condor pool results in a substantial increase in productivity and efficiency.
Reference: [9] <author> M. Livny and M. Melman, </author> <title> Load Balancing in Homogeneous Broadcast Distributed Systems, </title> <booktitle> Proc. of the ACM Computer Network Performance Symposium, </booktitle> <address> College Park, </address> <month> Mary-land </month> <year> (1982). </year>
Reference-contexts: At the same time, a small group of owners who belong to the same organization may have batch-mode computing needs that are by far larger than what their workstations can provide. It is therefore not uncommon to find a cluster of workstations in the undesirable wait-while-idle (WWI) state <ref> [9] </ref>, in which batch jobs are waiting while elsewhere resources capable of serving them are idle.
Reference: [10] <author> M.W. Mutka and M. Livny, </author> <title> Scheduling remote processing capacity in a workstation-processor bank network, </title> <booktitle> Proc. of the 7th Int'l Conf. on Distributed Computing Systems, </booktitle> <address> Berlin, Germany (1987) 2-9. </address>
Reference-contexts: The CM can be viewed as a matchmaker, matching job contexts and machine contexts. The CM performs scheduling by scanning its list of queued jobs for potential matches in an order based on a novel priority scheme <ref> [10] </ref> in which jobs are ranked according to the past resource-usage pattern of the user who submitted them. 4 We now present the protocol used by Condor for matching a job J queued on submission machine S and an execution machine E, and for the subsequent starting of the job (see
Reference: [11] <author> M.W. Mutka and M. Livny, </author> <title> Profiling workstations' available capacity for remote execution, Performance '87, </title> <booktitle> Proc. of the 12th IFIP WG 7.3 Int'l Symp. on Computer Performance Modeling, Measurement and Evaluation, </booktitle> <address> Brussels, Belgium (1987) 529-544. </address>
Reference: [12] <author> M.W. Mutka and M. Livny, </author> <title> The available capacity of a privately owned workstation environment, Performance Evaluation Vol. </title> <month> 12 </month> <year> (1991) </year> <month> 269-284. </month>
Reference-contexts: While the problem of managing resources that are physically distributed has been addressed by many researchers, the distributed-ownership aspect of clusters of desk-top workstations is new. It has been observed <ref> [12] </ref> that most owners have computation needs that are much smaller than the capacity of their workstations and therefore tend to leave them idle for long periods of time.
Reference: [13] <author> J. Pruyne and M. Livny, </author> <title> Providing resource management services to parallel applications, </title> <editor> in: J. Dongarra and B. Tourancheau, eds., </editor> <booktitle> Proc. of the Second Workshop on Environments and Tools for Parallel Scientific Computing, SIAM Proceedings Series (SIAM, </booktitle> <year> 1994) </year> <month> 152-161. </month>
Reference: [14] <author> J. Pruyne and M. Livny, </author> <title> Parallel processing on dynamic resources with CARMI, </title> <editor> in: D. G. Feitelson and L. Rudolph, eds., </editor> <title> Job Scheduling Strategies for Parallel Processing, </title> <booktitle> Lecture Notes in Computer Science Vol. </booktitle> <publisher> 949 (Springer-Verlag, </publisher> <year> 1995) </year> <month> 259-278. </month>
Reference: [15] <author> T. Tannenbaum and M.J. Litzkow, </author> <title> The Condor Distributed Processing System, </title> <journal> Dr Dobbs Journal Vol. </journal> <month> 20(2) </month> <year> (1995) </year> <month> 40-48. </month>
Reference-contexts: At the same time, there needs to be some guarantee that jobs make progress in spite of these preemptions. Condor, therefore, provides mechanisms that automatically checkpoint <ref> [15] </ref> and restart a job (usually on a different machine).
Reference: [16] <author> J.M. Voogd, P.M.A. Sloot and R. van Dantzig, </author> <note> Crystallization on a sphere, FGCS Vol. 10 (1994) 359-361 (and references therein). </note>
Reference-contexts: This first "World Flock" contained over 250 workstations, about 60 of which on the European continent (see "Crystallization on a Sphere" <ref> [16] </ref>, in which a very large number of time-consuming simulated annealing (SA) jobs for a variable number of particles (N ) were executed. Thousands of these production jobs were submitted|usually in batches of 100 jobs|in the various pools and executed remotely in the flock.

References-found: 16

