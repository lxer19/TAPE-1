URL: http://www.cm.deakin.edu.au/~zijian/Papers/ictai96.ps.gz
Refering-URL: http://www.cm.deakin.edu.au/~zijian/publications.html
Root-URL: 
Email: zijian@deakin.edu.au  
Title: Effects of Different Types of New Attribute on Constructive Induction  
Author: Zijian Zheng 
Address: Geelong Victoria 3217, Australia  
Affiliation: School of Computing and Mathematics Deakin University,  
Note: In Proceedings of the 8th IEEE ICTAI, Los Alamitos, CA: IEEE Computer Society Press, 254-257, 1996.  
Abstract: This paper studies the effects on decision tree learning of constructing four types of attribute (conjunctive, disjunctive, Mof-N, and Xof-N representations). To reduce effects of other factors such as tree learning methods, new attribute search strategies, evaluation functions, and stopping criteria, a single tree learning algorithm is developed. With different option settings, it can construct four different types of new attribute, but all other factors are fixed. The study reveals that conjunctive and disjunctive representations have very similar performance in terms of prediction accuracy and theory complexity on a variety of concepts. Moreover, the study demonstrates that the stronger representation power of Mof-N than conjunction and disjunction and the stronger representation power of Xof-N than these three types of new attribute can be reflected in the performance of decision tree learning. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> E. Bloedorn, R. S. Michalski, and J. Wnek. </author> <title> Multistrat-egy constructive induction: </title> <booktitle> AQ17-MCI. In Proceedings of the Second International Workshop on Multistrategy Learning, </booktitle> <pages> pages 188-203, </pages> <year> 1993. </year>
Reference-contexts: However, other factors also affect the performance of a learning system. Most constructive induction systems such as FRINGE [5] and LFC [7] use conjunction or disjunction as constructive operators. A few systems use other constructive operators, for example, Mof-N [4], "attribute counting operators" <ref> [2, 1] </ref>, and Xof-N [8]. To explore the effects on constructive induction, more specifically on decision tree learning, of conjunctive, disjunctive, Mof-N, and Xof-N representations as new attributes, this paper compares them by using a single constructive decision tree learning algorithm.
Reference: [2] <author> R. S. Michalski. </author> <title> Pattern recognition as knowledge-guided computer induction. </title> <type> Technical Report 927, </type> <institution> Department of Computer Science, The University of Illinois at Urbana-Champaign, Urbana, IL, </institution> <year> 1978. </year>
Reference-contexts: 1. Introduction To improve the performance of selective induction systems, constructive induction <ref> [2] </ref> transforms the original instance space into a more useful form by generating new attributes from task-supplied attributes (called primitive attributes). Many constructive induction systems have been developed. They perform differently in different domains. <p> However, other factors also affect the performance of a learning system. Most constructive induction systems such as FRINGE [5] and LFC [7] use conjunction or disjunction as constructive operators. A few systems use other constructive operators, for example, Mof-N [4], "attribute counting operators" <ref> [2, 1] </ref>, and Xof-N [8]. To explore the effects on constructive induction, more specifically on decision tree learning, of conjunctive, disjunctive, Mof-N, and Xof-N representations as new attributes, this paper compares them by using a single constructive decision tree learning algorithm.
Reference: [3] <author> P. M. Murphy and D. W. Aha. </author> <title> UCI repository of machine learning databases [machine-readable data repository]. </title> <institution> Department of Information and Computer Science, University of California, </institution> <address> Irvine, CA, </address> <note> Available by anonymous ftp at ics.uci.edu in the pub/machine-learning-databases directory, </note> <year> 1995. </year>
Reference-contexts: We use the same experimental method given by Pagallo [5], including the sizes of training and test sets. Experiments are repeated ten times in each of these domains. In addition to the fourteen artificial domains, ten real-world domains from the UCI repository of machine learning databases <ref> [3] </ref> are used. They consist of five medical domains (Cleveland Heart Disease, Hepatitis, Liver Disorders, Pima Indians Diabetes, Wisconsin Breast Cancer), one molecular biology domain (Promoters), three linguistics domains (Nettalk (Phoneme), Nettalk (Stress), Nettalk (Letter)), and one game domain (Tic-Tac-Toe).
Reference: [4] <author> P. M. Murphy and M. J. Pazzani. ID2-of-3: </author> <title> constructive induction of M-of-N concepts for discriminators in decision trees. </title> <booktitle> In Proceedings of the Eighth International Workshop on Machine Learning, </booktitle> <pages> pages 183-187. </pages> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann, </publisher> <year> 1991. </year>
Reference-contexts: Constructive operators play an important role in constructive induction. However, other factors also affect the performance of a learning system. Most constructive induction systems such as FRINGE [5] and LFC [7] use conjunction or disjunction as constructive operators. A few systems use other constructive operators, for example, Mof-N <ref> [4] </ref>, "attribute counting operators" [2, 1], and Xof-N [8]. To explore the effects on constructive induction, more specifically on decision tree learning, of conjunctive, disjunctive, Mof-N, and Xof-N representations as new attributes, this paper compares them by using a single constructive decision tree learning algorithm.
Reference: [5] <author> G. Pagallo. </author> <title> Adaptive decision tree algorithms for learning from examples (Ph.D. </title> <type> thesis). Technical report, </type> <institution> University of California at Santa Cruz, </institution> <address> Santa Cruz, CA, </address> <year> 1990. </year>
Reference-contexts: Constructive operators play an important role in constructive induction. However, other factors also affect the performance of a learning system. Most constructive induction systems such as FRINGE <ref> [5] </ref> and LFC [7] use conjunction or disjunction as constructive operators. A few systems use other constructive operators, for example, Mof-N [4], "attribute counting operators" [2, 1], and Xof-N [8]. <p> Here, we focus on prediction accuracy and theory complexity. The theory complexity [8] is the modified tree size that includes both decision nodes and leaves, and takes into account the sizes of new attributes at decision nodes. 4.1. Experimental domains and methods Fourteen artificial (logical) domains are from Pa-gallo <ref> [5] </ref>. We use the same experimental method given by Pagallo [5], including the sizes of training and test sets. Experiments are repeated ten times in each of these domains. In addition to the fourteen artificial domains, ten real-world domains from the UCI repository of machine learning databases [3] are used. <p> Experimental domains and methods Fourteen artificial (logical) domains are from Pa-gallo <ref> [5] </ref>. We use the same experimental method given by Pagallo [5], including the sizes of training and test sets. Experiments are repeated ten times in each of these domains. In addition to the fourteen artificial domains, ten real-world domains from the UCI repository of machine learning databases [3] are used.
Reference: [6] <author> J. R. Quinlan. C4.5: </author> <title> Programs for Machine Learning. </title> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann, </publisher> <year> 1993. </year>
Reference-contexts: This makes it hard to find further good tests for subtrees. Subsetting and subranging can help to alleviate this problem [8]. 3. A single algorithm for creating four different types of new attribute We use the most well-known learning algorithm C4.5 <ref> [6] </ref> for building decision trees. The method employed for generating new attributes is the simple but widely used greedy search. It does not favour any specific new attribute type.
Reference: [7] <author> H. Ragavan and L. Rendell. </author> <title> Lookahead feature construction for learning hard concepts. </title> <booktitle> In Proceedings of the Tenth International Conference on Machine Learning, </booktitle> <pages> pages 252-259. </pages> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann, </publisher> <year> 1993. </year>
Reference-contexts: Constructive operators play an important role in constructive induction. However, other factors also affect the performance of a learning system. Most constructive induction systems such as FRINGE [5] and LFC <ref> [7] </ref> use conjunction or disjunction as constructive operators. A few systems use other constructive operators, for example, Mof-N [4], "attribute counting operators" [2, 1], and Xof-N [8].
Reference: [8] <author> Z. Zheng. </author> <title> Constructing nominal Xof-N attributes. </title> <booktitle> In Proceedings of the Fourteenth International Joint Conference on Artificial Intelligence, </booktitle> <pages> pages 1064-1070. </pages> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann, </publisher> <year> 1995. </year>
Reference-contexts: However, other factors also affect the performance of a learning system. Most constructive induction systems such as FRINGE [5] and LFC [7] use conjunction or disjunction as constructive operators. A few systems use other constructive operators, for example, Mof-N [4], "attribute counting operators" [2, 1], and Xof-N <ref> [8] </ref>. To explore the effects on constructive induction, more specifically on decision tree learning, of conjunctive, disjunctive, Mof-N, and Xof-N representations as new attributes, this paper compares them by using a single constructive decision tree learning algorithm. <p> The conjunctive and disjunctive representations are conventional logical conjunctions and disjunctions of conditions respectively. Mof-N representations are at-least Mof-Ns. That is, given a set of conditions, an Mof-N answers whether at least M of conditions in the set are true. An Xof-N representation <ref> [8] </ref> states how many conditions in its set are true for a given example. Here, a condition (attribute-value pair) is true for an example if the corresponding attribute value of the example is the same as that in the condition. <p> Therefore, it has a tree representation containing only three decision nodes with disjunctions as tests: ( A _ B), ( C _ D), and ( E _ F ). In this paper, we consider Xof-N representations as nominal attributes <ref> [8] </ref>, although they can also be treated as continuous-valued attributes [9]. <p> When Xof-Ns with large values for N are used as tests for decision trees, they quickly split training sets into a large number of small subsets. This makes it hard to find further good tests for subtrees. Subsetting and subranging can help to alleviate this problem <ref> [8] </ref>. 3. A single algorithm for creating four different types of new attribute We use the most well-known learning algorithm C4.5 [6] for building decision trees. The method employed for generating new attributes is the simple but widely used greedy search. It does not favour any specific new attribute type. <p> The method employed for generating new attributes is the simple but widely used greedy search. It does not favour any specific new attribute type. The algorithm borrows the control structure, tree learning method, new attribute search method, evaluation function, and so on from the XofN algorithm <ref> [8] </ref>. Choosing the XofN algorithm as the basis of the algorithm for the comparison study in this paper is based on the soundness and effectiveness of XofN which are established in Zheng [8]. The details of the XofN algorithm can be found elsewhere [8]. <p> control structure, tree learning method, new attribute search method, evaluation function, and so on from the XofN algorithm <ref> [8] </ref>. Choosing the XofN algorithm as the basis of the algorithm for the comparison study in this paper is based on the soundness and effectiveness of XofN which are established in Zheng [8]. The details of the XofN algorithm can be found elsewhere [8]. Here, we only describe the main idea and a few specific points about the learning algorithm used in this paper. <p> and so on from the XofN algorithm <ref> [8] </ref>. Choosing the XofN algorithm as the basis of the algorithm for the comparison study in this paper is based on the soundness and effectiveness of XofN which are established in Zheng [8]. The details of the XofN algorithm can be found elsewhere [8]. Here, we only describe the main idea and a few specific points about the learning algorithm used in this paper. It is referred to as CONJ, DISJ, MofN, and XofN when constructing conjunctive, disjunctive, Mof-N, and Xof-N representations respectively. <p> Experiments This section reports experiments using C4.5, CONJ, DISJ, MofN, XofN, and XofN (r) in a set of artificial and real-world domains. Here, we focus on prediction accuracy and theory complexity. The theory complexity <ref> [8] </ref> is the modified tree size that includes both decision nodes and leaves, and takes into account the sizes of new attributes at decision nodes. 4.1. Experimental domains and methods Fourteen artificial (logical) domains are from Pa-gallo [5].
Reference: [9] <author> Z. Zheng. </author> <title> Continuous-valued Xof-N attributes versus nominal Xof-N attributes for constructive induction: a case study. </title> <booktitle> In Proceedings of the Fourth International Conference for Young Computer Scientists, </booktitle> <pages> pages 566-573. </pages> <address> Beijing: </address> <publisher> Peking University Press, </publisher> <year> 1995. </year>
Reference-contexts: Therefore, it has a tree representation containing only three decision nodes with disjunctions as tests: ( A _ B), ( C _ D), and ( E _ F ). In this paper, we consider Xof-N representations as nominal attributes [8], although they can also be treated as continuous-valued attributes <ref> [9] </ref>. Xof-N can directly represent all the following types of concept: conjunction (with or without internal disjunction), disjunction (with or without internal disjunction), at-least Mof-N, at-most Mof-N, exactly Mof-N, even parity, odd parity, and possible combinations of the above six types of concept.
Reference: [10] <author> Z. Zheng. </author> <title> A comparison of constructive induction with different types of new attribute. </title> <type> Technical Report TR C96/8, </type> <institution> School of Computing and Mathematics, Deakin University, </institution> <note> Geelong, Victo-ria, Australia (Available by anonymous ftp at gol-lum.cm.deakin.edu.au in "/pub/TR/Computing/TR-C96-8.ps.Z", </note> <year> 1996. </year>
Reference-contexts: However, this is not because conjunction and disjunction, as new attributes for tree learning, have different expressive power in the DNF1 and CNF1 domains. For a detailed explanation and further exploration, see Zheng <ref> [10] </ref>. MofN performs as well as CONJ and DISJ in the DNF, CNF, and multiplexor domains, and significantly better in the majority and parity domains in terms of accuracy and theory complexity.
References-found: 10

