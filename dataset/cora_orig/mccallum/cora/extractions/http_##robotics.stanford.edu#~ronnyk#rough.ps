URL: http://robotics.stanford.edu/~ronnyk/rough.ps
Refering-URL: http://robotics.stanford.edu/users/ronnyk/ronnyk-bib.html
Root-URL: 
Email: fronnyk,brianfg@CS.Stanford.EDU  
Title: Rough Sets and Soft Computing (RSSC 94) Useful Feature Subsets and Rough Set Reducts  
Author: Ron Kohavi and Brian Frasca 
Address: Stanford, CA 94305  
Affiliation: Computer Science Dept. Stanford University  
Note: To appear in the Third International Workshop on  
Abstract: In supervised classification learning, one attempts to induce a classifier that correctly predicts the label of novel instances. We demonstrate that by choosing a useful subset of features for the indis-cernibility relation, an induction algorithm based on simple decision table can have high prediction accuracy on artificial and real-world datasets. We show that useful feature subsets are not necessarily maximal independent sets (relative reducts) with respect to the label, and that, in practical situations, using a subset of the relative core features may lead to superior performance. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Aha, D. W. </author> <year> 1992. </year> <title> Tolerating noisy, irrelevant and novel attributes in instance-based learning algorithms. </title> <journal> International Journal of Man-Machine Studies 36(1) </journal> <pages> 267-287. </pages>
Reference: <author> Ben-Bassat, M. </author> <year> 1982. </year> <title> Use of distance measures, information measures and error bounds in feature evaluation. </title> <editor> In Krishnaiah, P. R., and Kanal, L. N., eds., </editor> <booktitle> Handbook of Statistics, </booktitle> <volume> volume 2. </volume> <publisher> North-Holland Publishing Company. </publisher> <pages> 773-791. </pages>
Reference: <author> Blum, A. L., and Rivest, R. L. </author> <year> 1992. </year> <title> Training a 3-node neural network is NP-complete. </title> <booktitle> Neural Networks 5 </booktitle> <pages> 117-127. </pages>
Reference: <author> Boyce, D.; Farhi, A.; and Weischedel, R. </author> <year> 1974. </year> <title> Optimal Subset Selection. </title> <publisher> Springer-Verlag. </publisher>
Reference: <author> Breiman, L.; Friedman, J. H.; Olshen, R. A.; and Stone, C. J. </author> <year> 1984. </year> <title> Classification and Regression Trees. </title> <publisher> Wadsworth International Group. </publisher>
Reference: <author> Devijver, P. A., and Kittler, J. </author> <year> 1982. </year> <title> Pattern Recognition: A Statistical Approach. </title> <booktitle> Prentice-Hall International. </booktitle>
Reference: <author> Efron, B., and Tibshirani, R. </author> <year> 1993. </year> <title> An introduction to the bootstrap. </title> <publisher> Chapman & Hall. </publisher>
Reference-contexts: To find a useful feature subset, we conduct a best-first search in the space of feature subsets, estimating the accuracy of the induced classifier at each state using the Bootstrap estimation method <ref> (Efron & Tibshirani 1993) </ref>. Rough set theory defines the unique core of a dataset to be the set of indispensable features (see Section 2). Removal of any feature from the core set changes the positive region with respect to the label. <p> In the wrapper model, a search for a good feature subset is conducted using the inducer itself as a black box; the future prediction accuracy is estimated using methods such as cross validation (Breiman et al. 1984; Weiss & Kulikowski 1991) or Bootstrap <ref> (Efron & Tibshirani 1993) </ref>. The Holte-II inducer 1 is an inducer that produces a table-majority classifier based on a subset of the original set of features. Given an instance, the classifier behaves like a table-majority classifier, except that only the subset of features is used for matching. <p> We face two problems when trying to find a good subset. The first is how to estimate the accuracy of each subset, and the second is which subsets to examine. For our experimental results, we evaluated each subset by using Efron's :632-bootstrap estimator <ref> (Efron & Tib-shirani 1993) </ref>. In order to avoid searching the full space, we conducted a best-first search (Ginsberg 1 The name was inspired by Holte's paper (1993), but the algorithm bears no resemblance to Holte's 1R algorithm. 1993), stopping after a predetermined number of non-improving node expansions. <p> Holte-II is biased to select a feature subset maximizing the Bootstrap accuracy estimate. Whenever the estimates are good, Holte-II should choose a feature subset leading to high accuracy. However, when the Bootstrap estimates are inappropriate <ref> (Efron & Tibshirani 1993, Section 7.4) </ref>, such as when the training set is not a representative of the true distribution, the selected feature subset might be inappropriate. Our results show that such a bias is indeed appropriate for the datasets used in our experiments.
Reference: <author> Fayyad, U. M., and Irani, K. B. </author> <year> 1993. </year> <title> Multi-interval discretization of continuous attributes for classification learning. </title> <editor> In Bajcsy, R., ed., </editor> <booktitle> Proceedings of the Thirteenth International Joint Conference on Artificial Intelligence. </booktitle> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Ginsberg, M. L. </author> <year> 1993. </year> <title> Essential of Artificial Intelligence. </title> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Grzymala-Busse, J. W. </author> <year> 1992. </year> <title> LERS|a system for learning from examples based on rough sets. </title>
Reference: <editor> In Slowinski, R., ed., </editor> <title> Intelligent Decision Support. </title> <publisher> Kluwer Academic Publishers. </publisher> <pages> 3-18. </pages>
Reference: <author> Holte, R. C. </author> <year> 1993. </year> <title> Very simple classification rules perform well on most commonly used datasets. </title> <booktitle> Machine Learning 11 </booktitle> <pages> 63-90. </pages>
Reference-contexts: If we ignore Monk 1, Monk 2, and parity|datasets that C4.5 does very badly on|the average accuracy for Holte-II is 91.2% and 88.5% for C4.5. Holte's 1R program <ref> (Holte 1993) </ref> built one-rules, that is, rules that test a single attribute, and was shown to perform reasonably well on some commonly used datasets. Table 3 compares Holte-II with 1R on the same datasets used in Holte's paper.
Reference: <author> Hurley, R. B. </author> <year> 1983. </year> <title> Decision Tables in Software Engineering. Van Nostrand Reinhold data processing series. </title>
Reference: <author> Hyafil, L., and Rivest, R. L. </author> <year> 1976. </year> <title> Constructing optimal binary decision trees is NP-complete. </title> <journal> Information Processing Letters 5(1) </journal> <pages> 15-17. </pages>
Reference: <author> John, G.; Kohavi, R.; and Pfleger, K. </author> <year> 1994. </year> <title> Irrelevant features and the subset selection problem. </title> <booktitle> In Machine Learning: Proceedings of the Eleventh International Conference. </booktitle> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: The main problem with such measures is that they ignore the utility of the features for a given inducer. An alternative method uses a wrapper embedding the induction algorithm <ref> (John, Kohavi, & Pfleger 1994) </ref>.
Reference: <author> Kohavi, R. </author> <year> 1994. </year> <title> Feature subset selection as search with probabilistic estimates. </title> <note> In AAAI Fall Symposium on Relevance. To appear. </note>
Reference-contexts: The main problem with such measures is that they ignore the utility of the features for a given inducer. An alternative method uses a wrapper embedding the induction algorithm <ref> (John, Kohavi, & Pfleger 1994) </ref>. <p> One way to improve the estimates without increasing the running time considerably is to dynamically decide on the number of bootstrap samples needed. An abstract description of this problem is described in <ref> (Kohavi 1994) </ref>. For the artificial datasets: Monk 1-3, parity, and tic-tac-toe, the test sets include the space of all possible instances, and therefore the test set accuracy is the actual real accuracy.
Reference: <author> Langley, P., and Sage, S. </author> <year> 1994. </year> <title> Oblivious decision trees and abstract cases. </title> <booktitle> In AAAI94 Workshop on Case-Based Reasoning. </booktitle>
Reference-contexts: Tom Dietterich, Pat Langley, Ofer Matan, and Ross Quinlan provided useful comments. Pat Langley referred the first author to the work on rough sets when he saw the work on Oblivious read-Once decision graphs (OODGs). His results on Oblivion <ref> (Langley & Sage 1994) </ref> sparked the first ideas toward this paper.
Reference: <author> Lenarcik, A., and Piasta, Z. </author> <year> 1992. </year> <title> Discretization of condition attributes. </title> <editor> In Slowinski, R., ed., </editor> <title> Intelligent Decision Support. </title> <publisher> Kluwer Academic Publishers. </publisher> <pages> 373-389. </pages>
Reference: <author> Miller, A. J. </author> <year> 1990. </year> <title> Subset Selection in Regression. </title> <publisher> Chapman and Hall. </publisher>
Reference: <author> Modrzejewski, M. </author> <year> 1993. </year> <title> Feature selection using rough sets theory. </title> <editor> In Brazdil, P. B., ed., </editor> <booktitle> Proceedings of the European Conference on Machine Learning, </booktitle> <pages> 213-226. </pages>
Reference: <author> Murphy, P. M., and Aha, D. W. </author> <year> 1994. </year> <title> UCI repository of machine learning databases. For information contact ml-repository@ics.uci.edu. </title>
Reference: <author> Neter, J.; Wasserman, W.; and Kutner, M. H. </author> <year> 1990. </year> <title> Applied Linear Statistical Models. </title> <type> Irwin: Homewood, </type> <institution> IL, </institution> <note> 3rd edition. </note>
Reference: <author> Pawlak, Z.; Wong, S.; and Ziarko, W. </author> <year> 1988. </year> <title> Rough sets: Probabilistic versus deterministic approach. </title> <journal> Internation Journal of Man Machine Studies 29 </journal> <pages> 81-95. </pages>
Reference-contexts: Some authors in the rough set community have suggested using the degree of dependency on the label (fl) for this selection process (Ziarko 1991; Modrzejewski 1993), or other measures such as normalized entropy <ref> (Pawlak, Wong, & Ziarko 1988) </ref>. In statistics, many measures have been investigated (Boyce, Farhi, & Weischedel 1974; Miller 1990; Neter, Wasserman, & Kutner 1990), and others have been investigated in the pattern recognition community (Devijver & Kittler 1982; Ben-Bassat 1982).
Reference: <author> Pawlak, Z. </author> <year> 1987. </year> <title> Decision tables | a rough sets approach. </title> <journal> Bull. of EATCS 33 </journal> <pages> 85-96. </pages>
Reference: <author> Pawlak, Z. </author> <year> 1991. </year> <title> Rough Sets. </title> <publisher> Kluwer Academic Publishers. </publisher>
Reference-contexts: A rough set <ref> (Pawlak 1991) </ref> approximates a given concept from below and from above using an indis-cernibility relation. Pawlak (1993) points out that one of the most important and fundamental notions to the rough sets philosophy is the need to discover redundancy and dependencies between features. <p> For example, the relative core is the union of all indispensable features with respect to the label, and it is the intersection of all relative reducts. The reader is referred to Pawlak's book on rough sets <ref> (Pawlak 1991, Chapter 3) </ref> for the precise definitions. Throughout this paper, the terms core and reduct will refer to the relative core and relative reduct with respect to the label. 3 Classifiers and Inducers A classifier maps an unlabelled instance to a class label using some internally stored structure.
Reference: <author> Pawlak, Z. </author> <year> 1993. </year> <title> Rough sets: present state and the future. </title> <booktitle> Foundations of Computing and Decision Sciences 18(3-4):157-166. </booktitle>
Reference: <author> Quinlan, J. R. </author> <year> 1993. </year> <title> C4.5: Programs for Machine Learning. </title> <address> Los Altos, California: </address> <publisher> Morgan Kauf-mann. </publisher>
Reference-contexts: Common classifiers are decision trees, neural networks, and nearest-neighbor methods. Many induction algorithms do not scale up well in the face of irrelevant features. Aha (1992) reports that "IB3's storage requirement increases exponentially with the number of irrelevant attributes." John, Kohavi, and Pfleger (1994) show that C4.5 <ref> (Quinlan 1993) </ref>, a state-of-the-art decision tree induction algorithm, drastically degrades in performance when an irrelevant feature and a relevant but noisy feature are added to a given dataset. A rough set (Pawlak 1991) approximates a given concept from below and from above using an indis-cernibility relation.
Reference: <author> Schaffer, C. </author> <year> 1994. </year> <title> A conservation law for generalization performance. </title> <booktitle> In Machine Learning: Proceedings of the Eleventh International Conference, </booktitle> <pages> 259-265. </pages> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Skowron, A., and Rauszer, C. </author> <year> 1991. </year> <title> The dis-cernibility matrices and functions in information systems. </title> <type> Technical Report Research Report, </type> <institution> ICS WUT, Warsaw. </institution>
Reference: <author> Skowron, A., and Ruaszer, C. </author> <year> 1992. </year> <title> The dis-cernibility matrices and functions in information systems. </title> <editor> In Slowinski, R., ed., </editor> <title> Intelligent Decision Support. </title> <publisher> Kluwer Academic Publishers. </publisher> <pages> 331-362. </pages>
Reference: <author> Thrun etal. </author> <year> 1991. </year> <title> The monk's problems: A performance comparison of different learning algorithms. </title> <type> Technical Report CMU-CS-91-197, </type> <institution> Carnegie Mellon University. </institution>
Reference-contexts: Monk 2-local is a variant of Monk 2 <ref> (Thrun etal. 1991) </ref> where each feature value is made into an indicator variable. Vote1 has the "physician-fee-freeze" feature deleted, something that is commonly done to make the problem harder.
Reference: <author> Weiss, S. M., and Kulikowski, C. A. </author> <year> 1991. </year> <title> Computer Systems that Learn. </title> <address> San Mateo, CA: </address> <publisher> Mor-gan Kaufmann. </publisher>
Reference: <author> Wong, S., and Ziarko, W. </author> <year> 1985. </year> <title> On optimal decision rules in decision tables. </title> <journal> In Bulletin of the Polish Academy of Sciences and Mathematics, </journal> <pages> 693-696. </pages>
Reference: <author> Ziarko, W. </author> <year> 1991. </year> <title> The discovery, analysis, and representation of data dependencies in databases. </title>
Reference: <editor> In Piatetsky-Shapiro, G., and Frawley, W., eds., </editor> <title> Knowledge Discovery in Databases. </title> <publisher> MIT Press. </publisher>
References-found: 35

