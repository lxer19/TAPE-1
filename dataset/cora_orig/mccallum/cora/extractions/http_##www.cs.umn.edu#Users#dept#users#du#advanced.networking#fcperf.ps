URL: http://www.cs.umn.edu/Users/dept/users/du/advanced.networking/fcperf.ps
Refering-URL: http://www.cs.umn.edu/Users/dept/users/du/advanced.networking/
Root-URL: http://www.cs.umn.edu
Title: Performance of High-Speed Network I/O Subsystems: Case Study of A Fibre Channel Network  
Author: Mengjou Lin, Jenwei Hsieh, and David H.C. Du James A. MacDonald 
Keyword: I/O Subsystem, Performance Evaluation, Fibre Channel, Host Interface.  
Note: To appear in Supercomputing'94 (November, 94).  
Address: Minneapolis, MN 55455  Minneapolis, MN 55415  
Affiliation: Computer Science Department University of Minnesota  Army High Performance Computing Research Center University of Minnesota  
Abstract: Emerging high-speed networks provide several hundred megabits per second to several gigabits per second of raw communication bandwidth. However, the maximum achievable throughput available to the end-user or application is quite limited. In order to fully utilize the network bandwidth and to improve the performance at the application level, a careful examination of I/O subsystems is essential. In this paper, we study one emerging high-speed network, the Fibre Channel network. The objectives of this study are : 1) to understand how the I/O subsystem relates to network operations, 2) to evaluate and analyze the performance of such a subsystem, and 3) to propose possible approaches for improving the maximum achievable bandwidth and reducing end-to-end communication latency. We will show (by simply modifying device driver code) a 75% maximum achievable bandwidth improvement and a 15.9% latency reduction for short packets. Other ways of improving network performance are also discussed. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <institution> Ancor Communications, Inc. </institution> <note> VME CIM 250 Reference/User's Manual, No. PUB 006 A Rev. </note> <editor> B, </editor> <year> 1994. </year>
Reference-contexts: Each workstation was equipped with an Ancor VME CIM 4 250 Fibre Channel interface board which was connected to an Ancor CXT 250 Fibre Channel switch. 2.2.1 Fibre Channel Interface Board A VME CIM 250 Fibre Channel interface board furnishes one N Port to a workstation <ref> [1] </ref>. It provides FC-2 (Signaling), FC-1 (Transmission), and FC-0 (interface) levels of the Fibre Channel protocol. The VME CIM 250 acts as a bus master when using DMA to moving data between the host system and the interface board.
Reference: [2] <institution> Ancor Communications, Inc. </institution> <note> CXT 250 16-Port Switch Installer's/User's Manual, No. PUB 007 A Rev. </note> <author> D, </author> <year> 1993. </year>
Reference-contexts: Space-division switching provides a circuit-switch mechanism that allows direct connections (FC Class 1 service) between nodes on the network. Time-division switching provides a packet-switch mechanism which allows time-multiplexed connections and datagram service (FC Class 2 and 3) among all nodes on the network <ref> [2] </ref>. 2.2.3 Host System The Silicon Graphics 4D/3xx series workstations are shared memory multiprocessor systems equipped with 33 MHz MIPS R3000 processors. The system bus has a bandwidth of 64 MByte/sec. The I/O bus (VME bus) connects to the system bus through an I/O adapter called the IO3 card.
Reference: [3] <author> T.M. Anderson and R.S. Cornelius. </author> <title> High-Performance Switching with Fiber Channel. </title> <booktitle> In Proceedings of CompCon, </booktitle> <pages> pages 261-264. </pages> <publisher> IEEE, </publisher> <month> February </month> <year> 1992. </year>
Reference-contexts: Its connections to the nodes are called F Ports. Some existing Fibre Channel switches are non-blocking Clos type switches <ref> [3, 14] </ref>. The interface cards give Fibre Channel capability to nodes. Their Fibre Channel connections are called N Ports. 2.2 System Configuration In our FC test environment, two SGI workstations were used. The source side was a 4D/320-VGX and the sink side was a 4D/340S.
Reference: [4] <author> ANSI X3T9.3. </author> <title> Fiber Channel Physical and Signaling Interface (FC-PH), </title> <address> 4.2 edition, </address> <month> November </month> <year> 1993. </year>
Reference-contexts: Fibre Channel is a promising and emerging high-speed network technology that can satisfy the communication requirements of many large-scale distributed applications. It offers both circuit switching and packet switching at multiple data rates: 25.805 MBytes/sec, 51.61 MBytes/sec, and 103.22 MBytes/sec <ref> [4] </ref>. Three classes of service (Class 1 through 3) are offered to users. Class 1 service establishes a dedicated connection which is retained by the switching fabric with guaranteed maximum bandwidth. It offers a reliable, sequenced, and flow-controlled data delivery. <p> Finally, we describe hardware and software performance monitoring tools. 2.1 The Fibre Channel Standard Fibre Channel is a standard being developed under the ANSI X3T9.3 task group. The Fibre Channel standard is organized into the following levels <ref> [4] </ref>. * FC-0 defines the physical media and links with the receivers and transmitters.
Reference: [5] <author> M.J. Bach. </author> <title> The Design of the Unix Operating System. </title> <booktitle> Prentice-Hall Software Series. </booktitle> <publisher> Prentice-Hall, </publisher> <address> London, </address> <year> 1987. </year>
Reference-contexts: In the Unix operating system, all 6 devices are treated as files <ref> [5] </ref>. The Fibre Channel controller is referenced by a special device file /dev/fcN, where N represents the Fibre Channel interface board number when multiple boards reside in the same host.
Reference: [6] <author> D. Banks and M. Prudence. </author> <title> A High-Performance Network Architecture for a PA-RISC Workstation. </title> <journal> IEEE Journal on Selected Areas in Communications, </journal> <volume> 11(2) </volume> <pages> 191-202, </pages> <month> February </month> <year> 1993. </year>
Reference-contexts: Clark analyzed the TCP protocol processing time and found the protocol processing time was not very significant [8]. Banks and Prudence presented an improvement for higher level protocols by reducing the number of data copies required across the system bus <ref> [6] </ref>. Zitterbart proposed a functional based communication model that allows applications to request individually tailored services from the network subsystem [19]. Other researchers have studied the performance of the network interface.
Reference: [7] <author> A. Berenbaum, J. Dixon, A. Iyengar, and S. Keshav. </author> <title> A Flexible ATM-Host Interface for XUNET II. </title> <journal> IEEE Network, </journal> <pages> pages 18-23, </pages> <month> July </month> <year> 1993. </year>
Reference-contexts: However, the maximum achievable bandwidth at the application level is still very far behind the available "raw" network bandwidth. For the XUNET II ATM trial, an application level bandwidth of 40 megabits per second (Mbits/sec) has been reported <ref> [7] </ref>. The ATM network had a raw communication bandwidth of 160 Mbits/sec 2 , so the application bandwidth was only one fourth of the available network bandwidth. Hsieh et al. observed a maximum bandwidth of only 92 Mbits/sec for an experimental 800 Mbits/sec HIPPI network [11]. <p> Zitterbart proposed a functional based communication model that allows applications to request individually tailored services from the network subsystem [19]. Other researchers have studied the performance of the network interface. Berenbaum et al. designed a programmable ATM host interface <ref> [7] </ref>. "Afterburner" is a network-independent interface which provides architectural support for a high-performance protocol [9].
Reference: [8] <author> D.D. Clark, V. Jacobson, J. Romkey, and H. Salwen. </author> <title> An Analysis of TCP Processing Overhead. </title> <journal> IEEE Communications Magazine, </journal> <volume> 27(6) </volume> <pages> 23-9, </pages> <month> June </month> <year> 1989. </year>
Reference-contexts: Pasquale et al. proposed operating system support to reduce data copying across different communication domains [15]. Lin et al. found significant overhead incurred by APIs using ATM networks [13]. Clark analyzed the TCP protocol processing time and found the protocol processing time was not very significant <ref> [8] </ref>. Banks and Prudence presented an improvement for higher level protocols by reducing the number of data copies required across the system bus [6]. Zitterbart proposed a functional based communication model that allows applications to request individually tailored services from the network subsystem [19]. <p> The interface vendor claims that this would be difficult, if not impossible, to accomplish. 31 * Timed Polling: Interrupt handling has long been blamed for high communication laten-cies <ref> [8] </ref>. One possible solution for reducing the latency caused by handling interrupts and context switches, is to disable the interrupts from the interface. The required cooperation between the network interface and device driver could be accomplished by using a timed polling scheme.
Reference: [9] <author> C. Dalton, G. Watson, D. Banks, C. Calamvokis, A. Edwards, and J. Lumley. </author> <title> Afterburner. </title> <journal> IEEE Network, </journal> <pages> pages 36-43, </pages> <month> July </month> <year> 1993. </year>
Reference-contexts: Other researchers have studied the performance of the network interface. Berenbaum et al. designed a programmable ATM host interface [7]. "Afterburner" is a network-independent interface which provides architectural support for a high-performance protocol <ref> [9] </ref>. The VMP network adapter board has on-board protocol processing to improve communication performance [12]. 2 This is not a regular ATM OC-3 rate. 1 In this paper we examine a network subsystem for an emerging high-speed network called Fibre Channel.
Reference: [10] <author> D. Getchell and P. Rupert. </author> <title> Fiber Channel in the Local Area Network. </title> <journal> IEEE LTS, </journal> <pages> pages 38-42, </pages> <month> May </month> <year> 1992. </year>
Reference-contexts: Class 3 is designed to be used for applications like the data link layer of connectionless network protocols such as IP <ref> [10] </ref>. the block diagram, an interconnection fabric may be used as a switching device between nodes. Its connections to the nodes are called F Ports. Some existing Fibre Channel switches are non-blocking Clos type switches [3, 14]. The interface cards give Fibre Channel capability to nodes.
Reference: [11] <author> J. Hsieh, M. Lin, D.H.C. Du, and J.A. MacDonald. </author> <title> Performance Characteristics of HIPPI Networks. </title> , <institution> Department of Computer Science, University of Minnesota, </institution> <note> (in preparation) 1994. </note>
Reference-contexts: The ATM network had a raw communication bandwidth of 160 Mbits/sec 2 , so the application bandwidth was only one fourth of the available network bandwidth. Hsieh et al. observed a maximum bandwidth of only 92 Mbits/sec for an experimental 800 Mbits/sec HIPPI network <ref> [11] </ref>. The major cause of this poor performance was the interaction required between the host and the network interface for each network operation. We shall refer to the I/O subsystem related to network operations as the Network Subsystem.
Reference: [12] <author> H. Kanakia and D.R. Cheriton. </author> <title> The VMP Network Adapter Board (NAB): High Performance Network Communication for Multiprocessors. </title> <booktitle> In Proc., ACM SIGCOMM '88, </booktitle> <pages> pages 175-187, </pages> <month> Aug. 16-19 </month> <year> 1988. </year>
Reference-contexts: Other researchers have studied the performance of the network interface. Berenbaum et al. designed a programmable ATM host interface [7]. "Afterburner" is a network-independent interface which provides architectural support for a high-performance protocol [9]. The VMP network adapter board has on-board protocol processing to improve communication performance <ref> [12] </ref>. 2 This is not a regular ATM OC-3 rate. 1 In this paper we examine a network subsystem for an emerging high-speed network called Fibre Channel.
Reference: [13] <author> M. Lin, J. Hsieh, D.H.C. Du, J.P. Thomas, and J.A. MacDonald. </author> <title> Distributed Network Comput--ing over Local ATM Networks. </title> <type> Technical Report, </type> <institution> TR-94-17, Department of Computer Science, University of Minnesota, </institution> <month> March </month> <year> 1994. </year>
Reference-contexts: Several articles have discussed communication latency in the context of host software systems. Pasquale et al. proposed operating system support to reduce data copying across different communication domains [15]. Lin et al. found significant overhead incurred by APIs using ATM networks <ref> [13] </ref>. Clark analyzed the TCP protocol processing time and found the protocol processing time was not very significant [8]. Banks and Prudence presented an improvement for higher level protocols by reducing the number of data copies required across the system bus [6].
Reference: [14] <author> K Malavalli and B. Stovhase. </author> <title> Distributed Computing with Fibre Channel Fabric. </title> <booktitle> In Proceedings of CompCon, </booktitle> <pages> pages 269-274. </pages> <publisher> IEEE, </publisher> <month> February </month> <year> 1992. </year>
Reference-contexts: Its connections to the nodes are called F Ports. Some existing Fibre Channel switches are non-blocking Clos type switches <ref> [3, 14] </ref>. The interface cards give Fibre Channel capability to nodes. Their Fibre Channel connections are called N Ports. 2.2 System Configuration In our FC test environment, two SGI workstations were used. The source side was a 4D/320-VGX and the sink side was a 4D/340S.
Reference: [15] <author> J. Pasquale, E. Anderson, and P.K. Muller. </author> <title> Container Shipping: Operating System Support for I/O-Intensive Applications. </title> <booktitle> IEEE Computer, </booktitle> <month> March </month> <year> 1994. </year>
Reference-contexts: The device driver acts as a system agent which interacts with the network interface. Several articles have discussed communication latency in the context of host software systems. Pasquale et al. proposed operating system support to reduce data copying across different communication domains <ref> [15] </ref>. Lin et al. found significant overhead incurred by APIs using ATM networks [13]. Clark analyzed the TCP protocol processing time and found the protocol processing time was not very significant [8].
Reference: [16] <author> W. Peterson. </author> <booktitle> The VMEbus handbook. VFEA International Trade Association, 3rd Edition, </booktitle> <year> 1993. </year>
Reference-contexts: The VME CIM 250 uses block transfer mode to speed up data movement. The maximum size for each VME block transfer is 256 bytes. The theoretical bandwidth of a VME bus is 40 MBytes/sec when applying the block transfer mode <ref> [16] </ref>. The device driver uses the following on board registers to communicate with the Ancor VME CIM 250: * Command FIFO (Cmd FIFO) (First In First Out Queue) written to by the device driver. <p> From Figures 9 and 10, we observed that the application level bandwidth is dominated by the DMA phase. The theoretical bandwidth of the VME bus is 40 MBytes/sec <ref> [16] </ref>, however that bandwidth has never been accomplished. A reasonable bandwidth for a VME bus is around 25 MBytes/sec [16]. 25 In Section 3.2, we observed the latency for writing one word (4 bytes) from main memory to the interface (T w 4 ) is 0.2515 sec, if the word is <p> From Figures 9 and 10, we observed that the application level bandwidth is dominated by the DMA phase. The theoretical bandwidth of the VME bus is 40 MBytes/sec <ref> [16] </ref>, however that bandwidth has never been accomplished. A reasonable bandwidth for a VME bus is around 25 MBytes/sec [16]. 25 In Section 3.2, we observed the latency for writing one word (4 bytes) from main memory to the interface (T w 4 ) is 0.2515 sec, if the word is not at either page (4096) or block (256) boundary. <p> The theoretical bandwidth provided by VME-64 is 80 MBytes/sec <ref> [16] </ref>. Another possible solution is to put two or more interface cards in one I/O bus and stripe across them.
Reference: [17] <author> K.K. Ramakrishnan. </author> <title> Performance Considerations in Designing Network Interfaces. </title> <journal> IEEE Journal on Selected Areas in Communications, </journal> <volume> 11(2) </volume> <pages> 203-219, </pages> <month> February </month> <year> 1993. </year>
Reference-contexts: The hardware components in the host architecture includes: memory, system bus, I/O adapter, and I/O bus. Ramakrishna compared the trade-offs between Direct-Memory Access (DMA) and Programmed I/O (PIO) for moving data between host memory and the network interface on a DECstation <ref> [17] </ref>. Host software consists of the operating system, the application programming interface (API), higher level protocol processes, and the device driver for the network interface. The device driver acts as a system agent which interacts with the network interface.
Reference: [18] <author> T.M. </author> <title> Ruwart and M.T. O'Keefe. Performance Characteristics of a 100MB/second Disk Array. </title> <type> Preprint 93-123, </type> <institution> Army High Performance Computing Research Center, University of Minnesota, </institution> <month> December </month> <year> 1993. </year>
Reference-contexts: The theoretical bandwidth provided by VME-64 is 80 MBytes/sec [16]. Another possible solution is to put two or more interface cards in one I/O bus and stripe across them. This has been shown to greatly improve the performance of disk I/O subsystems <ref> [18] </ref> Since most system buses provides much higher bandwidth than standard I/O buses, the I/O buses become the communication bottleneck in high-speed network environments. Another approach to improving the communication bandwidth is to connect the network interfaces directly to the system bus.
Reference: [19] <author> M. Zitterbart, B. Stiller, </author> <title> and A.N. Tantawy. A Model for Flexible High-Performance Communicatin Subsystems. </title> <journal> IEEE Journal on Selected Areas in Communications, </journal> <volume> 11(4) </volume> <pages> 507-518, </pages> <month> May </month> <year> 1993. </year> <month> 34 </month>
Reference-contexts: Banks and Prudence presented an improvement for higher level protocols by reducing the number of data copies required across the system bus [6]. Zitterbart proposed a functional based communication model that allows applications to request individually tailored services from the network subsystem <ref> [19] </ref>. Other researchers have studied the performance of the network interface. Berenbaum et al. designed a programmable ATM host interface [7]. "Afterburner" is a network-independent interface which provides architectural support for a high-performance protocol [9].
References-found: 19

