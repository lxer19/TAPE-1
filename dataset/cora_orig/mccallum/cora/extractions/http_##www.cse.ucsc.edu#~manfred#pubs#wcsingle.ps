URL: http://www.cse.ucsc.edu/~manfred/pubs/wcsingle.ps
Refering-URL: http://www.cse.ucsc.edu/~manfred/pubs.html
Root-URL: http://www.cse.ucsc.edu
Title: Relative loss bounds for single neurons  "Worst-case loss bounds for single neurons" in Neural  
Author: David P. Helmbold Jyrki Kivinen Manfred K. Warmuth 
Note: A preliminary version has been published as  Information Processing Systems 8, MIT Press, 1996, pp. 309-315. dph@cse.ucsc.edu; supported by NSF grant CCR 9700201 Jyrki.Kivinen@cs.helsinki.fi; supported by Emil Aaltonen Foundation, the Academy of Finland, and ESPRIT Project NeuroCOLT manfred@cse.ucsc.edu; supported by ONR grant NO0014-91-J-1162 and NSF grant CCR 9700201  
Address: Santa Cruz, CA 95064 USA  P.O. Box 26 (Teollisuuskatu 23)  Finland  Santa Cruz, CA 95064 USA  
Affiliation: Department of Computer Science University of California, Santa Cruz  Department of Computer Science  University of Helsinki  Department of Computer Science University of California, Santa Cruz  
Pubnum: FIN-00014  
Abstract: We analyze and compare the well-known Gradient Descent algorithm and the more recent Exponentiated Gradient algorithm for training a single neuron with an arbitrary transfer function. Both algorithms are easily generalized to larger neural networks, and the generalization of Gradient Descent is the standard back-propagation algorithm. In this paper we prove worst-case loss bounds for both algorithms in the single neuron case. Since local minima make it difficult to prove worst-case bounds for gradient-based algorithms, we must use a loss function that prevents the formation of spurious local minima. We define such a matching loss function for any strictly increasing differentiable transfer function and prove worst-case loss bounds for any such transfer function and its corresponding matching loss. For example, the matching loss for the identity function is the square loss and the matching loss for the logistic transfer function is the entropic loss. The different forms of the two algorithms' bounds indicates that Exponentiated Gradient outperforms Gradient Descent when the inputs contain a large number of irrelevant components. Simulations on synthetic data confirm these analytical results. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> P. Auer, M. Herbster, and M. K. Warmuth, </author> <title> "Exponentially many local minima for single neurons," </title> <booktitle> in Advances in Neural Information Processing Systems 8, </booktitle> <address> Cambridge, MA: </address> <publisher> MIT Press, </publisher> <year> 1996, </year> <pages> pp. 316-317. </pages>
Reference-contexts: Note that using the logistic activation function with the square loss can lead to a very large number of local minima <ref> [1, 2, 3] </ref>. Even in the batch setting there are reasons to use the entropic loss with the logistic transfer function [16]. We bound the loss of the GD and EG algorithms in Section 4. <p> As noted above, the matching loss for the logistic transfer function is the entropic loss, so this pair does not create local minima. No bounded transfer function matches the square loss in this sense <ref> [1, 2, 3] </ref>, and thus it seems impossible to get the same kind of strong loss bounds for a bounded transfer function and the square loss as we have for any (increasing and differentiable) transfer function and its matching loss function. <p> Further, we assume that Z is an upper bound for the derivative of . Recall that by a distance function we mean any mapping d from R N fi R N to <ref> [0; 1] </ref> such that d (w; w) = 0 for all w, and d (w; w 0 ) &gt; 0 when w 6= w 0 . Lemma 3 Let d be an arbitrary distance function.
Reference: [2] <author> M. Budinich, </author> <title> "Some notes on perceptron learning," </title> <journal> J. Phys. A.: Math. Gen., </journal> <volume> vol. 26, </volume> <pages> pp. 4237-4247, </pages> <year> 1993. </year>
Reference-contexts: Note that using the logistic activation function with the square loss can lead to a very large number of local minima <ref> [1, 2, 3] </ref>. Even in the batch setting there are reasons to use the entropic loss with the logistic transfer function [16]. We bound the loss of the GD and EG algorithms in Section 4. <p> As noted above, the matching loss for the logistic transfer function is the entropic loss, so this pair does not create local minima. No bounded transfer function matches the square loss in this sense <ref> [1, 2, 3] </ref>, and thus it seems impossible to get the same kind of strong loss bounds for a bounded transfer function and the square loss as we have for any (increasing and differentiable) transfer function and its matching loss function.
Reference: [3] <author> M. Budinich and E. Milotti, </author> <title> "Geometrical interpretation of the back-propagation algorithm for the perceptron," </title> <journal> Physica A, </journal> <volume> vol. 185, </volume> <pages> pp. 369-377, </pages> <year> 1992. </year>
Reference-contexts: Note that using the logistic activation function with the square loss can lead to a very large number of local minima <ref> [1, 2, 3] </ref>. Even in the batch setting there are reasons to use the entropic loss with the logistic transfer function [16]. We bound the loss of the GD and EG algorithms in Section 4. <p> As noted above, the matching loss for the logistic transfer function is the entropic loss, so this pair does not create local minima. No bounded transfer function matches the square loss in this sense <ref> [1, 2, 3] </ref>, and thus it seems impossible to get the same kind of strong loss bounds for a bounded transfer function and the square loss as we have for any (increasing and differentiable) transfer function and its matching loss function.
Reference: [4] <author> N. Cesa-Bianchi, </author> <title> "Analysis of two gradient-based algorithms for on-line regression," </title> <booktitle> in Proc. 10th Annu. Workshop on Comput. Learning Theory, </booktitle> <address> New York, NY: </address> <publisher> ACM Press, </publisher> <year> 1997, </year> <pages> pp. 163-170. </pages>
Reference-contexts: More immediately, this paper follows recent results on relative (or worst-case) loss bounds for linear regression [6, 10]. Currently related work is being done on applying these techniques to classification [7] and non-matching loss functions <ref> [4] </ref> and on generalizing the results about matching loss for multidimensional outputs [11]. The next section gives additional examples of matching loss and transfer functions and provides sufficient conditions for a loss or transfer function to be part of a matching pair.
Reference: [5] <author> N. Cesa-Bianchi, Y. Freund, D. Haussler, D. P. Helmbold, R. E. Schapire, and M. K. Warmuth, </author> <title> "How to use expert advice," </title> <journal> Journal of the ACM, </journal> <volume> vol. 44, </volume> <pages> pp. 427-485, </pages> <year> 1997. </year>
Reference-contexts: In certain limited cases it has been possible to show that based on worst-case upper bounds one can in an on-line manner systematically update the learning rate so that one achieves almost as good a result as by using the optimal learning rate from the beginning <ref> [5, 6] </ref>. It is important to realize that we bound the total loss of the algorithms over any ad-versarially chosen sequence of examples where the input vectors satisfy the norm bound.
Reference: [6] <author> N. Cesa-Bianchi, P. Long, and M. K. Warmuth, </author> <title> "Worst-case quadratic loss bounds for on-line prediction of linear functions by gradient descent," </title> <journal> IEEE Transactions on Neural Networks, </journal> <volume> vol. 7, </volume> <pages> pp. 604-619, </pages> <year> 1996. </year>
Reference-contexts: Multi-layer networks remain an interesting area for future study. Our work follows the path opened by Littlestone [12] with his work on learning thresh-olded neurons with sparse weight vectors. More immediately, this paper follows recent results on relative (or worst-case) loss bounds for linear regression <ref> [6, 10] </ref>. Currently related work is being done on applying these techniques to classification [7] and non-matching loss functions [4] and on generalizing the results about matching loss for multidimensional outputs [11]. <p> The full proof of Theorem 1 is given in the appendix. Here we give a brief outline of the main methods, which are direct generalizations of those applied for the case of identity transfer function and square loss <ref> [6, 10] </ref>. As in the motivation for the algorithms (Section 3), the distance function d plays a key role in the analysis. For the GD algorithm, we again use the squared Euclidean distance d (u; w) = 1 2 jju wjj 2 2 . <p> In certain limited cases it has been possible to show that based on worst-case upper bounds one can in an on-line manner systematically update the learning rate so that one achieves almost as good a result as by using the optimal learning rate from the beginning <ref> [5, 6] </ref>. It is important to realize that we bound the total loss of the algorithms over any ad-versarially chosen sequence of examples where the input vectors satisfy the norm bound. <p> As long as we lack matching lower bounds, we have no guarantee that the bounds are tight. Thus, the difference we have observed between the algorithms might be just an artifact of our proof technique. For the linear case some good lower bounds are known <ref> [6, 10] </ref>, but for nonlinear transfer functions such bounds have not been found.
Reference: [7] <author> A. J. Grove, N. Littlestone, and D. Schuurmans, </author> <title> "General convergence results for linear discriminant updates," </title> <booktitle> in Proc. 10th Annu. Conf. on Comput. Learning Theory, </booktitle> <address> New York, NY: </address> <publisher> ACM Press, </publisher> <year> 1997, </year> <pages> pp. 171-183. </pages>
Reference-contexts: More immediately, this paper follows recent results on relative (or worst-case) loss bounds for linear regression [6, 10]. Currently related work is being done on applying these techniques to classification <ref> [7] </ref> and non-matching loss functions [4] and on generalizing the results about matching loss for multidimensional outputs [11]. The next section gives additional examples of matching loss and transfer functions and provides sufficient conditions for a loss or transfer function to be part of a matching pair.
Reference: [8] <author> D. Helmbold, R. E. Schapire, Y. Singer, and M. K. Warmuth, </author> <title> "A comparison of new and old algorithms for a mixture estimation problem," </title> <journal> Machine Learning, </journal> <volume> vol. 27, </volume> <pages> pp. 97-119, </pages> <year> 1997. </year>
Reference-contexts: This results in the following easier to compute update: w t+1 = argmin w (d (w; w t ) + (L (y t ; (w t x t )) + (((w t x t ) y t )x t (w w t )))) : (See <ref> [8, 10] </ref> for further discussion on the approximation step.) With the squared Euclidean distance this leads to the GD update (7). When d is the relative entropy then the EG update (8) results when the weight vectors are constrained to lie in P N1 .
Reference: [9] <author> D. Helmbold and M. K. Warmuth, </author> <title> "On weak learning," </title> <journal> Journal of Computer and System Sciences, </journal> <volume> vol. 50, </volume> <pages> pp. 551-573, </pages> <year> 1995. </year>
Reference-contexts: One of the reasons that we focus on on-line learning is that good on-line algorithms can always be converted into good batch algorithms <ref> [9, 10, 14] </ref>. Instead of processing all the examples at once, the training algorithm begins with some fixed start vector w 1 , and produces a sequence of weight vectors, w 1 ; : : : ; w `+1 .
Reference: [10] <author> J. Kivinen and M. K. Warmuth, </author> <title> "Additive versus exponentiated gradient updates for linear prediction," </title> <journal> Information and Computation, </journal> <volume> vol. 132, </volume> <pages> pp. 1-64, </pages> <year> 1997. </year>
Reference-contexts: One of the reasons that we focus on on-line learning is that good on-line algorithms can always be converted into good batch algorithms <ref> [9, 10, 14] </ref>. Instead of processing all the examples at once, the training algorithm begins with some fixed start vector w 1 , and produces a sequence of weight vectors, w 1 ; : : : ; w `+1 . <p> Multi-layer networks remain an interesting area for future study. Our work follows the path opened by Littlestone [12] with his work on learning thresh-olded neurons with sparse weight vectors. More immediately, this paper follows recent results on relative (or worst-case) loss bounds for linear regression <ref> [6, 10] </ref>. Currently related work is being done on applying these techniques to classification [7] and non-matching loss functions [4] and on generalizing the results about matching loss for multidimensional outputs [11]. <p> Typically, one uses the zero start vector w 1 = 0. A more recent training algorithm, called the Exponentiated Gradient (EG) algorithm <ref> [10] </ref>, uses the same gradient in a different way. This algorithm makes multiplicative (rather than additive) changes to the weight vector, and the gradient appears in the exponent. <p> Although there might be situations in which we could expect to find a good weight vector from the probability simplex, in general it is not desirable to constrain the weight vectors to be non-negative and sum to one. Hence, following <ref> [10] </ref> we introduce the Exponentiated Gradient algorithm with positive and negative weights EG that simulates negative weights 6 and relaxes the constraint that they sum to one. In addition to the learning rate , the EG algorithm has a scaling factor U &gt; 0 as a parameter. <p> A distance term is used as a kind of regularizer to ensure that the updated weight vector stays close to the previous one. Following <ref> [10] </ref> we suggest the update rule w t+1 = argmin w (d (w; w t ) + L (y t ; (w x t ))) ; (10) where is a positive parameter controlling the relative importance of the distance term versus the loss term. <p> This results in the following easier to compute update: w t+1 = argmin w (d (w; w t ) + (L (y t ; (w t x t )) + (((w t x t ) y t )x t (w w t )))) : (See <ref> [8, 10] </ref> for further discussion on the approximation step.) With the squared Euclidean distance this leads to the GD update (7). When d is the relative entropy then the EG update (8) results when the weight vectors are constrained to lie in P N1 . <p> The full proof of Theorem 1 is given in the appendix. Here we give a brief outline of the main methods, which are direct generalizations of those applied for the case of identity transfer function and square loss <ref> [6, 10] </ref>. As in the motivation for the algorithms (Section 3), the distance function d plays a key role in the analysis. For the GD algorithm, we again use the squared Euclidean distance d (u; w) = 1 2 jju wjj 2 2 . <p> A formal treatment of this appears in several places <ref> [10, 13] </ref>. Furthermore, in contrast to standard convergence proofs [15], we bound the loss on the entire sequence of examples instead of studying the convergence behavior of the algorithm when it is arbitrarily close to the best weight vector. <p> In particular, the experimental evidence confirms that for large N EG is much better than GD when the best weight vector is sparse. See Section 5 for details. The bounds we give in this paper are very similar to the bounds obtained earlier <ref> [10] </ref> for the comparison class of linear functions and the square loss. There it was first observed how the relative performances of the GD and EG algorithms relate to the norms of the input vectors and the best weight vector in the linear case. <p> In the linear case it has also been possible to obtain lower bounds that are closely related to the upper bounds we have <ref> [10] </ref>. In particular, these lower bounds include products of norms of the input and weight vectors. This is quite natural, since simply multiplying the inputs or weights by a constant would cause a corresponding increase in the square losses. <p> As long as we lack matching lower bounds, we have no guarantee that the bounds are tight. Thus, the difference we have observed between the algorithms might be just an artifact of our proof technique. For the linear case some good lower bounds are known <ref> [6, 10] </ref>, but for nonlinear transfer functions such bounds have not been found.
Reference: [11] <author> J. Kivinen and M. K. Warmuth, </author> <title> "Relative loss bounds for multidimensional regression problems," </title> <booktitle> in Advances in Neural Information Processing Systems 10, </booktitle> <address> Cambridge, MA: </address> <publisher> MIT Press, </publisher> <year> 1998, </year> <pages> pp. 287-293. </pages>
Reference-contexts: More immediately, this paper follows recent results on relative (or worst-case) loss bounds for linear regression [6, 10]. Currently related work is being done on applying these techniques to classification [7] and non-matching loss functions [4] and on generalizing the results about matching loss for multidimensional outputs <ref> [11] </ref>. The next section gives additional examples of matching loss and transfer functions and provides sufficient conditions for a loss or transfer function to be part of a matching pair. Section 3 describes and motivates the Gradient Descent and Exponentiated Gradient algorithms. <p> One such generalization would be neural networks with multiple output nodes but no hidden nodes. This is essentially the setting analyzed in <ref> [11] </ref>, and includes for example multi-class logistic regression. As the bounds for EG depend only logarithmically on the input dimension, the following approach may be feasible.
Reference: [12] <author> N. Littlestone, </author> <title> "Learning when irrelevant attributes abound: A new linear-threshold algorithm," </title> <journal> Machine Learning, </journal> <volume> vol. 2, </volume> <pages> pp. 285-318, </pages> <year> 1988. </year>
Reference-contexts: Thus it might be possible to avoid multi-layer networks by introducing many new inputs, each of which is a non-linear function of the original inputs. Multi-layer networks remain an interesting area for future study. Our work follows the path opened by Littlestone <ref> [12] </ref> with his work on learning thresh-olded neurons with sparse weight vectors. More immediately, this paper follows recent results on relative (or worst-case) loss bounds for linear regression [6, 10].
Reference: [13] <author> N. Littlestone, </author> <title> "From on-line to batch learning," </title> <booktitle> in Proc. 2nd Annu. Workshop on Comput. Learning Theory, </booktitle> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann, </publisher> <year> 1989, </year> <pages> pp. 269-284. </pages>
Reference-contexts: A formal treatment of this appears in several places <ref> [10, 13] </ref>. Furthermore, in contrast to standard convergence proofs [15], we bound the loss on the entire sequence of examples instead of studying the convergence behavior of the algorithm when it is arbitrarily close to the best weight vector.
Reference: [14] <author> N. Littlestone, </author> <title> "Mistake bounds and logarithmic linear-threshold learning algorithms," </title> <type> PhD thesis, Technical Report UCSC-CRL-89-11, </type> <institution> University of California, Santa Cruz, </institution> <year> 1989. </year>
Reference-contexts: One of the reasons that we focus on on-line learning is that good on-line algorithms can always be converted into good batch algorithms <ref> [9, 10, 14] </ref>. Instead of processing all the examples at once, the training algorithm begins with some fixed start vector w 1 , and produces a sequence of weight vectors, w 1 ; : : : ; w `+1 .
Reference: [15] <author> D. G. Luenberger, </author> <title> Linear and Nonlinear Programming, </title> <address> Reading, MA: </address> <publisher> Addison-Wesley, </publisher> <year> 1984. </year>
Reference-contexts: A formal treatment of this appears in several places [10, 13]. Furthermore, in contrast to standard convergence proofs <ref> [15] </ref>, we bound the loss on the entire sequence of examples instead of studying the convergence behavior of the algorithm when it is arbitrarily close to the best weight vector. Theorems 1 and 2 give some insight into whether GD or EG is likely to be better in particular situations.
Reference: [16] <author> S. A. Solla, E. Levin, and M. Fleisher, </author> <title> "Accelerated learning in layered neural networks," </title> <journal> Complex Systems, </journal> <volume> vol. 2, </volume> <pages> pp. 625-639, </pages> <year> 1988. </year> <month> 23 </month>
Reference-contexts: Note that using the logistic activation function with the square loss can lead to a very large number of local minima [1, 2, 3]. Even in the batch setting there are reasons to use the entropic loss with the logistic transfer function <ref> [16] </ref>. We bound the loss of the GD and EG algorithms in Section 4. How much our bounds on the losses of the two algorithms exceed the least empirical loss depends on the maximum slope of the transfer function used.
References-found: 16

