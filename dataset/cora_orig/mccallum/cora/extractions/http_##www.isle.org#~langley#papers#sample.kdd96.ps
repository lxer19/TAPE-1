URL: http://www.isle.org/~langley/papers/sample.kdd96.ps
Refering-URL: http://www.isle.org/~langley/pubs.html
Root-URL: 
Email: fgjohn,langleyg@CS.Stanford.EDU  
Title: Static Versus Dynamic Sampling for Data Mining  
Author: George H. John and Pat Langley 
Note: In Proceedings of the Second International Conference on Knowledge Discovery in Databases and Data Mining, AAAI/MIT Press, 1996  
Web: http://robotics.stanford.edu/~fgjohn,langleyg/  
Address: Stanford, CA 94305-9010  
Affiliation: Computer Science Department Stanford University  
Abstract: As data warehouses grow to the point where one hundred gigabytes is considered small, the computational efficiency of data-mining algorithms on large databases becomes increasingly important. Using a sample from the database can speed up the data-mining process, but this is only acceptable if it does not reduce the quality of the mined knowledge. To this end, we introduce the "Probably Close Enough" criterion to describe the desired properties of a sample. Sampling usually refers to the use of static statistical tests to decide whether a sample is sufficiently similar to the large database, in the absence of any knowledge of the tools the data miner intends to use. We discuss dynamic sampling methods, which take into account the mining tool being used and can thus give better samples. We describe dynamic schemes that observe a mining tool's performance on training samples of increasing size and use these results to determine when a sample is sufficiently large. We evaluate these sampling methods on data from the UCI repository and conclude that dynamic sampling is preferable. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Casella, G. & Berger, R. L. </author> <year> (1990), </year> <title> Statistical Infer ence, </title> <publisher> Wadsworth & Brooks/Cole. </publisher>
Reference-contexts: These both look the same to univariate static sampling. Note that hypothesis tests are usually designed to minimize the probability of falsely claiming that two distributions are different <ref> (Casella & Berger 1990) </ref>. For example, in a 95% level hypothesis test, assuming the two samples do come from the same distribution, there is a 5% chance that the test will incorrectly reject (type I error) the null hypothesis that the distributions are the same.
Reference: <author> Catlett, J. </author> <year> (1992), </year> <title> Peepholing: Choosing attributes efficiently for megainduction, </title> <booktitle> in Machine Learning: Proceedings of the Ninth International Workshop, </booktitle> <publisher> Morgan Kaufmann, </publisher> <pages> pp. 49-54. </pages>
Reference: <author> Kadie, C. </author> <year> (1995), </year> <title> SEER: Maximum likelihood regres sion for learning-speed curves, </title> <type> PhD thesis, </type> <institution> Computer Science Department, University of Illinois at Urbana-Champaign. </institution>
Reference: <author> Kohavi, R. </author> <year> (1995), </year> <title> Wrappers for performance enhance ment and oblivious decision graphs, </title> <type> PhD thesis, </type> <institution> Computer Science Department, Stanford University, </institution> <address> Stan-ford, CA. </address>
Reference: <author> Langley, P. & Sage, S. </author> <year> (1994), </year> <title> Induction of selective Bayesian classifiers, </title> <booktitle> in Proceedings of the Tenth Conference on Uncertainty in Artificial Intelligence, </booktitle> <publisher> Mor-gan Kaufmann, </publisher> <address> Seattle, WA, </address> <pages> pp. 399-406. </pages>
Reference: <author> Langley, P., Iba, W. & Thompson, K. </author> <year> (1992), </year> <title> An anal ysis of Bayesian classifiers, </title> <booktitle> in Proceedings of the Tenth National Conference on Artificial Intelligence, </booktitle> <publisher> AAAI Press/MIT Press, </publisher> <pages> pp. 223-228. </pages>
Reference-contexts: Also, experimental studies suggest that naive Bayes tends to learn more rapidly, in terms of the number of training cases needed to achieve high accuracy, than most induction algorithms (Lang-ley & Sage 1994). Theoretical analyses <ref> (Langley, Iba & Thompson 1992) </ref> point to similar conclusions about the naive Bayesian classifier's rate of learning. A third feature is that naive Bayes can be implemented in an incremental manner that is not subject to order effects.
Reference: <author> Moller, M. </author> <year> (1993), </year> <title> "Supervised learning on large re dundant training sets", </title> <journal> International Journal of Neural Systems 4(1), </journal> <month> March, </month> <pages> 15-25. </pages>
Reference-contexts: Very large real databases also have high redundancy <ref> (Moller 1993) </ref>, so we do believe the results of these experiments will be informative, although real large databases would obviously have been preferable.
Reference: <author> Moore, A. & Lee, M. </author> <year> (1994), </year> <title> Efficient algorithms for minimizing cross-validation error, </title> <booktitle> in Machine Learning: Proceedings of the Eleventh International Conference, </booktitle> <publisher> Morgan Kaufmann, </publisher> <pages> pp. 190-198. </pages>
Reference: <author> Yuret, D. </author> <year> (1994), </year> <title> From genetic algorithms to efficient optimization, </title> <type> Master's thesis, </type> <institution> Computer Science Department, Massachusetts Institute of Technology. </institution>
Reference-contexts: We used Dynamic Hill Climbing <ref> (Yuret 1994) </ref>, which seems to work well. We know N , the total size of the database.
References-found: 9

