URL: http://www.cs.brown.edu/people/tld/postscript/DeanBasyeandShewchukMK-92.ps
Refering-URL: http://www.cs.brown.edu/research/ai/publications/
Root-URL: http://www.cs.brown.edu/
Title: Chapter 1 Reinforcement Learning for Planning and Control  
Author: Thomas Dean Ken Basye John Shewchuk 
Abstract-found: 0
Intro-found: 1
Reference: <author> James S. Albus. </author> <title> A new approach to manipulator control: The cerebellar model articulation controller (CMAC). Journal of Dynamic Systems, Measurement, </title> <journal> and Control, </journal> <volume> 97 </volume> <pages> 270-277, </pages> <year> 1975. </year>
Reference: <author> Andrew G. Barto, Richard S. Sutton, and Charles W. Anderson. </author> <title> Neu-ronlike adaptive elements that can solve difficult learning control problems. </title> <journal> IEEE Transactions on Systems, Man, and Cybernetics, </journal> <volume> 13(5) </volume> <pages> 835-846, </pages> <year> 1983. </year>
Reference: <author> Andrew G. Barto, Richard S. Sutton, and Christopher J. C. H. Watkins. </author> <title> Learning and sequential decision making. </title> <editor> In Michael Gabriel and John Moore, editors, </editor> <booktitle> Learning and Computational Neuroscience: Foundations of Adaptive Networks. </booktitle> <publisher> MIT Press, </publisher> <address> Cambridge, Mas-sachusetts, </address> <year> 1990. </year>
Reference: <author> Richard Bellman. </author> <title> Dynamic Programming. </title> <publisher> Princeton University Press, </publisher> <year> 1957. </year>
Reference: <author> Richard Bellman and Stuart Dreyfus. </author> <title> Applied Dynamic Programming. </title> <publisher> Princeton University Press, </publisher> <address> Princeton, New Jersey, </address> <year> 1962. </year>
Reference: <author> L. Breiman, J. H. Friedman, R. A. Olshen, and C. J. Stone. </author> <title> Classification and Regression Trees. </title> <publisher> Wadsworth and Brooks, </publisher> <address> Monterey, California, </address> <year> 1984. </year>
Reference: <author> David Chapman and Leslie Kaelbling. </author> <title> Input generalization in delayed reinforcement learning: An algorithm and performance comparisons. </title> <booktitle> In Proceedings IJCAI 12. </booktitle> <address> IJCAII, </address> <year> 1991. </year>
Reference: <author> David Cohn, Les Atlas, Richard Ladner, M. A. El-Sharkawi, R. J. </author> <title> Reinforcement Learning 19 Marks, M.E. </title> <editor> Aggoune, and D. C. </editor> <title> Park. Training connectionist networks with queries and selective sampling. </title> <booktitle> In Neural Information Processing Systems. </booktitle> <publisher> Morgan Kaufmann, </publisher> <year> 1990. </year>
Reference: <author> Thomas Dean and Michael Wellman. </author> <title> Planning and Control. </title> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, California, </address> <year> 1991. </year>
Reference: <author> R. O. Duda and P. E. Hart. </author> <title> Pattern Classification and Scene Analysis. </title> <publisher> John Wiley and Sons, </publisher> <address> New York, </address> <year> 1973. </year>
Reference-contexts: Shewchuk has developed methods for partitioning the state and action spaces to optimize the use of available storage based on variants of k-means and nearest-neighbor clustering techniques. All of these reinforcement methods are closely related to the unsupervised learning methods used in pattern recognition for statistical clustering <ref> (Duda and Hart, 1973) </ref>. Even relatively simple tasks (e.g., the task faced by a mobile robot in entering a particular room and docking with a charging unit) can require a training period that would challenge the most patient of teachers.
Reference: <author> G. C. Goodwin and K. S. </author> <title> Sin. Adaptive Filtering Prediction and Control. </title> <publisher> Prentice-Hall, </publisher> <address> Englewood Cliffs, New Jersey, </address> <year> 1984. </year>
Reference-contexts: In the first approach, the controller attempts to learn the transition probabilities and rewards, and then constructs an optimal policy o*ine using a method such as Bellman's value iteration. We call this approach the explicit-model approach and it is one of the standard methods used in adaptive control <ref> (Goodwin & Sin, 1984) </ref>. In the second approach, the controller attempts to learn an optimal policy by constructing an evaluation function to use in selecting the best action to take when in a given state.
Reference: <author> Ronald A. Howard. </author> <title> Dynamic Programming and Markov Processes. </title> <publisher> MIT Press, </publisher> <address> Cambridge, Massachusetts, </address> <year> 1960. </year>
Reference: <author> Long-Ji Lin. </author> <title> Programming robots using reinforcement learning and teaching. </title> <booktitle> In Proceedings AAAI-91. AAAI, </booktitle> <year> 1991. </year>
Reference: <author> Sridhar Mahadevan and Jonathan Connell. </author> <title> Automatic programming of behavior-based robots using reinforcement learning. </title> <booktitle> In Proceedings AAAI-91. AAAI, </booktitle> <year> 1991. </year>
Reference: <author> John E. Moody. </author> <title> Fast learning in multi-resolution hierarchies. </title> <editor> In David Touretsky, editor, </editor> <booktitle> Advances in Neural Information Processing. </booktitle> <publisher> Morgan-Kaufmann, </publisher> <address> Los Altos, California, </address> <year> 1989. </year>
Reference: <author> Tomaso Poggio and Federico Girosi. </author> <title> A theory of networks for approximation and learning. </title> <type> Technical Report AI Memo No. 1140, </type> <institution> MIT AI Laboratory, </institution> <year> 1989. </year>
Reference: <author> J. R. Quinlan. </author> <title> Induction of decision trees. </title> <journal> Machine Learning, </journal> <volume> 1(1) </volume> <pages> 81-106, </pages> <year> 1986. </year>
Reference: <author> John Shewchuk. </author> <title> Reinforcement Learning in Dynamical Systems With High Input and Output Dimensionality. </title> <type> PhD thesis, </type> <institution> Brown University, </institution> <address> Providence, RI, </address> <publisher> Forthcoming. </publisher>
Reference: <author> John Shewchuk and Thomas Dean. </author> <title> Towards learning time-varying functions with high input dimensionality. </title> <booktitle> In Proceedings of the Fifth IEEE International Symposium on Intelligent Control, </booktitle> <pages> pages 383-388. </pages> <publisher> IEEE, </publisher> <year> 1990. </year>
Reference: <author> Richard S. Sutton. </author> <title> Learning to predict by the methods of temporal differences. </title> <journal> Machine Learning, </journal> <volume> 3 </volume> <pages> 9-44, </pages> <year> 1988. </year>
Reference: <author> Richard S. Sutton. </author> <title> Integrated architectures for learning, planning, and reacting based on approximating dynamic programming. </title> <booktitle> In Proceedings 7th International Conference on Machine Learning, </booktitle> <year> 1990. </year> <note> 20 T. </note> <author> Dean, K. Basye and J. Shewchuk A. N. </author> <title> Tikhonov. Solution of incorrectly formulated problems and the regularization method. </title> <journal> Soviet Math. Dokl., </journal> <volume> 4 </volume> <pages> 1035-1038, </pages> <year> 1963. </year>
Reference: <author> C. J. C. H Watkins. </author> <title> Learning from Delayed Rewards. </title> <type> PhD thesis, </type> <institution> Cambridge University, </institution> <year> 1989. </year>
Reference: <author> Steven D. Whitehead. </author> <title> A complexity analysis of cooperative mechanisms in reinforcement learning. </title> <booktitle> In Proceedings AAAI-91. AAAI, </booktitle> <year> 1991. </year>
Reference: <author> B. Widrow and M. E. Hoff. </author> <title> Adaptive switching circuits. </title> <booktitle> In 1960 WESCON Convention Record Part IV, </booktitle> <editor> (Reprinted in J. A. Ander-son and E. Rosenfeld, Neurocomputing: </editor> <booktitle> Foundations of Research, </booktitle> <publisher> The MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1988), </year> <pages> pages 96-104, </pages> <year> 1960. </year>
References-found: 24

