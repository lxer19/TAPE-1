URL: http://www.robotics.stanford.edu/~birch/publications/headtracker_cvpr1998.ps.gz
Refering-URL: http://www.robotics.stanford.edu/~birch/headtracker/
Root-URL: http://www.robotics.stanford.edu
Email: birchfield@cs.stanford.edu  
Title: Elliptical Head Tracking Using Intensity Gradients and Color Histograms  
Author: Stan Birchfield 
Note: c fl1998 IEEE. Personal use of this material is permitted. However, permission to reprint/republish this material for advertising or promotional purposes or for creating new collective works for resale or redistribution to servers or lists, or to reuse any copyrighted component of this work in other works, must be obtained from the IEEE.  
Address: Stanford, CA 94305  
Affiliation: Computer Science Department Stanford University  
Abstract: An algorithm for tracking a person's head is presented. The head's projection onto the image plane is modeled as an ellipse whose position and size are continually updated by a local search combining the output of a module concentrating on the intensity gradient around the ellipse's perimeter with that of another module focusing on the color histogram of the ellipse's interior. Since these two modules have roughly orthogonal failure modes, they serve to complement one another. The result is a robust, real-time system that is able to track a person's head with enough accuracy to automatically control the camera's pan, tilt, and zoom in order to keep the person centered in the field of view at a desired size. Extensive experimentation shows the algorithm's robustness with respect to full 360-degree out-of-plane rotation, up to 90-degree tilting, severe but brief occlusion, arbitrary camera movement, and multiple moving people in the background. IEEE Conference on Computer Vision and Pattern Recognition, Santa Barbara, California, June 1998 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> A. M. Baumberg and D. C. Hogg. </author> <title> An efficient method for contour tracking using active shape models. </title> <booktitle> In Proceedings of the IEEE Workshop on Motion of Non-Rigid and Articulated Objects, </booktitle> <pages> pages 194-199, </pages> <year> 1994. </year>
Reference-contexts: Except for the fixed shape of the object's perimeter, the above formulation is nearly identical to that employed by most contour trackers <ref> [1, 3] </ref>. One minor difference is that the gradient is summed around the entire perimeter rather than just at select points. <p> However, besides the additional hardware, it is not clear whether this system would be able to handle multiple people at a similar depth as the subject. Also, promising results have been achieved using a shape-based contour tracker <ref> [1] </ref> that is more sophisticated than ours because it allows the shape to deform over time. However, in its present implementation the tracking criterion is the gradient magnitude alone, which will probably fail with quick movements in cluttered scenes.
Reference: [2] <author> S. Birchfield. </author> <title> An elliptical head tracker. </title> <booktitle> In Proc. of the 31st Asilomar Conf. on Signals, Systems and Computers, </booktitle> <year> 1997. </year>
Reference-contexts: The search space S is the set of all states within some range of the predicted location, using velocity prediction <ref> [2] </ref>. Somewhat surprisingly, this simple prediction scheme greatly improves the behavior of the tracker because it removes any restriction on the maximum lateral velocity of the subject only the amount of acceleration is limited. <p> ways in which they complement each other, and the robust behavior achieved with the complete system. 5.1 Gradient module alone It is somewhat surprising that the simple gradient module is sufficient to control the camera's pan and tilt in order to track a person walking around an untextured, unmodified room <ref> [2] </ref>. Even in the rather cluttered environment shown in Figure 1, the gradient module was able to consistently track the subject's slowly-moving head for about fifty pixels or so of image motion before becoming distracted by the background (The gradient magnitude performed slightly worse than the gradient dot product).
Reference: [3] <author> A. Blake, R. Curwen, and A. Zisserman. </author> <title> A framework for spatiotemporal control in the tracking of visual contours. </title> <journal> Intl. Journal of Computer Vision, </journal> <volume> 11(2) </volume> <pages> 127-145, </pages> <year> 1993. </year>
Reference-contexts: Except for the fixed shape of the object's perimeter, the above formulation is nearly identical to that employed by most contour trackers <ref> [1, 3] </ref>. One minor difference is that the gradient is summed around the entire perimeter rather than just at select points.
Reference: [4] <author> P. Fieguth and D. Terzopoulos. </author> <title> Color-based tracking of heads and other mobile objects at video frame rates. </title> <booktitle> In Proc. of the IEEE CVPR, </booktitle> <pages> pages 21-27, </pages> <year> 1997. </year>
Reference-contexts: We will use the notation s = (x; y; ) for the head's state or location. 1 The tracking task is to update the state by finding the location whose image values best match the values in the model. This is accomplished via a hypothesize-and-test procedure <ref> [4, 7] </ref> in which the goodness of the match is dependent upon the intensity gradients around the object's boundary and the color histogram of the object's interior: s fl = arg max f g (s i ) + c (s i )g; (1) 1 Throughout this paper, the term position refers <p> One minor difference is that the gradient is summed around the entire perimeter rather than just at select points. A more significant difference is that the current hypothesize-and-test paradigm <ref> [4, 7] </ref> allows all of the data to be examined before a decision is made, in contrast to the typical contour tracker in which each control point independently decides how to move based on purely local information. <p> former is converted to a percentage by subtracting the minimum and dividing by the range: g (s) = max s i 2S g (s i ) min s i 2S g (s i ) 4 Color Module Many researchers have exploited the relative uniqueness of skin color to track faces <ref> [4, 5, 9, 15, 16] </ref>. A weakness of these systems is their heavy reliance upon skin color that forbids skin-colored objects in the background and, more importantly, forbids the subject from turning around so that the back of his head, rather than his face, is visible. <p> Some researchers have ignored luminance information completely <ref> [4, 9] </ref>, but this is dangerous with out-of-plane rotation because, based on chrominance alone, dark brown hair looks similar to a white wall. <p> Template- and neural network-based trackers [6, 9, 11, 18], as well as trackers based on facial color <ref> [4, 5, 9, 15, 16, 18] </ref>, cannot handle severe out-of-plane rotation because such a rotation causes the face to disappear. The color-based techniques also tend to have difficulty with skin-colored objects or other people in the background.
Reference: [5] <author> H. P. Graf, E. Cosatto, D. Gibbon, M. Kocheisen, and E. Petajan. </author> <title> Multi-modal system for locating heads and faces. </title> <booktitle> In Proc. of the Second Intl. Conference on Automatic Face and Gesture Recognition, </booktitle> <pages> pages 88-93, </pages> <year> 1996. </year>
Reference-contexts: former is converted to a percentage by subtracting the minimum and dividing by the range: g (s) = max s i 2S g (s i ) min s i 2S g (s i ) 4 Color Module Many researchers have exploited the relative uniqueness of skin color to track faces <ref> [4, 5, 9, 15, 16] </ref>. A weakness of these systems is their heavy reliance upon skin color that forbids skin-colored objects in the background and, more importantly, forbids the subject from turning around so that the back of his head, rather than his face, is visible. <p> Template- and neural network-based trackers [6, 9, 11, 18], as well as trackers based on facial color <ref> [4, 5, 9, 15, 16, 18] </ref>, cannot handle severe out-of-plane rotation because such a rotation causes the face to disappear. The color-based techniques also tend to have difficulty with skin-colored objects or other people in the background. <p> The color-based techniques also tend to have difficulty with skin-colored objects or other people in the background. Trackers utilizing some form of background differencing <ref> [5, 10, 11, 12, 18, 19, 20] </ref> either require a static camera or restrict the camera's motion to rotation about its focal point. 3 Moreover, many of these techniques perform motion-based figure-ground segmentation, which tends to fail when the camera zooms or when multiple objects move 3 In [10], the camera
Reference: [6] <author> G. D. Hager and P. N. Belhumeur. </author> <title> Real-time tracking of image regions with changes in geometry and illumination. </title> <booktitle> In Proc. of the IEEE CVPR, </booktitle> <pages> pages 403-410, </pages> <year> 1996. </year>
Reference-contexts: Template- and neural network-based trackers <ref> [6, 9, 11, 18] </ref>, as well as trackers based on facial color [4, 5, 9, 15, 16, 18], cannot handle severe out-of-plane rotation because such a rotation causes the face to disappear. The color-based techniques also tend to have difficulty with skin-colored objects or other people in the background.
Reference: [7] <author> R. M. Haralick and L. G. Shapiro. </author> <title> Computer and Robot Vision, </title> <booktitle> volume 2. </booktitle> <address> Reading, Mass.: </address> <publisher> Addison-Wesley, </publisher> <year> 1993. </year>
Reference-contexts: We will use the notation s = (x; y; ) for the head's state or location. 1 The tracking task is to update the state by finding the location whose image values best match the values in the model. This is accomplished via a hypothesize-and-test procedure <ref> [4, 7] </ref> in which the goodness of the match is dependent upon the intensity gradients around the object's boundary and the color histogram of the object's interior: s fl = arg max f g (s i ) + c (s i )g; (1) 1 Throughout this paper, the term position refers <p> One minor difference is that the gradient is summed around the entire perimeter rather than just at select points. A more significant difference is that the current hypothesize-and-test paradigm <ref> [4, 7] </ref> allows all of the data to be examined before a decision is made, in contrast to the typical contour tracker in which each control point independently decides how to move based on purely local information.
Reference: [8] <author> F. </author> <title> Hausdorff. Set Theory. </title> <address> New York: </address> <publisher> Chelsea Publishing Company, </publisher> <address> third edition, </address> <year> 1978. </year>
Reference-contexts: According to elementary set theory, every closed set in the plane can be decomposed into two disjoint sets: the boundary and the interior <ref> [8] </ref>. Since these two sets are complementary (in the true, mathematical sense), it stands to reason that the failure modes of a tracking module focusing on the object's boundary will be orthogonal to those of a module focusing on the object's interior.
Reference: [9] <author> M. Hunke and A. Waibel. </author> <title> Face locating and tracking for human-computer interaction. </title> <booktitle> In Proc. of the 28th Asilo-mar Conf. on Signals, Systems and Computers, </booktitle> <pages> pages 1277-1281, </pages> <year> 1994. </year>
Reference-contexts: former is converted to a percentage by subtracting the minimum and dividing by the range: g (s) = max s i 2S g (s i ) min s i 2S g (s i ) 4 Color Module Many researchers have exploited the relative uniqueness of skin color to track faces <ref> [4, 5, 9, 15, 16] </ref>. A weakness of these systems is their heavy reliance upon skin color that forbids skin-colored objects in the background and, more importantly, forbids the subject from turning around so that the back of his head, rather than his face, is visible. <p> Some researchers have ignored luminance information completely <ref> [4, 9] </ref>, but this is dangerous with out-of-plane rotation because, based on chrominance alone, dark brown hair looks similar to a white wall. <p> Template- and neural network-based trackers <ref> [6, 9, 11, 18] </ref>, as well as trackers based on facial color [4, 5, 9, 15, 16, 18], cannot handle severe out-of-plane rotation because such a rotation causes the face to disappear. The color-based techniques also tend to have difficulty with skin-colored objects or other people in the background. <p> Template- and neural network-based trackers [6, 9, 11, 18], as well as trackers based on facial color <ref> [4, 5, 9, 15, 16, 18] </ref>, cannot handle severe out-of-plane rotation because such a rotation causes the face to disappear. The color-based techniques also tend to have difficulty with skin-colored objects or other people in the background.
Reference: [10] <author> D. P. Huttenlocher, J. J. Noh, and W. J. Rucklidge. </author> <title> Tracking non-rigid objects in complex scenes. </title> <booktitle> In Proc. of the 4th Intl. Conference on Computer Vision, </booktitle> <pages> pages 93-101, </pages> <year> 1993. </year>
Reference-contexts: The color-based techniques also tend to have difficulty with skin-colored objects or other people in the background. Trackers utilizing some form of background differencing <ref> [5, 10, 11, 12, 18, 19, 20] </ref> either require a static camera or restrict the camera's motion to rotation about its focal point. 3 Moreover, many of these techniques perform motion-based figure-ground segmentation, which tends to fail when the camera zooms or when multiple objects move 3 In [10], the camera <p> background differencing [5, 10, 11, 12, 18, 19, 20] either require a static camera or restrict the camera's motion to rotation about its focal point. 3 Moreover, many of these techniques perform motion-based figure-ground segmentation, which tends to fail when the camera zooms or when multiple objects move 3 In <ref> [10] </ref>, the camera may move occasionally but not continuously. in the scene. Reliable tracking was reported by combining a template-based tracker with stereo depth [14].
Reference: [11] <author> S. McKenna and S. Gong. </author> <title> Tracking faces. </title> <booktitle> In Proc. of the Second International Conference on Automatic Face and Gesture Recognition, </booktitle> <pages> pages 271-276, </pages> <year> 1996. </year>
Reference-contexts: Template- and neural network-based trackers <ref> [6, 9, 11, 18] </ref>, as well as trackers based on facial color [4, 5, 9, 15, 16, 18], cannot handle severe out-of-plane rotation because such a rotation causes the face to disappear. The color-based techniques also tend to have difficulty with skin-colored objects or other people in the background. <p> The color-based techniques also tend to have difficulty with skin-colored objects or other people in the background. Trackers utilizing some form of background differencing <ref> [5, 10, 11, 12, 18, 19, 20] </ref> either require a static camera or restrict the camera's motion to rotation about its focal point. 3 Moreover, many of these techniques perform motion-based figure-ground segmentation, which tends to fail when the camera zooms or when multiple objects move 3 In [10], the camera
Reference: [12] <author> D. Murray and A. Basu. </author> <title> Motion tracking with an active camera. </title> <journal> IEEE Transactions on Pattern Analysis and Machine Intelligence, </journal> <volume> 16(5) </volume> <pages> 449-459, </pages> <month> May </month> <year> 1994. </year>
Reference-contexts: The color-based techniques also tend to have difficulty with skin-colored objects or other people in the background. Trackers utilizing some form of background differencing <ref> [5, 10, 11, 12, 18, 19, 20] </ref> either require a static camera or restrict the camera's motion to rotation about its focal point. 3 Moreover, many of these techniques perform motion-based figure-ground segmentation, which tends to fail when the camera zooms or when multiple objects move 3 In [10], the camera
Reference: [13] <author> H. K. Nishihara. </author> <type> Personal Communication, </type> <year> 1996. </year>
Reference-contexts: A more sophisticated measure than the one in (2) is the one proposed by Nishihara <ref> [13] </ref>. Rather than just desiring large gradient magnitudes around the perimeter, it also desires the gradient direction to be perpendicular to the perimeter: g (s) = N i=1 where n (i) is the unit vector normal to the ellipse at pixel i and () denotes the dot product.
Reference: [14] <author> H. K. Nishihara, H. J. Thomas, and E. Huber. </author> <title> Real-time tracking of people using stereo and motion. </title> <booktitle> In SPIE Proceedings, </booktitle> <volume> volume 2183, </volume> <pages> pages 266-273, </pages> <year> 1994. </year>
Reference-contexts: Reliable tracking was reported by combining a template-based tracker with stereo depth <ref> [14] </ref>. However, besides the additional hardware, it is not clear whether this system would be able to handle multiple people at a similar depth as the subject.
Reference: [15] <author> Y. Raja, S. J. McKenna, and S. Gong. </author> <title> Segmentation and tracking using colour mixture models. </title> <booktitle> In Proceedings of the 3rd Asian Conference on Computer Vision, </booktitle> <volume> volume I, </volume> <pages> pages 607-614, </pages> <year> 1998. </year>
Reference-contexts: former is converted to a percentage by subtracting the minimum and dividing by the range: g (s) = max s i 2S g (s i ) min s i 2S g (s i ) 4 Color Module Many researchers have exploited the relative uniqueness of skin color to track faces <ref> [4, 5, 9, 15, 16] </ref>. A weakness of these systems is their heavy reliance upon skin color that forbids skin-colored objects in the background and, more importantly, forbids the subject from turning around so that the back of his head, rather than his face, is visible. <p> Template- and neural network-based trackers [6, 9, 11, 18], as well as trackers based on facial color <ref> [4, 5, 9, 15, 16, 18] </ref>, cannot handle severe out-of-plane rotation because such a rotation causes the face to disappear. The color-based techniques also tend to have difficulty with skin-colored objects or other people in the background.
Reference: [16] <author> K. Sobottka and I. Pitas. </author> <title> Segmentation and tracking of faces in color images. </title> <booktitle> In Proc. of the Second Intl. Conf. on Automatic Face and Gesture Recognition, </booktitle> <pages> pages 236-241, </pages> <year> 1996. </year>
Reference-contexts: former is converted to a percentage by subtracting the minimum and dividing by the range: g (s) = max s i 2S g (s i ) min s i 2S g (s i ) 4 Color Module Many researchers have exploited the relative uniqueness of skin color to track faces <ref> [4, 5, 9, 15, 16] </ref>. A weakness of these systems is their heavy reliance upon skin color that forbids skin-colored objects in the background and, more importantly, forbids the subject from turning around so that the back of his head, rather than his face, is visible. <p> Template- and neural network-based trackers [6, 9, 11, 18], as well as trackers based on facial color <ref> [4, 5, 9, 15, 16, 18] </ref>, cannot handle severe out-of-plane rotation because such a rotation causes the face to disappear. The color-based techniques also tend to have difficulty with skin-colored objects or other people in the background.
Reference: [17] <author> M. Swain and D. Ballard. </author> <title> Color indexing. </title> <journal> International Journal of Computer Vision, </journal> <volume> 7(1) </volume> <pages> 11-32, </pages> <year> 1991. </year>
Reference-contexts: The color of human heads is complex, however, being at the very least bimodal due to the skin and hair, and any system attempting to handle out-of-plane rotation must address this issue. The color histogram <ref> [17] </ref> is well suited to this task because of its ability to implicitly capture complex, multimodal patterns of color. Moreover, because it disregards all geometric information, it remains relatively invariant to many complicated, non-rigid motions. The procedure is as follows. <p> Then, at run time, the histogram intersection <ref> [17] </ref> is computed between the model histogram M and the image histogram I at each hypothesized location: 2 c (s) = i=1 min (I s (i); M (i)) i=1 I s (i) where I s (i) and M (i) are the numbers of pixels in the ith bin of the histograms, <p> Our color space consists of scaled versions of the three axes B G, GR, and B +G+R. The first two contain the chrominance information and are sampled into eight bins 2 This equation is identical to the one in <ref> [17] </ref>. <p> our goal is to match a single model to the best image patch, rather than to match a single image patch to the best model. (b) Horizontal and (c) vertical components of gradient. each, while the last one contains the luminance information and is sampled more coarsely into four bins <ref> [17] </ref>. Some researchers have ignored luminance information completely [4, 9], but this is dangerous with out-of-plane rotation because, based on chrominance alone, dark brown hair looks similar to a white wall.
Reference: [18] <author> K. Toyama and G. D. Hager. </author> <title> Incremental focus of attention for robust visual tracking. </title> <booktitle> In CVPR, </booktitle> <pages> pages 189-195, </pages> <year> 1996. </year>
Reference-contexts: Template- and neural network-based trackers <ref> [6, 9, 11, 18] </ref>, as well as trackers based on facial color [4, 5, 9, 15, 16, 18], cannot handle severe out-of-plane rotation because such a rotation causes the face to disappear. The color-based techniques also tend to have difficulty with skin-colored objects or other people in the background. <p> Template- and neural network-based trackers [6, 9, 11, 18], as well as trackers based on facial color <ref> [4, 5, 9, 15, 16, 18] </ref>, cannot handle severe out-of-plane rotation because such a rotation causes the face to disappear. The color-based techniques also tend to have difficulty with skin-colored objects or other people in the background. <p> The color-based techniques also tend to have difficulty with skin-colored objects or other people in the background. Trackers utilizing some form of background differencing <ref> [5, 10, 11, 12, 18, 19, 20] </ref> either require a static camera or restrict the camera's motion to rotation about its focal point. 3 Moreover, many of these techniques perform motion-based figure-ground segmentation, which tends to fail when the camera zooms or when multiple objects move 3 In [10], the camera
Reference: [19] <author> J. I. Woodfill. </author> <title> Motion Vision and Tracking for Robots in Dynamic, Unstructured Environments. </title> <type> PhD thesis, </type> <institution> Stanford University, </institution> <year> 1992. </year>
Reference-contexts: The color-based techniques also tend to have difficulty with skin-colored objects or other people in the background. Trackers utilizing some form of background differencing <ref> [5, 10, 11, 12, 18, 19, 20] </ref> either require a static camera or restrict the camera's motion to rotation about its focal point. 3 Moreover, many of these techniques perform motion-based figure-ground segmentation, which tends to fail when the camera zooms or when multiple objects move 3 In [10], the camera
Reference: [20] <author> C. R. Wren, A. Azarbayejani, T. Darrell, and A. P. Pent-land. Pfinder: </author> <title> Real-time tracking of the human body. </title> <journal> IEEE Transactions on Pattern Analysis and Machine Intelligence, </journal> <volume> 19(7) </volume> <pages> 780-785, </pages> <year> 1997. </year>
Reference-contexts: The color-based techniques also tend to have difficulty with skin-colored objects or other people in the background. Trackers utilizing some form of background differencing <ref> [5, 10, 11, 12, 18, 19, 20] </ref> either require a static camera or restrict the camera's motion to rotation about its focal point. 3 Moreover, many of these techniques perform motion-based figure-ground segmentation, which tends to fail when the camera zooms or when multiple objects move 3 In [10], the camera
References-found: 20

