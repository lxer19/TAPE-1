URL: http://www.cs.berkeley.edu/~debevec/Research/debevec-siggraph96-lite.ps.gz
Refering-URL: http://www.cs.berkeley.edu/projects/vision/publications.html
Root-URL: 
Title: Modeling and Rendering Architecture from Photographs: A hybrid geometry- and image-based approach  
Author: Paul E. Debevec Camillo J. Taylor Jitendra Malik 
Keyword: CR Descriptors: I.2.10 [Artificial Intelligence]: Vision and Scene Understanding Modeling and recovery of physical attributes; I.3.7 [Computer Graphics]: Three-Dimensional Graphics and Realism Color, shading, shadowing, and texture I.4.8 [Image Processing]: Scene Analysis Stereo; J.6 [Computer-Aided Engineering]: Computer-aided design (CAD).  
Address: Berkeley 1  
Affiliation: University of California at  
Note: To appear in the SIGGRAPH conference proceedings  
Abstract: We present a new approach for modeling and rendering existing architectural scenes from a sparse set of still photographs. Our modeling approach, which combines both geometry-based and image-based techniques, has two components. The first component is a photogrammetricmodeling method which facilitates the recovery of the basic geometry of the photographed scene. Our photogrammet-ric modeling approach is effective, convenient, and robust because it exploits the constraints that are characteristic of architectural scenes. The second component is a model-based stereo algorithm, which recovers how the real scene deviates from the basic model. By making use of the model, our stereo technique robustly recovers accurate depth from widely-spaced image pairs. Consequently, our approach can model large architectural environments with far fewer photographs than current image-based modeling approaches. For producing renderings, we present view-dependent texture mapping, a method of compositing multiple views of a scene that better simulates geometric detail on basic models. Our approach can be used to recover models for use in either geometry-based or image-based rendering systems. We present results that demonstrate our approach's ability to create realistic renderings of architectural scenes from viewpoints far from the original photographs. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Ali Azarbayejani and Alex Pentland. </author> <title> Recursive estimation of motion, structure, and focal length. </title> <journal> IEEE Trans. Pattern Anal. Machine Intell., </journal> 17(6) 562-575,June 1995. 
Reference-contexts: Attention has turned to using more than two views with image stream methods such as [19] or recursive approaches (e.g. <ref> [1] </ref>). [19] shows excellent results for the case of orthographic cameras, but direct solutions for the perspective case remain elusive.
Reference: [2] <author> H. H. Baker and T. O. Binford. </author> <title> Depth from edge and intensity based stereo. </title> <booktitle> In Proceedings of the Seventh IJCAI, Vancouver, BC, </booktitle> <pages> pages 631-636, </pages> <year> 1981. </year>
Reference-contexts: In humans, corresponding points in the two slightly differing images on the retinas are determined by the visual cortex in the process called binocular stereopsis. Years of research (e.g. <ref> [2, 4, 8, 9, 12, 15] </ref>) have shown that determining stereo correspondences by computer is difficult problem. <p> The fact that the epipolar geometry remains linear after the warping step also facilitates the use of the ordering constraint <ref> [2, 6] </ref> through a dynamic programming technique. 4.2 Stereo Results and Rerendering While the warping step makes it dramatically easier to determine stereo correspondences, a stereo algorithm is still necessary to actually determine them. The algorithm we developed to produce the images in this paper is described in [3].
Reference: [3] <author> Paul E. Debevec, Camillo J. Taylor, and Jitendra Malik. </author> <title> Modeling and rendering architecture from photographs: A hybrid geometry- and image-based approach. </title> <type> Technical Report UCB//CSD-96-893, </type> <institution> U.C. Berkeley, CS Division, </institution> <month> January </month> <year> 1996. </year>
Reference-contexts: The algorithm we developed to produce the images in this paper is described in <ref> [3] </ref>. Once a depth map has been computed for a particular image, we can rerender the scene from novel viewpoints using the methods described in [23, 16, 13].
Reference: [4] <editor> D.J.Fleet, A.D.Jepson, and M.R.M. Jenkin. </editor> <booktitle> Phase-based disparity measurement. CVGIP: Image Understanding, </booktitle> <volume> 53(2) </volume> <pages> 198-210, </pages> <year> 1991. </year>
Reference-contexts: In humans, corresponding points in the two slightly differing images on the retinas are determined by the visual cortex in the process called binocular stereopsis. Years of research (e.g. <ref> [2, 4, 8, 9, 12, 15] </ref>) have shown that determining stereo correspondences by computer is difficult problem.
Reference: [5] <author> Oliver Faugeras and Giorgio Toscani. </author> <title> The calibration problem for stereo. </title> <booktitle> In Proceedings IEEE CVPR 86, </booktitle> <pages> pages 15-20, </pages> <year> 1986. </year>
Reference-contexts: This mapping is determined by, among other parameters, the camera's focal length and its pattern of radial distortion. Camera calibration is a well-studied problem both in photogrammetry and computer vision; some successful methods include [20] and <ref> [5] </ref>.
Reference: [6] <author> Olivier Faugeras. </author> <title> Three-Dimensional Computer Vision. </title> <publisher> MIT Press, </publisher> <year> 1993. </year>
Reference-contexts: Since then, the problem's mathematical and algorithmic aspects have been explored starting from the fundamental work of Ullman [21] and Longuet-Higgins [11], in the early 1980s. Faugeras's book <ref> [6] </ref> overviews the state of the art as of 1992. So far, a key realization has been that the recovery of structure is very sensitive to noise in image measurements when the translation between the available camera positions is small. <p> In fact, the epipolar lines of the warped offset image coincide with the epipolar lines of the key image. 4.1 Model-Based Epipolar Geometry In traditional stereo, the epipolar constraint (see <ref> [6] </ref>) is often used to constrain the search for corresponding points in the offset image to searching along an epipolar line. This constraint simplifies stereo not only by reducing the search for each correspondence to one dimension, but also by reducing the chance of selecting a false matches. <p> The fact that the epipolar geometry remains linear after the warping step also facilitates the use of the ordering constraint <ref> [2, 6] </ref> through a dynamic programming technique. 4.2 Stereo Results and Rerendering While the warping step makes it dramatically easier to determine stereo correspondences, a stereo algorithm is still necessary to actually determine them. The algorithm we developed to produce the images in this paper is described in [3].
Reference: [7] <author> Olivier Faugeras, Stephane Laveau, Luc Robert, Gabriella Csurka, and Cyril Zeller. </author> <title> 3-d reconstruction of urban scenes from sequences of images. </title> <type> Technical Report 2572, </type> <institution> INRIA, </institution> <month> June </month> <year> 1995. </year>
Reference-contexts: Camera calibration is a well-studied problem both in photogrammetry and computer vision; some successful methods include [20] and [5]. While there has been recent progress in the use of uncalibrated views for 3D reconstruction <ref> [7] </ref>, we have found camera calibration to be a straightforward process that considerably simplifies the problem. 1.1.2 Structure from Motion Given the 2D projection of a point in the world, its position in 3D space could be anywhere on a ray extending out in a particular direction from the camera's optical
Reference: [8] <author> W. E. L. </author> <title> Grimson. From Images to Surface. </title> <publisher> MIT Press, </publisher> <year> 1981. </year>
Reference-contexts: In humans, corresponding points in the two slightly differing images on the retinas are determined by the visual cortex in the process called binocular stereopsis. Years of research (e.g. <ref> [2, 4, 8, 9, 12, 15] </ref>) have shown that determining stereo correspondences by computer is difficult problem.
Reference: [9] <author> D. Jones and J. Malik. </author> <title> Computational framework for determining stereo correspondence from a set of linear spatial filters. </title> <journal> Image and Vision Computing, </journal> <volume> 10(10) </volume> <pages> 699-708, </pages> <month> December </month> <year> 1992. </year>
Reference-contexts: In humans, corresponding points in the two slightly differing images on the retinas are determined by the visual cortex in the process called binocular stereopsis. Years of research (e.g. <ref> [2, 4, 8, 9, 12, 15] </ref>) have shown that determining stereo correspondences by computer is difficult problem.
Reference: [10] <author> E. Kruppa. </author> <title> Zur ermittlung eines objectes aus zwei perspektiven mit innerer ori-entierung. </title> <journal> Sitz.-Ber. Akad. Wiss., Wien, Math. Naturw. Kl., Abt. Ila., </journal> <volume> 122 </volume> <pages> 1939-1948, </pages> <year> 1913. </year>
Reference-contexts: This problem has been studied in the area of photogrammetry for the principal purpose of producing topographic maps. In 1913, Kruppa <ref> [10] </ref> proved the fundamental result that given two views of five distinct points, one could recover the rotation and translation between the two camera positions as well as the 3D locations of the points (up to a scale factor).
Reference: [11] <author> H.C. Longuet-Higgins. </author> <title> A computer algorithm for reconstructing a scene from two projections. </title> <journal> Nature, </journal> <volume> 293 </volume> <pages> 133-135, </pages> <month> September </month> <year> 1981. </year>
Reference-contexts: Since then, the problem's mathematical and algorithmic aspects have been explored starting from the fundamental work of Ullman [21] and Longuet-Higgins <ref> [11] </ref>, in the early 1980s. Faugeras's book [6] overviews the state of the art as of 1992. So far, a key realization has been that the recovery of structure is very sensitive to noise in image measurements when the translation between the available camera positions is small.
Reference: [12] <author> D. Marr and T. Poggio. </author> <title> A computationaltheory of human stereo vision. </title> <journal> Proceedings of the Royal Society of London, </journal> <volume> 204 </volume> <pages> 301-328, </pages> <year> 1979. </year>
Reference-contexts: In humans, corresponding points in the two slightly differing images on the retinas are determined by the visual cortex in the process called binocular stereopsis. Years of research (e.g. <ref> [2, 4, 8, 9, 12, 15] </ref>) have shown that determining stereo correspondences by computer is difficult problem.
Reference: [13] <author> Leonard McMillan and Gary Bishop. </author> <title> Plenoptic modeling: An image-based rendering system. </title> <booktitle> In SIGGRAPH '95, </booktitle> <year> 1995. </year>
Reference-contexts: Recently, creating models directly from photographs has received increased interest in computer graphics. Since real images are used as input, such an image-based system (Fig. 1c) has an advantage in producing photorealistic renderings as output. Some of the most promising of these systems <ref> [16, 13] </ref> rely on the computer vision technique of computational stereopsis to automatically determine the structure of the scene from the multiple photographs available. As a consequence, however, these systems are only as strong as the underlying stereo algorithms. <p> Using this property, [23] demonstrated how regularly spaced synthetic images (with their computed depth maps) could be warped and composited in real time to produce a virtual environment. More recently, <ref> [13] </ref> presented a real-time image-based rendering system that used panoramic photographs with depth computed, in part, from stereo correspondence. One finding of the paper was that extracting reliable depth estimates from stereo is very difficult. <p> The algorithm we developed to produce the images in this paper is described in [3]. Once a depth map has been computed for a particular image, we can rerender the scene from novel viewpoints using the methods described in <ref> [23, 16, 13] </ref>. Furthermore, when several images and their corresponding depth maps are available, we can use the view-dependent texture-mapping method of Section 3 to composite the multiple renderings.
Reference: [14] <author> Eric N. Mortensen and William A. Barrett. </author> <title> Intelligent scissors for image composition. </title> <booktitle> In SIGGRAPH '95, </booktitle> <year> 1995. </year>
Reference-contexts: Such constraints are specified using a graphical 3D interface. When such constraints are provided, they are used to simplify the reconstruction problem. The user marks edge features in the images using a point-and-click interface; a gradient-based technique as in <ref> [14] </ref> can be used to align the edges with sub-pixel accuracy. We use edge rather than point features since they are easier to localize and less likely to be completely obscured. Only a section of each edge needs to be marked, making it possible to use partially visible edges.
Reference: [15] <author> S. B. Pollard, J. E. W. Mayhew, and J. P. </author> <title> Frisby. A stereo correspondence algorithm using a disparity gradient limit. </title> <journal> Perception, </journal> <volume> 14 </volume> <pages> 449-470, </pages> <year> 1985. </year>
Reference-contexts: In humans, corresponding points in the two slightly differing images on the retinas are determined by the visual cortex in the process called binocular stereopsis. Years of research (e.g. <ref> [2, 4, 8, 9, 12, 15] </ref>) have shown that determining stereo correspondences by computer is difficult problem.
Reference: [16] <author> R. Szeliski. </author> <title> Image mosaicing for tele-reality applications. </title> <booktitle> In IEEE Computer Graphics and Applications, </booktitle> <year> 1996. </year>
Reference-contexts: Recently, creating models directly from photographs has received increased interest in computer graphics. Since real images are used as input, such an image-based system (Fig. 1c) has an advantage in producing photorealistic renderings as output. Some of the most promising of these systems <ref> [16, 13] </ref> rely on the computer vision technique of computational stereopsis to automatically determine the structure of the scene from the multiple photographs available. As a consequence, however, these systems are only as strong as the underlying stereo algorithms. <p> The algorithm we developed to produce the images in this paper is described in [3]. Once a depth map has been computed for a particular image, we can rerender the scene from novel viewpoints using the methods described in <ref> [23, 16, 13] </ref>. Furthermore, when several images and their corresponding depth maps are available, we can use the view-dependent texture-mapping method of Section 3 to composite the multiple renderings.
Reference: [17] <author> Camillo J. Taylor and David J. Kriegman. </author> <title> Structure and motion from line segments in multiple images. </title> <journal> IEEE Trans. Pattern Anal. Machine Intell., </journal> <volume> 17(11), </volume> <month> November </month> <year> 1995. </year>
Reference-contexts: In general, linear algorithms for the problem fail to make use of all available information while nonlinear minimization methods are prone to difficulties arising from local minima in the parameter space. An alternative formulation of the problem <ref> [17] </ref> uses lines rather than points as image measurements, but the previously stated concerns were shown to remain largely valid. <p> O = P where Err i represents the disparity computed for edge feature i. Thus, the unknown model parameters and camera positions are computed by minimizing O with respect to these variables. Our system uses the the error function Err i from <ref> [17] </ref>, described below. (a) (b) plane. (b) The error function used in the reconstruction algorithm. The heavy line represents the observed edge segment (marked by the user) and the lighter line represents the model edge predicted by the current camera and model parameters.
Reference: [18] <author> S. J. Teller, Celeste Fowler, Thomas Funkhouser, and Pat Hanrahan. </author> <title> Partitioning and ordering large radiosity computations. </title> <booktitle> In SIGGRAPH '94, </booktitle> <pages> pages 443-450, </pages> <year> 1994. </year>
Reference-contexts: For complex models where most images are entirely occluded for the typical view, it can be very inefficient to project every original photograph to the novel viewpoint. Some efficient techniques to determine such visibility a priori in architectural scenes through spatial partitioning are presented in <ref> [18] </ref>. 4 Model-Based Stereopsis The modeling system described in Section 2 allows the user to create a basic model of a scene, but in general the scene will have additional geometric detail (such as friezes and cornices) not captured in the model.
Reference: [19] <author> Carlo Tomasi and Takeo Kanade. </author> <title> Shape and motion from image streams under orthography: a factorization method. </title> <journal> International Journal of Computer Vision, </journal> <volume> 9(2) </volume> <pages> 137-154, </pages> <month> November </month> <year> 1992. </year>
Reference-contexts: So far, a key realization has been that the recovery of structure is very sensitive to noise in image measurements when the translation between the available camera positions is small. Attention has turned to using more than two views with image stream methods such as <ref> [19] </ref> or recursive approaches (e.g. [1]). [19] shows excellent results for the case of orthographic cameras, but direct solutions for the perspective case remain elusive. <p> Attention has turned to using more than two views with image stream methods such as <ref> [19] </ref> or recursive approaches (e.g. [1]). [19] shows excellent results for the case of orthographic cameras, but direct solutions for the perspective case remain elusive. In general, linear algorithms for the problem fail to make use of all available information while nonlinear minimization methods are prone to difficulties arising from local minima in the parameter space.
Reference: [20] <author> Roger Tsai. </author> <title> A versatile camera calibration technique for high accuracy 3d machine vision metrology using off-the-shelf tv cameras and lenses. </title> <journal> IEEE Journal of Robotics and Automation, </journal> <volume> 3(4) </volume> <pages> 323-344, </pages> <month> August </month> <year> 1987. </year>
Reference-contexts: This mapping is determined by, among other parameters, the camera's focal length and its pattern of radial distortion. Camera calibration is a well-studied problem both in photogrammetry and computer vision; some successful methods include <ref> [20] </ref> and [5].
Reference: [21] <author> S. Ullman. </author> <title> The Interpretation of Visual Motion. </title> <publisher> The MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1979. </year>
Reference-contexts: Since then, the problem's mathematical and algorithmic aspects have been explored starting from the fundamental work of Ullman <ref> [21] </ref> and Longuet-Higgins [11], in the early 1980s. Faugeras's book [6] overviews the state of the art as of 1992. So far, a key realization has been that the recovery of structure is very sensitive to noise in image measurements when the translation between the available camera positions is small.
Reference: [22] <author> L Williams. </author> <title> Casting curved shadows on curved surfaces. </title> <booktitle> In SIGGRAPH '78, </booktitle> <pages> pages 270-274, </pages> <year> 1978. </year>
Reference-contexts: While such shadowed regions could be determined using an object-space visible surface algorithm, or an image-space ray casting algorithm, we use an image-space shadow map algorithm based on <ref> [22] </ref> since it is efficiently implemented using z-buffer hardware. Fig. 11, upper left, shows the results of mapping a single image onto the high school building model. The recovered camera position for the projected image is indicated in the lower left corner of the image.
Reference: [23] <author> Lance Williams and Eric Chen. </author> <title> View interpolation for image synthesis. </title> <booktitle> In SIG-GRAPH '93, </booktitle> <year> 1993. </year>
Reference-contexts: Thus, a new image of the scene is created by warping the images according to their depth maps. A principal attraction of image-based rendering is that it offers a method of rendering arbitrarily complex scenes with a constant amount of computation required per pixel. Using this property, <ref> [23] </ref> demonstrated how regularly spaced synthetic images (with their computed depth maps) could be warped and composited in real time to produce a virtual environment. More recently, [13] presented a real-time image-based rendering system that used panoramic photographs with depth computed, in part, from stereo correspondence. <p> The lower right picture shows the results of compositing renderings of all twelve original images. Some pixels near the front edge of the roof not seen in any image have been filled in with the hole-filling algorithm from <ref> [23] </ref>. Even with this weighting, neighboring pixels can still be sampled from different views at the boundary of a projected image, since the contribution of an image must be zero outside its boundary. To 7 To appear in the SIGGRAPH conference proceedings mapping. <p> Any regions in the composite image which are occluded in every projected image are filled in using the hole-filling method from <ref> [23] </ref>. In the discussion so far, projected image weights are computed at every pixel of every projected rendering. Since the weighting function is smooth (though not constant) across flat surfaces, it is not generally not necessary to compute it for every pixel of every face of the model. <p> The algorithm we developed to produce the images in this paper is described in [3]. Once a depth map has been computed for a particular image, we can rerender the scene from novel viewpoints using the methods described in <ref> [23, 16, 13] </ref>. Furthermore, when several images and their corresponding depth maps are available, we can use the view-dependent texture-mapping method of Section 3 to composite the multiple renderings.
Reference: [24] <author> Mourad Zerroug and Ramakant Nevatia. </author> <title> Segmentation and recovery of shgcs from a real intensity image. </title> <booktitle> In European Conference on Computer Vision, </booktitle> <pages> pages 319-330, </pages> <year> 1994. </year>
Reference-contexts: First, surfaces of revolution represent an important component of architecture (e.g. domes, columns, and minarets) that are not recovered in our photogrammetric modeling approach. (As noted, the dome in Fig. 10 was manually sized by the user.) Fortunately, there has been much work (e.g. <ref> [24] </ref>) that presents methods of recovering such structures from image contours. Curved model geometry is also entirely consistent with our approach to recovering additional detail with model-based stereo. Second, our techniques should be extended to recognize and model the photometric properties of the materials in the scene.
References-found: 24

