URL: ftp://theory.lcs.mit.edu/pub/people/victor_l/SPAA98.ps
Refering-URL: http://theory.lcs.mit.edu/~victor_l/papers/SPAA98.html
Root-URL: 
Email: athena@theory.lcs.mit.edu  l@theory.lcs.mit.edu  
Title: Computation-Centric Memory Models  
Author: Matteo Frigo Victor Luchangco victor 
Address: 545 Technology Square NE43-203 Cambridge, MA 02139  545 Technology Square NE43-369 Cambridge, MA 02139  
Affiliation: MIT Laboratory for Computer Science  Massachusetts Institute of Technology  
Abstract: We present a computation-centric theory of memory models. Unlike traditional processor-centric models, computation-centric models focus on the logical dependencies among instructions rather than the processor that happens to execute them. This theory allows us to define what a memory model is, and to investigate abstract properties of memory models. In particular, we focus on constructibility, which is a necessary property of those models that can be implemented exactly by an online algorithm. For a noncon-structible model, we show that there is a natural way to define the constructible version of that model. We explore the implications of constructibility in the context of dag-consistent memory models, which do not require that memory locations be serialized. The strongest dag-consistent model, called NN-dag consistency, is not constructible. However, its constructible version is equivalent to a model that we call location consistency, in which each location is serialized independently. 
Abstract-found: 1
Intro-found: 1
Reference: [AG96] <author> Sarita V. Adve and Kourosh Gharachorloo. </author> <title> Shared memory consistency models: A tutorial. </title> <booktitle> IEEE Computer, </booktitle> <pages> pages 6676, </pages> <month> December </month> <year> 1996. </year>
Reference-contexts: Finally, many papers on memory models, starting with the seminal paper on sequential consistency [Lam79], have been written from an hardware viewpoint, without a strict formal framework. The reader is referred to [HP96] and <ref> [AG96] </ref> for good tutorials and further references on the subject.
Reference: [AH90] <author> Sarita V. Adve and Mark D. Hill. </author> <title> Weak ordering a new definition. </title> <booktitle> In Proceedings of the 17th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 214, </pages> <address> Seattle, Washington, </address> <month> May </month> <year> 1990. </year>
Reference-contexts: The computation-centric theory is based on the two concepts of a computation and an observer function. Most existing memory models <ref> [DSB86, AH90, Goo89, GLL + 90, KCZ92, BZS93, ISL96] </ref> are expressed in terms of processors acting on memory. We call these memory models processor-centric; the memory model specifies what happens when a processor performs some action on memory.
Reference: [AMNS96] <author> Arvind, J. W. Maessen, R. S. Nikhil, and Joe Stoy. Lambda-S: </author> <title> an implicitly parallel lambda-calculus with letrec, synchronization and side-effects. </title> <type> Technical report, </type> <institution> MIT Laboratory for Computer Science, </institution> <month> Nov </month> <year> 1996. </year> <note> Computation Structures Group Memo 393, also available at http://www.csg.lcs.mit. edu:8001/pubs/csgmemo.html. </note>
Reference-contexts: In the present paper, rather than focusing on the correctness of specific implementations of a memory model, we are more interested in the formal properties of models, such as con-structibility. A different formal approach has been taken by the proponents of the S calculus <ref> [AMNS96] </ref>, which is an extension of the calculus with synchronization and side-effects. The S calculus gives a unified semantics of language and memory which is based on a set of rewriting rules. Preliminary S descriptions of sequential consistency [Lam79] and location consistency (in the sense of Definition 18) exist [Arv98].
Reference: [Arv98] <author> Arvind. </author> <type> Personal communication, </type> <month> January </month> <year> 1998. </year>
Reference-contexts: The S calculus gives a unified semantics of language and memory which is based on a set of rewriting rules. Preliminary S descriptions of sequential consistency [Lam79] and location consistency (in the sense of Definition 18) exist <ref> [Arv98] </ref>. Finally, many papers on memory models, starting with the seminal paper on sequential consistency [Lam79], have been written from an hardware viewpoint, without a strict formal framework. The reader is referred to [HP96] and [AG96] for good tutorials and further references on the subject.
Reference: [BFJ + 96a] <author> Robert D. Blumofe, Matteo Frigo, Chrisopher F. Jo-erg, Charles E. Leiserson, and Keith H. Randall. </author> <title> An analysis of dag-consistent distributed shared-memory algorithms. </title> <booktitle> In Proceedings of the Eighth Annual ACM Symposium on Parallel Algorithms and Architectures (SPAA), </booktitle> <pages> pages 297308, </pages> <address> Padua, Italy, </address> <month> June </month> <year> 1996. </year>
Reference-contexts: In the second approach, a memory model is defined by imposing certain constraints on the value that the observer function can assume on paths in the computation dag. Using this approach, we explore the class of dag-consistent memory models, a generalization of the dag consistency of <ref> [BFJ + 96b, BFJ + 96a, Joe96] </ref>. Such models do not even require that a single location be serialized, and are thus strictly weaker than the other class of models. Nonetheless, we found an interesting link between location consistency, dag consistency and constructibility. <p> This need not be the case in the actual implementation. For example, the BACKER algorithm from <ref> [BFJ + 96b, BFJ + 96a] </ref> maintains location consistency, even though it may keep several incoherent copies of the same location. In Section 6, we prove that location consistency is the constructible version of a model we call NN-dag consistency. <p> Definition 20 is a generalization of the two definitions of dag consistency that the Cilk group of MIT (including one of the authors of the present paper) proposed in the past <ref> [BFJ + 96b, BFJ + 96a] </ref>. Varying the predicate Q in Condition 20.1 yields different memory models. Note that strengthening Q weakens the memory model. In the rest of the paper, we consider four specific predicates, NN, NW, WN and WW, and the dag consistency models they define. <p> The relations among NN, WN, NW, WW, LC and SC are shown in Figure 1. WW is the original dag consistency model defined in [BFJ + 96b, Joe96]. WN is the model called dag consistency in <ref> [BFJ + 96a] </ref>, strengthened to avoid anomalies such as the one illustrated in Figure 2. NN is the strongest dag-consistent memory model (as proven in Theorem 21 below). Symmetry suggests that we also consider NW. Theorem 21 NN Q-dag consistency for any predicate Q. <p> Motivated by the experience with dag consistency <ref> [BFJ + 96b, BFJ + 96a, Joe96] </ref>, we completely abstract away from a program, and assume the partial order (the computation) as our starting point. Post mortem analysis has been used by [GK94] to verify (after the fact) that a given execution is sequentially consistent. <p> This point of view can be thought of as looking for the weakest reasonable memory model. (See [Fri98] for a full discussion of this theme.) Dag consistency was also attractive because it is maintained by the BACKER algorithm used by Cilk, which has provably good performance <ref> [BFJ + 96a] </ref>. Variants of dag consistency were developed to forbid anomalies, or undesirable memory behaviors, as they were discovered. The papers [BFJ + 96b] and [BFJ + 96a] give two different definitions of dag consistency, which we call WW and WN. <p> of this theme.) Dag consistency was also attractive because it is maintained by the BACKER algorithm used by Cilk, which has provably good performance <ref> [BFJ + 96a] </ref>. Variants of dag consistency were developed to forbid anomalies, or undesirable memory behaviors, as they were discovered. The papers [BFJ + 96b] and [BFJ + 96a] give two different definitions of dag consistency, which we call WW and WN. We were surprised to discover that WN is not constructible, and we tried both to find a better definition of dag consistency, and to capture the exact semantics of BACKER. <p> Both problems have been solved. This paper presents a more or less complete picture of the various dag-consistent models and their mutual relationships. In another paper, Luchangco [Luc97] proves that BACKER supports location consistency. Consequently, the algorithmic analysis of <ref> [BFJ + 96a] </ref> and the experimental results from [BFJ + 96b] apply to location consistency with no change. There are many possible directions in which this research can be extended. One obvious open problem is finding a simple characterization of NW fl and WN fl .
Reference: [BFJ + 96b] <author> Robert D. Blumofe, Matteo Frigo, Christopher F. Jo-erg, Charles E. Leiserson, and Keith H. Randall. </author> <title> Dag-consistent distributed shared memory. </title> <booktitle> In Proceedings of the 10th International Parallel Processing Symposium, </booktitle> <address> Honolulu, Hawaii, </address> <month> April </month> <year> 1996. </year>
Reference-contexts: In the second approach, a memory model is defined by imposing certain constraints on the value that the observer function can assume on paths in the computation dag. Using this approach, we explore the class of dag-consistent memory models, a generalization of the dag consistency of <ref> [BFJ + 96b, BFJ + 96a, Joe96] </ref>. Such models do not even require that a single location be serialized, and are thus strictly weaker than the other class of models. Nonetheless, we found an interesting link between location consistency, dag consistency and constructibility. <p> This need not be the case in the actual implementation. For example, the BACKER algorithm from <ref> [BFJ + 96b, BFJ + 96a] </ref> maintains location consistency, even though it may keep several incoherent copies of the same location. In Section 6, we prove that location consistency is the constructible version of a model we call NN-dag consistency. <p> Definition 20 is a generalization of the two definitions of dag consistency that the Cilk group of MIT (including one of the authors of the present paper) proposed in the past <ref> [BFJ + 96b, BFJ + 96a] </ref>. Varying the predicate Q in Condition 20.1 yields different memory models. Note that strengthening Q weakens the memory model. In the rest of the paper, we consider four specific predicates, NN, NW, WN and WW, and the dag consistency models they define. <p> The relations among NN, WN, NW, WW, LC and SC are shown in Figure 1. WW is the original dag consistency model defined in <ref> [BFJ + 96b, Joe96] </ref>. WN is the model called dag consistency in [BFJ + 96a], strengthened to avoid anomalies such as the one illustrated in Figure 2. NN is the strongest dag-consistent memory model (as proven in Theorem 21 below). Symmetry suggests that we also consider NW. <p> Motivated by the experience with dag consistency <ref> [BFJ + 96b, BFJ + 96a, Joe96] </ref>, we completely abstract away from a program, and assume the partial order (the computation) as our starting point. Post mortem analysis has been used by [GK94] to verify (after the fact) that a given execution is sequentially consistent. <p> Variants of dag consistency were developed to forbid anomalies, or undesirable memory behaviors, as they were discovered. The papers <ref> [BFJ + 96b] </ref> and [BFJ + 96a] give two different definitions of dag consistency, which we call WW and WN. We were surprised to discover that WN is not constructible, and we tried both to find a better definition of dag consistency, and to capture the exact semantics of BACKER. <p> Both problems have been solved. This paper presents a more or less complete picture of the various dag-consistent models and their mutual relationships. In another paper, Luchangco [Luc97] proves that BACKER supports location consistency. Consequently, the algorithmic analysis of [BFJ + 96a] and the experimental results from <ref> [BFJ + 96b] </ref> apply to location consistency with no change. There are many possible directions in which this research can be extended. One obvious open problem is finding a simple characterization of NW fl and WN fl .
Reference: [BJK + 95] <author> Robert D. Blumofe, Christopher F. Joerg, Bradley C. Kuszmaul, Charles E. Leiserson, Keith H. Randall, and Yuli Zhou. Cilk: </author> <title> An efficient multithreaded runtime system. </title> <booktitle> In Proceedings of the Fifth ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming (PPoPP), pages 207216, </booktitle> <address> Santa Barbara, California, </address> <month> July </month> <year> 1995. </year>
Reference-contexts: For example, a computation could be generated using a multithreaded language with fork/join parallelism (such as Cilk <ref> [BJK + 95] </ref>). Computations are by no means limited to modeling multithreaded programs, however. In this paper, we assume that the computation is given, and defer the important problem of determining which computations a given program generates. <p> Historically, the abstract theory described in this paper arose from concrete problems in the context of research on dag consistency, a memory model for the Cilk multithreaded language for parallel computing <ref> [BJK + 95, Blu95, Joe96] </ref>. Dag consistency was developed to capture formally the minimal guarantees that users of Cilk expected from the memory. It was formulated to forbid particular behaviors considered undesirable when programming in Cilk.
Reference: [Blu95] <author> Robert D. Blumofe. </author> <title> Executing Multithreaded Programs Efficiently. </title> <type> PhD thesis, </type> <institution> Department of Electrical Engineering and Computer Science, Massachusetts Institute of Technology, </institution> <month> September </month> <year> 1995. </year>
Reference-contexts: Although the issues of how the computation is expressed and sched-uled are extremely important, they are outside the scope of this paper. The reader is referred to <ref> [Blu95, Joe96] </ref> for one way to deal with these issues. In this paper, we consider the computation as fixed and given a priori. In this paper, we consider only read-write memories. We denote reads and writes to location l by R (l) and W (l) respectively. <p> Completeness follows immediately from constructibility, since the empty computation is a prefix of all computations 3 This is the case with multithreaded languages, such as Cilk <ref> [Blu95, Joe96] </ref>. and, together with its unique observer function, belongs to every memory model. Not all memory models are constructible; we shall discuss some nonconstructible memory models in Section 5. However, a nonconstructible model can be strengthened in an essentially unique way until it becomes constructible. <p> Historically, the abstract theory described in this paper arose from concrete problems in the context of research on dag consistency, a memory model for the Cilk multithreaded language for parallel computing <ref> [BJK + 95, Blu95, Joe96] </ref>. Dag consistency was developed to capture formally the minimal guarantees that users of Cilk expected from the memory. It was formulated to forbid particular behaviors considered undesirable when programming in Cilk.
Reference: [BZS93] <author> Brian N. Bershad, Matthew J. Zekauskas, and Wayne A. Sawdon. </author> <title> The Midway distributed shared memory system. </title> <booktitle> In Digest of Papers from the Thirty-Eighth IEEE Computer Society International Conference (Spring COMPCON), </booktitle> <pages> pages 528537, </pages> <address> San Fran-cisco, California, </address> <month> February </month> <year> 1993. </year>
Reference-contexts: The computation-centric theory is based on the two concepts of a computation and an observer function. Most existing memory models <ref> [DSB86, AH90, Goo89, GLL + 90, KCZ92, BZS93, ISL96] </ref> are expressed in terms of processors acting on memory. We call these memory models processor-centric; the memory model specifies what happens when a processor performs some action on memory.
Reference: [DSB86] <author> Michel Dubois, Christoph Scheurich, and Faye A. Briggs. </author> <title> Memory access buffering in multiprocessors. </title> <booktitle> In Proceedings of the 13th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 434442, </pages> <month> June </month> <year> 1986. </year>
Reference-contexts: The computation-centric theory is based on the two concepts of a computation and an observer function. Most existing memory models <ref> [DSB86, AH90, Goo89, GLL + 90, KCZ92, BZS93, ISL96] </ref> are expressed in terms of processors acting on memory. We call these memory models processor-centric; the memory model specifies what happens when a processor performs some action on memory.
Reference: [Fri98] <author> Matteo Frigo. </author> <title> The weakest reasonable memory model. </title> <type> Master's thesis, </type> <institution> Massachusetts Institute of Technology, </institution> <year> 1998. </year>
Reference-contexts: Most of the simplicity of our theory comes from ignoring the fundamental issue of how programs gen 1 Location consistency is often called coherence in the literature [HP96]. It is not the model with the same name introduced by Gao and Sarkar [GS95]. See <ref> [Fri98] </ref> for a justification of this terminology. erate computations. This simplification does not come without cost, however. The computation generated by a program may depend on the values received from the memory, which in turn depend on the computation. <p> We now prove that they are also monotonic and constructible. Theorem 19 SC and LC are monotonic and constructible memory models. Proof: The monotonicity of both follows immediately from the definition since TS (C) TS (C 0 ) for all relaxations C 0 of C. 4 See <ref> [Fri98] </ref> for a discussion of this terminology. For constructibility, we give only the proof for SC; the proof for LC is similar. <p> Historically, we investigated the various dag-consistent models after discovering the problem with WN illustrated in Figure 4. Our attempts to find a better definition of dag consistency led us to the notion of constructibility. As criticism of WW) can be found in <ref> [Fri98] </ref>. At this stage of our research, little is known about WN fl and NW fl , which would be alternative ways of defining dag consistency. 6 Dag consistency and location consistency In this section, we investigate the relation between NN-dag consistency and location consistency. <p> Dag consistency was developed to capture formally the minimal guarantees that users of Cilk expected from the memory. It was formulated to forbid particular behaviors considered undesirable when programming in Cilk. This point of view can be thought of as looking for the weakest reasonable memory model. (See <ref> [Fri98] </ref> for a full discussion of this theme.) Dag consistency was also attractive because it is maintained by the BACKER algorithm used by Cilk, which has provably good performance [BFJ + 96a]. Variants of dag consistency were developed to forbid anomalies, or undesirable memory behaviors, as they were discovered.
Reference: [Gha95] <author> Kourosh Gharachorloo. </author> <title> Memory Consistency Models for Shared-Memory Multiprocessors. </title> <type> PhD thesis, </type> <institution> Department of Electrical Engineering, Stanford University, </institution> <month> December </month> <year> 1995. </year>
Reference-contexts: Finally, many papers on memory models, starting with the seminal paper on sequential consistency [Lam79], have been written from an hardware viewpoint, without a strict formal framework. The reader is referred to [HP96] and [AG96] for good tutorials and further references on the subject. Ghara-chorloo <ref> [Gha95] </ref> also distinguishes system-centric models, which expose the programmer to the details of how a system may reorder operations, and programmer-centric models, which require the programmer to provide program-level information about the intended behavior of shared-memory operations but then allow the programmer to reason as if the memory were sequentially consistent.
Reference: [GK94] <author> P. B. Gibbons and E. Korach. </author> <title> On testing cache-coherent shared memories. </title> <booktitle> In Proceedings of the Sixth Annual ACM Symposium on Parallel Algorithms and Architectures, </booktitle> <pages> pages 177188, </pages> <address> Cape May, NJ, </address> <year> 1994. </year>
Reference-contexts: Motivated by the experience with dag consistency [BFJ + 96b, BFJ + 96a, Joe96], we completely abstract away from a program, and assume the partial order (the computation) as our starting point. Post mortem analysis has been used by <ref> [GK94] </ref> to verify (after the fact) that a given execution is sequentially consistent. The need for formal frameworks for memory models has been felt by other researchers.
Reference: [GLL + 90] <author> Kourosh Gharachorloo, Daniel Lenoski, James Laudon, Phillip Gibbons, Anoop Gupta, and John Hennessy. </author> <title> Memory consistency and event ordering in scalable shared-memory multiprocessors. </title> <booktitle> In Proceedings of the 17th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 1526, </pages> <address> Seattle, Washington, </address> <month> June </month> <year> 1990. </year>
Reference-contexts: The computation-centric theory is based on the two concepts of a computation and an observer function. Most existing memory models <ref> [DSB86, AH90, Goo89, GLL + 90, KCZ92, BZS93, ISL96] </ref> are expressed in terms of processors acting on memory. We call these memory models processor-centric; the memory model specifies what happens when a processor performs some action on memory. <p> The need for formal frameworks for memory models has been felt by other researchers. Gibbons, Merrit, and Ghara-chorloo [GMG91] use the I/O automata model of Lynch and Tuttle [LT87] to give a formal specification of release consistency <ref> [GLL + 90] </ref>. Later work [GM92] extends the framework to nonblocking memories. The main concern of these papers is to expose the architectural assumptions that are implicit in previous literature on relaxed memory models. <p> It would also be useful to investigate whether any algorithm can be found that is more efficient than BACKER that implements a weaker memory model than LC. Another direction is to formulate other consistency models in the computation-centric framework. Some models, such as release consistency <ref> [GLL + 90] </ref>, require computations to be augmented with locks, and how to do this is a matter of active research. Finally, as mentioned previously, it is important to develop an integrated theory of memory and language semantics.
Reference: [GM92] <author> Phillip B. Gibbons and Michael Merritt. </author> <title> Specifying nonblocking shared memories. </title> <booktitle> In Proceedings of the Fourth Annual ACM Symposium on Parallel Algorithms and Architectures, </booktitle> <pages> pages 306315, </pages> <year> 1992. </year>
Reference-contexts: The need for formal frameworks for memory models has been felt by other researchers. Gibbons, Merrit, and Ghara-chorloo [GMG91] use the I/O automata model of Lynch and Tuttle [LT87] to give a formal specification of release consistency [GLL + 90]. Later work <ref> [GM92] </ref> extends the framework to nonblocking memories. The main concern of these papers is to expose the architectural assumptions that are implicit in previous literature on relaxed memory models.
Reference: [GMG91] <author> Phillip B. Gibbons, Michael Merritt, and Kourosh Gharachorloo. </author> <title> Proving sequential consistency of high-performance shared memories. </title> <booktitle> In Proceedings of the Third Annual ACM Symposium on Parallel Algorithms and Architectures, </booktitle> <pages> pages 292303, </pages> <year> 1991. </year>
Reference-contexts: Post mortem analysis has been used by [GK94] to verify (after the fact) that a given execution is sequentially consistent. The need for formal frameworks for memory models has been felt by other researchers. Gibbons, Merrit, and Ghara-chorloo <ref> [GMG91] </ref> use the I/O automata model of Lynch and Tuttle [LT87] to give a formal specification of release consistency [GLL + 90]. Later work [GM92] extends the framework to nonblocking memories.
Reference: [Goo89] <author> James R. Goodman. </author> <title> Cache consistency and sequential consistency. </title> <type> Technical Report 61, </type> <institution> IEEE Scalable Coherent Interface (SCI) Working Group, </institution> <month> March </month> <year> 1989. </year>
Reference-contexts: The computation-centric theory is based on the two concepts of a computation and an observer function. Most existing memory models <ref> [DSB86, AH90, Goo89, GLL + 90, KCZ92, BZS93, ISL96] </ref> are expressed in terms of processors acting on memory. We call these memory models processor-centric; the memory model specifies what happens when a processor performs some action on memory.
Reference: [GS95] <author> Guang R. Gao and Vivek Sarkar. </author> <title> Location consistency: Stepping beyond memory coherence barrier. </title> <booktitle> In Proceedings of the 1995 International Conference on Parallel Processing, </booktitle> <pages> pages 7376, </pages> <address> Oconomowoc, Wiscon-sin, </address> <month> August </month> <year> 1995. </year>
Reference-contexts: Most of the simplicity of our theory comes from ignoring the fundamental issue of how programs gen 1 Location consistency is often called coherence in the literature [HP96]. It is not the model with the same name introduced by Gao and Sarkar <ref> [GS95] </ref>. See [Fri98] for a justification of this terminology. erate computations. This simplification does not come without cost, however. The computation generated by a program may depend on the values received from the memory, which in turn depend on the computation. <p> The first model is sequential consistency [Lam79]. The second model is sometimes called coherence in the literature <ref> [GS95, HP96] </ref>; we call it location consistency. Both models are complete, monotonic and constructible. Because we define these models using computations, our definitions generalize traditional processor-centric ones without requiring explicit synchronization operations. <p> Sequential consistency requires that the topological sort be the same for all locations. By allowing a different topological sort for each location, we define a memory model that is often called coherence <ref> [GS95, HP96] </ref>. <p> By allowing a different topological sort for each location, we define a memory model that is often called coherence [GS95, HP96]. We believe that a more appropriate name for this model is location consistency, even though the same name is used in <ref> [GS95] </ref> for a different memory model. 4 Definition 18 Location consistency is the memory model LC = f (C; ) : 8l 9T l 2 T S (C) 8u; (l; u) = W T l (l; u)g Location consistency requires that all writes to the same location behave as if they <p> The idea that the partial order induced by a program should be the basis for defining memory semantics, as opposed to the sequential order of instructions within one processor, already appears in the work by Gao and Sarkar on their version of location consistency <ref> [GS95] </ref>. Motivated by the experience with dag consistency [BFJ + 96b, BFJ + 96a, Joe96], we completely abstract away from a program, and assume the partial order (the computation) as our starting point.
Reference: [HP96] <author> John L. Hennessy and David A. Patterson. </author> <title> Computer Architecture: A Quantitative Approach. </title> <publisher> Morgan Kauf-mann, </publisher> <address> San Francisco, CA, </address> <note> second edition, </note> <year> 1996. </year>
Reference-contexts: Third, our approach allows us to generalize familiar memory models, such as sequential consistency. Most of the simplicity of our theory comes from ignoring the fundamental issue of how programs gen 1 Location consistency is often called coherence in the literature <ref> [HP96] </ref>. It is not the model with the same name introduced by Gao and Sarkar [GS95]. See [Fri98] for a justification of this terminology. erate computations. This simplification does not come without cost, however. <p> The first model is sequential consistency [Lam79]. The second model is sometimes called coherence in the literature <ref> [GS95, HP96] </ref>; we call it location consistency. Both models are complete, monotonic and constructible. Because we define these models using computations, our definitions generalize traditional processor-centric ones without requiring explicit synchronization operations. <p> Sequential consistency requires that the topological sort be the same for all locations. By allowing a different topological sort for each location, we define a memory model that is often called coherence <ref> [GS95, HP96] </ref>. <p> Finally, many papers on memory models, starting with the seminal paper on sequential consistency [Lam79], have been written from an hardware viewpoint, without a strict formal framework. The reader is referred to <ref> [HP96] </ref> and [AG96] for good tutorials and further references on the subject.
Reference: [ISL96] <author> Liviu Iftode, Jaswinder Pal Singh, and Kai Li. </author> <title> Scope consistency: A bridge between release consistency and entry consistency. </title> <booktitle> In Proceedings of the Eighth Annual ACM Symposium on Parallel Algorithms and Architectures (SPAA), </booktitle> <pages> pages 277287, </pages> <address> Padua, Italy, </address> <month> June </month> <year> 1996. </year>
Reference-contexts: The computation-centric theory is based on the two concepts of a computation and an observer function. Most existing memory models <ref> [DSB86, AH90, Goo89, GLL + 90, KCZ92, BZS93, ISL96] </ref> are expressed in terms of processors acting on memory. We call these memory models processor-centric; the memory model specifies what happens when a processor performs some action on memory.
Reference: [Joe96] <author> Christopher F. Joerg. </author> <title> The Cilk System for Parallel Multithreaded Computing. </title> <type> PhD thesis, </type> <institution> Department of Electrical Engineering and Computer Science, Massa-chusetts Institute of Technology, </institution> <month> January </month> <year> 1996. </year>
Reference-contexts: In the second approach, a memory model is defined by imposing certain constraints on the value that the observer function can assume on paths in the computation dag. Using this approach, we explore the class of dag-consistent memory models, a generalization of the dag consistency of <ref> [BFJ + 96b, BFJ + 96a, Joe96] </ref>. Such models do not even require that a single location be serialized, and are thus strictly weaker than the other class of models. Nonetheless, we found an interesting link between location consistency, dag consistency and constructibility. <p> Although the issues of how the computation is expressed and sched-uled are extremely important, they are outside the scope of this paper. The reader is referred to <ref> [Blu95, Joe96] </ref> for one way to deal with these issues. In this paper, we consider the computation as fixed and given a priori. In this paper, we consider only read-write memories. We denote reads and writes to location l by R (l) and W (l) respectively. <p> Completeness follows immediately from constructibility, since the empty computation is a prefix of all computations 3 This is the case with multithreaded languages, such as Cilk <ref> [Blu95, Joe96] </ref>. and, together with its unique observer function, belongs to every memory model. Not all memory models are constructible; we shall discuss some nonconstructible memory models in Section 5. However, a nonconstructible model can be strengthened in an essentially unique way until it becomes constructible. <p> The relations among NN, WN, NW, WW, LC and SC are shown in Figure 1. WW is the original dag consistency model defined in <ref> [BFJ + 96b, Joe96] </ref>. WN is the model called dag consistency in [BFJ + 96a], strengthened to avoid anomalies such as the one illustrated in Figure 2. NN is the strongest dag-consistent memory model (as proven in Theorem 21 below). Symmetry suggests that we also consider NW. <p> Motivated by the experience with dag consistency <ref> [BFJ + 96b, BFJ + 96a, Joe96] </ref>, we completely abstract away from a program, and assume the partial order (the computation) as our starting point. Post mortem analysis has been used by [GK94] to verify (after the fact) that a given execution is sequentially consistent. <p> Historically, the abstract theory described in this paper arose from concrete problems in the context of research on dag consistency, a memory model for the Cilk multithreaded language for parallel computing <ref> [BJK + 95, Blu95, Joe96] </ref>. Dag consistency was developed to capture formally the minimal guarantees that users of Cilk expected from the memory. It was formulated to forbid particular behaviors considered undesirable when programming in Cilk.
Reference: [KCZ92] <author> P. Keleher, A. L. Cox, and W. Zwaenepoel. </author> <title> Lazy release consistency for software distributed shared memory. </title> <booktitle> In Proceedings of the 19th Annual International Symposium on Computer Architecture, </booktitle> <month> May </month> <year> 1992. </year>
Reference-contexts: The computation-centric theory is based on the two concepts of a computation and an observer function. Most existing memory models <ref> [DSB86, AH90, Goo89, GLL + 90, KCZ92, BZS93, ISL96] </ref> are expressed in terms of processors acting on memory. We call these memory models processor-centric; the memory model specifies what happens when a processor performs some action on memory.
Reference: [Lam79] <author> Leslie Lamport. </author> <title> How to make a multiprocessor computer that correctly executes multiprocess programs. </title> <journal> IEEE Transactions on Computers, </journal> <volume> C-28(9):690691, </volume> <month> September </month> <year> 1979. </year>
Reference-contexts: We discuss two approaches for specifying memory models within this theory. In the first approach, a memory model is defined in terms of topological sorts of the computation. Using this approach, we generalize the definition of sequential consistency <ref> [Lam79] </ref>, and define location consistency, 1 a memory model in which every location is serialized independently of other locations. In the second approach, a memory model is defined by imposing certain constraints on the value that the observer function can assume on paths in the computation dag. <p> In Section 3, we define constructibility, prove the uniqueness of the constructible version, and establish necessary and sufficient conditions for constructibility to hold. In Section 4, we discuss models based on a topological sort, and give computation-centric definitions of sequential consistency <ref> [Lam79] </ref> and location consistency. In Section 5, we define the class of dag-consistent memory models and investigate the relations among them. In Section 6, we prove that location consistency is the constructible version of NN-dag consistency. <p> The first model is sequential consistency <ref> [Lam79] </ref>. The second model is sometimes called coherence in the literature [GS95, HP96]; we call it location consistency. Both models are complete, monotonic and constructible. Because we define these models using computations, our definitions generalize traditional processor-centric ones without requiring explicit synchronization operations. <p> We define sequential consistency using last writer functions. Definition 17 Sequential consistency is the memory model SC = f (C; W T ) : T 2 T S (C)g This definition captures the spirit of Lamport's original model <ref> [Lam79] </ref>, that there exists a global total order of events observed by all nodes. However, unlike Lamport's definition, it does not restrict dependencies to be sequences of operations at each processor, nor does it depend on how the computation is mapped onto processors. <p> The S calculus gives a unified semantics of language and memory which is based on a set of rewriting rules. Preliminary S descriptions of sequential consistency <ref> [Lam79] </ref> and location consistency (in the sense of Definition 18) exist [Arv98]. Finally, many papers on memory models, starting with the seminal paper on sequential consistency [Lam79], have been written from an hardware viewpoint, without a strict formal framework. <p> Preliminary S descriptions of sequential consistency <ref> [Lam79] </ref> and location consistency (in the sense of Definition 18) exist [Arv98]. Finally, many papers on memory models, starting with the seminal paper on sequential consistency [Lam79], have been written from an hardware viewpoint, without a strict formal framework. The reader is referred to [HP96] and [AG96] for good tutorials and further references on the subject.
Reference: [LT87] <author> Nancy Lynch and Mark Tuttle. </author> <title> Hierarchical correctness proofs for distributed algorithms. </title> <booktitle> In 6th Annual ACM Symposium on Principles of Distributed Computation, </booktitle> <pages> pages 137151, </pages> <month> August </month> <year> 1987. </year>
Reference-contexts: Post mortem analysis has been used by [GK94] to verify (after the fact) that a given execution is sequentially consistent. The need for formal frameworks for memory models has been felt by other researchers. Gibbons, Merrit, and Ghara-chorloo [GMG91] use the I/O automata model of Lynch and Tuttle <ref> [LT87] </ref> to give a formal specification of release consistency [GLL + 90]. Later work [GM92] extends the framework to nonblocking memories. The main concern of these papers is to expose the architectural assumptions that are implicit in previous literature on relaxed memory models.
Reference: [Luc97] <author> Victor Luchangco. </author> <title> Precedence-based memory models. </title> <booktitle> In Eleventh International Workshop on Distributed Algorithms, number 1320 in Lecture Notes in Computer Science, </booktitle> <pages> pages 215229. </pages> <publisher> Springer-Verlag, </publisher> <year> 1997. </year> <title> The following space intentionally left blank. </title>
Reference-contexts: Both problems have been solved. This paper presents a more or less complete picture of the various dag-consistent models and their mutual relationships. In another paper, Luchangco <ref> [Luc97] </ref> proves that BACKER supports location consistency. Consequently, the algorithmic analysis of [BFJ + 96a] and the experimental results from [BFJ + 96b] apply to location consistency with no change. There are many possible directions in which this research can be extended.
References-found: 25

