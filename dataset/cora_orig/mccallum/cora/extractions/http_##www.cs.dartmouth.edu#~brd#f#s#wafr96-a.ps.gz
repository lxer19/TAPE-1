URL: http://www.cs.dartmouth.edu/~brd/f/s/wafr96-a.ps.gz
Refering-URL: http://www.cs.dartmouth.edu/~brd/www/
Root-URL: http://www.cs.dartmouth.edu
Email: briggs@mail.middlebury.edu brd@cs.cornell.edu  
Title: Robust Geometric Algorithms for Sensor Planning  
Author: Amy J. Briggs Bruce R. Donald 
Address: Upson Hall  Middlebury, VT 05753, USA Ithaca, NY 14853, USA  
Affiliation: Department of Mathematics Department of Computer Science and Computer Science  Middlebury College Cornell University  
Date: July 1996  
Note: Proceedings of the Second International Workshop on Algorithmic Foundations of Robotics, Toulouse, France,  
Abstract: We consider the problem of planning sensor strategies that enable a sensor to be automatically configured for robot tasks. In this paper we present robust and efficient algorithms for computing the regions from which a sensor has unobstructed or partially obstructed views of a target in a goal. We apply these algorithms to the Error Detection and Recovery problem of recognizing whether a goal or failure region has been achieved. Based on these methods and strategies for visually-cued camera control, we have built a robot surveillance system in which one mobile robot navigates to a viewing position from which it has an unobstructed view of a goal region, and then uses visual recognition to detect when a specific target has entered the room. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> T. Asano, T. Asano, L. Guibas, J. Hershberger, and H. Imai. </author> <title> Visibility of disjoint polygons. </title> <journal> Algorithmica, </journal> <volume> 1 </volume> <pages> 49-63, </pages> <year> 1986. </year>
Reference-contexts: was mentioned in the paper by Suri and O'Rourke [27], the triangles can be output in constant time per triangle: Asano et al. have shown that the visibility edges at a vertex v can be obtained sorted by slope in linear time with Welzl's algorithm for computing the visibility graph <ref> [30, 1] </ref>. Thus, the overall time for explicitly computing the boundary of the partial visibility region for target A at any fixed configuration q is O (n 2 (n + m) 2 ).
Reference: [2] <author> B. Bhattacharya, D. G. Kirkpatrick, and G. T. Tou-ssaint. </author> <title> Determining sector visibility of a polygon. </title> <booktitle> In Proc. ACM Symp. on Comp. Geom., </booktitle> <pages> pages 247-253, </pages> <month> June </month> <year> 1989. </year>
Reference-contexts: Their lower bound of (n 4 ) for explicitly constructing the boundary of the weak visibility region holds as well for our computation of recognizability regions under a weak visibility assumption. Bhattacharya, Kirkpatrick and Toussaint <ref> [2] </ref> introduce the concept of sector visibility of a polygon, and give fi (n) and (n log n) bounds, depending on the size of the visibility wedge, for determining if a polygon is weakly externally visible.
Reference: [3] <author> A. J. Briggs. </author> <title> Efficient Geometric Algorithms for Robot Sensing and Control. </title> <type> PhD thesis, </type> <institution> Department of Computer Science, Cornell University, </institution> <address> Ithaca, NY, </address> <month> Jan. </month> <year> 1995. </year>
Reference-contexts: As the target translates, free shadow vertices trace out point conics if their generating edges are anchored on the target <ref> [3] </ref>. 3.2.3 Swept shadows in the partial visibility model We have shown how to compute shadows for any fixed target position, and have discussed how these shadows change as the target translates.
Reference: [4] <author> R. G. Brown. </author> <title> Algorithms for Mobile Robot Localization and Building Flexible, Robust, Easy to Use Mobile Robots. </title> <type> PhD thesis, </type> <institution> Department of Computer Science, Cornell University, </institution> <address> Ithaca, NY, </address> <month> May </month> <year> 1995. </year>
Reference-contexts: The sensor is placed at position p and pointed in direction . The sensitive volume is the lightly shaded region SV (). It is partly bounded by obstacles (darkly shaded). laboratory at Cornell, and has been used for both map-making and robot localization <ref> [4, 5] </ref>. We denote the space of sensor placements C s p = IR 2 and the space of sensor aims C s c = S 1 . Our sensor configuration space is C s = C s p fiC s c = IR 2 fi S 1 .
Reference: [5] <author> R. G. Brown, L. P. Chew, and B. R. Donald. </author> <title> Localization and map-making algorithms for mobile robots. </title> <booktitle> In Proc. IASTED Int. Conf. on Robotics and Manufacturing, </booktitle> <pages> pages 185-190, </pages> <month> Sept. </month> <year> 1993. </year>
Reference-contexts: The sensor is placed at position p and pointed in direction . The sensitive volume is the lightly shaded region SV (). It is partly bounded by obstacles (darkly shaded). laboratory at Cornell, and has been used for both map-making and robot localization <ref> [4, 5] </ref>. We denote the space of sensor placements C s p = IR 2 and the space of sensor aims C s c = S 1 . Our sensor configuration space is C s = C s p fiC s c = IR 2 fi S 1 .
Reference: [6] <author> S. J. Buckley. </author> <title> Planning and Teaching Compliant Motion Strategies. </title> <type> PhD thesis, </type> <institution> MIT Artificial Intelligence Laboratory, </institution> <year> 1987. </year>
Reference-contexts: When in a particular configuration, the sensor returns a distance and normal reading to the nearest object, which is accurate to within some known bound. Such a ranging device has been developed in our robotics 2 This is similar to the notion introduced by Buckley <ref> [6] </ref> of a confusable set in the context of motion planning. There, two contact points x and y are said to be confusable if they are capable of generating the same position and force measurements. Page 3 SV () sensor at configuration = (p; ).
Reference: [7] <author> A. J. Cameron and H. Durrant-Whyte. </author> <title> A Bayesian approach to optimal sensor placement. </title> <journal> IJRR, </journal> <volume> 9(5) </volume> <pages> 70-88, </pages> <year> 1990. </year>
Reference-contexts: For more details, see Donald [12]. 1.2 Related Work The sensor placement problem has previously been addressed by Nelson and Khosla [22] and Kutu-lakos, Dyer, and Lumelsky [18] for visual tracking and vision-guided exploration. Several researchers have explored the problem of optimal sensor placement. Cameron and Durrant-Whyte <ref> [7] </ref> and Hager and Mintz [15] present a Bayesian approach to optimal sensor placement. Hutchinson [16] introduces the concept of a visual constraint surface to control motion. The idea is to combine position, force, and visual sensing in order to produce error-tolerant motion strategies.
Reference: [8] <author> J. F. Canny. </author> <title> A computational approach to edge detection. </title> <journal> IEEE Transactions on Pattern Analysis and Machine Intelligence, </journal> <volume> 8(6) </volume> <pages> 34-43, </pages> <year> 1986. </year>
Reference-contexts: We first built a calibrated visual model of Lily. We used the Panasonic CCD camera mounted on Tommy to take a picture of Lily from a known fixed distance (4 m). We then computed the intensity edges for that image using an implementation of Canny's edge detection algorithm <ref> [8] </ref>. The actual model of Lily that we created and used is shown in Figure 17. We did not alter the intensity edges that Canny's algorithm output, and experimentation demonstrated that our results are relatively insensitive to the particular image taken.
Reference: [9] <author> A. Casta~no and S. Hutchinson. </author> <title> Hybrid vision/position servo control of a robotic manipulator. </title> <booktitle> In Proc. IEEE ICRA, </booktitle> <pages> pages 1264-1269, </pages> <month> May </month> <year> 1992. </year>
Reference-contexts: The idea is to combine position, force, and visual sensing in order to produce error-tolerant motion strategies. His work builds on that of preimage planners by adding visual feedback to compensate for uncertainty. Details on the implementation of vision-based control are described by Hutchinson and Casta~no <ref> [9] </ref>. Sharma and Hutchinson [26] define a measure of robot motion observability based on the relationship between differential changes in the position of the robot to the corresponding differential changes in the observed visual features.
Reference: [10] <author> C. K. Cowan and P. D. Kovesi. </author> <title> Automatic sensor placement from vision task requirements. </title> <journal> IEEE Transactions on Pattern Analysis and Machine Intelligence, </journal> <volume> 10(3) </volume> <pages> 407-416, </pages> <month> May </month> <year> 1988. </year>
Reference-contexts: Lacroix, Grandjean, and Ghallab [19] describe a method for selecting view points and sensing tasks to confirm an identification hypothesis. Cowan and Kovesi <ref> [10] </ref> study the problem of automatic camera placement for vision tasks. They consider the constraints on camera location imposed by resolution and focus requirements, visibility and view angle, and forbidden regions depending on the task.
Reference: [11] <author> M. de Berg, L. Guibas, D. Halperin, M. Overmars, O. Schwarzkopf, M. Sharir, and M. Teillaud. </author> <title> Reaching a goal with directional uncertainty. </title> <booktitle> In Proc. Int. Symp. on Algorithms and Computation, </booktitle> <month> Dec. </month> <year> 1993. </year>
Reference-contexts: The problem of planar motion planning for a robot with bounded directional uncertainty is considered by de Berg et al. <ref> [11] </ref>. They give algorithms for constructing the regions from which goals may be reached, and show that the complexity of the regions depends on the magnitude of the uncertainty angle. Teller [29] solves the weak polygon visibility problem for a special case in 3D.
Reference: [12] <author> B. R. Donald. </author> <title> Error detection and recovery in Robotics, </title> <booktitle> volume 336 of Lecture Notes in Computer Science. </booktitle> <publisher> Springer-Verlag, </publisher> <address> Berlin, </address> <year> 1989. </year>
Reference-contexts: The first author was additionally supported by an AT&T Bell Laboratories Graduate Fellowship sponsored by the AT&T Foundation. The Error Detection and Recovery Framework <ref> [12] </ref> provides a natural problem domain in which to apply our strategies. An Error Detection and Recovery (EDR) strategy is one that is guaranteed to achieve a specified goal when the goal is recognizably achievable, and signals failure otherwise. <p> For more details, see Donald <ref> [12] </ref>. 1.2 Related Work The sensor placement problem has previously been addressed by Nelson and Khosla [22] and Kutu-lakos, Dyer, and Lumelsky [18] for visual tracking and vision-guided exploration. Several researchers have explored the problem of optimal sensor placement. <p> Zhang [31] considers the problem of optimally placing multiple sensors. A different approach from the one taken in this paper to the incorporation of sensor planning in the EDR framework was first presented by Donald <ref> [12] </ref>. In that approach, an equivalence is established between sensing and motion in configuration space. Active sensing for a mobile robot is reduced to motion, by exploiting the similarity between visibility and generalized damper motions. In contrast, we present here a framework that is closer to actual sensors. <p> The algorithms account for uncertainty in sensor placement and aim. The Error Detection and Recovery (EDR) system of Donald <ref> [12] </ref> provides a framework for constructing manipulation strategies when guaranteed plans cannot be found or do not exist. An EDR strategy attains the goal when the goal is recognizably reachable, and signals failure otherwise.
Reference: [13] <author> R. L. Graham and F. F. Yao. </author> <title> Finding the convex hull of a simple polygon. </title> <journal> Journal of Algorithms, </journal> <volume> 4(4) </volume> <pages> 324-331, </pages> <year> 1983. </year>
Reference-contexts: If A has m vertices and G has k vertices, CH (A ) and CH (G) can be computed in O (m log m) and O (k log k) time, respectively <ref> [13] </ref>. So CH (A G) = CH (A ) CH (G) has complexity O (m + k) and can be computed in time O (m log m + k log k).
Reference: [14] <author> L. J. Guibas, R. Motwani, and P. Raghavan. </author> <title> The robot localization problem. </title> <booktitle> In SODA, </booktitle> <year> 1992. </year>
Reference-contexts: Guibas, Motwani and Raghavan consider an abstraction of the robot localization problem <ref> [14] </ref>.
Reference: [15] <author> G. Hager and M. Mintz. </author> <title> Computational methods for task-directed sensor data fusion and sensor planning. </title> <journal> IJRR, </journal> <volume> 10(4) </volume> <pages> 285-313, </pages> <month> Aug. </month> <year> 1991. </year>
Reference-contexts: Several researchers have explored the problem of optimal sensor placement. Cameron and Durrant-Whyte [7] and Hager and Mintz <ref> [15] </ref> present a Bayesian approach to optimal sensor placement. Hutchinson [16] introduces the concept of a visual constraint surface to control motion. The idea is to combine position, force, and visual sensing in order to produce error-tolerant motion strategies.
Reference: [16] <author> S. Hutchinson. </author> <title> Exploiting visual constraints in robot motion planning. </title> <booktitle> In Proc. IEEE ICRA, </booktitle> <pages> pages 1722-1727, </pages> <month> Apr. </month> <year> 1991. </year>
Reference-contexts: Several researchers have explored the problem of optimal sensor placement. Cameron and Durrant-Whyte [7] and Hager and Mintz [15] present a Bayesian approach to optimal sensor placement. Hutchinson <ref> [16] </ref> introduces the concept of a visual constraint surface to control motion. The idea is to combine position, force, and visual sensing in order to produce error-tolerant motion strategies. His work builds on that of preimage planners by adding visual feedback to compensate for uncertainty.
Reference: [17] <author> D. P. Huttenlocher, M. E. Leventon, and W. J. Ruck-lidge. </author> <title> Visually-guided navigation by comparing two-dimensional edge images. </title> <booktitle> In CVPR, </booktitle> <pages> pages 842-847, </pages> <year> 1994. </year>
Reference-contexts: The matcher used in the experiment is based on the Hausdorff distance between sets of points and was written by William Rucklidge [25] and has been used extensively in the Cornell Robotics and Vision Laboratory for image comparison, motion tracking, and visually-guided navigation <ref> [17] </ref>. The particular matcher used here is a translation-only matcher that uses a fractional measure of the Hausdorff distance. Matches are found by searching the 2D space of translations of the model, and computing the Hausdorff distance between the image and the translated model.
Reference: [18] <author> K. N. Kutulakos, C. R. Dyer, and V. J. Lumelsky. </author> <title> Provable strategies for vision-guided exploration in three dimensions. </title> <booktitle> In Proc. IEEE ICRA, </booktitle> <pages> pages 1365-1372, </pages> <month> May </month> <year> 1994. </year>
Reference-contexts: For more details, see Donald [12]. 1.2 Related Work The sensor placement problem has previously been addressed by Nelson and Khosla [22] and Kutu-lakos, Dyer, and Lumelsky <ref> [18] </ref> for visual tracking and vision-guided exploration. Several researchers have explored the problem of optimal sensor placement. Cameron and Durrant-Whyte [7] and Hager and Mintz [15] present a Bayesian approach to optimal sensor placement. Hutchinson [16] introduces the concept of a visual constraint surface to control motion.
Reference: [19] <author> S. Lacroix, P. Grandjean, and M. Ghallab. </author> <title> Perception planning for a multi-sensory interpretation machine. </title> <booktitle> In Proc. IEEE ICRA, </booktitle> <pages> pages 1818-1824, </pages> <month> May </month> <year> 1992. </year>
Reference-contexts: Sharma and Hutchinson [26] define a measure of robot motion observability based on the relationship between differential changes in the position of the robot to the corresponding differential changes in the observed visual features. Lacroix, Grandjean, and Ghallab <ref> [19] </ref> describe a method for selecting view points and sensing tasks to confirm an identification hypothesis. Cowan and Kovesi [10] study the problem of automatic camera placement for vision tasks.
Reference: [20] <author> T. Lozano-Perez. </author> <title> Spatial planning: A configuration space approach. </title> <journal> IEEE Trans. on Computers (C-32), </journal> <pages> pages 108-120, </pages> <year> 1983. </year>
Reference-contexts: Note that this is not an approximation; only the outermost tangents with the distinguished polygon (in this case A G) generate shadows in the complete visibility model. We can compute the convex hull of A G efficiently by exploiting the fact that for polygons A and B <ref> [20] </ref> Note that A and B do not need to be convex. So instead of computing CH (A G) explicitly, we simply convolve CH (A ) and CH (G).
Reference: [21] <author> J. Matousek, N. Miller, J. Pach, M. Sharir, S. Sifrony, and E. Welzl. </author> <title> Fat triangles determine linearly many holes. </title> <booktitle> In Proc. IEEE FOCS, </booktitle> <pages> pages 49-58, </pages> <year> 1991. </year>
Reference-contexts: Otherwise, we classify the maximal visibility triangle as an *-fat triangle. After this processing, we now have O (n (n+m)) fat visibility triangles. We can now use a result of Matousek et al. <ref> [21] </ref> on the union of fat triangles. Their result bounds the number of holes in a union of fat triangles. In our case, the "holes" are shadows in a union of visibility triangles.
Reference: [22] <author> B. Nelson and P. K. Khosla. </author> <title> Integrating sensor placement and visual tracking strategies. </title> <booktitle> In Proc. IEEE ICRA, </booktitle> <pages> pages 1351-1356, </pages> <month> May </month> <year> 1994. </year>
Reference-contexts: For more details, see Donald [12]. 1.2 Related Work The sensor placement problem has previously been addressed by Nelson and Khosla <ref> [22] </ref> and Kutu-lakos, Dyer, and Lumelsky [18] for visual tracking and vision-guided exploration. Several researchers have explored the problem of optimal sensor placement. Cameron and Durrant-Whyte [7] and Hager and Mintz [15] present a Bayesian approach to optimal sensor placement.
Reference: [23] <author> J. O'Rourke. </author> <title> Art Gallery Theorems and Algorithms. </title> <publisher> Oxford University Press, </publisher> <address> New York, </address> <year> 1987. </year>
Reference-contexts: Research in the area of art gallery theory has introduced and addressed many problems pertaining to polygon visibility. The art gallery problem is to determine the minimum number of guards sufficient to guard the interior of a simple polygon (see <ref> [23] </ref> for more details). Sensor configuration planning addresses the related question of where sensors should be placed in order to monitor a region of interest. In this case we are interested in external visibility of a polygon rather than internal visibility. <p> The questions of detecting polygon visibility and constructing visibility regions under a variety of assumptions is a rich area of past and ongoing research in computational geometry. We mention here a few of the papers most closely related to our problem. Suri and O'Rourke <ref> [27, 23] </ref> give an fi (n 4 ) algorithm for the problem of computing the locus of points weakly visible from a distinguished edge in an environment of line segments.
Reference: [24] <author> R. Pollack, M. Sharir, and S. Sifrony. </author> <title> Separating two simple polygons by a sequence of translations. </title> <type> Technical Report 215, </type> <institution> Department of Computer Science, New York University, Courant Institute of Mathematical Sciences, </institution> <month> Apr. </month> <year> 1986. </year>
Reference-contexts: Computing a single cell in an arrangement is equivalent to computing the lower envelope of a set of line segments in the plane, which for a set of size n takes time O (nff (n)), where ff (n) is the inverse Ackerman function <ref> [24] </ref>.
Reference: [25] <author> W. J. Rucklidge. </author> <title> Efficient Computation of the Minimum Hausdorff Distance for Visual Recognition. </title> <type> PhD thesis, </type> <institution> Department of Computer Science, Cor-nell University, </institution> <address> Ithaca, NY, </address> <month> Jan. </month> <year> 1995. </year>
Reference-contexts: The matcher used in the experiment is based on the Hausdorff distance between sets of points and was written by William Rucklidge <ref> [25] </ref> and has been used extensively in the Cornell Robotics and Vision Laboratory for image comparison, motion tracking, and visually-guided navigation [17]. The particular matcher used here is a translation-only matcher that uses a fractional measure of the Hausdorff distance.
Reference: [26] <author> R. Sharma and S. Hutchinson. </author> <title> On the observability of robot motion under active camera control. </title> <booktitle> In Proc. IEEE ICRA, </booktitle> <pages> pages 162-167, </pages> <month> May </month> <year> 1994. </year>
Reference-contexts: His work builds on that of preimage planners by adding visual feedback to compensate for uncertainty. Details on the implementation of vision-based control are described by Hutchinson and Casta~no [9]. Sharma and Hutchinson <ref> [26] </ref> define a measure of robot motion observability based on the relationship between differential changes in the position of the robot to the corresponding differential changes in the observed visual features.
Reference: [27] <author> S. Suri and J. O'Rourke. </author> <title> Worst-case optimal algorithms for constructing visibility polygons with holes. </title> <booktitle> In Proc. ACM Symp. on Comp. Geom., </booktitle> <pages> pages 14-23, </pages> <year> 1986. </year>
Reference-contexts: The questions of detecting polygon visibility and constructing visibility regions under a variety of assumptions is a rich area of past and ongoing research in computational geometry. We mention here a few of the papers most closely related to our problem. Suri and O'Rourke <ref> [27, 23] </ref> give an fi (n 4 ) algorithm for the problem of computing the locus of points weakly visible from a distinguished edge in an environment of line segments. <p> For target A at configuration q 2 C r , we construct the partial visibility region using an approach similar to that given by Suri and O'Rourke for computing the region weakly visible from an edge <ref> [27] </ref>. Our algorithm is as follows: Partial visibility algorithm for a stationary target 1. Construct the visibility graph for the entire environment, consisting of distinguished polygon A and obstacles B. 2. Extend each edge of the visibility graph maximally until both ends touch an edge of the environment. <p> Therefore each vertex contributes O (n + m) visibility triangles, so we have O (n (n + m)) visibility triangles overall. In general, the union of these triangles has complexity O (n 2 (n + m) 2 ). As was mentioned in the paper by Suri and O'Rourke <ref> [27] </ref>, the triangles can be output in constant time per triangle: Asano et al. have shown that the visibility edges at a vertex v can be obtained sorted by slope in linear time with Welzl's algorithm for computing the visibility graph [30, 1].
Reference: [28] <author> K. Tarabanis and R. Y. Tsai. </author> <title> Computing occlusion-free viewpoints. </title> <booktitle> In CVPR, </booktitle> <pages> pages 802-807, </pages> <year> 1992. </year>
Reference-contexts: For an environment of total edge complexity n, he gives an O (n 2 ) time algorithm for computing the piecewise-quadratic boundary of the antipenumbra, which will be non-convex and disconnected in general. Tarabanis and Tsai <ref> [28] </ref> examine the question of complete visibility for general polyhedral environments in 3D.
Reference: [29] <author> S. J. Teller. </author> <title> Computing the antipenumbra of an area light source. </title> <booktitle> Computer Graphics (Proceedings SIG-GRAPH '92), </booktitle> <volume> 26(2) </volume> <pages> 139-148, </pages> <month> July </month> <year> 1992. </year>
Reference-contexts: They give algorithms for constructing the regions from which goals may be reached, and show that the complexity of the regions depends on the magnitude of the uncertainty angle. Teller <ref> [29] </ref> solves the weak polygon visibility problem for a special case in 3D. Namely, he computes the antipenumbra (the volume from which some, but not all, of a light source can be seen) of a convex area light source shining through a sequence of convex areal holes in three dimensions.
Reference: [30] <author> E. Welzl. </author> <title> Constructing the visibility graph for n line segments in O(n 2 ) time. </title> <journal> Information Processing Letters, </journal> <volume> 20 </volume> <pages> 167-171, </pages> <year> 1985. </year>
Reference-contexts: was mentioned in the paper by Suri and O'Rourke [27], the triangles can be output in constant time per triangle: Asano et al. have shown that the visibility edges at a vertex v can be obtained sorted by slope in linear time with Welzl's algorithm for computing the visibility graph <ref> [30, 1] </ref>. Thus, the overall time for explicitly computing the boundary of the partial visibility region for target A at any fixed configuration q is O (n 2 (n + m) 2 ).
Reference: [31] <author> H. Zhang. </author> <title> Optimal sensor placement. </title> <booktitle> In Proc. IEEE ICRA, </booktitle> <pages> pages 1825-1830, </pages> <month> May </month> <year> 1992. </year> <pages> Page 16 </pages>
Reference-contexts: They consider the constraints on camera location imposed by resolution and focus requirements, visibility and view angle, and forbidden regions depending on the task. Given values bounding these constraints, they compute the set of camera locations affording complete visibility of a surface in 3D. Zhang <ref> [31] </ref> considers the problem of optimally placing multiple sensors. A different approach from the one taken in this paper to the incorporation of sensor planning in the EDR framework was first presented by Donald [12]. In that approach, an equivalence is established between sensing and motion in configuration space.
References-found: 31

