URL: ftp://ftp.cs.umass.edu/pub/anw/pub/duff/NIPS94a-final.ps.Z
Refering-URL: http://www-anw.cs.umass.edu/People/duff/duff_papers.html
Root-URL: 
Email: bradtke@cs.umass.edu  duff@cs.umass.edu  
Title: Reinforcement Learning Methods for Continuous-Time Markov Decision Problems  
Author: Steven J. Bradtke Michael O. Duff 
Address: Amherst, MA 01003  Amherst, MA 01003  
Affiliation: Computer Science Department University of Massachusetts  Computer Science Department University of Massachusetts  
Abstract: Semi-Markov Decision Problems are continuous time generalizations of discrete time Markov Decision Problems. A number of reinforcement learning algorithms have been developed recently for the solution of Markov Decision Problems, based on the ideas of asynchronous dynamic programming and stochastic approximation. Among these are TD(), Q-learning, and Real-time Dynamic Programming. After reviewing semi-Markov Decision Problems and Bellman's optimality equation in that context, we propose algorithms similar to those named above, adapted to the solution of semi-Markov Decision Problems. We demonstrate these algorithms by applying them to the problem of determining the optimal control for a simple queueing system. We conclude with a discussion of circumstances under which these algorithms may be usefully ap plied.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> A. G. Barto, S. J. Bradtke, and S. P. Singh. </author> <title> Learning to act using real-time dynamic programming. </title> <journal> Artificial Intelligence. </journal> <note> Accepted. </note>
Reference-contexts: 1 Introduction A number of reinforcement learning algorithms based on the ideas of asynchronous dynamic programming and stochastic approximation have been developed recently for the solution of Markov Decision Problems. Among these are Sutton's TD () [10], Watkins' Q-learning [12], and Real-time Dynamic Programming (RTDP) <ref> [1, 3] </ref>. These learning alogorithms are widely used, but their domain of application has been limited to processes modeled by discrete-time Markov Decision Problems (MDP's). This paper derives analogous algorithms for semi-Markov Decision Problems (SMDP's) | extending the domain of applicability to continuous time. <p> Convergence is typically rather slow. Real-Time Dynamic Programming (RTDP) and Adaptive RTDP <ref> [1, 3] </ref> use a system model to speed convergence. RTDP assumes that a system model is known a priori; Adaptive RTDP builds a model as it interacts with the system. <p> Convergence is typically rather slow. Real-Time Dynamic Programming (RTDP) and Adaptive RTDP [1, 3] use a system model to speed convergence. RTDP assumes that a system model is known a priori; Adaptive RTDP builds a model as it interacts with the system. As discussed by Barto et al. <ref> [1] </ref>, these asynchronous DP algorithms can have computational advantages over traditional DP algorithms even when a system model is given.
Reference: [2] <author> D. P. Bertsekas. </author> <title> Dynamic Programming: Deterministic and Stochastic Models. </title> <publisher> Prentice Hall, </publisher> <address> Englewood Cliffs, NJ, </address> <year> 1987. </year>
Reference-contexts: If the process dynamics are governed by a continuous time Markov chain, then the model needed by RTDP can be analytically derived through uniformization <ref> [2] </ref>. In general, however, the model can be very difficult to analytically derive. In these cases Adaptive RTDP can be used to incrementally build a system model through direct interaction with the system.
Reference: [3] <author> S. J. Bradtke. </author> <title> Incremental Dynamic Programming for On-line Adaptive Optimal Control. </title> <type> PhD thesis, </type> <institution> University of Massachusetts, </institution> <year> 1994. </year>
Reference-contexts: 1 Introduction A number of reinforcement learning algorithms based on the ideas of asynchronous dynamic programming and stochastic approximation have been developed recently for the solution of Markov Decision Problems. Among these are Sutton's TD () [10], Watkins' Q-learning [12], and Real-time Dynamic Programming (RTDP) <ref> [1, 3] </ref>. These learning alogorithms are widely used, but their domain of application has been limited to processes modeled by discrete-time Markov Decision Problems (MDP's). This paper derives analogous algorithms for semi-Markov Decision Problems (SMDP's) | extending the domain of applicability to continuous time. <p> Convergence is typically rather slow. Real-Time Dynamic Programming (RTDP) and Adaptive RTDP <ref> [1, 3] </ref> use a system model to speed convergence. RTDP assumes that a system model is known a priori; Adaptive RTDP builds a model as it interacts with the system.
Reference: [4] <author> C. Darken, J. Chang, and J. Moody. </author> <title> Learning rate schedules for faster stochastic gradient search. </title> <booktitle> In Neural Networks for Signal Processing 2 | Proceedings of the 1992 IEEE Workshop. </booktitle> <publisher> IEEE Press, </publisher> <year> 1992. </year>
Reference-contexts: In the limit, the action that is greedy with respect to the Q-function estimate is selected. The temperature and the learning rate ff k are decreased over time using a "search then converge" method <ref> [4] </ref>. denotes a state visited, with n 1 (t) running along the x-axis, and n 2 (t) along the y-axis. The color of each square represents the probability of choosing action 1 (route arrivals to queue 1). Black represents probability 1, white represents probability 0.
Reference: [5] <author> P. Dayan and T. J. Sejnowski. </author> <title> Td(): Convergence with probability 1. </title> <booktitle> Machine Learning, </booktitle> <year> 1994. </year>
Reference: [6] <author> E. V. Denardo. </author> <title> Contraction mappings in the theory underlying dynamic programming. </title> <journal> SIAM Review, </journal> <volume> 9(2) </volume> <pages> 165-177, </pages> <month> April </month> <year> 1967. </year>
Reference-contexts: The TD () learning rule for SMDP's is straightforward to define from here. 4 Q-learning for SMDP's Denardo <ref> [6] </ref> and Watkins [12] define Q , the Q-function corresponding to the policy , as X P xy (a)V (y) (7) Notice that a can be any action. It is not necesarily the action (x) that would be chosen by policy .
Reference: [7] <author> B. Hajek. </author> <title> Optimal control of two interacting service stations. </title> <journal> IEEE-TAC, </journal> <volume> 29 </volume> <pages> 491-499, </pages> <year> 1984. </year>
Reference-contexts: This effort was originally motivated by the desire to apply reinforcement learning methods to problems of adaptive control of queueing systems, and to the problem of adaptive routing in computer networks in particular. We apply the new algorithms to the well-known problem of routing to two heterogeneous servers <ref> [7] </ref>. We conclude with a discussion of circumstances under which these algorithms may be usefully applied. 2 Semi-Markov Decision Problems A semi-Markov process is a continuous time dynamic system consisting of a countable state set, X , and a finite action set, A. <p> There are two actions available at every state: if an arrival occurs, route it to queue 1 or route it to queue 2. It is known for this problem (and many like it <ref> [7] </ref>), that the optimal policy is a threshold policy; i.e., the set of states S 1 for which it is optimal to route to the first queue is characterized by a monotonically nondecreasing threshold function F via S 1 = f (n 1 ; n 2 )jn 1 F (n 2
Reference: [8] <author> T. Jaakkola, M. I. Jordan, and S. P. Singh. </author> <title> On the convergence of stochastic iterative dynamic programming algorithms. </title> <booktitle> Neural Computation, </booktitle> <year> 1994. </year>
Reference: [9] <author> S. M. Ross. </author> <title> Applied Probability Models with Optimization Applications. </title> <publisher> Holden-Day, </publisher> <address> San Francisco, </address> <year> 1970. </year>
Reference-contexts: Suppose that the system is originally observed to be in state x 2 X , and that action a 2 A is applied. A semi-Markov process <ref> [9] </ref> then evolves as follows: * The next state, y, is chosen according to the transition probabilities P xy (a) * A reward rate (x; a) is defined until the next transition occurs * Conditional on the event that the next state is y, the time until the tran sition from
Reference: [10] <author> R. S. Sutton. </author> <title> Learning to predict by the method of temporal differences. </title> <journal> Machine Learning, </journal> <volume> 3 </volume> <pages> 9-44, </pages> <year> 1988. </year>
Reference-contexts: 1 Introduction A number of reinforcement learning algorithms based on the ideas of asynchronous dynamic programming and stochastic approximation have been developed recently for the solution of Markov Decision Problems. Among these are Sutton's TD () <ref> [10] </ref>, Watkins' Q-learning [12], and Real-time Dynamic Programming (RTDP) [1, 3]. These learning alogorithms are widely used, but their domain of application has been limited to processes modeled by discrete-time Markov Decision Problems (MDP's). <p> optimal value function for an SMDP satisfies the following version of the Bellman optimality equation: V fl (x) = max 8 : y2X Z 1 Z t e fis (x; a)dsdF xy (tja) + X P xy (a) 0 9 ; 3 Temporal Difference learning for SMDP's Sutton's TD (0) <ref> [10] </ref> is a stochastic approximation method for finding solutions to the system of equations (2).
Reference: [11] <author> J. N. Tsitsiklis. </author> <title> Asynchronous stochastic approximation and Q-learning. </title> <type> Technical Report LIDS-P-2172, </type> <institution> Laboratory for Information and Decision Systems, MIT, </institution> <address> Cambridge, MA, </address> <year> 1993. </year>
Reference-contexts: The TD (0) update rule for MDP's is where ff k is the learning rate. The sequence of value-function estimates generated by the TD (0) proceedure will converge to the true solution, V , with probability one <ref> [5,8, 11] </ref> under the appropriate conditions on the ff k and on the definition of the MDP.
Reference: [12] <author> C. J. C. H. Watkins. </author> <title> Learning from Delayed Rewards. </title> <type> PhD thesis, </type> <institution> Cambridge University, </institution> <address> Cambridge, England, </address> <year> 1989. </year>
Reference-contexts: 1 Introduction A number of reinforcement learning algorithms based on the ideas of asynchronous dynamic programming and stochastic approximation have been developed recently for the solution of Markov Decision Problems. Among these are Sutton's TD () [10], Watkins' Q-learning <ref> [12] </ref>, and Real-time Dynamic Programming (RTDP) [1, 3]. These learning alogorithms are widely used, but their domain of application has been limited to processes modeled by discrete-time Markov Decision Problems (MDP's). This paper derives analogous algorithms for semi-Markov Decision Problems (SMDP's) | extending the domain of applicability to continuous time. <p> The TD () learning rule for SMDP's is straightforward to define from here. 4 Q-learning for SMDP's Denardo [6] and Watkins <ref> [12] </ref> define Q , the Q-function corresponding to the policy , as X P xy (a)V (y) (7) Notice that a can be any action. It is not necesarily the action (x) that would be chosen by policy . The function Q fl corresponds to the optimal policy. <p> Equation (7) can be rewritten as Q (x; a) = R (x; a) + fl y2X and Q fl satisfies the Bellman-style optimality equation Q fl (x; a) = R (x; a) + fl y2X a 0 2A Q-learning, first described by Watkins <ref> [12] </ref>, uses stochastic approximation to iteratively refine an estimate for the function Q fl . The Q-learning rule is very similar to TD (0).
References-found: 12

