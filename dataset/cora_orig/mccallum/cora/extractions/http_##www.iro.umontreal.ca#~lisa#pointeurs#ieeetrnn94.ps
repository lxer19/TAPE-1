URL: http://www.iro.umontreal.ca/~lisa/pointeurs/ieeetrnn94.ps
Refering-URL: http://www.iro.umontreal.ca/labs/neuro/bib/journals/journals.html
Root-URL: http://www.iro.umontreal.ca
Title: Learning Long-Term Dependencies with Gradient Descent is Difficult  
Author: Yoshua Bengioy, Patrice Simardy, and Paolo Frasconiz 
Note: Paper to appear in the special issue on Recurrent Networks of the IEEE Transactions on Neural Networks  
Affiliation: yAT&T Bell Laboratories zDip. di Sistemi e Informatica, Universita di Firenze  
Abstract: Recurrent neural networks can be used to map input sequences to output sequences, such as for recognition, production or prediction problems. However, practical difficulties have been reported in training recurrent neural networks to perform tasks in which the temporal contingencies present in the input/output sequences span long intervals. We show why gradient based learning algorithms face an increasingly difficult problem as the duration of the dependencies to be captured increases. These results expose a trade-off between efficient learning by gradient descent and latching on information for long periods. Based on an understanding of this problem, alternatives to standard gradient descent are considered. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> P.L. Bartlett, and T. Downs, </author> <title> "Using Random Weights to train Multilayer Networks of Hard-Limiting Units," </title> <journal> IEEE Transactions on Neural Networks, </journal> <volume> vol. 3, no. 2, </volume> <year> 1992, </year> <pages> pp. 202-210. </pages>
Reference-contexts: Other methods have been explored in order to train layered networks of hard threshold units. For example, in <ref> [1] </ref> it is shown how to train two layered networks using a probabilistic 18 approach. In [9] a method is proposed that iterates two training steps: adjusting the network internal representation (units activations) and training the parameters to produce such representation.
Reference: [2] <author> S. Becker and Y. Le Cun, </author> <title> "Improving the convergence of back-propagation learning with second order methods", </title> <booktitle> Proceedings of the 1988 Connectionist Models Summer School, </booktitle> <editor> (eds. Touretzky, Hinton and Sejnowski), </editor> <publisher> Morgan Kaufmann, </publisher> <pages> pp. 29-37. </pages>
Reference-contexts: When a better point is found, it reduces the size of the hyperrectangle (by a factor of 0.9 in the experiments) and re-centers it around the new point. The stopping criterion is the same as for simulated annealing. 5.3 Time-Weighted Pseudo-Newton Optimization The pseudo-Newton algorithm <ref> [2] </ref> for neural networks has the advantage of re-scaling the learning rate of each weight dynamically to match the curvature of the energy function with respect to that weight. This is of interest because adjusting the learning rate could potentially circumvent the problem of vanishing gradient. <p> Consequently, updating w according to equation 6 does not actually follow the gradient (but neither would following equation 5). Instead, several gradient contributions are weighted using second derivatives, in order to make faster moves in the flatter directions. Like for the pseudo-Newton algorithm of <ref> [2] </ref>, we prefer using a diagonal approximation of the Hessian 2 The idea of using second derivatives in this way was inspired from discussions with L. Bottou. 17 which is cheap to compute and guaranteed to be positive. j is a global learning rate (0.01 in our experiments).
Reference: [3] <author> Y. Bengio, P. Frasconi, P. Simard, </author> <title> "The problem of learning long-term dependencies in recurrent networks", </title> <booktitle> invited paper at the IEEE International Conference on Neural Networks 1993, </booktitle> <address> San Francisco, </address> <publisher> IEEE Press. </publisher>
Reference-contexts: A set of simulations were carried out to evaluate the effectiveness of back-propagation (through time) on this simple task. In a first experiment we investigated the effect of the noise variance s and of different initial values w 0 for the self loop weight (see also <ref> [3] </ref>.) A density plot of convergence is shown in Fig. 2a, averaged over 18 runs for each of the selected pairs (w 0 ; s). It can be seen that convergence becomes very unlikely for large noise variance or small initial values of w. <p> This approach is in no way optimal and many other discrete error propagation algorithms are possible. Another very promising approach for instance is the trainable discrete flip-flop unit <ref> [3] </ref> which also preserves error information in time. Our only claim here is that discrete propagation of error offers interesting solutions to the vanishing gradient problem in recurrent network. Our preliminary results on toy problems (see next subsection and [3]) confirm this hypothesis. 5.5 Experimental Results Experiments were performed to evaluate <p> very promising approach for instance is the trainable discrete flip-flop unit <ref> [3] </ref> which also preserves error information in time. Our only claim here is that discrete propagation of error offers interesting solutions to the vanishing gradient problem in recurrent network. Our preliminary results on toy problems (see next subsection and [3]) confirm this hypothesis. 5.5 Experimental Results Experiments were performed to evaluate various alternative optimization approaches on problems on which one can increase the temporal span of input/output dependencies.
Reference: [4] <author> Y. Bengio, </author> <title> "Artificial Neural Networks and their Application to Sequence Recognition," </title> <type> Ph.D. Thesis, </type> <institution> McGill University, (Computer Science), </institution> <address> 1991, Montreal, Qc., Canada. </address>
Reference-contexts: A task displays long-term dependencies if computation of the desired output at time t depends on input presented at an earlier time o o t. Although recurrent networks can in many instances outperform static networks <ref> [4] </ref>, they appear more difficult to train optimally. Earlier experiments indicated that their parameters settle in sub-optimal solutions which take into account short-term dependencies but not long-term dependencies [5]. Similar results were obtained by Mozer [19]. <p> temporal span of input/output dependencies increasing. * The time-weighted pseudo-Newton algorithm appears to perform better than the other two variants of back-propagation but its performance also appears to worsen with increasing sequence length. 6 Conclusion Recurrent networks are very powerful in their ability to represent context, often outperforming static networks <ref> [4] </ref>. However, we have presented theoretical and experimental evidence showing that gradient descent of an error criterion may be inadequate to train them for tasks involving long-term dependencies.
Reference: [5] <author> Y. Bengio, R. De Mori, G. Flammia, and R. Kompe, </author> <title> "Global Optimization of a Neural Network Hidden Markov Model Hybrid," </title> <journal> IEEE Transactions on Neural Networks, </journal> <volume> vol. 3, no. 2, </volume> <year> 1992, </year> <pages> pp. 252-259. </pages>
Reference-contexts: Although recurrent networks can in many instances outperform static networks [4], they appear more difficult to train optimally. Earlier experiments indicated that their parameters settle in sub-optimal solutions which take into account short-term dependencies but not long-term dependencies <ref> [5] </ref>. Similar results were obtained by Mozer [19]. It was found that back-propagation was not sufficiently powerful to discover contingencies spanning long temporal intervals. In this paper, we present experimental and theoretical results in order to further the understanding of this problem.
Reference: [6] <author> A. Corana, M. Marchesi, C. Martini, and S. Ridella, </author> <title> "Minimizing Multimodal Functions of Continuous Variables with the Simulated Annealing Algorithm", </title> <journal> ACM Transactions on Mathematical Software, </journal> <volume> vol. 13, no. 13, </volume> <month> Sept. </month> <year> 1987, </year> <pages> pp. 262-280. </pages>
Reference-contexts: However, there are many instances where many long-term input/output dependencies are unknown and have to be learned from examples. 5.1 Simulated Annealing Global search methods such as simulated annealing can be applied to such problems, but they are generally very slow. We implemented the simulated annealing algorithm presented in <ref> [6] </ref> for optimizing functions of continuous variables. This is a "batch learning" algorithm (updating parameters after all examples of the training set have been seen). It performs a cycle of random moves, each along one coordinate (parameter) direction. Each point is accepted or rejected according to the Metropolis criterion [13].
Reference: [7] <author> P. Frasconi, M. Gori, and G. </author> <title> Soda, "Local Feedback Multilayered Networks", </title> <booktitle> Neural Computation 3, </booktitle> <year> 1992, </year> <pages> pp. 120-130. </pages>
Reference-contexts: It is local in time like the forward propagation algorithms and it requires computation only proportional to the number of weights, like the back-propagation through time algorithm. Unfortunately, the networks it can deal with have limited storage capabilities for dealing with general sequences <ref> [7] </ref>, thus limiting their representational power. A task displays long-term dependencies if computation of the desired output at time t depends on input presented at an earlier time o o t. Although recurrent networks can in many instances outperform static networks [4], they appear more difficult to train optimally. <p> = w f (a k t t = 1 : : : T 0 = a 1 (1) If w &gt; 1=f 0 (0) = 1, then the autonomous dynamic of this neuron has two attractors x &gt; 0 and x that depend on the value of the weight w <ref> [7, 8] </ref> (they can be easily obtained as non zero intersections of the curve x = tanh (a) with the line x = a=w).
Reference: [8] <author> P. Frasconi, M. Gori, M. Maggini, and G. </author> <title> Soda, "Unified Integration of Explicit Rules and Learning by Example in Recurrent Networks," </title> <journal> IEEE Trans. on Knowledge and Data Engineering, </journal> <note> in press. </note>
Reference-contexts: = w f (a k t t = 1 : : : T 0 = a 1 (1) If w &gt; 1=f 0 (0) = 1, then the autonomous dynamic of this neuron has two attractors x &gt; 0 and x that depend on the value of the weight w <ref> [7, 8] </ref> (they can be easily obtained as non zero intersections of the curve x = tanh (a) with the line x = a=w). <p> Assuming that the initial state at t = 0 is x 0 = x, it can be shown <ref> [8] </ref> that there exists a value h fl &gt; 0 of the input such that, (1) x t maintains its sign if jh t j &lt; h fl 8t , and, (2) there exists a finite number of steps L 1 such that x L 1 &gt; x if h t <p> One way to help in the training of recurrent networks is to set their connectivity and initial weights (and even constraints on the weights) using prior knowledge. For example, this is accomplished in <ref> [8] </ref> and [11] using prior rules and sequentiality constraints. In fact, the results in this paper strongly suggest that when such prior knowledge is given, it should be used, since the learning problem itself is so difficult. <p> It says that gradient descent becomes increasingly inefficient when the temporal span of the dependencies increases. Furthermore, for a given problem, there are sometimes ways to help the training by setting the network connectivity and initial weights (and even constraints on the weights) using prior knowledge (e.g., <ref> [8] </ref>, [11]). For some tasks, it is also possible to present a variety of examples of the input/output dependencies, including short-term dependencies which are sufficient to infer similar but longer term dependencies.
Reference: [9] <author> R.J. Gaynier, and T. Downs, </author> <title> "A Method of Training Multi-layer Networks with Heav-iside Characteristics Using Internal Representations," </title> <booktitle> IEEE International Conference on Neural Networks 1993, </booktitle> <address> San Francisco, </address> <pages> pp. 1812-1817. 29 </pages>
Reference-contexts: Other methods have been explored in order to train layered networks of hard threshold units. For example, in [1] it is shown how to train two layered networks using a probabilistic 18 approach. In <ref> [9] </ref> a method is proposed that iterates two training steps: adjusting the network internal representation (units activations) and training the parameters to produce such representation. This algorithm can be applied to recurrent networks as well.
Reference: [10] <author> M. Gori, Y. Bengio and R. De Mori, </author> <title> "BPS: a learning algorithm for capturing the dynamic nature of speech," </title> <booktitle> Proc. IEEE Int. Joint Conf. on Neural Networks, </booktitle> <address> Wash-ington DC, </address> <year> 1989, </year> <pages> pp. </pages> <month> II.417-II.424. </month>
Reference-contexts: Other algorithms, such as the forward propagation algorithms [14, 23], are much more computationally expensive (for a fully connected recurrent network) but are local in time, i.e., they can be applied in an on-line fashion, producing a partial gradient after each time step. Another algorithm was proposed <ref> [10, 18] </ref> for 2 training constrained recurrent networks in which dynamic neurons with a single feedback to themselves have only incoming connections from the input layer.
Reference: [11] <author> C.L. Giles and C.W. Omlin, </author> <title> "Inserting Rules into Recurrent Neural Networks", Neural Networks for Signal Processing II, </title> <booktitle> Proceedings of the 1992 IEEE workshop, </booktitle> <editor> (eds. Kung, Fallside, Sorenson and Kamm), </editor> <publisher> IEEE Press, </publisher> <pages> pp. 13-22. </pages>
Reference-contexts: One way to help in the training of recurrent networks is to set their connectivity and initial weights (and even constraints on the weights) using prior knowledge. For example, this is accomplished in [8] and <ref> [11] </ref> using prior rules and sequentiality constraints. In fact, the results in this paper strongly suggest that when such prior knowledge is given, it should be used, since the learning problem itself is so difficult. <p> It says that gradient descent becomes increasingly inefficient when the temporal span of the dependencies increases. Furthermore, for a given problem, there are sometimes ways to help the training by setting the network connectivity and initial weights (and even constraints on the weights) using prior knowledge (e.g., [8], <ref> [11] </ref>). For some tasks, it is also possible to present a variety of examples of the input/output dependencies, including short-term dependencies which are sufficient to infer similar but longer term dependencies.
Reference: [12] <author> T. Grossman, R. Meir and E. Domany, </author> <title> "Learning by choice of internal representation", Neural Information Processing Systems 1, </title> <editor> (ed. D.S. </editor> <booktitle> Touretzky), </booktitle> <pages> pp. 73-80. </pages>
Reference-contexts: This algorithm can be applied to recurrent networks as well. Both methods take advantage of probabilities in order to make differentiable the error function, thus permitting the use of gradient descent. Another approach, proposed in <ref> [12] </ref>, applies to two layer networks. The space of activities of hidden units is searched in a greedy way in order reduce output error. An earlier algorithm also related to the one presented here, but based on the propagation of targets was proposed in [16].
Reference: [13] <author> S. Kirkpatrick, </author> <title> C.D. Gelatt, M.P. Vecchi, "Optimization by simulated annealing", </title> <booktitle> Science 220, </booktitle> <month> 4598 (May </month> <year> 1983), </year> <month> pp.671-680. </month>
Reference-contexts: This is a "batch learning" algorithm (updating parameters after all examples of the training set have been seen). It performs a cycle of random moves, each along one coordinate (parameter) direction. Each point is accepted or rejected according to the Metropolis criterion <ref> [13] </ref>. New points are selected according to a uniform distribution inside a hyperrectangle around the last point. The dimensions of the hyperrectangle are updated in order to maintain the average percentage of accepted moves at about one-half of the total number of moves.
Reference: [14] <author> Kuhn G., </author> <title> "A first look at phonetic discrimination using connectionist models with recurrent links." </title> <institution> CCRP IDA SCIMP working paper No.4/87, Institute for Defense Analysis, Princeton, NJ, </institution> <year> 1987. </year>
Reference-contexts: The backward phase is also backward in time and recursively uses these activations to compute the required gradients. Other algorithms, such as the forward propagation algorithms <ref> [14, 23] </ref>, are much more computationally expensive (for a fully connected recurrent network) but are local in time, i.e., they can be applied in an on-line fashion, producing a partial gradient after each time step.
Reference: [15] <author> K.J. Lang and G.E. Hinton, </author> <title> "The development of the Time-Delay Neural Network architecture for speech recognition", </title> <type> Technical Report CMU-CS-88-152, </type> <institution> Carnegie-Mellon University, </institution> <year> 1988. </year>
Reference-contexts: In contrast, static networks (i.e., with no recurrent connection), even if they include delays (such as Time Delay Neural Networks <ref> [15] </ref>), have a finite impulse response and can't store a bit of information for an indefinite time.
Reference: [16] <author> Y. Le Cun, </author> <title> "Learning Processes in an Asymmetric Threshold Network", in Disordered systems and biological organization, </title> <editor> (eds. Bienenstock, E. and Fogelman-Soulie, F. and Weisbuch, G.), </editor> <publisher> Springer-Verlag, </publisher> <address> Les Houches, France, </address> <year> 1986, </year> <pages> pp. 233-240. </pages>
Reference-contexts: Another approach, proposed in [12], applies to two layer networks. The space of activities of hidden units is searched in a greedy way in order reduce output error. An earlier algorithm also related to the one presented here, but based on the propagation of targets was proposed in <ref> [16] </ref>. The algorithm introduced here, instead, relies on propagating discrete error information, obtained with a finite difference approach. A neural network can be represented as a series of local elements with each a forward propagation function and an error propagation function.
Reference: [17] <author> C.M. Marcus, F.R. Waugh, and R.M. Westervelt, </author> <title> "Nonlinear Dynamics and Stability of Analog Neural Networks", </title> <journal> Physica D 51 (special issue), </journal> <year> 1991, </year> <pages> pp. 234-247. 30 </pages>
Reference-contexts: In particular, for a network defined by a t = W tanh (a t1 ) + u t , if W is symmetric and its minimum eigenvalue is greater than -1, then the attractors are all fixed points <ref> [17] </ref>. On the other hand, if jW j &lt; 1 or if the system is linear and stable, the system has a single fixed point attractor at the origin.
Reference: [18] <author> Mozer M.C. </author> <title> "A focused back-propagation algorithm for temporal pattern recognition", </title> <journal> Complex Systems, </journal> <volume> 3, </volume> <year> 1989, </year> <pages> pp. 349-391. </pages>
Reference-contexts: Other algorithms, such as the forward propagation algorithms [14, 23], are much more computationally expensive (for a fully connected recurrent network) but are local in time, i.e., they can be applied in an on-line fashion, producing a partial gradient after each time step. Another algorithm was proposed <ref> [10, 18] </ref> for 2 training constrained recurrent networks in which dynamic neurons with a single feedback to themselves have only incoming connections from the input layer.
Reference: [19] <author> Mozer M.C., </author> <title> "Induction of multiscale temporal structure", </title> <booktitle> Advances in Neural Information Processing Systems 4, </booktitle> <editor> (eds. Moody, Hanson, Lippman), </editor> <publisher> Morgan Kaufmann, </publisher> <year> 1992, </year> <pages> pp. 275-282. </pages>
Reference-contexts: Although recurrent networks can in many instances outperform static networks [4], they appear more difficult to train optimally. Earlier experiments indicated that their parameters settle in sub-optimal solutions which take into account short-term dependencies but not long-term dependencies [5]. Similar results were obtained by Mozer <ref> [19] </ref>. It was found that back-propagation was not sufficiently powerful to discover contingencies spanning long temporal intervals. In this paper, we present experimental and theoretical results in order to further the understanding of this problem.
Reference: [20] <author> Ortega J.M. and Rheinboldt W.C. </author> <title> Iterative Solution of Non-linear Equations in Several Variables and Systems of Equations, </title> <publisher> Academic Press, </publisher> <address> New York, </address> <year> 1960. </year>
Reference-contexts: Theorem 2 Let M be a differentiable mapping on a convex set D. If 8x 2 D; jM 0 (x)j &lt; 1, then M is contracting on D. Proof: See <ref> [20] </ref>. <p> By Lagrange's mean value theorem and convexity of D t , 9z 2 D t s.t. kM (x) M (y)k jM 0 (z)jkx yk, but jM 0 (z)j &lt; t by hypothesis. Then by the contraction theorem <ref> [20] </ref> we have ae t+1 t d + b t . Now by hypothesis we have b t = (1 t )d, so ae t+1 &lt; d.
Reference: [21] <author> Rohwer R. </author> <title> "The `Moving Targets' Training Algorithm", </title> <booktitle> Advances in Neural Information Processing Systems 2, </booktitle> <editor> (ed. Touretzky), </editor> <publisher> Morgan Kaufmann, </publisher> <year> 1990, </year> <pages> pp. 558-565. </pages>
Reference-contexts: We restrict here our attention to discrete-time systems. Learning algorithms used for recurrent networks are usually based on computing the gradient of a cost function with respect to the weights of the network <ref> [22, 21] </ref>. For example, the back-propagation through time algorithm [22] is a generalization of back-propagation for static networks in which one stores the activations of the units while going forward in time. The backward phase is also backward in time and recursively uses these activations to compute the required gradients.
Reference: [22] <author> D.E. Rumelhart, G.E. Hinton and R.J. Williams, </author> <title> "Learning internal representation by error propagation," Parallel Distributed Processing volume 1. </title> <editor> Rumelhart D.E. and McClelland J.L. (eds.), </editor> <publisher> Bradford Books, MIT Press, </publisher> <year> 1986, </year> <pages> pp. 318-362. </pages>
Reference-contexts: We restrict here our attention to discrete-time systems. Learning algorithms used for recurrent networks are usually based on computing the gradient of a cost function with respect to the weights of the network <ref> [22, 21] </ref>. For example, the back-propagation through time algorithm [22] is a generalization of back-propagation for static networks in which one stores the activations of the units while going forward in time. The backward phase is also backward in time and recursively uses these activations to compute the required gradients. <p> We restrict here our attention to discrete-time systems. Learning algorithms used for recurrent networks are usually based on computing the gradient of a cost function with respect to the weights of the network [22, 21]. For example, the back-propagation through time algorithm <ref> [22] </ref> is a generalization of back-propagation for static networks in which one stores the activations of the units while going forward in time. The backward phase is also backward in time and recursively uses these activations to compute the required gradients.

References-found: 22

