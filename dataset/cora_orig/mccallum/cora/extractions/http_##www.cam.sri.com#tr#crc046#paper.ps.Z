URL: http://www.cam.sri.com/tr/crc046/paper.ps.Z
Refering-URL: http://www.cam.sri.com/tr/ABSTRACTS.html
Root-URL: 
Title: Estimating Performance of Pipelined Spoken Language Translation Systems  
Author: Manny Rayner and David Carter Patti Price Bertil Lyberg 
Address: Suite 23, Millers Yard Cambridge CB2 1RQ, UK  333 Ravenswood Ave. Menlo Park, CA 94025, USA  Rudsjoterassen 2 S-136 80 Haninge Sweden  
Affiliation: SRI International  SRI International  Telia Research AB  
Note: Conference on Spoken Language Processing, Yokohama,  
Email: fmanny,dmcg@cam.sri.com  pprice@speech.sri.com  Bertil.Lyberg@haninge.trab.se  
Web: URL: http://www.cam.sri.com/tr/crc046/paper.ps.ZInternational  
Date: 1994  July 27, 1994  
Abstract: Most spoken language translation systems developed to date rely on a pipelined architecture, in which the main stages are speech recognition, linguistic analysis, transfer, generation and speech synthesis. When making projections of error rates for systems of this kind, it is natural to assume that the error rates for the individual components are independent, making the system accuracy the product of the component accuracies. The paper reports experiments carried out using the SRI-SICS-Telia Research Spoken Language Translator and a 1000-utterance sample of unseen data. The results suggest that the naive performance model leads to serious overestimates of system error rates, since there are in fact strong dependencies between the components. Predicting the system error rate on the independence assumption by simple multiplication resulted in a 16% proportional overestimate for all utterances, fl This paper appears in the Proceedings of the International Conference on Spoken Language Processing, Yokohama, September 1994. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Agnas, M-S., Alshawi, H., Bretan, I., Carter, D.M. Ceder, K., Collins, M., Crouch, R., Digalakis, V., Ekholm, B., Gamback, B., Kaja, J., Karlgren, J., Lyberg, B., Price, P., Pulman, S., Rayner, M., 8 Samuelsson, C. and Svensson, T., </author> <title> Spoken Language Translator: First Year Report, </title> <type> joint SRI/SICS technical report, </type> <year> 1994. </year>
Reference-contexts: Intuitively, utterances which are hard to hear are also hard to understand and translate. The experiments reported were carried out on the SRI-SICS-Telia Research Spoken Language Translator <ref> [9, 11, 1] </ref>, using a 1000-utterance sample of previously unseen data. Processing was split into four phases, and the partial results for each phase evaluated by skilled judges. <p> Work on the project began in June 1992. The system is constructed from a set of general-purpose speech and language processing components. All the components existed prior to the start of the project; they have been adapted to the ATIS speech translation task in ways described at length elsewhere <ref> [9, 1] </ref>. In most cases, the customization process was fairly simple, and was performed using semiautomatic training methods. <p> Finally, the output text string is passed to the Prophon speech synthesizer [5], where it is converted into output speech using a polyphone synthesis method. The phrase-structure tree is used to improve the prosodic quality of the result. The SLT system is described in detail in <ref> [1] </ref>. 4 3 EXPERIMENTS Many researchers working in the field of automatic spoken language understanding have made the informal observation that utterances hard for one module in an integrated system have a greater than average chance of being hard for other modules; this effect is sometimes referred to as "synergy".
Reference: [2] <author> Alshawi, H., </author> <title> The Core Language Engine, </title> <address> Cambridge, Massachusetts: </address> <publisher> The MIT Press, </publisher> <year> 1992. </year>
Reference-contexts: N is normally set to a value between 5 and 10. 3 The source-language (English) copy of the CLE then performs linguistic analysis on all the utterance hypotheses in the N-best list. The CLE is a sophisticated unification-based language processing system which incorporates a broad-coverage domain-independent grammar for English <ref> [2] </ref>. In the SLT system, the general CLE grammar is specialized to the domain using the Explanation-Based Learning (EBL) algorithm [12]. The resulting grammar is parsed using an LR parser [13], giving a decrease in analysis time, compared to the normal CLE left-corner parser, of about a factor of ten.
Reference: [3] <author> Alshawi, H. and Carter, </author> <title> D.M., Training and Scaling Preference Functions for Disambiguation, </title> <note> To appear in Computational Linguistics, 1995. Also available as SRI technical report. </note>
Reference-contexts: The most plausible analysis (and hence, implicitly, the most plausible utterance hypothesis) is then selected by the "preference module". This module applies a variety of preference functions to each analysis, and combines their scores using scaling factors trained using a combination of least-squares optimization and hill-climbing <ref> [3, 10] </ref>. The training material for both the Explanation-Based Learning specialization process and the preference module comes from a "treebank" of about 5000 hand-verified examples.
Reference: [4] <author> Alshawi, H., Carter, D., Rayner, M. and Gamback, B., </author> <title> "Transfer through Quasi Logical Form", </title> <booktitle> Proc. 29th ACL, </booktitle> <address> Berkeley, </address> <year> 1991. </year>
Reference-contexts: The QLF selected by the preference module is passed to the transfer component, which uses a set of non-deterministic unification-based recursive rewriting rules to derive a set of possible corresponding target-language (Swedish) QLFs <ref> [4] </ref>. The preference component is then called again to select the most plausible transferred QLF. This is passed to a second copy of the CLE, loaded with a Swedish grammar, to generate a target-language text string. The Swedish grammar has been adapted fairly directly from the English one [7].
Reference: [5] <author> Ceder, K. and Lyberg, B., </author> <title> "Yet Another Rule Compiler for Text-to-Speech Conversion?", </title> <booktitle> Proc. ICSLP, </booktitle> <address> Banff, </address> <year> 1993. </year>
Reference-contexts: The Swedish grammar has been adapted fairly directly from the English one [7]. Generation is performed using the Semantic Head-Driven algorithm [14], which simultaneously constructs a phrase-structure tree as part of the generation process. Finally, the output text string is passed to the Prophon speech synthesizer <ref> [5] </ref>, where it is converted into output speech using a polyphone synthesis method. The phrase-structure tree is used to improve the prosodic quality of the result.
Reference: [6] <author> Digalakis, V. and Murveit, H., "Genones: </author> <title> Optimizing the Degree of Tying in a Large Vocabulary HMM Speech Recognizer", </title> <booktitle> Proc. of the Inter. Conf. on Acoust., Speech and Signal Proc., </booktitle> <year> 1994. </year>
Reference-contexts: The speech translation process begins with the SRI DECIPHER (TM) system, based on hidden Markov modeling and a progressive search <ref> [8, 6] </ref>. It outputs to the source language processor an N-best list of sentence hypotheses generated using acoustic and bigram language model scores.
Reference: [7] <author> Gamback, B. and Rayner, M., </author> <title> "The Swedish Core Language Engine", </title> <booktitle> Proc. 3rd NOTEX, </booktitle> <address> Linkoping, </address> <year> 1992. </year>
Reference-contexts: The preference component is then called again to select the most plausible transferred QLF. This is passed to a second copy of the CLE, loaded with a Swedish grammar, to generate a target-language text string. The Swedish grammar has been adapted fairly directly from the English one <ref> [7] </ref>. Generation is performed using the Semantic Head-Driven algorithm [14], which simultaneously constructs a phrase-structure tree as part of the generation process. Finally, the output text string is passed to the Prophon speech synthesizer [5], where it is converted into output speech using a polyphone synthesis method.
Reference: [8] <author> Murveit, H., Butzberger, J., Digalakis, V. and Weintraub, M., </author> <title> "Large Vocabulary Dictation using SRI's DECIPHER(TM) Speech Recognition System: Progressive Search Techniques", </title> <booktitle> Proc. of the Inter. Conf. on Acoust., Speech and Signal Proc., </booktitle> <address> Minneapolis, Minnesota, </address> <month> April </month> <year> 1993. </year>
Reference-contexts: The speech translation process begins with the SRI DECIPHER (TM) system, based on hidden Markov modeling and a progressive search <ref> [8, 6] </ref>. It outputs to the source language processor an N-best list of sentence hypotheses generated using acoustic and bigram language model scores.
Reference: [9] <author> Rayner, M., Alshawi, H., Bretan, I., Carter, D.M., Digalakis, V., Gamback, B., Kaja, J., Karlgren, J., Lyberg, B., Price, P., Pulman, S. and Samuelsson, C., </author> <title> "A Speech to Speech Translation System Built From Standard Components". </title> <booktitle> Proc. ARPA workshop on Human Language Technology, </booktitle> <year> 1993 </year>
Reference-contexts: Intuitively, utterances which are hard to hear are also hard to understand and translate. The experiments reported were carried out on the SRI-SICS-Telia Research Spoken Language Translator <ref> [9, 11, 1] </ref>, using a 1000-utterance sample of previously unseen data. Processing was split into four phases, and the partial results for each phase evaluated by skilled judges. <p> Work on the project began in June 1992. The system is constructed from a set of general-purpose speech and language processing components. All the components existed prior to the start of the project; they have been adapted to the ATIS speech translation task in ways described at length elsewhere <ref> [9, 1] </ref>. In most cases, the customization process was fairly simple, and was performed using semiautomatic training methods.
Reference: [10] <author> Rayner, M., D. Carter, V. Digalakis and P. Price, </author> <title> "Combining Knowledge Sources to Reorder N-Best Speech Hypothesis Lists". </title> <booktitle> To appear in Proc. ARPA workshop on Human Language Technology, </booktitle> <year> 1994 </year>
Reference-contexts: The most plausible analysis (and hence, implicitly, the most plausible utterance hypothesis) is then selected by the "preference module". This module applies a variety of preference functions to each analysis, and combines their scores using scaling factors trained using a combination of least-squares optimization and hill-climbing <ref> [3, 10] </ref>. The training material for both the Explanation-Based Learning specialization process and the preference module comes from a "treebank" of about 5000 hand-verified examples.
Reference: [11] <author> Rayner, M., Bretan, I., Carter, D., Collins, M., Digalakis, V., Gamback, B., Kaja, J., Karlgren, J., Lyberg, B., Price, P., Pulman S. and Samuelsson, C., </author> <title> "Spoken Language Translation with Mid-90's 9 Technology: A Case Study". </title> <booktitle> Proceedings of Eurospeech '93, </booktitle> <address> Berlin, </address> <year> 1993. </year>
Reference-contexts: Intuitively, utterances which are hard to hear are also hard to understand and translate. The experiments reported were carried out on the SRI-SICS-Telia Research Spoken Language Translator <ref> [9, 11, 1] </ref>, using a 1000-utterance sample of previously unseen data. Processing was split into four phases, and the partial results for each phase evaluated by skilled judges.
Reference: [12] <author> Samuelsson, C. and Rayner, M., </author> <title> "Quantitative Evaluation of Explanation-Based Learning as a Tuning Tool for a Large-Scale Natural Language System". </title> <booktitle> Proc. 12th International Joint Conference on Artificial Intelligence. </booktitle> <address> Sydney, Australia, </address> <year> 1991. </year>
Reference-contexts: The CLE is a sophisticated unification-based language processing system which incorporates a broad-coverage domain-independent grammar for English [2]. In the SLT system, the general CLE grammar is specialized to the domain using the Explanation-Based Learning (EBL) algorithm <ref> [12] </ref>. The resulting grammar is parsed using an LR parser [13], giving a decrease in analysis time, compared to the normal CLE left-corner parser, of about a factor of ten.
Reference: [13] <author> Samuelsson, C., </author> <title> Fast Natural Language Parsing Using Explanation-Based Learning, </title> <type> PhD thesis, </type> <institution> Royal Institute of Technology, Stockholm, Sweden, </institution> <year> 1994. </year>
Reference-contexts: The CLE is a sophisticated unification-based language processing system which incorporates a broad-coverage domain-independent grammar for English [2]. In the SLT system, the general CLE grammar is specialized to the domain using the Explanation-Based Learning (EBL) algorithm [12]. The resulting grammar is parsed using an LR parser <ref> [13] </ref>, giving a decrease in analysis time, compared to the normal CLE left-corner parser, of about a factor of ten.
Reference: [14] <author> Shieber, S. M., van Noord, G., Pereira, F.C.N and Moore, </author> <title> R.C., </title> <journal> "Semantic-Head-Driven Generation", Computational Linguistics, </journal> <volume> 16 </volume> <pages> 30-43, </pages> <year> 1990. </year>
Reference-contexts: This is passed to a second copy of the CLE, loaded with a Swedish grammar, to generate a target-language text string. The Swedish grammar has been adapted fairly directly from the English one [7]. Generation is performed using the Semantic Head-Driven algorithm <ref> [14] </ref>, which simultaneously constructs a phrase-structure tree as part of the generation process. Finally, the output text string is passed to the Prophon speech synthesizer [5], where it is converted into output speech using a polyphone synthesis method.
References-found: 14

