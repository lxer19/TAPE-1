URL: ftp://ftp.cs.rochester.edu/pub/papers/ai/94.tr509.i_speech.ps.Z
Refering-URL: http://www.cs.rochester.edu/trs/ai-trs.html
Root-URL: 
Title: i-speech: Experiments with a Single Level Feedback Paradigm  
Note: This material is based upon work supported by the National Science Foundation under Grant number IRI-8903582, and NIH/PHS research grant no. 1 R24 RR06853-02. The Government has certain rights in this material.  
Abstract: Ramesh R. Sarukkai, Dana H. Ballard The University of Rochester Computer Science Department Rochester, New York 14627 Technical Report 509 May 1994 Abstract Speech of multiple speakers is transformed to speech produced by a single speaker (speech normalization) using cross-coding networks[1]. Internal representations for classification are acquired by feeding back the internal speech (i-speech) produced. Training proceeds by unfolding the network through time, and combining the classification error with the intermediate speaker-normalization errors. Experimental results on multi-speaker syllable recognition tasks with trained and new speakers are discussed. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Ramesh R. Sarukkai, and Dana H. Ballard, </author> <title> "Cross-coding Networks for Speech Classifi cation", </title> <note> accepted for presentation at IEEE Intl. Conf. on Patt. Recog.'94. </note>
Reference-contexts: In this method, speech is "corrected" and mapped to speech produced by a reference speaker. Once this is done, the speaker dependent system can be used, and thus the recognition rates of the speaker independent systems are expected to improve. 2.2 Cross-coding Networks Cross-coding <ref> [1] </ref> is an interesting approach to transforming speech from various speakers to speech of a reference speaker. In particular, cross-coding develops hidden representations which are speaker invariant, and are class dependent. <p> Cross-coding networks are similar to auto-associative networks 1 Dynamic Time Warping 1 in the sense that they map from input space back onto input space, going through intermediate bottleneck representations. Such networks have been successfully applied for some speech classification tasks <ref> [1] </ref>. Let R be the reference training speaker, and = fa; b; c; :::g be a set of other training speakers. Let x a ! correspond to the acoustic feature vector of a speech token produced by speaker a belonging to class !. Let the network transformation be t .
Reference: [2] <author> Richard P. Lippmann, </author> <title> "Review of Neural Networks for Speech Recognition", Neural Com putation, </title> <address> pp.374-392, </address> <year> 1989. </year>
Reference: [3] <author> Jeffrey Elman, and David Zipser, </author> <title> "Learning the hidden structure of speech", </title> <journal> J. Acoust. Soc. Am., </journal> <volume> vol. 83, </volume> <month> April </month> <year> 1988. </year>
Reference: [4] <author> S. Davis, and P. Mermelstein, </author> " <title> Comparison of Parametric Representations for Mono syllabic Word Recognition in Continuously Spoken Sentences", </title> <journal> IEEE Transactions on Acoustics, Speech, and Signal Processing, </journal> <volume> ASSP-28, No. 4, </volume> <month> August </month> <year> 1980. </year>
Reference-contexts: From the edited waveforms, 256 point FFT transforms were computed. The window duration was 0.02 seconds, and the frame step size was 0.01 sec. A total of 24 Melscale <ref> [4] </ref> filter bank coefficients were computed by interpolating, and the linear frequency resolution was 10. Thus a total of 192 parameters were extracted for each speech token.
Reference: [5] <author> Scott E. Fahlman, </author> <title> "Faster-Learning Variations on Back-Propagation: An Empirical Study", </title> <booktitle> in Proceedings of the 1988 Connectionist Models Summer School, Pitts-burg (1988), </booktitle> <pages> 38-51, </pages> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: The i-speech network consists of 192 input units, two layers of hidden units (with 10, and 5 units respectively), and 192 -speech output units, and 4 labeling output units corresponding to the syllables ba,pa,da, and ga. The architecture of the network is shown in figure 1. QuickProp <ref> [5] </ref>, a modified form of back-propagation was used in all the following experiments. Training was performed till the mean square output class error was less than 0.001, or restarts performed after a fixed number of epochs.
Reference: [6] <author> Yochai Konig, and Nelson Morgan, </author> <title> "Supervised and Unsupervised Clustering of the speaker Space for connectionist Speech Recognition", </title> <booktitle> IEEE ICASSP , I-545:548, </booktitle> <year> 1993. </year>
Reference-contexts: Multi-speaker syllable recognition results indicate that the single level feedback paradigm improves the systems robustness towards noise. 2 i-speech : Single level feedback Paradigm 2.1 The Task of Speaker invariant recognition Speaker independent speech recognition systems have achieved a reasonable amount of success <ref> [6, 7, 8] </ref>. However, multi-speaker speech recognition seems to be an evasive goal. Studies [10] show that speaker dependent systems perform better than speaker independent systems. Typically, the speaker variance problem is attacked by partitioning the speaker space into regions of similarity, and associating each cluster with an approximating model.
Reference: [7] <author> Yochai Konig and Nelson Morgan, " GDNN: </author> <title> A Gender-Dependent Neural Network for Continuous Speech Recognition ", IEEE IJCNN, </title> <address> II-332:337, </address> <year> 1992. </year>
Reference-contexts: Multi-speaker syllable recognition results indicate that the single level feedback paradigm improves the systems robustness towards noise. 2 i-speech : Single level feedback Paradigm 2.1 The Task of Speaker invariant recognition Speaker independent speech recognition systems have achieved a reasonable amount of success <ref> [6, 7, 8] </ref>. However, multi-speaker speech recognition seems to be an evasive goal. Studies [10] show that speaker dependent systems perform better than speaker independent systems. Typically, the speaker variance problem is attacked by partitioning the speaker space into regions of similarity, and associating each cluster with an approximating model.
Reference: [8] <author> Huang, X. D., Lee, K. F., and Waibel, A., </author> <title> "Connectionist Speaker Normalization and Its Applications to Speech Recognition", Neural Networks for Signal Processing,, </title> <booktitle> Proc. of the 1991 IEEE Workshop, </booktitle> <address> Princeton, New Jersey, </address> <month> October </month> <year> 1991. </year>
Reference-contexts: Multi-speaker syllable recognition results indicate that the single level feedback paradigm improves the systems robustness towards noise. 2 i-speech : Single level feedback Paradigm 2.1 The Task of Speaker invariant recognition Speaker independent speech recognition systems have achieved a reasonable amount of success <ref> [6, 7, 8] </ref>. However, multi-speaker speech recognition seems to be an evasive goal. Studies [10] show that speaker dependent systems perform better than speaker independent systems. Typically, the speaker variance problem is attacked by partitioning the speaker space into regions of similarity, and associating each cluster with an approximating model.
Reference: [9] <author> Oded Ghitza, </author> <title> "Auditory Neural Feedback as a Basis for Speech Processing", </title> <booktitle> IEEE ICASSP, </booktitle> <pages> pp. 91-94, </pages> <year> 1988. </year>
Reference-contexts: This indicates the existence of a frequency-dependent feedback mechanism. Although the exact functionality of the feedback mechanism is unclear, researchers <ref> [9, 11] </ref> have shown that feedback is useful in gain control of the peripheral filters. The medial-olivocochlear nerve originates in the superior olivary complex, and projects back to the cochlear partition where synapses are located on the outer hair cells (on the contralateral cochlea). <p> Studies have shown that when this feedback loop is disrupted, the discrimination of stimuli in noise is affected, but not the actual detection. Such results have led researchers to view the feedback mechanism as an Automatic Gain Control (AGC) system. In particular, Ghitza <ref> [9] </ref> has employed such an AGC in a pre-processor to a DTW 1 -based, discrete utterance word recognition system. Based on neurophysiological plausibilities, the conventional Short Time Fourier Transform was replaced by an alternate pre-processor which produces the Ensemble Interval Histogram (EIH) representation [9]. <p> In particular, Ghitza <ref> [9] </ref> has employed such an AGC in a pre-processor to a DTW 1 -based, discrete utterance word recognition system. Based on neurophysiological plausibilities, the conventional Short Time Fourier Transform was replaced by an alternate pre-processor which produces the Ensemble Interval Histogram (EIH) representation [9]. In this paper, we propose to examine the frequency-based feedback mechanism by alternative means.
Reference: [10] <author> Huang X. D., and Lee K. F., </author> " <title> On Speaker-Independent, Speaker-Dependent, and Speaker Adaptive Speech Recognition", </title> <booktitle> IEEE ICASSP, </booktitle> <pages> pp. 877-880, </pages> <year> 1991. </year>
Reference-contexts: However, multi-speaker speech recognition seems to be an evasive goal. Studies <ref> [10] </ref> show that speaker dependent systems perform better than speaker independent systems. Typically, the speaker variance problem is attacked by partitioning the speaker space into regions of similarity, and associating each cluster with an approximating model.
Reference: [11] <author> David P. Morgan, and Christopher L. Scofield, </author> <title> Neural Networks for Speech Process ing, </title> <publisher> Kluwer Academic Publishers, </publisher> <year> 1991. </year>
Reference-contexts: This indicates the existence of a frequency-dependent feedback mechanism. Although the exact functionality of the feedback mechanism is unclear, researchers <ref> [9, 11] </ref> have shown that feedback is useful in gain control of the peripheral filters. The medial-olivocochlear nerve originates in the superior olivary complex, and projects back to the cochlear partition where synapses are located on the outer hair cells (on the contralateral cochlea).
Reference: [12] <author> Kai-Fu Lee, </author> <title> Automatic Speech Recognition: The Development of the SPHINX System, </title> <publisher> Kluwer Academic Publishers, </publisher> <year> 1989. </year>
Reference: [13] <author> Ramesh R. Sarukkai, </author> <title> "Supervised Learning Without Output Labels", </title> <type> Tech. report 510, </type> <institution> Dept. of Computer Science, Univ. of Rochester, </institution> <month> May </month> <year> 1994. </year> <title> 6 difference of the noisy spectrograms from non-noisy conditions for actual speech and i-speech. </title> <type> 7 </type>
Reference-contexts: This has been achieved by unfolding a feedback network, and combining the error signals corresponding to speaker cross-coding, and output label classification. Although the results have not convincingly demonstrated improvement with i-speech, adding other intermediate constraints such as class uniformity constraints derived in <ref> [13] </ref> could enable the network to develop useful i-speech representations. 5
References-found: 13

