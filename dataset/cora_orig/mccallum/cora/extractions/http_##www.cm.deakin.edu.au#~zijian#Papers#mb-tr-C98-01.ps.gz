URL: http://www.cm.deakin.edu.au/~zijian/Papers/mb-tr-C98-01.ps.gz
Refering-URL: http://www.cm.deakin.edu.au/~zijian/publications.html
Root-URL: 
Title: Multiple Boosting: A Combination of Boosting and Bagging  
Author: Zijian Zheng and Geoffrey I. Webb 
Keyword: Parallel datamining, Boosting, Bagging, Committee learning, Decision tree learning, Machine learning  
Address: Geelong Victoria 3217, Australia  
Affiliation: School of Computing and Mathematics Deakin University,  
Pubnum: Technical Report (TR C98/01)  
Email: fzijian,webbg@deakin.edu.au  
Date: February, 1998  
Abstract: Classifier committee learning approaches have demonstrated great success in increasing the prediction accuracy of classifier learning, which is a key technique for datamining. These approaches generate several classifiers to form a committee by repeated application of a single base learning algorithm. The committee members vote to decide the final classification. It has been shown that Boosting and Bagging, as two representative methods of this type, can significantly decrease the error rate of decision tree learning. Boosting is generally more accurate than Bagging, but the former is more variable than the latter. In addition, Bagging is amenable to parallel or distributed processing, while Boosting is not. In this paper, we study a new committee learning algorithm, namely MB (Multiple Boosting). It creates multiple subcommittees by combining Boosting and Bagging. Experimental results in a representative collection of natural domains show that MB is, on average, more accurate than either Bagging or Boosting alone. It is more stable than Boosting, and is amenable to parallel or distributed processing. These characteristics make MB a good choice for parallel datamin-ing. 
Abstract-found: 1
Intro-found: 1
Reference: [Ali and Pazzani, 1996] <author> Ali, K.M. and Pazzani, M.J. </author> <title> Error Reduction through Learning Multiple Descriptions. </title> <booktitle> Machine Learning 24: </booktitle> <pages> 173-202. </pages>
Reference: [Ali, 1996] <author> Ali, K.M. </author> <title> Learning Probabilistic Relational Concept Descriptions. </title> <type> Ph.D. </type> <institution> diss., Dept of Info. and Computer Science, Univ. of Cali-fornia, Irvine. </institution>
Reference-contexts: While much recent attention has focused on Boosting and Bagging, other classifier committee learning approaches have also been developed, including generating multiple trees by manually changing learning parameters [Kwok and Carter, 1990], error-correcting output codes [Dietterich and Bakiri, 1995], generating different classifiers by randomizing the base learning process <ref> [Dietterich and Kong, 1995; Ali, 1996] </ref>, and learning option trees [Kohavi and Kunz, 1997]. A collection of recent research in this area and reviews of related methods can be found in [Chan et al., 1996; Dietterich, 1997; Ali, 1996]. <p> A collection of recent research in this area and reviews of related methods can be found in <ref> [Chan et al., 1996; Dietterich, 1997; Ali, 1996] </ref>. In this paper, we investigate an approach, namely MB (Multiple Boosting), 2 to generating committees, which is more accurate than Bagging and is more stable than Boosting. MB creates multiple subcommittees by incorporating Bagging into Boosting using the multi-boosting technique [Webb, 1998].
Reference: [Bauer and Kohavi, 1998] <author> Bauer, E. and Kohavi, R. </author> <title> An Empirical Comparison of Voting Classification Algorithms: Bagging, Boosting, and Variants. </title> <note> Submitted to Machine Learning (available at http://reality.sgi.com/ronnyk/ vote.ps.gz). </note>
Reference-contexts: At the classification stage, the committee members vote to make the final decision. Bagging [Breiman, 1996a] and Boosting [Schapire, 1990; Freund and Schapire, 1996a; 1996b; Freund, 1996; Schapire et al., 1997], as two representative methods of this type, can significantly decrease the error rate of decision tree learning <ref> [Quinlan, 1996; Freund and Schapire, 1996b; Bauer and Kohavi, 1998] </ref>. They repeatedly build different classifiers using a base learning algorithm, such as a deci 1 Committees are also referred to as ensembles [Dietterich, 1997]. 1 sion tree generator, by changing the distribu-tion of the training set. <p> Although Boosting is generally more accurate than Bagging, the performance of Boosting is more variable than that of Bagging <ref> [Quinlan, 1996; Bauer and Kohavi, 1998] </ref>. Given an integer T as the committee size, both Boosting and Bagging need approximately T times as long as their base learning algorithm does for learning a single classifier. However, Bagging has an advantage over Boosting. <p> This confirms previous findings <ref> [Quinlan, 1996; Bauer and Kohavi, 1998] </ref>. The average error rate of Bag in the 40 domains is 16.35%, while it is 15.97% for Boost, 8% relatively lower, on average, than that of Bag. <p> It performs significantly worse than the probabilistic predictions without voting weights method using a one-tailed sign-test (p = 0:0401). Bagging uses the categorical predictions without voting weights method for voting in the previous research <ref> [Breiman, 1996a; Quin-lan, 1996; Bauer and Kohavi, 1998] </ref>. Our experiments show that this voting method achieves the lowest average error rate in the 40 domains among the four methods for Bag.
Reference: [Breiman, 1996a] <author> Breiman, L. </author> <title> Bagging Predictors. </title> <booktitle> Machine Learning 24: </booktitle> <pages> 123-140. </pages>
Reference-contexts: This type of technique generates several classifiers to form a committee by using a single base learning algorithm. At the classification stage, the committee members vote to make the final decision. Bagging <ref> [Breiman, 1996a] </ref> and Boosting [Schapire, 1990; Freund and Schapire, 1996a; 1996b; Freund, 1996; Schapire et al., 1997], as two representative methods of this type, can significantly decrease the error rate of decision tree learning [Quinlan, 1996; Freund and Schapire, 1996b; Bauer and Kohavi, 1998]. <p> w (t+1) (x) = w t (x)exp ((1) d (x) ff t ); (1) where ff t = 1 2 ln ((1 * t )=* t ); d (x) = 1 if H t correctly classifies x and d (x) = 0 otherwise. 2.2 Bagging The primary idea of Bagging <ref> [Breiman, 1996a] </ref> is to generate a committee of classifiers with each from a bootstrap sample of the original training set. Bag, our implementation of Bagging, uses C4.5 [Quinlan, 1993] as its base classifier learning algorithm. <p> In this case, each tree produces a single predicted class for an example. Then, the committee predicts the most frequent class returned by all trees. This voting method corresponds to the method used in the original Bagging <ref> [Breiman, 1996a] </ref>. The other two methods are the same as the two mentioned above but each tree is given a weight ff t for voting, which is a function of the performance of the decision tree on the training set, and is defined in Equation 1. <p> It performs significantly worse than the probabilistic predictions without voting weights method using a one-tailed sign-test (p = 0:0401). Bagging uses the categorical predictions without voting weights method for voting in the previous research <ref> [Breiman, 1996a; Quin-lan, 1996; Bauer and Kohavi, 1998] </ref>. Our experiments show that this voting method achieves the lowest average error rate in the 40 domains among the four methods for Bag.
Reference: [Breiman, 1996b] <author> Breiman, L. </author> <note> Arcing Classifiers. Technical Report (available at http://www. stat.Berkeley.EDU/users/breiman/). </note> <institution> Dept of Statistics, Univ of California, Berkeley, </institution> <address> CA. </address>
Reference: [Chan et al., 1996] <author> Chan, P., Stolfo, S., and Wolpert, D. (eds): </author> <title> Working Notes of AAAI Workshop on Integrating Multiple Learned Models for Improving and Scaling Machine Learning Algorithms (available at http://www.cs.fit.edu/~imlm/papers.html), Portland, </title> <booktitle> Oregon. </booktitle> <pages> 9 </pages>
Reference-contexts: A collection of recent research in this area and reviews of related methods can be found in <ref> [Chan et al., 1996; Dietterich, 1997; Ali, 1996] </ref>. In this paper, we investigate an approach, namely MB (Multiple Boosting), 2 to generating committees, which is more accurate than Bagging and is more stable than Boosting. MB creates multiple subcommittees by incorporating Bagging into Boosting using the multi-boosting technique [Webb, 1998].
Reference: [Dietterich and Bakiri, 1995] <author> Dietterich, T.G. and Bakiri, G. </author> <title> Solving Multiclass Learning Problems via Error-correcting Output Codes. </title> <journal> Journal of Artificial Intelligence Research 2: </journal> <pages> 263-286. </pages>
Reference-contexts: This makes Bagging appropriate for parallel datamining. While much recent attention has focused on Boosting and Bagging, other classifier committee learning approaches have also been developed, including generating multiple trees by manually changing learning parameters [Kwok and Carter, 1990], error-correcting output codes <ref> [Dietterich and Bakiri, 1995] </ref>, generating different classifiers by randomizing the base learning process [Dietterich and Kong, 1995; Ali, 1996], and learning option trees [Kohavi and Kunz, 1997].
Reference: [Dietterich and Kong, 1995] <author> Dietterich, T.G. and Kong, E.B. </author> <title> Machine Learning Bias, Statistical Bias, and Statistical Variance of Decision Tree Algorithms. </title> <type> Technical Report, </type> <institution> Dept of Computer Science, Oregon State University, Corvallis, Oregon (available at ftp://ftp. cs.orst.edu/pub/tgd/papers/tr-bias.ps.gz). </institution>
Reference-contexts: While much recent attention has focused on Boosting and Bagging, other classifier committee learning approaches have also been developed, including generating multiple trees by manually changing learning parameters [Kwok and Carter, 1990], error-correcting output codes [Dietterich and Bakiri, 1995], generating different classifiers by randomizing the base learning process <ref> [Dietterich and Kong, 1995; Ali, 1996] </ref>, and learning option trees [Kohavi and Kunz, 1997]. A collection of recent research in this area and reviews of related methods can be found in [Chan et al., 1996; Dietterich, 1997; Ali, 1996].
Reference: [Dietterich, 1997] <author> Dietterich, </author> <title> T.G. </title> <journal> Machine Learning Research. AI Magazine 18: </journal> <pages> 97-136. </pages>
Reference-contexts: They repeatedly build different classifiers using a base learning algorithm, such as a deci 1 Committees are also referred to as ensembles <ref> [Dietterich, 1997] </ref>. 1 sion tree generator, by changing the distribu-tion of the training set. Bagging generates different classifiers using different bootstrap samples. Boosting builds different classifiers sequentially. The weights of training examples used for creating each classifier are modified based on the performance of the previous classifiers. <p> A collection of recent research in this area and reviews of related methods can be found in <ref> [Chan et al., 1996; Dietterich, 1997; Ali, 1996] </ref>. In this paper, we investigate an approach, namely MB (Multiple Boosting), 2 to generating committees, which is more accurate than Bagging and is more stable than Boosting. MB creates multiple subcommittees by incorporating Bagging into Boosting using the multi-boosting technique [Webb, 1998].
Reference: [Domingos, 1997] <author> Domingos, P. </author> <title> Why does Bagging Work? a Bayesian Account and its Implications. </title> <booktitle> In Proceedings of the Third International Conference on Knowledge Discovery and Data Mining, </booktitle> <pages> 155-158. </pages> <publisher> AAAI Press. </publisher>
Reference: [Freund, 1996] <author> Freund, Y. </author> <title> Boosting a Weak Learning Algorithm by Majority. </title> <booktitle> Information and Computation 121(2): </booktitle> <pages> 256-285. </pages>
Reference: [Freund and Schapire, 1996a] <author> Freund, Y. and Schapire, R.E. </author> <title> A Decision-theoretic Generalization of On-line Learning and an Application to Boosting. </title> <note> Unpublished manuscript (available at http://www.research.att.com/~yoav). </note>
Reference: [Freund and Schapire, 1996b] <author> Freund, Y. and Schapire, R.E. </author> <title> Experiments with a New Boosting Algorithm. </title> <booktitle> In Proceedings of the Thirteenth International Conference on Machine Learning, </booktitle> <pages> 148-156. </pages> <address> San Francisco, CA: </address> <publisher> Mor-gan Kaufmann. </publisher>
Reference-contexts: At the classification stage, the committee members vote to make the final decision. Bagging [Breiman, 1996a] and Boosting [Schapire, 1990; Freund and Schapire, 1996a; 1996b; Freund, 1996; Schapire et al., 1997], as two representative methods of this type, can significantly decrease the error rate of decision tree learning <ref> [Quinlan, 1996; Freund and Schapire, 1996b; Bauer and Kohavi, 1998] </ref>. They repeatedly build different classifiers using a base learning algorithm, such as a deci 1 Committees are also referred to as ensembles [Dietterich, 1997]. 1 sion tree generator, by changing the distribu-tion of the training set.
Reference: [Kohavi, 1995] <author> Kohavi, R. </author> <title> A Study of Cross-validation and Bootstrap for Accuracy Estimation and Model Selection. </title> <booktitle> In Proceedings of the Fourteenth International Joint Conference on Artificial Intelligence, </booktitle> <pages> 1137-1143. </pages> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: This test suite covers a wide variety of different domains with respect to dataset size, the number of classes, the number of attributes, and types of attributes. In every domain, two stratified 10-fold cross-validations <ref> [Kohavi, 1995] </ref> were carried out for each algorithm. The result reported for each algorithm in each domain is an average value over 20 trials. All the algorithms are run on the same training and test set partitions with their default option settings, except when otherwise indicated.
Reference: [Kohavi and Kunz, 1997] <author> Kohavi, R. and Kunz, C. </author> <title> Option Decision Trees with Majority Votes. </title> <booktitle> In Proceedings of the Fourteenth International Conference on Machine Learning, </booktitle> <pages> 161-169. </pages> <address> San Francisco, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: and Bagging, other classifier committee learning approaches have also been developed, including generating multiple trees by manually changing learning parameters [Kwok and Carter, 1990], error-correcting output codes [Dietterich and Bakiri, 1995], generating different classifiers by randomizing the base learning process [Dietterich and Kong, 1995; Ali, 1996], and learning option trees <ref> [Kohavi and Kunz, 1997] </ref>. A collection of recent research in this area and reviews of related methods can be found in [Chan et al., 1996; Dietterich, 1997; Ali, 1996].
Reference: [Kwok and Carter, 1990] <author> Kwok, S.W. and Carter, C. </author> <title> Multiple Decision Trees. </title> <editor> In Schachter, R.D., Levitt, T.S., Kanal, L.N., and Lemmer, J.F. eds, </editor> <booktitle> Uncertainty in Artificial Intelligence, </booktitle> <pages> 327-335. </pages> <publisher> Elsevier Science. </publisher>
Reference-contexts: This makes Bagging appropriate for parallel datamining. While much recent attention has focused on Boosting and Bagging, other classifier committee learning approaches have also been developed, including generating multiple trees by manually changing learning parameters <ref> [Kwok and Carter, 1990] </ref>, error-correcting output codes [Dietterich and Bakiri, 1995], generating different classifiers by randomizing the base learning process [Dietterich and Kong, 1995; Ali, 1996], and learning option trees [Kohavi and Kunz, 1997].
Reference: [Merz and Murphy, 1997] <author> Merz, C.J. and Murphy, </author> <title> P.M. UCI Repository of machine learning databases [http://www.ics.uci.edu/~mlearn/ MLRepository.html]. </title> <address> Irvine, CA: </address> <institution> Univ of Cal-ifornia, Dept of Info and Computer Science. </institution>
Reference-contexts: It is compared with Boost and Bag. C4.5, the base decision tree learning algorithm of these committee learning algorithms, is used as the base line for the comparison. 4 4.1 Experimental Domains and Methods Forty natural domains from the UCI machine learning repository <ref> [Merz and Murphy, 1997] </ref> are used. They include all the domains used by Quinlan [1996] for studying Boosting and Bagging. Table 1 summarizes the characteristics of these domains, including dataset size, the number of classes, the number of continuous attributes, and the number of discrete attributes.
Reference: [Quinlan, 1993] <author> Quinlan, J.R. C4.5: </author> <title> Program for Machine Learning. </title> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Given a training set D consisting of m instances and an integer T, the number of trials, Boost builds T pruned trees over T trials by repeatedly invoking C4.5 <ref> [Quinlan, 1993] </ref>. Let w t (x) denote the weight of instance x in D at trial t. At the first trial, each instance has weight 1; that is, w 1 (x) = 1 for each x. <p> Bag, our implementation of Bagging, uses C4.5 <ref> [Quinlan, 1993] </ref> as its base classifier learning algorithm. Given a committee size T and a training set D consisting of m instances, Bag generates T 1 bootstrap samples with each being created by uniformly sampling m instances from D with replacement. <p> The last of these alternatives, weighted voting of cat 4 For multi-branch trees created by C4.5, some leaves may contain no training instances <ref> [Quinlan, 1993] </ref>. 3 egorical predictions, corresponds to the origi-nal AdaBoost.M1. These three voting methods perform either worse than or similarly to the method that we use here. 5 We will address this issue in Section 4.3. 3 MB: A Combination of Boosting and Bagging into Boost. MB generates N subcommittees.
Reference: [Quinlan, 1996] <author> Quinlan, J.R. Bagging, </author> <title> Boosting, </title> <booktitle> and C4.5. In Proceedings of the Thirteenth National Conference on Artificial Intelligence, </booktitle> <pages> 725-730. </pages> <publisher> AAAI Press. </publisher>
Reference-contexts: At the classification stage, the committee members vote to make the final decision. Bagging [Breiman, 1996a] and Boosting [Schapire, 1990; Freund and Schapire, 1996a; 1996b; Freund, 1996; Schapire et al., 1997], as two representative methods of this type, can significantly decrease the error rate of decision tree learning <ref> [Quinlan, 1996; Freund and Schapire, 1996b; Bauer and Kohavi, 1998] </ref>. They repeatedly build different classifiers using a base learning algorithm, such as a deci 1 Committees are also referred to as ensembles [Dietterich, 1997]. 1 sion tree generator, by changing the distribu-tion of the training set. <p> Although Boosting is generally more accurate than Bagging, the performance of Boosting is more variable than that of Bagging <ref> [Quinlan, 1996; Bauer and Kohavi, 1998] </ref>. Given an integer T as the committee size, both Boosting and Bagging need approximately T times as long as their base learning algorithm does for learning a single classifier. However, Bagging has an advantage over Boosting. <p> The key idea of Boosting was presented in Section 1. Here, we describe our implementation of the Boosting algorithm with decision tree learning, called Boost. It follows the Boosted C4.5 algorithm (AdaBoost.M1) 2 The idea of multiple Boosting was originally pro posed by Webb [Webb, 1998]. 2 <ref> [Quinlan, 1996] </ref> but uses a new Boosting equa-tion as shown in Equation 1, derived from Schapire et al. [Schapire et al., 1997]. <p> This confirms previous findings <ref> [Quinlan, 1996; Bauer and Kohavi, 1998] </ref>. The average error rate of Bag in the 40 domains is 16.35%, while it is 15.97% for Boost, 8% relatively lower, on average, than that of Bag.
Reference: [Schapire, 1990] <author> Schapire, R.E. </author> <title> The Strength of Weak Learnability. </title> <booktitle> Machine Learning 5: </booktitle> <pages> 197-227. </pages>
Reference: [Schapire et al., 1997] <author> Schapire, R.E., Freund, Y., Bartlett, P., and Lee, </author> <title> W.S. Boosting the Margin: A New Explanation for the Effectiveness of Voting Methods. </title> <booktitle> In Proceedings of the 14th International Conference on Machine Learning, </booktitle> <pages> 322-330. </pages> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: It follows the Boosted C4.5 algorithm (AdaBoost.M1) 2 The idea of multiple Boosting was originally pro posed by Webb [Webb, 1998]. 2 [Quinlan, 1996] but uses a new Boosting equa-tion as shown in Equation 1, derived from Schapire et al. <ref> [Schapire et al., 1997] </ref>. Given a training set D consisting of m instances and an integer T, the number of trials, Boost builds T pruned trees over T trials by repeatedly invoking C4.5 [Quinlan, 1993]. Let w t (x) denote the weight of instance x in D at trial t.
Reference: [Webb, 1998] <author> Webb, </author> <title> G.I. Idealized Models of Decision Committee Performance and Their Application to Reduce Committee Error. </title> <type> Tech Report (TR C98/11), </type> <institution> School of Computing and Mathematics, Deakin University, Australia. </institution> <month> 10 </month>
Reference-contexts: In this paper, we investigate an approach, namely MB (Multiple Boosting), 2 to generating committees, which is more accurate than Bagging and is more stable than Boosting. MB creates multiple subcommittees by incorporating Bagging into Boosting using the multi-boosting technique <ref> [Webb, 1998] </ref>. We expect that splitting one committee into multiple subcommittees, with each subcommittee being created from a bootstrap sample of the training set, can reduce the variability of Boosting, since the Boosting process is broken down into several small processes. <p> The key idea of Boosting was presented in Section 1. Here, we describe our implementation of the Boosting algorithm with decision tree learning, called Boost. It follows the Boosted C4.5 algorithm (AdaBoost.M1) 2 The idea of multiple Boosting was originally pro posed by Webb <ref> [Webb, 1998] </ref>. 2 [Quinlan, 1996] but uses a new Boosting equa-tion as shown in Equation 1, derived from Schapire et al. [Schapire et al., 1997]. <p> This is implemented through changing instance weights, resulting in 0 weights for instances not in the sample. Boosting propagates these values ensuring that these instances are not considered when inferring any member of the subcommittee. Note that MB differs from Webb's MultiBoost <ref> [Webb, 1998] </ref> by using bootstrap sampling in place of stochastic weighting at the start of the generation of each subcommittee. This difference is not expected to significantly affect performance.
References-found: 22

