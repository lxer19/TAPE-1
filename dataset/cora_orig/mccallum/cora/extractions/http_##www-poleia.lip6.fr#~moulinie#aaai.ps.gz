URL: http://www-poleia.lip6.fr/~moulinie/aaai.ps.gz
Refering-URL: 
Root-URL: 
Email: moulinie@laforia.ibp.fr  
Title: A Framework for Comparing Text Categorization Approaches  
Author: Isabelle Moulinier LAFORIA-IBP-CNRS 
Address: 4 place Jussieu F-75252 Paris Cedex 05 FRANCE  
Affiliation: Universite Paris VI  
Abstract: For the past few years, text categorization has emerged as an application domain to machine learning techniques. Several approaches have already been proposed. This paper does not present yet another technique. It is rather an attempt to unify the approaches encountered so far. Moreover this state-of- the-art enables us to stress a shortcoming in earlier research: the lack of evaluation of inductive learners in the categorization process. We present a first attempt to remedy this lack. We expose an experimental framework, that fits in with our unified view of text categorization methods. This framework allows us to conduct a set of tentative experiments in order to assess which characteristics allow a learner to perform well on the text categorization task. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Apte, C.; Damerau, F.; and Weiss, S. </author> <year> 1994. </year> <title> Automated learning of decision rules for text categoriza-tion. </title> <journal> ACM Transactions on Information Systems. </journal>
Reference-contexts: We can distinguish two axes for dimensionality reduction: its scope and its nature. The scope of reduction is concerned with the universality of the resulting feature set, whereas its nature describes how the features are selected. In <ref> (Apte, Damerau, & Weiss 1994) </ref>, two alternatives to the scope of reduction are suggested: category-oriented, or local, and overall, or global, feature set reduction. <p> The nature of reduction can also be qualified by two different means: filtering and construction. Filtering aims at reducing the number of features by selecting the best ones according to some criterion; such criteria include mutual information (Lewis & Ringuette 1994; Moulinier & Ganascia 1995), frequency <ref> (Apte, Dam- erau, & Weiss 1994) </ref>, term ranking (Fuhr et al. 1991; Wiener, Pedersen, & Weigend 1995) or expert's judgment (Maron 1961). Construction has a lesser impact in text categorization. Instead of selecting a subset of the original feature set, new features are constructed as combinations of original features. <p> Domain knowledge has been used by (Liddy, Paik, & Yu 1994), where a machine-readable dictionary was employed to build the initial representation. We also consider the assignment of a greater weight to words appearing in the headlines of a news-story <ref> (Apte, Dam- erau, & Weiss 1994) </ref> as domain knowledge. In (Cohen 1995), the expressive power of a relational formalism, i.e. language bias, enables the representation to take into account the positions of words inside a document. <p> However, few studies have reported experiments, where varying amounts of knowledge were involved. For instance, (Wiener, Pedersen, & Weigend 1995) reported an enhancement of 5% using LSI and a hierarchical net over boolean features using a flat network. Similarly, <ref> (Apte, Damerau, & Weiss 1994) </ref> reported a increase of 5% between a locally reduced representation based on frequency and weight assignment, and a global boolean representation. Limitations of this Unifying View There remain some text categorization approaches that do not fit into the preceding schema. <p> Results, reported in Table 1, show the best performances with regards to the F 1 criterion. Results from earlier experiments on the same corpus complete this summary. Neural nets refer to the experiments presented by (Wiener, Pedersen, & Weigend 1995), while Swap-1 is a production rule learner used in <ref> (Apte, Damerau, & Weiss 1994) </ref>. In both cases, the enriched model takes into account various kinds of knowledge sources, while the (local) boolean model is very close to our naive framework. These earlier experiments were not evaluated using the same criterion.
Reference: <author> Biebericher, P.; Fuhr, N.; Lustig, G.; Schwantner, M.; and Knorz, G. </author> <year> 1988. </year> <title> The automatic indexing system AIR/PHYS from research to application. </title> <booktitle> In Proceeding of the 11th SIGIR. </booktitle> <address> Learner # features Precision F 1 Recall ID3 75 81.2 78.1 75.4 ID3 10 80.9 75.2 70.3 Charade 75 76.9 78.3 76.9 Charade 50 79.5 77.6 75.6 Charade 10 86.4 59.0 44.8 IB 10 81.5 75.1 69.4 IB 75 86.4 74.1 64.7 NaiveBayes 10 64.3 70.2 77.5 NaiveBayes 4 73.5 70.0 66.7 NaiveBayes 75 49.8 62.9 85.4 Table 2: </address> <booktitle> Influence of the number of features on microaveraged performances. </booktitle>
Reference: <author> Cohen, W. </author> <year> 1995. </year> <title> Text categorization and relational learning. </title> <booktitle> In Proceedings of the Twelfth International Conference on Machine Learning. </booktitle>
Reference-contexts: Recent studies have introduced symbolic learners in order to build catego- rizers: decision tree constructors (Fuhr et al. 1991; Lewis & Ringuette 1994), relational k-DNF learners <ref> (Cohen 1995) </ref> and production rule inducers (Apte, Damerau, & Weiss 1994; Moulinier & Ganascia 1995). We now outline a couple of differences between these learners, that may be significant for the text categorization task. First, numerical and symbolic learners differ their abilities to handle structured features and produce understandable classifiers. <p> We also consider the assignment of a greater weight to words appearing in the headlines of a news-story (Apte, Dam- erau, & Weiss 1994) as domain knowledge. In <ref> (Cohen 1995) </ref>, the expressive power of a relational formalism, i.e. language bias, enables the representation to take into account the positions of words inside a document. There has been little research conducted on the use of knowledge during the inductive phase of categorization. <p> Reduc tion of the training set, as opposed to dimensionality reduction, has also been eluded in this schema. Sam- pling, as described in (Lewis & Catlett 1994) and used by <ref> (Cohen 1995) </ref>, is one such approach for reducing the number of training examples. Finally, for the sake of simplicity, we have not included feedback into the whole categorization system. Clearly, however, all systems perform hand- driven feedback, when tuning parameters to optimize some evaluation criterion.
Reference: <author> Creecy, R. H.; Masand, B. M.; Smith, S. J.; and Waltz, D. L. </author> <year> 1992. </year> <title> Trading MIPS and memory for knowledge engineering: Classifying census returns on the connection machine. </title> <journal> Communication of the ACM. </journal>
Reference-contexts: Limitations of this Unifying View There remain some text categorization approaches that do not fit into the preceding schema. The processing step between the initial representation and the final representation does not always imply dimensionality reduction. For instance, in <ref> (Creecy et al. 1992) </ref>, the authors expand the initial representation and their learner has to deal with over 4 million features. Reduc tion of the training set, as opposed to dimensionality reduction, has also been eluded in this schema.
Reference: <author> Deerwester, S.; Dumais, S. T.; Furnas, G. W.; Landauer, T. K.; and R., H. </author> <year> 1990. </year> <title> Indexing by latent semantic indexing. </title> <journal> Journal of the American Society for Information Science 41(6) </journal> <pages> 391-407. </pages>
Reference-contexts: Construction has a lesser impact in text categorization. Instead of selecting a subset of the original feature set, new features are constructed as combinations of original features. Latent Semantic Indexing (LSI) <ref> (Deerwester et al. 1990) </ref>, as used in (Wiener, Pedersen, & Weigend 1995), is such a constructive approach. Inductive Construction of Categorizers Once texts are turned into learning examples, inductive learners are used to induce categorizers.
Reference: <author> Fuhr, N.; Hartmann, S.; Lustig, G.; Schwantner, M.; and Tzeras, K. </author> <year> 1991. </year> <title> AIR/X | a Rule-Based Multistage Indexing System for Large Subject Fields. </title> <booktitle> In Proc. of RIAO'91. </booktitle>
Reference-contexts: There has been little research conducted on the use of knowledge during the inductive phase of categorization. Nevertheless, a noticeable attempt is presented in (Wiener, Pedersen, & Weigend 1995), where the authors group categories according to semantic characteristics and induce categorizers of these sub-domains. In <ref> (Fuhr et al. 1991) </ref>, the authors used knowledge to guide an indexing system: for instance, knowledge enabled the discrimination among candidate keywords issued from the inductive step.
Reference: <author> Ganascia, J.-G. </author> <year> 1991. </year> <title> Deriving the learning bias from rule properties. </title> <editor> In Hayes, J. E.; Mitchie, D.; and Tyngu, E., eds., </editor> <booktitle> Machine Intelligence 12. </booktitle> <publisher> Oxford: Clarendon Press. </publisher> <pages> 151-167. </pages>
Reference-contexts: This left us with 7789 learning and 3875 testing examples described by 22791 words provided by Lewis' processing (Lewis 1992, p. 99). Which Learners ? Our experiments were conducted on four learners which illustrate symbolic and numerical learning. An implementation of ID3 (Quinlan 1986) and Charade <ref> (Ganascia 1991) </ref>, a production rule learner, represent symbolic learners, while a k-nearest neighbors algorithm called IB, and a Bayesian approach, NaiveBayes, are instances of numerical learners.
Reference: <author> Ganascia, J.-G. </author> <year> 1993. </year> <title> TDIS: an Algebraic Formalization. </title> <booktitle> In International Joint Conference on Artificial Intelligence. </booktitle>
Reference-contexts: Finally, we believe that resistance to noise may be critical for the text categorization task, since textual databases are usually rather large and are bound to be noisy. Some symbolic learners like ID3 (Quinlan 1986) or Charade <ref> (Ganascia 1993) </ref> are said to construct consistent descriptions of concepts, i.e. a description is generated when all examples covered by this description belong to the same concept. Such learners are not noise-resistant. However, most ML techniques provide some means to take noise into account.
Reference: <author> Hayes, P., and Weinstein, S. </author> <year> 1990. </year> <title> CONSTRUE/TIS: a system for content-based indexing of a database of news stories. </title> <booktitle> In Second Annual Conference on Innovative Applications of Artificial Intelligence. </booktitle>
Reference-contexts: Text categorization is a domain where large data sets are available and which provides an application field to ML (Lewis & Catlett 1994; Cohen 1995). Indeed, manual categorization is known to be an expensive and time-consuming task. Hand-crafted knowledge engineered systems such as CONSTRUE <ref> (Hayes & Weinstein 1990) </ref> also have such drawbacks. ML approaches to classification (text categorization is a classification task) suggest the construction of categorization means using induction over pre-classified samples.
Reference: <author> Hayes, P.; Andersen, P.; Nirenburg, I.; and Schmandt, L. </author> <year> 1990. </year> <title> TCS: A Shell for Content-Based Text Categorization. </title> <booktitle> In Proceeding of the Sixth IEEE CAIA, </booktitle> <pages> 321-325. </pages>
Reference-contexts: Text categorization is a domain where large data sets are available and which provides an application field to ML (Lewis & Catlett 1994; Cohen 1995). Indeed, manual categorization is known to be an expensive and time-consuming task. Hand-crafted knowledge engineered systems such as CONSTRUE <ref> (Hayes & Weinstein 1990) </ref> also have such drawbacks. ML approaches to classification (text categorization is a classification task) suggest the construction of categorization means using induction over pre-classified samples.
Reference: <author> Holsheimer, M., and Siebes, A. </author> <year> 1994. </year> <title> Data mining. The search for knowledge in databases. </title> <type> Technical re-port, </type> <institution> CWI. </institution>
Reference-contexts: Text categorization can be used to support IR or to perform information extraction, document filtering and routing to topic-specific processing mechanisms (Hayes et al. 1990; Riloff & Lehnert 1994). On a machine learning (ML) point of view, recent research has be concerned with scaling-up (e.g. data mining <ref> (Holsheimer & Siebes 1994) </ref>). Text categorization is a domain where large data sets are available and which provides an application field to ML (Lewis & Catlett 1994; Cohen 1995). Indeed, manual categorization is known to be an expensive and time-consuming task.
Reference: <author> Kohavi, R.; John, G.; Long, R.; Manley, D.; and Pfleger, K. </author> <year> 1994. </year> <title> MLC++: A Machine Learning Library in C++. </title> <booktitle> In Tools with Artificial Intelligence, </booktitle> <pages> 740-743. </pages> <publisher> IEEE Computer Society Press. </publisher>
Reference-contexts: An implementation of ID3 (Quinlan 1986) and Charade (Ganascia 1991), a production rule learner, represent symbolic learners, while a k-nearest neighbors algorithm called IB, and a Bayesian approach, NaiveBayes, are instances of numerical learners. The ID3, IB and NaiveBayes are those implemented in the MLC++ library <ref> (Kohavi et al. 1994) </ref>. 1 The Reuters dataset can be obtained by anonymous ftp from /pub/reuters1 on ciir-ftp.cs.umass.edu. 2 Overlooking stories without category assignment was a misunderstanding of the original corpus labels.
Reference: <author> Lewis, D., and Catlett, J. </author> <year> 1994. </year> <title> Heterogeneous un-certainty sampling for supervised learning. </title> <booktitle> In Proceedings of the Eleventh International Conference on Machine Learning. </booktitle>
Reference-contexts: Then, we discuss the issue of choosing one technique rather than another. Actually, many approaches have been suggested; these include numerical learning such as Bayesian classification <ref> (Lewis & Ringuette 1994) </ref>, or symbolic learning like in (Moulin- ier & Ganascia 1995). However, no assessment has be conducted on whether a given learning technique was superior to another on the text categorization task, even though the sketch of an answer can be found in (Lewis & Ringuette 1994). <p> such as Bayesian classification <ref> (Lewis & Ringuette 1994) </ref>, or symbolic learning like in (Moulin- ier & Ganascia 1995). However, no assessment has be conducted on whether a given learning technique was superior to another on the text categorization task, even though the sketch of an answer can be found in (Lewis & Ringuette 1994). We first design an experimental framework which fits in with our unifying view of text categorization systems. <p> For instance, in (Creecy et al. 1992), the authors expand the initial representation and their learner has to deal with over 4 million features. Reduc tion of the training set, as opposed to dimensionality reduction, has also been eluded in this schema. Sam- pling, as described in <ref> (Lewis & Catlett 1994) </ref> and used by (Cohen 1995), is one such approach for reducing the number of training examples. Finally, for the sake of simplicity, we have not included feedback into the whole categorization system. <p> We are not aware of automatic feed-back in the context of text categorization. A Framework to Compare Learners Very few studies have conducted a thorough comparison between learners on the text categorization task. In <ref> (Lewis & Ringuette 1994) </ref>, two learning approaches are compared: Bayesian classification and decision tree construction; (Wiener, Pedersen, & Weigend 1995) experimented on several neural net models. However, most studies report some performance improvements of a given approach over others. <p> Moreover, as ML algorithms currently have difficulties to deal with both large feature and example sets, future research should be dedicated to reducing these sets. One path has been pointed out by <ref> (Lewis & Catlett 1994) </ref> and consists in reducing the sample set. We prefer another path, which includes designing specific algorithms for dimensionality reduction and enhancing the initial text representation, using for in <p>- stance linguistic knowledge.
Reference: <author> Lewis, D., and Ringuette, M. </author> <year> 1994. </year> <title> A comparison of two learning algorithms for text categorization. </title> <booktitle> In Symposium on Document Analysis and Information Retrieval. </booktitle>
Reference-contexts: Then, we discuss the issue of choosing one technique rather than another. Actually, many approaches have been suggested; these include numerical learning such as Bayesian classification <ref> (Lewis & Ringuette 1994) </ref>, or symbolic learning like in (Moulin- ier & Ganascia 1995). However, no assessment has be conducted on whether a given learning technique was superior to another on the text categorization task, even though the sketch of an answer can be found in (Lewis & Ringuette 1994). <p> such as Bayesian classification <ref> (Lewis & Ringuette 1994) </ref>, or symbolic learning like in (Moulin- ier & Ganascia 1995). However, no assessment has be conducted on whether a given learning technique was superior to another on the text categorization task, even though the sketch of an answer can be found in (Lewis & Ringuette 1994). We first design an experimental framework which fits in with our unifying view of text categorization systems. <p> For instance, in (Creecy et al. 1992), the authors expand the initial representation and their learner has to deal with over 4 million features. Reduc tion of the training set, as opposed to dimensionality reduction, has also been eluded in this schema. Sam- pling, as described in <ref> (Lewis & Catlett 1994) </ref> and used by (Cohen 1995), is one such approach for reducing the number of training examples. Finally, for the sake of simplicity, we have not included feedback into the whole categorization system. <p> We are not aware of automatic feed-back in the context of text categorization. A Framework to Compare Learners Very few studies have conducted a thorough comparison between learners on the text categorization task. In <ref> (Lewis & Ringuette 1994) </ref>, two learning approaches are compared: Bayesian classification and decision tree construction; (Wiener, Pedersen, & Weigend 1995) experimented on several neural net models. However, most studies report some performance improvements of a given approach over others. <p> Moreover, as ML algorithms currently have difficulties to deal with both large feature and example sets, future research should be dedicated to reducing these sets. One path has been pointed out by <ref> (Lewis & Catlett 1994) </ref> and consists in reducing the sample set. We prefer another path, which includes designing specific algorithms for dimensionality reduction and enhancing the initial text representation, using for in <p>- stance linguistic knowledge.
Reference: <author> Lewis, D. </author> <year> 1992. </year> <title> Representation and Learning in Information Retrieval. </title> <type> Ph.D. Dissertation, </type> <institution> Graduate School of the University of Massachusetts. </institution>
Reference-contexts: We choose to assess our experiments with an IR criterion, since accuracy, a measure commonly used in ML, is biased by the high disproportion between the assignment and the non-assignment of categories. Thus, we consider recall and precision as evaluation measures. We use micro-averaging <ref> (Lewis 1992, Sec. 6.4) </ref> as a means of cumulating performances over all categories. How- ever, since recall usually goes up (respectively down) when precision goes down (respectively up), it is rather tricky to assess performances on the basis of these two measures. <p> We decided to overlook stories without category assignment, since we could not possibly learn from them 2 . This left us with 7789 learning and 3875 testing examples described by 22791 words provided by Lewis' processing <ref> (Lewis 1992, p. 99) </ref>. Which Learners ? Our experiments were conducted on four learners which illustrate symbolic and numerical learning.
Reference: <author> Lewis, D. </author> <year> 1995. </year> <title> Evaluating and optimizing au-tonomous text classification systems. </title> <booktitle> In Proceedings of SIGIR-95. </booktitle>
Reference-contexts: How- ever, since recall usually goes up (respectively down) when precision goes down (respectively up), it is rather tricky to assess performances on the basis of these two measures. Among several summarizing measures that have been proposed, we choose the F fi -measure <ref> (Lewis 1995) </ref> as an evaluation criterion: F fi = fi 2 P + R where R denotes recall, P precision and fi varies from 0 to infinity.
Reference: <author> Liddy, E.; Paik, W.; and Yu, E. </author> <year> 1994. </year> <title> Text categoriza-tion for multiple users based oon semantic features from a machine-readable dictionary. </title> <journal> ACM Transactions on Information Systems. </journal>
Reference-contexts: In most categorization systems, induction is per-formed by a numerical learner. Linear regression (Biebericher et al. 1988; Fuhr et al. 1991), Bayesian classifiers (Maron 1961; Lewis 1992), k-nearest neighbors (Masand, Linoff, & Waltz 1992; Yang 1994), neural nets (Wiener, Pedersen, & Weigend 1995) and threshold computation <ref> (Liddy, Paik, & Yu 1994) </ref> are instances of such learners. <p> These three facets of knowledge are mostly evoked during the text representation step. Local selection, LSI or even the frequency based model can be considered as adding knowledge extracted from data to a global text representation based on a boolean model. Domain knowledge has been used by <ref> (Liddy, Paik, & Yu 1994) </ref>, where a machine-readable dictionary was employed to build the initial representation. We also consider the assignment of a greater weight to words appearing in the headlines of a news-story (Apte, Dam- erau, & Weiss 1994) as domain knowledge.
Reference: <author> Maron, M. E. </author> <year> 1961. </year> <title> Automatic indexing: an experi-mental inquiry. </title> <journal> Journal of the Association for Computing Machinery </journal> (8):404-417. 
Reference-contexts: aims at reducing the number of features by selecting the best ones according to some criterion; such criteria include mutual information (Lewis & Ringuette 1994; Moulinier & Ganascia 1995), frequency (Apte, Dam- erau, & Weiss 1994), term ranking (Fuhr et al. 1991; Wiener, Pedersen, & Weigend 1995) or expert's judgment <ref> (Maron 1961) </ref>. Construction has a lesser impact in text categorization. Instead of selecting a subset of the original feature set, new features are constructed as combinations of original features. Latent Semantic Indexing (LSI) (Deerwester et al. 1990), as used in (Wiener, Pedersen, & Weigend 1995), is such a constructive approach.
Reference: <author> Masand, B.; Linoff, G.; and Waltz, D. </author> <year> 1992. </year> <title> Classifying News Stories unsing Memory Based Reasoning. </title> <booktitle> In Proc. of the 15th SIGIR. </booktitle>
Reference: <author> Michalski, R. </author> <year> 1983. </year> <title> A theory and methodoly of in-ductive learning. </title> <booktitle> Artificial Intelligence </booktitle> (20):111-161. 
Reference-contexts: First, numerical and symbolic learners differ their abilities to handle structured features and produce understandable classifiers. The instance language, i.e. the feature set issued from text representation, is known to strongly bias the inductive learner <ref> (Michalski 1983) </ref>. Symbolic learners usually deal with a structured instance language but perform rather poorly when they are confronted with numerical data. On the other hand, numerical learners can not easily deal with structured features. Moreover, symbolic learners are often said to produce interpretable classifiers.

Reference: <author> Quinlan, J. R. </author> <year> 1986. </year> <title> Induction of decision trees. </title> <booktitle> Machine Learning 1 </booktitle> <pages> 81-106. </pages>
Reference-contexts: Finally, we believe that resistance to noise may be critical for the text categorization task, since textual databases are usually rather large and are bound to be noisy. Some symbolic learners like ID3 <ref> (Quinlan 1986) </ref> or Charade (Ganascia 1993) are said to construct consistent descriptions of concepts, i.e. a description is generated when all examples covered by this description belong to the same concept. Such learners are not noise-resistant. However, most ML techniques provide some means to take noise into account. <p> This left us with 7789 learning and 3875 testing examples described by 22791 words provided by Lewis' processing (Lewis 1992, p. 99). Which Learners ? Our experiments were conducted on four learners which illustrate symbolic and numerical learning. An implementation of ID3 <ref> (Quinlan 1986) </ref> and Charade (Ganascia 1991), a production rule learner, represent symbolic learners, while a k-nearest neighbors algorithm called IB, and a Bayesian approach, NaiveBayes, are instances of numerical learners.
Reference: <author> Riloff, E., and Lehnert, W. </author> <year> 1994. </year> <title> Information ex-traction as a basis for high-precision text classifi-cation. </title> <journal> ACM Transactions on Information Systems 12(3) </journal> <pages> 296-333. </pages>
Reference: <author> Wiener, E.; Pedersen, J.; and Weigend, A. </author> <year> 1995. </year> <title> A neural network approach to topic spotting. </title> <booktitle> In Symposium on Document Analysis and Information Retrieval. </booktitle>
Reference-contexts: Construction has a lesser impact in text categorization. Instead of selecting a subset of the original feature set, new features are constructed as combinations of original features. Latent Semantic Indexing (LSI) (Deerwester et al. 1990), as used in <ref> (Wiener, Pedersen, & Weigend 1995) </ref>, is such a constructive approach. Inductive Construction of Categorizers Once texts are turned into learning examples, inductive learners are used to induce categorizers. Since the ideas behind these learners are well known in ML, we only review those used in text categorization experiments. <p> In most categorization systems, induction is per-formed by a numerical learner. Linear regression (Biebericher et al. 1988; Fuhr et al. 1991), Bayesian classifiers (Maron 1961; Lewis 1992), k-nearest neighbors (Masand, Linoff, & Waltz 1992; Yang 1994), neural nets <ref> (Wiener, Pedersen, & Weigend 1995) </ref> and threshold computation (Liddy, Paik, & Yu 1994) are instances of such learners. <p> There has been little research conducted on the use of knowledge during the inductive phase of categorization. Nevertheless, a noticeable attempt is presented in <ref> (Wiener, Pedersen, & Weigend 1995) </ref>, where the authors group categories according to semantic characteristics and induce categorizers of these sub-domains. In (Fuhr et al. 1991), the authors used knowledge to guide an indexing system: for instance, knowledge enabled the discrimination among candidate keywords issued from the inductive step. <p> Most experiments reported in text categorization, which used additional knowledge in the representation and induction steps, show that an enriched categorization system outperforms a naive approach. However, few studies have reported experiments, where varying amounts of knowledge were involved. For instance, <ref> (Wiener, Pedersen, & Weigend 1995) </ref> reported an enhancement of 5% using LSI and a hierarchical net over boolean features using a flat network. Similarly, (Apte, Damerau, & Weiss 1994) reported a increase of 5% between a locally reduced representation based on frequency and weight assignment, and a global boolean representation. <p> We are not aware of automatic feed-back in the context of text categorization. A Framework to Compare Learners Very few studies have conducted a thorough comparison between learners on the text categorization task. In (Lewis & Ringuette 1994), two learning approaches are compared: Bayesian classification and decision tree construction; <ref> (Wiener, Pedersen, & Weigend 1995) </ref> experimented on several neural net models. However, most studies report some performance improvements of a given approach over others. Hence, there has been no conjecture on the properties a learner ought to possess so that it performs well on the text categorization task. <p> The evaluation was conducted on the set of 3875 testing examples. Results, reported in Table 1, show the best performances with regards to the F 1 criterion. Results from earlier experiments on the same corpus complete this summary. Neural nets refer to the experiments presented by <ref> (Wiener, Pedersen, & Weigend 1995) </ref>, while Swap-1 is a production rule learner used in (Apte, Damerau, & Weiss 1994). In both cases, the enriched model takes into account various kinds of knowledge sources, while the (local) boolean model is very close to our naive framework.
Reference: <author> Yang, Y. </author> <year> 1994. </year> <title> Expert network: Effective and effi-cient learning from human decisions in text catego-rization and retrieval. </title> <booktitle> In Proceedings of SIGIR-94. </booktitle>
References-found: 24

