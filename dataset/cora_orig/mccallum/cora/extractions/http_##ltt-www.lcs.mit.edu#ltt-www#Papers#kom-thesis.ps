URL: http://ltt-www.lcs.mit.edu/ltt-www/Papers/kom-thesis.ps
Refering-URL: http://ltt-www.lcs.mit.edu/ltt-www/Papers/Papers.html
Root-URL: 
Note: Page 1  
Abstract: This report is to fulfill the last part of the 6.199 Advanced Undergraduate Project requirement. The projects objective is to build mechanisms that detect the faults expected in the Library 2000 (L2000) system. The mechanisms include a local integrity checker and a remote integrity checker. The main accomplishments of the project are: Design of the detection mechanisms Analysis of the design in relation to the fault models Partial implementation of the detection mechanisms 
Abstract-found: 1
Intro-found: 1
Reference: [6.826-1] <author> 6.826 Class Handout #47. </author> <title> Replication Techniques. </title> <institution> MIT Laboratory of Computer Science. </institution> <month> November 22, </month> <year> 1993. </year> <title> [6.826-2] 6.826 Class Handout #42. </title> <type> Consensus. </type> <institution> MIT Laboratory of Computer Science. </institution> <month> November 15, </month> <year> 1993. </year>
Reference-contexts: General Criteria for Design The general criteria for design, as mentioned in [Library2000-1] , permeate the design choices made in this report. The criteria are as follows: Works on standard operating system, file system, disk storage system, and networking protocol. Replicas data may be weakly consistent <ref> [Golding1, Golding2, 6.826-1] </ref> and updates can be unavailable for a bounded time. Works with system with at least 1,000,000 Gbytes of data. Should be made simple by comprising a small number of independent mechanisms whose correctness can be verified.
Reference: [Dally1] <author> Dally, W. J. </author> <type> 6.823 Lectures Note #14. </type> <institution> MIT Laboratory of Computer Science. </institution> <month> Spring </month> <year> 1994. </year>
Reference-contexts: The following four sections, sections 5 through 8, are the substance of the project. The flow of the sections roughly follows Figure 1 <ref> [Dally1] </ref> . The fault models in section 5 discuss how faults occur. Section 6 describes overall designs and assumed environments of the system. The detection scheme is discussed in section 7 and finally, the analysis of the scheme is discussed in section 8.
Reference: [Golding1] <author> Richard A. Golding. </author> <title> A weak-consistency architecture for distributed information services. </title> <note> Technical Report UCSC-CRL-92-31 (6 July 1992). </note> <institution> Concurrent Systems Laboratory, University of California at Santa Cruz. </institution>
Reference-contexts: General Criteria for Design The general criteria for design, as mentioned in [Library2000-1] , permeate the design choices made in this report. The criteria are as follows: Works on standard operating system, file system, disk storage system, and networking protocol. Replicas data may be weakly consistent <ref> [Golding1, Golding2, 6.826-1] </ref> and updates can be unavailable for a bounded time. Works with system with at least 1,000,000 Gbytes of data. Should be made simple by comprising a small number of independent mechanisms whose correctness can be verified. <p> Lifetime of data exceeds the lifetime of any storage and exceeds the mean time between major disasters. - Page 9 - 3. Motivation Why do we design a new storage-replication service when a few <ref> [Liskov1, Satyanarayanan1, Page1, Hisgen1, Golding1, Golding2] </ref> have already been designed? This is because the demands on the replication service in L2000 are different from the systems that researchers have considered. <p> This property gives confidence that the data on the system will survive unaltered through decades. Fourth, the L2000 design must layer on top of the available file systems. Modifications to system programs are avoided. Not all the works cited, <ref> [Liskov1, Satyanarayanan1, Page1, Hisgen1, Golding1, Golding2] </ref> , have this need. 4. 6.199 Project The above three sections discussed the goals of the L2000 replication research, so how does this project contribute to the research? To correct or mask faults on a system, the faults must be detected. <p> The design has the property that, if the updates are reflected on the majority of the replicas, it is highly probable that the updates will become reliable within a bounded time. Comparison to or usage of update propagation schemes that rely on rumor mongery [Shroeder1] or anti-atrophy sessions <ref> [Golding1, Golding2] </ref> which may be more effective than the proposed design. The detection scheme works in the narrowly defined model.
Reference: [Golding2] <author> Richard A. Golding and Darrell D. E. </author> <title> Long. The Performance of Weak-consistency Replication Protocols. </title> <note> Technical Report UCSC-CRL-92-30 (6 July 1992). </note> <institution> Concurrent Systems Laboratory, University of California at Santa Cruz. </institution>
Reference-contexts: General Criteria for Design The general criteria for design, as mentioned in [Library2000-1] , permeate the design choices made in this report. The criteria are as follows: Works on standard operating system, file system, disk storage system, and networking protocol. Replicas data may be weakly consistent <ref> [Golding1, Golding2, 6.826-1] </ref> and updates can be unavailable for a bounded time. Works with system with at least 1,000,000 Gbytes of data. Should be made simple by comprising a small number of independent mechanisms whose correctness can be verified. <p> Lifetime of data exceeds the lifetime of any storage and exceeds the mean time between major disasters. - Page 9 - 3. Motivation Why do we design a new storage-replication service when a few <ref> [Liskov1, Satyanarayanan1, Page1, Hisgen1, Golding1, Golding2] </ref> have already been designed? This is because the demands on the replication service in L2000 are different from the systems that researchers have considered. <p> This property gives confidence that the data on the system will survive unaltered through decades. Fourth, the L2000 design must layer on top of the available file systems. Modifications to system programs are avoided. Not all the works cited, <ref> [Liskov1, Satyanarayanan1, Page1, Hisgen1, Golding1, Golding2] </ref> , have this need. 4. 6.199 Project The above three sections discussed the goals of the L2000 replication research, so how does this project contribute to the research? To correct or mask faults on a system, the faults must be detected. <p> The design has the property that, if the updates are reflected on the majority of the replicas, it is highly probable that the updates will become reliable within a bounded time. Comparison to or usage of update propagation schemes that rely on rumor mongery [Shroeder1] or anti-atrophy sessions <ref> [Golding1, Golding2] </ref> which may be more effective than the proposed design. The detection scheme works in the narrowly defined model.
Reference: [Gray1] <author> Jim Gray and Andreas Reuter, </author> <title> Fault Tolerance, from Transaction Processing: Concepts and Techniques, </title> <publisher> Morgan Kaufmann, </publisher> <year> 1993, </year> <pages> pp. 93-156. </pages>
Reference-contexts: What could go wrong with the data in a storage system? Something goes wrong (a fault occurs) when the stored data cannot be accessed or when the data returned by the system is incorrect. There are many causes of faults, but they can be broadly categorized as follows <ref> [Gray1] </ref> : environments, operations, maintenance, hardware, software, and process. Examples for each category are listed in Table 2 for concreteness. - Page 7 - Category Examples Environments Power failure, weather, earthquakes, fires. Operations Crucial processes killed, system configured improperly.
Reference: [Hisgen1] <author> Hisgen, Andy, et al. </author> <title> Granularity and Semantic Level of Replication in the Echo Distributed File System, </title> <booktitle> Proceedings of the Workshop on Management of replicated Data, </booktitle> <address> Houston, Texas (November 8, </address> <year> 1990), </year> <pages> pp. 2-4. </pages>
Reference-contexts: Lifetime of data exceeds the lifetime of any storage and exceeds the mean time between major disasters. - Page 9 - 3. Motivation Why do we design a new storage-replication service when a few <ref> [Liskov1, Satyanarayanan1, Page1, Hisgen1, Golding1, Golding2] </ref> have already been designed? This is because the demands on the replication service in L2000 are different from the systems that researchers have considered. <p> This property gives confidence that the data on the system will survive unaltered through decades. Fourth, the L2000 design must layer on top of the available file systems. Modifications to system programs are avoided. Not all the works cited, <ref> [Liskov1, Satyanarayanan1, Page1, Hisgen1, Golding1, Golding2] </ref> , have this need. 4. 6.199 Project The above three sections discussed the goals of the L2000 replication research, so how does this project contribute to the research? To correct or mask faults on a system, the faults must be detected.
Reference: [ Library2000-1 ] <institution> Storage Service Replication Design Thoughts, </institution> <note> Ideas from Mitchell Charity, Win Treese, and Jerry Saltzer, Library 2000, Draft of 1/7/93. /afs/athena.mit.edu/user/other/Saltzer/library/work-in-progress/think-pieces/storage-replication.txt. - Page 46 </note> - 
Reference-contexts: Moreover, since maximizing the replicas independence means locating them in a wide geographical area, the replication servicedetecting and repairing faults, and propagating legitimate changes to the replicasmust be done efficiently to offset communication cost and unreliability. 2. General Criteria for Design The general criteria for design, as mentioned in <ref> [Library2000-1] </ref> , permeate the design choices made in this report. The criteria are as follows: Works on standard operating system, file system, disk storage system, and networking protocol. Replicas data may be weakly consistent [Golding1, Golding2, 6.826-1] and updates can be unavailable for a bounded time. <p> Note that the equation relies on the assumption that disks fail independently. Also note that disks MTTF is in hours. Failures Per Year of disks Disk MTTF _ _ _ 10 000 Equation 1: Frequency of Disk Failures According to <ref> [ Library2000-1 ] </ref>, a typical data center has 1,000 disks. According to the specification, the Mean Time To Failure (MTTF) of 1992s disks is 500,000 hours. Therefore, the lower-bound of failure rate is 20 failures per year. <p> a case where a single fault causes a valid change on the system to disappear; the changes that have not been propagated to all the replicas are not fault-tolerant. - Page 15 - Hence, the legitimate changes to the replicas are said to be in two stages: fragile and fault-tolerant <ref> [ Library2000-1 ] </ref>. Changes that are in the fragile stage have not been propagated to all the replicas, and might disappear or be marked as irrecoverable before they are propagated. <p> systems checksum file with a supplied data Table 5: Replica Interface Primitives 6.3 UID to Location Translation Table (ULTT) What is a UID in the L2000 system? A UID is the token used to uniquely and universally identify a document stored on the L2000 systems and related electronic library systems <ref> [ Library2000-1 ] </ref>. The UID is a result of the need to identify documents uniquely and universally across multiple library systems. In todays system, data is stored using file systems. Hence, UID needs to be translated into the location of the data. <p> When As LIC runs next time, Ds new checksum will be incorporated into As validated checksum file. 9. Issues Not Addressed in this Project The issues that are important, but are not addressed in this project include: Whats new mechanism <ref> [Library2000-1] </ref> . The mapping from UIDs of documents to the data [Library2000-2] . The proposed system uses path names as UIDs. Algorithm that calculates good checksums of objects that contain large amount of data. Group membership protocol. The proposed system will have fixed group membership. <p> By scaling up the checking periods, we lengthen the window of vulnerability of the data. So it is unclear at this point which approach is more advantageous. 10. Acknowledgment My contribution to the L2000 replication research has been mostly the analysis of the scheme. Most ideas are from <ref> [ Library2000-1 ] </ref> paper, Mitchell Charity, and Jerry Saltzer. Contributions to the report were made by Thomas Lee, Jeremy Hylton, and Ali Alavi. I especially thank Mitchell and Jerry for patiently explaining to me issues that I needed to think aboutfrom embarrassingly simple issues to the most subtle ones.
Reference: [Library2000-2 ] <institution> Semantics of the Library 2000 Network Storage Service, </institution> <note> Version of March 9, 1993, Notes from meetings and discussion attended by Mitchell Charity, Quinton Zondervan, </note> <author> Manish Mazumdar, Rob Miller, Thomas Lee, Win Treese, Ron Weiss, and J. H. Saltzer. </author> <month> /afs/athena.mit.edu/user/other/Saltzer/library/work-in-progress/think-pieces/storage-server.txt </month>
Reference-contexts: Issues Not Addressed in this Project The issues that are important, but are not addressed in this project include: Whats new mechanism [Library2000-1] . The mapping from UIDs of documents to the data <ref> [Library2000-2] </ref> . The proposed system uses path names as UIDs. Algorithm that calculates good checksums of objects that contain large amount of data. Group membership protocol. The proposed system will have fixed group membership. Property that updates are either ignored or become reliable in a bounded time.
Reference: [Liskov1] <author> Barbara Liskov, et al. </author> <title> Replication in the Harp File System, </title> <type> MIT LCS Technical Report MIT/LCS/TM-456. </type> <month> August </month> <year> 1991. </year>
Reference-contexts: Lifetime of data exceeds the lifetime of any storage and exceeds the mean time between major disasters. - Page 9 - 3. Motivation Why do we design a new storage-replication service when a few <ref> [Liskov1, Satyanarayanan1, Page1, Hisgen1, Golding1, Golding2] </ref> have already been designed? This is because the demands on the replication service in L2000 are different from the systems that researchers have considered. <p> This property gives confidence that the data on the system will survive unaltered through decades. Fourth, the L2000 design must layer on top of the available file systems. Modifications to system programs are avoided. Not all the works cited, <ref> [Liskov1, Satyanarayanan1, Page1, Hisgen1, Golding1, Golding2] </ref> , have this need. 4. 6.199 Project The above three sections discussed the goals of the L2000 replication research, so how does this project contribute to the research? To correct or mask faults on a system, the faults must be detected.
Reference: [Page1] <author> Page, Thomas W., Jr., et al. </author> <title> Architecture of the Ficus Scaleable Replicated File System. </title> <institution> UCLA Computer Science Department Technical Report CSD-910005, </institution> <month> March </month> <year> 1991. </year>
Reference-contexts: Lifetime of data exceeds the lifetime of any storage and exceeds the mean time between major disasters. - Page 9 - 3. Motivation Why do we design a new storage-replication service when a few <ref> [Liskov1, Satyanarayanan1, Page1, Hisgen1, Golding1, Golding2] </ref> have already been designed? This is because the demands on the replication service in L2000 are different from the systems that researchers have considered. <p> This property gives confidence that the data on the system will survive unaltered through decades. Fourth, the L2000 design must layer on top of the available file systems. Modifications to system programs are avoided. Not all the works cited, <ref> [Liskov1, Satyanarayanan1, Page1, Hisgen1, Golding1, Golding2] </ref> , have this need. 4. 6.199 Project The above three sections discussed the goals of the L2000 replication research, so how does this project contribute to the research? To correct or mask faults on a system, the faults must be detected.
Reference: [Patterson1] <author> David A. Patterson, Garth Gibson, and Randy H. Katz. </author> <title> A Case for Redundant Arrays of Inexpensive Disks (RAID). </title> <booktitle> ACM SIGMOD Conference, </booktitle> <pages> pages 109-116, </pages> <month> June </month> <year> 1988. </year>
Reference-contexts: However, the lower bound can be estimated by using the failure characteristic of the storage device. - Page 12 - The frequency of disk failures per year can be calculated as in Equation 1 <ref> [Patterson1] </ref> . 10,000 is the approximate number of hours per year. Note that the equation relies on the assumption that disks fail independently. Also note that disks MTTF is in hours.
Reference: [Saltzer1] <author> Jerome H. Saltzer. </author> <title> LIBRARY 2000, A research prototype of the online electronic library of tomorrow. </title> <address> /afs/athena.mit.edu/user/other/Saltzer/library/work-in-progress/ prospectus.txt. </address> <month> October 31, </month> <year> 1991 </year>
Reference-contexts: 1. Introduction The technology of online storage, display, and communications will, by the year 2000, make it economically possible to place the entire contents of a library online, accessible from computer workstations located anywhere. <ref> [Saltzer1] </ref> Many engineering issues arise when such a library system is built. One of them is how to preserve the digital data stored in the system for several decades: a period of time longer than the storages lifetime.
Reference: [Satyanarayanan1] <author> Mahadev Satyanarayanan, et al. Coda: </author> <title> A Highly Available File System for a Distributed Workstation Environment, </title> <journal> IEEE Transactions on Computers 39, </journal> <month> 4 (April </month> <year> 1990), </year> <pages> pp. 447-459. </pages>
Reference-contexts: Lifetime of data exceeds the lifetime of any storage and exceeds the mean time between major disasters. - Page 9 - 3. Motivation Why do we design a new storage-replication service when a few <ref> [Liskov1, Satyanarayanan1, Page1, Hisgen1, Golding1, Golding2] </ref> have already been designed? This is because the demands on the replication service in L2000 are different from the systems that researchers have considered. <p> This property gives confidence that the data on the system will survive unaltered through decades. Fourth, the L2000 design must layer on top of the available file systems. Modifications to system programs are avoided. Not all the works cited, <ref> [Liskov1, Satyanarayanan1, Page1, Hisgen1, Golding1, Golding2] </ref> , have this need. 4. 6.199 Project The above three sections discussed the goals of the L2000 replication research, so how does this project contribute to the research? To correct or mask faults on a system, the faults must be detected.
Reference: [Schroeder1] <author> Michael D. Schroeder, Andrew D. Birrell, and Roger M. Needham. </author> <title> Experience with Grapevine: the growth of a distributed system. </title> <journal> ACM Transactions on Computer systems, </journal> <volume> 2 (1):3-23 (February 1984). - Page 47 </volume> -
References-found: 14

