URL: http://www.cs.brown.edu/people/tld/postscript/DeanGivanandKimAIPS-98.ps
Refering-URL: http://www.cs.brown.edu/research/ai/publications/
Root-URL: http://www.cs.brown.edu
Email: -tld,kek-@cs.brown.edu givan@ecn.purdue.edu  
Phone: (401) 863-7600 (765) 494-9068  
Title: Solving Stochastic Planning Problems With Large State and Action Spaces  
Author: Thomas Dean, Robert Givan, and Kee-Eung Kim Thomas Dean and Kee-Eung Kim Robert Givan 
Web: http://www.cs.brown.edu/~-tld,kek- http://dynamo.ecn.purdue.edu/~givan  
Address: Providence, RI 02912 West Lafayette, IN 47907  
Affiliation: Department of Computer Science Department of Electrical and Computer Engineering Brown University Purdue University  
Abstract: Planning methods for deterministic planning problems traditionally exploit factored representations to encode the dynamics of problems in terms of a set of parameters, e.g., the location of a robot or the status of a piece of equipment. Factored representations achieve economy of representation by taking advantage of structure in the form of dependency relationships among these parameters. In recent work, we have addressed the problem of achieving the same economy of representation and exploiting the resulting encoding of structure for stochastic planning problems represented as Markov decision processes. In this paper, we extend our earlier work on reasoning about such factored representations to handle problems with large action spaces that are also represented in factored form, where the parameters in this case might correspond to the control parameters for different effectors on a robot or the allocations for a set of resources. The techniques described in this paper employ factored representations for Markov decision processes to identify and exploit regularities in the dynamics to expedite inference. These regularities are in the form of sets of states (described for example by boolean formulas) that behave the same with respect to sets of actions where these sets are thought of as aggregate states and aggregate actions respectively. We present theoretical foundations, describe algorithms, provide examples in which our techniques provide leverage and examples in which they fail to do so, and summarize the results of experiments with a preliminary implementation. 
Abstract-found: 1
Intro-found: 1
Reference: [Boutilier et al., 1995] <author> Boutilier, Craig; Dearden, Richard; and Goldszmidt, Moises, </author> <year> 1995. </year> <title> Exploiting structure in policy construction. </title> <booktitle> In Proceedings IJCAI 14. IJCAII. </booktitle> <pages> 1104-1111. </pages>
Reference-contexts: 1. Introduction The methods developed in this paper extend specific planning algorithms <ref> [Boutilier et al., 1995; Dean & Givan, 1997] </ref> developed for handling large state spaces to handle large action spaces. The basic approach involves reformulating a problem with large state and action spaces as a problem with much smaller state and action spaces. <p> The nonlinear operator that is at the heart of value iteration requires computing for each state a value maximizing over all actions and taking expectations over all states (11) In structured methods for solving MDPs with factorial representations, such as those described in <ref> [Boutilier et al., 1995] </ref> [Dean & Givan, 1997], instead of considering the consequences of an action taking individual states to individual states, we consider how an action takes sets of states to sets of states.
Reference: [Dean and Givan, 1997] <author> Dean, Thomas and Givan, Robert, </author> <year> 1997. </year> <title> Model minimization in Markov decision processes. </title> <booktitle> In Proceedings AAAI-97. </booktitle> <publisher> AAAI. </publisher>
Reference: [Dean et al., 1997] <author> Dean, Thomas; Givan, Robert; and Leach, Sonia, </author> <year> 1997. </year> <title> Model reduction techniques for computing approximately optimal solutions for Markov decision processes. </title> <booktitle> In Thirteenth Conference on Uncertainty in Artificial Intelligence. </booktitle>
Reference-contexts: In some cases, this cost can be ameliorated by computing an -approximate model <ref> [Dean et al., 1997] </ref>, but such approximation methods offer no panacea for hard scheduling problems.
Reference: [Givan & Dean, 1997] <author> Givan, Robert; Dean, Thomas, </author> <year> 1997. </year> <title> Model Minimization, regression, and propositional STRIPS planning. </title> <booktitle> In Proceedings of IJCAI 15, </booktitle> <address> IJCAII. </address>
Reference-contexts: We explore the connections between model minimization and goal regression in <ref> [Givan & Dean, 1997] </ref>. Tsitsiklis and Van Roy [1996] also describe algorithms for solving Markov decision processes with factorial models. Our basic treatment Markov decision processes borrows from Puterman [1994].
Reference: [Lee and Yannakakis, 1992] <author> Lee, David and Yannakakis, Mihalis, </author> <year> 1992. </year> <title> On-line minimization of transition systems. </title> <booktitle> In Proceedings of 24th Annual ACM Symposium on the Theory of Computing. </booktitle>
Reference: [Puterman, 1994] <author> Puterman, Martin L., </author> <year> 1994. </year> <title> Markov Deci sion Processes. </title> <publisher> John Wiley & Sons, </publisher> <address> New York. </address>

References-found: 6

