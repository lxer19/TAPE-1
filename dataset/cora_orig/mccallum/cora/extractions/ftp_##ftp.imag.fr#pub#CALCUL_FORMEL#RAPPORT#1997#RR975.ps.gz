URL: ftp://ftp.imag.fr/pub/CALCUL_FORMEL/RAPPORT/1997/RR975.ps.gz
Refering-URL: http://www-lmc.imag.fr/CF/publi.html
Root-URL: http://www.imag.fr
Title: RAPPORT DE RECHERCHE A study of Coppersmith's block Wiedemann algorithm using matrix polynomials  
Author: G. Villard 
Note: N o 975 IM Avril 97  
Abstract-found: 0
Intro-found: 1
Reference: 1. <author> A.C. Antoulas, </author> <title> On recursiveness and related topics in linear systems, </title> <journal> IEEE Trans. Automat. Control AC-31 (1986), </journal> <volume> no. 12, </volume> <pages> 1121-1135. </pages>
Reference-contexts: Typically, Coppersmith has proposed a generalization of Berlekamp-Massey algorithm [28] to the matrix case. It can be seen that as precisely stated in the scalar case in [11] the algorithm is strongly related to Euclidean division <ref> [1, 6] </ref>. The reader may refer to [38] for a detailed study of these relations and a proof of the correctness of the algorithm. Its cost is O ((m + n)N 2 ) arithmetic operations in K. <p> It is well known that identity (4.3) has two main interpretations, in the reverse sense in relation with Pade approximation and in the direct sense in relation with Euclids's algorithm <ref> [1, 11, 6, 38] </ref>. As seen in x3.2, these interpretations actually give various methods to compute generating polynomials. For a polynomial g () of degree d, denote by ^g () its reversal ^g () = d g (1=).
Reference: 2. <author> D. Augot and P. Camion, </author> <title> The minimal polynomials, characteristic subspaces, normal bases and the Frobenius form, </title> <type> Tech. Report 2006, </type> <institution> INRIA France, </institution> <month> August </month> <year> 1993. </year>
Reference-contexts: The transformation P may be chosen such that A Y is shift-Hessenberg <ref> [2] </ref> or polycyclic form [31]. This form is 34 Gilles VILLARD block-triangular: A Y = 6 6 6 6 C 1 T 1;2 T 1;3 : : : T 1;OE Y . . . . . . . . . . . . . . .
Reference: 3. <author> B. Beckermann and G. Labahn, </author> <title> A uniform approach for the fast computation of matrix-type Pade approximants, </title> <journal> SIAM J. Matrix Anal. Appl. </journal> <volume> 15 (1994), no. 3, </volume> <pages> 804-823. </pages> <month> 4. </month> , <title> Recursiveness in matrix rational interpolation problems, </title> <type> Tech. Report Publication ANO 357, </type> <institution> Universite de Lille France, </institution> <year> 1996. </year>
Reference-contexts: The reader may refer to the uniqueness conditions for the matrix minimal Pade problem in [6] and to the oe-bases in <ref> [3] </ref>. 3. Coppersmith's block Wiedemann algorithm Using above terminology, we now give the Coppersmith's version [9] of Wiede-mann's algorithm for the solution of Aw = 0. The matrix A is singular and square of dimension N over a field K. <p> This will motivate us, in x9, to bound the probability of success either to compute one generating polynomial or several ones. The same remarks remain valid if we use instead a method based on Pade approximation. This approach enables one to use the superfast deterministic algorithm in <ref> [3] </ref>. Using FFT-based polynomial multiplication, generating polynomials will be computed at deterministic cost O ((m+n) 2 m (N=n) log 2 (mN=n) log log (mN=n)) [38]. Another point of view may be found in [22].
Reference: 5. <author> R.R. Bitmead and B.D.O. Anderson, </author> <title> Asymptotically fast solution iof toeplitz and related systems of linear equations, Linear Algebra and its Appl. </title> <booktitle> 34 (1980), </booktitle> <pages> 103-116. </pages>
Reference-contexts: Thus, step 3 of the block algorithm can be implemented as the computation of vectors in the kernel of M (ffi l + ; ffi r + 1) or of the associated block-Toeplitz matrix. Using the Bitmead-Anderson/Morf method <ref> [5, 30] </ref>, this can be done at probabilistic cost O ((m + n) 2 N log 2 N log log N ) [18]. 4. Characterization of good blocking matrices We have to characterize "good" matrices X and Y . This characterization is divided into two parts.
Reference: 6. <author> A. Bultheel and M. Van Barel, </author> <title> A matrix Euclidean algorithm and the matrix minimal Pade approximation problem, Continued Fractions and Pade Approximants (C. </title> <editor> Brezinski, ed.), </editor> <publisher> North-Holland, </publisher> <year> 1990, </year> <pages> pp. 11-51. </pages>
Reference-contexts: All the bases arranged as columns in a matrix of W differ by a right uni-modular multiplier. Thus the set of the right generating matrix polynomials of a sequence (2.2) can be uniquely determined by choosing a particular representative. As emphasized in <ref> [6, 37] </ref> several matrix polynomial normal form can be chosen. In next paragraph we focus on the Popov form which provides a notion of minimal polynomial. 2.2. Minimum generating polynomials. From a complexity point of view it is important to handle relations (2.3) or (2.4) of minimal length. <p> The reader may refer to the uniqueness conditions for the matrix minimal Pade problem in <ref> [6] </ref> and to the oe-bases in [3]. 3. Coppersmith's block Wiedemann algorithm Using above terminology, we now give the Coppersmith's version [9] of Wiede-mann's algorithm for the solution of Aw = 0. The matrix A is singular and square of dimension N over a field K. <p> Typically, Coppersmith has proposed a generalization of Berlekamp-Massey algorithm [28] to the matrix case. It can be seen that as precisely stated in the scalar case in [11] the algorithm is strongly related to Euclidean division <ref> [1, 6] </ref>. The reader may refer to [38] for a detailed study of these relations and a proof of the correctness of the algorithm. Its cost is O ((m + n)N 2 ) arithmetic operations in K. <p> It is well known that identity (4.3) has two main interpretations, in the reverse sense in relation with Pade approximation and in the direct sense in relation with Euclids's algorithm <ref> [1, 11, 6, 38] </ref>. As seen in x3.2, these interpretations actually give various methods to compute generating polynomials. For a polynomial g () of degree d, denote by ^g () its reversal ^g () = d g (1=).
Reference: 7. <author> W.A. Coppel, </author> <title> Matrices of rational functions, </title> <journal> Bull. Austral. Math. Soc. </journal> <volume> 11 (1974), </volume> <pages> 89-113. </pages>
Reference-contexts: The non-unity invariant factors of A o and of D W () are the same. This is a classical result. The proof may be found in several papers. We refer to <ref> [7, 40] </ref> or to [17] and references therein. Given D Y (), next result states precisely how to choose X so that the generating polynomial remains unchanged. It is proven using also classical arguments from linear system theory (see [17] for instance).
Reference: 8. <author> D. Coppersmith, </author> <title> Solving linear equations over GF(2): block Lanczos algorithm, Linear Algebra and its Applications 192 (1993), 33-60. 9. , Solving homogeneous linear equations over GF(2) via block Wiedemann algorithm, </title> <journal> Math. Comp. </journal> <volume> 62 (1994), no. 205, </volume> <pages> 333-350. </pages>
Reference-contexts: This has motivated several authors to develop fast finite-field counterpart to numerical iterative methods. The conjugate gradient method has been used in [23], the Lanczos method in [23, 12] and the block Lanczos method in <ref> [8, 29] </ref>. But up to now, only the probabilistic analysis of Wiedemann [39] was giving a provably reliable and efficient method to solve Aw = 0 over small fields. This method is based on finding relations in Krylov subspaces using the Berlekamp-Massey algorithm [28].
Reference: 10. <author> P. Delsarte, Y.V. Genin, and Y.G. Kamp, </author> <title> A generalization of the Levinson algorithm for Hermitian Toeplitz matrices with any rank profile, </title> <booktitle> IEEE Internat. Conf. Acoust. Speech Signal Process. 33 (1985), </booktitle> <pages> 964-971. </pages>
Reference-contexts: This concept catches what are, in general, the column degrees of the generating polynomials for sequences constructed from a given matrix A. This is the same concept than the generic rank profiles <ref> [10] </ref> that catches what are, in general, the ranks of leading principal submatrices of matrices equivalent to a given matrix A. We thus follow the technique from Kaltofen, Pan and Saunders [20, 21, 18].
Reference: 11. <author> J.L. Dornstetter, </author> <title> On the equivalence between Berlekamp's and Euclid's algorithms, </title> <journal> IEEE Trans. Inform. Theory 33 (1987), </journal> <pages> 428-431. </pages>
Reference-contexts: Indeed, it seems that for several known methods, one may compute one or several generating polynomials at the same asymptotical cost. Typically, Coppersmith has proposed a generalization of Berlekamp-Massey algorithm [28] to the matrix case. It can be seen that as precisely stated in the scalar case in <ref> [11] </ref> the algorithm is strongly related to Euclidean division [1, 6]. The reader may refer to [38] for a detailed study of these relations and a proof of the correctness of the algorithm. Its cost is O ((m + n)N 2 ) arithmetic operations in K. <p> It is well known that identity (4.3) has two main interpretations, in the reverse sense in relation with Pade approximation and in the direct sense in relation with Euclids's algorithm <ref> [1, 11, 6, 38] </ref>. As seen in x3.2, these interpretations actually give various methods to compute generating polynomials. For a polynomial g () of degree d, denote by ^g () its reversal ^g () = d g (1=).
Reference: 12. <author> W. Eberly and E. Kaltofen, </author> <title> On randomized Lanczos algorithms, </title> <booktitle> International Symposium on Symbolic and Algebraic Computation, </booktitle> <address> Maui, Hawaii, USA, </address> <publisher> ACM Press, </publisher> <month> July </month> <year> 1997. </year>
Reference-contexts: This has motivated several authors to develop fast finite-field counterpart to numerical iterative methods. The conjugate gradient method has been used in [23], the Lanczos method in <ref> [23, 12] </ref> and the block Lanczos method in [8, 29]. But up to now, only the probabilistic analysis of Wiedemann [39] was giving a provably reliable and efficient method to solve Aw = 0 over small fields.
Reference: 13. <author> G.D. Forney, </author> <title> Minimal bases of rational vector spaces, with applications to multivariable linear systems, </title> <journal> SIAM J. Control 13 (1975), </journal> <pages> 493-520. </pages>
Reference-contexts: Minimum generating polynomials. From a complexity point of view it is important to handle relations (2.3) or (2.4) of minimal length. We will define minimal bases for W, these will correspond to minimal bases of vector spaces <ref> [13, 36] </ref>. In addition, to extend the notion of minimal scalar polynomial, we will speak (by abuse of language) of minimal generating matrix polynomial. A basis given by the columns of a matrix D () will be minimal (see theorem 2.4 below) when D () will be column reduced. <p> Any two right equivalent matrices in M n (K []) are right equivalent to a same unique matrix in Popov form. Here is an important classical fact that identifies column reduced forms and minimal bases. Theorem 2.4. <ref> [13] </ref>. Let D () be a basis of W with j-th column degree d j , 1 j n. For any element w () of W let w () = D ()v (). <p> W have the same set of column degrees fd j g 1jn , they are called the Kronecker indices of W. 10 Gilles VILLARD If the indices are arranged in increasing order, identity (2.5) shows that the corresponding elements of W are the first linearly independent ones with minimal degrees <ref> [13, 17] </ref>. This motivates the following definition. Definition 2.5. The unique minimal basis D W () in Popov form is called the minimal (right) generating matrix polynomial for the matrix sequence fXA i Y g 1 i=0 . We may also look at this characterization column by column.
Reference: 14. <author> F.R. Gantmacher, </author> <title> Theorie des matrices, </title> <address> Dunod, Paris, France, </address> <year> 1966. </year>
Reference-contexts: Separation. In the scalar case, for a matrix A in M N (K) , if u and u 0 are two vectors whose minimal polynomials u 0 () and u 00 () with respect to A are relatively prime, then we know (see <ref> [14] </ref> for instance) that the minimal polynomial of u 0 +u 00 is u 0 () u 00 (). This result remains valid in the case of matrix polynomials. <p> We have focused on a particular block algorithm, the other block ones cited in the introduction may also be considered. We have focused on finite fields, our results may be used for other computable fields. Appendix A Some facts from linear algebra The reader may refer to <ref> [14] </ref> or to [16] for the following matrix basic facts. Theorem 11.1.
Reference: 15. <author> M. Giesbrecht, </author> <title> Nearly optimal algorithms for canonical matrix forms , Ph.D. </title> <type> thesis, </type> <institution> Department of Computer Science, University of Toronto, </institution> <year> 1993. </year>
Reference-contexts: Lower bounds for that probability are given in [39] and in <ref> [15] </ref>. To apply lemma 4.2, in x8.1 below we propose a way of bounding the probability in the matrix case. In particular, this will result in a new proof of Wiedemann's bounds in the scalar case. <p> For the first term of the main product, if ae + = 1 i.e. if n = OE then from theorem 3.15 of <ref> [15] </ref>: Y i which gives with the lower bound for (11.6): n = OE : n (f; OE) &gt; 1=(45 log q N ): If ae + &gt; 1 then using (11.4) Y (1 q (ae+)d i ) &gt; 1 + log (1 1=q ae+1 ); STUDY OF COPPERSMITH'S ALGORITHM USING
Reference: 16. <author> N. Jacobson, </author> <title> Basic Algebra I, W.H. </title> <publisher> Freeman and Company, </publisher> <year> 1974. </year>
Reference-contexts: STUDY OF COPPERSMITH'S ALGORITHM USING MATRIX POLYNOMIALS 9 To fully characterize and classify the generating polynomials, we use a module theoretic approach as done for instance in [35, 4] for matrix Pade approximants. For the classic facts in the ongoing presentation we refer to <ref> [16] </ref>. The set of the right generating vector polynomials for the sequence fH i g 1 i=0 is a K []-submodule W of K n []. We know that such a submodule W has a basis of at most n elements. <p> We have focused on finite fields, our results may be used for other computable fields. Appendix A Some facts from linear algebra The reader may refer to [14] or to <ref> [16] </ref> for the following matrix basic facts. Theorem 11.1.
Reference: 17. <author> T. Kailath, </author> <title> Linear systems, </title> <publisher> Prentice Hall, </publisher> <year> 1980. </year>
Reference-contexts: The Popov form is normal in the following sense: Theorem 2.3. <ref> [33, 17] </ref>. Any two right equivalent matrices in M n (K []) are right equivalent to a same unique matrix in Popov form. Here is an important classical fact that identifies column reduced forms and minimal bases. Theorem 2.4. [13]. <p> W have the same set of column degrees fd j g 1jn , they are called the Kronecker indices of W. 10 Gilles VILLARD If the indices are arranged in increasing order, identity (2.5) shows that the corresponding elements of W are the first linearly independent ones with minimal degrees <ref> [13, 17] </ref>. This motivates the following definition. Definition 2.5. The unique minimal basis D W () in Popov form is called the minimal (right) generating matrix polynomial for the matrix sequence fXA i Y g 1 i=0 . We may also look at this characterization column by column. <p> The non-unity invariant factors of A o and of D W () are the same. This is a classical result. The proof may be found in several papers. We refer to [7, 40] or to <ref> [17] </ref> and references therein. Given D Y (), next result states precisely how to choose X so that the generating polynomial remains unchanged. It is proven using also classical arguments from linear system theory (see [17] for instance). STUDY OF COPPERSMITH'S ALGORITHM USING MATRIX POLYNOMIALS 13 Lemma 4.2. <p> We refer to [7, 40] or to <ref> [17] </ref> and references therein. Given D Y (), next result states precisely how to choose X so that the generating polynomial remains unchanged. It is proven using also classical arguments from linear system theory (see [17] for instance). STUDY OF COPPERSMITH'S ALGORITHM USING MATRIX POLYNOMIALS 13 Lemma 4.2. Let D Y () and D W () be the minimal generating matrix polynomials of the sequences fA i Y g 1 i=0 and fXA i Y g 1 i=0 .
Reference: 18. <author> E. Kaltofen, </author> <title> Analysis of Coppersmith's block Wiedemann algorithm for the parallel solution of sparse linear systems, </title> <journal> Math. Comp. </journal> <volume> 64 (1995), no. 210, </volume> <pages> 777-806. </pages>
Reference-contexts: This should be viewed as a block version of the same algorithm. Blocks enable one to take advantage of simultaneous operations: either using the machine word over GF (2) [9] or a parallel machine <ref> [18] </ref>. Coppersmith's algorithm is very powerful [9, 19, 26] but raises theoretical questions. We are going to answer some of them in this paper. We refer to x2 for basic definitions and to x3.1 for a detailed presentation of the block algorithm. <p> In addition, he observed experimentally that it is sufficient to consider the first L = N=m + N=n + O (1) terms of the sequence fH i g i . The second result has been given by Kaltofen <ref> [18] </ref>. If A () denotes the minimal polynomial of A, it is possible to precondition A so that deg A () = rank A + 1. Then, if the field K has enough elements, the algorithm is guaranteed to compute a solution. <p> As presented in Coppersmith's pa per [9] or in the analysis proposed in <ref> [18] </ref>, we may also consider D () column by column. <p> Coppersmith's block Wiedemann algorithm Using above terminology, we now give the Coppersmith's version [9] of Wiede-mann's algorithm for the solution of Aw = 0. The matrix A is singular and square of dimension N over a field K. In x3.1, we follow the notations of Kaltofen <ref> [18] </ref> and his variant of the method. We will then briefly discuss in x3.2 of the computation of generating polynomials. 3.1. Coppersmith's algorithm for singular systems. <p> Using FFT-based polynomial multiplication, generating polynomials will be computed at deterministic cost O ((m+n) 2 m (N=n) log 2 (mN=n) log log (mN=n)) [38]. Another point of view may be found in [22]. In the current context, the reader will refer to <ref> [18] </ref> for a survey of the main probabilistic results and other references. Computing a generating polynomial can be done by finding a vector in the kernel of a block-Hankel matrix. <p> Using the Bitmead-Anderson/Morf method [5, 30], this can be done at probabilistic cost O ((m + n) 2 N log 2 N log log N ) <ref> [18] </ref>. 4. Characterization of good blocking matrices We have to characterize "good" matrices X and Y . This characterization is divided into two parts. <p> The assumptions of this lemma imply that the left 1 minimal generating polynomial for the sequence fXA i Y g 1 i=0 is of degree ffi l and that the right one is of degree ffi r . The main purpose of the analyses of the block algorithm <ref> [9, 18] </ref>, is precisely to show that these degrees are both small enough. 5. Separation and continuation We prove two additional facts that will be useful for the computation of minimal generating polynomials. <p> Generic degree profiles of minimal polynomials Given any matrix A, does the block algorithm work for any m and n? In Coppersmith's justification (x6 of [9]) one basic assumption is m n. In Kaltofen's analysis (x5 of <ref> [18] </ref>) there is no restriction on m and n but in return, there are restrictions on A. Indeed, let A fl denote a restriction of A to its range space. <p> Indeed, let A fl denote a restriction of A to its range space. If OE fl denotes the number of blocks of the Frobenius form of A fl (see theorem 11.1 in appendix A), then one must have OE fl = 1 <ref> [18] </ref>. In the following we will see that only m minfOE fl ; ng is required (see also proposition 8.2). In x9 we will see that this is not so restrictive. <p> This is the same concept than the generic rank profiles [10] that catches what are, in general, the ranks of leading principal submatrices of matrices equivalent to a given matrix A. We thus follow the technique from Kaltofen, Pan and Saunders <ref> [20, 21, 18] </ref>. <p> Note also that the entries of D Y are not, in general, over the ground field K but lie over K (AE 1;1 ; : : : ; AE N;n ). This is a main difference with the scalar case - e.g. see proposition 2 in <ref> [18] </ref>. Next example illustrates these remarks. Example 6.3. <p> s 1 (); : : : ; s minfOE;ng (), then the invariant factors of A &lt;Z&gt; are s 1 ()=; : : : ; s minfOE 0 ;ng ()=; s minfOE 0 ;ng+1 (); : : : ; s minfOE;ng (): This is a generalization of proposition 3 in <ref> [18] </ref> to any type of matrix. STUDY OF COPPERSMITH'S ALGORITHM USING MATRIX POLYNOMIALS 25 7. Following Wiedemann and Coppersmith's analyses We are going to rely on two main ingredients. The first one is a generalization of Wiedemann's work [39] in x8.1. <p> In the generic case - as characterized by proposition 6.1 the minimal generating polynomial has o = nd=ne columns of degree ffi r 1 and n o columns of degree ffi r . But, as implicitly noticed in <ref> [9, 18, 26] </ref>, to always have exactly the generic degrees seems unlikely. We will prove instead, that with a good probability, X and Y lead to degrees at most plus the generic ones, for a fixed positive integer. This will be done in x8.2 using certain results of Coppersmith [9]. <p> Clearly, all previously seen results with A; OE and , can be applied with A fl ; OE fl and fl . We do not know whether the block algorithm as stated in [9] and in <ref> [18] </ref> - produces a solution with good probability for any A; m and n. In the following, we will work under the assumption that m is at least greater than minfOE fl ; ng, fortunately this is not too restrictive in most cases. <p> Large fields Generalization of Kaltofen's analysis. For large fields, another technique can be used. We follow the work of <ref> [20, 21, 18] </ref>. These ideas have been successfully applied to singular matrices A whose minimal polynomial has degree deg A () = rank (A) + 1. Using the generalization of corollary 6.5, we get for any matrix: Theorem 9.6.
Reference: 19. <author> E. Kaltofen and A. Lobo, </author> <title> Distributed matrix-free solution of large sparse linear systems over finite fields, </title> <booktitle> High Performance Computing 1996, </booktitle> <address> San Diego, CA (A.M. </address> <institution> Tentner, ed.), Society for Computer Simulation, Simulation Councils, Inc, </institution> <year> 1996, </year> <pages> pp. 244-247. </pages>
Reference-contexts: One fundamental application of this problem is integer and polynomial factorization, where such linear systems arise with N over 200; 000 <ref> [23, 25, 19] </ref>. This has motivated several authors to develop fast finite-field counterpart to numerical iterative methods. The conjugate gradient method has been used in [23], the Lanczos method in [23, 12] and the block Lanczos method in [8, 29]. <p> This should be viewed as a block version of the same algorithm. Blocks enable one to take advantage of simultaneous operations: either using the machine word over GF (2) [9] or a parallel machine [18]. Coppersmith's algorithm is very powerful <ref> [9, 19, 26] </ref> but raises theoretical questions. We are going to answer some of them in this paper. We refer to x2 for basic definitions and to x3.1 for a detailed presentation of the block algorithm. We only consider the method intuitively in this introduction. <p> We refer to x8 and to x9 for a detailed study of the theoretical behaviour of the algorithm with respect to STUDY OF COPPERSMITH'S ALGORITHM USING MATRIX POLYNOMIALS 11 that key parameter. We also refer to the experiments reported in <ref> [26, 19] </ref>. Algorithm. Coppersmith's block Wiedemann. Input: A a N fi N matrix over K and the shift parameter , a non-negative integer. Step 1. Pick up random matrices X, Y . Let Z = AY: Step 2. Let ffi l = dN=me and ffi r = bN=nc. <p> Especially when m &gt;> OE fl : Corollary 9.3. If m 4OE fl , 8, V 4 then the probability of success is greater than 0:6. Remark 9.4. . It has been experimentally observed in <ref> [19, 26] </ref> that blocking may amplify the success probability. This can be easily explained by looking at the values taken by (9.1).
Reference: 20. <author> E. Kaltofen and V. Pan, </author> <title> Processor efficient parallel solution of linear systems over an abstract field, </title> <booktitle> Proc. 3rd Annual ACM Symposium on Parallel Algorithms and Architecture, </booktitle> <address> ACM-Press, </address> <year> 1991. </year>
Reference-contexts: This is the same concept than the generic rank profiles [10] that catches what are, in general, the ranks of leading principal submatrices of matrices equivalent to a given matrix A. We thus follow the technique from Kaltofen, Pan and Saunders <ref> [20, 21, 18] </ref>. <p> Large fields Generalization of Kaltofen's analysis. For large fields, another technique can be used. We follow the work of <ref> [20, 21, 18] </ref>. These ideas have been successfully applied to singular matrices A whose minimal polynomial has degree deg A () = rank (A) + 1. Using the generalization of corollary 6.5, we get for any matrix: Theorem 9.6.
Reference: 21. <author> E. Kaltofen and B.D. Saunders, </author> <title> On Wiedemann's method of solving sparse linear systems, </title> <booktitle> Proc. </booktitle> <address> AAECC-9, </address> <publisher> LNCS 539, Springer Verlag, </publisher> <year> 1991, </year> <pages> pp. 29-38. </pages>
Reference-contexts: This is the same concept than the generic rank profiles [10] that catches what are, in general, the ranks of leading principal submatrices of matrices equivalent to a given matrix A. We thus follow the technique from Kaltofen, Pan and Saunders <ref> [20, 21, 18] </ref>. <p> Large fields Generalization of Kaltofen's analysis. For large fields, another technique can be used. We follow the work of <ref> [20, 21, 18] </ref>. These ideas have been successfully applied to singular matrices A whose minimal polynomial has degree deg A () = rank (A) + 1. Using the generalization of corollary 6.5, we get for any matrix: Theorem 9.6.
Reference: 22. <author> S.Y. Kung, </author> <title> Multivariable and multidimensional systems: analysis and design, </title> <type> Ph.D. thesis, </type> <institution> Dpt. of Electrical Engineering, Stanford University, </institution> <month> June </month> <year> 1977. </year> <note> 38 Gilles VILLARD </note>
Reference-contexts: This approach enables one to use the superfast deterministic algorithm in [3]. Using FFT-based polynomial multiplication, generating polynomials will be computed at deterministic cost O ((m+n) 2 m (N=n) log 2 (mN=n) log log (mN=n)) [38]. Another point of view may be found in <ref> [22] </ref>. In the current context, the reader will refer to [18] for a survey of the main probabilistic results and other references. Computing a generating polynomial can be done by finding a vector in the kernel of a block-Hankel matrix.
Reference: 23. <author> B.A. LaMacchia and A.M. Odlyzko, </author> <title> Solving large sparse linear systems over finite fields, </title> <booktitle> Advances in Cryptology - CRYPTO'90, </booktitle> <publisher> Springer LNCS 537 (A.J. </publisher> <editor> Menezes and S.A. Vanstone, eds.), </editor> <year> 1991, </year> <pages> pp. 109-133. </pages>
Reference-contexts: One fundamental application of this problem is integer and polynomial factorization, where such linear systems arise with N over 200; 000 <ref> [23, 25, 19] </ref>. This has motivated several authors to develop fast finite-field counterpart to numerical iterative methods. The conjugate gradient method has been used in [23], the Lanczos method in [23, 12] and the block Lanczos method in [8, 29]. <p> One fundamental application of this problem is integer and polynomial factorization, where such linear systems arise with N over 200; 000 [23, 25, 19]. This has motivated several authors to develop fast finite-field counterpart to numerical iterative methods. The conjugate gradient method has been used in <ref> [23] </ref>, the Lanczos method in [23, 12] and the block Lanczos method in [8, 29]. But up to now, only the probabilistic analysis of Wiedemann [39] was giving a provably reliable and efficient method to solve Aw = 0 over small fields. <p> This has motivated several authors to develop fast finite-field counterpart to numerical iterative methods. The conjugate gradient method has been used in [23], the Lanczos method in <ref> [23, 12] </ref> and the block Lanczos method in [8, 29]. But up to now, only the probabilistic analysis of Wiedemann [39] was giving a provably reliable and efficient method to solve Aw = 0 over small fields.
Reference: 24. <author> R. Lambert, </author> <title> Computational aspects of discrete logarithms, </title> <type> Ph.D. thesis, </type> <institution> University of Waterloo, </institution> <address> Ontario, Canada, </address> <year> 1996. </year>
Reference-contexts: This method is based on finding relations in Krylov subspaces using the Berlekamp-Massey algorithm [28]. The same analysis could be applied to bound the probability of success of the (bi-orthogonal) Lanczos and conjugate gradient algorithms with look-ahead of Lambert <ref> [24] </ref>. Anyway, these various approaches are very similar: they can be understood in a unified theory [24]. But since they use generating polynomials of scalar sequences, these latter algorithms impose limitations if one wants to perform several operations at a time. <p> The same analysis could be applied to bound the probability of success of the (bi-orthogonal) Lanczos and conjugate gradient algorithms with look-ahead of Lambert <ref> [24] </ref>. Anyway, these various approaches are very similar: they can be understood in a unified theory [24]. But since they use generating polynomials of scalar sequences, these latter algorithms impose limitations if one wants to perform several operations at a time.
Reference: 25. <author> A.K. Lenstra, H.W. Lenstra, M.S. Manasse, and J.M. Pollard, </author> <title> The factorization of the ninth Fermat number, </title> <journal> Math. Comp. </journal> <volume> 61 (1993), </volume> <pages> 319-349. </pages>
Reference-contexts: One fundamental application of this problem is integer and polynomial factorization, where such linear systems arise with N over 200; 000 <ref> [23, 25, 19] </ref>. This has motivated several authors to develop fast finite-field counterpart to numerical iterative methods. The conjugate gradient method has been used in [23], the Lanczos method in [23, 12] and the block Lanczos method in [8, 29].
Reference: 26. <author> A. Lobo, </author> <title> Matrix-free linear system solving and applications to symbolic computation, </title> <type> Ph.D. thesis, </type> <institution> Dept. Comp. Sc., Rensselaer Polytech. </institution> <address> Instit., Troy, New York, </address> <month> Dec. </month> <year> 1995. </year>
Reference-contexts: This should be viewed as a block version of the same algorithm. Blocks enable one to take advantage of simultaneous operations: either using the machine word over GF (2) [9] or a parallel machine [18]. Coppersmith's algorithm is very powerful <ref> [9, 19, 26] </ref> but raises theoretical questions. We are going to answer some of them in this paper. We refer to x2 for basic definitions and to x3.1 for a detailed presentation of the block algorithm. We only consider the method intuitively in this introduction. <p> We refer to x8 and to x9 for a detailed study of the theoretical behaviour of the algorithm with respect to STUDY OF COPPERSMITH'S ALGORITHM USING MATRIX POLYNOMIALS 11 that key parameter. We also refer to the experiments reported in <ref> [26, 19] </ref>. Algorithm. Coppersmith's block Wiedemann. Input: A a N fi N matrix over K and the shift parameter , a non-negative integer. Step 1. Pick up random matrices X, Y . Let Z = AY: Step 2. Let ffi l = dN=me and ffi r = bN=nc. <p> In the generic case - as characterized by proposition 6.1 the minimal generating polynomial has o = nd=ne columns of degree ffi r 1 and n o columns of degree ffi r . But, as implicitly noticed in <ref> [9, 18, 26] </ref>, to always have exactly the generic degrees seems unlikely. We will prove instead, that with a good probability, X and Y lead to degrees at most plus the generic ones, for a fixed positive integer. This will be done in x8.2 using certain results of Coppersmith [9]. <p> Especially when m &gt;> OE fl : Corollary 9.3. If m 4OE fl , 8, V 4 then the probability of success is greater than 0:6. Remark 9.4. . It has been experimentally observed in <ref> [19, 26] </ref> that blocking may amplify the success probability. This can be easily explained by looking at the values taken by (9.1).
Reference: 27. <author> C.C. MacDuffee, </author> <title> The theory of matrices, </title> <publisher> Chelsea, </publisher> <address> New-York, </address> <year> 1956. </year>
Reference-contexts: This result remains valid in the case of matrix polynomials. A least common right multiple D () (lcrm) of two matrices P () and Q () is a common right multiple which is a left divisor of every common right multiple of P () and Q () <ref> [27] </ref>. In particular, P ()U () = Q ()V () = D () for some matrices U () and V (). <p> Everything can be done using row operations, we leave the reader to make the appropriate changes. STUDY OF COPPERSMITH'S ALGORITHM USING MATRIX POLYNOMIALS 17 a lcrm <ref> [27] </ref>. If P () and Q () have relatively prime determinants p () and q () then the determinant of a lcrm is p ()q (). Lemma 5.1. Let A be in M N (K) .
Reference: 28. <author> J.L. Massey, </author> <title> Shift-register synthesis and BCH decoding, </title> <journal> IEEE Trans. Inform. Theory 15 (1969), </journal> <pages> 122-127. </pages>
Reference-contexts: But up to now, only the probabilistic analysis of Wiedemann [39] was giving a provably reliable and efficient method to solve Aw = 0 over small fields. This method is based on finding relations in Krylov subspaces using the Berlekamp-Massey algorithm <ref> [28] </ref>. The same analysis could be applied to bound the probability of success of the (bi-orthogonal) Lanczos and conjugate gradient algorithms with look-ahead of Lambert [24]. Anyway, these various approaches are very similar: they can be understood in a unified theory [24]. <p> Indeed, it seems that for several known methods, one may compute one or several generating polynomials at the same asymptotical cost. Typically, Coppersmith has proposed a generalization of Berlekamp-Massey algorithm <ref> [28] </ref> to the matrix case. It can be seen that as precisely stated in the scalar case in [11] the algorithm is strongly related to Euclidean division [1, 6].
Reference: 29. <author> P.L. Montgomery, </author> <title> A block Lanczos algorithm for finding dependencies over GF(2), </title> <address> EURO-CRYPT'95, Heidelberg, Germany. </address> <publisher> Springer LNCS 921, </publisher> <year> 1995, </year> <pages> pp. 106-120. </pages>
Reference-contexts: This has motivated several authors to develop fast finite-field counterpart to numerical iterative methods. The conjugate gradient method has been used in [23], the Lanczos method in [23, 12] and the block Lanczos method in <ref> [8, 29] </ref>. But up to now, only the probabilistic analysis of Wiedemann [39] was giving a provably reliable and efficient method to solve Aw = 0 over small fields. This method is based on finding relations in Krylov subspaces using the Berlekamp-Massey algorithm [28].
Reference: 30. <author> M. Morf, </author> <title> Doubling algorithms for Toeplitz and related equations, </title> <booktitle> IEEE Internat. Conf. Acoust. Speech Signal Process., </booktitle> <address> Piscataway, NJ, </address> <year> 1980, </year> <pages> pp. 954-959. </pages>
Reference-contexts: Thus, step 3 of the block algorithm can be implemented as the computation of vectors in the kernel of M (ffi l + ; ffi r + 1) or of the associated block-Toeplitz matrix. Using the Bitmead-Anderson/Morf method <ref> [5, 30] </ref>, this can be done at probabilistic cost O ((m + n) 2 N log 2 N log log N ) [18]. 4. Characterization of good blocking matrices We have to characterize "good" matrices X and Y . This characterization is divided into two parts.
Reference: 31. <author> P. Ozello, </author> <title> Calcul exact des formes de Jordan et de Frobenius d'une matrice, </title> <type> Ph.D. thesis, </type> <institution> Universite Scientifique et Medicale de Grenoble, France, </institution> <year> 1987. </year>
Reference-contexts: The transformation P may be chosen such that A Y is shift-Hessenberg [2] or polycyclic form <ref> [31] </ref>. This form is 34 Gilles VILLARD block-triangular: A Y = 6 6 6 6 C 1 T 1;2 T 1;3 : : : T 1;OE Y . . . . . . . . . . . . . . .
Reference: 32. <author> P. Penfield Jr., R. Spencer, and S. Duinker, </author> <title> Tellegen's theorem and electrical networks, </title> <publisher> M.I.T. Press, </publisher> <address> Cambridge, MA, </address> <year> 1970. </year>
Reference-contexts: Indeed, in that case, on can find w t such that w t A t = 0 using a left version STUDY OF COPPERSMITH'S ALGORITHM USING MATRIX POLYNOMIALS 31 of the block algorithm. From a theoretical point of view, on may use Tellegen's theorem <ref> [32] </ref> which states that an algorithm computing A times a vector can be converted to one for A t times a vector. However, in the general case we have to make a slight additional restriction especially if the ground field is GF (2).
Reference: 33. <author> V.M. Popov, </author> <title> Invariant description of linear, time-invariant controllable systems, </title> <journal> SIAM J. Control 10 (1972), </journal> <pages> 252-264. </pages>
Reference-contexts: The coefficient vector of d j is the j-th leading column coefficient vector. We let [D ()] c be the matrix of these leading vectors. Definition 2.2. <ref> [33] </ref>. <p> The Popov form is normal in the following sense: Theorem 2.3. <ref> [33, 17] </ref>. Any two right equivalent matrices in M n (K []) are right equivalent to a same unique matrix in Popov form. Here is an important classical fact that identifies column reduced forms and minimal bases. Theorem 2.4. [13].
Reference: 34. <author> J.T. Schwartz, </author> <title> Fast probabilistic algorithms for verification of polynomial identities, </title> <editor> J. </editor> <booktitle> ACM 27 (1980), </booktitle> <pages> 701-717. </pages>
Reference-contexts: By corollary 1 of Schwartz <ref> [34] </ref> we know that the probability of hitting a zero is less than deg (D fl )=jKj 2 fl =jKj 2N=jKj: Summing this with the probability that w = 0 at most 1=V as seen at theorem 9.1 we conclude the proof.
Reference: 35. <author> M. Van Barel and A. Bultheel, </author> <title> A general module theoretic framework for vector M-Pade and matrix rational interpolation, </title> <booktitle> Numerical Algorithms 3 (1992), </booktitle> <pages> 451-462. </pages>
Reference-contexts: STUDY OF COPPERSMITH'S ALGORITHM USING MATRIX POLYNOMIALS 9 To fully characterize and classify the generating polynomials, we use a module theoretic approach as done for instance in <ref> [35, 4] </ref> for matrix Pade approximants. For the classic facts in the ongoing presentation we refer to [16]. The set of the right generating vector polynomials for the sequence fH i g 1 i=0 is a K []-submodule W of K n [].
Reference: 36. <author> G.C. Verghese and Kailath, </author> <title> Rational matrix structure, </title> <journal> IEEE Trans. Automat. Control 26 (1981), </journal> <pages> 434-438. </pages>
Reference-contexts: Minimum generating polynomials. From a complexity point of view it is important to handle relations (2.3) or (2.4) of minimal length. We will define minimal bases for W, these will correspond to minimal bases of vector spaces <ref> [13, 36] </ref>. In addition, to extend the notion of minimal scalar polynomial, we will speak (by abuse of language) of minimal generating matrix polynomial. A basis given by the columns of a matrix D () will be minimal (see theorem 2.4 below) when D () will be column reduced.
Reference: 37. <author> G. Villard, </author> <title> Computing Popov and Hermite forms of polynomial matrices, </title> <booktitle> International Symposium on Symbolic and Algebraic Computation, </booktitle> <address> Zurich, Suisse, </address> <publisher> ACM Press, </publisher> <month> July </month> <year> 1996, </year> <pages> pp. 250-258. </pages> <month> 38. </month> , <title> Computing minimum generating matrix polynomials, 1997, </title> <type> Preprint IMAG Grenoble, </type> <institution> France. </institution>
Reference-contexts: All the bases arranged as columns in a matrix of W differ by a right uni-modular multiplier. Thus the set of the right generating matrix polynomials of a sequence (2.2) can be uniquely determined by choosing a particular representative. As emphasized in <ref> [6, 37] </ref> several matrix polynomial normal form can be chosen. In next paragraph we focus on the Popov form which provides a notion of minimal polynomial. 2.2. Minimum generating polynomials. From a complexity point of view it is important to handle relations (2.3) or (2.4) of minimal length.
Reference: 39. <author> D. </author> <title> Wiedemann, Solving sparse linear equations over finite fields, </title> <journal> IEEE Transf. Inform. Theory 32 (1986), </journal> <pages> 54-62. </pages>
Reference-contexts: This has motivated several authors to develop fast finite-field counterpart to numerical iterative methods. The conjugate gradient method has been used in [23], the Lanczos method in [23, 12] and the block Lanczos method in [8, 29]. But up to now, only the probabilistic analysis of Wiedemann <ref> [39] </ref> was giving a provably reliable and efficient method to solve Aw = 0 over small fields. This method is based on finding relations in Krylov subspaces using the Berlekamp-Massey algorithm [28]. <p> We are going to answer some of them in this paper. We refer to x2 for basic definitions and to x3.1 for a detailed presentation of the block algorithm. We only consider the method intuitively in this introduction. In the Wiedemann algorithm <ref> [39] </ref>, one chooses at random a row vector x and a column vector y and one computes the lowest degree polynomial g () = g 0 + g 1 + g d d in K [] that linearly generates the sequence h i = xA i y, 0 i 2N 1. <p> Wiedemann in xVI of <ref> [39] </ref>, computes the probability that sequences fxA i yg i0 and fA i yg i0 have the same generating polynomials. This will be extended to the matrix case at x8.1. The concept of pathological matrix has no sense at this stage. <p> On the other hand, in the case of large fields, we will see that the preconditioning required by Kaltofen is not necessary, the algorithm computes a solution for any input matrix. This will result in a better probability bound. By analogy with the Wiedemann's deterministic algorithm <ref> [39] </ref>, we will conclude the paper with a modification of the above block version. Using the material of previous sections we will briefly sketch a deterministic block algorithm for computing matrix generating polynomials. The paper is organized as follows. <p> For that reason, corollary 5.2 will be useful to generalize Wiedemann's analysis in x8.1. Contrary to that, it seems difficult to use the same stategy with Coppersmith's analysis, which heavily depends on the actual degrees of the matrices. 5.2. Continuation. By analogy with Wiedemann's deterministic algorithm <ref> [39] </ref>, the variant of the block algorithm of Coppersmith we will propose in x10, will be based on Lemma 5.4. Let A be in M N (K) and Y in M N;n (K). <p> STUDY OF COPPERSMITH'S ALGORITHM USING MATRIX POLYNOMIALS 25 7. Following Wiedemann and Coppersmith's analyses We are going to rely on two main ingredients. The first one is a generalization of Wiedemann's work <ref> [39] </ref> in x8.1. We bound the probability of picking two matrices X and Y such that D Y () = D W () (following the notations of lemma 4.2). This equality only gives an incomplete answer with respect to Coppersmith's algorithm. <p> The quantity that should be considered then is the determinantal degree of D Y (). To establish his results, Wiedemann has proven the following fact: Proposition 7.1. <ref> [39] </ref>. Let x and y be a row and a column vector over K. Let xy () and y () be the minimal generating polynomials for fxA i yg i and for fA i yg i . <p> Lower bounds for that probability are given in <ref> [39] </ref> and in [15]. To apply lemma 4.2, in x8.1 below we propose a way of bounding the probability in the matrix case. In particular, this will result in a new proof of Wiedemann's bounds in the scalar case. <p> Note that for n = 1 our study reduces exactly to the analysis of Wiedemann <ref> [39] </ref>. Now, applying the lemma on the left: Proposition 8.2. Let A be a N fiN matrix over K =GF (q), let X and Y be chosen at random with m rows and n columns. <p> This can be easily explained by looking at the values taken by (9.1). Intuitively, the more one uses blocking vectors the more a block-Krylov subspace of dimension fl is easy to get (see also Wiedemann about the success of his algorithm 1 in <ref> [39] </ref>). If, for instance, OE fl = OE 0 is a constant. <p> Indeed, using lemma 5.4 it is not difficult to give an adaptation, in the matrix case, of Wiede-mann's deterministic algorithm (see xII of <ref> [39] </ref>). Again, since this is not the purpose of this paper to study the way minimal generating polynomials for a sequence are computed, the corresponding part of the algorithm is not given. <p> Thus: fi n (f; OE) 1 + i k=nOE We now bound the logarithm of above double product: log @ i k=nOE 1 X 1 X q kd i :(11.3) We may proceed using the computations done in <ref> [39] </ref> for the Euler's Phi function for polynomials over finite fields. We first address the case n OE 2. <p> Using the claim for n OE = 2 we get: i k=1 i ! and using proposition 3 in <ref> [39] </ref>: X q d i &lt; 1 + log log q N; thus, X 1 X q kd i 2 + 1=q + log log q N: Using (11.3) and (11.2) we finally obtain n OE = 1 : fi n (f; OE) 1 + exp (2 + 1=q) log q
Reference: 40. <author> H.K. Wimmer, </author> <title> A Jordan factorization for polynomial matrices, </title> <journal> Proceedings of the American Math. Soc. </journal> <volume> 75 (1979), no. 2, </volume> <pages> 201-206. </pages>
Reference-contexts: The non-unity invariant factors of A o and of D W () are the same. This is a classical result. The proof may be found in several papers. We refer to <ref> [7, 40] </ref> or to [17] and references therein. Given D Y (), next result states precisely how to choose X so that the generating polynomial remains unchanged. It is proven using also classical arguments from linear system theory (see [17] for instance).

References-found: 37

