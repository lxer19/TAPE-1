URL: http://www.cs.utexas.edu/users/plapack/papers/IPPS98.ps.Z
Refering-URL: http://www.cs.utexas.edu/users/rvdg/conference.html
Root-URL: 
Email: fgunnels,lin,morrow,rvdgg@cs.utexas.edu  
Title: Analysis of a Class of Parallel Matrix Multiplication Algorithms  
Author: John Gunnels Calvin Lin Greg Morrow Robert van de Geijn 
Date: 98  
Note: A Technical Paper Submitted to IPPS  
Address: Austin, TX 78712  
Affiliation: Department of Computer Sciences The University of Texas at Austin  
Abstract: Publications concerning parallel implementation of matrix-matrix multiplication continue to appear with some regularity. It may seem odd that an algorithm that can be expressed as one statement and three nested loops deserves this much attention. This paper provides some insights as to why this problem is complex: Practical algorithms that use matrix multiplication tend to use different shaped matrices, and the shape of the matrices can significantly impact the performance of matrix multiplication. We provide theoretical analysis and experimental results to explain the differences in performance achieved when these algorithms are applied to differently shaped matrices. This analysis sets the stage for hybrid algorithms which choose between the algorithms based on the shapes of the matrices involved. While the paper resolves a number of issues, it concludes with discussion of a number of directions yet to be pursued. fl Corresponding Author, Phone: (512) 471-9720, Fax: (512) 471-8885. y Regarding the length of this paper: There are a large number of tables and figures within the text, in addition to an extensive bibliography. Taking this into account, the length is within submission guidelines. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Agarwal, R. C., F. Gustavson, and M. Zubair, </author> <title> "A high-performance matrix multiplication algorithm on a distributed memory parallel computer using overlapped communication," </title> <journal> IBM Journal of Research and Development, </journal> <volume> Volume 38, Number 6, </volume> <year> 1994. </year>
Reference-contexts: An alternative generalization of this algorithm [19, 20] and an interesting algorithm based on a three-dimensional data distribution have also been developed [2]. The approach now considered the most practical, sometimes known as broadcast-broadcast, was first proposed by Agarwal et al. <ref> [1] </ref>, who showed that a sequence of parallel rank-k (panel-panel) updates is a highly effective way to parallelize C = AB. That same observation was independently made by van de Geijn et al. [24], who introduced the Scalable Universal Matrix Multiplication Algorithm (SUMMA).
Reference: [2] <author> Agarwal, R. C., S. M. Balle, F. G. Gustavson, M. Joshi, and P. Palkar, </author> <title> "A 3-Dimensional Approach to Parallel Matrix Multiplication," </title> <journal> IBM Journal of Research and Development, </journal> <volume> Volume 39, Number 5, </volume> <pages> pp. 1-8, </pages> <month> Sept. </month> <year> 1995. </year>
Reference-contexts: This last algorithm is a generalization of broadcast-multiply-roll to non-square meshes of processors. An alternative generalization of this algorithm [19, 20] and an interesting algorithm based on a three-dimensional data distribution have also been developed <ref> [2] </ref>. The approach now considered the most practical, sometimes known as broadcast-broadcast, was first proposed by Agarwal et al. [1], who showed that a sequence of parallel rank-k (panel-panel) updates is a highly effective way to parallelize C = AB.
Reference: [3] <author> Alpatov, P., G. Baker, C. Edwards, J. Gunnels, G. Morrow, J. Overfelt, Robert van de Geijn, and J. Wu, "PLAPACK: </author> <title> Parallel Linear Algebra Package Design Overview", Proceedings of SC97, </title> <note> to appear. </note>
Reference-contexts: Through empirical data they show some advantage when meshes are non-square or odd shaped. However, the algorithms being considered are not inherently more adapted to specific shapes of matrices, and thus limited benefit is observed. In earlier work <ref> [23, 4, 3] </ref> we observe the need for algorithms to be sensitive to the shape of the matrices, and we hint at the fact that a class of algorithms naturally supported by the Parallel Linear Algebra Package (PLAPACK) infrastructure provides a convenient set of algorithms to serve as the basis for
Reference: [4] <author> Alpatov, P., G. Baker, C. Edwards, J. Gunnels, G. Morrow, J. Overfelt, Robert van de Geijn, and J. Wu, "PLAPACK: </author> <title> Parallel Linear Algebra Package," </title> <booktitle> Proceedings of the SIAM Parallel Processing Conference, </booktitle> <year> 1997. </year> <month> 22 </month>
Reference-contexts: Through empirical data they show some advantage when meshes are non-square or odd shaped. However, the algorithms being considered are not inherently more adapted to specific shapes of matrices, and thus limited benefit is observed. In earlier work <ref> [23, 4, 3] </ref> we observe the need for algorithms to be sensitive to the shape of the matrices, and we hint at the fact that a class of algorithms naturally supported by the Parallel Linear Algebra Package (PLAPACK) infrastructure provides a convenient set of algorithms to serve as the basis for
Reference: [5] <author> Anderson, E., Z. Bai, C. Bischof, J. Demmel, J. Dongarra, J. DuCroz, A. Greenbaum, S. Hammarling, A. McKenney, and D. Sorensen, </author> <title> "Lapack: A Portable Linear Algebra Library for High Performance Computers," </title> <booktitle> Proceedings of Supercomputing '90, </booktitle> <publisher> IEEE Press, </publisher> <year> 1990, </year> <pages> pp. 1-10. </pages>
Reference: [6] <author> Barnett, M., S. Gupta, D. Payne, L. Shuler, R. van de Geijn, and J. Watts, </author> <title> "Interproces-sor Collective Communication Library (InterCom)," </title> <booktitle> Scalable High Performance Computing Conference 1994. </booktitle>
Reference-contexts: However, when n is 3 large, better algorithms are available. Details on the algorithms and estimated costs can be found in the literature (e.g., <ref> [6, 15, 18] </ref>). 2.3 Model of Computational Cost All computation on the individual nodes will be implemented using calls to highly optimized sequential matrix-matrix multiply kernels that are part of the Level-3 Basic Linear Algebra Subprograms (BLAS) [12].
Reference: [7] <author> Cannon, L.E., </author> <title> A Cellular Computer to Implement the Kalman Filter Algorithm, </title> <type> Ph.D. Thesis (1969), </type> <institution> Montana State University. </institution>
Reference-contexts: 1 Introduction Over the last three decades, a number of different approaches have been proposed for implementation of matrix-matrix multiplication on distributed memory architectures. These include Cannon's algorithm <ref> [7] </ref>, the broadcast-multiply-roll algorithm [16, 15], and Parallel Universal Matrix Multiplication Algorithm (PUMMA) [11]. This last algorithm is a generalization of broadcast-multiply-roll to non-square meshes of processors. An alternative generalization of this algorithm [19, 20] and an interesting algorithm based on a three-dimensional data distribution have also been developed [2].
Reference: [8] <author> Chtchelkanova, A., J. Gunnels, G. Morrow, J. Overfelt, R. van de Geijn, </author> <title> "Parallel Implementation of BLAS: General Techniques for Level 3 BLAS," </title> <institution> TR-95-40, Department of Computer Sciences, University of Texas, </institution> <month> Oct. </month> <year> 1995. </year>
Reference-contexts: In addition to computing C = AB as a sequence of panel-panel updates, SUMMA implements C = AB T and C = A T B as a sequence of matrix-panel (of vectors) multiplications, and C = A T B T as a sequence of panel-panel updates. Later work <ref> [8] </ref> showed how these techniques can be extended to a large class of commonly used matrix-matrix operations that are part of the Level-3 Basic Linear Algebra Subprograms [12]. All of the efforts above-mentioned have concentrated primarily on the special case where the input matrices are approximately square. <p> The kernels can be separated into two groups. For the first, at least two dimensions are large, leading to operations involving matrices and panels of rows or columns: the panel-panel update, matrix-panel multiply, and panel-matrix multiply. Notice that these cases are similar to kernels used in our paper <ref> [8] </ref> for parallel implementation of matrix-matrix operations as well as basic building blocks used by ScaLAPACK 2 for parallel implementation of Basic Linear Algebra Subprograms. <p> We recognize that due to page limitations, the explanations of the algorithms are somewhat terse. We refer the reader to the literature <ref> [8, 23, 24] </ref> for additional details. 3.1 Notation For simplicity, we will assume that the dimensions m, n, and k are integer multiples of the algorithmic block size b alg .
Reference: [9] <author> Choi J., J. J. Dongarra, R. Pozo, and D. W. Walker, </author> <title> "Scalapack: A Scalable Linear Algebra Library for Distributed Memory Concurrent Computers," </title> <booktitle> Proceedings of the Fourth Symposium on the Frontiers of Massively Parallel Computation. </booktitle> <publisher> IEEE Comput. Soc. Press, </publisher> <year> 1992, </year> <pages> pp. 120-127. </pages>
Reference-contexts: Thus, a number of hybrid algorithms are now part of PLAPACK. This observation was also made as part of the ScaLAPACK <ref> [9] </ref> project, and indeed ScaLAPACK includes a number of matrix-matrix operations that are implemented to choose algorithms based on the shape of the matrix. We comment on the difference between our approach and ScaLAPACK later.
Reference: [10] <author> Choi, J., J. J. Dongarra, and D. W. Walker, </author> <title> "Level 3 BLAS for distributed memory concurrent computers", </title> <booktitle> CNRS-NSF Workshop on Environments and Tools for Parallel Scientific Computing, </booktitle> <address> Saint Hilaire du Touvet, France, </address> <month> Sept. </month> <pages> 7-8, </pages> <address> 1992. </address> <publisher> Elsevier Science Publishers, </publisher> <year> 1992. </year>
Reference: [11] <author> Choi, J., J. J. Dongarra, and D. W. Walker, "PUMMA: </author> <title> Parallel Universal Matrix Multiplication Algorithms on distributed memory concurrent computers," </title> <journal> Concurrency: Practice and Experience, </journal> <volume> Vol 6(7), </volume> <pages> 543-570, </pages> <year> 1994. </year>
Reference-contexts: 1 Introduction Over the last three decades, a number of different approaches have been proposed for implementation of matrix-matrix multiplication on distributed memory architectures. These include Cannon's algorithm [7], the broadcast-multiply-roll algorithm [16, 15], and Parallel Universal Matrix Multiplication Algorithm (PUMMA) <ref> [11] </ref>. This last algorithm is a generalization of broadcast-multiply-roll to non-square meshes of processors. An alternative generalization of this algorithm [19, 20] and an interesting algorithm based on a three-dimensional data distribution have also been developed [2].
Reference: [12] <author> Dongarra, J. J., J. Du Croz, S. Hammarling, and I. Duff, </author> <title> "A Set of Level 3 Basic Linear Algebra Subprograms," </title> <journal> TOMS, </journal> <volume> Vol. 16, No. 1, </volume> <pages> pp. 1-16, </pages> <year> 1990. </year>
Reference-contexts: Later work [8] showed how these techniques can be extended to a large class of commonly used matrix-matrix operations that are part of the Level-3 Basic Linear Algebra Subprograms <ref> [12] </ref>. All of the efforts above-mentioned have concentrated primarily on the special case where the input matrices are approximately square. More recently, work by Li, et al. [22] attempts to create a "poly-algorithm" that chooses between three of the above algorithms (Cannon's, broadcast-multiply-roll, and broadcast-broadcast). <p> on the algorithms and estimated costs can be found in the literature (e.g., [6, 15, 18]). 2.3 Model of Computational Cost All computation on the individual nodes will be implemented using calls to highly optimized sequential matrix-matrix multiply kernels that are part of the Level-3 Basic Linear Algebra Subprograms (BLAS) <ref> [12] </ref>. In our model, we will represent the cost of one add or one multiply by fl. For many vendors, the shape of the matrices involved in a sequential matrix-matrix multiply (dgemm call) influences the performance of this kernel.
Reference: [13] <author> Dongarra, J. J., R. A. van de Geijn, and D. W. Walker, </author> <title> "Scalability Issues Affecting the Design of a Dense Linear Algebra Library," </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> Vol. 22, No. 3, </volume> <month> Sept. </month> <year> 1994, </year> <pages> pp. 523-537. </pages>
Reference: [14] <author> C. Edwards, P. Geng, A. Patra, and R. van de Geijn, </author> <title> Parallel matrix distributions: have we been doing it all wrong?, </title> <type> Tech. Report TR-95-40, </type> <institution> Dept of Computer Sciences, The University of Texas at Austin, </institution> <year> 1995. </year>
Reference-contexts: reported in the last column come from recognizing that when "small" is greater than one but small, a matrix that has a row or column dimension of one becomes a row panel or column panel of vectors, respectively. 2.4 Data distribution 2.4.1 Physically Based Matrix Distribution We have previously observed <ref> [14] </ref> that data distributions should focus on the decomposition of the physical problem to be solved, rather than on the decomposition of matrices. Typically, it is the elements of vectors that are associated with data of physical significance, and it is therefore their distribution to nodes that is significant.
Reference: [15] <author> Fox, G. C., M. A. Johnson, G. A. Lyzenga, S. W. Otto, J. K. Salmon, and D. W. Walker, </author> <title> Solving Problems on Concurrent Processors, </title> <journal> Vol. </journal> <volume> 1, </volume> <publisher> Prentice Hall, </publisher> <address> Englewood Cliffs, N.J., </address> <year> 1988. </year>
Reference-contexts: 1 Introduction Over the last three decades, a number of different approaches have been proposed for implementation of matrix-matrix multiplication on distributed memory architectures. These include Cannon's algorithm [7], the broadcast-multiply-roll algorithm <ref> [16, 15] </ref>, and Parallel Universal Matrix Multiplication Algorithm (PUMMA) [11]. This last algorithm is a generalization of broadcast-multiply-roll to non-square meshes of processors. An alternative generalization of this algorithm [19, 20] and an interesting algorithm based on a three-dimensional data distribution have also been developed [2]. <p> However, when n is 3 large, better algorithms are available. Details on the algorithms and estimated costs can be found in the literature (e.g., <ref> [6, 15, 18] </ref>). 2.3 Model of Computational Cost All computation on the individual nodes will be implemented using calls to highly optimized sequential matrix-matrix multiply kernels that are part of the Level-3 Basic Linear Algebra Subprograms (BLAS) [12].
Reference: [16] <author> Fox, G., S. Otto, and A. Hey, </author> <title> "Matrix algorithms on a hypercube I: matrix multiplication," </title> <booktitle> Parallel Computing 3 (1987), </booktitle> <pages> pp 17-31. 23 </pages>
Reference-contexts: 1 Introduction Over the last three decades, a number of different approaches have been proposed for implementation of matrix-matrix multiplication on distributed memory architectures. These include Cannon's algorithm [7], the broadcast-multiply-roll algorithm <ref> [16, 15] </ref>, and Parallel Universal Matrix Multiplication Algorithm (PUMMA) [11]. This last algorithm is a generalization of broadcast-multiply-roll to non-square meshes of processors. An alternative generalization of this algorithm [19, 20] and an interesting algorithm based on a three-dimensional data distribution have also been developed [2].
Reference: [17] <author> Gropp, W., E. Lusk, A. Skjellum, </author> <title> Using MPI: Portable Programming with the Message-Passing Interface, </title> <publisher> The MIT Press, </publisher> <year> 1994. </year>
Reference: [18] <author> C.-T. Ho and S. L. Johnsson, </author> <title> Distributed Routing Algorithms for Broadcasting and Personalized Communication in Hypercubes, </title> <booktitle> In Proceedings of the 1986 International Conference on Parallel Processing, </booktitle> <pages> pages 640-648, </pages> <publisher> IEEE, </publisher> <year> 1986. </year>
Reference-contexts: However, when n is 3 large, better algorithms are available. Details on the algorithms and estimated costs can be found in the literature (e.g., <ref> [6, 15, 18] </ref>). 2.3 Model of Computational Cost All computation on the individual nodes will be implemented using calls to highly optimized sequential matrix-matrix multiply kernels that are part of the Level-3 Basic Linear Algebra Subprograms (BLAS) [12].
Reference: [19] <author> Huss-Lederman, S., E. Jacobson, A. Tsao, </author> <title> "Comparison of Scalable Parallel Matrix Multiplication Libraries," </title> <booktitle> in Proceedings of the Scalable Parallel Libraries Conference, </booktitle> <address> Starksville, MS, </address> <month> Oct. </month> <year> 1993. </year>
Reference-contexts: These include Cannon's algorithm [7], the broadcast-multiply-roll algorithm [16, 15], and Parallel Universal Matrix Multiplication Algorithm (PUMMA) [11]. This last algorithm is a generalization of broadcast-multiply-roll to non-square meshes of processors. An alternative generalization of this algorithm <ref> [19, 20] </ref> and an interesting algorithm based on a three-dimensional data distribution have also been developed [2].
Reference: [20] <author> Huss-Lederman, S., E. Jacobson, A. Tsao, G. Zhang, </author> <title> "Matrix Multiplication on the Intel Touchstone DELTA," </title> <journal> Concurrency: Practice and Experience, </journal> <volume> Vol. 6 (7), </volume> <month> Oct. </month> <year> 1994, </year> <pages> pp. 571-594. </pages>
Reference-contexts: These include Cannon's algorithm [7], the broadcast-multiply-roll algorithm [16, 15], and Parallel Universal Matrix Multiplication Algorithm (PUMMA) [11]. This last algorithm is a generalization of broadcast-multiply-roll to non-square meshes of processors. An alternative generalization of this algorithm <ref> [19, 20] </ref> and an interesting algorithm based on a three-dimensional data distribution have also been developed [2].
Reference: [21] <author> Lin, C., and L. Snyder, </author> <title> "A Matrix Product Algorithm and its Comparative Performance on Hypercubes," </title> <booktitle> in Proceedings of Scalable High Performance Computing Conference, </booktitle> <editor> (Stout, Q, and M. Wolfe, eds.), </editor> <publisher> IEEE Press, Los Alamitos, </publisher> <address> CA, </address> <year> 1992, </year> <pages> pp. 190-3. </pages>
Reference: [22] <author> Li, J., A. Skjellum, and R. D. Falgout, </author> <title> "A Poly-Algorithm for Parallel Dense Matrix Multiplication on Two-Dimensional Process Grid Topologies," </title> <journal> Concurrency, Practice and Experience, </journal> <volume> VOL. 10, </volume> ??? <editor> [23] van de Geijn, R., </editor> <title> Using PLAPACK: Parallel Linear Algebra Package, </title> <publisher> The MIT Press, </publisher> <year> 1997. </year> <title> [24] van de Geijn, </title> <editor> R. and J. Watts, "SUMMA: </editor> <title> Scalable Universal Matrix Multiplication Algorithm," </title> <journal> Concurrency: Practice and Experience, </journal> <volume> VOL. 9(4), </volume> <month> 255-274 (April </month> <year> 1997). </year> <month> 24 </month>
Reference-contexts: All of the efforts above-mentioned have concentrated primarily on the special case where the input matrices are approximately square. More recently, work by Li, et al. <ref> [22] </ref> attempts to create a "poly-algorithm" that chooses between three of the above algorithms (Cannon's, broadcast-multiply-roll, and broadcast-broadcast). Through empirical data they show some advantage when meshes are non-square or odd shaped.
References-found: 22

