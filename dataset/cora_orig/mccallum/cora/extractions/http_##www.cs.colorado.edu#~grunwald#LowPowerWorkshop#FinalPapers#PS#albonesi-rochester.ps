URL: http://www.cs.colorado.edu/~grunwald/LowPowerWorkshop/FinalPapers/PS/albonesi-rochester.ps
Refering-URL: http://www.cs.colorado.edu/~grunwald/LowPowerWorkshop/agenda.html
Root-URL: http://www.cs.colorado.edu
Email: albonesi@ece.rochester.edu  
Title: The Inherent Energy Efficiency of Complexity-Adaptive Processors  
Author: David H. Albonesi 
Address: Rochester, NY 14627-0231  
Affiliation: Dept. of Electrical and Computer Engineering University of Rochester  
Abstract: Conventional microprocessor designs that statically set the functionality of the chip at design time may waste considerable energy when running applications whose requirements are poorly matched to the particular hardware organization chosen. This paper describes how Complexity-Adaptive Processors, which employ dynamic structures whose functionality can be modified at runtime, expend less energy as a byproduct of the way in which they optimize performance. Because CAPs attempt to efficiently utilize hardware resources to maximize performance, this improved resource usage results in energy efficiency as well. CAPs exploit repeater methodologies used increasingly in deep submicron designs to achieve these benefits with little or no speed degradation relative to a conventional static design. By tracking hardware activity via performance simulation, we demonstrate that CAPs reduce total expended energy by 23% and 50% for cache hierarchies and instruction queues, respectively, while outperforming conventional designs. The additive effect observed for several applications indicates that a much greater impact can be realized by applying the CAPs approach in concert to a number of hardware structures. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> D.H. Albonesi. </author> <title> Dynamic IPC/clock rate optimization. </title> <booktitle> Proceedings of the 25th International Symposium on Computer Architecture, </booktitle> <month> June </month> <year> 1998. </year>
Reference-contexts: Other major hardware structures, such as instruction issue queues, similarly waste energy while operating on a diverse workload. Complexity-Adaptive Processors (CAPs) make more efficient use of chip resources than conventional approaches by tailoring the complexity and clock speed of the chip to the requirements of each individual application. In <ref> [1] </ref>, we show how CAPs can achieve this flexibility without clock speed degradation compared to a conventional approach, and thus achieve significantly greater performance. In this paper, we describe how CAPs can achieve this performance gain while expending considerably less energy than a conventional microprocessor. <p> It is this inherent energy efficiency of CAPs that follows naturally from optimizing performance that we explore in the rest of this paper. 5 Experimental Methodology We examined the results of our preliminary performance analysis <ref> [1] </ref> of two-level on-chip Dcache hierarchies and instruction issue queues to estimate the relative energy expended by CAP and conventional approaches for these structures. <p> Unused entries were disabled. The best-performing conventional design contained 64 entries. We used the SimpleScalar simulator [2] and ran each benchmark for 100 million instructions. More details on the evaluation methodology and benchmarks used can be found in <ref> [1] </ref>. To estimate relative expended energy, we calculate the activity ratio for each approach by tracking the number and types of operations, and estimating the activity generated by each. <p> For benchmarks such as stereo and appcg whose requirements are not well-matched to the conventional organization, the energy savings with the CAP approach are significant: 44% for stereo and 62% for ap-pcg. Because these benchmarks run significantly faster on the CAPs configuration as well <ref> [1] </ref>, the reduction in the energy-delay product, an indicator of the efficiency with which a particular performance level is obtained, is even more striking: 70% for both benchmarks. Overall, 23% less energy is expended by the CAPs configuration for these benchmarks as a byproduct of performance optimization.
Reference: [2] <author> D. </author> <title> Burger and T.M. Austin. The simplescalar toolset, version 2.0. </title> <type> Technical Report TR-97-1342, </type> <institution> University of Wisconsin-Madison, </institution> <month> June </month> <year> 1997. </year>
Reference-contexts: We ran each benchmark on our cache simulator for 100 million memory references. The CAP instruction queue varied in size from 16 to 128 entries in steps of 16 entries. Unused entries were disabled. The best-performing conventional design contained 64 entries. We used the SimpleScalar simulator <ref> [2] </ref> and ran each benchmark for 100 million instructions. More details on the evaluation methodology and benchmarks used can be found in [1].
Reference: [3] <author> J. </author> <title> Fleischman. </title> <type> Private communication. </type> <month> November </month> <year> 1997. </year>
Reference-contexts: Note that wire buffers are used not only in busses between major functional blocks, but within self-contained hardware structures as well. The forthcoming HP PA-8500 microprocessor, which is also implemented in 0.25 micron CMOS, uses wire buffers for the global address and data busses of its on-chip caches <ref> [3] </ref>. As feature sizes decrease to 0.18 micron and below, other smaller structures will require the use of configured with four to eight elements. wire buffers in order to meet timing requirements [4].
Reference: [4] <author> D. Matzke. </author> <title> Will physical scalability sabotage performance gains? IEEE Computer, </title> <address> 30(9):3739, </address> <month> September </month> <year> 1997. </year>
Reference-contexts: As feature sizes decrease to 0.18 micron and below, other smaller structures will require the use of configured with four to eight elements. wire buffers in order to meet timing requirements <ref> [4] </ref>. For these reasons, we expect that many of the major hardware structures of future high performance microprocessors, such as caches, TLBs, branch predictor tables, and instruction queues, will be of the form shown in Figure 1.
Reference: [5] <editor> K.B. Normoyle et al. UltraSPARC-IIi: </editor> <title> Expanding the boundaries of a system on a chip. </title> <journal> IEEE Micro, </journal> <volume> 18(2):1424, </volume> <month> March </month> <year> 1998. </year> <month> 6 </month>
Reference-contexts: For example, the Sun UltraSPARC-IIi microprocessor, implemented in a 0.25 micron CMOS process, contains over 1,200 buffers to improve wire delay <ref> [5] </ref>. Note that wire buffers are used not only in busses between major functional blocks, but within self-contained hardware structures as well. The forthcoming HP PA-8500 microprocessor, which is also implemented in 0.25 micron CMOS, uses wire buffers for the global address and data busses of its on-chip caches [3].
References-found: 5

