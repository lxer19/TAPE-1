URL: http://c.gp.cs.cmu.edu:5103/afs/cs.cmu.edu/academic/class/15840a-f94/schedule/mpi-report.ps.Z
Refering-URL: http://c.gp.cs.cmu.edu:5103/afs/cs.cmu.edu/academic/class/15840a-f94/schedule/Course_schedule.html
Root-URL: http://www.cs.cmu.edu
Title: MPI: A Message-Passing Interface Standard Message Passing Interface  
Note: This work was supported in part by ARPA and NSF under grant ASC-9310330, the National Science Foundation Science and Technology Center Cooperative Agreement No. CCR-8809615, and by the Commission of the European Community through Esprit project P6643(PPPE).  
Date: May 5, 1994  
Pubnum: Forum  
Abstract-found: 0
Intro-found: 1
Reference: [1] <author> V. Bala and S. Kipnis. </author> <title> Process groups: a mechanism for the coordination of and communication among processes in the Venus collective communication library. </title> <type> Technical report, </type> <institution> IBM T. J. Watson Research Center, </institution> <month> October </month> <year> 1992. </year> <type> Preprint. </type>
Reference-contexts: Thus, MPI has been strongly influenced by work at the IBM T. J. Watson Research Center <ref> [1, 2] </ref>, Intel's NX/2 [23], Express [22], nCUBE's Vertex [21], p4 [7, 6], and PARMACS [5, 8]. Other important contributions have come from Zipcode [24, 25], Chimp [14, 15], PVM [4, 11], Chameleon [19], and PICL [18]. <p> zero particles */ j = 0; if (particle [i].class==0) - zdisp [j] = i; zblock [j] = 1; j++; /* create datatype for class zero particles */ MPI_Type_indexed ( j, zblock, zdisp, Particletype, &Zparticles); /* prepend particle count */ MPI_Address (&j, zzdisp); MPI_Address (particle, zzdisp+1); zztype [0] = MPI_INT; zztype <ref> [1] </ref> = Zparticles; MPI_Type_struct (2, zzblock, zzdisp, zztype, &Ztype); MPI_Type_commit ( &Ztype); MPI_Send ( MPI_BOTTOM, 1, Ztype, dest, tag, comm); 1 3 5 7 9 11 13 15 17 19 21 23 25 27 29 31 33 35 37 39 41 43 45 47 80 CHAPTER 3. <p> j, zblock, zdisp, Particletype, &Zparticles); /* 4.3: send the first two coordinates of all entries */ MPI_Datatype Allpairs; /* datatype for all pairs of coordinates */ MPI_Aint sizeofentry; MPI_Type_extent ( Particletype, &sizeofentry); /* sizeofentry can also be computed by subtracting the address of particle [0] from the address of particle <ref> [1] </ref> */ MPI_Type_hvector ( 1000, 2, sizeofentry, MPI_DOUBLE, &Allpairs); MPI_Type_commit ( &Allpairs); MPI_Send ( particle [0].d, 1, Allpairs, dest, tag, comm); /* an alternative solution to 4.3 */ MPI_Datatype Onepair; /* datatype for one pair of coordinates, with the extent of one particle entry */ MPI_Aint disp2 [3]; MPI_Datatype type2 [3] <p> (particle [i].index==0) - for (k=i+1; (k &lt; 1000)&&(particle [k].index = 0) ; k++); zdisp [j] = i; zblock [j] = k-i; j++; - MPI_Type_indexed ( j, zblock, zdisp, Particletype, &Zparticles); /* Zparticles describe particles with class zero, using their absolute addresses*/ /* prepend particle count */ MPI_Address (&j, zzdisp); zzdisp <ref> [1] </ref> = MPI_BOTTOM; zztype [0] = MPI_INT; zztype [1] = Zparticles; MPI_Type_struct (2, zzblock, zzdisp, zztype, &Ztype); MPI_Type_commit ( &Ztype); MPI_Send ( MPI_BOTTOM, 1, Ztype, dest, tag, comm); Example 3.35 Handling of unions. union - int ival; float fval; - u [1000] int utype; /* All entries of u have identical <p> [k].index = 0) ; k++); zdisp [j] = i; zblock [j] = k-i; j++; - MPI_Type_indexed ( j, zblock, zdisp, Particletype, &Zparticles); /* Zparticles describe particles with class zero, using their absolute addresses*/ /* prepend particle count */ MPI_Address (&j, zzdisp); zzdisp <ref> [1] </ref> = MPI_BOTTOM; zztype [0] = MPI_INT; zztype [1] = Zparticles; MPI_Type_struct (2, zzblock, zzdisp, zztype, &Ztype); MPI_Type_commit ( &Ztype); MPI_Send ( MPI_BOTTOM, 1, Ztype, dest, tag, comm); Example 3.35 Handling of unions. union - int ival; float fval; - u [1000] int utype; /* All entries of u have identical type; variable utype keeps track of their current <p> PACK AND UNPACK 83 MPI_Address ( u, &i); MPI_Address ( u+1, &j); disp [0] = 0; disp <ref> [1] </ref> = j-i; type [1] = MPI_UB; type [0] = MPI_INT; MPI_Type_struct (2, blocklen, disp, type, &mpi_utype [0]); type [0] = MPI_FLOAT; MPI_Type_struct (2, blocklen, disp, type, &mpi_utype [1]); for (i=0; i&lt;2; i++) MPI_Type_commit (&mpi_utype [i]); /* actual communication */ MPI_Send (u, 1000, mpi_utype [utype], dest, tag, comm); 3.13 Pack and <p> PACK AND UNPACK 83 MPI_Address ( u, &i); MPI_Address ( u+1, &j); disp [0] = 0; disp <ref> [1] </ref> = j-i; type [1] = MPI_UB; type [0] = MPI_INT; MPI_Type_struct (2, blocklen, disp, type, &mpi_utype [0]); type [0] = MPI_FLOAT; MPI_Type_struct (2, blocklen, disp, type, &mpi_utype [1]); for (i=0; i&lt;2; i++) MPI_Type_commit (&mpi_utype [i]); /* actual communication */ MPI_Send (u, 1000, mpi_utype [utype], dest, tag, comm); 3.13 Pack and unpack Some existing communication <p> PACK AND UNPACK 83 MPI_Address ( u, &i); MPI_Address ( u+1, &j); disp [0] = 0; disp <ref> [1] </ref> = j-i; type [1] = MPI_UB; type [0] = MPI_INT; MPI_Type_struct (2, blocklen, disp, type, &mpi_utype [0]); type [0] = MPI_FLOAT; MPI_Type_struct (2, blocklen, disp, type, &mpi_utype [1]); for (i=0; i&lt;2; i++) MPI_Type_commit (&mpi_utype [i]); /* actual communication */ MPI_Send (u, 1000, mpi_utype [utype], dest, tag, comm); 3.13 Pack and unpack Some existing communication libraries provide pack/unpack functions for sending noncontiguous data. <p> (MPI_Comm_world, &myrank); if (myrank == 0) - / * SENDER CODE */ int len [2]; MPI_Aint disp [2]; MPI_Datatype type [2], newtype; /* build datatype for i followed by a [0]...a [i-1] */ len [0] = 1; MPI_Address ( &i, disp); MPI_Address ( a, disp+1); type [0] = MPI_INT; type <ref> [1] </ref> = MPI_FLOAT; MPI_Type_struct ( 2, len, disp, type, &newtype); MPI_Type_commit ( &newtype); /* Pack i followed by a [0]...a [i-1]*/ position = 0; MPI_Pack ( MPI_BOTTOM, 1, newtype, buff, 1000, &position, MPI_COMM_WORLD); /* Send */ MPI_Send ( buff, position, MPI_PACKED, 1, 0, MPI_COMM_WORLD) /* ***** One can replace the last <p> ); rbuf = (int *)malloc (gsize*stride*sizeof (int)); displs = (int *)malloc (gsize*sizeof (int)); rcounts = (int *)malloc (gsize*sizeof (int)); for (i=0; i&lt;gsize; ++i) - displs [i] = i*stride; rcounts [i] = 100-i; - /* Create datatype for one int, with extent of entire row */ disp [0] = 0; disp <ref> [1] </ref> = 150*sizeof (int); type [0] = MPI_INT; type [1] = MPI_UB; blocklen [0] = 1; blocklen [1] = 1; MPI_Type_struct ( 2, blocklen, disp, type, &stype ); MPI_Type_commit ( &stype ); sptr = &sendarray [0][myrank]; MPI_Gatherv ( sptr, 100-myrank, stype, rbuf, rcounts, displs, MPI_INT, root, comm); Example 4.9 Same as <p> (int *)malloc (gsize*sizeof (int)); rcounts = (int *)malloc (gsize*sizeof (int)); for (i=0; i&lt;gsize; ++i) - displs [i] = i*stride; rcounts [i] = 100-i; - /* Create datatype for one int, with extent of entire row */ disp [0] = 0; disp <ref> [1] </ref> = 150*sizeof (int); type [0] = MPI_INT; type [1] = MPI_UB; blocklen [0] = 1; blocklen [1] = 1; MPI_Type_struct ( 2, blocklen, disp, type, &stype ); MPI_Type_commit ( &stype ); sptr = &sendarray [0][myrank]; MPI_Gatherv ( sptr, 100-myrank, stype, rbuf, rcounts, displs, MPI_INT, root, comm); Example 4.9 Same as Example 4.7 at sending side, but at receiving side <p> (gsize*sizeof (int)); for (i=0; i&lt;gsize; ++i) - displs [i] = i*stride; rcounts [i] = 100-i; - /* Create datatype for one int, with extent of entire row */ disp [0] = 0; disp <ref> [1] </ref> = 150*sizeof (int); type [0] = MPI_INT; type [1] = MPI_UB; blocklen [0] = 1; blocklen [1] = 1; MPI_Type_struct ( 2, blocklen, disp, type, &stype ); MPI_Type_commit ( &stype ); sptr = &sendarray [0][myrank]; MPI_Gatherv ( sptr, 100-myrank, stype, rbuf, rcounts, displs, MPI_INT, root, comm); Example 4.9 Same as Example 4.7 at sending side, but at receiving side we make the stride between received blocks vary <p> (gsize*sizeof (int)); displs [0] = 0; for (i=1; i&lt;gsize; ++i) - displs [i] = displs [i-1]+rcounts [i-1]; - /* And, create receive buffer */ rbuf = (int *)malloc (gsize*(displs [gsize-1]+rcounts [gsize-1]) *sizeof (int)); /* Create datatype for one int, with extent of entire row */ disp [0] = 0; disp <ref> [1] </ref> = 150*sizeof (int); type [0] = MPI_INT; type [1] = MPI_UB; blocklen [0] = 1; blocklen [1] = 1; MPI_Type_struct ( 2, blocklen, disp, type, &stype ); MPI_Type_commit ( &stype ); sptr = &sendarray [0][myrank]; MPI_Gatherv ( sptr, num, stype, rbuf, rcounts, displs, MPI_INT, root, comm); 1 3 5 7 <p> ++i) - displs [i] = displs [i-1]+rcounts [i-1]; - /* And, create receive buffer */ rbuf = (int *)malloc (gsize*(displs [gsize-1]+rcounts [gsize-1]) *sizeof (int)); /* Create datatype for one int, with extent of entire row */ disp [0] = 0; disp <ref> [1] </ref> = 150*sizeof (int); type [0] = MPI_INT; type [1] = MPI_UB; blocklen [0] = 1; blocklen [1] = 1; MPI_Type_struct ( 2, blocklen, disp, type, &stype ); MPI_Type_commit ( &stype ); sptr = &sendarray [0][myrank]; MPI_Gatherv ( sptr, num, stype, rbuf, rcounts, displs, MPI_INT, root, comm); 1 3 5 7 9 11 13 15 17 19 21 23 25 <p> - /* And, create receive buffer */ rbuf = (int *)malloc (gsize*(displs [gsize-1]+rcounts [gsize-1]) *sizeof (int)); /* Create datatype for one int, with extent of entire row */ disp [0] = 0; disp <ref> [1] </ref> = 150*sizeof (int); type [0] = MPI_INT; type [1] = MPI_UB; blocklen [0] = 1; blocklen [1] = 1; MPI_Type_struct ( 2, blocklen, disp, type, &stype ); MPI_Type_commit ( &stype ); sptr = &sendarray [0][myrank]; MPI_Gatherv ( sptr, num, stype, rbuf, rcounts, displs, MPI_INT, root, comm); 1 3 5 7 9 11 13 15 17 19 21 23 25 27 29 31 33 35 37 39 41 <p> MPI_TYPE_CONTIGUOUS (2, MPI_REAL, MPI_2REAL) Similar statements apply for MPI 2INTEGER, MPI 2DOUBLE PRECISION, and MPI 2INT. The datatype MPI FLOAT INT is as if defined by the following sequence of instructions. type [0] = MPI_FLOAT type <ref> [1] </ref> = MPI_INT disp [0] = 0 disp [1] = sizeof (float) block [0] = 1 block [1] = 1 MPI_TYPE_STRUCT (2, block, disp, type, MPI_FLOAT_INT) Similar statements apply for MPI LONG INT and MPI DOUBLE INT. Example 4.17 Each process has an array of 30 doubles, in C. <p> MPI_TYPE_CONTIGUOUS (2, MPI_REAL, MPI_2REAL) Similar statements apply for MPI 2INTEGER, MPI 2DOUBLE PRECISION, and MPI 2INT. The datatype MPI FLOAT INT is as if defined by the following sequence of instructions. type [0] = MPI_FLOAT type <ref> [1] </ref> = MPI_INT disp [0] = 0 disp [1] = sizeof (float) block [0] = 1 block [1] = 1 MPI_TYPE_STRUCT (2, block, disp, type, MPI_FLOAT_INT) Similar statements apply for MPI LONG INT and MPI DOUBLE INT. Example 4.17 Each process has an array of 30 doubles, in C. <p> The datatype MPI FLOAT INT is as if defined by the following sequence of instructions. type [0] = MPI_FLOAT type <ref> [1] </ref> = MPI_INT disp [0] = 0 disp [1] = sizeof (float) block [0] = 1 block [1] = 1 MPI_TYPE_STRUCT (2, block, disp, type, MPI_FLOAT_INT) Similar statements apply for MPI LONG INT and MPI DOUBLE INT. Example 4.17 Each process has an array of 30 doubles, in C. <p> requires 1 inter-communicator. main (int argc, char **argv) - MPI_Comm myComm; /* intra-communicator of local sub-group */ MPI_Comm myFirstComm; /* inter-communicator */ MPI_Comm mySecondComm; /* second inter-communicator (group 1 only) */ int membershipKey; int rank; MPI_Init (&argc, &argv); MPI_Comm_rank (MPI_COMM_WORLD, &rank); /* User code must generate membershipKey in the range <ref> [0, 1, 2] </ref> */ membershipKey = rank % 3; /* Build intra-communicator for local sub-group */ MPI_Comm_split (MPI_COMM_WORLD, membershipKey, rank, &myComm); /* Build inter-communicators. <p> GROUPS, CONTEXTS, AND COMMUNICATORS MPI_Comm mySecondComm; MPI_Status status; int membershipKey; int rank; MPI_Init (&argc, &argv); MPI_Comm_rank (MPI_COMM_WORLD, &rank); ... /* User code must generate membershipKey in the range <ref> [0, 1, 2] </ref> */ membershipKey = rank % 3; /* Build intra-communicator for local sub-group */ MPI_Comm_split (MPI_COMM_WORLD, membershipKey, rank, &myComm); /* Build inter-communicators.
Reference: [2] <author> V. Bala, S. Kipnis, L. Rudolph, and Marc Snir. </author> <title> Designing efficient, scalable, and portable collective communication libraries. </title> <type> Technical report, </type> <institution> IBM T. J. Watson Research Center, </institution> <month> October </month> <year> 1992. </year> <type> Preprint. </type>
Reference-contexts: Thus, MPI has been strongly influenced by work at the IBM T. J. Watson Research Center <ref> [1, 2] </ref>, Intel's NX/2 [23], Express [22], nCUBE's Vertex [21], p4 [7, 6], and PARMACS [5, 8]. Other important contributions have come from Zipcode [24, 25], Chimp [14, 15], PVM [4, 11], Chameleon [19], and PICL [18]. <p> tag, comm); /* 4.2: send only the entries of class zero particles, preceded by the number of such entries */ MPI_Datatype Zparticles; /* datatype describing all particles with class zero (needs to be recomputed if classes change) */ MPI_Datatype Ztype; MPI_Aint zdisp [1000]; int zblock [1000], j, k; int zzblock <ref> [2] </ref> = -1,1-; MPI_Aint zzdisp [2]; MPI_Datatype zztype [2]; /* compute displacements of class zero particles */ j = 0; if (particle [i].class==0) - zdisp [j] = i; zblock [j] = 1; j++; /* create datatype for class zero particles */ MPI_Type_indexed ( j, zblock, zdisp, Particletype, &Zparticles); /* prepend particle <p> only the entries of class zero particles, preceded by the number of such entries */ MPI_Datatype Zparticles; /* datatype describing all particles with class zero (needs to be recomputed if classes change) */ MPI_Datatype Ztype; MPI_Aint zdisp [1000]; int zblock [1000], j, k; int zzblock <ref> [2] </ref> = -1,1-; MPI_Aint zzdisp [2]; MPI_Datatype zztype [2]; /* compute displacements of class zero particles */ j = 0; if (particle [i].class==0) - zdisp [j] = i; zblock [j] = 1; j++; /* create datatype for class zero particles */ MPI_Type_indexed ( j, zblock, zdisp, Particletype, &Zparticles); /* prepend particle count */ MPI_Address (&j, zzdisp); <p> of class zero particles, preceded by the number of such entries */ MPI_Datatype Zparticles; /* datatype describing all particles with class zero (needs to be recomputed if classes change) */ MPI_Datatype Ztype; MPI_Aint zdisp [1000]; int zblock [1000], j, k; int zzblock <ref> [2] </ref> = -1,1-; MPI_Aint zzdisp [2]; MPI_Datatype zztype [2]; /* compute displacements of class zero particles */ j = 0; if (particle [i].class==0) - zdisp [j] = i; zblock [j] = 1; j++; /* create datatype for class zero particles */ MPI_Type_indexed ( j, zblock, zdisp, Particletype, &Zparticles); /* prepend particle count */ MPI_Address (&j, zzdisp); MPI_Address (particle, zzdisp+1); <p> addresses */ /* 5.1: send the entire array */ MPI_Type_commit ( &Particletype); MPI_Send ( MPI_BOTTOM, 1000, Particletype, dest, tag, comm); /* 5.2: send the entries of class zero, preceded by the number of such entries */ MPI_Datatype Zparticles, Ztype; MPI_Aint zdisp [1000] int zblock [1000], i, j, k; int zzblock <ref> [2] </ref> = -1,1-; 1 3 5 7 9 11 13 15 17 19 21 23 25 27 29 31 33 35 37 39 41 43 45 47 82 CHAPTER 3. POINT-TO-POINT COMMUNICATION MPI_Datatype zztype [2]; MPI_Aint zzdisp [2]; j=0; if (particle [i].index==0) - for (k=i+1; (k &lt; 1000)&&(particle [k].index = 0) <p> entries */ MPI_Datatype Zparticles, Ztype; MPI_Aint zdisp [1000] int zblock [1000], i, j, k; int zzblock <ref> [2] </ref> = -1,1-; 1 3 5 7 9 11 13 15 17 19 21 23 25 27 29 31 33 35 37 39 41 43 45 47 82 CHAPTER 3. POINT-TO-POINT COMMUNICATION MPI_Datatype zztype [2]; MPI_Aint zzdisp [2]; j=0; if (particle [i].index==0) - for (k=i+1; (k &lt; 1000)&&(particle [k].index = 0) ; k++); zdisp [j] = i; zblock [j] = k-i; j++; - MPI_Type_indexed ( j, zblock, zdisp, Particletype, &Zparticles); /* Zparticles describe particles with class zero, using their absolute addresses*/ /* prepend particle count <p> Zparticles, Ztype; MPI_Aint zdisp [1000] int zblock [1000], i, j, k; int zzblock <ref> [2] </ref> = -1,1-; 1 3 5 7 9 11 13 15 17 19 21 23 25 27 29 31 33 35 37 39 41 43 45 47 82 CHAPTER 3. POINT-TO-POINT COMMUNICATION MPI_Datatype zztype [2]; MPI_Aint zzdisp [2]; j=0; if (particle [i].index==0) - for (k=i+1; (k &lt; 1000)&&(particle [k].index = 0) ; k++); zdisp [j] = i; zblock [j] = k-i; j++; - MPI_Type_indexed ( j, zblock, zdisp, Particletype, &Zparticles); /* Zparticles describe particles with class zero, using their absolute addresses*/ /* prepend particle count */ MPI_Address (&j, <p> zzblock, zzdisp, zztype, &Ztype); MPI_Type_commit ( &Ztype); MPI_Send ( MPI_BOTTOM, 1, Ztype, dest, tag, comm); Example 3.35 Handling of unions. union - int ival; float fval; - u [1000] int utype; /* All entries of u have identical type; variable utype keeps track of their current type */ MPI_Datatype type <ref> [2] </ref>; int blocklen [2] = -1,1-; MPI_Aint disp [2]; MPI_Datatype mpi_utype [2]; MPI_Aint i,j; /* compute an MPI datatype for each possible union type; assume values are left-aligned in union storage. */ 1 3 5 7 9 11 13 15 17 19 21 23 25 27 29 31 33 35 37 <p> &Ztype); MPI_Type_commit ( &Ztype); MPI_Send ( MPI_BOTTOM, 1, Ztype, dest, tag, comm); Example 3.35 Handling of unions. union - int ival; float fval; - u [1000] int utype; /* All entries of u have identical type; variable utype keeps track of their current type */ MPI_Datatype type <ref> [2] </ref>; int blocklen [2] = -1,1-; MPI_Aint disp [2]; MPI_Datatype mpi_utype [2]; MPI_Aint i,j; /* compute an MPI datatype for each possible union type; assume values are left-aligned in union storage. */ 1 3 5 7 9 11 13 15 17 19 21 23 25 27 29 31 33 35 37 39 41 43 <p> ( MPI_BOTTOM, 1, Ztype, dest, tag, comm); Example 3.35 Handling of unions. union - int ival; float fval; - u [1000] int utype; /* All entries of u have identical type; variable utype keeps track of their current type */ MPI_Datatype type <ref> [2] </ref>; int blocklen [2] = -1,1-; MPI_Aint disp [2]; MPI_Datatype mpi_utype [2]; MPI_Aint i,j; /* compute an MPI datatype for each possible union type; assume values are left-aligned in union storage. */ 1 3 5 7 9 11 13 15 17 19 21 23 25 27 29 31 33 35 37 39 41 43 45 47 3.13. <p> Ztype, dest, tag, comm); Example 3.35 Handling of unions. union - int ival; float fval; - u [1000] int utype; /* All entries of u have identical type; variable utype keeps track of their current type */ MPI_Datatype type <ref> [2] </ref>; int blocklen [2] = -1,1-; MPI_Aint disp [2]; MPI_Datatype mpi_utype [2]; MPI_Aint i,j; /* compute an MPI datatype for each possible union type; assume values are left-aligned in union storage. */ 1 3 5 7 9 11 13 15 17 19 21 23 25 27 29 31 33 35 37 39 41 43 45 47 3.13. <p> upper bound, rather than an exact bound, since the exact amount of space needed to pack the message may depend on the context (e.g., first message packed in a packing unit may take more space). (End of rationale.) Example 3.36 An example using MPI PACK. int position, i, j, a <ref> [2] </ref>; char buff [1000]; .... <p> PACK AND UNPACK 87 .... MPI_Comm_rank (MPI_Comm_world, &myrank); if (myrank == 0) - / * SENDER CODE */ int len <ref> [2] </ref>; MPI_Aint disp [2]; MPI_Datatype type [2], newtype; /* build datatype for i followed by a [0]...a [i-1] */ len [0] = 1; MPI_Address ( &i, disp); MPI_Address ( a, disp+1); type [0] = MPI_INT; type [1] = MPI_FLOAT; MPI_Type_struct ( 2, len, disp, type, &newtype); MPI_Type_commit ( &newtype); /* Pack <p> PACK AND UNPACK 87 .... MPI_Comm_rank (MPI_Comm_world, &myrank); if (myrank == 0) - / * SENDER CODE */ int len <ref> [2] </ref>; MPI_Aint disp [2]; MPI_Datatype type [2], newtype; /* build datatype for i followed by a [0]...a [i-1] */ len [0] = 1; MPI_Address ( &i, disp); MPI_Address ( a, disp+1); type [0] = MPI_INT; type [1] = MPI_FLOAT; MPI_Type_struct ( 2, len, disp, type, &newtype); MPI_Type_commit ( &newtype); /* Pack i followed by <p> PACK AND UNPACK 87 .... MPI_Comm_rank (MPI_Comm_world, &myrank); if (myrank == 0) - / * SENDER CODE */ int len <ref> [2] </ref>; MPI_Aint disp [2]; MPI_Datatype type [2], newtype; /* build datatype for i followed by a [0]...a [i-1] */ len [0] = 1; MPI_Address ( &i, disp); MPI_Address ( a, disp+1); type [0] = MPI_INT; type [1] = MPI_FLOAT; MPI_Type_struct ( 2, len, disp, type, &newtype); MPI_Type_commit ( &newtype); /* Pack i followed by a [0]...a [i-1]*/ <p> We create a datatype that causes the correct striding at the sending end so that that we read a column of a C array. A similar thing was done in Example 3.32, Section 3.12.7. MPI_Comm comm; int gsize,sendarray [100][150],*sptr; int root, *rbuf, stride, myrank, disp <ref> [2] </ref>, blocklen [2]; MPI_Datatype stype,type [2]; int *displs,i,*rcounts; ... <p> We create a datatype that causes the correct striding at the sending end so that that we read a column of a C array. A similar thing was done in Example 3.32, Section 3.12.7. MPI_Comm comm; int gsize,sendarray [100][150],*sptr; int root, *rbuf, stride, myrank, disp <ref> [2] </ref>, blocklen [2]; MPI_Datatype stype,type [2]; int *displs,i,*rcounts; ... <p> A similar thing was done in Example 3.32, Section 3.12.7. MPI_Comm comm; int gsize,sendarray [100][150],*sptr; int root, *rbuf, stride, myrank, disp <ref> [2] </ref>, blocklen [2]; MPI_Datatype stype,type [2]; int *displs,i,*rcounts; ... <p> The complicating factor is that the various values of num are not known to root, so a separate gather must first be run to find these out. The data is placed contiguously at the receiving end. MPI_Comm comm; int gsize,sendarray [100][150],*sptr; int root, *rbuf, stride, myrank, disp <ref> [2] </ref>, blocklen [2]; MPI_Datatype stype,types [2]; int *displs,i,*rcounts,num; 1 3 5 7 9 11 13 15 17 19 21 23 25 27 29 31 33 35 37 39 41 43 45 47 102 CHAPTER 4. COLLECTIVE COMMUNICATION each set is placed stride [i] ints apart (a varying stride). ... <p> The complicating factor is that the various values of num are not known to root, so a separate gather must first be run to find these out. The data is placed contiguously at the receiving end. MPI_Comm comm; int gsize,sendarray [100][150],*sptr; int root, *rbuf, stride, myrank, disp <ref> [2] </ref>, blocklen [2]; MPI_Datatype stype,types [2]; int *displs,i,*rcounts,num; 1 3 5 7 9 11 13 15 17 19 21 23 25 27 29 31 33 35 37 39 41 43 45 47 102 CHAPTER 4. COLLECTIVE COMMUNICATION each set is placed stride [i] ints apart (a varying stride). ... <p> The data is placed contiguously at the receiving end. MPI_Comm comm; int gsize,sendarray [100][150],*sptr; int root, *rbuf, stride, myrank, disp <ref> [2] </ref>, blocklen [2]; MPI_Datatype stype,types [2]; int *displs,i,*rcounts,num; 1 3 5 7 9 11 13 15 17 19 21 23 25 27 29 31 33 35 37 39 41 43 45 47 102 CHAPTER 4. COLLECTIVE COMMUNICATION each set is placed stride [i] ints apart (a varying stride). ... <p> When using this operator, we must be careful to specify that it is non-commutative, as in the following. int i,base; SeqScanPair a, answer; MPI_Op myOp; MPI_Datatype type <ref> [2] </ref> = -MPI_DOUBLE, MPI_INT-; MPI_Aint disp [2]; int blocklen [2] = - 1, 1-; MPI_Datatype sspair; /* explain to MPI how type SegScanPair is defined */ 2 4 6 8 10 12 14 16 18 20 22 24 26 28 30 32 34 36 38 40 42 44 46 48 126 <p> When using this operator, we must be careful to specify that it is non-commutative, as in the following. int i,base; SeqScanPair a, answer; MPI_Op myOp; MPI_Datatype type <ref> [2] </ref> = -MPI_DOUBLE, MPI_INT-; MPI_Aint disp [2]; int blocklen [2] = - 1, 1-; MPI_Datatype sspair; /* explain to MPI how type SegScanPair is defined */ 2 4 6 8 10 12 14 16 18 20 22 24 26 28 30 32 34 36 38 40 42 44 46 48 126 CHAPTER 4. <p> When using this operator, we must be careful to specify that it is non-commutative, as in the following. int i,base; SeqScanPair a, answer; MPI_Op myOp; MPI_Datatype type <ref> [2] </ref> = -MPI_DOUBLE, MPI_INT-; MPI_Aint disp [2]; int blocklen [2] = - 1, 1-; MPI_Datatype sspair; /* explain to MPI how type SegScanPair is defined */ 2 4 6 8 10 12 14 16 18 20 22 24 26 28 30 32 34 36 38 40 42 44 46 48 126 CHAPTER 4. <p> MPI guarantees that a single communicator can do safe point-to-point and collective communication. #define TAG_ARBITRARY 12345 #define SOME_COUNT 50 main (int argc, char **argv) - MPI_Request request <ref> [2] </ref>; MPI_Status status [2]; MPI_Group MPI_GROUP_WORLD, subgroup; int ranks [] = -2, 4, 6, 8-; 1 3 5 7 9 11 13 15 17 19 21 23 25 27 29 31 33 35 37 39 41 43 45 47 5.5. MOTIVATING EXAMPLES 149 MPI_Comm the_comm; ... <p> MPI guarantees that a single communicator can do safe point-to-point and collective communication. #define TAG_ARBITRARY 12345 #define SOME_COUNT 50 main (int argc, char **argv) - MPI_Request request <ref> [2] </ref>; MPI_Status status [2]; MPI_Group MPI_GROUP_WORLD, subgroup; int ranks [] = -2, 4, 6, 8-; 1 3 5 7 9 11 13 15 17 19 21 23 25 27 29 31 33 35 37 39 41 43 45 47 5.5. MOTIVATING EXAMPLES 149 MPI_Comm the_comm; ... <p> requires 1 inter-communicator. main (int argc, char **argv) - MPI_Comm myComm; /* intra-communicator of local sub-group */ MPI_Comm myFirstComm; /* inter-communicator */ MPI_Comm mySecondComm; /* second inter-communicator (group 1 only) */ int membershipKey; int rank; MPI_Init (&argc, &argv); MPI_Comm_rank (MPI_COMM_WORLD, &rank); /* User code must generate membershipKey in the range <ref> [0, 1, 2] </ref> */ membershipKey = rank % 3; /* Build intra-communicator for local sub-group */ MPI_Comm_split (MPI_COMM_WORLD, membershipKey, rank, &myComm); /* Build inter-communicators. <p> GROUPS, CONTEXTS, AND COMMUNICATORS MPI_Comm mySecondComm; MPI_Status status; int membershipKey; int rank; MPI_Init (&argc, &argv); MPI_Comm_rank (MPI_COMM_WORLD, &rank); ... /* User code must generate membershipKey in the range <ref> [0, 1, 2] </ref> */ membershipKey = rank % 3; /* Build intra-communicator for local sub-group */ MPI_Comm_split (MPI_COMM_WORLD, membershipKey, rank, &myComm); /* Build inter-communicators.
Reference: [3] <author> Purushotham V. Bangalore, Nathan E. Doss, and Anthony Skjellum. </author> <title> MPI++: Issues and Features. </title> <note> In OON-SKI '94, page in press, </note> <year> 1994. </year>
Reference-contexts: POINT-TO-POINT COMMUNICATION - int class; /* particle class */ double d [6]; /* particle coordinates */ char b [7]; /* some additional information */ -; struct Partstruct particle [1000]; int i, dest, rank; MPI_Comm comm; /* build datatype describing structure */ MPI_Datatype Particletype; MPI_Datatype type <ref> [3] </ref> = -MPI_INT, MPI_DOUBLE, MPI_CHAR-; int blocklen [3] = -1, 6, 7-; MPI_Aint disp [3]; int base; /* compute displacements of structure components */ MPI_Address ( particle, disp); MPI_Address ( particle [0].d, disp+1); MPI_Address ( particle [0].b, disp+2); base = disp [0]; for (i=0; i &lt;3; i++) disp [i] -= base; <p> - int class; /* particle class */ double d [6]; /* particle coordinates */ char b [7]; /* some additional information */ -; struct Partstruct particle [1000]; int i, dest, rank; MPI_Comm comm; /* build datatype describing structure */ MPI_Datatype Particletype; MPI_Datatype type <ref> [3] </ref> = -MPI_INT, MPI_DOUBLE, MPI_CHAR-; int blocklen [3] = -1, 6, 7-; MPI_Aint disp [3]; int base; /* compute displacements of structure components */ MPI_Address ( particle, disp); MPI_Address ( particle [0].d, disp+1); MPI_Address ( particle [0].b, disp+2); base = disp [0]; for (i=0; i &lt;3; i++) disp [i] -= base; MPI_Type_struct ( 3, blocklen, disp, type, &Particletype); <p> double d [6]; /* particle coordinates */ char b [7]; /* some additional information */ -; struct Partstruct particle [1000]; int i, dest, rank; MPI_Comm comm; /* build datatype describing structure */ MPI_Datatype Particletype; MPI_Datatype type <ref> [3] </ref> = -MPI_INT, MPI_DOUBLE, MPI_CHAR-; int blocklen [3] = -1, 6, 7-; MPI_Aint disp [3]; int base; /* compute displacements of structure components */ MPI_Address ( particle, disp); MPI_Address ( particle [0].d, disp+1); MPI_Address ( particle [0].b, disp+2); base = disp [0]; for (i=0; i &lt;3; i++) disp [i] -= base; MPI_Type_struct ( 3, blocklen, disp, type, &Particletype); /* If compiler does padding in mysterious <p> address of particle [1] */ MPI_Type_hvector ( 1000, 2, sizeofentry, MPI_DOUBLE, &Allpairs); MPI_Type_commit ( &Allpairs); MPI_Send ( particle [0].d, 1, Allpairs, dest, tag, comm); /* an alternative solution to 4.3 */ MPI_Datatype Onepair; /* datatype for one pair of coordinates, with the extent of one particle entry */ MPI_Aint disp2 <ref> [3] </ref>; MPI_Datatype type2 [3] = -MPI_LB, MPI_DOUBLE, MPI_UB-; int blocklen2 [3] = -1, 2, 1-; MPI_Address ( particle, disp2); MPI_Address ( particle [0].d, disp2+1); MPI_Address ( particle+1, disp2+2); base = disp2 [0]; for (i=0; i&lt;2; i++) disp2 [i] -= base; 1 3 5 7 9 11 13 15 17 19 21 <p> [1] */ MPI_Type_hvector ( 1000, 2, sizeofentry, MPI_DOUBLE, &Allpairs); MPI_Type_commit ( &Allpairs); MPI_Send ( particle [0].d, 1, Allpairs, dest, tag, comm); /* an alternative solution to 4.3 */ MPI_Datatype Onepair; /* datatype for one pair of coordinates, with the extent of one particle entry */ MPI_Aint disp2 <ref> [3] </ref>; MPI_Datatype type2 [3] = -MPI_LB, MPI_DOUBLE, MPI_UB-; int blocklen2 [3] = -1, 2, 1-; MPI_Address ( particle, disp2); MPI_Address ( particle [0].d, disp2+1); MPI_Address ( particle+1, disp2+2); base = disp2 [0]; for (i=0; i&lt;2; i++) disp2 [i] -= base; 1 3 5 7 9 11 13 15 17 19 21 23 25 27 <p> MPI_DOUBLE, &Allpairs); MPI_Type_commit ( &Allpairs); MPI_Send ( particle [0].d, 1, Allpairs, dest, tag, comm); /* an alternative solution to 4.3 */ MPI_Datatype Onepair; /* datatype for one pair of coordinates, with the extent of one particle entry */ MPI_Aint disp2 <ref> [3] </ref>; MPI_Datatype type2 [3] = -MPI_LB, MPI_DOUBLE, MPI_UB-; int blocklen2 [3] = -1, 2, 1-; MPI_Address ( particle, disp2); MPI_Address ( particle [0].d, disp2+1); MPI_Address ( particle+1, disp2+2); base = disp2 [0]; for (i=0; i&lt;2; i++) disp2 [i] -= base; 1 3 5 7 9 11 13 15 17 19 21 23 25 27 29 31 33 35 37 39 41 <p> [0].d, 1000, Onepair, dest, tag, comm); Example 3.34 The same manipulations as in the previous example, but use absolute addresses in datatypes. struct Partstruct - int class; double d [6]; char b [7]; -; struct Partstruct particle [1000]; /* build datatype describing first array entry */ MPI_Datatype Particletype; MPI_Datatype type <ref> [3] </ref> = -MPI_INT, MPI_DOUBLE, MPI_CHAR-; int block [3] = -1, 6, 7-; MPI_Aint disp [3]; MPI_Address ( particle, disp); MPI_Address ( particle [0].d, disp+1); MPI_Address ( particle [0].b, disp+2); MPI_Type_struct ( 3, block, disp, type, &Particletype); /* Particletype describes first array entry -- using absolute addresses */ /* 5.1: send the <p> 3.34 The same manipulations as in the previous example, but use absolute addresses in datatypes. struct Partstruct - int class; double d [6]; char b [7]; -; struct Partstruct particle [1000]; /* build datatype describing first array entry */ MPI_Datatype Particletype; MPI_Datatype type <ref> [3] </ref> = -MPI_INT, MPI_DOUBLE, MPI_CHAR-; int block [3] = -1, 6, 7-; MPI_Aint disp [3]; MPI_Address ( particle, disp); MPI_Address ( particle [0].d, disp+1); MPI_Address ( particle [0].b, disp+2); MPI_Type_struct ( 3, block, disp, type, &Particletype); /* Particletype describes first array entry -- using absolute addresses */ /* 5.1: send the entire array */ MPI_Type_commit ( &Particletype); MPI_Send <p> previous example, but use absolute addresses in datatypes. struct Partstruct - int class; double d [6]; char b [7]; -; struct Partstruct particle [1000]; /* build datatype describing first array entry */ MPI_Datatype Particletype; MPI_Datatype type <ref> [3] </ref> = -MPI_INT, MPI_DOUBLE, MPI_CHAR-; int block [3] = -1, 6, 7-; MPI_Aint disp [3]; MPI_Address ( particle, disp); MPI_Address ( particle [0].d, disp+1); MPI_Address ( particle [0].b, disp+2); MPI_Type_struct ( 3, block, disp, type, &Particletype); /* Particletype describes first array entry -- using absolute addresses */ /* 5.1: send the entire array */ MPI_Type_commit ( &Particletype); MPI_Send ( MPI_BOTTOM, 1000, Particletype, dest, tag, comm); <p> Since the best libraries come with several variations on parallel systems (different data layouts, different strategies depending on the size of the system or problem, or type of floating point), this too needs to be hidden from the user. We refer the reader to [26] and <ref> [3] </ref> for further information on writing libraries in MPI, using the features described in this chapter. 5.1.1 Features Needed to Support Libraries The key features needed to support the creation of robust parallel libraries are as follows: * Safe communication space, that guarantees that libraries can communicate as they need to,
Reference: [4] <author> A. Beguelin, J. Dongarra, A. Geist, R. Manchek, and V. Sunderam. </author> <title> Visualization and debugging in a heterogeneous environment. </title> <journal> IEEE Computer, </journal> <volume> 26(6) </volume> <pages> 88-95, </pages> <month> June </month> <year> 1993. </year>
Reference-contexts: Thus, MPI has been strongly influenced by work at the IBM T. J. Watson Research Center [1, 2], Intel's NX/2 [23], Express [22], nCUBE's Vertex [21], p4 [7, 6], and PARMACS [5, 8]. Other important contributions have come from Zipcode [24, 25], Chimp [14, 15], PVM <ref> [4, 11] </ref>, Chameleon [19], and PICL [18]. The MPI standardization effort involved about 60 people from 40 organizations mainly from the United States and Europe. Most of the major vendors of concurrent computers were involved in MPI, along with researchers from universities, government laboratories, and industry. <p> MPI_Address ( particle, disp); MPI_Address ( particle [0].d, disp+1); MPI_Address ( particle [0].b, disp+2); base = disp [0]; for (i=0; i &lt;3; i++) disp [i] -= base; MPI_Type_struct ( 3, blocklen, disp, type, &Particletype); /* If compiler does padding in mysterious ways, the following may be safer */ MPI_Datatype type1 <ref> [4] </ref> = -MPI_INT, MPI_DOUBLE, MPI_CHAR, MPI_UB-; int blocklen1 [4] = -1, 6, 7, 1-; MPI_Aint disp1 [4]; /* compute displacements of structure components */ MPI_Address ( particle, disp1); MPI_Address ( particle [0].d, disp1+1); MPI_Address ( particle [0].b, disp1+2); MPI_Address ( particle+1, disp1+3); base = disp1 [0]; for (i=0; i &lt;4; i++) <p> disp+1); MPI_Address ( particle [0].b, disp+2); base = disp [0]; for (i=0; i &lt;3; i++) disp [i] -= base; MPI_Type_struct ( 3, blocklen, disp, type, &Particletype); /* If compiler does padding in mysterious ways, the following may be safer */ MPI_Datatype type1 <ref> [4] </ref> = -MPI_INT, MPI_DOUBLE, MPI_CHAR, MPI_UB-; int blocklen1 [4] = -1, 6, 7, 1-; MPI_Aint disp1 [4]; /* compute displacements of structure components */ MPI_Address ( particle, disp1); MPI_Address ( particle [0].d, disp1+1); MPI_Address ( particle [0].b, disp1+2); MPI_Address ( particle+1, disp1+3); base = disp1 [0]; for (i=0; i &lt;4; i++) disp1 [i] -= base; /* build datatype describing <p> disp [0]; for (i=0; i &lt;3; i++) disp [i] -= base; MPI_Type_struct ( 3, blocklen, disp, type, &Particletype); /* If compiler does padding in mysterious ways, the following may be safer */ MPI_Datatype type1 <ref> [4] </ref> = -MPI_INT, MPI_DOUBLE, MPI_CHAR, MPI_UB-; int blocklen1 [4] = -1, 6, 7, 1-; MPI_Aint disp1 [4]; /* compute displacements of structure components */ MPI_Address ( particle, disp1); MPI_Address ( particle [0].d, disp1+1); MPI_Address ( particle [0].b, disp1+2); MPI_Address ( particle+1, disp1+3); base = disp1 [0]; for (i=0; i &lt;4; i++) disp1 [i] -= base; /* build datatype describing structure */ 1 3 5 7 9 11
Reference: [5] <author> Luc Bomans and Rolf Hempel. </author> <title> The Argonne/GMD macros in FORTRAN for portable parallel programming and their implementation on the Intel iPSC/2. </title> <journal> Parallel Computing, </journal> <volume> 15 </volume> <pages> 119-132, </pages> <year> 1990. </year>
Reference-contexts: Thus, MPI has been strongly influenced by work at the IBM T. J. Watson Research Center [1, 2], Intel's NX/2 [23], Express [22], nCUBE's Vertex [21], p4 [7, 6], and PARMACS <ref> [5, 8] </ref>. Other important contributions have come from Zipcode [24, 25], Chimp [14, 15], PVM [4, 11], Chameleon [19], and PICL [18]. The MPI standardization effort involved about 60 people from 40 organizations mainly from the United States and Europe. <p> Edges in the communication graph are not weighted, so that processes are either simply connected or not connected at all. Rationale. Experience with similar techniques in PARMACS <ref> [5, 8] </ref> show that this information is usually sufficient for a good mapping.
Reference: [6] <author> R. Butler and E. Lusk. </author> <title> User's guide to the p4 programming system. </title> <type> Technical Report TM-ANL-92/17, </type> <institution> Argonne National Laboratory, </institution> <year> 1992. </year>
Reference-contexts: Thus, MPI has been strongly influenced by work at the IBM T. J. Watson Research Center [1, 2], Intel's NX/2 [23], Express [22], nCUBE's Vertex [21], p4 <ref> [7, 6] </ref>, and PARMACS [5, 8]. Other important contributions have come from Zipcode [24, 25], Chimp [14, 15], PVM [4, 11], Chameleon [19], and PICL [18]. The MPI standardization effort involved about 60 people from 40 organizations mainly from the United States and Europe. <p> POINT-TO-POINT COMMUNICATION - int class; /* particle class */ double d <ref> [6] </ref>; /* particle coordinates */ char b [7]; /* some additional information */ -; struct Partstruct particle [1000]; int i, dest, rank; MPI_Comm comm; /* build datatype describing structure */ MPI_Datatype Particletype; MPI_Datatype type [3] = -MPI_INT, MPI_DOUBLE, MPI_CHAR-; int blocklen [3] = -1, 6, 7-; MPI_Aint disp [3]; int base; <p> DERIVED DATATYPES 81 MPI_Type_struct ( 3, blocklen2, disp2, type2, &Onepair); MPI_Type_commit ( &Onepair); MPI_Send ( particle [0].d, 1000, Onepair, dest, tag, comm); Example 3.34 The same manipulations as in the previous example, but use absolute addresses in datatypes. struct Partstruct - int class; double d <ref> [6] </ref>; char b [7]; -; struct Partstruct particle [1000]; /* build datatype describing first array entry */ MPI_Datatype Particletype; MPI_Datatype type [3] = -MPI_INT, MPI_DOUBLE, MPI_CHAR-; int block [3] = -1, 6, 7-; MPI_Aint disp [3]; MPI_Address ( particle, disp); MPI_Address ( particle [0].d, disp+1); MPI_Address ( particle [0].b, disp+2); MPI_Type_struct
Reference: [7] <author> Ralph Butler and Ewing Lusk. </author> <title> Monitors, messages, and clusters: the p4 parallel programming system. </title> <journal> Journal of Parallel Computing, </journal> <note> 1994. to appear (Also Argonne National Laboratory Mathematics and Computer Science Division preprint P362-0493). </note>
Reference-contexts: Thus, MPI has been strongly influenced by work at the IBM T. J. Watson Research Center [1, 2], Intel's NX/2 [23], Express [22], nCUBE's Vertex [21], p4 <ref> [7, 6] </ref>, and PARMACS [5, 8]. Other important contributions have come from Zipcode [24, 25], Chimp [14, 15], PVM [4, 11], Chameleon [19], and PICL [18]. The MPI standardization effort involved about 60 people from 40 organizations mainly from the United States and Europe. <p> POINT-TO-POINT COMMUNICATION - int class; /* particle class */ double d [6]; /* particle coordinates */ char b <ref> [7] </ref>; /* some additional information */ -; struct Partstruct particle [1000]; int i, dest, rank; MPI_Comm comm; /* build datatype describing structure */ MPI_Datatype Particletype; MPI_Datatype type [3] = -MPI_INT, MPI_DOUBLE, MPI_CHAR-; int blocklen [3] = -1, 6, 7-; MPI_Aint disp [3]; int base; /* compute displacements of structure components */ <p> DERIVED DATATYPES 81 MPI_Type_struct ( 3, blocklen2, disp2, type2, &Onepair); MPI_Type_commit ( &Onepair); MPI_Send ( particle [0].d, 1000, Onepair, dest, tag, comm); Example 3.34 The same manipulations as in the previous example, but use absolute addresses in datatypes. struct Partstruct - int class; double d [6]; char b <ref> [7] </ref>; -; struct Partstruct particle [1000]; /* build datatype describing first array entry */ MPI_Datatype Particletype; MPI_Datatype type [3] = -MPI_INT, MPI_DOUBLE, MPI_CHAR-; int block [3] = -1, 6, 7-; MPI_Aint disp [3]; MPI_Address ( particle, disp); MPI_Address ( particle [0].d, disp+1); MPI_Address ( particle [0].b, disp+2); MPI_Type_struct ( 3, block,
Reference: [8] <author> Robin Calkin, Rolf Hempel, Hans-Christian Hoppe, and Peter Wypior. </author> <title> Portable programming with the parmacs message-passing library. Parallel Computing, </title> <journal> Special issue on message-passing interfaces, </journal> <note> to appear. </note>
Reference-contexts: Thus, MPI has been strongly influenced by work at the IBM T. J. Watson Research Center [1, 2], Intel's NX/2 [23], Express [22], nCUBE's Vertex [21], p4 [7, 6], and PARMACS <ref> [5, 8] </ref>. Other important contributions have come from Zipcode [24, 25], Chimp [14, 15], PVM [4, 11], Chameleon [19], and PICL [18]. The MPI standardization effort involved about 60 people from 40 organizations mainly from the United States and Europe. <p> Edges in the communication graph are not weighted, so that processes are either simply connected or not connected at all. Rationale. Experience with similar techniques in PARMACS <ref> [5, 8] </ref> show that this information is usually sufficient for a good mapping.
Reference: [9] <author> S. Chittor and R. J. Enbody. </author> <title> Performance evaluation of mesh-connected wormhole-routed networks for interprocessor communication in multicomputers. </title> <booktitle> In Proceedings of the 1990 Supercomputing Conference, </booktitle> <pages> pages 647-656, </pages> <year> 1990. </year>
Reference-contexts: On some machines, this will lead to unnecessary contention in the interconnection network. Some details about predicted and measured performance improvements that result from good process-to-processor mapping on modern wormhole-routing architectures can be found in <ref> [10, 9] </ref>. Besides possible performance benefits, the virtual topology can function as a convenient, process-naming structure, with tremendous benefits for program readability 1 3 5 7 9 11 13 15 17 19 21 23 25 27 29 31 33 35 37 39 41 43 45 47 6.2.
Reference: [10] <author> S. Chittor and R. J. Enbody. </author> <title> Predicting the effect of mapping on the communication performance of large multicomputers. </title> <booktitle> In Proceedings of the 1991 International Conference on Parallel Processing, vol. II (Software), </booktitle> <address> pages II-1 II-4, </address> <year> 1991. </year>
Reference-contexts: If we define a C procedure like this, void copyIntBuffer ( int *pin, int *pout, int len ) - int i; for (i=0; i&lt;len; ++i) *pout++ = *pin++; - then a call to it in the following code fragment has aliased arguments. int a <ref> [10] </ref>; copyIntBuffer ( a, a+3, 7); Although the C language allows this, such usage of MPI procedures is forbidden unless otherwise specified. Note that Fortran prohibits aliasing of arguments. All MPI functions are first specified in the language-independent notation. <p> 163 The actual server process would commit to running the following code: int Do_server (server_comm) MPI_Comm server_comm; - void init_queue (); int en_queue (), de_queue (); /* keep triplets of integers for later matching (fns not shown) */ MPI_Comm comm; MPI_Status status; int client_tag, client_source; int client_rank_in_new_world, pairs_rank_in_new_world; int buffer <ref> [10] </ref>, count = 1; void *queue; init_queue (&queue); for (;;) MPI_Recv (buffer, count, MPI_INT, MPI_ANY_SOURCE, MPI_ANY_TAG, server_comm, &status); /* accept from any client */ /* determine client: */ client_tag = status.MPI_TAG; client_source = status.MPI_SOURCE; client_rank_in_new_world = buffer [0]; if (client_tag == UNDO_SERVER_TAG_1) /* client that terminates server */ - while (de_queue <p> It uses the functionality just defined to create the name service. int Intercomm_name_create (local_comm, server_comm, tag, comm) MPI_Comm local_comm, server_comm; int tag; MPI_Comm *comm; - int error; int found; /* attribute acquisition mgmt for new_world */ /* comm in server_comm */ void *val; MPI_Comm new_world; int buffer <ref> [10] </ref>, rank; int local_leader = 0; MPI_Attr_get (server_comm, server_keyval, &val, &found); new_world = (MPI_Comm)val; /* retrieve cached handle */ MPI_Comm_rank (server_comm, &rank); /* rank in local group */ if (rank == local_leader) - 2 4 6 8 10 12 14 16 18 20 22 24 26 28 30 32 34 36 <p> On some machines, this will lead to unnecessary contention in the interconnection network. Some details about predicted and measured performance improvements that result from good process-to-processor mapping on modern wormhole-routing architectures can be found in <ref> [10, 9] </ref>. Besides possible performance benefits, the virtual topology can function as a convenient, process-naming structure, with tremendous benefits for program readability 1 3 5 7 9 11 13 15 17 19 21 23 25 27 29 31 33 35 37 39 41 43 45 47 6.2.
Reference: [11] <author> J. Dongarra, A. Geist, R. Manchek, and V. Sunderam. </author> <title> Integrated PVM framework supports heterogeneous network computing. </title> <journal> Computers in Physics, </journal> <volume> 7(2) </volume> <pages> 166-75, </pages> <booktitle> April 1993. </booktitle> <volume> 2 4 6 8 10 12 14 16 18 20 22 24 26 28 30 32 34 36 38 40 42 44 46 48 BIBLIOGRAPHY 205 </volume>
Reference-contexts: Thus, MPI has been strongly influenced by work at the IBM T. J. Watson Research Center [1, 2], Intel's NX/2 [23], Express [22], nCUBE's Vertex [21], p4 [7, 6], and PARMACS [5, 8]. Other important contributions have come from Zipcode [24, 25], Chimp [14, 15], PVM <ref> [4, 11] </ref>, Chameleon [19], and PICL [18]. The MPI standardization effort involved about 60 people from 40 organizations mainly from the United States and Europe. Most of the major vendors of concurrent computers were involved in MPI, along with researchers from universities, government laboratories, and industry.
Reference: [12] <author> J. J. Dongarra, R. Hempel, A. J. G. Hey, and D. W. Walker. </author> <title> A proposal for a user-level, message passing interface in a distributed memory environment. </title> <type> Technical Report TM-12231, </type> <institution> Oak Ridge National Laboratory, </institution> <month> February </month> <year> 1993. </year>
Reference-contexts: A preliminary draft proposal, known as MPI1, was put forward by Dongarra, Hempel, Hey, and Walker in November 1992, and a revised version was completed in February 1993 <ref> [12] </ref>. MPI1 embodied the main features that were identified at the Williamsburg workshop as being necessary in a message passing standard. Since MPI1 was primarily intended to promote discussion and "get the ball rolling," it focused mainly on point-to-point communications.
Reference: [13] <author> Nathan Doss, William Gropp, Ewing Lusk, and Anthony Skjellum. </author> <title> A model implementation of MPI. </title> <type> Technical report, </type> <institution> Argonne National Laboratory, </institution> <year> 1993. </year>
Reference-contexts: At the same time, implementations of MPI on top of standard Unix interprocessor communication protocols will provide portability to workstation clusters and heterogenous networks of workstations. Several proprietary, native implementations of MPI, and a public domain, portable implementation of MPI are in progress at the time of this writing <ref> [17, 13] </ref>. 1.4 What Is Included In The Standard? The standard includes: * Point-to-point communication * Collective operations * Process groups * Communication contexts * Process topologies * Bindings for Fortran 77 and C * Environmental Management and inquiry * Profiling interface 1.5 What Is Not Included In The Standard? The
Reference: [14] <institution> Edinburgh Parallel Computing Centre, University of Edinburgh. CHIMP Concepts, </institution> <month> June </month> <year> 1991. </year>
Reference-contexts: Thus, MPI has been strongly influenced by work at the IBM T. J. Watson Research Center [1, 2], Intel's NX/2 [23], Express [22], nCUBE's Vertex [21], p4 [7, 6], and PARMACS [5, 8]. Other important contributions have come from Zipcode [24, 25], Chimp <ref> [14, 15] </ref>, PVM [4, 11], Chameleon [19], and PICL [18]. The MPI standardization effort involved about 60 people from 40 organizations mainly from the United States and Europe. Most of the major vendors of concurrent computers were involved in MPI, along with researchers from universities, government laboratories, and industry.
Reference: [15] <institution> Edinburgh Parallel Computing Centre, University of Edinburgh. </institution> <note> CHIMP Version 1.0 Interface, </note> <month> May </month> <year> 1992. </year>
Reference-contexts: Thus, MPI has been strongly influenced by work at the IBM T. J. Watson Research Center [1, 2], Intel's NX/2 [23], Express [22], nCUBE's Vertex [21], p4 [7, 6], and PARMACS [5, 8]. Other important contributions have come from Zipcode [24, 25], Chimp <ref> [14, 15] </ref>, PVM [4, 11], Chameleon [19], and PICL [18]. The MPI standardization effort involved about 60 people from 40 organizations mainly from the United States and Europe. Most of the major vendors of concurrent computers were involved in MPI, along with researchers from universities, government laboratories, and industry.
Reference: [16] <author> D. Feitelson. </author> <title> Communicators: Object-based multiparty interactions for parallel programming. </title> <type> Technical Report 91-12, </type> <institution> Dept. Computer Science, The Hebrew University of Jerusalem, </institution> <month> November </month> <year> 1991. </year>
Reference-contexts: INTRODUCTION 131 5.1.2 MPI's Support for Libraries The corresponding concepts that MPI provides, specifically to support robust libraries, are as follows: * Contexts of communication, * Groups of processes, * Virtual topologies, * Attribute caching, * Communicators. Communicators (see <ref> [16, 24, 27] </ref>) encapsulate all of these ideas in order to provide the appropriate scope for all communication operations in MPI. Communicators are divided into two kinds: intra-communicators for operations within a single group of processes, and inter-communicators, for point-to-point communication between two groups of processes. Caching.
Reference: [17] <author> Hubertus Franke, Peter Hochschild, Pratap Pattnaik, and Marc Snir. </author> <title> An efficient implementation of MPI. </title> <booktitle> In 1994 International Conference on Parallel Processing, </booktitle> <year> 1994. </year>
Reference-contexts: At the same time, implementations of MPI on top of standard Unix interprocessor communication protocols will provide portability to workstation clusters and heterogenous networks of workstations. Several proprietary, native implementations of MPI, and a public domain, portable implementation of MPI are in progress at the time of this writing <ref> [17, 13] </ref>. 1.4 What Is Included In The Standard? The standard includes: * Point-to-point communication * Collective operations * Process groups * Communication contexts * Process topologies * Bindings for Fortran 77 and C * Environmental Management and inquiry * Profiling interface 1.5 What Is Not Included In The Standard? The
Reference: [18] <author> G. A. Geist, M. T. Heath, B. W. Peyton, and P. H. Worley. </author> <title> A user's guide to PICL: a portable instrumented communication library. </title> <type> Technical Report TM-11616, </type> <institution> Oak Ridge National Laboratory, </institution> <month> October </month> <year> 1990. </year>
Reference-contexts: J. Watson Research Center [1, 2], Intel's NX/2 [23], Express [22], nCUBE's Vertex [21], p4 [7, 6], and PARMACS [5, 8]. Other important contributions have come from Zipcode [24, 25], Chimp [14, 15], PVM [4, 11], Chameleon [19], and PICL <ref> [18] </ref>. The MPI standardization effort involved about 60 people from 40 organizations mainly from the United States and Europe. Most of the major vendors of concurrent computers were involved in MPI, along with researchers from universities, government laboratories, and industry.
Reference: [19] <author> William D. Gropp and Barry Smith. </author> <title> Chameleon parallel programming tools users manual. </title> <type> Technical Report ANL-93/23, </type> <institution> Argonne National Laboratory, </institution> <month> March </month> <year> 1993. </year>
Reference-contexts: Thus, MPI has been strongly influenced by work at the IBM T. J. Watson Research Center [1, 2], Intel's NX/2 [23], Express [22], nCUBE's Vertex [21], p4 [7, 6], and PARMACS [5, 8]. Other important contributions have come from Zipcode [24, 25], Chimp [14, 15], PVM [4, 11], Chameleon <ref> [19] </ref>, and PICL [18]. The MPI standardization effort involved about 60 people from 40 organizations mainly from the United States and Europe. Most of the major vendors of concurrent computers were involved in MPI, along with researchers from universities, government laboratories, and industry.
Reference: [20] <author> O. Kramer and H. Muhlenbein. </author> <title> Mapping strategies in message-based multiprocessor systems. </title> <journal> Parallel Computing, </journal> <volume> 9 </volume> <pages> 213-225, </pages> <year> 1989. </year> <title> [21] nCUBE Corporation. nCUBE 2 Programmers Guide, </title> <address> r2.0, </address> <month> December </month> <year> 1990. </year>
Reference-contexts: The basic point-to-point communication operations are send and receive. Their use is illustrated in the example below. #include "mpi.h" main ( argc, argv ) int argc; char **argv; - char message <ref> [20] </ref>; int myrank; MPI_Status status; MPI_Init ( &argc, &argv ); MPI_Comm_rank ( MPI_COMM_WORLD, &myrank ); if (myrank == 0) /* code for process zero */ - strcpy (message,"Hello, there"); MPI_Send (message, strlen (message), MPI_CHAR, 1, 99, MPI_COMM_WORLD); - else /* code for process one */ - MPI_Recv (message, 20, MPI_CHAR, 0, <p> There are well-known techniques for mapping grid/torus structures to hardware topologies such as hypercubes or grids. For more complicated graph structures good heuristics often yield nearly optimal results <ref> [20] </ref>. On the other hand, if there is no way for the user to specify the logical process arrangement as a "virtual topology," a random mapping is most likely to result. On some machines, this will lead to unnecessary contention in the interconnection network.
Reference: [22] <institution> Parasoft Corporation, Pasadena, CA. </institution> <note> Express User's Guide, version 3.2.5 edition, </note> <year> 1992. </year>
Reference-contexts: Thus, MPI has been strongly influenced by work at the IBM T. J. Watson Research Center [1, 2], Intel's NX/2 [23], Express <ref> [22] </ref>, nCUBE's Vertex [21], p4 [7, 6], and PARMACS [5, 8]. Other important contributions have come from Zipcode [24, 25], Chimp [14, 15], PVM [4, 11], Chameleon [19], and PICL [18]. The MPI standardization effort involved about 60 people from 40 organizations mainly from the United States and Europe. <p> Thus, special support for hypercube structures is not necessary. The local auxiliary function MPI DIMS CREATE can be used to compute a balanced distribution of processes among a given number of dimensions. Rationale. Similar functions are contained in EXPRESS <ref> [22] </ref> and PARMACS. (End of rationale.) The function MPI TOPO TEST can be used to inquire about the topology associated with a communicator.
Reference: [23] <author> Paul Pierce. </author> <title> The NX/2 operating system. </title> <booktitle> In Proceedings of the Third Conference on Hypercube Concurrent Computers and Applications, </booktitle> <pages> pages 384-390. </pages> <publisher> ACM Press, </publisher> <year> 1988. </year>
Reference-contexts: Thus, MPI has been strongly influenced by work at the IBM T. J. Watson Research Center [1, 2], Intel's NX/2 <ref> [23] </ref>, Express [22], nCUBE's Vertex [21], p4 [7, 6], and PARMACS [5, 8]. Other important contributions have come from Zipcode [24, 25], Chimp [14, 15], PVM [4, 11], Chameleon [19], and PICL [18].
Reference: [24] <author> A. Skjellum and A. Leung. </author> <title> Zipcode: a portable multicomputer communication library atop the reactive kernel. </title> <editor> In D. W. Walker and Q. F. Stout, editors, </editor> <booktitle> Proceedings of the Fifth Distributed Memory Concurrent Computing Conference, </booktitle> <pages> pages 767-776. </pages> <publisher> IEEE Press, </publisher> <year> 1990. </year>
Reference-contexts: Thus, MPI has been strongly influenced by work at the IBM T. J. Watson Research Center [1, 2], Intel's NX/2 [23], Express [22], nCUBE's Vertex [21], p4 [7, 6], and PARMACS [5, 8]. Other important contributions have come from Zipcode <ref> [24, 25] </ref>, Chimp [14, 15], PVM [4, 11], Chameleon [19], and PICL [18]. The MPI standardization effort involved about 60 people from 40 organizations mainly from the United States and Europe. <p> INTRODUCTION 131 5.1.2 MPI's Support for Libraries The corresponding concepts that MPI provides, specifically to support robust libraries, are as follows: * Contexts of communication, * Groups of processes, * Virtual topologies, * Attribute caching, * Communicators. Communicators (see <ref> [16, 24, 27] </ref>) encapsulate all of these ideas in order to provide the appropriate scope for all communication operations in MPI. Communicators are divided into two kinds: intra-communicators for operations within a single group of processes, and inter-communicators, for point-to-point communication between two groups of processes. Caching.
Reference: [25] <author> A. Skjellum, S. Smith, C. Still, A. Leung, and M. Morari. </author> <title> The Zipcode message passing system. </title> <type> Technical report, </type> <institution> Lawrence Livermore National Laboratory, </institution> <month> September </month> <year> 1992. </year>
Reference-contexts: Thus, MPI has been strongly influenced by work at the IBM T. J. Watson Research Center [1, 2], Intel's NX/2 [23], Express [22], nCUBE's Vertex [21], p4 [7, 6], and PARMACS [5, 8]. Other important contributions have come from Zipcode <ref> [24, 25] </ref>, Chimp [14, 15], PVM [4, 11], Chameleon [19], and PICL [18]. The MPI standardization effort involved about 60 people from 40 organizations mainly from the United States and Europe.
Reference: [26] <author> Anthony Skjellum, Nathan E. Doss, and Purushotham V. </author> <title> Bangalore. Writing Libraries in MPI. </title> <editor> In Anthony Skjellum and Donna S. Reese, editors, </editor> <booktitle> Proceedings of the Scalable Parallel Libraries Conference, </booktitle> <pages> pages 166-173. </pages> <publisher> IEEE Computer Society Press, </publisher> <month> October </month> <year> 1993. </year>
Reference-contexts: Since the best libraries come with several variations on parallel systems (different data layouts, different strategies depending on the size of the system or problem, or type of floating point), this too needs to be hidden from the user. We refer the reader to <ref> [26] </ref> and [3] for further information on writing libraries in MPI, using the features described in this chapter. 5.1.1 Features Needed to Support Libraries The key features needed to support the creation of robust parallel libraries are as follows: * Safe communication space, that guarantees that libraries can communicate as they
Reference: [27] <author> Anthony Skjellum, Steven G. Smith, Nathan E. Doss, Alvin P . Leung, and Manfred Morari. </author> <title> The Design and Evolution of Zipcode. </title> <booktitle> Parallel Computing, </booktitle> <year> 1994. </year> <title> (Invited Paper, </title> <journal> to appear in Special Issue on Message Passing). </journal> <volume> 1 3 5 7 9 11 13 15 17 19 21 23 25 27 29 31 33 35 37 39 41 43 45 47 206 BIBLIOGRAPHY </volume>
Reference-contexts: INTRODUCTION 131 5.1.2 MPI's Support for Libraries The corresponding concepts that MPI provides, specifically to support robust libraries, are as follows: * Contexts of communication, * Groups of processes, * Virtual topologies, * Attribute caching, * Communicators. Communicators (see <ref> [16, 24, 27] </ref>) encapsulate all of these ideas in order to provide the appropriate scope for all communication operations in MPI. Communicators are divided into two kinds: intra-communicators for operations within a single group of processes, and inter-communicators, for point-to-point communication between two groups of processes. Caching.
Reference: [28] <author> Anthony Skjellum, Steven G. Smith, Nathan E. Doss, Charles H. Still, Alvin P. Le-ung, and Manfred Morari. </author> <title> Zipcode: A Portable Communication Layer for High Performance Multicomputing. </title> <type> Technical Report UCRL-JC-106725 (revised 9/92, 12/93, 4/94), </type> <institution> Lawrence Livermore National Laboratory, </institution> <month> March </month> <year> 1991. </year> <note> To appear in Concur-rency: Practice & Experience. </note>
Reference-contexts: Algorithms like "reduce" and "allreduce" have strong enough source selectivity properties so that they are inherently okay (no backmasking), provided that MPI provides basic guarantees. So are multiple calls to a typical tree-broadcast algorithm with the same root or different roots (see <ref> [28] </ref>). Here we rely on two guarantees of MPI: pairwise ordering of messages between processes in the same context, and source selectivity | deleting either feature removes the guarantee that backmasking cannot be required.

References-found: 27

