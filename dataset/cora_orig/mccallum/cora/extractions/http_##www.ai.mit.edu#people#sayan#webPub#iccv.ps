URL: http://www.ai.mit.edu/people/sayan/webPub/iccv.ps
Refering-URL: http://www.ai.mit.edu/people/sayan/pub.html
Root-URL: 
Title: Automatic Generation of GRBF Networks for Visual Learning  
Author: Shayan Mukherjee Shree K. Nayar 
Address: New York, NY 10027 New York, NY 10027  
Affiliation: Dept. Applied Physics Dept. Computer Science Columbia University Columbia University  
Note: To appear in the International Conference on Computer Vision  
Abstract: Learning can often be viewed as the problem of mapping from an input space to an output space. Examples of these mappings are used to construct a continuous function that approximates given data and generalizes for intermediate instances. Generalized Radial Basis Function (GRBF) networks are used to formulate this approximating function. A novel method is introduced to construct an optimal GRBF network for a given mapping and error bound using the integral wavelet transform. Simple one-dimensional examples are used to demonstrate how the optimal network is superior to one constructed using standard ad hoc optimization techniques. The paper concludes with an application of optimal GRBF networks to object recognition and pose estimation. The results of this application are favorable. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> P. Baldi, </author> <title> "Computing with Arrays of Bell-Shaped and Sigmoidal Functions," </title> <booktitle> Advances in Neural Inf. Proc. Systems, </booktitle> <editor> R. P. Lippman, J. E. Moody, and D. S. Touret-zky (eds.), </editor> <publisher> Morgan Kaufman, </publisher> <pages> pp. 735-742, </pages> <year> 1991. </year>
Reference-contexts: Poggio and Girosi [12] discuss a particularly interesting class of neural networks called Radial Basis Function (RBF) networks. Unlike multi-layer perceptrons and other traditional neural networks, RBF networks have a rigorous formulation and can approximate any function to any specified degree of precision <ref> [1] </ref>. These networks have been used for recognition of stick figures [11] and face perception [3]. Although RBF networks have a rigorous formulation, this advantage is lost in most practical implementations. The reason is the assumption that RBF networks consist of as many basis functions as training examples.
Reference: [2] <author> M. D. Buhmann and C. A. Micchelli, </author> <title> "Spline Pre-wavelets for Non-uniform Knots," </title> <journal> Numerische Mathe-matik, </journal> <volume> Vol. 61, </volume> <pages> pp. 455-474, </pages> <year> 1992. </year>
Reference-contexts: The same procedure is used to calculate d j1 (k). Approximations due to the oversampling inherent in the peri-odogram are introduced in this technique. A more rigorous but much more computational intensive method to perform a IWT on unevenly sampled data has been developed by Buhmann and Micchelli <ref> [2] </ref>. (For more details on our exten sion of Mallat's algorithm see [9]). The mappings consider so far have been R 1 7! R 1 .
Reference: [3] <author> R. Brunelli and T. Poggio, </author> <title> "Caricatural Effects in Automated Face Perception," </title> <journal> Biological Cybernetics, </journal> <volume> Vol. 69, </volume> <pages> pp. 235-241, </pages> <year> 1993. </year>
Reference-contexts: Unlike multi-layer perceptrons and other traditional neural networks, RBF networks have a rigorous formulation and can approximate any function to any specified degree of precision [1]. These networks have been used for recognition of stick figures [11] and face perception <ref> [3] </ref>. Although RBF networks have a rigorous formulation, this advantage is lost in most practical implementations. The reason is the assumption that RBF networks consist of as many basis functions as training examples.
Reference: [4] <author> C. K. Chui, </author> <title> An Introduction to Wavelets, </title> <publisher> Academic Press, </publisher> <address> San Diego, </address> <year> 1992. </year>
Reference-contexts: DACA 76-92-C-0007. for the given mapping and error bound. This result is achieved through a novel construction and application of the integral wavelet transform <ref> [4] </ref>. Wavelet bases are constructed from radial basis functions. The magnitude of the wavelet coefficients determine the importance to the mapping of the radial basis functions that comprise the wavelet. We use the coefficients and Parseval's theorem to determine the number of bases required and their parameters. <p> The greatest disadvantage is that, given a data set and a specified error bound, the number of basis functions, as well as their parameters, required to perform the mapping cannot be determined analytically. 3 Integral Wavelet Transforms and GRBF Networks In this section, we describe how the integral wavelet transform <ref> [4] </ref> is used to generate a GRBF network. It shall be shown that by using the transform, the smallest number of basis functions required for a given mapping and error bound can be determined analytically. <p> The IWT allows us to construct orthonormal basis functions that are localized in space and then decompose a function in terms of these bases. For the case of a 1-D function, f (x) (R 1 7! R 1 ), the IWT has the basic form <ref> [4] </ref> : (T f )(b; a) = 1 where b;a (x) = jaj 1=2 ( a are the wavelet bases. <p> The wavelet bases are orthonormal across scale and either biorthonomal or orthonormal across position. Using or-thonormal bases the following exact representation of a func tion, f (x), can be obtained <ref> [4] </ref>: f (x) = j k where d j (k) =&lt; f (k); (2 x k) &gt; : (6) If we use a scaling function that approximates a radial ba sis function then (5) is identical to the approximating functions of (2) in section 2.
Reference: [5] <author> E. Horowitz and S. </author> <title> Sahni Fundamentals of Data Structures, </title> <publisher> Computer Science Press Int. Inc., </publisher> <year> 1993. </year>
Reference-contexts: Clearly, it is impossible to store and process 1024 15 entries. Instead, a sparse tensor (an extension of the concept of a sparse matrix <ref> [5] </ref>) was constructed with only the entries for which f (x i ) were known. The networks ability to learn and generalize examples presented to it was tested using two data sets.
Reference: [6] <author> N. R. Lomb, </author> <title> "Least-Squares Frequency Analysis of Unequally Spaced Data," </title> <booktitle> Astrophysics and Space Science, </booktitle> <volume> Vol. 39, </volume> <pages> pp. 447-462, </pages> <year> 1976. </year>
Reference-contexts: This brings up the question of how to implement: a (x) = f (x) fl g (x) (9) when f (x) is an unevenly sampled discrete signal and g (x) is a continuous function. The Lomb periodogram <ref> [6] </ref> is used to implement the above convolution. The periodogram performs a Fourier transform on unevenly sampled data, so we can map f (x) 7! F (!) where F (!) are the transform coefficients.
Reference: [7] <author> S. G. Mallat, </author> <title> "A Theory for Multiresolution Signal Decomposition: The Wavelet Representation," </title> <journal> IEEE Trans. on Pattern Analysis and Machine Intelligence, </journal> <volume> Vol. 11, No. 7, </volume> <pages> pp. 674-693, </pages> <year> 1989. </year>
Reference-contexts: One class of scaling functions that approximate radial basis functions are fi-splines. 3.2 Calculating the Wavelet Coefficients The training data given, x i 7! f (x i ) can be considered a discrete signal. So a Discrete Integral Wavelet Transform <ref> [7] </ref> is implemented. The input space is discretized into 2 J bins, where J typically ranges from 9 to 11. The f (x i ) values are then placed into the appropriate bin by rounding off x i . <p> Since the bins are small we assume that any error introduced from the rounding off is negligible. The next step involves calculating the transform coefficients d j;k for the wavelet bases j;k . Mallat's Fast Wavelet algorithm <ref> [7] </ref> is extended to unevenly sampled data to calculate these coefficients. Mallat's algorithm is pyramidal in nature [7]: c j1 (k) = [ v flc j ] #2 (k) (7) ffi where, # 2 denotes downsampling by two, keep every other term. <p> The next step involves calculating the transform coefficients d j;k for the wavelet bases j;k . Mallat's Fast Wavelet algorithm <ref> [7] </ref> is extended to unevenly sampled data to calculate these coefficients. Mallat's algorithm is pyramidal in nature [7]: c j1 (k) = [ v flc j ] #2 (k) (7) ffi where, # 2 denotes downsampling by two, keep every other term. The c 0 values correspond to the signal at the finest resolution level, i.e. the discrete function f (k). <p> The c 0 values correspond to the signal at the finest resolution level, i.e. the discrete function f (k). The formulae for ffi ffi w for the Battle-Lamarie wavelet basis <ref> [7] </ref> constructed from cubic fi-splines are given in Appendix A. This algorithm is iterated for j = 0 to 1 J , where 0 is the finest resolution level and 1 J is the coarsest level. <p> H (!) = F (!), and identical spatial properties at all known x i . The function h (x) has the form : h (x) = 2 m=0 i! o mx h (x) replaces f (x) in the convolution in (9). In the fast wavelet algorithm <ref> [7] </ref>, the above technique is used to compute a continuous function c 0 j1 (x): 0 1 N p X ffi i! o mx This continuous function c 0 j1 (x) is sampled at the j th resolution level and then downsampled by 2 to obtain c j1 (k).
Reference: [8] <author> C. A. Micchelli, </author> <title> "Interpolation of Scattered Data: Distance Matrices and Conditionally Positive Definite Functions," Constructive Approximation, </title> <journal> Vol. </journal> <volume> 2, </volume> <pages> pp. 11-22, </pages> <year> 1986. </year>
Reference-contexts: The approximating function that minimizes the above functional has the form : F (W; x) = i=1 where, G (x; x i ) are radial basis functions and c i are the coefficients of the functions. Two theorems by Micchelli <ref> [8] </ref> can be used to determine whether a function can be used as a radial basis function. Thin-plate splines and gaussians are two types of functions that meet these conditions (see [12] for a more extensive list). Casting the approximating function as a network is straightforward.
Reference: [9] <author> S. Mukherjee and S. K. Nayar, </author> <title> "Automatic Generation of RBF Networks," </title> <type> Technical Report CUCS-001-95, </type> <institution> Department of Computer Science, Columbia University, </institution> <month> December, </month> <year> 1994. </year>
Reference-contexts: A more rigorous but much more computational intensive method to perform a IWT on unevenly sampled data has been developed by Buhmann and Micchelli [2]. (For more details on our exten sion of Mallat's algorithm see <ref> [9] </ref>). The mappings consider so far have been R 1 7! R 1 . To extend the transform to functions that map R n 7! R m , multidimensional functions, (x) and (x), are introduced for both the scaling and wavelet functions.
Reference: [10] <author> H. Murase and S. K. Nayar, </author> <title> "Learning and Recognition of 3D Objects from Appearance," </title> <journal> International Journal of Computer Vision, </journal> <pages> pp. 1-24, </pages> <month> Jan </month> <year> 1995. </year>
Reference-contexts: The inputs and outputs that are used to construct the network are those used by the system developed by Murase and Na-yar <ref> [10] </ref> that creates a compact representation of object appearance parameterized by pose. Murase and Nayar construct a subspace (typically of 10-20 dimensions) called the eigenspace from a large image set of several objects in various poses. Every image of an object is mapped to a point in the eigenspace. <p> In our experiments, an optimal network was constructed for each of the 20 objects used by Nayar and Murase <ref> [10] </ref> (see was discretized into 1024 boxes in each of its dimensions. pose estimation experiments (from [10]). The input is a point in eigenspace and the outputs are the confidence C p , and pose sin ( p ) and cos ( p ). <p> In our experiments, an optimal network was constructed for each of the 20 objects used by Nayar and Murase <ref> [10] </ref> (see was discretized into 1024 boxes in each of its dimensions. pose estimation experiments (from [10]). The input is a point in eigenspace and the outputs are the confidence C p , and pose sin ( p ) and cos ( p ). Clearly, it is impossible to store and process 1024 15 entries.
Reference: [11] <author> T. Poggio and S. Edelman, </author> <title> "A Network That Learns to Recognize Three-dimensional Objects," </title> <journal> Nature, </journal> <volume> Vol. 343, </volume> <pages> pp. 263-266, </pages> <year> 1990. </year>
Reference-contexts: Unlike multi-layer perceptrons and other traditional neural networks, RBF networks have a rigorous formulation and can approximate any function to any specified degree of precision [1]. These networks have been used for recognition of stick figures <ref> [11] </ref> and face perception [3]. Although RBF networks have a rigorous formulation, this advantage is lost in most practical implementations. The reason is the assumption that RBF networks consist of as many basis functions as training examples.
Reference: [12] <author> T. Poggio and F. Girosi, </author> <title> "Networks for Approximation and Learning," </title> <journal> Proc. of IEEE, </journal> <volume> Vol. 78, </volume> <pages> pp. 1481-1497, </pages> <year> 1990. </year>
Reference-contexts: 1 Introduction Visual learning is fast emerging as an area of great research interest. Often, learning can be equated to constructing a continuous function that maps a given set of inputs to outputs. A common mechanism for performing such a mapping is a neural network. Poggio and Girosi <ref> [12] </ref> discuss a particularly interesting class of neural networks called Radial Basis Function (RBF) networks. Unlike multi-layer perceptrons and other traditional neural networks, RBF networks have a rigorous formulation and can approximate any function to any specified degree of precision [1]. <p> This is remedied by using Generalized Radial Basis Function (GRBF) networks that have fewer basis functions than examples. However, the parameters of the GRBF network are typically set in an ad hoc manner <ref> [12] </ref>. In this paper, we introduce an analytic method that sets the network parameters for any given input-output mapping and error bound. <p> The first part of this section is a brief overview of RBF and GRBF networks (see <ref> [12] </ref> for details). Learning the mapping between input and output spaces is equivalent to finding a continuous multivariate function that best approximates the given data and interpolates for intermediary instances. Several functions can approximate and interpolate given data. <p> Two theorems by Micchelli [8] can be used to determine whether a function can be used as a radial basis function. Thin-plate splines and gaussians are two types of functions that meet these conditions (see <ref> [12] </ref> for a more extensive list). Casting the approximating function as a network is straightforward. The RBF network has three layers, each fully connected to the next. The first layer consists of a single input unit, the vector x. The second layer consists of multidimensional RBF's.
Reference: [13] <author> A. N. Tikhonov and V. Y. </author> <title> Arsenin Solutions of Ill Posed Problems W.H. </title> <publisher> Winston, </publisher> <address> Washington D.C., </address> <year> 1977. </year>
Reference-contexts: The paper concludes with a discussion of other possible applications of the proposed technique. 2 Radial Basis Function Networks The strength of RBF networks is that input-output mappings are learned in a mathematically rigorous formalism using approximation theory and regularization techniques <ref> [13] </ref>. The first part of this section is a brief overview of RBF and GRBF networks (see [12] for details). Learning the mapping between input and output spaces is equivalent to finding a continuous multivariate function that best approximates the given data and interpolates for intermediary instances.
Reference: [14] <author> M. Unser, A. Aldroubi, M. Eden, </author> <title> "The L 2 Polynomial Spline Pyramid," </title> <journal> IEEE Trans. on Pattern Analysis and Machine Intelligence, </journal> <volume> Vol. 15, No. 4, </volume> <pages> pp. 364-378, </pages> <month> April </month> <year> 1993. </year>
Reference-contexts: Therefore, the ideas presented in this paper have far reaching implications. 7 Appendix A The filter formulae, ffi ffi w (k), that were used in section 3.2 to calculate the wavelet coefficients, d j;k , are given below. These formulae were calculated by Unser, Al droubi, and Eden [15] <ref> [14] </ref>. ffi 2 U 2 (f ) B 7 B 7 ffi 2 e U 2 (f + 1 s 1 (f + 1 B 7 B 7 1 (2416 + 1191 [z + z 1 ] + 120 [z 2 + z 2 ] + z 3 + z 3
Reference: [15] <author> M. Unser, A. Aldroubi, and M. Eden, </author> <title> "A Family of Polynomial Spline Wavelet Transforms," </title> <booktitle> Signal Processing, </booktitle> <volume> Vol. 30, </volume> <pages> pp. 141-162, </pages> <year> 1993. </year>
Reference-contexts: Therefore, the ideas presented in this paper have far reaching implications. 7 Appendix A The filter formulae, ffi ffi w (k), that were used in section 3.2 to calculate the wavelet coefficients, d j;k , are given below. These formulae were calculated by Unser, Al droubi, and Eden <ref> [15] </ref> [14]. ffi 2 U 2 (f ) B 7 B 7 ffi 2 e U 2 (f + 1 s 1 (f + 1 B 7 B 7 1 (2416 + 1191 [z + z 1 ] + 120 [z 2 + z 2 ] + z 3 + z
References-found: 15

