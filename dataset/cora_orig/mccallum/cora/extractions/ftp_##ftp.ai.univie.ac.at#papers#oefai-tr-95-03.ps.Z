URL: ftp://ftp.ai.univie.ac.at/papers/oefai-tr-95-03.ps.Z
Refering-URL: http://www.ai.univie.ac.at/cgi-bin/tr-online/?number+95-03
Root-URL: 
Email: E-mail: juffi@ai.univie.ac.at  
Title: A Tight Integration of Pruning and Learning  
Author: Johannes Furnkranz 
Keyword: Rule Learning, Inductive Logic Programming, Pruning, Noise  
Note: OEFAI-TR-95-03  
Address: Schottengasse 3, A-1010 Vienna, Austria  
Affiliation: Austrian Research Institute for Artificial Intelligence  
Abstract: This paper outlines some problems that may occur with Reduced Error Pruning in rule learning algorithms. In particular we show that pruning complete theories is incompatible with the separate-and-conquer learning strategy that is commonly used in propositional and relational rule learning systems. As a solution we propose to integrate pruning into learning and examine two algorithms, one that prunes at the clause level and one that prunes at the literal level. Experiments show that these methods are not only much more efficient, but also able to achieve small gains in accuracy by solving the outlined problem. fl An Extended Abstract of this paper appeared in the Proceedings of the ECML-95. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Brunk, C. A. and M. J. </author> <title> Pazzani (1991). An investigation of noise-tolerant relational concept learning algorithms. </title> <booktitle> In Proceedings of the 8th International Workshop on Machine Learning, </booktitle> <address> Evanston, </address> <publisher> Illinois, </publisher> <pages> pp. 389-393. </pages>
Reference-contexts: The most common post-pruning algorithm, Reduced Error Pruning (REP), has been adapted from propositional decision tree learning (Quinlan 1987) to relational rule learning <ref> (Brunk and Pazzani 1991) </ref>. However, this adaptation has several shortcomings. In particular we will argue in section 2 that post-pruning complete theories is incompatible with the commonly used separate-and-conquer rule learning strategy. <p> Pagallo and Haussler (1990) adapted it for learning decision lists. A version that can be used for relational learning was then introduced in <ref> (Brunk and Pazzani 1991) </ref>. The basic algorithm of this version is depicted in Fig. 1. <p> Most of the efficiency of the I-REP algorithm comes from the integration of pre-pruning and post-pruning by this definition of a stopping criterion based on the accuracy of the pruned clause on the pruning set. Thus I-REP does not need REP's delete-clause operator <ref> (Brunk and Pazzani 1991) </ref>, because the clauses of the final theory are constructed directly and learning stops when no more useful clauses can be found.
Reference: <author> Cameron-Jones, R. </author> <year> (1994, </year> <month> May). </month> <title> The complexity of Cohen's grow method. Unpublished draft for comments. </title>
Reference: <author> Clark, P. and R. </author> <title> Boswell (1991). Rule induction with CN2: Some recent improvements. </title> <booktitle> In Proceedings of the 5th European Working Session of Learning, Porto, Portugal, </booktitle> <pages> pp. 151-163. </pages>
Reference: <author> Clark, P. and T. </author> <title> Niblett (1989). The CN2 induction algorithm. </title> <booktitle> Machine Learning 3 (4), </booktitle> <pages> 261-283. </pages>
Reference-contexts: After splitting the training set into a growing and a pruning set according to some user-specified ratio, a concept description that covers all of the positive and none of the negative examples of the growing set is learned with a separate-and-conquer rule learning algorithm like the propositional learner CN2 <ref> (Clark and Niblett 1989) </ref> or the relational learner Foil (Quinlan 1990). This intermediate theory is then simplified by deleting literals and clauses until any further deletion would lead to a decrease of accuracy on the pruning set.
Reference: <author> Cohen, W. W. </author> <year> (1993). </year> <title> Efficient pruning methods for separate-and-conquer rule learning systems. </title> <booktitle> In Proceedings of the 13th International Joint Conference on Artificial Intelligence, </booktitle> <address> Chambery, France, </address> <pages> pp. 988-994. </pages>
Reference-contexts: A lot of work is wasted in learning and subsequently pruning superfluous literals and clauses. This argument has been formalized in <ref> (Cohen 1993) </ref>, where it was shown that the growing phase of REP has a time 1 procedure REP (Examples, SplitRatio) SplitExamples (SplitRatio, Examples, GrowingSet, PruningSet) T heory = SeparateAndConquer (GrowingSet) loop NewTheory = SimplifyTheory (Theory,PruningSet) if Accuracy (NewTheory,PruningSet) &lt; Accuracy (Theory,PruningSet) exit loop Theory = NewTheory return (Theory) complexity of (n <p> Thus both algorithms are significantly faster than the initial overfitting phase of post-pruning algorithms which typically has a time complexity of (n 2 log n) <ref> (Cohen 1993) </ref> (see also the first column of table 2). Asymptotically I 2 -REP even seems to be a little more efficient than I-REP, as can be expected from the fact that it does not have to learn overfitting clauses. However, the presented results do not allow this conclusion. <p> However, the presented results do not allow this conclusion. Our results also confirm that the pruning phase is the most expensive part of REP with a time complexity of (n 4 ). However, in <ref> (Cohen 1993) </ref>, where this problem was discussed for the first time, an efficient alternative was suggested 9 Table 2: Log-log analysis of the run-times on noisy KRK data. <p> Introduction of new variables was not allowed. For all 1 Similar experiments reported in (Furnkranz 1994a) confirm the result of (Cameron-Jones 1994) that the asymptotic time complexity of this top-down approach to pruning is still above the complexity of the initial overfitting phase contrary to a claim in <ref> (Cohen 1993) </ref>. However, in absolute terms the pruning costs of the top-down version were always neglible compared to the costs of initial rule growing, while the opposite was true for the bottom-up pruning used in REP. Table 3: Results in some propositional domains. Breast Cancer Accuracy Stnd. Dev.
Reference: <author> Dzeroski, S. and I. </author> <title> Bratko (1992). Handling noise in Inductive Logic Programming. </title> <booktitle> In Proceedings of the International Workshop on Inductive Logic Programming, </booktitle> <address> Tokyo, Japan. </address>
Reference-contexts: 1 Introduction Most rule learning algorithms deal with noise in the data during learning, i.e. they employ pre-pruning. In relational learning systems such as Foil (Quinlan and Cameron-Jones 1993), mFoil <ref> (Dzeroski and Bratko 1992) </ref>, or Fossil (Furnkranz 1994b) pre-pruning is commonly used in the form of so-called stopping criteria. An alternative way for dealing with noise | post-pruning | is to first learn a theory that overfits the data and then prune this theory to an appropriate level of generality.
Reference: <author> Furnkranz, J. </author> <year> (1994). </year> <title> A comparison of pruning methods for relational concept learning. </title> <booktitle> In Proceedings of the AAAI-94 Workshop on Knowledge Discovery in Databases, </booktitle> <pages> pp. 371-382. </pages> <note> 13 Furnkranz, J. </note> <year> (1994a). </year> <title> Efficient Pruning Methods for Relational Learning. </title> <publisher> Ph. </publisher>
Reference-contexts: They may change the evaluation of the candidate literals in subsequent learning and thus the "correct" literals might not be selected. A wrong choice of a literal cannot be undone by pruning. 3 I-REP Incremental Reduced Error Pruning (I-REP) <ref> (Furnkranz and Widmer 1994) </ref> was motivated by the observation that REP is incompatible with the separate-and-conquer learning strategy as we have discussed in section 2. <p> We have tabulated the slopes for adjacent training set sizes in table 2. The main result for our study is that I-REP and I 2 -REP both have a sub-quadratic time complexity in noisy domains. This is consistent with the conjecture of <ref> (Furnkranz and Widmer 1994) </ref>, where we estimated I-REP to have a time complexity of O (n log 2 n) on random data. <p> I-REP is the fastest algorithm in all tested domains except for the noise-free Mushroom domain. Here algorithms that do not prune at all are able to achieve an accuracy of 100% (see <ref> (Furnkranz 1994) </ref>). REP's post-pruning phase therefore does not change much on the theories produced by the initial overfitting phase and therefore is almost costless. <p> Doing so results in considerable gains in efficiency (both alternatives are significantly faster than REP's initial rule growing phase alone) and may also yield better results in terms of accuracy by avoiding the above 2 In (Furnkranz 1994a) and <ref> (Furnkranz 1994) </ref> we grouped the nine domains into three sets: In the first three domains post-pruning significantly improved upon the result from the initial overfitting phase, in the second three domains it did not change much and in the last three domains it worsens the result.
Reference: <author> D. </author> <type> thesis, </type> <institution> Vienna University of Technology. </institution>
Reference: <author> Furnkranz, J. </author> <year> (1994b). </year> <title> Fossil: A robust relational learner. </title> <booktitle> In Proceedings of the European Conference on Machine Learning, Catania, Italy, </booktitle> <pages> pp. 122-137. </pages> <publisher> Springer-Verlag. </publisher>
Reference-contexts: 1 Introduction Most rule learning algorithms deal with noise in the data during learning, i.e. they employ pre-pruning. In relational learning systems such as Foil (Quinlan and Cameron-Jones 1993), mFoil (Dzeroski and Bratko 1992), or Fossil <ref> (Furnkranz 1994b) </ref> pre-pruning is commonly used in the form of so-called stopping criteria. An alternative way for dealing with noise | post-pruning | is to first learn a theory that overfits the data and then prune this theory to an appropriate level of generality.
Reference: <author> Furnkranz, J. </author> <year> (1994). </year> <title> Pruning methods for rule learning algorithms. </title> <booktitle> In Proceedings of the 4th International Workshop on Inductive Logic Programming, Number 237 in GMD-Studien, </booktitle> <pages> pp. 321-336. </pages>
Reference-contexts: They may change the evaluation of the candidate literals in subsequent learning and thus the "correct" literals might not be selected. A wrong choice of a literal cannot be undone by pruning. 3 I-REP Incremental Reduced Error Pruning (I-REP) <ref> (Furnkranz and Widmer 1994) </ref> was motivated by the observation that REP is incompatible with the separate-and-conquer learning strategy as we have discussed in section 2. <p> We have tabulated the slopes for adjacent training set sizes in table 2. The main result for our study is that I-REP and I 2 -REP both have a sub-quadratic time complexity in noisy domains. This is consistent with the conjecture of <ref> (Furnkranz and Widmer 1994) </ref>, where we estimated I-REP to have a time complexity of O (n log 2 n) on random data. <p> I-REP is the fastest algorithm in all tested domains except for the noise-free Mushroom domain. Here algorithms that do not prune at all are able to achieve an accuracy of 100% (see <ref> (Furnkranz 1994) </ref>). REP's post-pruning phase therefore does not change much on the theories produced by the initial overfitting phase and therefore is almost costless. <p> Doing so results in considerable gains in efficiency (both alternatives are significantly faster than REP's initial rule growing phase alone) and may also yield better results in terms of accuracy by avoiding the above 2 In (Furnkranz 1994a) and <ref> (Furnkranz 1994) </ref> we grouped the nine domains into three sets: In the first three domains post-pruning significantly improved upon the result from the initial overfitting phase, in the second three domains it did not change much and in the last three domains it worsens the result.
Reference: <author> Furnkranz, J. and G. </author> <title> Widmer (1994). Incremental Reduced Error Pruning. </title> <booktitle> In Proceedings of the 11th International Conference on Machine Learning, </booktitle> <address> New Brunswick, NJ, </address> <pages> pp. 70-77. </pages>
Reference-contexts: They may change the evaluation of the candidate literals in subsequent learning and thus the "correct" literals might not be selected. A wrong choice of a literal cannot be undone by pruning. 3 I-REP Incremental Reduced Error Pruning (I-REP) <ref> (Furnkranz and Widmer 1994) </ref> was motivated by the observation that REP is incompatible with the separate-and-conquer learning strategy as we have discussed in section 2. <p> We have tabulated the slopes for adjacent training set sizes in table 2. The main result for our study is that I-REP and I 2 -REP both have a sub-quadratic time complexity in noisy domains. This is consistent with the conjecture of <ref> (Furnkranz and Widmer 1994) </ref>, where we estimated I-REP to have a time complexity of O (n log 2 n) on random data. <p> I-REP is the fastest algorithm in all tested domains except for the noise-free Mushroom domain. Here algorithms that do not prune at all are able to achieve an accuracy of 100% (see <ref> (Furnkranz 1994) </ref>). REP's post-pruning phase therefore does not change much on the theories produced by the initial overfitting phase and therefore is almost costless. <p> Doing so results in considerable gains in efficiency (both alternatives are significantly faster than REP's initial rule growing phase alone) and may also yield better results in terms of accuracy by avoiding the above 2 In (Furnkranz 1994a) and <ref> (Furnkranz 1994) </ref> we grouped the nine domains into three sets: In the first three domains post-pruning significantly improved upon the result from the initial overfitting phase, in the second three domains it did not change much and in the last three domains it worsens the result.
Reference: <author> Holte, R. C. </author> <year> (1993). </year> <title> Very simple classification rules perform well on most commonly used datasets. </title> <booktitle> Machine Learning 11, </booktitle> <pages> 63-91. </pages>
Reference-contexts: In this case our algorithms behave very similar to the CN2 rule induction system (Clark and Niblett 1989; Clark and Boswell 1991). The appendix of <ref> (Holte 1993) </ref> gives a summary of the results achieved by various algorithms on some of the most commonly used data sets of the UCI repository and a short description of these sets. We selected 9 of them for our experiments. <p> In the Lymphography data set we removed the 6 examples for the classes "normal find" and "fibrosis" in order to get a 2-class problem. All other data were used as described in <ref> (Holte 1993) </ref>. In all datasets the background knowledge consisted of &lt; and = relations with one variable and one constant argument. Wherever appropriate, comparisons between two different variables of the same data type were allowed as well (e.g. in the Vote domains). Introduction of new variables was not allowed. <p> Dev. Range CPU secs. REP 99.97 0.05 0.15 763.82 I-REP (1=2) 99.96 0.05 0.15 856.73 11 data sets the task was to learn a definition for the minority class. All experiments followed the setup used in <ref> (Holte 1993) </ref>, i.e. the algorithms were trained on 2=3 of the data and tested on the remaining 1=3. However, only 10 runs were performed for each algorithm on each data set.
Reference: <author> Mittenecker, E. </author> <year> (1977). </year> <institution> Planung und statistische Auswertung von Experimenten (8th ed.). Vienna, Austria: Verlag Franz Deuticke. </institution> <note> In German. </note>
Reference-contexts: The range information was used for a simple significance test which can be used to quickly determine significant differences between the averages for small (n &lt; 20) sample sizes <ref> (Mittenecker 1977) </ref>. For n = 10 the value of L = 1 2 R 1 +R 2 has to be &gt; 0:152 for a significance level of 5% and &gt; 0:210 for a significance level of 1%, where i are averages and R i are ranges.
Reference: <author> Pagallo, G. and D. </author> <title> Haussler (1990). Boolean feature discovery in empirical learning. </title> <booktitle> Machine Learning 5, </booktitle> <pages> 71-99. </pages>
Reference: <author> Quinlan, J. R. </author> <year> (1986). </year> <title> Induction of decision trees. </title> <booktitle> Machine Learning 1, </booktitle> <pages> 81-106. </pages>
Reference-contexts: Furnkranz and Widmer (1994) point out another problem with REP that is caused by the differences between the divide-and-conquer approach used for decision tree learning and the separate-and-conquer strategy commonly used for rule learning. The decision tree learning algorithms of the TDIDT family <ref> (Quinlan 1986) </ref> all use a divide-and-conquer strategy. After having selected an appropriate test for the root note of the decision tree, the algorithm divides training set into disjoint subsets, each one representing a possible outcome of the chosen test.
Reference: <author> Quinlan, J. R. </author> <year> (1987). </year> <title> Simplifying decision trees. </title> <journal> International Journal of Man-Machine Studies 27, </journal> <pages> 221-234. </pages>
Reference-contexts: An alternative way for dealing with noise | post-pruning | is to first learn a theory that overfits the data and then prune this theory to an appropriate level of generality. The most common post-pruning algorithm, Reduced Error Pruning (REP), has been adapted from propositional decision tree learning <ref> (Quinlan 1987) </ref> to relational rule learning (Brunk and Pazzani 1991). However, this adaptation has several shortcomings. In particular we will argue in section 2 that post-pruning complete theories is incompatible with the commonly used separate-and-conquer rule learning strategy. <p> In section 5 we will report some experiments in relational and propositional domains which show that these methods can improve the learning process in terms of speed and accuracy. 2 REP Reduced Error Pruning (REP) was originally proposed in <ref> (Quinlan 1987) </ref> as a method for post-pruning decision trees. Pagallo and Haussler (1990) adapted it for learning decision lists. A version that can be used for relational learning was then introduced in (Brunk and Pazzani 1991). The basic algorithm of this version is depicted in Fig. 1.
Reference: <author> Quinlan, J. R. </author> <year> (1990). </year> <title> Learning logical definitions from relations. </title> <booktitle> Machine Learning 5, </booktitle> <pages> 239-266. </pages>
Reference-contexts: and a pruning set according to some user-specified ratio, a concept description that covers all of the positive and none of the negative examples of the growing set is learned with a separate-and-conquer rule learning algorithm like the propositional learner CN2 (Clark and Niblett 1989) or the relational learner Foil <ref> (Quinlan 1990) </ref>. This intermediate theory is then simplified by deleting literals and clauses until any further deletion would lead to a decrease of accuracy on the pruning set.
Reference: <author> Quinlan, J. R. and R. M. </author> <title> Cameron-Jones (1993). FOIL: A midterm report. </title> <booktitle> In Proceedings of the European Conference on Machine Learning, </booktitle> <address> Vienna, Austria, </address> <pages> pp. 3-20. </pages>
Reference-contexts: 1 Introduction Most rule learning algorithms deal with noise in the data during learning, i.e. they employ pre-pruning. In relational learning systems such as Foil <ref> (Quinlan and Cameron-Jones 1993) </ref>, mFoil (Dzeroski and Bratko 1992), or Fossil (Furnkranz 1994b) pre-pruning is commonly used in the form of so-called stopping criteria.
Reference: <author> Weiss, S. M. and N. </author> <title> Indurkhya (1994). Small sample decision tree pruning. </title> <booktitle> In Proceedings of the 11th Conference on Machine Learning, </booktitle> <institution> Rutgers University, </institution> <address> New Brunswick, NJ, </address> <pages> pp. 335-342. 14 </pages>
Reference-contexts: Besides, I 2 -REP's procedure for selecting a literal is very similar to 2-fold cross-validation which has recently been shown to be a reliable procedure for comparing classifiers, in particular at low training set sizes <ref> (Weiss and Indurkhya 1994) </ref>.
References-found: 19

