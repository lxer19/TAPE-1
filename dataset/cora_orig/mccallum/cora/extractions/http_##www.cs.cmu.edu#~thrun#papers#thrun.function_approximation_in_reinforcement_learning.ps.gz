URL: http://www.cs.cmu.edu/~thrun/papers/thrun.function_approximation_in_reinforcement_learning.ps.gz
Refering-URL: http://www.cs.cmu.edu/~thrun/papers/full.html
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Email: thrun@cs.uni-bonn.de schwartz@cs.stanford.edu  
Title: Issues in Using Function Approximation for Reinforcement Learning  
Author: Sebastian Thrun Anton Schwartz 
Affiliation: Institut fur Informatik III Dept. of Computer Science Universitat Bonn Stanford University  
Date: Dec. 1993  
Address: Hillsdale, NJ,  Romerstr. 164, D-53225 Bonn, Germany Stanford, CA 94305  
Note: To appear in: Proceedings of the Fourth Connectionist Models Summer School Lawrence Erlbaum Publisher,  
Abstract: Reinforcement learning techniques address the problem of learning to select actions in unknown, dynamic environments. It is widely acknowledged that to be of use in complex domains, reinforcement learning techniques must be combined with generalizing function approximation methods such as artificial neural networks. Little, however, is understood about the theoretical properties of such combinations, and many researchers have encountered failures in practice. In this paper we identify a prime source of such failuresnamely, a systematic overestimation of utility values. Using Watkins' Q-Learning [18] as an example, we give a theoretical account of the phenomenon, deriving conditions under which one may expected it to cause learning to fail. Employing some of the most popular function approximators, we present experimental results which support the theoretical findings. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> A. G. Barto, S. J. Bradtke, and S. P. Singh, </author> <title> Learning to act using real-time dynamic programming, </title> <journal> Artificial Intelligence, </journal> <note> 1993. (to appear). </note>
Reference-contexts: 1 Introduction Reinforcement learning methods <ref> [1, 16, 18] </ref> address the problem of learning, through experimentation, to choose actions so as to maximize one's productivity in unknown, dynamic environments. <p> In recent years, various theoretical results have been obtained that characterize the convergence properties of reinforcement learning algorithms. Strong stochastic convergence has been shown for a class of learning algorithms including Q-Learning, the most frequently used reinforcement learning technique <ref> [1, 6, 18, 19] </ref>. Others [7, 20] have characterized the convergence speed in terms of the size and other key characteristics of the domain at hand.
Reference: [2] <author> J. A. Boyan, </author> <title> Modular neural networks for learning context-dependent game strategies, </title> <type> Master's thesis, </type> <institution> University of Cambridge, UK, </institution> <month> August </month> <year> 1992. </year>
Reference-contexts: Generalization replaces costly training experience. Indeed, generalizing approximation techniques such as artificial neural networks and instance-based methods have been used in practice with some remarkable success in domains such as game playing <ref> [17, 2] </ref> and robotics [5, 8, 10]. For example, Tesauro [17] reports a backgammon computer program that reaches grand-master level of play, which has been constructed using a combination of reinforcement learning techniques and artificial neural networks.
Reference: [3] <author> S. J. Bradtke, </author> <title> Reinforcement learning applied to linear quadratic regulation, </title> <booktitle> in Advances in Neural Information Processing Systems 5 S. </booktitle> <editor> J. Hanson, J. Cowan, and C. L. Giles, eds., </editor> <address> San Mateo, CA, </address> <pages> pp. 295-302, </pages> <publisher> Morgan Kaufmann, </publisher> <year> 1993. </year>
Reference-contexts: Despite these encouraging empirical results, little is known about the general implications of using function approximation in reinforcement learning. Recently, Bradtke <ref> [3] </ref> has shown the convergence of a particular policy iteration algorithm when combined with a quadratic function approximator. 1 To our knowledge this is the only convergence proof for a reinforcement learning method using a generalizing function approximator to date. <p> In other cases, however, failure was observed even though the function approximator at hand was able to represent a suitable value function, and thus a near-optimal policy could have been learned <ref> [3, 9] </ref>. In this paper we will focus our attention primarily on the latter cases. Obviously, there are inherent difficulties in combining function approximation and reinforcement learning techniques that do not exist for each component in isolation.
Reference: [4] <author> D. Chapman and L. P. Kaelbling, </author> <title> Input generalization in delayed reinforcement learning: an algorithm and performance comparisons, </title> <booktitle> in Proceedings of IJCAI-91, </booktitle> <address> Darling Habour, Sydney, Australia, </address> <institution> IJCAI, Inc., </institution> <year> 1991. </year>
Reference-contexts: Singh and Yee [14] proposed a theoretical justification for using certain error-bounded classes of function approximators in the context of reinforcement learning. 2 Others, however, report failure in applying function approximators such as the Backpropagation algorithm <ref> [4, 8, 9] </ref>. In some cases learning failed since the function approximator at hand was not capable of representing reasonable value functions at all [13].
Reference: [5] <author> V. Gullapalli, </author> <title> Reinforcement Learning and its Application to Control. </title> <type> PhD thesis, </type> <institution> Department of Computer and Information Science, University of Massachusetts, </institution> <year> 1992. </year>
Reference-contexts: Generalization replaces costly training experience. Indeed, generalizing approximation techniques such as artificial neural networks and instance-based methods have been used in practice with some remarkable success in domains such as game playing [17, 2] and robotics <ref> [5, 8, 10] </ref>. For example, Tesauro [17] reports a backgammon computer program that reaches grand-master level of play, which has been constructed using a combination of reinforcement learning techniques and artificial neural networks.
Reference: [6] <author> T. Jaakkola, M. I. Jordan, and S. P. Singh, </author> <title> On the convergence of stochastic iterative dynamic programming algorithms, </title> <type> Tech. Rep. 9307, </type> <institution> Department of Brain and Cognitive Sciences, Massachusetts Institut of Technology, </institution> <month> July </month> <year> 1993. </year>
Reference-contexts: In recent years, various theoretical results have been obtained that characterize the convergence properties of reinforcement learning algorithms. Strong stochastic convergence has been shown for a class of learning algorithms including Q-Learning, the most frequently used reinforcement learning technique <ref> [1, 6, 18, 19] </ref>. Others [7, 20] have characterized the convergence speed in terms of the size and other key characteristics of the domain at hand.
Reference: [7] <author> S. Koenig and R. G. Simmons, </author> <title> Complexity analysis of real-time reinforcement learning applied to finding shortest paths in deterministic domains, </title> <type> Tech. Rep. </type> <institution> CMU-CS-93-106, Carnegie Mellon University, </institution> <month> December </month> <year> 1992. </year>
Reference-contexts: In recent years, various theoretical results have been obtained that characterize the convergence properties of reinforcement learning algorithms. Strong stochastic convergence has been shown for a class of learning algorithms including Q-Learning, the most frequently used reinforcement learning technique [1, 6, 18, 19]. Others <ref> [7, 20] </ref> have characterized the convergence speed in terms of the size and other key characteristics of the domain at hand.
Reference: [8] <author> L.-J. Lin, </author> <title> Self-supervised Learning by Reinforcement and Artificial Neural Networks. </title> <type> PhD thesis, </type> <institution> Carnegie Mellon University, School of Computer Science, </institution> <address> Pittsburgh, PA, </address> <year> 1992. </year>
Reference-contexts: Generalization replaces costly training experience. Indeed, generalizing approximation techniques such as artificial neural networks and instance-based methods have been used in practice with some remarkable success in domains such as game playing [17, 2] and robotics <ref> [5, 8, 10] </ref>. For example, Tesauro [17] reports a backgammon computer program that reaches grand-master level of play, which has been constructed using a combination of reinforcement learning techniques and artificial neural networks. <p> Singh and Yee [14] proposed a theoretical justification for using certain error-bounded classes of function approximators in the context of reinforcement learning. 2 Others, however, report failure in applying function approximators such as the Backpropagation algorithm <ref> [4, 8, 9] </ref>. In some cases learning failed since the function approximator at hand was not capable of representing reasonable value functions at all [13]. <p> In all experiments the Q-functions for the different actions were represented by separate function approximators. In order to exploit the training information maximally, we used an off-line replay technique similar to that reported in <ref> [8] </ref>. Parameters were optimized individually for the different approximation techniques.
Reference: [9] <author> T. M. Mitchell and S. B. Thrun, </author> <title> Explanation-based neural network learning for robot control, </title> <booktitle> in Advances in Neural Information Processing Systems 5 S. </booktitle> <editor> J. Hanson, J. Cowan, and C. L. Giles, eds., </editor> <address> San Mateo, CA, </address> <pages> pp. 287-294, </pages> <publisher> Morgan Kaufmann, </publisher> <year> 1993. </year>
Reference-contexts: Singh and Yee [14] proposed a theoretical justification for using certain error-bounded classes of function approximators in the context of reinforcement learning. 2 Others, however, report failure in applying function approximators such as the Backpropagation algorithm <ref> [4, 8, 9] </ref>. In some cases learning failed since the function approximator at hand was not capable of representing reasonable value functions at all [13]. <p> In other cases, however, failure was observed even though the function approximator at hand was able to represent a suitable value function, and thus a near-optimal policy could have been learned <ref> [3, 9] </ref>. In this paper we will focus our attention primarily on the latter cases. Obviously, there are inherent difficulties in combining function approximation and reinforcement learning techniques that do not exist for each component in isolation. <p> All curves are averaged over eight runs. Note that learning fails completely if fl :98. (a) (b) learning and (b) plain Backpropagation (sigmoidal transfer functions). 4 Empirical Results The theoretical results were empirically validated using the simulated robot environment depicted in Fig. 2a (cf. <ref> [9] </ref>). The task is to learn a policy that carries the robot agent from arbitrary starting positions to the goal. The robot can sense the state it is in using sensors which measure the distance and orientation of both the goal and the obstacle. <p> It has five actions to chose from, as depicted in Fig. 2b. Positive reward +1 is received upon entering the goal region, while collisions are penalized by a reward of 1. Six series of simulations were performed using (a) instance-based learning with local polynomial approximation, as described in <ref> [9, 11] </ref>, and Backpropagation learning [12] with (b) standard sigmoidal, (c) sinusoidal and (d) radial basis transfer functions. In two further experiments we used radial basis functions for the hidden units only, together with (e) sigmoidal and (f) linear output units.
Reference: [10] <author> A. W. Moore, </author> <title> Efficient Memory-based Learning for Robot Control. </title> <type> PhD thesis, </type> <institution> Trinity Hall, University of Cambridge, </institution> <address> England, </address> <year> 1990. </year>
Reference-contexts: Generalization replaces costly training experience. Indeed, generalizing approximation techniques such as artificial neural networks and instance-based methods have been used in practice with some remarkable success in domains such as game playing [17, 2] and robotics <ref> [5, 8, 10] </ref>. For example, Tesauro [17] reports a backgammon computer program that reaches grand-master level of play, which has been constructed using a combination of reinforcement learning techniques and artificial neural networks.
Reference: [11] <author> A. W. Moore and C. G. Atkeson, </author> <title> Memory-based function approximators for learning control, </title> <publisher> MIT AI-Lab Memo, </publisher> <month> July </month> <year> 1992. </year>
Reference-contexts: It has five actions to chose from, as depicted in Fig. 2b. Positive reward +1 is received upon entering the goal region, while collisions are penalized by a reward of 1. Six series of simulations were performed using (a) instance-based learning with local polynomial approximation, as described in <ref> [9, 11] </ref>, and Backpropagation learning [12] with (b) standard sigmoidal, (c) sinusoidal and (d) radial basis transfer functions. In two further experiments we used radial basis functions for the hidden units only, together with (e) sigmoidal and (f) linear output units.
Reference: [12] <author> D. E. Rumelhart, G. E. Hinton, and R. J. Williams, </author> <title> Learning internal representations by error propagation, </title> <booktitle> in Parallel Distributed Processing. </booktitle> <volume> Vol. </volume> <editor> I + II D. E. Rumelhart and J. L. McClelland, eds., </editor> <publisher> MIT Press, </publisher> <year> 1986. </year>
Reference-contexts: Positive reward +1 is received upon entering the goal region, while collisions are penalized by a reward of 1. Six series of simulations were performed using (a) instance-based learning with local polynomial approximation, as described in [9, 11], and Backpropagation learning <ref> [12] </ref> with (b) standard sigmoidal, (c) sinusoidal and (d) radial basis transfer functions. In two further experiments we used radial basis functions for the hidden units only, together with (e) sigmoidal and (f) linear output units.
Reference: [13] <author> P. Sabes, </author> <title> Q-learning with a basis function representation for the Q-values, same volume. </title>
Reference-contexts: In some cases learning failed since the function approximator at hand was not capable of representing reasonable value functions at all <ref> [13] </ref>. In other cases, however, failure was observed even though the function approximator at hand was able to represent a suitable value function, and thus a near-optimal policy could have been learned [3, 9]. In this paper we will focus our attention primarily on the latter cases.
Reference: [14] <author> S. P. Singh and R. C. Yee, </author> <title> An upper bound on the loss from approximate optimal-value functions, </title> <note> (submitted for publication), </note> <year> 1993. </year>
Reference-contexts: Recently, Bradtke [3] has shown the convergence of a particular policy iteration algorithm when combined with a quadratic function approximator. 1 To our knowledge this is the only convergence proof for a reinforcement learning method using a generalizing function approximator to date. Singh and Yee <ref> [14] </ref> proposed a theoretical justification for using certain error-bounded classes of function approximators in the context of reinforcement learning. 2 Others, however, report failure in applying function approximators such as the Backpropagation algorithm [4, 8, 9].
Reference: [15] <author> R. S. Sutton, </author> <title> Learning to predict by the methods of temporal differences, </title> <journal> Machine Learning, </journal> <volume> vol. 3, </volume> <year> 1988. </year>
Reference-contexts: Approximators with bounded memory are often likely to have some minimum residual error, unless specific information about the environment is available that ensures that the error can go to zero. * Reinforcement learning techniques based on T D () <ref> [15] </ref> update values according to a multitude of values and observed rewards.
Reference: [16] <author> R. S. Sutton, </author> <title> Temporal Credit Assignment in Reinforcement Learning. </title> <type> PhD thesis, </type> <institution> Department of Computer and Information Science, University of Massachusetts, </institution> <year> 1984. </year>
Reference-contexts: 1 Introduction Reinforcement learning methods <ref> [1, 16, 18] </ref> address the problem of learning, through experimentation, to choose actions so as to maximize one's productivity in unknown, dynamic environments.
Reference: [17] <author> G. J. Tesauro, </author> <title> Practical issues in temporal difference learning, </title> <journal> Machine Learning Journal, </journal> <volume> vol. 8, </volume> <year> 1992. </year>
Reference-contexts: Generalization replaces costly training experience. Indeed, generalizing approximation techniques such as artificial neural networks and instance-based methods have been used in practice with some remarkable success in domains such as game playing <ref> [17, 2] </ref> and robotics [5, 8, 10]. For example, Tesauro [17] reports a backgammon computer program that reaches grand-master level of play, which has been constructed using a combination of reinforcement learning techniques and artificial neural networks. <p> Generalization replaces costly training experience. Indeed, generalizing approximation techniques such as artificial neural networks and instance-based methods have been used in practice with some remarkable success in domains such as game playing [17, 2] and robotics [5, 8, 10]. For example, Tesauro <ref> [17] </ref> reports a backgammon computer program that reaches grand-master level of play, which has been constructed using a combination of reinforcement learning techniques and artificial neural networks. Despite these encouraging empirical results, little is known about the general implications of using function approximation in reinforcement learning.
Reference: [18] <author> C. J. C. H. Watkins, </author> <title> Learning from Delayed Rewards. </title> <type> PhD thesis, </type> <institution> King's College, </institution> <address> Cambridge, England, </address> <year> 1989. </year>
Reference-contexts: 1 Introduction Reinforcement learning methods <ref> [1, 16, 18] </ref> address the problem of learning, through experimentation, to choose actions so as to maximize one's productivity in unknown, dynamic environments. <p> In recent years, various theoretical results have been obtained that characterize the convergence properties of reinforcement learning algorithms. Strong stochastic convergence has been shown for a class of learning algorithms including Q-Learning, the most frequently used reinforcement learning technique <ref> [1, 6, 18, 19] </ref>. Others [7, 20] have characterized the convergence speed in terms of the size and other key characteristics of the domain at hand. <p> Throughout this analysis we use Watkins' Q-Learning as an example <ref> [18] </ref>. Q-Learning is a technique for learning optimal policies in Markovian sequential decision tasks. It does this by incrementally learning a function Q (s; a) which it uses to evaluate the utility of performing action a in state s. <p> It has been shown that repeated application of this update equation eventually yields Q-values that give rise to a policy which maximizes the expected cumulative discounted reward <ref> [18] </ref>. However, such results only apply when the Q-values are stored precisely, e.g., by a look-up table. Assume, instead, that Q is represented by a function approximator that induces some noise on the estimates of Q.
Reference: [19] <author> C. J. Watkins and P. </author> <title> Dayan, </title> <journal> Q-learning, Machine Learning Journal, </journal> <volume> vol. 8, </volume> <pages> pp. 279-292, </pages> <year> 1992. </year>
Reference-contexts: In recent years, various theoretical results have been obtained that characterize the convergence properties of reinforcement learning algorithms. Strong stochastic convergence has been shown for a class of learning algorithms including Q-Learning, the most frequently used reinforcement learning technique <ref> [1, 6, 18, 19] </ref>. Others [7, 20] have characterized the convergence speed in terms of the size and other key characteristics of the domain at hand.
Reference: [20] <author> S. D. Whitehead, </author> <title> A study of cooperative mechanisms for faster reinforcement learning, </title> <type> Tech. Rep. 365, </type> <institution> University of Rochester, Computer Science Department, Rochester, </institution> <address> NY, </address> <month> March </month> <year> 1991. </year>
Reference-contexts: In recent years, various theoretical results have been obtained that characterize the convergence properties of reinforcement learning algorithms. Strong stochastic convergence has been shown for a class of learning algorithms including Q-Learning, the most frequently used reinforcement learning technique [1, 6, 18, 19]. Others <ref> [7, 20] </ref> have characterized the convergence speed in terms of the size and other key characteristics of the domain at hand.
References-found: 20

