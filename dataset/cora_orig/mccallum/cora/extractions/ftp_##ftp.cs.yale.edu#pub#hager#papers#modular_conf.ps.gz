URL: ftp://ftp.cs.yale.edu/pub/hager/papers/modular_conf.ps.gz
Refering-URL: http://www.cs.yale.edu/users/hager/papers.html
Root-URL: http://www.cs.yale.edu
Title: Servomatic: A Modular System for Robust Positioning Using Stereo Visual Servoing  
Author: Kentaro Toyama, Gregory D. Hager, and Jonathan Wang 
Address: New Haven, CT 06520-8285  
Affiliation: Department of Computer Science, Yale University  
Abstract: We introduce Servomatic, a modular system for robot motion control based on calibration-insensitive visual servoing. A small number of generic motion control operations referred to as primitive skills use stereo visual feedback to enforce a specific task-space kinematic constraint between a robot end-effector and a set of target features. Primitive skills are able to position with an accuracy that is independent of errors in hand-eye calibration and are easily combined to form more complex kinematic constraints as required by different applications. The system has been applied to a number of example problems, showing that modular, high precision, vision-based motion control is easily achieved with off-the-shelf hardware. Our continuing goal is to develop a system where low-level robot control ceases to be a concern to higher-level robotics researchers. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> P.K. Allen, B. Yoshimi, and A. Timcenko. </author> <title> Hand-eye coordination for robotics tracking and grasping. </title> <editor> In K. Hashimoto, editor, </editor> <booktitle> Visual Servoing, </booktitle> <pages> pages 33-70. </pages> <publisher> World Scientific, </publisher> <year> 1994. </year>
Reference-contexts: Visual servoing systems may employ a single camera, typically mounted on the arm itself [2, 6, 20, 21], or they may use a stereo arrangement <ref> [1, 16, 17, 22] </ref>. Stereo systems must deal with more input data but can also offer accurate 3-D information. This article discusses a free-standing stereo camera arrangement, although with minor modifications, the same formulation could be used for different configurations of more than one camera.
Reference: [2] <author> A. Castano and S. A. Hutchinson. </author> <title> Visual compliance: </title> <journal> Task-directed visual servo control. IEEE Trans. on Rob. and Autom., </journal> <volume> 10(3) </volume> <pages> 334-342, </pages> <month> June </month> <year> 1994. </year>
Reference-contexts: Visual servoing systems may employ a single camera, typically mounted on the arm itself <ref> [2, 6, 20, 21] </ref>, or they may use a stereo arrangement [1, 16, 17, 22]. Stereo systems must deal with more input data but can also offer accurate 3-D information.
Reference: [3] <author> F. Chaumette, P. Rives, and B. Espiau. </author> <title> Classification and realization of the different vision-based tasks. </title> <editor> In K. Hashimoto, editor, </editor> <booktitle> Visual Servoing, </booktitle> <pages> pages 199-228. </pages> <publisher> World Scientific, </publisher> <year> 1994. </year>
Reference-contexts: We describe an ECL system that uses a globally valid perspective model. Finally, most visual servoing research has concentrated on developing solutions to specific isolated problems. An exception is found in <ref> [3] </ref> where it is noted that it would be possible to compile a "library" of canonical visual tasks. In this paper, we provide the building blocks for such a library.
Reference: [4] <author> W.Z. Chen, U.A. Korde, </author> <title> and S.B. Skaar. Position control experiments using vision. </title> <journal> Int. J. Robot. Res., </journal> <volume> 13(3) </volume> <pages> 199-208, </pages> <month> June </month> <year> 1994. </year>
Reference-contexts: The more calibration-dependent endpoint-open-loop (EOL) controllers do not observe the manipulator. The difficulty with many ECL systems is that they use approximations to perspective transformations which are only locally valid or employ complex adaptive arrangements that require burdensome calculations <ref> [4, 14, 15, 25] </ref>. We describe an ECL system that uses a globally valid perspective model. Finally, most visual servoing research has concentrated on developing solutions to specific isolated problems.
Reference: [5] <author> P. I. Corke. </author> <title> Visual control of robot manipulators| a review. </title> <editor> In K. Hashimoto, editor, </editor> <booktitle> Visual Servoing, </booktitle> <pages> pages 1-32. </pages> <publisher> World Scientific, </publisher> <year> 1994. </year>
Reference-contexts: systems, vision-based robotic systems are still the exception rather than the rule for several reasons: sensitivity to miscalibration of cameras, unavailability of real-time vision systems which are easy to configure, and lack of vision-based motion control modules which can be used by "non-experts." For a review of visual-servoing techniques, see <ref> [5] </ref>. We have already developed a real-time vision system, which we call "XVision," that allows users to perform fast feature tracking based on gradient edges and texture patches [13].
Reference: [6] <author> B. Espiau, F. Chaumette, and P. Rives. </author> <title> A new approach to visual servoing in robotics. </title> <journal> IEEE Trans. on Rob. and Autom., </journal> <volume> 8 </volume> <pages> 313-326, </pages> <year> 1992. </year>
Reference-contexts: Visual servoing systems may employ a single camera, typically mounted on the arm itself <ref> [2, 6, 20, 21] </ref>, or they may use a stereo arrangement [1, 16, 17, 22]. Stereo systems must deal with more input data but can also offer accurate 3-D information. <p> 1 is an estimate of J 1 and M = J b J 1 : Thus, image error is an exponentially decreasing function of time and the system is asymptotically stable 1 The degree of the task-space error function, d, is closely related to the notion of "class" defined in <ref> [6] </ref>. as long as the eigenvalues of M have strictly positive real parts [9]. Since M varies continuously with camera calibration parameters, a "slightly miscalibrated" system will remain asymptotically stable. Asymptotic stability implies that lim t!1 e ! 0, and therefore, lim t!1 E ! 0 away from singularities.
Reference: [7] <author> O.D. Faugeras. </author> <title> Three-Dimensional Computer Vision. </title> <publisher> MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1993. </year>
Reference-contexts: It is assumed that estimates of camera parameters are available. To simplify the exposition, all observed values are expressed in normalized coordinates in which values are scaled to metric units for a camera fitted with a unit focal length lens <ref> [7] </ref>. A point in three-dimensional space is written in uppercase boldface Roman letters, e:g:; P. The stereo projection of P is written p = (p 1 ; p 2 ); where p i is the projection of P in camera i.
Reference: [8] <author> A. Fox and S. Hutchinson. </author> <title> Exploiting visual constraints in the synthesis of uncertainty-tolerant motion plans. </title> <journal> IEEE Trans. on Rob. and Autom., </journal> <volume> 11 </volume> <pages> 56-71, </pages> <year> 1995. </year>
Reference-contexts: Open problems include defining a wider variety of positioning skills, developing a richer notion of skill composition, formulating of globally stable control methods, making tracking more robust, and using projective invariants to specify robot positions. Work is already underway on some of these problems <ref> [8, 10, 11, 18, 23, 24] </ref>. Acknowledgments This research was supported by ARPA grant N00014-93-1-1235, Army DURIP grant DAAH04-95-1-0058, by National Science Foundation grant IRI-9420982, and by funds provided by Yale University.
Reference: [9] <author> G. Franklin, J. Powell, and A. Emami-Naeini. </author> <title> Feedback Control of Dynamic Systems. </title> <publisher> Addison-Wesley, </publisher> <address> 2nd edition, </address> <year> 1991. </year>
Reference-contexts: Singular configurations are end-effector poses where the constraint on position defined by the image error function is locally of lower degree than that of the equivalent kinematic error function. All feedback algorithms in this article employ image errors in proportional control arrangements <ref> [9] </ref>. Define J e (x e 0 ) = @x e fi fi : (1) If all quantities are considered functions of time, _ e = J e (x e )r (2) describes the relationship between change in end-effector pose and change in the image error. <p> 1 : Thus, image error is an exponentially decreasing function of time and the system is asymptotically stable 1 The degree of the task-space error function, d, is closely related to the notion of "class" defined in [6]. as long as the eigenvalues of M have strictly positive real parts <ref> [9] </ref>. Since M varies continuously with camera calibration parameters, a "slightly miscalibrated" system will remain asymptotically stable. Asymptotic stability implies that lim t!1 e ! 0, and therefore, lim t!1 E ! 0 away from singularities. By design, the control system is guaranteed to achieve the equivalent task-space kinematic constraint.
Reference: [10] <author> G. D. Hager. </author> <title> Real-time feature tracking and projective invariance as a basis for hand-eye coordination. </title> <address> pages 533-539. </address> <publisher> IEEE Computer Society Press, </publisher> <year> 1994. </year>
Reference-contexts: Open problems include defining a wider variety of positioning skills, developing a richer notion of skill composition, formulating of globally stable control methods, making tracking more robust, and using projective invariants to specify robot positions. Work is already underway on some of these problems <ref> [8, 10, 11, 18, 23, 24] </ref>. Acknowledgments This research was supported by ARPA grant N00014-93-1-1235, Army DURIP grant DAAH04-95-1-0058, by National Science Foundation grant IRI-9420982, and by funds provided by Yale University.
Reference: [11] <author> G. D. Hager. </author> <title> Calibration-free visual control using projective invariance. </title> <booktitle> In Proceedings of the ICCV, </booktitle> <pages> pages 1009-1015, </pages> <year> 1995. </year> <note> Also available as Yale CS-RR-1046. </note>
Reference-contexts: Open problems include defining a wider variety of positioning skills, developing a richer notion of skill composition, formulating of globally stable control methods, making tracking more robust, and using projective invariants to specify robot positions. Work is already underway on some of these problems <ref> [8, 10, 11, 18, 23, 24] </ref>. Acknowledgments This research was supported by ARPA grant N00014-93-1-1235, Army DURIP grant DAAH04-95-1-0058, by National Science Foundation grant IRI-9420982, and by funds provided by Yale University.
Reference: [12] <author> G. D. Hager. </author> <title> A modular system for robust hand-eye coordination. </title> <institution> DCS RR-1074, Yale University, </institution> <address> New Haven, CT, </address> <month> June </month> <year> 1995. </year>
Reference-contexts: Estimates of a line L from l are written ^ L (l). See [13] for a more detailed exposition on point and line representation. 3 Positioning Skills Next, we define two primitive positioning operations utilizing point and line features. We refer to <ref> [12] </ref> for an in-depth description of these skills and additional regulators for line-to-point positioning and line-to-line alignment. 3.1 Point-to-Point Positioning Consider the following problem: Given a reference point P fixed with respect to a target object and a point S rigidly attached to the end-effector, develop a regulator that positions the <p> baseline are coincident in space if and only if their stereo projections are coincident we define the error function as e pp (s; p) = s p: (11) Following the formulation in Section 2.1, we can compute the Jacobian relating the error projection in the cameras to robot translational velocities <ref> [12] </ref> and develop a primitive point-to-point positioning skill. The singular set of stationing configurations is exactly the singular set of the point projection function.
Reference: [13] <author> G. D. Hager. </author> <title> The "X-vision" system: A general purpose substrate for real-time vision-based robotics. </title> <booktitle> In Proceedings of the Workshop on Vision for Robots, </booktitle> <pages> pages 56-63, </pages> <year> 1995. </year> <note> Also available as Yale CS-RR-1078. </note>
Reference-contexts: We have already developed a real-time vision system, which we call "XVision," that allows users to perform fast feature tracking based on gradient edges and texture patches <ref> [13] </ref>. The XVision system comprises several modular software components which can be combined to track objects of varying complexity, ranging from a simple line segment to human faces. <p> Since L v is not defined when l 1 and l 2 are parallel, any epipolar plane is a singular region for stereo line projection. Estimates of a line L from l are written ^ L (l). See <ref> [13] </ref> for a more detailed exposition on point and line representation. 3 Positioning Skills Next, we define two primitive positioning operations utilizing point and line features. <p> The workstation and PC are connected by an ethernet link. All image processing and visual control calculations are performed on the Sun workstation using the XVision tracking system <ref> [13] </ref>. Cartesian velocities are continually sent to the PC which converts them into coordinated joint motions using a resolved-rate controller operating at 140 Hz. Screwdriver Placement Consider Figure 1 (top) in which a visual positioning operation places a screwdriver onto a screw. <p> Nevertheless, orientation was within 2 degrees of rotation and positioning was within a few millimeters of the correct value. 5 Software Design Parallel to the implementation of the XVision system with respect to visual tracking <ref> [13] </ref>, we are developing a system which can be easily used by researchers whose concerns are high-level robot control. To this end, we are writing a software package, Servomatic, which allows users to call primitive visual-servoing skills as subroutines or as continuously running background processes. <p> The core of this structure comprises two base classes, Tracker and Controller. Objects derived from the virtual Tracker class are object trackers which use the XVision system to track various objects such as rectangles, screwdrivers, chess pieces, or satellite docking ports <ref> [13] </ref>. Once initialized, Tracker class objects are run in the background and periodically polled for specific state information which is required to perform visual servoing tasks.
Reference: [14] <author> G. D. Hager, W-C. Chang, and A. S. Morse. </author> <title> Robot hand-eye coordination based on stereo vision. </title> <journal> IEEE Control Systems Magazine, </journal> <volume> 15(1) </volume> <pages> 30-39, </pages> <month> February </month> <year> 1995. </year>
Reference-contexts: The more calibration-dependent endpoint-open-loop (EOL) controllers do not observe the manipulator. The difficulty with many ECL systems is that they use approximations to perspective transformations which are only locally valid or employ complex adaptive arrangements that require burdensome calculations <ref> [4, 14, 15, 25] </ref>. We describe an ECL system that uses a globally valid perspective model. Finally, most visual servoing research has concentrated on developing solutions to specific isolated problems.
Reference: [15] <author> N. Hollinghurst and R. Cipolla. </author> <title> Uncalibrated stereo hand eye coordination. </title> <journal> Image and Vision Computing, </journal> <volume> 12(3) </volume> <pages> 187-192, </pages> <year> 1994. </year>
Reference-contexts: The more calibration-dependent endpoint-open-loop (EOL) controllers do not observe the manipulator. The difficulty with many ECL systems is that they use approximations to perspective transformations which are only locally valid or employ complex adaptive arrangements that require burdensome calculations <ref> [4, 14, 15, 25] </ref>. We describe an ECL system that uses a globally valid perspective model. Finally, most visual servoing research has concentrated on developing solutions to specific isolated problems.
Reference: [16] <author> K. Hosoda and M. Asada. </author> <title> Versatile visual servoing without knowledge of true jacobian. </title> <address> pages 186-191. </address> <publisher> IEEE Computer Society Press, </publisher> <year> 1994. </year>
Reference-contexts: Visual servoing systems may employ a single camera, typically mounted on the arm itself [2, 6, 20, 21], or they may use a stereo arrangement <ref> [1, 16, 17, 22] </ref>. Stereo systems must deal with more input data but can also offer accurate 3-D information. This article discusses a free-standing stereo camera arrangement, although with minor modifications, the same formulation could be used for different configurations of more than one camera.
Reference: [17] <author> N. Maru, H. Kase, A. Nishikawa, and F. Miyazaki. </author> <title> Manipulator control by visual servoing with the stereo vision. </title> <address> pages 1866-1870. </address> <publisher> IEEE Computer Society Press, </publisher> <year> 1993. </year>
Reference-contexts: Visual servoing systems may employ a single camera, typically mounted on the arm itself [2, 6, 20, 21], or they may use a stereo arrangement <ref> [1, 16, 17, 22] </ref>. Stereo systems must deal with more input data but can also offer accurate 3-D information. This article discusses a free-standing stereo camera arrangement, although with minor modifications, the same formulation could be used for different configurations of more than one camera.
Reference: [18] <author> J. Mundy and A. Zisserman. </author> <title> Geometric Invariance in Computer Vision. </title> <publisher> MIT Press, </publisher> <address> Cambridge, Mass., </address> <year> 1992. </year>
Reference-contexts: Open problems include defining a wider variety of positioning skills, developing a richer notion of skill composition, formulating of globally stable control methods, making tracking more robust, and using projective invariants to specify robot positions. Work is already underway on some of these problems <ref> [8, 10, 11, 18, 23, 24] </ref>. Acknowledgments This research was supported by ARPA grant N00014-93-1-1235, Army DURIP grant DAAH04-95-1-0058, by National Science Foundation grant IRI-9420982, and by funds provided by Yale University.
Reference: [19] <author> J. R. Munkres. </author> <title> Analysis on Manifolds. </title> <publisher> Addison-Wesley, </publisher> <year> 1991. </year>
Reference-contexts: x t ) = E (C e (x e ); C t (x t )): Suppose that E 0 (x e ; x t ) = 0; x t is held fixed, and E 0 considered as a function of x e satisfies the conditions of the implicit function theorem <ref> [19] </ref>.
Reference: [20] <author> B. Nelson and P. K. Khosla. </author> <title> Increasing the tracking region of an eye-in-hand system by singularity and joint limit avoidance. </title> <address> pages 418-423, </address> <year> 1993. </year>
Reference-contexts: Visual servoing systems may employ a single camera, typically mounted on the arm itself <ref> [2, 6, 20, 21] </ref>, or they may use a stereo arrangement [1, 16, 17, 22]. Stereo systems must deal with more input data but can also offer accurate 3-D information.
Reference: [21] <author> N. Papanikolopoulos, P. Khosla, and T. Kanade. </author> <title> Visual tracking of a moving target by a camera mounted on a robot: A combination of control and vision. </title> <type> 9(1), </type> <year> 1993. </year>
Reference-contexts: Visual servoing systems may employ a single camera, typically mounted on the arm itself <ref> [2, 6, 20, 21] </ref>, or they may use a stereo arrangement [1, 16, 17, 22]. Stereo systems must deal with more input data but can also offer accurate 3-D information.
Reference: [22] <author> A.A. Rizzi and D. E. Koditschek. </author> <title> Further progress in robot juggling: </title> <booktitle> The spatial two-juggle. </booktitle> <pages> pages 919-924, </pages> <year> 1993. </year>
Reference-contexts: Visual servoing systems may employ a single camera, typically mounted on the arm itself [2, 6, 20, 21], or they may use a stereo arrangement <ref> [1, 16, 17, 22] </ref>. Stereo systems must deal with more input data but can also offer accurate 3-D information. This article discusses a free-standing stereo camera arrangement, although with minor modifications, the same formulation could be used for different configurations of more than one camera.
Reference: [23] <author> K. Tarabanis and P. Allen. </author> <title> Sensor planning in computer vision. </title> <journal> IEEE Trans. on Rob. and Autom., </journal> <volume> 11(1) </volume> <pages> 96-105, </pages> <year> 1995. </year>
Reference-contexts: Open problems include defining a wider variety of positioning skills, developing a richer notion of skill composition, formulating of globally stable control methods, making tracking more robust, and using projective invariants to specify robot positions. Work is already underway on some of these problems <ref> [8, 10, 11, 18, 23, 24] </ref>. Acknowledgments This research was supported by ARPA grant N00014-93-1-1235, Army DURIP grant DAAH04-95-1-0058, by National Science Foundation grant IRI-9420982, and by funds provided by Yale University.
Reference: [24] <author> K. Toyama and G. D. Hager. </author> <title> Tracker fusion for robustness in visual feature tracking. </title> <booktitle> In SPIE Int'l Sym. Intel. Sys. and Adv. Manufacturing, volume 2589, </booktitle> <address> Philadelphia, PA, </address> <month> October </month> <year> 1995. </year>
Reference-contexts: Open problems include defining a wider variety of positioning skills, developing a richer notion of skill composition, formulating of globally stable control methods, making tracking more robust, and using projective invariants to specify robot positions. Work is already underway on some of these problems <ref> [8, 10, 11, 18, 23, 24] </ref>. Acknowledgments This research was supported by ARPA grant N00014-93-1-1235, Army DURIP grant DAAH04-95-1-0058, by National Science Foundation grant IRI-9420982, and by funds provided by Yale University.
Reference: [25] <author> S.W. Wijesoma, D.F.H Wolfe, and R.J. Richards. </author> <title> Eye-to-hand coordination for vision-guided robot control applications. </title> <journal> Int. J. Robot. Res., </journal> <volume> 12(1) </volume> <pages> 65-78, </pages> <year> 1993. </year>
Reference-contexts: The more calibration-dependent endpoint-open-loop (EOL) controllers do not observe the manipulator. The difficulty with many ECL systems is that they use approximations to perspective transformations which are only locally valid or employ complex adaptive arrangements that require burdensome calculations <ref> [4, 14, 15, 25] </ref>. We describe an ECL system that uses a globally valid perspective model. Finally, most visual servoing research has concentrated on developing solutions to specific isolated problems.
References-found: 25

