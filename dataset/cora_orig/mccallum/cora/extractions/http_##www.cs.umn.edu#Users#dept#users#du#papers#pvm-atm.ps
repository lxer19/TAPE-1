URL: http://www.cs.umn.edu/Users/dept/users/du/papers/pvm-atm.ps
Refering-URL: http://www.cs.umn.edu/Users/dept/users/du/papers/
Root-URL: http://www.cs.umn.edu
Title: Enhanced PVM Communications over a High-Speed Local Area Network  
Author: Sheue-Ling Chang, David H.C. Du Jenwei Hsieh, Mengjou Lin, Rose P. Tsang 
Keyword: Parallel Virtual Machine (PVM), Asynchronous Transfer Mode (ATM), Application Pro gramming Interface, Distributed Network Computing.  
Address: Minneapolis, MN 55455  
Affiliation: Distributed MultiMedia Center 2 Computer Science Department University of Minnesota  
Abstract: Emulating a parallel machine via a collection of heterogenous independent hosts and a general-purpose local area network has obvious advantages such as cost-effectiveness and very large aggregate processing power and memory. However, the ability of most current general-purpose local area networks to support communication-intensive parallel applications has been questionable. Today, with the emergence of several high-speed switch-based networks, such as HIgh Performance Parallel Interface (HIPPI), Fibre Channel, and Asynchronous Transfer Mode (ATM), the possibility of networks effectively supporting communication-intensive parallel applications may soon prove a reality. For this study, we have chosen to use ATM for the underlying high-speed network infrastructure. This choice is due to the flexibility of ATM and its rapid acceptance into the local area networking community. For parallel programming support, we have chosen Parallel Virtual Machine (PVM). PVM is a parallel programming environment which allows a heterogeneous network of computers to be used as a single parallel computer. It provides the functionality, e.g., message passing library, typically found in most distributed memory parallel machines. In this paper, we present results of the performance of PVM over a local ATM network. In addition to the high-speed transport support provided via the high-speed ATM network, we re-implemented PVM using the lower level Fore Systems' ATM API instead of the BSD socket interface for which PVM was originally intended. We also implemented an end-to-end flow control scheme with a selective retransmission mechanism (for message reliability), and re-implemented a more efficient multicasting operation. Our overall results show that parallel computing on a high-speed network (ATM), as opposed to a conventional network such as Ethernet, does provide increased communication bandwidth. However, the performance improvement at the application level is still far below that which the physical medium can provide. Before applications can benefit from the full potential of high speed networks, further improvements in the hardware and software components of network I/O subsystems must be performed. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Andersen, </author> <title> T.M., Cornelius, R.S., High Performance Switching with Fiber Channel, </title> <booktitle> IEEE Proceedings of CompCon, </booktitle> <pages> pages 261-264, </pages> <year> 1992. </year>
Reference-contexts: In shared medium architectures, network capacity is shared among all the interconnected processors. Aggregate network capacity is limited to speeds between 10 Mbits/sec (Ethernet) to 100 Mbits/sec (FDDI). High speed switch-based network architectures, such as the HIgh Performance Parallel Interface (HIPPI) [18], Fibre Channel <ref> [1, 2] </ref>, and Asynchronous Transfer Mode (ATM) [5, 11, 14], feature aggregate throughputs of several gigabits/sec. Moreover each host usually has a dedicated high-speed (100 Mbits/sec or more) connection to the switch. * Scalability.
Reference: [2] <institution> ANSI X3T9.3 Fiber Channel Physical and Signalling Interface (FC-PH), </institution> <address> 4.2 edition, </address> <month> November </month> <year> 1993. </year>
Reference-contexts: In shared medium architectures, network capacity is shared among all the interconnected processors. Aggregate network capacity is limited to speeds between 10 Mbits/sec (Ethernet) to 100 Mbits/sec (FDDI). High speed switch-based network architectures, such as the HIgh Performance Parallel Interface (HIPPI) [18], Fibre Channel <ref> [1, 2] </ref>, and Asynchronous Transfer Mode (ATM) [5, 11, 14], feature aggregate throughputs of several gigabits/sec. Moreover each host usually has a dedicated high-speed (100 Mbits/sec or more) connection to the switch. * Scalability.
Reference: [3] <author> Beguelin, A., Dongarra, J., Geist, A., Manchek,R., Sunderam, V., </author> <title> A User's Guide to PVM (Parallel Virtual Machine), </title> <type> Technical Report ORNL/TM-11826, </type> <institution> Oak Ridge National Laboratory, Oak Ridge, TN, </institution> <month> July </month> <year> 1991. </year>
Reference-contexts: Even though the ATM standard was initially developed and intended to serve as an infrastructure for wide-area (telecommunications) networks, it is currently being much more rapidly adopted for local area network computing. PVM <ref> [3, 8, 16] </ref> is a software system for the development and execution of parallel applications. It allows an interconnected collection of independent heterogenous computers to appear as a single `virtual' computational resource or a single parallel machine. The independent machines may be ordinary workstations, multiprocessors, supercomputers, or specialized processors. <p> We also provide a description of the particular API, the Fore Systems' ATM API, used in our environment. 2.1 PVM: A Parallel Programming Environment PVM <ref> [3, 8, 16] </ref> provides a unified computational framework for a network of heterogenous computing resources. As mentioned before, computing resources may include workstations, multiprocessors and special-purpose processors, and the underlying network may be a conventional Ethernet, the Internet, or as we have chosen, a high-speed network, such as ATM.
Reference: [4] <author> Biagioni, E., Coope, E., Samsom, R., </author> <title> Designing a Practical ATM LAN, </title> <journal> IEEE Network, </journal> <pages> pages 32-39, </pages> <month> March </month> <year> 1993. </year>
Reference-contexts: ATM local area switches and ATM interface cards for most workstations are also readily available. Current market forces as well as ATM's wide-spread acceptance in the networking community has caused it become the most likely transport mode for high-speed local and wide-area networking <ref> [4, 12, 14, 18] </ref>. In this study, we sought to achieve high performance (e.g., low latency, high bandwidth) not only by implementing PVM on a high speed medium, such as ATM, but also by minimizing possible sources of overhead. Overhead is incurred by hardware and software components. <p> The growing availability of high-speed networks, such as ATM, may make very large virtual machines more likely and feasable. 7 2.2 ATM: the Next Generation Network ATM <ref> [4, 5, 11] </ref> is a standard developed by the networking standards community (CCITT) which specifies the network layer protocol of future broadband networks (B-ISDNs Broadband Integrated Services Digital Network). It resides above the physical layer and directly below the ATM Adaptation Layer (AAL) (see Figure 1). <p> The i960 uses local memory to manage 16 pointers to packets, and uses DMA (Direct Memory Access) to move cells out of and into host memory. Cells are never stored in adapter memory. The ASX-100 local ATM switch <ref> [4] </ref> is based on a 2.4 Gbits/sec (gigabit per second) switch fabric and a RISC control processor. The ASX-100 supports Fore's SPANS signaling protocol, and can establish either Switched Virtual Circuits (SVCs) or Permanent Virtual Circuits (PVCs).
Reference: [5] <author> Boudec, J., </author> <title> The Asynchronous Transfer Mode: A Tutorial, </title> <journal> Computer Networks and ISDN Systems, </journal> <volume> Vol. 24, </volume> <pages> pp. 279-309, </pages> <year> 1992. </year>
Reference-contexts: Aggregate network capacity is limited to speeds between 10 Mbits/sec (Ethernet) to 100 Mbits/sec (FDDI). High speed switch-based network architectures, such as the HIgh Performance Parallel Interface (HIPPI) [18], Fibre Channel [1, 2], and Asynchronous Transfer Mode (ATM) <ref> [5, 11, 14] </ref>, feature aggregate throughputs of several gigabits/sec. Moreover each host usually has a dedicated high-speed (100 Mbits/sec or more) connection to the switch. * Scalability. <p> We begin by briefly discussing them (a more detailed description can be found in Section 2) and then providing rationale for their usage in our implementation. ATM <ref> [5, 11, 14] </ref> is a network standard. It specifies a fast packet switched network where data is fragmented into fixed-size 53 byte cells. More specifically, it specifies the manner in which cells from calls are routed through the network by way of packet switches and links. <p> The growing availability of high-speed networks, such as ATM, may make very large virtual machines more likely and feasable. 7 2.2 ATM: the Next Generation Network ATM <ref> [4, 5, 11] </ref> is a standard developed by the networking standards community (CCITT) which specifies the network layer protocol of future broadband networks (B-ISDNs Broadband Integrated Services Digital Network). It resides above the physical layer and directly below the ATM Adaptation Layer (AAL) (see Figure 1).
Reference: [6] <author> Clark, D., Jacobsen, V., Romkey, J., Salwen, H., </author> <title> An Analysis of TCP Processing Overhead, </title> <journal> IEEE Communications Magazine, </journal> <month> June </month> <year> 1989. </year>
Reference: [7] <institution> Fore Systems, Inc., </institution> <note> ForeRunner SBA-200 ATM SBus Adapter User's Manual, </note> <year> 1993. </year>
Reference-contexts: The physical media for the Series-200 adapter was the 100 Mbits/sec TAXI interface (FDDI fiber plant and signal encoding scheme). The local area switch was a Fore ASX-100. Four Sun Sparc 2 machines and two Sun 4/690 machines were directly connected to the Fore switch. The Series-200 host adapter <ref> [7] </ref> is Fore's second generation interface and uses an Intel i960 as an onboard processor. The i960 takes over most of the AAL and cell related tasks including the cell level segmentation and reassembly (SAR) functions for AAL 3/4 and AAL 5, and cell multiplexing.
Reference: [8] <author> Geist, G.A., Sunderam, </author> <title> V.S., Network-Based Concurrent Computing on the PVM System, </title> <journal> Concurrency: Practice and Experience, </journal> <volume> 4 </volume> (4):293-311, June 1992. 
Reference-contexts: Even though the ATM standard was initially developed and intended to serve as an infrastructure for wide-area (telecommunications) networks, it is currently being much more rapidly adopted for local area network computing. PVM <ref> [3, 8, 16] </ref> is a software system for the development and execution of parallel applications. It allows an interconnected collection of independent heterogenous computers to appear as a single `virtual' computational resource or a single parallel machine. The independent machines may be ordinary workstations, multiprocessors, supercomputers, or specialized processors. <p> We also provide a description of the particular API, the Fore Systems' ATM API, used in our environment. 2.1 PVM: A Parallel Programming Environment PVM <ref> [3, 8, 16] </ref> provides a unified computational framework for a network of heterogenous computing resources. As mentioned before, computing resources may include workstations, multiprocessors and special-purpose processors, and the underlying network may be a conventional Ethernet, the Internet, or as we have chosen, a high-speed network, such as ATM.
Reference: [9] <author> Guha, A., Liu, J., Pavan, A., Pugaczewski, J., </author> <title> Performance Evaluation Experiments in the MERCURI Wide Area ATM Network, </title> <journal> to appear in IEEE Journal on Selected Areas of Communication. </journal>
Reference-contexts: Performance results of this re-implementation and the original PVM multicast operation are presented in Section 4.4. 4 Performance Measurements 4.1 Experimental Network Computing Environment The ATM environment was provided by the MAGIC (Multidimensional Applications and Giga-bit Internetwork Consortium) [15] project, MERCURI (Multimedia Environment for Real-Time Control Using Remote Imagery) <ref> [9] </ref> project, and the Army High Performance Computing Research Center at the University of Minnesota. Fore Systems, Inc. host adapters and local area switches were used. The host adapter was a Series-200 interface for the Sun SBus.
Reference: [10] <author> Haas, Z., </author> <title> Protocol Structure for High-Speed Communication over Broadband ISDN, </title> <journal> IEEE Networks, </journal> <volume> 5(1) </volume> <pages> 64-70, </pages> <month> January </month> <year> 1991. </year>
Reference-contexts: Several recent papers have found a significant portion of communication overhead occurring due to these interactions <ref> [10, 23, 25] </ref>. A protocol stack is a conceptual diagram where each layer in the stack corresponds to a set of services provided to the adjacent higher layer. For example, the TCP and UDP layer corresponds to the transport layer, as defined by the Open Systems Interconnection Reference Model.
Reference: [11] <author> Kawarasaki, M., and Jabbari, B., </author> <title> B-ISDN Architecture and Protocol, </title> <journal> IEEE Journal on Selected Areas in Communications, </journal> <volume> Vol. 9, No. 9, </volume> <pages> pp. 1405-1415, </pages> <month> Dec. </month> <year> 1991. </year> <month> 25 </month>
Reference-contexts: Aggregate network capacity is limited to speeds between 10 Mbits/sec (Ethernet) to 100 Mbits/sec (FDDI). High speed switch-based network architectures, such as the HIgh Performance Parallel Interface (HIPPI) [18], Fibre Channel [1, 2], and Asynchronous Transfer Mode (ATM) <ref> [5, 11, 14] </ref>, feature aggregate throughputs of several gigabits/sec. Moreover each host usually has a dedicated high-speed (100 Mbits/sec or more) connection to the switch. * Scalability. <p> We begin by briefly discussing them (a more detailed description can be found in Section 2) and then providing rationale for their usage in our implementation. ATM <ref> [5, 11, 14] </ref> is a network standard. It specifies a fast packet switched network where data is fragmented into fixed-size 53 byte cells. More specifically, it specifies the manner in which cells from calls are routed through the network by way of packet switches and links. <p> The growing availability of high-speed networks, such as ATM, may make very large virtual machines more likely and feasable. 7 2.2 ATM: the Next Generation Network ATM <ref> [4, 5, 11] </ref> is a standard developed by the networking standards community (CCITT) which specifies the network layer protocol of future broadband networks (B-ISDNs Broadband Integrated Services Digital Network). It resides above the physical layer and directly below the ATM Adaptation Layer (AAL) (see Figure 1).
Reference: [12] <author> Lin, M., Hsieh, J., Du, D., Thomas, J., MacDonald, J., </author> <title> Distributed Network Computing Over Local ATM Networks, </title> <note> to appear in IEEE Journal on Selected Areas in Communications: Special Issue of ATM LANs (Early `95). </note>
Reference-contexts: ATM local area switches and ATM interface cards for most workstations are also readily available. Current market forces as well as ATM's wide-spread acceptance in the networking community has caused it become the most likely transport mode for high-speed local and wide-area networking <ref> [4, 12, 14, 18] </ref>. In this study, we sought to achieve high performance (e.g., low latency, high bandwidth) not only by implementing PVM on a high speed medium, such as ATM, but also by minimizing possible sources of overhead. Overhead is incurred by hardware and software components. <p> Since, the ATM API resides at a lower layer in the protocol stack, the overhead incurred when directly using this API is expected to be lower than the overhead incurred by using the BSD sockets programming API. A previous study <ref> [12] </ref> validated this performance gain when evaluating the performance of four API's: Fore Systems' ATM API, BSD socket programming interface, Sun's Remote Procedure Call (RPC), and PVM over the BSD socket programming interface. The Fore Systems' ATM API provided the best performance of the four APIs. <p> From this figure, we note that there are two possible APIs which we can use to interface to the ATM AAL layers namely the BSD socket programming interface which includes TCP/IP and UDP/IP, or the ATM API. In this paper, we sought to minimize unnecessary overhead <ref> [12] </ref>, and hence chose to implement PVM on the Fore Systems' ATM API rather than the BSD socket interface. 2.3.1 Fore Systems' ATM API The Fore Systems' ATM API library routines support the client-server model. <p> The detailed transmission facilities of the Direct and Normal modes are hidden from the end-users. The advantage of the Direct mode is that it provides a more efficient communication path than the Normal mode. A previous report <ref> [12] </ref> observed more than a twofold increase in communication performance between the two modes. The main reason PVM provides the Normal mode, despite its lower performance, is because of the limited number of file descriptors some Unix systems provide. Each open TCP connection consumes a file descriptor. <p> The communication latency for sending a M -byte message can be estimated as half of the total round-trip time. The communication throughput is calculated by dividing 2 fi M by the round-trip time (since 2 fi M bytes of message have been physically transmitted). A previous study <ref> [12] </ref> presents the round trip delay (milliseconds) as a function of message size for ATM/AAL5. <p> Again we observe a significant performance gain when using ATM as opposed to Ethernet. The maximal achievable throughput is bounded by the speed of the TAXI interface, 100 Mbits/sec. In our previous study <ref> [12] </ref>, we observed the maximum achievable throughput to be 46.08 Mbits/sec. In this study, we observed the maximum achievable throughput of PVM-ATM (AAL5) to be 27.202 Mbits/sec. <p> The greatest time difference occurs between using ATM or Ethernet. The overhead, in terms of latency, for the ATM network is thought to be primarily caused by the device driver. It is believed that the firmware code for Ethernet has been fine-tuned for better communication latency <ref> [12] </ref>. 4.4 Multicasting Measurements Over ATM, we measured the performance of the multicasting operations (PVM's original mul-ticast operation and our re-implementation) by iteratively executing the multicast operation.
Reference: [13] <author> Lin, M., Hsieh, J., Du, D., MacDonald, J., </author> <title> Performance of High-Speed Network I/O Subsystems: Case Study of a Fibre Channel Network, </title> <booktitle> to appear in IEEE Proceedings of Supercomputing 1994. </booktitle>
Reference-contexts: Software overhead is incurred 3 4 by interactions with the host system's operating system, device driver and higher layer proto-cols. The device driver overhead <ref> [13] </ref> is mainly caused by the design of the host interface and the bus architecture of the host computer. <p> The maximum achievable bandwidth at the application level, 27.202 Mbits/sec, is far below the "raw" available network bandwidth of 100 Mbits/sec, provided by the TAXI interface cards. Also the measured latency of ATM networks was slightly higher than that of Ethernet. A previous study <ref> [13] </ref> discussed how the communications overhead has shifted from the network transmission medium to the network subsystems, or I/O subsystem. The network subsystems consist of the hardware architecture of the host, the host software system, and the network interface [13]. <p> A previous study <ref> [13] </ref> discussed how the communications overhead has shifted from the network transmission medium to the network subsystems, or I/O subsystem. The network subsystems consist of the hardware architecture of the host, the host software system, and the network interface [13]. In order to benefit from the full advantages of a high-speed networking medium such as ATM, the overhead induced by these components and their interactions must be evaluated and reduced.
Reference: [14] <author> Lyles, J., Swinehart, D., </author> <title> The Emerging Gigabit Environment and the Role of Local ATM, </title> <journal> IEEE Communications Magazine, </journal> <month> April </month> <year> 1992. </year>
Reference-contexts: Aggregate network capacity is limited to speeds between 10 Mbits/sec (Ethernet) to 100 Mbits/sec (FDDI). High speed switch-based network architectures, such as the HIgh Performance Parallel Interface (HIPPI) [18], Fibre Channel [1, 2], and Asynchronous Transfer Mode (ATM) <ref> [5, 11, 14] </ref>, feature aggregate throughputs of several gigabits/sec. Moreover each host usually has a dedicated high-speed (100 Mbits/sec or more) connection to the switch. * Scalability. <p> We begin by briefly discussing them (a more detailed description can be found in Section 2) and then providing rationale for their usage in our implementation. ATM <ref> [5, 11, 14] </ref> is a network standard. It specifies a fast packet switched network where data is fragmented into fixed-size 53 byte cells. More specifically, it specifies the manner in which cells from calls are routed through the network by way of packet switches and links. <p> ATM local area switches and ATM interface cards for most workstations are also readily available. Current market forces as well as ATM's wide-spread acceptance in the networking community has caused it become the most likely transport mode for high-speed local and wide-area networking <ref> [4, 12, 14, 18] </ref>. In this study, we sought to achieve high performance (e.g., low latency, high bandwidth) not only by implementing PVM on a high speed medium, such as ATM, but also by minimizing possible sources of overhead. Overhead is incurred by hardware and software components.
Reference: [15] <author> Minnesota Supercomputer Center Inc., </author> <title> An Overview of the MAGIC Project, </title> <year> 1993. </year>
Reference-contexts: Performance results of this re-implementation and the original PVM multicast operation are presented in Section 4.4. 4 Performance Measurements 4.1 Experimental Network Computing Environment The ATM environment was provided by the MAGIC (Multidimensional Applications and Giga-bit Internetwork Consortium) <ref> [15] </ref> project, MERCURI (Multimedia Environment for Real-Time Control Using Remote Imagery) [9] project, and the Army High Performance Computing Research Center at the University of Minnesota. Fore Systems, Inc. host adapters and local area switches were used. The host adapter was a Series-200 interface for the Sun SBus.
Reference: [16] <author> Manchek, </author> <title> R.J., Design and Implementation of PVM Version 3, Thesis presented for the Master of Science Degree, </title> <institution> University of Tennessee, Knoxville. </institution>
Reference-contexts: Even though the ATM standard was initially developed and intended to serve as an infrastructure for wide-area (telecommunications) networks, it is currently being much more rapidly adopted for local area network computing. PVM <ref> [3, 8, 16] </ref> is a software system for the development and execution of parallel applications. It allows an interconnected collection of independent heterogenous computers to appear as a single `virtual' computational resource or a single parallel machine. The independent machines may be ordinary workstations, multiprocessors, supercomputers, or specialized processors. <p> We also provide a description of the particular API, the Fore Systems' ATM API, used in our environment. 2.1 PVM: A Parallel Programming Environment PVM <ref> [3, 8, 16] </ref> provides a unified computational framework for a network of heterogenous computing resources. As mentioned before, computing resources may include workstations, multiprocessors and special-purpose processors, and the underlying network may be a conventional Ethernet, the Internet, or as we have chosen, a high-speed network, such as ATM. <p> The network of independent PVM pvmds form the basis for support of important features for a network-based computing environment. These 6 features include dynamic reconfigurability, fault tolerance and scalability. PVM allows for dynamic reconfigurability by allowing hosts to enter and exit the host pool via notification messages <ref> [16] </ref>. PVM version 3 also supports the notion of dynamic process groups. Processes can belong to multiple named groups, and groups can be changed dynamically at any time during a computation. <p> The dynamic reconfigurability feature may be incorporated to gather more resources once a host has failed. Since management is decentralized and localized, a PVM virtual machine may potentially scale up to hundreds of hosts executing thousands of tasks. However, the largest reported virtual machines consist of approximately 100 hosts <ref> [16] </ref>. This is due to, in part, from the lack of ready availabilty of high-speed networks. Also, in general, there do not exist interesting algorithms which can make use of hundreds of relatively fast processors interconnected by a low-speed network. <p> Some operating systems limit the number of open files to as few as 32. If a virtual machine consists of N hosts, each machine must have N 1 connections to the other hosts. Thus the drawback of the Direct mode is its limited scalability <ref> [16] </ref>. There are two types of important and frequently used interactions: task-to-pvmd communication and pvmd-to-pvmd communication. As mentioned above, task-to-pvmd communication occurs via Unix domain sockets, and pvmd-to-pvmd communications occurs via UDP in the Normal mode.
Reference: [17] <author> Message Passing Interface Forum, </author> <title> MPI: A Message-Passing Interface Standard, </title> <month> May </month> <year> 1994. </year>
Reference-contexts: In the distributed parallel community, PVM has been met with much success and usage. Another message passing platform which is currently receiving much attention is the MPI (Message Passing Interface) <ref> [17] </ref>, a working standard. The following enumerates the advantages of the particular parallel programming environment (PVM) and high speed network platform (ATM) we have chosen as the basis for our parallel computing environment. * Fast message passing.
Reference: [18] <author> Renwick, J., </author> <title> Building a Practical HIPPI LAN, </title> <booktitle> IEEE Proceedings of 17th Conference on Local Computer Networks, </booktitle> <year> 1992. </year>
Reference-contexts: In shared medium architectures, network capacity is shared among all the interconnected processors. Aggregate network capacity is limited to speeds between 10 Mbits/sec (Ethernet) to 100 Mbits/sec (FDDI). High speed switch-based network architectures, such as the HIgh Performance Parallel Interface (HIPPI) <ref> [18] </ref>, Fibre Channel [1, 2], and Asynchronous Transfer Mode (ATM) [5, 11, 14], feature aggregate throughputs of several gigabits/sec. Moreover each host usually has a dedicated high-speed (100 Mbits/sec or more) connection to the switch. * Scalability. <p> ATM local area switches and ATM interface cards for most workstations are also readily available. Current market forces as well as ATM's wide-spread acceptance in the networking community has caused it become the most likely transport mode for high-speed local and wide-area networking <ref> [4, 12, 14, 18] </ref>. In this study, we sought to achieve high performance (e.g., low latency, high bandwidth) not only by implementing PVM on a high speed medium, such as ATM, but also by minimizing possible sources of overhead. Overhead is incurred by hardware and software components.
Reference: [19] <author> Request for Comment 1483, </author> <title> Multiprotocol Encapsulation over ATM Adaptation Layer 5, </title> <month> July </month> <year> 1993. </year>
Reference-contexts: Five service class are being standardized to provide these services. The CCITT recommendation for ATM specifies five AAL protocols <ref> [19] </ref> which are listed as follows: 1. AAL Type 1 supports constant bit rate services, such as traditional voice transmis sion. 8 2. AAL Type 2 supports variable bit rate video and audio information. Maintains the timing relation between the source and destination pair. 3.
Reference: [20] <author> Schoch, J., Hupp, J., </author> <title> Measured Performance of an Ethernet Local Network, </title> <journal> Communications of the ACM, </journal> <month> December </month> <year> 1980. </year>
Reference-contexts: Moreover each host usually has a dedicated high-speed (100 Mbits/sec or more) connection to the switch. * Scalability. In shared medium architectures, since network capacity is shared among all the interconnected processors, as the number of processing nodes is increased, network saturation quickly occurs <ref> [20] </ref>. High-speed switch-based networks may easily be scaled up, in terms of processing power or storage, by simply connecting the new devices via additional links and/or a switch. * Potentially low latency.
Reference: [21] <author> Tanenbaum, A., </author> <title> Computer Networks, </title> <publisher> Prentice Hall, </publisher> <address> Second Edition, </address> <year> 1988. </year>
Reference-contexts: The following items are the motivational factors for considering the implementation of a parallel computing platform over a high speed local area network. * High data transfer rates. Traditional local area networks, such as Ethernet and Fiber Distributed Data Interface (FDDI) <ref> [21] </ref>, are shared medium architectures. In shared medium architectures, network capacity is shared among all the interconnected processors. Aggregate network capacity is limited to speeds between 10 Mbits/sec (Ethernet) to 100 Mbits/sec (FDDI).
Reference: [22] <author> Thekkath, C.A., Levy, H.M., Lazowska, E.D., </author> <title> Efficient Support for Multicomputing on ATM Networks, </title> <type> Technical Report TR 93-04-03, </type> <institution> Department of Computer Science and Engineering, University of Washington, </institution> <month> April </month> <year> 1993. </year>
Reference: [23] <author> Von Eicken, T., Culler, T.E., Goldstein, S.C., and Schauser, K.E., </author> <title> Active Messages: a Mechanism for Integrated Communication and Computation, </title> <booktitle> The 19th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 256-266, </pages> <month> May </month> <year> 1992. </year>
Reference-contexts: Several recent papers have found a significant portion of communication overhead occurring due to these interactions <ref> [10, 23, 25] </ref>. A protocol stack is a conceptual diagram where each layer in the stack corresponds to a set of services provided to the adjacent higher layer. For example, the TCP and UDP layer corresponds to the transport layer, as defined by the Open Systems Interconnection Reference Model.
Reference: [24] <author> Wolman, A., Voelker, G., Chandramohan, A., Thekkath, </author> <title> C.A., Latency Analysis of TCP on an ATM Network, </title> <type> Technical Report TR 93-03-03, </type> <institution> Department of Computer Science and Engineering, University of Washington, </institution> <month> March </month> <year> 1993. </year>
Reference: [25] <author> Zitterbart, M., </author> <title> High-Speed Transport Components, </title> <journal> IEEE Network, </journal> <volume> 5(1) </volume> <pages> 54-63, </pages> <month> January </month> <year> 1991. </year>
Reference-contexts: Several recent papers have found a significant portion of communication overhead occurring due to these interactions <ref> [10, 23, 25] </ref>. A protocol stack is a conceptual diagram where each layer in the stack corresponds to a set of services provided to the adjacent higher layer. For example, the TCP and UDP layer corresponds to the transport layer, as defined by the Open Systems Interconnection Reference Model.
References-found: 25

