URL: http://www.neci.nj.nec.com/homepages/giles/papers/AAAI-97.overfitting.hard_to_do.ps.Z
Refering-URL: http://www.neci.nj.nec.com/homepages/giles/html/CLG_pub.html
Root-URL: 
Email: flawrence,gilesg@research.nj.nec.com, Ah Chung Tsoi@uow.edu.au  
Phone: 2  
Title: Lessons in Neural Network Training: Overfitting Lessons in Neural Network Training: Overfitting May be Harder
Author: Steve Lawrence C. Lee Giles Ah Chung Tsoi fl 
Address: 4 Independence Way, Princeton, NJ 08540,  Australia  
Affiliation: 1 NEC Research,  Faculty of Informatics, Uni. of Wollongong,  
Note: May be Harder than Expected, Proceedings of the Fourteenth National Conference on Artificial Intelligence, AAAI-97, AAAI Press, Menlo Park, California, pp. 540545, 1997. Copyright AAAI.  
Abstract: For many reasons, neural networks have become very popular AI machine learning models. Two of the most important aspects of machine learning models are how well the model generalizes to unseen data, and how well the model scales with problem complexity. Using a controlled task with known optimal training error, we investigate the convergence of the backpropagation (BP) algorithm. We find that the optimal solution is typically not found. Furthermore, we observe that networks larger than might be expected can result in lower training and generalization error. This result is supported by another real world example. We further investigate the training behavior by analyzing the weights in trained networks (excess degrees of freedom are seen to do little harm and to aid convergence), and contrasting the interpolation characteristics of multi-layer perceptron neural networks (MLPs) and polynomial models (overfitting behavior is very different the MLP is often biased towards smoother solutions). Finally, we analyze relevant theory outlining the reasons for significant practical differences. These results bring into question common beliefs about neural network training regarding convergence and optimal network size, suggest alternate guidelines for practical use (lower fear of excess degrees of freedom), and help to direct future work (e.g. methods for creation of more parsimonious solutions, importance of the MLP/BP bias and possibly worse performance of improved training algorithms). 
Abstract-found: 1
Intro-found: 1
Reference: <author> Akaike, H. </author> <year> 1973. </year> <title> Information theory and an extension of the maximum likelihood principle. </title> <editor> In Petrov, B. N., and Csaki, F., eds., </editor> <booktitle> Proceeding 2nd International Symposium on Information Theory. </booktitle> <address> Budapest: </address> <publisher> Akademia Kiado. </publisher> <pages> 267281. </pages>
Reference-contexts: Theory The selection of a model size which maximizes generalization is an important topic. There are several theories for determining the optimal network size e.g. the NIC (Network Information Criterion) which is a generalization of the AIC (Akaike Information Criterion) <ref> (Akaike 1973) </ref> widely used in statistical inference, the generalized final prediction error (GPE) as proposed by (Moody 1992), and the VC dimension (Vapnik 1995) which is a measure of the expressive power of a network.
Reference: <author> Barron, A. </author> <year> 1991. </year> <title> Complexity regularization with application to artificial neural networks. </title> <editor> In Roussas, G., ed., </editor> <title> Nonparametric Functional Estimation and Related Topics. </title> <address> Dordrecht, The Netherlands: </address> <publisher> Kluwer Academic Publishers. </publisher> <pages> 561576. </pages>
Reference: <author> Bartlett, P. </author> <year> 1996. </year> <title> The sample complexity of pattern classification with neural networks: the size of the weights is more important than the size of the network. </title> <type> Technical report, </type> <institution> Australian National University. </institution>
Reference-contexts: Specifically, this theory does not take into account limited training time, different rates of convergence for different f , or sub-optimal solutions. Recent work by <ref> (Bartlett 1996) </ref> correlates with the results reported here. Bartlett comments: the VC-bounds seem loose; neural networks often perform successfully with training sets that are considerably smaller than the number of network parameters.
Reference: <author> Caruana, R. </author> <year> 1993. </year> <title> Generalization vs. Net Size. </title> <address> Denver, CO: </address> <booktitle> Neural Information Processing Systems, Tutorial. </booktitle>
Reference-contexts: If the data is divided into two equal groups about the median, then the IQR is the difference between the medians of these groups. The IQR contains 50% of the points. 3 Caruana presented a tutorial at NIPS 93 <ref> (Caruana 1993) </ref> with generalization results on a variety of problems as the size of the networks was varied from too small to too large.
Reference: <author> Crane, R.; Fefferman, C.; Markel, S.; and Pearson, J. </author> <year> 1995. </year> <title> Characterizing neural network error surfaces with a sequential quadratic programming algorithm. In Machines That Learn. </title>
Reference-contexts: VC bounds are likely to be too conservative because they provide generalization guarantees simultaneously for any probability distribution and any training algorithm. The computation of VC bounds for practical networks is difficult. Student-Teacher Task To investigate empirical performance we will use a student-teacher task <ref> (Crane et al. 1995) </ref> so that we know the optimal solution and can carefully control the problem. The task is as follows: 1.
Reference: <author> Giles, C. L., and Lawrence, S. </author> <year> 1997. </year> <title> Presenting and analyzing the results of AI experiments: Data averaging and data snooping. </title> <booktitle> In Proceedings of the Fourteenth National Conference on Artificial Intelligence, </booktitle> <address> AAAI-97. Menlo Park, California: </address> <publisher> AAAI Press. </publisher> <pages> 362367. </pages>
Reference-contexts: A similar trend is observed. 2 The distribution of results is often not Gaussian and alternative means of presenting results other than the mean and standard deviation can be more informative <ref> (Giles & Lawrence 1997) </ref>. Box-whiskers plots (Tukey 1977) show the interquartile range (IQR) with a box and the median as a bar across the box. The whiskers extend from the ends of the box to the minimum and maximum values.
Reference: <author> Haykin, S. </author> <year> 1994. </year> <title> Neural Networks, A Comprehensive Foundation. </title> <address> New York, NY: </address> <publisher> Macmillan. </publisher>
Reference-contexts: The initial weights of these new networks are set randomly using the procedure suggested in <ref> (Haykin 1994) </ref> (i.e. they are not equal to the weights in the network used to create the dataset). <p> These techniques tend to result in networks with smaller weights. 4. A commonly recommended technique with MLP classification is to set the training targets away from the bounds of the activation function (e.g. (-0.8, 0.8) instead of (-1, 1) for the tanh activation function) <ref> (Haykin 1994) </ref>. MLP networks are, of course, not always this resistant to 7 Training details were as follows.
Reference: <author> Hornik, K.; Stinchcombe, M.; and White, H. </author> <year> 1989. </year> <title> Multilayer feedforward networks are universal approximators. </title> <booktitle> Neural Networks 2:359366. </booktitle>
Reference-contexts: An intuitive explanation for this is that weights typically start out reasonably small (for good reason), and may get trapped in local minima before reaching large values. 2. MLPs are universal approximators <ref> (Hornik, Stinch-combe, & White 1989) </ref>. However, the universal approximation result requires an infinite number of hidden nodes. For a given number of hidden nodes a network may be incapable of representing the required function and instead implement a simpler function which approximates the required function. 3.
Reference: <author> Krogh, A., and Hertz, J. </author> <year> 1992. </year> <title> A simple weight decay can improve generalization. </title> <editor> In Moody, J.; Hanson, S. J.; and Lippmann, R. P., eds., </editor> <booktitle> Advances in Neural Information Processing Systems, volume 4. </booktitle> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann. </publisher> <pages> 950957. </pages>
Reference-contexts: However, the universal approximation result requires an infinite number of hidden nodes. For a given number of hidden nodes a network may be incapable of representing the required function and instead implement a simpler function which approximates the required function. 3. Weight decay <ref> (Krogh & Hertz 1992) </ref> or weight elimination (Weigend, Rumelhart, & Huberman 1991) are often used in MLP training and aim to minimize a cost function which penalizes large weights. These techniques tend to result in networks with smaller weights. 4.
Reference: <author> Krose, B., and van der Smagt, P. </author> <year> 1993. </year> <title> An Introduction to Neural Networks. </title> <institution> University of Amsterdam, </institution> <address> fifth edition. </address>
Reference: <author> Lawrence, S.; Giles, C. L.; and Tsoi, A. </author> <year> 1996. </year> <title> What size neural network gives optimal generalization? Convergence properties of backpropagation. </title> <institution> Technical Report UMIACS-TR-96-22 and CS-TR-3617, Institute for Advanced Computer Studies, University of Maryland, College Park MD 20742. </institution>
Reference-contexts: Often, a result of attaining a sub-optimal solution is that not all of the network resources are efficiently used. Experiments with a controlled task have indicated that the sub-optimal solutions often have smaller weights on average <ref> (Lawrence, Giles, & Tsoi 1996) </ref>. An intuitive explanation for this is that weights typically start out reasonably small (for good reason), and may get trapped in local minima before reaching large values. 2. MLPs are universal approximators (Hornik, Stinch-combe, & White 1989).
Reference: <author> Moody, J. </author> <year> 1992. </year> <title> The effective number of parameters: An analysis of generalization and regularization in nonlinear learning systems. </title> <editor> In Moody, J.; Hanson, S. J.; and Lippmann, R. P., eds., </editor> <booktitle> Advances in Neural Information Processing Systems, volume 4. </booktitle> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann. </publisher> <pages> 847854. </pages>
Reference-contexts: There are several theories for determining the optimal network size e.g. the NIC (Network Information Criterion) which is a generalization of the AIC (Akaike Information Criterion) (Akaike 1973) widely used in statistical inference, the generalized final prediction error (GPE) as proposed by <ref> (Moody 1992) </ref>, and the VC dimension (Vapnik 1995) which is a measure of the expressive power of a network. NIC relies on a single well-defined minimum to the fitting function and can be unreliable when there are several local minima (Ripley 1995).
Reference: <author> Ripley, B. </author> <year> 1995. </year> <title> Statistical ideas for selecting network architectures. Invited Presentation, </title> <booktitle> Neural Information Processing Systems 8. </booktitle>
Reference-contexts: NIC relies on a single well-defined minimum to the fitting function and can be unreliable when there are several local minima <ref> (Ripley 1995) </ref>. There is very little published computational experience of the NIC, or the GPE. Their evaluation is prohibitively expensive for large networks. VC bounds have been calculated for various network types. Early VC-dimension work handles only the case of discrete outputs.
Reference: <author> Rumelhart, D.; Hinton, G.; and Williams, R. </author> <year> 1986. </year> <title> Learning internal representations by error propagation. </title> <editor> In Rumelhart, D., and McClelland, J., eds., </editor> <booktitle> Parallel Distributed Processing, </booktitle> <volume> volume 1. </volume> <publisher> Cambridge: MIT Press. </publisher> <address> chapter 8, </address> <month> 318362. </month>
Reference-contexts: Theoretically, the optimal training set error for all networks tested is zero, as m 0 h m h . However, none of the networks trained here obtained the optimal error (using backpropagation (BP) <ref> (Rumelhart, Hinton, & Williams 1986) </ref> for 5 fi 10 5 updates) 1 . Each configuration of the MLP was tested with ten simulations, each with a different starting condition (random weights).
Reference: <author> Tukey, J. </author> <year> 1977. </year> <title> Exploratory Data Analysis. </title> <address> Reading, MA: </address> <publisher> Addison-Wesley. </publisher>
Reference-contexts: A similar trend is observed. 2 The distribution of results is often not Gaussian and alternative means of presenting results other than the mean and standard deviation can be more informative (Giles & Lawrence 1997). Box-whiskers plots <ref> (Tukey 1977) </ref> show the interquartile range (IQR) with a box and the median as a bar across the box. The whiskers extend from the ends of the box to the minimum and maximum values.
Reference: <author> Vapnik, V. </author> <year> 1995. </year> <title> The Nature of Statistical Learning Theory. </title> <publisher> Springer. </publisher>
Reference-contexts: There are several theories for determining the optimal network size e.g. the NIC (Network Information Criterion) which is a generalization of the AIC (Akaike Information Criterion) (Akaike 1973) widely used in statistical inference, the generalized final prediction error (GPE) as proposed by (Moody 1992), and the VC dimension <ref> (Vapnik 1995) </ref> which is a measure of the expressive power of a network. NIC relies on a single well-defined minimum to the fitting function and can be unreliable when there are several local minima (Ripley 1995). There is very little published computational experience of the NIC, or the GPE. <p> In each case, the results are shown for two networks with different random starting weights. The plotting method is described in the text. Learning Theory The results are not in contradiction with statistical learning theory. <ref> (Vapnik 1995) </ref> states that machines with a small VC dimension are required to avoid overfitting.
Reference: <author> Weigend, A.; Rumelhart, D.; and Huberman, B. </author> <year> 1991. </year> <title> Generalization by weight-elimination with application to forecasting. </title> <editor> In Lippmann, R. P.; Moody, J.; and Touretzky, D. S., eds., </editor> <booktitle> Advances in Neural Information Processing Systems, volume 3. </booktitle> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann. </publisher> <pages> 875882. </pages>
Reference-contexts: For a given number of hidden nodes a network may be incapable of representing the required function and instead implement a simpler function which approximates the required function. 3. Weight decay (Krogh & Hertz 1992) or weight elimination <ref> (Weigend, Rumelhart, & Huberman 1991) </ref> are often used in MLP training and aim to minimize a cost function which penalizes large weights. These techniques tend to result in networks with smaller weights. 4.
Reference: <author> Yu, X.-H. </author> <year> 1992. </year> <title> Can backpropagation error surface not have local minima. </title> <journal> IEEE Transactions on Neural Networks 3:10191021. </journal>
Reference-contexts: Commonly, these solutions are referred to as local minima, about which much has been written and proven (e.g. <ref> (Yu 1992) </ref>). However, it is not only local minima that create trouble for BP other error surface features such as ravines and plateaus or flat spots can also be troublesome.
References-found: 18

