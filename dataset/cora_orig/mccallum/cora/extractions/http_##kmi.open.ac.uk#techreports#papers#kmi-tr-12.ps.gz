URL: http://kmi.open.ac.uk/techreports/papers/kmi-tr-12.ps.gz
Refering-URL: http://kmi.open.ac.uk/techreports/kmi-tr-list.html
Root-URL: 
Note: S.N.K.Watt@open.ac.uk The Naive Psychology Manifesto  
Abstract: Stuart Watt 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Simon Baron-Cohen, Alan M. Leslie, and Uta Frith. </author> <title> Does the autistic child have a `theory of mind'? , 21(1) </title> <type> 37-46, </type> <year> 1985. </year>
Reference-contexts: In one, Davis [13] described a model by which one agent could infer the limits on another agent's knowledge from knowledge about its perceptual limitsthis shows a close affinity with some of the `false belief' work in developmental psychology (e.g. <ref> [1, 48] </ref>.) And in another, Shultz [45] described a model for the ascription of intentions to other agents, using rules which relate behaviour and mental states. These models both show that it is possible to successfully construct working models for aspects of naive psychology. <p> So now the ascription can be described: These rules are steps towards a simple descriptive account of how an agent might come to pass the false belief test <ref> [1] </ref>. Note that both of these rules are intentional, that is, they only apply when taking the intentional stance to . Whether or not this account is correct or not isn't really the issue. <p> These rules, which a close affinity to those of Davis [13], are a first step to modelling Wellman's [48] account of belief progressing from a copy theory to a representational theory, but there are many other possible models (e.g. <ref> [1, 38] </ref>.) What is actually needed is a way of comparing and combining the different models.
Reference: [2] <author> Rodney A. Brooks. </author> <title> Elephants don't play chess. </title> <editor> In Pattie Maes, editor, </editor> . <publisher> MIT Press, </publisher> <address> Cambridge, Massachusetts, </address> <year> 1991. </year>
Reference-contexts: The most widely voiced criticisms of explicit representations in artificial intelligence (e.g. <ref> [2] </ref>) haven't really had much impact in this field, because they have to 3 the recognition deal with something rare in physical environments, this opacity that people have. Both situated and connectionist approaches break down with the opaque nature of other agents.
Reference: [3] <author> Rodney A. Brooks and Lynn Andrea Stein. </author> <title> Building brains for bodies. </title> <type> AI Memo 1439, </type> <institution> MIT AI Laboratory, </institution> <month> August </month> <year> 1993. </year>
Reference-contexts: Computers don't score too well on physical similarity, so this is likely to form a 6 be Model two: simulation Model three: theory persistent bias against people ascribing mental states to them, unless we build them with a physical resemblance to us (e.g. <ref> [3] </ref>.) Familiarity, fortunately, offers us a way out of this trapwe can in principle learn to see computers as minds. Anthropomorphism can be interpreted several ways. <p> It seems to show not that it is impossible in principle, but that it is just very hard for people to see things which don't physically, structurally, and behaviourally resemble people as being intelligent. Perhaps Brooks and Stein <ref> [3] </ref> are right to build Cog with a humanoid form, not for any technical reason, but simply because it will make it easier for us to see Cog as an intelligent system.
Reference: [4] <author> Richard W. Byrne and Andrew Whiten, </author> <title> editors. </title> . <publisher> Oxford University Press, Oxford, </publisher> <year> 1988. </year>
Reference-contexts: And any good chess program will depend on a player's of the moves, in terms of (often social) constructs such as `attacking' and `supporting.' Following on from this game metaphor for human interaction, there is evidence that a lot of human intelligence is `Machiavellian' <ref> [4] </ref> in the way people use it to outwit each other and to recognise and manipulate one another's mental states; our social environments are considerably more complex than our physical ones.
Reference: [5] <author> Linnda R. Caporael. Anthropomorphism and mechanomorphism: </author> <title> Two faces of the human machine. </title> , <booktitle> 2(3) </booktitle> <pages> 215-234, </pages> <year> 1986. </year>
Reference-contexts: After all, this is what the Turing test really tests for [20], which is why its results are sometimes so strange [10, 11, 26, 47]. The Turing test is, in many ways, a paradigm example of the effects of naive psychology <ref> [5] </ref>, in that it is, in effect, a test of the ability of one system to ascribe mental states to another. But most of the interesting stuff in the Turing test is going on in the head of the observer, not only in the system under test [11]. <p> One way of ascribing mental states to a system is just to anthropomorphise itto ascribe it human mental characteristics without reference to their real competences, but anthropomorphism is a complex and subtle phenomenon <ref> [5, 18] </ref> and not one that has been studied much. <p> Anthropomorphism can be interpreted several ways. Caporael <ref> [5] </ref> suggests that it is a `default schema' applied to non-social objects, one that is abandoned or modified in the face of contradictory evidence, but the evidence is against either animals or computers really being `non-social' [21, 37] and familiarity can increase rather than decrease the tendency to anthropomorphise [18].
Reference: [6] <editor> Susan Carey. </editor> . <publisher> MIT Press, </publisher> <address> Cambridge, Massachusetts, </address> <year> 1985. </year>
Reference-contexts: This paper will present a position orthogonal to that of Hayes. Hayes' initiative clarified many of the issues associated with common sense, and other developments in comparative and developmental psychology have further highlighted the apparently fundamental nature of naive physics <ref> [6, 40] </ref>, but they have also revealed a deeper and bigger problem than that of naive physicsnaive psychology. Naive psychology [7, 24] can be thought of as the natural human ability to infer and reason about other people's mental statesin short, to see one another as minds rather than bodies.
Reference: [7] <author> Andy Clark. </author> <title> From folk psychology to naive psychology. </title> , <booktitle> 11 </booktitle> <pages> 139-154, </pages> <year> 1987. </year>
Reference-contexts: Hayes' initiative clarified many of the issues associated with common sense, and other developments in comparative and developmental psychology have further highlighted the apparently fundamental nature of naive physics [6, 40], but they have also revealed a deeper and bigger problem than that of naive physicsnaive psychology. Naive psychology <ref> [7, 24] </ref> can be thought of as the natural human ability to infer and reason about other people's mental statesin short, to see one another as minds rather than bodies. This is an issue that artificial intelligence must also address. <p> Something is left over, and that something is a `theory' of mind not a theory in the scientific sense <ref> [7, 44] </ref>simply a theory in the sense of a set of tools for thinking about the unobservables of another person's mental states. This theory aspect of prediction is that aspect which is most similar to the representational artificial intelligence.
Reference: [8] <author> Andy Clark. </author> <title> Two kinds of cognitive science? In , pages 155-157, </title> <year> 1988. </year>
Reference-contexts: The methodological criticisms for the project have already been adequately aired, but none of these really attack the logicist position as a descriptive project, merely as a causal account of common sense reasoning <ref> [8, 33] </ref>. This paper will present a position orthogonal to that of Hayes. <p> psychological approaches to artificial intelligence is not a new one [28, 29], but there is little to add to this controversy beyond what has already been written (e.g. [28, 33, 35].) The logicist project as it stands is unlikely to succeed in producing a causal accountbut for a descriptive project <ref> [8] </ref> this doesn't matter. So there has been significant work in this field, but perhaps artificial intelligence just hasn't realised the scale or importance of the problem. Naive psychology is pressing problem. <p> Sometimes prediction of other people's mental states is better modelled by `simulating' the other person, by pretending to them, and to look at the world from their point of view. Clark <ref> [8] </ref> suggests that a similar simulation process could even account for naive physicsperhaps Hayes' [23] paper on liquids could be recast as a kind of simulation, and as far as the predictions are concerned, viewed externally, there needn't be any difference.
Reference: [9] <author> Philip R. Cohen and Hector J. Levesque. </author> <title> Intention is choice with commitment. </title> , <booktitle> 42 </booktitle> <pages> 213-261, </pages> <year> 1990. </year> <title> Behavioural and Brain Sciences Artificial Experts: Social Knowledge and Intelligent Machines Objectivity, </title> <booktitle> Simulation and the Unity of Consciousness Proceedings of the Seventh National Conference on Artificial Intelligence, AAAI Journal of Philosophy Minds, </booktitle> <editor> machines, </editor> <title> and evolution The Intentional Stance Daedalus Journal of Social Issues Mind Mind Science as Culture Behavioural and Brain Sciences Formal Theories of the Commonsense World </title>
Reference-contexts: Naive psychology isn't new to artificial intelligence, which has already tried a number of approaches to the problem. Perhaps the most successful have been the axiomatic formalisms (e.g. <ref> [9, 36] </ref>.) These provide a representation of naive psychology as the ability to make inferences about a set of beliefs, desires, and intentions, corresponding to an agent's mental states.
Reference: [10] <author> Kenneth M. Colby. </author> <title> Modeling a paranoid mind. </title> , <booktitle> 4 </booktitle> <pages> 515-560, </pages> <year> 1981. </year>
Reference-contexts: After all, this is what the Turing test really tests for [20], which is why its results are sometimes so strange <ref> [10, 11, 26, 47] </ref>. The Turing test is, in many ways, a paradigm example of the effects of naive psychology [5], in that it is, in effect, a test of the ability of one system to ascribe mental states to another.
Reference: [11] <author> Harry M. </author> <title> Collins. </title> . <publisher> MIT Press, </publisher> <address> Cambridge, Massachusetts, </address> <year> 1990. </year>
Reference-contexts: After all, this is what the Turing test really tests for [20], which is why its results are sometimes so strange <ref> [10, 11, 26, 47] </ref>. The Turing test is, in many ways, a paradigm example of the effects of naive psychology [5], in that it is, in effect, a test of the ability of one system to ascribe mental states to another. <p> But most of the interesting stuff in the Turing test is going on in the head of the observer, not only in the system under test <ref> [11] </ref>.
Reference: [12] <author> Martin Davies. </author> <title> The mental simulation debate. </title> <editor> In Christopher Peacocke, </editor> <booktitle> editor, </booktitle> , <pages> pages 99-128. </pages> <publisher> Oxford University Press for the British Academy, </publisher> <year> 1994. </year>
Reference-contexts: In principle, of course, a heuristic theory can generate a simulation <ref> [12] </ref>, and in practice an actual systemsuch as a trained connectionist network might show aspects of theory and simulation under different circumstances, just as electrons can behave like particles or like waves. These are valid points, but are more relevant to a causal account than to a descriptive one.
Reference: [13] <author> Ernest Davis. </author> <title> Inferring ignorance from the locality of visual perception. </title> <booktitle> In , pages 786-790, </booktitle> <year> 1988. </year>
Reference-contexts: These psychological studies are starting to probe the regularities and structures which underlie the development of human naive psychology. In addition to the theoretical models common in representational approaches to naive psychology, there are a few more practical examples. In one, Davis <ref> [13] </ref> described a model by which one agent could infer the limits on another agent's knowledge from knowledge about its perceptual limitsthis shows a close affinity with some of the `false belief' work in developmental psychology (e.g. [1, 48].) And in another, Shultz [45] described a model for the ascription of <p> Representational artificial intelligence does simulation all the timeit's just another kind of hypothetical reasoning. Simulation, or taking another person's role, is a way that we can understand some aspects of another's mental states; for instance, to recognise somebody's ignorance <ref> [13] </ref>. So simulation is another way that we can reason about another's mental states. <p> Note that both of these rules are intentional, that is, they only apply when taking the intentional stance to . Whether or not this account is correct or not isn't really the issue. These rules, which a close affinity to those of Davis <ref> [13] </ref>, are a first step to modelling Wellman's [48] account of belief progressing from a copy theory to a representational theory, but there are many other possible models (e.g. [1, 38].) What is actually needed is a way of comparing and combining the different models.
Reference: [14] <author> Daniel C. </author> <title> Dennett. </title> <journal> Intentional systems. </journal> , <volume> 68 </volume> <pages> 87-106, </pages> <year> 1971. </year>
Reference-contexts: Alternatively, perhaps our tendency to anthropomorphise is really a disposition to take the intentional stance <ref> [14] </ref>, to see others as minds rather than as bodies. If, instead of taking the intentional stance, the physical stance is taken, the very different faculty of naive physics will be deployed.
Reference: [15] <author> Daniel C. Dennett. </author> <title> Cognitive wheels: The frame problem of AI. </title> <booktitle> In Christo-pher Hookway, editor, </booktitle> , <pages> pages 129-151. </pages> <publisher> Cam-bridge University Press, </publisher> <year> 1984. </year>
Reference-contexts: The problems for common sense have generally come in two classes, technical and methodological. On the technical side, for example, there is the `frame problem' [32], which Dennett <ref> [15] </ref> describes as the smoking pistol behind a lot of the 1 understanding attacks on artificial intelligence, and as a major philosophical problem for everybody.
Reference: [16] <author> Daniel C. </author> <title> Dennett. </title> . <publisher> MIT Press, </publisher> <address> Cambridge, Mas-sachusetts, </address> <year> 1987. </year>
Reference-contexts: But besides this methodological criticism, there are deeper onesare mental states really best described in terms of beliefs, desires, and intentions? <ref> [16, 42] </ref>. <p> It plays the role of the rationality assumption in Dennett's modelalthough clearly anthropomorphism isn't the same thing as rationality and the suggestion that the rationality assumption is pre-theoretic <ref> [16] </ref> does allow us to interpret it as non-philosophical. Sometimes prediction of other people's mental states is better modelled by `simulating' the other person, by pretending to them, and to look at the world from their point of view. <p> too far; a strong representational 7 ! ! ( ( )) ( ( )) believes self; X believes self; believes agent; X believes self; perceived agent; X believes self; believes agent; X believes self; believes agent; X agent theory of mind is subject to too many philosophical and evolutionary objections <ref> [16] </ref>, and fails to account for all phenomena [25, 39]. But just because a representational theory of mind can't provide a complete naive psychology doesn't mean that it doesn't form part of a complete naive psychology. <p> Stepping outside our humanity is something that perhaps we can never do in principle, but that doesn't mean that we shouldn't trynot by a regress to the Skinnerian vantage point (with apologies to Dennett <ref> [16] </ref>) denying human mentalistic terms completely, but by indirectly looking at the effects of the ultimate unobservable, our anthropocentric point of view. Although there is no justification for a of intelligence as the behaviour 11
Reference: [17] <author> Herbert L. Dreyfus and Stuart E. Dreyfus. </author> <title> Making a mind versus modelling the brain: </title> <journal> Artificial intelligence back at a branchpoint. </journal> , <volume> 117(1) </volume> <pages> 185-197, </pages> <year> 1988. </year>
Reference-contexts: Common sense has generally been a big problem for artificial intelligence, and despite the attempts of many brave souls (e.g. Hayes, McCarthy, McDermott, and Lenat) it hasn't really yielded at all: the common sense knowledge problem has blocked all progress in theoretical artificial intelligence <ref> [17] </ref>. For twenty years there has been only slow progress in the field, and although many important technologies have been found while looking for ways to deal with common sense (e.g. nonmonotonic reasoning,) the core of the problem is still almost untouched. <p> Dreyfus and Dreyfus claim, for example, that the problem of finding a of common sense physics is insoluble because the domain has no theoretical structure <ref> [17, original emphasis] </ref>. Well, this depends on what you want from the theory. Even if, as Dreyfus and Dreyfus claim, naive physics can't be described fully by reference to abstract laws, that doesn't mean that we should just give up. <p> This is an over-reaction; certainly our anthropocentricity is a big problem, but not one that is inaccessible in principle to science. There is little evidence that, as Dreyfus and Dreyfus <ref> [17] </ref> claim, the problem of common sense cannot be solved in principle, although the apparent lack of progress in this area is disconcerting. While work is progressing, it is hampered by some rather big methodological problems.
Reference: [18] <author> Timothy J. Eddy, Gordon G. Gallup, and Daniel J. Povinelli. </author> <title> Attribution of cognitive states to animals: </title> <booktitle> Anthropomorphism in comparitive perspective. </booktitle> , <volume> 49(1) </volume> <pages> 87-101, </pages> <year> 1993. </year>
Reference-contexts: One way of ascribing mental states to a system is just to anthropomorphise itto ascribe it human mental characteristics without reference to their real competences, but anthropomorphism is a complex and subtle phenomenon <ref> [5, 18] </ref> and not one that has been studied much. <p> One way of ascribing mental states to a system is just to anthropomorphise itto ascribe it human mental characteristics without reference to their real competences, but anthropomorphism is a complex and subtle phenomenon [5, 18] and not one that has been studied much. Eddy et al. <ref> [18] </ref> looked at people's tendency to anthropo-morphise animals, and suggest that there are two primary mechanisms involved: people are likely to attribute similar experiences and cognitive abilities to other animals based on (1) the degree of physical similarity between themselves and the species in question (e.g. primates,) and (2) the degree <p> people are likely to attribute similar experiences and cognitive abilities to other animals based on (1) the degree of physical similarity between themselves and the species in question (e.g. primates,) and (2) the degree to which they have formed an attachment bond with a particular animal (e.g. dogs and cats) <ref> [18] </ref>. <p> Caporael [5] suggests that it is a `default schema' applied to non-social objects, one that is abandoned or modified in the face of contradictory evidence, but the evidence is against either animals or computers really being `non-social' [21, 37] and familiarity can increase rather than decrease the tendency to anthropomorphise <ref> [18] </ref>. Alternatively, perhaps our tendency to anthropomorphise is really a disposition to take the intentional stance [14], to see others as minds rather than as bodies. If, instead of taking the intentional stance, the physical stance is taken, the very different faculty of naive physics will be deployed.
Reference: [19] <author> Jerry A. Fodor. </author> <title> Fodor's guide to mental representation: </title> <journal> The intelligent auntie's vade-mecum. </journal> , <volume> 94 </volume> <pages> 55-97, </pages> <year> 1985. </year>
Reference-contexts: This theory aspect of prediction is that aspect which is most similar to the representational artificial intelligence. Some (e.g. <ref> [19] </ref>) even take it as the complete answer to naive psychology, but this stretches it too far; a strong representational 7 ! ! ( ( )) ( ( )) believes self; X believes self; believes agent; X believes self; perceived agent; X believes self; believes agent; X believes self; believes agent;
Reference: [20] <author> Robert M. </author> <title> French. Subcognition and the limits of the Turing test. </title> , <booktitle> 99 </booktitle> <pages> 53-65, </pages> <year> 1990. </year>
Reference-contexts: Perhaps the case for alien intelligence is poorer than it seemed at first. The onus is on those who claim systems to have it to demonstrate (not define) its independence from human intelligence. If there is no such thing as intelligence in general <ref> [20] </ref>; if the principles of thinking for artificial intelligence are the principles of thinking for human intelligence, we are reduced to the second possibility, and should take the term `intelligence' to mean culturally-oriented human intelligence. After all, this is what the Turing test really tests for [20], which is why its <p> as intelligence in general <ref> [20] </ref>; if the principles of thinking for artificial intelligence are the principles of thinking for human intelligence, we are reduced to the second possibility, and should take the term `intelligence' to mean culturally-oriented human intelligence. After all, this is what the Turing test really tests for [20], which is why its results are sometimes so strange [10, 11, 26, 47].
Reference: [21] <author> Donna Haraway. </author> <title> Otherworldly conversations; terran topics; local terms. </title> , <booktitle> 3(14) </booktitle> <pages> 64-98, </pages> <year> 1992. </year>
Reference-contexts: Anthropomorphism can be interpreted several ways. Caporael [5] suggests that it is a `default schema' applied to non-social objects, one that is abandoned or modified in the face of contradictory evidence, but the evidence is against either animals or computers really being `non-social' <ref> [21, 37] </ref> and familiarity can increase rather than decrease the tendency to anthropomorphise [18]. Alternatively, perhaps our tendency to anthropomorphise is really a disposition to take the intentional stance [14], to see others as minds rather than as bodies.
Reference: [22] <editor> John Haugeland. </editor> <booktitle> The nature and plausibility of cognitivism. </booktitle> , <volume> 1 </volume> <pages> 215-226, </pages> <year> 1978. </year>
Reference-contexts: If artificial intelligence is to overcome this barrier, we humans must be able to see minds in artifacts, to ascribe mental states to artificial intelligences in the same way that we do to people. An example from Haugeland <ref> [22] </ref> will make this clearer. Imagine that somebody presents you with an object and tells you that it can play chess. <p> Haugeland's point is that the meaning of the behaviour isn't an intrinsic property of the behaviour, but is attributed in an empirically justified interpretation <ref> [22] </ref>. But this ability to make the interpretation is itself in part a property of you, the judge. Now expand the problem to the whole of human interaction, which is similarly game-like.
Reference: [23] <author> Patrick J. Hayes. </author> <title> Naive physics I: Ontology for liquids. </title> <editor> In Jerry R. Hobbs and Robert C. Moore, editors, </editor> , <address> pages 71-108. </address> <publisher> Ablex, </publisher> <address> Norwood, New Jersey, </address> <year> 1985. </year> <title> 13 Formal Theories of the Commonsense World Understanding Other Minds: Perspectives From Autism The Mind's I: </title> <booktitle> Fantasies and Reflections on Self and Soul Consciousness Regained Formal Theories of the Commonsense World Science Proceedings of the Ted-dington Conference on the Mechanisation of Thought Processes Philosophical Perspectives in Artificial Intelligence Machine Intelligence 4 Computational Intelligence Artificial Intelligence The Society of Mind Formal Theories of the Commonsense World Proceedings of CHI'94 </booktitle>
Reference-contexts: Sometimes prediction of other people's mental states is better modelled by `simulating' the other person, by pretending to them, and to look at the world from their point of view. Clark [8] suggests that a similar simulation process could even account for naive physicsperhaps Hayes' <ref> [23] </ref> paper on liquids could be recast as a kind of simulation, and as far as the predictions are concerned, viewed externally, there needn't be any difference. For naive psychology, there is evidence that for some predictionsparticularly those involving affective statesan ability to simulate other people works well [25, 38].
Reference: [24] <author> Patrick J. Hayes. </author> <title> The second naive physics manifesto. </title> <editor> In Jerry R. Hobbs and Robert C. Moore, editors, </editor> , <address> pages 1-36. </address> <publisher> Ablex, </publisher> <address> Norwood, New Jersey, </address> <year> 1985. </year>
Reference-contexts: The paper concludes by drawing some conclusions about where and how AI can adapt to deal with these issues and to provide a new and stronger foundation for future research. Ten years ago, Hayes published the Second naive physics manifesto <ref> [24] </ref>. Hayes proposed that we put away childish things by building large-scale formalisations, beginning with our knowledge of the everyday physical world. He, and others, have since put a lot of effort into developing models of our common sense understanding of the physical world. <p> Hayes' initiative clarified many of the issues associated with common sense, and other developments in comparative and developmental psychology have further highlighted the apparently fundamental nature of naive physics [6, 40], but they have also revealed a deeper and bigger problem than that of naive physicsnaive psychology. Naive psychology <ref> [7, 24] </ref> can be thought of as the natural human ability to infer and reason about other people's mental statesin short, to see one another as minds rather than bodies. This is an issue that artificial intelligence must also address. <p> Perhaps Brooks and Stein [3] are right to build Cog with a humanoid form, not for any technical reason, but simply because it will make it easier for us to see Cog as an intelligent system. In the Second naive physics manifesto, Hayes <ref> [24] </ref> concluded with a discussion on the importance of common sense for artificial intelligence. The reasons for this proposal are almost completely orthogonal to the reasons for his, so the justifications are different.
Reference: [25] <author> Peter Hobson. </author> <title> Understanding persons: The role of affect. </title> <editor> In Simon Baron-Cohen, Helen Tager-Flusberg, and Donald J. Cohen, editors, </editor> , <address> pages 204-227. </address> <publisher> Oxford University Press, Oxford, </publisher> <year> 1993. </year>
Reference-contexts: For naive psychology, there is evidence that for some predictionsparticularly those involving affective statesan ability to simulate other people works well <ref> [25, 38] </ref>. Representational artificial intelligence does simulation all the timeit's just another kind of hypothetical reasoning. Simulation, or taking another person's role, is a way that we can understand some aspects of another's mental states; for instance, to recognise somebody's ignorance [13]. <p> So simulation is another way that we can reason about another's mental states. It is a way that works rather better for affective than for cognitive states <ref> [25] </ref> but it doesn't deal with everything: there are some tasks which children actually answer differently, but which they ought to answer the same if they use simulation to get the answer [39]. <p> ( ( )) ( ( )) believes self; X believes self; believes agent; X believes self; perceived agent; X believes self; believes agent; X believes self; believes agent; X agent theory of mind is subject to too many philosophical and evolutionary objections [16], and fails to account for all phenomena <ref> [25, 39] </ref>. But just because a representational theory of mind can't provide a complete naive psychology doesn't mean that it doesn't form part of a complete naive psychology.
Reference: [26] <author> Douglas R. Hofstadter and Daniel C. Dennett. </author> . <title> Basic Books, </title> <address> New York, </address> <year> 1981. </year>
Reference-contexts: After all, this is what the Turing test really tests for [20], which is why its results are sometimes so strange <ref> [10, 11, 26, 47] </ref>. The Turing test is, in many ways, a paradigm example of the effects of naive psychology [5], in that it is, in effect, a test of the ability of one system to ascribe mental states to another. <p> We are then trapped, as Hofstadter was, asking which level does Searle wish us to with? <ref> [26, added emphasis] </ref>. And variations on the theme change the identification: the story changes if we put the room in the head of a robot, partly because robots are a lot easier to anthropomorphise than rooms.
Reference: [27] <author> Nicholas K. </author> <title> Humphrey. </title> . <publisher> Oxford University Press, </publisher> <year> 1984. </year>
Reference-contexts: Survival in these social environments require us to become natural psychologists <ref> [27] </ref>, capable of recognising and reasoning about one another's mental states from their behaviour. Humphrey [27] even suggests that naive physics may, in part, be itself derived from a leaky naive psychology; he suggests that the transactional 2 trans recognise 2 A brief history of naive psychology nature of human social <p> Survival in these social environments require us to become natural psychologists <ref> [27] </ref>, capable of recognising and reasoning about one another's mental states from their behaviour. Humphrey [27] even suggests that naive physics may, in part, be itself derived from a leaky naive psychology; he suggests that the transactional 2 trans recognise 2 A brief history of naive psychology nature of human social interaction is so persistent that even in situations involving physical actions, we often see these
Reference: [28] <author> David Israel. </author> <title> A short companion to the naive physics manifesto. </title> <editor> In Jerry R. Hobbs and Robert C. Moore, editors, </editor> , <address> pages 427-447. </address> <publisher> Ablex, </publisher> <address> Norwood, New Jersey, </address> <year> 1985. </year>
Reference-contexts: If representations had to become situated to deal with the physical world, situated approaches have to become representational to deal with the psychological one. The conflict between logical and psychological approaches to artificial intelligence is not a new one <ref> [28, 29] </ref>, but there is little to add to this controversy beyond what has already been written (e.g. [28, 33, 35].) The logicist project as it stands is unlikely to succeed in producing a causal accountbut for a descriptive project [8] this doesn't matter. <p> The conflict between logical and psychological approaches to artificial intelligence is not a new one [28, 29], but there is little to add to this controversy beyond what has already been written (e.g. <ref> [28, 33, 35] </ref>.) The logicist project as it stands is unlikely to succeed in producing a causal accountbut for a descriptive project [8] this doesn't matter. So there has been significant work in this field, but perhaps artificial intelligence just hasn't realised the scale or importance of the problem. <p> These models both show that it is possible to successfully construct working models for aspects of naive psychology. This proposal is not one of psychologising, making the artificial imitate the natural <ref> [28] </ref>, although this might turn out to be necessary after more research. The 4 without 3 Human and alien intelligence point is that wethe people who are designing and evaluating these machines are people with relatively uniform cultures, societies, and biologiesat least when compared to machines.
Reference: [29] <author> Gina Kolata. </author> <title> How can computers get common sense? , 217 </title> <type> 1237-1238, </type> <year> 1982. </year>
Reference-contexts: If representations had to become situated to deal with the physical world, situated approaches have to become representational to deal with the psychological one. The conflict between logical and psychological approaches to artificial intelligence is not a new one <ref> [28, 29] </ref>, but there is little to add to this controversy beyond what has already been written (e.g. [28, 33, 35].) The logicist project as it stands is unlikely to succeed in producing a causal accountbut for a descriptive project [8] this doesn't matter. <p> Perhaps, as Searle [44] claims, these factors affect human mental phenomena. If so, it would be surprising if they didn't also affect our recognition and interpretation of those phenomena. We can, of course, take McCarthy's stance: this is artificial intelligence and so we don't care if it's psychologically real <ref> [29] </ref>. But as soon as we talk about minds we are, whether we like it or not, involved in something psychological, so to compare minds and computers will inevitably be partly a psychological question. The actual nature of the distinction between human intelligence and artificial intelligence does matter.
Reference: [30] <author> John McCarthy. </author> <title> Programs with common sense. </title> <booktitle> In , pages 75-91, </booktitle> <address> London, </address> <year> 1959. </year> <month> HMSO. </month>
Reference-contexts: Of all the naive disciplines proposed by Hayes and others, naive psychology is the only one that is obviously specifically human, but in all this work there is an implicit anthropocentricity. Right back to McCarthy's <ref> [30] </ref> proposal of common sense, it was assumed that the common sense to be used is human common sense.
Reference: [31] <author> John McCarthy. </author> <title> Ascribing mental qualities to machines. </title> <editor> In Martin Ringle, </editor> <booktitle> editor, </booktitle> , <pages> pages 161-195. </pages> <publisher> Harvester Press, </publisher> <address> Brighton, </address> <year> 1979. </year>
Reference-contexts: These stances deploy natural facultiesso when dealing with a physical system, naive physics is applied, but for a psychological system, naive psychology is applied. Often, both stances could in principle be taken to the same system (even a thermostat, <ref> [31] </ref>,) and although in practice there seems to be a mutual exclusion between the different stances, this is 8 be identify where individual differences in the dispositions and the social context can influence how different people see the same system. <p> The idea of a computational model of a philosopher is perhaps rather bewildering, but if we borrow from McCarthy the idea that a part of philosophy is the science of common sense <ref> [31] </ref> perhaps a computational version of this part isn't so strange after all. Another example of the effects of naive psychology is Woolgar's [50] description of a device which bolts on to a video recorder and splices out advertisements during recording.
Reference: [32] <author> John McCarthy and Patrick J. Hayes. </author> <title> Some philosophical problems from the standpoint of artificial intelligence. </title> <editor> In B. Meltzer and Donald Michie, editors, </editor> , <address> pages 463-502. </address> <publisher> Edinburgh University Press, Edinburgh, </publisher> <year> 1969. </year>
Reference-contexts: The problems for common sense have generally come in two classes, technical and methodological. On the technical side, for example, there is the `frame problem' <ref> [32] </ref>, which Dennett [15] describes as the smoking pistol behind a lot of the 1 understanding attacks on artificial intelligence, and as a major philosophical problem for everybody.
Reference: [33] <author> Drew McDermott. </author> <title> A critique of pure reason. </title> , <booktitle> 3 </booktitle> <pages> 151-160, </pages> <year> 1987. </year>
Reference-contexts: The methodological criticisms for the project have already been adequately aired, but none of these really attack the logicist position as a descriptive project, merely as a causal account of common sense reasoning <ref> [8, 33] </ref>. This paper will present a position orthogonal to that of Hayes. <p> These logics enable representation and reasoning about someone's mental states by making these states unobservable, so these `opaque' agents can believe something which other people know to be false, for example. But representational approaches to naive psychology also have their critics (e.g. <ref> [33, 35] </ref>.) McDermott's [33] criticism is that using axiomatic formalisms and assuming deductive reasoning leaves out somethingMcDermott argues that the representations and the use of those representations cannot be truly separated, as they are separated in formalisms based on pure logic. <p> These logics enable representation and reasoning about someone's mental states by making these states unobservable, so these `opaque' agents can believe something which other people know to be false, for example. But representational approaches to naive psychology also have their critics (e.g. [33, 35].) McDermott's <ref> [33] </ref> criticism is that using axiomatic formalisms and assuming deductive reasoning leaves out somethingMcDermott argues that the representations and the use of those representations cannot be truly separated, as they are separated in formalisms based on pure logic. <p> The conflict between logical and psychological approaches to artificial intelligence is not a new one [28, 29], but there is little to add to this controversy beyond what has already been written (e.g. <ref> [28, 33, 35] </ref>.) The logicist project as it stands is unlikely to succeed in producing a causal accountbut for a descriptive project [8] this doesn't matter. So there has been significant work in this field, but perhaps artificial intelligence just hasn't realised the scale or importance of the problem.
Reference: [34] <author> Donald Michie. </author> <title> Turing's test and conscious thought. </title> , <type> 60(1), </type> <year> 1993. </year>
Reference-contexts: This doesn't mean that naive psychology is logically prior to these problems, but that it is methodologically prior. If artificial intelligence is to tackle consciousness and subjectivity, it must study the complex of intuitions involved in our of subjectivityit is not enough to play operationalist games with it <ref> [34] </ref>. There is another source of evidence in naive psychology.
Reference: [35] <editor> Marvin Minsky. . Simon and Schuster, </editor> <address> New York, </address> <year> 1985. </year>
Reference-contexts: These logics enable representation and reasoning about someone's mental states by making these states unobservable, so these `opaque' agents can believe something which other people know to be false, for example. But representational approaches to naive psychology also have their critics (e.g. <ref> [33, 35] </ref>.) McDermott's [33] criticism is that using axiomatic formalisms and assuming deductive reasoning leaves out somethingMcDermott argues that the representations and the use of those representations cannot be truly separated, as they are separated in formalisms based on pure logic. <p> The conflict between logical and psychological approaches to artificial intelligence is not a new one [28, 29], but there is little to add to this controversy beyond what has already been written (e.g. <ref> [28, 33, 35] </ref>.) The logicist project as it stands is unlikely to succeed in producing a causal accountbut for a descriptive project [8] this doesn't matter. So there has been significant work in this field, but perhaps artificial intelligence just hasn't realised the scale or importance of the problem.
Reference: [36] <author> Robert C. Moore. </author> <title> A formal theory of knowledge and action. </title> <editor> In Jerry R. Hobbs and Robert C. Moore, editors, </editor> , <address> pages 319-358. </address> <publisher> Ablex, </publisher> <address> Norwood, New Jersey, </address> <year> 1985. </year>
Reference-contexts: Naive psychology isn't new to artificial intelligence, which has already tried a number of approaches to the problem. Perhaps the most successful have been the axiomatic formalisms (e.g. <ref> [9, 36] </ref>.) These provide a representation of naive psychology as the ability to make inferences about a set of beliefs, desires, and intentions, corresponding to an agent's mental states.
Reference: [37] <author> Clifford Nass, Jonathan Steuer, and Ellen R. Tauber. </author> <title> Computers are social actors. In , pages 72-78. </title> <publisher> ACM Press, </publisher> <year> 1994. </year> <title> 14 Understanding the Representational Mind Objectivity, Simulation and the Unity of Consciousness Behavioural and Brain Sciences The Concept of Mind Understanding Other Minds: Perspectives from Autism Behavioural and Brain Sciences The Rediscovery of the Mind Natural Theories of Mind: Evolution, Development and Simulation of Everyday Mindreading Mind Computer Power and Human Reason The Child's Theory of Mind Natural Theories of Mind: Evolution, Development and Simulation of Everyday Mindreading Sociology </title>
Reference-contexts: Anthropomorphism can be interpreted several ways. Caporael [5] suggests that it is a `default schema' applied to non-social objects, one that is abandoned or modified in the face of contradictory evidence, but the evidence is against either animals or computers really being `non-social' <ref> [21, 37] </ref> and familiarity can increase rather than decrease the tendency to anthropomorphise [18]. Alternatively, perhaps our tendency to anthropomorphise is really a disposition to take the intentional stance [14], to see others as minds rather than as bodies.
Reference: [38] <editor> Josef Perner. </editor> . <publisher> MIT Press, </publisher> <address> Cam-bridge, Massachusetts, </address> <year> 1991. </year>
Reference-contexts: There is another source of evidence in naive psychology. In developmental psychology in particular, there has been a recent rapid growth in interest in how peopleand some animalsacquire the ability to perceive and reason about one another's mental states (e.g. <ref> [38, 49] </ref>.) This often builds on the beliefs-desires-intentions nexus used in representational naive psychology, but it questions it in other ways. An understanding of beliefs, for instance, doesn't seem to be innate, but is developed between the ages of two-and-a-half and four [38, 48]. <p> An understanding of beliefs, for instance, doesn't seem to be innate, but is developed between the ages of two-and-a-half and four <ref> [38, 48] </ref>. These psychological studies are starting to probe the regularities and structures which underlie the development of human naive psychology. In addition to the theoretical models common in representational approaches to naive psychology, there are a few more practical examples. <p> For naive psychology, there is evidence that for some predictionsparticularly those involving affective statesan ability to simulate other people works well <ref> [25, 38] </ref>. Representational artificial intelligence does simulation all the timeit's just another kind of hypothetical reasoning. Simulation, or taking another person's role, is a way that we can understand some aspects of another's mental states; for instance, to recognise somebody's ignorance [13]. <p> These rules, which a close affinity to those of Davis [13], are a first step to modelling Wellman's [48] account of belief progressing from a copy theory to a representational theory, but there are many other possible models (e.g. <ref> [1, 38] </ref>.) What is actually needed is a way of comparing and combining the different models.
Reference: [39] <author> Josef Perner. </author> <title> The necessity and impossibility of simulation. </title> <booktitle> In Christo-pher Peacocke, editor, </booktitle> , <pages> pages 145-154. </pages> <publisher> Oxford University Press for the British Academy, </publisher> <year> 1994. </year>
Reference-contexts: It is a way that works rather better for affective than for cognitive states [25] but it doesn't deal with everything: there are some tasks which children actually answer differently, but which they ought to answer the same if they use simulation to get the answer <ref> [39] </ref>. Something is left over, and that something is a `theory' of mind not a theory in the scientific sense [7, 44]simply a theory in the sense of a set of tools for thinking about the unobservables of another person's mental states. <p> ( ( )) ( ( )) believes self; X believes self; believes agent; X believes self; perceived agent; X believes self; believes agent; X believes self; believes agent; X agent theory of mind is subject to too many philosophical and evolutionary objections [16], and fails to account for all phenomena <ref> [25, 39] </ref>. But just because a representational theory of mind can't provide a complete naive psychology doesn't mean that it doesn't form part of a complete naive psychology.
Reference: [40] <author> David Premack and Guy Woodruff. </author> <title> Does the chimpanzee have a `theory of mind'? , 4 </title> <type> 515-526, </type> <year> 1978. </year>
Reference-contexts: This paper will present a position orthogonal to that of Hayes. Hayes' initiative clarified many of the issues associated with common sense, and other developments in comparative and developmental psychology have further highlighted the apparently fundamental nature of naive physics <ref> [6, 40] </ref>, but they have also revealed a deeper and bigger problem than that of naive physicsnaive psychology. Naive psychology [7, 24] can be thought of as the natural human ability to infer and reason about other people's mental statesin short, to see one another as minds rather than bodies.
Reference: [41] <author> Gilbert Ryle. . Hutchinson, </author> <year> 1949. </year>
Reference-contexts: This, again, is an assumption, and while it works well a lot of the time, there are some mental states which fit uneasily in this model (e.g. moods, hostility.) A lot of these are dispositional a la Ryle <ref> [41] </ref>, who shows how a considerable part of typical human conversation is not in the indicative mood of propositional statements.
Reference: [42] <author> Jerry Samet. </author> <title> Autism and theory of mind: Some philosophical perspectives. </title> <editor> In Simon Baron-Cohen, Helen Tager-Flusberg, and Donald J. Cohen, editors, </editor> , <address> pages 427-449. </address> <publisher> Oxford University Press, Oxford, </publisher> <year> 1993. </year>
Reference-contexts: But besides this methodological criticism, there are deeper onesare mental states really best described in terms of beliefs, desires, and intentions? <ref> [16, 42] </ref>. <p> But this proposal is methodologically different from Hayes' in that working models of naive psychology are to be constructed. There are several reasons for this: first, simply because the psychologists and philosophers have asked for one and lament artificial intelligence having lost much of its ambition in this area <ref> [42] </ref>. Secondly, in the absence of a working model, theories become too hard to evaluate, especially for naive psychology which depends on comparison with people. A third and more fundamental reason is that for the higher orders of mental states, a compatibility between psychologies is essential.
Reference: [43] <author> John R. Searle. </author> <title> Minds, brains, </title> <booktitle> and programs. </booktitle> , <volume> 3 </volume> <pages> 417-424, </pages> <year> 1980. </year>
Reference-contexts: The theory stance, on the other hand, is better at dealing with external, behavioural, questions. How well do these models do? Although they barely hint at the true complexity of naive psychology, they do have some predictive powerthis can be shown with Searle's <ref> [43] </ref> Chinese Room. This is not the place for a new rebuttal or any analysis of Searle's argument, but the intuitive part of Searle's experiment is attractive because he appeals to plausibility and insists on us taking the first person stance when we look at the scenario.
Reference: [44] <author> John R. </author> <title> Searle. </title> . <publisher> MIT Press, </publisher> <address> Cambridge, Mas-sachusetts, </address> <year> 1992. </year>
Reference-contexts: The 4 without 3 Human and alien intelligence point is that wethe people who are designing and evaluating these machines are people with relatively uniform cultures, societies, and biologiesat least when compared to machines. Perhaps, as Searle <ref> [44] </ref> claims, these factors affect human mental phenomena. If so, it would be surprising if they didn't also affect our recognition and interpretation of those phenomena. We can, of course, take McCarthy's stance: this is artificial intelligence and so we don't care if it's psychologically real [29]. <p> But most of the interesting stuff in the Turing test is going on in the head of the observer, not only in the system under test [11]. It is not, as Searle <ref> [44] </ref> takes it to be, third person or `objective.' We might even suggest an inverted Turing test: putting a system in the role of the observer and looking at its ability to distinguish between humans and programs could be a useful way of evaluating its naive psychology. <p> Something is left over, and that something is a `theory' of mind not a theory in the scientific sense <ref> [7, 44] </ref>simply a theory in the sense of a set of tools for thinking about the unobservables of another person's mental states. This theory aspect of prediction is that aspect which is most similar to the representational artificial intelligence.
Reference: [45] <author> Thomas R. Shultz. </author> <title> From agency to intention: A rule-based computational approach. </title> <booktitle> In Andrew Whiten, editor, </booktitle> , <pages> pages 79-95. </pages> <publisher> Basil Blackwell, Oxford, </publisher> <year> 1991. </year>
Reference-contexts: In one, Davis [13] described a model by which one agent could infer the limits on another agent's knowledge from knowledge about its perceptual limitsthis shows a close affinity with some of the `false belief' work in developmental psychology (e.g. [1, 48].) And in another, Shultz <ref> [45] </ref> described a model for the ascription of intentions to other agents, using rules which relate behaviour and mental states. These models both show that it is possible to successfully construct working models for aspects of naive psychology.
Reference: [46] <author> Alan M. </author> <title> Turing. </title> <journal> Computing machinery and intelligence. </journal> , <volume> LIX(2236):433-460, </volume> <year> 1950. </year>
Reference-contexts: This is because human intelligence isn't restricted to any particular narrow set of behaviours. That which we recognise as intelligence can changeindeed it is pretty certain that computers are introducing such a change. One of Turing's <ref> [46] </ref> points was that much hangs on the use of words and general educated opinion. Artificial 5 Model one: anthropomorphism 4 Models for naive psychology intelligence isn't just a case of making machines smarter, but also of making us better at seeing their smartness.
Reference: [47] <author> Joseph Weizenbaum. . W. H. Freeman, </author> <year> 1976. </year>
Reference-contexts: After all, this is what the Turing test really tests for [20], which is why its results are sometimes so strange <ref> [10, 11, 26, 47] </ref>. The Turing test is, in many ways, a paradigm example of the effects of naive psychology [5], in that it is, in effect, a test of the ability of one system to ascribe mental states to another.
Reference: [48] <author> Henry M. </author> <title> Wellman. </title> . <publisher> MIT Press, </publisher> <address> Cambridge, Massachusetts, </address> <year> 1990. </year>
Reference-contexts: An understanding of beliefs, for instance, doesn't seem to be innate, but is developed between the ages of two-and-a-half and four <ref> [38, 48] </ref>. These psychological studies are starting to probe the regularities and structures which underlie the development of human naive psychology. In addition to the theoretical models common in representational approaches to naive psychology, there are a few more practical examples. <p> In one, Davis [13] described a model by which one agent could infer the limits on another agent's knowledge from knowledge about its perceptual limitsthis shows a close affinity with some of the `false belief' work in developmental psychology (e.g. <ref> [1, 48] </ref>.) And in another, Shultz [45] described a model for the ascription of intentions to other agents, using rules which relate behaviour and mental states. These models both show that it is possible to successfully construct working models for aspects of naive psychology. <p> Whether or not this account is correct or not isn't really the issue. These rules, which a close affinity to those of Davis [13], are a first step to modelling Wellman's <ref> [48] </ref> account of belief progressing from a copy theory to a representational theory, but there are many other possible models (e.g. [1, 38].) What is actually needed is a way of comparing and combining the different models.
Reference: [49] <editor> Andrew Whiten, editor. </editor> . <publisher> Basil Blackwell, Oxford, </publisher> <year> 1991. </year>
Reference-contexts: There is another source of evidence in naive psychology. In developmental psychology in particular, there has been a recent rapid growth in interest in how peopleand some animalsacquire the ability to perceive and reason about one another's mental states (e.g. <ref> [38, 49] </ref>.) This often builds on the beliefs-desires-intentions nexus used in representational naive psychology, but it questions it in other ways. An understanding of beliefs, for instance, doesn't seem to be innate, but is developed between the ages of two-and-a-half and four [38, 48].
Reference: [50] <author> Steve Woolgar. </author> <title> Why not a sociology of machines? the case of sociology and artificial intelligence. </title> , <booktitle> 19 </booktitle> <pages> 557-572, </pages> <year> 1985. </year> <month> 15 </month>
Reference-contexts: Another example of the effects of naive psychology is Woolgar's <ref> [50] </ref> description of a device which bolts on to a video recorder and splices out advertisements during recording. <p> if you then read the instructions, and they tell you that it actually works by detecting a particular 9 theory 5 Naive psychology for machines signal in the transmission, this changes the ascription of intelligence, and redefines and thus reserves the attribute of `intelligence' for some future assessment of performance <ref> [50] </ref>. Again, the change in our knowledge affects the stance that we takeaffects whether or not we see the system from the intentional stance. This integrated model shows a sensitivity to physical form and our knowledge of the system's design which is perhaps rather distressing for strong artificial intelligence.
References-found: 50

