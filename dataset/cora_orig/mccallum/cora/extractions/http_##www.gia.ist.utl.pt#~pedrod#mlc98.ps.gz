URL: http://www.gia.ist.utl.pt/~pedrod/mlc98.ps.gz
Refering-URL: http://www.cs.wisc.edu/icml98/schedule.html
Root-URL: 
Email: pedrod@gia.ist.utl.pt  
Title: A Process-Oriented Heuristic for Model Selection  
Author: Pedro Domingos 
Address: Lisbon 1096, Portugal  
Affiliation: Artificial Intelligence Group Instituto Superior Tecnico  
Abstract: Current methods to avoid overfitting are either data-oriented (using separate data for validation) or representation-oriented (penalizing complexity in the model). This paper proposes process-oriented evaluation, where a model's expected generalization error is computed as a function of the search process that led to it. The paper develops the necessary theoretical framework, and applies it to one type of learning: rule induction. A process-oriented version of the CN2 rule learner is empirically compared with the default CN2. The process-oriented version is more accurate in a large majority of the datasets, with high significance, and also produces simpler models. Experiments in artificial domains suggest that process-oriented evaluation is particularly useful in high-dimensional domains.
Abstract-found: 1
Intro-found: 1
Reference: <author> Akaike, H. </author> <year> (1978). </year> <title> A Bayesian analysis of the minimum AIC procedure. </title> <journal> Annals of the Institute of Statistical Mathematics, </journal> <volume> 30A, </volume> <pages> 9-14. </pages>
Reference: <author> Breiman, L., Friedman, J. H., Olshen, R. A., & Stone, C. J. </author> <year> (1984). </year> <title> Classification and Regression Trees. </title> <address> Belmont, CA: </address> <publisher> Wadsworth. </publisher>
Reference: <author> Brunk, C., & Pazzani, M. J. </author> <year> (1991). </year> <title> An investigation of noise-tolerant relational concept learning algorithms. </title> <booktitle> Proceedings of the Eighth International Workshop on Machine Learning (pp. </booktitle> <pages> 389-393). </pages> <address> Evanston, IL: </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Current methods to address this problem fall into two broad categories. Data-oriented evaluation uses separate data to learn and validate models, and includes methods like cross-validation ( Breiman, Friedman, Olshen & Stone, 1984; Stone, 1974 ) , the bootstrap ( Efron & Tibshirani, 1993 ) , and reduced-error pruning <ref> ( Brunk & Pazzani, 1991 ) </ref> . It has several disadvantages: it is often computationally intensive, reduces the data available for learning, can be unreliable if the validation set is small, and is itself prone to overfitting if a large number of models is compared ( Ng, 1997 ) .
Reference: <author> Chatfield, C. </author> <year> (1995). </year> <title> Model uncertainty, data mining and statistical inference. </title> <journal> Journal of the Royal Statistical Society A, </journal> <volume> 158. </volume>
Reference-contexts: Evaluating models that are the result of a search process, not just of fitting the parameters of a predetermined structure, has traditionally not been a concern of statisticians. However, this is beginning to change <ref> ( Chatfield, 1995 ) </ref> . Some of the arguments made here for taking into account the number of hypotheses attempted are made in greater detail in ( Cohen & Jensen, 1997 ) and ( Ng, 1997 ) .
Reference: <author> Cheeseman, P. </author> <year> (1990). </year> <title> On finding the most probable model. </title> <editor> In J. Shrager & P. Langley (Eds.), </editor> <booktitle> Computational Models of Scientific Discovery and Theory Formation (pp. </booktitle> <pages> 73-95). </pages> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Cheeseman, P., & Oldford, R. W. </author> <year> (1994). </year> <pages> Preface. </pages>
Reference-contexts: 1 INTRODUCTION Overfitting avoidance is often considered the central problem of machine learning (e.g., <ref> ( Cheeseman & Oldford, 1994 ) </ref> ). If a learner is sufficiently powerful, it must guard against selecting a model that fits the training data well but captures the underlying phenomenon poorly. Current methods to address this problem fall into two broad categories.
Reference: <editor> In P. Cheeseman & R. W. Oldford (Eds.), </editor> <title> Selecting Models from Data: </title> <booktitle> Artificial Intelligence and Statistics IV. </booktitle> <address> New York: </address> <publisher> Springer-Verlag. </publisher>
Reference: <author> Clark, P., & Boswell, R. </author> <year> (1991). </year> <title> Rule induction with CN2: Some recent improvements. </title> <booktitle> Proceedings of the Sixth European Working Session on Learning (pp. </booktitle> <pages> 151-163). </pages> <address> Porto, Portugal: </address> <publisher> Springer-Verlag. </publisher>
Reference-contexts: AQ ( Michalski, 1983 ) continues adding conditions until the rule is "pure" (i.e., until it covers examples of only one class). This can lead to severe overfitting. The latest version of the CN2 system <ref> ( Clark & Boswell, 1991 ) </ref> uses a simple and effective Bayesian method to combat this: induction of a rule stops when no specialization improves its error rate, and the latter is computed using a Laplace correction or m-estimate. <p> CN2 uses m=c. As conditions are added, the rule covers fewer and fewer examples, and ^* r tends to * 0 . Thus a rule making more misclassifications may be preferred if it covers more examples, causing induction to stop earlier and reducing overfitting. Clark and Boswell <ref> ( Clark & Boswell, 1991 ) </ref> found this version of CN2 to be more accurate than C4.5 ( Quinlan, 1993 ) on 10 of the 12 benchmark datasets they used for testing. <p> However, the Laplace correction distorts the values used by Equation 15. This will be particularly pronounced when there are many classes, since CN2 uses m = c. In order to minimize this problem, m = 2 was used with POE. 3 The experimental procedure of <ref> ( Clark & Boswell, 1991 ) </ref> was followed. Each dataset was randomly divided into 67% for training and 33% for testing, and the error rate and theory size (total number of conditions) were measured for default CN2 and CN2-POE. This was repeated 20 times. <p> and CN2-POE (error CN2 error CN2P OE ) should increase with the dataset's number of at 3 Simply changing m = c to m = 2 in default CN2 does not change its performance on the datasets used. 4 There are some differences between CN2's results and those reported in <ref> ( Clark & Boswell, 1991 ) </ref> . This may be due to the fact that the default version of CN2 uses a beam size of 5, whereas Clark and Boswell used b = 20. <p> those reported in <ref> ( Clark & Boswell, 1991 ) </ref> . This may be due to the fact that the default version of CN2 uses a beam size of 5, whereas Clark and Boswell used b = 20. The distribution version of CN2 may also differ from the one used in ( Clark & Boswell, 1991 ) . tributes, since this will increase the number of rule versions generated in each round. In order to test this hypothesis, experiments were carried out in artificial domains. Concepts defined as Boolean functions in disjunctive normal form were used as targets.
Reference: <author> Clark, P., & Niblett, T. </author> <year> (1989). </year> <title> The CN2 induction algorithm. </title> <journal> Machine Learning, </journal> <volume> 3, </volume> <pages> 261-283. </pages>
Reference-contexts: This paper begins to develop this approach, which we will call process-oriented evaluation (POE for short). The basic theoretical framework is presented, and then applied to the standard "separate and conquer" rule induction process <ref> ( Clark & Niblett, 1989 ) </ref> . An empirical study demonstrates the effectiveness of POE. The paper concludes with sections on related and future work. 2 PROCESS-ORIENTED EVALUATION Consider the simplest example of an overfitting avoidance problem, in a classification context.
Reference: <author> Cohen, P. R., & Jensen, D. </author> <year> (1997). </year> <title> Overfitting explained. </title> <booktitle> Preliminary Papers of the Sixth International Workshop on Artificial Intelligence and Statistics (pp. </booktitle> <pages> 115-122). </pages> <address> Fort Lauderdale, FL: </address> <booktitle> Society for Artificial Intelligence and Statistics. </booktitle>
Reference-contexts: However, this is beginning to change ( Chatfield, 1995 ) . Some of the arguments made here for taking into account the number of hypotheses attempted are made in greater detail in <ref> ( Cohen & Jensen, 1997 ) </ref> and ( Ng, 1997 ) .
Reference: <editor> DeGroot, M. H. </editor> <booktitle> (1986). Probability and Statistics (2nd ed.). </booktitle> <address> Reading, MA: </address> <publisher> Addison-Wesley. </publisher>
Reference-contexts: Suppose n = 100, e 1 = 12, and e 2 = 11. Should we prefer h 1 or h 2 ? According to the maximum likelihood principle <ref> ( DeGroot, 1986 ) </ref> , ^* 1 = 0:12 and ^* 2 = 0:11, so h 2 should be chosen. Assuming the two hypotheses have the same complexity or prior probability, representation-oriented evaluation would give the same answer.
Reference: <author> Domingos, P. </author> <year> (1997). </year> <title> Why does bagging work? A Bayesian account and its implications. </title> <booktitle> Proceedings of the Third International Conference on Knowledge Discovery and Data Mining (pp. </booktitle> <pages> 155-158). </pages> <address> Newport Beach, CA: </address> <publisher> AAAI Press. </publisher>
Reference-contexts: case, because a close approximation of E [* 2 jn; e 2 ] is not required; all that is required is that E [* 2 jn; e 2 ] &gt; E [* 1 jn; e 1 ] iff * 2 &gt; * 1 , which is a much weaker condition <ref> ( Domingos & Pazzani, 1997 ) </ref> .
Reference: <author> Domingos, P. </author> <year> (1998). </year> <title> Occam's two razors: The sharp and the blunt. </title> <note> Submitted. </note>
Reference: <author> Domingos, P., & Pazzani, M. </author> <year> (1997). </year> <title> On the optimality of the simple Bayesian classifier under zero-one loss. </title> <journal> Machine Learning, </journal> <volume> 29, </volume> <pages> 103-130. </pages>
Reference-contexts: case, because a close approximation of E [* 2 jn; e 2 ] is not required; all that is required is that E [* 2 jn; e 2 ] &gt; E [* 1 jn; e 1 ] iff * 2 &gt; * 1 , which is a much weaker condition <ref> ( Domingos & Pazzani, 1997 ) </ref> .
Reference: <author> Efron, B., & Tibshirani, R. J. </author> <year> (1993). </year> <title> An Introduction to the Bootstrap. </title> <address> New York: </address> <publisher> Chapman and Hall. </publisher>
Reference-contexts: Current methods to address this problem fall into two broad categories. Data-oriented evaluation uses separate data to learn and validate models, and includes methods like cross-validation ( Breiman, Friedman, Olshen & Stone, 1984; Stone, 1974 ) , the bootstrap <ref> ( Efron & Tibshirani, 1993 ) </ref> , and reduced-error pruning ( Brunk & Pazzani, 1991 ) .
Reference: <author> Gaines, B. R. </author> <year> (1989). </year> <title> An ounce of knowledge is worth a ton of data. </title> <booktitle> Proceedings of the Sixth International Workshop on Machine Learning (pp. </booktitle> <pages> 156-159). </pages> <address> Ithaca, NY: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Heckerman, D. </author> <year> (1996). </year> <title> Bayesian networks for knowledge discovery. </title> <editor> In U. M. Fayyad, G. Piatetsky-Shapiro, P. Smyth, & R. Uthurusamy (Eds.), </editor> <booktitle> Advances in Knowledge Discovery and Data Mining (pp. </booktitle> <pages> 273-305). </pages> <address> Menlo Park, CA: </address> <publisher> AAAI Press. </publisher>
Reference-contexts: the dependent case: p (* m jn; e m ) / p (e m jn; * m ) p (8 1im e m;i &gt; e m jn; * m ) (11) Evaluating this expression when high-order dependencies are present will generally not be feasible, but the standard Bayesian network approach <ref> ( Heckerman, 1996 ) </ref> is applicable here: the number of training errors e m;i of each hypothesis h m;i generated by L m can be viewed as a node in a Bayesian network, whose parents are the training errors of the hypotheses h m;j it is primarily dependent on.
Reference: <author> Holte, R. C., Acker, L. E., & Porter, B. W. </author> <year> (1989). </year> <title> Concept learning and the problem of small disjuncts. </title> <booktitle> Proceedings of the Eleventh International Joint Conference on Artificial Intelligence (pp. </booktitle> <pages> 813-818). </pages> <address> Detroit, MI: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Huber, P. J. </author> <year> (1994). </year> <title> Languages for statistics and data analysis. </title> <editor> In P. Dirschedl & R. Ostermann (Eds.), </editor> <booktitle> Computational Statistics. </booktitle> <address> Heidelberg: </address> <publisher> Physica-Verlag. </publisher>
Reference: <author> Jensen, D., & Schmill, M. </author> <year> (1997). </year> <title> Adjusting for multiple comparisons in decision tree pruning. </title> <booktitle> Proceedings of the Third International Conference on Knowledge Discovery and Data Mining (pp. </booktitle> <pages> 195-198). </pages> <address> Newport Beach, CA: </address> <publisher> AAAI Press. </publisher>
Reference-contexts: However, this is beginning to change ( Chatfield, 1995 ) . Some of the arguments made here for taking into account the number of hypotheses attempted are made in greater detail in <ref> ( Cohen & Jensen, 1997 ) </ref> and ( Ng, 1997 ) .
Reference: <author> Kass, G. V. </author> <year> (1980). </year> <title> An exploratory technique for investigating large quantities of categorical data. </title> <journal> Applied Statistics, </journal> <volume> 29, </volume> <pages> 119-127. </pages>
Reference: <author> Kearns, M. J., & Vazirani, U. V. </author> <year> (1994). </year> <title> An Introduction to Computational Learning Theory. </title> <address> Cam-bridge, MA: </address> <publisher> MIT Press. </publisher>
Reference-contexts: Structural risk minimization ( Vapnik, 1995 ) and PAC learning <ref> ( Kearns & Vazirani, 1994 ) </ref> are representation-oriented methods that seek to bound the difference between training and generalization error using a function of the model space's (effective) dimension. This typically produces bounds that are overly broad, and requires severely restricting the model space.
Reference: <author> Klockars, A. J., & Sax, G. </author> <year> (1986). </year> <title> Multiple Comparisons. </title> <address> Beverly Hills, CA: </address> <publisher> Sage. </publisher>
Reference: <author> Lawrence, S., Giles, C. L., & Tsoi, A. C. </author> <year> (1997). </year> <title> Lessons in neural network training: Overfitting may be harder than expected. </title> <booktitle> Proceedings of the Fourteenth National Conference on Artificial Intelligence (pp. </booktitle> <pages> 540-545). </pages> <address> Providence, RI: </address> <publisher> AAAI Press. </publisher>
Reference: <author> MacKay, D. </author> <year> (1992). </year> <title> Bayesian interpolation. </title> <journal> Neural Computation, </journal> <volume> 4, </volume> <pages> 415-447. </pages>
Reference: <author> Merz, C. J., Murphy, P. M., & Aha, D. W. </author> <year> (1997). </year> <title> UCI repository of machine learning databases. </title> <institution> Department of Information and Computer Science, University of California at Irvine, </institution> <address> Irvine, CA. </address>
Reference-contexts: Integrating Equation 14 every time E [* m k j~n m k ; ^* m k ] needs to be computed (once 2 With the exception of pole-and-cart, which is not available in the UCI repository <ref> ( Merz, Murphy & Aha, 1997 ) </ref> . per round) would generally significantly slow down the rule induction process. Instead, it was approximated by: B m (n^* m 1jn; * m ) B m (n^* m jn; * m ) (15) where n = 1 m i=1 n i .
Reference: <author> Michalski, R. S. </author> <year> (1983). </year> <title> A theory and methodology of inductive learning. </title> <journal> Artificial Intelligence, </journal> <volume> 20, </volume> <pages> 111-161. </pages>
Reference-contexts: In the beam search process used by many rule learners, at each step the best b versions of the rule according to some evaluation function are selected for further specialization. AQ <ref> ( Michalski, 1983 ) </ref> continues adding conditions until the rule is "pure" (i.e., until it covers examples of only one class). This can lead to severe overfitting.
Reference: <author> Miller, Jr., R. G. </author> <year> (1981). </year> <title> Simultaneous Statistical Inference (2nd ed.). </title> <address> New York: </address> <publisher> Springer-Verlag. </publisher>
Reference: <author> Moody, J. E. </author> <year> (1992). </year> <title> The effective number of parameters: An analysis of generalization and regularization in learning systems. </title> <editor> In J. E. Moody, S. J. Hanson, & R. P. Lippmann (Eds.), </editor> <booktitle> Advances in Neural Information Processing Systems 4 (pp. </booktitle> <pages> 847-854). </pages> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Murphy, P., & Pazzani, M. </author> <year> (1994). </year> <title> Exploring the decision forest. </title> <journal> Journal of Artificial Intelligence Research, </journal> <volume> 1, </volume> <pages> 257-275. </pages>
Reference: <author> Ng, A. Y. </author> <year> (1997). </year> <title> Preventing "overfitting" of cross-validation data. </title> <booktitle> Proceedings of the Fourteenth International Conference on Machine Learning (pp. </booktitle> <pages> 245-253). </pages> <address> Nashville, TN: </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: It has several disadvantages: it is often computationally intensive, reduces the data available for learning, can be unreliable if the validation set is small, and is itself prone to overfitting if a large number of models is compared <ref> ( Ng, 1997 ) </ref> . Representation-oriented evaluation seeks to avoid these problems by using the same data for training and validation, but a priori penalizing some models as more likely to overfit. Bayesian approaches in general fall into this category ( Cheeseman, 1990; MacKay, 1992 ) . <p> case, because a close approximation of E [* 2 jn; e 2 ] is not required; all that is required is that E [* 2 jn; e 2 ] &gt; E [* 1 jn; e 1 ] iff * 2 &gt; * 1 , which is a much weaker condition <ref> ( Domingos & Pazzani, 1997 ) </ref> . <p> However, this is beginning to change ( Chatfield, 1995 ) . Some of the arguments made here for taking into account the number of hypotheses attempted are made in greater detail in ( Cohen & Jensen, 1997 ) and <ref> ( Ng, 1997 ) </ref> .
Reference: <author> Pagallo, G., & Haussler, D. </author> <year> (1990). </year> <title> Boolean feature discovery in empirical learning. </title> <journal> Machine Learning, </journal> <volume> 3, </volume> <pages> 71-99. </pages>
Reference: <author> Pearl, J. </author> <year> (1978). </year> <title> On the connection between the complexity and credibility of inferred models. </title> <journal> International Journal of General Systems, </journal> <volume> 4, </volume> <pages> 255-264. </pages>
Reference: <author> Press, W. H., Teukolsky, S. A., Vetterling, W. T., & Flannery, B. P. </author> <year> (1992). </year> <title> Numerical Recipes in C (2nd ed.). </title> <address> Cambridge, UK: </address> <publisher> Cambridge University Press. </publisher>
Reference-contexts: Numerical integration (Equation 7) was performed using Simpson's rule, and B (ejn; *) (Equation 2) was computed using the incomplete beta function <ref> ( Press, Teukolsky, Vet-terling & Flannery, 1992 ) </ref> .
Reference: <author> Quinlan, J. R. </author> <year> (1993). </year> <title> C4.5: Programs for Machine Learning. </title> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Thus a rule making more misclassifications may be preferred if it covers more examples, causing induction to stop earlier and reducing overfitting. Clark and Boswell ( Clark & Boswell, 1991 ) found this version of CN2 to be more accurate than C4.5 <ref> ( Quinlan, 1993 ) </ref> on 10 of the 12 benchmark datasets they used for testing. However, this scheme ignores that, as more and more conditions are attempted, the probability of finding one that appears to reduce the rule's error merely by chance increases.
Reference: <author> Quinlan, J. R., & Cameron-Jones, R. M. </author> <year> (1995). </year> <title> Over-searching and layered search in empirical learning. </title> <booktitle> Proceedings of the Fourteenth International Joint Conference on Artificial Intelligence (pp. </booktitle> <pages> 1019-1024). </pages> <address> Montreal, Canada: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Rissanen, J. </author> <year> (1978). </year> <title> Modeling by shortest data description. </title> <journal> Automatica, </journal> <volume> 14, </volume> <pages> 465-471. </pages>
Reference: <author> Schaffer, C. </author> <year> (1993). </year> <title> Overfitting avoidance as bias. </title> <journal> Machine Learning, </journal> <volume> 10, </volume> <pages> 153-178. </pages>
Reference: <author> Schuurmans, D., Ungar, L. H., & Foster, D. P. </author> <year> (1997). </year> <title> Characterizing the generalization performance of model selection strategies. </title> <booktitle> Proceedings of the Fourteenth International Conference on Machine Learning (pp. </booktitle> <pages> 340-348). </pages> <address> Nashville, TN: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Schwarz, G. </author> <year> (1978). </year> <title> Estimating the dimension of a model. </title> <journal> Annals of Statistics, </journal> <volume> 6, </volume> <pages> 461-464. </pages>
Reference: <author> St. Amant, R., & Cohen, P. R. </author> <year> (1997). </year> <title> Building an EDA assistant: A progress report. </title> <booktitle> Preliminary Papers of the Sixth International Workshop on Artificial Intelligence and Statistics (pp. </booktitle> <pages> 501-512). </pages> <address> Ft. Lauderdale, FL: </address> <booktitle> Society for Artificial Intelligence and Statistics. </booktitle>
Reference: <author> Stone, M. </author> <year> (1974). </year> <title> Cross-validatory choice and assessment of statistical predictions. </title> <journal> Journal of the Royal Statistical Society B, </journal> <volume> 36, </volume> <pages> 111-147. </pages>
Reference: <author> Vapnik, V. N. </author> <year> (1995). </year> <title> The Nature of Statistical Learning Theory. </title> <address> New York: </address> <publisher> Springer-Verlag. </publisher>
Reference-contexts: Structural risk minimization <ref> ( Vapnik, 1995 ) </ref> and PAC learning ( Kearns & Vazirani, 1994 ) are representation-oriented methods that seek to bound the difference between training and generalization error using a function of the model space's (effective) dimension.
Reference: <author> Wallace, C. S., & Boulton, D. M. </author> <year> (1968). </year> <title> An information measure for classification. </title> <journal> Computer Journal, </journal> <volume> 11, </volume> <pages> 185-194. </pages>
Reference: <author> Webb, G. I. </author> <year> (1996). </year> <title> Further experimental evidence against the utility of Occam's razor. </title> <journal> Journal of Artificial Intelligence Research, </journal> <volume> 4, </volume> <pages> 397-417. </pages>
Reference: <author> Westfall, P. H., & Wolfinger, R. D. </author> <year> (1997). </year> <title> Multiple tests with discrete distributions. </title> <journal> American Statistician, </journal> <volume> 51, </volume> <pages> 3-8. </pages>
References-found: 46

