URL: http://utstat.toronto.edu/reports/tibs/combine.ps
Refering-URL: http://utstat.toronto.edu/reports/tibs/
Root-URL: 
Title: Combining estimates in regression and classification  
Author: Michael LeBlanc and Robert Tibshirani 
Note: c flUniversity of Toronto  
Date: December 13, 1993  
Affiliation: Department of Preventive Medicine and Biostatistics and Department of Statistics University of Toronto  
Abstract: We consider the problem of how to combine a collection of general regression fit vectors in order to obtain a better predictive model. The individual fits may be from subset linear regression, ridge regression, or something more complex like a neural network. We develop a general framework for this problem and examine a recent cross-validation-based proposal called "stacking" in this context. Combination methods based on the bootstrap and analytic methods are also derived and compared in a number of examples, including best subsets regression and regression trees. Finally, we apply these ideas to classification problems where the estimated combination weights can yield insight into the structure of the problem.
Abstract-found: 1
Intro-found: 1
Reference: <author> Breiman, L. </author> <year> (1993). </year> <title> Stacked regression. </title> <type> Technical report, </type> <institution> Univ. of Cal, Berkeley. </institution>
Reference: <author> Breiman, L., Friedman, J., Olshen, R., and Stone, C. </author> <year> (1984). </year> <title> Classification and Regression Trees. </title> <publisher> Wadsworth International Group. </publisher>
Reference-contexts: We consider the sequence of models derived by the cost-complexity pruning algorithm of the Classification and Regression Tree (CART) algorithm <ref> (Breiman, Friedman, Olshen and Stone, 1984) </ref>. The cost-complexity measure of tree performance is R ff (T ) = h2 ~ T R (h) + ff [ # of terminal nodes in T ]; 11 Table 1: Average model errors (and standard errors) for Example 1.
Reference: <author> Clark L. and Pregibon D. </author> <year> (1992). </year> <title> Statistical Models in S, chapter Tree-Based models. </title> <publisher> Wadsworth International Group. </publisher>
Reference: <author> Efron, B. </author> <year> (1983). </year> <title> Estimating the error rate of a prediction rule: some improvements on cross-validation. </title> <journal> J. Amer. Statist. Assoc., </journal> <volume> 78 </volume> <pages> 316-331. </pages>
Reference: <author> Efron, B. and Tibshirani, R. </author> <year> (1993). </year> <title> An Introduction to the Bootstrap. </title> <publisher> Chapman and Hall, </publisher> <address> New York. </address>
Reference: <author> Hastie, T.J. and Tibshirani, R.J. </author> <year> (1993). </year> <title> Varying coefficient models (with discussion). </title> <journal> J. Royal Statist. Soc. B, </journal> <volume> 55 </volume> <pages> 757-796. </pages> <note> 20 Lawson, </note> <editor> C.L. and Hanson, </editor> <address> R.J. </address> <year> (1974). </year> <title> Solving Least Squares Problems. </title>
Reference: <editor> Prentice-Hall: </editor> <address> Englewood Cliffs, N.J. </address>
Reference: <author> Lowe, David G. </author> <year> (1993). </year> <title> Similarity metric learning for a variable kernel classifier. </title> <type> Technical report, </type> <institution> Dept. of Comp Sci, Univ. of British Columbia. </institution>
Reference: <author> McCullagh, P. and Tibshirani, R. </author> <year> (1988). </year> <title> A simple adjustment for profile likelihoods. </title> <journal> J. Royal. Statist. Soc. B., </journal> <volume> 52(2) </volume> <pages> 325-344. </pages>
Reference: <author> Wolpert, D. </author> <year> (1992). </year> <title> Stacked generalization. </title> <booktitle> Neural Networks, </booktitle> <volume> 5 </volume> <pages> 241-259. 21 </pages>
References-found: 10

