URL: http://www.cs.berkeley.edu/~murphyk/Articles/agentMC.ps.gz
Refering-URL: http://www.cs.berkeley.edu/~murphyk/publ.html
Root-URL: http://www.cs.berkeley.edu
Title: Software Agent for Recommending Movies  
Author: Kevin P. Murphy 
Date: 14 December 1995  
Note: A  
Abstract: CS 289 Final Project Report: 
Abstract-found: 1
Intro-found: 1
Reference: [CS95] <author> P. Cheeseman and J. Stutz. </author> <title> Bayesian classification (autoclass): Theory and results. </title> <editor> In Fayyad, Pratetsky-Shapiro, Smyth, and Uthurasamy, editors, </editor> <booktitle> Advances in Knowledge Discovery and Data Mining. </booktitle> <publisher> MIT Press, </publisher> <year> 1995. </year>
Reference-contexts: Rather than using hierarchical clustering (see e.g., [Seb84]), something like Autoclass <ref> [CS95] </ref> might be more appropriate. Rather than forcing each point to belong to only one cluster, Autoclass can specify the probability that a point belongs to each cluster; this is a kind of "fuzzy membership" function.
Reference: [DH73] <author> R. O. Duda and P. E. Hart. </author> <title> Pattern Classification and Scene Analysis. </title> <publisher> Wiley Interscience, </publisher> <year> 1973. </year>
Reference-contexts: In the second, I apply a variety of standard machine learning techniques, including decision trees [Qui86] and Bayes classifiers <ref> [DH73] </ref>, to try and learn a mapping from attribute values to ratings. The use of the word "agent" in the title is a little misleading.
Reference: [EW94] <author> O. Etzioni and D. Weld. </author> <title> A softbot-based interface to the internet. </title> <journal> Comm. of the ACM, </journal> <volume> 37(7), </volume> <year> 1994. </year>
Reference-contexts: The use of the word "agent" in the title is a little misleading. My original intention was to write a program which would be capable of "crawling the web" to find out about new movies, similar to "softbots" <ref> [EW94] </ref>. However, I never had time to do this. The main difficulty is that the information is stored in many different locations, and in many different formats. As far as I know, all of the programs mentioned above require information about new movies to be entered by hand.
Reference: [Fel57] <author> W. Feller. </author> <title> An Introduction to Probability Theory and Its Applications. </title> <publisher> Wiley, </publisher> <year> 1957. </year>
Reference-contexts: Clearly we need to normalize the coefficients in each row of the similarity matrix to 1 (although this requires that we ignore the negative coefficients). * A correlation of zero does not mean the ratings are not related! (see e.g., <ref> [Fel57] </ref>). Nevertheless, it is not clear how to use the other user's ratings in such a case. The best we can do in this case is to predict the mean value. * The calculation is only reliable if the profiles of the users have many movies in common.
Reference: [Haa94] <author> K. B. Haase. FRAMER: </author> <title> A persistent portable representation library. </title> <booktitle> In European Conf. on AI, </booktitle> <year> 1994. </year> <note> Submitted. </note>
Reference-contexts: A more sophisticated lexical analyser would also help (see e.g., [vR79]). It would be interesting to try some techniques from the field of natural language understanding as well. For example, Framer <ref> [Haa94] </ref> is a knowledge representation library which can draw analogies in natural language texts, and could be useful in this context. Finally, the classifier results are also hard to interpret, partly because of the conflicting measures of performance.
Reference: [Hol93] <author> R. C. Holte. </author> <title> Very simple classification rules perform well on most commonly used datasets. </title> <journal> Machine Learning, </journal> <volume> 11 </volume> <pages> 63-91, </pages> <year> 1993. </year>
Reference-contexts: However, Geoff Zweig let me use his decision tree code, which does bottom-up 2 pruning. This achieved roughly the same performance as the best case of ID3 with top-down pruning, but resulted in a simpler, two-leaf tree. This is in agreement with the conclusions reached in <ref> [Hol93] </ref>, where he finds that simple, depth one decision trees ("one-rules") often perform nearly as well as much more complex trees. 4.2 Discussion The bottom line is that all of the classification techniques do very poorly in this domain, as judged by the low accuracy rates.
Reference: [JKP94] <author> G. H. John, R. Kohavi, and K. Pfleger. </author> <title> Irrelevant features and the subset selection problem. </title> <booktitle> In Machine Learning: Proc. of the 11th Intl. Conf, </booktitle> <year> 1994. </year>
Reference-contexts: Of course, some individuals direct a lot (relatively speaking) of movies, but this may still be only a weakly relevant feature. For example, you might have hated all the movies by Steven Spielberg except for "Schindler's List". In <ref> [JKP94] </ref> they give an example where including a feature which predicts the classification 75% of the time can actually lower the accuracy of a decision tree. In this project, I chose to only consider the genre feature, for simplicity. <p> Finally, the classifier results are also hard to interpret, partly because of the conflicting measures of performance. It certainly seems plausible that by allowing more features, such as director and cast, and using a feature subset selection algorithm (such as the one in <ref> [JKP94] </ref>), that the performance could equal that of 10 each rating category (1 through 10) which reach this node. The word inside the internal nodes represents a feature value to test for; 0 means absent, 1 means present.
Reference: [KJL + 94] <author> R. Kohavi, G. John, R. Long, D. Manley, and K. Pfleger. MLC++: </author> <title> A machine learning library in C++. </title> <booktitle> In Tools with Artificial Intelligence, </booktitle> <pages> pages 740-743, </pages> <year> 1994. </year> <note> Available by anonymous ftp from: starry.stanford.edu:pub/ronnyk/mlc/toolsmlc.ps. </note>
Reference-contexts: This is called a local encoding. In my dataset, the number of possible genres is 120 (including such things as "based-on-true-story", "part-animated", "culture-clash", etc.) By making use of an existing library of machine learning routines written in C++ <ref> [KJL + 94] </ref>, I had enough time to compare a variety of different classifiers. The results are summarized in Table 4. The majority classifier simply predicts the rating that occurs most often; in my dataset, this is 8. It forms a baseline for comparison purposes.
Reference: [Mae94] <author> P. Maes. </author> <title> Agents that reduce work and information overload. </title> <journal> Comm. of the ACM, </journal> <volume> 37(7), </volume> <year> 1994. </year> <month> 13 </month>
Reference-contexts: This method has been called "social information filtering" <ref> [SM95, Mae94] </ref>, and is in contrast to content-based filtering, which uses information about the movie itself, rather than just the ratings of other users. One source for such information is the huge Internet Movie Database (IMDB) 4 , which contains data on over 60,000 movies.
Reference: [Qui86] <author> J. R. Quinlan. </author> <title> Induction of decision trees. </title> <journal> Machine Learning, </journal> <volume> 1 </volume> <pages> 81-106, </pages> <year> 1986. </year> <note> Reprinted in Readings in Machine Learning, Shavlik and Dietterich, eds. </note>
Reference-contexts: In the first, the reviews for an unseen movie are compared, using the vector space model [SM83], with reviews of previously rated movies, and a weighted average of these ratings is computed. In the second, I apply a variety of standard machine learning techniques, including decision trees <ref> [Qui86] </ref> and Bayes classifiers [DH73], to try and learn a mapping from attribute values to ratings. The use of the word "agent" in the title is a little misleading. <p> ID3 is a simple method for inducing binary decision trees <ref> [Qui86] </ref>, which we discuss in more detail below. 4.1 Using decision trees Since there are so many features, it is important that we do not overfit the data by inducing a decision tree in which all the leaves are pure, i.e., only contain one example.
Reference: [Qui93] <author> J. R. Quinlan. </author> <title> C4.5 Programs for Machine Learning. </title> <publisher> Morgan Kauffman, </publisher> <year> 1993. </year>
Reference-contexts: The tree which consists of two leaves branches on the "action" attribute. The documentation for ML-C++ recommends that serious users of decision trees use C4.5, a more robust version of ID3 which comes with Quinlan's book <ref> [Qui93] </ref>. Unfortunately, the book costs $45 and the disk costs an additional $35, and I was not able to locate a free copy. However, Geoff Zweig let me use his decision tree code, which does bottom-up 2 pruning.
Reference: [Seb84] <author> G. A. F. Seber. </author> <title> Multivariate Observations. </title> <publisher> John Wiley, </publisher> <year> 1984. </year>
Reference-contexts: Rather than using hierarchical clustering (see e.g., <ref> [Seb84] </ref>), something like Autoclass [CS95] might be more appropriate. Rather than forcing each point to belong to only one cluster, Autoclass can specify the probability that a point belongs to each cluster; this is a kind of "fuzzy membership" function.
Reference: [She94] <author> B. D. Sheth. </author> <title> A learning approach to personalized information filtering. </title> <type> Master's thesis, </type> <institution> MIT, </institution> <year> 1994. </year>
Reference-contexts: So we can use a non-uniform prior on the classes. I shall discuss the results of using a Bayes classifer with a different set of attributes later. 3 Applying the vector space model to reviews In the vector space model <ref> [SM83, She94] </ref>, a text document is represented as a vector of weights, &lt; w ij &gt;, where w ij is the weight of term j in text T i . A term is usually a word, or possibly a word-stem.
Reference: [SM83] <author> Salton and McGill. </author> <title> Introduction to Modern Information Retrieval. </title> <publisher> McGraw-Hill, </publisher> <year> 1983. </year>
Reference-contexts: The purpose of this project is to compare the effectiveness of social-based and content-based filtering methods for recommending movies. I use two main approaches to content-based filtering. In the first, the reviews for an unseen movie are compared, using the vector space model <ref> [SM83] </ref>, with reviews of previously rated movies, and a weighted average of these ratings is computed. In the second, I apply a variety of standard machine learning techniques, including decision trees [Qui86] and Bayes classifiers [DH73], to try and learn a mapping from attribute values to ratings. <p> So we can use a non-uniform prior on the classes. I shall discuss the results of using a Bayes classifer with a different set of attributes later. 3 Applying the vector space model to reviews In the vector space model <ref> [SM83, She94] </ref>, a text document is represented as a vector of weights, &lt; w ij &gt;, where w ij is the weight of term j in text T i . A term is usually a word, or possibly a word-stem.
Reference: [SM95] <author> U. Shardanand and P. Maes. </author> <title> Social information filtering: algorithms for automating 'word of mouth'. </title> <booktitle> In Proc. CHI. Conf., </booktitle> <address> Denver, CO., </address> <month> May </month> <year> 1995. </year>
Reference-contexts: This method has been called "social information filtering" <ref> [SM95, Mae94] </ref>, and is in contrast to content-based filtering, which uses information about the movie itself, rather than just the ratings of other users. One source for such information is the huge Internet Movie Database (IMDB) 4 , which contains data on over 60,000 movies. <p> Unfortunately, 1 http://www.moviecritic.com and http://www.labs.bt.com/innovate/multimed/morse/morse.htm. 2 http://www.agents-inc.com. This system was first called Ringo, then HOMR, and now Firefly. It has been described in <ref> [SM95] </ref>. 3 http://www.surf.com/rare/home.html 4 http://rte66.com/Movies/ This is the US mirror of the original site in Cardiff, UK. 5 Part of the problem was that I was using Lincoln Stein's Perl CGI library, which may contain security loopholes. However, it is tremendously useful. <p> The rest of this report describes the results of various experiments on this dataset. 2 Social information filtering The key thing in the social information filtering algorithm is to specify how to compute similarity between two users' profiles. Shardananda and Maes <ref> [SM95] </ref> suggest using the (Pearson) correlation coefficient, defined as follows. <p> hindsight, I realise that I should have produced a much shorter list, so that more people would respond to my survey. 7 This scale was chosen to be compatible with IMDB, even though studies have shown that one does not gain much information in going beyond a seven point scale <ref> [SM95] </ref>. 2 where U is the set of users whose similarity to x is greater than , a threshold, and whose rating of movie i is not ?, meaning unknown. The idea of the threshold is to ignore data from users who are not sufficiently correlated. They suggest setting 0:35. <p> The best we can do in this case is to predict the mean value. * The calculation is only reliable if the profiles of the users have many movies in common. In my experience of using the Firefly system for recommending CDs <ref> [SM95] </ref>, I found it would recommend music on the basis of a single artist that I had rated similarly to another user, which is not a very reliable indicator. Nevertheless, Shardanand and Maes apparently get better results than the baseline (naive) strategy of simply predicting each user's mean rating. <p> As M increases, the tree gets smaller. information filtering: in <ref> [SM95] </ref> they find that jEj=1.3; using ID3, I find jEj 1:6; the naive method scores jEj=1.8. Thirdly, and perhaps most importantly, a single attribute such as genre just does not contain enough information to be a good predictor; after all, there are good dramas and bad dramas. <p> This issue is discussed more in the conclusions section. 5 Conclusions and further work My results suggest that social information filtering is much worse than the naive strategy of simply predicting the most common rating, but this is probably due to the small size of my dataset | results in <ref> [SM95] </ref> with 1000 users suggest that social information filtering is much better. It would be interesting to verify this result on a large dataset, and also to carry out the experiments on clustering and user stereotyping mentioned earlier. My results on the vector space model are inconclusive.
Reference: [vR79] <author> C. J. van Rijsbergen. </author> <title> Information Retrieval. </title> <publisher> Butterworths, </publisher> <year> 1979. </year> <month> 14 </month>
Reference-contexts: A more sophisticated lexical analyser would also help (see e.g., <ref> [vR79] </ref>). It would be interesting to try some techniques from the field of natural language understanding as well. For example, Framer [Haa94] is a knowledge representation library which can draw analogies in natural language texts, and could be useful in this context.
References-found: 16

