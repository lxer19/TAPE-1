URL: http://www-ccs.cs.umass.edu/rtdb/paper/two_phase.ps
Refering-URL: http://www-ccs.cs.umass.edu/rtdb/publications.html
Root-URL: 
Title: Towards Predictable Transaction Executions in Real-Time Database Systems  
Author: Patrick E. O'Neil Krithi Ramamritham Calton Pu 
Note: 1 partially supported by the National Science Foundation under grants CDA-8922572, IRI-9109210, and IRI-9114197. 2 partially supported by NSF,  
Address: Boston, MA 02125  Amherst, MA 01003  New York, NY 10027  Instruments.  
Affiliation: Dept. of Math Comp. Sc. Univ. of Massachusetts  Dept. of Computer Sc. Univ. of Massachusetts  Dept. of Computer Sc. Columbia University  IBM, DEC, AT&T, Oki Electric Ind. and Texas  
Abstract: Even though considerable research has been done on concurrency control protocols that take the timing constraints of transactions into account, most of these protocols do not predict before a transaction begins execution whether the transaction will meet its deadline. Hence the protocols do not directly tackle the problems introduced by the various sources of unpredictability encountered in typical database systems, including data dependence of transaction execution, data and resource conflicts, dynamic paging in virtual memory systems, disk I/O, and transaction aborts with the resulting rollbacks and restarts. On the other hand, the approach described in this paper has the potential to provide predictable transaction executions. This approach exploits the access invariance property that many transactions possess or can be designed to possess. The execution path of a transaction with this property is unlikely to change as a result of data modifications by other concurrent transactions. In this approach, a transaction goes through two phases. In the first phase, called the prefetch phase, a transaction is run once, bringing the necessary data into main memory that is not in memory already. No writes are performed in this phase and data conflicts with other transactions are not considered. The overall computational demands of the transaction are also determined during this phase. At the end of the prefetch phase, the system attempts to guarantee that the transaction will complete by its deadline. This is done by planning the execution of the transaction taking into account data and resource conflicts with the transactions already guaranteed such that the transaction meets its deadline. If such a plan can be constructed, the transaction's execution phase begins and if access invariance holds then the transaction is committed at the end of this phase. A number of variations on this approach (such as using optimistic concurrency control in the prefetch phase or handling occasional failures of access invariance) are also investigated. The benefits of the approach are also discussed. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> R. Abbott and H. Garcia-Molina, </author> <title> "Scheduling Real-Time Transactions: A Performance Evaluation," </title> <booktitle> Proceedings of the 14th VLDB Conference, </booktitle> <month> Aug. </month> <year> 1988. </year>
Reference-contexts: This paper considers the predictable processing of dynamically arriving transactions that have deadlines. It assumes that transactions operate on a disk-resident database, where only a small fraction of accessed data can be cached in memory. Although considerable research has been done recently on processing transactions with time constraints <ref> [1, 2, 4, 10, 13, 14, 15, 16, 20] </ref> for the most part, this work has not directly tackled the problems introduced by the various sources of unpredictability (discussed in Section 2).
Reference: [2] <author> R. Abbott and H. Garcia-Molina, </author> <title> "Scheduling Real-Time Transactions with Disk Resident Data," </title> <booktitle> Proceedings of the 15th VLDB Conference, </booktitle> <year> 1989. </year> <month> 21 </month>
Reference-contexts: This paper considers the predictable processing of dynamically arriving transactions that have deadlines. It assumes that transactions operate on a disk-resident database, where only a small fraction of accessed data can be cached in memory. Although considerable research has been done recently on processing transactions with time constraints <ref> [1, 2, 4, 10, 13, 14, 15, 16, 20] </ref> for the most part, this work has not directly tackled the problems introduced by the various sources of unpredictability (discussed in Section 2).
Reference: [3] <author> P.A. Bernstein, V. Hadzilacos, and N. Goodman. </author> <title> Concurrency Control and Recovery in Database Systems. </title> <publisher> Addison-Wesley, </publisher> <address> Reading, MA, </address> <year> 1987. </year>
Reference: [4] <author> A.P. Buchmann, D.R. McCarthy, M. Chu, and U. Dayal, </author> <title> "Time-Critical Database Scheduling: A Framework for Integrating Real-Time Scheduling and Concurrency Control", </title> <booktitle> Proceedings of the Conference on Data Engineering, </booktitle> <year> 1989. </year>
Reference-contexts: This paper considers the predictable processing of dynamically arriving transactions that have deadlines. It assumes that transactions operate on a disk-resident database, where only a small fraction of accessed data can be cached in memory. Although considerable research has been done recently on processing transactions with time constraints <ref> [1, 2, 4, 10, 13, 14, 15, 16, 20] </ref> for the most part, this work has not directly tackled the problems introduced by the various sources of unpredictability (discussed in Section 2). <p> begins execution and starts making changes to the data whether the transaction will meet its deadline. (Because of this, transactions may be aborted when their deadlines expire and this in turn can have adverse effects on other ongoing transactions.) Perhaps the only exceptions are the ideas discussed in [21] and <ref> [4] </ref>. Both consider transactions whose execution times as well as data requirements are assumed to be known a priori. Reference [21] considers periodic transactions operating on a main memory database. In [4], an approach is suggested where the system creates schedules that take into account the data requirements of contending transactions. <p> can have adverse effects on other ongoing transactions.) Perhaps the only exceptions are the ideas discussed in [21] and <ref> [4] </ref>. Both consider transactions whose execution times as well as data requirements are assumed to be known a priori. Reference [21] considers periodic transactions operating on a main memory database. In [4], an approach is suggested where the system creates schedules that take into account the data requirements of contending transactions. Unfortunately, not all applications can predeclare data requirements, since in many cases transaction executions are data dependent. Secondly, main memory database systems limit the size of the database.
Reference: [5] <author> R.G.G. Cattell, </author> <title> Object Data Management. </title> <publisher> Addison-Wesley, </publisher> <address> Reading, MA, </address> <year> 1987. </year>
Reference-contexts: After doing this in the prefetch phase we should be able to retain a pointer to the appropriate buffer for use in the execution phase, a practice known as swizzling in object-oriented databases <ref> [5] </ref>. * CPU overhead for physical I/Os, for example creating channel programs for reading and writing. * Process switching. When a process becomes blocked because of a need for I/O in the prefetch phase, a process switch is required. * Transaction Commitment, which is part of the execution phase.
Reference: [6] <author> S. Chen, J. Stankovic, J. Kurose, and D. Towsley, </author> <title> "Performance Evaluation of Two New Disk Scheduling Algorithms for Real-Time Systems", </title> <booktitle> Real-Time Systems, </booktitle> <month> Sept. </month> <year> 1991. </year>
Reference-contexts: Also, disk access delays constitute one of the primary sources of unpredictability of transaction executions. Naturally, the approach to I/O scheduling should also be deadline sensitive (see for example <ref> [6] </ref>). We point out that our approach keeps track of deadlines during the prefetch phase and will abort a transaction if it is already past its deadline.
Reference: [7] <author> K. Elhardt and R. Bayer, </author> " <title> A Database Cache for High Performance and Fast Restart in Database Systems", </title> <journal> ACM TODS, </journal> <volume> Vol 9, No. 4, </volume> <pages> pp. 503-525. </pages>
Reference-contexts: A variation of the scheme proposed in <ref> [7] </ref> can be used to accomplish this. In our case, a transaction makes changes only to copies of the data pages and discards the copies if its execution phase terminates prematurely. Thus, in some cases, a transaction may go through the prefetch phase multiple times.
Reference: [8] <author> Peter A. Franaszek, John T. Robinson, and Alexander Thomasian, </author> <title> "Access Invari-ance and its Use in High Contention Environments", </title> <booktitle> Proceedings of the Sixth International Conference on Database Engineering, </booktitle> <year> 1990, </year> <pages> pp 47-55. </pages>
Reference-contexts: This is a property known as access invariance, which is investigated, for instance, in <ref> [8] </ref>. Thus we assume, at the end of the prefetch phase, that all the necessary data is in memory. At this point, the system attempts to guarantee that the transaction will complete by its deadline. <p> If validation fails, we can retry and schedule the transaction for memory-resident execution. This is close to the approach taken in <ref> [8] </ref>. If we commonly have deadlines that will not be met because of I/O as discussed above, and we have a very large CPU-comp component, then we will be wasting a lot of CPU time as well before we find out that the deadline can't be met. <p> Other researchers have used a prefetch based approach implicitly or explicitly. In <ref> [8] </ref>, prefetch was used to improve throughput in high contention transactional environments. In [17], investigating time-critical stock trading situations, a type of prefetch was used to reduce contention and thus improve throughput. <p> In terms of work that remains to be done, we should note that we are beginning to undertake an implementation and evaluation of this approach. We also plan to test our assumptions about access invariance in the context of different database applications. In <ref> [8] </ref>, the authors have shown via simulation studies that access variance can be exploited in traditional databases to improve performance, that is, to reduce response times.
Reference: [9] <author> Jim Gray and Franco Putzolu, </author> <title> "The Five Minute Rule for Trading Memory for Disk Accesses and the 10 Byte Rule for Trading Memory for CPU Time", </title> <booktitle> Proceedings of the 1987 ACM SIGMOD Conference, </booktitle> <pages> pp 395-398. </pages>
Reference: [10] <author> J.R. Haritsa, M.J. Carey and M. Livny, </author> <title> "On Being Optimistic about Real-Time Constraints," </title> <booktitle> Proceedings of ACM PODS, </booktitle> <year> 1990. </year>
Reference-contexts: This paper considers the predictable processing of dynamically arriving transactions that have deadlines. It assumes that transactions operate on a disk-resident database, where only a small fraction of accessed data can be cached in memory. Although considerable research has been done recently on processing transactions with time constraints <ref> [1, 2, 4, 10, 13, 14, 15, 16, 20] </ref> for the most part, this work has not directly tackled the problems introduced by the various sources of unpredictability (discussed in Section 2).
Reference: [11] <author> J.R. Haritsa, M.J. Carey and M. Livny, </author> <title> "Dynamic Real-Time Optimistic Concur-rency Control," </title> <booktitle> Proceedings of the Real-Time Systems Symposium, </booktitle> <month> Dec. </month> <year> 1990. </year>
Reference: [12] <author> J.R. Haritsa, M.J. Carey and M. Livny, </author> <title> "Earliest Deadline Scheduling for Real-Time Database Systems," </title> <booktitle> Proceedings of the Real-Time Systems Symposium, </booktitle> <month> Dec. </month> <year> 1991. </year>
Reference-contexts: That is, it will be beneficial to employ a filter at the beginning of a transaction's prefetch phase so that it enters the system only if its chances of being completed by its deadline are high <ref> [12] </ref>. We expect to put further thought into these questions in future work. 5 Extensions to the Basic Approach In this section, we examine two extensions to the approach outlined earlier.
Reference: [13] <author> J. Huang, J.A. Stankovic, D. Towsley and K. Ramamritham, </author> <title> "Experimental Evaluation of Real-Time Transaction Processing," </title> <booktitle> Proceedings of the Real-Time Systems Symposium, </booktitle> <month> Dec. </month> <year> 1989 </year>
Reference-contexts: This paper considers the predictable processing of dynamically arriving transactions that have deadlines. It assumes that transactions operate on a disk-resident database, where only a small fraction of accessed data can be cached in memory. Although considerable research has been done recently on processing transactions with time constraints <ref> [1, 2, 4, 10, 13, 14, 15, 16, 20] </ref> for the most part, this work has not directly tackled the problems introduced by the various sources of unpredictability (discussed in Section 2).
Reference: [14] <author> J. Huang, J.A. Stankovic, K. Ramamritham and D. Towsley, </author> <title> "Experimental Evaluation of Real-Time Optimistic Concurrency Control Schemes", </title> <booktitle> Proceedings of the Conference on Very Large Data Bases, </booktitle> <month> Sep </month> <year> 1991. </year>
Reference-contexts: This paper considers the predictable processing of dynamically arriving transactions that have deadlines. It assumes that transactions operate on a disk-resident database, where only a small fraction of accessed data can be cached in memory. Although considerable research has been done recently on processing transactions with time constraints <ref> [1, 2, 4, 10, 13, 14, 15, 16, 20] </ref> for the most part, this work has not directly tackled the problems introduced by the various sources of unpredictability (discussed in Section 2).
Reference: [15] <author> J. Huang, J.A. Stankovic, K. Ramamritham and D. Towsley, </author> <title> "On Using Priority Inheritance in Real-Time Databases," </title> <booktitle> Proceedings of the Real-Time Systems Symposium December 1991. </booktitle>
Reference-contexts: This paper considers the predictable processing of dynamically arriving transactions that have deadlines. It assumes that transactions operate on a disk-resident database, where only a small fraction of accessed data can be cached in memory. Although considerable research has been done recently on processing transactions with time constraints <ref> [1, 2, 4, 10, 13, 14, 15, 16, 20] </ref> for the most part, this work has not directly tackled the problems introduced by the various sources of unpredictability (discussed in Section 2).
Reference: [16] <author> Y. Lin and S.H. Son, </author> <title> "Concurrency Control in Real-Time Databases by Dynamic Adjustment of Serialization Order," </title> <booktitle> Proceedings of the Real-Time Systems Symposium, </booktitle> <month> Dec. </month> <year> 1990. </year> <month> 22 </month>
Reference-contexts: This paper considers the predictable processing of dynamically arriving transactions that have deadlines. It assumes that transactions operate on a disk-resident database, where only a small fraction of accessed data can be cached in memory. Although considerable research has been done recently on processing transactions with time constraints <ref> [1, 2, 4, 10, 13, 14, 15, 16, 20] </ref> for the most part, this work has not directly tackled the problems introduced by the various sources of unpredictability (discussed in Section 2).
Reference: [17] <author> Peter Peinl, Andreas Reuter, </author> <title> and Harald Sammer "High Contention in a Stock Trad--ing Database, a Case Study", </title> <booktitle> Proceedings of the 1988 SIGMOD Conference, </booktitle> <pages> pp 260-268. </pages>
Reference-contexts: Other researchers have used a prefetch based approach implicitly or explicitly. In [8], prefetch was used to improve throughput in high contention transactional environments. In <ref> [17] </ref>, investigating time-critical stock trading situations, a type of prefetch was used to reduce contention and thus improve throughput. However, we believe that the current paper is the first one to explicitly use dynamic prefetching and conflict avoidance scheduling to execute real-time transactions predictably.
Reference: [18] <author> C. Pu and A. Leff. </author> <title> Replica Control in Distributed Systems: An Asynchronous Approach. </title> <booktitle> In Proceedings of the ACM SIGMOD International Conference on Management of Data, </booktitle> <pages> pages 377-386, </pages> <month> May </month> <year> 1991. </year>
Reference: [19] <author> K. Ramamritham, J. Stankovic, and P. Shiah, </author> <title> "Efficient Scheduling Algorithms for Real-Time Multiprocessor Systems," </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> Vol. 1, No. 2, </volume> <month> April </month> <year> 1990, </year> <pages> pp. 184-194. </pages>
Reference-contexts: If such a plan cannot be constructed, the system aborts the transaction. Otherwise, the transaction's execution phase begins. The transaction is committed at the end of the execution phase. The notion of guarantee and the scheduling algorithm are based on the resource constrained scheduling approach proposed for real-time systems <ref> [19] </ref>. Salient aspects of the algorithm are summarized in Section 4. <p> When a transaction is guaranteed, the scheduler attempts to plan a schedule for it and the previously guaranteed transactions so that all transactions can make their deadlines. In Section 4.1 we briefly discuss how the scheduler works. Further details can be found in <ref> [19] </ref>. Executing the prefetch phase and planning for transaction guarantees can be done in parallel with the execution of previously guaranteed transactions. <p> If the value of k is constant (and in practice, k will be small when compared to the transaction set size n), the complexity is linearly proportional to n, the size of the transaction set <ref> [19] </ref>. While the complexity is proportional to n, the algorithm is programmed so that it incurs a fixed worst case cost by limiting the number of H function evaluations permitted in any one 15 invocation of the algorithm. See [19] for a discussion on how to choose k. <p> is linearly proportional to n, the size of the transaction set <ref> [19] </ref>. While the complexity is proportional to n, the algorithm is programmed so that it incurs a fixed worst case cost by limiting the number of H function evaluations permitted in any one 15 invocation of the algorithm. See [19] for a discussion on how to choose k. The heuristic function H can be constructed by simple or integrated heuristics. In the context of real-time tasks, extensive simulation studies of the algorithm [19] show that an H function based on T d , a task's deadline and T est , <p> See <ref> [19] </ref> for a discussion on how to choose k. The heuristic function H can be constructed by simple or integrated heuristics. In the context of real-time tasks, extensive simulation studies of the algorithm [19] show that an H function based on T d , a task's deadline and T est , its earliest start time, has superior performance.
Reference: [20] <author> K. Ramamritham, </author> <title> "Real-Time Databases" International Journal of Distributed and Parallel Databases, </title> <note> to appear. </note>
Reference-contexts: This paper considers the predictable processing of dynamically arriving transactions that have deadlines. It assumes that transactions operate on a disk-resident database, where only a small fraction of accessed data can be cached in memory. Although considerable research has been done recently on processing transactions with time constraints <ref> [1, 2, 4, 10, 13, 14, 15, 16, 20] </ref> for the most part, this work has not directly tackled the problems introduced by the various sources of unpredictability (discussed in Section 2).
Reference: [21] <author> L. Sha, R. Rajkumar and J.P. Lehoczky, </author> <title> "Concurrency Control for Distributed Real-Time Databases," </title> <booktitle> ACM SIGMOD Record, </booktitle> <month> March </month> <year> 1988. </year>
Reference-contexts: a transaction begins execution and starts making changes to the data whether the transaction will meet its deadline. (Because of this, transactions may be aborted when their deadlines expire and this in turn can have adverse effects on other ongoing transactions.) Perhaps the only exceptions are the ideas discussed in <ref> [21] </ref> and [4]. Both consider transactions whose execution times as well as data requirements are assumed to be known a priori. Reference [21] considers periodic transactions operating on a main memory database. <p> may be aborted when their deadlines expire and this in turn can have adverse effects on other ongoing transactions.) Perhaps the only exceptions are the ideas discussed in <ref> [21] </ref> and [4]. Both consider transactions whose execution times as well as data requirements are assumed to be known a priori. Reference [21] considers periodic transactions operating on a main memory database. In [4], an approach is suggested where the system creates schedules that take into account the data requirements of contending transactions. Unfortunately, not all applications can predeclare data requirements, since in many cases transaction executions are data dependent.
Reference: [22] <author> C. Shen, K. Ramamritham, and J. Stankovic, </author> <title> Resource Reclaiming in Real-Time, </title> <booktitle> Proc Real-Time System Symposium, </booktitle> <month> December </month> <year> 1990. </year>
Reference-contexts: It should be pointed out that even though a transaction is guaranteed with respect to its worst case computation time and data requirements, it is possible to reclaim the time allocated to a transaction's execution phase should it terminate early <ref> [22] </ref>. Now let us examine some of the details of the approach. In the next subsection, we study the ramifications of the access invariance property. <p> Finally, recall that a guarantee is done with respect to a transaction's worst case computational needs. Resource reclaiming refers to the problem of correctly and effectively utilizing the resources unused by a transaction when a transaction executes less than its worst case computation time. <ref> [22] </ref> discusses two resource reclaiming algorithms that effectively reclaim the unused times. One of the positive fallouts of reclaiming is that now we can afford to be pessimistic about the computation times of transactions.
Reference: [23] <author> John R. Wilke, </author> <title> "Parallel Computing Finds Mainstream Use", Wall Street Journal Technology Column, </title> <note> January 6, 1992, pg B1. 23 </note>
Reference-contexts: The overall computational demands of the transaction are also determined during this phase. (Computation-intensive parts of the transactions, such as match finding algorithms that relate two patterns in vision systems as well as risk analysis in financial applications <ref> [23] </ref>, need not be run during 4 the access phase as long as (1) the execution of these parts is not necessary for determining the subsequent data requirements of the transactions, and (2) the execution times of these portions can be estimated a priori.) Let us first assume that the data
References-found: 23

