URL: http://www.cs.berkeley.edu/~yelick/yelick/psls95.ps
Refering-URL: http://www.cs.berkeley.edu/~yelick/papers.html
Root-URL: 
Email: fyelick,cpwen,soumen,deprit,jjones,arvindkg@cs.berkeley.edu  
Title: Portable Parallel Irregular Applications  
Author: Katherine Yelick, Chih-Po Wen, Soumen Chakrabarti, Etienne Deprit, Jeff Jones, and Arvind Krishnamurthy 
Affiliation: U.C. Berkeley, Computer Science Division  
Abstract: Software developers for distributed memory multiprocessors often complain about the lack of libraries and tools for developing and performance tuning their applications. While some tools exist for regular array-based computations, support for applications with pointer-based data structures, asynchronous communication patterns, or unpredictable computational costs is seriously lacking. In this paper we describe our experience with six irregular applications from CAD, Robotics, Genetics, Physics, and Computer Science, and offer them as application challenges for other systems that support irregular applications. The applications vary in the amount and kind of irregularity. We characterize their irregularity profiles and the implementation problems that arise from those profiles. In addition to performance, one of our goals is to provide implementations that run efficiently with minimal performance tuning across machine platforms, and our designs are influenced by this desire for performance portability. Each of our applications is organized around one or two distributed data structures, which are part of the Multipol data structure library. We describe these data structures, give an overview of some key features in our underlying runtime support, and present performance results for the applications on three platforms. 
Abstract-found: 1
Intro-found: 1
Reference: [And93] <author> Andrew A. Chien. </author> <title> Concurrent Aggregates: Supporting Modularity in Massively-Parallel Programs. </title> <publisher> MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1993. </year>
Reference-contexts: In all of these projects, the key parallelization task was the development and performance tuning of distributed data structures. Although several languages and runtime systems support the development of such data structures <ref> [And93, BBG + 93, FLR92, SK91, SL93, CCK92] </ref>, there are no comprehensive data structure libraries, such as those that exist for uniprocessors. Multipol is such a library. Multipol is designed to help programmers write irregular applications such as discrete event simulation, symbolic computation, and search problems.
Reference: [Bad91] <author> S. B. Baden. </author> <title> Programming Abstractions for Dynamically Partitioning and Coordinating Localized Scientific Calculations Running on Multiprocessors. </title> <journal> SIAM J. Sci. Stat. Comput., </journal> <volume> 12(1) </volume> <pages> 145-157, </pages> <year> 1991. </year>
Reference-contexts: For machines like the Paragon and workstation networks, which have high communication start-up cost, the former is very important. Many small messages are aggregated into one large physical message to amortize the overhead. Several other systems, including Chaos [DUSH94] and LPARX <ref> [Bad91] </ref>, also use message aggregation.
Reference: [BBG + 93] <author> F. Bodin, P. Beckman, D. Gannon, S. Yang, S. Kesavan, A. Maloney, and B. Mohr. </author> <title> Implementing a Parallel C++ Runtime System for Scalable Parallel System. </title> <booktitle> In Supercomputing '93, </booktitle> <pages> pages 588-597, </pages> <address> Portland, Oregon, </address> <month> November </month> <year> 1993. </year>
Reference-contexts: In all of these projects, the key parallelization task was the development and performance tuning of distributed data structures. Although several languages and runtime systems support the development of such data structures <ref> [And93, BBG + 93, FLR92, SK91, SL93, CCK92] </ref>, there are no comprehensive data structure libraries, such as those that exist for uniprocessors. Multipol is such a library. Multipol is designed to help programmers write irregular applications such as discrete event simulation, symbolic computation, and search problems.
Reference: [BL94] <author> Robert D. Blumofe and Charles E. Leiserson. </author> <title> Scheduling Multithreaded Computations by Work Stealing. </title> <booktitle> In Thirty-Fifth Annual Symposium on Foundations of Computer Science (FOCS '94), </booktitle> <pages> pages 356-368, </pages> <month> Novem--ber </month> <year> 1994. </year>
Reference-contexts: Work stealing, a variation on the task pushing data structure, provides load balance that is almost as good as pushing, but with better locality. (Work stealing is provably optimal, in the same sense that randomized task pushing is <ref> [BL94] </ref>.) The basic difference is that stealing leaves tasks on the processor that created them until another processor becomes idle.
Reference: [CCK92] <author> Peter Carlin, Mani Chandy, and Carl Kesselman. </author> <title> The Compositional C++ Language Definition. </title> <type> Technical Report CS-TR-92-02, </type> <institution> California Institute of Technology, </institution> <year> 1992. </year>
Reference-contexts: In all of these projects, the key parallelization task was the development and performance tuning of distributed data structures. Although several languages and runtime systems support the development of such data structures <ref> [And93, BBG + 93, FLR92, SK91, SL93, CCK92] </ref>, there are no comprehensive data structure libraries, such as those that exist for uniprocessors. Multipol is such a library. Multipol is designed to help programmers write irregular applications such as discrete event simulation, symbolic computation, and search problems.
Reference: [CDG + 93] <author> David E. Culler, Andrea Dusseau, Seth Copen Goldstein, Arvind Krish-namurthy, Steven Lumetta, Thorsten von Eicken, and Katherine Yelick. </author> <title> Parallel Programming in Split-C. </title> <booktitle> In Supercomputing '93, </booktitle> <pages> pages 262-273, </pages> <address> Portland, Oregon, </address> <month> November </month> <year> 1993. </year>
Reference-contexts: For simulations, the underlying data structure is usually a grid over the physical domain, which may be a regular multi-dimensional mesh or an unstructured graph. Our Multipol applications include an eigenvalue computation based on a divide-and-conquer algorithm [CRY94], an electromagnetics simulation kernel <ref> [CDG + 93, Wen95] </ref>, a symbolic algebra system [CY93a, CY93b, CY94], a timing level circuit simulator [Wen92, WY93, WY95], a solution to the phylogeny problem from computational genetics [Jon94, JY95], and a game tree search [WCD + 95, Wen95]. <p> For precise problem statements, alternate approaches, related applications work, and complete descriptions of our implementations, we refer the reader to more extensive papers on each application. 2.1 EM3D The EM3D application computes the flow of electro-magnetic waves through a three-dimensional object <ref> [CDG + 93] </ref>. Each object is modeled by an unstructured three-dimensional grid of convex polyhedral cells, which is called the "primary grid." A dual grid is defined with respect to the primary grid, having grid points at the centers of the primary grid's cells. <p> This leads to a relaxed consistency model for the data types. An operation completes sometime between the initiation and synchronization point, but no other ordering is guaranteed. Several applications can take advantage of relaxed consistency models. For bulk-synchronous problems such as EM3D <ref> [CDG + 93] </ref>, cell simulation [Ste94], n-body solvers, and the tripuzzle problem, data structure updates are delayed until the end of a computation phase, at which point all processors wait for all updates to complete. For these applications, the latency can be amortized by packing small messages into larger ones. <p> In the hash table, a factor of 2 in performance was gained when split-phase inserts with acknowledgements were replaced by batches of inserts followed by periodic global synchronization points. A Split-C version of the EM3D problem based on fine-grained communication also showed a noticeable improvement <ref> [CDG + 93] </ref>. 3.4 Multi-ported Structures In addition to communication overhead, many parallel applications lose performance on the local computation. <p> Languages that support a global view of distributed data structures, for example, may incur costs from translating global indices into local ones [Ste94] or from checking whether a possibly remote address is actually local <ref> [CDG + 93] </ref>. Message passing models in which objects cannot span processor boundaries avoid these overheads, but lose the ability to form abstractions across processors.
Reference: [CDPW92] <author> J. Choi, J. Dongarra, R. Pozo, and D. Walker. </author> <title> ScaLAPACK: A Scalable Linear Algebra Library for Distributed Memory Concurrent Computers. </title> <booktitle> In Symposium on the Frontiers of Massively Parallel Computation, </booktitle> <address> McLean, VA, </address> <month> October </month> <year> 1992. </year>
Reference-contexts: The solution structure may be something as simple as a single value: in branch and bound algorithms, the bound represents a current approximation to the solution. The bisection algorithm, an algorithm used in the ScaLAPACK library <ref> [CDPW92] </ref>, is a search-based method for computing the eigenvalues of symmetric tridiagonal matrices. A symmetric tridiagonal N fi N real matrix is known to have N real eigenvalues and it is easy to find an initial range on the real line containing all eigen-values.
Reference: [CM81] <author> K. M. Chandy and J. Misra. </author> <title> Asynchronous Distributed Simulation via a Sequence of Parallel Computations. </title> <journal> Communications of the ACM, </journal> <volume> 24(11), </volume> <month> April </month> <year> 1981. </year>
Reference-contexts: The propagation of subcircuit state is called an event. The event-driven approach significantly reduces the computation and communication required for the simulation. We adopt a conservative approach to parallel asynchronous simulation <ref> [CM81] </ref>, although in related work we describe a speculative version of this simulator which is more complex, but useful for circuits with feedback loops [Wen92, WY93]. The program uses a distributed event graph data structure to represent the circuit.
Reference: [CRY94] <author> Soumen Chakrabarti, Abhiram Ranade, and Katherine Yelick. </author> <title> Randomized Load Balancing for Tree-structured Computation. </title> <booktitle> In Proceedings of the Scalable High Performance Computing Conference, </booktitle> <address> Knoxville, TN, </address> <month> May </month> <year> 1994. </year>
Reference-contexts: For simulations, the underlying data structure is usually a grid over the physical domain, which may be a regular multi-dimensional mesh or an unstructured graph. Our Multipol applications include an eigenvalue computation based on a divide-and-conquer algorithm <ref> [CRY94] </ref>, an electromagnetics simulation kernel [CDG + 93, Wen95], a symbolic algebra system [CY93a, CY93b, CY94], a timing level circuit simulator [Wen92, WY93, WY95], a solution to the phylogeny problem from computational genetics [Jon94, JY95], and a game tree search [WCD + 95, Wen95]. <p> This scheduling structure has, in a sense, provably optimal performance, but poor locality properties if there is an advantage to executing tasks on the processor that created them. Given the replicated matrix, locality is not a concern in the bisection algorithm <ref> [CRY94] </ref>. The intervals stored in the task queue act as the approximate solution as well as the scheduling structure. As the intervals shrink, the approximation improves until a solution of the desired accuracy is obtained.
Reference: [CSS + 91] <author> D. Culler, A. Sah, K. Schauser, T. von Eicken, and J. Wawrzynek. </author> <title> Fine-grain Parallelism with Minimal Hardware Support: A Compiler-Controlled Threaded Abstract Machine. </title> <booktitle> In Proc. of 4th Int. Conf. on Architectural Support for Programming Languages and Operating Systems, </booktitle> <address> Santa-Clara, CA, </address> <month> April </month> <year> 1991. </year> <note> (Also available as Technical Report UCB/CSD 91/594, </note> <institution> CS Div., University of California at Berkeley). </institution>
Reference-contexts: The scheduling policy used by one data structure can be changed without introducing anomalies, such as unexpected livelock or deadlock, into other parts of the program. The Multipol threads are designed for direct programming, in contrast to compiler-controlled threads such as TAM <ref> [CSS + 91] </ref>, in that Multipol provides more flexibility such as arbitrary size threads and custom schedulers. A set of macros can be used to facilitate programming.
Reference: [CY93a] <author> Soumen Chakrabarti and Katherine Yelick. </author> <title> Implementing an Irregular Application on a Distributed Memory Multiprocessor. </title> <booktitle> In Proceedings of the 1993 Conference on Principles and Practice of Parallel Programming, </booktitle> <address> San Diego, CA, </address> <month> May </month> <year> 1993. </year>
Reference-contexts: Our Multipol applications include an eigenvalue computation based on a divide-and-conquer algorithm [CRY94], an electromagnetics simulation kernel [CDG + 93, Wen95], a symbolic algebra system <ref> [CY93a, CY93b, CY94] </ref>, a timing level circuit simulator [Wen92, WY93, WY95], a solution to the phylogeny problem from computational genetics [Jon94, JY95], and a game tree search [WCD + 95, Wen95]. In all of these projects, the key parallelization task was the development and performance tuning of distributed data structures.
Reference: [CY93b] <author> Soumen Chakrabarti and Katherine Yelick. </author> <title> On the Correctness of a Distributed Memory Grobner Basis Computation. In Rewriting Techniques and Applications, </title> <address> Montreal, Canada, </address> <month> June </month> <year> 1993. </year>
Reference-contexts: Our Multipol applications include an eigenvalue computation based on a divide-and-conquer algorithm [CRY94], an electromagnetics simulation kernel [CDG + 93, Wen95], a symbolic algebra system <ref> [CY93a, CY93b, CY94] </ref>, a timing level circuit simulator [Wen92, WY93, WY95], a solution to the phylogeny problem from computational genetics [Jon94, JY95], and a game tree search [WCD + 95, Wen95]. In all of these projects, the key parallelization task was the development and performance tuning of distributed data structures. <p> In the phylogeny application and Grobner basis problem, not only are updates to the global set of results lazy, but each processor keeps partially completed cached copies of the set. This yields a correct, albeit different, execution than the sequential program <ref> [CY93b, JY95] </ref>.
Reference: [CY94] <author> Soumen Chakrabarti and Katherine Yelick. </author> <title> Distributed Data Structures and Algorithms for Grobner Basis Computation. </title> <booktitle> Lisp and Symbolic Computation, </booktitle> <year> 1994. </year>
Reference-contexts: Our Multipol applications include an eigenvalue computation based on a divide-and-conquer algorithm [CRY94], an electromagnetics simulation kernel [CDG + 93, Wen95], a symbolic algebra system <ref> [CY93a, CY93b, CY94] </ref>, a timing level circuit simulator [Wen92, WY93, WY95], a solution to the phylogeny problem from computational genetics [Jon94, JY95], and a game tree search [WCD + 95, Wen95]. In all of these projects, the key parallelization task was the development and performance tuning of distributed data structures.
Reference: [DDR94] <author> J. Demmel, I. Dhillon, and H. Ren. </author> <title> On the Correctness of Parallel Bisection in Floating Point. </title> <type> Tech Report UCB//CSD-94-805, </type> <institution> UC Berke-ley Computer Science Division, </institution> <month> March </month> <year> 1994. </year> <note> available via anonymous ftp from tr-ftp.cs.berkeley.edu, in directory pub/tech-reports/csd/csd-94-805, file all.ps. </note>
Reference-contexts: A parallel implementation of bisection can use a static subdivision of the initial range, but this has poor parallel efficiency if the eigenvalues are clustered, because the work load is not balanced <ref> [DDR94] </ref>. A solution is to use a task queue with load balancing for the scheduling structure.
Reference: [DUSH94] <author> Raja Das, Mustafa Uysal, Joel Saltz, and Yuan-Shin Hwang. </author> <title> Communication Optimizations for Irregular Scientific Computations on Distributed Memory Architectures. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <month> September </month> <year> 1994. </year>
Reference-contexts: This allows for the kind of inspector/executor processing that is done in the Chaos system <ref> [DUSH94] </ref>, although the EM3D kernel is based on a synthetic graph, so the inspector phase is unnecessary in our code. <p> For machines like the Paragon and workstation networks, which have high communication start-up cost, the former is very important. Many small messages are aggregated into one large physical message to amortize the overhead. Several other systems, including Chaos <ref> [DUSH94] </ref> and LPARX [Bad91], also use message aggregation.
Reference: [FLR92] <author> J.A. Feldman, C.C. Lim, and T. Rauber. </author> <title> The Shared-memory Lan--guage pSather on a Distributed-memory Multiprocessor. </title> <booktitle> In Workshop on Languages, Compilers and Run-Time Environments for Distributed Memory Multiprocessors, </booktitle> <address> Boulder, CO, </address> <month> September </month> <year> 1992. </year>
Reference-contexts: In all of these projects, the key parallelization task was the development and performance tuning of distributed data structures. Although several languages and runtime systems support the development of such data structures <ref> [And93, BBG + 93, FLR92, SK91, SL93, CCK92] </ref>, there are no comprehensive data structure libraries, such as those that exist for uniprocessors. Multipol is such a library. Multipol is designed to help programmers write irregular applications such as discrete event simulation, symbolic computation, and search problems.
Reference: [Jon94] <author> Jeff Jones. </author> <title> Exploiting Parallelism in the Perfect Phylogeny Computation. </title> <type> Master's thesis, </type> <institution> University of California, Berkeley, Computer Science Division, </institution> <month> December </month> <year> 1994. </year>
Reference-contexts: Our Multipol applications include an eigenvalue computation based on a divide-and-conquer algorithm [CRY94], an electromagnetics simulation kernel [CDG + 93, Wen95], a symbolic algebra system [CY93a, CY93b, CY94], a timing level circuit simulator [Wen92, WY93, WY95], a solution to the phylogeny problem from computational genetics <ref> [Jon94, JY95] </ref>, and a game tree search [WCD + 95, Wen95]. In all of these projects, the key parallelization task was the development and performance tuning of distributed data structures.
Reference: [JY95] <author> J. Jones and K. Yelick. </author> <title> Parallelizing the Phylogeny Problem. </title> <booktitle> In Supercomputing '95, </booktitle> <month> December </month> <year> 1995. </year>
Reference-contexts: Our Multipol applications include an eigenvalue computation based on a divide-and-conquer algorithm [CRY94], an electromagnetics simulation kernel [CDG + 93, Wen95], a symbolic algebra system [CY93a, CY93b, CY94], a timing level circuit simulator [Wen92, WY93, WY95], a solution to the phylogeny problem from computational genetics <ref> [Jon94, JY95] </ref>, and a game tree search [WCD + 95, Wen95]. In all of these projects, the key parallelization task was the development and performance tuning of distributed data structures. <p> In the phylogeny application and Grobner basis problem, not only are updates to the global set of results lazy, but each processor keeps partially completed cached copies of the set. This yields a correct, albeit different, execution than the sequential program <ref> [CY93b, JY95] </ref>.
Reference: [KY94] <author> Arvind Krishnamurthy and Katherine Yelick. </author> <title> Optimizing parallel SPMD programs. </title> <booktitle> In Proceedings of the Workshop on Languages and Compilers for Parallel Computing, </booktitle> <month> August </month> <year> 1994. </year>
Reference-contexts: Techniques such as pipelining remote operations and multithreading can be used to hide latency. Even on a machines like the CM-5, with relatively low communication latency, the benefits from message overlap are noticeable: message pipelining of simple remote read and write operations can save as much as 30% <ref> [KY94] </ref> and on workstation networks with longer hardware latencies and expensive remote message handlers, the savings may be even higher. The latency hiding techniques require the operations be nonblocking, or split-phase. In Multipol, operations that would normally be long-running with unpredictable delay are divided into separate finite-length threads.
Reference: [SK91] <author> Wei Shu and L.V. Kale. </author> <title> Chare kernel a runtime support system for parallel computations. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 11 </volume> <pages> 198-211, </pages> <year> 1991. </year>
Reference-contexts: In all of these projects, the key parallelization task was the development and performance tuning of distributed data structures. Although several languages and runtime systems support the development of such data structures <ref> [And93, BBG + 93, FLR92, SK91, SL93, CCK92] </ref>, there are no comprehensive data structure libraries, such as those that exist for uniprocessors. Multipol is such a library. Multipol is designed to help programmers write irregular applications such as discrete event simulation, symbolic computation, and search problems.
Reference: [SL93] <author> Daniel J. Scales and Monica S. Lam. </author> <title> A flexible shared memory system for distributed memory machines. </title> <type> Unpublished manuscript, </type> <year> 1993. </year>
Reference-contexts: In all of these projects, the key parallelization task was the development and performance tuning of distributed data structures. Although several languages and runtime systems support the development of such data structures <ref> [And93, BBG + 93, FLR92, SK91, SL93, CCK92] </ref>, there are no comprehensive data structure libraries, such as those that exist for uniprocessors. Multipol is such a library. Multipol is designed to help programmers write irregular applications such as discrete event simulation, symbolic computation, and search problems.
Reference: [Ste94] <author> Stephen Steinberg. </author> <title> Parallelizing a cell simulation: Analysis, abstraction, and portability. </title> <type> Master's thesis, </type> <institution> University of California, Berkeley, Computer Science Division, </institution> <month> December </month> <year> 1994. </year>
Reference-contexts: This leads to a relaxed consistency model for the data types. An operation completes sometime between the initiation and synchronization point, but no other ordering is guaranteed. Several applications can take advantage of relaxed consistency models. For bulk-synchronous problems such as EM3D [CDG + 93], cell simulation <ref> [Ste94] </ref>, n-body solvers, and the tripuzzle problem, data structure updates are delayed until the end of a computation phase, at which point all processors wait for all updates to complete. For these applications, the latency can be amortized by packing small messages into larger ones. <p> Languages that support a global view of distributed data structures, for example, may incur costs from translating global indices into local ones <ref> [Ste94] </ref> or from checking whether a possibly remote address is actually local [CDG + 93]. Message passing models in which objects cannot span processor boundaries avoid these overheads, but lose the ability to form abstractions across processors.
Reference: [WCD + 95] <author> Chih-Po Wen, Soumen Chakrabarti, Etienne Deprit, Arvind Krishna-murthy, and Katherine Yelick. </author> <title> Runtime Support for Portable Distributed Data Structures. </title> <booktitle> In Third Workshop on Languages, Compilers, and Run-Time Systems for Scalable Computers (LCR), </booktitle> <month> May </month> <year> 1995. </year> <editor> Boleslaw K. Szymanski and Balaram Sinharoy (Editors), </editor> <publisher> Kluwer Academic Publishers, </publisher> <address> Boston, MA, </address> <pages> pp. 111-120. </pages>
Reference-contexts: applications include an eigenvalue computation based on a divide-and-conquer algorithm [CRY94], an electromagnetics simulation kernel [CDG + 93, Wen95], a symbolic algebra system [CY93a, CY93b, CY94], a timing level circuit simulator [Wen92, WY93, WY95], a solution to the phylogeny problem from computational genetics [Jon94, JY95], and a game tree search <ref> [WCD + 95, Wen95] </ref>. In all of these projects, the key parallelization task was the development and performance tuning of distributed data structures.
Reference: [Wen92] <author> Chih-Po Wen. </author> <title> Parallel Timing Simulation on a Distributed Memory Multiprocessor. </title> <type> Master's thesis, </type> <institution> University of California, Berkeley, </institution> <address> CA, </address> <year> 1992. </year>
Reference-contexts: Our Multipol applications include an eigenvalue computation based on a divide-and-conquer algorithm [CRY94], an electromagnetics simulation kernel [CDG + 93, Wen95], a symbolic algebra system [CY93a, CY93b, CY94], a timing level circuit simulator <ref> [Wen92, WY93, WY95] </ref>, a solution to the phylogeny problem from computational genetics [Jon94, JY95], and a game tree search [WCD + 95, Wen95]. In all of these projects, the key parallelization task was the development and performance tuning of distributed data structures. <p> The event-driven approach significantly reduces the computation and communication required for the simulation. We adopt a conservative approach to parallel asynchronous simulation [CM81], although in related work we describe a speculative version of this simulator which is more complex, but useful for circuits with feedback loops <ref> [Wen92, WY93] </ref>. The program uses a distributed event graph data structure to represent the circuit. Each graph node corresponds to a input signal port or a output voltage point of a subcir-cuit.
Reference: [Wen95] <author> Chih-Po Wen. </author> <title> Portable Library Support for Irregular Applications. </title> <type> PhD thesis, </type> <institution> University of California, Berkeley, </institution> <address> CA, </address> <year> 1995. </year>
Reference-contexts: For simulations, the underlying data structure is usually a grid over the physical domain, which may be a regular multi-dimensional mesh or an unstructured graph. Our Multipol applications include an eigenvalue computation based on a divide-and-conquer algorithm [CRY94], an electromagnetics simulation kernel <ref> [CDG + 93, Wen95] </ref>, a symbolic algebra system [CY93a, CY93b, CY94], a timing level circuit simulator [Wen92, WY93, WY95], a solution to the phylogeny problem from computational genetics [Jon94, JY95], and a game tree search [WCD + 95, Wen95]. <p> applications include an eigenvalue computation based on a divide-and-conquer algorithm [CRY94], an electromagnetics simulation kernel [CDG + 93, Wen95], a symbolic algebra system [CY93a, CY93b, CY94], a timing level circuit simulator [Wen92, WY93, WY95], a solution to the phylogeny problem from computational genetics [Jon94, JY95], and a game tree search <ref> [WCD + 95, Wen95] </ref>. In all of these projects, the key parallelization task was the development and performance tuning of distributed data structures. <p> The programs are analyzed and optimized using Mprof, a performance profiling toolkit for programs that use the Multipol library <ref> [Wen95] </ref>. The Multipol programs are portable across several distributed memory platforms, including the Thinking Machines CM5, the Intel Paragon, and the IBM SP1/SP2. Figure 2 gives the speedups of the SWEC, Eigenvalue, Phylogeny, and Tripuzzle programs on these three machines.
Reference: [WY93] <author> Chih-Po Wen and Katherine Yelick. </author> <title> Parallel Timing Simulation on a Distributed Memory Multiprocessor. </title> <booktitle> In International Conference on CAD, </booktitle> <address> Santa Clara, CA, </address> <month> November </month> <year> 1993. </year>
Reference-contexts: Our Multipol applications include an eigenvalue computation based on a divide-and-conquer algorithm [CRY94], an electromagnetics simulation kernel [CDG + 93, Wen95], a symbolic algebra system [CY93a, CY93b, CY94], a timing level circuit simulator <ref> [Wen92, WY93, WY95] </ref>, a solution to the phylogeny problem from computational genetics [Jon94, JY95], and a game tree search [WCD + 95, Wen95]. In all of these projects, the key parallelization task was the development and performance tuning of distributed data structures. <p> The event-driven approach significantly reduces the computation and communication required for the simulation. We adopt a conservative approach to parallel asynchronous simulation [CM81], although in related work we describe a speculative version of this simulator which is more complex, but useful for circuits with feedback loops <ref> [Wen92, WY93] </ref>. The program uses a distributed event graph data structure to represent the circuit. Each graph node corresponds to a input signal port or a output voltage point of a subcir-cuit.
Reference: [WY95] <author> Chih-Po Wen and Katherine Yelick. </author> <title> Portable Runtime Support for Asynchronous Simulation. </title> <booktitle> In International Conference on Parallel Processing, </booktitle> <address> Oconomowoc, Wisconsin, </address> <month> August </month> <year> 1995. </year>
Reference-contexts: Our Multipol applications include an eigenvalue computation based on a divide-and-conquer algorithm [CRY94], an electromagnetics simulation kernel [CDG + 93, Wen95], a symbolic algebra system [CY93a, CY93b, CY94], a timing level circuit simulator <ref> [Wen92, WY93, WY95] </ref>, a solution to the phylogeny problem from computational genetics [Jon94, JY95], and a game tree search [WCD + 95, Wen95]. In all of these projects, the key parallelization task was the development and performance tuning of distributed data structures. <p> The graph partitioning problems are difficult, but all of these optimizations can be done in a preprocessing phase at runtime. 2.2 CSWEC The CSWEC application is a classic example of discrete event simulation <ref> [WY95] </ref>. The program simulates the voltage output of combinational digital circuits, that is, circuits without feedback signal paths. The program partitions the circuit into loosely coupled subcircuits that can be simulated independently within a time step.
References-found: 27

