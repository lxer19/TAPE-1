URL: ftp://softlib.rice.edu/pub/CRPC-TRs/reports/CRPC-TR92249.ps.gz
Refering-URL: http://www.crpc.rice.edu/CRPC/softlib/TRs_online.html
Root-URL: 
Title: Fortran M: A Language for Modular Parallel Programming  
Author: Ian T. Foster K. Mani Chandy 
Date: June 1992  
Address: Argonne, IL 60439  Pasadena, CA 91125  
Affiliation: Mathematics and Computer Science Division Argonne National Laboratory  Department of Computer Science California Institute of Technology  
Abstract: Fortran M is a small set of extensions to Fortran 77 that supports a modular approach to the design of message-passing programs. It has the following features. (1) Modularity. Programs are constructed by using explicitly-declared communication channels to plug together program modules called processes. A process can encapsulate common data, subprocesses, and internal communication. (2) Safety. Operations on channels are restricted so as to guarantee deterministic execution, even in dynamic computations that create and delete processes and channels. Channels are typed, so a compiler can check for correct usage. (3) Architecture Independence. The mapping of processes to processors can be specified with respect to a virtual computer with size and shape different from that of the target computer. Mapping is specified by annotations that influence performance but not correctness. (4) Efficiency. Fortran M can be compiled efficiently for uniprocessors, shared-memory computers, distributed-memory computers, and networks of workstations. Because message passing is incorporated into the language, a compiler can optimize communication as well as computation. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Boyle, J., Butler, R., Disz, T., Glickfeld, B., Lusk, E., Overbeek, R., Patterson, J., and Stevens, R., </author> <title> Portable Programs for Parallel Processors, </title> <publisher> Holt, Rinehart, and Winston, </publisher> <year> 1987. </year>
Reference-contexts: Strand and PCN do not address the problem of Fortran common data. In message-passing library approaches, programmers call subroutines to communicate data between processes. The number of processes is often fixed at one per physical processor. Systems such as P4 <ref> [1] </ref>, Express [28], PVM [35], and Zipcode [34] provide, as primitives, an asynchronous send to a named process and a synchronous receive. The Mach operating system provides, in addition, a virtual channel construct (the port); ports can be transferred between processes in messages [38].
Reference: [2] <author> Briggs, P., Cooper, K., Hall, M., and Torczon, L., </author> <title> Goal-directed interprocedural optimization, </title> <note> Report CRPC-TR90102, Center for Research in Parallel Computation, </note> <institution> Rice University, Houston, Texas, </institution> <year> 1990. </year>
Reference-contexts: However, we also expect Fortran M to enable novel compiler optimizations that can significantly reduce communication and computation costs. Information about the types, contents, and sequence of messages should be obtainable by an interprocess analysis analogous to the interprocedural analysis performed by modern Fortran compilers <ref> [2, 3] </ref>; this information will allow a preprocessor to perform novel source-to-source transformations such as "process cloning", "channel merging", and "message merging". In addition, a code generator can generate specialized instruction sequences that use shared memory or drive message-passing hardware more efficiently than general purpose communication libraries.
Reference: [3] <author> Callahan, D., Cooper, K., Hood, R., Kennedy, K., and Torczon, L., </author> <title> Parascope: A parallel programming environment, </title> <journal> Intl J. Supercomputer Applications, </journal> <volume> 2(4), </volume> <year> 1988. </year>
Reference-contexts: However, we also expect Fortran M to enable novel compiler optimizations that can significantly reduce communication and computation costs. Information about the types, contents, and sequence of messages should be obtainable by an interprocess analysis analogous to the interprocedural analysis performed by modern Fortran compilers <ref> [2, 3] </ref>; this information will allow a preprocessor to perform novel source-to-source transformations such as "process cloning", "channel merging", and "message merging". In addition, a code generator can generate specialized instruction sequences that use shared memory or drive message-passing hardware more efficiently than general purpose communication libraries.
Reference: [4] <author> Carriero, N., and Gelernter, D., </author> <title> How to Write Parallel Programs, </title> <publisher> MIT Press, </publisher> <year> 1990. </year>
Reference-contexts: In contrast to Fortran M, message-passing libraries are nondeterministic and, as the name space of processes is global, do not enforce information hiding. Related to message-passing libraries is Linda, which provides read and write operations on a shared tuple space <ref> [4] </ref>. Tuple space operations can emulate both message-passing communication protocols and shared data structures. Tuple space operations, like message passing, are nondeterministic and do not enforce information hiding.
Reference: [5] <author> Chandy, K. M., and Foster, I., </author> <title> Communicating processes, </title> <type> Preprint, </type> <institution> Mathematics and Computer Science Division, Argonne National Laboratory, Argonne, Ill., </institution> <year> 1992. </year>
Reference-contexts: All other details of its implementation, which can include common data, subprocesses, process placement, and internal communication channels, are hidden. Fortran M is supported by a theory of parallel and sequential composition of communicating processes. Key characteristics of this theory, described in a separate paper <ref> [5] </ref>, include (1) proofs that a Fortran M program is deterministic even though processes and channels are created and deleted and channels are reconnected; (2) extension of sequential programming proof techniques to parallel programs; and (3) a compositional proof theory in which the specification of the whole is derived from the
Reference: [6] <author> Chandy, K. M., and Kesselman, C., </author> <title> Compositional parallel programming in CC++, </title> <type> Technical Report, </type> <institution> Caltech, </institution> <year> 1992. </year>
Reference-contexts: Tuple space operations, like message passing, are nondeterministic and do not enforce information hiding. Actor-based message passing systems such as CE/RK [33] have some points of similarity with Fortran M, but are fundamentally different in that they are nondeterministic. CC++ is a shared virtual memory extension of C++ <ref> [6] </ref>. It differs from Fortran M in many respects, in particular its use of a shared-memory programming model. In data parallel approaches, sequential languages are extended with directives that specify how arrays are to be decomposed and distributed over processors [37, 18, 8].
Reference: [7] <author> Chandy, K. M. and Taylor, S., </author> <title> An Introduction to Parallel Programming, </title> <editor> Jones and Bartlett, </editor> <year> 1991. </year>
Reference-contexts: This approach has the advantage of clearly separating parallel and sequential computation, but requires the programmer to learn a new language. Coordination languages include occam [20], Strand [17], PCN <ref> [7, 16] </ref>, and Delerium [24]. Delerium is purely a coordination language, while the others can be used to specify both coordination and computation. occam, derived from Hoare's Communicating Sequential Processes (CSP) [19], can specify only static computation and communication structures, does not enforce determinism, and employs synchronous communication.
Reference: [8] <author> Chapman, B., Mehrotra, P., and Zima, H., </author> <title> Vienna Fortran | A Fortran language extension for distributed memory systems, Languages, Compilers, and Run-time Environments for Distributed Memory Machines, </title> <publisher> Elsevier Press, </publisher> <year> 1992. </year>
Reference-contexts: CC++ is a shared virtual memory extension of C++ [6]. It differs from Fortran M in many respects, in particular its use of a shared-memory programming model. In data parallel approaches, sequential languages are extended with directives that specify how arrays are to be decomposed and distributed over processors <ref> [37, 18, 8] </ref>. A compiler then partitions the computation using the "owner computes" rule, with each operation in the sequential program allocated to the processor containing the data that is to be operated on. This approach permits succinct specifications of parallel algorithms for regular problems and guarantees deterministic execution.
Reference: [9] <author> Cooper, E., and Draves, R., </author> <title> C Threads, </title> <type> Technical Report, </type> <institution> Department of Computer Science, Carnegie Mellon University, Pittsburgh, </institution> <year> 1987. </year>
Reference-contexts: Fortran M performance also depends on the cost of process creation, scheduling, and termination operations. A preemptive scheduler is required so as to permit overlapping of computation and communication. Fortunately, these facilities are, increasingly, supported either at the operating system <ref> [38, 9] </ref> or hardware levels [21, 33, 11], or can be provided by a compiler [14]. 9 Related Work Programming notations for parallel scientific programming fall into three principal classes: coordination languages, message-passing libraries, and data parallel extensions.
Reference: [10] <author> Cox, B., and Novobilski, A., </author> <title> Object-Oriented Programming: An Evolutionary Approach, </title> <publisher> Addison-Wesley, </publisher> <year> 1991. </year>
Reference-contexts: Send electronic mail to fortran-m@mcs.anl.gov for details. 2 Defining Modules In modular program design, we develop components of a program separately, as independent modules, and then combine modules to obtain a complete program <ref> [29, 10] </ref>. Interactions between modules are restricted to well-defined interfaces. Hence, module implementations can be changed without modifying other components, and the properties of a program can be determined from the specifications for its modules and the code that plugs these modules together.
Reference: [11] <author> Dally, W. J., et al., </author> <title> The J-Machine: A fine-grain concurrent computer, Information Processing 89, </title> <editor> G. X. Ritter (ed.), </editor> <publisher> Elsevier Science Publishers B.V., North Holland, IFIP, </publisher> <year> 1989. </year>
Reference-contexts: Fortran M performance also depends on the cost of process creation, scheduling, and termination operations. A preemptive scheduler is required so as to permit overlapping of computation and communication. Fortunately, these facilities are, increasingly, supported either at the operating system [38, 9] or hardware levels <ref> [21, 33, 11] </ref>, or can be provided by a compiler [14]. 9 Related Work Programming notations for parallel scientific programming fall into three principal classes: coordination languages, message-passing libraries, and data parallel extensions.
Reference: [12] <author> Dijkstra, </author> <title> E.W., Guarded commands, nondeterminacy and the formal derivation of programs, </title> <journal> CACM, </journal> <volume> 18, </volume> <pages> 453-7, </pages> <year> 1975. </year>
Reference-contexts: Strand and PCN can specify dynamic structures. Communication and synchronization are specified in terms of read and write operations on single-assignment variables, and a form of guarded command <ref> [12] </ref> is used to specify choice between alternatives. A compiler cannot in general assert that a Strand or PCN program is deterministic, because it cannot always prove that choices in guarded commands are mutually exclusive.
Reference: [13] <author> Dongarra, J., van de Geijn, R., and Walker, D., </author> <title> A look at scalable dense linear algebra libraries, </title> <booktitle> Proc. 1992 Scalable High Performance Computers Conf., </booktitle> <publisher> IEEE Press, </publisher> <pages> 992. </pages> <note> [14] von Eicken, </note> <author> T., Culler, D., Goldstein, S., and Schauser, K., </author> <title> Active messages: A mechanism for integrating communication and computation, </title> <booktitle> Proc. 19th Intl Symp. Computer Architecture, ACM, </booktitle> <year> 1992. </year>
Reference-contexts: Parallel implementations of other commonly used functions, such as broadcast, multicast, parallel prefix, and parallel implementations of BLAS linear algebra routines <ref> [13] </ref>, can be encapsulated in the same way.
Reference: [15] <author> Foster, I., </author> <title> Information hiding in parallel programs, </title> <type> Preprint MCS-P290-0292, </type> <institution> Mathematics and Computer Science Division, Argonne National Laboratory, </institution> <year> 1992. </year> <month> 29 </month>
Reference-contexts: This nondeterminism is antithetical to both the scientist's need for reproducibility and ease of debugging. In addition, most message passing systems do not enforce information hiding and provide a global name space of processes. This makes it difficult to develop modular programs and reusable libraries <ref> [15] </ref>. In this paper, we describe message-passing extensions to sequential programming languages that enforce both deterministic execution and information hiding, while retaining much of the flexibility of traditional message-passing. We describe these extensions in the context of Fortran 77, and call the resulting language Fortran M. <p> The PROCESSORS declaration specifies the shape and dimension of a processor array, the LOCATION annotation maps processes to specified elements of this array, and the SUBMACHINE annotation specifies that a process should execute in a subset of the array <ref> [15] </ref>. The PROCESSORS declaration is similar in form and function to the array DIMENSION statement. It has the general form PROCESSORS (I 1 ,...,I n ) where n 0 and the I j have the same form as the arguments to a DIMENSION statement.
Reference: [16] <author> Foster, I., Olson, R., and Tuecke, S., </author> <title> Productive parallel programming: The PCN approach, </title> <journal> Scientific Programming, </journal> <volume> 1(1), </volume> <pages> 51-66, </pages> <year> 1992. </year>
Reference-contexts: This approach has the advantage of clearly separating parallel and sequential computation, but requires the programmer to learn a new language. Coordination languages include occam [20], Strand [17], PCN <ref> [7, 16] </ref>, and Delerium [24]. Delerium is purely a coordination language, while the others can be used to specify both coordination and computation. occam, derived from Hoare's Communicating Sequential Processes (CSP) [19], can specify only static computation and communication structures, does not enforce determinism, and employs synchronous communication.
Reference: [17] <author> Foster, I. and Taylor, S., Strand: </author> <title> New Concepts in Parallel Programming, </title> <publisher> Prentice-Hall, </publisher> <year> 1989. </year>
Reference-contexts: In coordination language approaches, a specialized language is used to specify con-currency, communication, and synchronization; Fortran routines are called to perform computation. This approach has the advantage of clearly separating parallel and sequential computation, but requires the programmer to learn a new language. Coordination languages include occam [20], Strand <ref> [17] </ref>, PCN [7, 16], and Delerium [24]. Delerium is purely a coordination language, while the others can be used to specify both coordination and computation. occam, derived from Hoare's Communicating Sequential Processes (CSP) [19], can specify only static computation and communication structures, does not enforce determinism, and employs synchronous communication.
Reference: [18] <author> Fox, G., Hiranandani, S., Kennedy, K., Koelbel, C., Kremer, U., Tseng, C., and Wu, M., </author> <title> Fortran D language specification, </title> <type> Technical Report TR90-141, </type> <institution> Department of Computer Science, Rice University, Houston, Texas, </institution> <year> 1990. </year>
Reference-contexts: The basic paradigm underlying Fortran M is task parallelism: the parallel execution of (possibly dissimilar) tasks. Hence, Fortran M complements data-parallel languages such as Fortran D <ref> [18] </ref> and High Performance Fortran (HPF). In particular, Fortran M can be used to coordinate multiple data-parallel computations. Our goal is to integrate HPF with Fortran M, thus combining the data-parallel and task-parallel programming paradigms in a single system. <p> CC++ is a shared virtual memory extension of C++ [6]. It differs from Fortran M in many respects, in particular its use of a shared-memory programming model. In data parallel approaches, sequential languages are extended with directives that specify how arrays are to be decomposed and distributed over processors <ref> [37, 18, 8] </ref>. A compiler then partitions the computation using the "owner computes" rule, with each operation in the sequential program allocated to the processor containing the data that is to be operated on. This approach permits succinct specifications of parallel algorithms for regular problems and guarantees deterministic execution.
Reference: [19] <author> Hoare, C., </author> <title> Communicating Sequential Processes, </title> <journal> CACM, </journal> <volume> 21(8), </volume> <pages> 666-677, </pages> <year> 1978. </year>
Reference-contexts: Coordination languages include occam [20], Strand [17], PCN [7, 16], and Delerium [24]. Delerium is purely a coordination language, while the others can be used to specify both coordination and computation. occam, derived from Hoare's Communicating Sequential Processes (CSP) <ref> [19] </ref>, can specify only static computation and communication structures, does not enforce determinism, and employs synchronous communication. Strand and PCN can specify dynamic structures.
Reference: [20] <author> Inmos, Ltd, </author> <title> occam Programming Manual, </title> <publisher> Prentice Hall, </publisher> <year> 1984. </year>
Reference-contexts: In coordination language approaches, a specialized language is used to specify con-currency, communication, and synchronization; Fortran routines are called to perform computation. This approach has the advantage of clearly separating parallel and sequential computation, but requires the programmer to learn a new language. Coordination languages include occam <ref> [20] </ref>, Strand [17], PCN [7, 16], and Delerium [24].
Reference: [21] <author> Inmos, Ltd, </author> <title> Transputer Reference Manual, </title> <publisher> Prentice Hall, </publisher> <year> 1988. </year>
Reference-contexts: Fortran M performance also depends on the cost of process creation, scheduling, and termination operations. A preemptive scheduler is required so as to permit overlapping of computation and communication. Fortunately, these facilities are, increasingly, supported either at the operating system [38, 9] or hardware levels <ref> [21, 33, 11] </ref>, or can be provided by a compiler [14]. 9 Related Work Programming notations for parallel scientific programming fall into three principal classes: coordination languages, message-passing libraries, and data parallel extensions.
Reference: [22] <author> Karp, A., and Babb, R., </author> <title> A comparison of 12 parallel Fortran dialects, </title> <journal> IEEE Software, </journal> <volume> 5(5), </volume> <pages> 52-67, </pages> <year> 1988. </year>
Reference-contexts: This process is used in the following program, which computes an approximation to the integral of F (X) over the interval (0,1). (For comparison, solutions to the same problem in several other parallel Fortran dialects are presented in <ref> [22] </ref>.) The process creates NP integrate processes, each of which evaluates the integral over a specified subinterval and stores its result in an element of the array results. <p> Here, we discuss how Fortran M differs from each of these approaches, focusing in particular on 18 the issues of modularity and safety. We do not consider systems based on shared-memory models <ref> [22] </ref>, as these are not easily adapted to distributed-memory machines. In coordination language approaches, a specialized language is used to specify con-currency, communication, and synchronization; Fortran routines are called to perform computation.
Reference: [23] <author> Koelbel, C., Mehrotra, P., and Van Rosendale, J., </author> <title> Supporting shared data structures on distributed memory machines, </title> <booktitle> Proc. 2nd ACM SIGPLAN Symp. on Principles and Practice of Parallel Programming, ACM, </booktitle> <year> 1990. </year>
Reference-contexts: This approach permits succinct specifications of parallel algorithms for regular problems and guarantees deterministic execution. When extended with support 19 for irregular data distributions, data parallel languages can also handle some irregular problems <ref> [23, 32] </ref>. However, there are broad classes of problems for which the approach has not yet been shown to be tractable. These include highly dynamic adaptive grid problems, multidisciplinary optimization problems, and reactive systems in which a program interacts with an external environment in a nondeterministic manner.
Reference: [24] <author> Lucco, S., and Sharp, O., </author> <title> Parallel programming with coordination structures, </title> <booktitle> Proc. 18th ACM POPL, ACM, </booktitle> <year> 1991. </year>
Reference-contexts: This approach has the advantage of clearly separating parallel and sequential computation, but requires the programmer to learn a new language. Coordination languages include occam [20], Strand [17], PCN [7, 16], and Delerium <ref> [24] </ref>. Delerium is purely a coordination language, while the others can be used to specify both coordination and computation. occam, derived from Hoare's Communicating Sequential Processes (CSP) [19], can specify only static computation and communication structures, does not enforce determinism, and employs synchronous communication.
Reference: [25] <author> Martin, A., </author> <title> The torus: An exercise in constructing a processing surface, </title> <booktitle> Proc. Conf. on VLSI, Caltech, </booktitle> <pages> 52-57, </pages> <year> 1979. </year>
Reference-contexts: on a parallel computer by changing mapping constructs. 7.1 Process Placement Constructs The Fortran M process placement constructs are based on the concept of a virtual computer: a collection of virtual processors, which may or may not have the same topology as the physical computer on which a program executes <ref> [25, 36] </ref>. For consistency with Fortran concepts, a Fortran M virtual computer is an N -dimensional array, and the mapping constructs are modeled on Fortran 77's array manipulation constructs.
Reference: [26] <author> Metcalf, M., and Reid, J., </author> <title> Fortran 90 Explained, </title> <publisher> Oxford Science Publications, </publisher> <year> 1990. </year>
Reference: [27] <author> Pancake, C., and Bergmark, D., </author> <title> Do parallel languages respond to the needs of scientific programmers?, </title> <booktitle> Computer 23(12), </booktitle> <pages> 13-23, </pages> <year> 1990. </year>
Reference-contexts: Its popularity stems from its simplicity, flexibility, and ease of implementation. A disadvantage of the message-passing model, particularly for scientific and engineering applications, is that it does not enforce deterministic execution <ref> [27] </ref>. Hence, the programmer has no a priori guarantee that a program will give the same result if executed more than once with the same input. This nondeterminism is antithetical to both the scientist's need for reproducibility and ease of debugging.
Reference: [28] <author> Parasoft Corporation, </author> <title> Express user manual, </title> <year> 1989. </year>
Reference-contexts: Strand and PCN do not address the problem of Fortran common data. In message-passing library approaches, programmers call subroutines to communicate data between processes. The number of processes is often fixed at one per physical processor. Systems such as P4 [1], Express <ref> [28] </ref>, PVM [35], and Zipcode [34] provide, as primitives, an asynchronous send to a named process and a synchronous receive. The Mach operating system provides, in addition, a virtual channel construct (the port); ports can be transferred between processes in messages [38].
Reference: [29] <author> Parnas, D., </author> <title> On the criteria to be used in decomposing systems into modules, </title> <journal> CACM, </journal> <volume> 15(12), </volume> <pages> 1053-1058, </pages> <year> 1972. </year>
Reference-contexts: Send electronic mail to fortran-m@mcs.anl.gov for details. 2 Defining Modules In modular program design, we develop components of a program separately, as independent modules, and then combine modules to obtain a complete program <ref> [29, 10] </ref>. Interactions between modules are restricted to well-defined interfaces. Hence, module implementations can be changed without modifying other components, and the properties of a program can be determined from the specifications for its modules and the code that plugs these modules together.
Reference: [30] <institution> Programming Language Fortran, American National Standard X3.9-1978, American National Standards Institute, </institution> <year> 1978. </year>
Reference-contexts: Syntax Backus-Naur form (BNF) is used to present new syntax, with nonterminal symbols in slanted font, terminal symbols in TYPEWRITER font, and symbols defined in Appendix F of the Fortran 77 standard <ref> [30] </ref> underlined. The syntax [symbol ] is used to represent zero or more comma-separated occurrences of symbol ; [symbol ] (1) represents one or more occurrences.
Reference: [31] <author> Rosing, M., and Saltz, J., </author> <title> Low-latency messages on distributed-memory multiprocessors, </title> <type> Technical Report, ICASE Report, </type> <institution> Institute for Computer Applications in Science and Engineering, Hampton, Virginia, </institution> <year> 1992. </year>
Reference-contexts: In addition, a code generator can generate specialized instruction sequences that use shared memory or drive message-passing hardware more efficiently than general purpose communication libraries. Recent research suggests that specialized communication code can improve message-passing performance by an order of magnitude <ref> [14, 31] </ref>. Fortran M performance also depends on the cost of process creation, scheduling, and termination operations. A preemptive scheduler is required so as to permit overlapping of computation and communication.
Reference: [32] <author> Saltz, J., Berryman, H., and Wu, J., </author> <title> Multiprocessors and run-time compilation, </title> <type> ICASE Report 90-59, </type> <institution> Institute for Computer Applications in Science and Engineering, Hampton, Virginia, </institution> <year> 1990. </year> <month> 30 </month>
Reference-contexts: This approach permits succinct specifications of parallel algorithms for regular problems and guarantees deterministic execution. When extended with support 19 for irregular data distributions, data parallel languages can also handle some irregular problems <ref> [23, 32] </ref>. However, there are broad classes of problems for which the approach has not yet been shown to be tractable. These include highly dynamic adaptive grid problems, multidisciplinary optimization problems, and reactive systems in which a program interacts with an external environment in a nondeterministic manner.
Reference: [33] <author> Seitz, C. L., </author> <title> Multicomputers, Developments in Concurrency and Communication, </title> <publisher> Addison-Wesley, </publisher> <year> 1991. </year>
Reference-contexts: Fortran M performance also depends on the cost of process creation, scheduling, and termination operations. A preemptive scheduler is required so as to permit overlapping of computation and communication. Fortunately, these facilities are, increasingly, supported either at the operating system [38, 9] or hardware levels <ref> [21, 33, 11] </ref>, or can be provided by a compiler [14]. 9 Related Work Programming notations for parallel scientific programming fall into three principal classes: coordination languages, message-passing libraries, and data parallel extensions. <p> Tuple space operations can emulate both message-passing communication protocols and shared data structures. Tuple space operations, like message passing, are nondeterministic and do not enforce information hiding. Actor-based message passing systems such as CE/RK <ref> [33] </ref> have some points of similarity with Fortran M, but are fundamentally different in that they are nondeterministic. CC++ is a shared virtual memory extension of C++ [6]. It differs from Fortran M in many respects, in particular its use of a shared-memory programming model.
Reference: [34] <author> Skjellum, A., and Leung, A., </author> <title> Zipcode: A portable multicomputer communication library atop the Reactive Kernel, </title> <booktitle> Proc. 5th Distributed Memory Computer Conf., </booktitle> <publisher> IEEE Press, </publisher> <pages> 767-776, </pages> <year> 1990. </year>
Reference-contexts: Strand and PCN do not address the problem of Fortran common data. In message-passing library approaches, programmers call subroutines to communicate data between processes. The number of processes is often fixed at one per physical processor. Systems such as P4 [1], Express [28], PVM [35], and Zipcode <ref> [34] </ref> provide, as primitives, an asynchronous send to a named process and a synchronous receive. The Mach operating system provides, in addition, a virtual channel construct (the port); ports can be transferred between processes in messages [38]. Mach does not restrict copying of ports, so determinism is not enforced.
Reference: [35] <author> Sunderam, V., </author> <title> PVM: A framework for parallel distributed computing, </title> <journal> Concurrency: Practice and Experience, </journal> <volume> 2, </volume> <pages> 315-339, </pages> <year> 1990. </year>
Reference-contexts: Strand and PCN do not address the problem of Fortran common data. In message-passing library approaches, programmers call subroutines to communicate data between processes. The number of processes is often fixed at one per physical processor. Systems such as P4 [1], Express [28], PVM <ref> [35] </ref>, and Zipcode [34] provide, as primitives, an asynchronous send to a named process and a synchronous receive. The Mach operating system provides, in addition, a virtual channel construct (the port); ports can be transferred between processes in messages [38].
Reference: [36] <author> Taylor, S., </author> <title> Parallel Logic Programming Techniques, </title> <publisher> Prentice-Hall, </publisher> <address> Englewood Cliffs, N.J., </address> <year> 1989. </year>
Reference-contexts: on a parallel computer by changing mapping constructs. 7.1 Process Placement Constructs The Fortran M process placement constructs are based on the concept of a virtual computer: a collection of virtual processors, which may or may not have the same topology as the physical computer on which a program executes <ref> [25, 36] </ref>. For consistency with Fortran concepts, a Fortran M virtual computer is an N -dimensional array, and the mapping constructs are modeled on Fortran 77's array manipulation constructs.
Reference: [37] <author> Thinking Machines Corporation, </author> <title> CM Fortran Reference Manual, </title> <address> Cambridge, Mass., </address> <year> 1989. </year>
Reference-contexts: CC++ is a shared virtual memory extension of C++ [6]. It differs from Fortran M in many respects, in particular its use of a shared-memory programming model. In data parallel approaches, sequential languages are extended with directives that specify how arrays are to be decomposed and distributed over processors <ref> [37, 18, 8] </ref>. A compiler then partitions the computation using the "owner computes" rule, with each operation in the sequential program allocated to the processor containing the data that is to be operated on. This approach permits succinct specifications of parallel algorithms for regular problems and guarantees deterministic execution.
Reference: [38] <author> Young, M., et al., </author> <title> The duality of memory and communication in Mach, </title> <booktitle> Proc. 11th Symp. on Operating System Principles, ACM, </booktitle> <pages> 63-76, </pages> <year> 1987. </year> <month> 31 </month>
Reference-contexts: Fortran M performance also depends on the cost of process creation, scheduling, and termination operations. A preemptive scheduler is required so as to permit overlapping of computation and communication. Fortunately, these facilities are, increasingly, supported either at the operating system <ref> [38, 9] </ref> or hardware levels [21, 33, 11], or can be provided by a compiler [14]. 9 Related Work Programming notations for parallel scientific programming fall into three principal classes: coordination languages, message-passing libraries, and data parallel extensions. <p> Systems such as P4 [1], Express [28], PVM [35], and Zipcode [34] provide, as primitives, an asynchronous send to a named process and a synchronous receive. The Mach operating system provides, in addition, a virtual channel construct (the port); ports can be transferred between processes in messages <ref> [38] </ref>. Mach does not restrict copying of ports, so determinism is not enforced. Libraries have the advantage of simplicity: they are language independent and do not require compiler modifications. This simplicity comes at a price, however. Compile-time checking for correct usage is not performed.
References-found: 37

