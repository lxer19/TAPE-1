URL: http://www.cse.psu.edu/~bishop/jvlsisp.ps
Refering-URL: http://www.cse.psu.edu/~bishop/
Root-URL: http://www.cse.psu.edu
Title: Aggressive Dynamic Execution of Decoded Traces  
Author: Benjamin Bishop Thomas P. Kelliher Robert M. Owens Mary Jane Irwin 
Address: Park, PA  
Affiliation: Department of Computer Science and Engineering The Pennsylvania State University University  
Abstract: In this paper, we consider the increased performance that can be obtained by using, in concert, three previously proposed enhancements. These enhancements are aggressive dynamic (run time) instruction scheduling, the reuse of decoded instructions, and trace scheduling (both aggressive dynamic instruction scheduling and decoded instruction reuse have been used in commercial systems). We show that these three enhancements complement and support one another. Hence, while each of these enhancements has been shown to have merit in its own right, when used in concert, we claim the overall advantage is greater than that obtained by using any one singly. To support this claim, we present the results from running several common multimedia kernels. Overall, our results show an average speedup of 3.50 times what can be had by using dynamic instruction scheduling alone. Subsequent simulations show results of 7.3 instructions completed per cycle for the best-performing benchmark for a reasonably aggressive microarchitecture that combines trace scheduling of decoded instruction (i.e. decoded traces) with aggressive dynamic execution. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <institution> "A Tour of the Pentium Pro Processor Microarchitecture" http://www.intel.com/procs/ppro/info/p6white/index.htm. </institution>
Reference-contexts: In fact, in processors using aggressive dynamic scheduling, performance may be limited not by the effective bandwidth of functional units but instead by the fetch and decode bandwidth. For example, while the Intel Pentium Pro <ref> [1] </ref> can issue five microoperations per cycle, it can (at best) produce only three from the fetch and decode unit. To help increase the effective fetch and decode bandwidth and to decrease its latency, the reuse of decoded instructions has been proposed [4, 3]. <p> consider the inner loop of one of the block error estimation kernels of the Berkeley MPEG encoder [11]. for ( y = 0; y &lt; 16; y++ ) - across = &(prev [fy+y][fx]); cacross = currentBlock [y]; localDiff = across [0]-cacross [0]; diff += ABS (localDiff); localDiff = across <ref> [1] </ref>-cacross [1]; diff += ABS (localDiff); localDiff = across [2]-cacross [2]; diff += ABS (localDiff); localDiff = across [3]-cacross [3]; diff += ABS (localDiff); localDiff = across [4]-cacross [4]; diff += ABS (localDiff); localDiff = across [5]-cacross [5]; diff += ABS (localDiff); localDiff = across [6]-cacross [6]; diff += ABS (localDiff); localDiff
Reference: [2] <author> W.M. Hwu and Y.N. Patt, "HPSm, </author> <title> A High Performance Restricted Data Flow Architecture Having Minimal Functionally" Proc. </title> <publisher> ISCA, </publisher> <address> Tokyo, </address> <year> 1986, </year> <pages> pp. 297-306. </pages>
Reference-contexts: This, in and of itself, has proved to be a very difficult problem to solve. What has been called restrictive dataflow <ref> [2, 3, 14] </ref> can be viewed as a compromise between completely static and completely dynamic scheduling. Conceptually, in restrictive dataflow, the instructions of a statically scheduled instruction stream are first decoded and then added to a pool of now dynamically schedulable instructions. <p> error estimation kernels of the Berkeley MPEG encoder [11]. for ( y = 0; y &lt; 16; y++ ) - across = &(prev [fy+y][fx]); cacross = currentBlock [y]; localDiff = across [0]-cacross [0]; diff += ABS (localDiff); localDiff = across [1]-cacross [1]; diff += ABS (localDiff); localDiff = across <ref> [2] </ref>-cacross [2]; diff += ABS (localDiff); localDiff = across [3]-cacross [3]; diff += ABS (localDiff); localDiff = across [4]-cacross [4]; diff += ABS (localDiff); localDiff = across [5]-cacross [5]; diff += ABS (localDiff); localDiff = across [6]-cacross [6]; diff += ABS (localDiff); localDiff = across [7]-cacross [7]; diff += ABS (localDiff); localDiff
Reference: [3] <author> S. Melvin, M. Shebanow, Y. Patt, </author> <title> "Hardware Support for Large Atomic Units in Dynamically Scheduled Machines" Proc. </title> <booktitle> 21th Ann. International Symposium on Microarchitecture, </booktitle> <month> December </month> <year> 1988. </year>
Reference-contexts: This, in and of itself, has proved to be a very difficult problem to solve. What has been called restrictive dataflow <ref> [2, 3, 14] </ref> can be viewed as a compromise between completely static and completely dynamic scheduling. Conceptually, in restrictive dataflow, the instructions of a statically scheduled instruction stream are first decoded and then added to a pool of now dynamically schedulable instructions. <p> For example, while the Intel Pentium Pro [1] can issue five microoperations per cycle, it can (at best) produce only three from the fetch and decode unit. To help increase the effective fetch and decode bandwidth and to decrease its latency, the reuse of decoded instructions has been proposed <ref> [4, 3] </ref>. The use of this technique has seen its greatest promise in the context of a CISC instruction set architecture (ISA) where the decode process is, of course, much more complex than it is for a RISC ISA. <p> for ( y = 0; y &lt; 16; y++ ) - across = &(prev [fy+y][fx]); cacross = currentBlock [y]; localDiff = across [0]-cacross [0]; diff += ABS (localDiff); localDiff = across [1]-cacross [1]; diff += ABS (localDiff); localDiff = across [2]-cacross [2]; diff += ABS (localDiff); localDiff = across <ref> [3] </ref>-cacross [3]; diff += ABS (localDiff); localDiff = across [4]-cacross [4]; diff += ABS (localDiff); localDiff = across [5]-cacross [5]; diff += ABS (localDiff); localDiff = across [6]-cacross [6]; diff += ABS (localDiff); localDiff = across [7]-cacross [7]; diff += ABS (localDiff); localDiff = across [8]-cacross [8]; diff += ABS (localDiff); localDiff
Reference: [4] <author> M. Smotherman, M. Franklin, </author> <title> "Improving CISC Instruction Decoding Performance Using a Fill Unit" Proc. </title> <booktitle> 28th Ann. International Symposium on Microarchitecture, </booktitle> <year> 1995. </year>
Reference-contexts: VLIW and wide-issue superscalar processors require substantial instruction fetch and decode bandwidth to achieve high utilization of the functional units. To help alleviate this problem, it is possible to reuse the decoded instructions instead of having to re-decode an instruction every time it is executed <ref> [4] </ref>. Hence, instead of having a cache of encoded instructions, a cache of decoded instructions is utilized. On average, the decode efficiency (in terms of latency and hardware cost) can thus be increased because instructions coming from the decoded instruction buffer require no additional decoding. 2.3 Trace Cache. <p> For example, while the Intel Pentium Pro [1] can issue five microoperations per cycle, it can (at best) produce only three from the fetch and decode unit. To help increase the effective fetch and decode bandwidth and to decrease its latency, the reuse of decoded instructions has been proposed <ref> [4, 3] </ref>. The use of this technique has seen its greatest promise in the context of a CISC instruction set architecture (ISA) where the decode process is, of course, much more complex than it is for a RISC ISA. <p> ) - across = &(prev [fy+y][fx]); cacross = currentBlock [y]; localDiff = across [0]-cacross [0]; diff += ABS (localDiff); localDiff = across [1]-cacross [1]; diff += ABS (localDiff); localDiff = across [2]-cacross [2]; diff += ABS (localDiff); localDiff = across [3]-cacross [3]; diff += ABS (localDiff); localDiff = across <ref> [4] </ref>-cacross [4]; diff += ABS (localDiff); localDiff = across [5]-cacross [5]; diff += ABS (localDiff); localDiff = across [6]-cacross [6]; diff += ABS (localDiff); localDiff = across [7]-cacross [7]; diff += ABS (localDiff); localDiff = across [8]-cacross [8]; diff += ABS (localDiff); localDiff = across [9]-cacross [9]; diff += ABS (localDiff); localDiff <p> However, while Rotenberg et al. assumed a trace of encoded instructions, we assumed a trace cache of decoded instructions so as to obtain the same increase in the ease of register renaming as that obtained in <ref> [4] </ref>. As indicated in this reference, register renaming remains a major constriction on the throughput when using a larger number of instructions (in our case due to parallel trace fetching). 4. SIMULATION METHODOLOGY Two simulators were designed based on the Intel Pentium Pro architecture.
Reference: [5] <author> M. </author> <title> Hiraki et al.,"Stage-Skip Pipeline: A Low Power Processor Architecture Using a Decoded Instruction Buffer" 1996 International Symposium on Low Power Electronics and Design, </title> <month> August </month> <year> 1996. </year>
Reference-contexts: [y]; localDiff = across [0]-cacross [0]; diff += ABS (localDiff); localDiff = across [1]-cacross [1]; diff += ABS (localDiff); localDiff = across [2]-cacross [2]; diff += ABS (localDiff); localDiff = across [3]-cacross [3]; diff += ABS (localDiff); localDiff = across [4]-cacross [4]; diff += ABS (localDiff); localDiff = across <ref> [5] </ref>-cacross [5]; diff += ABS (localDiff); localDiff = across [6]-cacross [6]; diff += ABS (localDiff); localDiff = across [7]-cacross [7]; diff += ABS (localDiff); localDiff = across [8]-cacross [8]; diff += ABS (localDiff); localDiff = across [9]-cacross [9]; diff += ABS (localDiff); localDiff = across [10]-cacross [10]; diff += ABS (localDiff); localDiff
Reference: [6] <author> E. Rotenberg et al., </author> <title> "Trace Cache: a Low Latency Approach to High Bandwidth Instruction Fetching" 29th Annual International Symposium on Microarchitecture, </title> <month> December </month> <year> 1996. </year>
Reference-contexts: On average, the decode efficiency (in terms of latency and hardware cost) can thus be increased because instructions coming from the decoded instruction buffer require no additional decoding. 2.3 Trace Cache. The trace cache technique, proposed by Rotenberg et al. <ref> [6] </ref>, was created to address the instruction fetch bandwidth bottleneck in superscalar processors without regard to the instruction scheduling method. This is done by organizing stored instructions in an on-chip trace cache according to actual run-time control flow. <p> This is done by organizing stored instructions in an on-chip trace cache according to actual run-time control flow. Thus, multiple basic blocks can be fetched from the trace cache per cycle that would be non-contiguous in a typical instruction cache. See <ref> [6] </ref> for a quantitative analysis of this technique. The trace cache is organized into a series of traces. Each trace structure contains a tag, branch target, and branch fall-through fields as well as branch prediction information. <p> (localDiff); localDiff = across [1]-cacross [1]; diff += ABS (localDiff); localDiff = across [2]-cacross [2]; diff += ABS (localDiff); localDiff = across [3]-cacross [3]; diff += ABS (localDiff); localDiff = across [4]-cacross [4]; diff += ABS (localDiff); localDiff = across [5]-cacross [5]; diff += ABS (localDiff); localDiff = across <ref> [6] </ref>-cacross [6]; diff += ABS (localDiff); localDiff = across [7]-cacross [7]; diff += ABS (localDiff); localDiff = across [8]-cacross [8]; diff += ABS (localDiff); localDiff = across [9]-cacross [9]; diff += ABS (localDiff); localDiff = across [10]-cacross [10]; diff += ABS (localDiff); localDiff = across [11]-cacross [11]; diff += ABS (localDiff); localDiff <p> Therefore, to maintain a large pool of dynamic instructions, we turn to a more direct approach (which has been suggested before). Using a trace cache, as proposed by Rotenberg et al. <ref> [6] </ref>, allows the pool of dynamically schedulable instructions to be quickly filled. However, while Rotenberg et al. assumed a trace of encoded instructions, we assumed a trace cache of decoded instructions so as to obtain the same increase in the ease of register renaming as that obtained in [4].
Reference: [7] <author> M. Slater, </author> <title> "The Land Beyond Benchmarks" Comput. </title> <journal> Commun. OEM , Mag. </journal> <volume> 4, 31, </volume> <pages> pp. 64-77, </pages> <month> September </month> <year> 1996. </year>
Reference-contexts: (localDiff); localDiff = across [2]-cacross [2]; diff += ABS (localDiff); localDiff = across [3]-cacross [3]; diff += ABS (localDiff); localDiff = across [4]-cacross [4]; diff += ABS (localDiff); localDiff = across [5]-cacross [5]; diff += ABS (localDiff); localDiff = across [6]-cacross [6]; diff += ABS (localDiff); localDiff = across <ref> [7] </ref>-cacross [7]; diff += ABS (localDiff); localDiff = across [8]-cacross [8]; diff += ABS (localDiff); localDiff = across [9]-cacross [9]; diff += ABS (localDiff); localDiff = across [10]-cacross [10]; diff += ABS (localDiff); localDiff = across [11]-cacross [11]; diff += ABS (localDiff); localDiff = across [12]-cacross [12]; diff += ABS (localDiff); localDiff <p> With the rising popularity of multimedia applications, MPEG compression and decompression kernels are becoming very important. This is shown by the inclusion of MPEG decompression in the Intel media benchmark <ref> [7, 10] </ref>. 6. RESULTS The simulations that have been conducted suggest that there are substantial performance gains to be made through the use of the decoded trace cache. Figures 1 and 2 compare the parallelism of the two simulators for the benchmarks tested.
Reference: [8] <author> J. L. Hennessy and D. A. Patterson, </author> <title> Computer Architecture a Quantitative Approach, </title> <publisher> Morgan Kaufmann Publishers, </publisher> <address> San Mateo, CA. </address> <note> [9] "The GCC Compiler Version 2.7.2" http://ftp.cs.umn.edu/pub/gnu/gcc-2.7.2.tar.gz. </note>
Reference-contexts: (localDiff); localDiff = across [3]-cacross [3]; diff += ABS (localDiff); localDiff = across [4]-cacross [4]; diff += ABS (localDiff); localDiff = across [5]-cacross [5]; diff += ABS (localDiff); localDiff = across [6]-cacross [6]; diff += ABS (localDiff); localDiff = across [7]-cacross [7]; diff += ABS (localDiff); localDiff = across <ref> [8] </ref>-cacross [8]; diff += ABS (localDiff); localDiff = across [9]-cacross [9]; diff += ABS (localDiff); localDiff = across [10]-cacross [10]; diff += ABS (localDiff); localDiff = across [11]-cacross [11]; diff += ABS (localDiff); localDiff = across [12]-cacross [12]; diff += ABS (localDiff); localDiff = across [13]-cacross [13]; diff += ABS (localDiff); localDiff <p> Instruction latencies are as follows: 1 cycle for integer operations, 2 cycles for floating point adds and memory operations of any kind, 5 cycles for floating point multiplies, and 19 cycles for floating point divides. Both simulators were constructed by using the DLX ISA and interface <ref> [8] </ref>. The simulators were tested with a number of popular benchmarks compiled under a modified version of GCC for the DLX ISA. 4.1 The Architectural Simulator. In both simulators, the architecture allows for superscalar dynamic instruction scheduling as described in Section 2.1.
Reference: [10] <author> A. Peleg, S. Wilkie, U. Weiser, </author> <booktitle> "Intel MMX for Multimedia PCs" Communications of the ACM , vol. </booktitle> <volume> 40, no. 1, </volume> <pages> pp. 25-38, </pages> <month> Jan. </month> <year> 1997. </year>
Reference-contexts: (localDiff); localDiff = across [5]-cacross [5]; diff += ABS (localDiff); localDiff = across [6]-cacross [6]; diff += ABS (localDiff); localDiff = across [7]-cacross [7]; diff += ABS (localDiff); localDiff = across [8]-cacross [8]; diff += ABS (localDiff); localDiff = across [9]-cacross [9]; diff += ABS (localDiff); localDiff = across <ref> [10] </ref>-cacross [10]; diff += ABS (localDiff); localDiff = across [11]-cacross [11]; diff += ABS (localDiff); localDiff = across [12]-cacross [12]; diff += ABS (localDiff); localDiff = across [13]-cacross [13]; diff += ABS (localDiff); localDiff = across [14]-cacross [14]; diff += ABS (localDiff); localDiff = across [15]-cacross [15]; diff += ABS (localDiff); if <p> Small loops could be unrolled to reduce branch overhead and larger loops could possibly be split so that the smaller loops fit in the decoded trace cache. Such complicated optimizations are beyond the scope of the current compiler. 5. BENCHMARKS As seen in industry <ref> [10] </ref>, there is a great demand for architectures specifically adapted for multimedia applications. In order to demonstrate the efficiency of this architecture for such applications, the simulators were tested on a variety of multimedia based benchmarks. 5.1 FAST Discrete Cosine Transform. <p> The Discrete Cosine Transform is an optimized version of the Discrete Fourier Transform. It is used in a number of signal processing and multimedia applications. The importance of the DCT can be seen by the discussion of it in <ref> [10] </ref>. 5.2 LPC Speech Compression Algorithms. This benchmark makes use of linear prediction kernels for speech encoding. This is accomplished through solving a system of linear equations by computing a matrix of covariants. <p> With the rising popularity of multimedia applications, MPEG compression and decompression kernels are becoming very important. This is shown by the inclusion of MPEG decompression in the Intel media benchmark <ref> [7, 10] </ref>. 6. RESULTS The simulations that have been conducted suggest that there are substantial performance gains to be made through the use of the decoded trace cache. Figures 1 and 2 compare the parallelism of the two simulators for the benchmarks tested.
Reference: [11] <author> L. Rowe et al., </author> <title> "Berkeley MPEG Tools" ftp://mm-ftp.cs.berkeley.edu/pub/multimedia/mpeg/bmt1r1.tar.gz </title> . 
Reference-contexts: This occurs for the following reason. The time-consuming inner loops of many signal processing applications have already been unrolled. For example, consider the inner loop of one of the block error estimation kernels of the Berkeley MPEG encoder <ref> [11] </ref>. for ( y = 0; y &lt; 16; y++ ) - across = &(prev [fy+y][fx]); cacross = currentBlock [y]; localDiff = across [0]-cacross [0]; diff += ABS (localDiff); localDiff = across [1]-cacross [1]; diff += ABS (localDiff); localDiff = across [2]-cacross [2]; diff += ABS (localDiff); localDiff = across [3]-cacross <p> (localDiff); localDiff = across [6]-cacross [6]; diff += ABS (localDiff); localDiff = across [7]-cacross [7]; diff += ABS (localDiff); localDiff = across [8]-cacross [8]; diff += ABS (localDiff); localDiff = across [9]-cacross [9]; diff += ABS (localDiff); localDiff = across [10]-cacross [10]; diff += ABS (localDiff); localDiff = across <ref> [11] </ref>-cacross [11]; diff += ABS (localDiff); localDiff = across [12]-cacross [12]; diff += ABS (localDiff); localDiff = across [13]-cacross [13]; diff += ABS (localDiff); localDiff = across [14]-cacross [14]; diff += ABS (localDiff); localDiff = across [15]-cacross [15]; diff += ABS (localDiff); if ( diff &gt; bestSoFar ) - return diff; - <p> Naturally a highly parallel task, this benchmark is well suited for gauging the success of the architectural enhancements. 5.3 MPEG Video Compression Algorithms. In the Berkeley MPEG encoding algorithm <ref> [11] </ref>, the program was executed with the Find Best Match Exhaustive search algorithm. The program was used on a number of images with proportional results. Two tests were conducted, one using mean square error (MSE) and the other using the mean absolute distance error (MAD) approximation.
Reference: [12] <author> K. Gong and L. Rowe, </author> <title> "Parallel MPEG-1 Video Encoding" 1994 Picture Coding Symposium, </title> <address> Sacramento, CA, </address> <month> September </month> <year> 1994. </year>
Reference-contexts: Efficient execution of these kernels yields significant overall performance gains due to the large amount of time spent in kernel execution. 1 Thomas P. Kelliher is with the Department of Mathematics & Computer Science, Goucher College, 1021 Dulaney Valley Rd., Baltimore, MD 21204. For example, <ref> [12] </ref> shows that over 90% of the execution time for the MPEG encoding program is spent in the block matching kernel. In Section 2, we present background information reviewing the three abovementioned techniques. In Section 3, we discuss the performance issues effecting the design. <p> (localDiff); localDiff = across [7]-cacross [7]; diff += ABS (localDiff); localDiff = across [8]-cacross [8]; diff += ABS (localDiff); localDiff = across [9]-cacross [9]; diff += ABS (localDiff); localDiff = across [10]-cacross [10]; diff += ABS (localDiff); localDiff = across [11]-cacross [11]; diff += ABS (localDiff); localDiff = across <ref> [12] </ref>-cacross [12]; diff += ABS (localDiff); localDiff = across [13]-cacross [13]; diff += ABS (localDiff); localDiff = across [14]-cacross [14]; diff += ABS (localDiff); localDiff = across [15]-cacross [15]; diff += ABS (localDiff); if ( diff &gt; bestSoFar ) - return diff; - - Normally, a large pool of dynamic instructions can
Reference: [13] <author> G. Bergland, </author> <title> "A Radix-eight Fast Fourier Transform Subroutine for Real-valued Series" IEEE Transactions on Audio and Electro-acoustics, </title> <booktitle> vol. AU-17, </booktitle> <pages> pp. 138-144, </pages> <year> 1969. </year>
Reference-contexts: (localDiff); localDiff = across [8]-cacross [8]; diff += ABS (localDiff); localDiff = across [9]-cacross [9]; diff += ABS (localDiff); localDiff = across [10]-cacross [10]; diff += ABS (localDiff); localDiff = across [11]-cacross [11]; diff += ABS (localDiff); localDiff = across [12]-cacross [12]; diff += ABS (localDiff); localDiff = across <ref> [13] </ref>-cacross [13]; diff += ABS (localDiff); localDiff = across [14]-cacross [14]; diff += ABS (localDiff); localDiff = across [15]-cacross [15]; diff += ABS (localDiff); if ( diff &gt; bestSoFar ) - return diff; - - Normally, a large pool of dynamic instructions can be maintained without branch prediction for loops with large <p> In order to demonstrate the efficiency of this architecture for such applications, the simulators were tested on a variety of multimedia based benchmarks. 5.1 FAST Discrete Cosine Transform. FAST is an algorithm that implements a radix-eight Discrete Cosine Transform as described by Bergland <ref> [13] </ref>. For this benchmark, an eight point kernel is used. Discrete Cosine Transforms typically use a great deal of looping, but the algorithm used was unrolled except for the primary loop. This had a very positive effect, as seen in Section 6.1.
Reference: [14] <author> Y.N.Patt, W.M. Hwu and M.C. Shebanow, "HPS, </author> <title> A New Microarchitec-ture: </title> <booktitle> Rational and Introduction" 18th Annual International Symposium on Microarchitecture, Asilomar, </booktitle> <month> December </month> <year> 1985, </year> <pages> pp. 103-108. </pages>
Reference-contexts: This, in and of itself, has proved to be a very difficult problem to solve. What has been called restrictive dataflow <ref> [2, 3, 14] </ref> can be viewed as a compromise between completely static and completely dynamic scheduling. Conceptually, in restrictive dataflow, the instructions of a statically scheduled instruction stream are first decoded and then added to a pool of now dynamically schedulable instructions. <p> (localDiff); localDiff = across [9]-cacross [9]; diff += ABS (localDiff); localDiff = across [10]-cacross [10]; diff += ABS (localDiff); localDiff = across [11]-cacross [11]; diff += ABS (localDiff); localDiff = across [12]-cacross [12]; diff += ABS (localDiff); localDiff = across [13]-cacross [13]; diff += ABS (localDiff); localDiff = across <ref> [14] </ref>-cacross [14]; diff += ABS (localDiff); localDiff = across [15]-cacross [15]; diff += ABS (localDiff); if ( diff &gt; bestSoFar ) - return diff; - - Normally, a large pool of dynamic instructions can be maintained without branch prediction for loops with large bodies and data independent loop control.
Reference: [15] <author> B. Bishop, T. P. Kelliher, R. M. Owens, and M. J. Irwin, </author> <title> "Re-Evaluating MPEG Motion Compensation Search Criteria." </title> <note> To appear in SiPS '98 </note> . 
Reference-contexts: (localDiff); localDiff = across [10]-cacross [10]; diff += ABS (localDiff); localDiff = across [11]-cacross [11]; diff += ABS (localDiff); localDiff = across [12]-cacross [12]; diff += ABS (localDiff); localDiff = across [13]-cacross [13]; diff += ABS (localDiff); localDiff = across [14]-cacross [14]; diff += ABS (localDiff); localDiff = across <ref> [15] </ref>-cacross [15]; diff += ABS (localDiff); if ( diff &gt; bestSoFar ) - return diff; - - Normally, a large pool of dynamic instructions can be maintained without branch prediction for loops with large bodies and data independent loop control. <p> The MSE approximation code is less sequential in nature and is therefore easier to execute without dependency related stalls. Interestingly, for the conventional simulator, the MAD algorithm ran faster, and for the enhanced simulator, the MSE algorithm had a faster execution time <ref> [15] </ref>. For both algorithms, the number of instructions that had to be fetched and decoded in the enhanced simulator was insignificant (again less than .1%). 6.4 Subsequent Results. Our subsequent research has been focused upon tuning the architecture for multimedia applications.
References-found: 14

