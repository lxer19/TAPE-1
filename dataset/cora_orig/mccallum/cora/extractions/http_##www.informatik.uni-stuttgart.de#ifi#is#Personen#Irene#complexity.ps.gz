URL: http://www.informatik.uni-stuttgart.de/ifi/is/Personen/Irene/complexity.ps.gz
Refering-URL: http://www.informatik.uni-stuttgart.de/ifi/is/Personen/weber.html
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Title: Language Series Revisited: The Complexity of Hypothesis Spaces in ILP  
Author: Irene Weber, Birgit Tausend and Irene Stahl 
Address: Stuttgart, Breitwiesenstr. 20-22, D-70565 Stuttgart  
Affiliation: Fakultat Informatik, Universitat  
Abstract: Restrictions on the number and depth of existential variables as defined in the language series of Clint [Rae92] are widely used in ILP and expected to produce a considerable reduction in the size of the hypothesis space. In this paper we show that this is generally not the case. The lower bounds we present lead to intractable hypothesis spaces except for toy domains. We argue that the parameters chosen in Clint are unsuitable for sensible bias shift operations, and propose alternative approaches resulting in the desired reduction of the hypothesis space and allowing for a natural integration of the shift of bias.
Abstract-found: 1
Intro-found: 1
Reference: [FP91] <author> A. </author> <title> Frisch and C.D. Page. Lerning constrained atoms. </title> <booktitle> In Proc. of the Eighth International Workshop on Machine Learning, </booktitle> <address> Ithaca, New York, 1991. </address> <publisher> Mor-gan Kaufmann. </publisher>
Reference-contexts: Pazzani and Kibler [PK92] have shown that the hypothesis space grows with the number of variables within the hypothesis clauses. The more existential variables are allowed, the more complex is the resulting hypothesis space. On the other hand, <ref> [FP91] </ref> have proved efficient PAC-learnability of constrained clauses, i.e., clauses without existential variables. The language series used in Clint basically combine the restrictions on the depth and the number of existential variables within the hypothesis clauses.
Reference: [LW92] <author> J. H. Lint and R. M. Wilson. </author> <title> A Course in Cominatorics. </title> <publisher> Cambridge University Press, </publisher> <year> 1992. </year>
Reference-contexts: This yields a variabilisation for each arbitrary partitioning, and all of them are non-equivalent. The Bell numbers B (n), well-known in combinatorics <ref> [LW92] </ref>, exactly capture the number of different disjunctive partitions of a set with n elements. They are defined as follows. B (0) = 1 n X s fl B (s) In order to obtain a hypothesis language with linked clauses only, each vari--abilisation must contain at least one old variable.
Reference: [MF90] <author> S. Muggleton and C. Feng. </author> <title> Efficient induction of logic programs. </title> <booktitle> In First Conference on Algorithmic Learning Theory, </booktitle> <address> Tokyo, 1990. </address> <publisher> Ohmsha. </publisher>
Reference-contexts: Language biases like linked or function-free clauses scarcely affect the expressiveness of the hypotheses [Rou92], but also fail to reduce the search space substantially. In contrast, a bias like ij-determinacy <ref> [MF90] </ref> leads to an efficiently searchable hypothesis space, but excludes many interesting concepts. An important and often used device for reducing the complexity of the search space is the number and depth of existential variables within the hypotheses.
Reference: [Mit90] <author> T. M. Mitchell. </author> <title> The need for biases in learning generalizations. </title> <editor> In J. W. Shavlik and T. G. Dietterich, editors, </editor> <booktitle> Readings in Machine Learning. </booktitle> <publisher> Morgan Kaufmann, </publisher> <year> 1990. </year>
Reference-contexts: Even if function-free Horn logic is used, it is intractably large. To allow for efficient learning procedures, it must be greatly restricted. The term bias refers to any basis for excluding hypotheses from the search space, other than strict completeness and consistency with the examples <ref> [Mit90] </ref>. An important part of a system's bias is the hypothesis language. It restricts the range of expressible concepts on the one hand, but enables tractable learning procedures on the other.
Reference: [Mug90] <author> S. Muggleton. </author> <title> Inductive Logic Programming. </title> <booktitle> In First Conference on Algorithmic Learning Theory, </booktitle> <address> Tokio, </address> <month> October </month> <year> 1990. </year> <pages> Ohmsha. </pages>
Reference-contexts: 1 Introduction The limited representation formalism of propositional learning algorithms and their difficulties in using substantial background knowledge has led to an increasing interest in investigating learning methods in a first order framework. Inductive Logic Programming (ILP ) <ref> [Mug90, Mug93] </ref> is an approach that has received a lot of attention recently. The task of ILP is to learn logic programs by induction from examples in the presence of background knowledge. The first order framework of ILP usually leads to an infinite hypothesis space.
Reference: [Mug93] <author> S. Muggleton. </author> <title> Inductive logic programming: Derivations, </title> <booktitle> successes and shortcoming. In Machine Learning: ECML-93, European Conference on Machine Learning, </booktitle> <address> Wien, Austria. </address> <publisher> Springer, </publisher> <year> 1993. </year>
Reference-contexts: 1 Introduction The limited representation formalism of propositional learning algorithms and their difficulties in using substantial background knowledge has led to an increasing interest in investigating learning methods in a first order framework. Inductive Logic Programming (ILP ) <ref> [Mug90, Mug93] </ref> is an approach that has received a lot of attention recently. The task of ILP is to learn logic programs by induction from examples in the presence of background knowledge. The first order framework of ILP usually leads to an infinite hypothesis space.
Reference: [PK92] <author> M. Pazzani and D. Kibler. </author> <title> The utility of knowledge in inductive learning. </title> <journal> Machine Learning, </journal> <volume> 9(1), </volume> <year> 1992. </year>
Reference-contexts: In contrast, a bias like ij-determinacy [MF90] leads to an efficiently searchable hypothesis space, but excludes many interesting concepts. An important and often used device for reducing the complexity of the search space is the number and depth of existential variables within the hypotheses. Pazzani and Kibler <ref> [PK92] </ref> have shown that the hypothesis space grows with the number of variables within the hypothesis clauses. The more existential variables are allowed, the more complex is the resulting hypothesis space. On the other hand, [FP91] have proved efficient PAC-learnability of constrained clauses, i.e., clauses without existential variables. <p> The set B consists of all literals built from predicates in L, the available variables, and new variables. A distribution of variables to the argument positions of a predicate is called a variabilisation of this predicate <ref> [PK92] </ref>. The number of variabilisations of a predicate depends on the number of given variables and the arity of the predicate. It is easy to compute the different distributions of the given variables to the argument positions of the predicate. <p> number of nonequivalent variabilisations v (a; k) of an a-ary predicate, given k old variables, is a X l fl k l fl B (a l) (1) ka if a 6= k Using the Bell numbers explicitly results in a better, more exact approximation for v (a; k) as in <ref> [PK92] </ref>, where equivalent variabilisations are counted as well. The language series of Clint use numerical parameters to impose restrictions on the number and depth of new variables.
Reference: [Rae92] <author> L. De Raedt. </author> <title> Interactive Theory Revision: an Inductive Logic Programming Approach. </title> <publisher> Academic Press, </publisher> <year> 1992. </year>
Reference-contexts: It restricts the range of expressible concepts on the one hand, but enables tractable learning procedures on the other. This tradeoff between expressiveness of the hypothesis language and efficiency of learning is reflected by most attempts to define suitable language biases. In the ILP system Clint <ref> [Rae92] </ref>, language series are introduced, which allow to dynamically adapt the expressiveness of the hypothesis language when learning fails in a less expressive language. Language biases like linked or function-free clauses scarcely affect the expressiveness of the hypotheses [Rou92], but also fail to reduce the search space substantially. <p> Bottom-up approaches like Clint start the learning process by constructing a starting clause that is subsequently generalized. For computing the starting clauses, Clint computes the most specific clause ? contained in the hypothesis language L H <ref> [Rae92] </ref>. Then Clint generalizes ? until a given positive example e + 2 E + is covered. The set of generalizations of ? which cover e + is called the set of justifications J us (K; L; e + ). <p> Relevant measures for the complexity of generalisation in Clint are 1) the length of a justification and 2) the number of justifications given a positive example. 3 The Complexity of ? As described in <ref> [Rae92] </ref>, Clint offers five different language series. <p> g n fx 1 ; : : : ; x k gj i and (3) each variable in fy 1 ; : : :; y n g " fx 1 ; : : : ; x k g occurs only once:g In the original definition for language series 1 in <ref> [Rae92] </ref>, restriction (3) is included in the definition of the language L i and not in the definition of B i (x 1 ; : : : ; x k ) as here. <p> However, the set B i (x 1 ; : : : ; x k ) is not properly defined without this restriction and the examples given in <ref> [Rae92] </ref> do not match the definition in [Rae92], but the definition given here (see [Web94] for details). We assume that our definition properly reflects the intentions of language series 1 and use it in the following. <p> However, the set B i (x 1 ; : : : ; x k ) is not properly defined without this restriction and the examples given in <ref> [Rae92] </ref> do not match the definition in [Rae92], but the definition given here (see [Web94] for details). We assume that our definition properly reflects the intentions of language series 1 and use it in the following. In condition (1) of the definition, the parameter i determines the maximum number of existential variables in a literal. <p> Thus, it is not sensible to use a language series L 0 ; L 1 ; L 2 ; : : : ; L A1 in a bias shift environment as described in <ref> [Rae92] </ref> for Clint. The cost of an unsuccesful learning attempt is exponential in the size of the starting clauses. <p> The level of existential quantification of a variable v which occurs only in the clause body is defined as 1 plus the minimal level of existential quantification of variables which occur in the same literal as v, formally Definition 1. <ref> [Rae92] </ref> The level (of existential quantification) lq (v) of a variable v in a clause c is lq (v) = 0 if v occurs in head (c) and lq (v) = minflq (y) + 1jy 6= v and y; v occur in the same literal in cg otherwise. <p> in B q i (x 1 ; : : : ; x k ) and consequently all variables with maximum level of existential quantification have only one occurence in B q lowing examples for B q i (x 1 ; : : : ; x k ) are taken from <ref> [Rae92] </ref>. Example 1. Given a set of predicates fmale=1; female=1; parent=2g we obtain the following literals sets.
Reference: [Rou92] <author> C. </author> <title> Rouveirol. ITOU: Induction of first order theories. </title> <editor> In S. Muggleton, editor, </editor> <booktitle> Inductive Logic Programming. </booktitle> <publisher> Academic Press, </publisher> <year> 1992. </year>
Reference-contexts: In the ILP system Clint [Rae92], language series are introduced, which allow to dynamically adapt the expressiveness of the hypothesis language when learning fails in a less expressive language. Language biases like linked or function-free clauses scarcely affect the expressiveness of the hypotheses <ref> [Rou92] </ref>, but also fail to reduce the search space substantially. In contrast, a bias like ij-determinacy [MF90] leads to an efficiently searchable hypothesis space, but excludes many interesting concepts.
Reference: [SW94] <author> I. Stahl and I. Weber. </author> <title> The arguments of newly invented predicates in ILP. </title> <booktitle> In Proc. of ILP-94, </booktitle> <year> 1994. </year>
Reference-contexts: This method leads to starting clauses equivalent to those in languages L q i of series 2 when the new predicates are unfolded. The main advantage is that methods for finding a minimal set of arguments for the new predicates <ref> [SW94] </ref> can be used when introducing the auxiliary predicates. Then, the number k of old variables will generally still be small when constructing a starting clause for the new predicate.
Reference: [Web94] <author> I. Weber. </author> <title> Komplexitat von Hypothesenraumen in der Induktiven Logischen Programmierung. Diplomarbeit Nr. </title> <type> 1164, </type> <institution> Universitat Stuttgart, Fakultat In-formatik, </institution> <year> 1994. </year>
Reference-contexts: However, the set B i (x 1 ; : : : ; x k ) is not properly defined without this restriction and the examples given in [Rae92] do not match the definition in [Rae92], but the definition given here (see <ref> [Web94] </ref> for details). We assume that our definition properly reflects the intentions of language series 1 and use it in the following. In condition (1) of the definition, the parameter i determines the maximum number of existential variables in a literal. <p> 3 are constructed analoguously to the sets B q i (x 1 ; : : :; x k ) of series 2, the equations for computing jB q can easily be adapted for determining jB i 1 ;:::;i n1 ;i n (x 1 ; : : :; x k )j <ref> [Web94] </ref>. Language series 3 offers great flexibility in choosing the parameters i 1 ; : : : ; i n . <p> Explicit unification can also be used with the language series of Clint, but, as we show in <ref> [Web94] </ref>, explicit unification alone cannot reduce the size of ? significantly. When constructing the set B q i (x 1 ; : : : ; x k ), (5) demands that new variables occur only in literals with variables with level of existential quanitfication q 1.
References-found: 11

