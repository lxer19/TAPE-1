URL: http://www.eecs.umich.edu/PPP/rabins-thesis.ps
Refering-URL: http://www.eecs.umich.edu/PPP/publist.html
Root-URL: http://www.cs.umich.edu
Email: rabin@eecs.umich.edu, sga@eecs.umich.edu  
Title: Multi-Configuration Simulation Algorithms for the Evaluation of Computer Architecture Designs  
Author: Rabin A. Sugumar and Santosh G. Abraham 
Date: August 26, 1993  
Address: Ann Arbor, MI 48109-2122  
Affiliation: Advanced Computer Architecture Laboratory Department of Electrical Engineering and Computer Science University of Michigan  
Abstract-found: 0
Intro-found: 1
Reference: [Abr88] <author> S. G. Abraham. </author> <title> Parallel simulation of shared memory multiprocessors. </title> <booktitle> In Proc. 3rd Int. Conf. on Supercomputing, </booktitle> <pages> pages 313-322, </pages> <year> 1988. </year>
Reference-contexts: Multi-configuration simulation and reduced trace simulation are complementary approaches in that multi-configuration simulation algorithms can be used for simulating reduced traces as well. Other approaches to efficient simulation include execution driven simulation <ref> [Abr88] </ref>, [Lin92], [DGH91], [RHL + 93] where some parts of the workloads are directly executed on the host machine; compiler assisted simulation [Wha92] where during instrumentation of the program the compiler performs optimizations which reduce simulation effort; 13 and compiled simulation [Eme].
Reference: [Aga88] <author> A. Agarwal. </author> <title> Analysis of Cache Performance for Operating Systems and Multiprogramming. </title> <type> PhD thesis, </type> <institution> Stanford University, </institution> <year> 1988. </year> <note> Available as Technical Report CSL-TR-87-332. </note>
Reference-contexts: It is seen from the table that the average number of comparisons approaches 3 as the loop size increases. When the smallest line size simulated is greater than one word, the average number of comparisons is less than two. Practical traces consist of interleaved runs of successive addresses <ref> [Aga88] </ref>. All misses and hits occurring on references to starting addresses of runs may be modeled as random accesses. The loop model may be used for hits that occur on references to second and later references in runs.
Reference: [AHH89] <author> A. Agarwal, M. Horowitz, and J. Hennessy. </author> <title> An analytical cache model. </title> <journal> ACM Trans. on Computer Systems, </journal> <volume> 7(2) </volume> <pages> 184-215, </pages> <month> May </month> <year> 1989. </year>
Reference-contexts: Some such models are those that have been proposed by Thibeaut and Stone [Thi89], Agarwal et al. <ref> [AHH89] </ref>, and Hill [Hil87]. Stone and Thiebaut's model, where misses are modeled as a fractal process, has been used in synthetic trace generation [Thi88], modeling locality [Sto87] and in optimizing cache partitioning between processes [WL91]. Agarwal's model has been applied to fast miss ratio estimation [AHH89]. <p> Stone [Thi89], Agarwal et al. <ref> [AHH89] </ref>, and Hill [Hil87]. Stone and Thiebaut's model, where misses are modeled as a fractal process, has been used in synthetic trace generation [Thi88], modeling locality [Sto87] and in optimizing cache partitioning between processes [WL91]. Agarwal's model has been applied to fast miss ratio estimation [AHH89]. In both the above models expressions for miss rates in terms of a few trace-dependent parameters are derived. The 3C's [Hil87, HP90] model is for classifying misses rather than modeling them.
Reference: [AVL62] <author> G. M. Ade lson-Ve lskii and E. M. Landis. </author> <title> An algorithm for the organization of information. </title> <journal> Soviet Math. Doklady, </journal> <volume> 3 </volume> <pages> 1259-1263, </pages> <year> 1962. </year>
Reference-contexts: Kim et al. [KHW91] describe a fast adaptation of the algorithm, when only a few cache capacities are of interest. Thompson [Tho87] presents extensions to the algorithm to count the number of dirty misses with a 3 Described in Chapter 3 4 A widely used balanced binary tree <ref> [AVL62, Smi87] </ref>. 12 write-back cache, and to simulate multiprocessor caches. Horspool and Huberman [HH87] present prefetching policies for which the inclusion property holds, and adapt the algorithm to evaluate systems with prefetching.
Reference: [Bel66] <author> L. A. Belady. </author> <title> A study of replacement algorithms for a virtual-storage computer. </title> <journal> IBM Systems Journal, </journal> <volume> 5(2) </volume> <pages> 78-101, </pages> <year> 1966. </year>
Reference-contexts: Finally, we present a parallel fully-associative cache simulation algorithm. Many of the algorithms presented were implemented and results of performance evaluations of the implementations on real traces are presented. In Chapter 4 we consider simulating caches under OPT replacement <ref> [Bel66] </ref>. OPT replacement uses future information. The earlier technique for simulating caches under 14 OPT replacement requires a reverse pass over the trace to collect the future information [MGST70]. The reverse pass is not feasible with on-the-fly simulation. We present three new techniques for simulating caches under OPT replacement. <p> Since OPT requires future information, it cannot be used in a real cache, and is mainly used in performance analysis. Belady described the first algorithm for simulating caches under OPT replacement in his paper introducing OPT replacement <ref> [Bel66] </ref>.
Reference: [BK75] <author> B. T. Bennett and V. J. Kruskal. </author> <title> LRU stack processing. </title> <journal> IBM J. of Research and Development, </journal> <pages> pages 353-357, </pages> <month> July </month> <year> 1975. </year>
Reference-contexts: Gecsei [Gec74] extends the algorithm to handle memory hierarchies with more than two levels. Coffman and Randell [CR71] modify the algorithm to allow simulation of segments of the stack separately, thus reducing memory requirement. Kruskal and Bennett <ref> [BK75] </ref> note that in its original form the algorithm performs poorly on data base traces and present a faster variation using m-ary trees. Olken [Olk81] evaluates various implementations of the stack and introduces a variation using AVL 4 trees. <p> Many researchers have noted that the sequential search of the stack is a bottleneck in stack simulation and have proposed more efficient implementations of the stack for least recently used (LRU) replacement. Bennett and Kruskal (B&K) describe an algorithm based on m-ary trees in <ref> [BK75] </ref> for LRU replacement exploiting the fact that the stack depth of reference with LRU replacement is the number of distinct lines referenced since the last reference to the line. <p> The lookup is done efficiently as a tree search. We show that the update may be done by examining only a few lines in the stack for some replacement algorithms. In [Olk81] an implementation of the m-ary tree algorithm of <ref> [BK75] </ref> is compared with the AVL tree algorithm. A beta-distribution is assumed for the trace and it is shown that both algorithms have O (log N d ) complexity but that the m-ary tree algorithm has a higher constant factor. <p> A complexity of O (log N d ) for the AVL tree is widely reported, for instance [Smi87]. For B&K's algorithm the number of entries examined on a reference to a line is O (log (Number of references after previous reference to line)) <ref> [BK75] </ref>.
Reference: [Bro78] <author> M. R. Brown. </author> <title> Implementation and analysis of binomial queue algorithms. </title> <journal> SIAM J. of Computing, </journal> <volume> 7 </volume> <pages> 298-319, </pages> <year> 1978. </year>
Reference-contexts: Algorithm GBF LS of Chapter 2 uses a gbt representation of set-associative caches, and is based on the properties proved here. As noted in Chapter 2 gbt's are composed of binomial trees and lists. Binomial tree have appeared many times in the literature <ref> [Vui78, Bro78] </ref>. The following is an inductive definition of binomial trees (Fig. A.1): A binomial tree of degree 1 0 (B 0 in Fig. A.1) has one node. <p> However, when n = 1, (n 1 ; n 2 ) = (1; 0) or (0; 1) and both cases result in the same structure for the tree obtained after combining. So gbt (1)s (ordinary binomial trees) of the same degree are structurally identical, i.e., isomorphic <ref> [Bro78] </ref>. The definition above does not specify how the lists of length n 1 and n 2 should be put together. This depends on the application. Terms such as rank, degree and subtree are now defined for gbt's and lemmas stating various properties are proved.
Reference: [BYA92] <author> G. R. Beck, D. W. L. Yen, and T. L. Anderson. </author> <title> The Cydra 5 mini-supercomputer: architecture and implementation. </title> <journal> The Journal of Supercomputing, </journal> 7(1/2):143-180, 1992. 
Reference-contexts: With the increasing gap between memory access times and CPU cycle times, the CPI penalty caused by these stalls on load misses is becoming significant [Mea91]. Machines such as the Multiflow Trace [CNO + 88] and the Cydra <ref> [BYA92, RYYT89] </ref>, take a different approach; they have a flat memory hierarchy, without caches. However, this results in memory always being distant from the CPU, and often the long latencies result in many empty slots in the schedule.
Reference: [CB92] <author> T-F. Chen and J-L. Baer. </author> <title> Reducing memory latency via non-blocking and prefetching caches. </title> <booktitle> In Proc. of ASPLOS V, </booktitle> <pages> pages 51-61, </pages> <year> 1992. </year>
Reference-contexts: Selective prefetching using compiler techniques for scientific code is also investigated in [KL91, MLG92]. Mowry et al. [MLG92] combine prefetching with software pipelining and also investigate the interaction between blocking and prefetching. Chen and Baer <ref> [CB92] </ref> investigate hardware-based prefetching, and moving loads within basic blocks statically to tolerate cache-miss latencies for some SPEC benchmarks. Chen et al. [CMH92] reduce the effect of primary cache latency by preloading all memory accesses for the SPEC benchmarks.
Reference: [CD89] <author> C-H. Chi and H. Dietz. </author> <title> Unified management of registers and cache using liveness and cache bypass. </title> <booktitle> In Proc. of the SIGPLAN '89 Conf. on Prog. Lang. Design and Implementation, </booktitle> <pages> pages 344-355, </pages> <year> 1989. </year>
Reference-contexts: They evaluate the degree of improvement that may be obtained by using program control of cache. Memory traffic is divided into three components | read, write-allocate and write-back, and reductions in all of them by program control is addressed. Chi and Dietz <ref> [CD89] </ref> also describe a scheme for software control of caches. They propose using the cache only for storing ambiguous references and unambiguous references that are spilled out of registers. They mention using liveness information and referencing characteristics to manage caches, but do not give any 114 scheme to do it.
Reference: [CH84] <author> F. Chow and J. Hennessy. </author> <title> Register allocation by priority-based coloring. </title> <booktitle> In Proc. of the 1984 Symp. on Compiler Construction, </booktitle> <pages> pages 222-232, </pages> <year> 1984. </year>
Reference-contexts: Some of them are branch profiling to schedule multiple basic blocks together to increase instruction level parallelism in very long instruction word (VLIW) and superscalar machines [Fis81, HMC + 93]. Basic block usage profiling to reduce instruction cache conflicts [McF89, HC89], variable usage profiling for register allocation <ref> [CH84] </ref>, and procedure usage profiling for inlining [CMCH92, McF88]. Here we propose using load/store instruction profiling for instruction scheduling and cache management. Simulation-driven profiling to determine the cache behavior of individual instructions is relatively expensive.
Reference: [CKP91] <author> D. Callahan, K. Kennedy, and A. Porterfield. </author> <title> Software prefetching. </title> <booktitle> In Proc. of ASPLOS IV, </booktitle> <pages> pages 40-52, </pages> <year> 1991. </year> <month> 138 </month>
Reference-contexts: Callahan and Porterfield [CP90] investigate the data cache behavior of scientific code using source-level instrumentation. For a suite of regular scientific applications, they note that a few source-level references contribute to most of the misses. In a later paper <ref> [CKP91] </ref>, they present techniques for prefetching to tolerate the latency of instructions that miss. They investigate both complete prefetching where all array accesses are prefetched, and selective prefetching where the compiler identifies prefetch data based on the size of the cache.
Reference: [CMCH92] <author> P. Chang, S. Mahlke, W. Chen, and W.W. Hwu. </author> <title> Profile-guided automatic inline expansion for C programs. </title> <journal> Software-Practice and Experience, </journal> <volume> 22(5) </volume> <pages> 349-369, </pages> <year> 1992. </year>
Reference-contexts: Basic block usage profiling to reduce instruction cache conflicts [McF89, HC89], variable usage profiling for register allocation [CH84], and procedure usage profiling for inlining <ref> [CMCH92, McF88] </ref>. Here we propose using load/store instruction profiling for instruction scheduling and cache management. Simulation-driven profiling to determine the cache behavior of individual instructions is relatively expensive. Profiling time could be one to two orders of magnitude greater than the run-time of the original program.
Reference: [CMH92] <author> W. Y. Chen, S. A. Mahlke, and W.W. Hwu. </author> <title> Tolerating first level memory access latency in high-performance systems. </title> <booktitle> In Intl. Conf. on Parallel Processing, </booktitle> <address> pages I-36 - I-43, </address> <year> 1992. </year>
Reference-contexts: Mowry et al. [MLG92] combine prefetching with software pipelining and also investigate the interaction between blocking and prefetching. Chen and Baer [CB92] investigate hardware-based prefetching, and moving loads within basic blocks statically to tolerate cache-miss latencies for some SPEC benchmarks. Chen et al. <ref> [CMH92] </ref> reduce the effect of primary cache latency by preloading all memory accesses for the SPEC benchmarks. There has thus been a fairly significant amount of work on prefetching for scientific code. Here we make two new contributions.
Reference: [CNO + 88] <author> R. P. Colwell, R. P. Nix, J. J. O'Donnell, D. B. Papworth, and P. K. </author> <note> Rodman. </note>
Reference-contexts: With the increasing gap between memory access times and CPU cycle times, the CPI penalty caused by these stalls on load misses is becoming significant [Mea91]. Machines such as the Multiflow Trace <ref> [CNO + 88] </ref> and the Cydra [BYA92, RYYT89], take a different approach; they have a flat memory hierarchy, without caches. However, this results in memory always being distant from the CPU, and often the long latencies result in many empty slots in the schedule.
References-found: 15

