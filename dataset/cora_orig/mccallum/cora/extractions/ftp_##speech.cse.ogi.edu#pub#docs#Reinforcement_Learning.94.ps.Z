URL: ftp://speech.cse.ogi.edu/pub/docs/Reinforcement_Learning.94.ps.Z
Refering-URL: http://www.cse.ogi.edu/CSLU/personnel/bios/johans.html
Root-URL: http://www.cse.ogi.edu
Title: NEURAL NETWORKS FOR CONTROL REINFORCEMENT LEARNING  
Author: by Johan Schalkwyk 
Degree: Dissertation submitted in partial fulfilment of the requirements for the degree Master of Engineering in the Faculty of Engineering, University of Pretoria  
Date: March 1994  
Abstract-found: 0
Intro-found: 1
Reference: [1] <author> R.E.Bellman. </author> <title> Dynamic Programming. </title> <publisher> Princeton University Press, </publisher> <address> New York, </address> <year> 1957. </year>
Reference-contexts: Recent advances in reinforcement learning by Watkins [2] have lead to a formal mathematical basis for these methods. Under this assumption Dynamic Programming <ref> [1] </ref> will be used as a sound mathematical platform for the development and study of optimal or near-optimal learning control laws. Dynamic Programming [1, 8, 9] has been studied extensively in control system design. <p> Recent advances in reinforcement learning by Watkins [2] have lead to a formal mathematical basis for these methods. Under this assumption Dynamic Programming [1] will be used as a sound mathematical platform for the development and study of optimal or near-optimal learning control laws. Dynamic Programming <ref> [1, 8, 9] </ref> has been studied extensively in control system design. The principle behind the Dynamic Programming approach can be found in almost any effective planning or control type problem. Dynamic Programming has made its mark in a large portion of the engineering field. <p> Step 1: For the current state x t , define the vector ~x t = <ref> [0; 0; :::; 1; :::] </ref> T , where the "1" corresponds to the position of the current state x. <p> Temporal-difference learning [7] employs the temporal relation between the reinforcement signal and the actions taken, in order to decide upon eventual optimal actions. Recent advances in reinforcement-learning techniques [2] have shown the relationship between Dynamic Programming <ref> [1] </ref> and the key aspects behind reinforcement learning. Dynamic Programming, used extensively in the field of optimal control, allows one to compute the guaranteed global optimal control policy. This is, however, based on the assumption that the plant is fully specified. <p> Dynamic Programming is the collection of mathematical tools used to analyze sequential decision processes. First formally proposed by Bellman <ref> [1] </ref>, it has led to a widespread interest in the field of optimal control. Since then, many authors have elaborated on related topics in this field [8, 9, 27]. <p> Thus for each state-action pair the agent receives a probabilistic reward based on the transition to the next state. Applying Policy Iteration with the known probability and reward matrices the following optimal policy ~ fl is obtained ~ fl = <ref> [3; 2; 1; 3; 1] </ref> ; (5:12) The associated value function is given by ~ V fl = [5:596; 4:513; 5:483; 4:267; 6:640] (5:13) Results and Discussion Results obtained for the three exploration coefficients (5; 20; 100) are depicted in figures 5.1-5.3 respectively. <p> Any wrong actions would therefore lead to much less than maximal long-term reward. Thus, a more global optimization is necessary here. Applying Policy Iteration using the known probability and reward matrices, the optimal policy ~ fl obtained is ~ fl = <ref> [3; 2; 3; 1; 1] </ref> (5:15) The associated value vector is given by ~ V fl = [5:697; 4:548; 4:349; 2:914; 6:970] (5:16) Results and Discussion Figures 5.4-5.6 depict the results obtained for the three levels of exploration EC = [5; 20; 100]. <p> previous transition probability row as ~ P xy j old (A:11) The row update to matrix P can be written as follows P n+1 = P n + P update ; (A:12) where P update = ~x (n)~y T (n) (A:13) represents the rank-one modification matrix, with ~x (n) = <ref> [0; 0; :::; 1; 0; :::] </ref> T denoting the current state vector at time step t = n.
Reference: [2] <author> C.J.C.H.Watkins and P.Dayan. </author> <title> Q-learning. </title> <journal> Machine Learning, </journal> <volume> 8 </volume> <pages> 279-292, </pages> <month> April </month> <year> 1992. </year>
Reference-contexts: Recent advances in reinforcement learning by Watkins <ref> [2] </ref> have lead to a formal mathematical basis for these methods. Under this assumption Dynamic Programming [1] will be used as a sound mathematical platform for the development and study of optimal or near-optimal learning control laws. Dynamic Programming [1, 8, 9] has been studied extensively in control system design. <p> Dynamic Programming has made its mark in a large portion of the engineering field. From game theory to cost inventory control, this method guarantees an optimal decision or control law. The work of Watkins <ref> [2] </ref> in applying Dynamic Programming to reinforcement learning has lead to a method referred to as Q-Learning. Q-Learning solves for the optimal control policy by incrementally updating its actions in order to maximize a certain performance measure the long term expected reward. <p> Temporal-difference learning [7] employs the temporal relation between the reinforcement signal and the actions taken, in order to decide upon eventual optimal actions. Recent advances in reinforcement-learning techniques <ref> [2] </ref> have shown the relationship between Dynamic Programming [1] and the key aspects behind reinforcement learning. Dynamic Programming, used extensively in the field of optimal control, allows one to compute the guaranteed global optimal control policy. This is, however, based on the assumption that the plant is fully specified. <p> Before explaining the inner workings of the Policy 4 A new method developed based on the Linear Programming approach 5 Similar to Incremental Linear Programming, but where the states are observed in an asynchronous fashion 6 An incremental method used for solving the Dynamic Programming equations, recently developed by Watkins <ref> [2] </ref> Reinforcement Learning Chapter 4: Reinforcement Learning: Discrete Space page 36 Iteration algorithm we need first to discuss how to improve on a given policy. This is commonly known as the policy improvement theorem and can be found in most texts on Dynamic Programming [8, 9]. <p> It will be shown (chapter 5) that both these methods outperform the next method discussed, Q-learning, as proposed by Watkins <ref> [2] </ref>. The linear programming (Section 4.3.3) approach used to solve the non-linear system of equations (V = J (V )) can be subdivided into N separate linear programs, one for each state. <p> Discrete Space page 45 Computational Cost Incremental Linear Programming iterates towards an optimal policy in much the same way as Gauss-Seidel iteration, and thus has an associated computational cost of O (N ) per time step. 4.3.6 Q-Learning Q-learning is a form of model-free reinforcement learning, first proposed by Watkins <ref> [2] </ref>. Like the previous computational methods it provides agents with the capability to learn how to act optimally within an initially unknown Markov decision environment. <p> Reinforcement learning was first introduced as a subset of Dynamic Programming by Watkins <ref> [2] </ref>. Given a Markov decision model (world model), either stochastic or deterministic, various computational techniques (or learning methods) were discussed. Using this model description, Dynamic Programming was introduced as a collection of mathematical tools which can be used to analyze these sequential decision tasks described by the Markov decision process. <p> Although similar to Gauss-Seidel Iteration, ALP uses the return probability for improved analytic accuracy. Return probabilities become especially important when the state space discretization is coarse. The relationship between Q-Learning (introduced by Watkins <ref> [2] </ref>) and Dynamic Programming was also shown. Q-Learning can be viewed simply as an iterative technique used for solving the Dynamic Programming equations in an incremental manner. <p> Thus for each state-action pair the agent receives a probabilistic reward based on the transition to the next state. Applying Policy Iteration with the known probability and reward matrices the following optimal policy ~ fl is obtained ~ fl = <ref> [3; 2; 1; 3; 1] </ref> ; (5:12) The associated value function is given by ~ V fl = [5:596; 4:513; 5:483; 4:267; 6:640] (5:13) Results and Discussion Results obtained for the three exploration coefficients (5; 20; 100) are depicted in figures 5.1-5.3 respectively. <p> Any wrong actions would therefore lead to much less than maximal long-term reward. Thus, a more global optimization is necessary here. Applying Policy Iteration using the known probability and reward matrices, the optimal policy ~ fl obtained is ~ fl = <ref> [3; 2; 3; 1; 1] </ref> (5:15) The associated value vector is given by ~ V fl = [5:697; 4:548; 4:349; 2:914; 6:970] (5:16) Results and Discussion Figures 5.4-5.6 depict the results obtained for the three levels of exploration EC = [5; 20; 100]. <p> A more dynamic approach towards learning controllers is required. Temporal-difference learning [7, 11] is a step towards solving this type of control problem. Temporal-difference methods use the temporal relation between cause and effect in order to effectively predict the eventual outcome of the system. Recent work by Watkins <ref> [2] </ref> on reinforcement learning has bridged the gap between temporal differences and Dynamic Programming, a methodology studied extensively in optimal control system design. Based on this discovery, Dynamic Programming was used to form a sound mathematical basis for the development of learning techniques with optimal or near-optimal behaviour (chapter 4). <p> Based on this discovery, Dynamic Programming was used to form a sound mathematical basis for the development of learning techniques with optimal or near-optimal behaviour (chapter 4). A new incremental learning technique, Asynchronous Linear Programming was introduced. In contrast to Q-learning <ref> [2] </ref>, Asynchronous Linear Programming uses a model-based approach in order to achieve superior convergence characteristics. All these techniques presented are, however, based on a discrete state-action space assumption. It was shown (chapter 6) that this in itself causes many difficulties as soon as the complexity of the control problem increases.
Reference: [3] <author> R.S.Sutton. </author> <title> Integrated architectures for learning, planning, and reacting based on approximating dynamic programming. </title> <booktitle> In The Seventh International Conference on Machine Learning, </booktitle> <pages> pages 216-224, </pages> <month> June </month> <year> 1990. </year>
Reference-contexts: Various techniques for solving this type of control problem have appeared in the last thirty odd years. Some of these are discussed in great detail in the book edited by Sutton and Werbos <ref> [3] </ref>. <p> Thus for each state-action pair the agent receives a probabilistic reward based on the transition to the next state. Applying Policy Iteration with the known probability and reward matrices the following optimal policy ~ fl is obtained ~ fl = <ref> [3; 2; 1; 3; 1] </ref> ; (5:12) The associated value function is given by ~ V fl = [5:596; 4:513; 5:483; 4:267; 6:640] (5:13) Results and Discussion Results obtained for the three exploration coefficients (5; 20; 100) are depicted in figures 5.1-5.3 respectively. <p> Any wrong actions would therefore lead to much less than maximal long-term reward. Thus, a more global optimization is necessary here. Applying Policy Iteration using the known probability and reward matrices, the optimal policy ~ fl obtained is ~ fl = <ref> [3; 2; 3; 1; 1] </ref> (5:15) The associated value vector is given by ~ V fl = [5:697; 4:548; 4:349; 2:914; 6:970] (5:16) Results and Discussion Figures 5.4-5.6 depict the results obtained for the three levels of exploration EC = [5; 20; 100]. <p> Moore and Atkeson [33] propose the philosophy of optimism in the face of uncertainty, closely related to the interval estimation technique of Kaelbling [35], and the exploration bonus heuristic found in Dyna Architectures <ref> [3] </ref>. This method is based on the principle that in the absence of contrary evidence, any action is assumed to lead directly to the goal state with a large immediate reward r opt . <p> As in Dyna architectures <ref> [3] </ref>, a fixed amount of computation is allowed between successive real world experiences. Prioritized sweeping uses the change in the value function between successive states to determine how "interesting" a particular state is, or put more formally, the priority of the intermediate computational process.
Reference: [4] <author> A.L.Samuel. </author> <title> Some studies in the game of chec&lt;kers. </title> <journal> IBM Journal on Research and Development, </journal> <volume> 3(2) </volume> <pages> 210-229, </pages> <year> 1959. </year>
Reference-contexts: Reinforcement-learning approaches can be found as far back as 1959, in Samuel's checkers-playing program <ref> [4] </ref>. Since then, many formal theories concerning the reinforcement-learning paradigm have appeared and been applied to fairly complex control problems [5, 6]. The paper by Sutton [7] has lead to a renewed interest in these control methodologies. <p> An important advantage of these methods is their on-line nature, thus they do not require any form of supervisory input in order to learn or predict. Temporal-difference methods have been studied over the past 30 years. Reference to related work can be found in Samuel's checkers-playing program <ref> [4] </ref>. In the last decade a widespread interest in these methods has arisen. This is mainly due to work by Barto, Sutton and Anderson [5, 16] in the early 1980's.
Reference: [5] <author> A.G. Barto, R.S. Sutton, and C.W. Anderson. </author> <title> Neuronlike adaptive elements that can solve difficult learning control problems. </title> <journal> IEEE Trans. on Systems, Man, and Cybernetics, </journal> <volume> SMC-13(5):834-846, </volume> <month> September/October </month> <year> 1983. </year>
Reference-contexts: Reinforcement-learning approaches can be found as far back as 1959, in Samuel's checkers-playing program [4]. Since then, many formal theories concerning the reinforcement-learning paradigm have appeared and been applied to fairly complex control problems <ref> [5, 6] </ref>. The paper by Sutton [7] has lead to a renewed interest in these control methodologies. Reinforcement learning offers an alternative approach to control, in that it does not depend on explicit domain knowledge. <p> These difficulties emphasize the need for design methodologies which take explicit note of the future consequences of current actions, such as the method of temporal differences <ref> [5, 7] </ref>. Reinforcement Learning Chapter 3 Temporal-Difference methods and Markov Models 3.1 Introduction The previous chapter concentrated on the application of gradient optimization techniques, when training a neural-network controller. It was shown that this method does, however, present numerous difficulties during the training (or learning) process. <p> Temporal-difference methods have been studied over the past 30 years. Reference to related work can be found in Samuel's checkers-playing program [4]. In the last decade a widespread interest in these methods has arisen. This is mainly due to work by Barto, Sutton and Anderson <ref> [5, 16] </ref> in the early 1980's. It is especially the work on neuron-like adaptive elements which has sparked off the search towards new learning procedures based on the principle of learning from predictions. Later, a strong theoretical basis for temporal-difference learning methods was provided by Sutton [7]. <p> the optimal policy ~ fl obtained is ~ fl = [3; 2; 3; 1; 1] (5:15) The associated value vector is given by ~ V fl = [5:697; 4:548; 4:349; 2:914; 6:970] (5:16) Results and Discussion Figures 5.4-5.6 depict the results obtained for the three levels of exploration EC = <ref> [5; 20; 100] </ref>. <p> Reinforcement Learning Chapter 6: A Question of Knowledge page 90 (a) Performance after 10 000 000 training samples (b) Performance after 10 000 000 training samples and 100 training sequences model (continued). 6.6.1 The Adaptive Heuristics Critic One of the more successful architectures proposed by Barto, Sutton and Anderson <ref> [5] </ref> is the Adaptive Heuristic Critic. The Adaptive Heuristic Critic (Figure 6.10) uses a set of two neural networks, acting in parallel. <p> The first, referred to as the Adaptive Critic Element, uses the temporal-difference error in the value-function estimate in order to form a function or mapping of the state variables, approximating the associated value function <ref> [5, 7, 37, 38, 39] </ref>. The temporal-difference error is a direct indication of the accuracy of the value function estimate. <p> The more well known approaches namely that of Widrow [13] (the Truck Backer-Upper), and Barto, Sutton and Anderson <ref> [5] </ref> (the Adaptive Heuristic Critic) were presented. The work of Widrow and Nguyen concentrated on using a neural-network controller with state feedback in order to design an effective controller for the truck backer-upper control problem.
Reference: [6] <author> J.D.R.Milan and C.Torras. </author> <title> A reinforcement connectionist approach to robot path finding in non-maze like environments. </title> <journal> Machine Learning, </journal> <volume> 8 </volume> <pages> 363-395, </pages> <year> 1992. </year>
Reference-contexts: Reinforcement-learning approaches can be found as far back as 1959, in Samuel's checkers-playing program [4]. Since then, many formal theories concerning the reinforcement-learning paradigm have appeared and been applied to fairly complex control problems <ref> [5, 6] </ref>. The paper by Sutton [7] has lead to a renewed interest in these control methodologies. Reinforcement learning offers an alternative approach to control, in that it does not depend on explicit domain knowledge. <p> This chapter uses the Dynamic Programming formulation as a fundamental mathematical basis for the reinforcement-learning methods presented. Subsequently, chapter 5, presents extensive convergence studies on a series of increasingly difficult learning control problems. Chapter 6 uses the learning techniques developed on a complex robot path-planning problem <ref> [6] </ref>. It is shown that the success or failure of any self-learning control methodology depends largely on its ability to use available information and furthermore its ability to gain information through efficient exploration of the control environment. Finally chapter 7 discusses future work and draws conclusions. <p> A clear disadvantage of any of the techniques discussed is the discrete nature (discretized state space) in which these methods solve for the optimal decision policy. Most real learning problems such as robot path planning <ref> [6] </ref> operate in large environments and using discretized state space methods would most likely fail, due to the large amount of processing required for each real world observation. A further disadvantage of a state space that becomes too large is inherent to the model-based approach of the analytic techniques. <p> This increase in confidence is normally reflected in the value function network (Adaptive Critic Element), when the temporal-difference error decreases. The Adaptive Search Element is trained by back-propagation of the temporal-difference error <ref> [6, 39] </ref>. This corresponds to a steepest ascent in the value space [19]. The advantage of such a parallel architecture lies both in the generalization capabilities with respect to the approximation of the value function, and the formation of an effective state-action space mapping.
Reference: [7] <author> R.S.Sutton. </author> <title> Learning to predict by the methods of temporal differences. </title> <journal> Machine Learning, </journal> <volume> 3(9) </volume> <pages> 9-44, </pages> <year> 1988. </year>
Reference-contexts: Reinforcement-learning approaches can be found as far back as 1959, in Samuel's checkers-playing program [4]. Since then, many formal theories concerning the reinforcement-learning paradigm have appeared and been applied to fairly complex control problems [5, 6]. The paper by Sutton <ref> [7] </ref> has lead to a renewed interest in these control methodologies. Reinforcement learning offers an alternative approach to control, in that it does not depend on explicit domain knowledge. <p> This learning methodology severely restricts the performance of the learning system. In order to effectively design a controller, given these harsh circumstances, one has to employ the temporal relation between cause and effect, in order to predict the eventual outcome of the system. Temporal-difference methods <ref> [7, 11] </ref> have proven to be useful in such circumstances. Chapter 3 introduces temporal-difference methods as efficient estimators of an underlying Markov process. Based on this approach the convergence properties with respect to the true analytic solution are investigated. <p> These difficulties emphasize the need for design methodologies which take explicit note of the future consequences of current actions, such as the method of temporal differences <ref> [5, 7] </ref>. Reinforcement Learning Chapter 3 Temporal-Difference methods and Markov Models 3.1 Introduction The previous chapter concentrated on the application of gradient optimization techniques, when training a neural-network controller. It was shown that this method does, however, present numerous difficulties during the training (or learning) process. <p> It is especially the work on neuron-like adaptive elements which has sparked off the search towards new learning procedures based on the principle of learning from predictions. Later, a strong theoretical basis for temporal-difference learning methods was provided by Sutton <ref> [7] </ref>. The primary aim of this chapter is to present the relationship between temporal differences and 14 Chapter 3: Temporal-Difference methods and Markov models page 15 Markov-model theory, as first proposed by Barnard [11]. <p> Denote this learning procedure as T D (1)-learning (for reasons explained in Sutton <ref> [7] </ref>). 3.3.2 Analytic Solution By assuming that states are generated by a first-order Markov model, it is possible to derive a different estimator for these probabilities (Barnard [11]). <p> Equation 3.29 corresponds exactly with the T D (0) learning method proposed by Sutton <ref> [7] </ref> ~w t = ffr w P t (P t+1 P t ); (3:30) where P t+1 corresponds to the prediction for reaching the terminal state Z given the state at time step t + 1. <p> The quest is therefore to search for new and improved techniques for the design of optimal or near-optimal controllers, within an initially unknown environment. One such method is known as temporal differences <ref> [7, 11] </ref> and is also closely related to reinforcement-learning techniques [19]. The describing feature of reinforcement-learning tasks is that the performance of the controller is judged solely on a single scalar reward or penalty function. <p> The describing feature of reinforcement-learning tasks is that the performance of the controller is judged solely on a single scalar reward or penalty function. This reinforcement signal which is supplied by the environment, is only an indication of the relative performance (success or failure) of the controller. Temporal-difference learning <ref> [7] </ref> employs the temporal relation between the reinforcement signal and the actions taken, in order to decide upon eventual optimal actions. Recent advances in reinforcement-learning techniques [2] have shown the relationship between Dynamic Programming [1] and the key aspects behind reinforcement learning. <p> The first, referred to as the Adaptive Critic Element, uses the temporal-difference error in the value-function estimate in order to form a function or mapping of the state variables, approximating the associated value function <ref> [5, 7, 37, 38, 39] </ref>. The temporal-difference error is a direct indication of the accuracy of the value function estimate. <p> It was shown in chapter 2 and in Schalkwyk [15] that this method suffers from an extensive number of local minima. A more dynamic approach towards learning controllers is required. Temporal-difference learning <ref> [7, 11] </ref> is a step towards solving this type of control problem. Temporal-difference methods use the temporal relation between cause and effect in order to effectively predict the eventual outcome of the system.
Reference: [8] <author> S.Ross. </author> <title> Introduction to Stochastic Dynamic Programming. </title> <publisher> Academic Press, </publisher> <year> 1985. </year>
Reference-contexts: Recent advances in reinforcement learning by Watkins [2] have lead to a formal mathematical basis for these methods. Under this assumption Dynamic Programming [1] will be used as a sound mathematical platform for the development and study of optimal or near-optimal learning control laws. Dynamic Programming <ref> [1, 8, 9] </ref> has been studied extensively in control system design. The principle behind the Dynamic Programming approach can be found in almost any effective planning or control type problem. Dynamic Programming has made its mark in a large portion of the engineering field. <p> Dynamic Programming is the collection of mathematical tools used to analyze sequential decision processes. First formally proposed by Bellman [1], it has led to a widespread interest in the field of optimal control. Since then, many authors have elaborated on related topics in this field <ref> [8, 9, 27] </ref>. <p> V fl (x) = V f fl f2 V f (x) 8 x 2 S (4:18) Furthermore it can be shown that V fl is the unique bounded solution of the optimality equation <ref> [8] </ref>: V fl (x) = max 2 X P xy (a)V fl (y) 5 8 x 2 S (4:19) This equation is one representation of the Dynamic Programming objective. Several techniques may be employed to solve for the optimal policy. <p> This is commonly known as the policy improvement theorem and can be found in most texts on Dynamic Programming <ref> [8, 9] </ref>. <p> Program 1 (Linear Programming) . Minimize X V (x) (4:44) Subject to the constraints V (x) (x; a) + fl y2S This computational method has been studied by many authors and convergence proofs are readily available <ref> [8, 9] </ref>. Since linear programming solves for the optimal policy given the transition probability and reward matrices (P xy (a) and (x; a)), it is closely related to Policy Iteration. The linear program given by program 1 may be solved using any of the available computational techniques.
Reference: [9] <author> D.P.Bertsekas. </author> <title> Dynamic Programming: Deterministic and Stochastic Models. </title> <address> Prentice-Hall,Inc., </address> <year> 1987. </year>
Reference-contexts: Recent advances in reinforcement learning by Watkins [2] have lead to a formal mathematical basis for these methods. Under this assumption Dynamic Programming [1] will be used as a sound mathematical platform for the development and study of optimal or near-optimal learning control laws. Dynamic Programming <ref> [1, 8, 9] </ref> has been studied extensively in control system design. The principle behind the Dynamic Programming approach can be found in almost any effective planning or control type problem. Dynamic Programming has made its mark in a large portion of the engineering field. <p> Dynamic Programming is the collection of mathematical tools used to analyze sequential decision processes. First formally proposed by Bellman [1], it has led to a widespread interest in the field of optimal control. Since then, many authors have elaborated on related topics in this field <ref> [8, 9, 27] </ref>. <p> This is commonly known as the policy improvement theorem and can be found in most texts on Dynamic Programming <ref> [8, 9] </ref>. <p> page 39 Using the true optimal value function V fl (), defined by the optimality equation (equation 4.19) V fl (x) = max [Q fl (x; a)] (4.42) and from equations 4.37, 4.38, it is subsequently clear that V fl () constitutes the minimization problem as described by program 1 <ref> [9] </ref>. Program 1 (Linear Programming) . Minimize X V (x) (4:44) Subject to the constraints V (x) (x; a) + fl y2S This computational method has been studied by many authors and convergence proofs are readily available [8, 9]. <p> Program 1 (Linear Programming) . Minimize X V (x) (4:44) Subject to the constraints V (x) (x; a) + fl y2S This computational method has been studied by many authors and convergence proofs are readily available <ref> [8, 9] </ref>. Since linear programming solves for the optimal policy given the transition probability and reward matrices (P xy (a) and (x; a)), it is closely related to Policy Iteration. The linear program given by program 1 may be solved using any of the available computational techniques. <p> This computational process is depicted in figure 4.2. Each iteration is represented by the value bar, with each state depicted as a subsection of the value bar. Convergence results of Gauss-Seidel iteration can be found in <ref> [9] </ref>. This philosophy of using interim results during the computation process can also be implemented in an asynchronous fashion, i.e. where the states are not visited sequentially as in pure Gauss-Seidel iteration. Implementing Gauss-Seidel iteration in an asynchronous fashion corresponds closely to what we expect from a learning controller.
Reference: [10] <author> J.Schalkwyk L.F.A.Wessels and E.Barnard. </author> <title> Convergence properties of learning methods in unknown markov decision environments. </title> <note> Submitted for publication (Machine Learning), </note> <month> November </month> <year> 1992. </year>
Reference-contexts: Of these a new method called Asynchronous Linear Programming is introduced as a variation on the linear programming approach used for solving the Dynamic Programming formulation <ref> [10] </ref>. As opposed to the iterative nature by which Q-Learning solves for the optimal control policy, Asynchronous Linear Programming solves for the optimal control policy with improved analytic accuracy. Extensive convergence tests are presented within a stationary finite-state markovian domain. <p> Since its first appearance, many authors have studied Q-learning and variations thereof [29, 30, 31]. However, the direct relationship to Dynamic Programming was initially not clearly reflected in literature. Wessels [32] shows this relationship more formally, and it can also be found in <ref> [10] </ref>. It is repeated here for the sake of clarity and is taken almost verbatim from Wessels [32] 8 .
Reference: [11] <author> E.Barnard. </author> <title> Temporal-difference methods and markov models. </title> <journal> IEEE Trans. Syst. Man, Cybern., </journal> <note> Submitted for Publication, 1991. 117 </note>
Reference-contexts: This learning methodology severely restricts the performance of the learning system. In order to effectively design a controller, given these harsh circumstances, one has to employ the temporal relation between cause and effect, in order to predict the eventual outcome of the system. Temporal-difference methods <ref> [7, 11] </ref> have proven to be useful in such circumstances. Chapter 3 introduces temporal-difference methods as efficient estimators of an underlying Markov process. Based on this approach the convergence properties with respect to the true analytic solution are investigated. <p> Later, a strong theoretical basis for temporal-difference learning methods was provided by Sutton [7]. The primary aim of this chapter is to present the relationship between temporal differences and 14 Chapter 3: Temporal-Difference methods and Markov models page 15 Markov-model theory, as first proposed by Barnard <ref> [11] </ref>. Deriving the temporal-difference equation, based on the Markov-model assumption, allows one to compute the predictions using an analytic approach rather than the iterative technique of pure temporal-difference methods. These issues are discussed in Sections 3.2 and 3.3 respectively. <p> Matters are simplified somewhat by assuming only two terminal states (Z = ), where z 0 denotes failure, and z 1 success. Denote z 1 , the state corresponding to successful termination as, Z. Generalizations for more than two terminal states are discussed in Barnard <ref> [11] </ref>. Given the parameters describing the Markov model one would like to determine the probability of arriving in terminal state Z 2 Z. <p> Denote this learning procedure as T D (1)-learning (for reasons explained in Sutton [7]). 3.3.2 Analytic Solution By assuming that states are generated by a first-order Markov model, it is possible to derive a different estimator for these probabilities (Barnard <ref> [11] </ref>). <p> the termination probabilities, using the analytic solution given in equa tion 3.20 ~w = (N M) 1 ~m (3:26) Reinforcement Learning Chapter 3: Temporal-Difference methods and Markov models page 20 This is a popular solution to equations of the form A ~w + ~ b = ~ 0 (3:28) Barnard <ref> [11] </ref> shows how this technique may be employed to derive the customary form of the temporal difference (TD) equation ~w ~w + ff (~x t ~x T T ) ~w + ~x t ffi t+1;z 1 (3:29) with ~x t and ~x t+1 N -dimensional unit vectors with a "1" in <p> t+1 = ~x T Similarly P t = ~x T From the above equation the gradient with respect to the probability vector ~w, yields r w P t = ~x t (3:33) In the special case where state x t+1 = Z, we have according to the notation of Barnard <ref> [11] </ref> x t+1 = 0 and ffi t+1;z 1 = 1. Combining equations 3.30 through 3.33 results in exactly the same update equation described in Barnard [11] (equation 3.29) ~w t = ffr w P t (P t+1 P t ) (3.34) fi t+1 ~w ~x T fl = ff (~x <p> P t = ~x t (3:33) In the special case where state x t+1 = Z, we have according to the notation of Barnard <ref> [11] </ref> x t+1 = 0 and ffi t+1;z 1 = 1. Combining equations 3.30 through 3.33 results in exactly the same update equation described in Barnard [11] (equation 3.29) ~w t = ffr w P t (P t+1 P t ) (3.34) fi t+1 ~w ~x T fl = ff (~x t ~x T t ) ~w + ~x t ffi t+1;z 1 (3.36) Algorithm 2 describes the implementation of the iterative T D (0) estimation method <p> The quest is therefore to search for new and improved techniques for the design of optimal or near-optimal controllers, within an initially unknown environment. One such method is known as temporal differences <ref> [7, 11] </ref> and is also closely related to reinforcement-learning techniques [19]. The describing feature of reinforcement-learning tasks is that the performance of the controller is judged solely on a single scalar reward or penalty function. <p> It was shown in chapter 2 and in Schalkwyk [15] that this method suffers from an extensive number of local minima. A more dynamic approach towards learning controllers is required. Temporal-difference learning <ref> [7, 11] </ref> is a step towards solving this type of control problem. Temporal-difference methods use the temporal relation between cause and effect in order to effectively predict the eventual outcome of the system.
Reference: [12] <author> D.E. Rumelhart, G.E. Hinton, and R.J. Williams. </author> <title> Learning internal representation by error propagation. </title> <booktitle> Distributed parallel processing, </booktitle> <volume> 1 </volume> <pages> 318-362, </pages> <year> 1986. </year>
Reference-contexts: One approach to compute the gradient under these circumstances is to "unfold" the network in time <ref> [12, 13] </ref> so that the recurrent network can be viewed as a feed-forward network with numerous layers (figure 2.2). This method does, however, present a number of disadvantages.
Reference: [13] <author> B. Widrow and D. Nguyen. </author> <title> The truck backer-upper: an example of self-teaching in neural networks. </title> <booktitle> In Proc. Int. Joint Conf. on Neural Networks, </booktitle> <volume> volume 2, </volume> <pages> pages 357-363, </pages> <address> Washington, </address> <year> 1989. </year> <pages> IJCNN. </pages>
Reference-contexts: One approach to compute the gradient under these circumstances is to "unfold" the network in time <ref> [12, 13] </ref> so that the recurrent network can be viewed as a feed-forward network with numerous layers (figure 2.2). This method does, however, present a number of disadvantages. <p> This implies that the time-unfolding method, which succeeds for certain first-order <ref> [13] </ref> and second-order [15] systems, will fail for several real world control problems. These difficulties emphasize the need for design methodologies which take explicit note of the future consequences of current actions, such as the method of temporal differences [5, 7]. <p> Many such systems currently rely on the fact that the control algorithm will, through interaction with the environment, be able to form a mapping of the plant, and from this mapping derive a control law satisfying the initially prescribed objective. One such approach was proposed by Widrow and Nguyen <ref> [13] </ref>. As discussed in chapter 2, this method 27 Chapter 4: Reinforcement Learning: Discrete Space page 28 trains a neural controller in an open-loop configuration, using the measured state variables as feedback into the controller. <p> The more well known approaches namely that of Widrow <ref> [13] </ref> (the Truck Backer-Upper), and Barto, Sutton and Anderson [5] (the Adaptive Heuristic Critic) were presented. The work of Widrow and Nguyen concentrated on using a neural-network controller with state feedback in order to design an effective controller for the truck backer-upper control problem.
Reference: [14] <author> R.S. Sutton, W. Thomas, and P.J. </author> <title> Werbos. </title> <booktitle> Neural Networks for Control, </booktitle> <pages> pages 495-498. </pages> <publisher> The MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1989. </year>
Reference-contexts: This is a way of modeling the effects of a ship's inertia and the resistance of the water. 2.3.2 The Ship Steering Dynamics The Dynamics of the ship steering system are modeled by the following non-linear differential equations: <ref> [14] </ref> dx = V sin (t) (2:7) dt d 2 r (t) _ (t) (2:9) where the following are used as implied constants: * T = 5 s; time constant of convergence to desired turning rate. <p> These include choice of network size, training parameters and local minima. Given these difficulties associated with neural architectures, several authors [37, 38] have proposed the use of a coded architecture known as the CMAC, in place of the neural network. 6.6.2 CMAC Cerebellar Model Arithmetic Computer The CMAC <ref> [14] </ref> (Figure 6.12) is a coarse-coding structure, where each region in state space is represented by a set of overlapping tiles, offset with respect to one another. Every quantization region, or tile, is defined by a set of quantization functions, operating on the input or state variables describing the system. <p> Apart from these disadvantages CMACs also suffer from the difficulties associated with the choice of training parameters. 6.6.3 The Back-propagated Adaptive Critic Adaptive Critic <ref> [14] </ref> operates on the principle of maximizing the long term expected reward through steepest ascent of the value function network. <p> These types of control problems might be effectively solved from a self-learning perspective. 1 Currently one of the planets receiving much attention from the scientific community. 96 Chapter 7: Conclusions page 97 Several approaches to these classes of control problems can be found in the literature <ref> [14] </ref>. The more well known approaches namely that of Widrow [13] (the Truck Backer-Upper), and Barto, Sutton and Anderson [5] (the Adaptive Heuristic Critic) were presented.
Reference: [15] <author> J. Schalkwyk and E. Barnard. </author> <title> Neural networks and the "broom balancer". </title> <booktitle> In Second Workshop on Pattern Recognition, </booktitle> <address> Stellenbosch, SA, </address> <month> November </month> <year> 1991. </year>
Reference-contexts: Research on the inverted-pendulum problem <ref> [15] </ref> has shown that the neural controller based on the approach described here repeatedly converges to a minimum leading to poor performance; however, if an identical network trained another way is used as a starting condition for this process, a much smaller value for the objective function is found and acceptable <p> This implies that the time-unfolding method, which succeeds for certain first-order [13] and second-order <ref> [15] </ref> systems, will fail for several real world control problems. These difficulties emphasize the need for design methodologies which take explicit note of the future consequences of current actions, such as the method of temporal differences [5, 7]. <p> This method, conceptually the most accurate, does, however, hold many complexities, mainly due to the learning method and architecture. It was shown in chapter 2 and in Schalkwyk <ref> [15] </ref> that this method suffers from an extensive number of local minima. A more dynamic approach towards learning controllers is required. Temporal-difference learning [7, 11] is a step towards solving this type of control problem.
Reference: [16] <author> R.S.Sutton and A.G.Barto. </author> <title> Toward a modern theory of adaptive networks: Expectation and prediction. </title> <journal> Psychological Review, </journal> <volume> 88(2) </volume> <pages> 135-170, </pages> <year> 1981. </year>
Reference-contexts: Temporal-difference methods have been studied over the past 30 years. Reference to related work can be found in Samuel's checkers-playing program [4]. In the last decade a widespread interest in these methods has arisen. This is mainly due to work by Barto, Sutton and Anderson <ref> [5, 16] </ref> in the early 1980's. It is especially the work on neuron-like adaptive elements which has sparked off the search towards new learning procedures based on the principle of learning from predictions. Later, a strong theoretical basis for temporal-difference learning methods was provided by Sutton [7].
Reference: [17] <author> U.N.Bhat. </author> <title> Elements of Applied Stochastic Processes. </title> <publisher> John Wiley and Sons, </publisher> <address> New York, </address> <note> second edition, </note> <year> 1984. </year>
Reference-contexts: These results are used to investigate the validity of the primary assumption which occurs throughout this thesis; namely that an analytic approach to the design of a learning system will in most cases concerning stationary finite-state systems be better than an iterative equivalent. 3.2 Markov Models Markov-model theory <ref> [17] </ref> provides an effective and efficient way to model processes that involve the gradual revelation of information through time. Generally, non-absorbing Markov models are modeled by the tuple fS; T g. S = f1; 2; :::; N g denotes the set of possible states the system can occupy.
Reference: [18] <author> R.O.Duda and P.E.Hart. </author> <title> Pattern Classification and Scene Analysis. </title> <publisher> John Wiley & Sons, </publisher> <year> 1985. </year>
Reference-contexts: = ~ h + P xy ~w; (3:11) which can be solved using the following matrix-vector equation ~w = (I P xy ) 1 ~ h (3:12) The probabilities P xy and ~ h are, however, not known beforehand and may be estimated; this is typically achieved with maximum-likelihood estimators <ref> [18] </ref>. <p> Experiences gained at each epoch "t" can, for instance, be employed to estimate the transition probabilities P a xy and the expected state-action reward (x; a). Typically maximum-likelihood estimators are used <ref> [18] </ref>. Let E t denote the event fx t ; y t+1 ; r t ; a t g. <p> Assuming each state is visited infinitely often and each action is taken infinitely often, then the estimates ^ P a xy and ^(x; a) will converge to the true probabilities P a xy and (x; a) respectively. This is commonly known as the law of large numbers <ref> [18] </ref>. Algorithm 4 summarizes the estimation of the process model. 4.3 Dynamic Programming Markov decision models have been shown to be useful when formalizing sequential decision tasks [20, 21, 22, 23, 24, 25, 26].
Reference: [19] <author> R.J.Williams. </author> <title> Towards a theory of reinforcement learning connectionist systems. </title> <type> Technical Report NU CCS 88 3, </type> <institution> North Eastern University, </institution> <address> Boston, </address> <year> 1988. </year>
Reference-contexts: The quest is therefore to search for new and improved techniques for the design of optimal or near-optimal controllers, within an initially unknown environment. One such method is known as temporal differences [7, 11] and is also closely related to reinforcement-learning techniques <ref> [19] </ref>. The describing feature of reinforcement-learning tasks is that the performance of the controller is judged solely on a single scalar reward or penalty function. This reinforcement signal which is supplied by the environment, is only an indication of the relative performance (success or failure) of the controller. <p> The second element, referred to as the Adaptive Search Element, forms a mapping from state to action space, such that the expected long term reward is a maximum. The Adaptive Search Element (Figure 6.11) consists of an action unit and a stochastic action selector <ref> [19] </ref>. The stochastic action selector operates by selecting actions from a Gaussian distribution, using the proposed action as the mean to the distribution, with the variance proportional to the temporal-difference error [6, Reinforcement Learning Chapter 6: A Question of Knowledge page 92 37]. <p> This increase in confidence is normally reflected in the value function network (Adaptive Critic Element), when the temporal-difference error decreases. The Adaptive Search Element is trained by back-propagation of the temporal-difference error [6, 39]. This corresponds to a steepest ascent in the value space <ref> [19] </ref>. The advantage of such a parallel architecture lies both in the generalization capabilities with respect to the approximation of the value function, and the formation of an effective state-action space mapping. The disadvantages, however, are directly coupled with the normal difficulties associated with neural networks.
Reference: [20] <author> P.Mandl. </author> <title> Estimation and control in markov chains. </title> <journal> Adv. Appl. Prob., </journal> <volume> 6 </volume> <pages> 40-60, </pages> <year> 1974. </year>
Reference-contexts: This is commonly known as the law of large numbers [18]. Algorithm 4 summarizes the estimation of the process model. 4.3 Dynamic Programming Markov decision models have been shown to be useful when formalizing sequential decision tasks <ref> [20, 21, 22, 23, 24, 25, 26] </ref>. <p> the optimal policy ~ fl obtained is ~ fl = [3; 2; 3; 1; 1] (5:15) The associated value vector is given by ~ V fl = [5:697; 4:548; 4:349; 2:914; 6:970] (5:16) Results and Discussion Figures 5.4-5.6 depict the results obtained for the three levels of exploration EC = <ref> [5; 20; 100] </ref>.
Reference: [21] <author> P.R.Kumar and W.Lin. </author> <title> Optimal adaptive controllers for unknown markov chains. </title> <journal> IEEE Trans. on Automatic Control, </journal> <volume> AC-27(4):765-774, </volume> <month> August </month> <year> 1982. </year>
Reference-contexts: This is commonly known as the law of large numbers [18]. Algorithm 4 summarizes the estimation of the process model. 4.3 Dynamic Programming Markov decision models have been shown to be useful when formalizing sequential decision tasks <ref> [20, 21, 22, 23, 24, 25, 26] </ref>. <p> The theory of Dynamic Programming offers much less computationally expensive methods of solving for the optimal policy of a Markov decision process. Of these methods, Policy Iteration has widely been studied in learning controllers <ref> [21, 24, 25] </ref>.
Reference: [22] <author> K.Abe M.Sato and H.Takeda. </author> <title> Learning control of finite markov chains with unknown transition probabilities. </title> <journal> IEEE Trans. on Automatic Control, </journal> <volume> AC-27(2):502-505, </volume> <month> April </month> <year> 1982. </year>
Reference-contexts: This is commonly known as the law of large numbers [18]. Algorithm 4 summarizes the estimation of the process model. 4.3 Dynamic Programming Markov decision models have been shown to be useful when formalizing sequential decision tasks <ref> [20, 21, 22, 23, 24, 25, 26] </ref>.
Reference: [23] <author> K.Abe M.Sato and H.Takeda. </author> <title> An asymptotically optimal learning controller for finite markov chains with unknown transition probabilities. </title> <journal> IEEE Trans. on Automatic Control, </journal> <volume> AC-30(11):1147-1149, </volume> <month> November </month> <year> 1985. </year>
Reference-contexts: This is commonly known as the law of large numbers [18]. Algorithm 4 summarizes the estimation of the process model. 4.3 Dynamic Programming Markov decision models have been shown to be useful when formalizing sequential decision tasks <ref> [20, 21, 22, 23, 24, 25, 26] </ref>.
Reference: [24] <author> K.Abe M.Sato and H.Takeda. </author> <title> Learning control of finite markov chains with an explicit trade-off between estimation and control. </title> <journal> IEEE Trans. Sys. Man and Cyber., </journal> <volume> 18(5) </volume> <pages> 677-684, </pages> <month> Septem-ber/October </month> <year> 1988. </year>
Reference-contexts: This is commonly known as the law of large numbers [18]. Algorithm 4 summarizes the estimation of the process model. 4.3 Dynamic Programming Markov decision models have been shown to be useful when formalizing sequential decision tasks <ref> [20, 21, 22, 23, 24, 25, 26] </ref>. <p> The theory of Dynamic Programming offers much less computationally expensive methods of solving for the optimal policy of a Markov decision process. Of these methods, Policy Iteration has widely been studied in learning controllers <ref> [21, 24, 25] </ref>. <p> Sato and Takeda <ref> [24] </ref> uses an explicit mechanism to force the learning agent to take non-optimal actions in order to gain information. This exploration mechanism also used by Barto and Singh [30], is implemented as follows. <p> This is typical of agents with a good estimate for the performance in the current state, but which are poor action selectors [34]. Weighted frequency coefficient Sato and Takeda <ref> [24] </ref> proposed a simple and straightforward way for comparing the relative perfor mance of a learning method. <p> Given true convergence, when all states have been visited infinitely often and all actions taken infinitely often, and exploration trade-off is turned off ( = 0), then it can be shown that <ref> [24] </ref>: lim f fl c (t) = 1; almost surely (5:9) True convergence of the statistics describing the model is closely related to the exploration efficiency of the learning method. <p> The accumulated reward at time step "t" is given by AR (t) = n=0 5.3.2 Example # 1 The first problem studied is described by Sato and Takeda <ref> [24] </ref>, and was also used by Barto and Singh [30]. The transition probability and reward matrices describing the model are given in tables 5.1 and 5.2 respectively. Using these values the expected reward matrix was calculated and is given in table 5.3. <p> Table 5.7 depicts the number of trials until an "optimal"-path was found. From these results it is clearly seen that the exploration heuristic of Sato and Takeda <ref> [24] </ref> does not gather enough information for the statistics to converge, and therefore an optimal path to the goal state cannot be found. the three mazes studied (figure 5.9). Here the intensity of the grey-scale image depicts the amount of exploration achieved by the robot in the maze environment. <p> By using the calculated policy determined from a predefined amount of exploration as initial condition for the incremental learning simulates to a certain extent a predefined amount of prior knowledge. Learning is performed using the exploration heuristic of Sato and Takeda <ref> [24] </ref>, with an exploration coefficient EC = 5. Using this exploration heuristic allows for sufficient forced non-optimal actions, and thus sufficient further exploration of the state space.
Reference: [25] <author> R.M.Wheeler and K.S.Narenda. </author> <title> Decentralized learning in finite markov chains. </title> <journal> IEEE Trans. on Automatic Control, </journal> <volume> AC-31(6):519-526, </volume> <month> June </month> <year> 1986. </year>
Reference-contexts: This is commonly known as the law of large numbers [18]. Algorithm 4 summarizes the estimation of the process model. 4.3 Dynamic Programming Markov decision models have been shown to be useful when formalizing sequential decision tasks <ref> [20, 21, 22, 23, 24, 25, 26] </ref>. <p> The theory of Dynamic Programming offers much less computationally expensive methods of solving for the optimal policy of a Markov decision process. Of these methods, Policy Iteration has widely been studied in learning controllers <ref> [21, 24, 25] </ref>.
Reference: [26] <author> Y.M.El-Fattah. </author> <title> Recursive algorithms for adaptive control of finite markov chains. </title> <journal> IEEE Trans. Sys., Man and Cyber., </journal> <volume> SMC-11(2):135-143, </volume> <month> February </month> <year> 1981. </year> <month> 118 </month>
Reference-contexts: This is commonly known as the law of large numbers [18]. Algorithm 4 summarizes the estimation of the process model. 4.3 Dynamic Programming Markov decision models have been shown to be useful when formalizing sequential decision tasks <ref> [20, 21, 22, 23, 24, 25, 26] </ref>.
Reference: [27] <author> E.V.Denardo. </author> <title> Dynamic Programming: Models and Applications. </title> <publisher> Prentice-Hall, Yale University, </publisher> <year> 1982. </year>
Reference-contexts: Dynamic Programming is the collection of mathematical tools used to analyze sequential decision processes. First formally proposed by Bellman [1], it has led to a widespread interest in the field of optimal control. Since then, many authors have elaborated on related topics in this field <ref> [8, 9, 27] </ref>.
Reference: [28] <author> S.A.Teukolsky W.H.Press, B.P.Flannery and W.T.Vetterling. </author> <title> Numerical Recipes in C. </title> <publisher> Cambridge University Press, </publisher> <address> New York, </address> <year> 1988. </year>
Reference-contexts: The linear program given by program 1 may be solved using any of the available computational techniques. Of these the Simplex method has been proven to be the most efficient and successful <ref> [28] </ref>. Computational Cost The Simplex method [28], well known in linear programming theory, uses the hyperplanes defined by the set of equalities and inequalities, in order to solve for a vertex (a point which satisfies the linear program). <p> The linear program given by program 1 may be solved using any of the available computational techniques. Of these the Simplex method has been proven to be the most efficient and successful <ref> [28] </ref>. Computational Cost The Simplex method [28], well known in linear programming theory, uses the hyperplanes defined by the set of equalities and inequalities, in order to solve for a vertex (a point which satisfies the linear program).
Reference: [29] <author> P.Dayan and G.E.Hinton. </author> <title> Feudal reinforcement learning. </title> <note> Submitted for publication, </note> <year> 1992. </year>
Reference-contexts: It has the added advantage that it does not need to construct an explicit model of the world/environment. Since its first appearance, many authors have studied Q-learning and variations thereof <ref> [29, 30, 31] </ref>. However, the direct relationship to Dynamic Programming was initially not clearly reflected in literature. Wessels [32] shows this relationship more formally, and it can also be found in [10].
Reference: [30] <author> A.G.Barto and S.P.Singh. </author> <title> On the computational economics of reinforcement learning. </title> <note> Submitted for publication, </note> <year> 1991. </year>
Reference-contexts: It has the added advantage that it does not need to construct an explicit model of the world/environment. Since its first appearance, many authors have studied Q-learning and variations thereof <ref> [29, 30, 31] </ref>. However, the direct relationship to Dynamic Programming was initially not clearly reflected in literature. Wessels [32] shows this relationship more formally, and it can also be found in [10]. <p> Sato and Takeda [24] uses an explicit mechanism to force the learning agent to take non-optimal actions in order to gain information. This exploration mechanism also used by Barto and Singh <ref> [30] </ref>, is implemented as follows. <p> The accumulated reward at time step "t" is given by AR (t) = n=0 5.3.2 Example # 1 The first problem studied is described by Sato and Takeda [24], and was also used by Barto and Singh <ref> [30] </ref>. The transition probability and reward matrices describing the model are given in tables 5.1 and 5.2 respectively. Using these values the expected reward matrix was calculated and is given in table 5.3. Because the agent makes a probabilistic transition the reward matrices are defined as such. <p> This seems contradictory because analytic solutions, although costly, are expected to be far more data efficient than an iterative equivalent. This peculiar response was shown to occur in Barto and Singh <ref> [30] </ref> as well.
Reference: [31] <author> S.D.Whitehead. </author> <title> Reinforcement Learning for Adaptive Control and Perception and Action. </title> <type> PhD thesis, </type> <institution> University of Rochester, </institution> <address> New York, </address> <month> February </month> <year> 1992. </year>
Reference-contexts: It has the added advantage that it does not need to construct an explicit model of the world/environment. Since its first appearance, many authors have studied Q-learning and variations thereof <ref> [29, 30, 31] </ref>. However, the direct relationship to Dynamic Programming was initially not clearly reflected in literature. Wessels [32] shows this relationship more formally, and it can also be found in [10]. <p> The temporal-difference error is a direct indication of the accuracy of the value function estimate. Using the Dynamic Programming formulation for the long term expected reward at time "t", V t (x) = E k=t+1 ) we can derive the temporal-difference error in the value function estimate as follows <ref> [31] </ref>: Define the n-step truncated return as r t = r t + flr t+1 + ::: + fl n1 r t+n1 (6:8) Using an estimate of the return from time t + n onward, define the corrected n-step truncated return as: [n] where U t (x t+n ) represents the
Reference: [32] <author> L.F.A.Wessels. </author> <title> Applying dynamic programming to markov decision models. </title> <type> Technical report, </type> <institution> MATTEK, CSIR, </institution> <month> August </month> <year> 1992. </year>
Reference-contexts: It has the added advantage that it does not need to construct an explicit model of the world/environment. Since its first appearance, many authors have studied Q-learning and variations thereof [29, 30, 31]. However, the direct relationship to Dynamic Programming was initially not clearly reflected in literature. Wessels <ref> [32] </ref> shows this relationship more formally, and it can also be found in [10]. It is repeated here for the sake of clarity and is taken almost verbatim from Wessels [32] 8 . <p> However, the direct relationship to Dynamic Programming was initially not clearly reflected in literature. Wessels <ref> [32] </ref> shows this relationship more formally, and it can also be found in [10]. It is repeated here for the sake of clarity and is taken almost verbatim from Wessels [32] 8 . Q-Learning is incremental Dynamic Programming The reader will recall that the Q-values (state-action values) which are defined as Q f (x; a) = (x; a) + fl y2S represent the total expected discounted reward for executing action a in state x and following policy ~ f thereafter.
Reference: [33] <author> A.W.Moore and C.G.Atkeson. </author> <title> Memory-based reinforcement learning: Converging with less data and less real time. </title> <note> Submitted for publication, </note> <year> 1992. </year>
Reference-contexts: This is commonly known as the exploration paradigm. Of particular interest is the Prioritized Sweeping algorithm (Section 9), proposed by Moore and Atkeson <ref> [33] </ref>. Prioritized Sweeping allows for extra computation between real-world experiences, thus eliminating to a certain extent the second form of convergence mentioned in the previous chapter. <p> The optimal decision policies for each of the three mazes are indicated by the arrows in figures 5.7-5.9 respectively. These maze tasks are exactly the same as studied by Moore and Atkeson <ref> [33] </ref>. Simulations were performed based on the exploration mechanism discussed in section 5.2. Simulations were terminated after 10 000 time steps or after an "optimal" 1 path was found. <p> + fl y2S i otherwise ALP V t+1 (x) = 1fl if n a x (t) &lt; T bored = max a2A ^(x;a)+fl y2S;y6=x 1fl ^ P xx (a) otherwise Table 5.8: Implementation of the Moore and Atkeson exploration heuristic in the computational methods used for the maze simulations Atkeson <ref> [33] </ref>. 5.4.1 Exploration Trade-off The efficiency of exploration forms an important factor in the success rate of any learning system. Moore and Atkeson [33] propose the philosophy of optimism in the face of uncertainty, closely related to the interval estimation technique of Kaelbling [35], and the exploration bonus heuristic found in <p> y2S;y6=x 1fl ^ P xx (a) otherwise Table 5.8: Implementation of the Moore and Atkeson exploration heuristic in the computational methods used for the maze simulations Atkeson <ref> [33] </ref>. 5.4.1 Exploration Trade-off The efficiency of exploration forms an important factor in the success rate of any learning system. Moore and Atkeson [33] propose the philosophy of optimism in the face of uncertainty, closely related to the interval estimation technique of Kaelbling [35], and the exploration bonus heuristic found in Dyna Architectures [3]. <p> Table 5.8 summarizes the implementation of the Moore and Atkeson exploration heuristic for the various computational techniques studied. 5.4.2 Prioritized Sweeping A further mechanism studied by Moore and Atkeson <ref> [33] </ref> was the effect of the computational effort during the learning process. This constitutes the difference, for example, between the purely iterative mechanism of Q-learning and the wholly analytic solution of Policy Iteration. <p> The effect of the fi-parameter is to eliminate to a certain extent the second form of convergence i.e. convergence due to iteration towards optimal policy given P xy (a) and (x; a). 5.4.3 Results and Discussion Using the improved active exploration heuristics of Moore and Atkeson <ref> [33] </ref>, simulations were performed for the three maze tasks studied earlier.
Reference: [34] <author> G.J.Tesauro. </author> <title> Practical issues in temporal difference learning. </title> <type> Technical report, </type> <institution> IBM Research Division, </institution> <month> September </month> <year> 1991. </year>
Reference-contexts: This is especially true in cases where the associated value function for a specific state does not differ significantly between two or more available actions. This is typical of agents with a good estimate for the performance in the current state, but which are poor action selectors <ref> [34] </ref>. Weighted frequency coefficient Sato and Takeda [24] proposed a simple and straightforward way for comparing the relative perfor mance of a learning method.
Reference: [35] <author> L.P.Kaelbling. </author> <title> A formal framework for learning in embedded systems. </title> <publisher> In Morgan Kaufmann, </publisher> <editor> editor, </editor> <booktitle> Proceedings of the Sixth International Workshop on Machine Learning, </booktitle> <year> 1987. </year>
Reference-contexts: Moore and Atkeson [33] propose the philosophy of optimism in the face of uncertainty, closely related to the interval estimation technique of Kaelbling <ref> [35] </ref>, and the exploration bonus heuristic found in Dyna Architectures [3]. This method is based on the principle that in the absence of contrary evidence, any action is assumed to lead directly to the goal state with a large immediate reward r opt .
Reference: [36] <author> A.W.Moore. </author> <title> Knowledge of knowledge and intelligent experimentation for learning control. </title> <note> Accepted for publication in Machine Learning, </note> <year> 1990. </year>
Reference-contexts: This restriction is clearly not necessary, because much can be learned from the environment by taking into consideration the previous actions taken in the region of the current state <ref> [36] </ref>. What is therefore needed is some way of generalization, so as to reduce the size of the problem and thus the fraction of the control environment which needs to be explored before an optimal or near-optimal control policy may be formed.
Reference: [37] <author> T.J.Prescott and J.E.W.Mayhew. </author> <title> Obstacle avoidance through reinforcement learning. </title> <editor> In S.J.Hanson J.H.Moody and R.P.Lippman, editors, </editor> <booktitle> Advances in Neural Information Processing Systems, </booktitle> <year> 1991. </year>
Reference-contexts: Based on this platform the complexity of the problem is successively increased (Sections 6.4, 6.5). Section 6.6 briefly discusses certain ideas related to generalization and finally section 6.7 summarizes and draws conclusions. 6.2 Problem description robot must learn to avoid hitting any of the obstacles. The robot, named Sprite <ref> [37] </ref>, uses a set of three directed range finding sensors. Each sensor determines, from set angles, a logarithmically-scaled distance value. The Sprite operates within a bounded region, usually in the order of 25m 2 . <p> The first, referred to as the Adaptive Critic Element, uses the temporal-difference error in the value-function estimate in order to form a function or mapping of the state variables, approximating the associated value function <ref> [5, 7, 37, 38, 39] </ref>. The temporal-difference error is a direct indication of the accuracy of the value function estimate. <p> The disadvantages, however, are directly coupled with the normal difficulties associated with neural networks. These include choice of network size, training parameters and local minima. Given these difficulties associated with neural architectures, several authors <ref> [37, 38] </ref> have proposed the use of a coded architecture known as the CMAC, in place of the neural network. 6.6.2 CMAC Cerebellar Model Arithmetic Computer The CMAC [14] (Figure 6.12) is a coarse-coding structure, where each region in state space is represented by a set of overlapping tiles, offset with <p> CMAC architectures have been used successfully in a number of complex control problems. These include multi-linked manipulator control [38], as well as robot path planning <ref> [37] </ref>. The problems associated with CMACs mostly appear in the choice of the quantization functions, determining the size of overlap between neighbouring tiles, and the number of tiles (quantization intervals) to use per input variable.
Reference: [38] <author> C.K.Tham and R.W.Prager. </author> <title> Reinforcement learning for multi-linked manipulator control. </title> <type> Technical Report CUED/F-INFENG/TR 104, </type> <institution> Cambrigde University, </institution> <address> Trumpington Street, Cambridge, CB2 1PZ, UK, </address> <month> June </month> <year> 1992. </year>
Reference-contexts: The first, referred to as the Adaptive Critic Element, uses the temporal-difference error in the value-function estimate in order to form a function or mapping of the state variables, approximating the associated value function <ref> [5, 7, 37, 38, 39] </ref>. The temporal-difference error is a direct indication of the accuracy of the value function estimate. <p> The disadvantages, however, are directly coupled with the normal difficulties associated with neural networks. These include choice of network size, training parameters and local minima. Given these difficulties associated with neural architectures, several authors <ref> [37, 38] </ref> have proposed the use of a coded architecture known as the CMAC, in place of the neural network. 6.6.2 CMAC Cerebellar Model Arithmetic Computer The CMAC [14] (Figure 6.12) is a coarse-coding structure, where each region in state space is represented by a set of overlapping tiles, offset with <p> These generalizations are based on the principle Reinforcement Learning Chapter 6: A Question of Knowledge page 93 that function values are similar in neighbouring states. CMAC architectures have been used successfully in a number of complex control problems. These include multi-linked manipulator control <ref> [38] </ref>, as well as robot path planning [37]. The problems associated with CMACs mostly appear in the choice of the quantization functions, determining the size of overlap between neighbouring tiles, and the number of tiles (quantization intervals) to use per input variable.
Reference: [39] <author> C.W.Anderson. </author> <title> Learning to control an inverted pendulum using neural networks. </title> <journal> IEEE Control Systems Magazine, </journal> <pages> pages 31-37, </pages> <month> April </month> <year> 1989. </year>
Reference-contexts: The first, referred to as the Adaptive Critic Element, uses the temporal-difference error in the value-function estimate in order to form a function or mapping of the state variables, approximating the associated value function <ref> [5, 7, 37, 38, 39] </ref>. The temporal-difference error is a direct indication of the accuracy of the value function estimate. <p> This increase in confidence is normally reflected in the value function network (Adaptive Critic Element), when the temporal-difference error decreases. The Adaptive Search Element is trained by back-propagation of the temporal-difference error <ref> [6, 39] </ref>. This corresponds to a steepest ascent in the value space [19]. The advantage of such a parallel architecture lies both in the generalization capabilities with respect to the approximation of the value function, and the formation of an effective state-action space mapping.
Reference: [40] <author> F.J.Pineda. </author> <title> Dynamic architecture for neural computation. </title> <journal> Journal of Complexity, </journal> <volume> 4 </volume> <pages> 216-245, </pages> <year> 1988. </year>
Reference: [41] <author> M.I.Jordan and R.A.Jacobs. </author> <title> Learning to control an unstable system with forward modeling. </title> <editor> In D.S.Touretzky, editor, </editor> <booktitle> Advances in neural information processing systems, </booktitle> <volume> volume 2, </volume> <pages> pages 324-331, </pages> <address> San Mateo, CA, </address> <year> 1990. </year> <month> NIPS. </month>
Reference-contexts: Information is therefore passed back in time (dotted line in of feedback from the key components of the chosen action u. This feedback accounts for both cause and effect information embedded within the model network. This approach has, however, been applied successfully only once in the recorded literature <ref> [41] </ref>. Although this methodology seems more logical than the Adaptive Heuristic Critic, the topology introduces many other difficulties due to the addition of the model estimation network.
Reference: [42] <author> C.F.N.Cowan and P.M.Grant. </author> <title> Adaptive Filters. </title> <publisher> Prentice-Hall, </publisher> <address> Englewood Cliffs, New Jersey, </address> <year> 1985. </year>
Reference-contexts: 102 Following this notation the inverse of matrix (I P n ) at time step t = n + 1 can be determined from equation A.16 as follows A (n + 1) = (I P n ) + ~x (n)~y T (n) (A.18) fi fl 1 Using the following identity <ref> [42] </ref> Identity 1 [A + BCD] = A 1 A 1 B C + DA 1 B DA 1 (A:20) equation A.18 can be written as A (n + 1) = A (n) 1 + ~x (n)~y T (n) (A.21) fi fl 1 Denote ~ k (n + 1) = A
Reference: [43] <author> E.Kreysig. </author> <title> Introductory Functional Analysis with Applications. </title> <publisher> John Wiley and Sons, </publisher> <address> New York, </address> <year> 1978. </year>
Reference-contexts: P xy (a)V (y) + flru (1 P xx (a)) 1 flP xx (a) (B.6) Since 0 P xx (a) 1, and 0 &lt; fl &lt; 1, it follows that 0 flru (1 P xx (a)) 1 flP xx (a) flru 8 a 2 A (B:7) Employing the triangular inequality <ref> [43] </ref> max [A (x) + B (x)] max [A (x)] + max [B (x)] (B:8) on equation B.5 we obtain G (V + ru) P 1flP xx (a) + h flru (1P xx (a)) 1flP xx (a) G (V )(x) + flru (B.10) Assuming equation B.4 holds for k = n, <p> For k = 1, we have G (V ru) = max P 1 flP xx (a) (B.21) a2A (x; a) + fl y6=x P xy (a)V (y) flru (1 P xx (a)) 1 flP xx (a) (B.22) Employing the inverse of the triangular inequality <ref> [43] </ref> max [A (x) B (x)] max [A (x)] max [B (x)] (B:23) on equation B.21 we obtain G (V ru) P 1flP xx (a) h flru (1P xx (a)) 1flP xx (a) G (V )(x) flru (B.25) Assuming equation B.20 holds for k = n, i.e G n (V ru)(x) <p> Incremental Linear Programming page 109 Therefore from equations B.38 and B.40 max jG k (V 0 (x) G k (V )(x)j fl k cu (B.42) x2S Since G k : B (S) ! B (S) is a k-stage contraction mapping there exists a unique fixed point (Banach's fixed point theorem <ref> [43] </ref>), i.e. a function V fl 2 B (S) (B:44) such that G (V fl ) = V fl (B:45) With an arbitrary V it follows that lim G k (V )(x) = V fl (x) (B:46) From above equations we know that the repeated application of the mapping G ()
References-found: 43

