URL: http://www.cs.cmu.edu/~wvcii/papers/CMU-CS-94-193.ps
Refering-URL: http://www.cs.cmu.edu/~wvcii/
Root-URL: 
Title: Backward Error Recovery in Redundant Disk Arrays  
Author: William V. Courtright II and Garth A. Gibson 
Date: 27 September 1994  
Note: To appear: Proceedings of the 1994 Computer Measurement Group Conference (CMG94) http://www.cs.cmu.edu:8001/afs/cs/project/pdl/WWW/HomePage.html  CMU-CS-94-193 This research was partially supported by the National Science Foundation under grant number ECD-8907068 and an AT&T fellowship. The views and conclusions contained in this document are those of the authors and should not be interpreted as represent ing the official policies, either expressed or implied, of AT&T or the U.S. government.  
Address: Pittsburgh, Pennsylvania 15213-3890  
Affiliation: School of Computer Science Carnegie Mellon University  
Abstract: Redundant disk arrays are single fault tolerant, incorporating a layer of error handling not found in nonredundant disk systems. Recovery from these errors is complex, due in part to the large number of erroneous states the system may reach. The established approach to error recovery in disk systems is to transition directly from an erroneous state to completion. This technique, known as forward error recovery, relies upon the context in which an error occurs to determine the steps required to reach completion, which implies forward error recovery is design specific. Forward error recovery requires the enumeration of all erroneous states the system may reach and the construction of a forward path from each erroneous state. We propose a method of error recovery which does not rely upon the enumeration of erroneous states or the context in which errors occur. When an error is encountered, we advocate mechanized recovery to an error-free state from which an operation may be retried. Using a form of backward error recovery, we are able to manage the complexity of error recovery in redundant disk arrays without sacrificing performance. 
Abstract-found: 1
Intro-found: 1
Reference: [Anderson79] <author> T. Anderson and B. Randell, </author> <title> Computing Systems Reliability, </title> <publisher> Cambridge University Press, </publisher> <year> 1979. </year>
Reference: [ANSI91] <institution> Small Computer System Interface - 2 (SCSI-2), American National Standard for Information systems, X3T9.2/86-109, Revision 10h, X3T9/89-042, Global Engineering Documents, </institution> <address> X3.131-199x, Irvine CA, </address> <month> October 17, </month> <year> 1991. </year>
Reference-contexts: The initiation of new operations is also resumed at this time. Page 13 of 18 Finally, it is important to note that some disk systems allow clients to specify the relative ordering of operations <ref> [ANSI91] </ref>. For example, some filesystems rely upon the ordering of writes to prevent filesystem corruption [Lefer90]. This ordering must be preserved throughout error recovery process. 4.3. Mechanism The recovery mechanism we present here allows operations to be executed to increase performance during normal operation.
Reference: [Anderson81] <author> T. Anderson and P. A. Lee, </author> <title> Fault Tolerance, </title> <booktitle> Principles and Practice, </booktitle> <publisher> Prentice-Hall, </publisher> <year> 1981. </year>
Reference-contexts: Forward Error Recovery is Inadequate The traditional approach to error recovery in disk systems, forward recovery, attempts to remove an error by applying selective corrections to the erroneous state, simultaneously moving operations forward to completion and bringing the system to a consistent state <ref> [Anderson81] </ref>. Construction of these corrective actions requires detailed foreknowledge of the errors which may occur and the damage that they cause. This requires enumeration of all erroneous states the system may reach. <p> Techniques such as conversations, which synchronize communicating processes, are known methods of avoiding the domino effect [Randell75]. A variety of backward error recovery techniques exist, all of which introduce varying degrees of overhead. These techniques fall into three broad classes: checkpointing, recursive caches, and audit trails <ref> [Anderson81] </ref>. We now examine the applicability of techniques from each of these classes to the domain of redundant disk arrays. 3.2. Checkpointing Systems employing checkpointing establish a recovery point, known as a checkpoint, by saving a subset of the system state, known as checkpoint data [Chandy72, Siewiorek93].
Reference: [Anderson82] <author> T. Anderson and P. A. </author> <title> Lee Fault tolerance terminology proposals. </title> <booktitle> In Proceedings of the 12th Annual International Symposium on Fault Tolerant Computing (FTCS), </booktitle> <address> Santa Monica CA, </address> <month> June </month> <year> 1982, </year> <pages> pp. 29-33. </pages> <note> Page 17 of 18 </note>
Reference-contexts: The definitions presented here are consistent with those of the IEEE Technical Committee on Fault Tolerant Computing <ref> [Melliar-Smith77, Anderson82, Lee82] </ref>. 2. Other faults, such as loss of power, mechanical failure of cabling, can be converted into independent single faults in orthogonal redundant disk arrays [Gibson93].
Reference: [Bhide92] <author> A. Bhide and D. Dias, </author> <title> RAID architectures for OLTP. </title> <institution> IBM Computer Science Research Report RC 17879, </institution> <year> 1992. </year>
Reference-contexts: This property limits the scope of modifications to an existing code base, thereby restricting a designers ability to explore the design space, confining experimentation to limited departures from the current code structure. Finally, researchers are investigating more aggressive redundant disk array architectures to boost performance <ref> [Bhide92, Blaum94, Cao93, Menon93, Stodolsky93, Holland94] </ref>. The acceptance of these proposals is put at risk due to their further increases in the complexity of error handling and the difficulty of modifying existing code structure.
Reference: [Bjork75] <author> L. A. Bjork, Jr., </author> <title> Generalized audit trail requirements and concepts for data base applications. </title> <journal> IBM Systems Journal, </journal> <volume> Vol. 14, No. 3, </volume> <year> 1975, </year> <pages> pp. 229-245. </pages>
Reference-contexts: Audit Trails Finally, audit trail, also known as logging or journaling, techniques provide the ability to record a subset of the system state but, unlike recovery cache techniques, do not require foreknowledge of the state which will be changed by an operation <ref> [Bjork75, Verhofstad78, Gray81] </ref>. Instead, all changes to the system state are recorded in stable storage. Recovery is performed by applying the inversion of these records in LIFO fashion, thereby removing state changes. As inverted records are applied, work is undone.
Reference: [Blaum94] <author> Mario Blaum, Jim Brady, Jehoshua Bruk, Jai Menon, EVENODD: </author> <title> An optimal scheme for tolerating double disk failures in RAID architectures. </title> <booktitle> In Proceedings of the 21st Annual International Symposium on Computer Architecture (ISCA), </booktitle> <address> Chicago IL, </address> <month> April 18-21, </month> <year> 1994, </year> <pages> pp. 245-254. </pages>
Reference-contexts: This property limits the scope of modifications to an existing code base, thereby restricting a designers ability to explore the design space, confining experimentation to limited departures from the current code structure. Finally, researchers are investigating more aggressive redundant disk array architectures to boost performance <ref> [Bhide92, Blaum94, Cao93, Menon93, Stodolsky93, Holland94] </ref>. The acceptance of these proposals is put at risk due to their further increases in the complexity of error handling and the difficulty of modifying existing code structure.
Reference: [Cao93] <author> Pei Cao, Swee Boon Lim, Shivakumar Venkataraman, and John Wilkes, </author> <title> The TickerTAIP parallel RAID architecture. </title> <booktitle> In Proceedings of the 20th Annual International Symposium on Computer Architecture, </booktitle> <address> San Diego CA, </address> <month> May </month> <year> 1993, </year> <pages> pp. 52-63. </pages>
Reference-contexts: This property limits the scope of modifications to an existing code base, thereby restricting a designers ability to explore the design space, confining experimentation to limited departures from the current code structure. Finally, researchers are investigating more aggressive redundant disk array architectures to boost performance <ref> [Bhide92, Blaum94, Cao93, Menon93, Stodolsky93, Holland94] </ref>. The acceptance of these proposals is put at risk due to their further increases in the complexity of error handling and the difficulty of modifying existing code structure.
Reference: [Chandy72] <author> K. M. Chandy and C. V. Ramamoorthy, </author> <title> Rollback and recovery strategies for computer programs. </title> <journal> IEEE Transactions on Computers, </journal> <volume> Vol. C-21, No. 6, </volume> <month> June </month> <year> 1972, </year> <pages> pp. 546-556. </pages>
Reference-contexts: We now examine the applicability of techniques from each of these classes to the domain of redundant disk arrays. 3.2. Checkpointing Systems employing checkpointing establish a recovery point, known as a checkpoint, by saving a subset of the system state, known as checkpoint data <ref> [Chandy72, Siewiorek93] </ref>. Erroneous state information is removed by returning the system to a checkpoint which is assumed to be free from error. The process of returning to a checkpoint, referred to as rollback, requires the checkpoint data associated with the checkpoint to be reinstated.
Reference: [Chandy85] <author> K. Mani Chandy and Leslie Lamport, </author> <title> Distributed snapshots: determining global states of distributed systems. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> Vol. 3, No. 1, </volume> <month> Feb. </month> <year> 1985, </year> <pages> pp. 63-75. </pages>
Reference-contexts: A more efficient alternative is to save only a subset of the system state. For instance, a technique commonly known as consistent checkpointing creates process checkpoints, which are checkpoints of the state of a process <ref> [Chandy85] </ref>. Collectively, these process checkpoints compose a checkpoint of the system. processes and the relationship between their recovery points and communications are shown to illustrate the domino effect. Consider recovery, from the present time, of each of the three processes.
Reference: [Gibson89] <author> Garth A. Gibson, </author> <title> Performance and reliability in redundant arrays of inexpensive disks (RAID). </title> <booktitle> In Proceedings of the 1989 Computer Measurement Group conference (CMG), </booktitle> <address> Reno NV, </address> <month> December </month> <year> 1989, </year> <pages> pp. 381-391. </pages>
Reference: [Gibson92] <author> Garth A. Gibson, </author> <title> Redundant Disk Arrays: Reliable, Parallel Secondary Storage, </title> <publisher> The MIT Press, </publisher> <year> 1992. </year>
Reference-contexts: Because reliability and availability are critical characteristics of storage systems, disk arrays are usually designed to be single fault tolerant. This is accomplished by storing redundant data in the disk array <ref> [Gib-son89, Gibson92] </ref>. Instead of propagating errors resulting from a disk failure to a client to handle, the redundant disk array now performs recovery from these errors, hiding their effects from clients and providing continuous service throughout the life of the fault. 2.2.
Reference: [Gibson93] <author> Garth A. Gibson, David A. Patterson, </author> <title> Designing disk arrays for high data reliability. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> Vol. 17, No. </volume> <pages> 1-2, </pages> <address> Jan.-Feb. </address> <year> 1993, </year> <pages> pp. 4-27. </pages>
Reference-contexts: As the number of disks in the array increases, reliability, the probability that the disk array will function correctly, and availability, the probability that the disk array is able to service requests, may decrease to unacceptable levels since data loss occurs on the first failure <ref> [Gibson93] </ref>. This problem may be further aggravated because, as Patterson, Gibson, and Katz suggest, commodity disks may be employed in order to reduce the cost of storage in the array [Patterson88]. <p> The definitions presented here are consistent with those of the IEEE Technical Committee on Fault Tolerant Computing [Melliar-Smith77, Anderson82, Lee82]. 2. Other faults, such as loss of power, mechanical failure of cabling, can be converted into independent single faults in orthogonal redundant disk arrays <ref> [Gibson93] </ref>.
Reference: [Gray81] <author> Jim Gray, </author> <booktitle> Notes on data base operating systems. lecture notes from The Advanced Course in Operating Systems, </booktitle> <address> July 28-August 5, </address> <year> 1977, </year> <institution> Technical University, </institution> <address> Munich, Federal Republic of Germany, </address> <booktitle> published in Operating Systems: An Advanced Course, Vol. 60 of the series Lecture Notes in Computer Science, </booktitle> <publisher> Springer-Verlag, </publisher> <year> 1981, </year> <pages> pp. 393-481. </pages>
Reference-contexts: Audit Trails Finally, audit trail, also known as logging or journaling, techniques provide the ability to record a subset of the system state but, unlike recovery cache techniques, do not require foreknowledge of the state which will be changed by an operation <ref> [Bjork75, Verhofstad78, Gray81] </ref>. Instead, all changes to the system state are recorded in stable storage. Recovery is performed by applying the inversion of these records in LIFO fashion, thereby removing state changes. As inverted records are applied, work is undone.
Reference: [Gray87] <author> Jim Gray, Paul McJones, Mike Blasgen, Bruce Lindsay, Raymond Lorie, Tom Price, Franco Putzolu, and Irving Traiger, </author> <title> The recovery manager of the System R database manager. </title> <journal> ACM Computing Surveys, </journal> <volume> Vol. 13, No. 2, </volume> <month> June </month> <year> 1981, </year> <pages> pp. 223-242. </pages>
Reference-contexts: As inverted records are applied, work is undone. Once the system is in a consistent state, some records may be applied in FIFO fashion to restore previously completed work. The System R database recovery manager implements such an approach <ref> [Gray87] </ref>. 3.5. Summary Backward error recovery is well suited for systems in which error recovery is complex. Atomicity is more easily achieved and error recovery is context free. Code modification and enhancement are also simplified. Unfortunately, backward error recovery introduces overhead which degrades normal (error-free) performance.
Reference: [Holland94] <author> Mark Holland, </author> <title> On-line data reconstruction in redundant disk arrays. </title> <type> Ph.D. dissertation, </type> <institution> Carn-egie Mellon University School of Computer Science technical report CMU-CS-94-164, </institution> <month> May </month> <year> 1994. </year>
Reference-contexts: This property limits the scope of modifications to an existing code base, thereby restricting a designers ability to explore the design space, confining experimentation to limited departures from the current code structure. Finally, researchers are investigating more aggressive redundant disk array architectures to boost performance <ref> [Bhide92, Blaum94, Cao93, Menon93, Stodolsky93, Holland94] </ref>. The acceptance of these proposals is put at risk due to their further increases in the complexity of error handling and the difficulty of modifying existing code structure.
Reference: [Horning74] <author> J. J. Horning, H. C. Lauer, P. M. Melliar-Smith, B. Randell, </author> <title> A program structure for error detection and recovery. </title> <booktitle> Proceedings of an International Symposium held at Rocquencourt, </booktitle> <month> April 23-25 </month> <year> 1974, </year> <booktitle> published in Lecture Notes in Computer Science, </booktitle> <volume> Vol. 16, </volume> <publisher> Springer-Verlag, </publisher> <year> 1974, </year> <pages> pp. 171-187. </pages>
Reference-contexts: Process 1 Process 2 Process 3 time present time process recovery pointinter-process communication 2.0 3.0 3.1 3.2 3.3 1.1 1.2 1.3 Page 11 of 18 3.3. Recursive Cache One solution to the problem of large amounts of recovery data is the recursive cache, also known as a recovery cache <ref> [Horning74] </ref>. By monitoring actions which modify the system state, specific state information is saved in a recursive cache, prior to modification.
Reference: [Katz89] <author> Randy H. Katz, Garth A. Gibson, David A. Patterson, </author> <title> Disk system architectures for high performance computing. </title> <booktitle> In Proceedings of the IEEE, </booktitle> <volume> Vol. 77, No. 12, </volume> <month> December </month> <year> 1989, </year> <pages> pp. 1842-1858. </pages> <note> Also published in CMG Transactions, issue 74, </note> <month> fall </month> <year> 1991, </year> <pages> pp. 27-46. </pages>
Reference-contexts: Application Filesystem/Database Redundant Disk Array Disk Drives Page 5 of 18 2. Motivation 2.1. Motivations for Redundant Disk Arrays Disk arrays are a well established method of using parallelism to reduce response time in disk storage systems <ref> [Kim86, Salem86, Katz89, Reddy89] </ref>.
Reference: [Kim86] <author> Michelle Y. Kim, </author> <title> Synchronized disk interleaving. </title> <journal> IEEE Transactions on Computers, </journal> <volume> Vol. 35, No. 11, </volume> <month> November </month> <year> 1986, </year> <pages> pp. 978-988. </pages>
Reference-contexts: Application Filesystem/Database Redundant Disk Array Disk Drives Page 5 of 18 2. Motivation 2.1. Motivations for Redundant Disk Arrays Disk arrays are a well established method of using parallelism to reduce response time in disk storage systems <ref> [Kim86, Salem86, Katz89, Reddy89] </ref>.
Reference: [Lampson79] <author> Butler W. Lampson and Howard E. Sturgis, </author> <title> Crash recovery in a distributed data storage system. </title> <institution> Xerox Palo Alto Research Center, </institution> <address> 3333 Coyote Hill Road, Palo Alto, California 94304, </address> <month> April 27, </month> <year> 1979. </year> <note> Page 18 of 18 </note>
Reference-contexts: Simple disk systems are not fault tolerant; a single fault can lead to data loss. The accepted failure model of such nonredundant disk systems requires only error detection, the recognition of the presence of an error <ref> [Lampson79] </ref>. This is acceptable since applications which require fault tolerance implement schemes to survive data loss at the application level of the system in the following way.
Reference: [Lee82] <author> P. A. Lee and T. Anderson, </author> <title> Fundamental concepts of fault tolerant computing: progress report. </title> <booktitle> In Proceedings of the 12th Annual International Symposium on Fault Tolerant Computing (FTCS), </booktitle> <address> Santa Monica CA, </address> <month> June </month> <year> 1982, </year> <pages> pp. 34-38. </pages>
Reference-contexts: The definitions presented here are consistent with those of the IEEE Technical Committee on Fault Tolerant Computing <ref> [Melliar-Smith77, Anderson82, Lee82] </ref>. 2. Other faults, such as loss of power, mechanical failure of cabling, can be converted into independent single faults in orthogonal redundant disk arrays [Gibson93].
Reference: [Lee90] <author> Edward K. Lee and Randy H. Katz, </author> <title> Performance considerations of parity placement in disk arrays. </title> <booktitle> In Proceedings of the Fourth International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS IV), </booktitle> <address> Palo Alto CA, </address> <month> April </month> <year> 1991, </year> <pages> pp. 190-199. </pages>
Reference: [Lefer90] <author> Samuel J. Lefer, Marshall Kirk McKusick, Michael J. Karels, John S. Quarterman, </author> <title> The Design and Implementation of the 4.3BSD UNIX Operating System, </title> <publisher> Addison-Wesley, </publisher> <address> Reading MA, </address> <year> 1990. </year>
Reference-contexts: Errors due to the existence of multiple disk failures may result in loss of data in the array. Filesystems generally do not provide additional fault tolerance, but do attempt to minimize the effects of data loss presented to an application <ref> [Lefer90] </ref>. Databases, however, usually do provide higher levels of fault tolerance; in particular, operations are generally atomic, meaning that transactions which fail leave the applicationss view of the system unchanged. Application Filesystem/Database Redundant Disk Array Disk Drives Page 5 of 18 2. Motivation 2.1. <p> Operations in a filesystem are complex and are executed concurrently; however, since filesystems are not fault-tolerant, errors which result in a data loss are acceptable. For instance, when the BSD 4.3 UNIX operating system unexpectedly loses access to a disk, data may be lost <ref> [Lefer90] </ref>. forward error recovery in a RAID level 5 small-write operation. In this illustration, the erroneous state characterized by the inability of a small-write operation to read old data has been reached. To proceed from this erroneous state and complete the operation, new parity must still be written. <p> The initiation of new operations is also resumed at this time. Page 13 of 18 Finally, it is important to note that some disk systems allow clients to specify the relative ordering of operations [ANSI91]. For example, some filesystems rely upon the ordering of writes to prevent filesystem corruption <ref> [Lefer90] </ref>. This ordering must be preserved throughout error recovery process. 4.3. Mechanism The recovery mechanism we present here allows operations to be executed to increase performance during normal operation. Performance is increased by allowing maximal concurrency of actions within an operation and not introducing overhead by saving recovery data.
Reference: [Melliar-Smith77] <author> P. M. Melliar-Smith and B. Randell, </author> <title> Software reliability: the role of programmed exception handling. </title> <booktitle> In Proceedings of an ACM Conference on Language Design for Reliable Software, </booktitle> <address> Raleigh NC, </address> <month> March </month> <year> 1977, </year> <pages> pp. 95-100. </pages>
Reference-contexts: The definitions presented here are consistent with those of the IEEE Technical Committee on Fault Tolerant Computing <ref> [Melliar-Smith77, Anderson82, Lee82] </ref>. 2. Other faults, such as loss of power, mechanical failure of cabling, can be converted into independent single faults in orthogonal redundant disk arrays [Gibson93].
Reference: [Menon93] <author> J. Menon, J. Roche, and J. Kasson, </author> <title> Floating parity and data disk arrays. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> Vol. 17, No. </volume> <pages> 1-2, </pages> <address> Jan.-Feb. </address> <year> 1993, </year> <pages> pp. 129-139. </pages>
Reference-contexts: This property limits the scope of modifications to an existing code base, thereby restricting a designers ability to explore the design space, confining experimentation to limited departures from the current code structure. Finally, researchers are investigating more aggressive redundant disk array architectures to boost performance <ref> [Bhide92, Blaum94, Cao93, Menon93, Stodolsky93, Holland94] </ref>. The acceptance of these proposals is put at risk due to their further increases in the complexity of error handling and the difficulty of modifying existing code structure.
Reference: [Patterson88] <author> David A. Patterson, Garth A. Gibson, and Randy H. Katz, </author> <title> A case for redundant arrays of inexpensive disks (RAID). </title> <booktitle> In Proceedings of the 1988 ACM Conference on Management of Data (SIGMOD), </booktitle> <address> Chicago IL, </address> <month> June </month> <year> 1988, </year> <pages> pp. 109-116. </pages> <note> Also published in CMG Transactions, issue 74, </note> <month> fall </month> <year> 1991, </year> <pages> pp. 13-25. </pages>
Reference-contexts: 1. Introduction Through the use of redundancy, disk arrays can be made single fault tolerant. Disk arrays which provide single fault tolerance, categorized by a taxonomy known as RAID (Redundant Arrays of Inexpensive Disks) in 1988 and summarized in Figure 1, have become a important class of storage devices <ref> [Patterson88] </ref>. As such, many companies are now engaged in the design of RAID systems. <p> This problem may be further aggravated because, as Patterson, Gibson, and Katz suggest, commodity disks may be employed in order to reduce the cost of storage in the array <ref> [Patterson88] </ref>. Because reliability and availability are critical characteristics of storage systems, disk arrays are usually designed to be single fault tolerant. This is accomplished by storing redundant data in the disk array [Gib-son89, Gibson92].
Reference: [Randell75] <author> Brian Randell, </author> <title> System structure for software fault tolerance. </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> Vol. SE-1, No. 2, </volume> <month> June </month> <year> 1975, </year> <pages> pp. 220-232. </pages>
Reference-contexts: This has a direct impact on performance since recovery data is saved as a part of normal processing. Page 10 of 18 Finally, as Randell points out, backward error recovery in systems characterized by communicating processes can lead to disastrous results <ref> [Randell75] </ref>. The problem, known as the domino effect, occurs when communication has taken place between the recovery point and the point in which an error is detected. When recovery is performed, the effects of the communication are undone, requiring recovery of the other processes involved in the communication. <p> An illustration of this problem, taken from [Stone89], is presented as Figure 6. Techniques such as conversations, which synchronize communicating processes, are known methods of avoiding the domino effect <ref> [Randell75] </ref>. A variety of backward error recovery techniques exist, all of which introduce varying degrees of overhead. These techniques fall into three broad classes: checkpointing, recursive caches, and audit trails [Anderson81].
Reference: [Randell78] <author> B. Randell, P. A. Lee, and P. C. Treleaven, </author> <title> Reliability issues in computing system design. </title> <journal> ACM Computing Surveys, </journal> <volume> Vol. 10, No. 2, </volume> <month> June </month> <year> 1978, </year> <pages> pp. 123-165. </pages>
Reference-contexts: This is a result of the dependence upon knowledge of the context in which an error occurs <ref> [Randell78] </ref>. Because of this, once a design is created, it can be very difficult to make changes to the design, particularly when new error types are introduced or when existing error types are altered. <p> A recovery point is established by storing recovery data, information which describes the state of the system, as a part of normal processing. When an error is detected, the system is returned to the recovery point by reinstating the recovery data <ref> [Randell78, Stone89] </ref>. Previously completed work which is undone as a result of moving backward to a recovery point must be redone. Backward error recovery does not rely upon the type of error or the errors context in removing the error from the system. Thus, context-free error recovery is possible.
Reference: [Reddy89] <author> A. L. Narasimha Reddy and Prithviraj Banerjee, </author> <title> An evaluation of multiple-disk I/O systems. </title> <journal> IEEE Transactions on Computers, </journal> <volume> Vol. 38, No. 12, </volume> <month> December </month> <year> 1989, </year> <pages> pp. 1680-1690. </pages>
Reference-contexts: Application Filesystem/Database Redundant Disk Array Disk Drives Page 5 of 18 2. Motivation 2.1. Motivations for Redundant Disk Arrays Disk arrays are a well established method of using parallelism to reduce response time in disk storage systems <ref> [Kim86, Salem86, Katz89, Reddy89] </ref>.
Reference: [Salem86] <author> K. Salem and H. Garcia-Molina, </author> <title> Disk Striping. </title> <booktitle> In Proceedings of the 2nd International Conference on Data Engineering, </booktitle> <publisher> IEEE CS Press, </publisher> <address> Los Alamitos, </address> <note> CA Order No. 827 (microfiche only), </note> <year> 1986, </year> <pages> pp. 336-342. </pages>
Reference-contexts: Application Filesystem/Database Redundant Disk Array Disk Drives Page 5 of 18 2. Motivation 2.1. Motivations for Redundant Disk Arrays Disk arrays are a well established method of using parallelism to reduce response time in disk storage systems <ref> [Kim86, Salem86, Katz89, Reddy89] </ref>.
Reference: [Siewiorek92] <author> Daniel P. Siewiorek and Robert S. Swarz, </author> <title> Reliable Computer Systems: Design and Evaluation, Second Edition, </title> <publisher> Digital Press, </publisher> <year> 1992 </year>
Reference: [Stone89] <author> R. F. Stone, </author> <title> Reliable computing systems - a review. </title> <institution> University of York, Department of Computer Science Technical Report YCS 110(1989), </institution> <year> 1989 </year>
Reference-contexts: A recovery point is established by storing recovery data, information which describes the state of the system, as a part of normal processing. When an error is detected, the system is returned to the recovery point by reinstating the recovery data <ref> [Randell78, Stone89] </ref>. Previously completed work which is undone as a result of moving backward to a recovery point must be redone. Backward error recovery does not rely upon the type of error or the errors context in removing the error from the system. Thus, context-free error recovery is possible. <p> When recovery is performed, the effects of the communication are undone, requiring recovery of the other processes involved in the communication. An illustration of this problem, taken from <ref> [Stone89] </ref>, is presented as Figure 6. Techniques such as conversations, which synchronize communicating processes, are known methods of avoiding the domino effect [Randell75]. A variety of backward error recovery techniques exist, all of which introduce varying degrees of overhead.
Reference: [Stodolsky93] <author> Daniel Stodolsky, Garth Gibson, Mark Holland, </author> <title> Parity logging: overcoming the small write problem in redundant disk arrays. </title> <booktitle> In Proceedings of the 20th Annual International Symposium on Computer Architecture, </booktitle> <address> San Diego CA, </address> <month> May </month> <year> 1993, </year> <pages> pp 64-75. </pages>
Reference-contexts: This property limits the scope of modifications to an existing code base, thereby restricting a designers ability to explore the design space, confining experimentation to limited departures from the current code structure. Finally, researchers are investigating more aggressive redundant disk array architectures to boost performance <ref> [Bhide92, Blaum94, Cao93, Menon93, Stodolsky93, Holland94] </ref>. The acceptance of these proposals is put at risk due to their further increases in the complexity of error handling and the difficulty of modifying existing code structure.
Reference: [Verhofstad78] <author> Joost S. M. Verhofstad, </author> <title> Recovery techniques for database systems. </title> <journal> ACM Computing Surveys, </journal> <volume> Vol. 10, No. 2, </volume> <month> June </month> <year> 1978, </year> <pages> pp. 167-195. </pages>
Reference-contexts: Audit Trails Finally, audit trail, also known as logging or journaling, techniques provide the ability to record a subset of the system state but, unlike recovery cache techniques, do not require foreknowledge of the state which will be changed by an operation <ref> [Bjork75, Verhofstad78, Gray81] </ref>. Instead, all changes to the system state are recorded in stable storage. Recovery is performed by applying the inversion of these records in LIFO fashion, thereby removing state changes. As inverted records are applied, work is undone.
References-found: 34

