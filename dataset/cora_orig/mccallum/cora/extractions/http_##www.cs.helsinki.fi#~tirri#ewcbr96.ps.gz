URL: http://www.cs.helsinki.fi/~tirri/ewcbr96.ps.gz
Refering-URL: http://www.cs.helsinki.fi/~tirri/publications.html
Root-URL: 
Phone: 26,  
Title: A Bayesian Framework for Case-Based Reasoning  
Author: Henry Tirri, Petri Kontkanen, and Petri Myllymaki 
Date: November 1996).  
Note: To appear in: Proceedings of the 3rd European Workshop on Case-Based Reasoning (Lausanne, Switzerland,  
Address: P.O.Box  FIN-00014 University of Helsinki, Finland  
Affiliation: Complex Systems Computation Group (CoSCo)  Department of Computer Science  
Abstract: In this paper we present a probabilistic framework for case-based reasoning in data-intensive domains, where only weak prior knowledge is available. In such a probabilistic viewpoint the attributes are interpreted as random variables, and the case base is used to approximate the underlying joint probability distribution of the attributes. Consequently structural case adaptation (and parameter adjustment in particular) can be viewed as prediction based on the full probability model constructed from the case history. The methodology addresses several problems encountered in building case-based reasoning systems. It provides a computationally efficient structural adaptation algorithm, avoids over-fitting by using Bayesian model selection and uses directly probabilities as measures of similarity. The methodology described has been implemented in the D-SIDE software package, and the approach is validated by presenting empirical results of the method's classification prediction performance for a set of public domain data sets.
Abstract-found: 1
Intro-found: 1
Reference: 1. <author> D. Aha, D. Kibler, and M. Albert. </author> <title> Instance-based learning algorithms. </title> <journal> Machine Learning, </journal> <volume> 6 </volume> <pages> 37-66, </pages> <year> 1991. </year>
Reference-contexts: MSG in [12] 75.5 RBF in [28] 75.7 NBCFSS in [12] 76.0 C4.5 in [33] 76.0 SMART in [28] 76.8 D-SIDE 77.3 Linear discr. in [28] 77.5 DIPOL92 in [28] 77.6 Logistic discr. in [28] 77.7 30 35 40 45 50 55 Success rate (% correct) PRIMARY TUMOR C4 in <ref> [1] </ref> 37.8 M [1] in [4] 41.4 Human expert in [22] 42.0 Assistant in [22] 44.0 D-SIDE 50.4 Naive Bayes in [21] 51.0 Successive Bayes in [21] 51.7 75 80 85 90 Success rate (% correct) HEPATITIS C4.5 in [33] 80.9 Assistant in [15] 83.0 Naive Bayes in [15] 84.0 1Rw <p> 75.5 RBF in [28] 75.7 NBCFSS in [12] 76.0 C4.5 in [33] 76.0 SMART in [28] 76.8 D-SIDE 77.3 Linear discr. in [28] 77.5 DIPOL92 in [28] 77.6 Logistic discr. in [28] 77.7 30 35 40 45 50 55 Success rate (% correct) PRIMARY TUMOR C4 in <ref> [1] </ref> 37.8 M [1] in [4] 41.4 Human expert in [22] 42.0 Assistant in [22] 44.0 D-SIDE 50.4 Naive Bayes in [21] 51.0 Successive Bayes in [21] 51.7 75 80 85 90 Success rate (% correct) HEPATITIS C4.5 in [33] 80.9 Assistant in [15] 83.0 Naive Bayes in [15] 84.0 1Rw in [15] 84.5
Reference: 2. <author> R. Barletta. </author> <title> An introduction to case-based reasoning. </title> <journal> AI Expert, </journal> <pages> pages 42-49, </pages> <year> 1991. </year>
Reference-contexts: However, while CBR has its roots in cognitive modeling [34, 20], from an engineering point of view it can be seen as an attempt to address many of the problems recognized in the design of traditional knowledge-intensive systems, in particular the brittleness problem <ref> [2] </ref>. Consequently, one of the advocated potential advantages of case-based reasoning has been the fact that CBR systems can be built in cases where only weak prior domain knowledge is available [39], and the "top-down" approaches with explicit model representations cannot be applied.
Reference: 3. <author> J.M. Bernardo and A.F.M Smith. </author> <title> Bayesian theory. </title> <publisher> John Wiley, </publisher> <year> 1994. </year>
Reference-contexts: The Bayesian predictive inference (see e.g. <ref> [3] </ref>) aims at predicting future (yet unobserved) quantities by means of already observed quantities.
Reference: 4. <author> B. Cestnik and I. Bratko. </author> <title> On estimating probabilities in tree pruning. </title> <editor> In Y. Kodratoff, editor, </editor> <booktitle> Machine Learning EWSL-91, </booktitle> <pages> pages 138-150. </pages> <publisher> Springer-Verlag, </publisher> <year> 1991. </year>
Reference-contexts: in [28] 75.7 NBCFSS in [12] 76.0 C4.5 in [33] 76.0 SMART in [28] 76.8 D-SIDE 77.3 Linear discr. in [28] 77.5 DIPOL92 in [28] 77.6 Logistic discr. in [28] 77.7 30 35 40 45 50 55 Success rate (% correct) PRIMARY TUMOR C4 in [1] 37.8 M [1] in <ref> [4] </ref> 41.4 Human expert in [22] 42.0 Assistant in [22] 44.0 D-SIDE 50.4 Naive Bayes in [21] 51.0 Successive Bayes in [21] 51.7 75 80 85 90 Success rate (% correct) HEPATITIS C4.5 in [33] 80.9 Assistant in [15] 83.0 Naive Bayes in [15] 84.0 1Rw in [15] 84.5 D-SIDE 88.0 <p> [15] 65.6 Backprop in [15] 71.5 Linear discr. in [15] 71.6 C4 in [15] 72.0 RMHC-PF1 in [36] 72.3 1Rw in [15] 72.7 CN2 in [15] 73.0 D-SIDE 76.6 CART in [15] 77.1 Assistant in [15] 78.0 Successive Bayes in [21] 78.4 Naive Bayes in [21] 79.2 M (12) in <ref> [4] </ref> 80.0 Success rate (% correct) IRIS MC1 in [36] 93.5 RMHC-PF1 in [36] 94.7 Flexible Bayes in [16] 95.3 CART in [16] 95.3 C4.5 in [16] 95.3 K-NN in [15] 96.0 Naive Bayes in [16] 96.0 Backprop. in [15] 96.7 Quadratic discr. in [15] 97.3 Linear discr. in [15] 98.0
Reference: 5. <author> P. Cheeseman, J. Kelly, M. Self, J. Stutz, W. Taylor, and D. Freeman. </author> <title> Autoclass: A Bayesian classification system. </title> <booktitle> In Proceedings of the Fifth International Conference on Machine Learning, </booktitle> <pages> pages 54-64, </pages> <address> Ann Arbor, </address> <month> June </month> <year> 1988. </year>
Reference-contexts: This combination of probabilistic finite mixture models with Bayesian model selection is akin to the approach adopted in the Autoclass system <ref> [5] </ref> with the notable difference in our focus to perform adaptation (prediction) rather than exploratory latent class analysis. In this paper we describe a methodology for probabilistic case-based reasoning with discrete attribute values.
Reference: 6. <author> D.M. Chickering and D. Heckerman. </author> <title> Efficient approximations for the marginal likelihood of incomplete data given a Bayesian network. </title> <type> Technical Report MSR-TR-96-08, </type> <institution> Microsoft Research, Advanced Technology Division, </institution> <year> 1996. </year>
Reference-contexts: Laplace's method [18]. The quality of the approximations has been tested empirically in the finite mixture framework by using both synthetic <ref> [23, 6] </ref> and natural [24] data. 4.2 Determining the model class parameters Let us assume that the model class M K is fixed.
Reference: 7. <author> W. Clancey. </author> <title> Heuristic classification. </title> <journal> Artificial Intelligence, </journal> <volume> 27 </volume> <pages> 289-350, </pages> <year> 1985. </year>
Reference-contexts: 1 Introduction In principle any computational system capable of reasoning can be placed somewhere in a spectrum based on the "knowledge-intensiveness" of the underlying approach. At one end of the spectrum are the systems relying on deep causal models <ref> [7] </ref>, closely followed by the classic Knowledge-Based Systems (KBS). At the opposite end of the spectrum are the learning systems with weak prior domain knowledge which base their reasoning on models built from data [8].
Reference: 8. <author> T. Cover and P. Hart. </author> <title> Nearest neighbor pattern classification. </title> <journal> IEEE Transactions on Information Theory, </journal> <volume> 13 </volume> <pages> 21-27, </pages> <year> 1967. </year>
Reference-contexts: At one end of the spectrum are the systems relying on deep causal models [7], closely followed by the classic Knowledge-Based Systems (KBS). At the opposite end of the spectrum are the learning systems with weak prior domain knowledge which base their reasoning on models built from data <ref> [8] </ref>. Along this spectrum the reasoning process itself gradually changes from the deductive reasoning of knowledge-intensive approaches to inductive reasoning of data-intensive approaches, i.e., from logical inference to statistical inference.
Reference: 9. <author> M.H. </author> <title> DeGroot. Optimal statistical decisions. </title> <publisher> McGraw-Hill, </publisher> <year> 1970. </year>
Reference-contexts: 1 ; : : :; K ) k=1 i=1 with density P (fi) = K X k K Y k 1 ( k ) K Y m Y n i X kil ) l=1 kil 1 ( kil ) : (7) The family of Dirichlet densities is conjugate (see e.g. <ref> [9] </ref>) to the family of mul-tinomials, i.e. the functional form of parameter distribution remains invariant in the prior-to-posterior transformation, thus the choice of Dirichlet priors is justified. The likelihood P (Djfi) has two different forms depending on whether we know to which cluster each data vector belongs to or not.
Reference: 10. <author> A.P. Dempster, N.M. Laird, and D.B. Rubin. </author> <title> Maximum likelihood from incomplete data via the EM algorithm. </title> <journal> Journal of the Royal Statistical Society, Series B, </journal> <volume> 39(1) </volume> <pages> 1-38, </pages> <year> 1977. </year>
Reference-contexts: The usual approach is to regard Z as missing data and estimate it iteratively [27]. This estimation can be performed by using the Expectation-Maximization (EM) algorithm <ref> [10] </ref>. The EM algorithm is an iterative algorithm, which monotonically increases the expected value of the posterior corresponding to incomplete data. It consists of two steps: Expectation (E) and Maximization (M).
Reference: 11. <author> B.S. </author> <title> Everitt and D.J. Hand. Finite Mixture Distributions. </title> <publisher> Chapman and Hall, </publisher> <address> London, </address> <year> 1981. </year>
Reference-contexts: In fact, construction of prototypical structures such as Generalized Episodes by grouping cases sharing similar properties [26] can be understood as approximations to such simpler mixture structures <ref> [11, 38] </ref>. Thus the probabilistic approach offers also a natural way of structuring the case storage 1 . There are several advantages of using the above probabilistic view of CBR. <p> Let us now assume that we are given a clustering of the problem domain, i.e., the data instantiation space is partitioned into K groups of vectors. In finite mixture models <ref> [11, 38] </ref> the problem domain probability distribution is approximated as a weighted sum of mixture distributions: P ( ~ d) = k=1 where the values of the discrete clustering random variable Y correspond to the separate clusters of the instantiation space, and each mixture distribution P ( ~ djY = y
Reference: 12. <author> N. Friedman and M. Goldszmidt. </author> <title> Building classifiers using Bayesian networks. </title> <booktitle> In Proceedings of AAAI-96 (to appear), </booktitle> <year> 1996. </year>
Reference-contexts: AUSTRALIAN Flexible Bayes in [16] 78.3 Quadratic discr. in [28] 79.3 CN2 in [28] 79.6 ALLOC80 in [28] 79.9 LVQ in [28] 80.3 NewID in [28] 81.9 AC2 in [28] 81.9 Bayes tree in [28] 82.9 SMART in [28] 84.2 Backprop in [28] 84.6 IndCART in [28] 84.8 TAN in <ref> [12] </ref> 85.1 CASTLE in [28] 85.2 CART in [28] 85.5 RBF in [28] 85.5 DIPOL92 in [28] 85.9 Linear discr. in [28] 85.9 Logistic discr. in [28] 85.9 C4.5 in [33] 86.0 Naive Bayes in [13] 86.2 BNG in [12] 86.2 ITrule in [28] 86.3 NBCFSS in [12] 86.7 MS in <p> 84.2 Backprop in [28] 84.6 IndCART in [28] 84.8 TAN in <ref> [12] </ref> 85.1 CASTLE in [28] 85.2 CART in [28] 85.5 RBF in [28] 85.5 DIPOL92 in [28] 85.9 Linear discr. in [28] 85.9 Logistic discr. in [28] 85.9 C4.5 in [33] 86.0 Naive Bayes in [13] 86.2 BNG in [12] 86.2 ITrule in [28] 86.3 NBCFSS in [12] 86.7 MS in [12] 86.8 D-SIDE 87.2 60 65 70 75 80 Success rate (% correct) DIABETES K-NN in [28] 67.6 ALLOC80 in [28] 69.9 CN2 in [28] 71.1 NewID in [28] 71.1 AC2 in [28] 72.4 LVQ in [28] 72.8 Bayes <p> 84.8 TAN in <ref> [12] </ref> 85.1 CASTLE in [28] 85.2 CART in [28] 85.5 RBF in [28] 85.5 DIPOL92 in [28] 85.9 Linear discr. in [28] 85.9 Logistic discr. in [28] 85.9 C4.5 in [33] 86.0 Naive Bayes in [13] 86.2 BNG in [12] 86.2 ITrule in [28] 86.3 NBCFSS in [12] 86.7 MS in [12] 86.8 D-SIDE 87.2 60 65 70 75 80 Success rate (% correct) DIABETES K-NN in [28] 67.6 ALLOC80 in [28] 69.9 CN2 in [28] 71.1 NewID in [28] 71.1 AC2 in [28] 72.4 LVQ in [28] 72.8 Bayes tree in [28] 72.9 IndCART in [28] 72.9 <p> 85.1 CASTLE in [28] 85.2 CART in [28] 85.5 RBF in [28] 85.5 DIPOL92 in [28] 85.9 Linear discr. in [28] 85.9 Logistic discr. in [28] 85.9 C4.5 in [33] 86.0 Naive Bayes in [13] 86.2 BNG in <ref> [12] </ref> 86.2 ITrule in [28] 86.3 NBCFSS in [12] 86.7 MS in [12] 86.8 D-SIDE 87.2 60 65 70 75 80 Success rate (% correct) DIABETES K-NN in [28] 67.6 ALLOC80 in [28] 69.9 CN2 in [28] 71.1 NewID in [28] 71.1 AC2 in [28] 72.4 LVQ in [28] 72.8 Bayes tree in [28] 72.9 IndCART in [28] 72.9 Quadratic discr. in [28] <p> AC2 in [28] 72.4 LVQ in [28] 72.8 Bayes tree in [28] 72.9 IndCART in [28] 72.9 Quadratic discr. in [28] 73.8 Flexible Bayes in [16] 73.9 CASTLE in [28] 74.2 CART in [28] 74.5 Cal5 in [28] 75.0 Backprop in [28] 75.2 ITrule in [28] 75.5 Naive Bayes in <ref> [12] </ref> 75.5 MSG in [12] 75.5 RBF in [28] 75.7 NBCFSS in [12] 76.0 C4.5 in [33] 76.0 SMART in [28] 76.8 D-SIDE 77.3 Linear discr. in [28] 77.5 DIPOL92 in [28] 77.6 Logistic discr. in [28] 77.7 30 35 40 45 50 55 Success rate (% correct) PRIMARY TUMOR C4 <p> LVQ in [28] 72.8 Bayes tree in [28] 72.9 IndCART in [28] 72.9 Quadratic discr. in [28] 73.8 Flexible Bayes in [16] 73.9 CASTLE in [28] 74.2 CART in [28] 74.5 Cal5 in [28] 75.0 Backprop in [28] 75.2 ITrule in [28] 75.5 Naive Bayes in <ref> [12] </ref> 75.5 MSG in [12] 75.5 RBF in [28] 75.7 NBCFSS in [12] 76.0 C4.5 in [33] 76.0 SMART in [28] 76.8 D-SIDE 77.3 Linear discr. in [28] 77.5 DIPOL92 in [28] 77.6 Logistic discr. in [28] 77.7 30 35 40 45 50 55 Success rate (% correct) PRIMARY TUMOR C4 in [1] 37.8 M <p> 72.9 IndCART in [28] 72.9 Quadratic discr. in [28] 73.8 Flexible Bayes in [16] 73.9 CASTLE in [28] 74.2 CART in [28] 74.5 Cal5 in [28] 75.0 Backprop in [28] 75.2 ITrule in [28] 75.5 Naive Bayes in <ref> [12] </ref> 75.5 MSG in [12] 75.5 RBF in [28] 75.7 NBCFSS in [12] 76.0 C4.5 in [33] 76.0 SMART in [28] 76.8 D-SIDE 77.3 Linear discr. in [28] 77.5 DIPOL92 in [28] 77.6 Logistic discr. in [28] 77.7 30 35 40 45 50 55 Success rate (% correct) PRIMARY TUMOR C4 in [1] 37.8 M [1] in [4] 41.4 Human expert in [22] <p> 96.0 Backprop. in [15] 96.7 Quadratic discr. in [15] 97.3 Linear discr. in [15] 98.0 D-SIDE 98.0 70 75 80 85 Success rate (% correct) HEART DISEASE 1Rw in [15] 76.6 K-NN in [15] 79.2 Flexible Bayes in [16] 80.0 Backprop. in [15] 80.6 MC1 in [36] 80.7 NBCFSS in <ref> [12] </ref> 81.9 BNG in [12] 82.2 RMHC-P in [36] 82.3 MSG in [12] 83.0 Naive Bayes in [16] 83.3 D-SIDE 84.8 65 70 75 80 85 Success rate (% correct) LYMPHOGRAPHY K-NN in [15] 72.9 C4.5 in [33] 77.0 NBCFSS in [12] 77.7 ID3 in [15] 78.4 Assistant in [15] 79.0 <p> 96.7 Quadratic discr. in [15] 97.3 Linear discr. in [15] 98.0 D-SIDE 98.0 70 75 80 85 Success rate (% correct) HEART DISEASE 1Rw in [15] 76.6 K-NN in [15] 79.2 Flexible Bayes in [16] 80.0 Backprop. in [15] 80.6 MC1 in [36] 80.7 NBCFSS in <ref> [12] </ref> 81.9 BNG in [12] 82.2 RMHC-P in [36] 82.3 MSG in [12] 83.0 Naive Bayes in [16] 83.3 D-SIDE 84.8 65 70 75 80 85 Success rate (% correct) LYMPHOGRAPHY K-NN in [15] 72.9 C4.5 in [33] 77.0 NBCFSS in [12] 77.7 ID3 in [15] 78.4 Assistant in [15] 79.0 MSG in [12] 80.4 <p> in [15] 98.0 D-SIDE 98.0 70 75 80 85 Success rate (% correct) HEART DISEASE 1Rw in [15] 76.6 K-NN in [15] 79.2 Flexible Bayes in [16] 80.0 Backprop. in [15] 80.6 MC1 in [36] 80.7 NBCFSS in <ref> [12] </ref> 81.9 BNG in [12] 82.2 RMHC-P in [36] 82.3 MSG in [12] 83.0 Naive Bayes in [16] 83.3 D-SIDE 84.8 65 70 75 80 85 Success rate (% correct) LYMPHOGRAPHY K-NN in [15] 72.9 C4.5 in [33] 77.0 NBCFSS in [12] 77.7 ID3 in [15] 78.4 Assistant in [15] 79.0 MSG in [12] 80.4 TAN in [12] 85.0 Naive Bayes in [15] <p> in [15] 80.6 MC1 in [36] 80.7 NBCFSS in <ref> [12] </ref> 81.9 BNG in [12] 82.2 RMHC-P in [36] 82.3 MSG in [12] 83.0 Naive Bayes in [16] 83.3 D-SIDE 84.8 65 70 75 80 85 Success rate (% correct) LYMPHOGRAPHY K-NN in [15] 72.9 C4.5 in [33] 77.0 NBCFSS in [12] 77.7 ID3 in [15] 78.4 Assistant in [15] 79.0 MSG in [12] 80.4 TAN in [12] 85.0 Naive Bayes in [15] 85.1 D-SIDE 86.6 Fig. 2. Experimental results on the DNA, Breast cancer, Iris, Heart disease and Lymphography databases. Table 1. <p> in <ref> [12] </ref> 82.2 RMHC-P in [36] 82.3 MSG in [12] 83.0 Naive Bayes in [16] 83.3 D-SIDE 84.8 65 70 75 80 85 Success rate (% correct) LYMPHOGRAPHY K-NN in [15] 72.9 C4.5 in [33] 77.0 NBCFSS in [12] 77.7 ID3 in [15] 78.4 Assistant in [15] 79.0 MSG in [12] 80.4 TAN in [12] 85.0 Naive Bayes in [15] 85.1 D-SIDE 86.6 Fig. 2. Experimental results on the DNA, Breast cancer, Iris, Heart disease and Lymphography databases. Table 1. The datasets and testing methods used in our experiments. <p> in [36] 82.3 MSG in <ref> [12] </ref> 83.0 Naive Bayes in [16] 83.3 D-SIDE 84.8 65 70 75 80 85 Success rate (% correct) LYMPHOGRAPHY K-NN in [15] 72.9 C4.5 in [33] 77.0 NBCFSS in [12] 77.7 ID3 in [15] 78.4 Assistant in [15] 79.0 MSG in [12] 80.4 TAN in [12] 85.0 Naive Bayes in [15] 85.1 D-SIDE 86.6 Fig. 2. Experimental results on the DNA, Breast cancer, Iris, Heart disease and Lymphography databases. Table 1. The datasets and testing methods used in our experiments.
Reference: 13. <author> N. Friedman and M. Goldszmidt. </author> <title> Discretizing continuous attributes while learning Bayesian networks. </title> <editor> In L. Saitta, editor, </editor> <booktitle> Machine Learning: Proceedings of the Thirteenth International Conference (to appear). </booktitle> <publisher> Morgan Kaufmann Publishers, </publisher> <year> 1996. </year>
Reference-contexts: 82.9 SMART in [28] 84.2 Backprop in [28] 84.6 IndCART in [28] 84.8 TAN in [12] 85.1 CASTLE in [28] 85.2 CART in [28] 85.5 RBF in [28] 85.5 DIPOL92 in [28] 85.9 Linear discr. in [28] 85.9 Logistic discr. in [28] 85.9 C4.5 in [33] 86.0 Naive Bayes in <ref> [13] </ref> 86.2 BNG in [12] 86.2 ITrule in [28] 86.3 NBCFSS in [12] 86.7 MS in [12] 86.8 D-SIDE 87.2 60 65 70 75 80 Success rate (% correct) DIABETES K-NN in [28] 67.6 ALLOC80 in [28] 69.9 CN2 in [28] 71.1 NewID in [28] 71.1 AC2 in [28] 72.4 LVQ <p> correct) HEPATITIS C4.5 in [33] 80.9 Assistant in [15] 83.0 Naive Bayes in [15] 84.0 1Rw in [15] 84.5 D-SIDE 88.0 30 35 40 45 50 55 60 65 70 75 80 85 Success rate (% correct) GLASS 1Rw in [15] 62.2 Flexible Bayes in [16] 66.2 Naive Bayes in <ref> [13] </ref> 71.5 C4.5 in [33] 71.6 D-SIDE 87.4 Fig. 1.
Reference: 14. <author> A. Gelman, J. Carlin, H. Stern, and D. Rubin. </author> <title> Bayesian Data Analysis. </title> <publisher> Chapman & Hall, </publisher> <year> 1995. </year>
Reference-contexts: There are several advantages of using the above probabilistic view of CBR. If a good approximative representation of the problem domain distribution can be found, we can use a minimum risk Bayes decision rule in the adaptation phase <ref> [14] </ref>. It should also be observed that although the experimental results presented in Section 5 are only for classification tasks, in our approach a full probability model is constructed. <p> As discussed above, to perform successful adaptation in an application with only weak domain knowledge, a model has to be extracted from the (noisy) set of cases. For constructing the component distributions from the case base, we have adopted the Bayesian approach (see e.g., <ref> [14] </ref>) which allows us to make a tradeoff between the complexity of our distribution structure and fit to the case data, thus resolving the overfitting problem.
Reference: 15. <author> R.C. Holte. </author> <title> Very simple classification rules perform well on most commonly used datasets. </title> <journal> Machine Learning, </journal> <volume> 11 </volume> <pages> 63-91, </pages> <year> 1993. </year>
Reference-contexts: rate (% correct) PRIMARY TUMOR C4 in [1] 37.8 M [1] in [4] 41.4 Human expert in [22] 42.0 Assistant in [22] 44.0 D-SIDE 50.4 Naive Bayes in [21] 51.0 Successive Bayes in [21] 51.7 75 80 85 90 Success rate (% correct) HEPATITIS C4.5 in [33] 80.9 Assistant in <ref> [15] </ref> 83.0 Naive Bayes in [15] 84.0 1Rw in [15] 84.5 D-SIDE 88.0 30 35 40 45 50 55 60 65 70 75 80 85 Success rate (% correct) GLASS 1Rw in [15] 62.2 Flexible Bayes in [16] 66.2 Naive Bayes in [13] 71.5 C4.5 in [33] 71.6 D-SIDE 87.4 Fig. <p> C4 in [1] 37.8 M [1] in [4] 41.4 Human expert in [22] 42.0 Assistant in [22] 44.0 D-SIDE 50.4 Naive Bayes in [21] 51.0 Successive Bayes in [21] 51.7 75 80 85 90 Success rate (% correct) HEPATITIS C4.5 in [33] 80.9 Assistant in <ref> [15] </ref> 83.0 Naive Bayes in [15] 84.0 1Rw in [15] 84.5 D-SIDE 88.0 30 35 40 45 50 55 60 65 70 75 80 85 Success rate (% correct) GLASS 1Rw in [15] 62.2 Flexible Bayes in [16] 66.2 Naive Bayes in [13] 71.5 C4.5 in [33] 71.6 D-SIDE 87.4 Fig. 1. <p> M [1] in [4] 41.4 Human expert in [22] 42.0 Assistant in [22] 44.0 D-SIDE 50.4 Naive Bayes in [21] 51.0 Successive Bayes in [21] 51.7 75 80 85 90 Success rate (% correct) HEPATITIS C4.5 in [33] 80.9 Assistant in <ref> [15] </ref> 83.0 Naive Bayes in [15] 84.0 1Rw in [15] 84.5 D-SIDE 88.0 30 35 40 45 50 55 60 65 70 75 80 85 Success rate (% correct) GLASS 1Rw in [15] 62.2 Flexible Bayes in [16] 66.2 Naive Bayes in [13] 71.5 C4.5 in [33] 71.6 D-SIDE 87.4 Fig. 1. <p> in [21] 51.7 75 80 85 90 Success rate (% correct) HEPATITIS C4.5 in [33] 80.9 Assistant in <ref> [15] </ref> 83.0 Naive Bayes in [15] 84.0 1Rw in [15] 84.5 D-SIDE 88.0 30 35 40 45 50 55 60 65 70 75 80 85 Success rate (% correct) GLASS 1Rw in [15] 62.2 Flexible Bayes in [16] 66.2 Naive Bayes in [13] 71.5 C4.5 in [33] 71.6 D-SIDE 87.4 Fig. 1. <p> 93.2 Logistic discr. in [28] 93.9 Quadratic discr. in [28] 94.1 Linear discr. in [28] 94.1 ALLOC80 in [28] 94.3 DIPOL92 in [28] 95.2 RBF in [28] 95.9 D-SIDE 97.0 50 55 60 65 70 75 80 Success rate (% correct) BREAST CANCER Human expert in [22] 64.0 K-NN in <ref> [15] </ref> 65.3 Quadratic discr. in [15] 65.6 Backprop in [15] 71.5 Linear discr. in [15] 71.6 C4 in [15] 72.0 RMHC-PF1 in [36] 72.3 1Rw in [15] 72.7 CN2 in [15] 73.0 D-SIDE 76.6 CART in [15] 77.1 Assistant in [15] 78.0 Successive Bayes in [21] 78.4 Naive Bayes in [21] <p> 93.9 Quadratic discr. in [28] 94.1 Linear discr. in [28] 94.1 ALLOC80 in [28] 94.3 DIPOL92 in [28] 95.2 RBF in [28] 95.9 D-SIDE 97.0 50 55 60 65 70 75 80 Success rate (% correct) BREAST CANCER Human expert in [22] 64.0 K-NN in <ref> [15] </ref> 65.3 Quadratic discr. in [15] 65.6 Backprop in [15] 71.5 Linear discr. in [15] 71.6 C4 in [15] 72.0 RMHC-PF1 in [36] 72.3 1Rw in [15] 72.7 CN2 in [15] 73.0 D-SIDE 76.6 CART in [15] 77.1 Assistant in [15] 78.0 Successive Bayes in [21] 78.4 Naive Bayes in [21] 79.2 M (12) in [4] <p> [28] 94.1 Linear discr. in [28] 94.1 ALLOC80 in [28] 94.3 DIPOL92 in [28] 95.2 RBF in [28] 95.9 D-SIDE 97.0 50 55 60 65 70 75 80 Success rate (% correct) BREAST CANCER Human expert in [22] 64.0 K-NN in <ref> [15] </ref> 65.3 Quadratic discr. in [15] 65.6 Backprop in [15] 71.5 Linear discr. in [15] 71.6 C4 in [15] 72.0 RMHC-PF1 in [36] 72.3 1Rw in [15] 72.7 CN2 in [15] 73.0 D-SIDE 76.6 CART in [15] 77.1 Assistant in [15] 78.0 Successive Bayes in [21] 78.4 Naive Bayes in [21] 79.2 M (12) in [4] 80.0 Success rate (% <p> [28] 94.1 ALLOC80 in [28] 94.3 DIPOL92 in [28] 95.2 RBF in [28] 95.9 D-SIDE 97.0 50 55 60 65 70 75 80 Success rate (% correct) BREAST CANCER Human expert in [22] 64.0 K-NN in <ref> [15] </ref> 65.3 Quadratic discr. in [15] 65.6 Backprop in [15] 71.5 Linear discr. in [15] 71.6 C4 in [15] 72.0 RMHC-PF1 in [36] 72.3 1Rw in [15] 72.7 CN2 in [15] 73.0 D-SIDE 76.6 CART in [15] 77.1 Assistant in [15] 78.0 Successive Bayes in [21] 78.4 Naive Bayes in [21] 79.2 M (12) in [4] 80.0 Success rate (% correct) IRIS MC1 in [36] <p> [28] 94.3 DIPOL92 in [28] 95.2 RBF in [28] 95.9 D-SIDE 97.0 50 55 60 65 70 75 80 Success rate (% correct) BREAST CANCER Human expert in [22] 64.0 K-NN in <ref> [15] </ref> 65.3 Quadratic discr. in [15] 65.6 Backprop in [15] 71.5 Linear discr. in [15] 71.6 C4 in [15] 72.0 RMHC-PF1 in [36] 72.3 1Rw in [15] 72.7 CN2 in [15] 73.0 D-SIDE 76.6 CART in [15] 77.1 Assistant in [15] 78.0 Successive Bayes in [21] 78.4 Naive Bayes in [21] 79.2 M (12) in [4] 80.0 Success rate (% correct) IRIS MC1 in [36] 93.5 RMHC-PF1 in [36] <p> [28] 95.9 D-SIDE 97.0 50 55 60 65 70 75 80 Success rate (% correct) BREAST CANCER Human expert in [22] 64.0 K-NN in <ref> [15] </ref> 65.3 Quadratic discr. in [15] 65.6 Backprop in [15] 71.5 Linear discr. in [15] 71.6 C4 in [15] 72.0 RMHC-PF1 in [36] 72.3 1Rw in [15] 72.7 CN2 in [15] 73.0 D-SIDE 76.6 CART in [15] 77.1 Assistant in [15] 78.0 Successive Bayes in [21] 78.4 Naive Bayes in [21] 79.2 M (12) in [4] 80.0 Success rate (% correct) IRIS MC1 in [36] 93.5 RMHC-PF1 in [36] 94.7 Flexible Bayes in [16] 95.3 CART in <p> 50 55 60 65 70 75 80 Success rate (% correct) BREAST CANCER Human expert in [22] 64.0 K-NN in <ref> [15] </ref> 65.3 Quadratic discr. in [15] 65.6 Backprop in [15] 71.5 Linear discr. in [15] 71.6 C4 in [15] 72.0 RMHC-PF1 in [36] 72.3 1Rw in [15] 72.7 CN2 in [15] 73.0 D-SIDE 76.6 CART in [15] 77.1 Assistant in [15] 78.0 Successive Bayes in [21] 78.4 Naive Bayes in [21] 79.2 M (12) in [4] 80.0 Success rate (% correct) IRIS MC1 in [36] 93.5 RMHC-PF1 in [36] 94.7 Flexible Bayes in [16] 95.3 CART in [16] 95.3 C4.5 in <p> 80 Success rate (% correct) BREAST CANCER Human expert in [22] 64.0 K-NN in <ref> [15] </ref> 65.3 Quadratic discr. in [15] 65.6 Backprop in [15] 71.5 Linear discr. in [15] 71.6 C4 in [15] 72.0 RMHC-PF1 in [36] 72.3 1Rw in [15] 72.7 CN2 in [15] 73.0 D-SIDE 76.6 CART in [15] 77.1 Assistant in [15] 78.0 Successive Bayes in [21] 78.4 Naive Bayes in [21] 79.2 M (12) in [4] 80.0 Success rate (% correct) IRIS MC1 in [36] 93.5 RMHC-PF1 in [36] 94.7 Flexible Bayes in [16] 95.3 CART in [16] 95.3 C4.5 in [16] 95.3 K-NN in [15] 96.0 <p> correct) BREAST CANCER Human expert in [22] 64.0 K-NN in <ref> [15] </ref> 65.3 Quadratic discr. in [15] 65.6 Backprop in [15] 71.5 Linear discr. in [15] 71.6 C4 in [15] 72.0 RMHC-PF1 in [36] 72.3 1Rw in [15] 72.7 CN2 in [15] 73.0 D-SIDE 76.6 CART in [15] 77.1 Assistant in [15] 78.0 Successive Bayes in [21] 78.4 Naive Bayes in [21] 79.2 M (12) in [4] 80.0 Success rate (% correct) IRIS MC1 in [36] 93.5 RMHC-PF1 in [36] 94.7 Flexible Bayes in [16] 95.3 CART in [16] 95.3 C4.5 in [16] 95.3 K-NN in [15] 96.0 Naive Bayes in [16] <p> in <ref> [15] </ref> 77.1 Assistant in [15] 78.0 Successive Bayes in [21] 78.4 Naive Bayes in [21] 79.2 M (12) in [4] 80.0 Success rate (% correct) IRIS MC1 in [36] 93.5 RMHC-PF1 in [36] 94.7 Flexible Bayes in [16] 95.3 CART in [16] 95.3 C4.5 in [16] 95.3 K-NN in [15] 96.0 Naive Bayes in [16] 96.0 Backprop. in [15] 96.7 Quadratic discr. in [15] 97.3 Linear discr. in [15] 98.0 D-SIDE 98.0 70 75 80 85 Success rate (% correct) HEART DISEASE 1Rw in [15] 76.6 K-NN in [15] 79.2 Flexible Bayes in [16] 80.0 Backprop. in [15] 80.6 MC1 <p> in [21] 78.4 Naive Bayes in [21] 79.2 M (12) in [4] 80.0 Success rate (% correct) IRIS MC1 in [36] 93.5 RMHC-PF1 in [36] 94.7 Flexible Bayes in [16] 95.3 CART in [16] 95.3 C4.5 in [16] 95.3 K-NN in <ref> [15] </ref> 96.0 Naive Bayes in [16] 96.0 Backprop. in [15] 96.7 Quadratic discr. in [15] 97.3 Linear discr. in [15] 98.0 D-SIDE 98.0 70 75 80 85 Success rate (% correct) HEART DISEASE 1Rw in [15] 76.6 K-NN in [15] 79.2 Flexible Bayes in [16] 80.0 Backprop. in [15] 80.6 MC1 in [36] 80.7 NBCFSS in [12] 81.9 BNG in <p> in [21] 79.2 M (12) in [4] 80.0 Success rate (% correct) IRIS MC1 in [36] 93.5 RMHC-PF1 in [36] 94.7 Flexible Bayes in [16] 95.3 CART in [16] 95.3 C4.5 in [16] 95.3 K-NN in <ref> [15] </ref> 96.0 Naive Bayes in [16] 96.0 Backprop. in [15] 96.7 Quadratic discr. in [15] 97.3 Linear discr. in [15] 98.0 D-SIDE 98.0 70 75 80 85 Success rate (% correct) HEART DISEASE 1Rw in [15] 76.6 K-NN in [15] 79.2 Flexible Bayes in [16] 80.0 Backprop. in [15] 80.6 MC1 in [36] 80.7 NBCFSS in [12] 81.9 BNG in [12] 82.2 RMHC-P in [36] <p> in [4] 80.0 Success rate (% correct) IRIS MC1 in [36] 93.5 RMHC-PF1 in [36] 94.7 Flexible Bayes in [16] 95.3 CART in [16] 95.3 C4.5 in [16] 95.3 K-NN in <ref> [15] </ref> 96.0 Naive Bayes in [16] 96.0 Backprop. in [15] 96.7 Quadratic discr. in [15] 97.3 Linear discr. in [15] 98.0 D-SIDE 98.0 70 75 80 85 Success rate (% correct) HEART DISEASE 1Rw in [15] 76.6 K-NN in [15] 79.2 Flexible Bayes in [16] 80.0 Backprop. in [15] 80.6 MC1 in [36] 80.7 NBCFSS in [12] 81.9 BNG in [12] 82.2 RMHC-P in [36] 82.3 MSG in [12] 83.0 <p> Flexible Bayes in [16] 95.3 CART in [16] 95.3 C4.5 in [16] 95.3 K-NN in <ref> [15] </ref> 96.0 Naive Bayes in [16] 96.0 Backprop. in [15] 96.7 Quadratic discr. in [15] 97.3 Linear discr. in [15] 98.0 D-SIDE 98.0 70 75 80 85 Success rate (% correct) HEART DISEASE 1Rw in [15] 76.6 K-NN in [15] 79.2 Flexible Bayes in [16] 80.0 Backprop. in [15] 80.6 MC1 in [36] 80.7 NBCFSS in [12] 81.9 BNG in [12] 82.2 RMHC-P in [36] 82.3 MSG in [12] 83.0 Naive Bayes in [16] 83.3 D-SIDE 84.8 65 70 75 80 85 Success rate (% correct) <p> 95.3 CART in [16] 95.3 C4.5 in [16] 95.3 K-NN in <ref> [15] </ref> 96.0 Naive Bayes in [16] 96.0 Backprop. in [15] 96.7 Quadratic discr. in [15] 97.3 Linear discr. in [15] 98.0 D-SIDE 98.0 70 75 80 85 Success rate (% correct) HEART DISEASE 1Rw in [15] 76.6 K-NN in [15] 79.2 Flexible Bayes in [16] 80.0 Backprop. in [15] 80.6 MC1 in [36] 80.7 NBCFSS in [12] 81.9 BNG in [12] 82.2 RMHC-P in [36] 82.3 MSG in [12] 83.0 Naive Bayes in [16] 83.3 D-SIDE 84.8 65 70 75 80 85 Success rate (% correct) LYMPHOGRAPHY K-NN in [15] <p> K-NN in <ref> [15] </ref> 96.0 Naive Bayes in [16] 96.0 Backprop. in [15] 96.7 Quadratic discr. in [15] 97.3 Linear discr. in [15] 98.0 D-SIDE 98.0 70 75 80 85 Success rate (% correct) HEART DISEASE 1Rw in [15] 76.6 K-NN in [15] 79.2 Flexible Bayes in [16] 80.0 Backprop. in [15] 80.6 MC1 in [36] 80.7 NBCFSS in [12] 81.9 BNG in [12] 82.2 RMHC-P in [36] 82.3 MSG in [12] 83.0 Naive Bayes in [16] 83.3 D-SIDE 84.8 65 70 75 80 85 Success rate (% correct) LYMPHOGRAPHY K-NN in [15] 72.9 C4.5 in [33] 77.0 NBCFSS in [12] 77.7 <p> <ref> [15] </ref> 79.2 Flexible Bayes in [16] 80.0 Backprop. in [15] 80.6 MC1 in [36] 80.7 NBCFSS in [12] 81.9 BNG in [12] 82.2 RMHC-P in [36] 82.3 MSG in [12] 83.0 Naive Bayes in [16] 83.3 D-SIDE 84.8 65 70 75 80 85 Success rate (% correct) LYMPHOGRAPHY K-NN in [15] 72.9 C4.5 in [33] 77.0 NBCFSS in [12] 77.7 ID3 in [15] 78.4 Assistant in [15] 79.0 MSG in [12] 80.4 TAN in [12] 85.0 Naive Bayes in [15] 85.1 D-SIDE 86.6 Fig. 2. Experimental results on the DNA, Breast cancer, Iris, Heart disease and Lymphography databases. Table 1. <p> in [36] 80.7 NBCFSS in [12] 81.9 BNG in [12] 82.2 RMHC-P in [36] 82.3 MSG in [12] 83.0 Naive Bayes in [16] 83.3 D-SIDE 84.8 65 70 75 80 85 Success rate (% correct) LYMPHOGRAPHY K-NN in <ref> [15] </ref> 72.9 C4.5 in [33] 77.0 NBCFSS in [12] 77.7 ID3 in [15] 78.4 Assistant in [15] 79.0 MSG in [12] 80.4 TAN in [12] 85.0 Naive Bayes in [15] 85.1 D-SIDE 86.6 Fig. 2. Experimental results on the DNA, Breast cancer, Iris, Heart disease and Lymphography databases. Table 1. The datasets and testing methods used in our experiments. <p> in [12] 81.9 BNG in [12] 82.2 RMHC-P in [36] 82.3 MSG in [12] 83.0 Naive Bayes in [16] 83.3 D-SIDE 84.8 65 70 75 80 85 Success rate (% correct) LYMPHOGRAPHY K-NN in <ref> [15] </ref> 72.9 C4.5 in [33] 77.0 NBCFSS in [12] 77.7 ID3 in [15] 78.4 Assistant in [15] 79.0 MSG in [12] 80.4 TAN in [12] 85.0 Naive Bayes in [15] 85.1 D-SIDE 86.6 Fig. 2. Experimental results on the DNA, Breast cancer, Iris, Heart disease and Lymphography databases. Table 1. The datasets and testing methods used in our experiments. <p> [12] 83.0 Naive Bayes in [16] 83.3 D-SIDE 84.8 65 70 75 80 85 Success rate (% correct) LYMPHOGRAPHY K-NN in <ref> [15] </ref> 72.9 C4.5 in [33] 77.0 NBCFSS in [12] 77.7 ID3 in [15] 78.4 Assistant in [15] 79.0 MSG in [12] 80.4 TAN in [12] 85.0 Naive Bayes in [15] 85.1 D-SIDE 86.6 Fig. 2. Experimental results on the DNA, Breast cancer, Iris, Heart disease and Lymphography databases. Table 1. The datasets and testing methods used in our experiments.
Reference: 16. <author> G.H. John and P. Langley. </author> <title> Estimating continuous distributions in Bayesian classifiers. </title> <editor> In P. Besnard and S. Hanks, editors, </editor> <booktitle> Proceedings of the 11th Conference on Uncertainty in Artificial Intelligence, </booktitle> <pages> pages 338-345. </pages> <publisher> Morgan Kaufmann Publishers, </publisher> <year> 1995. </year>
Reference-contexts: the individual Dirichlet's and is obtained by setting ^ff k = N + k=1 k K f kil + kil 1 P n i : (8) Assuming that the values of the cluster indicators are part of the training data D results in the simple naive Bayes model (see e.g. <ref> [16] </ref>). If, however, we do not consider the values of cluster indicators to be known, the learning problem becomes much more complex. <p> it can clearly be seen that our approach performs well not only when compared to memory-based methods 4 A running Java TM demo of the D-SIDE software can be accessed through our WWW homepage at URL "http: //www.cs.Helsinki.FI/research/cosco/". 70 75 80 85 Success rate (% correct) AUSTRALIAN Flexible Bayes in <ref> [16] </ref> 78.3 Quadratic discr. in [28] 79.3 CN2 in [28] 79.6 ALLOC80 in [28] 79.9 LVQ in [28] 80.3 NewID in [28] 81.9 AC2 in [28] 81.9 Bayes tree in [28] 82.9 SMART in [28] 84.2 Backprop in [28] 84.6 IndCART in [28] 84.8 TAN in [12] 85.1 CASTLE in [28] <p> 65 70 75 80 Success rate (% correct) DIABETES K-NN in [28] 67.6 ALLOC80 in [28] 69.9 CN2 in [28] 71.1 NewID in [28] 71.1 AC2 in [28] 72.4 LVQ in [28] 72.8 Bayes tree in [28] 72.9 IndCART in [28] 72.9 Quadratic discr. in [28] 73.8 Flexible Bayes in <ref> [16] </ref> 73.9 CASTLE in [28] 74.2 CART in [28] 74.5 Cal5 in [28] 75.0 Backprop in [28] 75.2 ITrule in [28] 75.5 Naive Bayes in [12] 75.5 MSG in [12] 75.5 RBF in [28] 75.7 NBCFSS in [12] 76.0 C4.5 in [33] 76.0 SMART in [28] 76.8 D-SIDE 77.3 Linear discr. <p> 85 90 Success rate (% correct) HEPATITIS C4.5 in [33] 80.9 Assistant in [15] 83.0 Naive Bayes in [15] 84.0 1Rw in [15] 84.5 D-SIDE 88.0 30 35 40 45 50 55 60 65 70 75 80 85 Success rate (% correct) GLASS 1Rw in [15] 62.2 Flexible Bayes in <ref> [16] </ref> 66.2 Naive Bayes in [13] 71.5 C4.5 in [33] 71.6 D-SIDE 87.4 Fig. 1. <p> 72.3 1Rw in [15] 72.7 CN2 in [15] 73.0 D-SIDE 76.6 CART in [15] 77.1 Assistant in [15] 78.0 Successive Bayes in [21] 78.4 Naive Bayes in [21] 79.2 M (12) in [4] 80.0 Success rate (% correct) IRIS MC1 in [36] 93.5 RMHC-PF1 in [36] 94.7 Flexible Bayes in <ref> [16] </ref> 95.3 CART in [16] 95.3 C4.5 in [16] 95.3 K-NN in [15] 96.0 Naive Bayes in [16] 96.0 Backprop. in [15] 96.7 Quadratic discr. in [15] 97.3 Linear discr. in [15] 98.0 D-SIDE 98.0 70 75 80 85 Success rate (% correct) HEART DISEASE 1Rw in [15] 76.6 K-NN in <p> 72.7 CN2 in [15] 73.0 D-SIDE 76.6 CART in [15] 77.1 Assistant in [15] 78.0 Successive Bayes in [21] 78.4 Naive Bayes in [21] 79.2 M (12) in [4] 80.0 Success rate (% correct) IRIS MC1 in [36] 93.5 RMHC-PF1 in [36] 94.7 Flexible Bayes in <ref> [16] </ref> 95.3 CART in [16] 95.3 C4.5 in [16] 95.3 K-NN in [15] 96.0 Naive Bayes in [16] 96.0 Backprop. in [15] 96.7 Quadratic discr. in [15] 97.3 Linear discr. in [15] 98.0 D-SIDE 98.0 70 75 80 85 Success rate (% correct) HEART DISEASE 1Rw in [15] 76.6 K-NN in [15] 79.2 Flexible Bayes <p> 73.0 D-SIDE 76.6 CART in [15] 77.1 Assistant in [15] 78.0 Successive Bayes in [21] 78.4 Naive Bayes in [21] 79.2 M (12) in [4] 80.0 Success rate (% correct) IRIS MC1 in [36] 93.5 RMHC-PF1 in [36] 94.7 Flexible Bayes in <ref> [16] </ref> 95.3 CART in [16] 95.3 C4.5 in [16] 95.3 K-NN in [15] 96.0 Naive Bayes in [16] 96.0 Backprop. in [15] 96.7 Quadratic discr. in [15] 97.3 Linear discr. in [15] 98.0 D-SIDE 98.0 70 75 80 85 Success rate (% correct) HEART DISEASE 1Rw in [15] 76.6 K-NN in [15] 79.2 Flexible Bayes in [16] 80.0 Backprop. <p> [15] 78.0 Successive Bayes in [21] 78.4 Naive Bayes in [21] 79.2 M (12) in [4] 80.0 Success rate (% correct) IRIS MC1 in [36] 93.5 RMHC-PF1 in [36] 94.7 Flexible Bayes in <ref> [16] </ref> 95.3 CART in [16] 95.3 C4.5 in [16] 95.3 K-NN in [15] 96.0 Naive Bayes in [16] 96.0 Backprop. in [15] 96.7 Quadratic discr. in [15] 97.3 Linear discr. in [15] 98.0 D-SIDE 98.0 70 75 80 85 Success rate (% correct) HEART DISEASE 1Rw in [15] 76.6 K-NN in [15] 79.2 Flexible Bayes in [16] 80.0 Backprop. in [15] 80.6 MC1 in [36] 80.7 NBCFSS in <p> C4.5 in <ref> [16] </ref> 95.3 K-NN in [15] 96.0 Naive Bayes in [16] 96.0 Backprop. in [15] 96.7 Quadratic discr. in [15] 97.3 Linear discr. in [15] 98.0 D-SIDE 98.0 70 75 80 85 Success rate (% correct) HEART DISEASE 1Rw in [15] 76.6 K-NN in [15] 79.2 Flexible Bayes in [16] 80.0 Backprop. in [15] 80.6 MC1 in [36] 80.7 NBCFSS in [12] 81.9 BNG in [12] 82.2 RMHC-P in [36] 82.3 MSG in [12] 83.0 Naive Bayes in [16] 83.3 D-SIDE 84.8 65 70 75 80 85 Success rate (% correct) LYMPHOGRAPHY K-NN in [15] 72.9 C4.5 in [33] 77.0 <p> 70 75 80 85 Success rate (% correct) HEART DISEASE 1Rw in [15] 76.6 K-NN in [15] 79.2 Flexible Bayes in <ref> [16] </ref> 80.0 Backprop. in [15] 80.6 MC1 in [36] 80.7 NBCFSS in [12] 81.9 BNG in [12] 82.2 RMHC-P in [36] 82.3 MSG in [12] 83.0 Naive Bayes in [16] 83.3 D-SIDE 84.8 65 70 75 80 85 Success rate (% correct) LYMPHOGRAPHY K-NN in [15] 72.9 C4.5 in [33] 77.0 NBCFSS in [12] 77.7 ID3 in [15] 78.4 Assistant in [15] 79.0 MSG in [12] 80.4 TAN in [12] 85.0 Naive Bayes in [15] 85.1 D-SIDE 86.6 Fig. 2.
Reference: 17. <author> M.I. Jordan and R.A. Jacobs. </author> <title> Hierarchical mixtures of experts and the EM algorithm. </title> <journal> Neural Computation, </journal> <volume> 6 </volume> <pages> 181-214, </pages> <year> 1994. </year>
Reference-contexts: In M step, the parameter values are updated in such a way that the obtained expected value of the posterior is maximized. The detailed derivation of the update formulas in our mixture case is somewhat involved and omitted here, but it is akin to the use of EM in <ref> [17] </ref>. The detailed derivation of the update formulas in our mixture case can be found in [25]. 5 Empirical results The approach described above has been implemented as part of a more general software package D-SIDE 4 for building systems capable of probabilistic inference.
Reference: 18. <author> R.E. Kass and A.E. Raftery. </author> <title> Bayes factors. </title> <type> Technical Report 254, </type> <institution> Department of Statistics, University of Washington, </institution> <year> 1994. </year>
Reference-contexts: The evidence integral is very hard to evaluate since the dimension of the parameter space is usually very large (K (1+ P m but the evidence can be approximated by using e.g. Laplace's method <ref> [18] </ref>. The quality of the approximations has been tested empirically in the finite mixture framework by using both synthetic [23, 6] and natural [24] data. 4.2 Determining the model class parameters Let us assume that the model class M K is fixed.
Reference: 19. <author> M. Keane. </author> <title> Analogical asides on case-based reasoning. </title> <editor> In S. Wess, K.-D. Althoff, and M Richter, editors, </editor> <booktitle> Topics in Case-Based Reasoning, volume 837 of Lecture Notes in Artificial Intelligence, </booktitle> <pages> pages 21-32. </pages> <publisher> Springer-Verlag, </publisher> <year> 1994. </year>
Reference-contexts: As discussed in [20], no old situation is ever exactly the same as a new one, hence old solutions must usually be adapted (for more discussion on the importance of adaptation in CBR systems see e.g., <ref> [19] </ref>). Traditional case-based reasoning systems base their structural adaptation on adjustment heuristics applied to the values of interest in relevant cases. This set of relevant cases is determined during the case retrieval phase typically by using a nearest neighbor matching [32] or indexes created by induction algorithms [33].
Reference: 20. <author> J. Kolodner. </author> <title> Case-Based Reasoning. </title> <publisher> Morgan Kaufmann Publishers, </publisher> <address> San Mateo, </address> <year> 1993. </year>
Reference-contexts: In its most general form, case-based reasoning (CBR) can be viewed as an approach to computational reasoning that solves new problems by adapting previously successful solutions to similar problems <ref> [20, 39] </ref>. This general definition does not by itself place CBR systems in any particular place in the spectrum discussed above|the adaptation step in case-based reasoning can embed a substantial amount of domain knowledge. <p> This general definition does not by itself place CBR systems in any particular place in the spectrum discussed above|the adaptation step in case-based reasoning can embed a substantial amount of domain knowledge. However, while CBR has its roots in cognitive modeling <ref> [34, 20] </ref>, from an engineering point of view it can be seen as an attempt to address many of the problems recognized in the design of traditional knowledge-intensive systems, in particular the brittleness problem [2]. <p> This type of case-based reasoning approaches can be paralleled to the fundamental quest in statistical inference, i.e., finding the most probable model given data and weak prior knowledge. In this view, structural case adaptation <ref> [20] </ref> (and parameter adjustment in particular) can be seen as probabilistic prediction based on the model constructed from the case history. This observation has been our starting point for developing a probabilistic foundation for data-intensive case-based reasoning. As discussed in [20], no old situation is ever exactly the same as a <p> In this view, structural case adaptation <ref> [20] </ref> (and parameter adjustment in particular) can be seen as probabilistic prediction based on the model constructed from the case history. This observation has been our starting point for developing a probabilistic foundation for data-intensive case-based reasoning. As discussed in [20], no old situation is ever exactly the same as a new one, hence old solutions must usually be adapted (for more discussion on the importance of adaptation in CBR systems see e.g., [19]). <p> The methodology described has been implemented in the D-SIDE software package. In order to validate our approach we present empirical results on a set of classification tasks (for discussion of exemplar-based classification see Chapter 13.1 in <ref> [20] </ref>). In order to be able to compare the probabilistic exemplar-based method with alternative classification approaches, we have used 1 It should be observed that although not assumed in the discussion below, nothing in our approach prohibits storing also the individual cases in addition to the mixture distribution model.
Reference: 21. <author> I. Kononenko. </author> <title> Successive naive Bayesian classifier. </title> <journal> Informatica, </journal> <volume> 17 </volume> <pages> 167-174, </pages> <year> 1993. </year>
Reference-contexts: 77.3 Linear discr. in [28] 77.5 DIPOL92 in [28] 77.6 Logistic discr. in [28] 77.7 30 35 40 45 50 55 Success rate (% correct) PRIMARY TUMOR C4 in [1] 37.8 M [1] in [4] 41.4 Human expert in [22] 42.0 Assistant in [22] 44.0 D-SIDE 50.4 Naive Bayes in <ref> [21] </ref> 51.0 Successive Bayes in [21] 51.7 75 80 85 90 Success rate (% correct) HEPATITIS C4.5 in [33] 80.9 Assistant in [15] 83.0 Naive Bayes in [15] 84.0 1Rw in [15] 84.5 D-SIDE 88.0 30 35 40 45 50 55 60 65 70 75 80 85 Success rate (% correct) <p> 77.5 DIPOL92 in [28] 77.6 Logistic discr. in [28] 77.7 30 35 40 45 50 55 Success rate (% correct) PRIMARY TUMOR C4 in [1] 37.8 M [1] in [4] 41.4 Human expert in [22] 42.0 Assistant in [22] 44.0 D-SIDE 50.4 Naive Bayes in <ref> [21] </ref> 51.0 Successive Bayes in [21] 51.7 75 80 85 90 Success rate (% correct) HEPATITIS C4.5 in [33] 80.9 Assistant in [15] 83.0 Naive Bayes in [15] 84.0 1Rw in [15] 84.5 D-SIDE 88.0 30 35 40 45 50 55 60 65 70 75 80 85 Success rate (% correct) GLASS 1Rw in [15] 62.2 <p> in [22] 64.0 K-NN in [15] 65.3 Quadratic discr. in [15] 65.6 Backprop in [15] 71.5 Linear discr. in [15] 71.6 C4 in [15] 72.0 RMHC-PF1 in [36] 72.3 1Rw in [15] 72.7 CN2 in [15] 73.0 D-SIDE 76.6 CART in [15] 77.1 Assistant in [15] 78.0 Successive Bayes in <ref> [21] </ref> 78.4 Naive Bayes in [21] 79.2 M (12) in [4] 80.0 Success rate (% correct) IRIS MC1 in [36] 93.5 RMHC-PF1 in [36] 94.7 Flexible Bayes in [16] 95.3 CART in [16] 95.3 C4.5 in [16] 95.3 K-NN in [15] 96.0 Naive Bayes in [16] 96.0 Backprop. in [15] 96.7 <p> [15] 65.3 Quadratic discr. in [15] 65.6 Backprop in [15] 71.5 Linear discr. in [15] 71.6 C4 in [15] 72.0 RMHC-PF1 in [36] 72.3 1Rw in [15] 72.7 CN2 in [15] 73.0 D-SIDE 76.6 CART in [15] 77.1 Assistant in [15] 78.0 Successive Bayes in <ref> [21] </ref> 78.4 Naive Bayes in [21] 79.2 M (12) in [4] 80.0 Success rate (% correct) IRIS MC1 in [36] 93.5 RMHC-PF1 in [36] 94.7 Flexible Bayes in [16] 95.3 CART in [16] 95.3 C4.5 in [16] 95.3 K-NN in [15] 96.0 Naive Bayes in [16] 96.0 Backprop. in [15] 96.7 Quadratic discr. in [15] 97.3
Reference: 22. <author> I. Kononenko and I. Bratko. </author> <title> Information-based evaluation criterion for classifier's performance. </title> <journal> Machine Learning, </journal> <volume> 6 </volume> <pages> 67-80, </pages> <year> 1991. </year>
Reference-contexts: [12] 76.0 C4.5 in [33] 76.0 SMART in [28] 76.8 D-SIDE 77.3 Linear discr. in [28] 77.5 DIPOL92 in [28] 77.6 Logistic discr. in [28] 77.7 30 35 40 45 50 55 Success rate (% correct) PRIMARY TUMOR C4 in [1] 37.8 M [1] in [4] 41.4 Human expert in <ref> [22] </ref> 42.0 Assistant in [22] 44.0 D-SIDE 50.4 Naive Bayes in [21] 51.0 Successive Bayes in [21] 51.7 75 80 85 90 Success rate (% correct) HEPATITIS C4.5 in [33] 80.9 Assistant in [15] 83.0 Naive Bayes in [15] 84.0 1Rw in [15] 84.5 D-SIDE 88.0 30 35 40 45 50 <p> [33] 76.0 SMART in [28] 76.8 D-SIDE 77.3 Linear discr. in [28] 77.5 DIPOL92 in [28] 77.6 Logistic discr. in [28] 77.7 30 35 40 45 50 55 Success rate (% correct) PRIMARY TUMOR C4 in [1] 37.8 M [1] in [4] 41.4 Human expert in <ref> [22] </ref> 42.0 Assistant in [22] 44.0 D-SIDE 50.4 Naive Bayes in [21] 51.0 Successive Bayes in [21] 51.7 75 80 85 90 Success rate (% correct) HEPATITIS C4.5 in [33] 80.9 Assistant in [15] 83.0 Naive Bayes in [15] 84.0 1Rw in [15] 84.5 D-SIDE 88.0 30 35 40 45 50 55 60 65 70 <p> Naive Bayes in [28] 93.2 Logistic discr. in [28] 93.9 Quadratic discr. in [28] 94.1 Linear discr. in [28] 94.1 ALLOC80 in [28] 94.3 DIPOL92 in [28] 95.2 RBF in [28] 95.9 D-SIDE 97.0 50 55 60 65 70 75 80 Success rate (% correct) BREAST CANCER Human expert in <ref> [22] </ref> 64.0 K-NN in [15] 65.3 Quadratic discr. in [15] 65.6 Backprop in [15] 71.5 Linear discr. in [15] 71.6 C4 in [15] 72.0 RMHC-PF1 in [36] 72.3 1Rw in [15] 72.7 CN2 in [15] 73.0 D-SIDE 76.6 CART in [15] 77.1 Assistant in [15] 78.0 Successive Bayes in [21] 78.4
Reference: 23. <author> P. Kontkanen, P. Myllymaki, and H. Tirri. </author> <title> Unsupervised Bayesian learning of discrete finite mixtures. </title> <type> Manuscript, </type> <note> submitted for publication. </note>
Reference-contexts: Laplace's method [18]. The quality of the approximations has been tested empirically in the finite mixture framework by using both synthetic <ref> [23, 6] </ref> and natural [24] data. 4.2 Determining the model class parameters Let us assume that the model class M K is fixed.
Reference: 24. <author> P. Kontkanen, P. Myllymaki, and H. Tirri. </author> <title> Comparing Bayesian model class selection criteria by discrete finite mixtures. </title> <booktitle> In Proceedings of the ISIS (Information, Statistics and Induction in Science) Conference, </booktitle> <address> Melbourne, Australia, </address> <month> August </month> <year> 1996. </year> <note> (To appear.). </note>
Reference-contexts: Laplace's method [18]. The quality of the approximations has been tested empirically in the finite mixture framework by using both synthetic [23, 6] and natural <ref> [24] </ref> data. 4.2 Determining the model class parameters Let us assume that the model class M K is fixed.
Reference: 25. <author> P. Kontkanen, P. Myllymaki, and H. Tirri. </author> <title> Constructing Bayesian finite mixture models by the EM algorithm. </title> <type> Technical Report C-1996-9, </type> <institution> University of Helsinki, Department of Computer Science, </institution> <month> February </month> <year> 1996. </year>
Reference-contexts: The detailed derivation of the update formulas in our mixture case is somewhat involved and omitted here, but it is akin to the use of EM in [17]. The detailed derivation of the update formulas in our mixture case can be found in <ref> [25] </ref>. 5 Empirical results The approach described above has been implemented as part of a more general software package D-SIDE 4 for building systems capable of probabilistic inference. As already discussed in the introduction, we are motivated in finding models with good adaptation performance.
Reference: 26. <author> P. Koton. </author> <title> Using experience in learning and problem solving. </title> <type> PhD thesis, </type> <institution> Mas-sachusetts Institute of Technology, </institution> <year> 1989. </year>
Reference-contexts: In order to address some of these drawbacks of the basic approach, many suggestions for structuring the case storage have been proposed in the literature. From our point of view the most interesting is the "conceptual clustering" approach where generalized episodes <ref> [26] </ref> are used to group similar cases under a more general structure. <p> In fact, construction of prototypical structures such as Generalized Episodes by grouping cases sharing similar properties <ref> [26] </ref> can be understood as approximations to such simpler mixture structures [11, 38]. Thus the probabilistic approach offers also a natural way of structuring the case storage 1 . There are several advantages of using the above probabilistic view of CBR.
Reference: 27. <author> R.J.A. Little and D.B. Rubin. </author> <title> Statistical analysis with missing data. </title> <publisher> Wiley, </publisher> <year> 1987. </year>
Reference-contexts: These clusterings are, however, exponential (K N ) in number, so exact MAP estimate cannot be found, and we are forced to use numerical approximation methods. The usual approach is to regard Z as missing data and estimate it iteratively <ref> [27] </ref>. This estimation can be performed by using the Expectation-Maximization (EM) algorithm [10]. The EM algorithm is an iterative algorithm, which monotonically increases the expected value of the posterior corresponding to incomplete data. It consists of two steps: Expectation (E) and Maximization (M).
Reference: 28. <editor> D. Michie, D.J. Spiegelhalter, and C.C. Taylor, editors. </editor> <title> Machine Learning, Neural and Statistical Classification. </title> <publisher> Ellis Horwood, </publisher> <address> London, </address> <year> 1994. </year>
Reference-contexts: This can sometimes be useful for example for satisfying the needs of the explanation mechanisms. public domain data sets (including data sets from the StatLog project <ref> [28] </ref>). <p> The selection of datasets was done on the basis of their reported use, i.e., we have preferred datasets that have been used for testing many different methods over datasets with only isolated results. Many of the results are from the StatLog project <ref> [28] </ref>, but we have also included more recent results. The descriptions of the datasets, the testing procedures used, and the best model classes (the number of mixtures) found for each data set are given in Table 1. <p> It should be observed that with the exception of the DNA dataset, all our current results are crossvalidated, and when possible (for the StatLog datasets) we have used the same crossvalidation scheme as described in <ref> [28] </ref>. The same does not hold for many of the results for the other methods. In many cases the testing procedure either was not reported, or the best result with a single test set was given. <p> that our approach performs well not only when compared to memory-based methods 4 A running Java TM demo of the D-SIDE software can be accessed through our WWW homepage at URL "http: //www.cs.Helsinki.FI/research/cosco/". 70 75 80 85 Success rate (% correct) AUSTRALIAN Flexible Bayes in [16] 78.3 Quadratic discr. in <ref> [28] </ref> 79.3 CN2 in [28] 79.6 ALLOC80 in [28] 79.9 LVQ in [28] 80.3 NewID in [28] 81.9 AC2 in [28] 81.9 Bayes tree in [28] 82.9 SMART in [28] 84.2 Backprop in [28] 84.6 IndCART in [28] 84.8 TAN in [12] 85.1 CASTLE in [28] 85.2 CART in [28] 85.5 <p> well not only when compared to memory-based methods 4 A running Java TM demo of the D-SIDE software can be accessed through our WWW homepage at URL "http: //www.cs.Helsinki.FI/research/cosco/". 70 75 80 85 Success rate (% correct) AUSTRALIAN Flexible Bayes in [16] 78.3 Quadratic discr. in <ref> [28] </ref> 79.3 CN2 in [28] 79.6 ALLOC80 in [28] 79.9 LVQ in [28] 80.3 NewID in [28] 81.9 AC2 in [28] 81.9 Bayes tree in [28] 82.9 SMART in [28] 84.2 Backprop in [28] 84.6 IndCART in [28] 84.8 TAN in [12] 85.1 CASTLE in [28] 85.2 CART in [28] 85.5 RBF in [28] 85.5 <p> compared to memory-based methods 4 A running Java TM demo of the D-SIDE software can be accessed through our WWW homepage at URL "http: //www.cs.Helsinki.FI/research/cosco/". 70 75 80 85 Success rate (% correct) AUSTRALIAN Flexible Bayes in [16] 78.3 Quadratic discr. in <ref> [28] </ref> 79.3 CN2 in [28] 79.6 ALLOC80 in [28] 79.9 LVQ in [28] 80.3 NewID in [28] 81.9 AC2 in [28] 81.9 Bayes tree in [28] 82.9 SMART in [28] 84.2 Backprop in [28] 84.6 IndCART in [28] 84.8 TAN in [12] 85.1 CASTLE in [28] 85.2 CART in [28] 85.5 RBF in [28] 85.5 DIPOL92 in [28] 85.9 <p> 4 A running Java TM demo of the D-SIDE software can be accessed through our WWW homepage at URL "http: //www.cs.Helsinki.FI/research/cosco/". 70 75 80 85 Success rate (% correct) AUSTRALIAN Flexible Bayes in [16] 78.3 Quadratic discr. in <ref> [28] </ref> 79.3 CN2 in [28] 79.6 ALLOC80 in [28] 79.9 LVQ in [28] 80.3 NewID in [28] 81.9 AC2 in [28] 81.9 Bayes tree in [28] 82.9 SMART in [28] 84.2 Backprop in [28] 84.6 IndCART in [28] 84.8 TAN in [12] 85.1 CASTLE in [28] 85.2 CART in [28] 85.5 RBF in [28] 85.5 DIPOL92 in [28] 85.9 Linear discr. in [28] <p> TM demo of the D-SIDE software can be accessed through our WWW homepage at URL "http: //www.cs.Helsinki.FI/research/cosco/". 70 75 80 85 Success rate (% correct) AUSTRALIAN Flexible Bayes in [16] 78.3 Quadratic discr. in <ref> [28] </ref> 79.3 CN2 in [28] 79.6 ALLOC80 in [28] 79.9 LVQ in [28] 80.3 NewID in [28] 81.9 AC2 in [28] 81.9 Bayes tree in [28] 82.9 SMART in [28] 84.2 Backprop in [28] 84.6 IndCART in [28] 84.8 TAN in [12] 85.1 CASTLE in [28] 85.2 CART in [28] 85.5 RBF in [28] 85.5 DIPOL92 in [28] 85.9 Linear discr. in [28] 85.9 Logistic discr. in <p> D-SIDE software can be accessed through our WWW homepage at URL "http: //www.cs.Helsinki.FI/research/cosco/". 70 75 80 85 Success rate (% correct) AUSTRALIAN Flexible Bayes in [16] 78.3 Quadratic discr. in <ref> [28] </ref> 79.3 CN2 in [28] 79.6 ALLOC80 in [28] 79.9 LVQ in [28] 80.3 NewID in [28] 81.9 AC2 in [28] 81.9 Bayes tree in [28] 82.9 SMART in [28] 84.2 Backprop in [28] 84.6 IndCART in [28] 84.8 TAN in [12] 85.1 CASTLE in [28] 85.2 CART in [28] 85.5 RBF in [28] 85.5 DIPOL92 in [28] 85.9 Linear discr. in [28] 85.9 Logistic discr. in [28] 85.9 C4.5 in <p> through our WWW homepage at URL "http: //www.cs.Helsinki.FI/research/cosco/". 70 75 80 85 Success rate (% correct) AUSTRALIAN Flexible Bayes in [16] 78.3 Quadratic discr. in <ref> [28] </ref> 79.3 CN2 in [28] 79.6 ALLOC80 in [28] 79.9 LVQ in [28] 80.3 NewID in [28] 81.9 AC2 in [28] 81.9 Bayes tree in [28] 82.9 SMART in [28] 84.2 Backprop in [28] 84.6 IndCART in [28] 84.8 TAN in [12] 85.1 CASTLE in [28] 85.2 CART in [28] 85.5 RBF in [28] 85.5 DIPOL92 in [28] 85.9 Linear discr. in [28] 85.9 Logistic discr. in [28] 85.9 C4.5 in [33] 86.0 Naive Bayes in <p> at URL "http: //www.cs.Helsinki.FI/research/cosco/". 70 75 80 85 Success rate (% correct) AUSTRALIAN Flexible Bayes in [16] 78.3 Quadratic discr. in <ref> [28] </ref> 79.3 CN2 in [28] 79.6 ALLOC80 in [28] 79.9 LVQ in [28] 80.3 NewID in [28] 81.9 AC2 in [28] 81.9 Bayes tree in [28] 82.9 SMART in [28] 84.2 Backprop in [28] 84.6 IndCART in [28] 84.8 TAN in [12] 85.1 CASTLE in [28] 85.2 CART in [28] 85.5 RBF in [28] 85.5 DIPOL92 in [28] 85.9 Linear discr. in [28] 85.9 Logistic discr. in [28] 85.9 C4.5 in [33] 86.0 Naive Bayes in [13] 86.2 BNG in <p> 70 75 80 85 Success rate (% correct) AUSTRALIAN Flexible Bayes in [16] 78.3 Quadratic discr. in <ref> [28] </ref> 79.3 CN2 in [28] 79.6 ALLOC80 in [28] 79.9 LVQ in [28] 80.3 NewID in [28] 81.9 AC2 in [28] 81.9 Bayes tree in [28] 82.9 SMART in [28] 84.2 Backprop in [28] 84.6 IndCART in [28] 84.8 TAN in [12] 85.1 CASTLE in [28] 85.2 CART in [28] 85.5 RBF in [28] 85.5 DIPOL92 in [28] 85.9 Linear discr. in [28] 85.9 Logistic discr. in [28] 85.9 C4.5 in [33] 86.0 Naive Bayes in [13] 86.2 BNG in [12] 86.2 ITrule in <p> Success rate (% correct) AUSTRALIAN Flexible Bayes in [16] 78.3 Quadratic discr. in <ref> [28] </ref> 79.3 CN2 in [28] 79.6 ALLOC80 in [28] 79.9 LVQ in [28] 80.3 NewID in [28] 81.9 AC2 in [28] 81.9 Bayes tree in [28] 82.9 SMART in [28] 84.2 Backprop in [28] 84.6 IndCART in [28] 84.8 TAN in [12] 85.1 CASTLE in [28] 85.2 CART in [28] 85.5 RBF in [28] 85.5 DIPOL92 in [28] 85.9 Linear discr. in [28] 85.9 Logistic discr. in [28] 85.9 C4.5 in [33] 86.0 Naive Bayes in [13] 86.2 BNG in [12] 86.2 ITrule in [28] 86.3 NBCFSS in <p> [16] 78.3 Quadratic discr. in <ref> [28] </ref> 79.3 CN2 in [28] 79.6 ALLOC80 in [28] 79.9 LVQ in [28] 80.3 NewID in [28] 81.9 AC2 in [28] 81.9 Bayes tree in [28] 82.9 SMART in [28] 84.2 Backprop in [28] 84.6 IndCART in [28] 84.8 TAN in [12] 85.1 CASTLE in [28] 85.2 CART in [28] 85.5 RBF in [28] 85.5 DIPOL92 in [28] 85.9 Linear discr. in [28] 85.9 Logistic discr. in [28] 85.9 C4.5 in [33] 86.0 Naive Bayes in [13] 86.2 BNG in [12] 86.2 ITrule in [28] 86.3 NBCFSS in [12] 86.7 MS in [12] 86.8 D-SIDE 87.2 <p> in <ref> [28] </ref> 79.3 CN2 in [28] 79.6 ALLOC80 in [28] 79.9 LVQ in [28] 80.3 NewID in [28] 81.9 AC2 in [28] 81.9 Bayes tree in [28] 82.9 SMART in [28] 84.2 Backprop in [28] 84.6 IndCART in [28] 84.8 TAN in [12] 85.1 CASTLE in [28] 85.2 CART in [28] 85.5 RBF in [28] 85.5 DIPOL92 in [28] 85.9 Linear discr. in [28] 85.9 Logistic discr. in [28] 85.9 C4.5 in [33] 86.0 Naive Bayes in [13] 86.2 BNG in [12] 86.2 ITrule in [28] 86.3 NBCFSS in [12] 86.7 MS in [12] 86.8 D-SIDE 87.2 60 65 70 75 <p> in <ref> [28] </ref> 79.6 ALLOC80 in [28] 79.9 LVQ in [28] 80.3 NewID in [28] 81.9 AC2 in [28] 81.9 Bayes tree in [28] 82.9 SMART in [28] 84.2 Backprop in [28] 84.6 IndCART in [28] 84.8 TAN in [12] 85.1 CASTLE in [28] 85.2 CART in [28] 85.5 RBF in [28] 85.5 DIPOL92 in [28] 85.9 Linear discr. in [28] 85.9 Logistic discr. in [28] 85.9 C4.5 in [33] 86.0 Naive Bayes in [13] 86.2 BNG in [12] 86.2 ITrule in [28] 86.3 NBCFSS in [12] 86.7 MS in [12] 86.8 D-SIDE 87.2 60 65 70 75 80 Success rate (% <p> in <ref> [28] </ref> 79.9 LVQ in [28] 80.3 NewID in [28] 81.9 AC2 in [28] 81.9 Bayes tree in [28] 82.9 SMART in [28] 84.2 Backprop in [28] 84.6 IndCART in [28] 84.8 TAN in [12] 85.1 CASTLE in [28] 85.2 CART in [28] 85.5 RBF in [28] 85.5 DIPOL92 in [28] 85.9 Linear discr. in [28] 85.9 Logistic discr. in [28] 85.9 C4.5 in [33] 86.0 Naive Bayes in [13] 86.2 BNG in [12] 86.2 ITrule in [28] 86.3 NBCFSS in [12] 86.7 MS in [12] 86.8 D-SIDE 87.2 60 65 70 75 80 Success rate (% correct) DIABETES K-NN in <p> <ref> [28] </ref> 80.3 NewID in [28] 81.9 AC2 in [28] 81.9 Bayes tree in [28] 82.9 SMART in [28] 84.2 Backprop in [28] 84.6 IndCART in [28] 84.8 TAN in [12] 85.1 CASTLE in [28] 85.2 CART in [28] 85.5 RBF in [28] 85.5 DIPOL92 in [28] 85.9 Linear discr. in [28] 85.9 Logistic discr. in [28] 85.9 C4.5 in [33] 86.0 Naive Bayes in [13] 86.2 BNG in [12] 86.2 ITrule in [28] 86.3 NBCFSS in [12] 86.7 MS in [12] 86.8 D-SIDE 87.2 60 65 70 75 80 Success rate (% correct) DIABETES K-NN in [28] 67.6 ALLOC80 in [28] <p> 81.9 AC2 in <ref> [28] </ref> 81.9 Bayes tree in [28] 82.9 SMART in [28] 84.2 Backprop in [28] 84.6 IndCART in [28] 84.8 TAN in [12] 85.1 CASTLE in [28] 85.2 CART in [28] 85.5 RBF in [28] 85.5 DIPOL92 in [28] 85.9 Linear discr. in [28] 85.9 Logistic discr. in [28] 85.9 C4.5 in [33] 86.0 Naive Bayes in [13] 86.2 BNG in [12] 86.2 ITrule in [28] 86.3 NBCFSS in [12] 86.7 MS in [12] 86.8 D-SIDE 87.2 60 65 70 75 80 Success rate (% correct) DIABETES K-NN in [28] 67.6 ALLOC80 in [28] 69.9 CN2 in [28] 71.1 <p> 84.6 IndCART in <ref> [28] </ref> 84.8 TAN in [12] 85.1 CASTLE in [28] 85.2 CART in [28] 85.5 RBF in [28] 85.5 DIPOL92 in [28] 85.9 Linear discr. in [28] 85.9 Logistic discr. in [28] 85.9 C4.5 in [33] 86.0 Naive Bayes in [13] 86.2 BNG in [12] 86.2 ITrule in [28] 86.3 NBCFSS in [12] 86.7 MS in [12] 86.8 D-SIDE 87.2 60 65 70 75 80 Success rate (% correct) DIABETES K-NN in [28] 67.6 ALLOC80 in [28] 69.9 CN2 in [28] 71.1 NewID in [28] 71.1 AC2 in [28] 72.4 LVQ in [28] 72.8 Bayes tree in [28] 72.9 <p> 85.9 Linear discr. in <ref> [28] </ref> 85.9 Logistic discr. in [28] 85.9 C4.5 in [33] 86.0 Naive Bayes in [13] 86.2 BNG in [12] 86.2 ITrule in [28] 86.3 NBCFSS in [12] 86.7 MS in [12] 86.8 D-SIDE 87.2 60 65 70 75 80 Success rate (% correct) DIABETES K-NN in [28] 67.6 ALLOC80 in [28] 69.9 CN2 in [28] 71.1 NewID in [28] 71.1 AC2 in [28] 72.4 LVQ in [28] 72.8 Bayes tree in [28] 72.9 IndCART in [28] 72.9 Quadratic discr. in [28] 73.8 Flexible Bayes in [16] 73.9 CASTLE in [28] 74.2 CART in [28] 74.5 Cal5 in <p> <ref> [28] </ref> 85.9 Logistic discr. in [28] 85.9 C4.5 in [33] 86.0 Naive Bayes in [13] 86.2 BNG in [12] 86.2 ITrule in [28] 86.3 NBCFSS in [12] 86.7 MS in [12] 86.8 D-SIDE 87.2 60 65 70 75 80 Success rate (% correct) DIABETES K-NN in [28] 67.6 ALLOC80 in [28] 69.9 CN2 in [28] 71.1 NewID in [28] 71.1 AC2 in [28] 72.4 LVQ in [28] 72.8 Bayes tree in [28] 72.9 IndCART in [28] 72.9 Quadratic discr. in [28] 73.8 Flexible Bayes in [16] 73.9 CASTLE in [28] 74.2 CART in [28] 74.5 Cal5 in [28] 75.0 Backprop in <p> in <ref> [28] </ref> 85.9 C4.5 in [33] 86.0 Naive Bayes in [13] 86.2 BNG in [12] 86.2 ITrule in [28] 86.3 NBCFSS in [12] 86.7 MS in [12] 86.8 D-SIDE 87.2 60 65 70 75 80 Success rate (% correct) DIABETES K-NN in [28] 67.6 ALLOC80 in [28] 69.9 CN2 in [28] 71.1 NewID in [28] 71.1 AC2 in [28] 72.4 LVQ in [28] 72.8 Bayes tree in [28] 72.9 IndCART in [28] 72.9 Quadratic discr. in [28] 73.8 Flexible Bayes in [16] 73.9 CASTLE in [28] 74.2 CART in [28] 74.5 Cal5 in [28] 75.0 Backprop in [28] 75.2 ITrule in <p> in [33] 86.0 Naive Bayes in [13] 86.2 BNG in [12] 86.2 ITrule in <ref> [28] </ref> 86.3 NBCFSS in [12] 86.7 MS in [12] 86.8 D-SIDE 87.2 60 65 70 75 80 Success rate (% correct) DIABETES K-NN in [28] 67.6 ALLOC80 in [28] 69.9 CN2 in [28] 71.1 NewID in [28] 71.1 AC2 in [28] 72.4 LVQ in [28] 72.8 Bayes tree in [28] 72.9 IndCART in [28] 72.9 Quadratic discr. in [28] 73.8 Flexible Bayes in [16] 73.9 CASTLE in [28] 74.2 CART in [28] 74.5 Cal5 in [28] 75.0 Backprop in [28] 75.2 ITrule in [28] 75.5 Naive Bayes <p> Bayes in [13] 86.2 BNG in [12] 86.2 ITrule in <ref> [28] </ref> 86.3 NBCFSS in [12] 86.7 MS in [12] 86.8 D-SIDE 87.2 60 65 70 75 80 Success rate (% correct) DIABETES K-NN in [28] 67.6 ALLOC80 in [28] 69.9 CN2 in [28] 71.1 NewID in [28] 71.1 AC2 in [28] 72.4 LVQ in [28] 72.8 Bayes tree in [28] 72.9 IndCART in [28] 72.9 Quadratic discr. in [28] 73.8 Flexible Bayes in [16] 73.9 CASTLE in [28] 74.2 CART in [28] 74.5 Cal5 in [28] 75.0 Backprop in [28] 75.2 ITrule in [28] 75.5 Naive Bayes in [12] 75.5 MSG <p> BNG in [12] 86.2 ITrule in <ref> [28] </ref> 86.3 NBCFSS in [12] 86.7 MS in [12] 86.8 D-SIDE 87.2 60 65 70 75 80 Success rate (% correct) DIABETES K-NN in [28] 67.6 ALLOC80 in [28] 69.9 CN2 in [28] 71.1 NewID in [28] 71.1 AC2 in [28] 72.4 LVQ in [28] 72.8 Bayes tree in [28] 72.9 IndCART in [28] 72.9 Quadratic discr. in [28] 73.8 Flexible Bayes in [16] 73.9 CASTLE in [28] 74.2 CART in [28] 74.5 Cal5 in [28] 75.0 Backprop in [28] 75.2 ITrule in [28] 75.5 Naive Bayes in [12] 75.5 MSG in [12] 75.5 RBF <p> in <ref> [28] </ref> 86.3 NBCFSS in [12] 86.7 MS in [12] 86.8 D-SIDE 87.2 60 65 70 75 80 Success rate (% correct) DIABETES K-NN in [28] 67.6 ALLOC80 in [28] 69.9 CN2 in [28] 71.1 NewID in [28] 71.1 AC2 in [28] 72.4 LVQ in [28] 72.8 Bayes tree in [28] 72.9 IndCART in [28] 72.9 Quadratic discr. in [28] 73.8 Flexible Bayes in [16] 73.9 CASTLE in [28] 74.2 CART in [28] 74.5 Cal5 in [28] 75.0 Backprop in [28] 75.2 ITrule in [28] 75.5 Naive Bayes in [12] 75.5 MSG in [12] 75.5 RBF in [28] 75.7 NBCFSS in <p> in [12] 86.7 MS in [12] 86.8 D-SIDE 87.2 60 65 70 75 80 Success rate (% correct) DIABETES K-NN in <ref> [28] </ref> 67.6 ALLOC80 in [28] 69.9 CN2 in [28] 71.1 NewID in [28] 71.1 AC2 in [28] 72.4 LVQ in [28] 72.8 Bayes tree in [28] 72.9 IndCART in [28] 72.9 Quadratic discr. in [28] 73.8 Flexible Bayes in [16] 73.9 CASTLE in [28] 74.2 CART in [28] 74.5 Cal5 in [28] 75.0 Backprop in [28] 75.2 ITrule in [28] 75.5 Naive Bayes in [12] 75.5 MSG in [12] 75.5 RBF in [28] 75.7 NBCFSS in [12] 76.0 C4.5 in <p> [12] 86.8 D-SIDE 87.2 60 65 70 75 80 Success rate (% correct) DIABETES K-NN in <ref> [28] </ref> 67.6 ALLOC80 in [28] 69.9 CN2 in [28] 71.1 NewID in [28] 71.1 AC2 in [28] 72.4 LVQ in [28] 72.8 Bayes tree in [28] 72.9 IndCART in [28] 72.9 Quadratic discr. in [28] 73.8 Flexible Bayes in [16] 73.9 CASTLE in [28] 74.2 CART in [28] 74.5 Cal5 in [28] 75.0 Backprop in [28] 75.2 ITrule in [28] 75.5 Naive Bayes in [12] 75.5 MSG in [12] 75.5 RBF in [28] 75.7 NBCFSS in [12] 76.0 C4.5 in [33] 76.0 SMART in [28] <p> Success rate (% correct) DIABETES K-NN in <ref> [28] </ref> 67.6 ALLOC80 in [28] 69.9 CN2 in [28] 71.1 NewID in [28] 71.1 AC2 in [28] 72.4 LVQ in [28] 72.8 Bayes tree in [28] 72.9 IndCART in [28] 72.9 Quadratic discr. in [28] 73.8 Flexible Bayes in [16] 73.9 CASTLE in [28] 74.2 CART in [28] 74.5 Cal5 in [28] 75.0 Backprop in [28] 75.2 ITrule in [28] 75.5 Naive Bayes in [12] 75.5 MSG in [12] 75.5 RBF in [28] 75.7 NBCFSS in [12] 76.0 C4.5 in [33] 76.0 SMART in [28] 76.8 D-SIDE 77.3 Linear discr. in [28] 77.5 DIPOL92 <p> DIABETES K-NN in <ref> [28] </ref> 67.6 ALLOC80 in [28] 69.9 CN2 in [28] 71.1 NewID in [28] 71.1 AC2 in [28] 72.4 LVQ in [28] 72.8 Bayes tree in [28] 72.9 IndCART in [28] 72.9 Quadratic discr. in [28] 73.8 Flexible Bayes in [16] 73.9 CASTLE in [28] 74.2 CART in [28] 74.5 Cal5 in [28] 75.0 Backprop in [28] 75.2 ITrule in [28] 75.5 Naive Bayes in [12] 75.5 MSG in [12] 75.5 RBF in [28] 75.7 NBCFSS in [12] 76.0 C4.5 in [33] 76.0 SMART in [28] 76.8 D-SIDE 77.3 Linear discr. in [28] 77.5 DIPOL92 in [28] 77.6 Logistic <p> 67.6 ALLOC80 in <ref> [28] </ref> 69.9 CN2 in [28] 71.1 NewID in [28] 71.1 AC2 in [28] 72.4 LVQ in [28] 72.8 Bayes tree in [28] 72.9 IndCART in [28] 72.9 Quadratic discr. in [28] 73.8 Flexible Bayes in [16] 73.9 CASTLE in [28] 74.2 CART in [28] 74.5 Cal5 in [28] 75.0 Backprop in [28] 75.2 ITrule in [28] 75.5 Naive Bayes in [12] 75.5 MSG in [12] 75.5 RBF in [28] 75.7 NBCFSS in [12] 76.0 C4.5 in [33] 76.0 SMART in [28] 76.8 D-SIDE 77.3 Linear discr. in [28] 77.5 DIPOL92 in [28] 77.6 Logistic discr. in [28] 77.7 <p> 69.9 CN2 in <ref> [28] </ref> 71.1 NewID in [28] 71.1 AC2 in [28] 72.4 LVQ in [28] 72.8 Bayes tree in [28] 72.9 IndCART in [28] 72.9 Quadratic discr. in [28] 73.8 Flexible Bayes in [16] 73.9 CASTLE in [28] 74.2 CART in [28] 74.5 Cal5 in [28] 75.0 Backprop in [28] 75.2 ITrule in [28] 75.5 Naive Bayes in [12] 75.5 MSG in [12] 75.5 RBF in [28] 75.7 NBCFSS in [12] 76.0 C4.5 in [33] 76.0 SMART in [28] 76.8 D-SIDE 77.3 Linear discr. in [28] 77.5 DIPOL92 in [28] 77.6 Logistic discr. in [28] 77.7 30 35 40 45 <p> 71.1 NewID in <ref> [28] </ref> 71.1 AC2 in [28] 72.4 LVQ in [28] 72.8 Bayes tree in [28] 72.9 IndCART in [28] 72.9 Quadratic discr. in [28] 73.8 Flexible Bayes in [16] 73.9 CASTLE in [28] 74.2 CART in [28] 74.5 Cal5 in [28] 75.0 Backprop in [28] 75.2 ITrule in [28] 75.5 Naive Bayes in [12] 75.5 MSG in [12] 75.5 RBF in [28] 75.7 NBCFSS in [12] 76.0 C4.5 in [33] 76.0 SMART in [28] 76.8 D-SIDE 77.3 Linear discr. in [28] 77.5 DIPOL92 in [28] 77.6 Logistic discr. in [28] 77.7 30 35 40 45 50 55 Success rate <p> Bayes tree in <ref> [28] </ref> 72.9 IndCART in [28] 72.9 Quadratic discr. in [28] 73.8 Flexible Bayes in [16] 73.9 CASTLE in [28] 74.2 CART in [28] 74.5 Cal5 in [28] 75.0 Backprop in [28] 75.2 ITrule in [28] 75.5 Naive Bayes in [12] 75.5 MSG in [12] 75.5 RBF in [28] 75.7 NBCFSS in [12] 76.0 C4.5 in [33] 76.0 SMART in [28] 76.8 D-SIDE 77.3 Linear discr. in [28] 77.5 DIPOL92 in [28] 77.6 Logistic discr. in [28] 77.7 30 35 40 45 50 55 Success rate (% correct) PRIMARY TUMOR C4 in [1] 37.8 M [1] in [4] 41.4 <p> <ref> [28] </ref> 73.8 Flexible Bayes in [16] 73.9 CASTLE in [28] 74.2 CART in [28] 74.5 Cal5 in [28] 75.0 Backprop in [28] 75.2 ITrule in [28] 75.5 Naive Bayes in [12] 75.5 MSG in [12] 75.5 RBF in [28] 75.7 NBCFSS in [12] 76.0 C4.5 in [33] 76.0 SMART in [28] 76.8 D-SIDE 77.3 Linear discr. in [28] 77.5 DIPOL92 in [28] 77.6 Logistic discr. in [28] 77.7 30 35 40 45 50 55 Success rate (% correct) PRIMARY TUMOR C4 in [1] 37.8 M [1] in [4] 41.4 Human expert in [22] 42.0 Assistant in [22] 44.0 D-SIDE 50.4 Naive <p> CASTLE in <ref> [28] </ref> 74.2 CART in [28] 74.5 Cal5 in [28] 75.0 Backprop in [28] 75.2 ITrule in [28] 75.5 Naive Bayes in [12] 75.5 MSG in [12] 75.5 RBF in [28] 75.7 NBCFSS in [12] 76.0 C4.5 in [33] 76.0 SMART in [28] 76.8 D-SIDE 77.3 Linear discr. in [28] 77.5 DIPOL92 in [28] 77.6 Logistic discr. in [28] 77.7 30 35 40 45 50 55 Success rate (% correct) PRIMARY TUMOR C4 in [1] 37.8 M [1] in [4] 41.4 Human expert in [22] 42.0 Assistant in [22] 44.0 D-SIDE 50.4 Naive Bayes in [21] 51.0 Successive Bayes in <p> CART in <ref> [28] </ref> 74.5 Cal5 in [28] 75.0 Backprop in [28] 75.2 ITrule in [28] 75.5 Naive Bayes in [12] 75.5 MSG in [12] 75.5 RBF in [28] 75.7 NBCFSS in [12] 76.0 C4.5 in [33] 76.0 SMART in [28] 76.8 D-SIDE 77.3 Linear discr. in [28] 77.5 DIPOL92 in [28] 77.6 Logistic discr. in [28] 77.7 30 35 40 45 50 55 Success rate (% correct) PRIMARY TUMOR C4 in [1] 37.8 M [1] in [4] 41.4 Human expert in [22] 42.0 Assistant in [22] 44.0 D-SIDE 50.4 Naive Bayes in [21] 51.0 Successive Bayes in [21] 51.7 75 80 <p> in <ref> [28] </ref> 75.0 Backprop in [28] 75.2 ITrule in [28] 75.5 Naive Bayes in [12] 75.5 MSG in [12] 75.5 RBF in [28] 75.7 NBCFSS in [12] 76.0 C4.5 in [33] 76.0 SMART in [28] 76.8 D-SIDE 77.3 Linear discr. in [28] 77.5 DIPOL92 in [28] 77.6 Logistic discr. in [28] 77.7 30 35 40 45 50 55 Success rate (% correct) PRIMARY TUMOR C4 in [1] 37.8 M [1] in [4] 41.4 Human expert in [22] 42.0 Assistant in [22] 44.0 D-SIDE 50.4 Naive Bayes in [21] 51.0 Successive Bayes in [21] 51.7 75 80 85 90 Success rate (% <p> Experimental results on the Australian, Diabetes, Primary tumor, Hep atitis and Glass datasets. 50 55 60 65 70 75 80 85 90 95 Success rate (% correct) DNA K-NN in <ref> [28] </ref> 85.4 ITrule in [28] 86.5 Cal5 in [28] 86.9 SMART in [28] 88.5 NewID in [28] 90.0 AC2 in [28] 90.0 Bayes tree in [28] 90.5 Backprop in [28] 91.2 CART in [28] 91.5 C4.5 in [28] 92.4 IndCART in [28] 92.7 CASTLE in [28] 92.8 Naive Bayes in [28] <p> Experimental results on the Australian, Diabetes, Primary tumor, Hep atitis and Glass datasets. 50 55 60 65 70 75 80 85 90 95 Success rate (% correct) DNA K-NN in <ref> [28] </ref> 85.4 ITrule in [28] 86.5 Cal5 in [28] 86.9 SMART in [28] 88.5 NewID in [28] 90.0 AC2 in [28] 90.0 Bayes tree in [28] 90.5 Backprop in [28] 91.2 CART in [28] 91.5 C4.5 in [28] 92.4 IndCART in [28] 92.7 CASTLE in [28] 92.8 Naive Bayes in [28] 93.2 Logistic discr. in <p> Experimental results on the Australian, Diabetes, Primary tumor, Hep atitis and Glass datasets. 50 55 60 65 70 75 80 85 90 95 Success rate (% correct) DNA K-NN in <ref> [28] </ref> 85.4 ITrule in [28] 86.5 Cal5 in [28] 86.9 SMART in [28] 88.5 NewID in [28] 90.0 AC2 in [28] 90.0 Bayes tree in [28] 90.5 Backprop in [28] 91.2 CART in [28] 91.5 C4.5 in [28] 92.4 IndCART in [28] 92.7 CASTLE in [28] 92.8 Naive Bayes in [28] 93.2 Logistic discr. in [28] 93.9 Quadratic discr. <p> Experimental results on the Australian, Diabetes, Primary tumor, Hep atitis and Glass datasets. 50 55 60 65 70 75 80 85 90 95 Success rate (% correct) DNA K-NN in <ref> [28] </ref> 85.4 ITrule in [28] 86.5 Cal5 in [28] 86.9 SMART in [28] 88.5 NewID in [28] 90.0 AC2 in [28] 90.0 Bayes tree in [28] 90.5 Backprop in [28] 91.2 CART in [28] 91.5 C4.5 in [28] 92.4 IndCART in [28] 92.7 CASTLE in [28] 92.8 Naive Bayes in [28] 93.2 Logistic discr. in [28] 93.9 Quadratic discr. in [28] 94.1 Linear <p> Experimental results on the Australian, Diabetes, Primary tumor, Hep atitis and Glass datasets. 50 55 60 65 70 75 80 85 90 95 Success rate (% correct) DNA K-NN in <ref> [28] </ref> 85.4 ITrule in [28] 86.5 Cal5 in [28] 86.9 SMART in [28] 88.5 NewID in [28] 90.0 AC2 in [28] 90.0 Bayes tree in [28] 90.5 Backprop in [28] 91.2 CART in [28] 91.5 C4.5 in [28] 92.4 IndCART in [28] 92.7 CASTLE in [28] 92.8 Naive Bayes in [28] 93.2 Logistic discr. in [28] 93.9 Quadratic discr. in [28] 94.1 Linear discr. in [28] 94.1 <p> Experimental results on the Australian, Diabetes, Primary tumor, Hep atitis and Glass datasets. 50 55 60 65 70 75 80 85 90 95 Success rate (% correct) DNA K-NN in <ref> [28] </ref> 85.4 ITrule in [28] 86.5 Cal5 in [28] 86.9 SMART in [28] 88.5 NewID in [28] 90.0 AC2 in [28] 90.0 Bayes tree in [28] 90.5 Backprop in [28] 91.2 CART in [28] 91.5 C4.5 in [28] 92.4 IndCART in [28] 92.7 CASTLE in [28] 92.8 Naive Bayes in [28] 93.2 Logistic discr. in [28] 93.9 Quadratic discr. in [28] 94.1 Linear discr. in [28] 94.1 ALLOC80 in [28] 94.3 <p> Diabetes, Primary tumor, Hep atitis and Glass datasets. 50 55 60 65 70 75 80 85 90 95 Success rate (% correct) DNA K-NN in <ref> [28] </ref> 85.4 ITrule in [28] 86.5 Cal5 in [28] 86.9 SMART in [28] 88.5 NewID in [28] 90.0 AC2 in [28] 90.0 Bayes tree in [28] 90.5 Backprop in [28] 91.2 CART in [28] 91.5 C4.5 in [28] 92.4 IndCART in [28] 92.7 CASTLE in [28] 92.8 Naive Bayes in [28] 93.2 Logistic discr. in [28] 93.9 Quadratic discr. in [28] 94.1 Linear discr. in [28] 94.1 ALLOC80 in [28] 94.3 DIPOL92 in [28] 95.2 RBF <p> atitis and Glass datasets. 50 55 60 65 70 75 80 85 90 95 Success rate (% correct) DNA K-NN in <ref> [28] </ref> 85.4 ITrule in [28] 86.5 Cal5 in [28] 86.9 SMART in [28] 88.5 NewID in [28] 90.0 AC2 in [28] 90.0 Bayes tree in [28] 90.5 Backprop in [28] 91.2 CART in [28] 91.5 C4.5 in [28] 92.4 IndCART in [28] 92.7 CASTLE in [28] 92.8 Naive Bayes in [28] 93.2 Logistic discr. in [28] 93.9 Quadratic discr. in [28] 94.1 Linear discr. in [28] 94.1 ALLOC80 in [28] 94.3 DIPOL92 in [28] 95.2 RBF in [28] 95.9 D-SIDE <p> 50 55 60 65 70 75 80 85 90 95 Success rate (% correct) DNA K-NN in <ref> [28] </ref> 85.4 ITrule in [28] 86.5 Cal5 in [28] 86.9 SMART in [28] 88.5 NewID in [28] 90.0 AC2 in [28] 90.0 Bayes tree in [28] 90.5 Backprop in [28] 91.2 CART in [28] 91.5 C4.5 in [28] 92.4 IndCART in [28] 92.7 CASTLE in [28] 92.8 Naive Bayes in [28] 93.2 Logistic discr. in [28] 93.9 Quadratic discr. in [28] 94.1 Linear discr. in [28] 94.1 ALLOC80 in [28] 94.3 DIPOL92 in [28] 95.2 RBF in [28] 95.9 D-SIDE 97.0 50 55 60 <p> 70 75 80 85 90 95 Success rate (% correct) DNA K-NN in <ref> [28] </ref> 85.4 ITrule in [28] 86.5 Cal5 in [28] 86.9 SMART in [28] 88.5 NewID in [28] 90.0 AC2 in [28] 90.0 Bayes tree in [28] 90.5 Backprop in [28] 91.2 CART in [28] 91.5 C4.5 in [28] 92.4 IndCART in [28] 92.7 CASTLE in [28] 92.8 Naive Bayes in [28] 93.2 Logistic discr. in [28] 93.9 Quadratic discr. in [28] 94.1 Linear discr. in [28] 94.1 ALLOC80 in [28] 94.3 DIPOL92 in [28] 95.2 RBF in [28] 95.9 D-SIDE 97.0 50 55 60 65 70 75 80 <p> 90 95 Success rate (% correct) DNA K-NN in <ref> [28] </ref> 85.4 ITrule in [28] 86.5 Cal5 in [28] 86.9 SMART in [28] 88.5 NewID in [28] 90.0 AC2 in [28] 90.0 Bayes tree in [28] 90.5 Backprop in [28] 91.2 CART in [28] 91.5 C4.5 in [28] 92.4 IndCART in [28] 92.7 CASTLE in [28] 92.8 Naive Bayes in [28] 93.2 Logistic discr. in [28] 93.9 Quadratic discr. in [28] 94.1 Linear discr. in [28] 94.1 ALLOC80 in [28] 94.3 DIPOL92 in [28] 95.2 RBF in [28] 95.9 D-SIDE 97.0 50 55 60 65 70 75 80 Success rate (% correct) <p> (% correct) DNA K-NN in <ref> [28] </ref> 85.4 ITrule in [28] 86.5 Cal5 in [28] 86.9 SMART in [28] 88.5 NewID in [28] 90.0 AC2 in [28] 90.0 Bayes tree in [28] 90.5 Backprop in [28] 91.2 CART in [28] 91.5 C4.5 in [28] 92.4 IndCART in [28] 92.7 CASTLE in [28] 92.8 Naive Bayes in [28] 93.2 Logistic discr. in [28] 93.9 Quadratic discr. in [28] 94.1 Linear discr. in [28] 94.1 ALLOC80 in [28] 94.3 DIPOL92 in [28] 95.2 RBF in [28] 95.9 D-SIDE 97.0 50 55 60 65 70 75 80 Success rate (% correct) BREAST CANCER Human expert <p> <ref> [28] </ref> 85.4 ITrule in [28] 86.5 Cal5 in [28] 86.9 SMART in [28] 88.5 NewID in [28] 90.0 AC2 in [28] 90.0 Bayes tree in [28] 90.5 Backprop in [28] 91.2 CART in [28] 91.5 C4.5 in [28] 92.4 IndCART in [28] 92.7 CASTLE in [28] 92.8 Naive Bayes in [28] 93.2 Logistic discr. in [28] 93.9 Quadratic discr. in [28] 94.1 Linear discr. in [28] 94.1 ALLOC80 in [28] 94.3 DIPOL92 in [28] 95.2 RBF in [28] 95.9 D-SIDE 97.0 50 55 60 65 70 75 80 Success rate (% correct) BREAST CANCER Human expert in [22] 64.0 K-NN in <p> 86.5 Cal5 in <ref> [28] </ref> 86.9 SMART in [28] 88.5 NewID in [28] 90.0 AC2 in [28] 90.0 Bayes tree in [28] 90.5 Backprop in [28] 91.2 CART in [28] 91.5 C4.5 in [28] 92.4 IndCART in [28] 92.7 CASTLE in [28] 92.8 Naive Bayes in [28] 93.2 Logistic discr. in [28] 93.9 Quadratic discr. in [28] 94.1 Linear discr. in [28] 94.1 ALLOC80 in [28] 94.3 DIPOL92 in [28] 95.2 RBF in [28] 95.9 D-SIDE 97.0 50 55 60 65 70 75 80 Success rate (% correct) BREAST CANCER Human expert in [22] 64.0 K-NN in [15] 65.3 Quadratic discr. in <p> SMART in <ref> [28] </ref> 88.5 NewID in [28] 90.0 AC2 in [28] 90.0 Bayes tree in [28] 90.5 Backprop in [28] 91.2 CART in [28] 91.5 C4.5 in [28] 92.4 IndCART in [28] 92.7 CASTLE in [28] 92.8 Naive Bayes in [28] 93.2 Logistic discr. in [28] 93.9 Quadratic discr. in [28] 94.1 Linear discr. in [28] 94.1 ALLOC80 in [28] 94.3 DIPOL92 in [28] 95.2 RBF in [28] 95.9 D-SIDE 97.0 50 55 60 65 70 75 80 Success rate (% correct) BREAST CANCER Human expert in [22] 64.0 K-NN in [15] 65.3 Quadratic discr. in [15] 65.6 Backprop in [15] <p> in <ref> [28] </ref> 90.0 AC2 in [28] 90.0 Bayes tree in [28] 90.5 Backprop in [28] 91.2 CART in [28] 91.5 C4.5 in [28] 92.4 IndCART in [28] 92.7 CASTLE in [28] 92.8 Naive Bayes in [28] 93.2 Logistic discr. in [28] 93.9 Quadratic discr. in [28] 94.1 Linear discr. in [28] 94.1 ALLOC80 in [28] 94.3 DIPOL92 in [28] 95.2 RBF in [28] 95.9 D-SIDE 97.0 50 55 60 65 70 75 80 Success rate (% correct) BREAST CANCER Human expert in [22] 64.0 K-NN in [15] 65.3 Quadratic discr. in [15] 65.6 Backprop in [15] 71.5 Linear discr. in [15] <p> in <ref> [28] </ref> 90.0 Bayes tree in [28] 90.5 Backprop in [28] 91.2 CART in [28] 91.5 C4.5 in [28] 92.4 IndCART in [28] 92.7 CASTLE in [28] 92.8 Naive Bayes in [28] 93.2 Logistic discr. in [28] 93.9 Quadratic discr. in [28] 94.1 Linear discr. in [28] 94.1 ALLOC80 in [28] 94.3 DIPOL92 in [28] 95.2 RBF in [28] 95.9 D-SIDE 97.0 50 55 60 65 70 75 80 Success rate (% correct) BREAST CANCER Human expert in [22] 64.0 K-NN in [15] 65.3 Quadratic discr. in [15] 65.6 Backprop in [15] 71.5 Linear discr. in [15] 71.6 C4 in [15] <p> tree in <ref> [28] </ref> 90.5 Backprop in [28] 91.2 CART in [28] 91.5 C4.5 in [28] 92.4 IndCART in [28] 92.7 CASTLE in [28] 92.8 Naive Bayes in [28] 93.2 Logistic discr. in [28] 93.9 Quadratic discr. in [28] 94.1 Linear discr. in [28] 94.1 ALLOC80 in [28] 94.3 DIPOL92 in [28] 95.2 RBF in [28] 95.9 D-SIDE 97.0 50 55 60 65 70 75 80 Success rate (% correct) BREAST CANCER Human expert in [22] 64.0 K-NN in [15] 65.3 Quadratic discr. in [15] 65.6 Backprop in [15] 71.5 Linear discr. in [15] 71.6 C4 in [15] 72.0 RMHC-PF1 in [36] <p> Backprop in <ref> [28] </ref> 91.2 CART in [28] 91.5 C4.5 in [28] 92.4 IndCART in [28] 92.7 CASTLE in [28] 92.8 Naive Bayes in [28] 93.2 Logistic discr. in [28] 93.9 Quadratic discr. in [28] 94.1 Linear discr. in [28] 94.1 ALLOC80 in [28] 94.3 DIPOL92 in [28] 95.2 RBF in [28] 95.9 D-SIDE 97.0 50 55 60 65 70 75 80 Success rate (% correct) BREAST CANCER Human expert in [22] 64.0 K-NN in [15] 65.3 Quadratic discr. in [15] 65.6 Backprop in [15] 71.5 Linear discr. in [15] 71.6 C4 in [15] 72.0 RMHC-PF1 in [36] 72.3 1Rw in [15]
Reference: 29. <author> P. Myllymaki and H. Tirri. </author> <title> Bayesian case-based reasoning with neural networks. </title> <booktitle> In Proceedings of the IEEE International Conference on Neural Networks, </booktitle> <volume> volume 1, </volume> <pages> pages 422-427, </pages> <address> San Francisco, March 1993. </address> <publisher> IEEE, </publisher> <address> Piscataway, NJ. </address>
Reference-contexts: Consequently, the predictive distributions can also be used directly for any adaptation task, since when the mixture model is constructed, all attributes are treated equally. In addition, such an adaptation computation can be performed efficiently <ref> [31, 30, 29] </ref>. As discussed above, to perform successful adaptation in an application with only weak domain knowledge, a model has to be extracted from the (noisy) set of cases. <p> for each model class M K given the data: P (M K jD) / P (DjM K )P (M K ); K = 1; : : : ; N; 3 If massively parallel hardware is available, the computations can be made even faster since the algorithms can be parallelized easily <ref> [30, 29] </ref>. where the normalizing constant P (D) can be omitted since we only need to compare different model classes. The number of clusters can safely be assumed to be bounded by N , since otherwise the sample size is clearly too small for the learning problem in question.
Reference: 30. <author> P. Myllymaki and H. Tirri. </author> <title> Massively parallel case-based reasoning with probabilistic similarity metrics. </title> <editor> In S. Wess, K.-D. Althoff, and M Richter, editors, </editor> <booktitle> Topics in Case-Based Reasoning, volume 837 of Lecture Notes in Artificial Intelligence, </booktitle> <pages> pages 144-154. </pages> <publisher> Springer-Verlag, </publisher> <year> 1994. </year>
Reference-contexts: Consequently, the predictive distributions can also be used directly for any adaptation task, since when the mixture model is constructed, all attributes are treated equally. In addition, such an adaptation computation can be performed efficiently <ref> [31, 30, 29] </ref>. As discussed above, to perform successful adaptation in an application with only weak domain knowledge, a model has to be extracted from the (noisy) set of cases. <p> for each model class M K given the data: P (M K jD) / P (DjM K )P (M K ); K = 1; : : : ; N; 3 If massively parallel hardware is available, the computations can be made even faster since the algorithms can be parallelized easily <ref> [30, 29] </ref>. where the normalizing constant P (D) can be omitted since we only need to compare different model classes. The number of clusters can safely be assumed to be bounded by N , since otherwise the sample size is clearly too small for the learning problem in question.
Reference: 31. <author> P. Myllymaki and H. Tirri. </author> <title> Constructing computationally efficient Bayesian models via unsupervised clustering. </title> <editor> In A.Gammerman, editor, </editor> <booktitle> Probabilistic Reasoning and Bayesian Belief Networks, </booktitle> <pages> pages 237-248. </pages> <publisher> Alfred Waller Publishers, </publisher> <address> Suffolk, </address> <year> 1995. </year>
Reference-contexts: Consequently, the predictive distributions can also be used directly for any adaptation task, since when the mixture model is constructed, all attributes are treated equally. In addition, such an adaptation computation can be performed efficiently <ref> [31, 30, 29] </ref>. As discussed above, to perform successful adaptation in an application with only weak domain knowledge, a model has to be extracted from the (noisy) set of cases.
Reference: 32. <author> C. Owens. </author> <title> Integrating feature extraction and memory search. </title> <journal> Machine Learning, </journal> <volume> 10(3) </volume> <pages> 311-340, </pages> <year> 1993. </year>
Reference-contexts: Traditional case-based reasoning systems base their structural adaptation on adjustment heuristics applied to the values of interest in relevant cases. This set of relevant cases is determined during the case retrieval phase typically by using a nearest neighbor matching <ref> [32] </ref> or indexes created by induction algorithms [33]. For many problem domains the basic approach of storing all the cases suffers from several drawbacks. First, the run-time computational costs for matching algorithms are high when the size of the case base grows large.
Reference: 33. <author> J.R. Quinlan. </author> <title> Improved use of continuous attributes in C4.5. </title> <journal> Journal of Artificial Intelligence Research, </journal> <volume> 4 </volume> <pages> 77-90, </pages> <year> 1996. </year>
Reference-contexts: Traditional case-based reasoning systems base their structural adaptation on adjustment heuristics applied to the values of interest in relevant cases. This set of relevant cases is determined during the case retrieval phase typically by using a nearest neighbor matching [32] or indexes created by induction algorithms <ref> [33] </ref>. For many problem domains the basic approach of storing all the cases suffers from several drawbacks. First, the run-time computational costs for matching algorithms are high when the size of the case base grows large. <p> 81.9 Bayes tree in [28] 82.9 SMART in [28] 84.2 Backprop in [28] 84.6 IndCART in [28] 84.8 TAN in [12] 85.1 CASTLE in [28] 85.2 CART in [28] 85.5 RBF in [28] 85.5 DIPOL92 in [28] 85.9 Linear discr. in [28] 85.9 Logistic discr. in [28] 85.9 C4.5 in <ref> [33] </ref> 86.0 Naive Bayes in [13] 86.2 BNG in [12] 86.2 ITrule in [28] 86.3 NBCFSS in [12] 86.7 MS in [12] 86.8 D-SIDE 87.2 60 65 70 75 80 Success rate (% correct) DIABETES K-NN in [28] 67.6 ALLOC80 in [28] 69.9 CN2 in [28] 71.1 NewID in [28] 71.1 <p> 72.9 Quadratic discr. in [28] 73.8 Flexible Bayes in [16] 73.9 CASTLE in [28] 74.2 CART in [28] 74.5 Cal5 in [28] 75.0 Backprop in [28] 75.2 ITrule in [28] 75.5 Naive Bayes in [12] 75.5 MSG in [12] 75.5 RBF in [28] 75.7 NBCFSS in [12] 76.0 C4.5 in <ref> [33] </ref> 76.0 SMART in [28] 76.8 D-SIDE 77.3 Linear discr. in [28] 77.5 DIPOL92 in [28] 77.6 Logistic discr. in [28] 77.7 30 35 40 45 50 55 Success rate (% correct) PRIMARY TUMOR C4 in [1] 37.8 M [1] in [4] 41.4 Human expert in [22] 42.0 Assistant in [22] <p> 45 50 55 Success rate (% correct) PRIMARY TUMOR C4 in [1] 37.8 M [1] in [4] 41.4 Human expert in [22] 42.0 Assistant in [22] 44.0 D-SIDE 50.4 Naive Bayes in [21] 51.0 Successive Bayes in [21] 51.7 75 80 85 90 Success rate (% correct) HEPATITIS C4.5 in <ref> [33] </ref> 80.9 Assistant in [15] 83.0 Naive Bayes in [15] 84.0 1Rw in [15] 84.5 D-SIDE 88.0 30 35 40 45 50 55 60 65 70 75 80 85 Success rate (% correct) GLASS 1Rw in [15] 62.2 Flexible Bayes in [16] 66.2 Naive Bayes in [13] 71.5 C4.5 in [33] <p> <ref> [33] </ref> 80.9 Assistant in [15] 83.0 Naive Bayes in [15] 84.0 1Rw in [15] 84.5 D-SIDE 88.0 30 35 40 45 50 55 60 65 70 75 80 85 Success rate (% correct) GLASS 1Rw in [15] 62.2 Flexible Bayes in [16] 66.2 Naive Bayes in [13] 71.5 C4.5 in [33] 71.6 D-SIDE 87.4 Fig. 1. <p> in [16] 80.0 Backprop. in [15] 80.6 MC1 in [36] 80.7 NBCFSS in [12] 81.9 BNG in [12] 82.2 RMHC-P in [36] 82.3 MSG in [12] 83.0 Naive Bayes in [16] 83.3 D-SIDE 84.8 65 70 75 80 85 Success rate (% correct) LYMPHOGRAPHY K-NN in [15] 72.9 C4.5 in <ref> [33] </ref> 77.0 NBCFSS in [12] 77.7 ID3 in [15] 78.4 Assistant in [15] 79.0 MSG in [12] 80.4 TAN in [12] 85.0 Naive Bayes in [15] 85.1 D-SIDE 86.6 Fig. 2. Experimental results on the DNA, Breast cancer, Iris, Heart disease and Lymphography databases. Table 1.
Reference: 34. <author> R. Schank. </author> <title> Dynamic Memory: A theory of reminding and learning in computers and people. </title> <publisher> Cambridge University Press, </publisher> <year> 1982. </year>
Reference-contexts: This general definition does not by itself place CBR systems in any particular place in the spectrum discussed above|the adaptation step in case-based reasoning can embed a substantial amount of domain knowledge. However, while CBR has its roots in cognitive modeling <ref> [34, 20] </ref>, from an engineering point of view it can be seen as an attempt to address many of the problems recognized in the design of traditional knowledge-intensive systems, in particular the brittleness problem [2].
Reference: 35. <author> D.W. Scott. </author> <title> Multivariate Density Estimation. Theory, Practice, and Visualization. </title> <publisher> John Wiley & Sons, </publisher> <address> New York, </address> <year> 1992. </year>
Reference-contexts: This basic case-based approach in numerical domains can in probabilistic terms be understood as a form of using kernel density estimators for inference (see e.g., <ref> [35] </ref>). In practice a situation where all the case distributions are required for a good approximation is very rare, as the case base usually exhibits some cluster structure in the attribute domain space.
Reference: 36. <author> D. Skalak. </author> <title> Prototype and feature selection by sampling and random mutation hill climbing algorithms. </title> <booktitle> In Machine Learning: Proceedings of the Eleventh International Conference, </booktitle> <pages> pages 293-301, </pages> <year> 1994. </year>
Reference-contexts: [28] 95.2 RBF in [28] 95.9 D-SIDE 97.0 50 55 60 65 70 75 80 Success rate (% correct) BREAST CANCER Human expert in [22] 64.0 K-NN in [15] 65.3 Quadratic discr. in [15] 65.6 Backprop in [15] 71.5 Linear discr. in [15] 71.6 C4 in [15] 72.0 RMHC-PF1 in <ref> [36] </ref> 72.3 1Rw in [15] 72.7 CN2 in [15] 73.0 D-SIDE 76.6 CART in [15] 77.1 Assistant in [15] 78.0 Successive Bayes in [21] 78.4 Naive Bayes in [21] 79.2 M (12) in [4] 80.0 Success rate (% correct) IRIS MC1 in [36] 93.5 RMHC-PF1 in [36] 94.7 Flexible Bayes in <p> [15] 71.6 C4 in [15] 72.0 RMHC-PF1 in <ref> [36] </ref> 72.3 1Rw in [15] 72.7 CN2 in [15] 73.0 D-SIDE 76.6 CART in [15] 77.1 Assistant in [15] 78.0 Successive Bayes in [21] 78.4 Naive Bayes in [21] 79.2 M (12) in [4] 80.0 Success rate (% correct) IRIS MC1 in [36] 93.5 RMHC-PF1 in [36] 94.7 Flexible Bayes in [16] 95.3 CART in [16] 95.3 C4.5 in [16] 95.3 K-NN in [15] 96.0 Naive Bayes in [16] 96.0 Backprop. in [15] 96.7 Quadratic discr. in [15] 97.3 Linear discr. in [15] 98.0 D-SIDE 98.0 70 75 80 85 Success rate (% <p> [15] 72.0 RMHC-PF1 in <ref> [36] </ref> 72.3 1Rw in [15] 72.7 CN2 in [15] 73.0 D-SIDE 76.6 CART in [15] 77.1 Assistant in [15] 78.0 Successive Bayes in [21] 78.4 Naive Bayes in [21] 79.2 M (12) in [4] 80.0 Success rate (% correct) IRIS MC1 in [36] 93.5 RMHC-PF1 in [36] 94.7 Flexible Bayes in [16] 95.3 CART in [16] 95.3 C4.5 in [16] 95.3 K-NN in [15] 96.0 Naive Bayes in [16] 96.0 Backprop. in [15] 96.7 Quadratic discr. in [15] 97.3 Linear discr. in [15] 98.0 D-SIDE 98.0 70 75 80 85 Success rate (% correct) HEART DISEASE 1Rw <p> Naive Bayes in [16] 96.0 Backprop. in [15] 96.7 Quadratic discr. in [15] 97.3 Linear discr. in [15] 98.0 D-SIDE 98.0 70 75 80 85 Success rate (% correct) HEART DISEASE 1Rw in [15] 76.6 K-NN in [15] 79.2 Flexible Bayes in [16] 80.0 Backprop. in [15] 80.6 MC1 in <ref> [36] </ref> 80.7 NBCFSS in [12] 81.9 BNG in [12] 82.2 RMHC-P in [36] 82.3 MSG in [12] 83.0 Naive Bayes in [16] 83.3 D-SIDE 84.8 65 70 75 80 85 Success rate (% correct) LYMPHOGRAPHY K-NN in [15] 72.9 C4.5 in [33] 77.0 NBCFSS in [12] 77.7 ID3 in [15] 78.4 <p> [15] 97.3 Linear discr. in [15] 98.0 D-SIDE 98.0 70 75 80 85 Success rate (% correct) HEART DISEASE 1Rw in [15] 76.6 K-NN in [15] 79.2 Flexible Bayes in [16] 80.0 Backprop. in [15] 80.6 MC1 in <ref> [36] </ref> 80.7 NBCFSS in [12] 81.9 BNG in [12] 82.2 RMHC-P in [36] 82.3 MSG in [12] 83.0 Naive Bayes in [16] 83.3 D-SIDE 84.8 65 70 75 80 85 Success rate (% correct) LYMPHOGRAPHY K-NN in [15] 72.9 C4.5 in [33] 77.0 NBCFSS in [12] 77.7 ID3 in [15] 78.4 Assistant in [15] 79.0 MSG in [12] 80.4 TAN in [12] 85.0
Reference: 37. <author> H. Tirri, P. Kontkanen, and P. Myllymaki. </author> <title> Probabilistic instance-based learning. </title> <editor> In L. Saitta, editor, </editor> <booktitle> Machine Learning: Proceedings of the Thirteenth International Conference, </booktitle> <pages> pages 507-515. </pages> <publisher> Morgan Kaufmann Publishers, </publisher> <year> 1996. </year>
Reference-contexts: The goodness of the results with respect to the memory based methods are not especially surprising as they can be seen as approximations to the finite mixture approach presented here <ref> [37] </ref>. A more interesting observation is that the EM algorithm finds MAP estimates that outperform also all other Bayesian approaches present in the StatLog comparison, including CASTLE, Naive Bayes, and Bayes tree algorithms.
Reference: 38. <author> D.M. Titterington, A.F.M. Smith, and U.E. Makov. </author> <title> Statistical Analysis of Finite Mixture Distributions. </title> <publisher> John Wiley & Sons, </publisher> <address> New York, </address> <year> 1985. </year>
Reference-contexts: In fact, construction of prototypical structures such as Generalized Episodes by grouping cases sharing similar properties [26] can be understood as approximations to such simpler mixture structures <ref> [11, 38] </ref>. Thus the probabilistic approach offers also a natural way of structuring the case storage 1 . There are several advantages of using the above probabilistic view of CBR. <p> Let us now assume that we are given a clustering of the problem domain, i.e., the data instantiation space is partitioned into K groups of vectors. In finite mixture models <ref> [11, 38] </ref> the problem domain probability distribution is approximated as a weighted sum of mixture distributions: P ( ~ d) = k=1 where the values of the discrete clustering random variable Y correspond to the separate clusters of the instantiation space, and each mixture distribution P ( ~ djY = y
Reference: 39. <author> I. Watson and F. Marir. </author> <title> Case-based reasoning: A review. </title> <journal> The Knowledge Engineering Review, </journal> <volume> 9(4) </volume> <pages> 327-354, </pages> <year> 1994. </year> <title> This article was processed using the L A T E X macro package with LLNCS style </title>
Reference-contexts: In its most general form, case-based reasoning (CBR) can be viewed as an approach to computational reasoning that solves new problems by adapting previously successful solutions to similar problems <ref> [20, 39] </ref>. This general definition does not by itself place CBR systems in any particular place in the spectrum discussed above|the adaptation step in case-based reasoning can embed a substantial amount of domain knowledge. <p> Consequently, one of the advocated potential advantages of case-based reasoning has been the fact that CBR systems can be built in cases where only weak prior domain knowledge is available <ref> [39] </ref>, and the "top-down" approaches with explicit model representations cannot be applied. In such domains CBR systems construct implicit domain models. These models are defined by a case history (a set of past cases) together with the accompanying domain-specific procedures for case matching and adaptation. <p> However, a more fundamental problem in storing all cases in real applications is related to the well-known"overfitting" problem inherent to any inductive model construction method for noisy domains. Finally, the adaptation performance of a case-based system can be very sensitive to the selection of a proper similarity function <ref> [39] </ref>. In order to address some of these drawbacks of the basic approach, many suggestions for structuring the case storage have been proposed in the literature.
References-found: 39

