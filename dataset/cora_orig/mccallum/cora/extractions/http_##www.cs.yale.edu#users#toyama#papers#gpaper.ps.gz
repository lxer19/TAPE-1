URL: http://www.cs.yale.edu/users/toyama/papers/gpaper.ps.gz
Refering-URL: http://www.cs.yale.edu/users/toyama/toyama.html
Root-URL: http://www.cs.yale.edu
Title: Feature-Based Visual Servoing and its Application to Telerobotics  
Author: Gregory D. Hager, a Gerhard Grunwald, b and Kentaro Toyama a 
Address: P.O. Box 208285 Yale Station, Yale University, New Haven, CT 06520-8285, U.S.A.  Munchenerstr. 20, D-82234 Oberpfaffenhofen, Germany  
Affiliation: a Department of Computer Science,  b German Aerospace Research Establishment (DLR), Institute of Robotics and System Dynamics,  
Abstract: Recent advances in visual servoing theory and practice now make it possible to accurately and robustly position a robot manipulator relative to a target. Both the vision and control algorithms are extremely simple, but they must be initialized on task-relevant features in order to be applied. Consequently, they are particularly well-suited to telerobotics systems where an operator can initialize the system but round-trip delay prohibits direct operator feedback during motion. This paper describes the basic theory behind feature-based visual servoing and discusses the issues involved in integrating visual servoing into the ROTEX space teleoperation system. 
Abstract-found: 1
Intro-found: 1
Reference: 1. <author> P. Anandan. </author> <title> A computational framework and an algorithm for the measurement of structure from motion. </title> <journal> Int. J. Computer Vision, </journal> <volume> 2 </volume> <pages> 283-310, </pages> <year> 1989. </year>
Reference: 2. <author> B. Brunner, G. Hirzinger, K. Landzettel, and J. Heindl. </author> <title> Multisensory shared autonomy and tele-sensor-programming: Key issues in the space robot technology experiment rotex. </title> <booktitle> Proceedings of the IEEE/RSJ International Conference on Intelligent Robots and Systems, </booktitle> <address> Yokohama, Japan, </address> <year> 1993. </year>
Reference-contexts: Unfortunately, our current lack of understanding of sensor data interpretation, robot motion control and artificial intelligence makes the prospect of complete autonomy for complex tasks unlikely in the near future <ref> [2] </ref>. One way out of this dilemma is sensor-based telerobotics. In particular, space operation tasks for which telerobotics could play an increasingly large role include inspection, assembly, servicing, and repair [10]. When teleoperating in space, relay satellites and computer networks insert a large delay into round-trip communication.
Reference: 3. <author> G. D. Hager. </author> <title> Six DOF visual control of relative position. </title> <institution> DCS RR-1018, Yale University, </institution> <address> New Haven, CT, </address> <month> May </month> <year> 1994. </year>
Reference-contexts: h i (L): The Jacobian of g is a function of the position of the observed point, and the Jacobian of h is a function of the fixed point on the line: J g (P ) = @x fi fi P @h fi fi fi : (9) As described in <ref> [3] </ref>, it is possible to estimate the parameters of both lines and points using relatively straightforward techniques. We refer the reader to that publication for the technical details of estimation and control and proceed to describe the structure of three representative controllers. <p> Applying the methods described above produces a regulator that can control all six degrees of freedom of a robot manipulator. For details on the form of the Jacobian and the estimation of unknown values, we refer the reader to <ref> [3] </ref>. Note that the assignment of features in the problem above was arbitrary. That is, the roles of points and lines can be interchanged without changing the final result.
Reference: 4. <author> G. D. Hager. </author> <title> Real-time feature tracking and projective invariance as a basis for hand-eye coordination. </title> <institution> DCS RR-993, Yale University, </institution> <address> New Haven, CT, </address> <month> November </month> <year> 1993. </year> <booktitle> To be presented at the 1994 International Conference on Computer Vision and Pattern Recognition. </booktitle>
Reference-contexts: As this example illustrates, a variety of geometric constructions can be used to provide increased functionality for visual serving. As with the basic servoing routines, these constructions rely on tracking specific features in an image and computing values based on those features. 2.3. Feature Tracking In <ref> [4] </ref>, a feature-based tracking system was described. The tracking system is based on two central ideas: window-based image processing and state-based programming of networks of tracked features. A window is an image defined by its height, width, position, and orientation in device (framebuffer) coordinates.
Reference: 5. <author> G. D. Hager, W.-C. Chang, and A. S. Morse. </author> <title> Robot feedback control based on stereo vision: Towards calibration-free hand-eye coordination. </title> <booktitle> In Proceedings of the 1994 IEEE International Conference on Robotics and Automation, </booktitle> <pages> pp. 2850-2856. </pages>
Reference-contexts: One reason is that most classical work on visual servoing relies heavily on a calibrated hand-eye system. As noted above, maintaining accurate calibration in orbit can be difficult. A second reason is that most vision algorithms are too complex to execute on hardware that is space-qualified. In <ref> [5] </ref>, an approach to feature-based visual servoing using closed-loop control was developed. The main advantages of the approach are that it is extremely robust to calibration error and that it uses simple visual features that are computationally simple to track. <p> Thus, this basic control formulation can operate on a wide variety of systems with different types of input. All of these controllers have been implemented and tested as described in <ref> [5] </ref>. 2.2. Geometry and Visual Servoing The controllers described above do not have to be based on directly observed features. Various types of geometric constructions can be used to define "virtual" lines and points.
Reference: 6. <author> G. Hirzinger. </author> <title> Rotex the first robot in space. </title> <booktitle> In International Conference on Advanced Robotics. </booktitle> <year> 1993. </year>
Reference-contexts: One way of lessening the dependence on prior geometric knowledge is to make remote operations sensor-based. Experience has shown that proper use of closed-loop control algorithms operating in orbit can significantly enhance the capabilities of the teleoperation system <ref> [6] </ref>. To date, there has been little progress in the use of visual feedback to support tele-operation. One reason is that most classical work on visual servoing relies heavily on a calibrated hand-eye system. As noted above, maintaining accurate calibration in orbit can be difficult. <p> If this can be done in the graphic simulation, visual servoing is essentially no more difficult than pointing to where the robot should go. 3.3. Integration into ROTEX The ROTEX sensor-based telerobotic system <ref> [6] </ref> is based on the shared autonomy concept that distributes intelligence to man and machine [8]. Global tasks like visual servoing are specified interactively by a human operator and carried out by local sensory feedback loops executed by the remote robot system. <p> This virtual world allows the operator to move around and "interact" with the robot and its environment. In addition, all remote ORU operations are mimicked in the graphical simulation. In order to support visual servoing, the functionality of the ROTEX ground station <ref> [6] </ref> will be expanded to include a menu-driven programming environment, the image processing system, and the capability of overlaying remote camera images with wireframe models of known objects. These visual aids make the feature selection easier for the operator.
Reference: 7. <author> G. Hirzinger, G. Grunwald, B. Brunner, and H. Heindl. </author> <title> A sensor-based telerobotic system for the space robot experiment rotex. </title> <booktitle> 2. International Symposium on Experimental Robotics, 1991. </booktitle> <address> Toulouse, France. </address>
Reference-contexts: 1. Introduction Automation and robotics are rapidly becoming extremely attractive areas within space technology. They hold the promise of assembly, servicing, and repair with a minimal number of expensive manned missions <ref> [7] </ref>. However, unlike a factory environment, space operations require the ability to work in an environment which is unstructured. For this reason, some amount of autonomous behavior is necessary to perform complex, diverse tasks.
Reference: 8. <author> G. Hirzinger, H. Heindl, K. Landzettel, and B. Brunner. </author> <title> Multisensory shared autonomy: A key issue in the space robot technology experiment rotex. </title> <booktitle> IEEE Conference on Intelligent Robots and Systems (IROS), </booktitle> <address> Raleigh, USA, </address> <year> 1992. </year>
Reference-contexts: If this can be done in the graphic simulation, visual servoing is essentially no more difficult than pointing to where the robot should go. 3.3. Integration into ROTEX The ROTEX sensor-based telerobotic system [6] is based on the shared autonomy concept that distributes intelligence to man and machine <ref> [8] </ref>. Global tasks like visual servoing are specified interactively by a human operator and carried out by local sensory feedback loops executed by the remote robot system. Coarse task-level planning activities are performed by human intelligence while fine path planning on manipulator level takes place through sensor-based control [13].
Reference: 9. <author> T. S. Huang and C. H. Lee. </author> <title> Motion and structure from orthographic projection. </title> <journal> IEEE Trans. Pattern Anal. Mach. Intelligence, </journal> <volume> 11(5) </volume> <pages> 536-540, </pages> <month> May </month> <year> 1989. </year>
Reference: 10. <author> D. </author> <title> Lavery. </title> <booktitle> Perspectives on future space telerobotics technology. In International Conference on Advanced Robotics. </booktitle> <year> 1993. </year>
Reference-contexts: One way out of this dilemma is sensor-based telerobotics. In particular, space operation tasks for which telerobotics could play an increasingly large role include inspection, assembly, servicing, and repair <ref> [10] </ref>. When teleoperating in space, relay satellites and computer networks insert a large delay into round-trip communication. Hence sensor data such as video images cannot be used as direct, real-time feedback by a remote operator.
Reference: 11. <author> H. Longuet-Higgins. </author> <title> A computer algorithm for reconstructing a scene from two projections. </title> <journal> Nature, </journal> <volume> 293 </volume> <pages> 133-135, </pages> <year> 1981. </year>
Reference: 12. <author> J. Mundy and A. Zisserman. </author> <title> Geometric Invariance in Computer Vision. </title> <publisher> MIT Press, </publisher> <address> Cam-bridge, Mass., </address> <year> 1992. </year>
Reference: 13. <author> T. B. Sheridan. </author> <title> Merging mind and machine. </title> <journal> Technology Review, </journal> <pages> pages 33 -40, </pages> <year> 1989. </year>
Reference-contexts: Global tasks like visual servoing are specified interactively by a human operator and carried out by local sensory feedback loops executed by the remote robot system. Coarse task-level planning activities are performed by human intelligence while fine path planning on manipulator level takes place through sensor-based control <ref> [13] </ref>. The shared autonomy concept also allows shared control in which the robot is guided by local sensory feedback as well as by the human operator via the teleoperational device.
Reference: 14. <author> C. Taylor and D. Kriegman. </author> <title> Structure and motion from line segments in multiple images. </title> <booktitle> In IEEE Int. Conf. on Robotics and Automation, </booktitle> <pages> pages 1615-1620. </pages> <year> 1992. </year>
Reference: 15. <author> C. Tomasi and T. Kanade. </author> <title> Shape and motion from image streams: a factorization method, full report on the orthographic case. </title> <address> CMU-CS 92-104, CMU, </address> <year> 1992. </year>
References-found: 15

