URL: ftp://ftp.cc.gatech.edu/pub/coc/tech_reports/1993/GIT-CC-93-42.ps.Z
Refering-URL: http://www.cs.gatech.edu/tech_reports/index.93.html
Root-URL: 
Email: amir@cc.gatech.edu;  gbenson@hto.usc.edu;  farach@dimacs.rutgers.edu;  
Phone: 90089-1113; (213) 740-2404;  (908) 932-5928;  
Title: Let Sleeping Files Lie: Pattern Matching in Z-compressed Files  
Author: Amihood Amir Gary Benson Martin Farach 
Note: Partially supported by NSF grant CCR-92-23699 and IRI-90-13055.  Partially supported by NSF grant DMS-90-05833. DIMACS,  Supported by DIMACS under NSF contract STC-88-09648.  
Date: July 1993  
Address: GIT-CC-93/42  Atlanta, Georgia 30332-0280  Atlanta, GA 30332-0280; (404) 853-0083;  DRB 155, 1024 W. 36th Pl., Los Angeles, CA  Piscataway, NJ 08855;  
Affiliation: Georgia Tech U. of Southern California DIMACS and Rutgers  College of Computing Georgia Institute of Technology  College of Computing, Georgia Institute of Technology,  Department of Mathematics, University of Southern California,  Box 1179, Rutgers University,  
Abstract: The most effective general purpose compression algorithms are adaptive, in that the text represented by each compression symbol is determined dynamically by the data. As a result, the encoding of a substring depends on its location. Thus the same substring may "look different" every time it appears in the compressed text. In this paper we consider pattern matching without decompression in the UNIX Z-compression. This is a variant of the Lempel-Ziv adaptive compression scheme. If n is the length of the compressed text and m is the length of the pattern, our algorithms find the first pattern occurrence in time O(n + m 2 ) or O(n log m + m). We also introduce a new criterion to measure compressed matching algorithms, that of extra space. We show how to modify our algorithms to achieve a tradeoff between the amount of extra space used and the algorithm's time complexity. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> A. Amir and G. Benson. </author> <title> Efficient two dimensional compressed matching. </title> <booktitle> Proc. of Data Compression Conference, Snow Bird, Utah, </booktitle> <pages> pages 279-288, </pages> <month> Mar </month> <year> 1992. </year>
Reference-contexts: We now need algorithms for pattern matching in time and space proportional to the compressed size, i.e. without the need to decompress. The compressed matching problem was formally defined by Amir and Benson <ref> [2, 1] </ref> as follows: Let = s 1 s u be a text string of length u over alphabet = fa 1 ; . . .; a q g. Let :c = t 1 t n be a compression of of length n u. <p> OUTPUT: The first text location i such that there is a pattern occurrence at s i , i.e. s i+j1 = p j ; j = 1; . . .; m. Amir and Benson <ref> [2, 1] </ref> also defined a compressed matching to be efficient if its time complexity is o (u), almost optimal if its time complexity is O (n log m + m) and optimal if it runs in time O (n + m). <p> That algorithm's running time was o (u) but ( u m ). Since the two dimensional run length compression size may be as small as n = p u, their algorithm is clearly far from optimal. Amir and Benson <ref> [2, 1] </ref> developed an almost optimal O (n log m) algorithm for two dimensional run-length compression, and Amir, Benson and Farach [3] devised an optimal O (n) algorithm for this problem. The main difficulty in the latter algorithms is due to the fact that the text is two dimensional. <p> We conclude with a plethora of open problems in section 6. 2 Preliminaries Definitions: Let = s 1 s u be a string over alphabet = fa 1 ; . . . ; a q g. The Z-compression :Z of is the string :Z <ref> [1] </ref> :Z [n], where 1 :Z [i] n + q 1, for i = 1; . . .; n. 1 This is essentially the algorithm that decompresses and searches in the decompressed string. <p> Theorem 2.2 The value assignment to location l in :Z, 1 l n 1, causes the definition and creation of node number l + q in the dictionary trie. Proof: By induction on l. For l = 1 we have :Z <ref> [1] </ref> = i, where s 1 = a i . The node added has string s 1 s 2 . This does not appear in the tree and thus causes a new node to be created as 4 a child of node number i. <p> The KMP automaton tells us that prefixes of P that are suffixes of p 1 p l end at indices 5, 2 and 1. From step 3, we know that of these, only l 3 = 1 has an entry in table N 1 (e.g. N 1 <ref> [1; j] </ref> = 1). But now we know the entries N 1 [2; j] = N 1 [5; j] = N 1 [1; j] = 1. <p> From step 3, we know that of these, only l 3 = 1 has an entry in table N 1 (e.g. N 1 <ref> [1; j] </ref> = 1). But now we know the entries N 1 [2; j] = N 1 [5; j] = N 1 [1; j] = 1. For all i such that l i l i 0 set N [l i ; j] N [l i 0 ; j]; if l i 0 6= 0; Time: Every table entry in the column is filled once.
Reference: [2] <author> A. Amir and G. Benson. </author> <title> Two-dimensional periodicity and its application. </title> <booktitle> Proc. of 3rd Symposium on Discrete Algorithms, </booktitle> <address> Orlando, FL, </address> <pages> pages 440-452, </pages> <month> Jan </month> <year> 1992. </year>
Reference-contexts: We now need algorithms for pattern matching in time and space proportional to the compressed size, i.e. without the need to decompress. The compressed matching problem was formally defined by Amir and Benson <ref> [2, 1] </ref> as follows: Let = s 1 s u be a text string of length u over alphabet = fa 1 ; . . .; a q g. Let :c = t 1 t n be a compression of of length n u. <p> OUTPUT: The first text location i such that there is a pattern occurrence at s i , i.e. s i+j1 = p j ; j = 1; . . .; m. Amir and Benson <ref> [2, 1] </ref> also defined a compressed matching to be efficient if its time complexity is o (u), almost optimal if its time complexity is O (n log m + m) and optimal if it runs in time O (n + m). <p> That algorithm's running time was o (u) but ( u m ). Since the two dimensional run length compression size may be as small as n = p u, their algorithm is clearly far from optimal. Amir and Benson <ref> [2, 1] </ref> developed an almost optimal O (n log m) algorithm for two dimensional run-length compression, and Amir, Benson and Farach [3] devised an optimal O (n) algorithm for this problem. The main difficulty in the latter algorithms is due to the fact that the text is two dimensional. <p> From step 3, we know that of these, only l 3 = 1 has an entry in table N 1 (e.g. N 1 [1; j] = 1). But now we know the entries N 1 <ref> [2; j] </ref> = N 1 [5; j] = N 1 [1; j] = 1.
Reference: [3] <author> A. Amir, G. Benson, and M. Farach. Witness-free dueling: </author> <title> The perfect crime! (or how to match in a compressed two-dimensional array). </title> <note> submitted for publication, 1993. 17 </note>
Reference-contexts: Since the two dimensional run length compression size may be as small as n = p u, their algorithm is clearly far from optimal. Amir and Benson [2, 1] developed an almost optimal O (n log m) algorithm for two dimensional run-length compression, and Amir, Benson and Farach <ref> [3] </ref> devised an optimal O (n) algorithm for this problem. The main difficulty in the latter algorithms is due to the fact that the text is two dimensional. In comparison, the algorithm for compressed string matching is straight-forward.
Reference: [4] <author> A. Amir, G.M. Landau, and U. Vishkin. </author> <title> Efficient pattern matching with scaling. </title> <journal> Journal of Algorithms, </journal> <volume> 13(1) </volume> <pages> 2-32, </pages> <year> 1992. </year>
Reference-contexts: The first compressed matching algorithms were side effects of papers by Eilam-Tsoreff and Vishkin [6] and Amir, Landau and Vishkin <ref> [4] </ref>. The techniques of Eilam-Tsoreff and Vishkin give a trivial optimal algorithm for compressed string matching under the run-length compression. Amir, Landau and Vishkin showed an efficient algorithm for two dimensional compressed matching. That algorithm's running time was o (u) but ( u m ). <p> Example: P = abcab, P (j) = bca. Then: * N 1 <ref> [4; j] </ref> = 1 because the longest suffix of abcajbca which is a prefix of P has length four and uses only the last letter from p 1 p 4 . * N 2 [4; j] = 4 because the first occurrence of P in abcajbca uses all four letters of <p> Example: P = abcab, P (j) = bca. Then: * N 1 <ref> [4; j] </ref> = 1 because the longest suffix of abcajbca which is a prefix of P has length four and uses only the last letter from p 1 p 4 . * N 2 [4; j] = 4 because the first occurrence of P in abcajbca uses all four letters of p 1 p 4 . Once tables N 1 and N 2 are constructed, we can answer queries Q 1 and Q 2 in constant time in the following fashion.
Reference: [5] <author> M. Dietzfelbinger, A. Karlin, K. Mehlhorn, F. Meyer auf der Heide, H. Rohnert, and R. Tarjan. </author> <title> Dynamic perfect hashing: Upper and lower bounds. </title> <booktitle> In Proc. of the 20th Ann. ACM Symp. on Theory of Computing, </booktitle> <pages> pages 524-531, </pages> <month> October </month> <year> 1988. </year>
Reference-contexts: From step 3, we know that of these, only l 3 = 1 has an entry in table N 1 (e.g. N 1 [1; j] = 1). But now we know the entries N 1 [2; j] = N 1 <ref> [5; j] </ref> = N 1 [1; j] = 1. For all i such that l i l i 0 set N [l i ; j] N [l i 0 ; j]; if l i 0 6= 0; Time: Every table entry in the column is filled once. <p> The balanced search tree means O (log m) lookup time. The hashing scheme is the preferred practical choice and gives constant time per lookup w.h.p.. A dynamic-hashing algorithm with O (1) expected amortized cost per insertion was introduced by <ref> [5] </ref>. Every element is verified for being an internal chunk. If it is, we need to check if the concatenation of the next chunk's first element still gives us an internal chunk.
Reference: [6] <author> T. Eilam-Tsoreff and U. Vishkin. </author> <title> Matching patterns in a string subject to multilinear trasformations. Procedings of the International Workshop on Sequences, Combinatorics, Compression, Security and Transmission, </title> <type> Salerno, </type> <institution> Italy, </institution> <month> June </month> <year> 1988. </year>
Reference-contexts: The first compressed matching algorithms were side effects of papers by Eilam-Tsoreff and Vishkin <ref> [6] </ref> and Amir, Landau and Vishkin [4]. The techniques of Eilam-Tsoreff and Vishkin give a trivial optimal algorithm for compressed string matching under the run-length compression. Amir, Landau and Vishkin showed an efficient algorithm for two dimensional compressed matching.
Reference: [7] <author> M. Gu, M. Farach, and R. Beigel. </author> <title> An efficient algorithm for dynamic text indexing. </title> <type> Personal communication, </type> <note> submitted for publication. </note>
Reference-contexts: In the failure tree, let p (v) be the parent of v and let E (v) be the edge from p (v) to v. In <ref> [7] </ref>, Gu, Farach and Beigel (GFB) introduced the border tree T S b = (V; E; L) as follows: * V is a subset of the prefixes of S such that v 2 V iff either L (v) = v or L (E (v)) 6= L (E (p (v))) in the <p> Here, a/b if ab and there is no prefix c so that acb. Note that this condition differs from the edge condition in that w was constrained to be in V while c can be any prefix of S. In <ref> [7] </ref>, it was shown that the border tree of a string can be constructed in linear time and that its depth is logarithmic in the original string. This data structre was in fact defined to answer queries of type Q 1 and Q 2 .
Reference: [8] <author> D.E. Knuth, J.H. Morris, and V.R. Pratt. </author> <title> Fast pattern matching in strings. </title> <journal> SIAM J. Comp., </journal> <volume> 6 </volume> <pages> 323-350, </pages> <year> 1977. </year>
Reference-contexts: For each table column, fill in the rest of the entries using the KMP automaton. end Table Construction Algorithm Implementation and Analysis: Step 1 is a standard KMP automaton construction that was shown in <ref> [8] </ref> to be accomplished in time and space O (m). Step 2 can be easily implemented in time and space O (m 2 ). We need to consider steps 3 and 4.
Reference: [9] <author> E. M. McCreight. </author> <title> A space-economical suffix tree construction algorithm. </title> <journal> Journal of the ACM, </journal> <volume> 23 </volume> <pages> 262-272, </pages> <year> 1976. </year>
Reference-contexts: Every pattern location where p i p j appears is always followed by p j+1 p l . Implementation: The compacted suffix tree can be constructed in time O (m) <ref> [9, 10] </ref>. However, since our tables are of size m 2 , we can simply construct the uncompacted suffix trie and then mark every node with more than one child and every leaf as explicit nodes.
Reference: [10] <author> P. Weiner. </author> <title> Linear pattern matching algorithm. </title> <booktitle> Proc. 14 IEEE Symposium on Switching and Automata Theory, </booktitle> <pages> pages 1-11, </pages> <year> 1973. </year>
Reference-contexts: Every pattern location where p i p j appears is always followed by p j+1 p l . Implementation: The compacted suffix tree can be constructed in time O (m) <ref> [9, 10] </ref>. However, since our tables are of size m 2 , we can simply construct the uncompacted suffix trie and then mark every node with more than one child and every leaf as explicit nodes.
Reference: [11] <author> T. A. Welch. </author> <title> A technique for high-performance data compression. </title> <journal> IEEE Computer, </journal> <volume> 17 </volume> <pages> 8-19, </pages> <year> 1984. </year>
Reference-contexts: This is a variation of the Lempel-Ziv compression introduced by Welch <ref> [11] </ref>. It is also an adaptive compression but encoding and decoding is much simpler than in the Lempel-Ziv compression. The main contributions of our paper are: * We show the first known almost optimal compressed matching algorithm for an adaptive compression. <p> Theorem 2.1 (Welch <ref> [11] </ref>): Given string , one can construct T and :Z in time O (u). Given string :Z, one can construct T and in time O (u). The following theorem and its corollary are crucial to our algorithm.
Reference: [12] <author> J. Ziv and A. Lempel. </author> <title> A universal algorithm for sequential data compression. </title> <journal> IEEE Trans. on Information Theory, </journal> <volume> IT-23(3):337-343, </volume> <year> 1977. </year>
Reference-contexts: For example, the first and last pattern elements have to be handled separately in the run-length compression case, and the starting bit of each encoded symbol needs to be found in the Huffman encoding. The challenge seems to be adaptive compressions such as Lempel-Ziv <ref> [12] </ref>. In an adaptive compression, the text represented by each compression symbol is determined dynamically by the data. As a result, the same substring will be encoded differently depending on its location in the text.
References-found: 12

