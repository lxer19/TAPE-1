URL: http://www.csc.calpoly.edu/~tpederse/aaai97.ps.gz
Refering-URL: http://www.csc.calpoly.edu/~tpederse/pubs.html
Root-URL: http://www.csc.calpoly.edu
Email: fpedersen,rbruceg@seas.smu.edu  
Title: A New Supervised Learning Algorithm for Word Sense Disambiguation  
Author: Ted Pedersen and Rebecca Bruce 
Address: Dallas, TX 75275-0122  
Affiliation: Department of Computer Science and Engineering Southern Methodist University  
Date: July 1997, Providence, RI  
Note: Appears in the Proceedings of the Fourteenth National Conference on Artificial Intelligence,  
Abstract: The Naive Mix is a new supervised learning algorithm that is based on a sequential method for selecting probabilistic models. The usual objective of model selection is to find a single model that adequately characterizes the data in a training sample. However, during model selection a sequence of models is generated that consists of the best-fitting model at each level of model complexity. The Naive Mix utilizes this sequence of models to define a probabilistic model which is then used as a probabilistic classifier to perform word-sense disambiguation. The models in this sequence are restricted to the class of decomposable log-linear models. This class of models offers a number of computational advantages. Experiments disambiguating twelve different words show that a Naive Mix formulated with a forward sequential search and Akaike's Information Criteria rivals established supervised learning algorithms such as decision trees (C4.5), rule induction (CN2) and nearest-neighbor classification (PEBLS). 
Abstract-found: 1
Intro-found: 1
Reference: <author> Akaike, H. </author> <year> 1974. </year> <title> A new look at the statistical model identification. </title> <journal> IEEE Transactions on Automatic Control AC-19(6):716-723. </journal>
Reference-contexts: The evaluation criterion judges how well the model characterizes the data in the training sample. We use Akaike's Information Criteria (AIC) <ref> (Akaike 1974) </ref> as the evaluation criterion based on the results of an extensive comparison of search strategies and selection criteria for model selection reported in (Pedersen, Bruce, & Wiebe 1997). Search Strategy BSS begins by designating the saturated model as the current model.
Reference: <author> Berger, A.; Della Pietra, S.; and Della Pietra, V. </author> <year> 1996. </year> <title> A maximum entropy approach to natural language processing. </title> <booktitle> Computational Linguistics 22(1) </booktitle> <pages> 39-71. </pages>
Reference-contexts: Because their joint distributions have such closed-form expressions, the parameters can be estimated directly from the training sample without the need for an iterative fitting procedure as is required, for example, to estimate the parameters of maximum entropy models (e.g., <ref> (Berger, Della Pietra, & Della Pietra 1996) </ref>). 2 F 2 and F 5 are conditionally independent given S if p (F 2 = f 2 jF 5 = f 5 ; S = s) = p (F 2 = f 2 jS = s).
Reference: <author> Bishop, Y.; Fienberg, S.; and Holland, P. </author> <year> 1975. </year> <title> Discrete Multivariate Analysis. </title> <address> Cambridge, MA: </address> <publisher> The MIT Press. </publisher>
Reference-contexts: We close with a discussion of related work. Decomposable Models Decomposable models are a subset of the class of graphical models (Whittaker 1990) which is in turn a subset of the class of log-linear models <ref> (Bishop, Fienberg, & Holland 1975) </ref>. Although there are far fewer decomposable models than log-linear models for a given set of feature variables, these classes have substantially the same expressive power (Whittaker 1990).
Reference: <author> Brodley, C. </author> <year> 1995. </year> <title> Recursive automatic bias selection for classifier construction. </title> <booktitle> Machine Learning 20 </booktitle> <pages> 63-94. </pages>
Reference-contexts: This supposition has lead to hybrid approaches that combine various methods (e.g., (Domingos 1996)) and approaches that select the most appropriate learning algorithm based on the characteristics of the training data (e.g., <ref> (Brodley 1995) </ref>). Conclusion The Naive Mix extends existing statistical model selection by taking advantage of intermediate models discovered during the selection process. Features are selected during a systematic model search and then appropriately weighted via averaged parameter estimates.
Reference: <author> Bruce, R., and Wiebe, J. </author> <year> 1994. </year> <title> Word-sense disambiguation using decomposable models. </title> <booktitle> In Proceedings of the 32nd Annual Meeting of the Association for Computational Linguistics, </booktitle> <pages> 139-146. </pages>
Reference-contexts: The lower accuracy of PEBLS relative to Naive Bayes indicates that some weighting is appropriate. Related Work Sequential model selection using decomposable models was first applied to word-sense disambiguation in <ref> (Bruce & Wiebe 1994) </ref>. The Naive Mix extends that work by considering an entire sequence of models rather than just the best-fitting model. <p> PEBLS was first applied to word-sense disambiguation in (Ng & Lee 1996). Using the same sense-tagged text for interest as used in this paper, they draw comparisons between PEBLS and a probabilistic classifier based on the best-fitting single model found during a model search <ref> (Bruce & Wiebe 1994) </ref>. They find that the combination of PEBLS and a broader set of features leads to significant improvements in accuracy.
Reference: <author> Bruce, R.; Wiebe, J.; and Pedersen, T. </author> <year> 1996. </year> <title> The measure of a model. </title> <booktitle> In Proceedings of the Conference on Empirical Methods in Natural Language Processing, </booktitle> <pages> 101-112. </pages>
Reference-contexts: This is unlikely for NLP data, which is often sparse and highly skewed (e.g. (Zipf 1935) and <ref> (Pedersen, Kayaalp, & Bruce 1996) </ref>). However, if the training sample can be adequately characterized by a less complex model with fewer interactions between features, then more reliable parameter estimates can be obtained. <p> FSS incrementally builds on the strongest interactions while BSS incrementally eliminates the weakest interactions. As a result, the intermediate models generated during BSS may contain irrelevant interactions. Experimental Data The sense-tagged text used in these experiments is that described in <ref> (Bruce, Wiebe, & Pedersen 1996) </ref> and consists of every sentence from the ACL/DCI Wall Street Journal corpus that contains any of the nouns interest, bill, concern, and drug, any of the verbs close, help, agree, and include, or any of the adjectives chief, public, last, and common. <p> These weights allow some discounting of less relevant features. As implemented here, PEBLS stores all instances of the training sample and treats each feature independently and equally, making it more susceptible to misclassification due to irrelevant features. As shown in <ref> (Bruce, Wiebe, & Pedersen 1996) </ref>, all of the features used in these experiments are good indicators of the classification variable, although not equally so. The lower accuracy of PEBLS relative to Naive Bayes indicates that some weighting is appropriate.
Reference: <author> Clark, P., and Niblett, T. </author> <year> 1989. </year> <title> The CN2 induction algorithm. </title> <booktitle> Machine Learning 3(4) </booktitle> <pages> 261-283. </pages>
Reference-contexts: The method is biased toward production of simple trees, trees with the fewest partitions, where classification is based on the smallest number of feature values. CN2 <ref> (Clark & Niblett 1989) </ref>: A rule induction algorithm that selects rules that cover the largest possible subsets of the training sample as measured by the Laplace error estimate. This method is biased towards the selection of simple rules that cover as many training instances as possible.
Reference: <author> Cost, S., and Salzberg, S. </author> <year> 1993. </year> <title> A weighted nearest neighbor algorithm for learning with symbolic features. </title> <booktitle> Machine Learning 10(1) </booktitle> <pages> 57-78. </pages>
Reference-contexts: PEBLS <ref> (Cost & Salzberg 1993) </ref>: A k nearest-neighbor algorithm where classification is performed by assigning a test instance to the majority class of the k closest training examples.
Reference: <author> Darroch, J.; Lauritzen, S.; and Speed, T. </author> <year> 1980. </year> <title> Markov fields and log-linear interaction models for contingency tables. </title> <journal> The Annals of Statistics 8(3) </journal> <pages> 522-539. </pages>
Reference-contexts: However, if the training sample can be adequately characterized by a less complex model with fewer interactions between features, then more reliable parameter estimates can be obtained. We restrict the search to the class of decomposable models <ref> (Darroch, Lau-ritzen, & Speed 1980) </ref>, since this reduces the model search space and simplifies parameter estimation. We begin with short introductions to decomposable models and model selection. The Naive Mix is discussed, followed by a description of the sense-tagged text used in our experiments.
Reference: <author> Domingos, P. </author> <year> 1996. </year> <title> Unifying instance-based and rule-based induction. </title> <booktitle> Machine Learning 24 </booktitle> <pages> 141-168. </pages>
Reference-contexts: A similar trend exists in machine learning based on the supposition that no learning algorithm is superior for all tasks. This supposition has lead to hybrid approaches that combine various methods (e.g., <ref> (Domingos 1996) </ref>) and approaches that select the most appropriate learning algorithm based on the characteristics of the training data (e.g., (Brodley 1995)). Conclusion The Naive Mix extends existing statistical model selection by taking advantage of intermediate models discovered during the selection process.
Reference: <author> Duda, R., and Hart, P. </author> <year> 1973. </year> <title> Pattern Classification and Scene Analysis. </title> <address> New York, NY: </address> <publisher> Wiley. </publisher>
Reference-contexts: Below, we briefly describe each algorithm. Majority classifier: The performance of a probabilistic classifier should not be worse than the majority classifier which assigns to each ambiguous word the most frequently occurring sense in the training sample. Naive Bayes classifier <ref> (Duda & Hart 1973) </ref>: A probabilistic classifier based on a model where the features (F 1 ; F 2 ; : : : ; F n1 ) are all conditionally independent given the value of the classification variable S. p (SjF 1 ; F 2 ; : : : ; F
Reference: <author> Leacock, C.; Towell, G.; and Voorhees, E. </author> <year> 1993. </year> <title> Corpus-based statistical sense resolution. </title> <booktitle> In Proceedings of the ARPA Workshop on Human Language Technology, </booktitle> <pages> 260-265. </pages>
Reference-contexts: The Naive Mix extends that work by considering an entire sequence of models rather than just the best-fitting model. Comparative studies of machine learning algorithms applied to word-sense disambiguation are relatively rare. <ref> (Leacock, Towell, & Voorhees 1993) </ref> compares a neural network, a Naive Bayes classifier, and a content vector when disambiguating six senses of line.
Reference: <author> Madigan, D., and Raftery, A. </author> <year> 1994. </year> <title> Model selection and accounting for model uncertainty in graphical models using Occam's Window. </title> <journal> Journal of American Statistical Association 89 </journal> <pages> 1535-1546. </pages>
Reference-contexts: They find that the combination of PEBLS and a broader set of features leads to significant improvements in accuracy. In recognition of the uncertainty in model selection, there has been a recent trend in model selection research away from the selection of a single model (e.g., <ref> (Madigan & Raftery 1994) </ref>); the Naive Mix reflects this trend. A similar trend exists in machine learning based on the supposition that no learning algorithm is superior for all tasks.
Reference: <author> Mooney, R. </author> <year> 1996. </year> <title> Comparative experiments on disambiguating word senses: An illustration of the role of bias in machine learning. </title> <booktitle> In Proceedings of the Conference on Empirical Methods in Natural Language Processing, </booktitle> <pages> 82-91. </pages>
Reference-contexts: Comparative studies of machine learning algorithms applied to word-sense disambiguation are relatively rare. (Leacock, Towell, & Voorhees 1993) compares a neural network, a Naive Bayes classifier, and a content vector when disambiguating six senses of line. They report that all three methods are equally accurate. <ref> (Mooney 1996) </ref> utilizes this same data and applies an even wider range of approaches comparing a Naive Bayes classifier, a perceptron, a decision-tree, a nearest-neighbor classifier, a logic based Disjunctive Normal Form learner, a logic based Conjunctive Normal Form learner, and a decision list learner.
Reference: <author> Ng, H., and Lee, H. </author> <year> 1996. </year> <title> Integrating multiple knowledge sources to disambiguate word sense: An exemplar-based approach. </title> <booktitle> In Proceedings of the 34th Annual Meeting of the Society for Computational Linguistics, </booktitle> <pages> 40-47. </pages>
Reference-contexts: It is perhaps not surprising that a simple model, such as Naive Bayes, would provide a manageable representation of such a large feature set. PEBLS was first applied to word-sense disambiguation in <ref> (Ng & Lee 1996) </ref>. Using the same sense-tagged text for interest as used in this paper, they draw comparisons between PEBLS and a probabilistic classifier based on the best-fitting single model found during a model search (Bruce & Wiebe 1994). <p> A similar trend exists in machine learning based on the supposition that no learning algorithm is superior for all tasks. This supposition has lead to hybrid approaches that combine various methods (e.g., <ref> (Domingos 1996) </ref>) and approaches that select the most appropriate learning algorithm based on the characteristics of the training data (e.g., (Brodley 1995)). Conclusion The Naive Mix extends existing statistical model selection by taking advantage of intermediate models discovered during the selection process.
Reference: <author> Pedersen, T.; Bruce, R.; and Wiebe, J. </author> <year> 1997. </year> <title> Sequential model selection for word sense disambiguation. </title> <booktitle> In Proceedings of the Fifth Conference on Applied Natural Language Processing. </booktitle>
Reference-contexts: The evaluation criterion judges how well the model characterizes the data in the training sample. We use Akaike's Information Criteria (AIC) (Akaike 1974) as the evaluation criterion based on the results of an extensive comparison of search strategies and selection criteria for model selection reported in <ref> (Pedersen, Bruce, & Wiebe 1997) </ref>. Search Strategy BSS begins by designating the saturated model as the current model. A saturated model has complexity level c = 2 , where n is the number of feature variables.
Reference: <author> Pedersen, T.; Kayaalp, M.; and Bruce, R. </author> <year> 1996. </year> <title> Significant lexical relationships. </title> <booktitle> In Proceedings of the Thirteenth National Conference on Artificial Intelligence, </booktitle> <pages> 455-460. </pages>
Reference-contexts: This is unlikely for NLP data, which is often sparse and highly skewed (e.g. (Zipf 1935) and <ref> (Pedersen, Kayaalp, & Bruce 1996) </ref>). However, if the training sample can be adequately characterized by a less complex model with fewer interactions between features, then more reliable parameter estimates can be obtained. <p> FSS incrementally builds on the strongest interactions while BSS incrementally eliminates the weakest interactions. As a result, the intermediate models generated during BSS may contain irrelevant interactions. Experimental Data The sense-tagged text used in these experiments is that described in <ref> (Bruce, Wiebe, & Pedersen 1996) </ref> and consists of every sentence from the ACL/DCI Wall Street Journal corpus that contains any of the nouns interest, bill, concern, and drug, any of the verbs close, help, agree, and include, or any of the adjectives chief, public, last, and common. <p> These weights allow some discounting of less relevant features. As implemented here, PEBLS stores all instances of the training sample and treats each feature independently and equally, making it more susceptible to misclassification due to irrelevant features. As shown in <ref> (Bruce, Wiebe, & Pedersen 1996) </ref>, all of the features used in these experiments are good indicators of the classification variable, although not equally so. The lower accuracy of PEBLS relative to Naive Bayes indicates that some weighting is appropriate.
Reference: <author> Quinlan, J. </author> <year> 1992. </year> <title> C4.5: Programs for Machine Learning. </title> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: With these parameter settings, PEBLS is a standard nearest-neighbor classifier and is most appropriate for data where all features are relevant and equally important for classification. C4.5 <ref> (Quinlan 1992) </ref>: A decision tree algorithm in which classification rules are formulated by recursively partitioning the training sample. Each nested partition is based on the feature value that provides the greatest increase in the information gain ratio for the current partition.
Reference: <author> Whittaker, J. </author> <year> 1990. </year> <title> Graphical Models in Applied Mul-tivariate Statistics. </title> <address> New York: </address> <publisher> John Wiley. </publisher>
Reference-contexts: Experimental results are summarized that compare the Naive Mix to a range of other supervised learning approaches. We close with a discussion of related work. Decomposable Models Decomposable models are a subset of the class of graphical models <ref> (Whittaker 1990) </ref> which is in turn a subset of the class of log-linear models (Bishop, Fienberg, & Holland 1975). Although there are far fewer decomposable models than log-linear models for a given set of feature variables, these classes have substantially the same expressive power (Whittaker 1990). <p> of the class of graphical models <ref> (Whittaker 1990) </ref> which is in turn a subset of the class of log-linear models (Bishop, Fienberg, & Holland 1975). Although there are far fewer decomposable models than log-linear models for a given set of feature variables, these classes have substantially the same expressive power (Whittaker 1990).
Reference: <author> Zipf, G. </author> <year> 1935. </year> <booktitle> The Psycho-Biology of Language. </booktitle> <address> Boston, MA: </address> <publisher> Houghton Mi*in. </publisher>
Reference-contexts: This is unlikely for NLP data, which is often sparse and highly skewed (e.g. <ref> (Zipf 1935) </ref> and (Pedersen, Kayaalp, & Bruce 1996)). However, if the training sample can be adequately characterized by a less complex model with fewer interactions between features, then more reliable parameter estimates can be obtained.
References-found: 20

