URL: http://www.cs.berkeley.edu/~dusseau/Papers/sort.ps
Refering-URL: http://www.cs.berkeley.edu/~randit/cs267/assignment1.html
Root-URL: 
Abstract: Modeling Parallel Sorts with LogP on the CM-5 Andrea Carol Dusseau Department of Electrical Engineering and Computer Science Computer Science Division University of California, Berkeley Technical Report: UCB//CSD-94-829 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> D. E. Culler, R. M. Karp, D. A. Patterson, A. Sahay, K. E. Schauser, E. Santos, R. Subramonian, and T. von Eicken, </author> <title> LogP: Towards a Realistic Model of Parallel Computation, </title> <booktitle> in Fourth ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming, </booktitle> <month> May </month> <year> 1993. </year>
Reference-contexts: In this paper, we study fast parallel sorting from the perspective of a new realistic parallel model, LogP <ref> [1] </ref>, which captures the key performance characteristics of modern large scale multiprocessors, such as the Thinking Machines CM-5. In particular, the model reflects the technological reality that these machines are essentially a collection of workstation-class nodes which communicate by point-to-point messages that travel through a dedicated, high performance network. <p> Finally, in Chapter 8, we compare the performance of the four algorithms. The Appendices contain the Split-C implementations of the sorting algorithims. 4 Chapter 2 LogP The LogP model <ref> [1] </ref> reflects the convergence of parallel machines towards systems formed by a collection of complete computers, each consisting of a powerful microprocessor, cache, and large DRAM memory, connected by a communication network. <p> Each node consists of a 33 MHz Sparc RISC processor chip-set and a network interface. The nodes are interconnected in two identical disjoint incomplete fat trees, and a broadcast/scan/prefix control network. The implementations of the sorting algorithms do not use the vector accelerators. In previous experiments on the CM-5 <ref> [10, 1] </ref>, we determined that o 2:2s and, on an unloaded network, L 6s. <p> However, we derive a more efficient data placement that was inspired by the mapping used for large FFTs <ref> [1] </ref>. Our bitonic sort starts with a blocked layout.
Reference: [2] <author> D. E. Culler, A. Dusseau, S. C. Goldstein, A. Krishnamurthy, S. Lumetta, T. von Eicken, and K. Yelick, </author> <title> Parallel Programming in Split-C, </title> <booktitle> in Supercomputing, </booktitle> <year> 1993. </year>
Reference-contexts: Our implementation language, Split-C <ref> [2] </ref>, provides an attractive basis for this study, because it exposes the capabilities modeled by LogP through a rich set of assignment operators in a distributed global address space. <p> We then discuss the probability distribution of the input keys used in our measurements. Next, we characterize the CM-5 in terms of the LogP parameters. Finally, we discuss our model of the local computation, focusing on the local sort. 3.1 Split-C Our sorting algorithms are written in Split-C <ref> [2] </ref>, a parallel extension of the C programming language that can express the capabilities offered by the LogP model. The language follows a SPMD (single program multiple data) model. Processors are distinguished by the value of the special constant, MYPROC.
Reference: [3] <author> G. Blelloch, C. Leiserson, and B. Maggs, </author> <title> A Comparison of Sorting Algorithms for the Connection Machine CM-2, </title> <booktitle> in Symposium on Parallel Algorithms and Architectures, </booktitle> <month> July </month> <year> 1991. </year>
Reference-contexts: Split-C, like C, provides a straightforward machine independent programming system, without attempting to hide the underlying performance characteristics of the machine. We were strongly influenced in this study by a previous comparison of sorting algorithms, which examined bitonic, radix, and sample sort implemented in microcode on the CM-2 <ref> [3] </ref>. We augment the comparison to include column sort, address a more general class of machines, formalized by LogP, and 3 implement the algorithms in a language that can be ported to a variety of parallel machines. This paper is organized as follows. <p> In the next four Chapters, we examine four sorting algorithms: bitonic sort [4], column sort [5], radix sort <ref> [3, 6] </ref>, and sample sort [3]. The order in which we discuss the sorts is based on the increasing complexity of their communication phases. <p> In the next four Chapters, we examine four sorting algorithms: bitonic sort [4], column sort [5], radix sort [3, 6], and sample sort <ref> [3] </ref>. The order in which we discuss the sorts is based on the increasing complexity of their communication phases. <p> for uniformly distributed keys. 3 Note that not all input sets which are initially sorted cause processors to store only to themselves, since the keys must be in sorted order for each of the b passes. 23 Chapter 7 Sample Sort An interesting recent algorithm, called sample (or splitter) sort <ref> [3, 13] </ref>, pushes the pattern of alternating phases of local computation, destination setup, and key distribution to the extreme it performs only one of each. The key distribution phase in sample sort exhibits the most complicated structure of any in the four sorts: irregular, unbalanced all-to-all communication. <p> Assuming a large sample size, a large number of keys per processor, and a random input key distribution, the expansion factor can be bounded by a small constant with high probability <ref> [3] </ref>. In our analysis of the distribution of keys in radix sort, we ignored the destination contention that occurs when multiple processors send to the same destination processor. The distribution phase for sample sort is similar, so we continue to ignore the potential contention.
Reference: [4] <author> K. Batcher, </author> <title> Sorting Networks and their Applications, </title> <booktitle> in Proceedings of the AFIPS Spring Joint Computing Conference, </booktitle> <year> 1986. </year>
Reference-contexts: In the next four Chapters, we examine four sorting algorithms: bitonic sort <ref> [4] </ref>, column sort [5], radix sort [3, 6], and sample sort [3]. The order in which we discuss the sorts is based on the increasing complexity of their communication phases. <p> adjust scan of bins 2.5 t addr determine destination 4.7 t compare compare key 0.9 to splitter Sample t localsort 8 local radix sort 5.0 of samples Table 3.1: Models of local computation rates. 11 Chapter 4 Bitonic Sort In this chapter, we discuss a variant of Batcher's bitonic sort <ref> [4] </ref>. After describing the general algorithm, we present a data layout that reduces communication and enables optimizations for the local computation. We then describe how the LogP model guides us to an efficient implementation of the important communication operations: remaps between cyclic and blocked layouts.
Reference: [5] <author> T. Leighton, </author> <title> Tight Bounds on the Complexity of Parallel Sorting, </title> <journal> IEEE Transactions on Computers, </journal> <month> Apr. </month> <year> 1985. </year>
Reference-contexts: In the next four Chapters, we examine four sorting algorithms: bitonic sort [4], column sort <ref> [5] </ref>, radix sort [3, 6], and sample sort [3]. The order in which we discuss the sorts is based on the increasing complexity of their communication phases. <p> different input key entropies, varying by only 12%. 4 With low entropies, our implementation for finding the minimum element in the bitonic sequence is slower because more keys must be examined before the direction of the sequence is determined, due to duplicate keys. 15 Chapter 5 Column Sort Column sort <ref> [5] </ref>, like bitonic sort, alternates between local sort and key distribution phases, but only four phases of each are required. Two key distribution phases use an all-to-all communication pattern and two use a one-to-one pattern.
Reference: [6] <author> M. Zagha and G. Blelloch, </author> <title> Radix Sort for Vector Multiprocessors, </title> <booktitle> in Supercomputing, </booktitle> <year> 1991. </year>
Reference-contexts: In the next four Chapters, we examine four sorting algorithms: bitonic sort [4], column sort [5], radix sort <ref> [3, 6] </ref>, and sample sort [3]. The order in which we discuss the sorts is based on the increasing complexity of their communication phases.
Reference: [7] <author> D. Lenoski, J. Laudon, K. Gharachorloo, W.-D. Weber, A. Gupta, J. Hennessy, M. Horowitz, and M. Lam, </author> <title> The Stanford Dash Multiprocessor, </title> <journal> IEEE Computer, </journal> <volume> vol. 25, </volume> <pages> pp. 63-79, </pages> <month> Mar. </month> <year> 1992. </year>
Reference-contexts: However, the cost to the senders is greater, since processors stall as a result of exceeding the capacity constraint. 1 On machines with hardware for shared memory access, the remote end may be serviced by an auxiliary processor that is part of the memory controller <ref> [7] </ref>. 6 If pairs of processors exchange n messages each, then the analysis is more complicated. Assuming that both processors begin sending at the same time, for L units of time each processor sends messages at an interval of g.
Reference: [8] <author> C. Shannon and W. Weaver, </author> <title> The Mathematical Theory of Communication. </title> <publisher> University of Illinois Press: </publisher> <address> Urbana, </address> <year> 1949. </year>
Reference-contexts: The probability distribution of each input set is characterized by its Shannon entropy <ref> [8] </ref>, defined as i=1 where p i is the probability associated with key i. To generate input data sets with various entropies, we produce keys whose individual bits have between 0 and 1 bits of entropy.
Reference: [9] <author> K. Thearling and S. Smith, </author> <title> An Improved Supercomputer Sorting Benchmark, </title> <type> tech. rep., </type> <institution> Thinking Machines Corporation, </institution> <year> 1991. </year>
Reference-contexts: To generate input data sets with various entropies, we produce keys whose individual bits have between 0 and 1 bits of entropy. Multiple keys from a uniform distribution are combined into a single key having a non-uniform distribution, as suggested in <ref> [9] </ref>. For example, if the binary AND operator is applied to two independent keys generated from a uniform distribution, then each bit in the resulting key has a 0.75 chance of being a zero and a 0.25 chance of being a one. <p> In <ref> [9] </ref>, similar results are presented for the execution time of the phases in radix sort as the entropy of keys is varied: a decrease in the total execution time as the entropy decreases, with a marked decrease with an entropy of 0.
Reference: [10] <author> T. von Eicken, D. E. Culler, S. C. Goldstein, and K. E. Schauser, </author> <title> Active Messages: a Mechanism for Integrated Communication and Computation, </title> <booktitle> in Proc. of the 19th International Symposium on Computer Architecture, </booktitle> <month> May </month> <year> 1992. </year>
Reference-contexts: Each node consists of a 33 MHz Sparc RISC processor chip-set and a network interface. The nodes are interconnected in two identical disjoint incomplete fat trees, and a broadcast/scan/prefix control network. The implementations of the sorting algorithms do not use the vector accelerators. In previous experiments on the CM-5 <ref> [10, 1] </ref>, we determined that o 2:2s and, on an unloaded network, L 6s.
Reference: [11] <author> P. Liu, W. Aiello, and S. Bhatt, </author> <title> An atomic model for message-passing, </title> <booktitle> in Symposium on Parallel Algorithms and Architectures, </booktitle> <year> 1993. </year>
Reference-contexts: Determining the expected slowdown due to contention of a random permutation under LogP is an interesting open problem. Our simulations and recent theoretical results <ref> [11] </ref> suggest that the slow-down is bounded by a small constant, but a thorough treatment of this problem is beyond the scope of this paper.
Reference: [12] <author> R. Karp, A. Sahay, E. Santos, and K. E. Schauser, </author> <title> Optimal Broadcast and Summation in the LogP Model, </title> <booktitle> in 5th Symp. on Parallel Algorithms and Architectures, </booktitle> <month> June </month> <year> 1993. </year>
Reference-contexts: Modifications could be made to the algorithm such that keys destined for the same processor are first gathered and then stored in bulk. 2 In <ref> [12] </ref> an optimal broadcast strategy is developed where the root sends each data element only once, but alternates among recipients in order to retain the logarithmic depth of a tree broadcast. 20 theory each processor could refuse to receive the next value until it has forwarded the present one and the
Reference: [13] <author> J. H. Reif and L. G. Valiant, </author> <title> A Logarithmic time Sort for Linear Size Networks, </title> <journal> Journal of the ACM, </journal> <volume> vol. 34, </volume> <pages> pp. 60-76, </pages> <month> Jan. </month> <year> 1987. </year> <month> 31 </month>
Reference-contexts: for uniformly distributed keys. 3 Note that not all input sets which are initially sorted cause processors to store only to themselves, since the keys must be in sorted order for each of the b passes. 23 Chapter 7 Sample Sort An interesting recent algorithm, called sample (or splitter) sort <ref> [3, 13] </ref>, pushes the pattern of alternating phases of local computation, destination setup, and key distribution to the extreme it performs only one of each. The key distribution phase in sample sort exhibits the most complicated structure of any in the four sorts: irregular, unbalanced all-to-all communication.
References-found: 13

