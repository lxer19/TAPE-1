URL: ftp://ftp.wins.uva.nl/pub/computer-systems/aut-sys/reports/Bos96.ps.gz
Refering-URL: http://www.fwi.uva.nl/research/neuro/publications/publications.html
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Title: Locally weighted approximations: yet another type of neural network  
Author: Sander Bosman 
Date: July 1996  
Address: Amsterdam  
Affiliation: Intelligent Autonomous Systems group Department of Computer Science University of  
Abstract-found: 0
Intro-found: 1
Reference: [1] <author> J.S. Albus. </author> <title> A new approach to manipulator control: The Cerebellar Model Articulation Controller (CMAC). </title> <journal> Transactions of the ASME, Journal of Dynamics Systems Measurement and Control, </journal> <volume> 97 </volume> <pages> 220-227, </pages> <year> 1975. </year>
Reference-contexts: At more complex parts there need to be more experts. Furthermore, region placement can be dictated by the number of samples available in that region: the expert needs a minimum number of samples to base its approximation on. CMAC networks <ref> [1] </ref> place experts at fixed intervals, completely circumventing the problem. The approach used by Moody and Darken [20] uses self-organization [23] to find the expert positions, following the input distribution of the samples.
Reference: [2] <author> C.G. Atkeson, A.W. Moore, and S. Schaal. </author> <title> Locally weighted learning. </title> <journal> Artificial Intelligence Review, </journal> <month> August </month> <year> 1995. </year>
Reference: [3] <author> C. de Boor. </author> <title> A practical guide to splines. </title> <publisher> Springer-Verlag, </publisher> <address> New York, </address> <year> 1978. </year>
Reference-contexts: A simple localized model is the locally constant model or lookup table, where the input space is split into disjunct regions, and each region is approximated by a constant (local average). Figure 2.3 shows a resulting approximation. Other examples of localized approximation methods are splines <ref> [3] </ref>, recursive partitioning [17], smoothers [2][5] and Radial Basis functions (which will be described in section 3.4). Although localized approximations work well in low dimensional (say 2) settings, problems arise when higher dimensions are involved.
Reference: [4] <author> S. Chen, F.N. Cowan, and P.M. Grant. </author> <title> Orthogonal least squares learning algorithm for radial basis function networks. </title> <journal> IEEE Transactions on Neural Networks, </journal> <volume> 2(2) </volume> <pages> 302-309, </pages> <year> 1991. </year>
Reference-contexts: This is certainly not a trivial change, because it is not clear what the optimal place is to insert a new expert. However, for related approximation methods a lot of methods already exist, such as <ref> [4] </ref> [8] [11] [12] [17] [24] [30] [32]. In addition to developing particular methods, more has to be known about the theoretical background of function approximation.
Reference: [5] <author> William S. Cleveland and Clive Loader. </author> <title> Smoothing by local regression: principles and methods. </title> <type> Technical report, </type> <institution> AT&T Bell Laboratories, </institution> <address> Murray Hill, NJ, </address> <year> 1995. </year>
Reference-contexts: Moody and Darken as well as the CMAC use locally constant models. Schaal and Atkeson apply local linear models. Using higher order polynomials as local model is discussed in <ref> [5] </ref>. 3.4 Radial Basis Functions This section is a deviation from the previous sections, describing another widely used class of local approximators: the so called Radial Basis Functions (RBF). Since they are very similar to locally weighted approximations, we will discuss them here.
Reference: [6] <author> P. Diaconis and M. Shahshahani. </author> <title> On nonlinear functions of linear combinations. </title> <journal> J.Sci.Statist.Comp., </journal> <volume> 5 </volume> <pages> 175-191, </pages> <year> 1984. </year>
Reference-contexts: It can be shown that each smooth function can be approximated well by the projection pursuit method, provided that N is large enough <ref> [6] </ref>. But even for small N the performance of these approximations is pretty good [7].
Reference: [7] <author> D.L. Donoho and I. Johnstone. </author> <title> Projection-based approximation and a duality with kernel methods. </title> <journal> Annals of Statistics, </journal> <volume> 17 </volume> <pages> 58-106, </pages> <year> 1989. </year>
Reference-contexts: It can be shown that each smooth function can be approximated well by the projection pursuit method, provided that N is large enough [6]. But even for small N the performance of these approximations is pretty good <ref> [7] </ref>. The big disadvantage lies in the difficulty of optimizing the parameters, resulting in non-optimal parameters and long optimization times. 2.5 Feed forward neural networks 2.5.1 Neurons Artificial neural networks were initially devised as simplified models of the brain.
Reference: [8] <author> Scott E. Fahlman and Christian Lebiere. </author> <booktitle> The cascade-correlation learning architecture. In Advances in Neural Information Processing Systems II, </booktitle> <pages> pages 524-532. </pages> <publisher> Morgan Kauf-man, </publisher> <year> 1990. </year>
Reference-contexts: This is certainly not a trivial change, because it is not clear what the optimal place is to insert a new expert. However, for related approximation methods a lot of methods already exist, such as [4] <ref> [8] </ref> [11] [12] [17] [24] [30] [32]. In addition to developing particular methods, more has to be known about the theoretical background of function approximation.
Reference: [9] <author> Jerome H. Friedman. </author> <title> Multivariate adaptive regression splines. </title> <journal> The annals of statistics, </journal> <volume> 19(1) </volume> <pages> 1-141, </pages> <year> 1991. </year>
Reference: [10] <author> J.H. Friedman and W. Stuetzle. </author> <title> Projection pursuit regression. </title> <journal> Journal of the American Statistical Association, </journal> <volume> 76(376) </volume> <pages> 817-823, </pages> <month> December </month> <year> 1981. </year>
Reference: [11] <author> Bernd Fritzke. </author> <title> Growing cell structures | a self-organizing network for unsupervised and supervised learning. </title> <type> Technical Report TR-93-026, </type> <institution> International Computer Science Institute, Berkely, </institution> <address> CA, </address> <month> May </month> <year> 1993. </year>
Reference-contexts: This is certainly not a trivial change, because it is not clear what the optimal place is to insert a new expert. However, for related approximation methods a lot of methods already exist, such as [4] [8] <ref> [11] </ref> [12] [17] [24] [30] [32]. In addition to developing particular methods, more has to be known about the theoretical background of function approximation.
Reference: [12] <author> Federico Girosi, Michael Jones, and Tomaso Poggio. </author> <title> Priors, stabilizers and basis functions: from regularization to radial, tensor and additive splines. Memo 1430, Center for Biological and Computational Learning, </title> <publisher> MIT, </publisher> <month> June </month> <year> 1993. </year>
Reference-contexts: In such situations it is possible to use regularization. Regularization penalizes approximations (settings of parameters) having high variance, by adding a regularization term to the error used in learning ([27], <ref> [12] </ref>): E = s2S 2 2 The first term is the error on the learn samples (the bias). The second term penalizes the variance of the approximation, where determines the balance between the two ( &gt; 0). <p> This is certainly not a trivial change, because it is not clear what the optimal place is to insert a new expert. However, for related approximation methods a lot of methods already exist, such as [4] [8] [11] <ref> [12] </ref> [17] [24] [30] [32]. In addition to developing particular methods, more has to be known about the theoretical background of function approximation. For example, why do methods like feed forward neural networks often generalize better than other (e.g., local) methods? More theoretical research might provide a better understanding of this.
Reference: [13] <editor> J.P. Gram. Uber Entwicklung reeller Functionen in Reihen mittelst der Methode der kleinsten Quadrate. J. </editor> <title> Math., </title> <type> 94 41-73, 1883. 45 46 BIBLIOGRAPHY </type>
Reference-contexts: This Master's thesis studies locally weighted approximations, which form a particular type of neural network. Such networks are based on local regression methods found in statistics; [14] reports that local regression was used by Danish actuaries as early as 1829, but the first published work appeared in 1883 <ref> [13] </ref>. 1 2 CHAPTER 1. INTRODUCTION Locally weighted approximations come in a large variety, the reason why relatively little is known about them. This text focuses on a small subset: various alternative forms are compared and the best form found is compared with other types of neural networks.
Reference: [14] <author> J.M. Hoem. </author> <title> The reticent trio: some little-known early discoveries in life insurance mathematics by l.h.f. oppermann, t.n. thiele and j.p. gram. </title> <journal> Inter. Stat. Rev., </journal> <volume> 51 </volume> <pages> 213-221, </pages> <year> 1983. </year>
Reference-contexts: Still, little is known about what methods should be used in what situations. This Master's thesis studies locally weighted approximations, which form a particular type of neural network. Such networks are based on local regression methods found in statistics; <ref> [14] </ref> reports that local regression was used by Danish actuaries as early as 1829, but the first published work appeared in 1883 [13]. 1 2 CHAPTER 1. INTRODUCTION Locally weighted approximations come in a large variety, the reason why relatively little is known about them.
Reference: [15] <author> J.H. Holland, K.J. Holyoak, R.E. Nisbett, and P.R. Thagard. </author> <title> Induction: processes of inference, learning and discovery. Computational Models of Cognition and Perception. </title> <publisher> M.I.T. press, </publisher> <address> Cambridge (MA), </address> <year> 1986. </year>
Reference-contexts: To make approximation possible on the complete input domain, the function D (x) must be redundant in the sense that a limited set of samples contains information about the rest of the mapping. Fortunately, in the real world this is often the case <ref> [15] </ref>. The generalization performance of N (x) can not be measured by the error on the samples used to optimize N (x)'s parameters.
Reference: [16] <author> K. Hornik, Stinchcombe M., and H. White. </author> <title> Multilayer feedforward networks are universal approximators. </title> <booktitle> Neural Networks, </booktitle> <volume> 2 </volume> <pages> 359-366, </pages> <year> 1989. </year>
Reference-contexts: In fact, equation 2.8 can be written as equation 2.5. It has been proven that these networks can approximate any function arbitrarily well, provided the number of hidden units h is large enough and that optimal parameters (weights) can be found <ref> [16] </ref>. In practice however, learning the weights is usually very hard and time consuming. Also there is considerable chance of reaching a non-optimal parameter setting (a local minimum). 2.6 Discussion The approximation methods discussed all have their advantages and disadvantages.
Reference: [17] <author> A. Jansen, P. van der Smagt, and F. Groen. </author> <title> Nested networks for robot control. </title> <editor> In A. F. Murray, editor, </editor> <booktitle> Applications of Neural Networks, </booktitle> <pages> pages 221-239. </pages> <publisher> Kluwer Academic Publishers, </publisher> <address> Dordrecht, the Netherlands, </address> <year> 1995. </year>
Reference-contexts: A simple localized model is the locally constant model or lookup table, where the input space is split into disjunct regions, and each region is approximated by a constant (local average). Figure 2.3 shows a resulting approximation. Other examples of localized approximation methods are splines [3], recursive partitioning <ref> [17] </ref>, smoothers [2][5] and Radial Basis functions (which will be described in section 3.4). Although localized approximations work well in low dimensional (say 2) settings, problems arise when higher dimensions are involved. <p> This is certainly not a trivial change, because it is not clear what the optimal place is to insert a new expert. However, for related approximation methods a lot of methods already exist, such as [4] [8] [11] [12] <ref> [17] </ref> [24] [30] [32]. In addition to developing particular methods, more has to be known about the theoretical background of function approximation. For example, why do methods like feed forward neural networks often generalize better than other (e.g., local) methods? More theoretical research might provide a better understanding of this.
Reference: [18] <author> Steve Lawrence, Ah Chung Tsoi, and Andrew D. </author> <title> Back. Function approximation with neural networks and local methods: bias, variance and smoothness. </title> <booktitle> In Australian Conference on Neural Networks, </booktitle> <year> 1996. </year> <note> To appear. </note>
Reference-contexts: APPROXIMATING SMOOTH MAPPINGS As already pointed out in the previous sections, approximators can roughly be divided into two classes: local and global approximators. The general characteristics of these two classes which can be found in the literature (e.g., <ref> [18] </ref>) are listed below: Global approximations: * A moderate number of samples is needed; * Scales pretty well to higher dimensions; * The flexibility of the approximation depends on a priori settings, e.g., the number of hidden units in a feed forward neural network; * Parameter optimization (learning) can take a
Reference: [19] <author> W.S. McCulloch and W. Pitts. </author> <title> A logical calculus of the ideas immanent in nervous activity. </title> <journal> Bulletin of Mathematical Biophysics, </journal> <volume> 5 </volume> <pages> 115-133, </pages> <year> 1943. </year>
Reference-contexts: When and what pulses are sent depends on the incoming pulses and the importance of each dendrite. The artificial neuron (figure 2.5) is a simplification of its biological counterpart, introduced in 1943 <ref> [19] </ref>.
Reference: [20] <author> John Moody and Christian Darken. </author> <title> Learning with localized receptive fields. </title> <editor> In D. Touret-zky, G. Hinton, and T. Sejnowski, editors, </editor> <booktitle> Proceedings of the 1988 Connectionist Models Summer School, </booktitle> <pages> pages 133-143, </pages> <address> San Mateo, 1988. </address> <publisher> Morgan Kaufman. </publisher>
Reference-contexts: Furthermore, region placement can be dictated by the number of samples available in that region: the expert needs a minimum number of samples to base its approximation on. CMAC networks [1] place experts at fixed intervals, completely circumventing the problem. The approach used by Moody and Darken <ref> [20] </ref> uses self-organization [23] to find the expert positions, following the input distribution of the samples. This reduces the chance of having too little samples available for an expert, but does not place more experts at `difficult' parts of D (x). <p> The minimal value of i is taken to be 0:05 (also based on visual indications), to prevent creation of infinitesimal small regions. The matrix M i is set so that the width equals i in every input direction. A similar method is used by Moody and Darken in <ref> [20] </ref>. * Local model parameters w i;j of the projections W i (x) are initialized with 1:0. The polynomial parameters c 1 to c 3 are set to 0:0.
Reference: [21] <author> M.J.D. Powell. </author> <title> Restart procedures for the conjugate gradient method. </title> <journal> Mathematical programming, </journal> <volume> 12 </volume> <pages> 241-254, </pages> <year> 1977. </year>
Reference-contexts: All these parameters are updated simultaneously, using the conjugate gradient method [26] (CG) with Powell restarts <ref> [21] </ref>. This method is a general parameter estimation algorithm, and uses the first order derivatives of the parameters to the MSE. The algorithm uses a maximum of 5 line search iterations for each CG iteration.
Reference: [22] <author> D.E. Rumelhart, G.E. Hinton, and R.J. Williams. </author> <title> Learning representations by back-propagating errors. </title> <journal> Nature, </journal> <volume> 323 </volume> <pages> 533-536, </pages> <year> 1986. </year>
Reference-contexts: The algorithm uses a maximum of 5 line search iterations for each CG iteration. Each time, CG was run for 500 iterations (after more than 500 iterations the errors did not decrease significantly anymore). We also tried other learning methods: the standard backpropagation <ref> [22] </ref> and Alopex [29]. These algorithms were found to be inferior to CG, mostly because of slower convergence. 4.2.5 Initial parameter settings The conjugate gradient algorithm is not guaranteed to find a global minimum of the MSE, i.e., the parameters found after optimization need not be optimal.
Reference: [23] <author> D.E. Rumelhart and D. Zipser. </author> <title> Feature discovery by competitive learning. </title> <journal> Cognitive Science, </journal> <volume> 9 </volume> <pages> 75-112, </pages> <year> 1985. </year>
Reference-contexts: CMAC networks [1] place experts at fixed intervals, completely circumventing the problem. The approach used by Moody and Darken [20] uses self-organization <ref> [23] </ref> to find the expert positions, following the input distribution of the samples. This reduces the chance of having too little samples available for an expert, but does not place more experts at `difficult' parts of D (x).
Reference: [24] <author> Stefan Schaal and Christopher C. Atkeson. </author> <title> From isolation to cooperation: an alternative view of a system of experts. </title> <editor> In D.S. Touretzky, M.C. Mozer, and M.E. Hasselmo, editors, </editor> <booktitle> Advances in Neural Information Processing Systems 8. </booktitle> <publisher> MIT Press, </publisher> <year> 1996. </year> <note> In press. </note>
Reference-contexts: This reduces the chance of having too little samples available for an expert, but does not place more experts at `difficult' parts of D (x). Schaal and Atkeson <ref> [24] </ref> place experts in an incremental way, inserting an expert at places where there are samples but no experts nearby. The exact value of `nearby' has to be set beforehand. <p> Section 4.4.1 discusses the results of experiments where the number of experts is varied. 4.3 The approximated function For the experiments we used a 2-dimensional function to be approximated, taken from <ref> [24] </ref> (figure 4.2): D (x) = max &lt; e 10x 2 e 50x 2 1:25 e 5 (x 2 2 ) 24 CHAPTER 4. <p> This is certainly not a trivial change, because it is not clear what the optimal place is to insert a new expert. However, for related approximation methods a lot of methods already exist, such as [4] [8] [11] [12] [17] <ref> [24] </ref> [30] [32]. In addition to developing particular methods, more has to be known about the theoretical background of function approximation. For example, why do methods like feed forward neural networks often generalize better than other (e.g., local) methods? More theoretical research might provide a better understanding of this.
Reference: [25] <author> T. J. Sejnowski and C. R. Rosenberg. NETtalk: </author> <title> A parallel network that learns to read aloud. </title> <type> Technical Report JHU/EECS-86/1, </type> <institution> John Hopkins University Department of Electrical Engineering and Computer Science, </institution> <year> 1986. </year>
Reference-contexts: Especially neural networks have induced great interest, since they are capable of finding rather difficult relations in the real world. A classical example is NETtalk, a neural network which found out how to read aloud written English text <ref> [25] </ref>. However, even neural networks are not perfect. Several variants have been proposed, which create better approximations in some situations, are faster or do have some other nice properties. Still, little is known about what methods should be used in what situations.
Reference: [26] <author> Jonathan Richard Shewchuk. </author> <title> An introduction to the conjugate gradient method without the agonizing pain. </title> <type> Technical report, </type> <institution> School of Computer Science, Carnegie Mellon University, </institution> <month> March </month> <year> 1994. </year>
Reference-contexts: All these parameters are updated simultaneously, using the conjugate gradient method <ref> [26] </ref> (CG) with Powell restarts [21]. This method is a general parameter estimation algorithm, and uses the first order derivatives of the parameters to the MSE. The algorithm uses a maximum of 5 line search iterations for each CG iteration.
Reference: [27] <author> A.N. Tikhonov and V.Ya. Arsenin. </author> <title> Solutions of ill-posed problems. </title> <publisher> Winston, </publisher> <address> Washington, </address> <year> 1977. </year> <note> Translation from the Russian "Metody resheniya nekor rektnykh zadach". </note>
Reference: [28] <author> Julius T. Tou and Rafael C. Gonzalez. </author> <title> Pattern recognition principles, </title> <booktitle> volume 7 of Applied mathematics and computation. </booktitle> <publisher> Addison-Wesley, </publisher> <address> Reading (MA), </address> <year> 1974. </year> <note> BIBLIOGRAPHY 47 </note>
Reference-contexts: used in equation 4.4: W i (x) = e p i M i (p i x) ) 1 This distance is also known in another context as the Mahalanobis distance: the distance between point x and a distribution with mean p i and covariance matrix M T i M i <ref> [28] </ref>. 4.2. GENERAL SETTINGS 21 M i is a matrix of size n fi n, where n is the number of input dimensions. The elements of M i determine the shape of the kernel.
Reference: [29] <author> Unnikrishnan and Kootala P. Venugopal. </author> <title> Learning in connectionist networks using the alopex algorithm. </title> <booktitle> Proceedings of IJCNN, </booktitle> <pages> pages 1926-1931, </pages> <year> 1992. </year>
Reference-contexts: The algorithm uses a maximum of 5 line search iterations for each CG iteration. Each time, CG was run for 500 iterations (after more than 500 iterations the errors did not decrease significantly anymore). We also tried other learning methods: the standard backpropagation [22] and Alopex <ref> [29] </ref>. These algorithms were found to be inferior to CG, mostly because of slower convergence. 4.2.5 Initial parameter settings The conjugate gradient algorithm is not guaranteed to find a global minimum of the MSE, i.e., the parameters found after optimization need not be optimal.
Reference: [30] <author> P. van der Smagt and F. Groen. </author> <title> Approximation with neural networks: Between local and global approximation. </title> <booktitle> In Proceedings of the 1995 International Conference on Neural Networks, </booktitle> <pages> pages II:1060-II:1064, </pages> <year> 1995. </year> <type> (Invited paper). </type>
Reference-contexts: This is certainly not a trivial change, because it is not clear what the optimal place is to insert a new expert. However, for related approximation methods a lot of methods already exist, such as [4] [8] [11] [12] [17] [24] <ref> [30] </ref> [32]. In addition to developing particular methods, more has to be known about the theoretical background of function approximation. For example, why do methods like feed forward neural networks often generalize better than other (e.g., local) methods? More theoretical research might provide a better understanding of this.
Reference: [31] <author> V. Vysniauskas, F. C. A. Groen, and B. J. A. Krose. </author> <title> The optimal number of learning samples and hidden units in function approximation with a feedforward network. </title> <type> Technical Report CS-93-15, </type> <institution> Dept. of Comp. Sys, Univ. of Amsterdam, </institution> <month> Nov. </month> <year> 1993. </year>
Reference-contexts: The optimal number of experts is hard, if not impossible, to predict. However, since the problem is essentially the same as finding the optimal number of hidden units in a feed forward neural network, existing heuristic prediction methods might be applicable <ref> [31] </ref>. 32 CHAPTER 4. EXPERIMENTS: LOCALLY WEIGHTED POLYNOMIALS The effect of local model and weighting function depends on the number of experts.
Reference: [32] <author> V. Vysniauskas, F.C.A. Groen, and B.J.A. Krose. </author> <title> Orthogonal incremental learning of a feedforward network. </title> <editor> In Fogelman-Soulie and Gallinari, editors, </editor> <booktitle> Proceedings of the 1995 International Conference on Neural Networks, </booktitle> <year> 1995. </year>
Reference-contexts: This is certainly not a trivial change, because it is not clear what the optimal place is to insert a new expert. However, for related approximation methods a lot of methods already exist, such as [4] [8] [11] [12] [17] [24] [30] <ref> [32] </ref>. In addition to developing particular methods, more has to be known about the theoretical background of function approximation. For example, why do methods like feed forward neural networks often generalize better than other (e.g., local) methods? More theoretical research might provide a better understanding of this.
References-found: 32

