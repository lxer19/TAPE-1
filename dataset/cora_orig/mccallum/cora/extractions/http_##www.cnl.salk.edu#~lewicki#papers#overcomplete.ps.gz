URL: http://www.cnl.salk.edu/~lewicki/papers/overcomplete.ps.gz
Refering-URL: http://www.cnl.salk.edu/~lewicki/
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Email: lewicki@salk.edu  terry@salk.edu  
Title: Learning overcomplete representations  
Author: Michael S. Lewicki Terrence J. Sejnowski 
Note: Submitted to Neural Computation  
Date: February 9, 1998  
Address: 10010 N. Torrey Pines Rd. La Jolla, CA 92037  
Affiliation: Howard Hughes Medical Institute Computational Neurobiology Lab The Salk Institute  
Abstract-found: 0
Intro-found: 1
Reference: <author> Amari, S., Cichocki, A., and Yang, H. H. </author> <year> (1996). </year> <title> A new learning algorithm for blind signal separation. </title> <booktitle> In Advances in Neural and Information Processing Systems, </booktitle> <volume> volume 8, </volume> <pages> pages 757-763, </pages> <address> San Mateo. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Note that this rule contains no matrix inverses and the vector z involves only the derivative of the log prior. We note that in the case where A is square, this form of the rule is exactly the natural gradient ICA learning rule for the basis matrix <ref> (Amari et al., 1996) </ref>. The difference in the more general case where A is rectangular is in how the coefficients s are calculated. <p> This can be alleviated however by multiplying the gradient by an appropriate positive definite matrix <ref> (Amari et al., 1996) </ref>. This rescales the components of the gradient, but still preserves a direction valid for optimization. Noting that A T W T = I, AA T r log P (xjA) = Az^s T AA T AH 1 + Ay^s T : (47) Additional simplifications can be made.
Reference: <author> Atick, J. J. </author> <year> (1992). </year> <title> Could information-theory provide an ecological theory of sensory processing. </title> <booktitle> Network-Computation in Neural Systems, </booktitle> <volume> 3(2) </volume> <pages> 213-251. </pages>
Reference: <author> Barlow, H. B. </author> <year> (1961). </year> <title> Possible principles underlying the transformation of sensory messages. </title> <editor> In Rosenbluth, W. A., editor, </editor> <booktitle> Sensory Communication, </booktitle> <pages> pages 217-234. </pages> <publisher> MIT Press, Cam-bridge. </publisher>
Reference: <author> Barlow, H. B. </author> <year> (1989). </year> <title> Unsupervised learning. </title> <journal> Neural Computation, </journal> <volume> 1 </volume> <pages> 295-311. </pages>
Reference: <author> Bell, A. J. and Sejnowski, T. J. </author> <year> (1995). </year> <title> An information maximization approach to blind separation and blind deconvolution. </title> <journal> Neural Computation, </journal> <volume> 7(6) </volume> <pages> 1129-1159. </pages> <address> 25 Cardoso, J.-F. </address> <year> (1997). </year> <title> Infomax and maximum likelihood for blind source separation. </title> <journal> IEEE Signal Processing Letters, </journal> <volume> 4 </volume> <pages> 109-111. </pages>
Reference: <author> Chen, S., Donoho, D. L., and Saunders, M. A. </author> <year> (1996). </year> <title> Atomic decomposition by basis pursuit. </title> <type> Technical report, </type> <institution> Dept. Stat., Stanford Univ., Stanford, </institution> <address> CA. </address>
Reference-contexts: A suitable initial condition is s = A T x or s = A + x. An alternative method, which can be used when the prior is Laplacian and * = 0, is to view the problem as a standard linear program <ref> (Chen et al., 1996) </ref>: min c T s subject to As = x; s &gt; 0: (4) Letting c = (1; : : : ; 1), the objective function in the linear program, c T s, corresponds to maximizing the probability under the Laplacian prior. <p> This separates the positive and negative coefficients of the solution s into the positive variables u and v, respectively. This can solved efficiently and exactly with interior point linear programming methods <ref> (Chen et al., 1996) </ref>. Quadratic programming approaches to this type of problem have also recently been suggested (Osuna et al., 1997) for similar problems. In this paper, we used both the linear programming and gradient-based methods. <p> In this paper, we used both the linear programming and gradient-based methods. The linear programming methods were superior for finding exact solutions in the case of zero noise. The standard implementation only handles the noiseless case but can be generalized <ref> (Chen et al., 1996) </ref>. We found gradient-based methods to be faster in obtaining good approximate solutions. <p> A Laplacian prior (P (s m ) / exp [js m j]) finds s with minimum L 1 norm. This is a nonlinear operation that essentially selects a subset of basis vectors to represent the data <ref> (Chen et al., 1996) </ref>. The resulting representation is sparse. (d) A 64-sample segment of natural speech was fit to a 2fi overcomplete Fourier representation (128 basis functions) (see section 7).
Reference: <author> Comon, P. </author> <year> (1994). </year> <title> Independent component analysis, a new concept. </title> <booktitle> Signal Processing, </booktitle> <volume> 36(3) </volume> <pages> 287-314. </pages>
Reference: <author> Daugman, J. G. </author> <year> (1988). </year> <title> Complete discrete 2-d gabor transforms by neural networks for image-analysis and compression. </title> <journal> IEEE Transactions on Acoustics Speech and Signal Processing, </journal> <volume> 36(7) </volume> <pages> 1169-1179. </pages>
Reference: <author> Daugman, J. G. </author> <year> (1989). </year> <title> Entropy reduction and decorrelation in visual coding by oriented neural receptive-fields. </title> <journal> IEEE Transactions on Biomedical Engineering, </journal> <volume> 36(1) </volume> <pages> 107-114. </pages>
Reference: <author> Field, D. J. </author> <year> (1994). </year> <title> What is the goal of sensory coding. </title> <journal> Neural Computation, </journal> <volume> 6(4) </volume> <pages> 559-601. </pages>
Reference: <author> Hinton, G. E. and Sejnowski, T. J. </author> <year> (1986). </year> <title> Learning and relearning in Boltzmann machines. </title> <editor> In Rumelhart, D. E. and McClelland, J. L., editors, </editor> <booktitle> Parallel Distributed Processing, </booktitle> <volume> volume 1, chapter 7, </volume> <pages> pages 282-317. </pages> <publisher> MIT Press, </publisher> <address> Cambridge. </address>
Reference: <author> Jutten, C. and Herault, J. </author> <year> (1991). </year> <title> Blind separation of sources .1. an adaptive algorithm based on neuromimetic architecture. </title> <booktitle> Signal Processing, </booktitle> <volume> 24(1) </volume> <pages> 1-10. </pages>
Reference: <author> Lewicki, M. S. and Olshausen, B. A. </author> <year> (1998). </year> <title> Inferring sparse, overcomplete image codes using an efficient coding framework. </title> <booktitle> In Advances in Neural and Information Processing Systems, volume 10, </booktitle> <address> San Mateo. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: If is large (low noise), then the Hessian is 24 dominated by A T A and AA T AH 1 = AA T A (A T A + B) 1 (48) It is also possible to obtain more accurate approximations of this term <ref> (Lewicki and Ol-shausen, 1998) </ref>. The vector y hides a computation involving the inverse Hessian. If the basis vectors in A are randomly distributed, then as the dimensionality of A increases the basis vectors become approximately orthogonal and consequently the Hessian becomes approximately diagonal.
Reference: <author> Linsker, R. </author> <year> (1988). </year> <title> Self-organization in a perceptual network. </title> <journal> Computer, </journal> <volume> 21(3) </volume> <pages> 105-117. </pages>
Reference: <author> MacKay, D. J. C. </author> <year> (1996). </year> <title> Maximum likelihood and covariant algorithms for independent component analysis. </title> <institution> University of Cambridge, Cavendish Laboratory. </institution> <note> Available at ftp://wol.ra.phy.cam.ac.uk/pub/mackay/ica.ps.gz. </note>
Reference: <author> Makeig, S., Jung, T. P., Bell, A. J., Ghahremani, D., and Sejnowski, T. J. </author> <year> (1996). </year> <title> Blind separation of event-related brain response components. </title> <publisher> Psychophysiology, 33:S58-S58. </publisher>
Reference-contexts: ICA is highly effective in several applications such as blind source separation of mixed audio signals (Jutten and Herault, 1991; Bell and Sejnowski, 1995), decomposition of electroencephalographic (EEG) signals <ref> (Makeig et al., 1996) </ref>, and the analysis of functional magnetic resonance imaging (fMRI) data (McKeown et al., 1998). In all of these techniques, the number of basis vectors is equal to the number of inputs.
Reference: <author> Mallat, S. G. and Zhang, Z. F. </author> <year> (1993). </year> <title> Matching pursuits with time-frequency dictionaries. </title> <journal> IEEE Transactions on Signal Processing, </journal> <volume> 41(12) </volume> <pages> 3397-3415. </pages>
Reference: <author> McKeown, M. J., Jung, T.-P., Makeig, S., Brown, G. G., Kindermann, S. S., Lee, T., and Sejnowski, T. </author> <year> (1998). </year> <title> Spatially independent activity patterns in functional magnetic resonance imaging data during the stroop color-naming task. </title> <booktitle> Proc. </booktitle> <institution> Natl. Acad. Sci. USA. </institution> <note> (in press). </note>
Reference-contexts: ICA is highly effective in several applications such as blind source separation of mixed audio signals (Jutten and Herault, 1991; Bell and Sejnowski, 1995), decomposition of electroencephalographic (EEG) signals (Makeig et al., 1996), and the analysis of functional magnetic resonance imaging (fMRI) data <ref> (McKeown et al., 1998) </ref>. In all of these techniques, the number of basis vectors is equal to the number of inputs. Because these bases span the input space, they are complete and are sufficient to represent the data, but, as we will see, this representation can be limited.
Reference: <author> Meszaros, C. </author> <year> (1997). </year> <title> BPMPD: An interior point linear programming solver. </title> <note> Code available at ftp://ftp.netlib.org/opt/bpmpd.tar.gz. </note>
Reference-contexts: The most probable coefficients were obtained using BPMPD and publicly available interior point linear programming package <ref> (Meszaros, 1997) </ref>. The first example is shown in figure 3a. The data were generated from the true basis vectors (shown in gray) using x = As. For purposes of illustration, the elements of s were drawn from an exponential distribution with unit mean.
Reference: <author> Olshausen, B. A. and Field, D. J. </author> <year> (1996). </year> <title> Emergence of simple-cell receptive-field properties by learning a sparse code for natural images. </title> <journal> Nature, </journal> <volume> 381 </volume> <pages> 607-609. </pages>
Reference-contexts: Developing algorithms for learning overcomplete basis is more difficult because of the nonlinearities that can arise in decomposing an input pattern. One recent success was developed from the viewpoint of learning sparse codes <ref> (Olshausen and Field, 1996) </ref>. This algorithm can be viewed as optimizing a probabilistic objective (Olshausen and Field, 1997), but relied on an approximation which has some drawbacks, including a tendency for learning to become increasingly slow and sensitive to parameter values for bases with higher degrees of overcompleteness.
Reference: <author> Olshausen, B. A. and Field, D. J. </author> <year> (1997). </year> <title> Sparse coding with an overcomplete basis set: A strategy employed by V1? Vision Res., </title> <type> 37(23). </type> <note> 26 Osuna, </note> <author> E., Freund, R., and Girosi, F. </author> <year> (1997). </year> <title> An improved training algorithm for support vector machines. </title> <booktitle> Proc. of IEEE NNSP'97, </booktitle> <pages> pages 24-26. </pages>
Reference-contexts: Overcomplete codes have also been proposed as a model of some of the response properties of neurons in primary visual cortex <ref> (Olshausen and Field, 1997) </ref>. simple two-dimensional dataspace. PCA assumes Gaussian structure, but if the data have non-Gaussian structure, the vectors can point in directions that contain very little data, and it will predict data where none occurs. By Shannon's theorem, this will limit the efficiency of the representation. <p> Developing algorithms for learning overcomplete basis is more difficult because of the nonlinearities that can arise in decomposing an input pattern. One recent success was developed from the viewpoint of learning sparse codes (Olshausen and Field, 1996). This algorithm can be viewed as optimizing a probabilistic objective <ref> (Olshausen and Field, 1997) </ref>, but relied on an approximation which has some drawbacks, including a tendency for learning to become increasingly slow and sensitive to parameter values for bases with higher degrees of overcompleteness. <p> In the case of overcomplete representations, however, such a solution is not possible, and recent approaches have tried to approximate this integral by evaluating P (s)P (xjA; s) at its maximum <ref> (Olshausen and Field, 1997) </ref>. One of the drawbacks of this approximation is that it ignores the volume information of the posterior.
Reference: <author> Pearlmutter, B. A. and Parra, L. C. </author> <year> (1997). </year> <title> Maximum likelihood blind source separation: A context-senstive generalization of ica. </title> <booktitle> In Advances in Neural and Information Processing Systems, volume 9, </booktitle> <address> San Mateo. </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Press, W. H., Teukolsky, S. A., Vetterling, W. T., and Flannery, B. P. </author> <year> (1992). </year> <title> Numerical Recipies in C: </title> <booktitle> The art of Scientific Programming (2nd ed.). </booktitle> <publisher> Cambridge University Press, </publisher> <address> Cambridge, England. </address>
Reference-contexts: This yielded solutions similar to equation 8, but resulted in fewer zero-length basis vectors. One hundred patterns were used to estimate each learning step. Learning was terminated after 5000 steps. The most probable basis function coefficients, ^s, were obtained using a modified conjugate gradient routine <ref> (Press et al., 1992) </ref>. The basic routine was modified to replace the line search with an approximate Newton step. This approach resulted in a substantial speed improvement and produced much better solutions in a fixed amount of time than the standard routine.
Reference: <author> Silverman, B. W. </author> <year> (1986). </year> <title> Density estimation for statistics and data analysis. </title> <publisher> Chapman and Hall, </publisher> <address> New York. </address>
Reference-contexts: We use the mean value of ffis i to obtain a single quantization level ffis for all coefficients. The function f (s) by applying kernel density estimation <ref> (Silverman, 1986) </ref> to the distribution of coefficients fit to a training data set. We use a Laplacian kernel with a window width of 2ffis.
Reference: <author> Simoncelli, E. P., Freeman, W. T., Adelson, E. H., and J., H. D. </author> <year> (1992). </year> <title> Shiftable multiscale transforms. </title> <journal> IEEE Trans. Info. Theory, </journal> <volume> 38 </volume> <pages> 587-607. 27 </pages>
References-found: 25

