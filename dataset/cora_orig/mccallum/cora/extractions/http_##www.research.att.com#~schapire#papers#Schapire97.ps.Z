URL: http://www.research.att.com/~schapire/papers/Schapire97.ps.Z
Refering-URL: http://www.cs.orst.edu/~margindr/ML_RG/fall97-mlrg.html
Root-URL: 
Email: schapire@research.att.com  
Title: Using output codes to boost multiclass learning problems  
Author: Robert E. Schapire 
Address: 600 Mountain Avenue, Room 2A-424 Murray Hill, NJ 07974  
Affiliation: AT&T Labs  
Note: Machine Learning: Proceedings of the Fourteenth International Conference, 1997.  
Abstract: This paper describes a new technique for solving multiclass learning problems by combining Freund and Schapire's boosting algorithm with the main ideas of Diet-terich and Bakiri's method of error-correcting output codes (ECOC). Boosting is a general method of improving the accuracy of a given base or weak learning algorithm. ECOC is a robust method of solving multiclass learning problems by reducing to a sequence of two-class problems. We show that our new hybrid method has advantages of both: Like ECOC, our method only requires that the base learning algorithm work on binary-labeled data. Like boosting, we prove that the method comes with strong theoretical guarantees on the training and generalization error of the final combined hypothesis assuming only that the base learning algorithm perform slightly better than random guessing. Although previous methods were known for boosting multi-class problems, the new method may be significantly faster and require less programming effort in creating the base learning algorithm. We also compare the new algorithm experimentally to other voting methods. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Leo Breiman. </author> <title> Bagging predictors. </title> <journal> Machine Learning, </journal> <volume> 24(2) </volume> <pages> 123-140, </pages> <year> 1996. </year>
Reference-contexts: Intuitively, it is easier for the weak learner to identify a set of labels which may plausibly be correct, rather than selecting a single label. 1 Freund and Schapire [8] allow soft hypotheses to take a more general form as functions mapping X fi Y into <ref> [0; 1] </ref>. The soft hypotheses we consider are equivalent to restricting theirs to have range f0; 1g. <p> further penalizes each incorrect label ` 6= y i which is included in the plausible set (so that ` 2 h t (x i )). (Recall that D t (i; y i ) = 0 so correct labels contribute nothing to the sum.) Note that the pseudoloss is always in <ref> [0; 1] </ref> and that pseudoloss 1=2 can be obtained trivially by setting h t (x) = ; for all x. Freund and Schapire's [8] ADABOOST.M2 algorithm works by increasing, on each round, the weight placed on examples x i and incorrect labels ` which contribute most to the pseudoloss. <p> Such a random code is highly likely to have error-correcting properties, but it is certainly plausible that a more carefully designed code would perform better than the results reported here. * Breiman's <ref> [1] </ref> bagging algorithm, which reruns the weak learner on randomly chosen bootstrap samples. * Breiman's [2] Arc-x4 algorithm, which, like AD-ABOOST, adaptively reweights the data, but using a different rule for computing the distribution over examples in a manner that does not require weak hypotheses with error less than 1=2.
Reference: [2] <author> Leo Breiman. </author> <title> Bias, variance, and arcing classifiers. </title> <type> Technical Report 460, </type> <institution> Statistics Department, University of California at Berkeley, </institution> <year> 1996. </year>
Reference-contexts: The first boosting algorithms were discovered by Scha-pire [17] and Freund [6]. Freund and Schapire's most recent boosting algorithm [8], called ADABOOST, has been shown to be very effective in experiments conducted by Drucker and Cortes [5], Jackson and Craven [13], Freund and Schapire [7], Quinlan [15], Breiman <ref> [2] </ref> and others. fl AT&T Labs is planning to move from Murray Hill. The new address will be: 180 Park Avenue, Florham Park, NJ 07932-0971. In its simplest form, ADABOOST requires that the accuracy of each weak hypothesis (or classification rule) produced by the weak learner must exceed 1=2. <p> For fairly powerful weak learners, such as decision-tree algorithms, this does not seem to be a problem. Experimentally, C4.5 and CART seem to be capable of producing hypotheses with accuracy 1=2, even on the difficult distributions of examples produced by boosting <ref> [2, 5, 7, 15] </ref>. However, the accuracy 1=2 requirement can often be a difficulty for less powerful weak learners, such as the simple attribute-value tests studied by Holte [12], and used by Jack-son and Craven [13] and Freund and Schapire [7] in their boosting experiments. <p> Such a random code is highly likely to have error-correcting properties, but it is certainly plausible that a more carefully designed code would perform better than the results reported here. * Breiman's [1] bagging algorithm, which reruns the weak learner on randomly chosen bootstrap samples. * Breiman's <ref> [2] </ref> Arc-x4 algorithm, which, like AD-ABOOST, adaptively reweights the data, but using a different rule for computing the distribution over examples in a manner that does not require weak hypotheses with error less than 1=2.
Reference: [3] <author> William Cohen. </author> <title> Fast effective rule induction. </title> <booktitle> In Proceedings of the Twelfth International Conference on Machine Learning, </booktitle> <pages> pages 115-123, </pages> <year> 1995. </year>
Reference-contexts: Such a rule is built up using an entropic potential as in C4.5 and then pruned back using held-out data. This method is based loosely on the rule-formation part of Cohen's RIPPER algorithm <ref> [3] </ref> and Furnkranz and Widmer's IREP algorithm [9]. The algorithms FINDATTRTEST and FINDDECRULE are described in more detail by Freund and Schapire [7]. Note 2 URL: http://www.ics.uci.edu/~mlearn/MLRepository.html 7 weak learner. that both algorithms find a single rule for the entire problem, as opposed to learning one rule per class.
Reference: [4] <author> Thomas G. Dietterich and Ghulum Bakiri. </author> <title> Solving multiclass learning problems via error-correcting output codes. </title> <journal> Journal of Artificial Intelligence Research, </journal> <volume> 2 </volume> <pages> 263-286, </pages> <month> January </month> <year> 1995. </year>
Reference-contexts: Typically, the running time of the weak learner is O (k) times slower than that of an error-based algorithm for a k-class problem. In this paper, we describe an alternative method for boosting multiclass learning algorithms. Our method combines boosting with Dietterich and Bakiri's <ref> [4] </ref> approach based on error-correcting output codes (ECOC), which is designed to handle multiclass problems using only a binary learning algorithm. Briefly, their approach works as follows: As in boosting, a given weak learning algorithm (which need only be designed for two-class problems) is rerun repeatedly. <p> Since our purpose was to derive an algorithm as effective as boosting but one that only requires an error-based (rather than pseudoloss-based) weak learner, we then compared our algorithm to various other methods which combine error-based weak hypotheses. These were: * Dietterich and Bakiri's ECOC method <ref> [4] </ref>. However, rather than searching for an error-correcting code, we chose the output code at random by selecting each t to be a random (nearly) even split (as described in Section 2.3).
Reference: [5] <author> Harris Drucker and Corinna Cortes. </author> <title> Boosting decision trees. </title> <booktitle> In Advances in Neural Information Processing Systems 8, </booktitle> <pages> pages 479-485, </pages> <year> 1996. </year>
Reference-contexts: Typically, the final combined hypothesis is a weighted vote of the weak hypotheses. The first boosting algorithms were discovered by Scha-pire [17] and Freund [6]. Freund and Schapire's most recent boosting algorithm [8], called ADABOOST, has been shown to be very effective in experiments conducted by Drucker and Cortes <ref> [5] </ref>, Jackson and Craven [13], Freund and Schapire [7], Quinlan [15], Breiman [2] and others. fl AT&T Labs is planning to move from Murray Hill. The new address will be: 180 Park Avenue, Florham Park, NJ 07932-0971. <p> For fairly powerful weak learners, such as decision-tree algorithms, this does not seem to be a problem. Experimentally, C4.5 and CART seem to be capable of producing hypotheses with accuracy 1=2, even on the difficult distributions of examples produced by boosting <ref> [2, 5, 7, 15] </ref>. However, the accuracy 1=2 requirement can often be a difficulty for less powerful weak learners, such as the simple attribute-value tests studied by Holte [12], and used by Jack-son and Craven [13] and Freund and Schapire [7] in their boosting experiments.
Reference: [6] <author> Yoav Freund. </author> <title> Boosting a weak learning algorithm by majority. </title> <journal> Information and Computation, </journal> <volume> 121(2) </volume> <pages> 256-285, </pages> <year> 1995. </year>
Reference-contexts: Boosting effectively forces the weak learning algorithm to concentrate on the hardest examples. Typically, the final combined hypothesis is a weighted vote of the weak hypotheses. The first boosting algorithms were discovered by Scha-pire [17] and Freund <ref> [6] </ref>. Freund and Schapire's most recent boosting algorithm [8], called ADABOOST, has been shown to be very effective in experiments conducted by Drucker and Cortes [5], Jackson and Craven [13], Freund and Schapire [7], Quinlan [15], Breiman [2] and others. fl AT&T Labs is planning to move from Murray Hill.
Reference: [7] <author> Yoav Freund and Robert E. Schapire. </author> <title> Experiments with a new boosting algorithm. </title> <booktitle> In Machine Learning: Proceedings of the Thirteenth International Conference, </booktitle> <pages> pages 148-156, </pages> <year> 1996. </year>
Reference-contexts: The first boosting algorithms were discovered by Scha-pire [17] and Freund [6]. Freund and Schapire's most recent boosting algorithm [8], called ADABOOST, has been shown to be very effective in experiments conducted by Drucker and Cortes [5], Jackson and Craven [13], Freund and Schapire <ref> [7] </ref>, Quinlan [15], Breiman [2] and others. fl AT&T Labs is planning to move from Murray Hill. The new address will be: 180 Park Avenue, Florham Park, NJ 07932-0971. <p> For fairly powerful weak learners, such as decision-tree algorithms, this does not seem to be a problem. Experimentally, C4.5 and CART seem to be capable of producing hypotheses with accuracy 1=2, even on the difficult distributions of examples produced by boosting <ref> [2, 5, 7, 15] </ref>. However, the accuracy 1=2 requirement can often be a difficulty for less powerful weak learners, such as the simple attribute-value tests studied by Holte [12], and used by Jack-son and Craven [13] and Freund and Schapire [7] in their boosting experiments. <p> However, the accuracy 1=2 requirement can often be a difficulty for less powerful weak learners, such as the simple attribute-value tests studied by Holte [12], and used by Jack-son and Craven [13] and Freund and Schapire <ref> [7] </ref> in their boosting experiments. Although overall error rate is often better when more powerful weak learners are used, these less expressive weak learners have the advantage that the final combined hypothesis is usually less complicated, and computation time may be more reasonable, especially for very large datasets. <p> This design gives the boosting algorithm the freedom to focus the weak learner not only on the hard to predict examples, but also on the labels which are hardest to distinguish from the correct label. This approach works well experimentally <ref> [7] </ref>, but suffers certain drawbacks. First, it requires the design of a weak learner which is responsive to the pseudoloss defined by the boosting algorithm and whose hypotheses generate predictions in the form of plausibility sets. <p> This method is based loosely on the rule-formation part of Cohen's RIPPER algorithm [3] and Furnkranz and Widmer's IREP algorithm [9]. The algorithms FINDATTRTEST and FINDDECRULE are described in more detail by Freund and Schapire <ref> [7] </ref>. Note 2 URL: http://www.ics.uci.edu/~mlearn/MLRepository.html 7 weak learner. that both algorithms find a single rule for the entire problem, as opposed to learning one rule per class. The last weak learner tested is Quinlan's C4.5 decision-tree algorithm [16], with all default options and pruning turned on.
Reference: [8] <author> Yoav Freund and Robert E. Schapire. </author> <title> A decision-theoretic generalization of on-line learning and an application to boosting. </title> <journal> Journal of Computer and System Sciences, </journal> <note> To appear. An extended abstract appeared in EuroCOLT'95. </note>
Reference-contexts: Boosting effectively forces the weak learning algorithm to concentrate on the hardest examples. Typically, the final combined hypothesis is a weighted vote of the weak hypotheses. The first boosting algorithms were discovered by Scha-pire [17] and Freund [6]. Freund and Schapire's most recent boosting algorithm <ref> [8] </ref>, called ADABOOST, has been shown to be very effective in experiments conducted by Drucker and Cortes [5], Jackson and Craven [13], Freund and Schapire [7], Quinlan [15], Breiman [2] and others. fl AT&T Labs is planning to move from Murray Hill. <p> Although overall error rate is often better when more powerful weak learners are used, these less expressive weak learners have the advantage that the final combined hypothesis is usually less complicated, and computation time may be more reasonable, especially for very large datasets. Freund and Schapire <ref> [8] </ref> provide one solution to this problem by modifying the form of the weak hypotheses and refining the goal of the weak learner. In this approach, rather than predicting a single class for each example, the weak learner chooses a set of plausible labels for each example. <p> To do this, we will reduce to the pseudoloss method used by Freund and Schapire <ref> [8] </ref> in the development of ADABOOST.M2, one of the multiclass versions of their boosting algorithm. This reduction will also lead to an analysis of the resulting algorithm. 2.1 REVIEW OF ADABOOST.M2 We begin with a review of Freund and Schapire's [8] pseu-doloss method and of the boosting algorithm ADA-BOOST.M2, shown in <p> reduce to the pseudoloss method used by Freund and Schapire <ref> [8] </ref> in the development of ADABOOST.M2, one of the multiclass versions of their boosting algorithm. This reduction will also lead to an analysis of the resulting algorithm. 2.1 REVIEW OF ADABOOST.M2 We begin with a review of Freund and Schapire's [8] pseu-doloss method and of the boosting algorithm ADA-BOOST.M2, shown in Figure 2. On each round, the boosting algorithm computes a distribution D t over f1; : : : ; mg fi Y such that D t (i; y i ) = 0 for all i. <p> Intuitively, it is easier for the weak learner to identify a set of labels which may plausibly be correct, rather than selecting a single label. 1 Freund and Schapire <ref> [8] </ref> allow soft hypotheses to take a more general form as functions mapping X fi Y into [0; 1]. The soft hypotheses we consider are equivalent to restricting theirs to have range f0; 1g. <p> Freund and Schapire's <ref> [8] </ref> ADABOOST.M2 algorithm works by increasing, on each round, the weight placed on examples x i and incorrect labels ` which contribute most to the pseudoloss. <p> The combined hypothesis then chooses the single label which occurs in the largest number of plausible label sets chosen by the weak hypotheses, where the votes of some weak hypotheses count for more than others. Let * t = 1=2 fl t . Freund and Schapire <ref> [8, Theorem 11] </ref> show that the training error of the combined hypothesis H final of ADABOOST.M2 is bounded by (k 1) t=1 1 4 fl 2 T X fl 2 ! Thus, if the fl t 's are bounded away from 1=2, then training error goes to zero exponentially fast. <p> The resulting algorithm, called ADABOOST.OC, is shown in Figure 3. By our method of derivation, this algorithm is in fact a special case of ADABOOST.M2 in which the weak soft hypothesis h t has a particular form. Therefore, we can immediately apply the results of Freund and Schapire <ref> [8] </ref> to obtain the following theorem, which is the main theoretical result of this paper: Theorem 1 Let 1 ; : : : ; T be any sequence of colorings and let h 1 ; : : : ; h T be any sequence of weak hypotheses returned by the weak
Reference: [9] <author> Johannes Furnkranz and Gerhard Widmer. </author> <title> Incremental reduced error pruning. </title> <booktitle> In Machine Learning: Proceedings of the Eleventh International Conference, </booktitle> <pages> pages 70-77, </pages> <year> 1994. </year>
Reference-contexts: Such a rule is built up using an entropic potential as in C4.5 and then pruned back using held-out data. This method is based loosely on the rule-formation part of Cohen's RIPPER algorithm [3] and Furnkranz and Widmer's IREP algorithm <ref> [9] </ref>. The algorithms FINDATTRTEST and FINDDECRULE are described in more detail by Freund and Schapire [7]. Note 2 URL: http://www.ics.uci.edu/~mlearn/MLRepository.html 7 weak learner. that both algorithms find a single rule for the entire problem, as opposed to learning one rule per class.
Reference: [10] <author> Michel X. Goemans and David P. Williamson. </author> <title> Improved approximation algorithms for maximum cut and satisfiability problems using semidefinite programming. </title> <journal> Journal of the Association for Computing Machinery, </journal> <volume> 42(6) </volume> <pages> 1115-1145, </pages> <month> November </month> <year> 1995. </year>
Reference-contexts: Written in this form, it is straightforward to show that maximizing U t is a special case of the MAX-CUT problem, which is known to be NP-complete [14], but for which various, rather sophisticated approximation methods are also known <ref> [10, 11] </ref>. We did not attempt to use any of these methods, and it is plausible that one of these might improve performance. In addition, various greedy hill-climbing methods can also be used which guarantee U t 1=2.
Reference: [11] <author> Oded Goldreich, Shafi Goldwasser, and Dana Ron. </author> <title> Property testing and its connection to learning and approximation. </title> <booktitle> In 37th Annual Symposium on Foundations of Computer Science, </booktitle> <pages> pages 339-348, </pages> <year> 1996. </year>
Reference-contexts: Written in this form, it is straightforward to show that maximizing U t is a special case of the MAX-CUT problem, which is known to be NP-complete [14], but for which various, rather sophisticated approximation methods are also known <ref> [10, 11] </ref>. We did not attempt to use any of these methods, and it is plausible that one of these might improve performance. In addition, various greedy hill-climbing methods can also be used which guarantee U t 1=2.
Reference: [12] <author> Robert C. Holte. </author> <title> Very simple classification rules perform well on most commonly used datasets. </title> <journal> Machine Learning, </journal> <volume> 11(1) </volume> <pages> 63-91, </pages> <year> 1993. </year>
Reference-contexts: However, the accuracy 1=2 requirement can often be a difficulty for less powerful weak learners, such as the simple attribute-value tests studied by Holte <ref> [12] </ref>, and used by Jack-son and Craven [13] and Freund and Schapire [7] in their boosting experiments. <p> For discrete attributes, equality is tested; for continuous attributes, a threshold value is compared. The best hypothesis of this form which minimizes error or pseudoloss can be found by a direct and efficient search method. These weak learners are similar in spirit to those studied by Holte <ref> [12] </ref>. The second weak learner, called FINDDECRULE, outputs a hypothesis which tests on a conjunction of attribute-value comparisons. Such a rule is built up using an entropic potential as in C4.5 and then pruned back using held-out data.
Reference: [13] <author> Jeffrey C. Jackson and Mark W. Craven. </author> <title> Learning sparse perceptrons. </title> <booktitle> In Advances in Neural Information Processing Systems 8, </booktitle> <pages> pages 654-660, </pages> <year> 1996. </year>
Reference-contexts: The first boosting algorithms were discovered by Scha-pire [17] and Freund [6]. Freund and Schapire's most recent boosting algorithm [8], called ADABOOST, has been shown to be very effective in experiments conducted by Drucker and Cortes [5], Jackson and Craven <ref> [13] </ref>, Freund and Schapire [7], Quinlan [15], Breiman [2] and others. fl AT&T Labs is planning to move from Murray Hill. The new address will be: 180 Park Avenue, Florham Park, NJ 07932-0971. <p> However, the accuracy 1=2 requirement can often be a difficulty for less powerful weak learners, such as the simple attribute-value tests studied by Holte [12], and used by Jack-son and Craven <ref> [13] </ref> and Freund and Schapire [7] in their boosting experiments.
Reference: [14] <author> R. M. Karp. </author> <title> Reducibility among combinatorial problems. </title> <editor> In R. E. Miller and J. W. Thatcher, editors, </editor> <booktitle> Complexity of Computer Computations, </booktitle> <pages> pages 85-103. </pages> <publisher> Plenum Press, </publisher> <year> 1972. </year>
Reference-contexts: Written in this form, it is straightforward to show that maximizing U t is a special case of the MAX-CUT problem, which is known to be NP-complete <ref> [14] </ref>, but for which various, rather sophisticated approximation methods are also known [10, 11]. We did not attempt to use any of these methods, and it is plausible that one of these might improve performance. In addition, various greedy hill-climbing methods can also be used which guarantee U t 1=2.
Reference: [15] <author> J. R. Quinlan. Bagging, </author> <title> boosting, </title> <booktitle> and C4.5. In Proceedings of the Thirteenth National Conference on Artificial Intelligence, </booktitle> <pages> pages 725-730, </pages> <year> 1996. </year>
Reference-contexts: The first boosting algorithms were discovered by Scha-pire [17] and Freund [6]. Freund and Schapire's most recent boosting algorithm [8], called ADABOOST, has been shown to be very effective in experiments conducted by Drucker and Cortes [5], Jackson and Craven [13], Freund and Schapire [7], Quinlan <ref> [15] </ref>, Breiman [2] and others. fl AT&T Labs is planning to move from Murray Hill. The new address will be: 180 Park Avenue, Florham Park, NJ 07932-0971. <p> For fairly powerful weak learners, such as decision-tree algorithms, this does not seem to be a problem. Experimentally, C4.5 and CART seem to be capable of producing hypotheses with accuracy 1=2, even on the difficult distributions of examples produced by boosting <ref> [2, 5, 7, 15] </ref>. However, the accuracy 1=2 requirement can often be a difficulty for less powerful weak learners, such as the simple attribute-value tests studied by Holte [12], and used by Jack-son and Craven [13] and Freund and Schapire [7] in their boosting experiments.
Reference: [16] <author> J. Ross Quinlan. C4.5: </author> <title> Programs for Machine Learning. </title> <publisher> Morgan Kaufmann, </publisher> <year> 1993. </year>
Reference-contexts: Note 2 URL: http://www.ics.uci.edu/~mlearn/MLRepository.html 7 weak learner. that both algorithms find a single rule for the entire problem, as opposed to learning one rule per class. The last weak learner tested is Quinlan's C4.5 decision-tree algorithm <ref> [16] </ref>, with all default options and pruning turned on. Also, rather than modify C4.5 to handle weighted examples, on each round t of boosting, we reran C4.5 on (unweighted) examples which were randomly resampled according to D t .
Reference: [17] <author> Robert E. Schapire. </author> <title> The strength of weak learnability. </title> <journal> Machine Learning, </journal> <volume> 5(2) </volume> <pages> 197-227, </pages> <year> 1990. </year>
Reference-contexts: Boosting effectively forces the weak learning algorithm to concentrate on the hardest examples. Typically, the final combined hypothesis is a weighted vote of the weak hypotheses. The first boosting algorithms were discovered by Scha-pire <ref> [17] </ref> and Freund [6].
Reference: [18] <author> Robert E. Schapire, Yoav Freund, Peter Bartlett, and Wee Sun Lee. </author> <title> Boosting the margin: A new explanation for the effectiveness of voting methods. </title> <booktitle> In Machine Learning: Proceedings of the Fourteenth International Conference, </booktitle> <year> 1997. </year> <month> 9 </month>
Reference-contexts: Note that, although the weak hypotheses are evaluated with respect to pseudoloss, the final hypothesis H final is analyzed with respect to the usual error measure. Freund and Schapire also give a method of bounding the generalization error of the combined hypothesis, but, more recently, Schapire et al. <ref> [18] </ref> have come up with a better analysis of voting methods such as ADABOOST.M2. <p> Although, for simplicity, we have focused only on the training error, the generalization error can also be bounded using the methods of Schapire et al. <ref> [18] </ref>. This leads to a bound on the generalization error of the combined hypothesis of the form (k 1) t=1 +O 1 m log (m=d)(k +d log (m=d)) 1=2 which holds for all &gt; 0 with probability at least 1 ffi.
References-found: 18

