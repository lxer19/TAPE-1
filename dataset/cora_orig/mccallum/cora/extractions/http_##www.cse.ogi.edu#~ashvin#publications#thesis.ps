URL: http://www.cse.ogi.edu/~ashvin/publications/thesis.ps
Refering-URL: http://www.cse.ogi.edu/~ashvin/research.html
Root-URL: http://www.cse.ogi.edu
Title: View Consistency for Optimistic Replication  
Degree: A thesis submitted in partial satisfaction of the requirements for the degree Master of Science in Computer Science by Ashvin Goel  
Date: 1996  
Affiliation: University of California Los Angeles  
Abstract-found: 0
Intro-found: 1
Reference: [ABC90] <author> Rafael Alonso, Daniel Barbara, and Luis L. Cova. </author> <title> "Using Stashing to Increase Node Autonomy in Distributed File Systems." </title> <booktitle> In Proceedings of the Ninth IEEE Symposium on Reliability in Distributed Software and Database Systems, </booktitle> <pages> pp. 12-21, </pages> <month> October </month> <year> 1990. </year>
Reference-contexts: Finally, they do not discuss session guarantees for distributed entities. Our work can be generalized to distributed entities, as described in Section 9.2 on future work. Client-based consistency has been used by Alonso, et el., <ref> [ABC90] </ref> to provide quasi-copy consistency. Quasi-copies are cached (or stashed) copies of data that may be somewhat out-of-date but are guaranteed to meet certain consistency predicates. For example, the predicate can state that the copy must not be more than ten minutes old, or more than two versions old.
Reference: [All83] <author> James E. Allchin. </author> <title> "A Suite of Robust Algorithms for Maintaining Replicated Data Using Weak Consistency Conditions." </title> <booktitle> In Proceedings of the Third IEEE Symposium on Reliability in Distributed Software and Database Systems, </booktitle> <month> October </month> <year> 1983. </year>
Reference-contexts: It provides instantaneous data consistency since each access yields consistent data. Moreover, an access yields the same data as is obtained in a non-distributed or a single copy system. This makes strong consistency an attractive consistency model. Unfortunately, strong consistency requires reliable communication links for high data availability <ref> [FM82, All83] </ref>. When a network becomes partitioned, data cannot be accessed in more than one partition. For example, suppose the network is partitioned into two sets of replicas, A and B. If updates are allowed in partition A, then access must be denied to replicas in partition B. <p> Data accesses do not have to be serialized for maintaining the semantics of several types of data <ref> [FM82, All83, GAB83] </ref>. For other data types, some 4 minimal inconsistency, or external intervention to restore the semantics of the data is often considered more acceptable than entirely inhibiting data accesses when communication is not possible [Sat89, GHM90, TTP95]. <p> Several approaches for supporting one-copy serializability in distributed systems have been proposed [BG81]. One-copy serializability uses a strong consistency replica control algorithm to map logical data into physical replicas. Strong consistency provides consistent data by restricting concurrency and availability of data <ref> [FM82, All83] </ref> to a single network partition. Moreover, it adds considerable communication overhead to support the needed synchronization. Both restricting availability and adding communication overhead contradict the goals of providing higher data availability and increased access speeds in distributed systems. Replication is used to overcome some of these problems. <p> However, availability is still limited to a single partition when strong consistency is used for replica control. 8.1 Typed Consistency Several researchers realized that accesses do not have to be serialized for maintaining data consistency for certain data types <ref> [FM82, All83, DS83, WB84] </ref>. The semantics of the data type can be used to define an alternative consistency cri 86 terion. Much of this research was aimed at providing consistency for the directory data structure that allows the insert, delete and list operations.
Reference: [BCK93] <author> Arindam Benerji, David L. Cohn, and Dinesh C. Kulkarni. </author> <booktitle> "Mobile Computing Personae." In Proceedings of the Fourth Workshop on Workstation Operating Systems, </booktitle> <pages> pp. 14-20, </pages> <address> Napa, California, </address> <month> October </month> <year> 1993. </year> <note> IEEE. </note>
Reference-contexts: Unfortunately, it may not be possible to guarantee age-dependent predicates in a large distributed system, where network partitions are frequent, and it is essential to provide data availability. Client-based computing has been proposed by Banerji and Cohn <ref> [BCK93] </ref>. They propose a mobile computing model where each user sees the computing world through a personalized view called the computing persona. Persona management includes cost-effective access to resources, and is the responsibility of the system software. <p> In effect, the view-entry database moves along with the user. Moving a user's state along with the user has been proposed earlier by Banerji and Cohn <ref> [BCK93] </ref>. Before files can be accessed from the new machine, the view 97 entry database from the old machine 1 is integrated with the database on the new machine. The integration involves taking a union of the view-entries of the two databases.
Reference: [BG81] <author> P. Bernstein and N. Goodman. </author> <title> "Concurrency Control in Distributed Database Systems." </title> <journal> Computing Surveys, </journal> <volume> 13(2) </volume> <pages> 185-221, </pages> <month> June </month> <year> 1981. </year>
Reference-contexts: Section 1.2 below describes eventual data consistency where each access may not provide consistent data but data eventually converges to a consistent state. Strong Consistency Traditionally, one-copy serializability has been used as a consistency criterion for replicated data <ref> [Pap79, BG81] </ref>. One-copy serializability ensures that the execution of a program does not affect or is not affected by the execution of other programs. In other words, each program (transaction) is isolated from the effects of other programs. <p> Several approaches for supporting one-copy serializability in distributed systems have been proposed <ref> [BG81] </ref>. One-copy serializability uses a strong consistency replica control algorithm to map logical data into physical replicas. Strong consistency provides consistent data by restricting concurrency and availability of data [FM82, All83] to a single network partition. Moreover, it adds considerable communication overhead to support the needed synchronization.
Reference: [BJ87] <author> Kenneth P. Birman and Thomas A. Joseph. </author> <title> "Exploiting Virtual Synchrony in Distributed Systems." </title> <booktitle> In Proceedings of the Eleventh Symposium on Operating Systems Principles, </booktitle> <pages> pp. 123-138. </pages> <publisher> ACM, </publisher> <month> Novem-ber </month> <year> 1987. </year>
Reference-contexts: This work borrows some of its ideas from previous work at ISIS <ref> [BJ87] </ref>. Since Ladin develops a more efficient solution for replicated systems, we will only discuss their work. Causal ordering is similar to Lamport's happened-before relationship [Lam78].
Reference: [CS93] <author> Brent Callaghan and Satinder Singh. </author> <booktitle> "The Autofs Automounter." In USENIX Conference Proceedings, </booktitle> <pages> pp. 59-68. </pages> <publisher> USENIX, </publisher> <month> June </month> <year> 1993. </year>
Reference-contexts: For example, wide-area file-system solutions provide consistent replicated (or cached) data, but do not aggressively search for the best available replica. The replica switching work by Zadok and Duchamp [ZD91] addresses the problem of providing data from the fastest available replica. They improve the amd daemon <ref> [CS93] </ref> in Unix systems (which demand mounts and unmounts file systems) and allow transparent switching of open files to replacement file systems that are dynamically discovered. The latency of the NFS lookup operation is monitored and used to assign availability values to different replicas.
Reference: [DS83] <author> Dean Daniels and Alfred Z. Spector. </author> <title> "An Algorithm for Replicated Directories." </title> <booktitle> Proceedings of the Second Annual ACM Symposium on Principles of Distributed Computing, </booktitle> <pages> pp. 104-113, </pages> <month> 17 Aug </month> <year> 1983. </year>
Reference-contexts: However, availability is still limited to a single partition when strong consistency is used for replica control. 8.1 Typed Consistency Several researchers realized that accesses do not have to be serialized for maintaining data consistency for certain data types <ref> [FM82, All83, DS83, WB84] </ref>. The semantics of the data type can be used to define an alternative consistency cri 86 terion. Much of this research was aimed at providing consistency for the directory data structure that allows the insert, delete and list operations.
Reference: [FE89] <author> Richard A. Floyd and Carla Schlatter Ellis. </author> <title> "Directory Reference Patterns in Hierarchical File Systems." </title> <journal> IEEE Transactions on Knowledge and Data Engineering, </journal> <volume> 1(2) </volume> <pages> 238-247, </pages> <month> June </month> <year> 1989. </year>
Reference-contexts: This is very expensive in normal Unix environments where opens can constitute 15 percent of the total time spent in file 2 We assume that the key of the disk block is in memory. 36 operations <ref> [FE89] </ref>. The number of readViewEntry operations that go to disk can be reduced by caching the view-entries more aggressively. Unix kernels already cache file handles in the dnlc or the name lookup cache. A mapping is maintained between the file name and its file handle.
Reference: [FM82] <author> Michael J. Fischer and Alan Michael. </author> <title> "Sacrificing Serializability to Attain High Availability of Data in an Unreliable Network." </title> <booktitle> In Proceedings of the ACM Symposium on Principles of Database Systems, </booktitle> <month> March </month> <year> 1982. </year>
Reference-contexts: It provides instantaneous data consistency since each access yields consistent data. Moreover, an access yields the same data as is obtained in a non-distributed or a single copy system. This makes strong consistency an attractive consistency model. Unfortunately, strong consistency requires reliable communication links for high data availability <ref> [FM82, All83] </ref>. When a network becomes partitioned, data cannot be accessed in more than one partition. For example, suppose the network is partitioned into two sets of replicas, A and B. If updates are allowed in partition A, then access must be denied to replicas in partition B. <p> For several applications such as banking applications, reservation systems, and personal applications like appointment calendars, design documents, meeting notes, and in general, mobile file accesses, continued access is critical even during network partitions <ref> [PWC81, FM82, Sat89, GHM90, TTP95] </ref>. In mobile environments, network partitions are a normal mode of operation, rather than a failure condition that must, or can be immediately corrected [HPG92, KS92]. <p> Data accesses do not have to be serialized for maintaining the semantics of several types of data <ref> [FM82, All83, GAB83] </ref>. For other data types, some 4 minimal inconsistency, or external intervention to restore the semantics of the data is often considered more acceptable than entirely inhibiting data accesses when communication is not possible [Sat89, GHM90, TTP95]. <p> Several approaches for supporting one-copy serializability in distributed systems have been proposed [BG81]. One-copy serializability uses a strong consistency replica control algorithm to map logical data into physical replicas. Strong consistency provides consistent data by restricting concurrency and availability of data <ref> [FM82, All83] </ref> to a single network partition. Moreover, it adds considerable communication overhead to support the needed synchronization. Both restricting availability and adding communication overhead contradict the goals of providing higher data availability and increased access speeds in distributed systems. Replication is used to overcome some of these problems. <p> However, availability is still limited to a single partition when strong consistency is used for replica control. 8.1 Typed Consistency Several researchers realized that accesses do not have to be serialized for maintaining data consistency for certain data types <ref> [FM82, All83, DS83, WB84] </ref>. The semantics of the data type can be used to define an alternative consistency cri 86 terion. Much of this research was aimed at providing consistency for the directory data structure that allows the insert, delete and list operations.
Reference: [GAB83] <author> Hector Garcia-Molina, Tim Allen, Barbara Blaustein, R. Mark Chilenskas, and Daniel R. Ries. "Data-patch: </author> <title> Integrating Inconsistent 101 Copies of a Database after a Partition." </title> <booktitle> In Proceedings of the Third IEEE Symposium on Reliability in Distributed Software and Database Systems, </booktitle> <pages> pp. 38-44, </pages> <month> October </month> <year> 1983. </year>
Reference-contexts: Data accesses do not have to be serialized for maintaining the semantics of several types of data <ref> [FM82, All83, GAB83] </ref>. For other data types, some 4 minimal inconsistency, or external intervention to restore the semantics of the data is often considered more acceptable than entirely inhibiting data accesses when communication is not possible [Sat89, GHM90, TTP95]. <p> The replicated log solution by Wuu [WB84] addresses this problem. Later, Heddaya, et al., [HHW89] propose a lower overhead solution, although garbage collection is more conservative. The acknowledgment algorithm presented in Section 6.5 borrows several ideas from Wuu. Consistency algorithms that use data semantics provide high data availability. Garcia <ref> [GAB83] </ref>, Sarin [Sar86] and Ladin [LLS90] generalize the directory solution for other data types. Garcia uses a data-patch tool that automates the data integration step. The correct final state of a database and corrective measures to 87 reach it are predefined for various potential non-serializable operations.
Reference: [GHM90] <author> Richard G. Guy, John S. Heidemann, Wai Mak, Thomas W. Page, Jr., Gerald J. Popek, and Dieter Rothmeier. </author> <title> "Implementation of the Ficus Replicated File System." </title> <booktitle> In USENIX Conference Proceedings, </booktitle> <pages> pp. 63-71. </pages> <publisher> USENIX, </publisher> <month> June </month> <year> 1990. </year>
Reference-contexts: For several applications such as banking applications, reservation systems, and personal applications like appointment calendars, design documents, meeting notes, and in general, mobile file accesses, continued access is critical even during network partitions <ref> [PWC81, FM82, Sat89, GHM90, TTP95] </ref>. In mobile environments, network partitions are a normal mode of operation, rather than a failure condition that must, or can be immediately corrected [HPG92, KS92]. <p> For other data types, some 4 minimal inconsistency, or external intervention to restore the semantics of the data is often considered more acceptable than entirely inhibiting data accesses when communication is not possible <ref> [Sat89, GHM90, TTP95] </ref>. Consider a calendar that is shared by several users and replicated on each user's portable. A user can schedule meetings in his personal replica of the calendar, even when other replicas are not available. <p> The lazy replication work by Ladin, et al., provides eventual consistency for updates and also supports causal ordering for reads. Utilizing data semantics for eventual consistency is further explored by several systems such as Locus [PWC81], its successor Ficus <ref> [GHM90] </ref> on which we have implemented view consistency, Coda [Sat89] and Bayou [TTP95]. The data integration procedure is further developed in these systems and a distinction is made between conflict detection and conflict resolution. Locus, Coda and Ficus use version vectors to detect write-write conflicts.
Reference: [Gif79] <author> David K. Gifford. </author> <title> "Weighted Voting for Replicated Data." </title> <booktitle> In Proceedings of the Seventh Symposium on Operating Systems Principles, </booktitle> <pages> pp. 150-162. </pages> <publisher> ACM, </publisher> <month> December </month> <year> 1979. </year>
Reference-contexts: Both restricting availability and adding communication overhead contradict the goals of providing higher data availability and increased access speeds in distributed systems. Replication is used to overcome some of these problems. Various algorithms have been proposed for maintaining consistency among the replicated copies <ref> [Tho79, Gif79] </ref>. However, availability is still limited to a single partition when strong consistency is used for replica control. 8.1 Typed Consistency Several researchers realized that accesses do not have to be serialized for maintaining data consistency for certain data types [FM82, All83, DS83, WB84].
Reference: [GL93] <author> Richard A. Golding and Darrell D. E. </author> <title> Long. "Modeling Replica Divergence in a Weak-Consistency Protocol for Global-Scale Distributed Data Bases." </title> <type> Technical Report UCSC-CRL-93-03, </type> <institution> Computer and Information Sciences, University of California Santa Cruz, </institution> <year> 1993. </year>
Reference-contexts: Other factors such as the data access pattern and the reconciliation topology affect the rate of database truncation, but do not directly depend on the truncation process itself. The view-entry 5 A similar simulation has been done by Golding <ref> [GL93] </ref>. We hope to use his results to fine-tune our reconciliation topology. 83 is removable when the file modification time (and not the file access time 6 ) in the view-entry is less than the acknowledgment time. Suppose the data is not modified often, or is mostly read-only.
Reference: [GPP93] <author> Richard G. Guy, Gerald J. Popek, and Thomas W. Page, Jr. </author> <title> "Consistency Algorithms for Optimistic Replication." </title> <booktitle> In Proceedings of the First International Conference on Network Protocols. IEEE, </booktitle> <month> October </month> <year> 1993. </year>
Reference-contexts: Since the update notification and propagation operations can fail, we cannot depend on these operations alone for files to reach eventual consistency. To encourage files to reach eventual consistency, pairs of file replicas are periodically compared using Ficus reconciliation. Replicas pass information directly or indirectly through a gossiping protocol <ref> [GPP93] </ref>, ensuring that information exchanged in pairwise reconciliations eventually reaches all replicas. Replicas compare their data by using vector timestamps. A vector timestamp A is later than another timestamp B, if each of A's components is greater than or equal to the corresponding component of B.
Reference: [GW82] <author> Hector Garcia-Molina and G. Wiederhold. </author> <title> "Read-Only Transactions in a Distributed Database." </title> <journal> ACM Transactions on Database Systems, </journal> <volume> 7(2), </volume> <month> June </month> <year> 1982. </year>
Reference-contexts: Weak Consistency Strong consistency always provides the latest data version. However it does so by sacrificing availability. Several consistency schemes that enhance availability have been proposed. These schemes trade availability against consistency. One such scheme, weak consistency, is similar to strong consistency except that it ignores read dependencies <ref> [GW82] </ref>. Thus weak consistency provides instantaneous data consistency with respect to writes only. Writes update the latest copy of data, but reads can return older copies. This increases read availability of data. However, writes can only occur in a single partition.
Reference: [HHW89] <author> Abdelsalam Heddaya, Meichun Hsu, and William Weihl. </author> <title> "Two Phase Gossip: Managing Distributed Event Histories." </title> <journal> Information Sciences, </journal> <volume> 49 </volume> <pages> 35-57, </pages> <month> October </month> <year> 1989. </year>
Reference-contexts: The resolution is done either automatically or manually depending on the semantics of the data type. The integration and resolution of data at other sites ensures that replicas eventually reach identical states or become mutually consistent if no new updates are generated in the system <ref> [WB84, HHW89] </ref>. This consistency approach, which guarantees eventual consistency of data, is called eventual data consistency. Eventual data consistency provides high availability by allowing updates to be made in any order. <p> The data propagation algorithm uses distributed logs. These logs must be garbage collected, so that the log sizes do not grow indefinitely, and their transfer between replicas does not become a bottleneck. The replicated log solution by Wuu [WB84] addresses this problem. Later, Heddaya, et al., <ref> [HHW89] </ref> propose a lower overhead solution, although garbage collection is more conservative. The acknowledgment algorithm presented in Section 6.5 borrows several ideas from Wuu. Consistency algorithms that use data semantics provide high data availability. Garcia [GAB83], Sarin [Sar86] and Ladin [LLS90] generalize the directory solution for other data types.
Reference: [HKM88] <author> John Howard, Michael Kazar, Sherri Menees, David Nichols, Mahadev Satyanarayanan, Robert Sidebotham, and Michael West. </author> <title> "Scale and Performance in a Distributed File System." </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 6(1) </volume> <pages> 51-81, </pages> <month> February </month> <year> 1988. </year>
Reference-contexts: SCSI disks are used for persistent storage, and each machine has 12 MB of main memory. The tests were performed during periods of minimal external network activity. There are seven benchmarks that are used in the evaluation. The first test is the modified Andrew Benchmark (mab) <ref> [HKM88, Ous90] </ref> that is intended to model a normal mix of filing operations, and hence be representative of performance in actual use. The second and third tests are local and remote recursive cp and the fourth test is grep. Each of these tests exercise the read and write file operations.
Reference: [HP94] <author> John S. Heidemann and Gerald J. Popek. </author> <title> "File-System Development with Stackable Layers." </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 12(1), </volume> <year> 1994. </year> <note> To appear, ACM Transactions on Computer Systems, February 1994. Also available as UCLA technical report CSD-930019. </note>
Reference-contexts: Both the components have been implemented on Ficus, a SunOS 4.1.1-based kernel, and have been running at UCLA since October 1995. The implementation uses a stackable layers framework <ref> [HP94] </ref> within the kernel, as described in Section 3.1, and a user-level view-entry database server. A deletion server obtains the acknowledgment information from the reconciliation (and acknowledgment) process and uses it to delete view-entries from the database.
Reference: [HPG92] <author> John S. Heidemann, Thomas W. Page, Jr., Richard G. Guy, and Ger-ald J. Popek. </author> <title> "Primarily Disconnected Operation: Experiences with Ficus." </title> <booktitle> In Proceedings of the Second Workshop on Management of Replicated Data, </booktitle> <pages> pp. 2-5. </pages> <publisher> IEEE, </publisher> <month> November </month> <year> 1992. </year> <month> 102 </month>
Reference-contexts: In mobile environments, network partitions are a normal mode of operation, rather than a failure condition that must, or can be immediately corrected <ref> [HPG92, KS92] </ref>. Mobile computers may not frequently connect with stationary networks, either be 3 cause of the cost or the bandwidth restrictions of wireless communication.
Reference: [KBM94] <author> E. D. Katz, M. Butler, and R. McGrath. </author> <title> "A Scalable HTTP Server: The NCSA Prototype." </title> <booktitle> In First International Conference on the World-Wide Web, </booktitle> <month> May </month> <year> 1994. </year>
Reference-contexts: This disallows load balancing between the replicas since all files are accessed from either one replica or the other. The NCSA scalable HTTP server by Katz, et al., <ref> [KBM94] </ref> also addresses some of the replica switching issues in their replicated server. The motivation for replicating the server was to serve an ever-growing body of clients. They use Transarc's Andrew File System to replicate their data.
Reference: [KP87] <author> Phil Karn and Craig Partridge. </author> <title> "Improving Round-Trip Time Estimates in Reliable Transport Protocols." </title> <booktitle> In Proceedings, SIGCOMM '87 Workshop, </booktitle> <pages> pp. 2-7. </pages> <publisher> ACM SIGCOMM, ACM Press, </publisher> <month> Aug </month> <year> 1987. </year>
Reference-contexts: This assumes that the access times to different volume replicas at the same site are roughly the same. 52 This ensures that the mean availability value responds quickly when the current availability value is larger than the mean. Otherwise the mean is not affected as much. Studies have shown <ref> [KP87] </ref> that this mean is sufficiently responsive to the current value (especially when it is large) and yet not too responsive. A new vnode operation has been implemented that transfers the availability value for each replica in the volume from the daemon process to the kernel.
Reference: [KPR94] <author> Geoffrey H. Kuenning, Gerald J. Popek, and Peter Reiher. </author> <title> "An Analysis of Trace Data for Predictive File Caching in Mobile Computing." </title> <booktitle> In USENIX Conference Proceedings, </booktitle> <pages> pp. 291-306. </pages> <publisher> USENIX, </publisher> <month> June </month> <year> 1994. </year>
Reference-contexts: Note that the user may access a much larger number of files but the view-entry of many of these files would be in the database because of file access locality <ref> [KPR94] </ref>. On this basis the view-entry database for each entity grows by 20 KB per day. The rate at which the database can be truncated clearly depends on the frequency with which truncation is attempted. The average database size will become smaller as the frequency of truncation is increased.
Reference: [KS92] <author> James J. Kistler and Mahadev Satyanarayanan. </author> <title> "Disconnected Operation in the Coda File System." </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 10(1) </volume> <pages> 3-25, </pages> <year> 1992. </year>
Reference-contexts: In mobile environments, network partitions are a normal mode of operation, rather than a failure condition that must, or can be immediately corrected <ref> [HPG92, KS92] </ref>. Mobile computers may not frequently connect with stationary networks, either be 3 cause of the cost or the bandwidth restrictions of wireless communication. <p> However, eventual consistency schemes raise a different problem; they do not provide any consistency guarantees during accesses. It has been observed that this does not necessarily cause serious problems for shared files <ref> [KS92, RHR94] </ref>. Often, files that are heavily shared are not frequently updated by multiple users. Since the level of write-sharing is low, few consistency conflicts are generated. Unfortunately, this argument ignores read-dependency issues for shared files. Moreover, it does not take non-shared files into account. <p> Moreover, if they do update the file, a conflict will necessarily happen. Worst of all, the user gets no hint that these problems might arise. Eventual consistency ignores write-dependencies also. Thus writes can be made to older copies of data. This is uncommon for many shared files <ref> [KS92, RHR94] </ref>. However, often a single user may update multiple copies of data and thereby cause conflicts. This is different from the old read problem because the 6 writes may be done without reading the data. Suppose a user is updating a replicated file on replica A.
Reference: [Kue94] <author> Geoffrey H. Kuenning. </author> <title> "Design of the SEER Predictive Caching System." </title> <booktitle> In Proceedings of the Workshop on Mobile Computing Systems and Applications, </booktitle> <address> Santa Cruz, CA, </address> <month> dec </month> <year> 1994. </year>
Reference-contexts: A local file replica deletion must record the version of the deleted data. The next access to a remote replica uses this version to ensure view consistency. 5 Replication agents have also been built at Ficus that automatically add (or drop) replicas of files <ref> [Kue94] </ref>. 42 CHAPTER 5 Replica Switching for Consistency and Improving Availability A distributed system provides data consistency as well as highly available data. Chapter 4 describes the view consistency algorithm and several efficiency issues for a centralized entity implementation. <p> Moreover, the number of files being accessed on the web by individuals in a given time period is much larger than in traditional environments. Much work has been done on automatically identifying the most vital files that may be needed and then caching them on the mobile computer <ref> [Kue94] </ref>. When the space on the palmtop is small, no automatic method can be 100 percent correct or even close.
Reference: [Kue95] <author> Geoffrey H. Kuenning. "Kitrace: </author> <title> Precise Interactive Measurement of Operating Systems Kernels." </title> <journal> Software|Practice and Experience, </journal> <volume> 25(1) </volume> <pages> 1-22, </pages> <month> January </month> <year> 1995. </year>
Reference-contexts: However, this does not explain the overall 185 percent overhead of grep and 11 percent overhead of cp although they perform similar kinds of operations. We again performed micro benchmarks with the kitrace <ref> [Kue95] </ref> kernel measurement tool. We found that the cost of getting remote attributes is 8.5 ms, cost of running grep on a single file is 17.4 ms and the cost of performing a cp from a remotely mounted file system to a remote replica in Ficus is 173.5 ms.
Reference: [Lam78] <author> Leslie Lamport. </author> <title> "Time, Clocks and the Ordering of Events in a Distributed System." </title> <journal> Communications of the ACM, </journal> <volume> 21(7) </volume> <pages> 558-565, </pages> <month> July </month> <year> 1978. </year>
Reference-contexts: This work borrows some of its ideas from previous work at ISIS [BJ87]. Since Ladin develops a more efficient solution for replicated systems, we will only discuss their work. Causal ordering is similar to Lamport's happened-before relationship <ref> [Lam78] </ref>. An operation A is causally related to operation B if the execution of B affects A, or if A's input can depend on B's output. The forced ordering ensures that a "forced" operation is performed in the same order at all the replicas with respect to other forced operations.
Reference: [LLS90] <author> Rivka Ladin, Barbara Liskov, and Liuba Shrira. </author> <title> "Lazy Replication: Exploiting the Semantics of Distributed Services." </title> <booktitle> In Proceedings of the Workshop on Management of Replicated Data, </booktitle> <pages> pp. 31-34. </pages> <publisher> IEEE, </publisher> <month> November </month> <year> 1990. </year>
Reference-contexts: Later, Heddaya, et al., [HHW89] propose a lower overhead solution, although garbage collection is more conservative. The acknowledgment algorithm presented in Section 6.5 borrows several ideas from Wuu. Consistency algorithms that use data semantics provide high data availability. Garcia [GAB83], Sarin [Sar86] and Ladin <ref> [LLS90] </ref> generalize the directory solution for other data types. Garcia uses a data-patch tool that automates the data integration step. The correct final state of a database and corrective measures to 87 reach it are predefined for various potential non-serializable operations. These are used to reach eventual consistency. <p> This solution is application specific and does not address issues related to file consistency in a replicated mobile environment. 89 8.3 Causal Consistency Ladin, et al., <ref> [LLS90] </ref> support causal ordering of data reads, and causal, forced and immediate ordering for updates. This work borrows some of its ideas from previous work at ISIS [BJ87]. Since Ladin develops a more efficient solution for replicated systems, we will only discuss their work.
Reference: [Ous90] <author> John K. Ousterhout. </author> <title> "Why Aren't Operating Systems Getting Faster As Fast as Hardware?" In USENIX Conference Proceedings, </title> <journal> pp. </journal> <pages> 247-256. </pages> <publisher> USENIX, </publisher> <month> June </month> <year> 1990. </year>
Reference-contexts: SCSI disks are used for persistent storage, and each machine has 12 MB of main memory. The tests were performed during periods of minimal external network activity. There are seven benchmarks that are used in the evaluation. The first test is the modified Andrew Benchmark (mab) <ref> [HKM88, Ous90] </ref> that is intended to model a normal mix of filing operations, and hence be representative of performance in actual use. The second and third tests are local and remote recursive cp and the fourth test is grep. Each of these tests exercise the read and write file operations.
Reference: [Pap79] <author> C. H. Papadimitriou. </author> <title> "The Serializability of Concurrent Database Updates." </title> <journal> Journal of the ACM, </journal> <volume> 26(4), </volume> <month> October </month> <year> 1979. </year>
Reference-contexts: Section 1.2 below describes eventual data consistency where each access may not provide consistent data but data eventually converges to a consistent state. Strong Consistency Traditionally, one-copy serializability has been used as a consistency criterion for replicated data <ref> [Pap79, BG81] </ref>. One-copy serializability ensures that the execution of a program does not affect or is not affected by the execution of other programs. In other words, each program (transaction) is isolated from the effects of other programs.
Reference: [PWC81] <author> Gerald Popek, Bruce Walker, Johanna Chow, David Edwards, Charles Kline, Gerald Rudisin, and Greg Thiel. </author> <title> "LOCUS: A Network Transparent, High Reliability Distributed System." </title> <booktitle> In Proceedings of the Eighth Symposium on Operating Systems Principles, </booktitle> <pages> pp. 169-177. </pages> <publisher> ACM, </publisher> <month> December </month> <year> 1981. </year> <month> 103 </month>
Reference-contexts: For several applications such as banking applications, reservation systems, and personal applications like appointment calendars, design documents, meeting notes, and in general, mobile file accesses, continued access is critical even during network partitions <ref> [PWC81, FM82, Sat89, GHM90, TTP95] </ref>. In mobile environments, network partitions are a normal mode of operation, rather than a failure condition that must, or can be immediately corrected [HPG92, KS92]. <p> The lazy replication work by Ladin, et al., provides eventual consistency for updates and also supports causal ordering for reads. Utilizing data semantics for eventual consistency is further explored by several systems such as Locus <ref> [PWC81] </ref>, its successor Ficus [GHM90] on which we have implemented view consistency, Coda [Sat89] and Bayou [TTP95]. The data integration procedure is further developed in these systems and a distinction is made between conflict detection and conflict resolution. Locus, Coda and Ficus use version vectors to detect write-write conflicts.
Reference: [Rat95] <author> David H. Ratner. </author> <title> "Selective Replication: Fine-Grain Control of Repli--cated Files." </title> <type> Technical Report CSD-950007, </type> <institution> University of California, </institution> <address> Los Angeles, </address> <month> March </month> <year> 1995. </year> <type> Master's Thesis. </type>
Reference-contexts: A complication occurs when files are selectively replicated. Replicas of a selectively replicated file are stored at some subset of sites at which the volume is replicated. Modifications to the replica selection process for selective replication are described in <ref> [Rat95] </ref> and are not discussed here any further. Default replica selection guarantees that file operations will keep accessing 29 the same file replica until the replica becomes unavailable. In that case, the replica is switched to some other available replica.
Reference: [RHR94] <author> Peter Reiher, John S. Heidemann, David Ratner, Gregory Skinner, and Gerald J. Popek. </author> <title> "Resolving File Conflicts in the Ficus File System." </title> <booktitle> In USENIX Conference Proceedings, </booktitle> <pages> pp. 183-195. </pages> <publisher> USENIX, </publisher> <month> June </month> <year> 1994. </year>
Reference-contexts: However, eventual consistency schemes raise a different problem; they do not provide any consistency guarantees during accesses. It has been observed that this does not necessarily cause serious problems for shared files <ref> [KS92, RHR94] </ref>. Often, files that are heavily shared are not frequently updated by multiple users. Since the level of write-sharing is low, few consistency conflicts are generated. Unfortunately, this argument ignores read-dependency issues for shared files. Moreover, it does not take non-shared files into account. <p> Moreover, if they do update the file, a conflict will necessarily happen. Worst of all, the user gets no hint that these problems might arise. Eventual consistency ignores write-dependencies also. Thus writes can be made to older copies of data. This is uncommon for many shared files <ref> [KS92, RHR94] </ref>. However, often a single user may update multiple copies of data and thereby cause conflicts. This is different from the old read problem because the 6 writes may be done without reading the data. Suppose a user is updating a replicated file on replica A. <p> If conflicting updates have been made to a file, the conflict is reliably detected during reconciliation and Ficus automatic conflict resolution is attempted. When two file replicas conflict, the file name and its type are used to search for an applicable conflict resolver <ref> [RHR94] </ref> that attempts to merge the replicas into one version and distribute the new version. If a resolver is not found or the merge is unsuccessful, the user must manually merge file versions. The view consistency implementation does not directly modify the reconciliation and resolution operations in Ficus. <p> These rules ensure that the same replica is accessed for all files within a volume, and this replica is the fastest available replica. This simple replica selection policy performs well for many applications; the number of conflicting updates that are generated is extremely low as reported in <ref> [RHR94] </ref>. A complication occurs when files are selectively replicated. Replicas of a selectively replicated file are stored at some subset of sites at which the volume is replicated. Modifications to the replica selection process for selective replication are described in [Rat95] and are not discussed here any further. <p> This assessment raises several problems. Although stale data writes (conflicting updates) are detected by using version vectors in Ficus, there is no easy mechanism for detecting reads to stale data versions. Moreover, very few conflicting updates are generated in current Ficus <ref> [RHR94] </ref>. This is partly because most 74 machines that run Ficus are in a well connected LAN environment. This makes it very difficult to measure the reduction in the number of stale reads and writes, and also the reduction in availability.
Reference: [Sar86] <author> Sunil K. Sarin. </author> <title> "Robust Application Design in Highly Available Distributed Databases." </title> <booktitle> In Proceedings of the Fifth IEEE Symposium on Reliability in Distributed Software and Database Systems, </booktitle> <pages> pp. 87-94, </pages> <month> January </month> <year> 1986. </year>
Reference-contexts: Later, Heddaya, et al., [HHW89] propose a lower overhead solution, although garbage collection is more conservative. The acknowledgment algorithm presented in Section 6.5 borrows several ideas from Wuu. Consistency algorithms that use data semantics provide high data availability. Garcia [GAB83], Sarin <ref> [Sar86] </ref> and Ladin [LLS90] generalize the directory solution for other data types. Garcia uses a data-patch tool that automates the data integration step. The correct final state of a database and corrective measures to 87 reach it are predefined for various potential non-serializable operations.
Reference: [Sat89] <author> Mahadev Satyanarayanan. "Coda: </author> <title> A Highly Available File System for a Distributed Workstation Environment." </title> <booktitle> In Proceedings of the Second Workshop on Workstation Operating Systems. </booktitle> <publisher> IEEE Computer Society Press, </publisher> <month> September </month> <year> 1989. </year>
Reference-contexts: For several applications such as banking applications, reservation systems, and personal applications like appointment calendars, design documents, meeting notes, and in general, mobile file accesses, continued access is critical even during network partitions <ref> [PWC81, FM82, Sat89, GHM90, TTP95] </ref>. In mobile environments, network partitions are a normal mode of operation, rather than a failure condition that must, or can be immediately corrected [HPG92, KS92]. <p> For other data types, some 4 minimal inconsistency, or external intervention to restore the semantics of the data is often considered more acceptable than entirely inhibiting data accesses when communication is not possible <ref> [Sat89, GHM90, TTP95] </ref>. Consider a calendar that is shared by several users and replicated on each user's portable. A user can schedule meetings in his personal replica of the calendar, even when other replicas are not available. <p> The lazy replication work by Ladin, et al., provides eventual consistency for updates and also supports causal ordering for reads. Utilizing data semantics for eventual consistency is further explored by several systems such as Locus [PWC81], its successor Ficus [GHM90] on which we have implemented view consistency, Coda <ref> [Sat89] </ref> and Bayou [TTP95]. The data integration procedure is further developed in these systems and a distinction is made between conflict detection and conflict resolution. Locus, Coda and Ficus use version vectors to detect write-write conflicts. Bayou uses an application dependent mechanism for detecting such conflicts.
Reference: [Sel91] <author> Margo Seltzer. </author> <title> "A New Hashing Package for UNIX." </title> <booktitle> In USENIX Conference Proceedings. USENIX, </booktitle> <month> January </month> <year> 1991. </year>
Reference-contexts: The truncation of this information is thus described separately in Chapter 6. 4.1.1 View-Entry Storage Service The view-entry storage requirements are large storage, fast read and write access and persistence. A good database satisfies all these requirements. We chose the db package built at UC Berkeley by Margo Seltzer <ref> [Sel91] </ref>. The db package is relatively small, provides compact persistent storage and caches large chunks of the database in memory for efficient access. The view consistency implementation has been done within the Ficus kernel. There were two choices regarding the placement of the database package.
Reference: [TDP94] <author> D.B. Terry, A.J. Demers, K. Petersen, M.J. Spreitzer, M.M. Theimer, and B.B. Welch. </author> <title> "Session Guarantees for Weakly Consistent Replicated Data." </title> <booktitle> In Proceedings of the Third International Conference on Parallel and Dsitributed Information Systems, </booktitle> <pages> pp. 140-149, </pages> <month> sep </month> <year> 1994. </year>
Reference-contexts: This provides the highest possible data availability. View consistency has been implemented on Ficus to reduce client inconsistencies. Similarly, Bayou provides session guarantees <ref> [TDP94] </ref> using the same version vector approach as ours. Session guarantees provide view consistency for processes or process groups. Bayou does not provide view consistency for other transient entities, or for persistent entities.
Reference: [Tho79] <author> Robert H. Thomas. </author> <title> "A Majority Consensus Approach to Concurrency Control for Multiple Copy Databases." </title> <journal> ACM Transactions on Database Systems, </journal> <volume> 4(2) </volume> <pages> 180-209, </pages> <month> June </month> <year> 1979. </year>
Reference-contexts: Both restricting availability and adding communication overhead contradict the goals of providing higher data availability and increased access speeds in distributed systems. Replication is used to overcome some of these problems. Various algorithms have been proposed for maintaining consistency among the replicated copies <ref> [Tho79, Gif79] </ref>. However, availability is still limited to a single partition when strong consistency is used for replica control. 8.1 Typed Consistency Several researchers realized that accesses do not have to be serialized for maintaining data consistency for certain data types [FM82, All83, DS83, WB84].
Reference: [TTP95] <author> Douglas B. Terry, Marvin M. Theimer, Karin Petersen, Alan J. De-mers, Mike J. Spreitzer, and Carl H. Hauser. </author> <title> "Managing Update Conflicts in Bayou, a Weakly Connected Replicated Storage System." </title> <booktitle> In Proceedings of the Fifteenth Symposium on Operating Systems Principles, </booktitle> <pages> pp. 172-183. </pages> <publisher> ACM, </publisher> <month> December </month> <year> 1995. </year>
Reference-contexts: For several applications such as banking applications, reservation systems, and personal applications like appointment calendars, design documents, meeting notes, and in general, mobile file accesses, continued access is critical even during network partitions <ref> [PWC81, FM82, Sat89, GHM90, TTP95] </ref>. In mobile environments, network partitions are a normal mode of operation, rather than a failure condition that must, or can be immediately corrected [HPG92, KS92]. <p> For other data types, some 4 minimal inconsistency, or external intervention to restore the semantics of the data is often considered more acceptable than entirely inhibiting data accesses when communication is not possible <ref> [Sat89, GHM90, TTP95] </ref>. Consider a calendar that is shared by several users and replicated on each user's portable. A user can schedule meetings in his personal replica of the calendar, even when other replicas are not available. <p> Utilizing data semantics for eventual consistency is further explored by several systems such as Locus [PWC81], its successor Ficus [GHM90] on which we have implemented view consistency, Coda [Sat89] and Bayou <ref> [TTP95] </ref>. The data integration procedure is further developed in these systems and a distinction is made between conflict detection and conflict resolution. Locus, Coda and Ficus use version vectors to detect write-write conflicts. Bayou uses an application dependent mechanism for detecting such conflicts.
Reference: [WB84] <author> Gene T. J. Wuu and Arthur J. Bernstein. </author> <title> "Efficient Solutions to the Replicated Log and Dictionary Problems." </title> <booktitle> In Proceedings of the Third Annual ACM Symposium on Principles of Distributed Computing, </booktitle> <month> August </month> <year> 1984. </year>
Reference-contexts: The resolution is done either automatically or manually depending on the semantics of the data type. The integration and resolution of data at other sites ensures that replicas eventually reach identical states or become mutually consistent if no new updates are generated in the system <ref> [WB84, HHW89] </ref>. This consistency approach, which guarantees eventual consistency of data, is called eventual data consistency. Eventual data consistency provides high availability by allowing updates to be made in any order. <p> However, availability is still limited to a single partition when strong consistency is used for replica control. 8.1 Typed Consistency Several researchers realized that accesses do not have to be serialized for maintaining data consistency for certain data types <ref> [FM82, All83, DS83, WB84] </ref>. The semantics of the data type can be used to define an alternative consistency cri 86 terion. Much of this research was aimed at providing consistency for the directory data structure that allows the insert, delete and list operations. <p> The data propagation algorithm uses distributed logs. These logs must be garbage collected, so that the log sizes do not grow indefinitely, and their transfer between replicas does not become a bottleneck. The replicated log solution by Wuu <ref> [WB84] </ref> addresses this problem. Later, Heddaya, et al., [HHW89] propose a lower overhead solution, although garbage collection is more conservative. The acknowledgment algorithm presented in Section 6.5 borrows several ideas from Wuu. Consistency algorithms that use data semantics provide high data availability.
Reference: [ZD91] <author> Erez Zadok and Dan Duchamp. </author> <title> "Discovery and Hot Replacement of Replicated Read-Only File Systems, with Application to Mobile Computing." </title> <booktitle> In USENIX Conference Proceedings, </booktitle> <pages> pp. 69-85, </pages> <address> Cincinatti, OH, </address> <month> June </month> <year> 1991. </year> <booktitle> USENIX. </booktitle> <pages> 104 </pages>
Reference-contexts: Systems have been built with either one or the other goal in mind. For example, wide-area file-system solutions provide consistent replicated (or cached) data, but do not aggressively search for the best available replica. The replica switching work by Zadok and Duchamp <ref> [ZD91] </ref> addresses the problem of providing data from the fastest available replica. They improve the amd daemon [CS93] in Unix systems (which demand mounts and unmounts file systems) and allow transparent switching of open files to replacement file systems that are dynamically discovered.
References-found: 40

