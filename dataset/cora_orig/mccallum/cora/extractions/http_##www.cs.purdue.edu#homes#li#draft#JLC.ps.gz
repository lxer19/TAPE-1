URL: http://www.cs.purdue.edu/homes/li/draft/JLC.ps.gz
Refering-URL: http://www.cs.purdue.edu/homes/li/publications.html
Root-URL: http://www.cs.purdue.edu
Title: An Efficient Solution to the Cache Thrashing Problem Caused by True Data Sharing  
Author: Guohua Jin Zhiyuan Li Fujie Chen 
Keyword: Multiprocessors, cache thrashing, true data sharing, parallel threads, loop transformations, parallelizing compilers.  
Note: Work supported by a grant from the National Natural Science Foundation, China.  Work supported by a National Science Foundation CAREER Award, grant number CDA 9414015.  Work sup ported by a grant from the National Natural Science Foundation, China.  
Address: 6100 Main Street, Houston, TX 77005-1892, USA, or Changsha  Changsha, Hunan 410073, CHINA.  Street, West Lafayette, IN 47907, USA.  Changsha, Hunan 410073, CHINA.  
Affiliation: Rice University, Department of Computer Science,  Institute of Technology, Department of Computer Science,  Purdue University, Department of Computer Science, 1398 University  Changsha Institute of Technology, Department of Computer Science,  
Date: September 5, 1997  
Abstract: When parallel programs are executed on multiprocessors with private caches, a set of data may be repeatedly used and modified by different threads. Such data sharing can often result in cache thrashing, which degrades memory performance. This paper presents and evaluates a loop restructuring method to reduce or even eliminate cache thrashing caused by true data sharing in nested parallel loops. This method uses a compiler analysis which applies linear algebra and the theory of numbers to the subscript expressions of array references. Due to this method's simplicity, it can be efficiently implemented in any parallel compiler. Experimental results show quite significant performance improvements over existing static and dynamic scheduling methods. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> S. Abraham and D. Hudak. </author> <title> Compile-time partitioning of iterative parallel loops to reduce cache coherence traffic. </title> <journal> IEEE Transactions on Parallel Distributed Systems, </journal> <volume> 2(3), </volume> <month> July </month> <year> 1991. </year>
Reference-contexts: Hudak and Abraham <ref> [1, 18] </ref> develop a static partitioning approach called adaptive data partitioning (ADP) to reduce interprocessor communication for iterative data-parallel loops. They also assume perfectly nested loops. The loop body is restricted to update a single data point A (i; j) within a two-dimensional global matrix A.
Reference: [2] <author> W. Abu-Sufah, D. Kuck, and D. Lawrie. </author> <title> On the performance enhancement of paging systems through program analysis and transformations. </title> <journal> IEEE Transactions on Computers, </journal> <volume> C-30(5), </volume> <month> May </month> <year> 1981. </year> <title> (a) n=100 (b) n=1000 </title>
Reference-contexts: Abu-Sufah, Kuck and Lawrie use loop blocking to improve paging performance by improving the locality of references <ref> [2] </ref>. Wolfe proposes iteration space tiling as a way to improve data reuse in a cache or a local memory [35]. Gallivan, Jalby and Gannon define a reference window for a dependence as the variables referenced by both the source and the sink of the dependence [15, 16].
Reference: [3] <author> A. Agarwal, D. Kranz, and V. Natarajan. </author> <title> Automatic partitioning of parallel loops and data arrays for distributed shared-memory multiprocessors. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, Vol.6, No.9, </journal> <volume> pp.943-962, </volume> <month> September </month> <year> 1995. </year>
Reference-contexts: Tomko and Abraham 4 [32] develop iteration partitioning techniques for data-parallel application programs. They assume that there is only one pair of data access functions and that each loop index variable can appear in only one dimension of each array subscript expression. Agarwal, Kranz, and Natarajan <ref> [3] </ref> propose a framework for automatically partitioning parallel loops to minimize cache coherence traffic on shared-memory multiprocessors. They restrict their discussion to perfectly nested doall loops, and assume rectangular iteration spaces. Unlike these previous works, our work considers nested loops which are not necessarily perfectly nested.
Reference: [4] <author> J. R. Allen and K. Kennedy. </author> <title> Automatic loop interchange. </title> <booktitle> In Proceedings of the SIGPLAN'84 Symposium on Compiler Construction, </booktitle> <address> Montreal, Canada, </address> <month> June </month> <year> 1984. </year>
Reference-contexts: We have performed experiments on a commercial multiprocessor, namely a Silicon Graphics Challenge Cluster, thereby obtaining real data regarding cache thrashing and its reduction. In contrast, previous data were mainly from simulations [11, 12, 26]. 3 Basic Concepts and Assumptions Data dependences between statements are defined in <ref> [6, 24, 4, 25] </ref>. If a statement S 1 uses the result of another statement S 2 , then S 1 is flow-dependent on S 2 .
Reference: [5] <author> J. Baer and W.Wang. </author> <title> Multilevel cache hierarchies: organizations, protocols, and performance. </title> <journal> Journal of Parallel and Distributed Computing, Vol.6, </journal> <volume> pp.451-476, </volume> <year> 1989. </year>
Reference-contexts: 1 Introduction Parallel processing systems with memory hierarchies have become quite common today. Commonly, most multiprocessor systems have a local cache in each processor to bridge the speed gap between the processor and the main memory. Some systems use multi-level caches <ref> [5, 14] </ref>. Very often, a copy-back snoopy cache protocol is employed to maintain cache coherence in these multiprocessor systems. Certain supercomputers also use a local memory which can be viewed as a program-controlled cache.
Reference: [6] <author> U. Banerjee. </author> <title> Dependence analysis for supercomputing. </title> <publisher> Kluwer Academic Publishers, </publisher> <year> 1988. </year>
Reference-contexts: We have performed experiments on a commercial multiprocessor, namely a Silicon Graphics Challenge Cluster, thereby obtaining real data regarding cache thrashing and its reduction. In contrast, previous data were mainly from simulations [11, 12, 26]. 3 Basic Concepts and Assumptions Data dependences between statements are defined in <ref> [6, 24, 4, 25] </ref>. If a statement S 1 uses the result of another statement S 2 , then S 1 is flow-dependent on S 2 .
Reference: [7] <author> D. Callahan, S. Carr, and K. Kennedy. </author> <title> Improving register allocation for subscripted variables. </title> <booktitle> In Proceedings of the ACM SIGPLAN'90 Conference on Programming Languages Design and Implementation, </booktitle> <month> June </month> <year> 1990. </year>
Reference-contexts: After executing the source of the dependence, they save the associated reference window in the cache until the sink has also 3 been executed, which may increase the number of cache hits. Carr, Callahan and Kennedy <ref> [7, 8] </ref> discuss options for compiler control of a uniprocessor's memory hierarchy. Wolf and Lam develop an algorithm that estimates all temporal and spatial reuse of a given loop permutation [34]. These optimizations all attempt to maximize the reuse of cached data on a single processor.
Reference: [8] <author> S. Carr and K. Kennedy. </author> <title> Compiling scientific code for complex memory hierarchies. </title> <booktitle> In Proceedings of Hawaii International Conference on System Sciences, </booktitle> <address> pp.536-544, </address> <year> 1991. </year>
Reference-contexts: After executing the source of the dependence, they save the associated reference window in the cache until the sink has also 3 been executed, which may increase the number of cache hits. Carr, Callahan and Kennedy <ref> [7, 8] </ref> discuss options for compiler control of a uniprocessor's memory hierarchy. Wolf and Lam develop an algorithm that estimates all temporal and spatial reuse of a given loop permutation [34]. These optimizations all attempt to maximize the reuse of cached data on a single processor.
Reference: [9] <author> E. D'Hollander. </author> <title> Partitioning and labeling of loops by unimodular transformations. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 3(4), </volume> <month> July </month> <year> 1992. </year>
Reference-contexts: In contrast, our work considers a multiprocessor environment where each processor has its own local cache or its own local memory, and where different processors may share data. The work by Peir and Cytron [28], Shang and Fortes [30], and by D'Hollander <ref> [9] </ref> share the common goal of partitioning an index set into independent execution subsets such that the corresponding loop iterations can execute on different processors without interprocessor communication.
Reference: [10] <author> S. J. Eggers and T. E. Jeremiassen. </author> <title> Eliminating False Sharing. </title> <booktitle> In Proceedings of 1991 International Conference on Parallel Processing, </booktitle> <month> Aug. </month> <year> 1991. </year>
Reference-contexts: Unlike these previous works, our work considers nested loops which are not necessarily perfectly nested. Loop bounds can be any variables, and array subscript expressions are much more general. Many researchers have studied the cache false sharing problem <ref> [10, 17, 19, 33] </ref> in which cache thrashing occurs when different processors share the same cache line of multiple words, although the processors do not share the same word. Many algorithms have been proposed to reduce false sharing by better memory allocation, better thread scheduling, or by program transformations.
Reference: [11] <author> J. Fang and M. Lu. </author> <title> A solution of cache ping-pong problem in RISC based parallel processing systems. </title> <booktitle> In Proceedings of 1991 International Conference on Parallel Processing, </booktitle> <month> Aug. </month> <year> 1991. </year>
Reference-contexts: J. Fang and M. Lu studied a large number of programs including the LINPACK benchmarks, the PERFECT Club benchmarks, and programs for mechanical CAE, computational chemistry, image and signal processing, and petroleum applications <ref> [11] </ref>. They reported that almost all of the most time-consuming loop nests contain at least three loop levels, out of which 60% contain at least one parallel loop. <p> Many algorithms have been proposed to reduce false sharing by better memory allocation, better thread scheduling, or by program transformations. Our work considers cache thrashing which is due to the true sharing of data words. Our work is most closely related to the research done by Fang and Lu <ref> [11, 12, 26, 13] </ref>. In their work, the iteration space is partitioned into a set of equivalence classes, and each processor uses a formula to determine which iterations belong to the same equivalence class at execution time. <p> We have performed experiments on a commercial multiprocessor, namely a Silicon Graphics Challenge Cluster, thereby obtaining real data regarding cache thrashing and its reduction. In contrast, previous data were mainly from simulations <ref> [11, 12, 26] </ref>. 3 Basic Concepts and Assumptions Data dependences between statements are defined in [6, 24, 4, 25]. If a statement S 1 uses the result of another statement S 2 , then S 1 is flow-dependent on S 2 . <p> Fang and Lu <ref> [11] </ref> reported that arrays involved in nested loops are usually two-dimensional or three-dimensional with a small-sized third dimension. The latter can be treated as a small number of two-dimensional arrays. Nested loops with the parallel loop at the innermost level are degenerate cases of the loop nest in Figure 3.
Reference: [12] <author> Z. Fang. </author> <title> Cache or local memory thrashing and compiler strategy in parallel processing systems. </title> <booktitle> In Proceedings of 1990 International Conference on Parallel Processing, </booktitle> <address> pp.271-275, </address> <month> Aug. </month> <year> 1990. </year>
Reference-contexts: If the threads in the same list are assigned to different processors, the data of array A will unnecessarily move back and forth between different caches in the system, causing a cache thrashing problem due to true data sharing <ref> [12] </ref>. The nested loop construct shown in the above example is quite common in parallel code used for scientific computation. J. Fang and M. <p> Many algorithms have been proposed to reduce false sharing by better memory allocation, better thread scheduling, or by program transformations. Our work considers cache thrashing which is due to the true sharing of data words. Our work is most closely related to the research done by Fang and Lu <ref> [11, 12, 26, 13] </ref>. In their work, the iteration space is partitioned into a set of equivalence classes, and each processor uses a formula to determine which iterations belong to the same equivalence class at execution time. <p> We have performed experiments on a commercial multiprocessor, namely a Silicon Graphics Challenge Cluster, thereby obtaining real data regarding cache thrashing and its reduction. In contrast, previous data were mainly from simulations <ref> [11, 12, 26] </ref>. 3 Basic Concepts and Assumptions Data dependences between statements are defined in [6, 24, 4, 25]. If a statement S 1 uses the result of another statement S 2 , then S 1 is flow-dependent on S 2 .
Reference: [13] <author> J. Fang and M. Lu. </author> <title> An iteration partition approach for cache or local memory thrashing on parallel processing. </title> <journal> IEEE Transactions on Computers, </journal> <volume> C-42, </volume> <month> May </month> <year> 1993. </year>
Reference-contexts: Many algorithms have been proposed to reduce false sharing by better memory allocation, better thread scheduling, or by program transformations. Our work considers cache thrashing which is due to the true sharing of data words. Our work is most closely related to the research done by Fang and Lu <ref> [11, 12, 26, 13] </ref>. In their work, the iteration space is partitioned into a set of equivalence classes, and each processor uses a formula to determine which iterations belong to the same equivalence class at execution time.
Reference: [14] <author> M. Galles and E. Williams. </author> <title> Performance optimizations, implementation, and verification of the SGI Challenge multiprocessor. </title> <booktitle> In Proceedings of the 27th Annual Hawaii International Conference on System Sciences, </booktitle> <year> 1994. </year> <month> 27 </month>
Reference-contexts: 1 Introduction Parallel processing systems with memory hierarchies have become quite common today. Commonly, most multiprocessor systems have a local cache in each processor to bridge the speed gap between the processor and the main memory. Some systems use multi-level caches <ref> [5, 14] </ref>. Very often, a copy-back snoopy cache protocol is employed to maintain cache coherence in these multiprocessor systems. Certain supercomputers also use a local memory which can be viewed as a program-controlled cache.
Reference: [15] <author> K. Gallivan, W. Jalby, and D. Gannon. </author> <title> On the problem of optimizing data transfers for complex memory systems. </title> <booktitle> In Proceedings of Supercomputing'88, </booktitle> <address> pp.238-253, </address> <year> 1988. </year>
Reference-contexts: Wolfe proposes iteration space tiling as a way to improve data reuse in a cache or a local memory [35]. Gallivan, Jalby and Gannon define a reference window for a dependence as the variables referenced by both the source and the sink of the dependence <ref> [15, 16] </ref>. After executing the source of the dependence, they save the associated reference window in the cache until the sink has also 3 been executed, which may increase the number of cache hits. Carr, Callahan and Kennedy [7, 8] discuss options for compiler control of a uniprocessor's memory hierarchy.
Reference: [16] <author> D. Gannon, W. Jalby, and K. Gallivan. </author> <title> Strategies for cache and local memory management by global program transformation. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> Vol.5, </volume> <year> 1988. </year>
Reference-contexts: Wolfe proposes iteration space tiling as a way to improve data reuse in a cache or a local memory [35]. Gallivan, Jalby and Gannon define a reference window for a dependence as the variables referenced by both the source and the sink of the dependence <ref> [15, 16] </ref>. After executing the source of the dependence, they save the associated reference window in the cache until the sink has also 3 been executed, which may increase the number of cache hits. Carr, Callahan and Kennedy [7, 8] discuss options for compiler control of a uniprocessor's memory hierarchy.
Reference: [17] <author> M. Gupta and D. Padua. </author> <title> Effects of program parallelization and stripmining transformation on cache performance in a multiprocessor. </title> <booktitle> In Proceedings of the 1991 International Conference on Parallel Processing, </booktitle> <month> Aug. </month> <year> 1991. </year>
Reference-contexts: Unlike these previous works, our work considers nested loops which are not necessarily perfectly nested. Loop bounds can be any variables, and array subscript expressions are much more general. Many researchers have studied the cache false sharing problem <ref> [10, 17, 19, 33] </ref> in which cache thrashing occurs when different processors share the same cache line of multiple words, although the processors do not share the same word. Many algorithms have been proposed to reduce false sharing by better memory allocation, better thread scheduling, or by program transformations.
Reference: [18] <author> D. Hudak and S. Abraham. </author> <title> Compiler techniques for data partitioning of sequentially iterated parallel loops. </title> <booktitle> In Proceedings of ACM International Conference on Supercomputing, </booktitle> <pages> pp. 187-200, </pages> <year> 1990. </year>
Reference-contexts: Hudak and Abraham <ref> [1, 18] </ref> develop a static partitioning approach called adaptive data partitioning (ADP) to reduce interprocessor communication for iterative data-parallel loops. They also assume perfectly nested loops. The loop body is restricted to update a single data point A (i; j) within a two-dimensional global matrix A.
Reference: [19] <author> T. E. Jeremiassen and S. J. Eggers. </author> <title> Reducing false sharing on shared memory multiprocessors through compile-time data transformations. </title> <booktitle> In Proceedings of Fifth ACM SIGPLAN Symposium on Principals and Practice of Paralle Programming, </booktitle> <pages> pp. 179-188, </pages> <year> 1995. </year>
Reference-contexts: Unlike these previous works, our work considers nested loops which are not necessarily perfectly nested. Loop bounds can be any variables, and array subscript expressions are much more general. Many researchers have studied the cache false sharing problem <ref> [10, 17, 19, 33] </ref> in which cache thrashing occurs when different processors share the same cache line of multiple words, although the processors do not share the same word. Many algorithms have been proposed to reduce false sharing by better memory allocation, better thread scheduling, or by program transformations.
Reference: [20] <author> G. Jin and F. Chen. </author> <title> The design and the implementation of a knowledge-based parallelizing tool. </title> <booktitle> In Proceedings of the 2nd IES Information Technology Conference, </booktitle> <month> July </month> <year> 1991, </year> <institution> Singapore. </institution>
Reference-contexts: (3,1) (0,0) (3,1) (I; J; K) A (J + K; I + J ); A (J + K + c; I + J ) (d; d), d 2 I (1,-1) (1,-1) 5 Experimental Results The thread alignment techniques described in this paper have been implemented as backend optimizations in KD-PARPRO <ref> [20] </ref>, a knowledge-based parallelizing tool which can perform intra- and inter-procedural data dependence analysis and a large number of parallelizing transformations, including loop interchange, loop distribution, loop skewing, and strip mining for FORTRAN programs.
Reference: [21] <author> G. Jin, Z. Li, and F. Chen. </author> <title> An efficient solution to the cache thrashing problem (Extended Version). </title> <type> Technical Report TR 96-020, </type> <institution> Department of Computer Science, University of Minnesota, </institution> <year> 1996. </year>
Reference-contexts: When they are not true, the parallel loops will be too small to be important. With these assumptions, we have the following theorem. Theorem 2 <ref> [21] </ref>: Let b fl;1 a fl;2 a fl;1 b fl;2 = 0. <p> Lemma 3 <ref> [21] </ref>: If the condition L k;1 L j;1 L j;2 (1 j; k m) in Definition 5 is true, then (a) the iterations (i; j) and (i; k), where j 6= k, belong to two different equivalent classes; and (b) the iterations (i; j) and (i ; k), where 0 &lt; <p> Theorem 4 <ref> [21] </ref>: If the condition L k;1 = L j;2 (1 j; k m) in Definition 5 is true, then the reduced iteration space must be staggered according to the unified staggering parameter (g; L 1;2 order to reduce or eliminate data sharing among the threads, i.e. the (i + g)-th row <p> Lemma 4: Given iterations (i; j) and (i 0 0 ) in the reduced iteration space, and a unified staggering parameter (g; g 0 ), if there exists integer r such that i = i + rg 0 0 then S 0 0 Lemma 5 <ref> [21] </ref>: Given iterations (i 0 0 0 0 0 0 ) in the reduced iteration space, if there exist integers r 1 ; r 2 ; :::; r m , not all zero, such that k = j + fl=1 m X r fl L fl;1 = 0 then S 0 <p> 0 0 0 0 0 0 ) in the reduced iteration space, if there exist integers r 1 ; r 2 ; :::; r m , not all zero, such that k = j + fl=1 m X r fl L fl;1 = 0 then S 0 0 Lemma 6 <ref> [21] </ref>: Given iterations (i; j) and (i 0 0 ) in the reduced iteration space, if (i 0 0 0 then S 0 0 Theorem 5 [21]: Given staggering parameters (L 1;1 ; L 1;2 ); (L 2;1 ; L 2;2 ); :::; (L m;1 ; L m;2 ), we have <p> not all zero, such that k = j + fl=1 m X r fl L fl;1 = 0 then S 0 0 Lemma 6 <ref> [21] </ref>: Given iterations (i; j) and (i 0 0 ) in the reduced iteration space, if (i 0 0 0 then S 0 0 Theorem 5 [21]: Given staggering parameters (L 1;1 ; L 1;2 ); (L 2;1 ; L 2;2 ); :::; (L m;1 ; L m;2 ), we have S i;j = S i;j for any iteration (i; j) in the reduced iteration space. <p> Next, we establish that S 0 i;j is the result of staggering with (g; g 0 ) followed by compacting with d. This is stated by Corollary 1 below. Lemma 7 <ref> [21] </ref>: Given staggering parameters (L i 1 ;1 ; L i 1 ;2 ); (L i 2 ;1 ; L i 2 ;2 ); :::; (L i j ;1 ; L i j ;2 ), suppose that r 1 ; r 2 ; :::; r j are integers that satisfy (1) <p> L i fl ;1 = 0 (r 1 &gt; 0), then r 1 r 0 For any integers r 00 00 00 j satisfying j X r fl L i fl ;1 = 0 (r 1 &gt; 0) there exists an integer k 1 such that r 00 Theorem 6 <ref> [21] </ref>: If d is the compacting parameter determined by Algorithm 1, and d 0 m P r fl L fl;2 , where r 1 ; r 2 ; :::; r m are integers, not all zero, which satisfy m X r fl L fl;1 = 0 then there exists an integer <p> By the definition of S 0 0 To further simplify the process of the staggering and the compacting of the reduced iteration space, the following theorem can be used to replace multiple staggering parameters, which are in proportion, with a single staggering parameter. Theorem 8 <ref> [21] </ref>: Given staggering parameters (L 1;1 ; L 1;2 ); (L 2;1 ; L 2;2 ); :::; (L m;1 ; L m;2 ), where (L i 1 ;1 , L i 1 ;2 ), (L i 2 ;1 , L i 2 ;2 ), ..., (L i p ;1 , L <p> We show the code for g 0 6= 0 in Code Segment 2, listed below (the code for g =0 is similar <ref> [21] </ref>).
Reference: [22] <author> G. Jin and F. Chen. </author> <title> Loop restructuring techniques for the thrashing problem. </title> <booktitle> In Proceedings of the 1992 International Conference on Parallel Architectures and Languages Europe, </booktitle> <year> 1992. </year>
Reference-contexts: The thread assignment then becomes highly efficient at run time. Previously, we presented preliminary algorithms <ref> [22, 23] </ref> to deal with a simple case in which data-dependent array references use the same linear function in the subscripts. No experimental data were given. In this paper, we extend the work by covering multiple linear functions and by clarifying the underlying theory.
Reference: [23] <author> G. Jin, X. Yang, and F. Chen. </author> <title> Loop staggering, loop staggering and loop compacting: restructuring techniques for the thrashing problem. </title> <booktitle> In Proceedings of 1991 International Conference on Parallel Processing, </booktitle> <month> Aug. </month> <year> 1991. </year>
Reference-contexts: The thread assignment then becomes highly efficient at run time. Previously, we presented preliminary algorithms <ref> [22, 23] </ref> to deal with a simple case in which data-dependent array references use the same linear function in the subscripts. No experimental data were given. In this paper, we extend the work by covering multiple linear functions and by clarifying the underlying theory.
Reference: [24] <author> D. Kuck. </author> <title> The structure of computers and computations. Vol.1, </title> <publisher> John Wiley and Sons, </publisher> <address> New York, </address> <year> 1978. </year>
Reference-contexts: We have performed experiments on a commercial multiprocessor, namely a Silicon Graphics Challenge Cluster, thereby obtaining real data regarding cache thrashing and its reduction. In contrast, previous data were mainly from simulations [11, 12, 26]. 3 Basic Concepts and Assumptions Data dependences between statements are defined in <ref> [6, 24, 4, 25] </ref>. If a statement S 1 uses the result of another statement S 2 , then S 1 is flow-dependent on S 2 .
Reference: [25] <author> D. Kuck, R. Kuhn, D. Padua, B. Leasure, and M. Wolfe. </author> <title> Dependence graphs and compiler optimizations. </title> <booktitle> In Proceedings of the 8th ACM Symposium on Principle of Programming Languages (POPL), </booktitle> <year> 1981. </year>
Reference-contexts: We have performed experiments on a commercial multiprocessor, namely a Silicon Graphics Challenge Cluster, thereby obtaining real data regarding cache thrashing and its reduction. In contrast, previous data were mainly from simulations [11, 12, 26]. 3 Basic Concepts and Assumptions Data dependences between statements are defined in <ref> [6, 24, 4, 25] </ref>. If a statement S 1 uses the result of another statement S 2 , then S 1 is flow-dependent on S 2 .
Reference: [26] <author> M. Lu and J. Fang. </author> <title> A solution of the cache ping-pong problem in multiprocessor systems. </title> <journal> Journal of Parallel and Distributed Computing 16, </journal> <month> Oct. </month> <year> 1992. </year>
Reference-contexts: Many algorithms have been proposed to reduce false sharing by better memory allocation, better thread scheduling, or by program transformations. Our work considers cache thrashing which is due to the true sharing of data words. Our work is most closely related to the research done by Fang and Lu <ref> [11, 12, 26, 13] </ref>. In their work, the iteration space is partitioned into a set of equivalence classes, and each processor uses a formula to determine which iterations belong to the same equivalence class at execution time. <p> We have performed experiments on a commercial multiprocessor, namely a Silicon Graphics Challenge Cluster, thereby obtaining real data regarding cache thrashing and its reduction. In contrast, previous data were mainly from simulations <ref> [11, 12, 26] </ref>. 3 Basic Concepts and Assumptions Data dependences between statements are defined in [6, 24, 4, 25]. If a statement S 1 uses the result of another statement S 2 , then S 1 is flow-dependent on S 2 .
Reference: [27] <author> I. Nivan, et al. </author> <title> An introduction to the theory of numbers. </title> <publisher> John Wiley & Sons, </publisher> <address> Fourth Edition New York, Chichester Brisbane Toronto, </address> <year> 1980. </year>
Reference-contexts: According to the theory of numbers <ref> [27] </ref>, there exist integers a 1 , a 2 , ..., a m that satisfy g = fl=1 Let g = fl=1 a fl L fl;2 . We call (g; g 0 ) a unified staggering parameter.
Reference: [28] <author> J. Peir and R. Cytron. </author> <title> Minimum distance: a method for partitioning recurrences for multiprocessors. </title> <journal> IEEE Transactions on Computers, </journal> <volume> C-38, </volume> <month> Aug. </month> <year> 1989. </year>
Reference-contexts: In contrast, our work considers a multiprocessor environment where each processor has its own local cache or its own local memory, and where different processors may share data. The work by Peir and Cytron <ref> [28] </ref>, Shang and Fortes [30], and by D'Hollander [9] share the common goal of partitioning an index set into independent execution subsets such that the corresponding loop iterations can execute on different processors without interprocessor communication.
Reference: [29] <author> C. D. Polychronopoulos and D. Kuck. </author> <title> Guided self-scheduling: a practical scheduling scheme for parallel supercomputers. </title> <journal> IEEE Transactions on Computers, </journal> <volume> C-36, </volume> <month> Dec. </month> <year> 1987. </year>
Reference-contexts: With dynamic scheduling, the iterations are also divided into CHUNK-sized chunks. As each process finishes a chunk, however, it enters a critical section to grab the next available chunk. With gss scheduling <ref> [29] </ref> , the chunk size is varied, depending on the number of iterations remaining. None of these SGI-provided methods consider task alignment.
Reference: [30] <author> W. Shang and J. Fortes. </author> <title> Time optimal linear schedules for algorithms with uniform dependencies. </title> <journal> IEEE Transactions on Computers, </journal> <volume> C-40, </volume> <month> June </month> <year> 1991. </year>
Reference-contexts: In contrast, our work considers a multiprocessor environment where each processor has its own local cache or its own local memory, and where different processors may share data. The work by Peir and Cytron [28], Shang and Fortes <ref> [30] </ref>, and by D'Hollander [9] share the common goal of partitioning an index set into independent execution subsets such that the corresponding loop iterations can execute on different processors without interprocessor communication.
Reference: [31] <author> Z. Shen, Z. Li, and P. -C. Yew. </author> <title> An empirical study of Fortran programs for parallelizing compilers. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 1(3), </volume> <pages> pp. 356-364, </pages> <month> July </month> <year> 1990. </year>
Reference-contexts: assuming that the loop bounds, N 2 and N 3 , are large enough to satisfy the following: N 2 &gt; max (j G:C:D:(b fl;1 ; c fl;1 ) c fl;2 j) b fl;1 j; j G:C:D:(b fl;2 ; c fl;2 ) These assumptions are almost always true in practice <ref> [31] </ref>. When they are not true, the parallel loops will be too small to be important. With these assumptions, we have the following theorem. Theorem 2 [21]: Let b fl;1 a fl;2 a fl;1 b fl;2 = 0.
Reference: [32] <author> K. Tomko and S. Abraham. </author> <title> Iteration partitioning for resolving stride conflicts on cache-coherent multiprocessors. </title> <booktitle> In Proceedings of 1993 International Conference on Parallel Processing, </booktitle> <month> Aug. </month> <year> 1993. </year> <month> 28 </month>
Reference-contexts: The subscript expressions of right-hand side array references are restricted to be the sum of a parallel loop index and a small constant, while the subscript expressions of left-hand array references are restricted to contain the parallel loop indices only. Tomko and Abraham 4 <ref> [32] </ref> develop iteration partitioning techniques for data-parallel application programs. They assume that there is only one pair of data access functions and that each loop index variable can appear in only one dimension of each array subscript expression.
Reference: [33] <author> J. Torrellas, M. S. Lam, and J. L. Hennessy. </author> <title> False sharing and spatial locality in multiprocessor caches. </title> <journal> IEEE Transactions on Computers, </journal> <volume> C-43, </volume> <month> June </month> <year> 1994. </year>
Reference-contexts: Unlike these previous works, our work considers nested loops which are not necessarily perfectly nested. Loop bounds can be any variables, and array subscript expressions are much more general. Many researchers have studied the cache false sharing problem <ref> [10, 17, 19, 33] </ref> in which cache thrashing occurs when different processors share the same cache line of multiple words, although the processors do not share the same word. Many algorithms have been proposed to reduce false sharing by better memory allocation, better thread scheduling, or by program transformations.
Reference: [34] <author> M. Wolf and M. Lam. </author> <title> A data locality optimizing algorithm. </title> <booktitle> In Proceedings of the ACM SIG-PLAN'91 Conference on Program Language Design and Implementation, </booktitle> <month> June </month> <year> 1991. </year>
Reference-contexts: Carr, Callahan and Kennedy [7, 8] discuss options for compiler control of a uniprocessor's memory hierarchy. Wolf and Lam develop an algorithm that estimates all temporal and spatial reuse of a given loop permutation <ref> [34] </ref>. These optimizations all attempt to maximize the reuse of cached data on a single processor. They also have a secondary effect of improving multiprocessor performance by reducing the bandwidth requirement of each processor, thereby reducing contention in the memory system.
Reference: [35] <author> M. Wolfe. </author> <title> More iteration space tiling. </title> <booktitle> In Proceedings of Supercomputing'89, </booktitle> <year> 1989. </year> <month> 29 </month>
Reference-contexts: Abu-Sufah, Kuck and Lawrie use loop blocking to improve paging performance by improving the locality of references [2]. Wolfe proposes iteration space tiling as a way to improve data reuse in a cache or a local memory <ref> [35] </ref>. Gallivan, Jalby and Gannon define a reference window for a dependence as the variables referenced by both the source and the sink of the dependence [15, 16].
References-found: 35

