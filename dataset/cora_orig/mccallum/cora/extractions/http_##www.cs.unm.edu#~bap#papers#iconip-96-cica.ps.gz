URL: http://www.cs.unm.edu/~bap/papers/iconip-96-cica.ps.gz
Refering-URL: http://www.cs.unm.edu/stats/
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Email: barak.pearlmutter@alumni.cs.cmu.edu  lucas@scr.siemens.com  
Title: A Context-Sensitive Generalization of ICA  
Author: Barak A. Pearlmuttery Lucas C. Parraz 
Address: La Jolla, California, USA,  Princeton, New Jersey, USA,  
Affiliation: yDept. of Cog. Sci., UCSD,  zSiemens Corporate Research,  
Abstract: Source separation arises in a surprising number of signal processing applications, from speech recognition to EEG analysis. In the square linear blind source separation problem without time delays, one must find an unmixing matrix which can detangle the result of mixing n unknown independent sources through an unknown n fi n mixing matrix. The recently introduced ICA blind source separation algorithm (Baram and Roth 1994; Bell and Sejnowski 1995) is a powerful and surprisingly simple technique for solving this problem. ICA is all the more remarkable for performing so well despite making absolutely no use of the temporal structure of its input! This paper presents a new algorithm, contextual ICA, which derives from a maximum likelihood density estimation formulation of the problem. cICA can incorporate arbitrarily complex adaptive history-sensitive source models, and thereby make use of the temporal structure of its input. This allows it to separate in a number of situations where standard ICA cannot, including sources with low kurtosis, colored gaussian sources, and sources which have gaussian histograms. Since ICA is a special case of cICA, the MLE derivation provides as a corollary a rigorous derivation of classic ICA. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Amari, S., Cichocki, A., and Yang, H. H. </author> <year> (1996). </year> <title> A new learning algorithm for blind signal separation.. </title> <note> In NIPS*95 (1996). In press. </note>
Reference: <author> Baram, Y. and Roth, Z. </author> <year> (1994). </year> <title> Density Shaping by Neural Networks with Application to Classification, Estimation and Forecasting. </title> <type> Tech. rep. </type> <institution> CIS-94-20, Center for Intelligent Systems, Technion, Israel Institute for Technology, Haifa. </institution>
Reference: <author> Baram, Y. and Roth, Z. </author> <year> (1995). </year> <title> Forecasting by Density Shaping Using Neural Networks. In Computational Intelligence for Financial Engineering New York City. </title> <publisher> IEEE Press. </publisher>
Reference-contexts: number of problems, from separation of digitally mixed speech signals (Bell and Sejnowski 1995), to separating the componenets of electroencephalo-graphic data (Makeig et al. 1996), to blind deconvolution (Bell and Sejnowski 1995), to finding the higher-order structure of a natural sound (Bell and Sejnowski 1996b), and even to financial forecasting <ref> (Baram and Roth 1995) </ref> and image processing (Bell and Sejnowski 1996a). There have been attempts to generalize the algorithm, the most notable being extensions to tolerate time delays and echos introduced by Torkkola (1996a, 1996b).
Reference: <author> Bell, A. J. and Sejnowski, T. J. </author> <year> (1995). </year> <title> An Information-Maximization Approach to Blind Separation and Blind Deconvolution. </title> <journal> Neural Computation, </journal> <volume> 7(6), </volume> <pages> 1129-1159. </pages>
Reference-contexts: The ICA algorithm, in various configurations, has been applied to a surprising number of problems, from separation of digitally mixed speech signals <ref> (Bell and Sejnowski 1995) </ref>, to separating the componenets of electroencephalo-graphic data (Makeig et al. 1996), to blind deconvolution (Bell and Sejnowski 1995), to finding the higher-order structure of a natural sound (Bell and Sejnowski 1996b), and even to financial forecasting (Baram and Roth 1995) and image processing (Bell and Sejnowski 1996a). <p> The ICA algorithm, in various configurations, has been applied to a surprising number of problems, from separation of digitally mixed speech signals <ref> (Bell and Sejnowski 1995) </ref>, to separating the componenets of electroencephalo-graphic data (Makeig et al. 1996), to blind deconvolution (Bell and Sejnowski 1995), to finding the higher-order structure of a natural sound (Bell and Sejnowski 1996b), and even to financial forecasting (Baram and Roth 1995) and image processing (Bell and Sejnowski 1996a).
Reference: <author> Bell, A. J. and Sejnowski, T. J. </author> <year> (1996a). </year> <title> The Independent Components of Natural Scenes. </title> <journal> Vision Research. </journal> <note> Submitted. </note>
Reference-contexts: mixed speech signals (Bell and Sejnowski 1995), to separating the componenets of electroencephalo-graphic data (Makeig et al. 1996), to blind deconvolution (Bell and Sejnowski 1995), to finding the higher-order structure of a natural sound (Bell and Sejnowski 1996b), and even to financial forecasting (Baram and Roth 1995) and image processing <ref> (Bell and Sejnowski 1996a) </ref>. There have been attempts to generalize the algorithm, the most notable being extensions to tolerate time delays and echos introduced by Torkkola (1996a, 1996b).
Reference: <author> Bell, A. J. and Sejnowski, T. J. </author> <year> (1996b). </year> <title> Learning the higher-order structure of a natural sound. Network. </title> <publisher> In press. </publisher>
Reference-contexts: in various configurations, has been applied to a surprising number of problems, from separation of digitally mixed speech signals (Bell and Sejnowski 1995), to separating the componenets of electroencephalo-graphic data (Makeig et al. 1996), to blind deconvolution (Bell and Sejnowski 1995), to finding the higher-order structure of a natural sound <ref> (Bell and Sejnowski 1996b) </ref>, and even to financial forecasting (Baram and Roth 1995) and image processing (Bell and Sejnowski 1996a). There have been attempts to generalize the algorithm, the most notable being extensions to tolerate time delays and echos introduced by Torkkola (1996a, 1996b).
Reference: <author> Bialek, W., Rieke, F., de Ruyter van Stevenick, R. R., and Warland, D. </author> <year> (1991). </year> <title> Reading a Neural Code. </title> <journal> Science, </journal> <volume> 252, </volume> <pages> 1854-1857. </pages>
Reference-contexts: ICA was motivated by considerations of biological optimality, which flow from experiments showing that, when presented with natural stimuli, many neurons appear to make good use of their available axonal channel capacity <ref> (Bialek et al. 1991) </ref>.
Reference: <author> Comon, P. </author> <year> (1994). </year> <title> Independent component analysis: A new concept. </title> <booktitle> Signal Processing, </booktitle> <volume> 36, </volume> <pages> 287-314. </pages>
Reference: <author> Hotelling, H. </author> <year> (1933). </year> <title> Analysis of a complex of statistical variables into principal components. </title> <journal> Journal of Educational Psychology, </journal> <volume> 24, </volume> <pages> 417-441, 498-520. </pages>
Reference-contexts: The length of the history used is varied from zero, which corresponds to conventional ICA (left), to one (center), to two (right). Finally, we would like to compare ICA with PCA. The principal components algorithm <ref> (Hotelling 1933) </ref> fits a linear mixture of one-dimensional Gaussian sources of minimal variance to samples from a high-dimensional distribution. ICA performs a similar action, but instead uses a linear mixture of potentially non-Gaussian distributions.
Reference: <author> Makeig, S., Bell, A. J., Jung, T.-P., and Sejnowski, T. J. </author> <year> (1996). </year> <title> Independent component analysis of Electroen-cephalographic data.. </title> <note> In NIPS*95 (1996). In press. </note>
Reference-contexts: The ICA algorithm, in various configurations, has been applied to a surprising number of problems, from separation of digitally mixed speech signals (Bell and Sejnowski 1995), to separating the componenets of electroencephalo-graphic data <ref> (Makeig et al. 1996) </ref>, to blind deconvolution (Bell and Sejnowski 1995), to finding the higher-order structure of a natural sound (Bell and Sejnowski 1996b), and even to financial forecasting (Baram and Roth 1995) and image processing (Bell and Sejnowski 1996a).
Reference: <author> Mendel, J. M. and Burrus, C. S. </author> <year> (1990). </year> <title> Maximum-likelihood deconvolution: a journey into model-based signal processing. </title> <publisher> Springer-Verlag. </publisher> <month> NIPS*95 </month> <year> (1996). </year> <booktitle> Advances in Neural Information Processing Systems 8. </booktitle> <publisher> MIT Press. In press. </publisher>
Reference-contexts: One desires to estimate some true distribution p (x) over a space R n from which samples x 1 ; x 2 ; : : : have been drawn. The maximum likelihood approach <ref> (Mendel and Burrus 1990) </ref> is to use a density estimator of some parametric form, say ^p (x; w). Given a setting of the parameter vector w, this will constitute the estimated probability density.
Reference: <author> Nowlan, S. J. and Hinton, G. E. </author> <year> (1992). </year> <title> Adaptive Soft Weight Tying using Gaussian Mixtures. </title> <booktitle> In Advances in Neural Information Processing Systems 4, </booktitle> <pages> pp. 993-1000. </pages> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Obradovic, D. and Deco, G. </author> <year> (1995). </year> <title> Linear Feature Extraction in non-Gaussian Networks. </title> <booktitle> In World Congress on Neural Networks, </booktitle> <volume> Vol. 1, </volume> <pages> pp. </pages> <address> 523-526 Washington. </address>
Reference: <author> Parra, L. C., Deco, G., and Miesbach, S. </author> <year> (1995). </year> <title> Redundancy reduction with information-preserving maps. Network: </title> <booktitle> Computation in Neural Systems, </booktitle> <volume> 6, </volume> <pages> 61-72. </pages>
Reference: <author> Pearlmutter, B. A. </author> <year> (1992). </year> <title> Temporally Continuous vs. Clocked Networks. </title> <booktitle> In Neural Networks in Robotics, </booktitle> <pages> pp. 237-252. </pages> <publisher> Kluwer Academic Publishers. </publisher>
Reference: <author> Robbins, H. and Monro, S. </author> <year> (1951). </year> <title> A Stochastic Approximation Method. </title> <journal> Annals of Mathematical Statistics, </journal> <volume> 22, </volume> <pages> 400-407. </pages>
Reference: <author> Torkkola, K. </author> <year> (1996a). </year> <title> Blind separation of convolved sources based on information maximization. </title> <booktitle> In Neural Net- works for Signal Processing VI Kyoto, </booktitle> <address> Japan. </address> <publisher> IEEE Press. In press. </publisher>
Reference: <author> Torkkola, K. </author> <year> (1996b). </year> <title> Blind separation of delayed sources based on information maximization. </title> <booktitle> In Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing Atlanta, </booktitle> <address> GA. </address> <publisher> In press. </publisher>
Reference: <author> Verhulst, P. </author> <type> F. </type> <institution> (1844) Nouveaux memoires de l'Academie royale des sciences et belles-lettres de Bruxelles, </institution> <note> 18, 1. Also 1846, 20, 1. </note>
References-found: 19

