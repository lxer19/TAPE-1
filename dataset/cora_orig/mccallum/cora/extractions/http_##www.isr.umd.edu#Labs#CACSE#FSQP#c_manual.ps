URL: http://www.isr.umd.edu/Labs/CACSE/FSQP/c_manual.ps
Refering-URL: http://www.isr.umd.edu/Labs/CACSE/FSQP/fsqp.html
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Title: User's Guide for CFSQP Version 2.5: A C Code for Solving (Large Scale) Constrained Nonlinear
Author: Craig Lawrence, Jian L. Zhou, and Andre L. Tits 
Note: (Institute for Systems Research TR-94-16r1) 1 This research was supported in part by NSF's Engineering Research Centers Program No. NSFD-CDR-88-03012, by NSF grant Nos. DMC-88-15996, DMI-9313286 and by a grant from the Westinghouse Corporation.  
Address: College Park, MD 20742  
Affiliation: Electrical Engineering Department and Institute for Systems Research University of Maryland,  
Abstract-found: 0
Intro-found: 1
Reference: [1] <author> D.Q. Mayne & E. Polak, </author> <title> "Feasible Directions Algorithms for Optimization Problems with Equality and Inequality Constraints," </title> <journal> Math. </journal> <note> Programming 11 (1976) , 67-80. </note>
Reference-contexts: Then, using a scheme due to Mayne and Polak <ref> [1] </ref> and adapted to the FSQP framework in [2], nonlinear equality constraints are turned into inequality constraints 2 h j (x) 0; j = 1; : : : ; n e and the original objective function max i2I f ff i (x)g is replaced by the modified objective function f m <p> In [3], an essentially arbitrary feasible descent direction d 1 = d 1 (x) is then computed. Then for a certain scalar = (x) 2 <ref> [0; 1] </ref>, a feasible descent direction d = (1 )d 0 + d 1 is obtained, asymptotically close to d 0 : Finally a second order correction ~ d = ~ d (x; d; H) is computed, involving auxiliary function evaluations at x + d; and an Armijo type search is <p> First a feasible direction d 1 = d 1 (x) is computed, which is nonzero even at Karush-Kuhn-Tucker points (and thus is not everywhere a descent direction). Then for a certain scalar ` = ` (x) 2 <ref> [0; 1] </ref>; a "local" feasible direction d ` = (1 ` )d 0 + ` d 1 is obtained, and at x+d ` the objective functions are tested and feasibility is checked. If the requirements pointed out above are satisfied, x+d ` is accepted as next iterate. <p> Increase k by 1. 3 This is a refinement (saving much computation and memory) of the scheme proposed in <ref> [1] </ref>. 9 v. Go back to Step 1. 2 Algorithm FSQP-NL. Parameters. = 3:0, - = 0:01, ff = 0:1, fi = 0:5, = 0:2, = 0:5, fl = 2:5, C = 0:01, d = 5:0, t = 0:1, * 1 = 0:1, * 2 = 2, ffi = 2. <p> Define values g g k;j equal to zero if g j (x k ) + hrg j (x k ); d 0 10 or equal to the maximum in <ref> [0; 1] </ref> such that g j (x k ) + hrg j (x k ); (1 )d 0 k i v k otherwise. Similarly, define values h k;j for j = 1; : : : ; n e . <p> related set of objectives, hence we only list them once.) void objmad (nparam,j,x,fj,cd) int nparam,j; 40 double *x,*fj; double *cd; - double pi,theta; int i; theta=pi*(8.5e0+j*0.5e0)/180.e0; *fj=0.e0; *fj=*fj+cos (2.e0*pi*x [i]*sin (theta)); *fj=2.e0*(*fj+cos (2.e0*pi*3.5e0*sin (theta)))/15.e0 +1.e0/15.e0; return; - void cnmad (nparam,j,x,gj,cd) int nparam,j; double *x,*gj; double *cd; - double ss; ss=cd <ref> [1] </ref>; switch (j) - case 1: *gj=ss-x [0]; break; case 2: *gj=ss+x [0]-x [1]; break; case 3: *gj=ss+x [1]-x [2]; break; case 4: *gj=ss+x [2]-x [3]; break; 41 case 5: *gj=ss+x [3]-x [4]; break; case 6: *gj=ss+x [4]-x [5]; break; case 7: *gj=ss+x [5]-3.5e0; break; - return; - After running the <p> int nparam,j; 40 double *x,*fj; double *cd; - double pi,theta; int i; theta=pi*(8.5e0+j*0.5e0)/180.e0; *fj=0.e0; *fj=*fj+cos (2.e0*pi*x [i]*sin (theta)); *fj=2.e0*(*fj+cos (2.e0*pi*3.5e0*sin (theta)))/15.e0 +1.e0/15.e0; return; - void cnmad (nparam,j,x,gj,cd) int nparam,j; double *x,*gj; double *cd; - double ss; ss=cd <ref> [1] </ref>; switch (j) - case 1: *gj=ss-x [0]; break; case 2: *gj=ss+x [0]-x [1]; break; case 3: *gj=ss+x [1]-x [2]; break; case 4: *gj=ss+x [2]-x [3]; break; 41 case 5: *gj=ss+x [3]-x [4]; break; case 6: *gj=ss+x [4]-x [5]; break; case 7: *gj=ss+x [5]-3.5e0; break; - return; - After running the first algorithm on a SUN 4/SPARC station 1, the following output is obtained <p> break; - return; - void grcn71 (nparam,j,x,gradgj,dummy,cd) int nparam,j; double *x,*gradgj; void (* dummy)(); void *cd; - switch (j) - case 1: gradgj [0]=-x <ref> [1] </ref>*x [2]*x [3]; gradgj [1]=-x [0]*x [2]*x [3]; gradgj [2]=-x [0]*x [1]*x [3]; gradgj [3]=-x [0]*x [1]*x [2]; break; case 2: gradgj [0]=2.e0*x [0]; gradgj [1]=2.e0*x [1]; gradgj [2]=2.e0*x [2]; gradgj [3]=2.e0*x [3]; break; - return; - After running the algorithm on a SUN 4/SPARC station 1, the following output is obtained CFSQP Version 2.5 (Released April 1997) Copyright (c) 1993 --- 1997 C.T. Lawrence, J.L. Zhou and A.L. <p> The original semi-infinite programming problem is defined by x2IR 2 3 x 2 2 + 1 s.t. x 2 1 t 2 ) 2 0 8 t 2 <ref> [0; 1] </ref>: In order to use the algorithm, we must choose a finite subset ffi [0; 1] (using the notation introduced in x 3). There are, of course, many ways that we could do this. <p> The original semi-infinite programming problem is defined by x2IR 2 3 x 2 2 + 1 s.t. x 2 1 t 2 ) 2 0 8 t 2 <ref> [0; 1] </ref>: In order to use the algorithm, we must choose a finite subset ffi [0; 1] (using the notation introduced in x 3). There are, of course, many ways that we could do this. <p> (f); free (g); free (lambda); return 0; - Following are the functions that define the objective, constraint, and their gradients. void obj (nparam,j,x,fj,cd) int nparam,j; double *x,*fj; void *cd; - return; - void grob (nparam,j,x,gradfj,dummy,cd) int nparam,j; double *x,*gradfj; void (* dummy)(); void *cd; - gradfj [0]=(2.e0/3.e0)*x [0]+0.5e0; gradfj <ref> [1] </ref>=2.e0*x [1]; return; - void 58 cntr (nparam,j,x,gj,cd) int nparam,j; double *x,*gj; void *cd; - double y; y=(j-1)/100.e0; +x [1]; return; - void grcn (nparam,j,x,gradgj,dummy,cd) int nparam,j; double *x,*gradgj; void (* dummy)(); void *cd; - double y; y=(j-1)/100.e0; gradgj [0]=-4.e0*(1.e0-pow (y*x [0],2.e0))*y*y*x [0]-y*y; gradgj [1]=-2.e0*x [1]+1.e0; return; - After running the algorithm <p> their gradients. void obj (nparam,j,x,fj,cd) int nparam,j; double *x,*fj; void *cd; - return; - void grob (nparam,j,x,gradfj,dummy,cd) int nparam,j; double *x,*gradfj; void (* dummy)(); void *cd; - gradfj [0]=(2.e0/3.e0)*x [0]+0.5e0; gradfj <ref> [1] </ref>=2.e0*x [1]; return; - void 58 cntr (nparam,j,x,gj,cd) int nparam,j; double *x,*gj; void *cd; - double y; y=(j-1)/100.e0; +x [1]; return; - void grcn (nparam,j,x,gradgj,dummy,cd) int nparam,j; double *x,*gradgj; void (* dummy)(); void *cd; - double y; y=(j-1)/100.e0; gradgj [0]=-4.e0*(1.e0-pow (y*x [0],2.e0))*y*y*x [0]-y*y; gradgj [1]=-2.e0*x [1]+1.e0; return; - After running the algorithm on a SUN 4/SPARC station 1, the following output was obtained CFSQP Version 2.5 (Released April 1997) Copyright
Reference: [2] <author> C.T. Lawrence & A.L. </author> <title> Tits, "Nonlinear Equality Constraints in Feasible Sequential Quadratic Programming," </title> <note> Optimization Methods and Software 6 (1996) , 265-282. </note>
Reference-contexts: Then, using a scheme due to Mayne and Polak [1] and adapted to the FSQP framework in <ref> [2] </ref>, nonlinear equality constraints are turned into inequality constraints 2 h j (x) 0; j = 1; : : : ; n e and the original objective function max i2I f ff i (x)g is replaced by the modified objective function f m (x; p) = max ff i (x)g j=1 <p> the solution of the quadratic programming subproblems, CFSQP is set up to call a C version of QLD [9], converted from Fortran via f2c (see [10]) and provided with the CFSQP distribution for the user's convenience. 2 Description of the Basic Algorithms The algorithms described and analyzed in [3], [4], <ref> [2] </ref>, and [5] are as follows. For simplicity of exposition, we describe the algorithms as they pertain to solving problem (P ), deferring the discussion of the algorithm described in [6] and [7] for the solution of (P sr ) until x 3. <p> (f); free (g); free (lambda); return 0; - Following are the functions defining the objective, constraints, and their gradients. void obj32 (nparam,j,x,fj,cd) int nparam,j; double *x,*fj; 35 void *cd; - return; - void grob32 (nparam,j,x,gradfj,dummy,cd) int nparam,j; double *x,*gradfj; void (* dummy)(); void *cd; - double fa,fb; fa=2.e0*(x [0]+3.e0*x [1]+x <ref> [2] </ref>); gradfj [0]=fa+fb; gradfj [1]=fa*3.e0-fb; gradfj [2]=fa; return; - void cntr32 (nparam,j,x,gj,cd) int nparam,j; double *x,*gj; void *cd; - switch (j) - case 1: *gj=pow (x [0],3.e0)-6.e0*x [1]-4.e0*x [2]+3.e0; break; case 2: *gj=1.e0-x [0]-x [1]-x [2]; break; - return; 36 void grcn32 (nparam,j,x,gradgj,dummy,cd) int nparam,j; double *x,*gradgj; void (* dummy)(); void <p> int nparam,j; double *x,*gradfj; void (* dummy)(); void *cd; - double fa,fb; fa=2.e0*(x [0]+3.e0*x [1]+x <ref> [2] </ref>); gradfj [0]=fa+fb; gradfj [1]=fa*3.e0-fb; gradfj [2]=fa; return; - void cntr32 (nparam,j,x,gj,cd) int nparam,j; double *x,*gj; void *cd; - switch (j) - case 1: *gj=pow (x [0],3.e0)-6.e0*x [1]-4.e0*x [2]+3.e0; break; case 2: *gj=1.e0-x [0]-x [1]-x [2]; break; - return; 36 void grcn32 (nparam,j,x,gradgj,dummy,cd) int nparam,j; double *x,*gradgj; void (* dummy)(); void *cd; - switch (j) - case 1: gradgj [0]=3.e0*x [0]*x [0]; gradgj [1]=-6.e0; gradgj [2]=-4.e0; break; case 2: gradgj [0]=gradgj [1]=gradgj [2]=-1.e0; break; - return; - The file containing the user-provided main programs and functions <p> *cd; - double pi,theta; int i; theta=pi*(8.5e0+j*0.5e0)/180.e0; *fj=0.e0; *fj=*fj+cos (2.e0*pi*x [i]*sin (theta)); *fj=2.e0*(*fj+cos (2.e0*pi*3.5e0*sin (theta)))/15.e0 +1.e0/15.e0; return; - void cnmad (nparam,j,x,gj,cd) int nparam,j; double *x,*gj; double *cd; - double ss; ss=cd [1]; switch (j) - case 1: *gj=ss-x [0]; break; case 2: *gj=ss+x [0]-x [1]; break; case 3: *gj=ss+x [1]-x <ref> [2] </ref>; break; case 4: *gj=ss+x [2]-x [3]; break; 41 case 5: *gj=ss+x [3]-x [4]; break; case 6: *gj=ss+x [4]-x [5]; break; case 7: *gj=ss+x [5]-3.5e0; break; - return; - After running the first algorithm on a SUN 4/SPARC station 1, the following output is obtained (the results for the set of <p> constraints and their gradients. void obj71 (nparam,j,x,fj,cd) int nparam,j; double *x,*fj; void *cd; - return; - void grob71 (nparam,j,x,gradfj,dummy,cd) int nparam,j; double *x,*gradfj; void (* dummy)(); void *cd; - gradfj [0]=x [3]*(x [0]+x [1]+x <ref> [2] </ref>)+x [0]*x [3]; gradfj [1]=x [0]*x [3]; gradfj [2]=x [0]*x [3]+1.e0; gradfj [3]=x [0]*(x [0]+x [1]+x [2]); return; - void cntr71 (nparam,j,x,gj,cd) int nparam,j; double *x,*gj; void *cd; - switch (j) - case 1: *gj=25.e0-x [0]*x [1]*x [2]*x [3]; 48 break; case 2: *gj=x [0]*x [0]+x [1]*x [1]+x [2]*x [2]+x [3]*x [3]-40.e0; break; - return; - void grcn71 (nparam,j,x,gradgj,dummy,cd) int nparam,j; double *x,*gradgj; void (* dummy)(); void <p> *gj=x [0]*x [0]+x [1]*x [1]+x <ref> [2] </ref>*x [2]+x [3]*x [3]-40.e0; break; - return; - void grcn71 (nparam,j,x,gradgj,dummy,cd) int nparam,j; double *x,*gradgj; void (* dummy)(); void *cd; - switch (j) - case 1: gradgj [0]=-x [1]*x [2]*x [3]; gradgj [1]=-x [0]*x [2]*x [3]; gradgj [2]=-x [0]*x [1]*x [3]; gradgj [3]=-x [0]*x [1]*x [2]; break; case 2: gradgj [0]=2.e0*x [0]; gradgj [1]=2.e0*x [1]; gradgj [2]=2.e0*x [2]; gradgj [3]=2.e0*x [3]; break; - return; - After running the algorithm on a SUN 4/SPARC station 1, the following output is obtained CFSQP Version 2.5 (Released April 1997) Copyright (c) 1993 --- 1997 C.T. Lawrence, J.L. <p> - void grcn71 (nparam,j,x,gradgj,dummy,cd) int nparam,j; double *x,*gradgj; void (* dummy)(); void *cd; - switch (j) - case 1: gradgj [0]=-x [1]*x <ref> [2] </ref>*x [3]; gradgj [1]=-x [0]*x [2]*x [3]; gradgj [2]=-x [0]*x [1]*x [3]; gradgj [3]=-x [0]*x [1]*x [2]; break; case 2: gradgj [0]=2.e0*x [0]; gradgj [1]=2.e0*x [1]; gradgj [2]=2.e0*x [2]; gradgj [3]=2.e0*x [3]; break; - return; - After running the algorithm on a SUN 4/SPARC station 1, the following output is obtained CFSQP Version 2.5 (Released April 1997) Copyright (c) 1993 --- 1997 C.T. Lawrence, J.L. Zhou and A.L.
Reference: [3] <author> E.R. Panier & A.L. </author> <title> Tits, "On Combining Feasibility, Descent and Superlinear Convergence in Inequality Constrained Optimization," </title> <journal> Math. </journal> <note> Programming 59 (1993) , 261-276. </note>
Reference-contexts: Thus, CFSQP solves the original problem with nonlinear equality constraints by solving a modified optimization problem with only linear constraints and nonlinear inequality constraints. For the transformed problem, it implements algorithms that are described and analyzed in <ref> [3] </ref>, [4], [5], [6] and [7], with some additional refinements. These algorithms are based on a Sequential Quadratic Programming (SQP) iteration, modified so as to generate feasible iterates. The merit function is the objective function. <p> After obtaining feasibility, either (i) an Armijo-type line search may be used, yielding a monotone decrease of the objective function at each iteration <ref> [3] </ref>; or (ii) a nonmonotone line search (inspired from [8] and analyzed in [4] and [5] in the present context) may be selected, forcing a decrease of the objective function within at most four iterations. <p> For the solution of the quadratic programming subproblems, CFSQP is set up to call a C version of QLD [9], converted from Fortran via f2c (see [10]) and provided with the CFSQP distribution for the user's convenience. 2 Description of the Basic Algorithms The algorithms described and analyzed in <ref> [3] </ref>, [4], [2], and [5] are as follows. For simplicity of exposition, we describe the algorithms as they pertain to solving problem (P ), deferring the discussion of the algorithm described in [6] and [7] for the solution of (P sr ) until x 3. <p> In <ref> [3] </ref>, an essentially arbitrary feasible descent direction d 1 = d 1 (x) is then computed. <p> Conditions are given in <ref> [3] </ref> on d 1 (), () and ~ d (; ) that result in a globally convergent, locally superlinear convergent algorithm. The algorithm in [4] is somewhat more sophisticated. <p> not accepted, a "global" feasible descent direction d g = (1 g )d 0 + g d 1 is obtained with g = g (x) 2 [0; ` ]: A second order correction ~ d = ~ d (x; d g ; H) is computed the same way as in <ref> [3] </ref>, and a "nonmonotone" search is performed along the arc x+ td g + t 2 ~ d: Here the purpose of ~ d is to suitably initialize the sequence for the "four iterate" rule. <p> The quadratic program that yields ~ d involves only a subset of "active" functions, thus decreasing the number of function evaluations. The details are given below. The analysis in <ref> [3] </ref>, [4], and [5] can be easily extended to these modified algorithms. <p> theta=pi*(8.5e0+j*0.5e0)/180.e0; *fj=0.e0; *fj=*fj+cos (2.e0*pi*x [i]*sin (theta)); *fj=2.e0*(*fj+cos (2.e0*pi*3.5e0*sin (theta)))/15.e0 +1.e0/15.e0; return; - void cnmad (nparam,j,x,gj,cd) int nparam,j; double *x,*gj; double *cd; - double ss; ss=cd [1]; switch (j) - case 1: *gj=ss-x [0]; break; case 2: *gj=ss+x [0]-x [1]; break; case 3: *gj=ss+x [1]-x [2]; break; case 4: *gj=ss+x [2]-x <ref> [3] </ref>; break; 41 case 5: *gj=ss+x [3]-x [4]; break; case 6: *gj=ss+x [4]-x [5]; break; case 7: *gj=ss+x [5]-3.5e0; break; - return; - After running the first algorithm on a SUN 4/SPARC station 1, the following output is obtained (the results for the set of objectives have been deleted to save <p> 47 free (lambda); return 0; - Following are the functions that define the objective, constraints and their gradients. void obj71 (nparam,j,x,fj,cd) int nparam,j; double *x,*fj; void *cd; - return; - void grob71 (nparam,j,x,gradfj,dummy,cd) int nparam,j; double *x,*gradfj; void (* dummy)(); void *cd; - gradfj [0]=x <ref> [3] </ref>*(x [0]+x [1]+x [2])+x [0]*x [3]; gradfj [1]=x [0]*x [3]; gradfj [2]=x [0]*x [3]+1.e0; gradfj [3]=x [0]*(x [0]+x [1]+x [2]); return; - void cntr71 (nparam,j,x,gj,cd) int nparam,j; double *x,*gj; void *cd; - switch (j) - case 1: *gj=25.e0-x [0]*x [1]*x [2]*x [3]; 48 break; case 2: *gj=x [0]*x [0]+x [1]*x [1]+x [2]*x [2]+x [3]*x [3]-40.e0; break; <p> 0; - Following are the functions that define the objective, constraints and their gradients. void obj71 (nparam,j,x,fj,cd) int nparam,j; double *x,*fj; void *cd; - return; - void grob71 (nparam,j,x,gradfj,dummy,cd) int nparam,j; double *x,*gradfj; void (* dummy)(); void *cd; - gradfj [0]=x <ref> [3] </ref>*(x [0]+x [1]+x [2])+x [0]*x [3]; gradfj [1]=x [0]*x [3]; gradfj [2]=x [0]*x [3]+1.e0; gradfj [3]=x [0]*(x [0]+x [1]+x [2]); return; - void cntr71 (nparam,j,x,gj,cd) int nparam,j; double *x,*gj; void *cd; - switch (j) - case 1: *gj=25.e0-x [0]*x [1]*x [2]*x [3]; 48 break; case 2: *gj=x [0]*x [0]+x [1]*x [1]+x [2]*x [2]+x [3]*x [3]-40.e0; break; - return; - void <p> *x,*gradfj; void (* dummy)(); void *cd; - gradfj [0]=x <ref> [3] </ref>*(x [0]+x [1]+x [2])+x [0]*x [3]; gradfj [1]=x [0]*x [3]; gradfj [2]=x [0]*x [3]+1.e0; gradfj [3]=x [0]*(x [0]+x [1]+x [2]); return; - void cntr71 (nparam,j,x,gj,cd) int nparam,j; double *x,*gj; void *cd; - switch (j) - case 1: *gj=25.e0-x [0]*x [1]*x [2]*x [3]; 48 break; case 2: *gj=x [0]*x [0]+x [1]*x [1]+x [2]*x [2]+x [3]*x [3]-40.e0; break; - return; - void grcn71 (nparam,j,x,gradgj,dummy,cd) int nparam,j; double *x,*gradgj; void (* dummy)(); void *cd; - switch (j) - case 1: gradgj [0]=-x [1]*x [2]*x [3]; gradgj [1]=-x [0]*x [2]*x [3]; gradgj [2]=-x [0]*x [1]*x [3]; <p> - switch (j) - case 1: *gj=25.e0-x [0]*x [1]*x [2]*x <ref> [3] </ref>; 48 break; case 2: *gj=x [0]*x [0]+x [1]*x [1]+x [2]*x [2]+x [3]*x [3]-40.e0; break; - return; - void grcn71 (nparam,j,x,gradgj,dummy,cd) int nparam,j; double *x,*gradgj; void (* dummy)(); void *cd; - switch (j) - case 1: gradgj [0]=-x [1]*x [2]*x [3]; gradgj [1]=-x [0]*x [2]*x [3]; gradgj [2]=-x [0]*x [1]*x [3]; gradgj [3]=-x [0]*x [1]*x [2]; break; case 2: gradgj [0]=2.e0*x [0]; gradgj [1]=2.e0*x [1]; gradgj [2]=2.e0*x [2]; gradgj [3]=2.e0*x [3]; break; - return; - After running the algorithm on a SUN 4/SPARC station 1, the following output is obtained CFSQP <p> 1: *gj=25.e0-x [0]*x [1]*x [2]*x <ref> [3] </ref>; 48 break; case 2: *gj=x [0]*x [0]+x [1]*x [1]+x [2]*x [2]+x [3]*x [3]-40.e0; break; - return; - void grcn71 (nparam,j,x,gradgj,dummy,cd) int nparam,j; double *x,*gradgj; void (* dummy)(); void *cd; - switch (j) - case 1: gradgj [0]=-x [1]*x [2]*x [3]; gradgj [1]=-x [0]*x [2]*x [3]; gradgj [2]=-x [0]*x [1]*x [3]; gradgj [3]=-x [0]*x [1]*x [2]; break; case 2: gradgj [0]=2.e0*x [0]; gradgj [1]=2.e0*x [1]; gradgj [2]=2.e0*x [2]; gradgj [3]=2.e0*x [3]; break; - return; - After running the algorithm on a SUN 4/SPARC station 1, the following output is obtained CFSQP Version 2.5 (Released April 1997) <p> <ref> [3] </ref>; 48 break; case 2: *gj=x [0]*x [0]+x [1]*x [1]+x [2]*x [2]+x [3]*x [3]-40.e0; break; - return; - void grcn71 (nparam,j,x,gradgj,dummy,cd) int nparam,j; double *x,*gradgj; void (* dummy)(); void *cd; - switch (j) - case 1: gradgj [0]=-x [1]*x [2]*x [3]; gradgj [1]=-x [0]*x [2]*x [3]; gradgj [2]=-x [0]*x [1]*x [3]; gradgj [3]=-x [0]*x [1]*x [2]; break; case 2: gradgj [0]=2.e0*x [0]; gradgj [1]=2.e0*x [1]; gradgj [2]=2.e0*x [2]; gradgj [3]=2.e0*x [3]; break; - return; - After running the algorithm on a SUN 4/SPARC station 1, the following output is obtained CFSQP Version 2.5 (Released April 1997) Copyright (c) 1993 --- 1997 <p> (nparam,j,x,gradgj,dummy,cd) int nparam,j; double *x,*gradgj; void (* dummy)(); void *cd; - switch (j) - case 1: gradgj [0]=-x [1]*x [2]*x <ref> [3] </ref>; gradgj [1]=-x [0]*x [2]*x [3]; gradgj [2]=-x [0]*x [1]*x [3]; gradgj [3]=-x [0]*x [1]*x [2]; break; case 2: gradgj [0]=2.e0*x [0]; gradgj [1]=2.e0*x [1]; gradgj [2]=2.e0*x [2]; gradgj [3]=2.e0*x [3]; break; - return; - After running the algorithm on a SUN 4/SPARC station 1, the following output is obtained CFSQP Version 2.5 (Released April 1997) Copyright (c) 1993 --- 1997 C.T. Lawrence, J.L. Zhou and A.L.
Reference: [4] <author> J.F. Bonnans, E.R. Panier, A.L. Tits & J.L. Zhou, </author> <title> "Avoiding the Maratos Effect by Means of a Nonmonotone Line Search. II. Inequality Constrained Problems Feasible Iterates," </title> <journal> SIAM J. Numer. Anal. </journal> <month> 29 </month> <year> (1992) </year> <month> , 1187-1202. </month>
Reference-contexts: Thus, CFSQP solves the original problem with nonlinear equality constraints by solving a modified optimization problem with only linear constraints and nonlinear inequality constraints. For the transformed problem, it implements algorithms that are described and analyzed in [3], <ref> [4] </ref>, [5], [6] and [7], with some additional refinements. These algorithms are based on a Sequential Quadratic Programming (SQP) iteration, modified so as to generate feasible iterates. The merit function is the objective function. <p> After obtaining feasibility, either (i) an Armijo-type line search may be used, yielding a monotone decrease of the objective function at each iteration [3]; or (ii) a nonmonotone line search (inspired from [8] and analyzed in <ref> [4] </ref> and [5] in the present context) may be selected, forcing a decrease of the objective function within at most four iterations. <p> For the solution of the quadratic programming subproblems, CFSQP is set up to call a C version of QLD [9], converted from Fortran via f2c (see [10]) and provided with the CFSQP distribution for the user's convenience. 2 Description of the Basic Algorithms The algorithms described and analyzed in [3], <ref> [4] </ref>, [2], and [5] are as follows. For simplicity of exposition, we describe the algorithms as they pertain to solving problem (P ), deferring the discussion of the algorithm described in [6] and [7] for the solution of (P sr ) until x 3. <p> Conditions are given in [3] on d 1 (), () and ~ d (; ) that result in a globally convergent, locally superlinear convergent algorithm. The algorithm in <ref> [4] </ref> is somewhat more sophisticated. An essential difference is that while feasibility is still required, the requirement of decrease of the max objective value is replaced by the weaker requirement that the max objective value at the new point be lower than its maximum over the last four iterates. <p> Conditions are given in <ref> [4] </ref> on d 1 (), ` (), g (), and ~ d (; ) that result in a globally convergent, locally superlinear convergent algorithm. In [5], the algorithm of [4] is refined for the case of unconstrained minimax problems. The major difference over the algorithm of [4] is that there is <p> Conditions are given in <ref> [4] </ref> on d 1 (), ` (), g (), and ~ d (; ) that result in a globally convergent, locally superlinear convergent algorithm. In [5], the algorithm of [4] is refined for the case of unconstrained minimax problems. The major difference over the algorithm of [4] is that there is no need for d 1 . As in [4], ~ d is required to initialize superlinear convergence. <p> Conditions are given in <ref> [4] </ref> on d 1 (), ` (), g (), and ~ d (; ) that result in a globally convergent, locally superlinear convergent algorithm. In [5], the algorithm of [4] is refined for the case of unconstrained minimax problems. The major difference over the algorithm of [4] is that there is no need for d 1 . As in [4], ~ d is required to initialize superlinear convergence. The CFSQP implementation corresponds to a specific choice of d 1 (), (), ~ d (; ), ` (), and g () with some modifications as follows. <p> In [5], the algorithm of <ref> [4] </ref> is refined for the case of unconstrained minimax problems. The major difference over the algorithm of [4] is that there is no need for d 1 . As in [4], ~ d is required to initialize superlinear convergence. The CFSQP implementation corresponds to a specific choice of d 1 (), (), ~ d (; ), ` (), and g () with some modifications as follows. <p> The quadratic program that yields ~ d involves only a subset of "active" functions, thus decreasing the number of function evaluations. The details are given below. The analysis in [3], <ref> [4] </ref>, and [5] can be easily extended to these modified algorithms. Also obvious simplifications are introduced concerning linear constraints: the iterates are allowed (for inequality constraints) or are forced (for equality constraints) to stay on the boundary of these constraints and these constraints are not checked in the line search. <p> (2.e0*pi*3.5e0*sin (theta)))/15.e0 +1.e0/15.e0; return; - void cnmad (nparam,j,x,gj,cd) int nparam,j; double *x,*gj; double *cd; - double ss; ss=cd [1]; switch (j) - case 1: *gj=ss-x [0]; break; case 2: *gj=ss+x [0]-x [1]; break; case 3: *gj=ss+x [1]-x [2]; break; case 4: *gj=ss+x [2]-x [3]; break; 41 case 5: *gj=ss+x [3]-x <ref> [4] </ref>; break; case 6: *gj=ss+x [4]-x [5]; break; case 7: *gj=ss+x [5]-3.5e0; break; - return; - After running the first algorithm on a SUN 4/SPARC station 1, the following output is obtained (the results for the set of objectives have been deleted to save space) CFSQP Version 2.5 (Released April 1997)
Reference: [5] <author> J.L. Zhou & A.L. </author> <title> Tits, "Nonmonotone Line Search for Minimax Problems," </title> <journal> J. Optim. Theory Appl. </journal> <month> 76 </month> <year> (1993) </year> <month> , 455-476. </month>
Reference-contexts: Thus, CFSQP solves the original problem with nonlinear equality constraints by solving a modified optimization problem with only linear constraints and nonlinear inequality constraints. For the transformed problem, it implements algorithms that are described and analyzed in [3], [4], <ref> [5] </ref>, [6] and [7], with some additional refinements. These algorithms are based on a Sequential Quadratic Programming (SQP) iteration, modified so as to generate feasible iterates. The merit function is the objective function. <p> After obtaining feasibility, either (i) an Armijo-type line search may be used, yielding a monotone decrease of the objective function at each iteration [3]; or (ii) a nonmonotone line search (inspired from [8] and analyzed in [4] and <ref> [5] </ref> in the present context) may be selected, forcing a decrease of the objective function within at most four iterations. <p> of the quadratic programming subproblems, CFSQP is set up to call a C version of QLD [9], converted from Fortran via f2c (see [10]) and provided with the CFSQP distribution for the user's convenience. 2 Description of the Basic Algorithms The algorithms described and analyzed in [3], [4], [2], and <ref> [5] </ref> are as follows. For simplicity of exposition, we describe the algorithms as they pertain to solving problem (P ), deferring the discussion of the algorithm described in [6] and [7] for the solution of (P sr ) until x 3. <p> Conditions are given in [4] on d 1 (), ` (), g (), and ~ d (; ) that result in a globally convergent, locally superlinear convergent algorithm. In <ref> [5] </ref>, the algorithm of [4] is refined for the case of unconstrained minimax problems. The major difference over the algorithm of [4] is that there is no need for d 1 . As in [4], ~ d is required to initialize superlinear convergence. <p> The quadratic program that yields ~ d involves only a subset of "active" functions, thus decreasing the number of function evaluations. The details are given below. The analysis in [3], [4], and <ref> [5] </ref> can be easily extended to these modified algorithms. Also obvious simplifications are introduced concerning linear constraints: the iterates are allowed (for inequality constraints) or are forced (for equality constraints) to stay on the boundary of these constraints and these constraints are not checked in the line search. <p> cnmad (nparam,j,x,gj,cd) int nparam,j; double *x,*gj; double *cd; - double ss; ss=cd [1]; switch (j) - case 1: *gj=ss-x [0]; break; case 2: *gj=ss+x [0]-x [1]; break; case 3: *gj=ss+x [1]-x [2]; break; case 4: *gj=ss+x [2]-x [3]; break; 41 case 5: *gj=ss+x [3]-x [4]; break; case 6: *gj=ss+x [4]-x <ref> [5] </ref>; break; case 7: *gj=ss+x [5]-3.5e0; break; - return; - After running the first algorithm on a SUN 4/SPARC station 1, the following output is obtained (the results for the set of objectives have been deleted to save space) CFSQP Version 2.5 (Released April 1997) Copyright (c) 1993 --- 1997 C.T.
Reference: [6] <author> J.L. Zhou & A.L. </author> <title> Tits, "An SQP Algorithm for Finely Discretized Continuous Minimax Problems and Other Minimax Problems with Many Objective Functions," </title> <note> SIAM J. on Optimization 6 (1996) , 461-487. </note>
Reference-contexts: Thus, CFSQP solves the original problem with nonlinear equality constraints by solving a modified optimization problem with only linear constraints and nonlinear inequality constraints. For the transformed problem, it implements algorithms that are described and analyzed in [3], [4], [5], <ref> [6] </ref> and [7], with some additional refinements. These algorithms are based on a Sequential Quadratic Programming (SQP) iteration, modified so as to generate feasible iterates. The merit function is the objective function. <p> The method of selecting appropriate subsets of the sequentially related constraints and objectives is outlined and analyzed in <ref> [6] </ref> and [7]. <p> For simplicity of exposition, we describe the algorithms as they pertain to solving problem (P ), deferring the discussion of the algorithm described in <ref> [6] </ref> and [7] for the solution of (P sr ) until x 3. <p> The algorithm employed is described and analyzed in <ref> [6] </ref> and [7]. Below we describe the algorithm as implemented in CFSQP, omitting "isolated" objectives and constraints for simplicity of exposition. <p> Specifically, for some * &gt; 0, we consider the "*-active left local maximizers" at x k , which, for the objectives, we shall denote llm * . Using the notation and definitions from <ref> [6] </ref>, a discretization point ! i 2 := f! 1 ; : : : ; ! jj g is called *-active if it is in the set We call the point a left-local maximizer if it satisfies one of the following three conditions: (i) i 2 f2; : : : ;
Reference: [7] <author> C.T. Lawrence & A.L. </author> <title> Tits, "Feasible Sequential Quadratic Programming for Finely Dis-cretized Problems from SIP," in Semi-Infinite Programming, in the series Nonconvex Optimization and its Applications, </title> <editor> R. Reemtsen & J.-J. Ruckmann, eds., </editor> <publisher> Kluwer Academic Publishers, </publisher> <year> 1997, </year> <note> to appear. 64 </note>
Reference-contexts: Thus, CFSQP solves the original problem with nonlinear equality constraints by solving a modified optimization problem with only linear constraints and nonlinear inequality constraints. For the transformed problem, it implements algorithms that are described and analyzed in [3], [4], [5], [6] and <ref> [7] </ref>, with some additional refinements. These algorithms are based on a Sequential Quadratic Programming (SQP) iteration, modified so as to generate feasible iterates. The merit function is the objective function. <p> The method of selecting appropriate subsets of the sequentially related constraints and objectives is outlined and analyzed in [6] and <ref> [7] </ref>. <p> For simplicity of exposition, we describe the algorithms as they pertain to solving problem (P ), deferring the discussion of the algorithm described in [6] and <ref> [7] </ref> for the solution of (P sr ) until x 3. <p> The algorithm employed is described and analyzed in [6] and <ref> [7] </ref>. Below we describe the algorithm as implemented in CFSQP, omitting "isolated" objectives and constraints for simplicity of exposition.
Reference: [8] <author> L. Grippo, F. Lampariello & S. Lucidi, </author> <title> "A Nonmonotone Line Search Technique for Newton's Method," </title> <journal> SIAM J. Numer. Anal. </journal> <month> 23 </month> <year> (1986) </year> <month> , 707-716. </month>
Reference-contexts: After obtaining feasibility, either (i) an Armijo-type line search may be used, yielding a monotone decrease of the objective function at each iteration [3]; or (ii) a nonmonotone line search (inspired from <ref> [8] </ref> and analyzed in [4] and [5] in the present context) may be selected, forcing a decrease of the objective function within at most four iterations.
Reference: [9] <author> K. Schittkowski, </author> <title> QLD : A FORTRAN Code for Quadratic Programming, User's Guide, </title> <institution> Mathematisches Institut, Universitat Bayreuth, Germany, </institution> <year> 1986. </year>
Reference-contexts: The method of selecting appropriate subsets of the sequentially related constraints and objectives is outlined and analyzed in [6] and [7]. For the solution of the quadratic programming subproblems, CFSQP is set up to call a C version of QLD <ref> [9] </ref>, converted from Fortran via f2c (see [10]) and provided with the CFSQP distribution for the user's convenience. 2 Description of the Basic Algorithms The algorithms described and analyzed in [3], [4], [2], and [5] are as follows. <p> [6]=0.e0; gradfj [7]=0.e0; gradfj [8]=0.e0; gradfj <ref> [9] </ref>=1.e0; return; - 53 void cntr (nparam,j,x,gj,cd) int nparam,j; double *x,*gj; void *cd; - double t,z,s1,s2; int k; if (j&lt;=r) t=3.14159265e0*(j-1)*0.025e0; else - if (j&lt;=2*r) t=3.14159265e0*(j-1-r)*0.025e0; else t=3.14159265e0*(1.2e0+(j-1-2*r)*0.2e0)*0.25e0; - s1=s1+x [k-1]*cos (k*t); - if (j&lt;=r) *gj=(1.e0-x [9])*(1.e0-x [9])-z; else - if (j&lt;=2*r) *gj=z-(1.e0+x [9])*(1.e0+x [9]); else *gj=z-x [9]*x [9]; - return; - We should mention that this problem is apparently very sensitive to the choice of the parameter udelta, and on some platforms (in particular, the HP170) the program does not terminate succesfully unless udelta is increased by the user. <p> [8]=0.e0; gradfj <ref> [9] </ref>=1.e0; return; - 53 void cntr (nparam,j,x,gj,cd) int nparam,j; double *x,*gj; void *cd; - double t,z,s1,s2; int k; if (j&lt;=r) t=3.14159265e0*(j-1)*0.025e0; else - if (j&lt;=2*r) t=3.14159265e0*(j-1-r)*0.025e0; else t=3.14159265e0*(1.2e0+(j-1-2*r)*0.2e0)*0.25e0; - s1=s1+x [k-1]*cos (k*t); - if (j&lt;=r) *gj=(1.e0-x [9])*(1.e0-x [9])-z; else - if (j&lt;=2*r) *gj=z-(1.e0+x [9])*(1.e0+x [9]); else *gj=z-x [9]*x [9]; - return; - We should mention that this problem is apparently very sensitive to the choice of the parameter udelta, and on some platforms (in particular, the HP170) the program does not terminate succesfully unless udelta is increased by the user.
Reference: [10] <author> S. I. Feldman, D. M. Gay, M. W. Maimone & N. L. Schryer, </author> <title> "A Fortran to C Converter," </title> <institution> AT & T Bell Laboratories, </institution> <note> Computer Science Technical Report No. 149, </note> <year> 1990. </year>
Reference-contexts: The method of selecting appropriate subsets of the sequentially related constraints and objectives is outlined and analyzed in [6] and [7]. For the solution of the quadratic programming subproblems, CFSQP is set up to call a C version of QLD [9], converted from Fortran via f2c (see <ref> [10] </ref>) and provided with the CFSQP distribution for the user's convenience. 2 Description of the Basic Algorithms The algorithms described and analyzed in [3], [4], [2], and [5] are as follows.
Reference: [11] <author> M.J.D. Powell, </author> <title> "A Fast Algorithm for Nonlinearly Constrained Optimization Calculations," in Numerical Analysis, Dundee, </title> <booktitle> 1977, Lecture Notes in Mathematics 630, </booktitle> <address> G.A. Watson, </address> <publisher> ed., Springer-Verlag, </publisher> <year> 1978, </year> <pages> 144-157. </pages>
Reference-contexts: Updates. i. If nset &gt; 5n and t k &lt; t, set H k+1 = H 0 and nset = 0. Otherwise, set nset = nset + 1 and compute a new approximation H k+1 to the Hessian of the Lagrangian using the BFGS formula with Powell's modification <ref> [11] </ref>. ii. Set x k+1 = x k + t k d k + t 2 ~ d k . iii. <p> Updates. i. If nset &gt; 5n and t k &lt; t, set H k+1 = H 0 and nset = 0. Otherwise, set nset = nset + 1 and compute a new approximation H k+1 to the Hessian of the Lagrangian using the BFGS formula with Powell's modification <ref> [11] </ref>. ii. If kd 0 k k &gt; d, set C k+1 = maxf0:5C k ; Cg: Otherwise, if g j (x k + d ` k ) 0; j = 1; : : : ; n i , set C k+1 = C k . <p> Otherwise, set nset = nset + 1 and compute a new approximation H k+1 to the Hessian of the Lagrangian using the BFGS formula with Powell's modification <ref> [11] </ref>. Set x k+1 = x k + t k d k + t 2 ~ d k . Increase k by 1.
Reference: [12] <author> W. Hock & K. Schittkowski, </author> <title> Test Examples for Nonlinear Programming Codes, </title> <booktitle> Lecture Notes in Economics and Mathematical Systems (187), </booktitle> <publisher> Springer Verlag, </publisher> <year> 1981. </year>
Reference-contexts: functions of the same name): diagnl di1 dqp error estlam fool fuscmp indexs matrcp matrvc nullvc resign sbout1 sbout2 scaprd shift slope small element Finally, the following memory management utilities are used within CFSQP: make_dv free_dv convert make_iv free_iv make_dm free_dm 33 9 Examples The first problem is borrowed from <ref> [12] </ref> (Problem 32). It involves a single objective function, simple bounds on the variables, nonlinear inequality constraints, and linear equality constraints. <p> For this example, since only a small subset of the objectives were used to construct the QP subproblems at each iteration, algorithm FSQP-SR executed in significantly less time, and required far fewer gradient evaluations. Our third example is borrowed from <ref> [12] </ref> (Problem 71). <p> The results listed in the first three tables were all obtained without the use of the algorithm FSQP-SR. Table 1 contains results obtained for some non-minimax test problems from <ref> [12] </ref> (the same initial points as in [12] were selected). prob indicates the problem number as in [12], nparam the number of free variables, nineqn the number of nonlinear (inequality) constraints, ncallf the total number of evaluations of the objective function, ncallg the total number of evaluations of the (scalar) nonlinear <p> The results listed in the first three tables were all obtained without the use of the algorithm FSQP-SR. Table 1 contains results obtained for some non-minimax test problems from <ref> [12] </ref> (the same initial points as in [12] were selected). prob indicates the problem number as in [12], nparam the number of free variables, nineqn the number of nonlinear (inequality) constraints, ncallf the total number of evaluations of the objective function, ncallg the total number of evaluations of the (scalar) nonlinear constraint functions, iter the total number of <p> The results listed in the first three tables were all obtained without the use of the algorithm FSQP-SR. Table 1 contains results obtained for some non-minimax test problems from <ref> [12] </ref> (the same initial points as in [12] were selected). prob indicates the problem number as in [12], nparam the number of free variables, nineqn the number of nonlinear (inequality) constraints, ncallf the total number of evaluations of the objective function, ncallg the total number of evaluations of the (scalar) nonlinear constraint functions, iter the total number of iterations, objective the final value of the objective function, d0norm <p> In most cases, eps was selected so as to achieve the same field precision as in <ref> [12] </ref>. Whether 60 FSQP-AL (0) or FSQP-NL (1) is used is indicated in column "B". Results obtained on selected minimax problems are summarized in Table 2. <p> All of the above are either unconstrained or linearly constrained minimax problems. Unable to find nonlinearly constrained minimax test problems in the literature, we constructed problems p43m through p117m from problems 43, 84, 113 and 117 in <ref> [12] </ref> by removing certain constraints and including instead additional objectives of the form f i (x) = f (x) + ff i g i (x) where the ff i 's are positive scalars and g i (x) 0: Specifically, p43m is constructed from problem 43 by taking out the first two <p> Table 3 contains results of problems with nonlinear equality constraints from <ref> [12] </ref>. <p> as it is set to 10 4 for all of the problems except p46, where it was 5.E-3, and p27, where it was 1.E-3 (increased due to slow convergence). epseqn is the norm requirement on the values of the equality constraints and is chosen close to the corresponding values in <ref> [12] </ref>. It can be checked that the second order sufficient conditions of optimality are not satisfied at the known optimal solution for problems 26, 27, 46 and 47. Table 4 contains the results for problems with a set (or sets) of sequentially related contraints solved via the algorithm FSQP-SR.
Reference: [13] <author> K. Madsen & H. Schjr-Jacobsen, </author> <title> "Linearly Constrained Minimax Optimization," </title> <journal> Math. </journal> <note> Programming 14 (1978) , 208-223. </note>
Reference-contexts: feasible for inequality constraints and linear equality constraints: 1.00000000000000e-01 2.00000000000000e-01 objectives 37 constraints -1.99900000000000e+00 iteration 3 inform 0 x -9.86076131526265e-32 1.00000000000000e+00 objectives 1.00000000000000e+00 constraints -1.00000000000000e+00 d0norm 1.39452223873684e-31 ktnorm 1.06098265851897e-30 ncallf 3 ncallg 5 Normal termination: You have obtained a solution !! Our second example is taken from example 6 in <ref> [13] </ref>. We will use two methods to solve this problem. First we solve the problem considering all objective functions as isolated and unrelated. Next we will solve the problem considering the objectives as a single set of sequentially related functions. <p> from [16]; cb2, cb3, r-s, wong and colv are from [17; Examples 5.1-5] (more recent results on problems bard down to wong can be found in [18]); kiw1 and kiw4 are from [19] (results for kiw2 and kiw3 are not reported due to data disparity); mad1 to mad8 are from <ref> [13, Examples 1-8] </ref>; polk1 to polk4 are from [20]. Some of these test problems allow one to freely select the number of variables; problems wat6 and wat20 correspond to 6 and 20 variables respectively, and mad810, mad830 and mad850 to 10, 30 and 50 variables respectively.
Reference: [14] <author> K. Schittkowski, </author> <title> "Solving Nonlinear Programming Problems with Very Many Constraints," </title> <institution> Mathematisches Institut, Universitat Bayreuth, </institution> <note> Report No. 294, Bayreuth, Ger-many, </note> <year> 1991. </year>
Reference-contexts: feasible for inequality constraints and linear equality constraints: 1.00000000000000e+00 5.00000000000000e+00 objectives 1.60000000000000e+01 constraints 0.00000000000000e+00 iteration 7 inform 0 x 1.00000000000000e+00 3.82114999306416e+00 objectives 1.70140172891565e+01 constraints -4.05009359383257e-13 SNECV 4.05009359383257e-13 d0norm 1.12049770833730e-08 ktnorm 1.90658262018579e-08 ncallf 7 ncallg 30 Normal termination: You have obtained a solution !! 50 Our fourth example is borrowed from <ref> [14] </ref> (Problem TP374). <p> Problems cw 2 through cw 7 are borrowed from [15], hu 1 through hu 12 are from [21], hz 1 is from [22], oet 1 through oet 7 are from [23], pt 1 is from [24], and sch 3 is from <ref> [14] </ref>. Columns prob, B, ncallf, ncallg, iter, objective, and d0norm 61 are as before. ncsrl is the number of linear sequentially related constraint sets, while ncsrn is the number of nonlinear sequentially related constraint sets. <p> Once again, the norm requirement for the SQP direction, eps, was set to 10 4 for all problems. All problems except for sch u3, which is taken from <ref> [14] </ref>, are the same as the corresponding problems in Table 4, except that they are rewritten in minimax form.
Reference: [15] <author> I.D. Coope & G.A. Watson, </author> <title> "A Projected Lagrangian Algorithm for Semi-Infinite Programming," </title> <journal> Math. </journal> <note> Programming 32 (1985) , 337-356. </note>
Reference-contexts: 1.00000000000000e-01 1.00000000000000e-01 1.00000000000000e-01 1.00000000000000e-01 1.00000000000000e-01 objectives 1.00000000000000e+00 constraints -1.28661830051682e-04 -1.90000000000000e-01 iteration 26 inform 0 |Xi_k| 54 -2.50585946189374e-10 2.29104435357056e-10 -1.13048092092404e-11 -5.12778083977221e-11 5.00000029897489e-01 objectives 5.00000029897489e-01 constraints -2.93322754973957e-08 55 d0norm 2.99002061268652e-08 ktnorm 1.39205756738162e-08 ncallf 26 ncallg 12612 Normal termination: You have obtained a solution !! Our fifth and final example is borrowed from <ref> [15] </ref> (Problem 2) and is an example of a discretized semi-infinite program. <p> Table 4 contains the results for problems with a set (or sets) of sequentially related contraints solved via the algorithm FSQP-SR. All problems in this table are discretized semi-infinite programs. Problems cw 2 through cw 7 are borrowed from <ref> [15] </ref>, hu 1 through hu 12 are from [21], hz 1 is from [22], oet 1 through oet 7 are from [23], pt 1 is from [24], and sch 3 is from [14].
Reference: [16] <author> G.A. Watson, </author> <title> "The Minimax Solution of an Overdetermined System of Non-linear Equations," </title> <journal> J. Inst. Math. Appl. </journal> <month> 23 </month> <year> (1979) </year> <month> , 167-180. </month>
Reference-contexts: Whether 60 FSQP-AL (0) or FSQP-NL (1) is used is indicated in column "B". Results obtained on selected minimax problems are summarized in Table 2. Problems bard, davd2, f&r, hettich, and wats are from <ref> [16] </ref>; cb2, cb3, r-s, wong and colv are from [17; Examples 5.1-5] (more recent results on problems bard down to wong can be found in [18]); kiw1 and kiw4 are from [19] (results for kiw2 and kiw3 are not reported due to data disparity); mad1 to mad8 are from [13, Examples
Reference: [17] <author> C. Charalambous & A.R. Conn, </author> <title> "An Efficient Method to Solve the Minimax Problem Directly," </title> <journal> SIAM J. Numer. Anal. </journal> <month> 15 </month> <year> (1978) </year> <month> , 162-187. </month>
Reference-contexts: Whether 60 FSQP-AL (0) or FSQP-NL (1) is used is indicated in column "B". Results obtained on selected minimax problems are summarized in Table 2. Problems bard, davd2, f&r, hettich, and wats are from [16]; cb2, cb3, r-s, wong and colv are from <ref> [17; Examples 5.1-5] </ref> (more recent results on problems bard down to wong can be found in [18]); kiw1 and kiw4 are from [19] (results for kiw2 and kiw3 are not reported due to data disparity); mad1 to mad8 are from [13, Examples 1-8]; polk1 to polk4 are from [20].
Reference: [18] <author> A.R. Conn & Y. Li, </author> <title> "An Efficient Algorithm for Nonlinear Minimax Problems," </title> <institution> University of Waterloo, Research Report CS-88-41, Waterloo, </institution> <address> Ontario, N2L 3G1 Canada, </address> <month> November, </month> <year> 1989 </year> . 
Reference-contexts: Results obtained on selected minimax problems are summarized in Table 2. Problems bard, davd2, f&r, hettich, and wats are from [16]; cb2, cb3, r-s, wong and colv are from [17; Examples 5.1-5] (more recent results on problems bard down to wong can be found in <ref> [18] </ref>); kiw1 and kiw4 are from [19] (results for kiw2 and kiw3 are not reported due to data disparity); mad1 to mad8 are from [13, Examples 1-8]; polk1 to polk4 are from [20].
Reference: [19] <author> K.C. Kiwiel, </author> <title> Methods of Descent in Nondifferentiable Optimization, </title> <booktitle> Lecture Notes in Mathematics #1133, </booktitle> <publisher> Springer-Verlag, </publisher> <address> Berlin, Heidelberg, New-York, Tokyo, </address> <year> 1985. </year>
Reference-contexts: Problems bard, davd2, f&r, hettich, and wats are from [16]; cb2, cb3, r-s, wong and colv are from [17; Examples 5.1-5] (more recent results on problems bard down to wong can be found in [18]); kiw1 and kiw4 are from <ref> [19] </ref> (results for kiw2 and kiw3 are not reported due to data disparity); mad1 to mad8 are from [13, Examples 1-8]; polk1 to polk4 are from [20].
Reference: [20] <author> E. Polak, D.Q. Mayne & J.E. Higgins, </author> <title> "A Superlinearly Convergent Algorithm for Min-max Problems," </title> <booktitle> Proceedings of the 28th IEEE Conference on Decision and Control, </booktitle> <address> Tampa, Florida (December 1989) </address> . 
Reference-contexts: are from [17; Examples 5.1-5] (more recent results on problems bard down to wong can be found in [18]); kiw1 and kiw4 are from [19] (results for kiw2 and kiw3 are not reported due to data disparity); mad1 to mad8 are from [13, Examples 1-8]; polk1 to polk4 are from <ref> [20] </ref>. Some of these test problems allow one to freely select the number of variables; problems wat6 and wat20 correspond to 6 and 20 variables respectively, and mad810, mad830 and mad850 to 10, 30 and 50 variables respectively. All of the above are either unconstrained or linearly constrained minimax problems.
Reference: [21] <author> M. Huth, </author> <title> "Superlinear konvergente Verfahren zur Losung semi-infiniter Optimierungsauf-gaben," Padagog. Hochsch. </title> <type> Halle, Ph.D. dissertation, </type> <year> 1987 </year> . 
Reference-contexts: Table 4 contains the results for problems with a set (or sets) of sequentially related contraints solved via the algorithm FSQP-SR. All problems in this table are discretized semi-infinite programs. Problems cw 2 through cw 7 are borrowed from [15], hu 1 through hu 12 are from <ref> [21] </ref>, hz 1 is from [22], oet 1 through oet 7 are from [23], pt 1 is from [24], and sch 3 is from [14].
Reference: [22] <author> R. Hettich & P. Zencke, </author> <title> Numerische Methoden der Approximation und Semi-Infiniten Optimierung, </title> <publisher> Teubner Studienbucher, </publisher> <year> 1982. </year> <month> 65 </month>
Reference-contexts: All problems in this table are discretized semi-infinite programs. Problems cw 2 through cw 7 are borrowed from [15], hu 1 through hu 12 are from [21], hz 1 is from <ref> [22] </ref>, oet 1 through oet 7 are from [23], pt 1 is from [24], and sch 3 is from [14].
Reference: [23] <author> K. Oettershagen, </author> <title> Ein Superlinear Konvergenter Algorithmus zur Losung Semi-Infiniter Optimierungsprobleme, </title> <type> Ph.D. Thesis, </type> <institution> Bonn University, </institution> <year> 1982. </year>
Reference-contexts: All problems in this table are discretized semi-infinite programs. Problems cw 2 through cw 7 are borrowed from [15], hu 1 through hu 12 are from [21], hz 1 is from [22], oet 1 through oet 7 are from <ref> [23] </ref>, pt 1 is from [24], and sch 3 is from [14]. Columns prob, B, ncallf, ncallg, iter, objective, and d0norm 61 are as before. ncsrl is the number of linear sequentially related constraint sets, while ncsrn is the number of nonlinear sequentially related constraint sets.

References-found: 23

