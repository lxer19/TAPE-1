URL: ftp://ftp.cs.orst.edu/pub/tgd/papers/cairo.ps.gz
Refering-URL: http://www.cs.orst.edu/~tgd/cv/pubs.html
Root-URL: 
Title: Performance Comparison between Human Engineered Machine Learned Letter-to-Sound Rules for English: A Machine Learning Success Story.  
Author: Ghulum Bakiri P. O. Thomas G. Dietterich 
Address: Box 32038, Isa  Corvallis, OR 97331-3102  
Affiliation: Department of Computer Science University of Bahrain  Town State of Bahrain.  Department of Computer Science Oregon State University  
Abstract: The task of mapping spelled English words into strings of phonemes and stresses ("reading aloud") has many practical applications. Several commercial systems perform this task by applying a knowledge base of expert-supplied letter-to-sound rules. This paper presents a set of machine learning methods for automatically constructing letter-to-sound rules by analyzing a dictionary of words and their pronunciations. Our results demonstrate that these methods, taken together, provide a substantial performance improvement over the best commercial system|DECtalk from Digital Equipment Corporation|-that utilizes hand-crafted letter-to-sound rules. The results are significant since these machine learning techniques are general and can be applied to many other tasks including the task of discovering pronunciation rules in other languages, there-by eliminating the need for hand-crafting a rule-base for each language. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Abramson, N. </author> <year> (1963). </year> <title> Information theory and coding. </title> <publisher> McGraw-Hill Book Company, Inc. </publisher>
Reference: <author> Bakiri, G. </author> <year> (1991a). </year> <title> Converting English Text to Speech: A Machine Learning Approach. </title> <type> A PhD Thesis, </type> <institution> Oregon State University, </institution> <year> 1991. </year> <note> (Available through U.M.I, 300 N. </note> <editor> Zeeb Rd., </editor> <address> Ann Arbor, MI 48106, U.S.A.) </address> <note> Bakiri, </note> <author> G., & Dietterich, T. G. </author> <year> (1991). </year> <title> Boosting the Performance of Inductive Learning Programs via Error-Correcting Output Codes. </title> <booktitle> Proceedings of the Eighth International Conference on Machine Learning. </booktitle> <address> Anaheim Ca: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Bakiri, G., & Dietterich, T. G. </author> <year> (1993). </year> <title> Applying machine learning techniques to construct high-performance letter-to-sound rules for English. </title> <publisher> Forthcoming. </publisher>
Reference: <author> Bose, R. C., & Ray-Chaudhuri, D. K. </author> <year> (1960). </year> <title> On a class of error-correcting binary group codes. </title> <journal> Inf. Control, </journal> <volume> 3, </volume> <pages> pp. 68-79, </pages> <month> March </month> <year> 1960. </year>
Reference: <author> Breiman, L, Friedman, J. H., Olshen, R. A., and Stone, C. J. </author> <year> (1984). </year> <title> Classification and Regression Trees. </title> <address> Monterey, CA: </address> <note> Wadsworth and Brooks. 17 Church, </note> <author> K. W. </author> <year> (1985). </year> <title> Stress assignment in letter-to-sound rules for speech synthesis. </title> <booktitle> Proc. 23rd Meeting Assoc. Comp. Ling., </booktitle> <pages> 246-253. </pages>
Reference-contexts: For cases in which f takes only the values f0; 1g|binary functions|there are many algorithms available. For example, the decision tree methods, such as ID3 (Quinlan, 1 1986b) and CART <ref> (Breiman, Friedman, Olshen & Stone, 1984) </ref> can construct trees whose leaves are labelled with binary values. Most artificial neural network algorithms, such as the perceptron algorithm (Rosenblatt, 1958) and the error backpropagation (BP) algorithm (Rumelhart, Hinton & Williams, 1986), are best suited to learning binary functions.
Reference: <author> Cole, R. Muthusamy, Y. and Fanty, M. </author> <year> (1990). </year> <title> The ISOLET spoken letter database. </title> <type> Technical Report No. CSE 90-004. </type> <institution> Beaverton, OR: Oregon Graduate Institute of Science and Technology, Department of Computer Science and Engineering. </institution>
Reference: <author> Dietterich, T. G., Hild, H., Bakiri, G. </author> <title> (1990a) A comparative study of ID3 and backpropagation for English text-to-speech mapping. </title> <booktitle> Proceedings of the Seventh International Conference on Machine Learning (pp. </booktitle> <pages> 24-31). </pages> <address> Austin, TX: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Halle, M. & Keyser S. J. </author> <year> (1971). </year> <title> English stress: its form, its growth, and its role in verse. </title> <address> New York: </address> <publisher> Harper and Row. </publisher>
Reference: <author> Klatt, D. H. </author> <year> (1979). </year> <title> Synthesis by Rule of Consonant-Vowel Syllables. </title> <booktitle> In Speech Communications Group Working Papers, </booktitle> <address> Cambridge, </address> <institution> Massachusetts: Massachusetts Institute of Technology. </institution>
Reference: <author> Klatt, D. H., & Shipman, D. W. </author> <year> (1982). </year> <title> Letter-to-phoneme rules: A semi-automatic discovery procedure. </title> <journal> J. Acoust. Soc. Am. </journal> <volume> Suppl. 1 72, </volume> <month> S48. </month>
Reference: <author> Klatt, D. H. </author> <year> (1987). </year> <title> Review of text-to-speech conversion for English. </title> <journal> J. Acoust. Soc. Am., </journal> <volume> 82, (3), </volume> <pages> 737-793. </pages>
Reference: <author> LeCun, Y. Boser, B., Denker, J. S., Henderson, D. Howard, R. E., Hubbard, W., and Jackel, L. D. </author> <year> (1989). </year> <title> Backpropagation applied to handwritten zip code recognition. </title> <journal> Neural Computation, </journal> <volume> 1 (4), </volume> <pages> 541-551. </pages>
Reference-contexts: For example, in medical diagnosis, the function might map a description of a patient to one of k possible diseases. In digit recognition <ref> (LeCun, et al., 1989) </ref>, the function maps each hand-printed digit to one of k = 10 classes. (We will see shortly that our domain of interest the text-to-speech mapping domain falls under this category of Learning from Examples.) There are several methods for handling these "multiclass" learning problems.
Reference: <author> Lucassen, J. M. </author> <year> (1983). </year> <title> Discovering phonemic base forms automatically: An information theoretic approach. </title> <type> Research Report RC 9833. </type> <institution> Yorktown Heights, NY: IBM T. J. Watson Research Center. </institution>
Reference: <author> Lucassen, J. M., and Mercer, R. L. </author> <year> (1984). </year> <title> An information theoretic approach to the automatic determination of phonemic base forms. </title> <booktitle> Proc. Int. Conf. Acoust. Speech Signal Process. </booktitle> <address> ICASSP-84, 42.5.1-42.5.4. </address>
Reference: <author> Quinlan, J. R. </author> <year> (1983). </year> <title> Learning efficient classification procedures and their application to chess endgames, </title> <editor> in Michalski, R. S., Carbonell, J., and Mitchell, T. M., (eds.), </editor> <booktitle> Machine learning: An artificial intelligence approach, Vol. I, </booktitle> <address> Palo Alto: </address> <publisher> Tioga Press. </publisher> <pages> 463-482. </pages>
Reference-contexts: The basic operation of ID3 is quite similar to the CART algorithm developed by Breiman, Friedman, Olshen & Stone (1984) and to the tree-growing method developed by Lucassen & Mercer (1984). In our implementation of ID3, we did not employ windowing <ref> (Quinlan, 1983) </ref>, CHI-square forward pruning (Quinlan, 1986a), or any kind of reverse pruning (Quinlan, 1987). Experiments in Dietterich, Hild, and Bakiri (1990b) showed that these pruning methods did not improve performance.
Reference: <author> Quinlan, J. R. </author> <year> (1986a). </year> <title> The effect of noise on concept learning. </title> <editor> In Michalski, R. S., Car-bonell, J., and Mitchell, T. M., (eds.), </editor> <booktitle> Machine learning, Vol. II, </booktitle> <address> Palo Alto: </address> <publisher> Tioga Press. </publisher> <pages> 149-166. </pages>
Reference-contexts: The basic operation of ID3 is quite similar to the CART algorithm developed by Breiman, Friedman, Olshen & Stone (1984) and to the tree-growing method developed by Lucassen & Mercer (1984). In our implementation of ID3, we did not employ windowing (Quinlan, 1983), CHI-square forward pruning <ref> (Quinlan, 1986a) </ref>, or any kind of reverse pruning (Quinlan, 1987). Experiments in Dietterich, Hild, and Bakiri (1990b) showed that these pruning methods did not improve performance.
Reference: <author> Quinlan, J. R. </author> <year> (1986b). </year> <title> Induction of Decision Trees, </title> <journal> Machine Learning, </journal> <volume> 1(1), </volume> <pages> 81-106. </pages>
Reference-contexts: For cases in which f takes only the values f0; 1g|binary functions|there are many algorithms available. For example, the decision tree methods, such as ID3 <ref> (Quinlan, 1 1986b) </ref> and CART (Breiman, Friedman, Olshen & Stone, 1984) can construct trees whose leaves are labelled with binary values.
Reference: <author> Quinlan, J. R., </author> <year> (1987). </year> <title> Simplifying decision trees. </title> <journal> International Journal of Man-Machine Studies, </journal> <volume> 27, </volume> <pages> 221-234. </pages> <note> 18 Rosenblatt, </note> <author> F. </author> <year> (1958). </year> <title> The perceptron: a probabilistic model for information storage and organization in the brain. </title> <journal> Psychological Review, </journal> <volume> 65 (6), </volume> <pages> 386-408. </pages>
Reference-contexts: In our implementation of ID3, we did not employ windowing (Quinlan, 1983), CHI-square forward pruning (Quinlan, 1986a), or any kind of reverse pruning <ref> (Quinlan, 1987) </ref>. Experiments in Dietterich, Hild, and Bakiri (1990b) showed that these pruning methods did not improve performance.
Reference: <author> Rumelhart, D. E., Hinton, G. E., and Williams, R. J. </author> <year> (1986). </year> <title> Learning internal representations by error propagation. </title> <editor> In Rumelhart, D. E., and McClelland, J. L., (eds.) </editor> <booktitle> Parallel Distributed Processing, </booktitle> <volume> Vol 1. </volume> <pages> 318-362. </pages>
Reference-contexts: For example, the decision tree methods, such as ID3 (Quinlan, 1 1986b) and CART (Breiman, Friedman, Olshen & Stone, 1984) can construct trees whose leaves are labelled with binary values. Most artificial neural network algorithms, such as the perceptron algorithm (Rosenblatt, 1958) and the error backpropagation (BP) algorithm <ref> (Rumelhart, Hinton & Williams, 1986) </ref>, are best suited to learning binary functions. Theoretical studies of learning have focused almost entirely on learning binary functions (Valiant, 1984; COLT 1988, 1989, 1990).
Reference: <author> Sejnowski, T. J., and Rosenberg, C. R. </author> <year> (1987). </year> <title> Parallel networks that learn to pronouce English text. </title> <journal> Complex Systems, </journal> <volume> 1, </volume> <pages> 145-168. </pages>
Reference: <author> Shavlik, J. W., Mooney, R. J., and Towell, G. G. </author> <year> (1990). </year> <title> Symbolic and neural learning algorithms: An experimental comparison (revised). </title> <type> Technical Report 955. </type> <institution> Madison, WI: University of Wisconsin-Madison, Computer Sciences Department. </institution>
Reference: <author> Shannon, C. E. </author> <year> (1948). </year> <title> A mathematical theory of communication. </title> <journal> Bell Syst. J., </journal> <volume> 27, </volume> <pages> pp. </pages> <booktitle> 379-423 (part I), 623-656 (Part II), </booktitle> <month> July </month> <year> 1948. </year>
Reference: <author> Valiant, L. G. </author> <year> (1984). </year> <title> A theory of the learnable. </title> <journal> Communications of the ACM, </journal> <volume> 27, </volume> <pages> 1134-1142. </pages>
Reference: <author> Tony Vitali (1990). </author> <title> Personal communication (electronic mail) between Tony Vitali of Digital Equipment Corporation (DEC) and Tom Dietterich. </title> <type> 19 </type>
References-found: 24

