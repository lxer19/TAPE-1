URL: ftp://ftp.aic.nrl.navy.mil/pub/schultz/papers/robolearn96.ps.gz
Refering-URL: http://www.aic.nrl.navy.mil/~schultz/perception-action-learning.html
Root-URL: 
Email: schultz@aic.nrl.navy.mil  
Phone: (202) 767-2684, fax (202) 767-3172  
Title: RoboShepherd: Learning a complex behavior  
Author: Alan C. Schultz, John J. Grefenstette, and William Adams 
Address: (Code 5514)  DC 20375-5337, U.S.A.  
Affiliation: Navy Center for Applied Research in Artificial Intelligence  Naval Research Laboratory, Washington,  
Abstract: This paper reports on recent results using genetic algorithms to learn decision rules for complex robot behaviors. The method involves evaluating hypothetical rule sets on a simulator and applying simulated evolution to evolve more effective rules. The main contributions of this paper are (1) the task learned is a complex behavior involving multiple mobile robots, and (2) the learned rules are verified through experiments on operational mobile robots. The case study involves a shep-herding task in which one mobile robot attempts to guide another robot to a specified area. 
Abstract-found: 1
Intro-found: 1
Reference: [Dorigo, 1993] <author> Dorigo, M. </author> <year> (1993). </year> <title> "Genetic and Non-Genetic Operators in Alecsys," </title> <journal> Evolutionary Computation, </journal> <volume> 1(2): </volume> <pages> 151-164. </pages>
Reference-contexts: We start with designed parts, and with a specific decomposition of behaviors, and let the system learn the rules for each behavior. Although similar in that robot behaviors are being learned, Dorigo <ref> [Dorigo, 1993] </ref> takes an entirely different architectural approach to evolution, using classifier systems to learn behaviors. Here the entire population of the genetic algorithm is taken as the behavior. In our work, each individual in the population is a behavior, and the population consists of competing behaviors.
Reference: [Grefenstette, 1990] <author> Grefenstette, J. J., Ram-sey, Connie L., and Schultz, Alan C., </author> <year> (1990). </year> <title> "Learning sequential decision rules using simulation models and competition," </title> <journal> Machine Learning, </journal> <volume> 5(4), </volume> <pages> 355-381 </pages>
Reference-contexts: At a higher level of granularity, entire strategies compete with one another using a genetic algorithm. Previous papers on SAMUEL focused on the operations of the system in purely simulated environments <ref> [Grefenstette, 1990] </ref> [Grefenstette, 1991]. <p> Conflicts are resolved in favor of rules with higher strength. Rule strengths are updated based on rewards received after each training episode. See <ref> [Grefenstette, 1990] </ref> for further details. 4 Learning under simula tion The rule set for the shepherd is learned under simulation while the sheep's rule set is fixed.
Reference: [Grefenstette, 1991] <author> Grefenstette, J.J. </author> <year> (1991). </year> <title> "Lamarckian learning in multi-agent environments," </title> <booktitle> Proc. Fourth International Conference of Genetic Algorithms, </booktitle> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann, </publisher> <pages> 303-310. </pages>
Reference-contexts: At a higher level of granularity, entire strategies compete with one another using a genetic algorithm. Previous papers on SAMUEL focused on the operations of the system in purely simulated environments [Grefenstette, 1990] <ref> [Grefenstette, 1991] </ref>.
Reference: [Grefenstette, 1994] <author> Grefenstette, J.J. and A. C. </author> <title> Schultz (1994). "An evolutionary approach to learning in robots," Machine Learning Workshop on Robot Learning, </title> <address> New Brunswick, NJ. </address>
Reference-contexts: Previous papers on SAMUEL focused on the operations of the system in purely simulated environments [Grefenstette, 1990] [Grefenstette, 1991]. We have also previ ously reported on using this method to learn simple robot behaviors such as navigation and collision avoidance <ref> [Grefenstette, 1994] </ref> [Schultz, 1992] [Schultz, 1994]. 3.1 Representation Each stimulus-response rule consists of conditions that match against the current sensors of the robot, and an action that suggests a translation or steering velocity command to the robot based on the current situation. <p> The resulting rules are then placed on an actual robotic system for testing. The learning system, SAMUEL, uses genetic algorithms applied to symbolic rules. We have previously reported on using this method to learn simple robot behaviors such as navigation and collision avoidance <ref> [Grefenstette, 1994] </ref> [Schultz, 1992] [Schultz, 1994].
Reference: [Cliff, 1991] <author> Cliff, D., I. Harvey, and P. </author> <note> Husbands (1991). Cognitive Science Research Paper No. 318, </note> <institution> School of Cognitive and Computer Science, University of Sussex. </institution>
Reference-contexts: In addition, given similar starting conditions, the robots tend to succeed and fail in similar ways. 9 Related work Other approaches to evolutionary robotics have been used before. The work of (Har-vey, Cliff and Husbands) <ref> [Cliff, 1991] </ref> has concentrated on learning from the smallest detail up; for example, the perception system is learned as a neural network via evolutionary algorithms. Our approach differs in that we take an engineering view, and do not insist that all components must be learned.
Reference: [Harvey, 1993] <author> Harvey, Inman, P. Husbands, and D. </author> <title> Cliff (1993). "Issues in evolutionary robotics," From Animals to Animats 2 (Ed. </title> <editor> Meyer, Roitblat and Wilson), </editor> <address> 364-373, </address> <publisher> MIT Press: </publisher> <address> Cambridge. </address>
Reference-contexts: This issue has also been explored by others <ref> [Harvey, 1993] </ref>. As an example, we report on learning a shepherding task, in which one mobile robot seeks to guide another to a specified area. The behavior is learned under simulation. The resulting rules are then used to control an operational mobile robot. <p> In [Ramsey, 1990], the effect of noise and initial conditions between the simulation and real world are examined with respect to the robustness of the learned rules in the real world. Additional issues in the use of simulation models are also examined in <ref> [Harvey, 1993] </ref>. 10 Conclusions and future work This paper reports on an approach for learning behaviors for mobile robots by evaluating hypothetical rule sets on a simulator and applying simulated evolution to evolve more effective rules. The resulting rules are then placed on an actual robotic system for testing.
Reference: [Holland, 1975] <author> Holland, J. H. </author> <year> (1975). </year> <title> Adaptation in Natural and Artificial Systems. </title> <publisher> Univ. Michigan Press, </publisher> <address> Ann Arbor, </address> <year> 1975. </year>
Reference: [Ram, 1994] <author> Ram, Ashwin, R. Arkin, G. Boone, and M. </author> <title> Pearce (1994). "Using Genetic Algorithms to Learn Reactive Control Parameters for Autonomous Robotic Navigation," Adaptive Behavior, </title> <type> 2(3), </type> <year> 1994. </year>
Reference-contexts: Here the entire population of the genetic algorithm is taken as the behavior. In our work, each individual in the population is a behavior, and the population consists of competing behaviors. In the system GA-Robot by Ram Et. Al. <ref> [Ram, 1994] </ref>, a genetic algorithm with a floating point representation is used to optimize parameters that effect the behavior. In SAMUEL, the entire behavior is learned in a high-level stimulus-response language. Several research groups have examined the use of simulation as a viable way to learn in robotic tasks.
Reference: [Ramsey, 1990] <author> Ramsey, C. L., Alan C. Schultz and J. J. </author> <title> Grefenstette (1990). "Simulation-assisted learning by competition: Effects of noise differences between training model and target environment," </title> <booktitle> Proceedings of the Seventh International Conference on Machine Learning, </booktitle> <year> 1990, </year> <pages> 211-215, </pages> <address> Austin, TX: </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: In previous work, we have described how the noise and conditions in the simulation must be more varied than expected in the real world, and that under these conditions, learning will be slower, but more robust behaviors will be learned <ref> [Ramsey, 1990] </ref>. This issue has also been explored by others [Harvey, 1993]. As an example, we report on learning a shepherding task, in which one mobile robot seeks to guide another to a specified area. The behavior is learned under simulation. <p> In SAMUEL, the entire behavior is learned in a high-level stimulus-response language. Several research groups have examined the use of simulation as a viable way to learn in robotic tasks. In <ref> [Ramsey, 1990] </ref>, the effect of noise and initial conditions between the simulation and real world are examined with respect to the robustness of the learned rules in the real world.
Reference: [Schultz, 1992] <author> Schultz, Alan C. and Grefen-stette, John J. </author> <year> (1992). </year> <title> "Using a genetic algorithm to learn behaviors for autonomous vehicles," </title> <booktitle> Proceedings of the of the AIAA Guidance, Navigation and Control Conference, </booktitle> <address> Hilton Head, SC, </address> <month> August 10-12, </month> <year> 1992. </year>
Reference-contexts: Previous papers on SAMUEL focused on the operations of the system in purely simulated environments [Grefenstette, 1990] [Grefenstette, 1991]. We have also previ ously reported on using this method to learn simple robot behaviors such as navigation and collision avoidance [Grefenstette, 1994] <ref> [Schultz, 1992] </ref> [Schultz, 1994]. 3.1 Representation Each stimulus-response rule consists of conditions that match against the current sensors of the robot, and an action that suggests a translation or steering velocity command to the robot based on the current situation. <p> The resulting rules are then placed on an actual robotic system for testing. The learning system, SAMUEL, uses genetic algorithms applied to symbolic rules. We have previously reported on using this method to learn simple robot behaviors such as navigation and collision avoidance [Grefenstette, 1994] <ref> [Schultz, 1992] </ref> [Schultz, 1994].
Reference: [Schultz, 1994] <author> Schultz, Alan C. </author> <year> (1994). </year> <title> "Learning robot behaviors using genetic algorithms," </title> <booktitle> Intelligent Automation and Soft Computing: Trends in Research, Development, and Applications, v1, </booktitle> <editor> Mohammad Jamshidi and Charles Nguyen, editors, </editor> <booktitle> Proceedings of the First World Automation Congress (WAC '94) and Fifth International Symposium on Robotics and Manufacturing (ISRAM '94) Manufacturing, </booktitle> <pages> 607-612, </pages> <publisher> TSI Press: </publisher> <address> Albuquerque. </address>
Reference-contexts: Previous papers on SAMUEL focused on the operations of the system in purely simulated environments [Grefenstette, 1990] [Grefenstette, 1991]. We have also previ ously reported on using this method to learn simple robot behaviors such as navigation and collision avoidance [Grefenstette, 1994] [Schultz, 1992] <ref> [Schultz, 1994] </ref>. 3.1 Representation Each stimulus-response rule consists of conditions that match against the current sensors of the robot, and an action that suggests a translation or steering velocity command to the robot based on the current situation. The condition and action values are described in more detail below. <p> The resulting rules are then placed on an actual robotic system for testing. The learning system, SAMUEL, uses genetic algorithms applied to symbolic rules. We have previously reported on using this method to learn simple robot behaviors such as navigation and collision avoidance [Grefenstette, 1994] [Schultz, 1992] <ref> [Schultz, 1994] </ref>.
References-found: 11

