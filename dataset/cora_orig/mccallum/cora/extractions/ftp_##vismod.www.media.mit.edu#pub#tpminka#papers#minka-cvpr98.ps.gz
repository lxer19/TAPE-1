URL: ftp://vismod.www.media.mit.edu/pub/tpminka/papers/minka-cvpr98.ps.gz
Refering-URL: http://www.media.mit.edu/~tpminka/papers/databases.html
Root-URL: http://www.media.mit.edu
Title: An Optimized Interaction Strategy for Bayesian Relevance Feedback experiments with real users on a database
Author: Ingemar J. Cox, Matthew L. Miller, Thomas P. Minka, Peter N. Yianilos 
Note: where a &lt; 1 depends on the structure of the database. This theoretical advantage is reflected by  
Address: 4 Independence Way Princeton, NJ 08540  
Affiliation: NEC Research Institute  
Abstract: A new algorithm and systematic evaluation is presented for searching a database via relevance feedback. It represents a new image display strategy for the PicHunter system [2, 1]. The algorithm takes feedback in the form of relative judgments (item A is more relevant than item B) as opposed to the stronger assumption of categorical relevance judgments (item A is relevant but item B is not). It also exploits a learned probabilistic model of human behavior to make better use of the feedback it obtains. The algorithm can be viewed as an extension of indexing schemes like the k-d tree to a stochastic setting, hence the name stochastic-comparison search. In simulations, the amount of feedback required for the new algorithm scales like log 2 jDj, where jDj is the size of the database, while a simple query-by-example approach scales like jDj a
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> I. J. Cox, J. Ghosn, M. L. Miller, T. V. Papathomas, and P. N. Yian-ilos. </author> <title> Hidden annotation in content based image retrieval. </title> <booktitle> In Proc IEEE Workshop on Content-Based Access of Image and Video Libraries, </booktitle> <pages> pages 7681, </pages> <year> 1997. </year>
Reference-contexts: This kind of input still can be used effectively without a notion of category or when no retrieved items are in the same category as the target. Our retrieval system, PicHunter <ref> [2, 1] </ref>, uses a minimally sufficient interface for this kind of interaction (figure 1). A set of n images is displayed. After selecting zero or more images, the user calls up another set of images by hitting the GO button. <p> A complete representation for the information received is a probability distribution over possible targets, which is what this paper uses. The initial query, if any, provides the starting condition for this distribution, which may be multi-modal or have other complex structure. This scenario was also considered in earlier papers <ref> [2, 1] </ref>. The new observation here is that once we decide on (1) target testing, (2) relative judgments, and (3) representing the full target distribution, database retrieval reduces to the specific computer science problem of comparison searching. <p> Ties are resolved randomly. This algorithm was used in previous work <ref> [2, 1] </ref>. Sampling Sample X 1 ::X n from the distribution p (T = X ). Query by Example Let X 1 ::X n be the n closest items to the winner of the last comparison. This is a favorite approach in systems without relevance feedback [3]. <p> This yielded 3,681 samples over 9 different subjects. The system assumed independent comparisons (p indep ) generated by p sigmoid . The distance metric was a weighted sum of distances based on the 18 image features in [2] plus keyword annotations <ref> [1] </ref>. The parameter and the mixing weights were chosen to approximately maximize the likelihood of the training trials. A disjoint set of six subjects was used for testing.
Reference: [2] <author> I. J. Cox, M. L. Miller, S. M. Omohundro, and P. N. Yianilos. Pichunter: </author> <title> Bayesian relevance feedback for image retrieval. </title> <booktitle> In Int. Conf. on Pattern Recognition, </booktitle> <pages> pages 361369, </pages> <year> 1996. </year>
Reference-contexts: A more quantitative approach is to count how much feedback is required to achieve a well-defined level of retrieval accuracy. The desired level of accuracy used in this paper is simple: find a particular target item. This target testing measure, introduced in <ref> [2] </ref>, is unambiguous, simple and robust. For example, having a large collection of eagle images will make it easy to retrieve random eagle images, but it will remain difficult to find a particular eagle image on the basis of relevance feedback. <p> This kind of input still can be used effectively without a notion of category or when no retrieved items are in the same category as the target. Our retrieval system, PicHunter <ref> [2, 1] </ref>, uses a minimally sufficient interface for this kind of interaction (figure 1). A set of n images is displayed. After selecting zero or more images, the user calls up another set of images by hitting the GO button. <p> A complete representation for the information received is a probability distribution over possible targets, which is what this paper uses. The initial query, if any, provides the starting condition for this distribution, which may be multi-modal or have other complex structure. This scenario was also considered in earlier papers <ref> [2, 1] </ref>. The new observation here is that once we decide on (1) target testing, (2) relative judgments, and (3) representing the full target distribution, database retrieval reduces to the specific computer science problem of comparison searching. <p> Section 4 again contrasts the PicHunter approach with repeated query-by-example, this time with real users searching a database of natural scenes. 2 Multidimensional comparison searching This section presents a new derivation of the algorithm in <ref> [2] </ref> that better motivates the new display strategy. It is based on a generalization of existing algorithms for comparison searching, so that they can accommodate arbitrary kinds of comparisons, stochastic comparison outcomes, and comparisons made in parallel. <p> Ties are resolved randomly. This algorithm was used in previous work <ref> [2, 1] </ref>. Sampling Sample X 1 ::X n from the distribution p (T = X ). Query by Example Let X 1 ::X n be the n closest items to the winner of the last comparison. This is a favorite approach in systems without relevance feedback [3]. <p> This yielded 3,681 samples over 9 different subjects. The system assumed independent comparisons (p indep ) generated by p sigmoid . The distance metric was a weighted sum of distances based on the 18 image features in <ref> [2] </ref> plus keyword annotations [1]. The parameter and the mixing weights were chosen to approximately maximize the likelihood of the training trials. A disjoint set of six subjects was used for testing.
Reference: [3] <author> M. Flickner, H. Sawhney, W. Niblack, J. Ashley, Q. Huang, B. Dom, M. Gorkani, J. Hafner, D. Lee, D. Petkovic, D. Steele, and P. Yanker. </author> <title> Query by image and video content: The QBIC system. </title> <journal> IEEE Computer, </journal> <volume> 28(9):2332, </volume> <year> 1995. </year>
Reference-contexts: Sampling Sample X 1 ::X n from the distribution p (T = X ). Query by Example Let X 1 ::X n be the n closest items to the winner of the last comparison. This is a favorite approach in systems without relevance feedback <ref> [3] </ref>. It does not exploit previous comparisons or a model of nondeterminism. The idea is to simulate a user's responses by sampling from the stochastic comparison model. The database is synthetic, consisting of points uniformly-distributed inside the unit square. This allows databases of varying sizes to be easily drawn.
Reference: [4] <author> Yoav Freund, H. Sebastian Seung, Eli Shamir, and Naftali Tishby. </author> <title> Selective sampling using the query by committee algorithm. </title> <booktitle> In Advances in Neural Information Processing Systems, </booktitle> <address> Cambridge, MA, 1993. </address> <publisher> MIT Press. </publisher>
Reference-contexts: The general idea of maximizing the expected information from a query has also been pursued in the machine learning literature under the name Active Learning or Learning with Queries <ref> [4] </ref>. Active learning techniques have been shown to significantly outperform simple probability ranking for document classification [5]. We know of no application of active learning techniques to database retrieval. 2.3 Simultaneous comparisons Bringing the problem even closer to reality, suppose multiple comparisons can be made at once.
Reference: [5] <author> David D. Lewis and William A. Gale. </author> <title> A sequential algorithm for training text classifiers. </title> <editor> In W. Bruce Croft and C. J. van Rijsber-gen, editors, </editor> <booktitle> Proc. of ACM-SIGIR Conf. on R&D in Information Retrieval, </booktitle> <address> Dublin, Ireland, July 1994. </address> <publisher> Springer-Verlag. </publisher>
Reference-contexts: The general idea of maximizing the expected information from a query has also been pursued in the machine learning literature under the name Active Learning or Learning with Queries [4]. Active learning techniques have been shown to significantly outperform simple probability ranking for document classification <ref> [5] </ref>. We know of no application of active learning techniques to database retrieval. 2.3 Simultaneous comparisons Bringing the problem even closer to reality, suppose multiple comparisons can be made at once. This is readily handled by changing the comparison model.
Reference: [6] <author> T. P. Minka and R. W. </author> <title> Picard. Interactive learning using a society of models. </title> <journal> Pattern Recognition, </journal> <volume> 30(4), </volume> <year> 1997. </year>
Reference-contexts: The second step is to determine what information the user can still provide, given that the query language has proven inadequate. Many systems use categorical feedback, where the user indicates what items are in the same category of the target <ref> [6, 11] </ref>. However, this forces the user to decide on a useful category: one that is not too large or too small and contains the target. This is a drawback in the applications we envision, where the user has little familiarity with the content of the database.
Reference: [7] <author> K. V. S. Murthy. </author> <title> On Growing Better Decision Trees from Data. </title> <type> PhD thesis, </type> <institution> Johns Hopkins University, </institution> <year> 1995. </year>
Reference-contexts: The resulting tree, when used with Euclidean distance, is a recursive binary subdivision of the representation space using half-planes. This is similar to oblique classification trees <ref> [7] </ref>, except search time is being minimized, not classification error. 2.1 Nondeterminism In the retrieval application we envision, a human will be carrying out the comparisons. This requires that the algorithm be able to cope with an imperfectly modeled, possibly nondeterministic comparison operation. <p> Again the sum is over the entire database. This offers an alternative interpretation of minimizing future cost: maximizing immediate information gain. This heuristic has also been used to design classification trees <ref> [7] </ref>. The remaining question is how to update p (T = X) after the outcome of a comparison d (X 1 ; T ) &lt; d (X 2 ; T ).
Reference: [8] <author> T. V. Papathomas, T. E. Conway, I. J. Cox, J. Ghosn, M. L. Miller, T. P. Minka, and P. N. Yianilos. </author> <title> Psychophysical studies of the performance of an image database retrieval system. </title> <booktitle> In IS&T/SPIE Symposium on Electronic Imaging: Science and Technology, Conference on Human Vision and Electronic Imaging III, </booktitle> <year> 1998. </year>
Reference-contexts: The entropy-minimization strategy was not implemented at the time so the Most Probable strategy (section 3) was used instead. The Query-by-Example strategy required 33 inputs in average, while the Most Probable strategy required 25. The full experiment is described in <ref> [8] </ref>. Based on the results of the previous section, the performance difference is expected to increase with a bigger database and more accurate user model. As this paper went to press, a preliminary experiment was run with two subjects which placed entropy-minimization ahead of both of these two strategies.
Reference: [9] <author> A. Pelc. </author> <title> Searching with known error probability. </title> <booktitle> Theoretical Computer Science, </booktitle> <address> 63:185202, </address> <year> 1989. </year>
Reference-contexts: The algorithm in [10] assumes that the number of errors has a known bound. Nevertheless, their algorithm is similar to the one presented here, in the sense that it minimizes at each step an information-theoretic bound on the number of future comparisons. The algorithm in <ref> [9] </ref> allows errors to occur at random but requires them to be independent of the comparison and the target and furthermore does not guarantee that the target is found. So while both of these algorithms run in provably logarithmic time, they also operate under more restrictive conditions than stochastic-comparison search.
Reference: [10] <author> R. L. Rivest, A. R. Meyer, D. J. Kleitman, K. Winklmann, and J. Spencer. </author> <title> Coping with errors in binary search procedures. </title> <journal> Journal of Computer and System Sciences, </journal> <volume> 20:396404, </volume> <year> 1980. </year>
Reference-contexts: The parameter can therefore be interpreted as the degree of precision in the distance measurements. 2.2 Related work Comparison searching with errors has also been studied in the theoretical computer science literature. The algorithm in <ref> [10] </ref> assumes that the number of errors has a known bound. Nevertheless, their algorithm is similar to the one presented here, in the sense that it minimizes at each step an information-theoretic bound on the number of future comparisons.
Reference: [11] <author> Y. Rui, T. S. Huang, and S. Mehrotra. </author> <title> Content-based image retrieval with relevance feedback in MARS. </title> <booktitle> In Proc. of IEEE Int. Conf. on Image Processing, </booktitle> <address> Santa Barbara, CA, </address> <month> October </month> <year> 1997. </year>
Reference-contexts: The second step is to determine what information the user can still provide, given that the query language has proven inadequate. Many systems use categorical feedback, where the user indicates what items are in the same category of the target <ref> [6, 11] </ref>. However, this forces the user to decide on a useful category: one that is not too large or too small and contains the target. This is a drawback in the applications we envision, where the user has little familiarity with the content of the database.
Reference: [12] <author> P. N. Yianilos. </author> <title> Data structures and algorithms for nearest neighbor search in general metric spaces. </title> <booktitle> In Proceedings of the Fifth Annual ACM-SIAM Symposium on Discrete Algorithms (SODA), </booktitle> <year> 1993. </year>
Reference-contexts: The familiar binary search algorithm is a one-dimensional special case, where comparisons are of the form X &lt; T ?. For the more general case, the vantage-point tree has recently been proposed <ref> [12] </ref>. It is a variant of the k-d tree which is based on thresholds of distance measurements, i.e. the comparison d (X; T ) &lt; t?. <p> Optimizing this criterion over X is difficult when the distance measure is arbitrary, so a Monte Carlo approach was used: sample several random vantage points and choose the one which minimizes the lookahead cost. For generically distributed data, this greedy algorithm has been shown to perform near-perfect <ref> [12] </ref>. The comparison d (X 1 ; T ) &lt; d (X 2 ; T )?, i.e. is X 1 or X 2 closer to the target?, is more relevant for the relevance feedback scenario.
References-found: 12

