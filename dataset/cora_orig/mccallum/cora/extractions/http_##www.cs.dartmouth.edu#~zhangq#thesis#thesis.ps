URL: http://www.cs.dartmouth.edu/~zhangq/thesis/thesis.ps
Refering-URL: http://www.cs.dartmouth.edu/~zhangq/thesis.html
Root-URL: http://www.cs.dartmouth.edu
Title: Document Image Compression Via Pattern Matching  
Author: Qin Zhang Henry Baird Dennis Healy Geoff Davis Neal Young Roger D. Sloboda Dean 
Degree: A Thesis Submitted to the Faculty in partial fulfillment of the requirements for the degree of Doctor of Philosophy in Computer Science by  Examining Committee: (chairman) John Danskin  
Date: 9 September 1997  
Address: Hanover, New Hampshire  
Affiliation: DARTMOUTH COLLEGE  of Graduate Studies  
Abstract-found: 0
Intro-found: 1
Reference: [AB91] <author> J.C. Anigbogu A. Belaid. </author> <title> Text recognition using stochastic models. </title> <booktitle> In Proceedings of the fifth international symposium on applied stochastic models and data analysis, </booktitle> <address> Granada, Spain, </address> <month> April </month> <year> 1991. </year>
Reference-contexts: Ho et al. used multi-level contextual knowledge for recognition of degraded and connected character strings [THS91]. People have also applied Hidden Markov Models (HMM) and the Viterbi algorithm [GF73] to the problem of robust machine recognition of poorly print text <ref> [RS79, AB91] </ref>. The transition probability between character models is usually determined from the statistical studies of the type of text material the recognizer is expected to be used on.
Reference: [AN74] <author> R. N. Ascher and G. Nagy. </author> <title> A means for achieving a high degree of compaction on scan-digitized printed text. </title> <journal> In IEEE Transactions on Computers, </journal> <volume> c-23(11), </volume> <pages> pages 1174-1179, </pages> <year> 1974. </year>
Reference-contexts: Since these methods exploit only pixel-level structure of the image, they fail to provide good compression for document images. In contract, pattern-matching based compression (PMBC) techniques exploit the glyph-level structure of document images. Pattern-matching based compression techniques are effective for text pages. These techniques were originally proposed in <ref> [AN74] </ref> and further studied in [HX86], [Moh82], and [WBEI94]. [WBEI94] and [Moh84] discuss lossless compression based on pattern matching. The International Standards Organization (ISO) is in the process of adopting a new JBIG2 bi-level image coding standard, for which the leading proposal is also pattern matching based [How96]. <p> A PMBC system usually compresses these three components to get further compression. Below is a brief overview of the previous PMBC systems. Ascher and Nagy first proposed document image compression based on pattern matching <ref> [AN74] </ref>. Their major contribution was laying out the overall framework for 18 PMBC. Their system used a 2-dimensional 10 fi 15 binary array to represent proto-type glyphs in the codebook and fixed-length codes, which encode symbols uniformly without any compression, to represent glyph indices and positions, achieving about 16:1 compression. <p> One improved version of this algorithm called "weighted and-not" (WAN) distinguishes black-to-white errors from white-to-black errors [HX86]. These algorithms tend to have a high rejection rate if the threshold function is set to eliminate symbol mismatches. Ascher and Nagy <ref> [AN74] </ref>, Mohiuddin [Moh84], and more recently Howard [How96] used similar weighted global criteria based pattern matching methods. * In other algorithms, such as the Pattern Matching and Substitution (PMS) [JSC83] and Combined Size Independent Strategy (CSIS), the matching decision is based on local criteria [Hal88]. <p> We regard this as no compression at all. Ascher and Nagy proposed the use of zero-order statistics and Huffman codes for glyph indices <ref> [AN74] </ref>. Huffman codes yield the best compression possible for a prefix-free code for the given probability distribution, but zero-order statistics do not yield 111 very good compression. Witten et al. used the PPM text compression scheme for glyph indices [WBEI94]. This scheme uses variable order models and arithmetic coding. <p> Previous work in this area only partially exploit the geometrical structure. They went as far as coding the glyph offsets, but these offset values include inter-word spaces and line-breaks, which make the offsets harder to predict and compress. Ascher and Nagy <ref> [AN74] </ref> used fixed length codes for absolute glyph positions. They proposed but did not implement coding the offset from the previous glyph. <p> Document images contain four types of structure: pixel-level structure, glyph-level structure, geometrical structure, and contextual structure. Pattern matching based coding techniques exploit these structures of document images. Pattern matching based compression has been studied by several research groups <ref> [AN74, HX86, Moh84, WBEI94] </ref> and ISO is in the process of adopt 146 ing a new JBIG2 bi-level image-coding standard of which the leading proposal is also pattern matching based [How96]. We have discussed the details of a PMBC system. The following is a review of the basic steps: 1. <p> Our method reduces the size of the compressed documents by from 50% to 80%. 13.2 Previous Work Previous Pattern Matching Based Compression systems have used proprietary file formats requiring special decompression programs which must be distributed to each user, allowing these users to view and print the compressed documents <ref> [AN74, HX86, Moh84, WBEI94, How96, ZD95] </ref>. Because there are several groups working on PMBC systems, no standard file format has been established. 148 PostScript is the standard page description language for desktop publishing. There are several image conversion utility programs that convert scanned images into PostScript format.
Reference: [Arp69] <author> R. B. </author> <title> Arps. Entropy of printed matter at the threshold of legibility for efficient coding in digital image processing. </title> <type> PhD thesis, </type> <institution> Standford University, Stanford, </institution> <address> CA, </address> <year> 1969. </year>
Reference-contexts: First the new context contains non-causal pixels from the matching glyph. Arps had investigated the use of different templates for compression of images using context-based predictive encoding, and found that the templates that include non-causal pixels are better than those consisting only of causal elements <ref> [Arp69] </ref>. This result was rediscovered by Moffat [Mof91]. Moffat called the templates that include non-causal pixels, clairvoyant contexts, because pixel values ahead of the current one will not be known by the decoder until after they have been decoded, making them useless for conventional context-based compression.
Reference: [Bai88] <author> Henry Baird. </author> <title> Global-to-local layout analysis. </title> <booktitle> In Proc. IAPR Workshop on Syntactic and Structural Pattern Recognition, Pont-a-Mousson, </booktitle> <address> France, </address> <month> Sept. </month> <year> 1988. </year>
Reference-contexts: OCR usually has a geometric layout analysis or segmentation stage before actual symbol recognition. This stage usually does connected components analysis, skew correction, and segmentation. Researchers in the OCR community have proposed many segmentation and glyph extraction methods <ref> [CHP96, Bai88] </ref>. For a survey of these segmentation methods see 30 [FNK92, TZAK91]. Some of these methods are robust to skew error [CHP96, Bai88]. For our work, we assume the document is free of skewing. <p> This stage usually does connected components analysis, skew correction, and segmentation. Researchers in the OCR community have proposed many segmentation and glyph extraction methods <ref> [CHP96, Bai88] </ref>. For a survey of these segmentation methods see 30 [FNK92, TZAK91]. Some of these methods are robust to skew error [CHP96, Bai88]. For our work, we assume the document is free of skewing.
Reference: [BCW90] <author> Timonthy C. Bell, John G. Cleary, and Ian H. Witten. </author> <title> Text Compression. </title> <publisher> Prentice Hall, </publisher> <address> Englewood Cliffs, NJ, </address> <year> 1990. </year>
Reference-contexts: An arithmetic coder is used to optimally encode the pixels given the probabilities. Arithmetic coding is a technique that can code symbols arbitrarily close to the entropy using a given model <ref> [BCW90] </ref>. the pixels that follow the current pixel as part of the context, because the colors of these pixels have not been encoded yet. The selection of context greatly affects the compression results. <p> Witten et al. described a two-stage lossy/lossless compression system for document images called Textual Image Compression (TIC) [WBEI94]. They independently invented MCC for lossless compression. TIC's main contribution was better 19 compression for glyph indices and positions. TIC used Prediction by Partial Match--ing (PPM) [CW84] modeling and arithmetic coding <ref> [BCW90] </ref> to code glyph indices. In lossy mode, TIC has the best compression ratio of all previous document image compression schemes; in lossless mode, TIC has results similar to Mohiuddin's lossless image compression system. TIC achieved about 38:1 compression in lossy mode. <p> We use a finite-context model to estimate the indexing cost, because this model is simple and used by PPM, which is the compression method we use for glyph indices. (Other possible models are finite-state models, grammar models, and ergodic models <ref> [BCW90] </ref>.) Among the possible finite-context models, we rule out the order-1 fixed-context model. This model gives every symbol an equal probability, thus assigns the same 84 cost to each symbol. The order-1 model is too primitive to have any effect on the result of codebook design. <p> For this reason our CDIS includes spaces in the glyph index stream. The method of inserting space glyphs was discussed in Chapter 4. We used the PPM text compression method to compress glyph indices <ref> [CW84, BCW90] </ref>. PPM is a state of the art text compression technique. We will discuss this text compression method later in this chapter. 9.1 Previous Work Early PMBC systems, including Ascher and Nagy's system and Pratt's CSM, used fixed-length codes to represent glyph indices. <p> We have followed Witten et al. in the use of the PPM text compression scheme for glyph indices. It is possible to use other text compression methods for glyph indices. Dictionary based schemes, such as LZ77 or LZW, do not compress as well as PPM, but are very fast <ref> [BCW90] </ref>. They are not especially suitable for our PMBC system because we are already using arithmetic coding for codebook bitmaps and glyph positions. <p> The encoder transmits the errors between the predicted positions and the actual positions at some predefined resolution. If this resolution is lower than the resolution of the input, we have lossy compression. The position encoder uses an adaptive arithmetic coder <ref> [BCW90] </ref> to code x position errors, and y positions relative to each block. A block is a glyph sequence terminated by the special new block glyph index. Within a block, glyphs are placed in a horizontal line with a small amount of space, w s , between them. <p> Our variable length code is not the best way to compress glyph indices and positions. In our regular CDIS, 152 we use Moffatt's two-level context-based method [WBEI94] for the codebook, PPMC compression for glyph indices <ref> [BCW90] </ref> and structure-based position coding for glyph positions [ZD95]. These techniques achieve higher compression ratios via increased complexity. In a self-extracting PostScript program, this increased complexity would be included in the final file size, which would be prohibitive. <p> This overhead is kept minimal, especially when we delete the comments and reduce the long variable and procedure names. 13.6 GZIP Gzip is a popular compression program which uses the Lempel-Ziv algorithm (LZ77) <ref> [BCW90] </ref>. In essence, this compression algorithm recognizes and efficiently encodes repeated substrings. The amount of compression obtained depends on the size of the input and the distribution of common substrings. Typically, text such as source code or English is reduced by 60-70%.
Reference: [CA97] <author> C. Constantinescu and R. </author> <title> Arps. Fast residue coding for lossless textual image compression. </title> <editor> In James Storer and Martin Cohn, editors, </editor> <booktitle> Proceedings IEEE Data Compression Conference, </booktitle> <address> Snowbird, Utah, March 1997. </address> <publisher> IEEE Computer Society Press. </publisher> <pages> 173 </pages>
Reference-contexts: Constantinescu and Arps proposed a Typical Prediction for Residue Coding (TPR), which is basically an implicit encoding method, although it works on encoding the whole lossless image rather than a single glyph <ref> [CA97] </ref>. TPR works as the following: Before coding the next pixel, check this pixel to see if it is in an area of locally solid `background' in the lossy image plane.
Reference: [CCI] <author> CCITT. </author> <title> Ccitt test images. </title> <note> available from ftp.funet.fi. </note>
Reference-contexts: High resolution pattern matching improves matching efficiency somewhat because it allows a more precise alignment of images. A bigger payoff is the ability to reconstruct an original image at high resolution, as discussed below. 5.6 Experiments We chose ten images from CCITT's standard test images <ref> [CCI] </ref>, and the MIT AI lab's classic hit technical report collection [MIT], for our test. These images are a mix of business letters and technical papers, so the compression ratio on these images should be a reasonable indication of the performance of each system. <p> MGTIC has a similar multi-pass method that repeats this averaging and matching several times to reduce the codebook size. 6.4 Experiments We chose ten images from CCITT's standard test images <ref> [CCI] </ref>, and the MIT AI lab's classic hit technical report collection [MIT], for our test. These images are a mix of business letters and technical papers, so the compression ratio on these images should be a reasonable indication of the performance of each system. <p> Moffat's 2-level coding uses context modeling and arithmetic coding to achieve compression. This method fully exploits the pixel-level structure in the original documents. TIC is the PMBC system with the best previous compression results. We chose ten images from CCITT's standard test images <ref> [CCI] </ref>, and the MIT AI lab's classic hit technical report collection [MIT], for our test. These images are a mix of business letters and technical papers, so the compression ratio on these images should be a reasonable indication of the performance of each system. <p> We will also compare pbm2cps with a conventional PMBC system, TIC. We want to see how much data inflation results from our use of PostScript rather than a custom output file format. We chose three single page documents from CCITT's standard test images <ref> [CCI] </ref>, two 4-page documents from the MIT AI lab's classic hits technical report collection [MIT], and one 8-page legal document [CP] for our test.
Reference: [CHP96] <author> Su Chen, Robert M. Haralick, and Ihsin T. Phillips. </author> <title> Extraction of text words in document images based on a statistical characterization. </title> <journal> Journal of Electronic Imaging, </journal> <volume> 5(1) </volume> <pages> 25-36, </pages> <month> January </month> <year> 1996. </year>
Reference-contexts: OCR usually has a geometric layout analysis or segmentation stage before actual symbol recognition. This stage usually does connected components analysis, skew correction, and segmentation. Researchers in the OCR community have proposed many segmentation and glyph extraction methods <ref> [CHP96, Bai88] </ref>. For a survey of these segmentation methods see 30 [FNK92, TZAK91]. Some of these methods are robust to skew error [CHP96, Bai88]. For our work, we assume the document is free of skewing. <p> This stage usually does connected components analysis, skew correction, and segmentation. Researchers in the OCR community have proposed many segmentation and glyph extraction methods <ref> [CHP96, Bai88] </ref>. For a survey of these segmentation methods see 30 [FNK92, TZAK91]. Some of these methods are robust to skew error [CHP96, Bai88]. For our work, we assume the document is free of skewing.
Reference: [Chv79] <author> V. Chvatal. </author> <title> A greedy heuristic for the set-covering problem. </title> <journal> Mathematics of Operations Research, </journal> <volume> 4(3) </volume> <pages> 233-235, </pages> <year> 1979. </year>
Reference-contexts: Roughly, Hochbaum observes that the problem reduces to a traditional weighted set-cover problem with exponentially many sets. She then argues it suffices to consider only quadratically many of these sets and obtains a cubic-time algorithm by adapting the weighted greedy set cover algorithm of Chvatal <ref> [Chv79] </ref>. 74 For the variant in which the desired cost k is specified, and the problem is to minimize the resulting distortion, Lin and Vitter [LV92] give a polynomial-time approximation algorithm with the following performance guarantee: given * &gt; 0, the algorithm returns a set of cost at most (1 +
Reference: [CP] <author> Inc. </author> <title> Cartesian Products. Eight-page financial prospectus. </title> <note> available from www.cartesianinc.com/Tech. </note>
Reference-contexts: We chose three single page documents from CCITT's standard test images [CCI], two 4-page documents from the MIT AI lab's classic hits technical report collection [MIT], and one 8-page legal document <ref> [CP] </ref> for our test. These images are a mix of business letters and technical papers, so the compression ratio on these images should be a reasonable indication of the performance of each system. Table 13.1 compares the performance of the various versions of pbm2cps with and without gzip.
Reference: [CTW95] <author> J.G. Cleary, W.J. Teahan, and I.H. Witten. </author> <title> Unbounded length contexts for ppm. </title> <editor> In James Storer and Martin Cohn, editors, </editor> <booktitle> Proceedings IEEE Data Compression Conference, </booktitle> <address> Snowbird, Utah, March 1995. </address> <publisher> IEEE Computer Society Press. </publisher>
Reference-contexts: Dictionary based coding would not be flexible enough for bitmap and position coding, and extra speed in this section of the system would have little effect on overall performance. There are some text compression schemes using arithmetic coding which outperform PPM. PPM* is one of these <ref> [CTW95] </ref>. None of these schemes are practical for short text sequences such as we have because they require huge amounts of training data before high compression is achieved. 9.2 Prediction by Partial Matching PPM stands for Prediction by Partial Matching. This technique uses finite-context models of characters.
Reference: [CW84] <author> J.G. Clearry and I.H. Witten. </author> <title> Data compression using adaptive coding and partial string matching. </title> <journal> IEEE Trans. Communications, </journal> <volume> COM-32(4):396-402, </volume> <month> April </month> <year> 1984. </year>
Reference-contexts: Witten et al. described a two-stage lossy/lossless compression system for document images called Textual Image Compression (TIC) [WBEI94]. They independently invented MCC for lossless compression. TIC's main contribution was better 19 compression for glyph indices and positions. TIC used Prediction by Partial Match--ing (PPM) <ref> [CW84] </ref> modeling and arithmetic coding [BCW90] to code glyph indices. In lossy mode, TIC has the best compression ratio of all previous document image compression schemes; in lossless mode, TIC has results similar to Mohiuddin's lossless image compression system. TIC achieved about 38:1 compression in lossy mode. <p> For this reason our CDIS includes spaces in the glyph index stream. The method of inserting space glyphs was discussed in Chapter 4. We used the PPM text compression method to compress glyph indices <ref> [CW84, BCW90] </ref>. PPM is a state of the art text compression technique. We will discuss this text compression method later in this chapter. 9.1 Previous Work Early PMBC systems, including Ascher and Nagy's system and Pratt's CSM, used fixed-length codes to represent glyph indices.
Reference: [FNK92] <author> H. Fujisawa, Y. Nakano, and K. Kurino. </author> <title> Segemntation method s fro character recognition: from segmentation to document structure analysis. </title> <journal> Proceeding of the IEEE, </journal> <volume> 80(7) </volume> <pages> 1079-1092, </pages> <month> July </month> <year> 1992. </year>
Reference-contexts: OCR usually has a geometric layout analysis or segmentation stage before actual symbol recognition. This stage usually does connected components analysis, skew correction, and segmentation. Researchers in the OCR community have proposed many segmentation and glyph extraction methods [CHP96, Bai88]. For a survey of these segmentation methods see 30 <ref> [FNK92, TZAK91] </ref>. Some of these methods are robust to skew error [CHP96, Bai88]. For our work, we assume the document is free of skewing.
Reference: [GF73] <author> Jr. G.D. Forney. </author> <title> The viterbi algorithm. </title> <journal> Proc. IEEE, </journal> <volume> 77(2) </volume> <pages> 257-286, </pages> <month> March </month> <year> 1973. </year>
Reference-contexts: For example, many OCR systems, both commercial and research ones, have built-in spelling checkers. Ho et al. used multi-level contextual knowledge for recognition of degraded and connected character strings [THS91]. People have also applied Hidden Markov Models (HMM) and the Viterbi algorithm <ref> [GF73] </ref> to the problem of robust machine recognition of poorly print text [RS79, AB91]. The transition probability between character models is usually determined from the statistical studies of the type of text material the recognizer is expected to be used on.
Reference: [GG92] <author> Allen Gersho and Robert Gray. </author> <title> Vector Quantization and signal Compression, chapter 11. </title> <publisher> Kluwer Academic Publishers, </publisher> <year> 1992. </year> <month> 174 </month>
Reference-contexts: The glyphs are then reparti-tioned, with each glyph being assigned to the first matching pattern. Patterns with no matched glyphs are deleted. This process is iterated until little or no improvement results. This multi-pass codebook design method bears a resemblance to Lloyd's algorithm for vector quantizer design <ref> [GG92] </ref>. The details of this multi-pass codebook design method are presented in Chapter 5. 25 The codebook design algorithm requires a glyph distance measure method. We use the cross entropy as our measurement for the closeness of pairs of glyphs. <p> The glyphs are then repartitioned, with each glyph being assigned to the first matching pattern. Patterns with no matched glyphs are deleted. This process is iterated until little or no improvement results. This method bears a resemblance to Lloyd's algorithm for vector quantizer design <ref> [GG92] </ref>. First Fit and its variations generate codebooks which are dependent on the ordering of the glyphs. This algorithm has no guaranteed performance, but it is widely used because of its simplicity. <p> In subsequent passes, the averaged glyphs are used in the matching process. Each glyph is matched with its closest library pattern. Library patterns with no matched glyphs are deleted. This algorithm can be thought of as a specialization of the Generalized Lloyd Algorithm (k-means algorithm) for vector quantizer design <ref> [GG92] </ref>. The following is the document image compression system with modified k-means codebook generation algorithm: 1. Extract the glyphs from the degraded image (same as before). 2. Run First Fit to generate the original codebook. Average the glyphs in each equivalence class, creating an ideal pattern for each class. 3. <p> The main contribution of this chapter is a new method for the partitioning step. This work is a joint work with Danskin and Young [ZDY97]. Recent work has used relatively ad-hoc heuristics for this step, with the most successful bearing a resemblance to Lloyd's algorithm for vector quantizer design <ref> [GG92] </ref>. The conceptual basis of the new method is an extension of a particular cross-entropy model, proposed and approximated in Chapter 5 and [ZD96b], for measuring distances between bitmaps. Here is a summary of this model. <p> We also test a variant of this algorithm which reassigns each glyph to its best matching pattern. This algorithm is a specialization of the Generalized Lloyd (k-means) algorithm for vector quantizer design, because the repartitioned equivalence classes satisfy the nearest-neighbor condition <ref> [GG92] </ref>. We call this algorithm "modified k-means" because the averaging and thresholding steps do not necessarily produce 73 an optimal pattern for each equivalence class. This can degrade the performance of the algorithm. 7.2 The Greedy K-median Algorithm The weighted fixed-cost k-median problem is the following. <p> Patterns without any matched glyphs are deleted. This process is iterated until there is little or no reduction in the size of the codebook. This multi-pass codebook generation method bears a resemblance to Lloyd's algorithm for vector quantizer design <ref> [GG92] </ref>. More sophisticated codebook design algorithms are possible as we presented in Chapter 7 and [ZDY97]. For this work, we opt for the simpler First Fit. 13.5 PostScript Output After partitioning the glyphs, we encode the codebook, glyph indices and positions, and the decompression routine in a PostScript file.
Reference: [Hal88] <author> M. J. Halt. </author> <title> A fast binary template matching algorithm for document image data compression. </title> <booktitle> In Proc. 4th International Conference on Pattern Recognition,, </booktitle> <pages> pages 230-239, </pages> <address> Berlin, 1988. </address> <publisher> Springer-Verlag. </publisher>
Reference-contexts: Each such pattern has a succinct representation as a bitmap. The distance between a pattern and a glyph is the conditional entropy of the glyph given the distribution on ideal characters associated with the pattern. 5.1 Previous Work A number of pattern matching algorithms have been proposed <ref> [PCC + 80, HX86, Hal88, IW94, Kia97] </ref>. All of these methods begin by aligning the two bitmaps, and then examining the error pixels, which are the pixels which have different values in the two bitmaps. <p> Ascher and Nagy [AN74], Mohiuddin [Moh84], and more recently Howard [How96] used similar weighted global criteria based pattern matching methods. * In other algorithms, such as the Pattern Matching and Substitution (PMS) [JSC83] and Combined Size Independent Strategy (CSIS), the matching decision is based on local criteria <ref> [Hal88] </ref>. PMS rejects a match if any position has 4 39 or more error pixels around it; CSIS looks for four error pixels clustered in a square, rejecting potential matches when this pattern is found. These rules are heuristic and do not have a firm theoretical basis.
Reference: [HB96] <author> John Hobby and Henry Baird. </author> <title> Degraded character image restoration. </title> <booktitle> In Proceedings of the fifth Annual Symposium on document Analysis and Image Retrieval, </booktitle> <pages> pages 233-245, </pages> <year> 1996. </year>
Reference-contexts: This can result in a noisy reconstruction. Witten et al. used a simple bitmap image averaging method which superimposes 56 the input images, adds up the intensities at each point, and thresholds the result [WAB94]. Hobby and Baird <ref> [HB96] </ref> called this method the Naive Averaging algorithm (NA). NA is simple but it does not always reduce the noise from bitmaps when there are only a few glyphs in an equivalence class. Mark and Shieber used a modified NA in their system [MS]. <p> Their methods, although effective in reducing the noise, are complicated and do not fit into our PMBC framework. Hobby and Baird present a sophisticated bitmap image averaging algorithm that preprocesses each input image and postprocesses the final result <ref> [HB96] </ref>. Preprocessing is done by generating smooth-shaded outlines, which are fuzzy edges that simulate the result of averaging all the possible underlying images. Postprocessing is done to sharpen corners. This algorithm is designed for degraded character image restoration. <p> NA has several problems <ref> [HB96] </ref>: First, under perfect noise free conditions with only spatial sampling errors, for a 90 degree corner, the Naive Averaging algorithm produces a rounded corner, even as the number of input images approaches infinity. <p> In our experiment, we simply double the resolution of each glyph. Then, we apply the same filter that we use in HEPM, shown in Figure 5.5, to smooth the glyphs. This is very similar to the smooth shading preprocessing approach <ref> [HB96] </ref>, except that our filter is derived from a physical model. As you can see in Column C of Figure 6.1, rough bumps along the edges become smoother. After preprocessing, we superimpose the matched bitmaps, centering on the centroid of each bitmap.
Reference: [Hoc82] <author> Dorit S. Hockbaum. </author> <title> Heuristics for the fixed cost median problem. </title> <journal> Mathematical Programming, </journal> <volume> 22 </volume> <pages> 148-162, </pages> <year> 1982. </year>
Reference-contexts: Because of the properties of glyphs and their patterns, the weights in this graph satisfy the usual triangle inequality, as well as c (v) c (u) + d (u; v) for all u; v. 7.2.1 Previous K-median Algorithm Hochbaum <ref> [Hoc82] </ref> gives a polynomial-time algorithm that finds a set for which the cost plus the distortion is at most 1 + ln n times the minimum possible. Roughly, Hochbaum observes that the problem reduces to a traditional weighted set-cover problem with exponentially many sets.
Reference: [Hon95] <author> Tao Hong. </author> <title> Degraded text recognition using visual and linguistic context. </title> <type> PhD thesis, </type> <institution> State University of New York at Buffalo, </institution> <month> September </month> <year> 1995. </year>
Reference-contexts: The transition probability between character models is usually determined from the statistical studies of the type of text material the recognizer is expected to be used on. Interestingly, some researchers also resort to low level bitmap matching to postprocess OCR results <ref> [Hon95] </ref>. 7.4.3 Indexing Cost Model There are two decisions we need to make to implement the codebook design algorithm with indexing cost. We need to choose the method to estimate the indexing cost and decide where to incorporate the indexing cost.
Reference: [How96] <author> Paul Howard. </author> <title> Lossless and lossy compression of text images by soft pattern matchig. </title> <booktitle> In Proceedings of IEEE Data Compression Conference, </booktitle> <year> 1996. </year>
Reference-contexts: The International Standards Organization (ISO) is in the process of adopting a new JBIG2 bi-level image coding standard, for which the leading proposal is also pattern matching based <ref> [How96] </ref>. Pattern Matching Based Compression involves the following steps (Illustrated in Figure 1.6): 5 6 1. Extract from the image of the scanned document a sequence of glyphs. Each glyph typically represents one connected blob of ink occuring somewhere in the document, represented as a positioned bitmap. 2. <p> They presented a voting scheme in conjunction with a plurality of similarity tests to improve pattern matching accuracy. They also presented a glyph composition scheme enhance document images by removing the noise. Howard proposed a system which is similar to the previous PMBC systems <ref> [How96] </ref>. His main contribution is to preprocess document images to obtain better compression. The preprocessing steps included smoothing and selective pixel reversal, which reverses the color of isolated pixels which are difficult to compress. Howard reinvented Mohiuddin's MCC method and called it Soft Pattern Matching. <p> Optionally the resolution of the position is relaxed to allow lossy encoding of glyph positions. For lossless compression, we also transmit the difference image, which is usually called the residual image. We code the residuals on a per-glyph basis (Mohiuddin [Moh84] and Howard's approach <ref> [How96] </ref>) rather than coding it for the entire image (as in MGTIC [WBEI94]). We used a method similar to Mohiuddin's which uses the 26 matching glyph in the codebook as part of the context to code the residual image. <p> Then we cannot exploit the geometrical and contextual structure in document images at all, and glyph indices and positions cannot be compressed efficiently. Because of its importance, most previous PMBC systems have this step. Some of them, Witten et al [WAB94], Howard <ref> [How96] </ref>, Mark and Shieber [MS], reported their methods. Others did not give details of their methods. Sorting glyphs into text order is straight-forward if we assume any skew has been 33 corrected and the text is single-column and oriented horizontally on the page. <p> One improved version of this algorithm called "weighted and-not" (WAN) distinguishes black-to-white errors from white-to-black errors [HX86]. These algorithms tend to have a high rejection rate if the threshold function is set to eliminate symbol mismatches. Ascher and Nagy [AN74], Mohiuddin [Moh84], and more recently Howard <ref> [How96] </ref> used similar weighted global criteria based pattern matching methods. * In other algorithms, such as the Pattern Matching and Substitution (PMS) [JSC83] and Combined Size Independent Strategy (CSIS), the matching decision is based on local criteria [Hal88]. <p> We hope to use this contextual information to improve clustering accuracy and compression performance. We investigate different ways to estimate the indexing cost and adopt the order-1 context model. This experiment, however, does not show any significant improvement over the plain GKM. 7.1 Previous Work Previous systems <ref> [WBEI94, How96] </ref> used a simple codebook design algorithm which we call the First Fit algorithm. This algorithm was introduced in Chapter 5. We review this algorithm here: FIRST-FIT (S) 1. <p> The best known method for encoding lossless images is called the Modified Clairvoyant Context (MCC) method. This method was originally proposed by Mohiuddin [Moh82], and later used by Witten [WBEI94] and Howard <ref> [How96] </ref>. The basic idea of MCC is encode each isolated glyph losslessly using a matching glyph in the codebook as part of the conditioning region of support (ROS). <p> Mohiuddin and Howard used this per-glyph approach, while Witten 134 used the whole image approach. 11.2 Modified Clairvoyant Context Method The coding method that we use in CDIS was invented by Mohiuddin [Moh84], and later called Soft Pattern Matching (SPM) by Howard <ref> [How96] </ref>. We call it the Modified Clairvoyant Context method (MCC). MCC is similar to the context-based compression method that we discussed in Chapter 6. The main difference between conventional context-based compression and MCC is the choice of context template. <p> Pattern matching based compression has been studied by several research groups [AN74, HX86, Moh84, WBEI94] and ISO is in the process of adopt 146 ing a new JBIG2 bi-level image-coding standard of which the leading proposal is also pattern matching based <ref> [How96] </ref>. We have discussed the details of a PMBC system. The following is a review of the basic steps: 1. Extract a sequence of glyphs from the image of the scanned document . <p> Our method reduces the size of the compressed documents by from 50% to 80%. 13.2 Previous Work Previous Pattern Matching Based Compression systems have used proprietary file formats requiring special decompression programs which must be distributed to each user, allowing these users to view and print the compressed documents <ref> [AN74, HX86, Moh84, WBEI94, How96, ZD95] </ref>. Because there are several groups working on PMBC systems, no standard file format has been established. 148 PostScript is the standard page description language for desktop publishing. There are several image conversion utility programs that convert scanned images into PostScript format.
Reference: [HX86] <author> M. J. Holt and C. S. Xydeas. </author> <title> Recent developments in image data compression for digital facsimile. </title> <journal> ICL Technical Journal, </journal> <pages> pages 123-146, </pages> <year> 1986. </year>
Reference-contexts: In contract, pattern-matching based compression (PMBC) techniques exploit the glyph-level structure of document images. Pattern-matching based compression techniques are effective for text pages. These techniques were originally proposed in [AN74] and further studied in <ref> [HX86] </ref>, [Moh82], and [WBEI94]. [WBEI94] and [Moh84] discuss lossless compression based on pattern matching. The International Standards Organization (ISO) is in the process of adopting a new JBIG2 bi-level image coding standard, for which the leading proposal is also pattern matching based [How96]. <p> Each such pattern has a succinct representation as a bitmap. The distance between a pattern and a glyph is the conditional entropy of the glyph given the distribution on ideal characters associated with the pattern. 5.1 Previous Work A number of pattern matching algorithms have been proposed <ref> [PCC + 80, HX86, Hal88, IW94, Kia97] </ref>. All of these methods begin by aligning the two bitmaps, and then examining the error pixels, which are the pixels which have different values in the two bitmaps. <p> A match is rejected if the total weighted count is higher than some threshold. A similar algorithm and a thorough discussion can be found in [Moh84]. One improved version of this algorithm called "weighted and-not" (WAN) distinguishes black-to-white errors from white-to-black errors <ref> [HX86] </ref>. These algorithms tend to have a high rejection rate if the threshold function is set to eliminate symbol mismatches. <p> Document images contain four types of structure: pixel-level structure, glyph-level structure, geometrical structure, and contextual structure. Pattern matching based coding techniques exploit these structures of document images. Pattern matching based compression has been studied by several research groups <ref> [AN74, HX86, Moh84, WBEI94] </ref> and ISO is in the process of adopt 146 ing a new JBIG2 bi-level image-coding standard of which the leading proposal is also pattern matching based [How96]. We have discussed the details of a PMBC system. The following is a review of the basic steps: 1. <p> Our method reduces the size of the compressed documents by from 50% to 80%. 13.2 Previous Work Previous Pattern Matching Based Compression systems have used proprietary file formats requiring special decompression programs which must be distributed to each user, allowing these users to view and print the compressed documents <ref> [AN74, HX86, Moh84, WBEI94, How96, ZD95] </ref>. Because there are several groups working on PMBC systems, no standard file format has been established. 148 PostScript is the standard page description language for desktop publishing. There are several image conversion utility programs that convert scanned images into PostScript format.
Reference: [Inc85] <author> Adobe Systems Inc. </author> <title> PostScript Language Tutorial and Cookbook. </title> <publisher> Addison-Wesley Publishing Company, </publisher> <year> 1985. </year>
Reference-contexts: Because pbmtolps encodes images line by line, the effect is to run-length-code the images, which is similar to FAX Group 3 compression. 13.3 PostScript Basics This section gives a brief introduction to PostScript. A few important concepts, which are related to our PostScript implementation of compressed images, are presented. <ref> [Inc85] </ref> and [Inc90] have detailed information about PostScript. [Wei], which this section is based on, is an excellent PostScript tutorial. PostScript is a stack-based programming language. A program pushes arguments 149 to an operator onto a stack and then invokes the operator.
Reference: [Inc90] <author> Adobe Systems Inc. </author> <title> PostScript Language Reference Manual, Second Edition. </title> <publisher> Addison-Wesley Publishing Company, </publisher> <year> 1990. </year>
Reference-contexts: For this work, we sacrifice a little compression for portability. Particularly, we drop the arithmetic encoder to use a much simpler variable length encoder instead. 13.1 PostScript and PMBC Adobe PostScript is a page description language used to communicate graphics to printers and other output devices <ref> [Inc90] </ref>. Since its introduction in 1985, PostScript has become the printing and imaging technology of choice for corporations, professional publishers, and government agencies throughout the world. PostScript provides a printer-independent standard for representing the printed page. <p> A few important concepts, which are related to our PostScript implementation of compressed images, are presented. [Inc85] and <ref> [Inc90] </ref> have detailed information about PostScript. [Wei], which this section is based on, is an excellent PostScript tutorial. PostScript is a stack-based programming language. A program pushes arguments 149 to an operator onto a stack and then invokes the operator.
Reference: [IW94] <author> S. Inglis and I. H. Witten. </author> <title> Compression-based template matching. </title> <booktitle> In Proc. IEEE Data Compression Conference, </booktitle> <pages> pages 106-115, </pages> <address> Los Alamitos, CA, 1994. </address> <publisher> IEEE Computer Society Press. </publisher> <pages> 175 </pages>
Reference-contexts: Each such pattern has a succinct representation as a bitmap. The distance between a pattern and a glyph is the conditional entropy of the glyph given the distribution on ideal characters associated with the pattern. 5.1 Previous Work A number of pattern matching algorithms have been proposed <ref> [PCC + 80, HX86, Hal88, IW94, Kia97] </ref>. All of these methods begin by aligning the two bitmaps, and then examining the error pixels, which are the pixels which have different values in the two bitmaps. <p> These rules are heuristic and do not have a firm theoretical basis. Most of these rules do not scale across different image resolutions. * Inglis and Witten proposed a compression-based pattern matching scheme (CTM) <ref> [IW94] </ref>, which is based on the cross-entropy measurement idea proposed by Mo-huiddin [Moh84]. CTM uses a 5-pixel clairvoyant mask (one pixel in the middle with four pixels surrounding the middle one) to build the model of one pattern with respect to the other. <p> They found out that the straightforward feature "number of black pixels" was of little use because of its high variability and dependency upon the other feature. Inglis used the distance between the centroids of two bitmaps and sub-bitmaps for screening <ref> [IW94] </ref>. Specifically, he registered glyphs according to their centroids, and then divided them into quadrants defined by this position. A local centroid was calculated for each of the quadrants, and the distances between the corresponding centroids in two glyphs were calculated.
Reference: [JSC83] <author> O. Johnsen, J. Segen, and G. L. Cash. </author> <title> Coding of two-level pictures by pattern matching and substitution. </title> <journal> Bell Systems Technical Journal, </journal> <volume> 62(8) </volume> <pages> 2513-2545, </pages> <year> 1983. </year>
Reference-contexts: Glyphs are defined as connected blobs of black pixels. We use Johnsen et al.'s mark boundary tracing method <ref> [JSC83] </ref> to isolate and extract the glyphs from an image, as does TIC. For English text, most symbols are extracted as a single glyph. Some symbols, however, have two parts and are initially extracted as two glyphs. <p> Despite its simplicity, this method can successfully identify a nested glyph. The drawback of this run-length based method is that the bi-level image must be preprocessed by run length encoding. Our system uses Johnsen et al.'s mark boundary tracing method <ref> [JSC83] </ref> to isolate 31 first check pixel 1, which is two steps clockwise from the previous pixel. Pixel 1 is white, so we move clockwise to pixel 2 and 3. <p> The process scans a document image line by line from the upper-left element. When a black pixel is encountered, the boundary tracing algorithm attempts to trace the boundary of a black region, clockwise. This tracing algorithm is described in <ref> [JSC83] </ref> and [WAB94]. We repeat it here for further reference. Let us call the first black pixel (x 1 ; y 1 ). The eight neighbors of the pixel are examined, beginning at (x 1 + 1; y 1 ) and searching clockwise around (x 1 ; y 1 ). <p> Ascher and Nagy [AN74], Mohiuddin [Moh84], and more recently Howard [How96] used similar weighted global criteria based pattern matching methods. * In other algorithms, such as the Pattern Matching and Substitution (PMS) <ref> [JSC83] </ref> and Combined Size Independent Strategy (CSIS), the matching decision is based on local criteria [Hal88]. PMS rejects a match if any position has 4 39 or more error pixels around it; CSIS looks for four error pixels clustered in a square, rejecting potential matches when this pattern is found. <p> Some features are required for this screening. Johnsen el al. chose four features for the screening: glyph width, glyph height, the number of horizontal and the number of vertical white runs <ref> [JSC83] </ref>. They found out that the straightforward feature "number of black pixels" was of little use because of its high variability and dependency upon the other feature. Inglis used the distance between the centroids of two bitmaps and sub-bitmaps for screening [IW94]. <p> He reported that this progressive centroid method achieved about 94% screening. Inglis's screening method is effective but the feature is not obvious or easy to compute. 7.3.2 Divide-and-conquer Approach Our approach to boost the speed of GKM is similar to the idea of screening <ref> [JSC83] </ref>. We want to find some obvious features to precluster the glyphs into smaller clusters, then apply our regular GKM along with EPM pattern matching to further separate these smaller clusters and get the final clusters and codebook. <p> This scheme was better than coding absolute glyph positions. Johnsen et al. <ref> [JSC83] </ref> used fixed length codes for absolute horizontal positions and variable-length codes to represent vertical shift (relative position) of glyphs. Mohiuddin proposed to code glyph offsets using the statistical distribution of position information and arithmetic coding [Moh82]. This model, using arithmetic coding, outperformed all previous variable-length codes. <p> First, it merges some glyphs; second, it inserts two special glyphs, space and new block, to represent the spaces between words, and line breaks, respectively. The glyph extractor uses Johnsen et al.'s mark boundary tracing method <ref> [JSC83] </ref> to isolate and extract the glyphs from an image. For most of the previous PMBC systems, if a glyph has two disconnected parts, as i does, it is extracted as two separate glyphs. <p> The first step of any pattern matching based compression is to extract the glyphs from the binary image. Glyphs are defined as connected blobs of black pixels. We use Johnsen et al.'s mark boundary tracing method <ref> [JSC83] </ref> to isolate and extract the glyphs from an image. For English text, most symbols are extracted as a single glyph. Some symbols, however, have two parts and are initially extracted as two glyphs.
Reference: [KHP93] <author> T. Kanungo, R.M. Haralick, and I.T. Phillips. </author> <title> Global and local document degradation models. </title> <booktitle> In Proceedings of the International Conference on Document Analysis and Recognition, </booktitle> <pages> pages 730-734, </pages> <year> 1993. </year>
Reference-contexts: Kia observed previous research results which indicated that pixel errors occur more often close to the edges of glyphs. Specifically, the probability of a pixel changing its value decays as a squared exponential in the distance to the edge <ref> [KHP93] </ref>. Notice that this result is consistent with the error model of spatial sampling error, which states that these errors occur on the edges. Kia's matching model, therefore, assigns weights based on relative distances from the prototype glyph's edge.
Reference: [Kia97] <author> Omid E. </author> <title> Kia. Document Image Compression and Analysis. </title> <type> PhD thesis, </type> <institution> University of Maryland at College Park, </institution> <year> 1997. </year>
Reference-contexts: The preprocessing steps included smoothing and selective pixel reversal, which reverses the color of isolated pixels which are difficult to compress. Howard reinvented Mohiuddin's MCC method and called it Soft Pattern Matching. Kia described a unique compression system which allows easy processing of images in the compressed domain <ref> [Kia97] </ref>. He also presented a method to represent residual images hierarchically and achieved progressive transmission of document images. Specifically, his method ordered residual pixels according to the pixel's distance to the edge of the glyph and code the pixels further away from the edge first. <p> Each such pattern has a succinct representation as a bitmap. The distance between a pattern and a glyph is the conditional entropy of the glyph given the distribution on ideal characters associated with the pattern. 5.1 Previous Work A number of pattern matching algorithms have been proposed <ref> [PCC + 80, HX86, Hal88, IW94, Kia97] </ref>. All of these methods begin by aligning the two bitmaps, and then examining the error pixels, which are the pixels which have different values in the two bitmaps. <p> They expected the error to occur only at the edges of the glyphs. They did not, however, develop any matching method based on this observation. * Kia proposed a distance-based template matching (DTM) system <ref> [Kia97] </ref>. This method is similar to CSM, except that it attempts to weight each error pixel according to the probability that it was corrupted by degradation or noise. <p> Mohiuddin proposed to code glyph offsets using the statistical distribution of position information and arithmetic coding [Moh82]. This model, using arithmetic coding, outperformed all previous variable-length codes. Kia divided each page into blocks of 256 fi 256 pixels, each of which can be addressed by a single byte <ref> [Kia97] </ref>. A component is indexed by the relative location of the upper left corner of its bounding box with respect to the upper left corner of the block. These locations and the block's address yield the absolute addresses of the components.
Reference: [LR81] <author> G. Langdon and J. Rissanen. </author> <title> Compression of black-white images with arithmetic coding. </title> <journal> IEEE Trans. Communications, </journal> <volume> COM(6):158-167, </volume> <year> 1981. </year>
Reference-contexts: As we discussed early in Chapter 2, context based compression of bi-level images is designed to exploit this pixel level structures of document images. Langdon and Rissanen proposed the application of arithmetic coding to the problem of compressing bi-level images <ref> [LR81] </ref>. They showed that arithmetic coding allows a finite state machine model to be used to record a "context" based on a number of pixels in the immediate vicinity of the pixel that is to be coded, and that conditioning the probabilities based upon this context gives good compression.
Reference: [LV92] <author> J-H Lin and J. S. Vitter. </author> <title> Approximations with miminum packing constraint violation. </title> <booktitle> In Proceedings 24th ACM symposium on Theory of Computing, </booktitle> <pages> pages 771-782, </pages> <year> 1992. </year>
Reference-contexts: it suffices to consider only quadratically many of these sets and obtains a cubic-time algorithm by adapting the weighted greedy set cover algorithm of Chvatal [Chv79]. 74 For the variant in which the desired cost k is specified, and the problem is to minimize the resulting distortion, Lin and Vitter <ref> [LV92] </ref> give a polynomial-time approximation algorithm with the following performance guarantee: given * &gt; 0, the algorithm returns a set of cost at most (1 + 1=*)(1 + ln (jV j))k with distortion at most 1 + * times the minimum distortion for a set of cost k.
Reference: [MBS92] <author> Kenneth R. McConnell, Dennis Bodson, and Richard Schaphorst. FAX: </author> <title> Digital Facsimile Technology and Applications. </title> <publisher> Artech House, </publisher> <address> 685 Canton Street, Norwood, MA 02062, </address> <note> second edition, </note> <year> 1992. </year>
Reference-contexts: Examples of these methods are the run-length coding used by the CCITT (International Telephone and Telegraph Consultative Committee) standard <ref> [MBS92] </ref>, and the context-based predictive coding used in the JBIG (Joint Bi-level Image Experts Group) standard [TC93]. Since these methods exploit only pixel-level structure of the image, they fail to provide good compression for document images. In contract, pattern-matching based compression (PMBC) techniques exploit the glyph-level structure of document images. <p> The Group 3 standard provides for at least every fourth line to be transmitted using the one-dimensional method, and the remaining lines using the two-dimensional coding. For a detailed description of the Group 3 and 4 FAX coding standards see <ref> [MBS92] </ref>. The strength of CCITT FAX encoding is its simplicity. The static variable length coding is simple and fast: Given the length of a run or an offset of a color change, we only need to look up one predefined table for the corresponding codeword.
Reference: [MIT] <institution> MIT. Mit ai lab's classic hit document. </institution> <note> available from ai.mit.edu/pulications/. </note>
Reference-contexts: A bigger payoff is the ability to reconstruct an original image at high resolution, as discussed below. 5.6 Experiments We chose ten images from CCITT's standard test images [CCI], and the MIT AI lab's classic hit technical report collection <ref> [MIT] </ref>, for our test. These images are a mix of business letters and technical papers, so the compression ratio on these images should be a reasonable indication of the performance of each system. Figures 5.6, 6.2 and 6.3 show original and reconstructed images from our test set. <p> MGTIC has a similar multi-pass method that repeats this averaging and matching several times to reduce the codebook size. 6.4 Experiments We chose ten images from CCITT's standard test images [CCI], and the MIT AI lab's classic hit technical report collection <ref> [MIT] </ref>, for our test. These images are a mix of business letters and technical papers, so the compression ratio on these images should be a reasonable indication of the performance of each system. Figures 5.6, 6.2 and 6.3 show original and reconstructed images from our test set. <p> This method fully exploits the pixel-level structure in the original documents. TIC is the PMBC system with the best previous compression results. We chose ten images from CCITT's standard test images [CCI], and the MIT AI lab's classic hit technical report collection <ref> [MIT] </ref>, for our test. These images are a mix of business letters and technical papers, so the compression ratio on these images should be a reasonable indication of the performance of each system. Figures 5.6, 6.2 and 6.3 show original and reconstructed images from our test set. <p> We want to see how much data inflation results from our use of PostScript rather than a custom output file format. We chose three single page documents from CCITT's standard test images [CCI], two 4-page documents from the MIT AI lab's classic hits technical report collection <ref> [MIT] </ref>, and one 8-page legal document [CP] for our test. These images are a mix of business letters and technical papers, so the compression ratio on these images should be a reasonable indication of the performance of each system.
Reference: [Mof91] <author> A. Moffat. </author> <title> Two-level context-based compression of binary images. </title> <booktitle> In Proc. IEEE Data Compression Conference, </booktitle> <pages> pages 382-391, </pages> <address> Snowbird, Utah, </address> <year> 1991. </year>
Reference-contexts: These models are very small compared to the size of common glyphs in the document. Thus, context-based compression does not effectively exploit the glyph-level structure of document image. Moffatt's 2-level coding scheme can use a many-pixel model without paying a high learning cost <ref> [Mof91] </ref>. This is done by coding each pixel in the full context only if that context has already been observed a certain number of times before. Otherwise, a smaller context is used to code the current pixel. <p> The codebook bitmaps are sometimes called prototype glyphs or library patterns. For a typical single-page document compressed with a typical PMBC system, about 50% of the compressed file is codebook bitmaps. It is very important to effectively compress this part of the document. We use Moffat's 2-level method <ref> [Mof91] </ref> to compress the codebook bitmaps. This is the best method available to compress general bi-level images. We presented context-based bi-level image compression briefly in Chapter 2. The rest of this chapter will discuss this 2-level method in detail. 105 Context pixels are circled pixels. <p> to 1 so a pixel in a context which has never occurred always has a non-zero probability, which allows us to code it using a finite number of bits. 8.2 Moffat's 2-level Method The basic idea of Moffat's 2-level context based compression is borrowed from the PPM text compression scheme <ref> [Mof91] </ref>. The PPM scheme employs one heuristic, variable order models, that improve the compression in a practical sense. Because the use of a large finite state model may improve the compression asymptotically, but might also require a long learning period to fill the model with useful frequencies. 107 white pixels. <p> Arps had investigated the use of different templates for compression of images using context-based predictive encoding, and found that the templates that include non-causal pixels are better than those consisting only of causal elements [Arp69]. This result was rediscovered by Moffat <ref> [Mof91] </ref>. Moffat called the templates that include non-causal pixels, clairvoyant contexts, because pixel values ahead of the current one will not be known by the decoder until after they have been decoded, making them useless for conventional context-based compression.
Reference: [Moh82] <author> K. M. Mohiuddin. </author> <title> Pattern matching with application to binary image compression. </title> <type> PhD thesis, </type> <institution> Standford University, Stanford, </institution> <address> CA, </address> <year> 1982. </year> <month> 176 </month>
Reference-contexts: In contract, pattern-matching based compression (PMBC) techniques exploit the glyph-level structure of document images. Pattern-matching based compression techniques are effective for text pages. These techniques were originally proposed in [AN74] and further studied in [HX86], <ref> [Moh82] </ref>, and [WBEI94]. [WBEI94] and [Moh84] discuss lossless compression based on pattern matching. The International Standards Organization (ISO) is in the process of adopting a new JBIG2 bi-level image coding standard, for which the leading proposal is also pattern matching based [How96]. <p> In <ref> [Moh82] </ref>, Mohiuddin proved that g k (X) = I (Xjk) = log 2 P (Xjk); (5.1) where I (Xjk) is the conditional information of X given class [k], is the right measure for cross entropy between two glyphs and the optimal discriminate function. <p> However, Mohiuddin showed that we can get greater discrimination between an incorrect and a correct glyph class by using the independent pixel model than using a model which takes into account all of the adjacent pixel dependencies in the input glyph <ref> [Moh82] </ref>. The 43 following is a sketch of his proof. <p> This scheme was better than coding absolute glyph positions. Johnsen et al. [JSC83] used fixed length codes for absolute horizontal positions and variable-length codes to represent vertical shift (relative position) of glyphs. Mohiuddin proposed to code glyph offsets using the statistical distribution of position information and arithmetic coding <ref> [Moh82] </ref>. This model, using arithmetic coding, outperformed all previous variable-length codes. Kia divided each page into blocks of 256 fi 256 pixels, each of which can be addressed by a single byte [Kia97]. <p> The best known method for encoding lossless images is called the Modified Clairvoyant Context (MCC) method. This method was originally proposed by Mohiuddin <ref> [Moh82] </ref>, and later used by Witten [WBEI94] and Howard [How96]. The basic idea of MCC is encode each isolated glyph losslessly using a matching glyph in the codebook as part of the conditioning region of support (ROS).
Reference: [Moh84] <author> K. M. Mohiuddin. </author> <title> Lossless binary image compression based on pattern matching. </title> <booktitle> In Proc. International Conference on Computers, System and Signal Processing, </booktitle> <pages> pages 447-451, </pages> <year> 1984. </year>
Reference-contexts: In contract, pattern-matching based compression (PMBC) techniques exploit the glyph-level structure of document images. Pattern-matching based compression techniques are effective for text pages. These techniques were originally proposed in [AN74] and further studied in [HX86], [Moh82], and [WBEI94]. [WBEI94] and <ref> [Moh84] </ref> discuss lossless compression based on pattern matching. The International Standards Organization (ISO) is in the process of adopting a new JBIG2 bi-level image coding standard, for which the leading proposal is also pattern matching based [How96]. <p> CSM left non-textual marks such as line drawings in the residue and compressed this residue using the 2-dimensional run-length coding. CSM also used fixed-length codes for glyph indices, but it compressed glyph positions using a static variable-length coding scheme. CSM achieved about 25:1 compression. Mohiuddin <ref> [Moh84] </ref> studied pattern-matching algorithms and developed a lossy and lossless compression system. His main contribution was a lossless compression scheme, the Modified Clairvoyant Context (MCC) method. His lossless compression scheme used the matching prototype glyph in the codebook as part of the context to code each new glyph bitmap. <p> This method is described in detail in Chapter 10. Optionally the resolution of the position is relaxed to allow lossy encoding of glyph positions. For lossless compression, we also transmit the difference image, which is usually called the residual image. We code the residuals on a per-glyph basis (Mohiuddin <ref> [Moh84] </ref> and Howard's approach [How96]) rather than coding it for the entire image (as in MGTIC [WBEI94]). We used a method similar to Mohiuddin's which uses the 26 matching glyph in the codebook as part of the context to code the residual image. <p> A number of methods are available for extracting the glyphs. The simplest method 32 would be to cast the smallest rectangle that would completely enclose the glyph and accept all the pixels within the rectangle as belonging to the pattern. Although this method, used by Mohiuddin <ref> [Moh84] </ref>, is simple, it has the following disadvantage: if there are glyphs within a glyph, as for example text enclosed within boxes, the extractor would identify it as one single glyph. <p> This algorithm computes a weighted count of the error pixels, in which higher weights are given to clustered error pixels. A match is rejected if the total weighted count is higher than some threshold. A similar algorithm and a thorough discussion can be found in <ref> [Moh84] </ref>. One improved version of this algorithm called "weighted and-not" (WAN) distinguishes black-to-white errors from white-to-black errors [HX86]. These algorithms tend to have a high rejection rate if the threshold function is set to eliminate symbol mismatches. Ascher and Nagy [AN74], Mohiuddin [Moh84], and more recently Howard [How96] used similar weighted <p> and a thorough discussion can be found in <ref> [Moh84] </ref>. One improved version of this algorithm called "weighted and-not" (WAN) distinguishes black-to-white errors from white-to-black errors [HX86]. These algorithms tend to have a high rejection rate if the threshold function is set to eliminate symbol mismatches. Ascher and Nagy [AN74], Mohiuddin [Moh84], and more recently Howard [How96] used similar weighted global criteria based pattern matching methods. * In other algorithms, such as the Pattern Matching and Substitution (PMS) [JSC83] and Combined Size Independent Strategy (CSIS), the matching decision is based on local criteria [Hal88]. <p> These rules are heuristic and do not have a firm theoretical basis. Most of these rules do not scale across different image resolutions. * Inglis and Witten proposed a compression-based pattern matching scheme (CTM) [IW94], which is based on the cross-entropy measurement idea proposed by Mo-huiddin <ref> [Moh84] </ref>. CTM uses a 5-pixel clairvoyant mask (one pixel in the middle with four pixels surrounding the middle one) to build the model of one pattern with respect to the other. <p> Mohiuddin and Howard used this per-glyph approach, while Witten 134 used the whole image approach. 11.2 Modified Clairvoyant Context Method The coding method that we use in CDIS was invented by Mohiuddin <ref> [Moh84] </ref>, and later called Soft Pattern Matching (SPM) by Howard [How96]. We call it the Modified Clairvoyant Context method (MCC). MCC is similar to the context-based compression method that we discussed in Chapter 6. The main difference between conventional context-based compression and MCC is the choice of context template. <p> Document images contain four types of structure: pixel-level structure, glyph-level structure, geometrical structure, and contextual structure. Pattern matching based coding techniques exploit these structures of document images. Pattern matching based compression has been studied by several research groups <ref> [AN74, HX86, Moh84, WBEI94] </ref> and ISO is in the process of adopt 146 ing a new JBIG2 bi-level image-coding standard of which the leading proposal is also pattern matching based [How96]. We have discussed the details of a PMBC system. The following is a review of the basic steps: 1. <p> Our method reduces the size of the compressed documents by from 50% to 80%. 13.2 Previous Work Previous Pattern Matching Based Compression systems have used proprietary file formats requiring special decompression programs which must be distributed to each user, allowing these users to view and print the compressed documents <ref> [AN74, HX86, Moh84, WBEI94, How96, ZD95] </ref>. Because there are several groups working on PMBC systems, no standard file format has been established. 148 PostScript is the standard page description language for desktop publishing. There are several image conversion utility programs that convert scanned images into PostScript format.
Reference: [MS] <author> Peter B. Mark and Stuart M. Shieber. </author> <title> Method and apparatus for compression of images. </title> <type> US patent 94:31761. </type>
Reference-contexts: In lossy mode, TIC has the best compression ratio of all previous document image compression schemes; in lossless mode, TIC has results similar to Mohiuddin's lossless image compression system. TIC achieved about 38:1 compression in lossy mode. Mark and Shieber had a similar PMBC system to TIC <ref> [MS] </ref>. Their system has some enhancements over TIC. For example, they performed precompression of document images prior to pattern matching to improve efficiency. They presented a voting scheme in conjunction with a plurality of similarity tests to improve pattern matching accuracy. <p> The exceptions are some composite symbols like i, j, : etc. Also certain symbol pairs are connected because of ligatures or noise in the image. Mark and Shieber proposed an efficient way of segmenting a bi-level image encoded by run-length <ref> [MS] </ref>. This method works by associating with each black run an identifier for the glyphs it is a part of. Each black run in the bi-level image is examined in top-to-bottom, left-to-right order. <p> Then we cannot exploit the geometrical and contextual structure in document images at all, and glyph indices and positions cannot be compressed efficiently. Because of its importance, most previous PMBC systems have this step. Some of them, Witten et al [WAB94], Howard [How96], Mark and Shieber <ref> [MS] </ref>, reported their methods. Others did not give details of their methods. Sorting glyphs into text order is straight-forward if we assume any skew has been 33 corrected and the text is single-column and oriented horizontally on the page. <p> When comparing two glyphs, one filled with black pixels, and one filled with white pixels, CTM's estimated cross entropy value is 0, resulting in a match. * Mark and Shieber used a voting scheme, where a plurality of tests are used for each glyph pair <ref> [MS] </ref>. Each passing test contributes one vote to a sum of votes. A predetermined number of votes must be received for a match. <p> Hobby and Baird [HB96] called this method the Naive Averaging algorithm (NA). NA is simple but it does not always reduce the noise from bitmaps when there are only a few glyphs in an equivalence class. Mark and Shieber used a modified NA in their system <ref> [MS] </ref>. Their method used the appearance of one of the glyphs, for example, the first glyph encountered, but allowed the other glyphs to vote on changing any given pixel. <p> So the final partition is ffa; bg; fcg; fdgg. If we change the order of the list to fb; a; c; dg, however, the algorithm finds the partition ffb; a; c; dgg. Several compression systems, including MGTIC [Wit], our CDIS [ZD96a], and <ref> [MS] </ref> combine a bitmap averaging method with the First-Fit algorithm to form a multi-pass pattern classification method, which was presented in Chapter 6. <p> These locations and the block's address yield the absolute addresses of the components. Kia's method of addressing provide direct access to the glyphs. Mark and Shieber coded the offset between two glyphs <ref> [MS] </ref>. They got additional 119 compression by figuring out the most frequent value of the inter-glyph space for the pair and encode the difference from this standard inter-glyph space. They also proposed the idea of encode the glyph position in reduced resolution.
Reference: [NW88] <editor> G. Nemhauser and L. Wolsey. </editor> <booktitle> Integer and Combinatorial Optimization, </booktitle> <pages> pages 709-712. </pages> <publisher> John Wiley & Sons, </publisher> <year> 1988. </year>
Reference-contexts: The algorithm is based on the greedy algorithm for minimizing a linear function subject to a submodular constraint <ref> [NW88] </ref> (a generalization of the greedy set-cover algorithm). Define the capped distortion of a set S to be ffi (S) = v2V The algorithm is the following. GREEDY-K-MEDIAN (G = (V; E); c; w) 1.
Reference: [PCC + 80] <author> W. K. Pratt, P. J. Captitant, W. H. Chen, E. R. Hamilton, and R.H. Wallis. </author> <title> Combing symbol matching facsimile data compression system. </title> <journal> Proc. IEEE, </journal> <volume> 68(7) </volume> <pages> 786-796, </pages> <year> 1980. </year>
Reference-contexts: These steps included Huffman coding the glyph indices according to their frequency distribution, run length coding the prototype glyph bitmaps in the codebook, and relative position coding. They estimated that their compression could be improved to 40:1. Pratt et al.'s combined symbol matching (CSM) <ref> [PCC + 80] </ref> improved the early work by coding regular symbols and non-textual marks separately. CSM left non-textual marks such as line drawings in the residue and compressed this residue using the 2-dimensional run-length coding. <p> Each such pattern has a succinct representation as a bitmap. The distance between a pattern and a glyph is the conditional entropy of the glyph given the distribution on ideal characters associated with the pattern. 5.1 Previous Work A number of pattern matching algorithms have been proposed <ref> [PCC + 80, HX86, Hal88, IW94, Kia97] </ref>. All of these methods begin by aligning the two bitmaps, and then examining the error pixels, which are the pixels which have different values in the two bitmaps. <p> These algorithms fall into two main types, depending on whether global or local criteria are employed in obtaining the matching decision. * One algorithm based on global criteria is Combined Symbol Matching (CSM) <ref> [PCC + 80] </ref>. This algorithm computes a weighted count of the error pixels, in which higher weights are given to clustered error pixels. A match is rejected if the total weighted count is higher than some threshold. A similar algorithm and a thorough discussion can be found in [Moh84]. <p> Ascher and Nagy [AN74] used fixed length codes for absolute glyph positions. They proposed but did not implement coding the offset from the previous glyph. Pratt et al.'s CSM <ref> [PCC + 80] </ref> adopted the following scheme: for any blank scan line, or the trailing blank part of a line, a special endof line code is used, and for patterns on the same line, the column address is encoded incrementally. This scheme was better than coding absolute glyph positions.
Reference: [RS79] <author> G.T. Toussaint R. Shinghal. </author> <title> Experiments in text recognition with the modified viterbi algorithm. </title> <journal> IEEE Trans. Pattern Anal. Machine Intell., </journal> <volume> PAMI-1:184-192, </volume> <month> April </month> <year> 1979. </year>
Reference-contexts: Ho et al. used multi-level contextual knowledge for recognition of degraded and connected character strings [THS91]. People have also applied Hidden Markov Models (HMM) and the Viterbi algorithm [GF73] to the problem of robust machine recognition of poorly print text <ref> [RS79, AB91] </ref>. The transition probability between character models is usually determined from the statistical studies of the type of text material the recognizer is expected to be used on.
Reference: [Sci] <institution> Cornell Computer Science. </institution> <note> Networked computer science technical reports library (ncstrl). web address www.ncstrl.org. </note>
Reference-contexts: Tiff2ps with FAX Group 4 encoding is used often to store scanned images; for instance, the Networked Computer Science Technical Reports Library (NCSTRL) <ref> [Sci] </ref>, a popular online library of computer science documents, stores documents scanned this way. Pbmtolps, one of the netpbm image utilities, is an alternative program for converting scanned images to PostScript. This program reads a portable bitmap as input, and outputs PostScript.
Reference: [SGI] <author> SGI. </author> <title> Tiff software distribution. </title> <note> The software is available for ftp on sgi.com (192.48.153.1), graphics/tiff/tiff-v3.4-tar.Z. </note>
Reference-contexts: Because there are several groups working on PMBC systems, no standard file format has been established. 148 PostScript is the standard page description language for desktop publishing. There are several image conversion utility programs that convert scanned images into PostScript format. For example, the popular program, tiff2ps, <ref> [SGI] </ref> converts one or more scanned images in tiff format to a single PostScript file, with each tiff file as one page in this PostScript file. This program does not apply further compression to the original tiff files.
Reference: [TC93] <author> International Telegraph and Telephone Consultative Committee (CCITT). </author> <title> Progressive bi-level image compression. </title> <booktitle> In Recommendation T.82, </booktitle> <year> 1993. </year>
Reference-contexts: Examples of these methods are the run-length coding used by the CCITT (International Telephone and Telegraph Consultative Committee) standard [MBS92], and the context-based predictive coding used in the JBIG (Joint Bi-level Image Experts Group) standard <ref> [TC93] </ref>. Since these methods exploit only pixel-level structure of the image, they fail to provide good compression for document images. In contract, pattern-matching based compression (PMBC) techniques exploit the glyph-level structure of document images. Pattern-matching based compression techniques are effective for text pages. <p> In the JBIG standard the arithmetic coders are a variation of the arithmetic coder know as the QM coder. JBIG has some other features such as progressive coding and adaptive context. A detailed description of JBIG can be found in <ref> [TC93] </ref>. 2.3 Pattern Matching Based Compression We introduced the basic idea of PMBC in Chapter 1. Document images have not only pixel-level structure but also glyph-level structure, and PMBC exploits both kinds of structures.
Reference: [THS91] <author> J.J. </author> <title> Hull T.K. Ho and S.N. Sarihari. Word recognition with multi-level contextual knowledge. </title> <booktitle> In Proceedings of the first international conference on document analysis and recognition, </booktitle> <address> Sanit-Malo, France, </address> <month> September </month> <year> 1991. </year> <month> 177 </month>
Reference-contexts: This kind of information is usually used in a postprocessing step. For example, many OCR systems, both commercial and research ones, have built-in spelling checkers. Ho et al. used multi-level contextual knowledge for recognition of degraded and connected character strings <ref> [THS91] </ref>. People have also applied Hidden Markov Models (HMM) and the Viterbi algorithm [GF73] to the problem of robust machine recognition of poorly print text [RS79, AB91].
Reference: [TZAK91] <author> Q. Tian, P. Zhang, T. Alexander, and Y. Kim. </author> <title> Survey: omnifont printed character recognition. </title> <booktitle> In SPIE Visual Communications of Image Processing 91: Image Processing, </booktitle> <volume> volume 1606, </volume> <pages> pages 260-268, </pages> <year> 1991. </year>
Reference-contexts: OCR usually has a geometric layout analysis or segmentation stage before actual symbol recognition. This stage usually does connected components analysis, skew correction, and segmentation. Researchers in the OCR community have proposed many segmentation and glyph extraction methods [CHP96, Bai88]. For a survey of these segmentation methods see 30 <ref> [FNK92, TZAK91] </ref>. Some of these methods are robust to skew error [CHP96, Bai88]. For our work, we assume the document is free of skewing.
Reference: [WAB94] <author> I. H. Witten, Moffat A, and T. C. Bell. </author> <title> Managing Gigabytes: compressing and indexing documents and images. </title> <publisher> Van Nostrand Reinhold, </publisher> <address> New York, </address> <year> 1994. </year>
Reference-contexts: The process scans a document image line by line from the upper-left element. When a black pixel is encountered, the boundary tracing algorithm attempts to trace the boundary of a black region, clockwise. This tracing algorithm is described in [JSC83] and <ref> [WAB94] </ref>. We repeat it here for further reference. Let us call the first black pixel (x 1 ; y 1 ). The eight neighbors of the pixel are examined, beginning at (x 1 + 1; y 1 ) and searching clockwise around (x 1 ; y 1 ). <p> Then we cannot exploit the geometrical and contextual structure in document images at all, and glyph indices and positions cannot be compressed efficiently. Because of its importance, most previous PMBC systems have this step. Some of them, Witten et al <ref> [WAB94] </ref>, Howard [How96], Mark and Shieber [MS], reported their methods. Others did not give details of their methods. Sorting glyphs into text order is straight-forward if we assume any skew has been 33 corrected and the text is single-column and oriented horizontally on the page. <p> Most previous systems reconstruct by using the first encountered glyph for each equivalence class. This can result in a noisy reconstruction. Witten et al. used a simple bitmap image averaging method which superimposes 56 the input images, adds up the intensities at each point, and thresholds the result <ref> [WAB94] </ref>. Hobby and Baird [HB96] called this method the Naive Averaging algorithm (NA). NA is simple but it does not always reduce the noise from bitmaps when there are only a few glyphs in an equivalence class. Mark and Shieber used a modified NA in their system [MS]. <p> Witten's experiments have shown that ordinary text with spaces can be compressed better than text without spaces, even though these space characters increased the number of symbols that have to be coded <ref> [WAB94] </ref>. The extra structured information that the space character provides to the compression algorithm more than compensates for the overhead of coding the characters. For this reason our CDIS includes spaces in the glyph index stream. The method of inserting space glyphs was discussed in Chapter 4. <p> For the purposes of a fair comparison with TIC, the previous state of the art system which 126 is part of the public domain Managing Gigabytes (MG) system <ref> [WAB94] </ref>, we started from the TIC source-pool and replaced only the position coding module, so that the rest of our system is identical. We ran CDIS (really TIC with improved position coding here) and TIC on the same test document set and measured the overall compression ratio for each system.
Reference: [WBEI94] <author> I. H. Witten, T. C. Bell, H. Emberson, and S. Inglis. </author> <title> Textual image compression: two-stage lossy/lossless encoding of textual images. </title> <journal> Proc. IEEE, </journal> <volume> 86(6) </volume> <pages> 878-888, </pages> <year> 1994. </year>
Reference-contexts: In contract, pattern-matching based compression (PMBC) techniques exploit the glyph-level structure of document images. Pattern-matching based compression techniques are effective for text pages. These techniques were originally proposed in [AN74] and further studied in [HX86], [Moh82], and <ref> [WBEI94] </ref>. [WBEI94] and [Moh84] discuss lossless compression based on pattern matching. The International Standards Organization (ISO) is in the process of adopting a new JBIG2 bi-level image coding standard, for which the leading proposal is also pattern matching based [How96]. <p> In contract, pattern-matching based compression (PMBC) techniques exploit the glyph-level structure of document images. Pattern-matching based compression techniques are effective for text pages. These techniques were originally proposed in [AN74] and further studied in [HX86], [Moh82], and <ref> [WBEI94] </ref>. [WBEI94] and [Moh84] discuss lossless compression based on pattern matching. The International Standards Organization (ISO) is in the process of adopting a new JBIG2 bi-level image coding standard, for which the leading proposal is also pattern matching based [How96]. <p> He coded the glyph positions using the statistical distribution of position information and arithmetic coding. For glyph indices, he used fixed-length coding. Mohiuddin's system achieved about 30:1 compression ratio. Witten et al. described a two-stage lossy/lossless compression system for document images called Textual Image Compression (TIC) <ref> [WBEI94] </ref>. They independently invented MCC for lossless compression. TIC's main contribution was better 19 compression for glyph indices and positions. TIC used Prediction by Partial Match--ing (PPM) [CW84] modeling and arithmetic coding [BCW90] to code glyph indices. <p> For lossless compression, we also transmit the difference image, which is usually called the residual image. We code the residuals on a per-glyph basis (Mohiuddin [Moh84] and Howard's approach [How96]) rather than coding it for the entire image (as in MGTIC <ref> [WBEI94] </ref>). We used a method similar to Mohiuddin's which uses the 26 matching glyph in the codebook as part of the context to code the residual image. Furthermore, most pixels in the residual image are predicted by our EPM model. <p> We hope to use this contextual information to improve clustering accuracy and compression performance. We investigate different ways to estimate the indexing cost and adopt the order-1 context model. This experiment, however, does not show any significant improvement over the plain GKM. 7.1 Previous Work Previous systems <ref> [WBEI94, How96] </ref> used a simple codebook design algorithm which we call the First Fit algorithm. This algorithm was introduced in Chapter 5. We review this algorithm here: FIRST-FIT (S) 1. <p> We compute cost (cjp) using 87 the spatial sampling error based cross entropy measure mentioned above [ZD96b]. We also implemented the First Fit algorithm with multi-pass optimization as used in the MGTIC system <ref> [WBEI94] </ref>. We have tested these configurations on ten high quality scanned document pages with over 20,000 glyphs. Two of the pages, CCITT1 and CCITT4, are from the CCITT test image set, scanned at 200 dpi. <p> GKM followed by modified k-means results in smaller number of pattern for all the test documents. The average improvement is 13.7%. (used in MGTIC <ref> [WBEI94] </ref>) because it produces slightly better results and runs faster than the best match version. Compared with the First Fit algorithm, GKM reduces the codebook size by an average of 26%, a remarkable improvement. Notice that for CCITT4, the number of patterns grows. <p> Huffman codes yield the best compression possible for a prefix-free code for the given probability distribution, but zero-order statistics do not yield 111 very good compression. Witten et al. used the PPM text compression scheme for glyph indices <ref> [WBEI94] </ref>. This scheme uses variable order models and arithmetic coding. Variable-order models capture the contextual structure better than order-0 model. For instance, while zero-order models learn character frequencies, variable order models can learn word and phrase frequencies. Arithmetic coding is also somewhat more efficient than Huffman coding. <p> They got additional 119 compression by figuring out the most frequent value of the inter-glyph space for the pair and encode the difference from this standard inter-glyph space. They also proposed the idea of encode the glyph position in reduced resolution. Witten et al.'s TIC <ref> [WBEI94] </ref> codes the offset between one glyph and the next, instead of the absolute positions. These offsets are coded using a first-order predictive model based on the glyph index. This method predicted offsets more accurately than just looking at the zero-order offset frequency distribution as Mohiuddin proposed. <p> The best known method for encoding lossless images is called the Modified Clairvoyant Context (MCC) method. This method was originally proposed by Mohiuddin [Moh82], and later used by Witten <ref> [WBEI94] </ref> and Howard [How96]. The basic idea of MCC is encode each isolated glyph losslessly using a matching glyph in the codebook as part of the conditioning region of support (ROS). <p> It seems that we can compress the residual image more efficiently than the original. However, as Witten et al. have observed, this residual image is actually more difficult to compress than the original image <ref> [WBEI94] </ref>. The original image is far more compressible than the residual image because most of the black pixels it contains form predictable parts of characters. On the other hand, in the residual image, the black pixels are mostly noise along the edges of the glyphs. <p> Document images contain four types of structure: pixel-level structure, glyph-level structure, geometrical structure, and contextual structure. Pattern matching based coding techniques exploit these structures of document images. Pattern matching based compression has been studied by several research groups <ref> [AN74, HX86, Moh84, WBEI94] </ref> and ISO is in the process of adopt 146 ing a new JBIG2 bi-level image-coding standard of which the leading proposal is also pattern matching based [How96]. We have discussed the details of a PMBC system. The following is a review of the basic steps: 1. <p> Our method reduces the size of the compressed documents by from 50% to 80%. 13.2 Previous Work Previous Pattern Matching Based Compression systems have used proprietary file formats requiring special decompression programs which must be distributed to each user, allowing these users to view and print the compressed documents <ref> [AN74, HX86, Moh84, WBEI94, How96, ZD95] </ref>. Because there are several groups working on PMBC systems, no standard file format has been established. 148 PostScript is the standard page description language for desktop publishing. There are several image conversion utility programs that convert scanned images into PostScript format. <p> Some symbols, however, have two parts and are initially extracted as two glyphs. The glyphs are sorted into the original text order after they are extracted, using some simple heuristics similar to those used by Witten's Textual Image Compression <ref> [WBEI94] </ref>. Once glyphs are extracted and sorted in test order, their positions can be more economically coded as relative offsets. Relative coding results in a stream of small integers which are much more compressible than absolute positions would be. <p> The decompression routine is slightly expanded FAX Group 4 is not the best bitmap compression method. Our variable length code is not the best way to compress glyph indices and positions. In our regular CDIS, 152 we use Moffatt's two-level context-based method <ref> [WBEI94] </ref> for the codebook, PPMC compression for glyph indices [BCW90] and structure-based position coding for glyph positions [ZD95]. These techniques achieve higher compression ratios via increased complexity. In a self-extracting PostScript program, this increased complexity would be included in the final file size, which would be prohibitive.
Reference: [Wei] <author> Peter Weingartner. </author> <note> A first guide to postscript. available from www.cs.indiana.edu/docproject/. </note>
Reference-contexts: A few important concepts, which are related to our PostScript implementation of compressed images, are presented. [Inc85] and [Inc90] have detailed information about PostScript. <ref> [Wei] </ref>, which this section is based on, is an excellent PostScript tutorial. PostScript is a stack-based programming language. A program pushes arguments 149 to an operator onto a stack and then invokes the operator. As an example, let us say we want to add 2 and 14.
Reference: [Wit] <author> I. H. Witten. </author> <title> Software kit for mg. </title> <note> Available via anonymous ftp from munnari.oz.au in the directory /pub/mg. </note>
Reference-contexts: Half of the code consists of the supporting utilities: the arithmetic coder, the image libraries to read and write images, and the basic glyph extractor. Some of these libraries were written by Inglis for the MGTIC system <ref> [Wit] </ref>. We also built a special version of CDIS, which is called pbm2cps. This version provides better access for the compressed documents. It does not have all the components of the complete version. <p> So the final partition is ffa; bg; fcg; fdgg. If we change the order of the list to fb; a; c; dg, however, the algorithm finds the partition ffb; a; c; dgg. Several compression systems, including MGTIC <ref> [Wit] </ref>, our CDIS [ZD96a], and [MS] combine a bitmap averaging method with the First-Fit algorithm to form a multi-pass pattern classification method, which was presented in Chapter 6.
Reference: [ZD95] <author> Qin Zhang and John M. Danskin. </author> <title> A pattern based document image compression system. </title> <journal> Electronic Publishing, </journal> <volume> 8(2 </volume> & 3):235-246, June and September 1995. 
Reference-contexts: The mixed results of the k-means algorithm may be due to thresholding effects. Table 7.3 compares overall compression ratios achieved using GKM and First Fit. We used the same lossy glyph position coding scheme <ref> [ZD95] </ref> and Entropy-based Pattern Matching (EPM) [ZD96b] to compress the test images. With the exception of CCITT4, GKM dominated First Fit. The use of GKM reduced the size of the compressed documents by an average of 17%. We also compared reconstructed images resulting from plain First Fit and GKM. <p> Our method reduces the size of the compressed documents by from 50% to 80%. 13.2 Previous Work Previous Pattern Matching Based Compression systems have used proprietary file formats requiring special decompression programs which must be distributed to each user, allowing these users to view and print the compressed documents <ref> [AN74, HX86, Moh84, WBEI94, How96, ZD95] </ref>. Because there are several groups working on PMBC systems, no standard file format has been established. 148 PostScript is the standard page description language for desktop publishing. There are several image conversion utility programs that convert scanned images into PostScript format. <p> Our variable length code is not the best way to compress glyph indices and positions. In our regular CDIS, 152 we use Moffatt's two-level context-based method [WBEI94] for the codebook, PPMC compression for glyph indices [BCW90] and structure-based position coding for glyph positions <ref> [ZD95] </ref>. These techniques achieve higher compression ratios via increased complexity. In a self-extracting PostScript program, this increased complexity would be included in the final file size, which would be prohibitive.
Reference: [ZD96a] <author> Qin Zhang and John M. Danskin. </author> <title> Bitmap reconstruction for document image compression. </title> <booktitle> In Proceedings SPIE Multimedia Storage and Archiving Systems, </booktitle> <volume> volume 2916, </volume> <pages> pages 188-199, </pages> <address> Boston, Massachuetts, </address> <month> November </month> <year> 1996. </year>
Reference-contexts: So the final partition is ffa; bg; fcg; fdgg. If we change the order of the list to fb; a; c; dg, however, the algorithm finds the partition ffb; a; c; dgg. Several compression systems, including MGTIC [Wit], our CDIS <ref> [ZD96a] </ref>, and [MS] combine a bitmap averaging method with the First-Fit algorithm to form a multi-pass pattern classification method, which was presented in Chapter 6.
Reference: [ZD96b] <author> Qin Zhang and John M. Danskin. </author> <title> Entropy-based pattern matching for document image compression. </title> <booktitle> In Proceedings of the International Con 178 ference on Image Processing 1996, </booktitle> <pages> pages 192-199, </pages> <address> Lausanne,Switzerland, </address> <month> September </month> <year> 1996. </year>
Reference-contexts: Recent work has used relatively ad-hoc heuristics for this step, with the most successful bearing a resemblance to Lloyd's algorithm for vector quantizer design [GG92]. The conceptual basis of the new method is an extension of a particular cross-entropy model, proposed and approximated in Chapter 5 and <ref> [ZD96b] </ref>, for measuring distances between bitmaps. Here is a summary of this model. Each bitmap is assumed to have been generated by scanning an ideal character (a blob of ink) at some random 70 offset. Thus, any ideal character induces a probability distribution on the bitmaps. <p> We compute cost (cjp) using 87 the spatial sampling error based cross entropy measure mentioned above <ref> [ZD96b] </ref>. We also implemented the First Fit algorithm with multi-pass optimization as used in the MGTIC system [WBEI94]. We have tested these configurations on ten high quality scanned document pages with over 20,000 glyphs. <p> Table 7.1 compares the sizes of the codebook produced by the First Fit code-book design algorithm with and without the multi-pass optimization, and our GKM algorithm. In each case we used the same underlying pattern matching algorithm, EPM <ref> [ZD96b] </ref>. We use the first match version of the modified k-means optimization 88 cluster is in a bounding box, and is the first glyph in a group of glyphs. The order of the clusters in the page is the order of clusters and centers produced by running GKM. <p> The mixed results of the k-means algorithm may be due to thresholding effects. Table 7.3 compares overall compression ratios achieved using GKM and First Fit. We used the same lossy glyph position coding scheme [ZD95] and Entropy-based Pattern Matching (EPM) <ref> [ZD96b] </ref> to compress the test images. With the exception of CCITT4, GKM dominated First Fit. The use of GKM reduced the size of the compressed documents by an average of 17%. We also compared reconstructed images resulting from plain First Fit and GKM.
Reference: [ZD97] <author> Qin Zhang and John M. Danskin. </author> <title> Better postscript than postscript: portable self-extracting postscript representation of scanned document images. </title> <editor> In Luc M. Vincent and Jonathan J. Hull, editors, </editor> <booktitle> Proceedings SPIE Document Recognition IV, </booktitle> <volume> volume 3027, </volume> <pages> pages 219-230, </pages> <address> San Jose, Califor-nia, </address> <month> February </month> <year> 1997. </year>
Reference-contexts: P bm2cps has 2500 lines of code in C++ and uses the C++ Standard Template Library. It has been successfully ported to Linux and OpenStep operating system. 27 Our report of this version was <ref> [ZD97] </ref> one of the most notable contributions in the SPIE Document Recognition IV Conference. The source code is freely available on www:cs:dartmouth:edu= ~ zhang and has been down-loaded over 200 times (last counted on May 14, 1997).
Reference: [ZDY97] <author> Qin Zhang, John M. Danskin, and Neal E. Young. </author> <title> A codebook generation algorithm for document image compression. </title> <editor> In James Storer and Martin Cohn, editors, </editor> <booktitle> Proceedings IEEE Data Compression Conference, </booktitle> <pages> pages 300-309, </pages> <address> Snowbird, Utah, March 1997. </address> <publisher> IEEE Computer Society Press. </publisher>
Reference-contexts: GKM runs in polynomial time and produces a codebook which does not depend on the order of the glyph. This is joint work with Neal Young <ref> [ZDY97] </ref>. * Entropy-based pattern matching (EPM): We assume that the most important errors are quantization errors due to the shift in registration of the characters 12 relative to a scanning grid, and these error give rise to many versions of the same character. <p> The main contribution of this chapter is a new method for the partitioning step. This work is a joint work with Danskin and Young <ref> [ZDY97] </ref>. Recent work has used relatively ad-hoc heuristics for this step, with the most successful bearing a resemblance to Lloyd's algorithm for vector quantizer design [GG92]. <p> This process is iterated until there is little or no reduction in the size of the codebook. This multi-pass codebook generation method bears a resemblance to Lloyd's algorithm for vector quantizer design [GG92]. More sophisticated codebook design algorithms are possible as we presented in Chapter 7 and <ref> [ZDY97] </ref>. For this work, we opt for the simpler First Fit. 13.5 PostScript Output After partitioning the glyphs, we encode the codebook, glyph indices and positions, and the decompression routine in a PostScript file.
Reference: [Zha] <author> Qin Zhang. </author> <title> pbm2cps binary for dec alpha unix workstation. </title> <note> available from www.cs.dartmouth.edu/ ~ zhangq. </note>
Reference-contexts: P bm2cps has 2500 lines of code in C++ and uses the C++ Standard Template Library. It runs under DEC Unix, Linux, SunOS, and OpenStep operating systems. The source code and DEC Alpha binaries for pbm2cps are currently freely available on the World Wide Web <ref> [Zha] </ref>. It has been down-loaded over 200 times (last counted on June 29, 1997). 167 Chapter 14 Discussion 14.1 Summary This thesis presented a complete Pattern Matching Based Compression (PMBC) system for document images. In Chapter 2 we studied some previous document image compression systems.
Reference: [ZLD97] <author> Yeqing Zhang, Robert P. Loce, and Edward R. Dougherty. </author> <title> Document restoration and enhancement using optimal iterative and paired morphological filters. </title> <editor> In Luc M. Vincent and Jonathan J. Hull, editors, </editor> <booktitle> SPIE Proceedings Document Recognition IV, </booktitle> <volume> volume 3027, </volume> <pages> pages 109-123, </pages> <address> San Jose, California, </address> <month> February </month> <year> 1997. </year> <month> 179 </month>
Reference-contexts: The method can prevent artifacts of the averaging process, which can occur when a small number of glyphs are averaged. Y. Zhang, Loce and Dougherty discussed designing optimal iterative and paired morphological filters to restore and enhance documents <ref> [ZLD97] </ref>. They applied different filters to enhance digital documents. Iterative design is illustrated in the context of document restoration for dilated, background-noise images; iterative, paired design is illustrated in for restoration of edge-degraded characters; and paired design is used for integer resolution conversion.
References-found: 54

