URL: ftp://whitechapel.media.mit.edu/pub/tech-reports/TR-329.ps.Z
Refering-URL: http://www-white.media.mit.edu/cgi-bin/tr_pagemaker/
Root-URL: http://www.media.mit.edu
Email: bobick pinhanez@media.mit.edu  
Title: Using Approximate Models as Source of Contextual Information for Vision Processing  
Author: Aaron F. Bobick and Claudio S. Pinhanez 
Abstract: M.I.T Media Laboratory Perceptual Computing Section Technical Report No. 329 To appear in the IEEE Workshop on Context-Based Vision (ICCV'95) This paper is a revised version of TR #313 (January 1995) April 1995 Abstract Most computer vision algorithms are based on strong assumptions about the objects and the actions depicted in the image. To safely apply those algorithms in real world image sequences, it is necessary to verify that their assumptions are satisfied in the context of the visual process. We propose the use of approximate world models coarse descriptions of objects and actions in the world | as the appropriate representation for contextual information. The approximate world models are employed to verify the applicability of a vision routine in a given situation. Under these conditions, a task module can reliably use the outputs of the contextually-safe vision routines, without having to refer to an accurate reconstruction of the world. We are using approximate world models in a project to control cameras in a TV studio. In our Intelligent Studio automatic cameras respond to verbal requests for shots from the TV director. Contextual information is obtained from the script of the TV show and from the imagery provided by wide-angle, low-resolution cameras monitoring the studio. Some examples of the cameras' responses to different requests are shown in the domain of a cooking show. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> J. F. Allen, </author> <title> "Towards a General Theory of Action and Time," </title> <journal> Artificial Intelligence, </journal> <volume> vol. 23, </volume> <pages> pp. 123-154, </pages> <year> 1984. </year>
Reference: [2] <author> J. Y. Aloimonos, </author> <title> "Purposive and Qualitative Active Vision," </title> <booktitle> Proc. of Image Understanding Workshop, </booktitle> <address> Pittsburgh, Pennsylvania, </address> <pages> pp. 816-828, </pages> <month> September </month> <year> 1990. </year>
Reference: [3] <author> D. J. Beymer, </author> <title> "Face Recognition Under Varying Pose," </title> <booktitle> Proc. of CVPR'94, </booktitle> <address> Seattle, Washington, </address> <month> June 21-23, </month> <pages> pp. 756-761, </pages> <year> 1994. </year>
Reference: [4] <author> A. F. Bobick and R. C. Bolles, </author> <title> "Representation Space: An Approach to the Integration of Visual Information," </title> <booktitle> Proc. of the IEEE Conference on Computer Vision and Pattern Recognition, </booktitle> <address> San Diego, California, </address> <month> June 4-8, </month> <pages> pp. 492-499, </pages> <year> 1989. </year>
Reference-contexts: The basic idea is to use an approximate world model to determine whether the applicability conditions of vision routines are satisfied by the current world situation. We are employing a multi-representational system (similar to <ref> [4] </ref>) where the right representation for a given object is selected depending upon the task and the situation. Our proposal falls between the strict reconstructionist and purely purposive strategies currently debated in the community ([2, 16]).
Reference: [5] <author> D. D. Fu, K. J. Hammond, and M. J. Swain, </author> <title> "Vision and Navigation in Man-made Environments: Looking for syrup in all the right places," </title> <booktitle> Proc. of the Workshop on Visual Behaviors, </booktitle> <address> Seattle, Washington, </address> <month> June 19, </month> <pages> pp. 20-26, </pages> <year> 1994. </year>
Reference: [6] <author> A. R. Hanson and E. M. Riseman, </author> <title> "VISIONS: A computer system for interpreting scenes," Computer Vision Systems, </title> <publisher> Academic Press, </publisher> <address> New York, </address> <pages> pp. 303-333, </pages> <year> 1978. </year>
Reference: [7] <author> B. Horn and M. J. Brooks (eds.), </author> <title> Shape from Shading, </title> <publisher> The MIT Press, </publisher> <address> Cambridge, Mas-sachusetts, 577 pgs, </address> <year> 1989. </year>
Reference: [8] <author> D. Israel, J. Perry, and S. Tutiya, </author> <title> "Actions and Movements," </title> <booktitle> 12th IJCAI, </booktitle> <address> Sydney, Australia, </address> <month> August 24-30, </month> <pages> pp. 1060-1065, </pages> <year> 1991. </year>
Reference: [9] <author> R. C. Jain and T. O. Binford, </author> <title> "Ignorance, </title> <booktitle> Myopia, and Naivete in Computer Vision Systems," CVGIP: Image Understanding, </booktitle> <volume> vol. 53(1), </volume> <pages> pp. 112-117, </pages> <month> January </month> <year> 1991. </year>
Reference-contexts: For instance, if the context representations contain information about depth and shape, it is possible to check for occluded objects and to avoid the calling of routines sensitive to occlusion. Our hope is to be able to cope with one of the recurring criticisms (e.g. <ref> [9] </ref>) of much of computer vision, that many of the developed techniques are brittle, functioning well only if some set of restrictive assumptions about the situation are true. These assumptions may be considered "restrictive" if they are often false.
Reference: [10] <author> Y. Kuniyoshi and H. Inoue, </author> <title> "Qualitative Recognition of Ongoing Human Action Sequences," </title> <booktitle> Proc. of IJCAI-93, </booktitle> <pages> pp. 1600-1609, </pages> <year> 1993. </year>
Reference-contexts: Research in action recognition has been more rare (see the work of Kuniyoshi and Inoue <ref> [10] </ref>), though we believe the use of approximate models can significantly facilitate the provision of the contextual information which is essential for action recognition.
Reference: [11] <author> D. B. Lenat and R. V. Guha, </author> <title> Building Large Knowledge-Based Systems, </title> <publisher> Addison-Wesley, </publisher> <year> 1990. </year>
Reference: [12] <author> Y. Mansour and R. Rivest, </author> <title> "Results on Learnabil-ity and the Vapnik-Chervonenkis Dimension," </title> <journal> Information and Computation, </journal> <volume> vol. 90(1), </volume> <pages> pp. 33-49, </pages> <month> January </month> <year> 1991. </year>
Reference: [13] <author> B. Moghaddam and A. Pentland, </author> <title> "Face Recognition using View-Based and Modular Eigenspaces," Automatic Systems for the Identification and Inspection of Humans, </title> <booktitle> SPIE vol. </booktitle> <volume> 2277, </volume> <month> July </month> <year> 1994. </year>
Reference: [14] <author> D. Newtson, G. Engquist, and J. Bois, </author> <title> "The Objective Basis of Behavior Units," </title> <journal> Journal of Personality and Social Psychology, </journal> <volume> vol. 35(12), </volume> <pages> pp. 847-862, </pages> <month> December </month> <year> 1977. </year>
Reference: [15] <author> J. </author> <title> Oliensis,"Shape from Shading as a Partially Well-Constrained Problem," CVGIP: </title> <booktitle> Image Understanding, </booktitle> <volume> vol. 54(2), </volume> <pages> pp. 75-104, </pages> <year> 1991. </year>
Reference: [16] <author> C. Pinhanez, </author> <title> "Controlling a Highly Reactive Camera Using a Subsumption Architecture," </title> <booktitle> Proc. of Applications of AI 93: Machine Vision and Robotics, </booktitle> <address> Orlando, Florida, </address> <pages> pp. 100-111, </pages> <year> 1993. </year>
Reference: [17] <author> R. Polana and R. Nelson, </author> <title> "Low Level Recognition of Human Motion," </title> <booktitle> Proc. of IEEE Workshop on Motion of Non-Rigid and Articulated Objects, </booktitle> <address> Austin, Texas, </address> <month> November 11-12, </month> <pages> pp. 77-82, </pages> <year> 1994. </year>
Reference: [18] <author> P. N. Prokopowicz, M. J. Swain, and R. E. Kahn, </author> <title> "Task and Environment-Sensitive Tracking," </title> <booktitle> Proc. of the Workshop on Visual Behaviors, </booktitle> <address> Seattle, Washington, </address> <month> June 19. </month> <pages> pp. 73-78, </pages> <year> 1994. </year>
Reference: [19] <author> K. Rohr, </author> <title> "Towards Model-Based Recognition of Human Movements in Image Sequences," </title> <booktitle> CGVIP: Image Understanding, </booktitle> <volume> vol. 59(1), </volume> <pages> pp. 94-15, </pages> <month> January </month> <year> 1994. </year>
Reference: [20] <author> T. M. Strat and M. A. Fischler, </author> <title> "Context-Based Vision: Recognizing Objects Using Information from Both 2-D and 3-D Imagery," </title> <journal> IEEE PAMI, </journal> <volume> vol. 13(10), </volume> <pages> pp. 1050-1065, </pages> <month> October </month> <year> 1991. </year>
Reference: [21] <author> M. J. Tarr and M. J. Black, </author> <title> "A Computational and Evolutionary Perspective of the Role of Representation in Vision," CVGIP: </title> <booktitle> Image Understanding, </booktitle> <volume> vol. 60(1), </volume> <pages> pp. 65-73, </pages> <month> July </month> <year> 1994. </year>
Reference-contexts: The approximate world model is not to be used directly as a source of the perceptual information required by specific tasks. It is interesting to situate our scheme in the ongoing debate about reconstructionist vs. purposive vision (see <ref> [21] </ref> and the replies in the same issue). We are arguing that reconstruction should exist at the approximate level, guiding the purposive vision routines of the view-based level of representation.
Reference: [22] <author> R. </author> <title> Thibadeau, </title> <journal> "Artificial Perception of Actions," Cognitive Science, </journal> <volume> vol. 10, </volume> <pages> pp. 117-149, </pages> <year> 1986. </year>
Reference: [23] <author> J. K. Tsotsos, J. Mylopoulos, H. D. Covvey, and S. W. Zucker, </author> <title> "A Framework for Visual Motion Understanding," </title> <journal> IEEE-PAMI, </journal> <volume> vol. 2(6), </volume> <pages> pp. 563-573, </pages> <month> November </month> <year> 1980. </year> <month> 8 </month>
Reference: [24] <author> J. K. Tsotsos, </author> <title> "On the Relative Complexity of Active vs. Passive Visual Search," </title> <journal> IJCV, </journal> <volume> vol. 7(2), </volume> <pages> pp. 127-141, </pages> <year> 1992. </year>
Reference: [25] <author> H. Zettl, </author> <title> Television Production Handbook, 4th Edition, </title> <publisher> Wadsworth Publishing, </publisher> <address> Belmont, Cali-fornia, </address> <year> 1984. </year> <month> 9 </month>
Reference-contexts: Direction of sight is important because profiles are always framed leaving some space in front of the face (called "nose room"). The height of the eyes is used in a rule of thumb stating that eyes should be leveled at two thirds of the height of the screen (see <ref> [25] </ref>, pp.111-122, for this and other simple rules). Framing also requires knowledge about the current actions, as is exemplified by an example detailed later. A "cooking show" is the first domain in which we are experimenting with our SmartCams.
References-found: 25

