URL: http://www.cs.toronto.edu/~micah/pubs/mst.ps
Refering-URL: http://www.cs.toronto.edu/~micah/pubs/pubs.html
Root-URL: 
Title: Communication-Optimal Parallel Minimum Spanning Tree Algorithms Extended Abstract  
Author: Micah Adler Wolfgang Dittrich Ben Juurlink Miros law Kuty lowski Ingo Rieping 
Abstract: Lower and upper bounds for finding a minimum spanning tree (MST) in a weighted undirected graph on the BSP model are presented. We provide the first non-trivial lower bounds on the communication volume required to solve the MST problem. Let p denote the number of processors, n the number of nodes of the input graph, and m the number of edges of the input graph. We show that in the worst case a total of (min(m; pn)) bits need to be transmitted in order to solve the MST problem, where is the number of bits required to represent a single edge weight. This implies that if each message contains bits, any BSP algorithm for finding an MST requires communication time (g min(m=p; n)), where g is the gap parameter of the BSP model. In addition, we present two algorithms whose running times match the lower bounds in different situations. Both algorithms perform linear work and use a number of super-steps independent of the input size. The first algorithm is simple but can employ at most m=n processors efficiently. Hence, it should be applied in situations where the input graph is relatively dense. The second algorithm is a randomized algorithm that performs linear work with high probability, provided that m n log p. This is the first linear work BSP algorithm for finding an MST in sparse graphs. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> M. Adler, J. W. Byers, and R. M. Karp. </author> <title> Parallel sort ing with limited bandwidth. </title> <booktitle> In Annual ACM Symposium on Parallel Algorithms and Architecture, </booktitle> <pages> pages 129-136, </pages> <year> 1995. </year>
Reference-contexts: Such algorithms are called com-munication-efficient. For "regular" problems such as matrix multiplication, FFT and LU decomposition, this is relatively easy to achieve. Communication-efficient algorithms have also been obtained for a number of "irregular" problems such as sorting <ref> [15, 1, 17, 14] </ref>, multisearch [5, 4, 18] and parallel priority queues [6]. However, for many graph problems such as list ranking, connected components and minimum spanning tree, it seems difficult to find communication-efficient algorithms.
Reference: [2] <author> M. Adler, W. Dittrich, B. Juurlink, M. Kuty lowski, and I. Rieping. </author> <title> Towards an Efficient Minimum Spanning Tree Algorithm. </title> <type> Technical report, </type> <institution> University of Pader-born, Computer Science Dept., </institution> <year> 1998. </year>
Reference-contexts: Algorithm mst-dense is described in Section 4. A top-level description of algorithm mst-sparse is given in Section 5. It employs three subalgo-rithms which are described in Sections 6 through Section 8. The full version of this paper is available as a technical report <ref> [2] </ref>. 2 Communication Lower Bounds for MST Communication Volume. We here present a general, model independent, framework for proving lower bounds on the amount of communication required by an algorithm. <p> It uses three subroutines: bsp-as, bsp-as-select, and bsp-verify. They are described in Section 6, 7 and 8, respectively. Subroutine bsp-verify is only sketched here, a detailed description can be found in the full paper <ref> [2] </ref>. Let k = m=n be the density of the input graph. Algorithm 2 mst-sparse (G; k) (1) Reduce the number of nodes of G by a factor k by performing log k rounds of algorithm bsp-as-select. <p> It improves upon the algorithm given in [23] by using fewer super-steps. Furthermore, bsp-verify employs a simple load balancing technique in order to minimize the number of times the edges have to be communicated. For a detailed description of the algorithm, the reader is referred to <ref> [2] </ref>. Lemma 8 Let n p and m n log p. The BSP cost of bsp-verify is given by W = O ( m p + n 6 m p ) + O ( n
Reference: [3] <author> B. Awerbuch and Y. Shiloach. </author> <title> New connectivity and MSF algorithms for Shu*e-Exchange network and PRAM. </title> <journal> IEEE Transactions on Computers, </journal> <volume> C-36(10):1258-1263, </volume> <year> 1987. </year>
Reference-contexts: For example, it uses a BSP implementation of the algorithm due to Awerbuch and Shiloach <ref> [3] </ref>. This is a CRCW PRAM algorithm, and thus the best known work-preserving emulation on the BSP model needs (n + m)=p p * slackness, constant * &gt; 0. <p> Thus, it performs linear work provided that m n log p, and when np m n log p, the communication time matches the lower bound. Previous Work. Many PRAM algorithms for computing the MST exist. The algorithm due to Awerbuch and Shiloach <ref> [3] </ref> runs in O (log n) time on an (n + m)-processor PRIORITY CRCW PRAM, where the priority of a processor is determined by the weight of the edge assigned to it. This is a very powerful contention resolution rule. <p> It is easy to verify that this choice affects the running time of the algorithm by at most a constant factor. 2 6 BSP Implementation of the Awerbuch Shiloach Algorithm In this section, we describe a BSP implementation of the CRCW PRAM algorithm due to Awerbuch and Shiloach <ref> [3] </ref>, which will be denoted by bsp-as. Briefly, the algorithm iteratively executes three steps. In the first step, all processors assigned to edges emanating from a star (tree of depth 1) try to hook the star onto another tree.
Reference: [4] <author> A. Baumker and W. Dittrich. </author> <title> Fully Dynamic Search Trees for an Extension of the BSP Model. </title> <booktitle> In Annual ACM Symposium on Parallel Algorithms and Architecture, </booktitle> <year> 1996. </year>
Reference-contexts: Such algorithms are called com-munication-efficient. For "regular" problems such as matrix multiplication, FFT and LU decomposition, this is relatively easy to achieve. Communication-efficient algorithms have also been obtained for a number of "irregular" problems such as sorting [15, 1, 17, 14], multisearch <ref> [5, 4, 18] </ref> and parallel priority queues [6]. However, for many graph problems such as list ranking, connected components and minimum spanning tree, it seems difficult to find communication-efficient algorithms. For many of these problems, very fast (linear time) sequential algorithms exist, but efficient parallel algorithms are unknown.
Reference: [5] <editor> A. Baumker, W. Dittrich, and F. Meyer auf der Heide. </editor> <title> Truly efficient parallel algorithms: c-optimal multi-search for an extension of the BSP model. </title> <booktitle> In Proc. of the Annual European Symposium on Algorithms, </booktitle> <year> 1995. </year>
Reference-contexts: 1 Introduction Parallel computation models like the BSP [26], the BSP* <ref> [5] </ref>, the QSM [16], the CGM [12] and the LogP [11] are used more and more often for designing and analyzing parallel algorithms. <p> Such algorithms are called com-munication-efficient. For "regular" problems such as matrix multiplication, FFT and LU decomposition, this is relatively easy to achieve. Communication-efficient algorithms have also been obtained for a number of "irregular" problems such as sorting [15, 1, 17, 14], multisearch <ref> [5, 4, 18] </ref> and parallel priority queues [6]. However, for many graph problems such as list ranking, connected components and minimum spanning tree, it seems difficult to find communication-efficient algorithms. For many of these problems, very fast (linear time) sequential algorithms exist, but efficient parallel algorithms are unknown.
Reference: [6] <editor> A. Baumker, W. Dittrich, F. Meyer auf der Heide, and I. Rieping. </editor> <title> Realistic parallel algorithms: Priority queue operations and selection for the BSP* model. </title> <booktitle> In Proc. of the Annual International EURO-PAR Conference, </booktitle> <publisher> LNCS, </publisher> <year> 1996. </year>
Reference-contexts: Such algorithms are called com-munication-efficient. For "regular" problems such as matrix multiplication, FFT and LU decomposition, this is relatively easy to achieve. Communication-efficient algorithms have also been obtained for a number of "irregular" problems such as sorting [15, 1, 17, 14], multisearch [5, 4, 18] and parallel priority queues <ref> [6] </ref>. However, for many graph problems such as list ranking, connected components and minimum spanning tree, it seems difficult to find communication-efficient algorithms. For many of these problems, very fast (linear time) sequential algorithms exist, but efficient parallel algorithms are unknown. <p> For result (b) we employ the selection algorithm given in <ref> [6] </ref>, but slightly increase the degree of the broadcast tree. This increases the communication time, but reduces the number of supersteps. Since other parts of algorithm mst-sparse require g (n + m)=p communication time, this technique affects the total complexity only by a constant factor.
Reference: [7] <author> G. E. Blelloch. </author> <title> Scans as primitive parallel operations. </title> <journal> IEEE Transactions on Computers, </journal> <pages> pages 1526-1538, </pages> <year> 1989. </year>
Reference-contexts: Our approach uses a data structure in which all edges incident to a node are stored consecutively. We show that this technique, which is similar to a technique developed by Blelloch <ref> [7] </ref> for the SCAN model of parallel computation, can also be applied for the BSP model. One novelty of our technique is that two edge arrays are needed; one to represent the graph G and the other to represent the forest which is created in the course of the algorithm.
Reference: [8] <author> E. Caceres, F. Dehne, A. Ferreira, P. Flocchini, I. Rieping, A. Roncato, N. Santoro, and S. Song. </author> <title> Efficient parallel graph algorithms for coarse grained mul-ticomputers and BSP. </title> <booktitle> In Annual International Colloquium on Automata, Languages and Programming (ICALP), </booktitle> <year> 1997. </year>
Reference-contexts: Lemma 3 (a) <ref> [8] </ref> Ranking a list of size n and computing the Euler-Tour of a tree of size n can be performed in time O ( n p log p + g p log p + L log p) on the BSP model. (b) Let s p and let s keys be distributed <p> Erase internal edges (edges whose endpoints are the same) and restore the edge ordering by E [i]:Src, as explained in Algorithm 3. (2) Contract every connected component as defined by the forest edges into a single vertex by first applying the Euler tour algorithm of <ref> [8] </ref>, after which every segment leader fetches the new node number. (3) Every processor locally computes the MSF of the graph defined by the remaining supervertices and the edges stored in its local memory. (4) Merge the MSFs as in Step (4) of algorithm mst dense.
Reference: [9] <author> K. Chong. </author> <title> Finding Minimum Spanning Trees on the EREW PRAM. </title> <type> Manuscript, </type> <year> 1997. </year>
Reference-contexts: This is a very powerful contention resolution rule. Johnson and Metaxas [19] gave an algorithm running in O (log 3=2 n) time on an (n + m)-processor EREW PRAM. Using the same number of processors, Chong <ref> [9] </ref> presented an algorithm running in O (log n log log n) time. Karger [22] was the first to use randomization in parallel MST algorithms. He gave an EREW PRAM algorithm that requires O (log n) time and uses m= log n + n 1+* processors.
Reference: [10] <author> R. Cole, P. N. Klein, and R. E. Tarjan. </author> <title> Finding min imum spanning forests in logarithmic time and linear work using random sampling. </title> <booktitle> In Annual ACM Symposium on Parallel Algorithms and Architecture, </booktitle> <pages> pages 243-250. </pages> <publisher> ACM, </publisher> <year> 1996. </year>
Reference-contexts: A randomized sequential linear-time algorithm is given in [21]. A parallel version of this algorithm, running in O (log n) time and performing linear work on a CRCW PRAM, is described in <ref> [10] </ref>. Concurrent to our work, several groups have investigated the MST problem on different parallel computation models. Dehne and Gotz [13] presented MST algorithms for the CGM model.
Reference: [11] <author> D. Culler, R. Karp, D. Patterson, A. Sahay, K. S. an d E. Santos, R. Subramonian, and T. von Eicken. </author> <title> LogP: Towards a realistic model of parallel computation. </title> <booktitle> In Proc. 4th Symp. on Principles and Practice of Parallel Programming, </booktitle> <pages> pages 1-12. </pages> <booktitle> ACM SIGPLAN, </booktitle> <month> May </month> <year> 1993. </year>
Reference-contexts: 1 Introduction Parallel computation models like the BSP [26], the BSP* [5], the QSM [16], the CGM [12] and the LogP <ref> [11] </ref> are used more and more often for designing and analyzing parallel algorithms. In contrast to PRAM models, these models account for communication cost by introducing parameters that correspond to considerations such as communication bandwidth, latency or the startup cost of a message transmission. <p> Furthermore, the techniques used to prove this bound do not assume the BSP model, and thus can be used to provide lower bounds for other models that limit interprocessor communication, including the LogP <ref> [11] </ref> and the QSM [16]. The lower bound techniques we develop are also quite general. For example, they can be used to provide similar lower bounds on the communication volume required for the problem of finding a minimum matching.
Reference: [12] <author> F. Dehne, A. Fabri, and A. Rau-Chaplin. </author> <title> Scalable par allel computational geometry for coarse grained multi-computers. </title> <booktitle> In Annual ACM Symposium on Computational Geometry, </booktitle> <year> 1993. </year>
Reference-contexts: 1 Introduction Parallel computation models like the BSP [26], the BSP* [5], the QSM [16], the CGM <ref> [12] </ref> and the LogP [11] are used more and more often for designing and analyzing parallel algorithms. In contrast to PRAM models, these models account for communication cost by introducing parameters that correspond to considerations such as communication bandwidth, latency or the startup cost of a message transmission.
Reference: [13] <author> F. Dehne and S. Gotz. </author> <title> Efficient Parallel Minimum Spanning Tree Algorithms for Coarse Grained Multi-computers and BSP. </title> <type> Manuscript, </type> <year> 1997. </year>
Reference-contexts: A parallel version of this algorithm, running in O (log n) time and performing linear work on a CRCW PRAM, is described in [10]. Concurrent to our work, several groups have investigated the MST problem on different parallel computation models. Dehne and Gotz <ref> [13] </ref> presented MST algorithms for the CGM model. They described a randomized algorithm that can be implemented on the BSP in time O ( n+m p log p + g p log p + L log p).
Reference: [14] <author> A. V. Gerbessiotis and C. J. Siniolakis. </author> <title> Determinis tic sorting and randomized median finding on the BSP model. </title> <booktitle> In Annual ACM Symposium on Parallel Algorithms and Architecture, </booktitle> <year> 1996. </year>
Reference-contexts: Such algorithms are called com-munication-efficient. For "regular" problems such as matrix multiplication, FFT and LU decomposition, this is relatively easy to achieve. Communication-efficient algorithms have also been obtained for a number of "irregular" problems such as sorting <ref> [15, 1, 17, 14] </ref>, multisearch [5, 4, 18] and parallel priority queues [6]. However, for many graph problems such as list ranking, connected components and minimum spanning tree, it seems difficult to find communication-efficient algorithms.
Reference: [15] <author> A. V. Gerbessiotis and L. Valiant. </author> <title> Direct bulk synchronous parallel algorithms. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 22 </volume> <pages> 251-267, </pages> <year> 1994. </year>
Reference-contexts: Such algorithms are called com-munication-efficient. For "regular" problems such as matrix multiplication, FFT and LU decomposition, this is relatively easy to achieve. Communication-efficient algorithms have also been obtained for a number of "irregular" problems such as sorting <ref> [15, 1, 17, 14] </ref>, multisearch [5, 4, 18] and parallel priority queues [6]. However, for many graph problems such as list ranking, connected components and minimum spanning tree, it seems difficult to find communication-efficient algorithms.
Reference: [16] <author> P. B. Gibbons, Y. Matias, and V. Ramachandran. </author> <title> Can a shared-memory model serve as a bridging model for parallel computation? In Annual ACM Symposium on Parallel Algorithms and Architecture, </title> <year> 1997. </year>
Reference-contexts: 1 Introduction Parallel computation models like the BSP [26], the BSP* [5], the QSM <ref> [16] </ref>, the CGM [12] and the LogP [11] are used more and more often for designing and analyzing parallel algorithms. <p> Furthermore, the techniques used to prove this bound do not assume the BSP model, and thus can be used to provide lower bounds for other models that limit interprocessor communication, including the LogP [11] and the QSM <ref> [16] </ref>. The lower bound techniques we develop are also quite general. For example, they can be used to provide similar lower bounds on the communication volume required for the problem of finding a minimum matching.
Reference: [17] <author> M. T. Goodrich. </author> <title> Communication-efficient parallel sort ing. </title> <booktitle> In Proc. of the ACM Symposium on Theory of Computing, </booktitle> <year> 1996. </year>
Reference-contexts: Such algorithms are called com-munication-efficient. For "regular" problems such as matrix multiplication, FFT and LU decomposition, this is relatively easy to achieve. Communication-efficient algorithms have also been obtained for a number of "irregular" problems such as sorting <ref> [15, 1, 17, 14] </ref>, multisearch [5, 4, 18] and parallel priority queues [6]. However, for many graph problems such as list ranking, connected components and minimum spanning tree, it seems difficult to find communication-efficient algorithms. <p> The crux in our algorithm is to show how concurrent reads and writes can be implemented efficiently on the BSP model. This is usually done by sorting but sorting requires too many supersteps if the slackness is less than polynomial <ref> [17] </ref>. Our approach uses a data structure in which all edges incident to a node are stored consecutively. We show that this technique, which is similar to a technique developed by Blelloch [7] for the SCAN model of parallel computation, can also be applied for the BSP model.
Reference: [18] <author> M. T. Goodrich. </author> <title> Randomized fully-scalable BSP tech niques for multi-searching and convex hull construction. </title> <booktitle> In ACM-SIAM Symposium on Discrete Algorithms, </booktitle> <year> 1997. </year>
Reference-contexts: Such algorithms are called com-munication-efficient. For "regular" problems such as matrix multiplication, FFT and LU decomposition, this is relatively easy to achieve. Communication-efficient algorithms have also been obtained for a number of "irregular" problems such as sorting [15, 1, 17, 14], multisearch <ref> [5, 4, 18] </ref> and parallel priority queues [6]. However, for many graph problems such as list ranking, connected components and minimum spanning tree, it seems difficult to find communication-efficient algorithms. For many of these problems, very fast (linear time) sequential algorithms exist, but efficient parallel algorithms are unknown.
Reference: [19] <author> D. B. Johnson and P. Metaxas. </author> <title> A parallel algorithm for computing minimum spanning trees. </title> <journal> Journal of Algorithms, </journal> <volume> 19 </volume> <pages> 383-410, </pages> <year> 1995. </year>
Reference-contexts: We show that the amount of slackness required as well as the number of supersteps can be reduced by keeping the graph in a representation in which all edges incident to a node are stored consecutively. Furthermore, mst-sparse uses an observation due to Johnson and Metaxas <ref> [19] </ref>, which states that in order to reduce the number of nodes by a factor of k, it is sufficient to consider only the k cheapest edges incident to each node. In our algorithm, however, edges are discarded much more aggressively in order to keep the communication time low. <p> This is a very powerful contention resolution rule. Johnson and Metaxas <ref> [19] </ref> gave an algorithm running in O (log 3=2 n) time on an (n + m)-processor EREW PRAM. Using the same number of processors, Chong [9] presented an algorithm running in O (log n log log n) time. Karger [22] was the first to use randomization in parallel MST algorithms. <p> By choosing k = m=n runtime O (g (m=p)) is achieved. This idea of growing components only by a bounded factor and therefore not having to consider all edges was originally used by Johnson and Metaxas <ref> [19] </ref> in their EREW PRAM algorithm. In our algorithm, however, edges are deleted much more aggressively in order to keep the communication requirements low. There are two problems that need to be resolved.
Reference: [20] <author> B. H. H. Juurlink and H. A. G. Wijshoff. </author> <title> Commu nication Primitives for BSP computers. </title> <journal> Information Processing Letters, </journal> <volume> 58(6) </volume> <pages> 303-310, </pages> <year> 1996. </year>
Reference-contexts: If p log log p = o (m=n), then mst-dense is 1-optimal. Proof: Omitted. In Step (2) we employ the two-phase broadcast given in <ref> [20] </ref>. 2 When the density of the input graph is slightly larger, a constant number of supersteps can be achieved, albeit at the cost of a slight increase in the communication time. In this case, Step (2) and (3) are omitted.
Reference: [21] <author> D. Karger, P. Klein, and R. Tarjan. </author> <title> A Random ized Linear-Time Algorithm to find Minimum Spanning Trees. </title> <journal> Journal of the ACM, </journal> <volume> 42 </volume> <pages> 321-328, </pages> <year> 1995. </year>
Reference-contexts: Furthermore, when m np, the communication time matches the lower bound. However, the main drawback of the algorithm is that it can employ at most m=n processors efficiently. mst-sparse is a randomized algorithm based on the sequential linear-time algorithm presented in <ref> [21] </ref>. This algorithm requires a stronger assumption on the initial distribution of edges to processors. Specifically, we assume that every edge is stored twice (once in each direction), and that the edges are sorted by source node. <p> Karger [22] was the first to use randomization in parallel MST algorithms. He gave an EREW PRAM algorithm that requires O (log n) time and uses m= log n + n 1+* processors. A randomized sequential linear-time algorithm is given in <ref> [21] </ref>. A parallel version of this algorithm, running in O (log n) time and performing linear work on a CRCW PRAM, is described in [10]. Concurrent to our work, several groups have investigated the MST problem on different parallel computation models. <p> By the sampling lemma of <ref> [21] </ref>, the remaining graph G 00 has O (k n 0 ) = O (n) edges. (5) Compute the MST T 00 of the graph G 00 , again by using algorithm bsp-as. <p> Given a graph G with n 0 nodes, let H be a subgraph of G obtained by including each edge with probability p, and let F be the minimum spanning forest of H. The sampling lemma of Karger, Klein and Tarjan <ref> [21] </ref> shows that the number of F -light edges of G (edges which can not be discarded) is at most n 0 =p with probability at least 1exp (cn 0 ).
Reference: [22] <author> D. R. Karger. </author> <title> Random sampling for minimum spanning trees and other optimization problems. </title> <booktitle> In Annual ACM Symposium on Foundations of Computer Science, </booktitle> <year> 1993. </year>
Reference-contexts: Johnson and Metaxas [19] gave an algorithm running in O (log 3=2 n) time on an (n + m)-processor EREW PRAM. Using the same number of processors, Chong [9] presented an algorithm running in O (log n log log n) time. Karger <ref> [22] </ref> was the first to use randomization in parallel MST algorithms. He gave an EREW PRAM algorithm that requires O (log n) time and uses m= log n + n 1+* processors. A randomized sequential linear-time algorithm is given in [21].
Reference: [23] <author> V. King, C. Poon, V. Ramachandran, and S. Sinha. </author> <title> An optimal EREW PRAM algorithm for minimum spanning tree verification. </title> <journal> Information Processing Letters, </journal> <volume> 62(3) </volume> <pages> 153-159, </pages> <year> 1997. </year>
Reference-contexts: Our verification algorithm, called bsp-verify, is derived from the EREW PRAM algorithm presented in <ref> [23] </ref>. It improves upon the algorithm given in [23] by using fewer super-steps. Furthermore, bsp-verify employs a simple load balancing technique in order to minimize the number of times the edges have to be communicated. For a detailed description of the algorithm, the reader is referred to [2]. <p> Our verification algorithm, called bsp-verify, is derived from the EREW PRAM algorithm presented in <ref> [23] </ref>. It improves upon the algorithm given in [23] by using fewer super-steps. Furthermore, bsp-verify employs a simple load balancing technique in order to minimize the number of times the edges have to be communicated. For a detailed description of the algorithm, the reader is referred to [2]. Lemma 8 Let n p and m n log p.
Reference: [24] <author> C. Poon and V. Ramachandran. </author> <title> A randomized lin ear work EREW PRAM algorithm to find a minimum spanning forest. </title> <booktitle> In International Symposium on Algorithms and Computation (ISAAC), </booktitle> <volume> LNCS 1350, </volume> <pages> pages 212-222, </pages> <year> 1997. </year>
Reference-contexts: Furthermore, Dehne and Gotz always assume that sufficient slackness is available ( n+m p p), whereas we assume arbitrary slackness (n + m p). Moreover, their algorithms are not work-efficient for sparse input graphs. Poon and Ramachandran <ref> [24] </ref> considered the MST problem on the EREW PRAM. They designed a randomized algorithm that performs linear expected work and runs in ~ T = O (log n log log n 2 log fl n ) expected time.
Reference: [25] <author> J. Reif, </author> <title> editor. Synthesis of parallel algorithms. </title> <publisher> Morgan Kaufmann Publishers, </publisher> <year> 1993. </year>
Reference-contexts: Edges not in any MSF cannot belong to the MST and can therefore be discarded. (W.l.o.g. we assume that the weights are different.) (2) Perform 2 log log p so-called Bor _uvka-steps <ref> [25] </ref>. In every step the cheapest edge incident to each node is selected, and every connected component defined by the selected edges is contracted into a single "super-vertex". This is done as follows: (2.1) Every processor computes the locally cheapest edge incident to each supervertex.
Reference: [26] <author> L. Valiant. </author> <title> A bridging model for parallel computation. </title> <journal> Communications of the ACM, </journal> <volume> 33(8), </volume> <month> Aug. </month> <year> 1990. </year>
Reference-contexts: 1 Introduction Parallel computation models like the BSP <ref> [26] </ref>, the BSP* [5], the QSM [16], the CGM [12] and the LogP [11] are used more and more often for designing and analyzing parallel algorithms. <p> For many of these problems, very fast (linear time) sequential algorithms exist, but efficient parallel algorithms are unknown. In this paper, we consider one of these problems, that of finding a minimum spanning tree (MST), and use the BSP model <ref> [26] </ref> as our cost model. Briefly, a BSP computer consists of a set of processor/memory modules, a routing network, and a mechanism for synchronizing the processors in a barrier style. <p> Communication time of a BSP algorithm is then computed as g H + L S. For more details and motivation for the BSP model, the reader is referred to <ref> [26] </ref>. Definition of the MST problem. Given a connected undi-rected graph G = (V; E), each of whose edges has a weight w (e). We let n = jV j denote the number of nodes and m = jEj denote the number of edges of the input graph.
Reference: [27] <author> A. Yao. </author> <title> Lower bounds by probabilistic arguments. </title> <booktitle> In 24 th IEEE Symposium on Foundations of Computer Science, </booktitle> <pages> pages 420-428, </pages> <year> 1983. </year>
Reference-contexts: We show that according to distribution fi the communication volume is large. So for any deterministic algorithm and a method of encoding the transcript, there must exist some input which also requires a large communication volume. Furthermore, from Yao's Lemma <ref> [27] </ref>, this also implies the stated result about randomized algorithms.
Reference: [28] <author> A. C. Yao. </author> <title> Some complexity questions related to dis tributive computing. </title> <booktitle> In Proc. of the ACM Symposium on Theory of Computing, </booktitle> <pages> pages 209-213, </pages> <year> 1979. </year>
Reference-contexts: From the linearity of expectation, Lemma 2 gives us that the total communication volume is (pn). Thus, Theorem 1 follows from Lemma 2. Proof of Lemma 2. We use a variant of a technique developed by Yao for the study of communication complexity <ref> [28] </ref>. We consider the actions of processor P j under the assumption that no other processor has been scrambled. Since this occurs with probability at least 1 2 , this can increase the expected number of bits by at most a factor of 2.
References-found: 28

