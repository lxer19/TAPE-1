URL: http://ai.fri.uni-lj.si/papers/kononenko95-ecml.ps
Refering-URL: http://ai.fri.uni-lj.si/papers/index.html
Root-URL: 
Email: e-mail: igor.kononenko@fer.uni-lj.si  
Phone: tel.: +386 61 1768390; fax.: +386 61 264990  
Title: A counter example to the stronger version of the binary tree hypothesis  
Author: Igor Kononenko 
Keyword: binary decision trees, counter example, RELIEFF  
Address: Trzaska 25, SI-61001 Ljubljana, Slovenia  
Affiliation: University of Ljubljana Faculty of electrical engineering computer science  
Abstract: The paper describes a counter example to the hypothesis which states that a greedy decision tree generation algorithm that constructs binary decision trees and branches on a single attribute-value pair rather than on all values of the selected attribute will always lead to a tree with fewer leaves for any given training set. We show also that RELIEFF is less myopic than other impurity functions and that it enables the induction algorithm that generates binary decision trees to reconstruct optimal (the smallest) decision trees in more cases. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Breiman L., Friedman J.H., Olshen R.A., Stone C.J. </author> <title> (1984) Classification and Regression Trees, </title> <address> Monterey, CA: </address> <publisher> Wadsforth International Group. </publisher>
Reference-contexts: In this paper we give a counter example to the stronger version of the binary tree hypothesis. The binary tree hypothesis was intended to hold for any impurity function (Fayyad & Irani, 1992). The counter example, described in this paper, works for information gain (Quinlan, 1986), gini-index <ref> (Breiman et al., 1984) </ref>, distance measure (Mantaras, 1991), J-measure (Smyth & Goodman, 1990), information gain normalized with the logarithm of the number of values of an attribute (Kononenko et al, 1984; Quinlan, 1986), and gain ratio (Quinlan, 1986). 2 Counter example The target decision tree of a counter example should contain
Reference: <author> Kira K. & Rendell L. </author> <title> (1992a) A practical approach to feature selection, </title> <booktitle> Proc. Intern. Conf. on Machine Learning (Aberdeen, </booktitle> <address> July 1992) D.Sleeman & P.Edwards (eds.), </address> <publisher> Morgan Kaufmann, pp.249-256. </publisher>
Reference: <author> Kira K. & Rendell L. </author> <title> (1992b) The feature selection problem: traditional methods and new algorithm. </title> <booktitle> Proc. AAAI'92, </booktitle> <address> San Jose, CA, </address> <month> July </month> <year> 1992. </year>
Reference: <author> Kononenko I. </author> <title> (1994) Estimating attributes: Analysis and extensions of RELIEF. </title> <booktitle> Proc. Eu-ropean Conf. on Machine Learning (Catania, </booktitle> <month> April </month> <year> 1994), </year> <editor> L. De Raedt & F.Bergadano (eds.), </editor> <publisher> Springer Verlag, pp.171-182. </publisher>
Reference-contexts: For our counter example, if the multi-branch tree construction is employed, RELIEF is able to select attributes that lead to the reconstruction of the optimal tree. However, for binary tree, RELIEF selects attribute A2 for the root which leads to a non-optimal tree. In <ref> (Kononenko, 1994) </ref> it was shown that the estimates by RELIEF are highly related to impurity functions. An extended version, called RELIEFF, was developed that is able to deal with noisy and incomplete data and with multiclass problems.
Reference: <author> Kononenko, I., Bratko, I., Roskar, E. </author> <title> (1984) Experiments in automatic learning of medical diagnostic rules, </title> <type> Technical report, </type> <institution> Faculty of electrical engineering & computer science, Ljubljana, </institution> <note> Slovenia (presented at ISSEK Workshop, Bled, Slovenia, </note> <month> august, </month> <year> 1984). </year>
Reference-contexts: For a discrete attribute either one attribute-value pair against the others is selected (e.g. Hunt et al., 1966; Breiman et al., 1984; Fayyad, 1991; Fayyad & Irani, 1992) or a set of attribute values is divided into two disjoint subsets <ref> (Kononenko et al., 1984) </ref>.
Reference: <author> Fayyad U.M. </author> <title> (1991) On the induction of decision trees for multiple concept learning. </title> <type> PhD dissertation, </type> <institution> The University of Michigan. </institution>
Reference: <author> Fayyad U.M. </author> & <title> Irani K.B. (1992) The attribute selection problem in decision tree generation. </title> <booktitle> Proc. AAAI-92, </booktitle> <address> July 1992, San Jose, CA. </address> <publisher> MIT Press. </publisher>
Reference-contexts: In this paper we give a counter example to the stronger version of the binary tree hypothesis. The binary tree hypothesis was intended to hold for any impurity function <ref> (Fayyad & Irani, 1992) </ref>.
Reference: <author> Hunt E., Martin J & Stone P. </author> <title> (1966) Experiments in Induction, </title> <address> New York, </address> <publisher> Academic Press. </publisher>
Reference: <author> Li M., Vitanyi P. </author> <title> (1993) An introduction to Kolmogorov Complexity and its applications, </title> <publisher> Springer Verlag. </publisher>
Reference-contexts: The smallest decision tree among all the trees induced in a top down manner that have the same classification accuracy is considered the best. Its prediction capability should be the best according to the minimal description length principle <ref> (Li & Vitanyi, 1993) </ref> and it is potentially the most comprehensible among all decision trees that correspond to the same input data. In this paper we will consider only consistent and complete hypotheses without any loose in generality.
Reference: <author> Mantaras R.L. </author> <title> (1991) A distance-based attribute selection measure for decision tree induction, </title> <journal> Machine Learning, </journal> <volume> 6 </volume> <pages> 81-92. </pages>
Reference-contexts: The binary tree hypothesis was intended to hold for any impurity function (Fayyad & Irani, 1992). The counter example, described in this paper, works for information gain (Quinlan, 1986), gini-index (Breiman et al., 1984), distance measure <ref> (Mantaras, 1991) </ref>, J-measure (Smyth & Goodman, 1990), information gain normalized with the logarithm of the number of values of an attribute (Kononenko et al, 1984; Quinlan, 1986), and gain ratio (Quinlan, 1986). 2 Counter example The target decision tree of a counter example should contain a multivalued attribute at the root,
Reference: <author> Paterson, A., Niblett, T. </author> <title> (1982) The ACLS User Manual, Intelligent Terminals Ltd, </title> <publisher> Glas-gow. </publisher>
Reference: <author> Quinlan, J.R. </author> <title> (l986) Induction of Decision Trees. </title> <journal> Machine Learning. </journal> <volume> 1: </volume> <pages> 81-106. </pages>
Reference: <author> Ragavan H. & Rendell L. </author> <title> (1993) Lookahead feature construction for learning hard concepts. </title> <booktitle> Proc. 10th Intern. Conf. on Machine Learning. </booktitle> <address> (Amherst, June 1993), </address> <publisher> Morgan Kaufmann, pp.252-259. </publisher>
Reference-contexts: To overcome the myopia of the greedy search a less myopic selection measure may be used, such as RELIEFF, and a lookahead may be employed, such as e.g. in the LFC algorithm <ref> (Ragavan & Rendell, 1993) </ref>. Acknowledgements A part of this work was done during the author's stay at California Institute of Technology in Pasadena, CA. I would like to thank Padhraic Smyth and Prof.
Reference: <author> Smyth P. & Goodman R.M. </author> <title> (1990) Rule induction using information theory. </title> <editor> In: G.Piarersky & W.Frawley (eds.) </editor> <title> Knowledge Discovery in Databases, </title> <publisher> MIT Press. </publisher>
Reference-contexts: The binary tree hypothesis was intended to hold for any impurity function (Fayyad & Irani, 1992). The counter example, described in this paper, works for information gain (Quinlan, 1986), gini-index (Breiman et al., 1984), distance measure (Mantaras, 1991), J-measure <ref> (Smyth & Goodman, 1990) </ref>, information gain normalized with the logarithm of the number of values of an attribute (Kononenko et al, 1984; Quinlan, 1986), and gain ratio (Quinlan, 1986). 2 Counter example The target decision tree of a counter example should contain a multivalued attribute at the root, which is more
References-found: 14

