URL: http://www.cs.princeton.edu/~gws/isca.ps
Refering-URL: http://www.cs.princeton.edu/~felten/
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Title: Evaluating Multi-Port Frame Buffer Designs for a Mesh-Connected Multicomputer  
Author: Gordon Stoll, Bin Wei, Douglas Clark, Edward W. Felten, and Kai Li Patrick Hanrahan 
Affiliation: Department of Computer Science Princeton University  Department of Computer Science Stanford University  
Abstract: Multicomputers can be effectively used for interactive graphics rendering only if there are mechanisms available to rapidly composite and transfer images to an external display device. One method for achieving the necessary bandwidth for this operation is to provide multiple high-bandwidth ports into a frame buffer. In this paper, we evaluate the design space of a multi-port frame buffer design for the Intel Paragon mesh routing network. We use an instrumented rendering system to capture the graphics operations needed for rendering a number of three-dimensional scenes; we then use those workloads as input to test programs running on the Paragon to estimate the performance of our hardware. Our experiments consider three major design questions: how many network ports the frame buffer needs, whether Z-Buffering should be done in hardware on the frame buffer or in software on the computing nodes, and whether the design alternatives are scalable. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Brian Apgar, Bret Bersack, and Abranham Mam-men. </author> <title> A display system for the Stellar Graphics supercomputer model GS1000. </title> <journal> Computer Graphics, </journal> <volume> 22(4) </volume> <pages> 255-262, </pages> <month> July </month> <year> 1988. </year>
Reference-contexts: All of these factors argue against using HiPPI for high end graphics applications. Another way to integrate frame buffers with multiprocessors is to assign part of the processors' memory as the frame buffer memory. For example, in the shared memory Stellar GS1000 <ref> [1] </ref> machine, virtual frame buffers are allocated out of primary space. The frame buffers can use main memory bandwidth, so accesses can be fast.
Reference: [2] <author> Robert Benner. </author> <title> Parallel graphics algorithms on a 1024-processor hypercube. </title> <booktitle> In Proc. of the 4th Conference on Hypercubes, Concurrent Computers, and Applications, </booktitle> <pages> pages 133-140, </pages> <month> March </month> <year> 1989. </year>
Reference-contexts: The frame buffers can use main memory bandwidth, so accesses can be fast. The problem is that memory bandwidth is already a scarce resource in a shared memory machine; in-memory graphics imposes an extra burden at the bottleneck of the shared memory machine. The NCUBE/ten system <ref> [2] </ref> supported a 16-port distributed frame buffer, using a special "graphics node" to manage each piece of the frame buffer.
Reference: [3] <author> Michael Cox. </author> <title> Algorithms for Parallel Rendering. </title> <type> PhD thesis, </type> <institution> Princeton University, </institution> <month> May </month> <year> 1995. </year>
Reference-contexts: In fact it has often been claimed in the object-parallel rendering literature that load-balancing falls out of the technique automatically [12, 4]. This claim is questioned in <ref> [3] </ref> on the basis of the potentially high variance in the primitive population, and our results tend to support this. The variance in the number of fragments generated per primitive is very high.
Reference: [4] <author> Michael Cox and Pat Hanrahan. </author> <title> Depth complexity in object-parallel graphics architecture. </title> <booktitle> In Proc. Seventh Workshop on Graphics Hardware, Euro-graphics Technical Report Series, </booktitle> <pages> pages 204-222, </pages> <year> 1992. </year>
Reference-contexts: Output Control RAMDAC &gt;>>>> &gt;>>>> NIC NIC Input Control Z-buffering Input Control Z-buffering RGB 0 Buffer RGB 0 Buffer n-2 n-1 Input Control Z-buffering Input Control Z-buffering RGB 0 Buffer RGB 0 Buffer . . . . . . sent to the frame buffer. This requires substantially more bandwidth <ref> [4, 11] </ref> and is not considered here. Given that a sparse merge is used, the data required for each fragment for composition must minimally include the screen coordinates (x; y: two bytes each), depth (z: four bytes), and color (r; g; b: one byte each). <p> It is not clear a priori which approach is better; our experiments will address this question. 4 Experimental Framework We use an instrumented version of the Photoreal-istic RenderMan rendering system [17], as described in <ref> [4] </ref>. Output library calls have been added in the rasterization subroutines in order to capture every pixel fragment generated by the system. <p> In fact it has often been claimed in the object-parallel rendering literature that load-balancing falls out of the technique automatically <ref> [12, 4] </ref>. This claim is questioned in [3] on the basis of the potentially high variance in the primitive population, and our results tend to support this. The variance in the number of fragments generated per primitive is very high.
Reference: [5] <author> Michael Deering and Scott Nelson. Leo: </author> <title> A system for cost effective 3D shaded graphics. </title> <journal> Computer Graphics, </journal> <volume> 27(4) </volume> <pages> 101-108, </pages> <month> August </month> <year> 1993. </year>
Reference-contexts: This suggests that the scalability of the redistribution phase can be improved by detecting, geometrically subdividing, and redistributing large primitives. This technique has been utilized effectively in a hardware polygon mesh rendering system <ref> [5] </ref> to alleviate load imbalance in front-end geometric processing. In a system such as Photorealistic RenderMan which uses large complex primitives the detection and subdivision tasks are correspondingly more expensive, and the benefit potentially greater.
Reference: [6] <author> Michael Deering, Stephen Schlapp, and Michael Lavalle. FBRAM: </author> <title> A new form of memory optimized for 3D graphics. </title> <journal> Computer Graphics, </journal> <volume> 28(3) </volume> <pages> 167-174, </pages> <month> July </month> <year> 1994. </year>
Reference-contexts: Z-Buffering demands high network bandwidth to move the fragments, and high memory bandwidth to perform the fragment comparisons. It is well established that custom hardware is very effective at satisfying the processing and memory bandwidth demands of rendering, particularly in the later stages of the process <ref> [7, 18, 6] </ref>. In the simple organization outlined above the general-purpose processing power and memory bandwidth of the computational nodes of the multicomputer are potentially squandered on the highly specialized operations of the latest stages of the rendering pipeline.
Reference: [7] <author> Henry Fuchs and John Poulton. </author> <title> Pixel Planes: A VLSI-oriented design for a raster graphics engine. </title> <booktitle> VLSI Design, </booktitle> <volume> 2(3) </volume> <pages> 20-28, </pages> <year> 1981. </year>
Reference-contexts: Z-Buffering demands high network bandwidth to move the fragments, and high memory bandwidth to perform the fragment comparisons. It is well established that custom hardware is very effective at satisfying the processing and memory bandwidth demands of rendering, particularly in the later stages of the process <ref> [7, 18, 6] </ref>. In the simple organization outlined above the general-purpose processing power and memory bandwidth of the computational nodes of the multicomputer are potentially squandered on the highly specialized operations of the latest stages of the rendering pipeline.
Reference: [8] <author> Vineet Kumar. </author> <title> A host interface architecture for HIPPI. </title> <booktitle> In Proc. Scalable High Performance Computing Conference, </booktitle> <pages> pages 142-149, </pages> <month> May </month> <year> 1994. </year>
Reference-contexts: A high-quality stereo display requires 11.25 MBytes of data to be transferred per frame because there are 1920 fi 1024 pixels and 2 views per frame. This type of solution is the only one in widespread use, with transfer generally performed via a HiPPI interface <ref> [8] </ref>. With HiPPI's physical bandwidth limita-tion of 100 MB/sec, the theoretical peak performance of a HiPPI frame buffer is 26.7 frames/sec for the workstation display or 8.7 frames/sec for the high-quality display.
Reference: [9] <author> Silvio Levy, Delle Maxwell, and Tamara Munzner. </author> <title> Outside In. AK Peters, </title> <publisher> Ltd., </publisher> <address> Wellesley, MA, </address> <year> 1994. </year> <note> (Video). </note>
Reference-contexts: Capi tol building, constructed in AutoCAD. * Centrosome A frame from a scientific visualization of a growing centrosome modeled by many small spheres. * Sphere A frame from an animation of the eversion of a sphere, from the animation Outside In <ref> [9] </ref>. * Bike Shop An interior scene from a photorealistic animation produced using RenderMan [13]. More information about these scenes is shown in figure 3. All scenes were rendered at a typical workstation resolution of 1280 by 1024 pixels.
Reference: [10] <author> Kai Li, Jeffrey F. Naughton, and James S. Plank. </author> <title> An efficient checkpointing method for multicom-puters with wormhole routing. </title> <journal> Intl. J. of Parallel Programming, </journal> <volume> 20(3) </volume> <pages> 159-180, </pages> <month> June </month> <year> 1991. </year>
Reference-contexts: Since the CM-2 is a SIMD machine, pixel data movement can't be overlapped with computation. Our method for synchronization on control operations is similar in spirit to the network sweeping algorithm developed by Li, Naughton, and Plank <ref> [10] </ref>. 7 Conclusions Using information from traces of the Photorealistic RenderMan rendering system, we have evaluated a variety of architectures for graphics systems to attach to multicomputers. Our experiments show the following results: * For small numbers of processors, hardware Z-Buffering is much faster than software Z-Buffering.
Reference: [11] <author> S. Molnar, M. Cox, D. Ellsworth, and H. Fuchs. </author> <title> A sorting classification of parallel rendering. </title> <journal> IEEE Computer Graphics and Application, </journal> <volume> 14(4) </volume> <pages> 23-31, </pages> <month> July </month> <year> 1994. </year>
Reference-contexts: Thus the final stage in object-parallel rendering can be viewed as a sorting of this pixel coverage information from object space to image space <ref> [15, 12, 11] </ref>. The basic architecture of an object-parallel graphics system consists of a front end and a back end. The front end is given geometric primitives to render (that is, to transform, shade, and rasterize), and produces streams of image position, color, and depth information. <p> If Z-Buffering is implemented in hardware, the compute processors can send all generated fragments directly to the frame buffer with no local compositing (a sparse pixel merge, in the terminology of <ref> [11] </ref>). Alternatively, compositing could be performed locally on each compute processor, and the resulting frame . . . <p> Output Control RAMDAC &gt;>>>> &gt;>>>> NIC NIC Input Control Z-buffering Input Control Z-buffering RGB 0 Buffer RGB 0 Buffer n-2 n-1 Input Control Z-buffering Input Control Z-buffering RGB 0 Buffer RGB 0 Buffer . . . . . . sent to the frame buffer. This requires substantially more bandwidth <ref> [4, 11] </ref> and is not considered here. Given that a sparse merge is used, the data required for each fragment for composition must minimally include the screen coordinates (x; y: two bytes each), depth (z: four bytes), and color (r; g; b: one byte each).
Reference: [12] <author> Steven Molnar. </author> <title> Image-Composition Architecture for Read-Time Image Generation. </title> <type> PhD thesis, </type> <institution> University of North Carolina at Chapel Hill, </institution> <month> Oct </month> <year> 1991. </year>
Reference-contexts: Thus the final stage in object-parallel rendering can be viewed as a sorting of this pixel coverage information from object space to image space <ref> [15, 12, 11] </ref>. The basic architecture of an object-parallel graphics system consists of a front end and a back end. The front end is given geometric primitives to render (that is, to transform, shade, and rasterize), and produces streams of image position, color, and depth information. <p> The primitives in a scene are assumed to be initially distributed uniformly across the host processors. This is accomplished in the simulation by a round-robin assignment of primitives to processors as used in <ref> [12] </ref>. The quantity of pixel-fragment data generated from the rasterization stage on a given processor is computed from the trace data according to this primitive distribution. The next stage in the rendering pipeline is composition (Z-Buffering). <p> In fact it has often been claimed in the object-parallel rendering literature that load-balancing falls out of the technique automatically <ref> [12, 4] </ref>. This claim is questioned in [3] on the basis of the potentially high variance in the primitive population, and our results tend to support this. The variance in the number of fragments generated per primitive is very high.
Reference: [13] <author> Eben Ostby and Bill Reeves. </author> <title> A Night in the Bike Store. </title> <journal> Computer Graphics, </journal> <volume> 21(4), </volume> <month> July </month> <year> 1987. </year> <pages> (Cover). </pages>
Reference-contexts: AutoCAD. * Centrosome A frame from a scientific visualization of a growing centrosome modeled by many small spheres. * Sphere A frame from an animation of the eversion of a sphere, from the animation Outside In [9]. * Bike Shop An interior scene from a photorealistic animation produced using RenderMan <ref> [13] </ref>. More information about these scenes is shown in figure 3. All scenes were rendered at a typical workstation resolution of 1280 by 1024 pixels.
Reference: [14] <author> Micheal Potmesil and Eric Hoffert. </author> <title> The Pixel Machine: A parallel image computer. </title> <journal> Computer Graphics, </journal> <volume> 23(3) </volume> <pages> 69-78, </pages> <month> July </month> <year> 1989. </year>
Reference-contexts: By contrast, in our system no buffer management is required since the hardware consumes incoming data directly from the network FIFO. Memory accesses are pipelined to match the network speed. The AT&T Pixel Machine <ref> [14] </ref> distributes the frame buffer among the pixel node array, which connect to the processor node array via a VME bus and a specialized broadcast bus. The merit of this approach is that the design of pixel node array and the processor node array are orthogonal.
Reference: [15] <author> I. E. Sutherland, R. F. Sproull, and R. A. Schu-makeer. </author> <title> A characterization of ten hidden-surface algorithms. </title> <journal> Computer Surveys, </journal> <volume> 6(1) </volume> <pages> 1-55, </pages> <month> March </month> <year> 1974. </year>
Reference-contexts: Thus the final stage in object-parallel rendering can be viewed as a sorting of this pixel coverage information from object space to image space <ref> [15, 12, 11] </ref>. The basic architecture of an object-parallel graphics system consists of a front end and a back end. The front end is given geometric primitives to render (that is, to transform, shade, and rasterize), and produces streams of image position, color, and depth information.
Reference: [16] <institution> Connection Machine CM-200 Series Technical Summary. </institution> <type> Technical report, </type> <institution> Thinking Machines Corporation, </institution> <year> 1991. </year>
Reference-contexts: But the Pixel Machine is not scalable to greater numbers of processors and the limited amount of memory per processor restricts the complexity of the algorithms that can be run on it. The Thinking Machines CM-2 <ref> [16] </ref> has a frame buffer that can be accessed in parallel. The CM-2 frame buffer uses 256 I/O lines connecting to 4K processors. All pixel data from all processors must be routed through these 4K processors to get into the frame buffer.
Reference: [17] <author> S. Upstill. </author> <title> The RenderMan Companion. </title> <publisher> Addison-Wesley, </publisher> <year> 1989. </year>
Reference-contexts: It is not clear a priori which approach is better; our experiments will address this question. 4 Experimental Framework We use an instrumented version of the Photoreal-istic RenderMan rendering system <ref> [17] </ref>, as described in [4]. Output library calls have been added in the rasterization subroutines in order to capture every pixel fragment generated by the system.
Reference: [18] <author> Mary Whitton. </author> <title> Memory design for raster graphics displays. </title> <journal> IEEE Computer Graphics and Applications, </journal> <volume> 4(3) </volume> <pages> 48-65, </pages> <month> March </month> <year> 1984. </year>
Reference-contexts: Z-Buffering demands high network bandwidth to move the fragments, and high memory bandwidth to perform the fragment comparisons. It is well established that custom hardware is very effective at satisfying the processing and memory bandwidth demands of rendering, particularly in the later stages of the process <ref> [7, 18, 6] </ref>. In the simple organization outlined above the general-purpose processing power and memory bandwidth of the computational nodes of the multicomputer are potentially squandered on the highly specialized operations of the latest stages of the rendering pipeline.
References-found: 18

