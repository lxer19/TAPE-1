URL: http://www.cs.utexas.edu/users/rvdg/papers/SC96.ps
Refering-URL: http://www.cs.utexas.edu/users/rvdg/SL_library/pbmd.html
Root-URL: 
Title: Towards Usable and Lean Parallel Linear Algebra Libraries  
Author: Almadena Chtchelkanova Carter Edwards John Gunnels Greg Morrow James Overfelt Robert van de Geijn 
Note: This project is sponsored in part by the Office of Naval Research under Contract N00014-95-1-0401, the NASA High Performance Computing and Communications Program's Earth and Space Sciences Project under NRA Grant NAG5-2497, the PRISM project under ARPA grant P-95006. Corresponding and presenting author.  
Address: Austin, Texas 78712  Austin, TX 78712, (512) 471-9720 (Office),  
Affiliation: Department of Computer Sciences and Texas Institute for Computational and Applied Mathematics The University of Texas at Austin  Dept. of Computer Sciences, The University of Texas at Austin,  
Email: rvdg@cs.utexas.edu.  
Phone: (512) 471-8885 (Fax),  
Date: May 1, 1996  
Abstract: In this paper, we introduce a new parallel library effort, as part of the PLAPACK project, that attempts to address discrepencies between the needs of applications and parallel libraries. A number of contributions are made, including a new approach to matrix distribution, new insights into layering parallel linear algebra libraries, and the application of "object based" programming techniques which have recently become popular for (parallel) scientific libraries. We present an overview of a prototype library, the SL Library, which incorporates these ideas. Preliminary performance data shows this more application-centric approach to libraries does not necessarily adversely impact performance, compared to more traditional approaches. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Purushotham Bangalore, Anthony Skjellum, Chuck Baldwin, and Steven G. Smith, </author> <title> "Dense and Iterative Concurrent Linear Algebra in the Multicomputer Toolbox," </title> <booktitle> in Proceedings of the Scalable Parallel Libraries Conference (SPLC '93), </booktitle> <pages> pages 132-141, </pages> <month> Oct. </month> <year> 1993. </year>
Reference-contexts: In addition to layering the library using these techniques, we have adopted an "object based" approach to programming. This approach has already popularized for high performance parallel computing by libraries like the Toolbox being developed at Mississippi State University <ref> [1] </ref>, the PETSc library at Argonne National Laboratory [15] and the Message-Passing Interface [14]. 6.1 Scope The goal of our scaled down effort is to demonstrate how the observed techniques simplify the implementation of linear algebra libraries by providing a systematic (building-block) approach. <p> The LocalN indicates the dimension of the matrix on each node. Thus, as the number of nodes (mesh size) increases, the local memory usage remains constant. We report MFLOPs per node, giving a clear indication of how efficiently each individual node is used. 23 In <ref> [1] </ref> and other references, it is pointed out that parallel linear algebra libraries should allow for highly irregular distributions. In [5, 10], we show how all two-dimensional cartesian matrix distributions can be viewed as being induced by vector distributions and how this allows for even more flexible libraries.
Reference: [2] <author> J. Choi, J. J. Dongarra, R. Pozo, and D. W. Walker, </author> <title> "Scalapack: A Scalable Linear Algebra Library for Distributed Memory Concurrent Computers, </title> <booktitle> Proceedings of the Fourth Symposium on the Frontiers of Massively Parallel Computation. </booktitle> <publisher> IEEE Comput. Soc. Press, </publisher> <year> 1992, </year> <pages> pp. 120-127. </pages>
Reference: [3] <author> J. Choi, J. Dongarra, and D. Walker, </author> <title> "The Design of a Parallel Dense Linear Algebra Software Library: Reduction to Hessenberg, Tridiagonal, and Bidiagonal Form," </title> <address> UT, 24 CS-95-275, </address> <month> February </month> <year> 1995. </year> <note> Related technical reports: http://www.netlib.org/lapack/lawns </note>
Reference: [4] <author> Almadena Chtchelkanova, John Gunnels, Greg Morrow, James Overfelt, Robert A. van de Geijn, </author> <title> "Parallel Implementation of BLAS: General Techniques for Level 3 BLAS," </title> <journal> Concurrency: </journal> <note> Practice and Experience to appear. http://www.cs.utexas.edu/users/rvdg/abstracts/sB BLAS.html </note>
Reference-contexts: For further details on parallel implementation of matrix-matrix multiplication and level 3 BLAS, see our papers "SUMMA: Scalable Universal Matrix Multiplication Algorithm" [23] and "Parallel Implementation of BLAS: General Techniques for Level 3 BLAS" <ref> [4] </ref>. A reference manual for this library is maintained at the website given in Section 8. 1 This statement is less true for sparse iterative libraries. 2 2 Physically Based Matrix Distribution The discussion in this section applies equally to dense and sparse linear systems. <p> Better (near peak) performance can be attained by replacing matrix-vector multiplication by matrix-panel-of-vectors multiplication, and rank-1 updates by rank-k updates. The algorithms outlined above can be easily altered to accomo-date this. 5.3 Other level 3 BLAS In <ref> [4] </ref>, we describe how all level 3 BLAS can be implemented using variations of the above scheme, attaining within 10-20% of peak performance on the Intel Paragon. 6 A Simple Library As part of the PLAPACK project at the University of Texas at Austin, we have set out to investigate the <p> While our preliminary performance numbers are only for a small number of nodes, the techniques are perfectly scalable and thus very high performance can be expected, even for very large numbers of nodes. We justify this statement in <ref> [4] </ref> and other papers. While the given data is by no means conclusive, we should note that much more extensive data for the techniques is given in a previous paper of ours [4], where we show how indeed all level 3 BLAS can be implemented in this fashion. <p> We justify this statement in <ref> [4] </ref> and other papers. While the given data is by no means conclusive, we should note that much more extensive data for the techniques is given in a previous paper of ours [4], where we show how indeed all level 3 BLAS can be implemented in this fashion. In that paper, we also show the techniques to be scalable.
Reference: [5] <author> Almadena Chtchelkanova, Carter Edwards, John Gunnels, Greg Morrow, James Over-felt, Robert A. van de Geijn, </author> <title> "Comprehensive Approach to Parallel Linear Algebra Libraries," unpublished manuscript prepared for the BLAS Technical Workshop, </title> <institution> Knoxvill Tennessee, </institution> <month> Nov. </month> <pages> 13-14, </pages> <note> 1995 http://www.cs.utexas.edu/users/rvdg/papers/plapack.html </note>
Reference-contexts: For further details on the underlying techniques for data distribution and duplication, see our unpublished manuscript prepared for the BLAS workshop "A Comprehensive Approach to Parallel Linear Algebra Libraries" <ref> [5] </ref>. For further details on parallel implementation of matrix-matrix multiplication and level 3 BLAS, see our papers "SUMMA: Scalable Universal Matrix Multiplication Algorithm" [23] and "Parallel Implementation of BLAS: General Techniques for Level 3 BLAS" [4]. <p> We report MFLOPs per node, giving a clear indication of how efficiently each individual node is used. 23 In [1] and other references, it is pointed out that parallel linear algebra libraries should allow for highly irregular distributions. In <ref> [5, 10] </ref>, we show how all two-dimensional cartesian matrix distributions can be viewed as being induced by vector distributions and how this allows for even more flexible libraries.
Reference: [6] <author> Tom Cwik, Robert van de Geijn, and Jean Patterson, </author> <title> "The Application of Parallel Computation to Integral Equation Models of Electromagnetic Scattering," </title> <journal> Journal of the Optical Society of America A , Vol. </journal> <volume> 11, No. 4, </volume> <pages> pp. 1538-1545, </pages> <month> April </month> <year> 1994. </year>
Reference: [7] <author> J. J. Dongarra, J. Du Croz, S. Hammarling, and I. Duff, </author> <title> "A Set of Level 3 Basic Linear Algebra Subprograms," </title> <journal> TOMS, </journal> <volume> Vol. 16, No. 1, </volume> <pages> pp. 1-17, </pages> <year> 1990. </year>
Reference: [8] <author> Jack Dongarra, Robert van de Geijn, and David Walker, </author> <title> "Scalability Issues Affecting the Design of a Dense Linear Algebra Library," Special Issue on Scalability of Parallel Algorithms, </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> Vol. 22, No. 3, </volume> <month> Sept. </month> <year> 1994. </year> <note> Related technical reports: http://www.netlib.org/lapack/lawns </note>
Reference: [9] <author> A. Edelman, </author> <title> "Large Dense Numerical Linear Algebra in 1993: The Parallel Computing Influence". </title> <journal> Journal of Supercomputing Applications. </journal> <volume> 7 (1993), </volume> <pages> pp. 113-128. </pages>
Reference-contexts: Also in the planning are out-of-core extensions [17]. It is often questioned whether the emphasis on dense linear algebra is misplaced in the first place <ref> [9] </ref>. It is our position that the future of parallel dense linear algebra is in a support role for parallel sparse linear solvers, to solve dense subproblems that are part of very large sparse problems (e.g. exploited by parallel supernodal methods [21, 22]).
Reference: [10] <author> C. Edwards, P. Geng, A. Patra, and R. van de Geijn, </author> <title> "Parallel Matrix Decompositions: have we been doing it all wrong?" TR-95-39, </title> <institution> Department of Computer Sciences, University of Texas, </institution> <month> Oct. </month> <year> 1995. </year> <note> http://www.cs.utexas.edu/users/rvdg/abstracts/PBMD.html </note>
Reference-contexts: Because it is an overview, the reader should not expect a complete document: For further details on why this approach is more application friendly than traditional approaches, we refer to our paper "Parallel Matrix Distributions: have we been doing it all wrong" <ref> [10] </ref>. For further details on the underlying techniques for data distribution and duplication, see our unpublished manuscript prepared for the BLAS workshop "A Comprehensive Approach to Parallel Linear Algebra Libraries" [5]. <p> We report MFLOPs per node, giving a clear indication of how efficiently each individual node is used. 23 In [1] and other references, it is pointed out that parallel linear algebra libraries should allow for highly irregular distributions. In <ref> [5, 10] </ref>, we show how all two-dimensional cartesian matrix distributions can be viewed as being induced by vector distributions and how this allows for even more flexible libraries. <p> It is thus important that the approach to distributing and manipulating the dense matrices is conformal with how they naturally occur as part of the sparse problem. Since our matrix distribution starts with the vector, we believe the described approach meets this criteria. For details, see <ref> [10] </ref>.
Reference: [11] <editor> G. Fox, et al., </editor> <booktitle> Solving Problems on Concurrent Processors: </booktitle> <volume> Volume 1, </volume> <publisher> Prentice Hall, </publisher> <address> Englewood Cliffs, NJ, </address> <year> 1988. </year>
Reference: [12] <author> Po Geng, J. Tinsley Oden, Robert van de Geijn, </author> <title> "Massively Parallel Computation for Acoustical Scattering Problems using Boundary Element Methods," </title> <journal> Journal of Sound and Vibration, </journal> <volume> 191 (1), </volume> <pages> pp. 145-165, </pages> <year> 1996. </year> <month> 25 </month>
Reference: [13] <author> Golub, G., Van Loan, C., F., </author> <title> Matrix Computations, 2nd Ed., 1989, </title> <publisher> The John Hop--kins University Press. </publisher>
Reference: [14] <author> W. Gropp, E. Lusk, and A. Skjellum, </author> <title> Using MPI, </title> <publisher> MPI Press, </publisher> <year> 1994. </year>
Reference-contexts: This approach has already popularized for high performance parallel computing by libraries like the Toolbox being developed at Mississippi State University [1], the PETSc library at Argonne National Laboratory [15] and the Message-Passing Interface <ref> [14] </ref>. 6.1 Scope The goal of our scaled down effort is to demonstrate how the observed techniques simplify the implementation of linear algebra libraries by providing a systematic (building-block) approach. <p> We will later argue that extending the library to the totally general case does not require massive changes, nor will it result in unacceptable added complexity in the code. 6.2 MPI We assume that the reader is familiar with the Message-Passing Interface <ref> [14] </ref>, and its use of opaque objects.
Reference: [15] <author> W. Gropp and B. Smith, </author> <title> "The design of data-structure-neutral libraries for the iterative solution of sparse linear systems," </title> <journal> Scientific Programming, </journal> <note> to appear. http://www.mcs.anl.gov/petsc/petsc.html </note>
Reference-contexts: In addition to layering the library using these techniques, we have adopted an "object based" approach to programming. This approach has already popularized for high performance parallel computing by libraries like the Toolbox being developed at Mississippi State University [1], the PETSc library at Argonne National Laboratory <ref> [15] </ref> and the Message-Passing Interface [14]. 6.1 Scope The goal of our scaled down effort is to demonstrate how the observed techniques simplify the implementation of linear algebra libraries by providing a systematic (building-block) approach.
Reference: [16] <author> B. A. Hendrickson and D. E. Womble, </author> <title> "The Torus-Wrap Mapping for Dense Matrix Calculations on Massively Parallel Computers," </title> <journal> SIAM J. Sci. Comput., check issue number </journal>
Reference-contexts: Also y i = j=0 so there is a natural tie between subvectors of P y y and corresponding blocks of rows of P y AP T It has been well documented <ref> [16] </ref> that for scalability reasons, it is often important to assign matrices to nodes of a distributed memory architecture using a so-called two-dimensional matrix distribution.
Reference: [17] <author> Ken Klimkowski and Robert van de Geijn, </author> <title> "Anatomy of an out-of-core dense linear solver", </title> <booktitle> Vol III Algorithms and Applications Proceedings of the 1995 International Conference on Parallel Processing , pp. </booktitle> <pages> 29-33, </pages> <year> 1995. </year>
Reference-contexts: It is this ability to work with highly general and irregular blockings and distributions of matrices that will ultimately allow us to implement more general libraries than previously possible, as part of the full PLAPACK library. Also in the planning are out-of-core extensions <ref> [17] </ref>. It is often questioned whether the emphasis on dense linear algebra is misplaced in the first place [9].
Reference: [18] <author> J. G. Lewis and R. A. van de Geijn, </author> <title> "Implementing Matrix-Vector Multiplication and Conjugate Gradient Algorithms on Distributed Memory Multicomputers," </title> <booktitle> Supercomputing '93. </booktitle>
Reference: [19] <author> J. G. Lewis, D. G. Payne, and R. A. van de Geijn, </author> <title> "Matrix-Vector Multiplication and Conjugate Gradient Algorithms on Distributed Memory Computers," </title> <booktitle> Scalable High Performance Computing Conference 1994. </booktitle>
Reference: [20] <author> W. Lichtenstein and S. L. Johnsson, </author> <title> "Block-Cyclic Dense Linear Algebra", </title> <institution> Harvard University, Center for Research in Computing Technology, TR-04-92, </institution> <month> Jan., </month> <year> 1992. </year>
Reference: [21] <author> E. Rothberg and R. Schreiber, </author> <title> "Improved load distribution in parallel sparse Cholesky factorization." </title> <booktitle> in Proceedings of Supercomputing 94, </booktitle> <pages> pp. 783-792. </pages>
Reference-contexts: It is our position that the future of parallel dense linear algebra is in a support role for parallel sparse linear solvers, to solve dense subproblems that are part of very large sparse problems (e.g. exploited by parallel supernodal methods <ref> [21, 22] </ref>). It is thus important that the approach to distributing and manipulating the dense matrices is conformal with how they naturally occur as part of the sparse problem. Since our matrix distribution starts with the vector, we believe the described approach meets this criteria. For details, see [10].
Reference: [22] <author> E. Rothberg and R. Schreiber, </author> <title> "Efficient parallel sparse Cholesky factorization," </title> <booktitle> in Proceedings of the Seventh SIAM Conference on Parallel Processing for Scientific Computing (R. </booktitle> <editor> Schreiber, et al. eds.), </editor> <publisher> SIAM, </publisher> <year> 1994, </year> <pages> pp. 407-412. </pages>
Reference-contexts: It is our position that the future of parallel dense linear algebra is in a support role for parallel sparse linear solvers, to solve dense subproblems that are part of very large sparse problems (e.g. exploited by parallel supernodal methods <ref> [21, 22] </ref>). It is thus important that the approach to distributing and manipulating the dense matrices is conformal with how they naturally occur as part of the sparse problem. Since our matrix distribution starts with the vector, we believe the described approach meets this criteria. For details, see [10].
Reference: [23] <author> Robert van de Geijn and Jerrell Watts, "SUMMA: </author> <title> Scalable Universal Matrix Multiplication Algorithm," </title> <journal> Concurrency: Practice and Experience, </journal> <note> to appear. 26 </note>
Reference-contexts: For further details on parallel implementation of matrix-matrix multiplication and level 3 BLAS, see our papers "SUMMA: Scalable Universal Matrix Multiplication Algorithm" <ref> [23] </ref> and "Parallel Implementation of BLAS: General Techniques for Level 3 BLAS" [4].
References-found: 23

