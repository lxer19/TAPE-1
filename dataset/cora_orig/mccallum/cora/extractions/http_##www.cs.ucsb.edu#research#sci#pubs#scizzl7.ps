URL: http://www.cs.ucsb.edu/research/sci/pubs/scizzl7.ps
Refering-URL: http://www.cs.ucsb.edu/research/sci/pubs.html
Root-URL: http://www.cs.ucsb.edu
Email: fibel,schauser,chriss,weisg@cs.ucsb.edu  
Title: High-Performance Cluster Computing Using Scalable Coherent Interface  
Author: Maximilian Ibel, Klaus E. Schauser, Chris J. Scheiman, and Manfred Weis 
Web: http://www.cs.ucsb.edu/research/sci  
Address: Santa Barbara, CA 93106  
Affiliation: Department of Computer Science University of California, Santa Barbara  
Abstract: In this paper we discuss our experience with high-performance cluster computing using Scalable Coherent Interface (SCI), an IEEE communication standard. We present detailed measurements of basic SCI transactions on a cluster of 8 UltraSparcs connected via SBus-2 SCI adapter cards and an SCI switch. Our experiments show that the simple remote load and store transactions supported by SCI allow for a very low latency implementation of data transfers, resulting in much lower latencies than other cluster of workstations using networks such as Myrinet, and parallel machines such as the Meiko CS-2. We have implemented Active Messages (a low-latency communication substrate) and Split-C (a simple global address-space language) on this cluster. Supporting Active Message requires implementing a remote queue. We discuss several implementation alternatives for remote queues and evaluate these under micro-benchmarks as well as under full Split-C applications. Our performance measurements show that making remote queues deeper increases the round-trip latency of the individual communication operations, but improves the overall performance of full applications. Our experiments also reveal the need for co-scheduling to run parallel applications efficiently. 
Abstract-found: 1
Intro-found: 1
Reference: [ACP95] <author> T.E. Anderson, D.E. Culler, and D. Patterson. </author> <title> A case for NOW (Networks of Workstations). </title> <journal> IEEE Micro, </journal> <volume> 15(1), </volume> <month> February </month> <year> 1995. </year>
Reference-contexts: 1 Introduction Clusters of workstations connected via commodity networks have been tremendously successful <ref> [ACP95] </ref>. Like for parallel computers, low latency and high throughput communication is essential for those clusters.
Reference: [AHKL96] <author> G. Acher, H. Hellwagner, W. Karl, and M. Leberecht. </author> <title> A PCI-SCI Bridge for Building a PC Cluster with Distributed Shared Memory. </title> <booktitle> In Sixth International Workshop on SCI-based Low-cost/High-performance Computing, </booktitle> <month> Septem-ber </month> <year> 1996. </year>
Reference-contexts: Robinson developed a good analytical model for the network performance of SCI [Rob96]. Several other groups even build their own SCI network cards, for example the group at the University of Munich <ref> [AHKL96] </ref>. 7 Conclusions and Future Work In this paper we described our experience with a cluster of UltraSparc workstations connected by Dolphin SCI hardware. We are truly excited about the capabilities of SCI. First, the latency we observed on the system is very low compared to other cluster technologies.
Reference: [BCL + 95] <author> E. A. Brewer, F. T. Chong, L. T. Liu, S. D. Sharma, and J. Kubiatowicz. </author> <title> Remote Queues: Exposing Message Queues for Optimization and Atomicity. </title> <booktitle> In 7th Annual Symposium on Parallel Algorithms and Architectures, </booktitle> <month> July </month> <year> 1995. </year>
Reference-contexts: This is especially true for Active Messages, where each message handler is executed on the receiver. The receiving processor must be notified of new message arrival and synchronization requirements must be obeyed. Remote queues are an abstraction to address the notification and synchronization problems associated with message delivery <ref> [SS95, BCL + 95] </ref>. Sending processors just enqueue their messages on the remote queue. The receiver is notified of the message either using interrupts or polling. During a poll, the receiving processor checks whether something has been enqueued. If so, it removes the message from the queue and processes it.
Reference: [CA96] <author> R. Clark and K. Alnes. </author> <title> An SCI Interconnect Chipset and Adapter. </title> <booktitle> In Proc. of Hot Interconnects IV, </booktitle> <month> August </month> <year> 1996. </year>
Reference-contexts: SCI has been approved as an IEEE/ANSI high speed interconnection standard in 1992 [IEE93] and has since been used in several parallel machines (e.g. HP Convex) and forms the basis for existing and future large scale enterprise servers (e.g. Sequent NUMA-Q [LCS96], Sun Starfire [Mic96], Data General <ref> [CA96] </ref>). Dolphin has developed SCI adapter cards and switches for connecting workstations with various buses, including SBus-1, SBus-2 and PCI.
Reference: [CDG + 93] <author> D. E. Culler, A. Dusseau, S. C. Golstein, A. Krish-namurthy, S. Lumetta, T. von Eicken, and K. Yelick. </author> <title> Parallel Programming in Split-C. </title> <booktitle> In Proc. of Supercomputing, </booktitle> <month> November </month> <year> 1993. </year>
Reference-contexts: We also want to use the simplest communication substrate that still allows us to determine the basic trade-offs underlying every message passing scheme. Therefore, we use Active Messages as the communication substrate and the explicitly parallel language Split-C <ref> [CDG + 93] </ref>. To examine scalability, we measure the performance of a number of Split-C programs. We discuss a number of issues relevant to scalability, including the optimization of the Split-C communication library, SMP support, and co-scheduling.
Reference: [Dol95a] <author> Dolphin. </author> <title> Multiprocessor Systems Design With SCI and Dolphin Technology. Dolphin Interconnect Solutions, </title> <year> 1995. </year>
Reference-contexts: The Dolphin SCI adapter cards only supports read, write and move transaction (no locking). The device drivers for these adapter cards support two communication modes: synchronous read/write system calls implemented using the embedded DMA engines, and distributed shared memory mode <ref> [Dol95a, Dol95b] </ref>. The distributed shared memory interface uses mmap () system calls to map shared segments from other processors directly into the virtual address space of an application.
Reference: [Dol95b] <author> Dolphin. </author> <note> SBus-to-SCI Adapter User's Guide, DIS303 SBus-2. Dolphin Interconnect Solutions A.S., </note> <year> 1995. </year>
Reference-contexts: The Dolphin SCI adapter cards only supports read, write and move transaction (no locking). The device drivers for these adapter cards support two communication modes: synchronous read/write system calls implemented using the embedded DMA engines, and distributed shared memory mode <ref> [Dol95a, Dol95b] </ref>. The distributed shared memory interface uses mmap () system calls to map shared segments from other processors directly into the virtual address space of an application.
Reference: [GL95] <author> D. B. Gustavson and Q. Li. </author> <title> Local-Area MultiProcessor: the Scalable Coherent Interface. </title> <type> Technical report, </type> <institution> SCIzzl, Santa Clara University, Department of Computer Engineering, </institution> <address> Santa Clara, California, </address> <year> 1995. </year>
Reference-contexts: implementing efficient message passing primitives on our cluster of 8 UltraSparc workstations connected with a Scalable Coherent Interface (SCI) network. processor Ultra-2's, all others are uniprocessor Ultra-1 workstations. 1.1 Scalable Coherent Interface Gustavson and Li have had for a long time the vision of using SCI for high-performance cluster computing <ref> [GL95] </ref>. SCI has been approved as an IEEE/ANSI high speed interconnection standard in 1992 [IEE93] and has since been used in several parallel machines (e.g. HP Convex) and forms the basis for existing and future large scale enterprise servers (e.g. Sequent NUMA-Q [LCS96], Sun Starfire [Mic96], Data General [CA96]). <p> Gustavson and Li provided some of the earliest motivation for why SCI is extremely suitable for loosely connected network of workstations <ref> [GL95] </ref>. Probably the most comprehensive performance study of SBus-2 based SCI adapters is from Omang and Parady [Oma95, OP96]. In this study, important hardware parameters like bandwidth and latency in various configurations of two nodes, connected with one or two SCI rings, are explored.
Reference: [GPT + 96] <author> A. George, W. Phipps, R. Todd, D. Zirpoli, K. Justice, M. Giacoboni, and M. Sarwar. </author> <title> SCI and the scalable cluster architecture latency-hiding (SCALE) project. </title> <booktitle> In Proceedings of the Sixth International Workshop on SCI-based High-Performance Low-Cost Computing, </booktitle> <year> 1996. </year>
Reference-contexts: In this study, important hardware parameters like bandwidth and latency in various configurations of two nodes, connected with one or two SCI rings, are explored. SCI cluster research is reported by George et al., where SCI clusters are being described in the context of the SCALE cluster project <ref> [GPT + 96] </ref>, which strives to investigate techniques for efficient programming of workstation clusters for high performance computing. Robinson developed a good analytical model for the network performance of SCI [Rob96].
Reference: [IEE93] <editor> IEEE, </editor> <address> 345 East 47th Street, New York. </address> <institution> IEEE Standard for Scalable Coherent Interface (SCI), </institution> <year> 1993. </year>
Reference-contexts: SCI has been approved as an IEEE/ANSI high speed interconnection standard in 1992 <ref> [IEE93] </ref> and has since been used in several parallel machines (e.g. HP Convex) and forms the basis for existing and future large scale enterprise servers (e.g. Sequent NUMA-Q [LCS96], Sun Starfire [Mic96], Data General [CA96]).
Reference: [ISSW96] <author> M. Ibel, K. E. Schauser, C. J. Scheiman, and M. Weis. </author> <title> Implementing Active Messages and Split-C for SCI Clusters and Some Architectural Implications. </title> <booktitle> In Sixth International Workshop on SCI-based Low-cost/High-performance Computing, </booktitle> <month> September </month> <year> 1996. </year>
Reference-contexts: We can solve this problem by maintaining a separate message buffer for each sender on every processor. This setup, originally presented in <ref> [ISSW96] </ref>, is shown in Figure 3. Each message buffer can contain exactly one message. To check whether a message has arrived, a processor checks all of the incoming message (request) buffers. If any one contains a message, the receiving processor extracts the message, processes it, and sends a reply back.
Reference: [Jam96] <author> D. V. James. </author> <title> Combinable locks: NullSwap efficiently delays cache-line transfers. </title> <booktitle> In Sixth International Workshop on SCI-based Low-cost/High-performance Computing, </booktitle> <month> September </month> <year> 1996. </year>
Reference-contexts: Since many processes can concurrently enqueue data, we need some sort of atomic read/write operation, such as test-and-set, to prevent write conflicts [Oma97]. The SCI standard provides support for lock transactions <ref> [Jam96] </ref>. 1 Given such a lock transaction, our scheme for sending a message could be: lock the remote queue, read the destination write pointer, increment the pointer by the message size, check for overflow, write the pointer back to the receiver, put the message in the 1 Unfortunately, lock transactions are
Reference: [KSS + 96] <author> A. Krishnamurthy, K. E. Schauser, C. J. Scheiman, R. Y. Wang, D. E. Culler, and K. Yelick. </author> <title> Evaluation of Architectural Support for Global Address-Based Communication in Large-Scale Parallel Machines. </title> <booktitle> In 7th International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <month> October </month> <year> 1996. </year>
Reference-contexts: The numbers for the CM-5, Meiko CS-2, Paragon, T3D and Myrinet NOW are from <ref> [KSS + 96] </ref>. sender. Before sending the next segment of a bulk message the sender has to wait for this confirmation, to make sure the buffer can be reused.
Reference: [LCS96] <author> T. D. Lovett, R. M. Clapp, and R. J. Safranek. NUMA-Q: </author> <title> An SCI based Enterprise Server. </title> <booktitle> In Sixth International Workshop on SCI-based Low-cost/High-performance Computing, </booktitle> <month> Septem-ber </month> <year> 1996. </year>
Reference-contexts: SCI has been approved as an IEEE/ANSI high speed interconnection standard in 1992 [IEE93] and has since been used in several parallel machines (e.g. HP Convex) and forms the basis for existing and future large scale enterprise servers (e.g. Sequent NUMA-Q <ref> [LCS96] </ref>, Sun Starfire [Mic96], Data General [CA96]). Dolphin has developed SCI adapter cards and switches for connecting workstations with various buses, including SBus-1, SBus-2 and PCI.
Reference: [Mic96] <author> Sun Microsystem. </author> <title> Sun Cluster Channel. </title> <note> http://www.sun.com/products-n-solutions/hw/servers/hpc/tech/interconnect.html, 1996. </note>
Reference-contexts: SCI has been approved as an IEEE/ANSI high speed interconnection standard in 1992 [IEE93] and has since been used in several parallel machines (e.g. HP Convex) and forms the basis for existing and future large scale enterprise servers (e.g. Sequent NUMA-Q [LCS96], Sun Starfire <ref> [Mic96] </ref>, Data General [CA96]). Dolphin has developed SCI adapter cards and switches for connecting workstations with various buses, including SBus-1, SBus-2 and PCI.
Reference: [Oma95] <author> K. Omang. </author> <title> Performance results from SALMON, a cluster of Workstations Connected by SCI. </title> <type> Technical report, </type> <institution> Department of Informatics, University of Oslo, Norway, </institution> <year> 1995. </year>
Reference-contexts: Gustavson and Li provided some of the earliest motivation for why SCI is extremely suitable for loosely connected network of workstations [GL95]. Probably the most comprehensive performance study of SBus-2 based SCI adapters is from Omang and Parady <ref> [Oma95, OP96] </ref>. In this study, important hardware parameters like bandwidth and latency in various configurations of two nodes, connected with one or two SCI rings, are explored.
Reference: [Oma97] <author> K. Omang. </author> <title> Synchronization Support in I/O Adapter Based SCI Clusters. </title> <booktitle> In Proceedings of Workshop on Communication and Architectural Support for Network-based Parallel Computing, </booktitle> <address> CANPC'97, </address> <month> April </month> <year> 1997. </year>
Reference-contexts: Since many processes can concurrently enqueue data, we need some sort of atomic read/write operation, such as test-and-set, to prevent write conflicts <ref> [Oma97] </ref>.
Reference: [OP96] <author> K. Omang and B. Parady. </author> <title> Performance of Low-Cost UltraSparc Multiprocessors connected by SCI. </title> <type> Technical report, </type> <institution> Department of Informatics, University of Oslo, Norway and Sun Microsystems Inc., </institution> <year> 1996. </year>
Reference-contexts: Gustavson and Li provided some of the earliest motivation for why SCI is extremely suitable for loosely connected network of workstations [GL95]. Probably the most comprehensive performance study of SBus-2 based SCI adapters is from Omang and Parady <ref> [Oma95, OP96] </ref>. In this study, important hardware parameters like bandwidth and latency in various configurations of two nodes, connected with one or two SCI rings, are explored.
Reference: [Rob96] <author> J. G. Robinson. </author> <title> Performance of SCI Clusters. </title> <booktitle> In Sixth Workshop on Scalable Shared-Memory Multiprocessors, </booktitle> <month> September </month> <year> 1996. </year>
Reference-contexts: Robinson developed a good analytical model for the network performance of SCI <ref> [Rob96] </ref>. Several other groups even build their own SCI network cards, for example the group at the University of Munich [AHKL96]. 7 Conclusions and Future Work In this paper we described our experience with a cluster of UltraSparc workstations connected by Dolphin SCI hardware.
Reference: [SS95] <author> K. E. Schauser and C. J. Scheiman. </author> <title> Experience with Active Messages on the Meiko CS-2. </title> <booktitle> In 9th International Parallel Processing Symposium, </booktitle> <month> April </month> <year> 1995. </year>
Reference-contexts: This is especially true for Active Messages, where each message handler is executed on the receiver. The receiving processor must be notified of new message arrival and synchronization requirements must be obeyed. Remote queues are an abstraction to address the notification and synchronization problems associated with message delivery <ref> [SS95, BCL + 95] </ref>. Sending processors just enqueue their messages on the remote queue. The receiver is notified of the message either using interrupts or polling. During a poll, the receiving processor checks whether something has been enqueued. If so, it removes the message from the queue and processes it.
Reference: [SW95] <author> P. G. Sobalvarro and W. E. Weihl. </author> <title> Demand-based coscheduling of parallel jobs on multiprogrammed multiprocessors. </title> <booktitle> In Proceedings of the Parallel Job Scheduling Workshop at IPPS 95, </booktitle> <year> 1995. </year>
Reference-contexts: While traditional co-scheduling has been researched for a long time, it proves difficult to implement on workstation clusters. Instead, we plan to use Glunix [VGA94] or a demand-based or dynamic co-scheduling techniques, as described in <ref> [SW95] </ref>. 6 Related Work Several other groups are also studying SCI to connect network of workstations. Gustavson and Li provided some of the earliest motivation for why SCI is extremely suitable for loosely connected network of workstations [GL95].
Reference: [vEBBV95] <author> T. von Eicken, A. Basu, V. Buch, and W. Vogels. U-Net: </author> <title> A User-Level Network Interface for Parallel and Distributed Computing. </title> <booktitle> In Proc. Symposium on Operating Systems Principles, </booktitle> <year> 1995. </year>
Reference-contexts: In particular we are interested in Active Messages [vECGS92], an efficient communication primitive. An Active Message contains the address of a handler function, which, upon arrival, is executed on the receiving processor. In contrast to interconnects like Ethernet and ATM <ref> [vEBBV95] </ref>, SCI uses shared memory semantics for data transfer, that is, an application transfers data by using plain load and store operations that are automatically transformed into remote communication. We base our Active Messages implementation on shared memory, and use the DMA-based transfer only for long message to maximize throughput.
Reference: [vECGS92] <author> T. von Eicken, D. E. Culler, S. C. Goldstein, and K. E. Schauser. </author> <title> Active Messages: a Mechanism for Integrated Communication and Computation. </title> <booktitle> In Proc. of the 19th Int'l Symposium on Computer Architecture, </booktitle> <address> Gold Coast, Australia, </address> <month> May </month> <year> 1992. </year>
Reference-contexts: In particular we are interested in Active Messages <ref> [vECGS92] </ref>, an efficient communication primitive. An Active Message contains the address of a handler function, which, upon arrival, is executed on the receiving processor.
Reference: [VGA94] <author> A. Vahdat, D. Ghormley, and T. Anderson. </author> <title> Efficient, Portable, and Robust Extension of Operating System Functionality. </title> <type> CS, </type> <institution> UC Berkeley, </institution> <month> December </month> <year> 1994. </year>
Reference-contexts: The programs run on all 8 nodes. enhance the Solaris scheduler with some gang-scheduling capabilities. While traditional co-scheduling has been researched for a long time, it proves difficult to implement on workstation clusters. Instead, we plan to use Glunix <ref> [VGA94] </ref> or a demand-based or dynamic co-scheduling techniques, as described in [SW95]. 6 Related Work Several other groups are also studying SCI to connect network of workstations. Gustavson and Li provided some of the earliest motivation for why SCI is extremely suitable for loosely connected network of workstations [GL95].
Reference: [WBvE96] <author> M. Welsh, A. Basu, and T. von Eicken. </author> <title> Low-Latency Communication over Fast Ethernet. </title> <booktitle> In Proceedings of Euro-Par '96, </booktitle> <month> August </month> <year> 1996. </year>
Reference-contexts: The results are summarized in Table 1 and show roundtrip latencies between 7:9s for 4-byte stores up to 12s for 64-byte bulk read instructions. These results are better than the fastest measurements reported for comparable commodity networks, for example fast Ethernet (60s) <ref> [WBvE96] </ref> or Myrinet (20s for 4-byte transfers as reported by Myricom).
References-found: 25

