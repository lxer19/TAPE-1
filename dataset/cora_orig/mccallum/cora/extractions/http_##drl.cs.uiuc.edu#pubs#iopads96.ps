URL: http://drl.cs.uiuc.edu/pubs/iopads96.ps
Refering-URL: http://drl.cs.uiuc.edu/pubs/iopads96.html
Root-URL: http://www.cs.uiuc.edu
Email: fying,winslettg@cs.uiuc.edu  
Phone: (217) 244-6381 (voice) (217) 244-6500 (fax)  
Title: Scalable Message Passing in Panda  
Author: Y. Chen, M. Winslett, K. E. Seamons, S. Kuo, Y. Cho, M. Subramaniam 
Affiliation: Computer Science Department, University of Illinois  
Abstract: To provide high performance for applications with a wide variety of i/o requirements and to support many different parallel platforms, the design of a parallel i/o system must provide for efficient utilization of available bandwidth both for disk traffic and for message passing. In this paper we discuss the message-passing scalability of the server-directed i/o architecture of Panda, a library for synchronized i/o of multidimensional arrays on parallel platforms. We show how to improve i/o performance in situations where message-passing is a bottleneck, by combining the server-directed i/o strategy for highly efficient use of available disk bandwidth with new mechanisms to minimize internal communication and computation overhead in Panda. We present experimental results that show that with these improvements, Panda will provide high i/o performance for a wider range of applications, such as applications running with slow interconnects, applications performing i/o operations on large numbers of arrays, or applications that require drastic data rearrangements as data are moved between memory and disk (e.g., array transposition). We also argue that in the future, the improved approach to message-passing will allow Panda to support applications that are not closely synchronized or that run in heterogeneous environments. 
Abstract-found: 1
Intro-found: 1
Reference: [Bordawekar93] <author> R. Bordawekar, J. Miguel del Rosario, and A. Choudary, </author> <title> Design and Evaluation of Primitives for Parallel I/O, </title> <booktitle> Proceedings of Supercomputing '93, </booktitle> <pages> pages 452-461, </pages> <year> 1993. </year>
Reference-contexts: Besides the Panda library, several parallel file systems and multidimensional array libraries have provided a collective i/o interface [Corbett95, del Rosario94, Karpovich94, Seamons94, Seamons95, Seligman94]; some of them have shown that collective i/o provides high performance given appropriate language, compiler and run-time system support. <ref> [Bordawekar93] </ref> considers a `two-phase' access strategy for collective i/o.
Reference: [Corbett95] <author> P. Corbett, D. Feitelson, Y. Hsu, J. Prost, M. Snir, S. Fineberg, B. Nitzberg, B. Traversat, and P. Wong. </author> <title> MPI-IO: A Parallel File I/O Interface for MPI, </title> <type> Technical Report NAS-95-002, </type> <institution> NASA Ames Research Center, </institution> <month> January </month> <year> 1995. </year>
Reference-contexts: Besides the Panda library, several parallel file systems and multidimensional array libraries have provided a collective i/o interface <ref> [Corbett95, del Rosario94, Karpovich94, Seamons94, Seamons95, Seligman94] </ref>; some of them have shown that collective i/o provides high performance given appropriate language, compiler and run-time system support. [Bordawekar93] considers a `two-phase' access strategy for collective i/o.
Reference: [del Rosario94] <author> J. M. del Rosario, M. Harry, A. Choudhary, </author> <title> The Design of VIP-FS: A Virtual, Parallel File System for High Performance Parallel and Distributed Computing, </title> <type> Technical Report SCCS-628, </type> <institution> NPAC, Syracuse, </institution> <address> NY, </address> <month> May </month> <year> 1994. </year>
Reference-contexts: Besides the Panda library, several parallel file systems and multidimensional array libraries have provided a collective i/o interface <ref> [Corbett95, del Rosario94, Karpovich94, Seamons94, Seamons95, Seligman94] </ref>; some of them have shown that collective i/o provides high performance given appropriate language, compiler and run-time system support. [Bordawekar93] considers a `two-phase' access strategy for collective i/o.
Reference: [Galbreath93] <author> N. Galbreath, W. Gropp, and D. Levine, </author> <title> Applications-Driven Parallel I/O, </title> <booktitle> Proceedings of Supercomputing '93, </booktitle> <pages> pages 462-471, </pages> <year> 1993. </year>
Reference: [Karpovich94] <author> J. F. Karpovich, A. S. Grimshaw, J. C. </author> <title> French, Extensible File Systems (ELFS): An Object-Oriented Approach to High Performance File I/O, </title> <booktitle> Proceedings of the International Conference on Object-Oriented Programming, Systems, Languages, and Applications, </booktitle> <address> Portland OR, </address> <month> August </month> <year> 1994. </year>
Reference-contexts: Besides the Panda library, several parallel file systems and multidimensional array libraries have provided a collective i/o interface <ref> [Corbett95, del Rosario94, Karpovich94, Seamons94, Seamons95, Seligman94] </ref>; some of them have shown that collective i/o provides high performance given appropriate language, compiler and run-time system support. [Bordawekar93] considers a `two-phase' access strategy for collective i/o.
Reference: [Kotz94a] <author> D. Kotz, and N. Nieuwejaar, </author> <title> Dynamic File-Access Characteristics of a Production Parallel Scientific Workload. </title> <booktitle> In Proceedings of Supercomputing '94, </booktitle> <pages> pages 640-649, </pages> <month> November </month> <year> 1994. </year>
Reference: [Kotz94b] <author> D. Kotz, </author> <title> Disk-Directed I/O for MIMD Multiprocessors, </title> <booktitle> First Symposium on Operating Systems Design and Implementation, </booktitle> <month> November </month> <year> 1994. </year>
Reference-contexts: The design of Panda 2.0 is based on the `disk-directed i/o' for collective i/o operations that is proposed in <ref> [Kotz94b] </ref>. While different in many respects, under both architectures nodes direct the data flow in an i/o request in order to make sequential file system requests and highly utilize the underlying file system throughput.
Reference: [mpi94] <author> Message Passing Interface Forum, </author> <title> MPI: A Message-Passing Interface Standard. Messge Passing Interface Forum, </title> <month> May 5, </month> <year> 1994. </year>
Reference-contexts: Each boundary point contains N numbers, where N is the rank of the array. Because Panda uses the MPI derived data type <ref> [mpi94] </ref> to transfer the portions of array data which may be stored in non-contiguous memory locations on the sender side, and received into non-contiguous memory locations on the receiver side, the transfer rate of such data can be quite low, and T message (data) can be quite high. <p> First, a nonblocking message passing facility, such as the MPI-F implementation used by Panda on the SP2, is needed for array data messages, to avoid serializing the data transfers. With nonblocking MPI communication <ref> [mpi94] </ref>, a message transfer is separated into two phases. In the first phase, the message sends and receives are initiated but not completed. A sender can return from the initiation call before the message is copied out of the send buffer. <p> There are many possible ways to solve this buffer overflow problem; Panda 2.1 uses the synchronous mode of MPI nonblocking message sends, which is part of the MPI standard <ref> [mpi94] </ref>. Using the synchronous mode with nonblock-ing message sends, in the case of write requests, all clients can initiate message sends to the relevant servers, but the data messages will not be copied out of clients' send buffers until the corresponding servers initiate MPI receive calls.
Reference: [Pasquale94] <author> B. Pasquale, and G. Polyzos, </author> <title> Dynamic I/O Characterization of I/O Intensive Scientific Applications, </title> <type> Technical Report No. </type> <institution> CS94-364, University of California, </institution> <address> San Diego, </address> <month> April </month> <year> 1994. </year>
Reference: [Purakayastha94] <author> A. Purakayastha, C. Ellis, D. Kotz, N. Nieuwejaar, and M. </author> <title> Best, Characterizing Parallel File-Access Patterns on a Large-Scale Multiprocessor, </title> <institution> Duke University Technical Report CS-1994-33, </institution> <month> October </month> <year> 1994. </year>
Reference: [Seamons94] <author> K. E. Seamons and M. Winslett, </author> <title> An Efficient Abstract Interface for Multidimensional Array I/O, </title> <booktitle> Proceedings of Supercomputing '94, </booktitle> <address> Wash-ington D.C., </address> <month> November </month> <year> 1994. </year>
Reference-contexts: Besides the Panda library, several parallel file systems and multidimensional array libraries have provided a collective i/o interface <ref> [Corbett95, del Rosario94, Karpovich94, Seamons94, Seamons95, Seligman94] </ref>; some of them have shown that collective i/o provides high performance given appropriate language, compiler and run-time system support. [Bordawekar93] considers a `two-phase' access strategy for collective i/o.
Reference: [Seamons95] <author> K. E. Seamons, Y. Chen, P. Jones, J. Jozwiak, and M. Winslett, </author> <title> Server-Directed Collective I/O in Panda, </title> <booktitle> Proceedings of Supercomputing '95, </booktitle> <address> San Diego, CA, </address> <month> December </month> <year> 1995. </year>
Reference-contexts: This results in poor i/o performance due to buffering errors and excessive disk seeks, the two flaws that Panda seeks to remedy. As described in <ref> [Seamons95] </ref>, Panda is distributed across both compute nodes and i/o nodes. Panda clients reside on the compute nodes, and Panda servers reside on the i/o nodes, as shown in Figure 1. <p> Then in order to realize sequential writes, Panda adopts a server-directed i/o approach. In the implementation of server-directed i/o in Panda 2.0 <ref> [Seamons95] </ref>, Panda servers direct the flow of i/o requests by gathering the data for each array chunk in sequence. <p> Once a chunk is gathered, the server writes it out to disk. The reverse strategy is employed for reads. Thus for each array data transfer between a client and a server, Panda 2.0 requires two messages, a data request message and an array data message. <ref> [Seamons95] </ref> presents the architecture of Panda 2.0 in more detail and shows the excellent performance levels Panda 2.0 achieves in reading and writing large arrays on the NASA Ames NAS IBM SP2. <p> The changes sketched above only affect the internal design of Panda, not the interface seen by users. As described in <ref> [Seamons95] </ref>, the use of high level interfaces in Panda provides application portability, frees application developers from worrying about the low-level details of file system calls, and more importantly, it gives us great freedom to do performance optimizations and to experiment with new underlying implementations. <p> We began experimentation by regression-testing Panda 2.1 against benchmark 2.0 (reading and writing very large arrays, both with real disks and simulated infinitely fast disks). As expected, we found that in all cases Panda 2.1 delivered the same or better performance than Panda 2.0 <ref> [Seamons95] </ref>, as discussed briefly at the end of this section. We then moved on to experiments with large numbers of small arrays, discussed immediately below. The new experiments characterize the i/o requirements of the black hole application as described in section 1. <p> Panda's normalized throughput for array reads and writes is calculated by dividing Panda's throughput per i/o node by the peak AIX throughput. The experiments described below all measure write operations, as reads and writes in Panda show the same performance trends <ref> [Seamons95] </ref>. Under our experimental setup, we expect that when the number of i/o nodes is small, Panda's throughput on each i/o node can be very close to the peak AIX file system throughput. <p> Besides the Panda library, several parallel file systems and multidimensional array libraries have provided a collective i/o interface <ref> [Corbett95, del Rosario94, Karpovich94, Seamons94, Seamons95, Seligman94] </ref>; some of them have shown that collective i/o provides high performance given appropriate language, compiler and run-time system support. [Bordawekar93] considers a `two-phase' access strategy for collective i/o.
Reference: [Seligman94] <author> E. Seligman, and A. Beguelin, </author> <title> High-Level Fault Tolerance in Distributed Programs, </title> <institution> School of Computer Science, Carnegie Mellon University, USA. </institution> <type> Technical report CMU-CS-94-223, </type> <month> December </month> <year> 1994. </year>
Reference-contexts: Besides the Panda library, several parallel file systems and multidimensional array libraries have provided a collective i/o interface <ref> [Corbett95, del Rosario94, Karpovich94, Seamons94, Seamons95, Seligman94] </ref>; some of them have shown that collective i/o provides high performance given appropriate language, compiler and run-time system support. [Bordawekar93] considers a `two-phase' access strategy for collective i/o.
Reference: [Sio94] <institution> Application Working Group of the Scalable I/O Initiative. </institution> <note> Preliminary Survey of I/O Intensive Applications. Scalable I/O Initiative Working Paper No. 1. </note>
Reference-contexts: From our collaborations with scientists at NASA Ames Research Center and at NCSA at the University of Illinois who deal with multidimensional arrays, and the listed i/o requirements of several data-intensive applications collected by the Application Working Group of the Scalable I/O Initiative <ref> [Sio94] </ref>, it is evident that synchronized collective i/o operations are indeed widely and intensively used.

References-found: 14

