URL: http://c.gp.cs.cmu.edu:5103/afs/cs.cmu.edu/project/scandal/public/papers/CMU-CS-96-197.ps.gz
Refering-URL: http://c.gp.cs.cmu.edu:5103/afs/cs.cmu.edu/project/scandal/public/papers/CMU-CS-96-197.html
Root-URL: http://www.cs.cmu.edu
Title: A Framework for Space and Time Efficient Scheduling of Parallelism  
Author: Girija J. Narlikar Guy E. Blelloch 
Address: Pittsburgh, PA 15213  
Affiliation: School of Computer Science Carnegie Mellon University  
Date: December 1996  
Pubnum: CMU-CS-96-197  
Abstract: Many of today's high level parallel languages support dynamic, fine-grained parallelism. These languages allow the user to expose all the parallelism in the program, which is typically of a much higher degree than the number of processors. Hence an efficient scheduling algorithm is required to assign computations to processors at runtime. Besides having low overheads and good load balancing, it is important for the scheduling algorithm to minimize the space usage of the parallel program. In this paper, we first present a general framework to model non-preemptive parallel computations based on task graphs, in which schedules of the graphs represent executions of the computations. We then prove bounds on the space and time requirements of certain classes of schedules that can be generated by an offline scheduler. Next, we present an online scheduling algorithm that is provably space-efficient and time-efficient for multithreaded computations with nested parallelism. If a serial execution requires S 1 units of memory for a computation of depth D and work W , our algorithm results in an execution on p processors that requires S 1 + O(pD log p) units of memory, and O(W=p + D log p) time, including scheduling overheads. Finally, we demonstrate that our scheduling algorithm is efficient in practice. We have implemented a runtime system that uses our algorithm to schedule parallel threads. The results of executing parallel programs on this system show that our scheduling algorithm significantly reduces memory usage compared to previous techniques, without compromising performance. This research is sponsored by the Wright Laboratory, Aeronautical Systems Center, Air Force Materiel Command, USAF, and the Advanced Research Projects Agency (ARPA) under grant F33615-93-1-1330. The U.S. Government is authorized to reproduce and distribute reprints for Government purposes, notwithstanding any copyright notation thereon. Views and conclusions contained in this document are those of the author and should not be interpreted as representing the official policies, either expressed or implied, of ARPA or the U.S. Government. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Arvind, Rishiyur S. Nikhil, and Keshav K. Pingali. I-structures: </author> <title> Data structures for parallel computing. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 11(4):598632, </volume> <month> October </month> <year> 1989. </year>
Reference-contexts: 1 Introduction Many of today's high level parallel programming languages provide constructs to express dynamic, fine-grained parallelism. Such languages include data-parallel languages such as NESL [3] and HPF [21], as well as control-parallel languages such as ID <ref> [1] </ref>, Cilk [5], CC++ [12], Sisal [17], and Proteus [26]. These languages allow the user to expose all the parallelism in the program, which is typically of a much higher degree than the number of processors. The language implementation is responsible for scheduling this parallelism onto the processors. <p> We say the timestep t of S 0 p completes a level l 0 of C 1 if it is the first timestep at the end of which all nodes in C 1 <ref> [1] </ref>; : : : ; C 1 [l 0 ] are computed; that is, if 1. 9l 2 f1; : : : ; l 0 g; C 1 [l] 6 C t1 ; and, Let C i be any configuration of S 0 p , and let C 1 j be
Reference: [2] <author> G. E. Blelloch, P. B. Gibbins, and Y. Matias. </author> <title> Provably efficient scheduling for languages with fine-grained parallelism. </title> <booktitle> In Proceedings of the 1995 ACM Symposium on Parallel Algorithms and Architectures, </booktitle> <address> Santa Barbara, </address> <month> July </month> <year> 1995. </year> <note> ACM. </note>
Reference-contexts: If S 1 is the space required by the serial execution, these techniques generate executions for a multithreaded computation on p processors that require no more than p S 1 space. A scheduling algorithm that significantly improved this bound was recently proposed <ref> [2] </ref>, and was used to prove time and space bounds for the implementation of NESL [4]. <p> We then present an online, asynchronous scheduling algorithm called Async-Q, which generates a schedule with the same space bound of S 1 + O (p D log p) (including scheduler space) that is obtained in <ref> [2] </ref>; the algorithm assumes an EREW PRAM with a unit time fetch-and-add operation. As with their scheduling algorithm, our Async-Q algorithm applies to task graphs representing nested parallelism. <p> This results in better locality, and since threads are suspended and rescheduled less often, it has low scheduling overheads. The asynchronous nature of our algorithm 1 results in an execution order that differs from the order generated by the algorithm in <ref> [2] </ref>. This requires a different approach to prove space-efficiency. In addition to proving space-efficiency, as with [2], we bound the time required to execute the schedule generated by our algorithm in terms of the total work (number of operations) W and depth D of the parallel computation. <p> The asynchronous nature of our algorithm 1 results in an execution order that differs from the order generated by the algorithm in <ref> [2] </ref>. This requires a different approach to prove space-efficiency. In addition to proving space-efficiency, as with [2], we bound the time required to execute the schedule generated by our algorithm in terms of the total work (number of operations) W and depth D of the parallel computation. <p> Finally, we summarize and discuss future work in Section 8. 2 Modeling parallel computations: graphs and schedules In this section, we describe a model for parallel computations based on directed acyclic graphs (DAGs), similar to the one defined in <ref> [2] </ref>. In this model, a parallel computation can be represented by a DAG called a computation graph. In a computation graph G = (V; E), each node v 2 V corresponds to a unit computation, which requires a single timestep to be executed on one processor. <p> Blelloch, Gibbons and Matias proved the following theorem in <ref> [2] </ref>. Theorem 2.1 Let G be a computation graph with weight function w and depth D, and let S 1 be any 1-schedule of G. <p> We call a prioritized schedule that is greedy a greedy prioritized schedule. Let S 1 = (fv 1 g; fv 2 g; : : : ; fv n g) be a 1-schedule for a task graph with n tasks. As defined in <ref> [2] </ref>, we say a prioritized p-schedule is based on S 1 if the relative priorities of tasks are based on their serial execution order: 8i; j 2 f1; : : :; ng; i &lt; j ) priority (v i ) &gt; priority (v j ). <p> For the model of nested parallelism described above, the parallel computation results in a series-parallel task graph. A series-parallel task graph, similar to a series-parallel computation graph <ref> [2] </ref>, can be defined inductively: the graph G 0 consisting of a single task (which is both its source and sink) and no edges, is series parallel. <p> Lemma 6.1 The threads in R are always in decreasing order of priorities. Proof: This can be proved by induction; the proof is similar to the one given in <ref> [2] </ref>, and we present the outline here. <p> However, there may be some unit computations that allocate more than K units; they are handled in the following manner, similar to the technique suggested in <ref> [2] </ref>. The key idea is to delay the big allocations, so that if tasks with lower 1DF-numbers become ready, they will be executed instead. Consider a unit computation of a thread that allocates m units of space (m &gt; K), in a parallel computation with work w and depth d.
Reference: [3] <author> Guy E. Blelloch, Siddhartha Chatterjee, Jonathan C. Hardwick, Jay Sipelstein, and Marco Zagha. </author> <title> Implementation of a portable nested data-parallel language. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 21(1):414, </volume> <month> April </month> <year> 1994. </year> <month> 29 </month>
Reference-contexts: 1 Introduction Many of today's high level parallel programming languages provide constructs to express dynamic, fine-grained parallelism. Such languages include data-parallel languages such as NESL <ref> [3] </ref> and HPF [21], as well as control-parallel languages such as ID [1], Cilk [5], CC++ [12], Sisal [17], and Proteus [26]. These languages allow the user to expose all the parallelism in the program, which is typically of a much higher degree than the number of processors.
Reference: [4] <author> Guy E. Blelloch and John Greiner. </author> <title> A provable time and space efficient implementation of nesl. </title> <booktitle> In ACM SIGPLAN International Conference on Functional Programming, </booktitle> <pages> pages 213225, </pages> <month> May </month> <year> 1996. </year>
Reference-contexts: A scheduling algorithm that significantly improved this bound was recently proposed [2], and was used to prove time and space bounds for the implementation of NESL <ref> [4] </ref>. It generates a schedule that uses only S 1 + O (p D log p) space on a standard p-processor EREW PRAM, where D is the depth of the parallel computation (i.e., the longest sequence of dependencies or the critical path in the computation).
Reference: [5] <author> R. D. Blumofe, C. F. Joerg, B. C. Kuszmaul, C. E. Leiserson, K. H. Randall, and Y. Zhou. Cilk: </author> <title> An efficient multithreaded runtime system. </title> <booktitle> In Proceedings of the Fifth ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming, </booktitle> <pages> pages 207216, </pages> <month> November </month> <year> 1995. </year>
Reference-contexts: 1 Introduction Many of today's high level parallel programming languages provide constructs to express dynamic, fine-grained parallelism. Such languages include data-parallel languages such as NESL [3] and HPF [21], as well as control-parallel languages such as ID [1], Cilk <ref> [5] </ref>, CC++ [12], Sisal [17], and Proteus [26]. These languages allow the user to expose all the parallelism in the program, which is typically of a much higher degree than the number of processors. The language implementation is responsible for scheduling this parallelism onto the processors. <p> Therefore, for simplicity, we have implemented the scheduler as a serial computation, and the worker processors take turns executing it. The parallel programs executed on this system have been explicitly hand-coded in the continuation-passing style, similar to the code generated by the Cilk preprocessor <ref> [5] </ref>. Each continuation points to a C function representing the next computation of a thread, and a structure containing all its arguments. These continuations are created and moved between the queues. <p> We have used a value of K = 1000 bytes in our experiments. Figure 13 also shows the speedups for the same programs running on an existing space-efficient system Cilk <ref> [5] </ref>; the time performance on our system is comparable with that on Cilk. 25 The speedup on p processors is the time taken for the serial C version of the program divided by the time for our runtime system to run it on p processors. <p> For each application, the solid line represents the speedup on our system, while the dashed line represents the speedup on the Cilk <ref> [5] </ref> system. The low overheads of our runtime system account for the speedups for p = 1 being marginally lower than 1. matrix multiplication. Serial work is the time taken by a single processor executing the equivalent serial C program. For ideal speedups, all the other components would be zero. <p> We are currently working on ways to incorporate coarsening in the algorithm itself, instead of relying on compiler support. 7.2 Space performance each program. One implementation is on the existing Cilk <ref> [5] </ref> system. The other two implementations are on our scheduling system; in one case, large memory allocations are delayed by inserting dummy nodes, and in the other case they are allowed to proceed immediately. <p> We compare the memory usage of each program when the big memory allocations are delayed by inserting dummy threads (using K = 1000), with when they are allowed to proceed without any delay, as well as with the memory usage on Cilk <ref> [5] </ref>. The version without the delay on our system (labeled as No delay) is an estimate of the memory usage resulting from previous scheduling techniques.
Reference: [6] <author> R. D. Blumofe and C. E. Leiserson. </author> <title> Space-efficient scheduling of multithreaded computations. </title> <booktitle> In Proc. 25th ACM Symp. on Theory of Computing, </booktitle> <pages> pages 362371, </pages> <month> May </month> <year> 1993. </year>
Reference-contexts: Early attempts to reduce the memory usage of parallel computations were based on heuristics that limited the parallelism [10, 15, 28, 32], and are not guaranteed to be space-efficient in general. These were followed by scheduling techniques that provide proven space bounds for parallel programs <ref> [6, 8, 9] </ref>. If S 1 is the space required by the serial execution, these techniques generate executions for a multithreaded computation on p processors that require no more than p S 1 space. <p> Assuming that F (B,i,j) does not allocate any space, the serial execution requires O (n) space, since the space for array B is reused for each i-iteration. Now consider the parallel implementation of this function on p processors, where p &lt; n. Previous scheduling systems <ref> [6, 8, 9, 19, 23, 28, 32] </ref>, which include both heuristic-based and provably space-efficient techniques, would schedule the outer level of parallelism first. This results in all the p processors executing one i-iteration each, and hence the total space allocated is O (p n). <p> A node v is added to the set of ready nodes after all its parents have been computed, and all the latencies imposed by the edges into v are satisfied (i.e., at timestep t ready (v)). Blumofe and Leiserson <ref> [6] </ref> showed that any greedy p-schedule for a computation graph with depth D and work W has a length of at most W=p + D. In this section, we prove a similar upper bound on the length of any greedy p-schedule for latency-weighted DAGs.
Reference: [7] <author> R. D. Blumofe and C. E. Leiserson. </author> <title> Scheduling multithreaded computations by work stealing. </title> <booktitle> In Proc. 35th IEEE Symp. on Foundations of Computer Science, </booktitle> <pages> pages 356368, </pages> <month> November </month> <year> 1994. </year>
Reference-contexts: If the scheduling is done at runtime, then the performance of the high-level code relies heavily on the scheduling algorithm, which should have low scheduling overheads and good load balancing. Several systems providing dynamic parallelism have been implemented with efficient runtime sched-ulers <ref> [7, 11, 13, 18, 19, 22, 23, 29, 30, 31] </ref>, resulting in good parallel performance. However, in addition to good time performance, the memory requirements of the parallel computation must be taken into consideration.
Reference: [8] <author> F. W. Burton. </author> <title> Storage management in virtual tree machines. </title> <journal> IEEE Trans. on Computers, </journal> <volume> 37(3):321 328, </volume> <year> 1988. </year>
Reference-contexts: Early attempts to reduce the memory usage of parallel computations were based on heuristics that limited the parallelism [10, 15, 28, 32], and are not guaranteed to be space-efficient in general. These were followed by scheduling techniques that provide proven space bounds for parallel programs <ref> [6, 8, 9] </ref>. If S 1 is the space required by the serial execution, these techniques generate executions for a multithreaded computation on p processors that require no more than p S 1 space. <p> Assuming that F (B,i,j) does not allocate any space, the serial execution requires O (n) space, since the space for array B is reused for each i-iteration. Now consider the parallel implementation of this function on p processors, where p &lt; n. Previous scheduling systems <ref> [6, 8, 9, 19, 23, 28, 32] </ref>, which include both heuristic-based and provably space-efficient techniques, would schedule the outer level of parallelism first. This results in all the p processors executing one i-iteration each, and hence the total space allocated is O (p n). <p> Cilk uses less memory than this estimate due to its use of randomization: an idle processor steals the topmost thread (representing the outermost parallelism) from the private queue of a randomly picked processor; this thread may not represent the outermost parallelism in the entire computation. Previous techniques like <ref> [8, 9, 19] </ref> use a strategy similar to that of Cilk. The results show that when big allocations are delayed with dummy nodes, our algorithm results in a significantly lower memory usage in all the programs.
Reference: [9] <author> F. W. Burton and D. J. Simpson. </author> <title> Space efficient execution of deterministic parallel programs. </title> <type> Manuscript, </type> <month> December </month> <year> 1994. </year>
Reference-contexts: Early attempts to reduce the memory usage of parallel computations were based on heuristics that limited the parallelism [10, 15, 28, 32], and are not guaranteed to be space-efficient in general. These were followed by scheduling techniques that provide proven space bounds for parallel programs <ref> [6, 8, 9] </ref>. If S 1 is the space required by the serial execution, these techniques generate executions for a multithreaded computation on p processors that require no more than p S 1 space. <p> Assuming that F (B,i,j) does not allocate any space, the serial execution requires O (n) space, since the space for array B is reused for each i-iteration. Now consider the parallel implementation of this function on p processors, where p &lt; n. Previous scheduling systems <ref> [6, 8, 9, 19, 23, 28, 32] </ref>, which include both heuristic-based and provably space-efficient techniques, would schedule the outer level of parallelism first. This results in all the p processors executing one i-iteration each, and hence the total space allocated is O (p n). <p> Cilk uses less memory than this estimate due to its use of randomization: an idle processor steals the topmost thread (representing the outermost parallelism) from the private queue of a randomly picked processor; this thread may not represent the outermost parallelism in the entire computation. Previous techniques like <ref> [8, 9, 19] </ref> use a strategy similar to that of Cilk. The results show that when big allocations are delayed with dummy nodes, our algorithm results in a significantly lower memory usage in all the programs.
Reference: [10] <author> F. W. Burton and M. R. Sleep. </author> <title> Executing functional programs on a virtual tree of processors. </title> <booktitle> In Conference on Functional Programming Languages and Computer Architecture, </booktitle> <month> October </month> <year> 1981. </year>
Reference-contexts: Many researchers have addressed this problem in the past. Early attempts to reduce the memory usage of parallel computations were based on heuristics that limited the parallelism <ref> [10, 15, 28, 32] </ref>, and are not guaranteed to be space-efficient in general. These were followed by scheduling techniques that provide proven space bounds for parallel programs [6, 8, 9].
Reference: [11] <author> Rohit Chandra, Anoop Gupta, and John Hennessy. </author> <title> COOL: An object-based language for parallel programming. </title> <journal> IEEE Computer, </journal> <volume> 27(8):1326, </volume> <month> August </month> <year> 1994. </year>
Reference-contexts: If the scheduling is done at runtime, then the performance of the high-level code relies heavily on the scheduling algorithm, which should have low scheduling overheads and good load balancing. Several systems providing dynamic parallelism have been implemented with efficient runtime sched-ulers <ref> [7, 11, 13, 18, 19, 22, 23, 29, 30, 31] </ref>, resulting in good parallel performance. However, in addition to good time performance, the memory requirements of the parallel computation must be taken into consideration.
Reference: [12] <author> K. M. Chandy and C. Kesselman. </author> <title> Compositional c++: Compositional parallel programming. </title> <booktitle> In Proc. 5th. Intl. Wkshp. on Languages and Compilers for Parallel Computing, </booktitle> <pages> pages 124144, </pages> <address> New Haven, CT, </address> <month> August </month> <year> 1992. </year>
Reference-contexts: 1 Introduction Many of today's high level parallel programming languages provide constructs to express dynamic, fine-grained parallelism. Such languages include data-parallel languages such as NESL [3] and HPF [21], as well as control-parallel languages such as ID [1], Cilk [5], CC++ <ref> [12] </ref>, Sisal [17], and Proteus [26]. These languages allow the user to expose all the parallelism in the program, which is typically of a much higher degree than the number of processors. The language implementation is responsible for scheduling this parallelism onto the processors.
Reference: [13] <author> J. S. Chase, F. G. Amador, and E. D. Lazowska. </author> <title> The amber system: Parallel programming on a network of multiprocessors. </title> <booktitle> In Proceedings of the Twelfth ACM Symposium on Operating Systems Principles, </booktitle> <month> December </month> <year> 1989. </year>
Reference-contexts: If the scheduling is done at runtime, then the performance of the high-level code relies heavily on the scheduling algorithm, which should have low scheduling overheads and good load balancing. Several systems providing dynamic parallelism have been implemented with efficient runtime sched-ulers <ref> [7, 11, 13, 18, 19, 22, 23, 29, 30, 31] </ref>, resulting in good parallel performance. However, in addition to good time performance, the memory requirements of the parallel computation must be taken into consideration.
Reference: [14] <author> J. H. Chow and W. L. Harrison III. Switch-stacks: </author> <title> A scheme for microtasking nested parallel loops. </title> <booktitle> In Proceedings of Supercomputing, pages 190199, </booktitle> <address> New York, NY, </address> <month> November </month> <year> 1990. </year>
Reference-contexts: On the other hand, scheduling the outer parallelism would allocate space for the p branches at the top level, and then execute each subtree serially. Hence we use our algorithm without the delay to estimate the memory requirements of previous techniques like <ref> [14, 23] </ref>, which schedule the outer parallelism with higher priority.
Reference: [15] <author> D. E. Culler and Arvind. </author> <title> Resource requirements of dataflow programs. </title> <booktitle> In Proceedings of the 15th Annual International Symposium on Computer Architecture, </booktitle> <month> May </month> <year> 1988. </year>
Reference-contexts: However, in addition to good time performance, the memory requirements of the parallel computation must be taken into consideration. In an attempt to expose a sufficient degree of parallelism to keep all processors busy, schedulers often create many more parallel threads than necessary, leading to excessive memory usage <ref> [15, 28, 32] </ref>. <p> Many researchers have addressed this problem in the past. Early attempts to reduce the memory usage of parallel computations were based on heuristics that limited the parallelism <ref> [10, 15, 28, 32] </ref>, and are not guaranteed to be space-efficient in general. These were followed by scheduling techniques that provide proven space bounds for parallel programs [6, 8, 9].
Reference: [16] <author> E.G.Coffman, Jr., </author> <title> editor. Computer and job-shop scheduling theory. </title> <publisher> John Wiley & Sons, </publisher> <address> New York, </address> <year> 1976. </year>
Reference-contexts: This definition is similar to the class of list schedules described in <ref> [16] </ref>. We call a prioritized schedule that is greedy a greedy prioritized schedule. Let S 1 = (fv 1 g; fv 2 g; : : : ; fv n g) be a 1-schedule for a task graph with n tasks.
Reference: [17] <author> John T. Feo, David C. Cann, and Rodney R. Oldehoeft. </author> <title> A report on the Sisal language project. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 10(4):349366, </volume> <month> December </month> <year> 1990. </year>
Reference-contexts: 1 Introduction Many of today's high level parallel programming languages provide constructs to express dynamic, fine-grained parallelism. Such languages include data-parallel languages such as NESL [3] and HPF [21], as well as control-parallel languages such as ID [1], Cilk [5], CC++ [12], Sisal <ref> [17] </ref>, and Proteus [26]. These languages allow the user to expose all the parallelism in the program, which is typically of a much higher degree than the number of processors. The language implementation is responsible for scheduling this parallelism onto the processors.
Reference: [18] <author> V. W. Freeh, D. K. Lowenthal, and G. R. Andrews. </author> <title> Distributed filaments: efficient fine-grain parallelism on a cluster of workstations. </title> <booktitle> In First Symposium on Operating Systems Design and Implementation, pages 201212, </booktitle> <address> Monterey, CA, </address> <month> November </month> <year> 1994. </year>
Reference-contexts: If the scheduling is done at runtime, then the performance of the high-level code relies heavily on the scheduling algorithm, which should have low scheduling overheads and good load balancing. Several systems providing dynamic parallelism have been implemented with efficient runtime sched-ulers <ref> [7, 11, 13, 18, 19, 22, 23, 29, 30, 31] </ref>, resulting in good parallel performance. However, in addition to good time performance, the memory requirements of the parallel computation must be taken into consideration.
Reference: [19] <author> S. C. Goldstein, D. E. Culler, and K. E. Schauser. </author> <title> Enabling primitives for compiling parallel languages. </title> <booktitle> In Third Workshop on Languages, Compilers, and Run-Time Systems for Scalable Computers, </booktitle> <address> Rochester, NY, </address> <month> May </month> <year> 1995. </year> <month> 30 </month>
Reference-contexts: If the scheduling is done at runtime, then the performance of the high-level code relies heavily on the scheduling algorithm, which should have low scheduling overheads and good load balancing. Several systems providing dynamic parallelism have been implemented with efficient runtime sched-ulers <ref> [7, 11, 13, 18, 19, 22, 23, 29, 30, 31] </ref>, resulting in good parallel performance. However, in addition to good time performance, the memory requirements of the parallel computation must be taken into consideration. <p> Assuming that F (B,i,j) does not allocate any space, the serial execution requires O (n) space, since the space for array B is reused for each i-iteration. Now consider the parallel implementation of this function on p processors, where p &lt; n. Previous scheduling systems <ref> [6, 8, 9, 19, 23, 28, 32] </ref>, which include both heuristic-based and provably space-efficient techniques, would schedule the outer level of parallelism first. This results in all the p processors executing one i-iteration each, and hence the total space allocated is O (p n). <p> Cilk uses less memory than this estimate due to its use of randomization: an idle processor steals the topmost thread (representing the outermost parallelism) from the private queue of a randomly picked processor; this thread may not represent the outermost parallelism in the entire computation. Previous techniques like <ref> [8, 9, 19] </ref> use a strategy similar to that of Cilk. The results show that when big allocations are delayed with dummy nodes, our algorithm results in a significantly lower memory usage in all the programs.
Reference: [20] <author> L. Greengard. </author> <title> The rapid evaluation of potential fields in particle systems. </title> <publisher> The MIT Press, </publisher> <year> 1987. </year>
Reference-contexts: The high-level program is broken into such functions at points where it executes a parallel fork, a recursive call, or a memory allocation. We implemented five parallel programs on our runtime system: blocked recursive matrix multiplication, Strassen's matrix multiplication [33], the Fast Multipole Method for the n-body problem <ref> [20] </ref>, sparse matrix vector multiplication and the ID3 algorithm for building decision trees [27]. The implementation of these programs is described in appendix A, along with the problem sizes we used in our experiments. 7.1 Time performance The above programs were executed on an 8-processor Power Challenge.
Reference: [21] <author> High Performance Fortran Forum. </author> <title> High Performance Fortran language specification, </title> <month> May </month> <year> 1993. </year>
Reference-contexts: 1 Introduction Many of today's high level parallel programming languages provide constructs to express dynamic, fine-grained parallelism. Such languages include data-parallel languages such as NESL [3] and HPF <ref> [21] </ref>, as well as control-parallel languages such as ID [1], Cilk [5], CC++ [12], Sisal [17], and Proteus [26]. These languages allow the user to expose all the parallelism in the program, which is typically of a much higher degree than the number of processors.
Reference: [22] <author> W. E. Hseih, P. Wang, and W. E. Weihl. </author> <title> Computation migration: enhancing locality for distributed memory parallel systems. </title> <booktitle> In Proceedings of the Fourth ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming, </booktitle> <address> San Francisco, California, </address> <month> May </month> <year> 1993. </year>
Reference-contexts: If the scheduling is done at runtime, then the performance of the high-level code relies heavily on the scheduling algorithm, which should have low scheduling overheads and good load balancing. Several systems providing dynamic parallelism have been implemented with efficient runtime sched-ulers <ref> [7, 11, 13, 18, 19, 22, 23, 29, 30, 31] </ref>, resulting in good parallel performance. However, in addition to good time performance, the memory requirements of the parallel computation must be taken into consideration.
Reference: [23] <author> S. F. Hummel and E. Schonberg. </author> <title> Low-overhead scheduling of nested parallelsim. </title> <journal> IBM Journal of Research and Development, </journal> <volume> 35(5-6):74365, </volume> <year> 1991. </year>
Reference-contexts: If the scheduling is done at runtime, then the performance of the high-level code relies heavily on the scheduling algorithm, which should have low scheduling overheads and good load balancing. Several systems providing dynamic parallelism have been implemented with efficient runtime sched-ulers <ref> [7, 11, 13, 18, 19, 22, 23, 29, 30, 31] </ref>, resulting in good parallel performance. However, in addition to good time performance, the memory requirements of the parallel computation must be taken into consideration. <p> Assuming that F (B,i,j) does not allocate any space, the serial execution requires O (n) space, since the space for array B is reused for each i-iteration. Now consider the parallel implementation of this function on p processors, where p &lt; n. Previous scheduling systems <ref> [6, 8, 9, 19, 23, 28, 32] </ref>, which include both heuristic-based and provably space-efficient techniques, would schedule the outer level of parallelism first. This results in all the p processors executing one i-iteration each, and hence the total space allocated is O (p n). <p> On the other hand, scheduling the outer parallelism would allocate space for the p branches at the top level, and then execute each subtree serially. Hence we use our algorithm without the delay to estimate the memory requirements of previous techniques like <ref> [14, 23] </ref>, which schedule the outer parallelism with higher priority.
Reference: [24] <author> S. F. Hummel, E. Schonberg, and L. E. Flynn. </author> <title> Factoring: a method for scheduling parallel loops. </title> <journal> Communications of the ACM, </journal> <volume> 35(8):90101, </volume> <month> Aug </month> <year> 1992. </year>
Reference-contexts: We are currently considering methods to further improve the scheduling algorithm, particularly to provide better support for fine-grained computations. At present, fine-grained iterations of innermost loops are statically grouped into fixed-size chunks. A dynamic, decreasing-size chunking scheme such as <ref> [24, 25, 34] </ref> can be used instead. We are considering ways of automatically introducing such coarsening at runtime through the scheduling algorithm; for example, by allowing the execution order to differ to a limited extent from the order dictated by the 1DF-numbers.
Reference: [25] <author> C. D. Polychronopoulos; D.J. Kuck. </author> <title> Guided self-scheduling: a practical scheduling scheme for parallel supercomputers. </title> <journal> IEEE Transactions on Computers, </journal> <volume> C-36(12):142539, </volume> <month> Dec </month> <year> 1987. </year>
Reference-contexts: We are currently considering methods to further improve the scheduling algorithm, particularly to provide better support for fine-grained computations. At present, fine-grained iterations of innermost loops are statically grouped into fixed-size chunks. A dynamic, decreasing-size chunking scheme such as <ref> [24, 25, 34] </ref> can be used instead. We are considering ways of automatically introducing such coarsening at runtime through the scheduling algorithm; for example, by allowing the execution order to differ to a limited extent from the order dictated by the 1DF-numbers.
Reference: [26] <author> Peter H. Mills, Lars S. Nyland, Jan F. Prins, John H. Reif, and Robert A. Wagner. </author> <title> Prototyping parallel and distributed programs in Proteus. </title> <type> Technical Report UNC-CH TR90-041, </type> <institution> Computer Science Department, University of North Carolina, </institution> <year> 1990. </year>
Reference-contexts: 1 Introduction Many of today's high level parallel programming languages provide constructs to express dynamic, fine-grained parallelism. Such languages include data-parallel languages such as NESL [3] and HPF [21], as well as control-parallel languages such as ID [1], Cilk [5], CC++ [12], Sisal [17], and Proteus <ref> [26] </ref>. These languages allow the user to expose all the parallelism in the program, which is typically of a much higher degree than the number of processors. The language implementation is responsible for scheduling this parallelism onto the processors.
Reference: [27] <author> J. R. Quinlan. </author> <title> Induction of decision trees. </title> <booktitle> Machine learning, </booktitle> <address> 1(1):81106, </address> <year> 1986. </year>
Reference-contexts: We implemented five parallel programs on our runtime system: blocked recursive matrix multiplication, Strassen's matrix multiplication [33], the Fast Multipole Method for the n-body problem [20], sparse matrix vector multiplication and the ID3 algorithm for building decision trees <ref> [27] </ref>. The implementation of these programs is described in appendix A, along with the problem sizes we used in our experiments. 7.1 Time performance The above programs were executed on an 8-processor Power Challenge.
Reference: [28] <author> Jr. R. H. Halstead. </author> <title> Multilisp: A language for concurrent symbolic computation. </title> <journal> ACM Trans. on Programming Languages and Systems, </journal> <volume> 7(4):501538, </volume> <year> 1985. </year>
Reference-contexts: However, in addition to good time performance, the memory requirements of the parallel computation must be taken into consideration. In an attempt to expose a sufficient degree of parallelism to keep all processors busy, schedulers often create many more parallel threads than necessary, leading to excessive memory usage <ref> [15, 28, 32] </ref>. <p> Many researchers have addressed this problem in the past. Early attempts to reduce the memory usage of parallel computations were based on heuristics that limited the parallelism <ref> [10, 15, 28, 32] </ref>, and are not guaranteed to be space-efficient in general. These were followed by scheduling techniques that provide proven space bounds for parallel programs [6, 8, 9]. <p> Assuming that F (B,i,j) does not allocate any space, the serial execution requires O (n) space, since the space for array B is reused for each i-iteration. Now consider the parallel implementation of this function on p processors, where p &lt; n. Previous scheduling systems <ref> [6, 8, 9, 19, 23, 28, 32] </ref>, which include both heuristic-based and provably space-efficient techniques, would schedule the outer level of parallelism first. This results in all the p processors executing one i-iteration each, and hence the total space allocated is O (p n).
Reference: [29] <author> M. C. Rinard, D. J. Scales, and M. S. Lam. </author> <title> Jade: A high-level, machine-independent language for parallel programming. </title> <booktitle> IEEE Computer, </booktitle> <month> June </month> <year> 1993. </year>
Reference-contexts: If the scheduling is done at runtime, then the performance of the high-level code relies heavily on the scheduling algorithm, which should have low scheduling overheads and good load balancing. Several systems providing dynamic parallelism have been implemented with efficient runtime sched-ulers <ref> [7, 11, 13, 18, 19, 22, 23, 29, 30, 31] </ref>, resulting in good parallel performance. However, in addition to good time performance, the memory requirements of the parallel computation must be taken into consideration.
Reference: [30] <author> A. Rogers, M. Carlisle, J. Reppy, and L. Hendren. </author> <title> Supporting dynamic data structures on distributed memory machines. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 17(2):233263, </volume> <month> March </month> <year> 1995. </year>
Reference-contexts: If the scheduling is done at runtime, then the performance of the high-level code relies heavily on the scheduling algorithm, which should have low scheduling overheads and good load balancing. Several systems providing dynamic parallelism have been implemented with efficient runtime sched-ulers <ref> [7, 11, 13, 18, 19, 22, 23, 29, 30, 31] </ref>, resulting in good parallel performance. However, in addition to good time performance, the memory requirements of the parallel computation must be taken into consideration.
Reference: [31] <author> R.S.Nikhil. Cid: </author> <title> A parallel, shared-memory c for distributed memory machines. </title> <booktitle> In Proc. 7th. Ann. Wkshp. on Languages and Compilers for Parallel Computing, </booktitle> <pages> pages 376390, </pages> <month> August </month> <year> 1994. </year>
Reference-contexts: If the scheduling is done at runtime, then the performance of the high-level code relies heavily on the scheduling algorithm, which should have low scheduling overheads and good load balancing. Several systems providing dynamic parallelism have been implemented with efficient runtime sched-ulers <ref> [7, 11, 13, 18, 19, 22, 23, 29, 30, 31] </ref>, resulting in good parallel performance. However, in addition to good time performance, the memory requirements of the parallel computation must be taken into consideration.
Reference: [32] <author> C. A. Rugguero and J. Sargeant. </author> <title> Control of parallelism in the manchester dataflow machine. </title> <booktitle> In Functional Programming Languages and Computer Architecture, volume 174 of Lecture Notes in Computer Science, </booktitle> <pages> pages 115. </pages> <publisher> Springer-Verlag, </publisher> <year> 1987. </year>
Reference-contexts: However, in addition to good time performance, the memory requirements of the parallel computation must be taken into consideration. In an attempt to expose a sufficient degree of parallelism to keep all processors busy, schedulers often create many more parallel threads than necessary, leading to excessive memory usage <ref> [15, 28, 32] </ref>. <p> Many researchers have addressed this problem in the past. Early attempts to reduce the memory usage of parallel computations were based on heuristics that limited the parallelism <ref> [10, 15, 28, 32] </ref>, and are not guaranteed to be space-efficient in general. These were followed by scheduling techniques that provide proven space bounds for parallel programs [6, 8, 9]. <p> Assuming that F (B,i,j) does not allocate any space, the serial execution requires O (n) space, since the space for array B is reused for each i-iteration. Now consider the parallel implementation of this function on p processors, where p &lt; n. Previous scheduling systems <ref> [6, 8, 9, 19, 23, 28, 32] </ref>, which include both heuristic-based and provably space-efficient techniques, would schedule the outer level of parallelism first. This results in all the p processors executing one i-iteration each, and hence the total space allocated is O (p n).
Reference: [33] <author> V. Strassen. </author> <title> Gaussian elimination is not optimal. Numerische Mathematik, </title> <address> 13:354356, </address> <year> 1969. </year>
Reference-contexts: The high-level program is broken into such functions at points where it executes a parallel fork, a recursive call, or a memory allocation. We implemented five parallel programs on our runtime system: blocked recursive matrix multiplication, Strassen's matrix multiplication <ref> [33] </ref>, the Fast Multipole Method for the n-body problem [20], sparse matrix vector multiplication and the ID3 algorithm for building decision trees [27].
Reference: [34] <author> T. H. Tzen and L. M. Ni. </author> <title> Trapezoid self-scheduling: a practical scheduling scheme for parallel compilers. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 4(1):8798, </volume> <month> Jan </month> <year> 1993. </year> <month> 31 </month>
Reference-contexts: We are currently considering methods to further improve the scheduling algorithm, particularly to provide better support for fine-grained computations. At present, fine-grained iterations of innermost loops are statically grouped into fixed-size chunks. A dynamic, decreasing-size chunking scheme such as <ref> [24, 25, 34] </ref> can be used instead. We are considering ways of automatically introducing such coarsening at runtime through the scheduling algorithm; for example, by allowing the execution order to differ to a limited extent from the order dictated by the 1DF-numbers.
References-found: 34

