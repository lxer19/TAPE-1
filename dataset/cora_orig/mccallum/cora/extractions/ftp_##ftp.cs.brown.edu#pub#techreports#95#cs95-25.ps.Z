URL: ftp://ftp.cs.brown.edu/pub/techreports/95/cs95-25.ps.Z
Refering-URL: http://www.cs.brown.edu/publications/techreports/reports/CS-95-25.html
Root-URL: http://www.cs.brown.edu/
Abstract-found: 0
Intro-found: 1
Reference: [1] <author> F. Benhamou, D. McAllester, and P. Van Hentenryck. </author> <title> CLP(Intervals) Revisited. </title> <booktitle> In Proceedings of the International Symposium on Logic Programming (ILPS-94), </booktitle> <pages> pages 124-138, </pages> <address> Ithaca, NY, </address> <month> November </month> <year> 1994. </year>
Reference-contexts: Newton associates an interval with each variable and it is the role of constraints to narrow down these intervals to the required precision. 1 Consider now the problem of finding a solution to the equation x 4 + x 2 = 1 in the interval <ref> [0; 1] </ref>. The Newton program simpleRoot (X) :- constraint [ X in [0,1] , X^4 + X^2 = 1 ]. returns the solution X in [0.78615137475949814493,0.78615138123592631648]. when queried with ?- simpleRoot (X). The same problem in the interval [1; 1] has two solutions. <p> The Newton program simpleRoot (X) :- constraint [ X in [0,1] , X^4 + X^2 = 1 ]. returns the solution X in [0.78615137475949814493,0.78615138123592631648]. when queried with ?- simpleRoot (X). The same problem in the interval <ref> [1; 1] </ref> has two solutions. The Newton program which finds them both is as follows: simpleRoots (X) :- constraint [ X in [-1,1] , X^4 + X^2 = 1 ], split (X). <p> For instance, the interval function is the most precise interval extension of addition (i.e., it returns the smallest possible interval containing all real results) while a function always returning <ref> [1; 1] </ref> would be the least accurate. In the following, we assume fixed interval extensions for the basic real operators +; ; fi and exponentiation (for instance, the interval extension of + is defined by ) and the basic real relations =; . <p> For simplicity of exposition, we restrict attention to equations. It is straightforward to generalize our results to inequalities. For convenience, we also assume in the rest of this section that all constraints are defined over variables x 1 ; : : :; x n . 3.2.3 Box Consistency Box-consistency <ref> [1] </ref> is an approximation of arc-consistency, a notion well-known in artificial intelligence [18] which states a simple local condition on a constraint c and the set of possible values for each of its variables, say D 1 ; : : : ; D n . <p> Example 3 Let c be the constraint x 2 1 + x 2 2 = 1. c is arc-consistent wrt h <ref> [1; 1] </ref> ; [1; 1] i but is not arc-consistent wrt h [1; 1] ; [2; 2] i since, for instance, there is no value r 1 for x 1 in [1; 1] such that r 2 1 + 2 2 = 1. <p> Example 3 Let c be the constraint x 2 1 + x 2 2 = 1. c is arc-consistent wrt h <ref> [1; 1] </ref> ; [1; 1] i but is not arc-consistent wrt h [1; 1] ; [2; 2] i since, for instance, there is no value r 1 for x 1 in [1; 1] such that r 2 1 + 2 2 = 1. <p> Example 3 Let c be the constraint x 2 1 + x 2 2 = 1. c is arc-consistent wrt h <ref> [1; 1] </ref> ; [1; 1] i but is not arc-consistent wrt h [1; 1] ; [2; 2] i since, for instance, there is no value r 1 for x 1 in [1; 1] such that r 2 1 + 2 2 = 1. <p> Let c be the constraint x 2 1 + x 2 2 = 1. c is arc-consistent wrt h <ref> [1; 1] </ref> ; [1; 1] i but is not arc-consistent wrt h [1; 1] ; [2; 2] i since, for instance, there is no value r 1 for x 1 in [1; 1] such that r 2 1 + 2 2 = 1. <p> Moreover, decomposing complex constraints into simple constraints entails a substantial loss in pruning, making this approach unpractical on many applications. See <ref> [1] </ref> for experimental results on this approach and their comparison with the approach presented in this paper. The notion of box-consistency introduced in [1] is a coarser approximation of arc-consistency which provides a much better trade-off between efficiency and pruning. <p> Moreover, decomposing complex constraints into simple constraints entails a substantial loss in pruning, making this approach unpractical on many applications. See <ref> [1] </ref> for experimental results on this approach and their comparison with the approach presented in this paper. The notion of box-consistency introduced in [1] is a coarser approximation of arc-consistency which provides a much better trade-off between efficiency and pruning. It consists in replacing the existential quantification in the above condition by the evaluation of an interval extension of the constraint on the intervals of the existential variables. <p> The difference between arc-consistency and box-consistency appears essentially when there are multiple occurrences of the same variable. 13 Example 4 Consider the constraint x 1 + x 2 x 1 = 0. The constraint is not arc-consistent wrt h <ref> [1; 1] </ref>; [1; 1]i since there is no value r 1 for x 1 which satisfies r 1 + 1 r 1 = 0. On the other hand, the interval constraint X 1 +X 2 X 1 = 0 is box-consistent, since ([1; 1]+[1; 1 + ][1; 1])"[0; 0] and ([1; <p> The difference between arc-consistency and box-consistency appears essentially when there are multiple occurrences of the same variable. 13 Example 4 Consider the constraint x 1 + x 2 x 1 = 0. The constraint is not arc-consistent wrt h <ref> [1; 1] </ref>; [1; 1]i since there is no value r 1 for x 1 which satisfies r 1 + 1 r 1 = 0. On the other hand, the interval constraint X 1 +X 2 X 1 = 0 is box-consistent, since ([1; 1]+[1; 1 + ][1; 1])"[0; 0] and ([1; 1] + <p> On the other hand, the interval constraint X 1 +X 2 X 1 = 0 is box-consistent, since ([1; 1]+[1; 1 + ][1; 1])"[0; 0] and ([1; 1] + <ref> [1 ; 1] </ref> [1; 1]) " [0; 0] are non-empty. 3.2.4 Interval Extensions for Box Consistency Box-consistency strongly depends on the interval extensions chosen for the constraints and different interval extensions can produce very different (often incomparable) tradeoffs between pruning and computational complexity. <p> On the other hand, the interval constraint X 1 +X 2 X 1 = 0 is box-consistent, since (<ref> [1; 1] </ref>+[1; 1 + ][1; 1])"[0; 0] and ([1; 1] + [1 ; 1] [1; 1]) " [0; 0] are non-empty. 3.2.4 Interval Extensions for Box Consistency Box-consistency strongly depends on the interval extensions chosen for the constraints and different interval extensions can produce very different (often incomparable) tradeoffs between pruning and computational complexity. <p> = x 14 0:46588640 0:21790395 x 13 x 3 x 10 0 = x 16 0:26516898 0:21037773 x 4 x 19 x 9 0 = x 18 0:56003141 0:18114505 x 6 x 13 x 8 0 = x 20 0:57001682 0:17949149 x 1 x 3 x 11 with initial intervals <ref> [1; 2] </ref>. Benchmark i3 has the same set of equations as i2 but has initial intervals [2; 2]. <p> 0:44166728 0:19950920 x 2 6 x 2 0 = x 2 8 x 2 10 7 0:42937161 0:21180486 x 2 5 x 2 0 = x 2 1 x 2 6 9 0:34504906 0:19612740 x 2 6 x 2 0 = x 2 4 x 2 1 and initial intervals <ref> [1; 1] </ref>. The number of solutions must be a multiple of 1024. <p> x 3 3 + x 4 3 8 x 3 10 + x 4 10 2 x 3 8 + x 4 8 1 x 3 6 + x 4 6 10 x 3 8 + x 4 8 4 x 3 1 + x 4 1 and initial intervals <ref> [1; 1] </ref>. Kinematics Applications Application kin1 comes from robotics and describes the inverse kinematics of an elbow manipulator [10]. <p> [10; 20] 6.69 267 Schwefel3 2 [10 7 ; 10 7 ] 0.03 0 Rosenbrock 2 [10 7 ; 10 7 ] 0.33 10 Ratz1 5 [500; 600] 1.19 0 Ratz25 4 [0; 10] 2.86 0 Ratz27 4 [0; 10] 4.44 0 Ratz210 4 [0; 10] 7.42 0 Ratz3 6 <ref> [0; 1] </ref> 9.13 2 More1 3 [4; 4] 10.04 5 More2 4 [25; 25] 189.56 32 Table 3: Summary of the Experimental Results on Unconstrained Optimization. 25 Levy1 is the function f (x) = x 6 15x 4 + 27x 2 + 250: Levy2 is the function f (x) = i=1 <p> Experimental results on numerous benchmarks from numerical analysis, and comparison with other tools, show the effectiveness of Newton. Acknowledgments The equation-solving algorithm was developed jointly with D. McAllester and D. Kapur [37]. The formalization of box-consistency was developed jointly with F. Benhamou <ref> [1] </ref>. Special thanks to A. Colmerauer and B. Le Charlier for several helpful comments and suggestions, and to P. Codognet for his invitation to CCP'95.
Reference: [2] <author> F. Benhamou and W. </author> <title> Older. Applying Interval Arithmetic to Real, Integer and Boolean Constraints. </title> <journal> Journal of Logic Programming, </journal> <note> 1994. To appear. </note>
Reference-contexts: Example 3 Let c be the constraint x 2 1 + x 2 2 = 1. c is arc-consistent wrt h [1; 1] ; [1; 1] i but is not arc-consistent wrt h [1; 1] ; <ref> [2; 2] </ref> i since, for instance, there is no value r 1 for x 1 in [1; 1] such that r 2 1 + 2 2 = 1. <p> = x 4 0:19807914 0:15585316 x 7 x 1 x 6 0 = x 6 0:14654113 0:18922793 x 8 x 5 x 10 0 = x 8 0:07056438 0:17081208 x 1 x 7 x 6 0 = x 10 0:42651102 0:21466544 x 4 x 8 x 1 with initial intervals <ref> [2; 2] </ref>. <p> = x 14 0:46588640 0:21790395 x 13 x 3 x 10 0 = x 16 0:26516898 0:21037773 x 4 x 19 x 9 0 = x 18 0:56003141 0:18114505 x 6 x 13 x 8 0 = x 20 0:57001682 0:17949149 x 1 x 3 x 11 with initial intervals <ref> [1; 2] </ref>. Benchmark i3 has the same set of equations as i2 but has initial intervals [2; 2]. <p> Benchmark i3 has the same set of equations as i2 but has initial intervals <ref> [2; 2] </ref>. <p> These ideas have been developed and made popular by the CLP system BNR-Prolog [30] and generalized to constraint solving over discrete quantities in its successor CLP (BNR) <ref> [28, 2] </ref>. Many other systems (e.g [16, 36]) have been developed on similar principles.
Reference: [3] <author> F. Benhamou and W. </author> <title> Older. Applying Interval Arithmetic to Real, Integer and Boolean Constraints. </title> <journal> Journal of Logic Programming, </journal> <note> 1995. To appear. </note>
Reference-contexts: j 9r 1 2 I 1 ; : : : ; 9r i1 2 I i1 ; : : : ; 9r i+1 2 I i+1 ; 9r n 2 I n : c (r 1 ; : : :; r n ) gg: This condition, used in systems like <ref> [29, 3] </ref>, is easily enforced on simple constraints such as x 1 = x 2 + x 3 ; x 1 = x 2 x 3 ; x 1 = x 2 fi x 3 but it is also computationally very expensive for complex constraints with multiple occurrences of the same
Reference: [4] <author> J.G Cleary. </author> <title> Logical Arithmetic. </title> <journal> Future Generation Computing Systems, </journal> <volume> 2(2) </volume> <pages> 125-149, </pages> <year> 1987. </year>
Reference-contexts: These functions come from the discretization of a nonlinear integral equation, giving a constraint system denser than the sparse constraint system for the Broyden banded functions. The variables x i were given initial domains <ref> [4; 5] </ref> as in [32]. 5 Some interval methods such as [7] are more sophisticated than HRB but the sophistication aims at speeding up the computation near a solution. <p> it to HRB is meaningful. 19 Benchmarks v d range Newton HRB CONT Broyden 10 3 10 [-1,1] 1.65 18.23 Broyden 20 3 20 [-1,1] 4.25 ? Broyden 320 3 320 [-1,1] 113.71 ? Broyden 320 3 320 [10 8 ; 10 8 ] 143.40 ? More-Cosnard 20 3 20 <ref> [4; 5] </ref> 24.49 968.25 More-Cosnard 40 3 40 [4; 5] 192.81 ? More-Cosnard 80 3 80 [4; 5] 1752.64 ? More-Cosnard 80 3 80 [10 8 ; 0] 1735.09 ? i1 10 3 10 [-2,2] 0.06 14.28 i3 20 3 20 [-2,2] 0.31 5640.80 i5 10 11 10 [-1,1] 0.08 33.58 <p> d range Newton HRB CONT Broyden 10 3 10 [-1,1] 1.65 18.23 Broyden 20 3 20 [-1,1] 4.25 ? Broyden 320 3 320 [-1,1] 113.71 ? Broyden 320 3 320 [10 8 ; 10 8 ] 143.40 ? More-Cosnard 20 3 20 <ref> [4; 5] </ref> 24.49 968.25 More-Cosnard 40 3 40 [4; 5] 192.81 ? More-Cosnard 80 3 80 [4; 5] 1752.64 ? More-Cosnard 80 3 80 [10 8 ; 0] 1735.09 ? i1 10 3 10 [-2,2] 0.06 14.28 i3 20 3 20 [-2,2] 0.31 5640.80 i5 10 11 10 [-1,1] 0.08 33.58 kin2 8 256 [10 8 ; 10 8 <p> 10 [-1,1] 1.65 18.23 Broyden 20 3 20 [-1,1] 4.25 ? Broyden 320 3 320 [-1,1] 113.71 ? Broyden 320 3 320 [10 8 ; 10 8 ] 143.40 ? More-Cosnard 20 3 20 <ref> [4; 5] </ref> 24.49 968.25 More-Cosnard 40 3 40 [4; 5] 192.81 ? More-Cosnard 80 3 80 [4; 5] 1752.64 ? More-Cosnard 80 3 80 [10 8 ; 0] 1735.09 ? i1 10 3 10 [-2,2] 0.06 14.28 i3 20 3 20 [-2,2] 0.31 5640.80 i5 10 11 10 [-1,1] 0.08 33.58 kin2 8 256 [10 8 ; 10 8 ] 353.06 4730.34 35.61 eco 5 54 [10 <p> [10 7 ; 10 7 ] 0.03 0 Rosenbrock 2 [10 7 ; 10 7 ] 0.33 10 Ratz1 5 [500; 600] 1.19 0 Ratz25 4 [0; 10] 2.86 0 Ratz27 4 [0; 10] 4.44 0 Ratz210 4 [0; 10] 7.42 0 Ratz3 6 [0; 1] 9.13 2 More1 3 <ref> [4; 4] </ref> 10.04 5 More2 4 [25; 25] 189.56 32 Table 3: Summary of the Experimental Results on Unconstrained Optimization. 25 Levy1 is the function f (x) = x 6 15x 4 + 27x 2 + 250: Levy2 is the function f (x) = i=1 Levy3 is the function f (x <p> 2 + 5 fl x 2 7 fl (x 8 11) 2 + 2 fl (x 9 10) 2 + (x 1 0 7) 2 + 45 200 31 5 Related Work The introduction of a relational form of interval arithmetic in logic programming has been proposed by Cleary in <ref> [4] </ref>. These ideas have been developed and made popular by the CLP system BNR-Prolog [30] and generalized to constraint solving over discrete quantities in its successor CLP (BNR) [28, 2]. Many other systems (e.g [16, 36]) have been developed on similar principles.
Reference: [5] <author> R. Hammer, M. Hocks, M. Kulisch, and D. Ratz. </author> <title> Numerical Toolbox for Verified Computing I Basic Numerical Problems, Theory, Algorithms, and PASCAL-XSC Programs. </title> <publisher> Springer-Verlag, </publisher> <address> Heidelberg, </address> <year> 1993. </year>
Reference-contexts: These functions come from the discretization of a nonlinear integral equation, giving a constraint system denser than the sparse constraint system for the Broyden banded functions. The variables x i were given initial domains <ref> [4; 5] </ref> as in [32]. 5 Some interval methods such as [7] are more sophisticated than HRB but the sophistication aims at speeding up the computation near a solution. <p> it to HRB is meaningful. 19 Benchmarks v d range Newton HRB CONT Broyden 10 3 10 [-1,1] 1.65 18.23 Broyden 20 3 20 [-1,1] 4.25 ? Broyden 320 3 320 [-1,1] 113.71 ? Broyden 320 3 320 [10 8 ; 10 8 ] 143.40 ? More-Cosnard 20 3 20 <ref> [4; 5] </ref> 24.49 968.25 More-Cosnard 40 3 40 [4; 5] 192.81 ? More-Cosnard 80 3 80 [4; 5] 1752.64 ? More-Cosnard 80 3 80 [10 8 ; 0] 1735.09 ? i1 10 3 10 [-2,2] 0.06 14.28 i3 20 3 20 [-2,2] 0.31 5640.80 i5 10 11 10 [-1,1] 0.08 33.58 <p> d range Newton HRB CONT Broyden 10 3 10 [-1,1] 1.65 18.23 Broyden 20 3 20 [-1,1] 4.25 ? Broyden 320 3 320 [-1,1] 113.71 ? Broyden 320 3 320 [10 8 ; 10 8 ] 143.40 ? More-Cosnard 20 3 20 <ref> [4; 5] </ref> 24.49 968.25 More-Cosnard 40 3 40 [4; 5] 192.81 ? More-Cosnard 80 3 80 [4; 5] 1752.64 ? More-Cosnard 80 3 80 [10 8 ; 0] 1735.09 ? i1 10 3 10 [-2,2] 0.06 14.28 i3 20 3 20 [-2,2] 0.31 5640.80 i5 10 11 10 [-1,1] 0.08 33.58 kin2 8 256 [10 8 ; 10 8 <p> 10 [-1,1] 1.65 18.23 Broyden 20 3 20 [-1,1] 4.25 ? Broyden 320 3 320 [-1,1] 113.71 ? Broyden 320 3 320 [10 8 ; 10 8 ] 143.40 ? More-Cosnard 20 3 20 <ref> [4; 5] </ref> 24.49 968.25 More-Cosnard 40 3 40 [4; 5] 192.81 ? More-Cosnard 80 3 80 [4; 5] 1752.64 ? More-Cosnard 80 3 80 [10 8 ; 0] 1735.09 ? i1 10 3 10 [-2,2] 0.06 14.28 i3 20 3 20 [-2,2] 0.31 5640.80 i5 10 11 10 [-1,1] 0.08 33.58 kin2 8 256 [10 8 ; 10 8 ] 353.06 4730.34 35.61 eco 5 54 [10
Reference: [6] <author> E. Hansen. </author> <title> Global Optimization Using Interval Analysis. </title> <publisher> Marcel Dekker, </publisher> <address> New York, </address> <year> 1992. </year>
Reference: [7] <author> E.R. Hansen and R.I. Greenberg. </author> <title> An Interval Newton Method. </title> <journal> Appl. Math. Comput., </journal> <volume> 12 </volume> <pages> 89-98, </pages> <year> 1983. </year>
Reference-contexts: Newton is essentially linear on Broyden and quadratic on More-Cosnard. Broyden Banded Functions This is a traditional benchmark of interval techniques and was used for instance in <ref> [7] </ref>. It consists in finding the zeros of the functions f i (x 1 ; : : :; x n ) = x i (2 + 5x 2 P where J i = fj j j 6= i & max (1; i 5) j min (n; i + 1)g. <p> These functions come from the discretization of a nonlinear integral equation, giving a constraint system denser than the sparse constraint system for the Broyden banded functions. The variables x i were given initial domains [4; 5] as in [32]. 5 Some interval methods such as <ref> [7] </ref> are more sophisticated than HRB but the sophistication aims at speeding up the computation near a solution.
Reference: [8] <author> E.R. Hansen and S. Sengupta. </author> <title> Bounding Solutions of Systems of Equations Using Interval Analysis. </title> <journal> BIT, </journal> <volume> 21 </volume> <pages> 203-211, </pages> <year> 1981. </year>
Reference-contexts: Box-consistency is parametrized by an interval extension operator for the constraint and can be instantiated to produce various narrowing operators. In particular, box-consistency on the Taylor extension of the constraint produces a generalization of the Hansen-Segupta operator <ref> [8] </ref>, well-known in interval methods. In addition, box-consistency on the natural extension produces narrowing operators which are more effective when the algorithm is not near a solution. Newton has been applied to numerous applications. <p> It considers successively equation solving, unconstrained optimization, and constrained optimization. 4.1 Equation Solving This section reports experimental results of Newton on a variety of standard benchmarks for equation solving. The benchmarks were taken from papers on numerical analysis [24], interval analysis <ref> [8, 10, 23] </ref>, and continuation methods [38, 26, 25, 19]. We also compare Newton with a traditional interval method using the Hansen-Segupta's operator, range testing, and branching.
Reference: [9] <author> W. Hock and K. Schittkowski. </author> <title> Test Examples for Nonlinear Programming Codes. </title> <booktitle> Lecture Notes in Economics and Mathematical Systems. </booktitle> <publisher> Springer Verlag, </publisher> <year> 1981. </year> <month> 33 </month>
Reference-contexts: Note that Newton returns the global optimum, not a local optimum. Newton has also solved a variety of constrained optimization problems. A beautiful example taken from <ref> [9] </ref> is shown in Figure 3. Newton solves this problem in about 2 seconds. Newton has two primitives for constrained optimization, minof and safe minof. They differ on the way they compute the upper bound to the minimum value (and hence the optimum solutions). <p> Table 4 summarizes some of our computation results on some of the toughest problems from <ref> [9] </ref>. We give the number of variables in the initial statement (v), the number of constraints (c), the CPU time, and the number of splits.
Reference: [10] <author> H. Hong and V. Stahl. </author> <title> Safe Starting Regions by Fixed Points and Tightening. </title> <booktitle> Computing, </booktitle> <address> 53(3-4):323-335, </address> <year> 1994. </year>
Reference-contexts: A more interesting application is robot kinematics, where the goal is to find the angles for the joints of a robot arm so that the robot hand ends up in a specified position. A Newton program, solving a problem given in <ref> [10] </ref>, is depicted in Figure 1. Once again, the program follows the simple structure that we have encountered so far. It takes about 15 seconds to find all solutions on a SUN Sparc-10 workstation. Nonlinear constraint techniques are also of primary importance to solve chemical equilibrium systems. <p> A typical problem taken from [17] is given by the minimization of the function 2 Y 5 X i cos ((i + 1)x k + i): Restricting attention to the range <ref> [10; 10] </ref> for the variables, a simple Newton program to solve this problem is as follows: minimization (Min,Boxes) :- minof (cos (2*X1+1)+2*cos (3*X1+2)+3*cos (4*X1+3)+4*cos (5*X1+4)+5*cos (6*X1+5)) * in isin Min for Boxes. <p> It considers successively equation solving, unconstrained optimization, and constrained optimization. 4.1 Equation Solving This section reports experimental results of Newton on a variety of standard benchmarks for equation solving. The benchmarks were taken from papers on numerical analysis [24], interval analysis <ref> [8, 10, 23] </ref>, and continuation methods [38, 26, 25, 19]. We also compare Newton with a traditional interval method using the Hansen-Segupta's operator, range testing, and branching. <p> 29.88 5.87 eco 7 486 [10 8 ; 10 8 ] 127.65 ? 991.45 eco 9 4374 [10 8 ; 10 8 ] 8600.28 ? combustion 10 96 [10 8 ; 10 8 ] 9.94 ? 57.40 chemistry 5 108 [0; 10 8 ] 6.32 ? 56.55 neuro 6 1024 <ref> [10; 10] </ref> 0.91 28.84 5.02 neuro 6 1024 [1000; 1000] 172.71 ? 5.02 Table 1: Summary of the Experimental Results on Equation Solving. 20 Interval Arithmetic Benchmarks These are traditional benchmarks from interval arithmetic papers [22, 10]. <p> ? 57.40 chemistry 5 108 [0; 10 8 ] 6.32 ? 56.55 neuro 6 1024 [10; 10] 0.91 28.84 5.02 neuro 6 1024 [1000; 1000] 172.71 ? 5.02 Table 1: Summary of the Experimental Results on Equation Solving. 20 Interval Arithmetic Benchmarks These are traditional benchmarks from interval arithmetic papers <ref> [22, 10] </ref>. <p> Kinematics Applications Application kin1 comes from robotics and describes the inverse kinematics of an elbow manipulator <ref> [10] </ref>. <p> Hump is the three-hump camel function f (x 1 ; x 2 ) = 12x 2 1 + x 6 24 Benchmarks v Range Time Splits Hump 2 [10 7 ; 10 8 ] 0.17 3 Levy1 1 [10 7 ; 10 7 ] 0.09 2 Levy2 1 <ref> [10; 10] </ref> 0.61 4 Levy3 2 [10; 10] 16.14 30 Levy4 2 [10; 10] 2.13 7 Levy5 3 [10; 10] 0.75 1 Levy5 5 [10; 10] 1.45 1 Levy5 10 [10; 10] 4.35 1 Levy5 20 [10; 10] 15.27 1 Levy5 40 [10; 10] 59.08 1 Levy5 80 [10; 10] 235.22 <p> camel function f (x 1 ; x 2 ) = 12x 2 1 + x 6 24 Benchmarks v Range Time Splits Hump 2 [10 7 ; 10 8 ] 0.17 3 Levy1 1 [10 7 ; 10 7 ] 0.09 2 Levy2 1 <ref> [10; 10] </ref> 0.61 4 Levy3 2 [10; 10] 16.14 30 Levy4 2 [10; 10] 2.13 7 Levy5 3 [10; 10] 0.75 1 Levy5 5 [10; 10] 1.45 1 Levy5 10 [10; 10] 4.35 1 Levy5 20 [10; 10] 15.27 1 Levy5 40 [10; 10] 59.08 1 Levy5 80 [10; 10] 235.22 1 Levy6 3 [10; 10] 0.96 <p> x 2 ) = 12x 2 1 + x 6 24 Benchmarks v Range Time Splits Hump 2 [10 7 ; 10 8 ] 0.17 3 Levy1 1 [10 7 ; 10 7 ] 0.09 2 Levy2 1 <ref> [10; 10] </ref> 0.61 4 Levy3 2 [10; 10] 16.14 30 Levy4 2 [10; 10] 2.13 7 Levy5 3 [10; 10] 0.75 1 Levy5 5 [10; 10] 1.45 1 Levy5 10 [10; 10] 4.35 1 Levy5 20 [10; 10] 15.27 1 Levy5 40 [10; 10] 59.08 1 Levy5 80 [10; 10] 235.22 1 Levy6 3 [10; 10] 0.96 1 Levy6 5 [10; 10] 1.57 <p> 1 + x 6 24 Benchmarks v Range Time Splits Hump 2 [10 7 ; 10 8 ] 0.17 3 Levy1 1 [10 7 ; 10 7 ] 0.09 2 Levy2 1 <ref> [10; 10] </ref> 0.61 4 Levy3 2 [10; 10] 16.14 30 Levy4 2 [10; 10] 2.13 7 Levy5 3 [10; 10] 0.75 1 Levy5 5 [10; 10] 1.45 1 Levy5 10 [10; 10] 4.35 1 Levy5 20 [10; 10] 15.27 1 Levy5 40 [10; 10] 59.08 1 Levy5 80 [10; 10] 235.22 1 Levy6 3 [10; 10] 0.96 1 Levy6 5 [10; 10] 1.57 1 Levy6 10 [10; 10] 4.29 <p> v Range Time Splits Hump 2 [10 7 ; 10 8 ] 0.17 3 Levy1 1 [10 7 ; 10 7 ] 0.09 2 Levy2 1 <ref> [10; 10] </ref> 0.61 4 Levy3 2 [10; 10] 16.14 30 Levy4 2 [10; 10] 2.13 7 Levy5 3 [10; 10] 0.75 1 Levy5 5 [10; 10] 1.45 1 Levy5 10 [10; 10] 4.35 1 Levy5 20 [10; 10] 15.27 1 Levy5 40 [10; 10] 59.08 1 Levy5 80 [10; 10] 235.22 1 Levy6 3 [10; 10] 0.96 1 Levy6 5 [10; 10] 1.57 1 Levy6 10 [10; 10] 4.29 1 Levy6 20 [10; 10] 14.86 <p> [10 7 ; 10 8 ] 0.17 3 Levy1 1 [10 7 ; 10 7 ] 0.09 2 Levy2 1 <ref> [10; 10] </ref> 0.61 4 Levy3 2 [10; 10] 16.14 30 Levy4 2 [10; 10] 2.13 7 Levy5 3 [10; 10] 0.75 1 Levy5 5 [10; 10] 1.45 1 Levy5 10 [10; 10] 4.35 1 Levy5 20 [10; 10] 15.27 1 Levy5 40 [10; 10] 59.08 1 Levy5 80 [10; 10] 235.22 1 Levy6 3 [10; 10] 0.96 1 Levy6 5 [10; 10] 1.57 1 Levy6 10 [10; 10] 4.29 1 Levy6 20 [10; 10] 14.86 1 Levy6 40 [10; 10] 64.11 <p> 0.17 3 Levy1 1 [10 7 ; 10 7 ] 0.09 2 Levy2 1 <ref> [10; 10] </ref> 0.61 4 Levy3 2 [10; 10] 16.14 30 Levy4 2 [10; 10] 2.13 7 Levy5 3 [10; 10] 0.75 1 Levy5 5 [10; 10] 1.45 1 Levy5 10 [10; 10] 4.35 1 Levy5 20 [10; 10] 15.27 1 Levy5 40 [10; 10] 59.08 1 Levy5 80 [10; 10] 235.22 1 Levy6 3 [10; 10] 0.96 1 Levy6 5 [10; 10] 1.57 1 Levy6 10 [10; 10] 4.29 1 Levy6 20 [10; 10] 14.86 1 Levy6 40 [10; 10] 64.11 1 Levy6 80 [10; 10] 372.39 <p> ; 10 7 ] 0.09 2 Levy2 1 <ref> [10; 10] </ref> 0.61 4 Levy3 2 [10; 10] 16.14 30 Levy4 2 [10; 10] 2.13 7 Levy5 3 [10; 10] 0.75 1 Levy5 5 [10; 10] 1.45 1 Levy5 10 [10; 10] 4.35 1 Levy5 20 [10; 10] 15.27 1 Levy5 40 [10; 10] 59.08 1 Levy5 80 [10; 10] 235.22 1 Levy6 3 [10; 10] 0.96 1 Levy6 5 [10; 10] 1.57 1 Levy6 10 [10; 10] 4.29 1 Levy6 20 [10; 10] 14.86 1 Levy6 40 [10; 10] 64.11 1 Levy6 80 [10; 10] 372.39 1 Beale 2 [4:5; 4:5] 2.50 <p> Levy2 1 <ref> [10; 10] </ref> 0.61 4 Levy3 2 [10; 10] 16.14 30 Levy4 2 [10; 10] 2.13 7 Levy5 3 [10; 10] 0.75 1 Levy5 5 [10; 10] 1.45 1 Levy5 10 [10; 10] 4.35 1 Levy5 20 [10; 10] 15.27 1 Levy5 40 [10; 10] 59.08 1 Levy5 80 [10; 10] 235.22 1 Levy6 3 [10; 10] 0.96 1 Levy6 5 [10; 10] 1.57 1 Levy6 10 [10; 10] 4.29 1 Levy6 20 [10; 10] 14.86 1 Levy6 40 [10; 10] 64.11 1 Levy6 80 [10; 10] 372.39 1 Beale 2 [4:5; 4:5] 2.50 3 Beale 2 [10 2 ; <p> Levy3 2 <ref> [10; 10] </ref> 16.14 30 Levy4 2 [10; 10] 2.13 7 Levy5 3 [10; 10] 0.75 1 Levy5 5 [10; 10] 1.45 1 Levy5 10 [10; 10] 4.35 1 Levy5 20 [10; 10] 15.27 1 Levy5 40 [10; 10] 59.08 1 Levy5 80 [10; 10] 235.22 1 Levy6 3 [10; 10] 0.96 1 Levy6 5 [10; 10] 1.57 1 Levy6 10 [10; 10] 4.29 1 Levy6 20 [10; 10] 14.86 1 Levy6 40 [10; 10] 64.11 1 Levy6 80 [10; 10] 372.39 1 Beale 2 [4:5; 4:5] 2.50 3 Beale 2 [10 2 ; 10 2 ] 3.31 12 Beale <p> Levy4 2 <ref> [10; 10] </ref> 2.13 7 Levy5 3 [10; 10] 0.75 1 Levy5 5 [10; 10] 1.45 1 Levy5 10 [10; 10] 4.35 1 Levy5 20 [10; 10] 15.27 1 Levy5 40 [10; 10] 59.08 1 Levy5 80 [10; 10] 235.22 1 Levy6 3 [10; 10] 0.96 1 Levy6 5 [10; 10] 1.57 1 Levy6 10 [10; 10] 4.29 1 Levy6 20 [10; 10] 14.86 1 Levy6 40 [10; 10] 64.11 1 Levy6 80 [10; 10] 372.39 1 Beale 2 [4:5; 4:5] 2.50 3 Beale 2 [10 2 ; 10 2 ] 3.31 12 Beale 2 [10 4 ; 10 4 <p> Levy5 3 <ref> [10; 10] </ref> 0.75 1 Levy5 5 [10; 10] 1.45 1 Levy5 10 [10; 10] 4.35 1 Levy5 20 [10; 10] 15.27 1 Levy5 40 [10; 10] 59.08 1 Levy5 80 [10; 10] 235.22 1 Levy6 3 [10; 10] 0.96 1 Levy6 5 [10; 10] 1.57 1 Levy6 10 [10; 10] 4.29 1 Levy6 20 [10; 10] 14.86 1 Levy6 40 [10; 10] 64.11 1 Levy6 80 [10; 10] 372.39 1 Beale 2 [4:5; 4:5] 2.50 3 Beale 2 [10 2 ; 10 2 ] 3.31 12 Beale 2 [10 4 ; 10 4 ] 5.52 31 Beale 2 [10 <p> Levy5 5 <ref> [10; 10] </ref> 1.45 1 Levy5 10 [10; 10] 4.35 1 Levy5 20 [10; 10] 15.27 1 Levy5 40 [10; 10] 59.08 1 Levy5 80 [10; 10] 235.22 1 Levy6 3 [10; 10] 0.96 1 Levy6 5 [10; 10] 1.57 1 Levy6 10 [10; 10] 4.29 1 Levy6 20 [10; 10] 14.86 1 Levy6 40 [10; 10] 64.11 1 Levy6 80 [10; 10] 372.39 1 Beale 2 [4:5; 4:5] 2.50 3 Beale 2 [10 2 ; 10 2 ] 3.31 12 Beale 2 [10 4 ; 10 4 ] 5.52 31 Beale 2 [10 7 ; 10 7 ] 23.29 <p> Levy5 10 <ref> [10; 10] </ref> 4.35 1 Levy5 20 [10; 10] 15.27 1 Levy5 40 [10; 10] 59.08 1 Levy5 80 [10; 10] 235.22 1 Levy6 3 [10; 10] 0.96 1 Levy6 5 [10; 10] 1.57 1 Levy6 10 [10; 10] 4.29 1 Levy6 20 [10; 10] 14.86 1 Levy6 40 [10; 10] 64.11 1 Levy6 80 [10; 10] 372.39 1 Beale 2 [4:5; 4:5] 2.50 3 Beale 2 [10 2 ; 10 2 ] 3.31 12 Beale 2 [10 4 ; 10 4 ] 5.52 31 Beale 2 [10 7 ; 10 7 ] 23.29 61 Schwefel1 3 [10 7 ; <p> Levy5 20 <ref> [10; 10] </ref> 15.27 1 Levy5 40 [10; 10] 59.08 1 Levy5 80 [10; 10] 235.22 1 Levy6 3 [10; 10] 0.96 1 Levy6 5 [10; 10] 1.57 1 Levy6 10 [10; 10] 4.29 1 Levy6 20 [10; 10] 14.86 1 Levy6 40 [10; 10] 64.11 1 Levy6 80 [10; 10] 372.39 1 Beale 2 [4:5; 4:5] 2.50 3 Beale 2 [10 2 ; 10 2 ] 3.31 12 Beale 2 [10 4 ; 10 4 ] 5.52 31 Beale 2 [10 7 ; 10 7 ] 23.29 61 Schwefel1 3 [10 7 ; 10 7 ] 0.24 0 Booth <p> [10 2 ; 10 2 ] 3.31 12 Beale 2 [10 4 ; 10 4 ] 5.52 31 Beale 2 [10 7 ; 10 7 ] 23.29 61 Schwefel1 3 [10 7 ; 10 7 ] 0.24 0 Booth 2 [10 7 ; 10 7 ] 0.11 0 Powell 4 <ref> [10; 20] </ref> 6.69 267 Schwefel3 2 [10 7 ; 10 7 ] 0.03 0 Rosenbrock 2 [10 7 ; 10 7 ] 0.33 10 Ratz1 5 [500; 600] 1.19 0 Ratz25 4 [0; 10] 2.86 0 Ratz27 4 [0; 10] 4.44 0 Ratz210 4 [0; 10] 7.42 0 Ratz3 6 [0; <p> ; 10 7 ] 0.24 0 Booth 2 [10 7 ; 10 7 ] 0.11 0 Powell 4 [10; 20] 6.69 267 Schwefel3 2 [10 7 ; 10 7 ] 0.03 0 Rosenbrock 2 [10 7 ; 10 7 ] 0.33 10 Ratz1 5 [500; 600] 1.19 0 Ratz25 4 <ref> [0; 10] </ref> 2.86 0 Ratz27 4 [0; 10] 4.44 0 Ratz210 4 [0; 10] 7.42 0 Ratz3 6 [0; 1] 9.13 2 More1 3 [4; 4] 10.04 5 More2 4 [25; 25] 189.56 32 Table 3: Summary of the Experimental Results on Unconstrained Optimization. 25 Levy1 is the function f (x) <p> Booth 2 [10 7 ; 10 7 ] 0.11 0 Powell 4 [10; 20] 6.69 267 Schwefel3 2 [10 7 ; 10 7 ] 0.03 0 Rosenbrock 2 [10 7 ; 10 7 ] 0.33 10 Ratz1 5 [500; 600] 1.19 0 Ratz25 4 <ref> [0; 10] </ref> 2.86 0 Ratz27 4 [0; 10] 4.44 0 Ratz210 4 [0; 10] 7.42 0 Ratz3 6 [0; 1] 9.13 2 More1 3 [4; 4] 10.04 5 More2 4 [25; 25] 189.56 32 Table 3: Summary of the Experimental Results on Unconstrained Optimization. 25 Levy1 is the function f (x) = x 6 15x 4 + <p> 7 ] 0.11 0 Powell 4 [10; 20] 6.69 267 Schwefel3 2 [10 7 ; 10 7 ] 0.03 0 Rosenbrock 2 [10 7 ; 10 7 ] 0.33 10 Ratz1 5 [500; 600] 1.19 0 Ratz25 4 <ref> [0; 10] </ref> 2.86 0 Ratz27 4 [0; 10] 4.44 0 Ratz210 4 [0; 10] 7.42 0 Ratz3 6 [0; 1] 9.13 2 More1 3 [4; 4] 10.04 5 More2 4 [25; 25] 189.56 32 Table 3: Summary of the Experimental Results on Unconstrained Optimization. 25 Levy1 is the function f (x) = x 6 15x 4 + 27x 2 + 250: Levy2 is <p> i=1 where y i = 1 + (x i 1)=4 (1 i n): Levy6 is the function f (x 1 ; : : : ; x n ) = sin (3x 1 ) 2 + i=1 For n = 2, this problem has 900 local minima when variables range over <ref> [10; 10] </ref>. For n = 3 and n = 4, the number of local minima goes up to 2700 and 71000. <p> On the other hand, box-consistency on the natural and distributed extensions is really orthogonal to the pruning obtained from the Taylor expansion, producing a particularly effective algorithm. It is interesting to note that the idea of using approximations of arc-consistency was also used independently by Hong and Stahl <ref> [10] </ref>, who were also exposed to research on Constraint Logic Programming. Their use of projections is however quite different from ours. The key idea is to work with a set of boxes and to use projections to split a box into several subboxes by isolating all zeros of a projection.
Reference: [11] <author> More. J., B. Garbow, and K. Hillstrom. </author> <title> Testing Unconstrained Optimization Software. </title> <journal> ACM Transactions on Mathematical Software, </journal> <volume> 7(1) </volume> <pages> 17-41, </pages> <year> 1981. </year>
Reference-contexts: The benchmarks were taken mainly from <ref> [17, 11, 33, 35] </ref> and, for each of them, we give the number of variables, the range of the variables, the CPU time, and the number of splits. The experimental results once again exhibit a number of interesting facts.
Reference: [12] <author> R.B. Kearfott. </author> <title> Preconditioners for the Interval Gauss-Seidel Method. </title> <journal> SIAM Journal of Numerical Analysis, </journal> <volume> 27, </volume> <year> 1990. </year>
Reference: [13] <author> R.B. Kearfott. </author> <title> A Review of Preconditioners for the Interval Gauss-Seidel Method. Interval Computations 1, </title> <booktitle> 1 </booktitle> <pages> 59-85, </pages> <year> 1991. </year>
Reference: [14] <author> R.B. Kearfott. </author> <title> A Review of Techniques in the Verified Solution of Constrained Global Optimization Problems. </title> <note> (To Appear), </note> <year> 1994. </year>
Reference: [15] <author> R. Krawczyk. </author> <title> Newton-Algorithmen zur Bestimmung von Nullstellen mit Fehlerschranken. </title> <journal> Computing, </journal> <volume> 4 </volume> <pages> 187-201, </pages> <year> 1969. </year>
Reference-contexts: Many of these techniques use ideas behind Newton root finding method, exploit properties such as differentiability, and define various pruning operators, many of which extend the seminal work of Krawczyk <ref> [15] </ref>. These algorithms can often be viewed as an iteration 1 of two steps, constraint propagation and splitting, although they are rarely presented this way and it is not always clear what the constraint propagation step computes.
Reference: [16] <author> J.H.M. Lee and M.H. van Emden. </author> <title> Interval Computation as Deduction in CHIP. </title> <journal> Journal of Logic Programming, </journal> <volume> 16(3-4):255-276, </volume> <year> 1993. </year>
Reference-contexts: These ideas have been developed and made popular by the CLP system BNR-Prolog [30] and generalized to constraint solving over discrete quantities in its successor CLP (BNR) [28, 2]. Many other systems (e.g <ref> [16, 36] </ref>) have been developed on similar principles.
Reference: [17] <author> A.V. Levy and A. Montalvo. </author> <title> The Tunnelling Algorithm for the Global Minimization of Functions. </title> <journal> SIAM J. Sci. Stat. Comput., </journal> <volume> 6(1) </volume> <pages> 15-29, </pages> <year> 1985. </year>
Reference-contexts: Our first example in this area is an unconstrained minimization, i.e., the search for the minimum value of a nonlinear function. A typical problem taken from <ref> [17] </ref> is given by the minimization of the function 2 Y 5 X i cos ((i + 1)x k + i): Restricting attention to the range [10; 10] for the variables, a simple Newton program to solve this problem is as follows: minimization (Min,Boxes) :- minof (cos (2*X1+1)+2*cos (3*X1+2)+3*cos (4*X1+3)+4*cos (5*X1+4)+5*cos <p> The benchmarks were taken mainly from <ref> [17, 11, 33, 35] </ref> and, for each of them, we give the number of variables, the range of the variables, the CPU time, and the number of splits. The experimental results once again exhibit a number of interesting facts.
Reference: [18] <author> A.K. Mackworth. </author> <title> Consistency in Networks of Relations. </title> <journal> Artificial Intelligence, </journal> <volume> 8(1) </volume> <pages> 99-118, </pages> <year> 1977. </year>
Reference-contexts: Traditionally, CLP (Intervals) languages were designed in terms of simple primitive constraints (e.g. add (x,y,z), mult (x,y,z), cos (x,z), : : :) on which they apply simple approximations of arc-consistency <ref> [18, 20] </ref>, a notion well-known in artificial intelligence. Complex constraints are simply rewritten into a set of primitive constraints. The advantage of this methodology is the elegant operational semantics of the language. <p> For convenience, we also assume in the rest of this section that all constraints are defined over variables x 1 ; : : :; x n . 3.2.3 Box Consistency Box-consistency [1] is an approximation of arc-consistency, a notion well-known in artificial intelligence <ref> [18] </ref> which states a simple local condition on a constraint c and the set of possible values for each of its variables, say D 1 ; : : : ; D n .
Reference: [19] <author> K. Meintjes and A.P. Morgan. </author> <title> Chemical Equilibrium Systems as Numerical test Problems. </title> <journal> ACM Transactions on Mathematical Software, </journal> <volume> 16 </volume> <pages> 143-151, </pages> <year> 1990. </year>
Reference-contexts: It takes about 15 seconds to find all solutions on a SUN Sparc-10 workstation. Nonlinear constraint techniques are also of primary importance to solve chemical equilibrium systems. Our next example is an equilibrium system to describe the propulsion of propane in air and is taken from <ref> [19] </ref>. The Newton program which finds the solution in about 5 seconds is depicted in Figure 2. The above programs guarantee that any solution to the constraint system are located in at least one box returned by Newton. <p> It considers successively equation solving, unconstrained optimization, and constrained optimization. 4.1 Equation Solving This section reports experimental results of Newton on a variety of standard benchmarks for equation solving. The benchmarks were taken from papers on numerical analysis [24], interval analysis [8, 10, 23], and continuation methods <ref> [38, 26, 25, 19] </ref>. We also compare Newton with a traditional interval method using the Hansen-Segupta's operator, range testing, and branching. <p> &gt; &gt; : x 3 + x 8 = 3 10 5 x 4 + 2 x 7 = 10 5 1 2 4 0:6194411 10 7 x 9 = x 1 x 2 2 which is typical of chemical equilibrium systems. 23 Chemical Equilibrium Application This problem originates from <ref> [19] </ref> and describes a chemical equilibrium system.
Reference: [20] <author> U. Montanari. </author> <title> Networks of Constraints : Fundamental Properties and Applications to Picture Processing. </title> <journal> Information Science, </journal> <volume> 7(2) </volume> <pages> 95-132, </pages> <year> 1974. </year>
Reference-contexts: Traditionally, CLP (Intervals) languages were designed in terms of simple primitive constraints (e.g. add (x,y,z), mult (x,y,z), cos (x,z), : : :) on which they apply simple approximations of arc-consistency <ref> [18, 20] </ref>, a notion well-known in artificial intelligence. Complex constraints are simply rewritten into a set of primitive constraints. The advantage of this methodology is the elegant operational semantics of the language. <p> [10 2 ; 10 2 ] 3.31 12 Beale 2 [10 4 ; 10 4 ] 5.52 31 Beale 2 [10 7 ; 10 7 ] 23.29 61 Schwefel1 3 [10 7 ; 10 7 ] 0.24 0 Booth 2 [10 7 ; 10 7 ] 0.11 0 Powell 4 <ref> [10; 20] </ref> 6.69 267 Schwefel3 2 [10 7 ; 10 7 ] 0.03 0 Rosenbrock 2 [10 7 ; 10 7 ] 0.33 10 Ratz1 5 [500; 600] 1.19 0 Ratz25 4 [0; 10] 2.86 0 Ratz27 4 [0; 10] 4.44 0 Ratz210 4 [0; 10] 7.42 0 Ratz3 6 [0;
Reference: [21] <author> R.E. Moore. </author> <title> Interval Analysis. </title> <publisher> Prentice-Hall, </publisher> <address> Englewood Cliffs, NJ, </address> <year> 1966. </year>
Reference-contexts: This extension is an example of centered forms which are interval extensions introduced by Moore <ref> [21] </ref> and studied by many authors, since they have important properties. The Taylor interval extension of a constraint is parametrized by the intervals for the variables in the constraint.
Reference: [22] <author> R.E. Moore. </author> <title> Methods and Applications of Interval Analysis. </title> <publisher> SIAM Publ., </publisher> <year> 1979. </year>
Reference-contexts: ? 57.40 chemistry 5 108 [0; 10 8 ] 6.32 ? 56.55 neuro 6 1024 [10; 10] 0.91 28.84 5.02 neuro 6 1024 [1000; 1000] 172.71 ? 5.02 Table 1: Summary of the Experimental Results on Equation Solving. 20 Interval Arithmetic Benchmarks These are traditional benchmarks from interval arithmetic papers <ref> [22, 10] </ref>.
Reference: [23] <author> R.E. Moore and S.T. Jones. </author> <title> Safe Starting Regions for Iterative Methods. </title> <journal> SIAM Journal on Numerical Analysis, </journal> <volume> 14 </volume> <pages> 1051-1065, </pages> <year> 1977. </year>
Reference-contexts: It considers successively equation solving, unconstrained optimization, and constrained optimization. 4.1 Equation Solving This section reports experimental results of Newton on a variety of standard benchmarks for equation solving. The benchmarks were taken from papers on numerical analysis [24], interval analysis <ref> [8, 10, 23] </ref>, and continuation methods [38, 26, 25, 19]. We also compare Newton with a traditional interval method using the Hansen-Segupta's operator, range testing, and branching.
Reference: [24] <author> J.J. More and M.Y. Cosnard. </author> <title> Numerical Solution of Nonlinear Equations. </title> <journal> ACM Transactions on Mathematical Software, </journal> <volume> 5 </volume> <pages> 64-85, </pages> <year> 1979. </year>
Reference-contexts: It considers successively equation solving, unconstrained optimization, and constrained optimization. 4.1 Equation Solving This section reports experimental results of Newton on a variety of standard benchmarks for equation solving. The benchmarks were taken from papers on numerical analysis <ref> [24] </ref>, interval analysis [8, 10, 23], and continuation methods [38, 26, 25, 19]. We also compare Newton with a traditional interval method using the Hansen-Segupta's operator, range testing, and branching. <p> One of the interesting features of this benchmark is that it is easy to scale up to an arbitrary dimension and hence provides a good basis to compare various methods. Discretization of a Nonlinear Integral Equation This example comes from <ref> [24] </ref> and is also a standard benchmark for nonlinear equation solving .
Reference: [25] <author> A.P. Morgan. </author> <title> Computing All Solutions To Polynomial Systems Using Homotopy Continuation. </title> <journal> Appl. Math. Comput., </journal> <volume> 24 </volume> <pages> 115-138, </pages> <year> 1987. </year>
Reference-contexts: It considers successively equation solving, unconstrained optimization, and constrained optimization. 4.1 Equation Solving This section reports experimental results of Newton on a variety of standard benchmarks for equation solving. The benchmarks were taken from papers on numerical analysis [24], interval analysis [8, 10, 23], and continuation methods <ref> [38, 26, 25, 19] </ref>. We also compare Newton with a traditional interval method using the Hansen-Segupta's operator, range testing, and branching. <p> c 4 + c 1 c 2 + c 1 c 3 + c 1 c 2 = 4:0616 s 2 + s 3 + s 4 + s 2 + s 3 + s 2 = 3:9701 i + c 2 The second benchmark, denoted by kin2, is from <ref> [25] </ref> and describes the inverse position problem for a six-revolute-joint problem in mechanics. <p> 0.03 0 Rosenbrock 2 [10 7 ; 10 7 ] 0.33 10 Ratz1 5 [500; 600] 1.19 0 Ratz25 4 [0; 10] 2.86 0 Ratz27 4 [0; 10] 4.44 0 Ratz210 4 [0; 10] 7.42 0 Ratz3 6 [0; 1] 9.13 2 More1 3 [4; 4] 10.04 5 More2 4 <ref> [25; 25] </ref> 189.56 32 Table 3: Summary of the Experimental Results on Unconstrained Optimization. 25 Levy1 is the function f (x) = x 6 15x 4 + 27x 2 + 250: Levy2 is the function f (x) = i=1 Levy3 is the function f (x 1 ; x 2 ) =
Reference: [26] <author> A.P. Morgan. </author> <title> Solving Polynomial Systems Using Continuation for Scientific and Engineering Problems. </title> <publisher> Prentice-Hall, </publisher> <address> Englewood Cliffs, NJ, </address> <year> 1987. </year>
Reference-contexts: In addition, box-consistency on the natural extension produces narrowing operators which are more effective when the algorithm is not near a solution. Newton has been applied to numerous applications. It has been shown to be competitive on constraint-solving benchmarks with state-of-the-art continuation methods (e.g., <ref> [26, 38] </ref>) and to outperform traditional interval methods [37]. In addition, Newton has been applied to many benchmarks in global optimization (unconstrained and constrained optimization), outperforming interval methods we are aware of. <p> It considers successively equation solving, unconstrained optimization, and constrained optimization. 4.1 Equation Solving This section reports experimental results of Newton on a variety of standard benchmarks for equation solving. The benchmarks were taken from papers on numerical analysis [24], interval analysis [8, 10, 23], and continuation methods <ref> [38, 26, 25, 19] </ref>. We also compare Newton with a traditional interval method using the Hansen-Segupta's operator, range testing, and branching. <p> In both examples, the initial intervals were given as [10 8 ; 10 8 ]. An Economics Modelling Application The following example is taken from <ref> [26] </ref>. It is a difficult economic modelling problem that can be scaled up to arbitrary dimensions. <p> Combustion Application This problem is also from Morgan's book <ref> [26] </ref> and represents a combustion problem for a temperature of 3000 ffi .
Reference: [27] <author> A. Neumaier. </author> <title> Interval Methods for Systems of Equations. </title> <booktitle> PHI Series in Computer Science. </booktitle> <publisher> Cambridge University Press, </publisher> <address> Cambridge, </address> <year> 1990. </year> <month> 34 </month>
Reference: [28] <editor> W. Older and F. Benhamou. </editor> <booktitle> Programming in CLP(BNR). In PPCP'93, </booktitle> <address> Newport, RI (USA), </address> <year> 1993. </year>
Reference-contexts: These ideas have been developed and made popular by the CLP system BNR-Prolog [30] and generalized to constraint solving over discrete quantities in its successor CLP (BNR) <ref> [28, 2] </ref>. Many other systems (e.g [16, 36]) have been developed on similar principles.
Reference: [29] <author> W. Older and A. Vellino. </author> <title> Extending Prolog with Constraint Arithmetics on Real Intervals. </title> <booktitle> In Canadian Conference on Computer & Electrical Engineering, </booktitle> <address> Ottawa, </address> <year> 1990. </year>
Reference-contexts: j 9r 1 2 I 1 ; : : : ; 9r i1 2 I i1 ; : : : ; 9r i+1 2 I i+1 ; 9r n 2 I n : c (r 1 ; : : :; r n ) gg: This condition, used in systems like <ref> [29, 3] </ref>, is easily enforced on simple constraints such as x 1 = x 2 + x 3 ; x 1 = x 2 x 3 ; x 1 = x 2 fi x 3 but it is also computationally very expensive for complex constraints with multiple occurrences of the same
Reference: [30] <author> W. Older and A. Vellino. </author> <title> Constraint Arithmetic on Real Intervals. In Constraint Logic Programming: Selected Research. </title> <publisher> The MIT Press, </publisher> <address> Cambridge, Massachussetts, </address> <year> 1993. </year>
Reference-contexts: These ideas have been developed and made popular by the CLP system BNR-Prolog <ref> [30] </ref> and generalized to constraint solving over discrete quantities in its successor CLP (BNR) [28, 2]. Many other systems (e.g [16, 36]) have been developed on similar principles.
Reference: [31] <author> L.B. Rall. </author> <title> Automatic Differentiation: Techniques and Applications. </title> <booktitle> Springer Lectures Notes in Computer Science, </booktitle> <publisher> Springer Verlag, </publisher> <address> New York, </address> <year> 1981. </year>
Reference-contexts: extension of c wrt ~ I, denoted by c t ( ~ I) , is the interval constraint b f (m 1 ; : : :; m n ) + i=1 @x i In the current version of our system, the partial derivatives are computed numerically using auto matic differentiation <ref> [31] </ref>. 3.2.5 Specification of the Constraint Solver We are now in position to specify the behaviour of the constraint solver of Newton. Informally speaking, Newton enforces box-consistency on the three interval extensions of the constraints without removing any solution.
Reference: [32] <author> H. Ratschek and J. Rokne. </author> <title> New Computer Methods for Global Optimization. </title> <publisher> Ellis Horwood Limited, </publisher> <address> Chichester, </address> <year> 1988. </year>
Reference-contexts: These functions come from the discretization of a nonlinear integral equation, giving a constraint system denser than the sparse constraint system for the Broyden banded functions. The variables x i were given initial domains [4; 5] as in <ref> [32] </ref>. 5 Some interval methods such as [7] are more sophisticated than HRB but the sophistication aims at speeding up the computation near a solution.
Reference: [33] <author> D. Ratz. </author> <title> Box-Splitting Strategies for the Interval Gauss-Seidel Step in a Global Optimization Method. </title> <booktitle> Computing, </booktitle> <address> 53(3-4):337-353, </address> <year> 1994. </year>
Reference-contexts: The benchmarks were taken mainly from <ref> [17, 11, 33, 35] </ref> and, for each of them, we give the number of variables, the range of the variables, the CPU time, and the number of splits. The experimental results once again exhibit a number of interesting facts. <p> The experimental results once again exhibit a number of interesting facts. Newton is able to solve problems such as Levy5 and Levy6 in essentially linear time in the number of variables. Newton solves the problems Ratz25, Ratz27, and Ratz210 without splitting. These problems were used in <ref> [33] </ref> to study splitting strategies. Finally, Newton does not exhibit the behaviour of traditional interval methods on problems such as the Rosenbrock function.
Reference: [34] <author> S.M. Rump. </author> <title> Verification Methods for Dense and Sparse Systems of Equations. </title> <editor> In J. (Ed.) Herzberger, editor, </editor> <booktitle> Topics in Validated Computations, </booktitle> <pages> pages 217-231. </pages> <publisher> Elsevier, </publisher> <year> 1988. </year>
Reference: [35] <author> H. Schwefel. </author> <title> Numerical Optimization of Computer Models. </title> <publisher> Wiley, </publisher> <address> New York, </address> <year> 1981. </year>
Reference-contexts: The benchmarks were taken mainly from <ref> [17, 11, 33, 35] </ref> and, for each of them, we give the number of variables, the range of the variables, the CPU time, and the number of splits. The experimental results once again exhibit a number of interesting facts.
Reference: [36] <author> G. Sidebottom and W. havens. </author> <title> Hierarchical Arc Consistency Applied to Numeric Processing in Constraint Logic Programming. </title> <journal> Computational Intelligence, </journal> <volume> 8(4), </volume> <year> 1992. </year>
Reference-contexts: These ideas have been developed and made popular by the CLP system BNR-Prolog [30] and generalized to constraint solving over discrete quantities in its successor CLP (BNR) [28, 2]. Many other systems (e.g <ref> [16, 36] </ref>) have been developed on similar principles.
Reference: [37] <author> P. Van Hentenryck, D. McAllister, and D. Kapur. </author> <title> Solving Polynomial Systems Using a Branch and Prune Approach. </title> <note> SIAM Journal on Numerical Analysis, 1995. (to appear). </note>
Reference-contexts: Newton has been applied to numerous applications. It has been shown to be competitive on constraint-solving benchmarks with state-of-the-art continuation methods (e.g., [26, 38]) and to outperform traditional interval methods <ref> [37] </ref>. In addition, Newton has been applied to many benchmarks in global optimization (unconstrained and constrained optimization), outperforming interval methods we are aware of. This paper is an introduction to Newton, a specification of its main functionalities, and a description of its performance on a number of benchmarks. <p> The internal details of the solver are not covered here. Instead, we focus on a high-level "static" specification of the language. The reader interested in the details of the solver can consult <ref> [37] </ref>. hProgrami ::= hClausesi hClauses i ::= hHeadi. j hHeadi :- hBodyi. j hClausesi hClausesi hHeadi ::= hAtomi hBodyi ::= hConstrainti j hGoali j hBodyi , hBodyi hConstrainti ::= hCi j [hCi] fl hCi ::= hExpri &gt;= hExpri j hExpri &lt;= hExpri j hExpri = hExpri j hExpri in hRangei hExpri <p> The constraint is provably satisfied if l 0. Determining if a box provably satisfies an equation or a system of equations is obviously more difficult and this is an active research area in interval analysis. The techniques currently used in Newton are described in <ref> [37] </ref>. We now specify the facilities to obtain safe and canonical boxes in Newton. Specification 3 [Safe and Canonical Boxes] The procedure Box is a Sbox of System holds iff System is box-consistent wrt Box and Box is a safe box of System. <p> Newton uses box-consistency to solve constraint systems as well as unconstrained and constrained optimization problems. Experimental results on numerous benchmarks from numerical analysis, and comparison with other tools, show the effectiveness of Newton. Acknowledgments The equation-solving algorithm was developed jointly with D. McAllester and D. Kapur <ref> [37] </ref>. The formalization of box-consistency was developed jointly with F. Benhamou [1]. Special thanks to A. Colmerauer and B. Le Charlier for several helpful comments and suggestions, and to P. Codognet for his invitation to CCP'95.
Reference: [38] <author> J Verschelde, P. Verlinden, and R. Cools. </author> <title> Homotopies Exploiting Newton Polytopes For Solving Sparse Polynomial Systems. </title> <journal> SIAM Journal on Numerical Analysis, </journal> <volume> 31(3) </volume> <pages> 915-930, </pages> <year> 1994. </year> <month> 35 </month>
Reference-contexts: In addition, box-consistency on the natural extension produces narrowing operators which are more effective when the algorithm is not near a solution. Newton has been applied to numerous applications. It has been shown to be competitive on constraint-solving benchmarks with state-of-the-art continuation methods (e.g., <ref> [26, 38] </ref>) and to outperform traditional interval methods [37]. In addition, Newton has been applied to many benchmarks in global optimization (unconstrained and constrained optimization), outperforming interval methods we are aware of. <p> It considers successively equation solving, unconstrained optimization, and constrained optimization. 4.1 Equation Solving This section reports experimental results of Newton on a variety of standard benchmarks for equation solving. The benchmarks were taken from papers on numerical analysis [24], interval analysis [8, 10, 23], and continuation methods <ref> [38, 26, 25, 19] </ref>. We also compare Newton with a traditional interval method using the Hansen-Segupta's operator, range testing, and branching. <p> We also compare Newton with a traditional interval method using the Hansen-Segupta's operator, range testing, and branching. This method uses the same implementation technology as Newton and is denoted by HRB in the following. 5 Finally, we compare Newton with a state-of-the-art continuation method <ref> [38] </ref>, denoted by CONT in the following. Note that all results given in this section were obtained by running Newton on a Sun Sparc 10 workstation to obtain all solutions. In addition, the final intervals must have widths smaller than 10 8 . The results are summarized in Table 1. <p> A Neurophysiology Application This example illustrates the limitations of Newton. The application is from neurophysiology <ref> [38] </ref> and consists of the following system of equations: 8 &gt; &gt; &gt; &gt; &gt; &gt; &gt; : 1 + x 2 x 2 4 = 1 3 + x 6 x 3 x 5 x 3 2 = c 2 3 + x 6 x 2 x 5 x 2
References-found: 38

