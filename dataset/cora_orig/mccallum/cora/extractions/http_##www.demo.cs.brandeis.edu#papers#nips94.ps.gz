URL: http://www.demo.cs.brandeis.edu/papers/nips94.ps.gz
Refering-URL: http://www.demo.cs.brandeis.edu/papers/long.html
Root-URL: http://www.cs.brandeis.edu
Email: saunders@cis.ohio-state.edu  
Author: Gregory M. Saunders, Peter J. Angeline, and Jordan B. Pollack 
Address: Columbus, Ohio 43210  
Affiliation: Laboratory for Artificial Intelligence Research Department of Computer and Information Science The Ohio State University  
Abstract: This paper introduces GNARL, an evolutionary program which induces recurrent neural networks that are structurally unconstrained. In contrast to constructive and destructive algorithms, GNARL employs a population of networks and uses a fitness functions unsupervised feedback to guide search through network space. Annealing is used in generating both gaussian weight changes and structural modifications. Applying GNARL to a complex search and collection task demonstrates that the system is capable of inducing networks with complex internal dynamics.
Abstract-found: 1
Intro-found: 1
Reference: <author> Barto, A. G. </author> <year> (1990). </year> <title> Connectionist learning for control. </title> <editor> In Miller, W. T. III, Sutton, R. S., and Werbos, P. J., editors, </editor> <booktitle> Neural Networks for Control. Chapter 1, </booktitle> <pages> pages 5-58. </pages> <publisher> MIT Press, </publisher> <address> Cambridge. </address>
Reference-contexts: T N ( ) 1 f max measure of Ns performance is used to anneal the structural and parametric <ref> (Barto, 1990) </ref> similarity between parent and offspring, so that networks with a high temperature are mutated severely, and those with a low temperature are mutated only slightly. This allows a coarse-grained search initially, and a finer-grained search as a network approaches a solution (cf. Kirkpatrick et al., 1983).

Reference: <author> Cun, Y.L., Denker, J., and Solla, S. </author> <year> (1990). </year> <title> Optimal brain damage. </title> <editor> In Touretzky, D., editor, </editor> <booktitle> Advances in Neural Information Processing Systems 2. </booktitle> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Fahlman, S. and Lebiere, C. </author> <year> (1990). </year> <title> The cascade-correlation architecture. </title> <editor> In Touretzky, D. S., editor, </editor> <booktitle> Advances in Neural Information Processing Structures 2, </booktitle> <pages> pages 524532. </pages> <publisher> Mor-gan Kaufmann. </publisher>
Reference: <author> Fahlman, S. </author> <year> (1991). </year> <title> The recurrent cascade-correlation architecture. </title> <editor> In Lippmann, R., </editor> <title> output units Move, Right, Left. (a) Fixed point attractor that results for sequence of 500 food signals; (b) Limit cycle attractor that results when a sequence of 500 no food signals is given to network; (c) All states visited while traversing the trail; (d) The x position of the ant over time when run on an empty grid. (c) (d) Time Step x position (a) Moody, </title> <editor> J., and Touretzky, D., editors, </editor> <booktitle> Advances in Neural Information Processing Systems 3, pages 190196. </booktitle> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo. </address>
Reference-contexts: But these constructive and destructive algorithms are monotonic extremes that ignore a more moderate solution: dynamically add or remove pieces of architecture as needed. Moreover, by exclusively exploring either feedforward networks (e.g., Ash, 1989), fully-connected recurrent networks (e.g., Chen, et al. 1993), or some restricted middle ground <ref> (e.g., Fahlman, 1991) </ref>, these algorithms allow only limited structural change. Finally, constructive and destructive algorithms are supervised methods Structural and Behavioral Evolution of Recurrent Networks which rely on complex predicates to determine when to add or delete pieces of network architecture (e.g., when rate of improvement falls below threshold).
Reference: <author> Fogel, D. </author> <year> (1992). </year> <title> Evolving Artificial Intelligence. </title> <type> Ph.D. thesis, </type> <institution> University of California, </institution> <address> San Diego. </address>
Reference-contexts: Most studies, however, still assume a fixed structure for the network (e.g., Belew et al., 1990; Jefferson, et al., 1991; see also Schaffer, et al. 1992), and those that do not allow only limited structural change (e.g., Potter, 1992, and Karu-nanithi et al., 1992). Evolutionary programming <ref> (Fogel, 1992) </ref> is an alternate optimization technique which, when applied to network induction, obviates the need for a bitstring-to-network mapping by mutating networks directly.
Reference: <author> Fogel, D., Fogel, L., and Porto, V. W. </author> <year> (1990). </year> <title> Evolving neural networks. </title> <journal> Biological Cybernetics. 63:487493. </journal>
Reference: <author> Hanson, S. J. </author> <year> (1990). </year> <title> Meiosis networks. </title> <editor> In Touretzky, D., editor, </editor> <booktitle> Advances in Neural Information Processing Systems 2, </booktitle> <pages> pages 533541. </pages> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo. </address>
Reference: <author> Hassibi, B. and Stork, D. G. </author> <year> (1993). </year> <title> Second order derivatives for network pruning: Optimal brain surgeon. </title> <editor> In Hanson, S. J., Cowan, J. D., and Giles, C. L., editors, </editor> <booktitle> Advances in Neural Information Processing Systems 5, </booktitle> <pages> pages 164171. </pages> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Holland, J. </author> <year> (1975). </year> <title> Adaptation in Natural and Artificial Systems. </title> <publisher> The University of Mich-igan Press, </publisher> <address> Ann Arbor, MI. </address>
Reference-contexts: Finally, constructive and destructive algorithms are supervised methods Structural and Behavioral Evolution of Recurrent Networks which rely on complex predicates to determine when to add or delete pieces of network architecture (e.g., when rate of improvement falls below threshold). Genetic algorithms <ref> (Holland 1975) </ref>, on the other hand, are unsupervised methods which can induce networks by making stochastic modifications to a population of bitstrings, each of which is interpreted as a network.
Reference: <author> Jefferson, D., Collins, R., Cooper, C., Dyer, M., Flowers, M., Korf, R., Taylor, C., and Wang, A. </author> <year> (1991). </year> <title> Evolution as a theme in artificial life: The genesys/tracker system. </title> <editor> In Langton, C. G., Taylor, C., Farmer, J. D., and Rasmussen, S., editors, </editor> <booktitle> Artificial Life II: Proceedings of the Workshop on Artificial Life. </booktitle> <pages> pages 549577. </pages> <publisher> Addison-Wesley. </publisher>
Reference: <author> Karunanithi, N., Das, R., and Whitley, D. </author> <year> (1992). </year> <title> Genetic cascade learning for neural networks. </title> <booktitle> In Proceedings of COGANN-92 International Workshop on Combinations of Genetic Algorithms and Neural Networks. </booktitle>
Reference: <author> Kirkpatrick, S., Gelatt, C. D., and Vecchi, M. P. </author> <year> (1983). </year> <title> Optimization by simulated annealing. </title> <journal> Science, </journal> <volume> 220 </volume> <pages> 671-680. </pages>
Reference-contexts: This allows a coarse-grained search initially, and a finer-grained search as a network approaches a solution <ref> (cf. Kirkpatrick et al., 1983) </ref>.
Reference: <author> Koza, J. </author> <year> (1992). </year> <title> Genetic evolution and co-evolution of computer programs. </title> <editor> In Christopher G. Langton, Charles Taylor, J. D. F. and Rasmussen, S., editors, </editor> <booktitle> Artificial Life II. </booktitle> <publisher> Addison Wesley Publishing Company, </publisher> <address> Reading Mass. </address>
Reference: <author> Mozer, M. and Smolensky, P. </author> <year> (1989). </year> <title> Skeletonization: A technique for trimming the fat from a network via relevance assessment. </title> <editor> In Touretzky, D., editor, </editor> <booktitle> Advances in Neural Information Processing Systems 1, </booktitle> <pages> pages 107115. </pages> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo. </address>
Reference: <author> Omlin, C. W. and Giles, C. L. </author> <month> (April </month> <year> 1993). </year> <title> Pruning recurrent neural networks for improved generalization performance. </title> <type> Technical Report Tech Report No 93-6, </type> <institution> Computer Science Department, Rensselaer Polytechnic Institute. </institution>
Reference: <author> Pollack, J. B. </author> <year> (1991). </year> <title> The induction of dynamical recognizer. </title> <journal> Machine Learning. </journal> <volume> 7 </volume> <pages> 227-252. </pages>
Reference: <author> Potter, M. A. </author> <year> (1992). </year> <title> A genetic cascade-correlation learning algorithm. </title> <booktitle> In Proceedings of COGANN-92 International Workshop on Combinations of Genetic Algorithms and Neural Networks. </booktitle>
Reference-contexts: Most studies, however, still assume a fixed structure for the network (e.g., Belew et al., 1990; Jefferson, et al., 1991; see also Schaffer, et al. 1992), and those that do not allow only limited structural change <ref> (e.g., Potter, 1992, and Karu-nanithi et al., 1992) </ref>. Evolutionary programming (Fogel, 1992) is an alternate optimization technique which, when applied to network induction, obviates the need for a bitstring-to-network mapping by mutating networks directly.
Reference: <editor> Schaffer, J. D., Whitley, D., and Eshelman, L. J. </editor> <year> (1992). </year> <title> Combinations of genetic algorithms and neural networks: A survey of the state of the art. </title> <booktitle> In Proceedings of COGANN-92 International Workshop on Combinations of Genetic Algorithms and Neural Networks. </booktitle>
References-found: 18

