URL: ftp://ftp.cis.ufl.edu/cis/tech-reports/tr91/tr91-008.ps
Refering-URL: http://www.cis.ufl.edu/tech-reports/tech-reports/tr91-abstracts.html
Root-URL: http://www.cis.ufl.edu
Email: Email: zt@beach.cis.ufl.edu  Email:fishwick@cis.ufl.edu  
Phone: Phone: 904-392-9600  Phone: 904-392-1414  
Title: Feed-forward Neural Nets as Models for Time Series Forecasting  
Author: Zaiyong Tang Paul A. Fishwick 
Keyword: Computer Science: Feed-forward neural networks, application Forecasting: Time series  
Address: Gainesville, FL 32611  Gainesville, FL 32611  
Affiliation: BUS 351 Dept. of Decision Information Sciences University of Florida,  301 CSE, Department of Computer Information Sciences University of Florida,  
Abstract-found: 0
Intro-found: 1
Reference: <author> Becker, S. and Le Cun, Y. </author> <year> 1989. </year> <title> Improving the convergence of back-propagation learning with second order methods. </title> <editor> In Touretzky, D., Hinton, G., and Sejnowski, T., editors, </editor> <booktitle> Proceedings of the 1988 Connectionist Models Summer School, </booktitle> <pages> pages 29-37, </pages> <address> San Mateo. (Pittsburg 1988), </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Bochereau, L. and Bourgine, P. </author> <year> 1990. </year> <title> Rule extraction and validity domain on a multilayer neural network. </title> <booktitle> In International Joint Conference on Neural Networks, </booktitle> <volume> volume 1, </volume> <pages> pages 97-100, </pages> <address> San Diego. </address>
Reference: <author> Bowerman, B. L. and O'connell, R. T. </author> <year> 1987. </year> <title> Time Series Forecasting: Unified Concepts and Computer Implementation, 2nd ed. </title> <publisher> Duxbury Press, </publisher> <address> Boston, TX. </address>
Reference: <author> Box, G. E. P. and Jenkins, G. M. </author> <year> 1970. </year> <title> Time Series Analysis: Forecasting and Control. </title> <publisher> Holden-Day, </publisher> <address> San Francisco, CA. </address>
Reference-contexts: Time series forecasting has an especially high utility for predicting economic and business trends. Many forecasting methods have been developed in the last few decades (Makridakis 1982). The Box-Jenkins method is one of the most widely used time series forecasting methods in practice <ref> (Box and Jenkins 1970) </ref>. Recently, artificial neural networks that serve as powerful computational frameworks have gained much popularity in business applications as well as in computer science, psychology and cognitive science.
Reference: <author> Dutta, S. and Shekhar, S. </author> <year> 1988. </year> <title> Bond rating: A non-conservative application of neural networks. </title> <booktitle> In International Joint Conference on Neural Networks, </booktitle> <volume> volume 2, </volume> <pages> pages 443-450. </pages> <institution> TR91-008 Computer and Information Sciences, University of Florida 24 Fahlman, S. and Lebiere, </institution> <address> C. </address> <year> 1990. </year> <title> The cascade-correlation learning architecture. </title> <editor> In Touretzky, D., editor, </editor> <booktitle> Advances in Neural Information Processing Systems, </booktitle> <volume> volume 2, </volume> <pages> pages 524-532, </pages> <address> San Mateo. (Denver 1989), </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Fisher, D. H. and McKusick, K. B. </author> <year> 1989. </year> <title> An empirical comparison of id3 and back-propagation. </title> <booktitle> In Proceedings of the Eleventh International Joint Conference on Artificial Intelligence, </booktitle> <address> De-troit, MI. </address>
Reference: <author> Fishwick, P. A. </author> <year> 1989. </year> <title> Neural network models in simulation: A comparison with traditional modeling approaches. </title> <booktitle> In Proceedings of Winter Simulation Conference, </booktitle> <pages> pages 702-710, </pages> <address> Wash-ington, DC. </address>
Reference: <author> Frean, M. </author> <year> 1990. </year> <title> The upstart algorithm: A method for constructing and training feedforward neural networks. </title> <journal> Neural Computation, </journal> <volume> 2 </volume> <pages> 198-209. </pages>
Reference: <author> Fu, L. </author> <year> 1991. </year> <title> Rule learning by searching on adapted nets. </title> <booktitle> In Proceedings of AAAI-91 Conference, </booktitle> <pages> pages 590-595, </pages> <address> Anaheim, CA. </address>
Reference: <author> Funahashi, K. </author> <year> 1989. </year> <title> On the approximate realization of continuous mappings by neural networks. </title> <booktitle> Neural Networks, </booktitle> <volume> 2 </volume> <pages> 183-192. </pages>
Reference: <author> Hecht-Nielsen, R. </author> <year> 1989. </year> <title> Theory of the backpropagation neural network. </title> <booktitle> In International Joint Conference on Neural Networks, </booktitle> <volume> volume 1, </volume> <pages> pages 593-605, </pages> <address> New York. (Washington 1989), </address> <publisher> IEEE. </publisher>
Reference: <author> Hinton, G. E. </author> <year> 1989. </year> <title> Connectionist learning procedures. </title> <journal> Artificial Intelligence, </journal> <volume> 40 </volume> <pages> 185-234. </pages>
Reference: <author> Hoff, C. J. </author> <year> 1983. </year> <title> A Practical Guide to BOX-JENKINS Forecasting. Lifetime Learning Publications, </title> <address> Belmont, CA. </address>
Reference-contexts: Thus, in the following, we present only a brief account of the Box-Jenkins method and give a more detailed description of the neural net approach. Complete treatments of the two methods can be found in <ref> (Hoff 1983) </ref> and (Rumelhart, McClelland and the PDP Research Group 1986). 1 We later determined that the neural network approach (for the ballistics data) performed quite adequately as long as the initial data were randomized with respect to training order.
Reference: <author> Hornik, K., Stinchcombe, M., and White, H. </author> <year> 1990. </year> <title> Using multilayer feedforward networks for universal approximation. </title> <booktitle> Neural Networks, </booktitle> <volume> 3 </volume> <pages> 551-560. </pages>
Reference: <author> Jacobs, R. </author> <year> 1988. </year> <title> Increased rates of convergence through learning rate adaptation. </title> <booktitle> Neural Networks, </booktitle> <volume> 1 </volume> <pages> 295-307. </pages>
Reference-contexts: Some researchers have studied dynamically constructed neural nets. That is, the network structure is determined during the learning process of a particular problem (Fahlman and Lebiere 1990; Frean 1990). The training parameters can also be determined automatically, hence freeing the user from having to make an educated guess <ref> (Jacobs 1988) </ref>. Another disadvantage of neural net models is that the trained neural nets do not offer us much information about the underlying structure (function) of a time series.
Reference: <author> Lapedes, A. and Farber, R. </author> <year> 1987. </year> <title> Nonlinear signal processing using neural networks: Prediction and system modelling. </title> <type> Technical Report LA-UR-87-2662, </type> <institution> Los Alamos National Laboratory, </institution> <address> Los Alamos, NM. </address>
Reference: <author> Lee, C. J. and Chen, C. </author> <year> 1990. </year> <title> Structural changes and the forecasting of quarterly accounting earnings in the utility industry. </title> <journal> Journal of Accounting and Economics, </journal> <volume> 13 </volume> <pages> 93-122. </pages> <institution> TR91-008 Computer and Information Sciences, University of Florida 25 Lippmann, R. </institution> <year> 1987. </year> <title> An introduction to computing with neural nets. </title> <journal> IEEE ASSP Magazine, </journal> <pages> pages 4-22. </pages>
Reference-contexts: determined by a linear model as described above, the Box-Jenkins method can do well as long as the pattern does not change. 2 If the series is determined by a nonlinear 2 For series with pattern shift, more sophisticated procedures are needed to identify the shift and modify the model <ref> (Lee and Chen 1990) </ref>. TR91-008 Computer and Information Sciences, University of Florida 17 process, for instance, the logistic series generated by x (t + 1) = ax (t)(1 x (t)). The Box-Jenkins method is likely to fail since no higher order terms exist in the models.
Reference: <author> Makridakis, S. e. </author> <year> 1982. </year> <title> The accuracy of extrapolation (time series) mehtods: Results of a forecasting competition. </title> <journal> Journal of Forecasting, </journal> <volume> 1 </volume> <pages> 111-153. </pages>
Reference-contexts: For example, one can consider time-dependent sales volume or rain fall data to be examples of time series. Time series forecasting has an especially high utility for predicting economic and business trends. Many forecasting methods have been developed in the last few decades <ref> (Makridakis 1982) </ref>. The Box-Jenkins method is one of the most widely used time series forecasting methods in practice (Box and Jenkins 1970). Recently, artificial neural networks that serve as powerful computational frameworks have gained much popularity in business applications as well as in computer science, psychology and cognitive science. <p> A discussion of the convergence and the performance of BP is beyond the scope of this paper. Interested readers are referred to White (1989) and Hecht-Nielsen (1989). 2 Forecast Experiments 2.1 Data Sharda and Patil (1990) used 75 series taken from the well known M-Competition <ref> (Makridakis 1982) </ref> series that are suitable for the Box-Jenkins analysis. Their results showed that the relative performance of the neural nets and the Box-Jenkins models vary substantially for different series. To explore why there were such differences, we took 14 series from the 75 time series used in their study.
Reference: <author> Newton, H. </author> <year> 1988. </year> <title> TIMESLAB: A Time Series Analysis Laboratory. </title> <publisher> Wadsworth & Brooks Cole, </publisher> <address> California. </address>
Reference-contexts: The first one is the international airline passenger data from 1949 to 1960. This series has been used in the classic work by Box and TR91-008 Computer and Information Sciences, University of Florida 8 Jenkins (1970). The other series is company sales data taken from TIMESLAB <ref> (Newton 1988) </ref> (Figure 4). 2.2 Experimental Design Following convention, the last 8 terms of the quarterly series and the last 18 terms of the monthly series were used as hold-out samples to test the forecasting performance of the models. Different neural net structures and training parameters were tested. <p> The other two test series were used to examine the performance of the Box-Jenkins method and the neural net approach for forecasting with different forecast horizons. Box-Jenkins model forecasting was carried out with the time series analysis package TIMESLAB <ref> (Newton 1988) </ref>. TR91-008 Computer and Information Sciences, University of Florida 9 TR91-008 Computer and Information Sciences, University of Florida 10 TR91-008 Computer and Information Sciences, University of Florida 11 2.3 Forecasting results We first tested the performance of general neural net models that are used by most other researchers.
Reference: <author> Rumelhart, D., Hinton, G., and Williams, R. </author> <year> 1986. </year> <title> Learning representations by back-propagating errors. </title> <journal> Nature, </journal> <volume> 323 </volume> <pages> 533-536. </pages> <note> Reprinted in Anderson 1988. </note>
Reference-contexts: Thus, in the following, we present only a brief account of the Box-Jenkins method and give a more detailed description of the neural net approach. Complete treatments of the two methods can be found in (Hoff 1983) and <ref> (Rumelhart, McClelland and the PDP Research Group 1986) </ref>. 1 We later determined that the neural network approach (for the ballistics data) performed quite adequately as long as the initial data were randomized with respect to training order. <p> Less progress has been made in attacking the second problem. Fortunately, the problem is rare for most applications (Hecht-Nielsen 1989) and can be alleviated by changing network structures <ref> (Rumelhart, Hinton and Williams 1986) </ref>. Most of the time needed to build a neural net model is spent on the training process. Some experiments might also need to be done to determine a good neural net structure. But the neural net models are fairly robust as shown above.
Reference: <editor> Rumelhart, D., McClelland, J., </editor> <booktitle> and the PDP Research Group 1986. Parallel Distributed Processing: Explorations in the Microstructure of Cognition, </booktitle> <volume> volume 1. </volume> <publisher> MIT Press, </publisher> <address> Cambridge. </address>
Reference-contexts: Thus, in the following, we present only a brief account of the Box-Jenkins method and give a more detailed description of the neural net approach. Complete treatments of the two methods can be found in (Hoff 1983) and <ref> (Rumelhart, McClelland and the PDP Research Group 1986) </ref>. 1 We later determined that the neural network approach (for the ballistics data) performed quite adequately as long as the initial data were randomized with respect to training order. <p> Less progress has been made in attacking the second problem. Fortunately, the problem is rare for most applications (Hecht-Nielsen 1989) and can be alleviated by changing network structures <ref> (Rumelhart, Hinton and Williams 1986) </ref>. Most of the time needed to build a neural net model is spent on the training process. Some experiments might also need to be done to determine a good neural net structure. But the neural net models are fairly robust as shown above.
Reference: <author> Sharda, R. and Ireland, T. </author> <year> 1987. </year> <title> A empirical test of automatic forecasting systems. </title> <type> Technical Report ORSA/TIMS Meeting, </type> <address> New Orleans. </address>
Reference-contexts: The data series are labeled as they appear in Sharda and Patil (1990). Series numbered less than 382 are quarterly and those numbered greater than 382 are monthly series. Although AUTOBOX, the computer expert system used in Sharda and Patil (1990), was reported to be comparable to real experts <ref> (Sharda and Ireland 1987) </ref>, we feel that AUTOBOX might not give the best results derived from the Box-Jenkins method. To obtain a more fair comparison, two more standard test series are also used. The first one is the international airline passenger data from 1949 to 1960.
Reference: <author> Sharda, R. and Patil, R. </author> <year> 1990. </year> <title> Neural networks as forecasting experts: An empirical test. </title> <booktitle> In International Joint Conference on Neural Networks, </booktitle> <volume> volume 1, </volume> <pages> pages 491-494, </pages> <address> Washington, D.C. </address> <publisher> IEEE. </publisher>
Reference: <author> Simpson, P. </author> <year> 1990. </year> <title> Artificial Neural Systems: Foundations, Paradigms, Applications, and Implementations. </title> <publisher> Pergamon Press. </publisher>
Reference-contexts: Neural nets have been successfully applied to loan evaluation, signature recognition, time series forecasting (Dutta and Shekhar 1988; Sharda and Patil 1990), classification analysis (Fisher and McKusick 1989; Singleton and Surkan 1990), and many other difficult pattern recognition problems <ref> (Simpson 1990) </ref>. While it is often difficult or impossible to explicitly write down a set of rules for such pattern recognition problems, a neural network can be trained with raw data to produce a solution. Concerning the application of neural nets to time series forecasting, there have been mixed reviews.
Reference: <author> Singleton, J. C. and Surkan, A. J. </author> <year> 1990. </year> <title> Modeling the judgement of bond rating agencies: </title> <booktitle> Artificial intelligence applied to finance. In 1990 Midwest Finance Association Meetings, </booktitle> <address> Chicago, IL. </address>
Reference: <author> Solla, S., Levin, E., and Fleisher, M. </author> <year> 1988. </year> <title> Accelerated learning in layered neural networks. </title> <journal> Complex Systems, </journal> <volume> 2 </volume> <pages> 625-639. </pages>
Reference: <author> Sontag, E. </author> <year> 1990. </year> <title> On the recognition capabilities of feedforward nets. </title> <type> Technical Report Report SYCON-90-03, </type> <institution> Rutgers Center for System and Control. </institution>
Reference-contexts: Theoretically, direct connections increase the recognition power of a feed-forward neural net <ref> (Sontag 1990) </ref>. The insertion of direction connections is equivalent to adding linear components to the net (see the discussion in 3.1). TR91-008 Computer and Information Sciences, University of Florida 15 Table-6: Forecast error (MAPS) of neural nets with direct connections.
Reference: <author> Tong, H. and Lim, K. </author> <year> 1980. </year> <title> Threshold autoregression, limit cycle and cyclical data. </title> <journal> Journal of Royal Statistical Society B, 42:245. </journal>
Reference-contexts: Weigend et al. (1990) applied feed-forward neural nets to forecasting with noisy real-world data from sun spots and computational ecosystems. A neural network, trained with past data, generated accurate future predictions and consistently out-performed traditional statistical methods such as the TAR (threshold autoregressive) model <ref> (Tong and Lim 1980) </ref>. Sharda and Patil (1990) conducted a forecasting competition between neural network models and a traditional forecasting technique (namely the Box-Jenkins method) using 75 time series of various nature. They concluded that simple neural nets could forecast about as well as the Box-Jenkins forecasting system.
Reference: <author> Utans, J. and Moody, J. </author> <year> 1991. </year> <title> Selecting neural network architectures via the prediction risk: Application to corporate bond rating prediction. </title> <booktitle> In The First International Conference on Artificial Intelligence Applications on Wall Street, </booktitle> <address> Los Alamitos, CA. </address> <note> IEEE. TR91-008 Computer and Information Sciences, University of Florida 26 Weigend, </note> <author> A., Huberman, B., and Rumelhart, D. </author> <year> 1990. </year> <title> Predicting the future: A connectionist approach. </title> <type> Technical Report Stanford-PDP-90-01, </type> <institution> Stanfor University, Stanford, </institution> <address> CA. </address>
Reference: <author> White, H. </author> <year> 1989. </year> <title> Some asymptotic results for learning in single hidden-layer feedforward neural models. </title> <journal> Journal of the American Statistical Association, </journal> <volume> 184 </volume> <pages> 1003-1013. </pages>
Reference-contexts: Thus, the neural net models are fairly robust. The robustness of neural net models is also reflected by the fact that they are essentially assumption-free models, although assumptions about the training set can be made and statistic inferences can be carried out <ref> (White 1989) </ref>. This assumption-free property makes them applicable to a wide range of pattern recognition problems.
References-found: 30

