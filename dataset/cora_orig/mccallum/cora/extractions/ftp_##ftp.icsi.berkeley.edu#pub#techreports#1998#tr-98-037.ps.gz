URL: ftp://ftp.icsi.berkeley.edu/pub/techreports/1998/tr-98-037.ps.gz
Refering-URL: http://www.icsi.berkeley.edu/techreports/1998.html
Root-URL: http://www.icsi.berkeley.edu
Title: Robust speech recognition using articulatory information  
Phone: (510) 643-9153 FAX (510) 643-7684  
Author: Katrin Kirchhoff 
Date: August 1998  
Address: I 1947 Center St. Suite 600 Berkeley, California 94704-1198  
Affiliation: INTERNATIONAL COMPUTER SCIENCE INSTITUTE  
Pubnum: TR-98-037  
Abstract-found: 0
Intro-found: 1
Reference: [1] <author> A.M.A. Ali, J. van der Spiegel and P. </author> <title> Mueller,"An acoustic-phonetic feature-based system for the automatic recognition of fricative consonants", </title> <booktitle> Proceedings ICASSP 98, </booktitle> <pages> pp. 961-964 </pages>
Reference-contexts: In cases where these are not available for the test set, classifiers are trained to map the acoustic signal to these parameters on the training set; during testing, the output parameters from these classifiers are used instead of direct articulatory measurements. The third approach <ref> [4, 5, 15, 1] </ref> seeks to emphasize those properties of the speech signal which correspond to articulatory and acoustic-phonetic categories by specially designed preprocessing, such as detection of energy in specific frequency regions which are considered most informative for the categories in question.
Reference: [2] <author> F. Alimoglu and E. Alpaydin, </author> <title> "Combining multiple representations and classifiers for pen-based handwritten digit recognition", </title> <booktitle> Proceedings of the Fourth International Conference on Document Analysis and Recognition (ICDAR 97), </booktitle> <address> Ulm, Germany, </address> <year> 1997 </year>
Reference: [3] <author> B.B. Atal, J.J. Chang, M.V. Mathews, and J.W. Tukey, </author> <title> "Inversion of articulatory-to-acoustic transformation in the vocal tract by a computer-sorting technique", </title> <journal> Journal of the Acoustical Society of America 63, </journal> <pages> pp. 1535-1555, </pages> <year> 1978 </year>
Reference-contexts: Thus, data-driven reduction of the feature set (e.g. Principal Components Analysis (PCA) or Linear Discriminant Analysis (LDA)) is usually required. Finally, the mapping from acoustics to articulation is not biunique: various articulatory constellations may produce highly similar acoustic signals <ref> [35, 3, 8] </ref>. 4 This entails the problem of reliably estimating articulatory features from the acous-tic signal as well. On the other hand, high detection rates have been reported for articulatory features.
Reference: [4] <author> N.N. Bitar and C.Y. Espy-Wilson, </author> <title> "Knowledge-based parameters for HMM speech recognition", </title> <booktitle> Proceedings ICASSP-96, </booktitle> <pages> pp. 29-32, </pages> <year> 1996 </year>
Reference-contexts: In cases where these are not available for the test set, classifiers are trained to map the acoustic signal to these parameters on the training set; during testing, the output parameters from these classifiers are used instead of direct articulatory measurements. The third approach <ref> [4, 5, 15, 1] </ref> seeks to emphasize those properties of the speech signal which correspond to articulatory and acoustic-phonetic categories by specially designed preprocessing, such as detection of energy in specific frequency regions which are considered most informative for the categories in question. <p> The preprocessing strategy will also be affected by speaker-dependent spectral shifts, thus requiring speaker normalization. As above, this approach has only been applied to limited tasks like classification of sounds into broad phonetic categories <ref> [4] </ref>. We are unaware of any application to actual word recognition on a sizeable corpus. The fourth approach has so far mainly concentrated on developing appropriate statistical mapping functions for the acoustic-articulatory inversion problem and for the modeling of articulatory trajectories.
Reference: [5] <author> N.N. Bitar and C.Y. Espy-Wilson, </author> <title> "The design of acoustic parameters for speaker-independent speech recognition", </title> <booktitle> Proceedings Eurospeech 97, </booktitle> <pages> pp. 1239-1242, </pages> <year> 1997 </year>
Reference-contexts: In cases where these are not available for the test set, classifiers are trained to map the acoustic signal to these parameters on the training set; during testing, the output parameters from these classifiers are used instead of direct articulatory measurements. The third approach <ref> [4, 5, 15, 1] </ref> seeks to emphasize those properties of the speech signal which correspond to articulatory and acoustic-phonetic categories by specially designed preprocessing, such as detection of energy in specific frequency regions which are considered most informative for the categories in question.
Reference: [6] <author> C.P. Browman and L. Goldstein, </author> <title> "Towards an articulatory phonology", </title> <type> Phonology Yearbook 3, </type> <pages> pp. 219-252 </pages>
Reference-contexts: Finally, the fourth approach [13, 29, 30, 31, 32] attempts to infer vocal tract shapes from the acoustic signal by nonlinear statistical functions, based on speech production theories like Articulatory Phonology <ref> [6] </ref> or the Distinctive Regions Model [25]. Each of the above approaches has its strengths and weaknesses. Articulatory feature sets have the disadvantage of quantizing articulatory information rather than providing continuous measurements of articulatory parameters. This may not be sufficient to classify inherently continuous speech segments, e.g. vowels.
Reference: [7] <author> J. Carson-Berndsen and K. Huebener, </author> <title> Phoneme Recognition using Acoustic Events, </title> <type> Verbmobil Technical Report, </type> <institution> Universities of Bielefeld and Hamburg, </institution> <year> 1994 </year>
Reference-contexts: On the other hand, high detection rates have been reported for articulatory features. Typically, frame accuracy rates range between 70% and 95% (e.g. <ref> [14, 7] </ref>), with place features having the lowest and voicing features having the highest detection rates. Articulatory parameters obtained from actual physical measurements describe articulation in a more fine-grained manner. However, these measurements are usually not available during testing and thus have to be estimated from the acoustic signal.
Reference: [8] <author> T. Gay. B. Lindblom, and J. </author> <title> Lubker,"Production of bite-block vowels: acoustic equivalence by selective compensation", </title> <journal> Journal of the Acoustical Society of America 69, </journal> <pages> 802-810 </pages>
Reference-contexts: Thus, data-driven reduction of the feature set (e.g. Principal Components Analysis (PCA) or Linear Discriminant Analysis (LDA)) is usually required. Finally, the mapping from acoustics to articulation is not biunique: various articulatory constellations may produce highly similar acoustic signals <ref> [35, 3, 8] </ref>. 4 This entails the problem of reliably estimating articulatory features from the acous-tic signal as well. On the other hand, high detection rates have been reported for articulatory features.
Reference: [9] <author> S. Greenberg and B. Kingsbury, </author> <title> "The modulation spectrogram: in pursuit of an invariant representation of speech", </title> <booktitle> Proceedings ICASSP-97, </booktitle> <pages> pp. 1647-1650, </pages> <year> 1997 </year>
Reference-contexts: These are described in Table 6. Two different types of acoustic preprocessing were used: (a) eight log-RASTA-PLP coefficients, and deltas of those coefficients, and (b) 15 modulation spectrogram features. The latter have shown to yield promising results on noisy and reverberant speech <ref> [9] </ref>. All systems are hybrid ANN/HMM recognizers and were trained using embedded training. Baseline system II additionally uses an optimized lexicon which was obtained from iterative re-alignment of the signals with labels generated from each training pass. We used the OGI Numbers95 corpus [26] for the present study.
Reference: [10] <author> P. Steingrimsson, B. Markussen, O. Andersen, P. Dalsgaard and W. Barry, </author> <title> "From Acoustic Signal to Phonetic Features: dynamically constrained self-organising neural network", </title> <booktitle> Proceedings ICPhS-95 </booktitle>
Reference: [11] <author> L. Deng and D. Sun, </author> <title> "A statistical approach to ASR using atomic units constructed from overlapping articulatory features", </title> <journal> Journal of the Acoustical Society of America 95, </journal> <pages> pp. 2702-2719 27 </pages>
Reference-contexts: Systems in the first category <ref> [14, 11, 12, 23, 20] </ref> make use of a pre-defined set of features describing articulation, e.g. voiced, voiceless, fricative, nasal etc. These features are detected from the parameterized acoustic signal by means of neural networks, HMMs, or some other statistical classifier.
Reference: [12] <author> L. Deng and J. Wu, </author> <title> "Hierarchical partitioning of articulatory state space for articulatory-feature based speech recognition", </title> <booktitle> Proceedings ICSLP-96, </booktitle> <pages> pp. 2266-2269 </pages>
Reference-contexts: Systems in the first category <ref> [14, 11, 12, 23, 20] </ref> make use of a pre-defined set of features describing articulation, e.g. voiced, voiceless, fricative, nasal etc. These features are detected from the parameterized acoustic signal by means of neural networks, HMMs, or some other statistical classifier.
Reference: [13] <author> L. Deng, </author> <title> "A dynamic, feature-based approach to speech modeling and recognition", </title> <booktitle> Proceedings IEEE Workshop on Speech Understanding and Recognition, </booktitle> <address> Santa Barbara, </address> <month> December </month> <year> 1997, </year> <pages> pp. 107-113, </pages> <year> 1997 </year>
Reference-contexts: Finally, the fourth approach <ref> [13, 29, 30, 31, 32] </ref> attempts to infer vocal tract shapes from the acoustic signal by nonlinear statistical functions, based on speech production theories like Articulatory Phonology [6] or the Distinctive Regions Model [25]. Each of the above approaches has its strengths and weaknesses.
Reference: [14] <author> E. Eide, J.R. Rohlicek, H. Gish and S. Mitter, </author> <title> "A linguistic feature representation of the speech waveform", </title> <booktitle> Proceedings ICASSP 93, </booktitle> <pages> pp. 483-486, </pages> <year> 1993 </year>
Reference-contexts: Systems in the first category <ref> [14, 11, 12, 23, 20] </ref> make use of a pre-defined set of features describing articulation, e.g. voiced, voiceless, fricative, nasal etc. These features are detected from the parameterized acoustic signal by means of neural networks, HMMs, or some other statistical classifier. <p> On the other hand, high detection rates have been reported for articulatory features. Typically, frame accuracy rates range between 70% and 95% (e.g. <ref> [14, 7] </ref>), with place features having the lowest and voicing features having the highest detection rates. Articulatory parameters obtained from actual physical measurements describe articulation in a more fine-grained manner. However, these measurements are usually not available during testing and thus have to be estimated from the acoustic signal.
Reference: [15] <author> A. Varnich Hansen, </author> <title> "Acoustic parameters optimised for recognition of phonetic features", </title> <booktitle> Proceedings Eurospeech-97, </booktitle> <pages> pp. 397-400 </pages>
Reference-contexts: In cases where these are not available for the test set, classifiers are trained to map the acoustic signal to these parameters on the training set; during testing, the output parameters from these classifiers are used instead of direct articulatory measurements. The third approach <ref> [4, 5, 15, 1] </ref> seeks to emphasize those properties of the speech signal which correspond to articulatory and acoustic-phonetic categories by specially designed preprocessing, such as detection of energy in specific frequency regions which are considered most informative for the categories in question.
Reference: [16] <author> T.K. Ho, J.J. Hu.. </author> <title> and S.N. Srihari, "Decision Combination in Multiple Classifier Systems", </title> <journal> IEEE Trans. Pattern Analysis and Machine Intelligence 16, </journal> <pages> pp. 66-75, </pages> <year> 1994 </year>
Reference-contexts: Combinations of classifiers have been employed for various pattern recognition tasks. The methods by which classifiers are combined include: majority vote [19], class ranking <ref> [16] </ref>, linear combination of a posteriori probabilities [21], local accuracy estimates [36], Dempster-Shafer theory [33], mixture of experts [18].
Reference: [17] <author> F.V. Jensen, </author> <title> An Introduction to Bayesian Networks, </title> <publisher> NewYork/Heidelberg: Springer, </publisher> <year> 1996 </year>
Reference: [18] <author> M.I. Jordan, </author> <title> "Hierarchical mixtures of experts and the EM algorithm", </title> <booktitle> Neural Computation 6, </booktitle> <pages> 181-214, </pages> <year> 1994 </year>
Reference-contexts: Combinations of classifiers have been employed for various pattern recognition tasks. The methods by which classifiers are combined include: majority vote [19], class ranking [16], linear combination of a posteriori probabilities [21], local accuracy estimates [36], Dempster-Shafer theory [33], mixture of experts <ref> [18] </ref>. In a hybrid ANN/HMM recognition system, the HMM decoder uses the Bayes rule P (W jO) = P (O) to compute the probability P (W jO) of a word sequence W given a sequence of acoustic observations O.
Reference: [19] <author> F. Kimura and M. Shridhar, </author> <title> "Handwritten Numerical Recognition based on Multiple Algorithms", </title> <booktitle> Pattern Recognition 24, </booktitle> <pages> pp. 969-983, </pages> <year> 1991 </year>
Reference-contexts: Combinations of classifiers have been employed for various pattern recognition tasks. The methods by which classifiers are combined include: majority vote <ref> [19] </ref>, class ranking [16], linear combination of a posteriori probabilities [21], local accuracy estimates [36], Dempster-Shafer theory [33], mixture of experts [18].
Reference: [20] <author> K. </author> <title> Kirchhoff, "Syllable-level desynchronisation of phonetic features for speech recognition", </title> <booktitle> Proceedings ICSLP-96, </booktitle> <pages> pp. 2274-2276 </pages>
Reference-contexts: Systems in the first category <ref> [14, 11, 12, 23, 20] </ref> make use of a pre-defined set of features describing articulation, e.g. voiced, voiceless, fricative, nasal etc. These features are detected from the parameterized acoustic signal by means of neural networks, HMMs, or some other statistical classifier.
Reference: [21] <author> J. Kittler, M. Hatef, R.P.W. Duin and J. Matas, </author> <title> "On combining classifiers", </title> <journal> IEEE Trans. on Pattern Analysis and Machine Intelligence 20, </journal> <pages> pp. 226-239, </pages> <year> 1998 </year>
Reference-contexts: Combinations of classifiers have been employed for various pattern recognition tasks. The methods by which classifiers are combined include: majority vote [19], class ranking [16], linear combination of a posteriori probabilities <ref> [21] </ref>, local accuracy estimates [36], Dempster-Shafer theory [33], mixture of experts [18]. In a hybrid ANN/HMM recognition system, the HMM decoder uses the Bayes rule P (W jO) = P (O) to compute the probability P (W jO) of a word sequence W given a sequence of acoustic observations O. <p> The most widely used linear probability combination rules are the sum rule and the product rule. These are derived as follows (cf. <ref> [21] </ref>): Assume that there are N different classifiers fn 1 ; n 2 ; :::n N g which are applied to the same task of distinguishing K possible classes f! 1 ; ! 2 ; :::; ! K g, using N different representations of the object to be classified, x 1 <p> rewritten as 1 N Y P (! k jx n ) = P (! k ) n=1 N X ffi kn (10) which leads to P (! k jx 1 ; :::; x R ) = (1 R)P (! k ) + n=1 In various classification experiments, Kittler et al. <ref> [21] </ref> observed that the sum rule provided the best results although it makes the most restrictive statistical assumptions. This is explained by the greater robustness of the sum rule to estimation errors in the individual classifiers.
Reference: [22] <author> D. Koller and M. Sahami, </author> <title> "Toward optimal feature selection", </title> <booktitle> Machine Learning: Proceedings of the Thirteenth International Conference, </booktitle> <publisher> Morgan Kaufmann, </publisher> <year> 1996 </year>
Reference-contexts: It would be more economical to select the articulatory features in a way that does not require previous generation of all features and that does not interact negatively with embedded training. For this reason, a feature selection algorithm based on information-theoretic criteria <ref> [22] </ref> was chosen, which is aimed at eliminating both irrelevant and redundant features. This method successively eliminates features from the original feature set while minimizing the relative entropy between the original distribution over the output classes and the distribution resulting from the reduced feature set.
Reference: [23] <author> S.A. Liu, </author> <title> "Landmark detection for distinctive feature-based speech recognition", </title> <journal> Journal of the Acoustical Society of America 100, </journal> <pages> pp. 3417-3430, </pages> <year> 1996 </year>
Reference-contexts: Systems in the first category <ref> [14, 11, 12, 23, 20] </ref> make use of a pre-defined set of features describing articulation, e.g. voiced, voiceless, fricative, nasal etc. These features are detected from the parameterized acoustic signal by means of neural networks, HMMs, or some other statistical classifier.
Reference: [24] <author> N. Morgan and H. Bourlard, </author> <title> "An introduction to hybrid HMM/Connectionist Continuous Speech Recognition", </title> <journal> IEEE Signal Processing Magazine, </journal> <pages> pp. 25-42, </pages> <year> 1995 </year> <month> 28 </month>
Reference-contexts: These are then transformed into likelihoods by division by the class priors <ref> [24] </ref>. Since the output produced by MLPs can be interpreted as class-conditional a posteriori probabilities [28], hybrid ANN/HMM recognizers readily lend themselves to various probability-theoretic combination schemes which are commonly employed in pattern recognition and machine learning.
Reference: [25] <author> M. Myrayati, R. Carre and B. Guerin, </author> <title> "Distinctive regions and modes: a new theory in speech production", </title> <booktitle> Speech Communication 7, </booktitle> <pages> pp. 257-286, </pages> <year> 1988 </year>
Reference-contexts: Finally, the fourth approach [13, 29, 30, 31, 32] attempts to infer vocal tract shapes from the acoustic signal by nonlinear statistical functions, based on speech production theories like Articulatory Phonology [6] or the Distinctive Regions Model <ref> [25] </ref>. Each of the above approaches has its strengths and weaknesses. Articulatory feature sets have the disadvantage of quantizing articulatory information rather than providing continuous measurements of articulatory parameters. This may not be sufficient to classify inherently continuous speech segments, e.g. vowels.
Reference: [26] <institution> Center for Spoken Language Understanding, Department of Computer Science and Engineering, Oregon Graduate Institute. Numbers corpus, release 1.0, </institution> <year> 1995 </year>
Reference-contexts: All systems are hybrid ANN/HMM recognizers and were trained using embedded training. Baseline system II additionally uses an optimized lexicon which was obtained from iterative re-alignment of the signals with labels generated from each training pass. We used the OGI Numbers95 corpus <ref> [26] </ref> for the present study. This corpus consists of continuously spoken numbers recorded over the telephone. The training set for this database consists of 3590 sentences (about three hours), 327 of which are used as a cross-validation set for MLP training.
Reference: [27] <author> G. Papcun, J. Hochberg, T.R. Thomas, F. Larouche, J. Zacks and S. Levy, </author> <title> "Inferring articulation and recognizing gestures from acoustics with a neural network trained on x-ray microbeam data", </title> <journal> Journal of the Acoustical Society of America 92, </journal> <pages> pp. 688-700, </pages> <year> 1992 </year>
Reference-contexts: During the subsequent recognition stages, articulatory features are largely used in the same way as acoustic features, i.e. they are input to a second, higher-level classifier detecting standard speech segments like context-dependent phones. The systems in the second category <ref> [27, 34, 37, 38] </ref> use articulatory parameters obtained directly by physical measurements, such as X-ray data.
Reference: [28] <author> M.D. Richard and R.P. Lippmann, </author> <title> "Neural network classifiers estimate Bayesian a posteriori probabilities", </title> <booktitle> Neural Computation 3, </booktitle> <pages> pp. 461-483, </pages> <year> 1991 </year>
Reference-contexts: These are then transformed into likelihoods by division by the class priors [24]. Since the output produced by MLPs can be interpreted as class-conditional a posteriori probabilities <ref> [28] </ref>, hybrid ANN/HMM recognizers readily lend themselves to various probability-theoretic combination schemes which are commonly employed in pattern recognition and machine learning. The most widely used linear probability combination rules are the sum rule and the product rule.
Reference: [29] <author> H.B. Richard, J.S. Mason, M.J. Hunt and J.S. Bridle, </author> <title> "Deriving articulatory representations of speech", </title> <booktitle> Proceedings Eurospeech 95, </booktitle> <pages> pp. 761-764, </pages> <year> 1995 </year>
Reference-contexts: Finally, the fourth approach <ref> [13, 29, 30, 31, 32] </ref> attempts to infer vocal tract shapes from the acoustic signal by nonlinear statistical functions, based on speech production theories like Articulatory Phonology [6] or the Distinctive Regions Model [25]. Each of the above approaches has its strengths and weaknesses.
Reference: [30] <author> H.B. Richard, J.S. Mason, M.J. Hunt and J.S. Bridle, </author> <title> "Deriving articulatory representations of speech with various excitation modes", </title> <booktitle> Proceedings ICSLP-96, </booktitle> <pages> pp. 1233-1236, </pages> <year> 1996 </year>
Reference-contexts: Finally, the fourth approach <ref> [13, 29, 30, 31, 32] </ref> attempts to infer vocal tract shapes from the acoustic signal by nonlinear statistical functions, based on speech production theories like Articulatory Phonology [6] or the Distinctive Regions Model [25]. Each of the above approaches has its strengths and weaknesses.
Reference: [31] <author> H.B. Richards and J.S. Mason, </author> <title> "Imposing dynamic constraints on articulatory representations", </title> <booktitle> Proceedings ICSLP-96, </booktitle> <pages> pp., </pages> <year> 1996 </year>
Reference-contexts: Finally, the fourth approach <ref> [13, 29, 30, 31, 32] </ref> attempts to infer vocal tract shapes from the acoustic signal by nonlinear statistical functions, based on speech production theories like Articulatory Phonology [6] or the Distinctive Regions Model [25]. Each of the above approaches has its strengths and weaknesses.
Reference: [32] <author> H.B. Richards, J.S. Bridle, J.S. Mason, and M.J. Hunt, </author> <title> "Vocal tract shape trajectory estimation using MLP analysis-by-synthesis", </title> <booktitle> Proceedings ICASSP-97, </booktitle> <pages> pp. 1287-1290, </pages> <year> 1997 </year>
Reference-contexts: Finally, the fourth approach <ref> [13, 29, 30, 31, 32] </ref> attempts to infer vocal tract shapes from the acoustic signal by nonlinear statistical functions, based on speech production theories like Articulatory Phonology [6] or the Distinctive Regions Model [25]. Each of the above approaches has its strengths and weaknesses.
Reference: [33] <author> G. Rogova, </author> <title> "Combining the results of several neural network classifiers", </title> <booktitle> Neural Networks 7, </booktitle> <pages> pp. 777-781, </pages> <year> 1994 </year>
Reference-contexts: Combinations of classifiers have been employed for various pattern recognition tasks. The methods by which classifiers are combined include: majority vote [19], class ranking [16], linear combination of a posteriori probabilities [21], local accuracy estimates [36], Dempster-Shafer theory <ref> [33] </ref>, mixture of experts [18]. In a hybrid ANN/HMM recognition system, the HMM decoder uses the Bayes rule P (W jO) = P (O) to compute the probability P (W jO) of a word sequence W given a sequence of acoustic observations O.
Reference: [34] <author> O. Schmidbauer, F. Casacuberta, M.J. Castro, G. Hegerl, H. Hoge, J.A. Sanchez and I. Zlokarnik, </author> <title> "Articulatory representation and speech technology", </title> <booktitle> Language and Speech 36, </booktitle> <pages> pp. 331-351, </pages> <year> 1993 </year>
Reference-contexts: During the subsequent recognition stages, articulatory features are largely used in the same way as acoustic features, i.e. they are input to a second, higher-level classifier detecting standard speech segments like context-dependent phones. The systems in the second category <ref> [27, 34, 37, 38] </ref> use articulatory parameters obtained directly by physical measurements, such as X-ray data.
Reference: [35] <author> M.R. Schroeder, </author> <title> "Determination of the geometry of the human vocal tract by acoustic measurements", </title> <journal> Journal of the Acoustical Society of America 41, </journal> <pages> pp. 1002-1010 </pages>
Reference-contexts: Thus, data-driven reduction of the feature set (e.g. Principal Components Analysis (PCA) or Linear Discriminant Analysis (LDA)) is usually required. Finally, the mapping from acoustics to articulation is not biunique: various articulatory constellations may produce highly similar acoustic signals <ref> [35, 3, 8] </ref>. 4 This entails the problem of reliably estimating articulatory features from the acous-tic signal as well. On the other hand, high detection rates have been reported for articulatory features.
Reference: [36] <author> K. Woods, </author> <title> "Combination of multiple classifiers using local accuracy estimates", </title> <journal> IEEE Trans. on Pattern Analysis and Machine Intelligence 19, </journal> <pages> pp. 405-410, </pages> <year> 1997 </year>
Reference-contexts: Combinations of classifiers have been employed for various pattern recognition tasks. The methods by which classifiers are combined include: majority vote [19], class ranking [16], linear combination of a posteriori probabilities [21], local accuracy estimates <ref> [36] </ref>, Dempster-Shafer theory [33], mixture of experts [18]. In a hybrid ANN/HMM recognition system, the HMM decoder uses the Bayes rule P (W jO) = P (O) to compute the probability P (W jO) of a word sequence W given a sequence of acoustic observations O.
Reference: [37] <author> J. Zacks and T.R. Thomas, </author> <title> "A new neural network for articulatory speech recognition and its application to vowel identification", </title> <booktitle> Computer, Speech and Language 8, </booktitle> <pages> 189-209 </pages>
Reference-contexts: During the subsequent recognition stages, articulatory features are largely used in the same way as acoustic features, i.e. they are input to a second, higher-level classifier detecting standard speech segments like context-dependent phones. The systems in the second category <ref> [27, 34, 37, 38] </ref> use articulatory parameters obtained directly by physical measurements, such as X-ray data. <p> Moreover, there are as yet very few articulatory databases that approximate the size of the corpora typically used in speech recognition. As a consequence, this approach has so far only been applied to fairly small classification tasks, like vowel identification <ref> [37] </ref>. Articulatory preprocessing relies on extracting information specific to certain frequency regions. A major problem of this approach is that some frequency regions may be missing due to bandpass filtering (as in telephone speech) or may be masked by noise.
Reference: [38] <author> I. Zlokarnik, J. Hogden, D. Nix. and G. Papcun, </author> <title> Using articulatory measurements in automatic speech recognition and in speech displays for hearing impaired. Abstract from ACCOR Workshop on Articulatory Databases, </title> <address> Munich, </address> <month> May </month> <year> 1995 </year> <month> 30 </month>
Reference-contexts: During the subsequent recognition stages, articulatory features are largely used in the same way as acoustic features, i.e. they are input to a second, higher-level classifier detecting standard speech segments like context-dependent phones. The systems in the second category <ref> [27, 34, 37, 38] </ref> use articulatory parameters obtained directly by physical measurements, such as X-ray data.
References-found: 38

