URL: ftp://ftp.ai.univie.ac.at/papers/oefai-tr-97-17.ps.Z
Refering-URL: http://www.ai.univie.ac.at/cgi-bin/biblio_ora?sort_by_author=yes&tailor=1&loc=0&format=ml/ml&keyword=Publications&keyword=WWW_ML&relop=/
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Email: stefan@ai.univie.ac.at  
Title: Stochastic Propositionalization of Non-Determinate Background Knowledge  
Author: Stefan Kramer 
Date: April 10, 1997  
Address: Schottengasse 3 A-1010 Vienna, Austria  
Affiliation: Austrian Research Institute for Artificial Intelligence  
Abstract: It is a well-known fact that propositional learning algorithms require "good" features to perform well in practice. So a major step in data engineering for inductive learning is the construction of good features by domain experts. These features often represent properties of structured objects, where a property typically is the occurrence of a certain substructure having certain properties. To partly automate the process of "feature engineering", we devised an algorithm that searches for features which are defined by such substructures. The algorithm stochastically conducts a top-down search for first-order clauses, where each clause represents a binary feature. It differs from existing algorithms in that its search is not class-blind, and that it is capable of considering clauses ("context") of almost arbitrary length (size). Preliminary experiments are favorable, and support the view that this approach is promising.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> P. Auer, W. Maass, and R. Holte. </author> <title> Theory and applications of agnostic pac-learning with small decision trees. </title> <editor> In A. Prieditis and S. Russell, editors, </editor> <booktitle> Proceedings of the 12th International Conference on Machine Learning (ML95). </booktitle> <publisher> Morgan Kaufmann, </publisher> <year> 1995. </year>
Reference-contexts: This is an example of a combination that was found to be deactivating: new_f4 (A) :- functional_group (A, B, methyl), connected (B, C), functional_group (A, C, ring_size_5). After propositionalization, we applied C4.5 [14]. In table 1, we summarize the results for various methods in this domain. T2 <ref> [1] </ref> induces 2-level decision trees. FOIL [12] and Progol [11] 2 are state-of-the-art ILP algorithms. M5 [13] learns regression trees with linear regression models in the leaves. SRT [9] learns relational regression trees. The propositional learning algorithms listed here utilize global features available in addition to the non-determinate background knowledge.
Reference: [2] <editor> W.W. Cohen. Pac-learning nondeterminate clauses. </editor> <booktitle> In Proc. Twelfth National Conference on Artificial Intelligence (AAAI-94), </booktitle> <year> 1994. </year>
Reference-contexts: To bridge this gap, various researchers (e.g., [10]) have proposed a transformation approach. This type of transformation is called propositionalization. In general, full equivalence between the original and the transformed problem can only be achieved for certain subsets of first-order logic and certain types of background knowledge ([10], <ref> [2] </ref>, [3]). But even if there could be equivalent transformations theoretically, most interesting cases would still require feature subset selection. So for pragmatic reasons we should not expect the transformed 1 problem to be equivalent to the original problem. <p> DINUS [10] weakens the language bias of LINUS so that the system can learn clauses with a restricted form of new variables, namely determinate variables. This allows for the same transformation approach as the one taken in LINUS. Cohen <ref> [2] </ref> introduces a new restriction on non-determinate free variables called "locality constraint". This can be thought of in terms of schemata or cliches [15] of literals. Newly introduced non-determinate variables may only be reused in literals within the same instantiated schema or cliche.
Reference: [3] <author> W.W. Cohen. </author> <title> Learning trees and rules with set-valued features. </title> <booktitle> In Proceedings of the Thirteenth National Conference on Artificial Intelligence (AAAI-96), </booktitle> <pages> pages 709-716, </pages> <year> 1996. </year>
Reference-contexts: To bridge this gap, various researchers (e.g., [10]) have proposed a transformation approach. This type of transformation is called propositionalization. In general, full equivalence between the original and the transformed problem can only be achieved for certain subsets of first-order logic and certain types of background knowledge ([10], [2], <ref> [3] </ref>). But even if there could be equivalent transformations theoretically, most interesting cases would still require feature subset selection. So for pragmatic reasons we should not expect the transformed 1 problem to be equivalent to the original problem. <p> Turney [17] described a special purpose program for translating the "trains" problem of the East-West challenge into a propositional representation. Zucker and Ganascia [18] proposed to decompose structured examples into several learning examples, which are descriptions of parts of what they call the "natural example". Cohen <ref> [3] </ref> introduced the notion of "set-valued features", which can be used to transform certain types of background knowledge. Srinivasan and King [16] presented a method for feature construction based on hypotheses returned by Progol [11]. For each clause, each input-output connected subset of literals is used to define a feature.
Reference: [4] <author> D.J. Cook and L.B. Holder. </author> <title> Substructure discovery using minimum description length and background knowledge. </title> <journal> Journal of Artificial Intelligence Research, </journal> <volume> 1 </volume> <pages> 231-255, </pages> <year> 1994. </year> <month> 9 </month>
Reference-contexts: The outer loop of the algorithm employs the more conventional separate-and-conquer strategy. SUBDUE <ref> [4] </ref> is an MDL-based algorithm for substructure discovery in graphs.
Reference: [5] <author> P. Geibel and F. Wysotzki. </author> <title> Relational learning with decision trees. </title> <booktitle> In Proc. Twelfth European Conference on Artificial Intelligence (ECAI-96), </booktitle> <pages> pages 428-432, </pages> <year> 1996. </year>
Reference-contexts: In contrast to all previously discussed methods, this method works for all types of background knowledge. However, it is not yet clear why particularly these features should be useful for transforming relational learning problems. Geibel and Wysotzki <ref> [5] </ref> propose a method for feature construction in a graph-based representation. The features are obtained through fixed-length paths in the neighborhood of a node in the graph. The constructed features are either "context-dependent node attributes of depth n" or "context dependent edge attributes of depth n".
Reference: [6] <author> A. Giordana, L. Saitta, and F. Zini. </author> <title> Learning disjunctive concepts by means of genetic algorithms. </title> <booktitle> In Proceedings of the Eleventh International Conference on Machine Learning, </booktitle> <pages> pages 96-104, </pages> <year> 1994. </year>
Reference-contexts: The overall approach would not work, if the clauses in the population were the same extensionally. In other words, there would be no "division of labour" among the clauses (R3). (This is also the motivation for the universal suffrage selection algorithm presented in <ref> [6] </ref>.) We took a simple extension-driven approach to solve this problem: the algorithm only considers those refinements that yield clauses with an extension different from the extensions of clauses in the current population. <p> The constructed features are either "context-dependent node attributes of depth n" or "context dependent edge attributes of depth n". This method also works in general (for graphs), but using fixed-length paths obviously becomes prohibitive for large n. In contrast to SP, REGAL <ref> [6] </ref> is a concept learning algorithm. It is a full-fledged genetic algorithm. REGAL's universal suffrage selection algorithm is the first extension-driven approach to stochastic search in machine learning. 8 MILP [8] is an ILP algorithm that performs stochastic search for single clauses to overcome the myopic behavior of greedy search.
Reference: [7] <author> R.D. King and A. Srinivasan. </author> <title> Prediction of rodent carcinogenicity bioassays from molecular structure using inductive logic programming. Environmental Health Perspectives, </title> <year> 1997. </year>
Reference-contexts: In most real-world domains, however, the chance of finding the correct concept during propositionalization is very small. So usually the work is divided by the propositionalization algorithm and by the subsequently applied learning algorithm. 3.2 Carcinogenicity Domain Next, we performed experiments in the carcinogenicity domain <ref> [7] </ref>. <p> The hypothesis language of LINUS is restricted to function-free constrained DHDB (deductive hierarchical database) clauses. This implies that no recursion is allowed, and that no new variables may be introduced. 2 The experiment with Progol has been described in <ref> [7] </ref>. 7 Method Accuracy Default 55.00% Ames Test 63.00% C4.5 prune 58.79% C4.5 rules 60.76% T2 65.00% FOIL 25.15% Progol 63.00% SRT 72.46% SP/C4.5 prune 66.78% Table 1: Quantitative results for the carcinogenicity domain obtained by 5-fold cross-validation.
Reference: [8] <author> M. Kovacic. MILP: </author> <title> a stochastic approach to Inductive Logic Programming. </title> <booktitle> In Proceedings of the Fourth International Workshop on Inductive Logic Programming (ILP-94), GMD-Studien Nr. </booktitle> <volume> 237, </volume> <pages> pages 123-138, </pages> <year> 1994. </year>
Reference-contexts: In contrast to SP, REGAL [6] is a concept learning algorithm. It is a full-fledged genetic algorithm. REGAL's universal suffrage selection algorithm is the first extension-driven approach to stochastic search in machine learning. 8 MILP <ref> [8] </ref> is an ILP algorithm that performs stochastic search for single clauses to overcome the myopic behavior of greedy search. The outer loop of the algorithm employs the more conventional separate-and-conquer strategy. SUBDUE [4] is an MDL-based algorithm for substructure discovery in graphs.
Reference: [9] <author> S. Kramer. </author> <title> Structural regression trees. </title> <booktitle> In Proceedings of the Thirteenth National Conference on Artificial Intelligence (AAAI-96), </booktitle> <year> 1996. </year>
Reference-contexts: After propositionalization, we applied C4.5 [14]. In table 1, we summarize the results for various methods in this domain. T2 [1] induces 2-level decision trees. FOIL [12] and Progol [11] 2 are state-of-the-art ILP algorithms. M5 [13] learns regression trees with linear regression models in the leaves. SRT <ref> [9] </ref> learns relational regression trees. The propositional learning algorithms listed here utilize global features available in addition to the non-determinate background knowledge. Quantitatively, SP/C4.5 performs quite well, and the improvement over other propositional algorithms is due to the newly constructed features.
Reference: [10] <author> N. Lavrac and S. Dzeroski. </author> <title> Inductive Logic Programming. </title> <publisher> Ellis Horwood, </publisher> <address> Chichester, UK, </address> <year> 1994. </year>
Reference-contexts: 1 Introduction A very large number of algorithms require a propositional representation, whereas many real-world learning problems are essentially relational. To bridge this gap, various researchers (e.g., <ref> [10] </ref>) have proposed a transformation approach. This type of transformation is called propositionalization. In general, full equivalence between the original and the transformed problem can only be achieved for certain subsets of first-order logic and certain types of background knowledge ([10], [2], [3]). <p> The only algorithms that significantly perform better (M5, SRT) are those which utilize the additional information given in the formulation as a regression problem. 4 Related Work In this section we briefly review related work on propositionalization and stochastic search in machine learning and Inductive Logic Programming. LINUS <ref> [10] </ref> was the first system to transform a relational representation into a propositional representation. The hypothesis language of LINUS is restricted to function-free constrained DHDB (deductive hierarchical database) clauses. <p> DINUS <ref> [10] </ref> weakens the language bias of LINUS so that the system can learn clauses with a restricted form of new variables, namely determinate variables. This allows for the same transformation approach as the one taken in LINUS. Cohen [2] introduces a new restriction on non-determinate free variables called "locality constraint".
Reference: [11] <author> S. Muggleton. </author> <title> Inverse Entailment and Progol. </title> <journal> New Generation Computing, </journal> <volume> 13 </volume> <pages> 245-286, </pages> <year> 1995. </year>
Reference-contexts: After propositionalization, we applied C4.5 [14]. In table 1, we summarize the results for various methods in this domain. T2 [1] induces 2-level decision trees. FOIL [12] and Progol <ref> [11] </ref> 2 are state-of-the-art ILP algorithms. M5 [13] learns regression trees with linear regression models in the leaves. SRT [9] learns relational regression trees. The propositional learning algorithms listed here utilize global features available in addition to the non-determinate background knowledge. <p> Cohen [3] introduced the notion of "set-valued features", which can be used to transform certain types of background knowledge. Srinivasan and King [16] presented a method for feature construction based on hypotheses returned by Progol <ref> [11] </ref>. For each clause, each input-output connected subset of literals is used to define a feature. In contrast to all previously discussed methods, this method works for all types of background knowledge. However, it is not yet clear why particularly these features should be useful for transforming relational learning problems.
Reference: [12] <author> J.R. Quinlan. </author> <title> Learning logical definitions from relations. </title> <journal> Machine Learning, </journal> <volume> 5 </volume> <pages> 239-266, </pages> <year> 1990. </year>
Reference-contexts: Second, the number of literals can be restricted. 1 Third, no negation is used in the clauses, since it often is detrimental to comprehensibility. 3 Experimental Results 3.1 Family Domain We conducted experiments in the family domain <ref> [12] </ref> with the relations son (A,B) and niece (A,B). Although the family domain is determinate, it helped us see which things work and which do not. <p> After propositionalization, we applied C4.5 [14]. In table 1, we summarize the results for various methods in this domain. T2 [1] induces 2-level decision trees. FOIL <ref> [12] </ref> and Progol [11] 2 are state-of-the-art ILP algorithms. M5 [13] learns regression trees with linear regression models in the leaves. SRT [9] learns relational regression trees. The propositional learning algorithms listed here utilize global features available in addition to the non-determinate background knowledge.
Reference: [13] <author> J.R. Quinlan. </author> <title> Learning with continuous classes. </title> <editor> In Sterling Adams, editor, </editor> <booktitle> Proceedings AI'92, </booktitle> <pages> pages 343-348, </pages> <address> Singapore, 1992. </address> <publisher> World Scientific. </publisher>
Reference-contexts: After propositionalization, we applied C4.5 [14]. In table 1, we summarize the results for various methods in this domain. T2 [1] induces 2-level decision trees. FOIL [12] and Progol [11] 2 are state-of-the-art ILP algorithms. M5 <ref> [13] </ref> learns regression trees with linear regression models in the leaves. SRT [9] learns relational regression trees. The propositional learning algorithms listed here utilize global features available in addition to the non-determinate background knowledge.
Reference: [14] <author> J.R. Quinlan. C4.5: </author> <title> Programs for Machine Learning. </title> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, CA, </address> <year> 1993. </year>
Reference-contexts: This is an example of a combination that was found to be deactivating: new_f4 (A) :- functional_group (A, B, methyl), connected (B, C), functional_group (A, C, ring_size_5). After propositionalization, we applied C4.5 <ref> [14] </ref>. In table 1, we summarize the results for various methods in this domain. T2 [1] induces 2-level decision trees. FOIL [12] and Progol [11] 2 are state-of-the-art ILP algorithms. M5 [13] learns regression trees with linear regression models in the leaves. SRT [9] learns relational regression trees.
Reference: [15] <author> G. Silverstein and M.J. Pazzani. </author> <title> Relational cliches: Constraining constructive induction during relational learning. In L.A. </title> <editor> Birnbaum and G.C. Collins, editors, </editor> <booktitle> Machine Learning: Proceedings of the Eighth International Workshop (ML91), </booktitle> <pages> pages 203-207, </pages> <address> San Mateo, CA, 1991. </address> <publisher> Morgan Kauf-mann. </publisher>
Reference-contexts: Subsequently, a literal is selected for the specialization of the chosen parent clause with a probability proportional to the evaluation of the resulting clause. The refinement operator is a kind of specialization using schemata <ref> [15] </ref>. Note that the only operator used in the algorithm is a refinement operator. Currently, the evaluation function employed is the inverse of the chi-square statistic. Due to lack of space, we cannot describe the expected frequencies here in detail. <p> This allows for the same transformation approach as the one taken in LINUS. Cohen [2] introduces a new restriction on non-determinate free variables called "locality constraint". This can be thought of in terms of schemata or cliches <ref> [15] </ref> of literals. Newly introduced non-determinate variables may only be reused in literals within the same instantiated schema or cliche. Turney [17] described a special purpose program for translating the "trains" problem of the East-West challenge into a propositional representation.
Reference: [16] <author> A. Srinivasan and R.D. King. </author> <title> Feature construction with Inductive Logic Programming: a study of quantitative predictions of chemical activity aided by structural attributes. </title> <booktitle> In Proceedings of the 6th International Workshop on Inductive Logic Programming (ILP-96), </booktitle> <year> 1996. </year>
Reference-contexts: Zucker and Ganascia [18] proposed to decompose structured examples into several learning examples, which are descriptions of parts of what they call the "natural example". Cohen [3] introduced the notion of "set-valued features", which can be used to transform certain types of background knowledge. Srinivasan and King <ref> [16] </ref> presented a method for feature construction based on hypotheses returned by Progol [11]. For each clause, each input-output connected subset of literals is used to define a feature. In contrast to all previously discussed methods, this method works for all types of background knowledge.
Reference: [17] <author> P. Turney. </author> <title> Low size-complexity Inductive Logic Programming: the East-West challenge considered as a problem in cost-sensitive classification. </title> <booktitle> In Proceedings of the 5th International Workshop on Inductive Logic Programming (ILP-95), </booktitle> <pages> pages 247-263. </pages> <institution> Katholieke Universiteit Leuven, </institution> <year> 1995. </year> <month> 10 </month>
Reference-contexts: Cohen [2] introduces a new restriction on non-determinate free variables called "locality constraint". This can be thought of in terms of schemata or cliches [15] of literals. Newly introduced non-determinate variables may only be reused in literals within the same instantiated schema or cliche. Turney <ref> [17] </ref> described a special purpose program for translating the "trains" problem of the East-West challenge into a propositional representation. Zucker and Ganascia [18] proposed to decompose structured examples into several learning examples, which are descriptions of parts of what they call the "natural example".
Reference: [18] <author> J.D. Zucker and J.G. Ganascia. </author> <title> Representation changes for efficient learn-ing in structural domains. </title> <booktitle> In Proceedings of the Thirteenth International Conference on Machine Learning, </booktitle> <pages> pages 543-551, </pages> <year> 1996. </year> <month> 11 </month>
Reference-contexts: Newly introduced non-determinate variables may only be reused in literals within the same instantiated schema or cliche. Turney [17] described a special purpose program for translating the "trains" problem of the East-West challenge into a propositional representation. Zucker and Ganascia <ref> [18] </ref> proposed to decompose structured examples into several learning examples, which are descriptions of parts of what they call the "natural example". Cohen [3] introduced the notion of "set-valued features", which can be used to transform certain types of background knowledge.
References-found: 18

