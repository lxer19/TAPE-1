URL: http://www.cs.cornell.edu/Info/People/amaldi/tcs2.ps
Refering-URL: http://www.cs.cornell.edu/Info/Projects/ccop/reports.html
Root-URL: 
Email: amaldi@cs.cornell.edu  viggo@nada.kth.se  
Title: On the approximability of minimizing nonzero variables or unsatisfied relations in linear systems  
Author: Edoardo Amaldi Viggo Kann 
Keyword: Linear systems, unsatisfied relations, nonzero variables, approximability bounds, designing linear classifiers.  
Address: Ithaca, NY 14853  S-100 44 Stockholm  
Affiliation: School of Operations Research and Center for Applied Mathematics Cornell University  Department of Numerical Analysis and Computing Science Royal Institute of Technology  
Abstract: We investigate the computational complexity of two closely related classes of combinatorial optimization problems for linear systems which arise in various fields such as machine learning, operations research and pattern recognition. In the first class (Min ULR) one wishes, given a possibly infeasible system of linear relations, to find a solution that violates as few relations as possible while satisfying all the others. In the second class (Min RVLS) the linear system is supposed to be feasible and one looks for a solution with as few nonzero variables as possible. For both Min ULR and Min RVLS the four basic types of relational operators =, , &gt; and 6= are considered. While Min RVLS with equations was known to be NP-hard in [27], we established in [2, 5] that Min ULR with equalities and inequalities are NP-hard even when restricted to homogeneous systems with bipolar coefficients. The latter problems have been shown hard to approximate in [7]. In this paper we determine strong bounds on the approximability of various variants of Min RVLS and Min ULR, including constrained ones where the variables are restricted to take binary values or where some relations are mandatory while others are optional. The various NP-hard versions turn out to have different approximability properties depending on the type of relations and the additional constraints, but none of them can be approximated within any constant factor, unless P=NP. Particular attention is devoted to two interesting special cases that occur in discriminant analysis and machine learning. In particular, we disprove a conjecture in [64] regarding the existence of a polynomial time algorithm to design linear classifiers (or perceptrons) that involve a close-to-minimum number of features. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> E. Amaldi. </author> <title> On the complexity of training perceptrons. </title> <editor> In T. Kohonen, K. Makisara, O. Sim-ula, and J. Kangas, editors, </editor> <booktitle> Artificial Neural Networks, </booktitle> <pages> pages 55-60, </pages> <address> Amsterdam, 1991. </address> <publisher> Elsevier science publishers B.V. </publisher>
Reference-contexts: Many variants of these combinatorial optimization problems arise in various fields such as operations research [39, 31, 30], pattern recognition [66, 21, 49] and machine learning <ref> [1, 38, 53] </ref>. It is well known that feasible systems with equalities or inequalities can be solved in polynomial time using an adequate linear programming method [44]. But least-square methods are not appropriate for infeasible systems when the objective is to minimize unsatisfied relations. <p> While some heuristic algorithms were devised in [26, 25], Amaldi showed that solving these problems to optimality is NP-hard even when restricted to perceptrons with 1 inputs <ref> [1] </ref>. In [38] minimizing the number of misclassifications was proved at least as hard to approximate as the hitting set problem (see [27]). In recent years a growing attention has been paid to infeasible linear programs [30].
Reference: [2] <author> E. Amaldi. </author> <title> Complexity of problems related to training perceptrons. </title> <editor> In M. Chaleyat-Maurel, M. Cottrell, and J.-C. Fort, editors, Actes du congres "Aspects theoriques des reseaux de neurones", Congres Europeen de Mathematiques. </editor> <address> Paris, </address> <month> July </month> <year> 1992. </year>
Reference-contexts: As we shall see, our results do not depend on the specific value of f , so long as it is fixed a priori. 3.1 Basic versions In <ref> [2, 5] </ref> we proved that Min ULR R with R 2 f=; ; &gt;g is NP-hard even when restricted to homogeneous systems with bipolar coefficients in f1; 1g.
Reference: [3] <author> E. Amaldi. </author> <title> From finding maximum feasible subsystems of linear systems to feedforward neural network design. </title> <type> PhD thesis, </type> <institution> Department of Mathematics, Swiss Federal Institute of Technology, Lausanne, </institution> <year> 1994. </year>
Reference-contexts: The consequences of Theorem 12 on the hardness of designing compact feedforward networks are discussed in detail in <ref> [3] </ref>. From an artificial neural network perspective, Theorem 12 shows that designing close-to-minimum size networks in terms of nonzero weights is very hard even for linearly separable training sets that are performable by the simplest type of networks, namely perceptrons. <p> The non-approximability bounds such as n 1" for any " &gt; 0 makes the existence of any nontrivial approximation algorithm extremely unlikely. It is worth noting that the overall situation for Min ULR differs considerably from that for the complementary class of problems Max FLS (see <ref> [3, 5] </ref>). Unlike for Max FLS, Min ULR with equations and (nonstrict) inequalities are equivalent to approximate.
Reference: [4] <author> E. Amaldi and V. Kann. </author> <title> On the approximability of removing the smallest number of relations from linear systems to achieve feasibility. </title> <type> Technical Report ORWP-6-94, </type> <institution> Department of Mathematics, Swiss Federal Institute of Technology, Lausanne, </institution> <year> 1994. </year>
Reference-contexts: Section 7 contains a summary of the main results and some concluding remarks. An earlier version of this paper appeared as a technical report <ref> [4] </ref>. 2 Approximability of minimization problems An NP optimization (NPO) problem over an alphabet is a four-tuple = (I ; S ; f ; opt ) where I fl is the set of instances, S (I) fl is the set of feasible solutions for instance I 2 I , f :
Reference: [5] <author> E. Amaldi and V. Kann. </author> <title> The complexity and approximability of finding maximum feasible subsystems of linear relations. </title> <journal> Theoretical Computer Science, </journal> <volume> 147 </volume> <pages> 181-210, </pages> <year> 1995. </year>
Reference-contexts: For a list of the currently best approximability upper and lower bounds for optimization problems, see [17]. In <ref> [5] </ref> we performed a thorough study of the approximability of the complementary problems of Min ULR, named Max FLS, where one looks for maximum Feasible subsystems of Linear Systems. <p> As we shall see, our results do not depend on the specific value of f , so long as it is fixed a priori. 3.1 Basic versions In <ref> [2, 5] </ref> we proved that Min ULR R with R 2 f=; ; &gt;g is NP-hard even when restricted to homogeneous systems with bipolar coefficients in f1; 1g. <p> Furthermore, the above proof is much simpler than those given in [7]. Unlike for Max FLS = <ref> [5] </ref>, for Min ULR = we can guarantee in polynomial time a performance ratio that is linear in the number of variables. This fact is mentioned without proof in [7, 8]. <p> In this case, the constrained versions of Min ULR are equally hard to approximate as the corresponding basic versions. It is worth noting that no such relation exists between constrained and unweighted versions of the complementary problems Max FLS. As we proved in <ref> [5] </ref>, enforcing some mandatory relations makes Max FLS with inequalities harder to approximate. <p> Indeed, by applying Gaussian elimination to the mandatory equations, each variable is expressed in terms of other possibly free variables and it then suffices to substitute the variables in the optional relations accordingly. Since C MaxFLS R;6= with R 2 f=; ; &gt;; 6=g is solvable in polynomial time <ref> [5] </ref>, all the problems C Min ULR R;6= are solvable in polynomial time. Fact 5 C Min ULR ;= is equally hard to approximate as Min ULR . According to Fact 1, Min ULR can be reduced to Min ULR = with nonnegative variables. <p> coefficients. 2 The same reduction implies that the complementary maximization problem Max IVLS = (maximum number of Irrelevant Variables in Linear Systems) restricted to homogeneous systems is equally hard to approximate as homogeneous Max FLS = , i.e. not approximable within p " for some " &gt; 0 unless P=NP <ref> [5] </ref>. Interestingly, Max IVLS and Max IVLS &gt; are much harder to approximate than Max FLS and Max FLS &gt; , respectively. <p> It is easy to show that the former problems are harder than the maximum independent set problem (which is not approximable within n 1" for any " &gt; 0 unless NP co-RP, where n is the number of nodes [36]), while the latter ones can 11 be approximated within 2 <ref> [5] </ref>. <p> In the general situation where T is nonlinearly separable, a natural objective is to minimize the number of vectors a k that are misclassified (see [49, 25] and the included references). This problem is referred to as Min Misclassifications. Note that we have studied in <ref> [5] </ref> the approximability of the complementary problem where one looks for a hyperplane which is consistent with as many a k 2 T as possible. <p> The non-approximability bounds such as n 1" for any " &gt; 0 makes the existence of any nontrivial approximation algorithm extremely unlikely. It is worth noting that the overall situation for Min ULR differs considerably from that for the complementary class of problems Max FLS (see <ref> [3, 5] </ref>). Unlike for Max FLS, Min ULR with equations and (nonstrict) inequalities are equivalent to approximate.
Reference: [6] <author> D. Angluin and C. H. Smith. </author> <title> Inductive inference: theory and methods. </title> <journal> ACM Computing Surveys, </journal> <volume> 15 </volume> <pages> 237-269, </pages> <year> 1983. </year>
Reference-contexts: This objective, which is related to the concept of parsimony, is crucial because the number of nonzero parameters of a classifier has a strong impact on its performance on unknown data <ref> [6, 46] </ref>. In [48] a genetic search strategy has been proposed for designing optimal linear classifiers with as few nonzero parameters as possible. Since the late 80's, various complexity classes and approximation preserving reductions have been introduced and used to investigate the approximability of NP-hard optimization problems (see [43]). <p> According to Occam's principle, among all models that account for a given set of data, the simplest ones -with the smallest number of free parameters- are more likely to exhibit good generalization (see for instance <ref> [6, 46] </ref>). The problem of identifying a subset of most relevant features is well known in the statistical discriminant analysis literature under the name of variable selection [54].
Reference: [7] <author> S. Arora, L. Babai, J. Stern, and Z. Sweedyk. </author> <title> The hardness of approximate optima in lattices, codes, and systems of linear equations. </title> <booktitle> In Proc. of 34th Ann. IEEE Symp. on Foundations of Comput. Sci., </booktitle> <pages> pages 724-733. </pages> <publisher> IEEE Comput. Soc., </publisher> <year> 1993. </year>
Reference-contexts: Although complementary pairs of problems such as Min ULR and Max FLS are equivalent to solve optimally, their approximability properties can differ enormously (e.g., the minimum node cover and the maximum independent set problems [9, 27]). In <ref> [7] </ref> Arora, Babai, Stern and Sweedyk established that Min ULR = cannot be approximated within any constant, unless P=NP, and within a factor of 2 log 1" n for any " &gt; 0 unless NP DTIME (n polylog n ) (see also [8]). <p> Finally, any system Ax b has a solution if and only if the system Ax &lt; b + "1 has a solution, where " = 2 2L and L is the size (in bits) of the binary encoded input instance [56]. As previously mentioned, Arora et al. showed in <ref> [7] </ref> (see also [8]) that Min ULR = cannot be approximated within any constant, unless P=NP, and within a factor of 2 log 1" n , for any " &gt; 0, unless NP DTIME (n polylog n ), where n is the number of variables. <p> Furthermore, the above proof is much simpler than those given in <ref> [7] </ref>. Unlike for Max FLS = [5], for Min ULR = we can guarantee in polynomial time a performance ratio that is linear in the number of variables. This fact is mentioned without proof in [7, 8]. <p> Furthermore, the above proof is much simpler than those given in [7]. Unlike for Max FLS = [5], for Min ULR = we can guarantee in polynomial time a performance ratio that is linear in the number of variables. This fact is mentioned without proof in <ref> [7, 8] </ref>. Proposition 3 Min ULR R with R 2 f=; ; &gt;g is approximable within n + 1, where n is the number of variables. <p> This problem is referred to as Min Misclassifications. Note that we have studied in [5] the approximability of the complementary problem where one looks for a hyperplane which is consistent with as many a k 2 T as possible. In <ref> [7] </ref> a way of extending the non-approximability bounds for Min ULR = to the symmetric version of Min Misclassifications where we ask a k w &lt; w 0 for negative examples is suggested. Although the argument used does not suffice to complete the proof, it can easily be fixed. <p> The problem is related to the fact that starting with any instance of Min ULR = we must construct a system with strict inequalities with a particular variable playing the role of the bias w 0 . As mentioned in <ref> [7] </ref>, one can easily associate to any considered instance of Min ULR = an equivalent inhomogeneous instance of Min ULR &gt; . <p> Clearly, any solution of this new system such that w 0 &gt; 0 gives a solution of the original system. Thus by adding a large enough number of copies of w 0 &gt; 0 the two problems are guaranteed to be equivalent. In order to complete the reduction in <ref> [7] </ref>, we just apply this technique to the system consisting of aw + ffi &gt; 1=(2L) inequalities and a large enough number of copies of ffi &lt; 1=L. <p> Table 1 summarizes the non-approximability results that hold for Min ULR variants unless P=NP. The results are valid for inhomogeneous systems with integer coefficients and no pairs Real variables Binary variables Min ULR = not within any constant <ref> [7] </ref> Min ULR Min ULR 6= trivial C Min ULR ; as hard as Min ULR C Min ULR ;&gt; at least as hard as Min ULR &gt; NPO PB-complete C Min ULR ;= at least as hard as Min ULR C Min ULR 6=; C Min ULR 6=;= Min Dominating <p> Arora et al. showed that Min ULR with equalities or inequalities is not approximable within any constant, unless P=NP, and within a factor of 2 log 1" n , for any " &gt; 0, unless NP DTIME (n polylog n ) <ref> [7, 8] </ref>. Using a simple reduction from Min Dominating Set, we have obtained a weaker but more likely logarithmic lower bound for Min ULR with strict and nonstrict inequalities.
Reference: [8] <author> S. Arora, L. Babai, J. Stern, and Z. Sweedyk. </author> <title> The hardness of approximate optima in lattices, codes, and systems of linear equations. </title> <journal> J. Comput. System Sci., </journal> <volume> 54 </volume> <pages> 317-331, </pages> <year> 1997. </year>
Reference-contexts: In [7] Arora, Babai, Stern and Sweedyk established that Min ULR = cannot be approximated within any constant, unless P=NP, and within a factor of 2 log 1" n for any " &gt; 0 unless NP DTIME (n polylog n ) (see also <ref> [8] </ref>). Moreover, they noted that this non-approximability result also holds for systems of inequalities and they suggested a way of extending it to the special case which occurs when minimizing the number of misclassifications of a perceptron. <p> As previously mentioned, Arora et al. showed in [7] (see also <ref> [8] </ref>) that Min ULR = cannot be approximated within any constant, unless P=NP, and within a factor of 2 log 1" n , for any " &gt; 0, unless NP DTIME (n polylog n ), where n is the number of variables. <p> Furthermore, the above proof is much simpler than those given in [7]. Unlike for Max FLS = [5], for Min ULR = we can guarantee in polynomial time a performance ratio that is linear in the number of variables. This fact is mentioned without proof in <ref> [7, 8] </ref>. Proposition 3 Min ULR R with R 2 f=; ; &gt;g is approximable within n + 1, where n is the number of variables. <p> Proof For Min RVLS = , we show that there exists a simple cost preserving reduction from Min ULR = and use the fact that the latter problem cannot be approximated within a factor of 2 log 1" n <ref> [8] </ref>. Let (A; b) be an arbitrary instance of Min ULR = and s 2 R p be the vector of slack variables. <p> To obtain the 2 log 1" n factor, we proceed by self-improvement as in <ref> [8] </ref>. The idea is to show that any gap c &gt; 1 can be increased recursively. <p> The same is true for homogeneous Min RVLS R with R 2 f=; g. Indeed, the reduction for Min ULR = given in <ref> [8] </ref> (cf. also the proof of Theorem 12) can be easily extended to the case of homogeneous systems in which the trivial solution with all zero variables is discarded. <p> Proof To show non-approximability within any constant factor, we adapt the reduction from Min Set Cover used for Min ULR = in <ref> [8] </ref>. <p> Arora et al. showed that Min ULR with equalities or inequalities is not approximable within any constant, unless P=NP, and within a factor of 2 log 1" n , for any " &gt; 0, unless NP DTIME (n polylog n ) <ref> [7, 8] </ref>. Using a simple reduction from Min Dominating Set, we have obtained a weaker but more likely logarithmic lower bound for Min ULR with strict and nonstrict inequalities.
Reference: [9] <author> S. Arora, C. Lund, R. Motwani, M. Sudan, and M. Szegedy. </author> <title> Proof verification and hardness of approximation problems. </title> <booktitle> In Proc. of 33rd Ann. IEEE Symp. on Foundations of Comput. Sci., </booktitle> <pages> pages 14-23. </pages> <publisher> IEEE Comput. Soc., </publisher> <year> 1992. </year> <month> 18 </month>
Reference-contexts: Using a connection with interactive proof systems, strong bounds were derived on the approximability of several famous problems like maximum independent set, minimum graph coloring and minimum set cover <ref> [9, 51, 11, 10, 36, 37] </ref>. For a list of the currently best approximability upper and lower bounds for optimization problems, see [17]. <p> Although complementary pairs of problems such as Min ULR and Max FLS are equivalent to solve optimally, their approximability properties can differ enormously (e.g., the minimum node cover and the maximum independent set problems <ref> [9, 27] </ref>). In [7] Arora, Babai, Stern and Sweedyk established that Min ULR = cannot be approximated within any constant, unless P=NP, and within a factor of 2 log 1" n for any " &gt; 0 unless NP DTIME (n polylog n ) (see also [8]).
Reference: [10] <author> M. Bellare, O. Goldreich, and M. Sudan. </author> <title> Free bits, PCPs and non-approximability towards tight results. </title> <booktitle> In Proc. of 36th Ann. IEEE Symp. on Foundations of Comput. Sci., </booktitle> <pages> pages 422-431. </pages> <publisher> IEEE Comput. Soc., </publisher> <year> 1995. </year>
Reference-contexts: Using a connection with interactive proof systems, strong bounds were derived on the approximability of several famous problems like maximum independent set, minimum graph coloring and minimum set cover <ref> [9, 51, 11, 10, 36, 37] </ref>. For a list of the currently best approximability upper and lower bounds for optimization problems, see [17].
Reference: [11] <author> M. Bellare, S. Goldwasser, C. Lund, and A. Russell. </author> <title> Efficient probabilistically checkable proofs and applications to approximation. </title> <booktitle> In Proc. Twenty fifth Ann. ACM Symp. on Theory of Comp., </booktitle> <pages> pages 294-304. </pages> <publisher> ACM, </publisher> <year> 1993. </year>
Reference-contexts: Using a connection with interactive proof systems, strong bounds were derived on the approximability of several famous problems like maximum independent set, minimum graph coloring and minimum set cover <ref> [9, 51, 11, 10, 36, 37] </ref>. For a list of the currently best approximability upper and lower bounds for optimization problems, see [17]. <p> In [51] Lund and Yannakakis established a lower bound on the approximability of Min Set Cover and of several closely related problems such as Min Dominating Set. In <ref> [11] </ref> Bellare et al. improved this result by showing, among others, that Min Set Cover cannot be approximated within any constant factor unless P=NP. <p> Any such C 0 is a cover of S. If all the sets in C 0 are pairwise disjoint, it is an exact cover. According to <ref> [11] </ref>, for every c &gt; 1, there exists a polynomial time reduction that transforms any instance of the satisfiability problem Sat (see [27]) into an instance of Min Set Cover with a positive integer K such that * if is satisfiable there exists an exact cover C 0 of size K,
Reference: [12] <author> K. P. Bennett and O. L. Mangasarian. </author> <title> Robust linear programming discrimination of two linearly inseparable sets. </title> <journal> Optimization Methods and Software, </journal> <volume> 1 </volume> <pages> 23-34, </pages> <year> 1992. </year>
Reference-contexts: This general procedure can be used to solve Min ULR with any of the four types of relations. During the last decade many mathematical programming formulations have been studied to design linear discriminant classifiers (see <ref> [49, 12, 52] </ref> as well as the included references). When the goal is to determine optimal linear classifiers which misclassify the least number of points in the training set, the problem amounts to a special case of Min ULR &gt; and Min ULR . <p> Increasingly sophisticated models have been proposed in order to try to avoid unacceptable or trivial solutions (see <ref> [12] </ref>). The same type of problem has also attracted a considerable interest in machine learning (artificial neural networks) because it arises when training perceptrons, in particular when minimizing the number of misclassifications.
Reference: [13] <author> J. Chinneck. </author> <title> An effective polynomial-time heuristic for the minimum-cardinality IIS set-covering problem. </title> <journal> Annals of Mathematics and Artificial Intelligence, </journal> <volume> 17 </volume> <pages> 127-144, </pages> <year> 1996. </year>
Reference-contexts: Several methods have been proposed in order to try to locate the source of infeasibility. While earlier ones look for minimal infeasible subsystems [28, 15], the later ones aim at removing as few constraints as possible to achieve feasibility <ref> [60, 57, 58, 13, 29] </ref>. The more practical approach in which the modeler is allowed to weight the constraints according to their importance and flexibility leads to weighted versions of Min ULR [58, 57]. The second class of problems we consider pertains to feasible linear systems.
Reference: [14] <editor> J. Chinneck. Feasibility and viability. In T. Gal and H.J. Greenberg, editors, </editor> <title> Recent advances in sensitivity analysis and parametric programming. </title> <publisher> Kluwer Academic Publishers, </publisher> <year> 1997. </year> <note> To appear. </note>
Reference-contexts: See <ref> [14, 33] </ref> for the problem of analyzing mixed-integer and integer linear programs. Since systems with bounded discrete variables can be reduced to systems with binary variables in f0; 1g, we study the latter class of problems that is referred to as Bin Min ULR.
Reference: [15] <author> J. Chinneck and E. Dravnieks. </author> <title> Locating minimal infeasible constraint sets in linear programs. </title> <journal> ORSA Journal on Computing, </journal> <volume> 3 </volume> <pages> 157-168, </pages> <year> 1991. </year>
Reference-contexts: Infeasible programs with thousands of constraints frequently occur and cannot be repaired by simple inspection. Several methods have been proposed in order to try to locate the source of infeasibility. While earlier ones look for minimal infeasible subsystems <ref> [28, 15] </ref>, the later ones aim at removing as few constraints as possible to achieve feasibility [60, 57, 58, 13, 29]. The more practical approach in which the modeler is allowed to weight the constraints according to their importance and flexibility leads to weighted versions of Min ULR [58, 57].
Reference: [16] <author> V. Chvatal. </author> <title> Linear programming. </title> <publisher> Freeman and Co., </publisher> <address> New York, </address> <year> 1983. </year>
Reference-contexts: This fact is mentioned without proof in [7, 8]. Proposition 3 Min ULR R with R 2 f=; ; &gt;g is approximable within n + 1, where n is the number of variables. Proof When applied to linear systems, Helly's theorem (see <ref> [16] </ref>) implies that, for any infeasible system of inequalities or equations in n variables, all minimal infeasible subsystems contain at most n + 1 relations. Such a Helly obstruction can be found using any polynomial time method for linear programming (LP) [22].
Reference: [17] <author> P. Crescenzi and V. Kann. </author> <title> A compendium of NP optimization problems. </title> <type> Technical Report SI/RR-95/02, </type> <institution> Dipartimento di Scienze dell'Informazione, Universita di Roma "La Sapienza", </institution> <year> 1995. </year> <note> The list is updated continuously. The latest version is available by anonymous ftp from nada.kth.se as Theory/Viggo-Kann/compendium.ps.Z. </note>
Reference-contexts: For a list of the currently best approximability upper and lower bounds for optimization problems, see <ref> [17] </ref>. In [5] we performed a thorough study of the approximability of the complementary problems of Min ULR, named Max FLS, where one looks for maximum Feasible subsystems of Linear Systems.
Reference: [18] <author> P. Crescenzi, V. Kann, R. Silvestri, and L. Trevisan. </author> <title> Structure in approximation classes. </title> <booktitle> In Proc. of 1st International Conf. on Computing and Combinatorics, </booktitle> <pages> pages 539-548. </pages> <publisher> Springer-Verlag, </publisher> <year> 1995. </year> <note> Lecture Notes in Comput. Sci. 959. </note>
Reference-contexts: Furthermore, Kann proved that Min Ind Dom Set is complete for NPO PB in the sense that every polynomially bounded NPO problem can be reduced to it using an approximation preserving reduction <ref> [41, 18] </ref>. 3 Approximability of Min ULR variants In this section we discuss lower and upper bounds on the approximability of the basic versions of Min ULR with the different types of relations and then focus on the weighted as well as constrained variants.
Reference: [19] <author> P. Crescenzi and A. Panconesi. </author> <title> Completeness in approximation classes. </title> <journal> Inform. and Com-put., </journal> <volume> 93(2) </volume> <pages> 241-262, </pages> <year> 1991. </year>
Reference-contexts: See <ref> [19] </ref> for a formal definition. For any instance I and for any feasible solution x 2 S (I) of a minimization problem, the performance ratio of x with respect to the optimum is denoted by R (I; x) = f (I; x)=opt (I).
Reference: [20] <author> P. Crescenzi, R. Silvestri, and L. Trevisan. </author> <title> To weight or not to weight: Where is the question? In Proc. </title> <booktitle> of 4th Israel Symp. on Theory of Computing and Systems, </booktitle> <pages> pages 68-77. </pages> <publisher> IEEE Comput. Soc., </publisher> <year> 1996. </year>
Reference-contexts: Proof Basic Min ULR R is clearly a special case of weighted Min ULR R where all weights are equal to one. For proving the other direction, we first use the following result from <ref> [20] </ref>: For any "nice subset problem" with polynomially bounded weights that is approximable within a polynomial r (n) in the size of the input, the unrestricted version of the same problem where the weights are not polynomially bounded is approximable within r (n) + 1=n.
Reference: [21] <author> R. O. Duda and P. E. Hart. </author> <title> Pattern classification and scene analysis. </title> <publisher> Wiley, </publisher> <address> New York, </address> <year> 1973. </year>
Reference-contexts: Many variants of these combinatorial optimization problems arise in various fields such as operations research [39, 31, 30], pattern recognition <ref> [66, 21, 49] </ref> and machine learning [1, 38, 53]. It is well known that feasible systems with equalities or inequalities can be solved in polynomial time using an adequate linear programming method [44]. But least-square methods are not appropriate for infeasible systems when the objective is to minimize unsatisfied relations. <p> number of variables and relations, unless P=NP [42]. 6 Special cases from discriminant analysis and machine learning In this section we discuss two interesting special cases of Min ULR and Min RVLS with inequalities which arise in discriminant analysis and machine learning, more precisely, when designing 13 two-class linear classifiers <ref> [21] </ref> and when training perceptrons [55].
Reference: [22] <author> J. Edmonds. </author> <title> Redundency and Helly for linear programming. </title> <booktitle> Lecture notes, </booktitle> <month> April </month> <year> 1994. </year>
Reference-contexts: Such a Helly obstruction can be found using any polynomial time method for linear programming (LP) <ref> [22] </ref>. According to Farkas' lemma (see [61]), a system Ax b with p inequalities and n variables is infeasible if and only if there exists a nonnegative vector y 0 such that y t A = 0 and y t b &lt; 0.
Reference: [23] <author> G. Even, J. Naor, B. Schieber, and M. Sudan. </author> <title> Approximating minimum feedback sets and multi-cuts in directed graphs. </title> <booktitle> In Proc. of 4th Int. Conf. on Integer Prog. and Combinatorial Optimization, </booktitle> <pages> pages 14-28. </pages> <publisher> Springer-Verlag, </publisher> <year> 1995. </year> <note> Lecture Notes in Comput. Sci. 920. </note>
Reference-contexts: Since Min Feedback Arc Set is Apx-hard (see for example [40]), it cannot be approximated within every constant unless P = NP. However, it is known to be approximable within O (log n log log n), where n is the number of nodes in the graph <ref> [23] </ref>. 3.2 Weighted and constrained versions In many practical situations, all relations do not have the same importance. This can be taken into account by assigning a weight to each one of them and by looking for a solution that minimizes the total weight of the unsatisfied relations [31, 57].
Reference: [24] <author> U. Feige. </author> <title> A threshold of ln n for approximating set cover. </title> <booktitle> In Proc. Twenty eigth Ann. ACM Symp. on Theory of Comp., </booktitle> <pages> pages 314-318. </pages> <publisher> ACM, </publisher> <year> 1996. </year>
Reference-contexts: In [11] Bellare et al. improved this result by showing, among others, that Min Set Cover cannot be approximated within any constant factor unless P=NP. A stronger lower bound obtained under a stronger assumption was further improved by Feige <ref> [24] </ref> who recently showed that approximating Min Set Cover within (1 ") ln n, for any " &gt; 0, would imply NP DTIME (n log log n ), where n is the number of elements in the ground set.
Reference: [25] <author> M. R. Frean. </author> <title> A thermal perceptron learning rule. </title> <journal> Neural Computation, </journal> <volume> 4 </volume> <pages> 946-957, </pages> <year> 1992. </year> <month> 19 </month>
Reference-contexts: The same type of problem has also attracted a considerable interest in machine learning (artificial neural networks) because it arises when training perceptrons, in particular when minimizing the number of misclassifications. While some heuristic algorithms were devised in <ref> [26, 25] </ref>, Amaldi showed that solving these problems to optimality is NP-hard even when restricted to perceptrons with 1 inputs [1]. In [38] minimizing the number of misclassifications was proved at least as hard to approximate as the hitting set problem (see [27]). <p> In the general situation where T is nonlinearly separable, a natural objective is to minimize the number of vectors a k that are misclassified (see <ref> [49, 25] </ref> and the included references). This problem is referred to as Min Misclassifications. Note that we have studied in [5] the approximability of the complementary problem where one looks for a hyperplane which is consistent with as many a k 2 T as possible.
Reference: [26] <author> S. I. Gallant. </author> <title> Perceptron-based learning algorithms. </title> <journal> IEEE Trans. on Neural Networks, </journal> <volume> 1 </volume> <pages> 179-191, </pages> <year> 1990. </year>
Reference-contexts: The same type of problem has also attracted a considerable interest in machine learning (artificial neural networks) because it arises when training perceptrons, in particular when minimizing the number of misclassifications. While some heuristic algorithms were devised in <ref> [26, 25] </ref>, Amaldi showed that solving these problems to optimality is NP-hard even when restricted to perceptrons with 1 inputs [1]. In [38] minimizing the number of misclassifications was proved at least as hard to approximate as the hitting set problem (see [27]).
Reference: [27] <author> M. R. Garey and D. S. Johnson. </author> <title> Computers and Intractability: a guide to the theory of NP-completeness. </title> <editor> W. H. </editor> <publisher> Freeman and Company, </publisher> <address> San Francisco, </address> <year> 1979. </year>
Reference-contexts: While some heuristic algorithms were devised in [26, 25], Amaldi showed that solving these problems to optimality is NP-hard even when restricted to perceptrons with 1 inputs [1]. In [38] minimizing the number of misclassifications was proved at least as hard to approximate as the hitting set problem (see <ref> [27] </ref>). In recent years a growing attention has been paid to infeasible linear programs [30]. When formulating or modifying very large and complex models, it is hard to prevent errors and to guarantee feasibility. Infeasible programs with thousands of constraints frequently occur and cannot be repaired by simple inspection. <p> Min RVLS = is known to be NP-hard and was referred to as minimum weight solution to linear equations in <ref> [27] </ref>, but nothing is known about its approximability properties. 2 A special case of Min RVLS with strict and nonstrict inequalities is of particular interest in discriminant analysis and machine learning. <p> Although complementary pairs of problems such as Min ULR and Max FLS are equivalent to solve optimally, their approximability properties can differ enormously (e.g., the minimum node cover and the maximum independent set problems <ref> [9, 27] </ref>). In [7] Arora, Babai, Stern and Sweedyk established that Min ULR = cannot be approximated within any constant, unless P=NP, and within a factor of 2 log 1" n for any " &gt; 0 unless NP DTIME (n polylog n ) (see also [8]). <p> Sankaran showed in [60] that the NP-complete problem Min Feedback Arc Set <ref> [27] </ref>, in which one wishes to remove a smallest set of arcs from a directed graph to make it acyclic, reduces to Min ULR with exactly one 1 and one 1 in each row of A and all right hand sides equal to 1. <p> They cannot be approximated within any constant, unless P = NP, and within (1 ") ln n, for any " &gt; 0, unless NP DTIME (n log log n ), where n is the number of variables. Proof We proceed by cost preserving reduction from Min Dominating Set <ref> [27] </ref> similarly to [38]. <p> Note that the shortest codeword problem in coding theory (see MS7 entry in <ref> [27] </ref>) is the same problem as Min RVLS = over GF (2). By similar methods as above this problem can be shown to have the same non-approximability bound as ordinary Min RVLS = . <p> We proceed by reduction from Min Ind Dom Set in which, given an undirected graph G = (V; E), one seeks a minimum cardinality independent set V 0 V that dominates all nodes of G <ref> [27] </ref>. For each node v i 2 V , 1 i n, of an arbitrary instance G = (V; E), we consider the optional inequality x i 0 (7) and the mandatory one x i + j2N (v i ) where N (v i ) is defined as above. <p> Any such C 0 is a cover of S. If all the sets in C 0 are pairwise disjoint, it is an exact cover. According to [11], for every c &gt; 1, there exists a polynomial time reduction that transforms any instance of the satisfiability problem Sat (see <ref> [27] </ref>) into an instance of Min Set Cover with a positive integer K such that * if is satisfiable there exists an exact cover C 0 of size K, * if is unsatisfiable no set cover has size less than bc Kc.
Reference: [28] <author> J. Gleeson and J. Ryan. </author> <title> Identifying minimally infeasible subsystems of inequalities. </title> <journal> ORSA Journal on Computing, </journal> <volume> 3 </volume> <pages> 61-63, </pages> <year> 1990. </year>
Reference-contexts: Infeasible programs with thousands of constraints frequently occur and cannot be repaired by simple inspection. Several methods have been proposed in order to try to locate the source of infeasibility. While earlier ones look for minimal infeasible subsystems <ref> [28, 15] </ref>, the later ones aim at removing as few constraints as possible to achieve feasibility [60, 57, 58, 13, 29]. The more practical approach in which the modeler is allowed to weight the constraints according to their importance and flexibility leads to weighted versions of Min ULR [58, 57].
Reference: [29] <author> H. J. Greenberg. </author> <title> Consistency, redundancy, and implied equalities in linear systems. </title> <journal> Annals of Mathematics and Artificial Intelligence, </journal> <volume> 17 </volume> <pages> 37-83, </pages> <year> 1996. </year>
Reference-contexts: Several methods have been proposed in order to try to locate the source of infeasibility. While earlier ones look for minimal infeasible subsystems [28, 15], the later ones aim at removing as few constraints as possible to achieve feasibility <ref> [60, 57, 58, 13, 29] </ref>. The more practical approach in which the modeler is allowed to weight the constraints according to their importance and flexibility leads to weighted versions of Min ULR [58, 57]. The second class of problems we consider pertains to feasible linear systems.
Reference: [30] <author> H. J. Greenberg and F. H. Murphy. </author> <title> Approaches to diagnosing infeasible linear programs. </title> <journal> ORSA Journal on Computing, </journal> <volume> 3 </volume> <pages> 253-261, </pages> <year> 1991. </year>
Reference-contexts: Many variants of these combinatorial optimization problems arise in various fields such as operations research <ref> [39, 31, 30] </ref>, pattern recognition [66, 21, 49] and machine learning [1, 38, 53]. It is well known that feasible systems with equalities or inequalities can be solved in polynomial time using an adequate linear programming method [44]. <p> In [38] minimizing the number of misclassifications was proved at least as hard to approximate as the hitting set problem (see [27]). In recent years a growing attention has been paid to infeasible linear programs <ref> [30] </ref>. When formulating or modifying very large and complex models, it is hard to prevent errors and to guarantee feasibility. Infeasible programs with thousands of constraints frequently occur and cannot be repaired by simple inspection. Several methods have been proposed in order to try to locate the source of infeasibility.
Reference: [31] <author> R. </author> <title> Greer. Trees and Hills: Methodology for Maximizing Functions of Systems of Linear Relations, </title> <booktitle> volume 22 of Annals of Discrete Mathematics. </booktitle> <publisher> Elsevier science publishing company, </publisher> <address> Amsterdam, </address> <year> 1984. </year>
Reference-contexts: Many variants of these combinatorial optimization problems arise in various fields such as operations research <ref> [39, 31, 30] </ref>, pattern recognition [66, 21, 49] and machine learning [1, 38, 53]. It is well known that feasible systems with equalities or inequalities can be solved in polynomial time using an adequate linear programming method [44]. <p> Greer developed a tree algorithm for optimizing functions of systems of linear relations that is more efficient than complete enumeration but still exponential in the worst case <ref> [31] </ref>. This general procedure can be used to solve Min ULR with any of the four types of relations. During the last decade many mathematical programming formulations have been studied to design linear discriminant classifiers (see [49, 12, 52] as well as the included references). <p> Note that if the number of variables n is constant these three basic versions of Min ULR can be solved in polynomial time using Greer's algorithm which has an O (n p n =2 n1 ) time-complexity, where p denotes the number of relations and n the number of variables <ref> [31] </ref>. These problems are trivial when the number of relations p is constant because all subsystems can be checked in O (n) time. <p> This can be taken into account by assigning a weight to each one of them and by looking for a solution that minimizes the total weight of the unsatisfied relations <ref> [31, 57] </ref>. Proposition 4 Weighted Min ULR R with R 2 f=; ; &gt;g and positive integer (rational) weights is equally hard to approximate as the corresponding basic version. <p> for each relation a number of copies equal to the corresponding weight. 8 The number of relations will still be polynomial since the weights are polynomially bounded. 2 Interesting special cases of weighted Min ULR include the constrained versions where some relations are mandatory while the others are optional (see <ref> [31] </ref> for an example from the field of linear numeric editing). C Min ULR R 1 ;R 2 with R 1 ; R 2 2 f=; ; &gt;; 6=g denotes the variant where the mandatory relations are of type R 1 and the optional ones of type R 2 .
Reference: [32] <author> M. Grigni, V. Mirelli, and C. H. Papadimitriou. </author> <title> On the difficulty of designing good classifiers. </title> <booktitle> In Proc. of 2nd International Conf. on Computing and Combinatorics, </booktitle> <pages> pages 273-279. </pages> <publisher> Springer-Verlag, </publisher> <year> 1996. </year> <note> Lecture Notes in Comput. Sci. 1090. </note>
Reference-contexts: size of the examples n, the same non-approximability bound is also valid with respect to p. 2 Note that, while this paper was being reviewed, Grigni, Mirelli and Papadimitriou addressed the parameterized complexity of designing linear classifiers with a number of nonzero parameters smaller or equal to a given bound <ref> [32] </ref>. The consequences of Theorem 12 on the hardness of designing compact feedforward networks are discussed in detail in [3].
Reference: [33] <author> O. Guieu and J. Chinneck. </author> <title> Analyzing infeasible mixed-integer and integer linear programs. </title> <type> Technical Report Technical report SCE-96-05, </type> <institution> Department of Systems and Computer Engineering, Carleton University, </institution> <address> Ottawa, Canada, </address> <year> 1996. </year>
Reference-contexts: See <ref> [14, 33] </ref> for the problem of analyzing mixed-integer and integer linear programs. Since systems with bounded discrete variables can be reduced to systems with binary variables in f0; 1g, we study the latter class of problems that is referred to as Bin Min ULR.
Reference: [34] <author> M. M. Halldorsson. </author> <title> Approximating the minimum maximal independence number. </title> <journal> Inform. Process. Lett., </journal> <volume> 46 </volume> <pages> 169-172, </pages> <year> 1993. </year>
Reference-contexts: If we require the dominating set in Min Dominating Set to be independent, we get the minimum independent dominating set problem or Min Ind Dom Set. Halldorsson established in <ref> [34] </ref> that, assuming P6=NP, Min Ind Dom Set cannot be approximated within a factor of n 1" for any " &gt; 0, where n is the number of nodes in the graph.
Reference: [35] <author> M. Hassoun. </author> <title> Fundamentals of artificial neural networks. </title> <publisher> MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1995. </year>
Reference-contexts: In other words, we seek a discriminant hyperplane separating the examples in the first class from those in the second class. In the artificial neural network literature, such a linear threshold unit is known as a perceptron and its parameters w j , 1 j n as its weights <ref> [35] </ref>. In the general situation where T is nonlinearly separable, a natural objective is to minimize the number of vectors a k that are misclassified (see [49, 25] and the included references). This problem is referred to as Min Misclassifications. <p> For instance, the O (n d o ) higher-order products of the original features may be included for several values of d 2 <ref> [35] </ref>. Other simple functions of the original features such as radial basis functions are also frequently introduced [35]. Since any training set can be correctly classified given enough additional features, the objective is to minimize the overall number of features that are actually used. <p> For instance, the O (n d o ) higher-order products of the original features may be included for several values of d 2 <ref> [35] </ref>. Other simple functions of the original features such as radial basis functions are also frequently introduced [35]. Since any training set can be correctly classified given enough additional features, the objective is to minimize the overall number of features that are actually used.
Reference: [36] <author> J. H-astad. </author> <title> Clique is hard to approximate within n 1* . In Proc. </title> <booktitle> of 37th Ann. IEEE Symp. on Foundations of Comput. Sci., </booktitle> <pages> pages 627-636. </pages> <publisher> IEEE Comput. Soc., </publisher> <year> 1996. </year>
Reference-contexts: Using a connection with interactive proof systems, strong bounds were derived on the approximability of several famous problems like maximum independent set, minimum graph coloring and minimum set cover <ref> [9, 51, 11, 10, 36, 37] </ref>. For a list of the currently best approximability upper and lower bounds for optimization problems, see [17]. <p> It is easy to show that the former problems are harder than the maximum independent set problem (which is not approximable within n 1" for any " &gt; 0 unless NP co-RP, where n is the number of nodes <ref> [36] </ref>), while the latter ones can 11 be approximated within 2 [5].
Reference: [37] <author> J. H-astad. </author> <title> Some optimal inapproximability results. </title> <booktitle> In Proc. Twenty ninth Ann. ACM Symp. on Theory of Comp. ACM, </booktitle> <year> 1997. </year> <note> To appear. </note>
Reference-contexts: Using a connection with interactive proof systems, strong bounds were derived on the approximability of several famous problems like maximum independent set, minimum graph coloring and minimum set cover <ref> [9, 51, 11, 10, 36, 37] </ref>. For a list of the currently best approximability upper and lower bounds for optimization problems, see [17].
Reference: [38] <author> K-U. Hoffgen, H-U. Simon, and K. van Horn. </author> <title> Robust trainability of single neurons. </title> <journal> J. Comput. System Sci., </journal> <volume> 50 </volume> <pages> 114-125, </pages> <year> 1995. </year>
Reference-contexts: Many variants of these combinatorial optimization problems arise in various fields such as operations research [39, 31, 30], pattern recognition [66, 21, 49] and machine learning <ref> [1, 38, 53] </ref>. It is well known that feasible systems with equalities or inequalities can be solved in polynomial time using an adequate linear programming method [44]. But least-square methods are not appropriate for infeasible systems when the objective is to minimize unsatisfied relations. <p> While some heuristic algorithms were devised in [26, 25], Amaldi showed that solving these problems to optimality is NP-hard even when restricted to perceptrons with 1 inputs [1]. In <ref> [38] </ref> minimizing the number of misclassifications was proved at least as hard to approximate as the hitting set problem (see [27]). In recent years a growing attention has been paid to infeasible linear programs [30]. <p> Proof We proceed by cost preserving reduction from Min Dominating Set [27] similarly to <ref> [38] </ref>. Given an undirected graph G = (V; E), one seeks a minimum cardinality set V 0 V that dominates all nodes of G, i.e. for all v 2 V n V 0 there exists v 0 2 V 0 such that [v; v 0 ] 2 E.
Reference: [39] <author> D. S. Johnson and F. P. Preparata. </author> <title> The densest hemisphere problem. </title> <journal> Theoretical Computer Science, </journal> <volume> 6 </volume> <pages> 93-107, </pages> <year> 1978. </year>
Reference-contexts: Many variants of these combinatorial optimization problems arise in various fields such as operations research <ref> [39, 31, 30] </ref>, pattern recognition [66, 21, 49] and machine learning [1, 38, 53]. It is well known that feasible systems with equalities or inequalities can be solved in polynomial time using an adequate linear programming method [44]. <p> Johnson and Preparata showed that the special cases of Min ULR &gt; and Min ULR with homogeneous systems are NP-hard and devised a complete enumeration method which is also applicable to the weighted and mixed variants <ref> [39] </ref>. Greer developed a tree algorithm for optimizing functions of systems of linear relations that is more efficient than complete enumeration but still exponential in the worst case [31]. This general procedure can be used to solve Min ULR with any of the four types of relations. <p> For homogeneous systems, which have the simplest right-hand sides, we are obviously not interested in trivial solutions where all variables occurring in the satisfied equalities or nonstrict inequalities are zero (see <ref> [39] </ref> for an example). Even if we forbid the solution x = 0, there might be other undesirable solutions where almost all variables occurring in the set of satisfied relations are zero except a few that only occur in a few satisfied relations.
Reference: [40] <author> V. Kann. </author> <title> On the Approximability of NP-complete Optimization Problems. </title> <type> PhD thesis, </type> <institution> Department of Numerical Analysis and Computing Science, Royal Institute of Technology, Stockholm, </institution> <year> 1992. </year>
Reference-contexts: Although various reductions preserving approximability within constants have been proposed (see <ref> [40] </ref>), we will use the S-reduction which is suited to relate problems that cannot be approximated within any constant. <p> In fact, it is readily verified that the two special cases of Min ULR with inequalities are equivalent to Min Feedback Arc Set. Since Min Feedback Arc Set is Apx-hard (see for example <ref> [40] </ref>), it cannot be approximated within every constant unless P = NP.
Reference: [41] <author> V. Kann. </author> <title> Polynomially bounded minimization problems that are hard to approximate. </title> <journal> Nordic J. Computing, </journal> <volume> 1 </volume> <pages> 317-331, </pages> <year> 1994. </year>
Reference-contexts: Although various reductions preserving approximability within constants have been proposed (see [40]), we will use the S-reduction which is suited to relate problems that cannot be approximated within any constant. Definition 1 <ref> [41] </ref> Given two NPO problems and 0 , an S-reduction with size amplification a (n) from to 0 is a four-tuple t = (t 1 ; t 2 ; a (n); c) such that i) t 1 , t 2 are polynomial time computable functions, a (n) is a monotonously increasing <p> Furthermore, Kann proved that Min Ind Dom Set is complete for NPO PB in the sense that every polynomially bounded NPO problem can be reduced to it using an approximation preserving reduction <ref> [41, 18] </ref>. 3 Approximability of Min ULR variants In this section we discuss lower and upper bounds on the approximability of the basic versions of Min ULR with the different types of relations and then focus on the weighted as well as constrained variants. <p> Thus the reduction is an S-reduction with size amplification O (nN ) and we get the non-approximability bound n 0:5" , where n is the number of variables. 2 Note that Bin Min RVLS is equivalent to Min Polynomially Bounded 0-1 Programming, which was shown to be NPO PB-complete in <ref> [41] </ref>.
Reference: [42] <author> V. Kann. </author> <title> Strong lower bounds on the approximability of some NPO PB-complete maximization problems. </title> <booktitle> In Proc. of 20th Ann. Mathematical Foundations of Comput. Sci., </booktitle> <pages> pages 227-236. </pages> <publisher> Springer-Verlag, </publisher> <year> 1995. </year> <note> Lecture Notes in Comput. Sci. 969. 20 </note>
Reference-contexts: Moreover, the corresponding maximization problem Bin Max IVLS R with R 2 f=; ; &gt;; 6=g is NPO PB-complete and cannot be approximated within s 1=3" for any " &gt; 0, where s is the sum of the number of variables and relations, unless P=NP <ref> [42] </ref>. 6 Special cases from discriminant analysis and machine learning In this section we discuss two interesting special cases of Min ULR and Min RVLS with inequalities which arise in discriminant analysis and machine learning, more precisely, when designing 13 two-class linear classifiers [21] and when training perceptrons [55].
Reference: [43] <author> V. Kann and A. Panconesi. </author> <title> Hardness of approximation. </title> <editor> In M. Dell'Amico, F. Maffioli, and S. Martello, editors, </editor> <title> Annotated Bibliographies in Combinatorial Optimization, chapter 2. </title> <publisher> John Wiley & Sons, </publisher> <year> 1997. </year>
Reference-contexts: In [48] a genetic search strategy has been proposed for designing optimal linear classifiers with as few nonzero parameters as possible. Since the late 80's, various complexity classes and approximation preserving reductions have been introduced and used to investigate the approximability of NP-hard optimization problems (see <ref> [43] </ref>). Using a connection with interactive proof systems, strong bounds were derived on the approximability of several famous problems like maximum independent set, minimum graph coloring and minimum set cover [9, 51, 11, 10, 36, 37].
Reference: [44] <author> N. Karmarkar. </author> <title> A new polynomial time algorithm for linear programming. </title> <journal> Combinatorica, </journal> <volume> 4 </volume> <pages> 373-395, </pages> <year> 1984. </year>
Reference-contexts: It is well known that feasible systems with equalities or inequalities can be solved in polynomial time using an adequate linear programming method <ref> [44] </ref>. But least-square methods are not appropriate for infeasible systems when the objective is to minimize unsatisfied relations.
Reference: [45] <author> M. Kearns and L. Valiant. </author> <title> Cryptographic limitations on learning boolean formulae and finite automata. </title> <journal> Journal of the ACM, </journal> <volume> 41 </volume> <pages> 67-95, </pages> <year> 1994. </year>
Reference-contexts: It is worth noting that Kearns and Valiant established in <ref> [45] </ref> a stronger non-approximability bound but under a stronger cryptographic assumption.
Reference: [46] <author> M. Kearns and U. Vazirani. </author> <title> An introduction to Computational Learning Theory. </title> <publisher> MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1994. </year>
Reference-contexts: This objective, which is related to the concept of parsimony, is crucial because the number of nonzero parameters of a classifier has a strong impact on its performance on unknown data <ref> [6, 46] </ref>. In [48] a genetic search strategy has been proposed for designing optimal linear classifiers with as few nonzero parameters as possible. Since the late 80's, various complexity classes and approximation preserving reductions have been introduced and used to investigate the approximability of NP-hard optimization problems (see [43]). <p> According to Occam's principle, among all models that account for a given set of data, the simplest ones -with the smallest number of free parameters- are more likely to exhibit good generalization (see for instance <ref> [6, 46] </ref>). The problem of identifying a subset of most relevant features is well known in the statistical discriminant analysis literature under the name of variable selection [54].
Reference: [47] <author> S. Khanna, M. Sudan, and L. Trevisan. </author> <title> Constraint satisfaction: the approximability of minimization problems. </title> <booktitle> In Proc. 12th Annual IEEE Conf. </booktitle> <institution> Comput. Complexity. IEEE Comput. Soc., </institution> <year> 1997. </year> <note> To appear. </note>
Reference-contexts: By similar methods as above this problem can be shown to have the same non-approximability bound as ordinary Min RVLS = . This result can also be shown directly by using the recent structural results by Khanna, Sudan, and Trevisan <ref> [47] </ref>. In fact, not only Min ULR = is a special case of Min RVLS = but we also have: Proposition 8 Min ULR R with R 2 f=; ; &gt;g is at least as hard to approximate as Min RVLS R with the same type of relations.
Reference: [48] <author> G. J. Koehler. </author> <title> Linear discriminant functions determined by genetic search. </title> <journal> ORSA Journal on Computing, </journal> <volume> 3 </volume> <pages> 345-357, </pages> <year> 1991. </year>
Reference-contexts: The problem occurs when, given a linearly separable set of positive and negative examples, one wants to minimize the number of attributes that are required to correctly classify all given examples <ref> [48, 64] </ref>. This objective, which is related to the concept of parsimony, is crucial because the number of nonzero parameters of a classifier has a strong impact on its performance on unknown data [6, 46]. <p> This objective, which is related to the concept of parsimony, is crucial because the number of nonzero parameters of a classifier has a strong impact on its performance on unknown data [6, 46]. In <ref> [48] </ref> a genetic search strategy has been proposed for designing optimal linear classifiers with as few nonzero parameters as possible. Since the late 80's, various complexity classes and approximation preserving reductions have been introduced and used to investigate the approximability of NP-hard optimization problems (see [43]). <p> The problem occurs when, given a linearly separable training set T , we want to minimize the number of parameters w j , 1 j n, that are required to correctly classify all examples in T <ref> [48, 50, 65] </ref>. This objective plays a crucial role because it 14 has been shown theoretically and experimentally that the number of nonzero parameters has a strong impact on the performance of the classifier (perceptron) for unseen data.
Reference: [49] <author> G. J. Koehler and S. S. Erenguc. </author> <title> Minimizing misclassifications in linear discriminant analysis. </title> <journal> Decision Sciences, </journal> <volume> 21 </volume> <pages> 63-85, </pages> <year> 1990. </year>
Reference-contexts: Many variants of these combinatorial optimization problems arise in various fields such as operations research [39, 31, 30], pattern recognition <ref> [66, 21, 49] </ref> and machine learning [1, 38, 53]. It is well known that feasible systems with equalities or inequalities can be solved in polynomial time using an adequate linear programming method [44]. But least-square methods are not appropriate for infeasible systems when the objective is to minimize unsatisfied relations. <p> This general procedure can be used to solve Min ULR with any of the four types of relations. During the last decade many mathematical programming formulations have been studied to design linear discriminant classifiers (see <ref> [49, 12, 52] </ref> as well as the included references). When the goal is to determine optimal linear classifiers which misclassify the least number of points in the training set, the problem amounts to a special case of Min ULR &gt; and Min ULR . <p> In the general situation where T is nonlinearly separable, a natural objective is to minimize the number of vectors a k that are misclassified (see <ref> [49, 25] </ref> and the included references). This problem is referred to as Min Misclassifications. Note that we have studied in [5] the approximability of the complementary problem where one looks for a hyperplane which is consistent with as many a k 2 T as possible.
Reference: [50] <author> J.-H. Lin and J. S. Vitter. </author> <title> Complexity results on learning by neural nets. </title> <journal> Machine Learning, </journal> <volume> 6 </volume> <pages> 211-230, </pages> <year> 1991. </year>
Reference-contexts: The problem occurs when, given a linearly separable training set T , we want to minimize the number of parameters w j , 1 j n, that are required to correctly classify all examples in T <ref> [48, 50, 65] </ref>. This objective plays a crucial role because it 14 has been shown theoretically and experimentally that the number of nonzero parameters has a strong impact on the performance of the classifier (perceptron) for unseen data. <p> While Lin and Vitter showed that Min Relevant Features with binary inputs is NP-hard <ref> [50] </ref>, van Horn and Martinez established that the symmetric variant with strict inequalities is at least as hard to approximate as Min Set Cover [64, 65].
Reference: [51] <author> C. Lund and M. Yannakakis. </author> <title> On the hardness of approximating minimization problems. </title> <journal> Journal of the ACM, </journal> <volume> 41 </volume> <pages> 960-981, </pages> <year> 1994. </year>
Reference-contexts: Using a connection with interactive proof systems, strong bounds were derived on the approximability of several famous problems like maximum independent set, minimum graph coloring and minimum set cover <ref> [9, 51, 11, 10, 36, 37] </ref>. For a list of the currently best approximability upper and lower bounds for optimization problems, see [17]. <p> In <ref> [51] </ref> Lund and Yannakakis established a lower bound on the approximability of Min Set Cover and of several closely related problems such as Min Dominating Set.
Reference: [52] <author> O. L. Mangasarian. </author> <title> Misclassification minimization. </title> <journal> J. of Global Optimization, </journal> <volume> 5 </volume> <pages> 309-323, </pages> <year> 1994. </year>
Reference-contexts: This general procedure can be used to solve Min ULR with any of the four types of relations. During the last decade many mathematical programming formulations have been studied to design linear discriminant classifiers (see <ref> [49, 12, 52] </ref> as well as the included references). When the goal is to determine optimal linear classifiers which misclassify the least number of points in the training set, the problem amounts to a special case of Min ULR &gt; and Min ULR .
Reference: [53] <author> M. Marchand and M. Golea. </author> <title> On learning simple neural concepts: from halfspace intersections to neural decision lists. Network: </title> <booktitle> Computation in Neural Systems, </booktitle> <volume> 4 </volume> <pages> 67-85, </pages> <year> 1993. </year>
Reference-contexts: Many variants of these combinatorial optimization problems arise in various fields such as operations research [39, 31, 30], pattern recognition [66, 21, 49] and machine learning <ref> [1, 38, 53] </ref>. It is well known that feasible systems with equalities or inequalities can be solved in polynomial time using an adequate linear programming method [44]. But least-square methods are not appropriate for infeasible systems when the objective is to minimize unsatisfied relations.
Reference: [54] <author> G. J. McLachlan. </author> <title> Discriminant analysis and statistical pattern recognition. </title> <publisher> Wiley, </publisher> <address> New York, </address> <year> 1992. </year>
Reference-contexts: The problem of identifying a subset of most relevant features is well known in the statistical discriminant analysis literature under the name of variable selection <ref> [54] </ref>. In practice, when a linear classifier (perceptron) cannot satisfactorily classify the training set based on the original n o features, new features derived from the original ones are added.
Reference: [55] <author> M. L. Minsky and S. Papert. </author> <title> Perceptrons: An introduction to computational Geometry. </title> <publisher> MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1988. </year> <note> Expanded edition. </note>
Reference-contexts: unless P=NP [42]. 6 Special cases from discriminant analysis and machine learning In this section we discuss two interesting special cases of Min ULR and Min RVLS with inequalities which arise in discriminant analysis and machine learning, more precisely, when designing 13 two-class linear classifiers [21] and when training perceptrons <ref> [55] </ref>.
Reference: [56] <author> C. H. Papadimitriou and K. Steiglitz. </author> <title> Combinatorial Optimization, Algorithms and Complexity. </title> <publisher> Prentice Hall, </publisher> <year> 1982. </year>
Reference-contexts: Finally, any system Ax b has a solution if and only if the system Ax &lt; b + "1 has a solution, where " = 2 2L and L is the size (in bits) of the binary encoded input instance <ref> [56] </ref>.
Reference: [57] <author> M. Parker. </author> <title> A set covering approach to infeasibility analysis of linear programming problems and related issues. </title> <type> PhD thesis, </type> <institution> Department of Mathematics, University of Colorado at Denver, </institution> <year> 1995. </year>
Reference-contexts: Several methods have been proposed in order to try to locate the source of infeasibility. While earlier ones look for minimal infeasible subsystems [28, 15], the later ones aim at removing as few constraints as possible to achieve feasibility <ref> [60, 57, 58, 13, 29] </ref>. The more practical approach in which the modeler is allowed to weight the constraints according to their importance and flexibility leads to weighted versions of Min ULR [58, 57]. The second class of problems we consider pertains to feasible linear systems. <p> The more practical approach in which the modeler is allowed to weight the constraints according to their importance and flexibility leads to weighted versions of Min ULR <ref> [58, 57] </ref>. The second class of problems we consider pertains to feasible linear systems. The goal is then to minimize the number of Relevant Variables in the Linear System. <p> This can be taken into account by assigning a weight to each one of them and by looking for a solution that minimizes the total weight of the unsatisfied relations <ref> [31, 57] </ref>. Proposition 4 Weighted Min ULR R with R 2 f=; ; &gt;g and positive integer (rational) weights is equally hard to approximate as the corresponding basic version.
Reference: [58] <author> M. Parker and J. Ryan. </author> <title> Finding the minimum weight IIS cover of an infeasible system of linear inequalities. </title> <journal> Annals of Mathematics and Artificial Intelligence, </journal> <volume> 17 </volume> <pages> 107-126, </pages> <year> 1996. </year>
Reference-contexts: Several methods have been proposed in order to try to locate the source of infeasibility. While earlier ones look for minimal infeasible subsystems [28, 15], the later ones aim at removing as few constraints as possible to achieve feasibility <ref> [60, 57, 58, 13, 29] </ref>. The more practical approach in which the modeler is allowed to weight the constraints according to their importance and flexibility leads to weighted versions of Min ULR [58, 57]. The second class of problems we consider pertains to feasible linear systems. <p> The more practical approach in which the modeler is allowed to weight the constraints according to their importance and flexibility leads to weighted versions of Min ULR <ref> [58, 57] </ref>. The second class of problems we consider pertains to feasible linear systems. The goal is then to minimize the number of Relevant Variables in the Linear System.
Reference: [59] <author> R. Rivest. </author> <title> Cryptography. </title> <editor> In J. van Leeuwen, editor, </editor> <booktitle> Handbook of Theoretical Computer Science, volume A: Algorithms and complexity, </booktitle> <pages> pages 717-755. </pages> <publisher> Elsevier science publishing company, </publisher> <year> 1990. </year> <month> 21 </month>
Reference-contexts: do even for the simplest type of networks is to devise efficient heuristics with good average-case behavior. 1 A trapdoor function T is a one-to-one function such that T and its inverse are easy to evaluate but, given T , the inverse T 1 cannot be constructed in polynomial time <ref> [59] </ref>. 16 7 Conclusions The various versions of Min ULR R and Min RVLS R with R 2 f=; ; &gt;; 6=g that we have considered are obtained by restricting the range of the variables and of the coefficients or by assigning a weight to each relation.
Reference: [60] <author> J. Sankaran. </author> <title> A note on resolving infeasibility in linear programs by constraint relaxation. </title> <journal> Oper. Res. Letters, </journal> <volume> 13 </volume> <pages> 19-20, </pages> <year> 1993. </year>
Reference-contexts: Several methods have been proposed in order to try to locate the source of infeasibility. While earlier ones look for minimal infeasible subsystems [28, 15], the later ones aim at removing as few constraints as possible to achieve feasibility <ref> [60, 57, 58, 13, 29] </ref>. The more practical approach in which the modeler is allowed to weight the constraints according to their importance and flexibility leads to weighted versions of Min ULR [58, 57]. The second class of problems we consider pertains to feasible linear systems. <p> Sankaran showed in <ref> [60] </ref> that the NP-complete problem Min Feedback Arc Set [27], in which one wishes to remove a smallest set of arcs from a directed graph to make it acyclic, reduces to Min ULR with exactly one 1 and one 1 in each row of A and all right hand sides equal <p> Furthermore, they are easy when all maximal feasible subsystems contain a maximum number of relations because a greedy procedure is guaranteed to give a solution that minimizes the number of unsatisfied relations. A polynomial-time solvable special case of Min ULR involving total unimodularity is also mentioned in <ref> [60] </ref>. Before turning to lower and upper bounds on the approximability of Min ULR, we point out a few straightforward facts. Fact 1 Min ULR is at least as hard to approximate as Min ULR = but not harder than Min ULR = with nonnegative variables. <p> This follows using a straightforward modification of the polynomial time reduction from Min Feedback Arc Set to Min ULR with relations given in <ref> [60] </ref>. For each arc (v i ; v j ) in a given instance of Min Feedback Arc Set, we consider the nonstrict inequality x i x j 1 or, respectively, the strict inequality x i x j &gt; 0.
Reference: [61] <author> A. Schrijver. </author> <title> Theory of linear and integer programming. Interscience series in discrete mathematics and optimization. </title> <publisher> Wiley, </publisher> <address> New York, </address> <year> 1986. </year>
Reference-contexts: Such a Helly obstruction can be found using any polynomial time method for linear programming (LP) [22]. According to Farkas' lemma (see <ref> [61] </ref>), a system Ax b with p inequalities and n variables is infeasible if and only if there exists a nonnegative vector y 0 such that y t A = 0 and y t b &lt; 0.
Reference: [62] <author> P. Seymour. </author> <title> Packing directed circuits fractionally. </title> <journal> Combinatorica, </journal> <volume> 15 </volume> <pages> 281-288, </pages> <year> 1995. </year>
Reference-contexts: Acknowledgments The authors thank Peter Jonsson for a discussion on the relationship between Min ULR and the problem of finding optimum acyclic subgraphs of directed graphs as well as David Shmoys for pointing out <ref> [62] </ref>. They are also grateful to the anonymous referees for helpful comments, especially regarding the first part of the proof of Theorem 7.
Reference: [63] <author> L. G. Valiant. </author> <title> A theory of the learnable. </title> <journal> Communications of ACM, </journal> <volume> 27 </volume> <pages> 1134-1142, </pages> <year> 1984. </year>
Reference-contexts: More precisely, for such an Occam algorithm the number of training examples needed to learn in Valiant's Probably Approximately Correct (PAC) sense <ref> [63] </ref> would be almost linear in the minimum number of nonzero parameters s. If s t n this is much less than the O (n) examples required by a simplistic training procedure without feature minimization. The following result provides strong evidence that no such approximation algorithm exists.
Reference: [64] <author> K. van Horn and T. Martinez. </author> <title> The minimum feature set problem. </title> <type> Technical Report CS-92-8, </type> <institution> Computer Science Department, Brigham Young University, Provo, </institution> <year> 1992. </year>
Reference-contexts: The problem occurs when, given a linearly separable set of positive and negative examples, one wants to minimize the number of attributes that are required to correctly classify all given examples <ref> [48, 64] </ref>. This objective, which is related to the concept of parsimony, is crucial because the number of nonzero parameters of a classifier has a strong impact on its performance on unknown data [6, 46]. <p> Moreover, they noted that this non-approximability result also holds for systems of inequalities and they suggested a way of extending it to the special case which occurs when minimizing the number of misclassifications of a perceptron. In <ref> [64, 65] </ref> the variant of Min RVLS with inequalities which arises in discriminant analysis and machine learning was proved to be at least as hard to approximate as the minimum set cover problem. <p> Finally, it was left as an open question whether this number could be approximated within a factor of O (log p) <ref> [64] </ref>. This paper is organized as follows. Section 2 briefly mentions the facts about the approximation of minimization problems used in the sequel. In Section 3 we recall the known approx-imability results for the basic versions of Min ULR and determine alternative upper and lower bounds on their approximability. <p> In particular, we show that no polynomial time algorithm is guaranteed to minimize the number of nonzero parameters of a linear classifier (perceptron) within a logarithmic factor, hereby disproving a conjecture in <ref> [64] </ref>. Section 7 contains a summary of the main results and some concluding remarks. <p> While Lin and Vitter showed that Min Relevant Features with binary inputs is NP-hard [50], van Horn and Martinez established that the symmetric variant with strict inequalities is at least as hard to approximate as Min Set Cover <ref> [64, 65] </ref>. Furthermore, they showed that an approximation algorithm that also minimizes the number of nonzero parameters within a factor of O (log p) would require far fewer examples to achieve a given level of accuracy than any algorithm that does not minimize the number of relevant features. <p> Finally, we have shown that the interesting special case Min Relevant Features, arising when designing linear classifiers and compact perceptrons, is not approximable within a logarithmic factor as conjectured in <ref> [64] </ref>, unless all problems in NP are solvable in quasi-polynomial time. Acknowledgments The authors thank Peter Jonsson for a discussion on the relationship between Min ULR and the problem of finding optimum acyclic subgraphs of directed graphs as well as David Shmoys for pointing out [62].
Reference: [65] <author> K. van Horn and T. Martinez. </author> <title> The minimum feature set problem. </title> <journal> IEEE Trans. on Neural Networks, </journal> <volume> 7 </volume> <pages> 491-494, </pages> <year> 1994. </year>
Reference-contexts: Moreover, they noted that this non-approximability result also holds for systems of inequalities and they suggested a way of extending it to the special case which occurs when minimizing the number of misclassifications of a perceptron. In <ref> [64, 65] </ref> the variant of Min RVLS with inequalities which arises in discriminant analysis and machine learning was proved to be at least as hard to approximate as the minimum set cover problem. <p> The problem occurs when, given a linearly separable training set T , we want to minimize the number of parameters w j , 1 j n, that are required to correctly classify all examples in T <ref> [48, 50, 65] </ref>. This objective plays a crucial role because it 14 has been shown theoretically and experimentally that the number of nonzero parameters has a strong impact on the performance of the classifier (perceptron) for unseen data. <p> While Lin and Vitter showed that Min Relevant Features with binary inputs is NP-hard [50], van Horn and Martinez established that the symmetric variant with strict inequalities is at least as hard to approximate as Min Set Cover <ref> [64, 65] </ref>. Furthermore, they showed that an approximation algorithm that also minimizes the number of nonzero parameters within a factor of O (log p) would require far fewer examples to achieve a given level of accuracy than any algorithm that does not minimize the number of relevant features.
Reference: [66] <author> R. E. Warmack and R. C. Gonzalez. </author> <title> An algorithm for optimal solution of linear inequalities and its application to pattern recognition. </title> <journal> IEEE Trans. on Computers, </journal> <volume> 22 </volume> <pages> 1065-1075, </pages> <year> 1973. </year> <month> 22 </month>
Reference-contexts: Many variants of these combinatorial optimization problems arise in various fields such as operations research [39, 31, 30], pattern recognition <ref> [66, 21, 49] </ref> and machine learning [1, 38, 53]. It is well known that feasible systems with equalities or inequalities can be solved in polynomial time using an adequate linear programming method [44]. But least-square methods are not appropriate for infeasible systems when the objective is to minimize unsatisfied relations.
References-found: 66

