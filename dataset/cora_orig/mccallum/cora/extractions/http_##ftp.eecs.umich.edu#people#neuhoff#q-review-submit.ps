URL: http://ftp.eecs.umich.edu/people/neuhoff/q-review-submit.ps
Refering-URL: http://ftp.eecs.umich.edu/people/neuhoff/
Root-URL: http://www.eecs.umich.edu
Title: FIRST DRAFT Quantization  
Author: Robert M. Gray and David L. Neuhoff 
Date: January 12, 1998  
Abstract: The theory of quantization began in 1898 with Sheppard's study of roundoff error, but its importance for modulation and analog-to-digital conversion was first recognized during the early development of pulse code modulation systems, especially in the 1948 paper of Oliver, Pierce, and Shannon. Also in 1948, Bennett published the first high resolution analysis of quantization and an exact analysis of quantization noise for Gaussian processes, and Shannon published the beginnings of rate-distortion theory, which would provide a theory for quantization as analog-to-digital conversion and as data compression. Beginning with these three papers of fifty years ago, we trace the history of quantization from its origins through this decade, and we survey the fundamentals of the theory and many of the popular and promising techniques for quantization. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> E. Abaya and G.L. Wise, </author> <title> "Some notes on optimal quantization," </title> <booktitle> Proceedings of the International Conference on Communications, </booktitle> <volume> Vol. 2, </volume> <pages> pp. </pages> <address> 30.7.1-30.7.5, </address> <month> June </month> <year> 1981. </year>
Reference: [2] <author> H. Abut, </author> <title> Vector Quantization, IEEE Reprint Collection, </title> <publisher> IEEE Press, </publisher> <address> Piscataway, NJ, </address> <year> 1990. </year>
Reference-contexts: Many of the quantization techniques, results, and applications can be found in original form in Swaszek's 1985 reprint collection on quantization [340] and Abut's 1990 IEEE Reprint Collection on Vector Quantization <ref> [2] </ref>. We close this section with a brief discussion of two specific works which deal with optimizing variable-rate scalar quantizers without additional structure, the problem that leads to the general 23 formulation of optimal quantization in the next section.
Reference: [3] <author> J. P. Adoul and C. Collin and D. Dalle, </author> <title> "Block encoding and its application to data compression of PCM speech Montreal," </title> <booktitle> Proceedings of the Canadian Communications and EHV Conference, </booktitle> <pages> pp. 145-148, </pages> <year> 1978. </year>
Reference-contexts: Also in 1978, Adoul, Collin, and Dalle <ref> [3] </ref> used clustering ideas to design two-dimensional vector quantizers for speech coding. Caprio, Westin, and Esposito in 1978 [61] and Menez, Boeri, and Esteban in 1979 [252] also considered clustering algorithms for the design of vector quantizers with squared error and magnitude error distortion measures.
Reference: [4] <author> J.-P. Adoul, J.-L. Debray, and D. Dalle, </author> <title> "Spectral distance measure applied to the optimum design of DPCM coders with L predictors Signal Processing," </title> <booktitle> Proc. Intl. Conf. on Acoust. Speech, and Signal Processing, </booktitle> <pages> pp. 512-515, </pages> <address> Denver, CO, </address> <year> 1980. </year>
Reference-contexts: The result was an 800 bits per second LPC speech coder with intelligible quality comparable to that scalar quantized LPC speech coders of four times the rate. (See also [378].) In the same year Adoul, Debray, and Dalle <ref> [4] </ref> also used a spectral distance measure to optimize predictors for DPCM and the first thorough study of vector quantization for image compression was published by Yamada, Fujita, and Tazaki [386]. There followed an active period for all facets of quantization theory and design.
Reference: [5] <author> E. Agrell and T. Eriksson, </author> <title> "Optimization of lattices for quantization," </title> <note> submitted for publication, 1996. This work also appears in "Lattice-based quantization," Part I., Report No. 17, </note> <institution> Department of Information Theory, Chalmers University of Technology, Goteborg, Sweden, </institution> <month> Oct. </month> <year> 1996. </year>
Reference-contexts: One lower bound is the normalized moment of inertia of a sphere of the same dimension; another is given in [84]. One upper bound was developed by Zador; others derive from the currently best known tesselations (c.f. <ref> [84, 5] </ref>). The Zador factors fi k and fl k can be computed straightforwardly for k = 1 and, also, for k 2 for IID sources. In some cases, analytical formulas can be found (e.g. Gaussian, Laplacian, gamma densities). In other cases numerical integration can be used. <p> These latter give the best sphere packings and coverings in their respective dimensions. Recently, Agrell 61 and Eriksson <ref> [5] </ref> have found improved lattices in dimensions 9 and 10 dimensions.
Reference: [6] <author> V.R. Algazi, </author> <title> "Useful approximation to optimum quantization," </title> <journal> IEEE Trans. Comm. Tech., </journal> <volume> Vol. 14, </volume> <pages> pp. 297-301, </pages> <year> 1966. </year>
Reference-contexts: Numerous extensions of the Bennett-style asymptotic approximations and the approximation of r (D) or ffi (R) and the characterizations of properties of optimal high resolution quantization for both fixed- and variable-rate quantization for squared error and other error moments appeared during the 1960s, e.g., <ref> [350, 351, 45, 330, 6] </ref>. An excellent summary of the early work is contained in a 1970 paper by Elias [110]. <p> For example, Lloyd [237] used this approach to show that, ignoring overload distortion, the approximation error in the Panter-Dite formula is o (1=N 2 ), which means that it tends to zero, even when divided by 1=N 2 . Roe [311], Algazi <ref> [6] </ref> and Wood [379] also used Taylor series.
Reference: [7] <author> M. R. Anderberg, </author> <title> Cluster Analysis for Applications, </title> <publisher> Academic Press, </publisher> <address> San Diego, </address> <year> 1973. </year>
Reference-contexts: These algorithms were developed for statistical clustering applications, the selection of a finite collection of templates that well represented a large collection of data in the MSE sense, i.e., a fixed-rate VQ with an MSE distortion measure in quantization terminology. (See, e.g., Anderberg <ref> [7] </ref>, Hartigan [178], or Diday and Simon [103].) MacQueen used an incremental incorporation of successive samples of a training set to design the codes, each vector being first mapped into a minimum distortion reproduction level representing a cluster, and then the level for that cluster being replaced by an adjusted centroid.
Reference: [8] <author> J. B. A. Anderson and J. B. Bodie, </author> <title> "Tree encoding of speech," </title> <journal> IEEE Trans. on Information theory, </journal> <volume> Vol. 20, </volume> <pages> pp. 379-387, </pages> <year> 1975. </year>
Reference-contexts: In the early 1970s the algorithms for tree decoding channel codes were inverted to form tree-encoding algorithms for sources by Jelinek, Anderson, and others <ref> [198, 199, 9, 8, 102] </ref>. Later trellis channel decoding algorithms were modified to trellis-encoding algorithms for sources by Viterbi and Omura [368].
Reference: [9] <author> J. B. Anderson and F. Jelinek, </author> <title> "A 2-cycle algorithm for source coding with a fidelity criterion," </title> <journal> IEEE Trans. on Information theory, </journal> <volume> Vol. 19, </volume> <pages> pp. 77-92, </pages> <month> Jan. </month> <year> 1973. </year>
Reference-contexts: In the early 1970s the algorithms for tree decoding channel codes were inverted to form tree-encoding algorithms for sources by Jelinek, Anderson, and others <ref> [198, 199, 9, 8, 102] </ref>. Later trellis channel decoding algorithms were modified to trellis-encoding algorithms for sources by Viterbi and Omura [368].
Reference: [10] <author> M. Antonini, M. Barlaud, P. Mathieu, and I. Daubechies, </author> <title> "Image coding using vector quantization in the wavelet transform domain," </title> <booktitle> in Proceedings ICASSP, </booktitle> <address> Albuquerque, </address> <month> April </month> <year> 1990, </year> <pages> pp. 2297-2300. </pages>
Reference-contexts: The extension of subband filtering from 1-D to 2-D was made by Vetterli [364] and 2-D subband filtering was first applied to image coding by Woods et al. [380, 371, 381]. Early wavelet coding techniques emphasized scalar or lattice vector quantization <ref> [10, 11, 100, 321, 12, 27, 140] </ref> and other vector quantization techniques have also been applied to wavelet coefficients, including tree encoding [261], residual vector quantization [212], and other methods [85].
Reference: [11] <author> M. Antonini, M. Barlaud, and P. Mathieu, </author> <title> "Image coding using lattice vector quantization of wavelet coefficients," </title> <booktitle> in Proceedings ICASSP, </booktitle> <address> Toronto, Canada, </address> <month> May </month> <year> 1991. </year>
Reference-contexts: The extension of subband filtering from 1-D to 2-D was made by Vetterli [364] and 2-D subband filtering was first applied to image coding by Woods et al. [380, 371, 381]. Early wavelet coding techniques emphasized scalar or lattice vector quantization <ref> [10, 11, 100, 321, 12, 27, 140] </ref> and other vector quantization techniques have also been applied to wavelet coefficients, including tree encoding [261], residual vector quantization [212], and other methods [85].
Reference: [12] <author> M. Antonini, M. Barlaud, P. Mathieu, and I. Daubechies, </author> <title> "Image coding using wavelet transform," </title> <journal> IEEE Trans. Image Processing, </journal> <volume> vol. 1, no. 2, </volume> <pages> pp. 205-220, </pages> <month> April </month> <year> 1992. </year>
Reference-contexts: The extension of subband filtering from 1-D to 2-D was made by Vetterli [364] and 2-D subband filtering was first applied to image coding by Woods et al. [380, 371, 381]. Early wavelet coding techniques emphasized scalar or lattice vector quantization <ref> [10, 11, 100, 321, 12, 27, 140] </ref> and other vector quantization techniques have also been applied to wavelet coefficients, including tree encoding [261], residual vector quantization [212], and other methods [85].
Reference: [13] <author> R. Aravind and A. Gersho, </author> <title> "Low-rate image coding with finite-state vector quantization," </title> <journal> pp. </journal> <pages> 137-140, </pages> <booktitle> Proceedings of the Intl. Conf. on Acoustics, Speech, and Signal Processing, </booktitle> <address> Tokyo, </address> <year> 1986. </year>
Reference-contexts: The result is a finite-state version of a predictive quantizer, referred to as a finite-state vector quantizer and depicted in Figure 10. Although little theory has been developed for finite-state quantizers [120, 134, 135], a variety of design methods exist <ref> [130, 131, 105, 176, 13, 14, 207, 149] </ref>. Lloyd's optimal decoder extends in a natural way to finite-state vector quantizers, the optimal reproduction decoder is a conditional expection of the input vector given the binary codeword and the state.
Reference: [14] <author> R. Aravind and A. Gersho, </author> <title> "Image compression based on vector quantization with finite memory," </title> <journal> Optical Engineering, </journal> <volume> Vol. 26, </volume> <pages> pp. 570-580, </pages> <month> July </month> <year> 1987. </year>
Reference-contexts: The result is a finite-state version of a predictive quantizer, referred to as a finite-state vector quantizer and depicted in Figure 10. Although little theory has been developed for finite-state quantizers [120, 134, 135], a variety of design methods exist <ref> [130, 131, 105, 176, 13, 14, 207, 149] </ref>. Lloyd's optimal decoder extends in a natural way to finite-state vector quantizers, the optimal reproduction decoder is a conditional expection of the input vector given the binary codeword and the state.
Reference: [15] <author> D. S. Arnstein, </author> <title> "Quantization error in predictive coders," </title> <journal> IEEE Trans. on Communications, ol. </journal> <volume> 23, </volume> <pages> pp. 423-429, </pages> <month> April </month> <year> 1975. </year>
Reference: [16] <author> E. Ayanoglu and R. M. Gray, </author> <title> "The design of predictive trellis waveform coders using the generalized Lloyd algorithm," </title> <journal> IEEE Trans. on Communications, </journal> <volume> Vol. 34, </volume> <pages> pp. 1073-1080, </pages> <month> Nov. </month> <year> 1986. </year>
Reference-contexts: While linear encoders sufficed for channel coding, nonlinear decoders were required for the source coding application, and a variety of design algorithms were developed for designing the decoder to populate the trellis searched by the encoder <ref> [229, 375, 337, 16, 33] </ref>.
Reference: [17] <author> E. Ayanoglu and R. M. Gray, </author> <title> "The design of joint source and channel trellis waveform coders," </title> <journal> IEEE Trans. on Information theory, pp. </journal> <volume> 855-865, Vol 33, </volume> <month> Nov. </month> <year> 1987. </year>
Reference: [18] <author> R. L. Baker and R. M. Gray, </author> <title> "Image compression using non-adaptive spatial vector quantization," </title> <booktitle> Conference Record of the Sixteenth Asilomar Conference on Circuits Systems and Computers, Asilomar, </booktitle> <address> CA, </address> <month> Oct. </month> <year> 1982. </year>
Reference-contexts: A variety of product vector quantizers use a cartesian product reproduction codebook, which often can be rapidly searched. Examples include polar vector quantizers [291, 292, 47, 376, 345, 346, 339, 342, 344], mean-removed vector quantizers <ref> [18, 19] </ref>, and shape-gain vector quantizers [313, 281]. Trellis-coded quantizers [249, 247, 123, 124, 248, 324] use a Viterbi algorithm encoder matched to a reproduction codebook with a trellis structure. <p> In the case of the shape-gain VQ, the optimal lossy encoder is happily a simple sequential operation, where the gain quantizer is scalar, but the selection depends on the result of another quantizer, the shape quantizer. Similar ideas can be used for mean-removed VQ <ref> [18, 19] </ref> and mean/gain/shape VQ [281]. Fischer's pyramid VQ [121] is also a kind of shape-gainVQ. In this case, the codevectors of the shape codebook are constrained to lie on the surface of a k dimensional pyramid, namely, the set of all vectors whose components have magnitudes summing to one.
Reference: [19] <author> R. L. Baker and R. M. Gray, </author> <title> "Differential vector quantization of achromatic imagery," </title> <booktitle> Proceedings of the International Picture Coding Symposium, </booktitle> <month> March </month> <year> 1983. </year>
Reference-contexts: A variety of product vector quantizers use a cartesian product reproduction codebook, which often can be rapidly searched. Examples include polar vector quantizers [291, 292, 47, 376, 345, 346, 339, 342, 344], mean-removed vector quantizers <ref> [18, 19] </ref>, and shape-gain vector quantizers [313, 281]. Trellis-coded quantizers [249, 247, 123, 124, 248, 324] use a Viterbi algorithm encoder matched to a reproduction codebook with a trellis structure. <p> In the case of the shape-gain VQ, the optimal lossy encoder is happily a simple sequential operation, where the gain quantizer is scalar, but the selection depends on the result of another quantizer, the shape quantizer. Similar ideas can be used for mean-removed VQ <ref> [18, 19] </ref> and mean/gain/shape VQ [281]. Fischer's pyramid VQ [121] is also a kind of shape-gainVQ. In this case, the codevectors of the shape codebook are constrained to lie on the surface of a k dimensional pyramid, namely, the set of all vectors whose components have magnitudes summing to one.
Reference: [20] <author> A.S. Balamesh, </author> <title> "Block-Constrained Methods of Fixed-Rate Entropy Constrained Quantization," </title> <institution> Ph.D.Dissertation, University of Michigan, </institution> <month> Jan. </month> <year> 1993. </year>
Reference-contexts: Scalar-vector Quantization Like permutation vector quantization and Fischer's pyramid vector quantizer, Laroia and Far-vardin's [219] scalar-vector quantization attempts to match the performance of an optimal entropy constrained scalar quantizer with a low complexity fixed-rate structured vector quantizer. A derivative technique called block constrained quantization <ref> [21, 24, 20, 25] </ref> is simpler and easier to describe. Here the reproduction codebook is a subset of the k-fold product of some scalar codebook. <p> For IID Gaussian sources these methods obtain SNR within about 2 dB of ffi (R) with k on the order of 100, which is about .5 dB from the goal of 1.53 dB larger than ffi (R). A high resolution analysis is given in <ref> [23, 20] </ref>. The scalar-vector method extends to sources with memory by combining it with transform coding using a decorrelating or approximately decorrelating transform [219].
Reference: [21] <author> A. S. Balamesh and D.L. Neuhoff, </author> <title> "New methods of fixed-rate entropy-coded quantization," </title> <booktitle> Proc. 1992 Conference on Information Sciences and Systems, </booktitle> <address> Princeton, NJ, </address> <pages> pp. 665-670, </pages> <month> March </month> <year> 1992. </year>
Reference-contexts: Scalar-vector Quantization Like permutation vector quantization and Fischer's pyramid vector quantizer, Laroia and Far-vardin's [219] scalar-vector quantization attempts to match the performance of an optimal entropy constrained scalar quantizer with a low complexity fixed-rate structured vector quantizer. A derivative technique called block constrained quantization <ref> [21, 24, 20, 25] </ref> is simpler and easier to describe. Here the reproduction codebook is a subset of the k-fold product of some scalar codebook.
Reference: [22] <institution> Unpublished notes. </institution>
Reference-contexts: (say rate 10), after choosing the diameter of the support region to minimize this lower bound, it has been found that the dimension k must be larger than 250 in order that the resulting signal to noise ratio be within 1 dB of that predicted by the Shannon distortion-rate function <ref> [22] </ref>. Similar results were reported by Pepin [294]. On the other hand, as mentioned earlier, a quantizer with dimension 12 can achieve this same distortion.
Reference: [23] <author> A.S. Balamesh and D.L. Neuhoff, </author> <title> "Block-constrained quantization: asymptotic analysis," </title> <journal> DIMACS Series in Discrete Math. and Theoretical Computer Science, </journal> <volume> Vol. 14, </volume> <pages> pp. 67-74, </pages> <year> 1993. </year>
Reference-contexts: For IID Gaussian sources these methods obtain SNR within about 2 dB of ffi (R) with k on the order of 100, which is about .5 dB from the goal of 1.53 dB larger than ffi (R). A high resolution analysis is given in <ref> [23, 20] </ref>. The scalar-vector method extends to sources with memory by combining it with transform coding using a decorrelating or approximately decorrelating transform [219].
Reference: [24] <author> A.S. Balamesh and D.L. Neuhoff, </author> <title> "A new fixed-rate quantization scheme based on arithmetic coding," </title> <booktitle> Proc. IEEE International Symposium on Information Theory, </booktitle> <address> San Antonio, p.435, </address> <month> Jan. </month> <year> 1993. </year> <month> 75 </month>
Reference-contexts: Scalar-vector Quantization Like permutation vector quantization and Fischer's pyramid vector quantizer, Laroia and Far-vardin's [219] scalar-vector quantization attempts to match the performance of an optimal entropy constrained scalar quantizer with a low complexity fixed-rate structured vector quantizer. A derivative technique called block constrained quantization <ref> [21, 24, 20, 25] </ref> is simpler and easier to describe. Here the reproduction codebook is a subset of the k-fold product of some scalar codebook.
Reference: [25] <author> A.S. Balamesh and D.L. Neuhoff, </author> <title> "Block-constrained methods of fixed-rate entropy-coded, scalar quantization," </title> <note> submitted to IEEE Trans. Information Theory. </note>
Reference-contexts: Scalar-vector Quantization Like permutation vector quantization and Fischer's pyramid vector quantizer, Laroia and Far-vardin's [219] scalar-vector quantization attempts to match the performance of an optimal entropy constrained scalar quantizer with a low complexity fixed-rate structured vector quantizer. A derivative technique called block constrained quantization <ref> [21, 24, 20, 25] </ref> is simpler and easier to describe. Here the reproduction codebook is a subset of the k-fold product of some scalar codebook.
Reference: [26] <author> G.B. Ball, </author> <title> "Data analysis in the social sciences: what about the details?," </title> <booktitle> Proceedings of the Fall Joint Computing Conference," </booktitle> <pages> pp. 533-559, </pages> <address> Washington, D.C., </address> <publisher> Spartan Books. </publisher>
Reference-contexts: This completed what he and Schutzenberger had begun. In the mid-1960s the optimality properties described by Steinhaus, Lloyd, and Zador and the design algorithm of Steinhaus and Lloyd were rediscovered in the statistical clustering literature. A similar algorithm was introduced in 1965 by Forgey 1965 [128], Ball and Hall <ref> [26, 174] </ref>, Jancey [194], and in 1969 by MacQueen's "k-means" algorithm [245].
Reference: [27] <author> M. Barlaud, P. Sole, T. Gaidon, M. Antonini, and P. Mathieu, </author> <title> "Pyramidal lattice vector quantization for multiscale image coding," </title> <journal> IEEE Transactions on Image Processing, </journal> <volume> vol. 3, </volume> <pages> pp. 367-381, </pages> <month> July </month> <year> 1994. </year>
Reference-contexts: The extension of subband filtering from 1-D to 2-D was made by Vetterli [364] and 2-D subband filtering was first applied to image coding by Woods et al. [380, 371, 381]. Early wavelet coding techniques emphasized scalar or lattice vector quantization <ref> [10, 11, 100, 321, 12, 27, 140] </ref> and other vector quantization techniques have also been applied to wavelet coefficients, including tree encoding [261], residual vector quantization [212], and other methods [85].
Reference: [28] <author> C.F. Barnes, </author> <title> "New multiple path search technique for residual vector quantizers," </title> <booktitle> Proc. Data Compression Conference, </booktitle> <address> Snowbird, UT, </address> <pages> pp. 42-51, </pages> <year> 1994. </year>
Reference-contexts: More sophisticated (than greedy) encoding algorithms can take advantage of the direct sum nature of the codebook to make optimal or nearly optimal searches, though with some (and sometimes a great deal) of increased complexity. And more sophisticated design algorithms (than the greedy one) can also have benefits <ref> [29, 133, 28, 30] </ref>. Variable-rate multistage quantizers have been developed [182, 213, 214, 309].
Reference: [29] <author> C.F. Barnes and R.L. Frost, </author> <title> "Vector quantizers with direct sum codebooks," </title> <journal> IEEE Trans. on Information theory, </journal> <volume> Vol. 39, No. 2, </volume> <pages> pp. 565-580, </pages> <month> March </month> <year> 1993. </year>
Reference-contexts: Tree-structured VQ uses a tree-structured reproduction codebook with a matched tree-structured search algorithm [58, 308, 74]. A tree-structured VQ with far less memory is provided by a multistage or residual VQ <ref> [203, 182, 29, 223, 133, 66, 29, 289, 309, 30] </ref>. A variety of product vector quantizers use a cartesian product reproduction codebook, which often can be rapidly searched. <p> More sophisticated (than greedy) encoding algorithms can take advantage of the direct sum nature of the codebook to make optimal or nearly optimal searches, though with some (and sometimes a great deal) of increased complexity. And more sophisticated design algorithms (than the greedy one) can also have benefits <ref> [29, 133, 28, 30] </ref>. Variable-rate multistage quantizers have been developed [182, 213, 214, 309].
Reference: [30] <author> C.F. Barnes, S.A. Rizvi, and N.M. Nasrabadi, </author> <title> "Advances in residual vector quantization: a review," </title> <journal> IEEE Trans. on Image Processing, </journal> <volume> Vol. 5, </volume> <pages> pp. 226-262, </pages> <month> Feb. </month> <year> 1996. </year>
Reference-contexts: Tree-structured VQ uses a tree-structured reproduction codebook with a matched tree-structured search algorithm [58, 308, 74]. A tree-structured VQ with far less memory is provided by a multistage or residual VQ <ref> [203, 182, 29, 223, 133, 66, 29, 289, 309, 30] </ref>. A variety of product vector quantizers use a cartesian product reproduction codebook, which often can be rapidly searched. <p> More sophisticated (than greedy) encoding algorithms can take advantage of the direct sum nature of the codebook to make optimal or nearly optimal searches, though with some (and sometimes a great deal) of increased complexity. And more sophisticated design algorithms (than the greedy one) can also have benefits <ref> [29, 133, 28, 30] </ref>. Variable-rate multistage quantizers have been developed [182, 213, 214, 309].
Reference: [31] <author> C. W. Barnes, B. N. Tran, S. H. Leung," </author> <title> "On the statistics of fixed-point roundoff error," </title> <journal> IEEE Trans. Acoustics, Speech, and Signal Processing, </journal> <volume> Vol. 3, </volume> <pages> pp. 595-606, </pages> <month> June </month> <year> 1985. </year>
Reference: [32] <author> C.D. </author> <title> Bei and R.M. Gray, "An improvement of the minimum distortion encoding algorithm for vector quantization," </title> <journal> IEEE Transactions on Communications, </journal> <pages> pp. 1132-1133, </pages> <month> October </month> <year> 1985. </year>
Reference-contexts: We here only mention several examples with references and leave further discussion to Section 5. Fast search algorithms have been developed for unstructured reproduction codebooks <ref> [71, 335, 32, 67, 149] </ref>. Even faster searches are possible when the reproduction codebook is constrained to have a simple structure, for example to be a subset of points of a regular lattice as in a lattice vector quantizer [147, 81, 82, 83, 84, 151, 116]. <p> Some quantitative analysis of the increased distortion is given in [253] for a case where the prequantization is a lattice quantizer. Other fast search methods include the partial distortion method of <ref> [71, 32] </ref> and the transform subspace domain approach of [67]. Consideration of methods based on prequantization leads to the question of how fine should the prequantization cells be.
Reference: [33] <author> C.D. </author> <title> Bei and R.M. Gray, "Simulation of vector trellis encoding systems," </title> <journal> IEEE Trans. Commun., </journal> <volume> Vol. 34, </volume> <pages> pp. 214-218, </pages> <month> March </month> <year> 1986. </year>
Reference-contexts: While linear encoders sufficed for channel coding, nonlinear decoders were required for the source coding application, and a variety of design algorithms were developed for designing the decoder to populate the trellis searched by the encoder <ref> [229, 375, 337, 16, 33] </ref>.
Reference: [34] <author> P. Bello, R. Lincoln, and H. Gish, </author> <title> "Statistical delta modulation," </title> <journal> Proc. IEEE, </journal> <volume> Vol. 55, </volume> <pages> pp. 308-319, </pages> <month> March </month> <year> 1967. </year>
Reference-contexts: Conditions for use in code design resembling the Lloyd optimality conditions have been studied for feedback quantization <ref> [120, 154, 34, 134] </ref>, but the conditions are not optimality conditions in the Lloyd sense, i.e., they are not necessary conditions for a quantizer within a feedback loop to yield the minimum average distortion subject to a rate constraint.
Reference: [35] <author> W.R. Bennett, </author> <title> "Spectra of quantized signals," </title> <journal> Bell Systems Technical Journal, </journal> <volume> Vol. 27, </volume> <pages> pp. 446-472, </pages> <month> July </month> <year> 1948. </year>
Reference-contexts: The second approach is that of high resolution (or high rate or asymptotic) quantization theory, which had its origins in the 1948 paper on PCM by Oliver, Pierce, and Shannon [282] and the 1948 paper on quantization error spectra by Bennett <ref> [35] </ref> and its subsequent development in 1951 by Panter and Dite [290] and in 1957 by Lloyd [237]. Much of the history and state of the art of quantization derives from these seminal works, half of which first appeared in 1948. <p> The oldest such results are the exact analyses for special nonasymptotic cases, such as Clavier, Panter, and Grieg's 1947 analysis of the spectra of the quantization error for uniformly quantized sinusoidal signals [78] and Bennett's 1948 derivation of the power spectral density of a uniformly quantized Gaussian random process <ref> [35] </ref>. The most important nonasymptotic results, however, are the basic optimality conditions and iterative descent algorithms for quantizer design, such as first developed by Steinhaus (1956) [325] and Lloyd (1957) [237], and later popularized by Max (1960) [250]. <p> The complicated sums of Bessel functions resembled the early analyses of another nonlinear modulation technique, FM, and left little hope for general closed forms solutions for interesting signals. The first general contributions to quantization theory came in 1948 with the papers of Oliver, Pierce, and Shannon [282] and Bennett <ref> [35] </ref>. As part of their analysis of PCM for communications, they developed the oft-quoted result that for large rate or resolution, a uniform quantizer with cell width yields average distortion D (q) = 2 =12. <p> Asymptotic Distortion As mentioned earlier, the first and most elementary result in high resolution theory is the 2 =12 approximation to the mean squared error of a uniform scalar quantizer with step size <ref> [329, 282, 35] </ref>, which we now derive. Consider an N -level uniform quantizer q whose levels are y 1 ; : : : ; y N , with y i = y i1 + . <p> to the distortion of a vector quantizer D = N 2=k m (x) f k (x) dx: (23) For scalar quantizers (k = 1) with points in the middle of the cells, this reduces to D = 12 N 2 1 f 1 (x) dx (24) which is what Bennett <ref> [35] </ref> found for companders, as restated in terms of point densities by Lloyd [237]. Both (24) and the more general formula (23) are called Bennett's integral. <p> Similar comments apply to informal vs. rigorous analyses of asymptotic entropy. In the following we review the development of rigorous theory. Many analyses informal and rigorous explicitly assume the source has finite range (i.e. a probability distribution with bounded support); so there is no overload distortion to be ignored <ref> [35, 290, 334] </ref>. In some cases the source has finite range. In others, for example speech and images, the source samples have infinite range, but the measurement device has finite range. <p> We view that this differs only stylistically from an explicit assumption of finite support, for both approaches ignore overload distortion. However, assuming finite support is, arguably, humbler and mathematically more honest. The earliest quantizer distortion analyses to appear in the open literature <ref> [35, 290, 334] </ref> assumed finite range and used the density-approximately-constant-in-cells assumption. Several papers avoided the latter by using a Taylor series expansion of the source density. <p> Both theories have been extended to to continuous-time random processes. However, the high-resolution results are somewhat sketchy <ref> [35, 237, 155] </ref>. Both can be applied to two or higher dimensional sources such as images or video. Both have have been developed the most for Gaussian sources in the context of squared error distortion, which is not surprising in view of the tractability of squared error and Gaussianity.
Reference: [36] <author> J.L. Bentley, </author> <title> "Multidimensional binary search trees used for associative searching," </title> <journal> Comm. ACM, </journal> <pages> pp. 209-226, </pages> <month> Sept. </month> <year> 1975. </year>
Reference-contexts: Then to encode a source vector x, one applies the prequantization, finds the index of the prequantization cell in which x is contained, and performs a full search on the corresponding bucket for the closest codevector to x. Techniques of this type may be found in <ref> [36, 132, 71, 72, 239, 112] </ref>. Another class of techniques is like the previous except that the low complexity prequantization has much smaller cells than the Voronoi cells of C, i.e. it is finer.
Reference: [37] <author> T. Berger, </author> <title> Rate Distortion Theory, </title> <address> Prentice-Hall,Englewood Cliffs, NJ, </address> <year> 1971. </year>
Reference-contexts: Accordingly, the theory is often called rate-distortion theory, c.f. <ref> [37] </ref>. <p> This is not usually the case for other sources. Shannon's approach was subsequently generalized to sources with memory. (See, e.g., <ref> [136, 37, 165, 40] </ref>.) The general definitions of distortion-rate and rate-distortion functions resemble those for operational distortion-rate and rate-distortion functions in that they are infima of kth order functions. <p> It is well known that D slb (R)=D (R) approaches one as R increases <ref> [37, 232] </ref>, which is entirely consistent with the fact that Z (R)=ffi (R) approaches one as R increases. Such relationships are summarized below. The inequalities marked with a "*" become tight as dimension k increases, and those marked with a "+" become tight as R increases. <p> The Turing complexity of higher dimensional quantizers is as yet unknown. Computability First-order Shannon distortion-rate functions can be computed analytically for squared error and several source densites, such as Gaussian, Laplacian and uniform (c.f. <ref> [37, 164] </ref>). For sources with memory, higher order Shannon distortion-rate functions are known only for Gaussian sources and for some discrete Markov sources at small distortions. <p> For other cases, the Blahut algorithm [42] can be used to compute D k (R), though its computational complexity becomes overwhelming unless k is small. Due to the difficulty of computing it, many (mostly lower) bounds to the Shannon distortion-rate function have been developed (c.f. <ref> [327, 235, 37, 164] </ref>). One important upper bound derives from the fact that with respect to squared error, the Gaussian source has the largest Shannon distortion-rate function (kth-order or in the limit) of any source with the same covariance function.
Reference: [38] <author> T. Berger, </author> <title> "Optimum quantizers and permutation codes," </title> <journal> IEEE Transactions on Information Theory, </journal> <volume> Vol. 18, No. 6, </volume> <pages> pp. 759-765, </pages> <year> 1972. </year>
Reference-contexts: In 1972 Berger, Jelinek and Wolf [41] and Berger <ref> [38] </ref> considered the permutation source codes introduced by Dunn [106] and developed lower complexity encoding algorithms. Berger [38] showed that for large block sizes, the rate-distortion tradeoffs of the fixed-rate permutation codes were approximately those of optimal variable-rate scalar quantization. <p> In 1972 Berger, Jelinek and Wolf [41] and Berger <ref> [38] </ref> considered the permutation source codes introduced by Dunn [106] and developed lower complexity encoding algorithms. Berger [38] showed that for large block sizes, the rate-distortion tradeoffs of the fixed-rate permutation codes were approximately those of optimal variable-rate scalar quantization. In the derivation he described Lloyd-like conditions for optimality of an entropy constrained scalar quantizer for squared error distortion. <p> We close this section with a brief discussion of two specific works which deal with optimizing variable-rate scalar quantizers without additional structure, the problem that leads to the general 23 formulation of optimal quantization in the next section. In 1984 Farvardin and Modestino [117] extended Berger's <ref> [38] </ref> necessary conditions for optimality of an entropy-constrained scalar quantizer to more general distortion measures and described two design algorithms: the first is similar to Berger's iterative algorithm, but the second was a fixed-point algorithm which can be considered as a natural extension on Lloyd's Method I from fixed-rate to variable-rate <p> In 1989 Chou et al. [76] developed a generalized Lloyd algorithm for constrained entropy vector quantization that generalized Berger's <ref> [38, 39] </ref> Lagrangian formulation for scalar quantization and Farvardin and Modestino's fixed-point design algorithm [117] to vectors. Optimality properties for minimizing a Lagrangian distortion D (q) + R (q) were derived, where rate could be either average length or entropy. <p> This is a good place to again mention Gish and Pierce's result that if the rate is high, optimal entropy-constrained scalar or vector quantization can provide no more than roughly 1/4 bit improvement over uniform scalar quantization with block entropy coding. Berger <ref> [38] </ref> showed that permutation codes achieved roughly the same performance with a fixed-rate vector quantizer.
Reference: [39] <author> T. Berger, </author> <title> "Minimum entropy quantizers and permutation codes," </title> <journal> IEEE Transactions on Information Theory, </journal> <volume> Vol. 28, No. 2, </volume> <pages> pp. 149-157, </pages> <year> 1982. </year>
Reference-contexts: special issue on Quantization of these Transactions 3 , which published the Bell Laboratories Technical Memos of Lloyd, Newman, and Zador along with Berger's extension of the optimality properties of entropy-constrained scalar quantization to rth power distortion measures and his extensive comparison of minimum entropy quantizers and fixed-rate permutation codes <ref> [39] </ref>, generalizations by Trushkin of Fleischer's conditions for uniqueness of local optima [356], results on the asymptotic behavior 3 The idea for a special issue on Quantization was first proposed by by Neil Sloane at the 1980 Allerton Conference. 22 of Lloyd's algorithm with training sequence size based on the theory <p> In 1989 Chou et al. [76] developed a generalized Lloyd algorithm for constrained entropy vector quantization that generalized Berger's <ref> [38, 39] </ref> Lagrangian formulation for scalar quantization and Farvardin and Modestino's fixed-point design algorithm [117] to vectors. Optimality properties for minimizing a Lagrangian distortion D (q) + R (q) were derived, where rate could be either average length or entropy.
Reference: [40] <author> T. Berger, </author> <title> "Source Coding," </title> <note> this issue. </note>
Reference-contexts: Some such material has already been introduced and more will be introduced in Section 2. However, for completeness, Section 3 will be largely selfcontained. Section 4 reviews the development of quantization theory. As another paper in this issue is devoted specifically to Shannon's source coding theory <ref> [40] </ref>, our 4 emphasis will be on high resolution theory and on those aspects of Shannon information theory pertaining specifically to quantization. We also offer a comparison of the two theories. Finally, Section 5 describes a number of specific quantization techniques. <p> This is not usually the case for other sources. Shannon's approach was subsequently generalized to sources with memory. (See, e.g., <ref> [136, 37, 165, 40] </ref>.) The general definitions of distortion-rate and rate-distortion functions resemble those for operational distortion-rate and rate-distortion functions in that they are infima of kth order functions.
Reference: [41] <author> T. Berger, F. Jelinek, and J.K. Wolf, </author> <title> "Permutation codes for sources," </title> <journal> IEEE Transactions on Information Theory, </journal> <volume> Vol. 18, </volume> <pages> pp. 160-169, </pages> <month> Jan. </month> <year> 1972. </year>
Reference-contexts: In 1972 Berger, Jelinek and Wolf <ref> [41] </ref> and Berger [38] considered the permutation source codes introduced by Dunn [106] and developed lower complexity encoding algorithms. Berger [38] showed that for large block sizes, the rate-distortion tradeoffs of the fixed-rate permutation codes were approximately those of optimal variable-rate scalar quantization.
Reference: [42] <author> R. E. Blahut, </author> <title> "Computation of channel capacity and rate-distortion functions," </title> <journal> IEEE Transactions on Information Theory, </journal> <volume> Vol. 18. </volume> <pages> pp. 460-473, </pages> <year> 1972. </year>
Reference-contexts: Shannon's distortion-rate function is analytically computable for several interesting sources, including IID Gaussian and discrete alphabet sources, and it can be evaluated numerically <ref> [42] </ref> or bounded by the Shannon or other lower bounds. This contrasts with the operational distortion-rate functions, which are generally not directly computable. Thus Shannon's results, like those of Panter, Dite, and Lloyd, provide benchmarks for comparison for quantizers. <p> For sources with memory, higher order Shannon distortion-rate functions are known only for Gaussian sources and for some discrete Markov sources at small distortions. For other cases, the Blahut algorithm <ref> [42] </ref> can be used to compute D k (R), though its computational complexity becomes overwhelming unless k is small. Due to the difficulty of computing it, many (mostly lower) bounds to the Shannon distortion-rate function have been developed (c.f. [327, 235, 37, 164]).
Reference: [43] <author> L. Breiman, J. H. Friedman, R. A. Olshen, and C. J. Stone, </author> <title> Classification and Regression Trees. </title> <address> Belmont, CA: </address> <publisher> Wadsworth, </publisher> <year> 1984. </year>
Reference-contexts: A tree structured quantizer is analogous to a classification or regression tree, and as such unbalanced TSVQs can be designed by algorithms based on a gardening metaphor of growing and pruning. The most well known is the CART algorithm of Breiman, Friedman, Olshen, and Stone <ref> [43] </ref>, and the variation of CART for designing TSVQs bears their initials: the BFOS algorithm [74, 306, 149]. In this method, a balanced or unbalanced tree with more leaves than needed is first grown and then pruned.
Reference: [44] <author> Lee K. Brinton, "Nonsubtractive Dither," </author> <title> M.S. </title> <type> Thesis, </type> <institution> Electrical Engineering Department, University of Utah, </institution> <address> Salt Lake City, Utah, </address> <month> Aug. </month> <year> 1984. </year>
Reference-contexts: The properties of nonsubtractive dither were originally developed in unpublished work by Wright [382] in 1979 and Brinton <ref> [44] </ref> in 1984 and subsequently extended and refined with a variety of proofs [363, 362, 236, 172].
Reference: [45] <author> J.D. Bruce, </author> <title> "On the optimum quantization of stationary signals," </title> <booktitle> 1964 IEEE Int. Conv. Rec., </booktitle> <volume> Part 1, </volume> <pages> pp. 118-124, </pages> <year> 1964. </year>
Reference-contexts: Numerous extensions of the Bennett-style asymptotic approximations and the approximation of r (D) or ffi (R) and the characterizations of properties of optimal high resolution quantization for both fixed- and variable-rate quantization for squared error and other error moments appeared during the 1960s, e.g., <ref> [350, 351, 45, 330, 6] </ref>. An excellent summary of the early work is contained in a 1970 paper by Elias [110].
Reference: [46] <author> J.A. Bucklew and N.C. Gallagher, Jr., </author> <title> "A note on optimum quantization," </title> <journal> IEEE Transactions on Information Theory, </journal> <volume> Vol. 25, </volume> <pages> pp. 365-366, </pages> <year> 1979. </year>
Reference-contexts: The centroid property of optimal reproduction decoders has interesting implications in the special case of a squared error distortion measure, where it follows easily <ref> [106, 147, 46, 139, 149] </ref> that * E [q (X)] = E [X], so that the quantizer output can be considered as an unbiased estimator of the input. * E [q (X) t (q (X) X)] = 0, so that the quantizer output is orthogonal of the quantizer error, an example
Reference: [47] <author> J.A. Bucklew and N.C. Gallagher, Jr., </author> <title> "Quantization schemes for bivariate Gaussian random variables," </title> <journal> IEEE Transactions on Information Theory, </journal> <volume> Vol. 25, </volume> <pages> pp. 537-543, </pages> <year> 1979. </year>
Reference-contexts: A tree-structured VQ with far less memory is provided by a multistage or residual VQ [203, 182, 29, 223, 133, 66, 29, 289, 309, 30]. A variety of product vector quantizers use a cartesian product reproduction codebook, which often can be rapidly searched. Examples include polar vector quantizers <ref> [291, 292, 47, 376, 345, 346, 339, 342, 344] </ref>, mean-removed vector quantizers [18, 19], and shape-gain vector quantizers [313, 281]. Trellis-coded quantizers [249, 247, 123, 124, 248, 324] use a Viterbi algorithm encoder matched to a reproduction codebook with a trellis structure. <p> Such quantizers are called "unrestricted" polar quantization [376, 344]. High resolution analysis can be used to study the rate-distortion performance of these quantiz-ers <ref> [47, 48, 339, 341, 344] </ref>. Among other things such analyses find the optimal point density for the magnitude quantizer and the optimal bit allocation between magnitude and phase.
Reference: [48] <author> J.A. Bucklew and N.C. Gallagher, Jr., </author> <title> "Two-Dimensional Quantization of Bivariate Circularly Symmetric Densities," </title> <journal> IEEE Trans. on Information theory, </journal> <volume> Vol. 25, </volume> <pages> pp. 667-671, </pages> <month> Nov. </month> <year> 1979. </year>
Reference-contexts: Such quantizers are called "unrestricted" polar quantization [376, 344]. High resolution analysis can be used to study the rate-distortion performance of these quantiz-ers <ref> [47, 48, 339, 341, 344] </ref>. Among other things such analyses find the optimal point density for the magnitude quantizer and the optimal bit allocation between magnitude and phase.
Reference: [49] <author> J.A. Bucklew and N.C. Gallagher, Jr., </author> <title> "Some properties of uniform step size quantizers," </title> <journal> IEEE Transactions on Information Theory, </journal> <volume> Vol. IT-26, </volume> <pages> pp. 610-613, </pages> <year> 1980. </year> <month> 76 </month>
Reference-contexts: But it does not apply to quantizers with finitely many cells and sources with infinite support, because it does not deal with the overload region of such quantizers. The next contribution is that of Bucklew and Gallager (1980) <ref> [49] </ref>, who studied asymptotic properties of fixed-rate uniform scalar quantization.
Reference: [50] <author> J.A. Bucklew, </author> <title> "Companding and random quantization in several dimensions," </title> <journal> IEEE Trans. Information Theory, </journal> <volume> Vol. 27, </volume> <pages> pp. 207-211, </pages> <month> Mar. </month> <year> 1981. </year>
Reference-contexts: This was first mentioned in Panter-Dite [290] and rediscovered several times. Unfortunately, at higher dimensions, companders cannot implement an optimal point density without creating large oblongitis <ref> [147, 50, 52] </ref>. So there is no direct way to construct vector quantizers with the high resolution philosophy.
Reference: [51] <author> J. A. Bucklew and G. L. Wise, </author> <title> "Multidimensional asymptotic quantization theory with rth power distortion measures, </title> <journal> IEEE Trans. on Information theory, </journal> <volume> Vol. 28, </volume> <pages> pp. 239-247, </pages> <month> March </month> <year> 1982. </year>
Reference-contexts: 1980 Allerton Conference. 22 of Lloyd's algorithm with training sequence size based on the theory of k-means consistency by Pollard [296], two seminal papers on lattice quantization by Conway and Sloane [81], rigorous developments of the Bennett theory for vector quantizers and rth power distortion measures by Bucklew and Wise <ref> [51] </ref>, Kieffer's demonstration of stochastic stability for a general class of feedback quantizers including the historic class of predictive quantizers and delta modulators along with adaptive generalizations [205], Kieffer's study of the convergence rate of Lloyd's algorithm [204], and the demonstration by Garey, Johnson, and Witsenhausen that the Lloyd-Max optimization was <p> It was not until much later that the asymptotic form of N and D N were found, as will be described later. Formal theory advanced further in papers by Bucklew and Wise, Cambanis and Gerr and Buck-lew. The first of these (1982) <ref> [51] </ref> demonstrated Zador's fixed-rate result for rth power distortion kx yk r , assuming only that EkXk r+ffi &lt; 1 for some ffi &gt; 0. It also contained a generalization to random vectors without probability densities, i.e. with distributions that are not absolutely continuous or even continuous.
Reference: [52] <author> J.A. Bucklew, </author> <title> "A note on optimal multidimensional companders," </title> <journal> IEEE Trans. Information Theory, </journal> <volume> Vol. 29, </volume> <editor> p. </editor> <volume> 279, </volume> <month> March </month> <year> 1983. </year>
Reference-contexts: This was first mentioned in Panter-Dite [290] and rediscovered several times. Unfortunately, at higher dimensions, companders cannot implement an optimal point density without creating large oblongitis <ref> [147, 50, 52] </ref>. So there is no direct way to construct vector quantizers with the high resolution philosophy.
Reference: [53] <author> J.A. Bucklew, </author> <title> "Two results on the asymptotic performance of quantizers," </title> <journal> IEEE Transactions on Information Theory, </journal> <volume> Vol. IT-30, </volume> <pages> pp. 341-348, </pages> <year> 1984. </year>
Reference-contexts: Cambanis and Gerr (1983) [59] claimed a similar result, but it had more restrictive conditions and suffered from the same sort of problems. A subsequent paper by Bucklew (1984) <ref> [53] </ref> derived a result for vector quantizers that lies between Bennett's integral and Zador's formula. <p> Even assuming Gersho's conjecture is correct, there is no rigorous proof of the Zador-Gersho formulas (26) and (30) along the lines of the informal derivations that start with Bennett's integral. We mention that the tail conditions given in some of the rigorous results (e.g. <ref> [53, 260] </ref>) are very difficult to check. Simpler ones are needed. Finally, as discussed in Section 2 there are no convincing (let alone rigorous) asymptotic analyses of the operational distortion-rate function of DPCM.
Reference: [54] <author> J. Buhmann and H. Kuhnel, </author> <title> "Vector quantization with complexity costs," </title> <journal> IEEE Trans. Inform. Theory, </journal> <volume> vol. 39, no. 4, </volume> <pages> pp. 1133-1145, </pages> <month> July </month> <year> 1988. </year>
Reference-contexts: Some of the quantizer design algorithms developed as alternatives to Lloyd's algorithm include simulated annealing [107, 360, 125, 208], deterministic annealing [312], pairwise nearest neighbor [112] (which had its origins in earlier clustering techniques [370]), stochastic relaxation [394, 396], self organizing feature maps [209, 384, 385] and other neural nets <ref> [348, 217, 347, 241, 54] </ref>.
Reference: [55] <author> P. J. Burt and E. H. Adelson, </author> <title> "The Laplacian pyramid as a compact image code," </title> <journal> IEEE Trans. Comm., </journal> <volume> vol. COM-31, </volume> <pages> pp. 532-540, </pages> <month> April </month> <year> 1983. </year>
Reference-contexts: The oldest of these methods (so far as quantization is concerned) is the pyramid code of Burt and Adelsen <ref> [55] </ref> (which quite different from Fischer's pyramid VQ).
Reference: [56] <author> A. Buzo, R.M. Gray, A.H. Gray, Jr., and J.D. Markel, </author> <title> "Optimal Quantizations of Coefficient Vectors in LPC Speech," </title> <booktitle> 1978 Joint Meeting of the Acoustical Society of America and the Acoustical Society of Japan, </booktitle> <address> Honolulu, HI, </address> <month> Dec. </month> <year> 1978. </year>
Reference-contexts: Also in 1977, Chen used an algorithm equivalent to a 2-dimensional Lloyd algorithm to design 2-dimensional vector quantizers [70]. In 1978 and 1979 a vector extension of Lloyd's Method I was applied to linear predictive coded (LPC) speech parameters by Buzo and others <ref> [167, 56, 57, 169] </ref> with a weighted quadratic distortion measure on parameter vectors closely related to the Itakura-Saito spectral distortion measure. Also in 1978, Adoul, Collin, and Dalle [3] used clustering ideas to design two-dimensional vector quantizers for speech coding.
Reference: [57] <author> A. Buzo, A. H. Gray, Jr., R. M. Gray and J. D. Markel, </author> <title> "Optimal quantizations of coefficient vectors in LPC speech," </title> <booktitle> Proc. Intl. Conf. on Acoust. Speech, and Signal Processing, </booktitle> <pages> pp. 52-55, </pages> <address> Washington, D. C., </address> <month> April </month> <year> 1979. </year>
Reference-contexts: Also in 1977, Chen used an algorithm equivalent to a 2-dimensional Lloyd algorithm to design 2-dimensional vector quantizers [70]. In 1978 and 1979 a vector extension of Lloyd's Method I was applied to linear predictive coded (LPC) speech parameters by Buzo and others <ref> [167, 56, 57, 169] </ref> with a weighted quadratic distortion measure on parameter vectors closely related to the Itakura-Saito spectral distortion measure. Also in 1978, Adoul, Collin, and Dalle [3] used clustering ideas to design two-dimensional vector quantizers for speech coding.
Reference: [58] <author> A. Buzo, A. H. Gray, Jr., R. M. Gray, and J. D. Markel, </author> <title> "Speech coding based upon vector quantization," </title> <journal> IEEE Trans. Acoustics, Speech, and Signal Processing, </journal> <volume> Vol. 28, </volume> <pages> pp. 562-574, </pages> <month> Oct. </month> <year> 1980. </year>
Reference-contexts: Later in the same year, Buzo et al. <ref> [58] </ref>, developed a tree-structured vector quantizer (TSVQ) for 10-dimensional LPC vectors that greatly reduced the encoder complexity from exponential growth with codebook size to linear growth by searching a sequence of small codebooks instead of a single large codebook. <p> Tree-structured VQ uses a tree-structured reproduction codebook with a matched tree-structured search algorithm <ref> [58, 308, 74] </ref>. A tree-structured VQ with far less memory is provided by a multistage or residual VQ [203, 182, 29, 223, 133, 66, 29, 289, 309, 30]. A variety of product vector quantizers use a cartesian product reproduction codebook, which often can be rapidly searched. <p> A high resolution analysis is given in [23, 20]. The scalar-vector method extends to sources with memory by combining it with transform coding using a decorrelating or approximately decorrelating transform [219]. Tree-Structured Quantization In its original and simplest form, a k dimensional tree-structured vector quantizer (TSVQ) <ref> [58] </ref> is a fixed-rate quantizer with, say, rate R whose encoding is guided by a balanced (fixed-depth) binary tree of depth kR. <p> This reduces the arithmetic complexity and storage roughly in half to approximately kR operations per sample and 2 kR vectors. Further reductions in storage are possible, as described in [190] The usual (but not necessarily optimal) greedy method for designing a balanced TSVQ <ref> [58, 171] </ref> is to design the test vectors stemming from the root node using the Lloyd algorithm on a training set.
Reference: [59] <author> S. Cambanis and N. Gerr, </author> <title> "A simple class of asymptotically optimal quantizers," </title> <journal> IEEE Trans. Information Theory, </journal> <volume> Vol. 29, </volume> <pages> pp. 664-676, </pages> <month> Sept. </month> <year> 1983. </year>
Reference-contexts: However, as pointed out by Linder (1991) [230], there was "a gap in the proof concerning the convergence of Riemann sums with increasing support to a Riemann integral." Linder fixed this and presented a correct derivation with weaker assumptions. Cambanis and Gerr (1983) <ref> [59] </ref> claimed a similar result, but it had more restrictive conditions and suffered from the same sort of problems. A subsequent paper by Bucklew (1984) [53] derived a result for vector quantizers that lies between Bennett's integral and Zador's formula.
Reference: [60] <author> J. C. Candy and O. J. Benjamin, </author> <title> "The structure of quantization noise from Sigma-Delta modulation," </title> <journal> IEEE Trans. on Communications, </journal> <volume> Vol. 29, </volume> <pages> pp. 1316-1323, </pages> <month> Sept. </month> <year> 1981. </year>
Reference-contexts: stationarity, to analyses of distortion via Hermite polynomial expansions for Gaussian processes [95, 323, 15, 246, 179, 180, 193, 118, 143, 144, 262, 263, 264, 210], and to exact results for constant and sinusoidal signals using Rice's method, extensions of Panter, Clavier, and Grieg to quantizers inside a feedback loop <ref> [191, 60, 163] </ref>.
Reference: [61] <author> J.R. Caprio, N. Westin, and J. Esposito, </author> <title> "Optimum quantization for minimum distortion," </title> <booktitle> Proc. of the Intl Telemetering Conf., </booktitle> <pages> pp. 315-323, </pages> <year> 1978. </year>
Reference-contexts: Also in 1978, Adoul, Collin, and Dalle [3] used clustering ideas to design two-dimensional vector quantizers for speech coding. Caprio, Westin, and Esposito in 1978 <ref> [61] </ref> and Menez, Boeri, and Esteban in 1979 [252] also considered clustering algorithms for the design of vector quantizers with squared error and magnitude error distortion measures. The most important paper on quantization during the 1970s was without a doubt Gersho's pa 21 per on "Asymptotically optimal block quantization" [147].
Reference: [62] <author> N. Chaddha, M. Vishwanath and P.A. Chou, </author> <title> "Hierarchical vector quantization of perceptually weighted block transforms," </title> <booktitle> Proceedings 1995 Data Compression Conference (DCC). </booktitle>
Reference-contexts: Trellis-coded quantizers [249, 247, 123, 124, 248, 324] use a Viterbi algorithm encoder matched to a reproduction codebook with a trellis structure. Hierarchical table-lookup vector quantizers provide fixed-rate vector quantizers with minimal computational complexity, both lossy encoder and reproduction decoder being accomplished by table-lookups <ref> [69, 255, 367, 62] </ref>. Many of the quantization techniques, results, and applications can be found in original form in Swaszek's 1985 reprint collection on quantization [340] and Abut's 1990 IEEE Reprint Collection on Vector Quantization [2]. <p> Due to the fact that not every bucket contains one codevector, such techniques, which may be found in <ref> [69, 255, 254, 62, 166] </ref>, do not do a perfect full search. Some quantitative analysis of the increased distortion is given in [253] for a case where the prequantization is a lattice quantizer.
Reference: [63] <author> D. L. Chaffee, </author> <title> "Applications of rate distortion theory to the bandwidth compression," </title> <type> Ph.D. Dissertation, </type> <institution> Electrical Engineering Department, Univ. of California, </institution> <address> Los Angeles, </address> <year> 1975. </year>
Reference-contexts: The first serious studies of vector quantizer design algorithms without constrained structure began to appear in the mid-1970's, when several independent results were reported describing applications of clustering algorithms, usually k-means, to problems of vector quantization. In 1974-1975 Chaffee <ref> [63] </ref> and Chafee and Omura [64] used clustering ideas to design a vector quantizer for very low rate speech vocoding. In 1977 Hilbert used clustering algorithms for joint image compression and image classification [181].
Reference: [64] <author> D. L. Chaffee and J. K. Omura, </author> <title> "A very low rate voice compression system," </title> <booktitle> Abstracts of Papers,IEEE Int. Symp. on Information Theory, </booktitle> <month> Oct. </month> <year> 1974. </year>
Reference-contexts: The first serious studies of vector quantizer design algorithms without constrained structure began to appear in the mid-1970's, when several independent results were reported describing applications of clustering algorithms, usually k-means, to problems of vector quantization. In 1974-1975 Chaffee [63] and Chafee and Omura <ref> [64] </ref> used clustering ideas to design a vector quantizer for very low rate speech vocoding. In 1977 Hilbert used clustering algorithms for joint image compression and image classification [181]. These papers appear to be the first applications of direct vector quantization for speech and image coding applications.
Reference: [65] <author> W.-Y. Chan and A. Gersho, </author> <title> "High fidelity audio transform coding with vector quantization," </title> <booktitle> Proc. IEEE ICASSP Albuquerque, New Mexico, </booktitle> <volume> Vol. 2, </volume> <pages> pp. </pages> <month> 1109-1112 </month> <year> 1990. </year>
Reference-contexts: In other words, multistage quantization can be used (and often is) with very different kinds of quantizers in its stages (different dimensions and much different structures, e.g. DPCM or wavelet coding). For example, structuring the stage quantizers leads to good performance further substantial reductions in complexity, e.g. <ref> [182, 65] </ref>. Of course, the multistage structuring leads to a suboptimal VQ for its given dimension.
Reference: [66] <author> W.Y. Chan, S. Gupta, and A. Gersho, </author> <title> "Enhanced multistage vector quantization by joint codebook design," </title> <journal> IEEE Trans. Comm., </journal> <volume> vol. 40, no. 11, </volume> <pages> pp. 1693-1697, </pages> <month> Nov. </month> <year> 1992. </year>
Reference-contexts: Tree-structured VQ uses a tree-structured reproduction codebook with a matched tree-structured search algorithm [58, 308, 74]. A tree-structured VQ with far less memory is provided by a multistage or residual VQ <ref> [203, 182, 29, 223, 133, 66, 29, 289, 309, 30] </ref>. A variety of product vector quantizers use a cartesian product reproduction codebook, which often can be rapidly searched.
Reference: [67] <author> C.-K. Chan and L.-M. </author> <title> Po, "A complexity reduction technique for image vector quantization," </title> <journal> IEEE Trans. Image Processing, </journal> <volume> Vol. 1, No. 3, </volume> <pages> pp. 312-321, </pages> <month> July </month> <year> 1992. </year>
Reference-contexts: We here only mention several examples with references and leave further discussion to Section 5. Fast search algorithms have been developed for unstructured reproduction codebooks <ref> [71, 335, 32, 67, 149] </ref>. Even faster searches are possible when the reproduction codebook is constrained to have a simple structure, for example to be a subset of points of a regular lattice as in a lattice vector quantizer [147, 81, 82, 83, 84, 151, 116]. <p> Some quantitative analysis of the increased distortion is given in [253] for a case where the prequantization is a lattice quantizer. Other fast search methods include the partial distortion method of [71, 32] and the transform subspace domain approach of <ref> [67] </ref>. Consideration of methods based on prequantization leads to the question of how fine should the prequantization cells be.
Reference: [68] <author> P. C. Chang and R. M. Gray, </author> <title> "Gradient algorithms for designing predictive vector quantizers," </title> <journal> IEEE Trans. Acoustics, Speech, and Signal Processing Vol. </journal> <volume> 34, </volume> <pages> pp. 679-690, </pages> <year> 1986, </year> <month> Aug. </month>
Reference-contexts: This approach was further developed by Swaszek in [343]. Feedback Vector Quantization Just as with scalar quantizers, a vector quantizer can be made predictive by a straightforward generalization of predictive quantization to vectors <ref> [175, 91, 68, 295] </ref> as depicted in Figure 3 if one replaces the scalars by vectors. The encoder and decoder can have a finite set of states, each with a quantizer custom designed for the state.
Reference: [69] <author> P. C. Chang, J. May, and R. M. Gray, </author> <title> "Hierarchical vector quantizers with table-lookup encoders," </title> <booktitle> Proceedings 1985 IEEE International Conference on Communications, </booktitle> <volume> Vol. 3, </volume> <pages> pp. 1452-1455, </pages> <month> June </month> <year> 1985. </year>
Reference-contexts: Trellis-coded quantizers [249, 247, 123, 124, 248, 324] use a Viterbi algorithm encoder matched to a reproduction codebook with a trellis structure. Hierarchical table-lookup vector quantizers provide fixed-rate vector quantizers with minimal computational complexity, both lossy encoder and reproduction decoder being accomplished by table-lookups <ref> [69, 255, 367, 62] </ref>. Many of the quantization techniques, results, and applications can be found in original form in Swaszek's 1985 reprint collection on quantization [340] and Abut's 1990 IEEE Reprint Collection on Vector Quantization [2]. <p> Due to the fact that not every bucket contains one codevector, such techniques, which may be found in <ref> [69, 255, 254, 62, 166] </ref>, do not do a perfect full search. Some quantitative analysis of the increased distortion is given in [253] for a case where the prequantization is a lattice quantizer. <p> Thus the question becomes: what is the best partition into N cells, each of which is the union of some number of fine cells. The codevectors in C should then be the centroids of these cells. Such techniques have been exploited in <ref> [69, 255] </ref>. One technique worth particular mention is called hierarchical table lookup VQ [69, 166]. In this case, the prequantizer is itself an unstructured codebook that is searched with a fine prequantizer that is in turn searched with an even finer prequantizer, and so on. <p> The codevectors in C should then be the centroids of these cells. Such techniques have been exploited in [69, 255]. One technique worth particular mention is called hierarchical table lookup VQ <ref> [69, 166] </ref>. In this case, the prequantizer is itself an unstructured codebook that is searched with a fine prequantizer that is in turn searched with an even finer prequantizer, and so on. Specifically, the first prequantizer uses a high rate scalar quantizer k times.
Reference: [70] <author> D. T. S. Chen, </author> <title> "On two or more dimensional optimum quantizers," </title> <booktitle> Proc. Intl. Conf. on Acoust. Speech, and Signal Processing, </booktitle> <pages> pp. 640-643, </pages> <address> Hartford,CT, </address> <year> 1977. </year>
Reference-contexts: These papers appear to be the first applications of direct vector quantization for speech and image coding applications. Also in 1977, Chen used an algorithm equivalent to a 2-dimensional Lloyd algorithm to design 2-dimensional vector quantizers <ref> [70] </ref>. In 1978 and 1979 a vector extension of Lloyd's Method I was applied to linear predictive coded (LPC) speech parameters by Buzo and others [167, 56, 57, 169] with a weighted quadratic distortion measure on parameter vectors closely related to the Itakura-Saito spectral distortion measure.
Reference: [71] <author> D. Y. Cheng, A. Gersho, B. Ramamurthi, and Y. Shoham, </author> <title> "Fast Search Algorithms for Vector Quantization and Pattern Matching," pp. </title> <booktitle> 911.1-911.4, Proceedings of the Intl. Conf. on Acoustics, Speech, and Signal Processing, </booktitle> <address> San Diego, </address> <month> March </month> <year> 1984. </year>
Reference-contexts: We here only mention several examples with references and leave further discussion to Section 5. Fast search algorithms have been developed for unstructured reproduction codebooks <ref> [71, 335, 32, 67, 149] </ref>. Even faster searches are possible when the reproduction codebook is constrained to have a simple structure, for example to be a subset of points of a regular lattice as in a lattice vector quantizer [147, 81, 82, 83, 84, 151, 116]. <p> Then to encode a source vector x, one applies the prequantization, finds the index of the prequantization cell in which x is contained, and performs a full search on the corresponding bucket for the closest codevector to x. Techniques of this type may be found in <ref> [36, 132, 71, 72, 239, 112] </ref>. Another class of techniques is like the previous except that the low complexity prequantization has much smaller cells than the Voronoi cells of C, i.e. it is finer. <p> Some quantitative analysis of the increased distortion is given in [253] for a case where the prequantization is a lattice quantizer. Other fast search methods include the partial distortion method of <ref> [71, 32] </ref> and the transform subspace domain approach of [67]. Consideration of methods based on prequantization leads to the question of how fine should the prequantization cells be.
Reference: [72] <author> D.-Y. Cheng and A. Gersho, </author> <title> "A fast codebook search algorithm for nearest-neighbor pattern matching," </title> <booktitle> Proc. IEEE ICASSP, Tokyo, </booktitle> <volume> Vol. 1, </volume> <pages> pp. 265-268, </pages> <month> April </month> <year> 1986. </year> <month> 77 </month>
Reference-contexts: Then to encode a source vector x, one applies the prequantization, finds the index of the prequantization cell in which x is contained, and performs a full search on the corresponding bucket for the closest codevector to x. Techniques of this type may be found in <ref> [36, 132, 71, 72, 239, 112] </ref>. Another class of techniques is like the previous except that the low complexity prequantization has much smaller cells than the Voronoi cells of C, i.e. it is finer.
Reference: [73] <author> P. A. Chou and T. Lookabaugh and R. M. Gray, </author> <title> "Entropy-Constrained Vector Quantization," </title> <journal> IEEE Trans. Acoustics, Speech, and Signal Processing Vol. </journal> <volume> 37, No. 1, </volume> <pages> pp. 31-42, </pages> <month> Jan. </month> <year> 1989. </year>
Reference: [74] <author> P. A. Chou and T. Lookabaugh and R. M. Gray, </author> <title> "Optimal pruning with applications to tree-structured source coding and modeling," </title> <journal> IEEE Trans. Information Theory, </journal> <volume> Vol. 35, No. 2, </volume> <pages> pp. 299-315, </pages> <month> Mar. </month> <year> 1989 </year>
Reference-contexts: Tree-structured VQ uses a tree-structured reproduction codebook with a matched tree-structured search algorithm <ref> [58, 308, 74] </ref>. A tree-structured VQ with far less memory is provided by a multistage or residual VQ [203, 182, 29, 223, 133, 66, 29, 289, 309, 30]. A variety of product vector quantizers use a cartesian product reproduction codebook, which often can be rapidly searched. <p> The resulting optimality properties are summarized below. The proofs are simple and require no calculus of variations or differentiation. Proofs may be found, e.g., in <ref> [74] </ref>. * For a fixed lossy encoder ff , regardless of the lossless encoder fl , the optimal reproduction decoder fi is given by fi (i) = argmin y the output minimizing the conditional expectated distortion between the output and the input given that the encoder produced index i. <p> We will not delve into the large literature of transforms, but will observe that bit allocation becomes an important issue, and one can either use the high resolution approximations or a variety of non-asymptotic allocation algorithms such as the "fixed slope" or Pareto-optimality considered in <ref> [372, 332, 74, 306, 307, 321] </ref> . The method involves operating all quantizers at points on their operational distortion-rate curves of equal slopes. For a survey of some of these methods, see [85] or Chapter 10 of [149]. A combinatorial optimization method is given in [383]. <p> The TSVQ will still be competitive in terms of throughput, however, as the tree-structured search is amenable to pipelining. TSVQ's can be generalized to unbalanced trees (with variable depth as opposed to the fixed depth discussed above) <ref> [244, 74, 306, 149] </ref> and with larger branching factors than two or even variable branching factors [318]. <p> The most well known is the CART algorithm of Breiman, Friedman, Olshen, and Stone [43], and the variation of CART for designing TSVQs bears their initials: the BFOS algorithm <ref> [74, 306, 149] </ref>. In this method, a balanced or unbalanced tree with more leaves than needed is first grown and then pruned. <p> It can be shown that, for quite general measures of distortion, pruning can be done in optimal fashion and the optimal subtrees of decreasing rate are nested <ref> [74] </ref>. Though this conjecture has never been tested, it seems likely that in the moderate to high rate case, pruning removes leaves corresponding to cells that are oblong such as cubes cut in half, leaving mainly cubic cells.
Reference: [75] <author> P. A. Chou and T. Lookabaugh, </author> <title> "Conditional entropy-constrained vector quantization of linear predictive coeficients," </title> <journal> pp. </journal> <pages> 187-200, </pages> <booktitle> Proceedings of the Intl. Conf. on Acoustics, Speech, and Signal Processing, </booktitle> <year> 1990. </year>
Reference-contexts: When memory is included, the optimality properties become conditional on past behavior and hence more complicated, but the resulting performance gains can be significant. (See, e.g., <ref> [75] </ref> for an example involving conditional entropy optimization so that the lossless coding component involves memory.) 4 Quantization Theory This section presents an overview of high resolution theory and compares its results to those of Shannon rate-distortion theory. <p> One can of course also make the lossless code depend on the state, or be conditional on the previous binary codeword. One can use a memoryless VQ combined with a conditional lossless code (conditioned on the previous binary codeword) with a conditional entropy constraint <ref> [75] </ref>. The introduction of memory seriously complicates the theory and we shall emphasize memoryless vector quantizers.
Reference: [76] <author> P. A. Chou, T. Lookabaugh, and R. M. Gray, </author> <title> "Entropy constrained vector quantization," </title> <journal> IEEE Transactions on Acoustics, Speech, and Signal Processing, </journal> <volume> Vol. ASSP-37, </volume> <pages> pp. 31-42, </pages> <month> January </month> <year> 1989. </year>
Reference-contexts: In 1989 Chou et al. <ref> [76] </ref> developed a generalized Lloyd algorithm for constrained entropy vector quantization that generalized Berger's [38, 39] Lagrangian formulation for scalar quantization and Farvardin and Modestino's fixed-point design algorithm [117] to vectors. <p> The results in a modified average distortion of E [d (X; fi ( ff (X)))] + H ( ff (X))], where H is Shannon's entropy, and the resulting code is an entropy constrained vector quantizer <ref> [76] </ref> The minimum distortion search for the optimal encoder becomes prohibitively expensive as the number of codewords grows, which occurs for increasing rate or for fixed rate and increasing dimension.
Reference: [77] <author> R. J. Clarke, </author> <title> Transform Cdoing of Images, </title> <publisher> Academic Press, </publisher> <address> Orlanda, FL, </address> <year> 1985. </year>
Reference: [78] <author> A. G. Clavier, P. F. Panter, and D. D. Grieg, </author> <title> "Distortion in a Pulse Count Modulation System," </title> <journal> AIEE Trans., </journal> <volume> Vol. 66, </volume> <pages> pp. 989-1005, </pages> <year> 1947. </year>
Reference-contexts: The oldest such results are the exact analyses for special nonasymptotic cases, such as Clavier, Panter, and Grieg's 1947 analysis of the spectra of the quantization error for uniformly quantized sinusoidal signals <ref> [78] </ref> and Bennett's 1948 derivation of the power spectral density of a uniformly quantized Gaussian random process [35]. <p> In an early contribution to the theory of quantization, Clavier, Panter, and Grieg (1947) <ref> [78] </ref> applied Rice's characteristic function or transform method [305] to provide exact expressions for the quantization error and its moments resulting from uniform quantization for certain specific inputs, including constants and sinusoids.
Reference: [79] <author> A. G. Clavier, P. F. Panter, and D. D. Grieg, </author> <title> "PCM Distortion Analysis," </title> <booktitle> Electrical Engineering, </booktitle> <pages> pp. 1110-1122, </pages> <month> Nov. </month> <year> 1947. </year>
Reference: [80] <author> R.R. Coifman and M.V. Wickerhauser. </author> <title> Entropy-based algorithms for best basis selection. </title> <journal> IEEE Transactions on Information Theory, </journal> <volume> Vol. 38, No. 2, </volume> <pages> pp. 713-718, </pages> <month> March </month> <year> 1992. </year>
Reference: [81] <author> J. H. Conway and N. J. A. Sloane, </author> <title> "Voronoi regions of lattices,second moments of polytopes,and quantization," </title> <journal> IEEE Transactions on Information Theory, </journal> <volume> Vol. 28, </volume> <pages> pp. 211-226, </pages> <year> 1982, </year> <month> March. </month>
Reference-contexts: behavior 3 The idea for a special issue on Quantization was first proposed by by Neil Sloane at the 1980 Allerton Conference. 22 of Lloyd's algorithm with training sequence size based on the theory of k-means consistency by Pollard [296], two seminal papers on lattice quantization by Conway and Sloane <ref> [81] </ref>, rigorous developments of the Bennett theory for vector quantizers and rth power distortion measures by Bucklew and Wise [51], Kieffer's demonstration of stochastic stability for a general class of feedback quantizers including the historic class of predictive quantizers and delta modulators along with adaptive generalizations [205], Kieffer's study of the <p> Fast search algorithms have been developed for unstructured reproduction codebooks [71, 335, 32, 67, 149]. Even faster searches are possible when the reproduction codebook is constrained to have a simple structure, for example to be a subset of points of a regular lattice as in a lattice vector quantizer <ref> [147, 81, 82, 83, 84, 151, 116] </ref>. Additional structure can be imposed for faster searches with virtually no loss of performance, as in Fisher's pyramid VQ [121] which takes advantage of the asymptotic equipartition property to choose a structured support region for the quantizer. <p> near optimality in the fixed-rate case, as well. (These assume that the Gersho's conjecture holds and that the best lattice quantizer is approximately as good as the best tesselation.) Especially important is the fact that their highly structured nature has lead to encoding algorithms with very low complexity and storage <ref> [81, 82, 83, 84, 151] </ref>. Conway and Sloane [82, 84] have reported the best known lattices for several dimensions, as well as fast quantizing and decoding algorithms.
Reference: [82] <author> J. H. Conway and N. J. A. Sloane, </author> <title> "Fast quantizing and decoding algorithms for lattice quantizers and codes," </title> <journal> IEEE Transactions on Information Theory, </journal> <volume> Vol. 28, </volume> <pages> pp. 227-232, </pages> <month> March </month> <year> 1982. </year>
Reference-contexts: Fast search algorithms have been developed for unstructured reproduction codebooks [71, 335, 32, 67, 149]. Even faster searches are possible when the reproduction codebook is constrained to have a simple structure, for example to be a subset of points of a regular lattice as in a lattice vector quantizer <ref> [147, 81, 82, 83, 84, 151, 116] </ref>. Additional structure can be imposed for faster searches with virtually no loss of performance, as in Fisher's pyramid VQ [121] which takes advantage of the asymptotic equipartition property to choose a structured support region for the quantizer. <p> near optimality in the fixed-rate case, as well. (These assume that the Gersho's conjecture holds and that the best lattice quantizer is approximately as good as the best tesselation.) Especially important is the fact that their highly structured nature has lead to encoding algorithms with very low complexity and storage <ref> [81, 82, 83, 84, 151] </ref>. Conway and Sloane [82, 84] have reported the best known lattices for several dimensions, as well as fast quantizing and decoding algorithms. <p> Conway and Sloane <ref> [82, 84] </ref> have reported the best known lattices for several dimensions, as well as fast quantizing and decoding algorithms.
Reference: [83] <author> J. H. Conway and N. J. A. Sloane, </author> <title> "A fast encoding method for lattice codes and quantizers, </title> <journal> IEEE Transactions on Information Theory," </journal> <volume> Vol 29, </volume> <pages> pp. 820-824, </pages> <year> 1983. </year>
Reference-contexts: Fast search algorithms have been developed for unstructured reproduction codebooks [71, 335, 32, 67, 149]. Even faster searches are possible when the reproduction codebook is constrained to have a simple structure, for example to be a subset of points of a regular lattice as in a lattice vector quantizer <ref> [147, 81, 82, 83, 84, 151, 116] </ref>. Additional structure can be imposed for faster searches with virtually no loss of performance, as in Fisher's pyramid VQ [121] which takes advantage of the asymptotic equipartition property to choose a structured support region for the quantizer. <p> near optimality in the fixed-rate case, as well. (These assume that the Gersho's conjecture holds and that the best lattice quantizer is approximately as good as the best tesselation.) Especially important is the fact that their highly structured nature has lead to encoding algorithms with very low complexity and storage <ref> [81, 82, 83, 84, 151] </ref>. Conway and Sloane [82, 84] have reported the best known lattices for several dimensions, as well as fast quantizing and decoding algorithms.
Reference: [84] <author> J. H. Conway and N. J. A. Sloane, </author> <title> Sphere Packings,Lattices and Groups, </title> <publisher> Springer-Verlag, </publisher> <address> New York, </address> <year> 1988. </year>
Reference-contexts: Fast search algorithms have been developed for unstructured reproduction codebooks [71, 335, 32, 67, 149]. Even faster searches are possible when the reproduction codebook is constrained to have a simple structure, for example to be a subset of points of a regular lattice as in a lattice vector quantizer <ref> [147, 81, 82, 83, 84, 151, 116] </ref>. Additional structure can be imposed for faster searches with virtually no loss of performance, as in Fisher's pyramid VQ [121] which takes advantage of the asymptotic equipartition property to choose a structured support region for the quantizer. <p> In this light, Gersho's conjecture is true if and only if at high rates one may obtain an asymptotically optimal quantizer for a uniform distribution by tesselating with T k . The latter statement has been proven for k = 1 (c.f. <ref> [84] </ref>, p. 59) and for k = 2 by Toth (1959) [354], see also [274]). For k = 3, it is known that best lattice tesselation is the body-centered cubic lattice, which is generated by a truncated octahedron. <p> Though M k is known only for k 3, there are bounds for other values of k. One lower bound is the normalized moment of inertia of a sphere of the same dimension; another is given in <ref> [84] </ref>. One upper bound was developed by Zador; others derive from the currently best known tesselations (c.f. [84, 5]). The Zador factors fi k and fl k can be computed straightforwardly for k = 1 and, also, for k 2 for IID sources. <p> One lower bound is the normalized moment of inertia of a sphere of the same dimension; another is given in [84]. One upper bound was developed by Zador; others derive from the currently best known tesselations (c.f. <ref> [84, 5] </ref>). The Zador factors fi k and fl k can be computed straightforwardly for k = 1 and, also, for k 2 for IID sources. In some cases, analytical formulas can be found (e.g. Gaussian, Laplacian, gamma densities). In other cases numerical integration can be used. <p> near optimality in the fixed-rate case, as well. (These assume that the Gersho's conjecture holds and that the best lattice quantizer is approximately as good as the best tesselation.) Especially important is the fact that their highly structured nature has lead to encoding algorithms with very low complexity and storage <ref> [81, 82, 83, 84, 151] </ref>. Conway and Sloane [82, 84] have reported the best known lattices for several dimensions, as well as fast quantizing and decoding algorithms. <p> Conway and Sloane <ref> [82, 84] </ref> have reported the best known lattices for several dimensions, as well as fast quantizing and decoding algorithms.
Reference: [85] <author> P.C. Cosman, R.M. Gray, and M. Vetterli, </author> <title> "Vector quantization of image subbands: A survey," </title> <journal> IEEE Transactions on Image Processing, vol.5, no.2, </journal> <pages> pp. 202-25. </pages> <month> February, </month> <year> 1996. </year>
Reference-contexts: The method involves operating all quantizers at points on their operational distortion-rate curves of equal slopes. For a survey of some of these methods, see <ref> [85] </ref> or Chapter 10 of [149]. A combinatorial optimization method is given in [383]. As a final comment on traditional transform coding, the code can be considered as being suboptimal as a k dimensional quantizer because of the constrained structure (transform and product code). <p> Early wavelet coding techniques emphasized scalar or lattice vector quantization [10, 11, 100, 321, 12, 27, 140] and other vector quantization techniques have also been applied to wavelet coefficients, including tree encoding [261], residual vector quantization [212], and other methods <ref> [85] </ref>. A major breakthrough in performance and complexity came with the introduction of zerotrees [220, 328, 316], which provided an extremely efficient embedded representation of scalar quantized wavelet coefficients, called embedded zerotree wavelet (EZW) coding.
Reference: [86] <author> P.C. Cosman, S.M. Perlmutter, and K.O. Perlmutter, </author> <title> "Tree-structured vector quantization with significance map for wavelet image coding," </title> <booktitle> in Proceedings of the 1995 IEEE Data Compression Conference (DCC), </booktitle> <editor> J.A. Storer and M. Cohn, Eds., </editor> <address> Snowbird, Utah, March 1995, </address> <publisher> IEEE Computer Society Press. </publisher>
Reference-contexts: A major breakthrough in performance and complexity came with the introduction of zerotrees [220, 328, 316], which provided an extremely efficient embedded representation of scalar quantized wavelet coefficients, called embedded zerotree wavelet (EZW) coding. The zerotree approach has been extended to vector quantization <ref> [86] </ref>, but the slight improvement comes at a significant cost in added complexity. Rate-distortion ideas have been used to optimize the rate-distortion tradeoffs using wavelet packets by minimizing a Lagrangian distortion over code trees and bit assignments [301].
Reference: [87] <author> T.M. Cover and J.A. Thomas, </author> <title> Elements of information theory, </title> <publisher> Wiley, </publisher> <address> Chichester, UK, </address> <year> 1991. </year>
Reference-contexts: How is it that two such disparate point densities do in fact yield the same distortion? The answer is provided by the asymptotic equiparti-tion property (AEP) <ref> [87] </ref>, which is the key fact upon which most of information theory rests. For a stationary, ergodic source with continuous random variables, the AEP says that when dimension is 43 large, the k-dimensional probability density is approximately constant, except on a set with small probability.
Reference: [88] <author> D.R. Cox, </author> <title> "Note on grouping," </title> <journal> J. Amer. Statistical Assoc., </journal> <volume> Vol. 52, </volume> <pages> 543-547, </pages> <year> (1957). </year>
Reference-contexts: Lukaszewicz and H. Steinhaus [240] (1955) developed what we now consider to be the Lloyd optimality conditions using variational techniques in a study of optimum go/no-go gauge sets (as acknowledged by Lloyd). Cox in 1957 <ref> [88] </ref> also derived similar conditions. Some additional early work, which can now be seen as relating to vector quantization, will be reviewed later [325, 354, 389].
Reference: [89] <author> R. E. Crochiere, S. M. Webber, and J. K. L. Flanagan. </author> <title> Digital coding of speech in sub-bands. </title> <journal> Bell Syst. Tech. J., </journal> <volume> 55 </volume> <pages> 1069-1086, </pages> <month> Oct. </month> <year> 1976. </year>
Reference-contexts: Hence we content ourselves with the mention of a few highlights. The interested reader is referred to the book by Vetterli and Kovacevic on wavelets and subband coding [365]. Subband coding was introduced in the context of speech coding in 1976 by Crochiere et al. <ref> [89] </ref>. The extension of subband filtering from 1-D to 2-D was made by Vetterli [364] and 2-D subband filtering was first applied to image coding by Woods et al. [380, 371, 381].
Reference: [90] <author> I. Csiszar, </author> <title> "Generalized entropy and quantization problems," </title> <booktitle> Proc. Sixth Prague Conference, </booktitle> <pages> pp. 159-174, </pages> <year> 1973. </year>
Reference-contexts: A companion paper [111] considers similar bounds to the performance of vector quantizers with an analogous average cell size distortion measure. In 1973 Csiszar <ref> [90] </ref> presented a rigorous generalization of (45) to higher dimensional quantizers.
Reference: [91] <author> V. Cuperman and A. Gersho, </author> <title> "Vector predictive coding of speech at 16 Kb/s," </title> <journal> IEEE Trans. on Communications, </journal> <volume> Vol. 33, </volume> <pages> pp. 685-696, </pages> <month> July </month> <year> 1985. </year>
Reference-contexts: This approach was further developed by Swaszek in [343]. Feedback Vector Quantization Just as with scalar quantizers, a vector quantizer can be made predictive by a straightforward generalization of predictive quantization to vectors <ref> [175, 91, 68, 295] </ref> as depicted in Figure 3 if one replaces the scalars by vectors. The encoder and decoder can have a finite set of states, each with a quantizer custom designed for the state.
Reference: [92] <author> C.C. Cutler, </author> <title> "Differential quantization of communication signals," </title> <type> U.S. Patent 2 605 361, </type> <month> July 29, </month> <year> 1952. </year>
Reference-contexts: Nevertheless, removing redundancy leads to much improved codes. Predictive quantization appears to originate in the 1946 delta modulation patent of Derjavitch, Deloraine, and Van Mierlo [99], but the most commonly cited early references are Cutler's patent <ref> [92] </ref> 2,605,361 on "Differential quantization of communication signals" and on DeJager's Philips technical report on delta modulation [98].
Reference: [93] <author> T. Dalenius, </author> <title> "The problem of optimum stratification," </title> <journal> Skandinavisk Aktuarietidskrift, </journal> <volume> Vol. 33, </volume> <pages> pp. 201-213, </pages> <year> 1950. </year>
Reference-contexts: We conclude this subsection by mentioning early work that appeared in the mathematical and statistical literature and which, in hindsight, can be viewed as related to scalar quantization. Specifically, in 1950-1951 Dalenius et al. <ref> [93, 94] </ref> used variational techniques to consider optimal grouping of Gaussian data with respect to average squared error. Lukaszewicz and H. Steinhaus [240] (1955) developed what we now consider to be the Lloyd optimality conditions using variational techniques in a study of optimum go/no-go gauge sets (as acknowledged by Lloyd).
Reference: [94] <author> T. Dalenius and M. Gurney, </author> <title> "The problem of optimum stratification II," </title> <journal> Skandinavisk Aktuarietidskrift, </journal> <volume> Vol. 34, </volume> <pages> pp. </pages> <month> 203-213 </month> <year> 1951. </year>
Reference-contexts: We conclude this subsection by mentioning early work that appeared in the mathematical and statistical literature and which, in hindsight, can be viewed as related to scalar quantization. Specifically, in 1950-1951 Dalenius et al. <ref> [93, 94] </ref> used variational techniques to consider optimal grouping of Gaussian data with respect to average squared error. Lukaszewicz and H. Steinhaus [240] (1955) developed what we now consider to be the Lloyd optimality conditions using variational techniques in a study of optimum go/no-go gauge sets (as acknowledged by Lloyd).
Reference: [95] <author> L. D. Davission, </author> <title> "Information ratesfor data compression," </title> <journal> IEEE WESCON, </journal> <note> Session 8, Paper 1, </note> <year> 1968. </year>
Reference: [96] <editor> L. D. Davisson and R. M. Gray, Eds., </editor> <booktitle> Data Compression, </booktitle> <volume> Vol. 14, </volume> <booktitle> in Benchmark Papers in Electrical Engineering and Computer Science, </booktitle> <editor> Dowden,Hutchinson and Ross, Stroudsburg,Penn., </editor> <year> 1976. </year>
Reference: [97] <author> L.D. Davisson, A. Leon-Garcia and D.L. Neuhoff, </author> <title> "New results on coding of stationary nonergodic sources," </title> <journal> IEEE Trans. Information Theory, </journal> <volume> Vol. 25, </volume> <pages> pp. 137-144, </pages> <month> Mar. </month> <year> 1979. </year>
Reference-contexts: As such, it applies to sources that are stationary in either the strict sense or some weaker sense, such as asymptotic mean stationarity (c.f. [165], p. 16). Though originally derived for ergodic sources, it has been extended to nonergodic sources <ref> [161, 331, 97] </ref>. In contrast, high resolution theory applies, fundamentally, to finite dimensional random vectors. However, for stationary (or asymptotically stationary) sources, taking limits yields results for random processes. For example, the operational distortion-rate function ffi (R) was found to equal Z (R) in this way; see (26).
Reference: [98] <author> F. DeJager, </author> <title> "Delta modulation, a method of PCM trnsmission using a one-unit code," </title> <journal> Philips Research Rept., </journal> <volume> Vol. 7, </volume> <year> 1952. </year> <month> 78 </month>
Reference-contexts: Predictive quantization appears to originate in the 1946 delta modulation patent of Derjavitch, Deloraine, and Van Mierlo [99], but the most commonly cited early references are Cutler's patent [92] 2,605,361 on "Differential quantization of communication signals" and on DeJager's Philips technical report on delta modulation <ref> [98] </ref>.
Reference: [99] <author> B. Derjavitch, E. M. Deloraine and Van Mierlo, </author> <title> French Patent No. </title> <type> 932, 140, </type> <month> Aug. </month> <year> 1946. </year>
Reference-contexts: Nevertheless, removing redundancy leads to much improved codes. Predictive quantization appears to originate in the 1946 delta modulation patent of Derjavitch, Deloraine, and Van Mierlo <ref> [99] </ref>, but the most commonly cited early references are Cutler's patent [92] 2,605,361 on "Differential quantization of communication signals" and on DeJager's Philips technical report on delta modulation [98].
Reference: [100] <author> R. A. DeVore, B. Jawerth, and B. Lucier, </author> <title> "Image compression through wavelet transform coding," </title> <journal> IEEE Trans. Inform. Theory, </journal> <volume> vol. 38, no. 2, </volume> <pages> pp. 719-746, </pages> <year> 1992. </year>
Reference-contexts: The extension of subband filtering from 1-D to 2-D was made by Vetterli [364] and 2-D subband filtering was first applied to image coding by Woods et al. [380, 371, 381]. Early wavelet coding techniques emphasized scalar or lattice vector quantization <ref> [10, 11, 100, 321, 12, 27, 140] </ref> and other vector quantization techniques have also been applied to wavelet coefficients, including tree encoding [261], residual vector quantization [212], and other methods [85].
Reference: [101] <author> L. Devroye, L. Gyorfi, and G. Lugosi, </author> <title> A Probabalistic Theory of Pattern Recognition, </title> <publisher> Springer, </publisher> <address> New York, </address> <year> 1996. </year>
Reference-contexts: We focus on the Lloyd algorithm because of its simplicity, its proven merit at designing codes, and because of the wealth of theoretical results regarding its convergence properties <ref> [315, 296, 101] </ref>.
Reference: [102] <author> R.J. Dick, T. Berger, and F. Jelinek, </author> <title> "Tree encoding of Gaussian sources," </title> <journal> IEEE Trans. Information Theory, </journal> <volume> Vol. 20, No. 3, </volume> <pages> pp. 332-336, </pages> <year> 1974. </year>
Reference-contexts: In the early 1970s the algorithms for tree decoding channel codes were inverted to form tree-encoding algorithms for sources by Jelinek, Anderson, and others <ref> [198, 199, 9, 8, 102] </ref>. Later trellis channel decoding algorithms were modified to trellis-encoding algorithms for sources by Viterbi and Omura [368].
Reference: [103] <author> E. Diday and J. C. Simon, </author> <title> "Clustering analysis," in Digital Pattern Recognition, </title> <editor> K. S. Fu, Ed. </editor> <publisher> Springer-Verlag, </publisher> <address> NY, </address> <year> 1976. </year>
Reference-contexts: These algorithms were developed for statistical clustering applications, the selection of a finite collection of templates that well represented a large collection of data in the MSE sense, i.e., a fixed-rate VQ with an MSE distortion measure in quantization terminology. (See, e.g., Anderberg [7], Hartigan [178], or Diday and Simon <ref> [103] </ref>.) MacQueen used an incremental incorporation of successive samples of a training set to design the codes, each vector being first mapped into a minimum distortion reproduction level representing a cluster, and then the level for that cluster being replaced by an adjusted centroid.
Reference: [104] <author> J. G. Dunham and R. M. Gray, </author> <title> "Joint source and noisy channel trellis encoding," </title> <journal> IEEE Trans. on Information theory, </journal> <volume> Vol. 27, </volume> <pages> pp. 516-519, </pages> <month> July </month> <year> 1981. </year>
Reference: [105] <author> M. Ostendorf Dunham and R. M. Gray, </author> <title> "An algorithm for the design of labeled-transition finite-state vector quantizers," </title> <journal> IEEE Trans. on Communications, </journal> <volume> Vol. 33, </volume> <pages> pp. 83-89, </pages> <month> Jan. </month> <year> 1985. </year>
Reference-contexts: The result is a finite-state version of a predictive quantizer, referred to as a finite-state vector quantizer and depicted in Figure 10. Although little theory has been developed for finite-state quantizers [120, 134, 135], a variety of design methods exist <ref> [130, 131, 105, 176, 13, 14, 207, 149] </ref>. Lloyd's optimal decoder extends in a natural way to finite-state vector quantizers, the optimal reproduction decoder is a conditional expection of the input vector given the binary codeword and the state.
Reference: [106] <author> J.G. Dunn, </author> <title> "The performance of a class of -dimensional quantizers for a Gaussian source, </title> <booktitle> Proc. Columbia Symp. Signal Trasmission Processing, </booktitle> <institution> Columbia Univ., </institution> <address> NY, 76-81, </address> <year> 1965. </year> <title> Reprinted in Data Compression, </title> <editor> L. D. Davisson and R. M. Gray, Ed., </editor> <booktitle> Benchmark Papers in Electrical Engineering and Computer Science, </booktitle> <volume> Vol. 14, </volume> <editor> Dowden, Hutchinson and Ross, Stroudsberg, </editor> <address> PA, </address> <year> 1975. </year>
Reference-contexts: We now momentarily leave the discussion of variable-rate scalar quantization to discuss one of the first vector quantizers since this early example provided the vehicle for the development of optimal varible-rate scalar quantizers. In 1965 Dunn <ref> [106] </ref> introduced a form of vector quantization for quantizing multidimensional IID Gaussian vectors and argued that his code was effectively a permutation code as earlier used by Slepian [322] for channel coding. <p> In 1972 Berger, Jelinek and Wolf [41] and Berger [38] considered the permutation source codes introduced by Dunn <ref> [106] </ref> and developed lower complexity encoding algorithms. Berger [38] showed that for large block sizes, the rate-distortion tradeoffs of the fixed-rate permutation codes were approximately those of optimal variable-rate scalar quantization. In the derivation he described Lloyd-like conditions for optimality of an entropy constrained scalar quantizer for squared error distortion. <p> The first to explicitly apply Shannon's source coding theory to the problem of analog-to-digital conversion combined with digital transmission appear to be Goblick and Holsinger [158] in 1967, and the first to make explicit comparisons of scalar and vector quantizer performance to Shannon's rate-distortion function was Dunn <ref> [106] </ref> in 1965. The Earliest Vector Quantization Work Outside of Shannon's sketch of rate-distortion theory in 1948, the earliest work with a definite vector quantization flavor appeared in the mathematical and statistical literature. <p> The centroid property of optimal reproduction decoders has interesting implications in the special case of a squared error distortion measure, where it follows easily <ref> [106, 147, 46, 139, 149] </ref> that * E [q (X)] = E [X], so that the quantizer output can be considered as an unbiased estimator of the input. * E [q (X) t (q (X) X)] = 0, so that the quantizer output is orthogonal of the quantizer error, an example
Reference: [107] <author> A. E. El Gamal, L. A. Hemachandra, I. Shperling, and V. K. Wei, </author> <title> "Using simulated annealing to design good codes," </title> <journal> IEEE Trans. on Information theory, </journal> <volume> Vol. 33, </volume> <pages> pp. 116-123, </pages> <month> Jan. </month> <year> 1987. </year>
Reference-contexts: The Mid 1980's to the Present In the late 80's a wide variety of vector quantizer design algorithms were developed and tested for speech, images, video, and other signal sources. Some of the quantizer design algorithms developed as alternatives to Lloyd's algorithm include simulated annealing <ref> [107, 360, 125, 208] </ref>, deterministic annealing [312], pairwise nearest neighbor [112] (which had its origins in earlier clustering techniques [370]), stochastic relaxation [394, 396], self organizing feature maps [209, 384, 385] and other neural nets [348, 217, 347, 241, 54].
Reference: [108] <author> P. Elias, </author> <title> "Predictive coding," </title> <type> Ph.D. </type> <institution> Disseration, Harvard University, </institution> <address> Cambridge, MA, </address> <year> 1950. </year>
Reference-contexts: In 1950 Elias <ref> [108] </ref> provided an information theoretic development of the benefits of predictive coding, but the work was not published until 1955 [109]. Other early references include [283, 216, 177, 361, 397].
Reference: [109] <author> P. Elias, </author> <title> "Predictive coding I and II," </title> <journal> IRE Trans. Inform. Theory, </journal> <volume> Vol. 1, </volume> <pages> pp. 16-33, </pages> <month> March </month> <year> 1955. </year>
Reference-contexts: In 1950 Elias [108] provided an information theoretic development of the benefits of predictive coding, but the work was not published until 1955 <ref> [109] </ref>. Other early references include [283, 216, 177, 361, 397].
Reference: [110] <author> P. Elias, </author> <title> "Bounds on performance of optimum quantizers," </title> <journal> IEEE Trans. Information Theory, </journal> <volume> Vol. 16, </volume> <pages> pp. 172-184, </pages> <month> March </month> <year> 1970. </year>
Reference-contexts: An excellent summary of the early work is contained in a 1970 paper by Elias <ref> [110] </ref>. We now momentarily leave the discussion of variable-rate scalar quantization to discuss one of the first vector quantizers since this early example provided the vehicle for the development of optimal varible-rate scalar quantizers. <p> But as to the details it offered only that: "The complete proof is surprisingly long and will not be given here." Though Gish and Pierce were the first to informally derive (29), neither this paper nor any paper to date has provided a rigorous derivation. Elias (1970) <ref> [110] </ref> also made a rigorous analysis of scalar quantization, giving asymptotic bounds to the distortion of scalar quantizers with a rather singularly defined measure of distortion, namely, the rth root of the average of the rth power of the cell widths.
Reference: [111] <author> P. Elias, </author> <title> "Bounds and asymptotes for the performance of multivariate quantizers," </title> <journal> Annals of Mathematical Statistics, </journal> <volume> Vol. 41, No. </volume> <pages> 4 pp. </pages> <month> 1249-1259 </month> <year> 1970. </year>
Reference-contexts: Elias (1970) [110] also made a rigorous analysis of scalar quantization, giving asymptotic bounds to the distortion of scalar quantizers with a rather singularly defined measure of distortion, namely, the rth root of the average of the rth power of the cell widths. A companion paper <ref> [111] </ref> considers similar bounds to the performance of vector quantizers with an analogous average cell size distortion measure. In 1973 Csiszar [90] presented a rigorous generalization of (45) to higher dimensional quantizers.
Reference: [112] <author> W. H. Equitz, </author> <title> "A new vector quantization clustering algorithm," </title> <journal> IEEE Trans. Acoustics, Speech, and Signal Processing, </journal> <volume> Vol. 37, </volume> <pages> pp. 1568-1575, </pages> <month> Oct. </month> <year> 1989. </year>
Reference-contexts: Some of the quantizer design algorithms developed as alternatives to Lloyd's algorithm include simulated annealing [107, 360, 125, 208], deterministic annealing [312], pairwise nearest neighbor <ref> [112] </ref> (which had its origins in earlier clustering techniques [370]), stochastic relaxation [394, 396], self organizing feature maps [209, 384, 385] and other neural nets [348, 217, 347, 241, 54]. <p> Then to encode a source vector x, one applies the prequantization, finds the index of the prequantization cell in which x is contained, and performs a full search on the corresponding bucket for the closest codevector to x. Techniques of this type may be found in <ref> [36, 132, 71, 72, 239, 112] </ref>. Another class of techniques is like the previous except that the low complexity prequantization has much smaller cells than the Voronoi cells of C, i.e. it is finer.
Reference: [113] <author> W. Equitz and T. </author> <title> Cover, "Successive refinement of information," </title> <journal> IEEE Trans. Information Theory, </journal> <volume> Vol. 37, </volume> <pages> pp. 269-275, </pages> <month> Mar. </month> <year> 1991. </year>
Reference-contexts: An important question is whether the performance of a successive refinement quanitzer will be larger than one that does quantization in one step. On the one hand, a rate-distortion theory analysis <ref> [113] </ref> has shown that there are situations where successive approximation can be done without loss of optimality.
Reference: [114] <author> T. Eriksson and E. Agrell, </author> <title> "Lattice-based quantization, Part II," </title> <type> Report No. 18, </type> <institution> Department of Information Theory, Chalmers University of Technology, Goteborg, Sweden, </institution> <month> Oct. </month> <year> 1996. </year>
Reference-contexts: In most dimensions the best known tesselation is a lattice. However, tesselations that are better than the best known lattices have recently been found for dimensions seven and nine by Agrell and Eriksson <ref> [114] </ref>. <p> Specifically, Hui and Neuhoff [186, 187, 188] have found that for a Gaussian density with variance oe 2 N!1 4oeN 1 ln N N!1 (4=3)oe 2 N 2 ln N This result was independently found by Eriksson and Agrell <ref> [114] </ref>. Moreover, it was shown that overload distortion is asymptotically negligible and that D N =( 2 =12) ! 1, which is the first time this has been proved for a source with infinite support. <p> For lattice quantizers the asymptotic form of the optimal scaling of a lattice quantizer for an IID Gaussian source was found recently in [256] and also in <ref> [114] </ref>. We conclude this subsection by mentioning some gaps in rigorous high resolution theory. One, of course, is a proof or counterproof of Gersho's conjecture in dimensions three and higher. Another is the open question of whether the best tesselation in three or more dimensions is a lattice. <p> The theory becomes more difficult if, as is usually the case, only a bounded portion of the lattice is used as the codebook and one must separately consider granular and overload distortion. There are a variety of ways of considering the tradeoffs involved (see, e.g., <ref> [116, 256, 114] </ref>). In any case, the essence of a lattice code is its uniform point density and nicely shaped cells with low normalized moment of inertia. For fixed-rate coding, they work well for uniform sources or other sources with bounded support.
Reference: [115] <author> A. M. Eskicioglu and P. S. Fisher, </author> <title> "Image Quality Measures and Their Performance," </title> <journal> IEEE Transactions on Communications, </journal> <volume> Vol 43, </volume> <pages> pp. 2959-2965, </pages> <month> December, </month> <year> 1995. </year>
Reference-contexts: to a class of distortion measures which have proved useful in perceptual coding, the input weighted quadratic distortion measures of the form d (x; ^x) = (x ^x) t B x (x ^x); (18) where B x is a nonnegative definite matrix that depends on the input. (See, for example, <ref> [276, 275, 115, 141, 227, 234, 233] </ref>.) Most of the theory and design techniques considered here extend to such measures. <p> Nevertheless, these conditions are usually satisfied by perceptual distortion measures. Examples can be found in Eskicioglu and Fisher <ref> [115] </ref>. Also Nill's [275] definition of quality measure analyzed in detail later in this paper satisfies these conditions.
Reference: [116] <author> M. Vedat Eyuboglu and G. David Forney, Jr., </author> <title> "Lattice and trellis quantization with lattice- and trellis-bounded codebooks-High-rate theory for memoryless sources," </title> <journal> IEEE Transactions on Information Theory, vol.39, </journal> <volume> no.1, </volume> <pages> pp. 46-59, </pages> <month> Jan. </month> <year> 1993/ </year>
Reference-contexts: Fast search algorithms have been developed for unstructured reproduction codebooks [71, 335, 32, 67, 149]. Even faster searches are possible when the reproduction codebook is constrained to have a simple structure, for example to be a subset of points of a regular lattice as in a lattice vector quantizer <ref> [147, 81, 82, 83, 84, 151, 116] </ref>. Additional structure can be imposed for faster searches with virtually no loss of performance, as in Fisher's pyramid VQ [121] which takes advantage of the asymptotic equipartition property to choose a structured support region for the quantizer. <p> The theory becomes more difficult if, as is usually the case, only a bounded portion of the lattice is used as the codebook and one must separately consider granular and overload distortion. There are a variety of ways of considering the tradeoffs involved (see, e.g., <ref> [116, 256, 114] </ref>). In any case, the essence of a lattice code is its uniform point density and nicely shaped cells with low normalized moment of inertia. For fixed-rate coding, they work well for uniform sources or other sources with bounded support.
Reference: [117] <author> N. Farvardin and J.W. Modestino, </author> <title> "Optimal quantizer performance for a class of non-Gaussian memoryless sources," </title> <journal> IEEE Transactions on Information Theory, </journal> <volume> Vol. 30, No. 3, </volume> <pages> pp. 485-497, </pages> <year> 1984. </year>
Reference-contexts: We close this section with a brief discussion of two specific works which deal with optimizing variable-rate scalar quantizers without additional structure, the problem that leads to the general 23 formulation of optimal quantization in the next section. In 1984 Farvardin and Modestino <ref> [117] </ref> extended Berger's [38] necessary conditions for optimality of an entropy-constrained scalar quantizer to more general distortion measures and described two design algorithms: the first is similar to Berger's iterative algorithm, but the second was a fixed-point algorithm which can be considered as a natural extension on Lloyd's Method I from <p> In 1989 Chou et al. [76] developed a generalized Lloyd algorithm for constrained entropy vector quantization that generalized Berger's [38, 39] Lagrangian formulation for scalar quantization and Farvardin and Modestino's fixed-point design algorithm <ref> [117] </ref> to vectors. Optimality properties for minimizing a Lagrangian distortion D (q) + R (q) were derived, where rate could be either average length or entropy.
Reference: [118] <author> N. Farvardin and J.W. Modestino, </author> <title> "Rate-distortion performance of DPCM schemes," </title> <journal> IEEE Transactions on Information Theory, </journal> <volume> Vol. 31, No. 3, </volume> <pages> pp. 402-418, </pages> <month> May </month> <year> 1985. </year>
Reference-contexts: Indeed, it is still an open question whether this type of analysis, which typically uses Bennett and Panter-Dite formulas, is asymptotically correct. Nevertheless, the results of such high resolution approximations are widely accepted and often compare well with experimental results <ref> [197, 118] </ref>.
Reference: [119] <author> N. Farvardin and F.Y. Lin, </author> <title> "Performance of entropy-constrained block transform quantizers," </title> <journal> IEEE Transactions on Information Theory, </journal> <volume> Vol. 37, No. 5, </volume> <month> September </month> <year> 1991. </year>
Reference-contexts: Yet another approach was taken by Noll and Zelinski (1978) [280]. Variable-rate quantization was also extended to DPCM and transform coding where high resolution analysis shows that it gains the same relative to fixed-rate quantization as it does when applied to direct scalar quantizing <ref> [285, 119] </ref>. We note, however, that the derivation of this result for DPCM suffers from the same flaws as for fixed-rate quantization.
Reference: [120] <author> T. </author> <title> Fine, "Properties of an Optimal Digital System and Applications," </title> <journal> IEEE Trans. on Information theory, </journal> <volume> Volume10, </volume> <pages> pp. 287-296, </pages> <month> Oct. </month> <year> 1964. </year> <title> [121] "A pyramid vector quantizer," </title> <journal> IEEE Trans. on Information theory, </journal> <volume> Vol. 32, </volume> <pages> pp. 568-583, </pages> <month> July </month> <year> 1986. </year> <title> [122] "Geometric source coding and vector quantization," </title> <journal> IEEE Trans. on Information theory, </journal> <volume> Vol. 35, </volume> <pages> pp. 137-145, </pages> <month> July </month> <year> 1989. </year>
Reference-contexts: Conditions for use in code design resembling the Lloyd optimality conditions have been studied for feedback quantization <ref> [120, 154, 34, 134] </ref>, but the conditions are not optimality conditions in the Lloyd sense, i.e., they are not necessary conditions for a quantizer within a feedback loop to yield the minimum average distortion subject to a rate constraint. <p> The result is a finite-state version of a predictive quantizer, referred to as a finite-state vector quantizer and depicted in Figure 10. Although little theory has been developed for finite-state quantizers <ref> [120, 134, 135] </ref>, a variety of design methods exist [130, 131, 105, 176, 13, 14, 207, 149]. Lloyd's optimal decoder extends in a natural way to finite-state vector quantizers, the optimal reproduction decoder is a conditional expection of the input vector given the binary codeword and the state.
Reference: [123] <author> T. R. Fischer, M. W. Marcellin, and M. Wang, </author> <title> "Trellis-coded vector quantization," </title> <journal> IEEE Trans. on Information theory, </journal> <volume> Vol. 37, </volume> <pages> pp. </pages> <month> 1551-1566 November </month> <year> 1991. </year> <month> 79 </month>
Reference-contexts: A variety of product vector quantizers use a cartesian product reproduction codebook, which often can be rapidly searched. Examples include polar vector quantizers [291, 292, 47, 376, 345, 346, 339, 342, 344], mean-removed vector quantizers [18, 19], and shape-gain vector quantizers [313, 281]. Trellis-coded quantizers <ref> [249, 247, 123, 124, 248, 324] </ref> use a Viterbi algorithm encoder matched to a reproduction codebook with a trellis structure. Hierarchical table-lookup vector quantizers provide fixed-rate vector quantizers with minimal computational complexity, both lossy encoder and reproduction decoder being accomplished by table-lookups [69, 255, 367, 62]. <p> Trellis Coded Quantization Trellis coded quantization, both scalar and vector, improve upon traditional encoded systems by labeling the trellis branches with entire subcodebooks (or "subsets") rather than with individual reproduction levels <ref> [249, 247, 123, 124, 369, 248, 324] </ref>. The primary gain resulting is a reduction in encoder complexity for a given level of performance.
Reference: [124] <author> T. R. Fischer and M. Wang, </author> <title> "Entropy-constrained trellis-coded quantization," </title> <journal> IEEE Trans. on Information theory, </journal> <volume> Vol. 38, </volume> <pages> pp. 415-426, </pages> <month> March </month> <year> 1992. </year>
Reference-contexts: A variety of product vector quantizers use a cartesian product reproduction codebook, which often can be rapidly searched. Examples include polar vector quantizers [291, 292, 47, 376, 345, 346, 339, 342, 344], mean-removed vector quantizers [18, 19], and shape-gain vector quantizers [313, 281]. Trellis-coded quantizers <ref> [249, 247, 123, 124, 248, 324] </ref> use a Viterbi algorithm encoder matched to a reproduction codebook with a trellis structure. Hierarchical table-lookup vector quantizers provide fixed-rate vector quantizers with minimal computational complexity, both lossy encoder and reproduction decoder being accomplished by table-lookups [69, 255, 367, 62]. <p> Trellis Coded Quantization Trellis coded quantization, both scalar and vector, improve upon traditional encoded systems by labeling the trellis branches with entire subcodebooks (or "subsets") rather than with individual reproduction levels <ref> [249, 247, 123, 124, 369, 248, 324] </ref>. The primary gain resulting is a reduction in encoder complexity for a given level of performance.
Reference: [125] <author> J. K. Flanagan, D. R. Morrell, R.L. Frost, C.J. Read, and B. E. Nelson, </author> <title> "Vector quantization codebook Generation using simulated annealing," </title> <journal> pp. </journal> <pages> 1759-1762, </pages> <booktitle> Proceedings of the Intl. Conf. on Acoustics, Speech, and Signal Processing, </booktitle> <address> Glasgow, Scotland, </address> <month> May </month> <year> 1989. </year>
Reference-contexts: The Mid 1980's to the Present In the late 80's a wide variety of vector quantizer design algorithms were developed and tested for speech, images, video, and other signal sources. Some of the quantizer design algorithms developed as alternatives to Lloyd's algorithm include simulated annealing <ref> [107, 360, 125, 208] </ref>, deterministic annealing [312], pairwise nearest neighbor [112] (which had its origins in earlier clustering techniques [370]), stochastic relaxation [394, 396], self organizing feature maps [209, 384, 385] and other neural nets [348, 217, 347, 241, 54].
Reference: [126] <author> P. Fleischer, </author> <title> "Sufficient conditions for achieving minimum distortion in a quantizer," </title> <booktitle> IEEE Int. Conv. Rec., </booktitle> <pages> pp. 104-111, </pages> <year> 1964. </year>
Reference-contexts: All of the independent rediscoveries, however, used the variational derivations and not Lloyd's simple derivations, which were essential for later extensions to vector sources. To our knowledge, the first mention of Lloyd's work in the IEEE literature came in 1964 with Fleischer's <ref> [126] </ref> derivation of a sufficient condition (namely, that the log of the source density be concave) in order that the optimal quantizer be the only local optimal quantizer, and consequently, that Lloyd's Method I will yield a globally optimal quantizer. (The condition is satisfied for common densities such as Gaussian and
Reference: [127] <author> B.A. </author> <title> Flury "Principal points," </title> <journal> Biometrika, </journal> <volume> Vol. 77, No. 1, </volume> <pages> pp. 31-41, </pages> <year> 1990. </year>
Reference-contexts: It is difficult to resist pointing out, however, that in 1990 Lloyd's algorithm was rediscovered in the statistical literature under the name of "principal points," which are distinguished from traditional k-means by the assumption of an absolutely continuous distribution instead of an empirical distribution <ref> [127, 349] </ref>, a formulation included in the VQ formulation for a general distribution. Unfortunately, these works reflect no awareness of the rich quantization literature. Most quantizers today are indeed uniform and scalar, but are combined with prediction or transforms.
Reference: [128] <author> E. Forgey, </author> <title> "Cluster analysis of multivariate data: efficiency vs. interpretability of classification," </title> <journal> Biometrics, </journal> <volume> Vol. 21, </volume> <pages> pp. 768, </pages> <year> 1965. </year> <note> (Abstract) </note>
Reference-contexts: This completed what he and Schutzenberger had begun. In the mid-1960s the optimality properties described by Steinhaus, Lloyd, and Zador and the design algorithm of Steinhaus and Lloyd were rediscovered in the statistical clustering literature. A similar algorithm was introduced in 1965 by Forgey 1965 <ref> [128] </ref>, Ball and Hall [26, 174], Jancey [194], and in 1969 by MacQueen's "k-means" algorithm [245].
Reference: [129] <author> G. D. Forney, Jr., </author> <title> "The Viterbi Algorithm," </title> <journal> Proc. IEEE, </journal> <volume> Vol. 61, </volume> <pages> pp. 268-278, </pages> <month> March </month> <year> 1973. </year>
Reference-contexts: Since the shift register is finite, the tree becomes redundant and new nodes will correspond to previously seen states so that the tree diagram becomes a merged tree or trellis, which can be searched by a dynamic programming algorithm, the Viterbi algorithm (see, e.g., <ref> [129] </ref>). In the early 1970s the algorithms for tree decoding channel codes were inverted to form tree-encoding algorithms for sources by Jelinek, Anderson, and others [198, 199, 9, 8, 102]. Later trellis channel decoding algorithms were modified to trellis-encoding algorithms for sources by Viterbi and Omura [368].
Reference: [130] <author> J. Foster and R. M. Gray, </author> <title> "Finite-state vector quantization," </title> <booktitle> Abstracts of the 1982 IEEE International Symposium on Information Theory, </booktitle> <address> Les Arcs France, </address> <month> June </month> <year> 1982. </year>
Reference-contexts: The result is a finite-state version of a predictive quantizer, referred to as a finite-state vector quantizer and depicted in Figure 10. Although little theory has been developed for finite-state quantizers [120, 134, 135], a variety of design methods exist <ref> [130, 131, 105, 176, 13, 14, 207, 149] </ref>. Lloyd's optimal decoder extends in a natural way to finite-state vector quantizers, the optimal reproduction decoder is a conditional expection of the input vector given the binary codeword and the state.
Reference: [131] <author> J. Foster, R. M. Gray, and M. Ostendorf Dunham, </author> <title> "Finite-state vector quantization for waveform coding," </title> <journal> IEEE Trans. on Information theory, </journal> <volume> Vol. 31, </volume> <pages> pp. 348-359, </pages> <month> May </month> <year> 1985. </year>
Reference-contexts: The result is a finite-state version of a predictive quantizer, referred to as a finite-state vector quantizer and depicted in Figure 10. Although little theory has been developed for finite-state quantizers [120, 134, 135], a variety of design methods exist <ref> [130, 131, 105, 176, 13, 14, 207, 149] </ref>. Lloyd's optimal decoder extends in a natural way to finite-state vector quantizers, the optimal reproduction decoder is a conditional expection of the input vector given the binary codeword and the state.
Reference: [132] <author> J.H. Friedman, F. Baskett and L.J. Shustek, </author> <title> "An algorithm for finding nearest neighbors," </title> <journal> IEEE Trans. Computers, </journal> <volume> Vol. 24, </volume> <pages> pp. 1000-1006, </pages> <month> Oct. </month> <year> 1975. </year>
Reference-contexts: Then to encode a source vector x, one applies the prequantization, finds the index of the prequantization cell in which x is contained, and performs a full search on the corresponding bucket for the closest codevector to x. Techniques of this type may be found in <ref> [36, 132, 71, 72, 239, 112] </ref>. Another class of techniques is like the previous except that the low complexity prequantization has much smaller cells than the Voronoi cells of C, i.e. it is finer.
Reference: [133] <author> R. L. Frost, C. F. Barnes, and F. Xu, </author> <title> "Design and performance of residual quantizers," </title> <booktitle> in Proceedings Data Compression Conference, </booktitle> <editor> J. A. Storer and J. H. Reif, Eds., </editor> <address> Snowbird, Utah, </address> <month> April </month> <year> 1991, </year> <pages> pp. 129-138, </pages> <publisher> IEEE Computer Society Press. </publisher>
Reference-contexts: Tree-structured VQ uses a tree-structured reproduction codebook with a matched tree-structured search algorithm [58, 308, 74]. A tree-structured VQ with far less memory is provided by a multistage or residual VQ <ref> [203, 182, 29, 223, 133, 66, 29, 289, 309, 30] </ref>. A variety of product vector quantizers use a cartesian product reproduction codebook, which often can be rapidly searched. <p> More sophisticated (than greedy) encoding algorithms can take advantage of the direct sum nature of the codebook to make optimal or nearly optimal searches, though with some (and sometimes a great deal) of increased complexity. And more sophisticated design algorithms (than the greedy one) can also have benefits <ref> [29, 133, 28, 30] </ref>. Variable-rate multistage quantizers have been developed [182, 213, 214, 309].
Reference: [134] <author> N. T. Gaarder and D. Slepian, </author> <title> "On optimal finite-state digital transmission systems," </title> <journal> IEEE Trans. on Information theory, </journal> <volume> Vol. 28, </volume> <pages> pp. 167-186, </pages> <month> March </month> <year> 1982. </year>
Reference-contexts: Conditions for use in code design resembling the Lloyd optimality conditions have been studied for feedback quantization <ref> [120, 154, 34, 134] </ref>, but the conditions are not optimality conditions in the Lloyd sense, i.e., they are not necessary conditions for a quantizer within a feedback loop to yield the minimum average distortion subject to a rate constraint. <p> The result is a finite-state version of a predictive quantizer, referred to as a finite-state vector quantizer and depicted in Figure 10. Although little theory has been developed for finite-state quantizers <ref> [120, 134, 135] </ref>, a variety of design methods exist [130, 131, 105, 176, 13, 14, 207, 149]. Lloyd's optimal decoder extends in a natural way to finite-state vector quantizers, the optimal reproduction decoder is a conditional expection of the input vector given the binary codeword and the state.
Reference: [135] <author> G. Gabor and Z. Gyorfi, </author> <title> "Recursive Source Coding," </title> <publisher> Springer-Verlag, </publisher> <address> New York, </address> <year> 1986. </year>
Reference-contexts: The result is a finite-state version of a predictive quantizer, referred to as a finite-state vector quantizer and depicted in Figure 10. Although little theory has been developed for finite-state quantizers <ref> [120, 134, 135] </ref>, a variety of design methods exist [130, 131, 105, 176, 13, 14, 207, 149]. Lloyd's optimal decoder extends in a natural way to finite-state vector quantizers, the optimal reproduction decoder is a conditional expection of the input vector given the binary codeword and the state.
Reference: [136] <author> R.G. Gallager, </author> <title> Information Theory and Reliable Communication, </title> <publisher> Wiley, </publisher> <address> NY, </address> <year> 1968. </year>
Reference-contexts: This is not usually the case for other sources. Shannon's approach was subsequently generalized to sources with memory. (See, e.g., <ref> [136, 37, 165, 40] </ref>.) The general definitions of distortion-rate and rate-distortion functions resemble those for operational distortion-rate and rate-distortion functions in that they are infima of kth order functions. <p> It is not known whether or not ffi k+1 (R) is always less than or equal to ffi k (R). However, it can be shown that subadditivity implies (c.f. <ref> [136] </ref>, p. 112) ffi (R) = lim ffi k (R): (21) The distinction between a fixed dimension and dimension as a parameter can cause ambiguity in the meaning of "optimal." For example, a "suboptimal" (overall) code can outperform an "optimal" (for a fixed dimension) code and hence, for example, a suboptimal
Reference: [137] <author> N.C. Gallagher, Jr., </author> <title> "Discrete Spectral Phase Coding," </title> <journal> IEEE Trans. on Information theory, </journal> <volume> Vol. 22, </volume> <pages> pp. 622-624, </pages> <month> Sept. </month> <year> 1976. </year>
Reference: [138] <author> N.C. Gallagher, Jr., </author> <title> "Quantizing Schemes for the Discrete Fourier Transform of a Random Time-Series," </title> <journal> IEEE Trans. on Information theory, </journal> <volume> Vol. 24, </volume> <pages> pp. 156-163, </pages> <month> March </month> <year> 1978. </year>
Reference: [139] <author> N.C. Gallagher and J.A. </author> <title> "Properties of minimum mean squared error block quantizers," </title> <journal> IEEE Trans. Information Theory, </journal> <volume> Vol. 28, </volume> <pages> pp. 105-107, </pages> <month> Jan. </month> <year> 1982. </year>
Reference-contexts: The centroid property of optimal reproduction decoders has interesting implications in the special case of a squared error distortion measure, where it follows easily <ref> [106, 147, 46, 139, 149] </ref> that * E [q (X)] = E [X], so that the quantizer output can be considered as an unbiased estimator of the input. * E [q (X) t (q (X) X)] = 0, so that the quantizer output is orthogonal of the quantizer error, an example
Reference: [140] <author> Z. Gao, F. Chen, B. Belzer, and J. Villasenor, </author> <title> "A comparison of the Z, E 8 , and Leech lattices for image subband quantization," </title> <booktitle> in Proceedings of the 1995 IEEE Data Compression Conference, </booktitle> <editor> J.A. Storer and M. Cohn, Eds., </editor> <address> Snowbird, Utah, </address> <month> March </month> <year> 1995, </year> <pages> pp. 312-321, </pages> <publisher> IEEE Computer Society Press. </publisher>
Reference-contexts: The extension of subband filtering from 1-D to 2-D was made by Vetterli [364] and 2-D subband filtering was first applied to image coding by Woods et al. [380, 371, 381]. Early wavelet coding techniques emphasized scalar or lattice vector quantization <ref> [10, 11, 100, 321, 12, 27, 140] </ref> and other vector quantization techniques have also been applied to wavelet coefficients, including tree encoding [261], residual vector quantization [212], and other methods [85].
Reference: [141] <author> W. R. Gardner and B. D. Rao, </author> <title> "Theoretical Analysis of the High-Rate Vector Quantization of LPC Parameters," </title> <journal> IEEE Transactions on Speech and Audio Processing, </journal> <volume> Vol 3, </volume> <pages> pp. 367-381, </pages> <month> September </month> <year> 1995. </year>
Reference-contexts: to a class of distortion measures which have proved useful in perceptual coding, the input weighted quadratic distortion measures of the form d (x; ^x) = (x ^x) t B x (x ^x); (18) where B x is a nonnegative definite matrix that depends on the input. (See, for example, <ref> [276, 275, 115, 141, 227, 234, 233] </ref>.) Most of the theory and design techniques considered here extend to such measures. <p> Actually, combined with condition 1, d (x; y) being nonneg ative and being zero if and only if x = y implies that B (y) is semi-positive definite. Gardner and Rao <ref> [141] </ref> used a similar d (x; y) to model perceptual distortion measure for speech. The matrix B (y) is the `sensitivity matrix' in [141]. <p> Gardner and Rao <ref> [141] </ref> used a similar d (x; y) to model perceptual distortion measure for speech. The matrix B (y) is the `sensitivity matrix' in [141]. We follow [227] and place additional regularity constraints in 9 This differs slightly from the previous definition of subadditive because the d k are not assumed to be normalized. <p> These distortion measures have been found useful for modeling perceptual masking effects in speech and image coding. The Bennett integral has been extended to this type of distortion and approximations for both fixed-rate and variable-rate operational distortion-rate functions have been developed <ref> [141, 227] </ref>. Parallel results for Shannon lower bounds to the rate-distortion function have been developed by Linder and Zamir [234] and similar results for multidimensional companding with lattice codes for similar distortion measures have been developed by Linder, Zamir, and Zeger [233]. <p> High resolution theory has the most results for rth power difference distortion measures, and as mentioned previously, some of its results have recently extended to nondifference distortion measures such as (x y) t B x (x y) <ref> [141, 227, 233] </ref>. In any event both theories are the most fully developed for squared error distortion measure, expecially for Gaussian sources. In addition, both theories require a finite moment condition, specific to the distortion measure.
Reference: [142] <author> M. Garey and D. S. Johnson and H. S. Witsenhausen, </author> <title> "The complexity of the generalized Lloyd-Max problem," </title> <journal> IEEE Trans. on Information theory, </journal> <volume> Vol. 28, </volume> <pages> pp. 255-266, </pages> <month> March </month> <year> 1982. </year>
Reference-contexts: demonstration of stochastic stability for a general class of feedback quantizers including the historic class of predictive quantizers and delta modulators along with adaptive generalizations [205], Kieffer's study of the convergence rate of Lloyd's algorithm [204], and the demonstration by Garey, Johnson, and Witsenhausen that the Lloyd-Max optimization was NP-hard <ref> [142] </ref>. Towards the middle of the 1980's, several tutorial articles on vector quantization appeared, which greatly increased the accessibility of the subject [148, 162, 244, 265].
Reference: [143] <author> N. L. Gerr and S. Cambanis, </author> <title> "Analysis of delayed delta modulation," </title> <journal> IEEE Trans. on Information Theory, </journal> <volume> Vol. 32, </volume> <pages> pp. 496-512, </pages> <month> July </month> <year> 1986. </year>
Reference: [144] <author> N. L. Gerr and S. Cambanis, </author> <title> "Analysis of adaptive differential PCM of a stationary Gauss-Markov input," </title> <journal> IEEE Trans. on Information Theory, </journal> <volume> Vol. </volume> <pages> 3e, pp. 350-359, </pages> <month> May </month> <year> 1987. </year>
Reference: [145] <author> A. Gersho, </author> <title> "Stochastic stability of delta modulation," </title> <journal> Bell Systems Technical Journal, </journal> <volume> Vol. 51, </volume> <pages> pp. 821-841, </pages> <month> April </month> <year> 1972. </year>
Reference-contexts: Because it has not been rigorously shown that one may apply Bennett's integral or the Panter-Dite formula directly to the prediction error, the analysis of such feedback quantization systems has proved to be notoriously difficult, with results limited to proofs of stability <ref> [145, 205, 206] </ref>, i.e. asymptotic stationarity, to analyses of distortion via Hermite polynomial expansions for Gaussian processes [95, 323, 15, 246, 179, 180, 193, 118, 143, 144, 262, 263, 264, 210], and to exact results for constant and sinusoidal signals using Rice's method, extensions of Panter, Clavier, and Grieg to quantizers
Reference: [146] <author> A. Gersho, </author> <title> "Principles of Quantization," </title> <journal> IEEE Trans. Circuits Syst, </journal> <volume> Vol. 25, </volume> <pages> pp. 427-436, </pages> <year> 1978. </year>
Reference-contexts: of the point density function yields its more common form D (q) = 12 N 2 f (x) The idea of a quantizer point density function will generalize to vectors, while the compander approach will not in the sense that not all vector quantizers can be represented as a companders <ref> [146] </ref>. Bennett also demonstrated that under an assumption of high resolution, and smooth densities, the quantization error behaved much like random "noise": it had small correlation with the signal and had approximately a flat ("white") spectrum. <p> Panter and Dite stated that (8) had been earlier derived by P.R. Aigrain. For example, if the input density is Gaussian with variance oe 2 , then <ref> [146] </ref> ffi (R) = 12 p ffi (R) can also be derived directly from Bennett's integral using variational methods (as did Lloyd (1957) [237] and Smith (1957) [334] and, much later without apparent knowledge of these early works, by Roe (1964) [311]).
Reference: [147] <author> A. Gersho, </author> <title> "Asymptotically optimal block quantization," </title> <journal> IEEE Trans. Inform. Theory., </journal> <volume> Vol. 25, </volume> <pages> pp. 373-380, </pages> <month> July </month> <year> 1979. </year>
Reference-contexts: Unfortunately, the results of Zador's thesis were not published until 1982 [391] and were little known outside of Bell Laboratories until, probably, Gersho's important paper of 1979 <ref> [147] </ref>, to be described later. Zador's remarkable dissertation also dealt with the analysis of variable-rate vector quantization, but the asymptotic formula given there is not the correct one. <p> The most important paper on quantization during the 1970s was without a doubt Gersho's pa 21 per on "Asymptotically optimal block quantization" <ref> [147] </ref>. <p> Fast search algorithms have been developed for unstructured reproduction codebooks [71, 335, 32, 67, 149]. Even faster searches are possible when the reproduction codebook is constrained to have a simple structure, for example to be a subset of points of a regular lattice as in a lattice vector quantizer <ref> [147, 81, 82, 83, 84, 151, 116] </ref>. Additional structure can be imposed for faster searches with virtually no loss of performance, as in Fisher's pyramid VQ [121] which takes advantage of the asymptotic equipartition property to choose a structured support region for the quantizer. <p> The centroid property of optimal reproduction decoders has interesting implications in the special case of a squared error distortion measure, where it follows easily <ref> [106, 147, 46, 139, 149] </ref> that * E [q (X)] = E [X], so that the quantizer output can be considered as an unbiased estimator of the input. * E [q (X) t (q (X) X)] = 0, so that the quantizer output is orthogonal of the quantizer error, an example <p> Both (24) and the more general formula (23) are called Bennett's integral. The extension of Bennett's integral to vector quantizers was first made by Gersho (1979) <ref> [147] </ref> for quantizers with congruent cells for which the concept of inertial profile was not needed, and then to vector quantizers with varying cell shapes (and codevector placements) by Na and Neuhoff (1995) [260]. <p> Unfortunately, it is not known how to find the best inertial profile. Indeed, it is not even known what functions are allowable as inertial profiles. However, Gersho (1979) <ref> [147] </ref> made the now widely accepted conjecture that when rate is large, most cells of a k-dimensional quantizer with rate R and minimum or nearly 34 minimum MSE are approximately congruent to some basic tesselating 4 k-dimensional cell shape T k that is not influenced by the source density. <p> In this case, the optimum inertial profile is a constant and Bennett's integral can be minimized by variational techniques or Holder's inequality <ref> [168, 147] </ref>, resulting in the optimal point density fl f k (x) f k (x 0 ) dx 0 and the following approximation to the operational distortion-rate function: for large R ffi (R) = M k fi k oe 2 2 2R j Z (R) (26) where M k j M <p> Since fixed-rate coding is a special case of variable-length coding, it must be that fl k in (30) is less than or equal to fi k in (26). This can be directly verified using Jensen's inequality <ref> [147] </ref>. As mentioned in Section 2, Zador's seminal thesis also dealt with the analysis of variable-rate quantization, but the asymptotic formula given there is not the correct one. Rather it was left to his subsequent unpublished 1966 memo [390] to derive the correct formula. <p> The above derivation of (30) is due to Gersho (1979) <ref> [147] </ref>. <p> This operational distortion-rate function was also derived by Zador [389], who showed that his unknown factors b k and c k converged to (2e) 1 . But the argument given here is due to Gersho <ref> [147] </ref>. Notice that in this limiting case, there is no doubt about the constant M . <p> Interestingly, fl = fi, as shown by Gersho <ref> [147] </ref>, who credits Thomas Liggett. Comparing the above with (30), one sees that entropy coding can reduce distortion by the factor fi k =fi, which is the shape loss for k-dimensional quantizers. <p> This is like Bennett's integral in that f (1) (x), and consequently (x), can be arbitrary, but like Zador's result (or Gersho's generalization of Bennett's integral <ref> [147] </ref>) in that, in essence, it is assumed that the quantizers have optimal cell shapes. 50 In 1994 Linder and Zeger [231] generalized (36) to quantizers generated by tesselations by showing that the quantizer q ff based formed by tesselating with some basic cell shape S scaled by a postive number <p> then combined the above with Csiszar's result (47) to show that under fairly weak conditions (finite differential entropy and finite output entropy for some ff &gt; 0) the output entropy H ff and the distortion D ff are asymptotically related via lim D ff which is what Gersho derived informally <ref> [147] </ref>. The generalization of Bennett's integral to fixed-rate vector quantizers with rather arbitrary cell shapes was accomplished by Na and Neuhoff (1995) [260], who presented both rigorous and informal derivations. <p> This was first mentioned in Panter-Dite [290] and rediscovered several times. Unfortunately, at higher dimensions, companders cannot implement an optimal point density without creating large oblongitis <ref> [147, 50, 52] </ref>. So there is no direct way to construct vector quantizers with the high resolution philosophy. <p> The resulting Voronoi partition is a tesselation with all cells having the same size and orientation, as well as size and shape, and with the quantization vector taken to be the centroid of the cell. Lattice quantization was proposed by Gersho <ref> [147] </ref> because of its near optimality for high resolution variable-rate quantization and, for a source with a uniform density, near optimality in the fixed-rate case, as well. (These assume that the Gersho's conjecture holds and that the best lattice quantizer is approximately as good as the best tesselation.) Especially important is
Reference: [148] <author> A. Gersho and V. Cuperman, </author> <title> "Vector Quantization: A pattern-matching technique for speech coding," </title> <journal> IEEE Communications Magazine, </journal> <volume> Vol. 21, </volume> <pages> pp. 15-21, </pages> <month> Dec. </month> <year> 1983. </year> <month> 80 </month>
Reference-contexts: Towards the middle of the 1980's, several tutorial articles on vector quantization appeared, which greatly increased the accessibility of the subject <ref> [148, 162, 244, 265] </ref>. The Mid 1980's to the Present In the late 80's a wide variety of vector quantizer design algorithms were developed and tested for speech, images, video, and other signal sources.
Reference: [149] <author> A. Gersho and R.M. Gray, </author> <title> Vector Quantization and Signal Compression, </title> <publisher> Kluwer Academic Publishers, </publisher> <address> Boston, </address> <year> 1992. </year>
Reference-contexts: In speech coding they form the basis of ITU-G.721, 722, 723, and 726, and in video coding they form the basis of the interframe coding schemes standardized in the MPEG and H.26X series. Comprehensive discussions may be found in books <ref> [197, 268, 149, 317] </ref> and survey papers [196, 153]. Though decorrelation was an early motivation for predictive quantization, the most common view at present is that the primary role of the predictor is to reduce the variance of the variable to be scalar quantized. <p> We here only mention several examples with references and leave further discussion to Section 5. Fast search algorithms have been developed for unstructured reproduction codebooks <ref> [71, 335, 32, 67, 149] </ref>. Even faster searches are possible when the reproduction codebook is constrained to have a simple structure, for example to be a subset of points of a regular lattice as in a lattice vector quantizer [147, 81, 82, 83, 84, 151, 116]. <p> The centroid property of optimal reproduction decoders has interesting implications in the special case of a squared error distortion measure, where it follows easily <ref> [106, 147, 46, 139, 149] </ref> that * E [q (X)] = E [X], so that the quantizer output can be considered as an unbiased estimator of the input. * E [q (X) t (q (X) X)] = 0, so that the quantizer output is orthogonal of the quantizer error, an example <p> In contrast to codebooks to be considered later these will be called unstructured. As a group these techniques use substantial amounts of additional memory in order to significantly reduce arithmetic complexity. A number of such techniques are mentioned in Section 12.16 of <ref> [149] </ref>. A number of fast search techniques are similar in spirit to the following: the Euclidean distances between all pairs of codevectors are precomputed and stored in a table. Now, given a source vector x to quantize, some initial codevector ~y is chosen. <p> It has been successively used for video coding. Structured Quantizers We now turn to quantizers with structured partitions or reproduction codebooks, which in turn lend themselves to fast searching techniques and, in some cases, to greatly reduced storage. Many of these techniques are discussed in <ref> [149, 317] </ref>. Lattice Quantizers Lattice quantization can be viewed as a vector generalization of uniform scalar quantization. <p> The method involves operating all quantizers at points on their operational distortion-rate curves of equal slopes. For a survey of some of these methods, see [85] or Chapter 10 of <ref> [149] </ref>. A combinatorial optimization method is given in [383]. As a final comment on traditional transform coding, the code can be considered as being suboptimal as a k dimensional quantizer because of the constrained structure (transform and product code). <p> The TSVQ will still be competitive in terms of throughput, however, as the tree-structured search is amenable to pipelining. TSVQ's can be generalized to unbalanced trees (with variable depth as opposed to the fixed depth discussed above) <ref> [244, 74, 306, 149] </ref> and with larger branching factors than two or even variable branching factors [318]. <p> The most well known is the CART algorithm of Breiman, Friedman, Olshen, and Stone [43], and the variation of CART for designing TSVQs bears their initials: the BFOS algorithm <ref> [74, 306, 149] </ref>. In this method, a balanced or unbalanced tree with more leaves than needed is first grown and then pruned. <p> The result is a finite-state version of a predictive quantizer, referred to as a finite-state vector quantizer and depicted in Figure 10. Although little theory has been developed for finite-state quantizers [120, 134, 135], a variety of design methods exist <ref> [130, 131, 105, 176, 13, 14, 207, 149] </ref>. Lloyd's optimal decoder extends in a natural way to finite-state vector quantizers, the optimal reproduction decoder is a conditional expection of the input vector given the binary codeword and the state.
Reference: [150] <author> A. Gersho and B. Ramamurthi, </author> <title> "Image coding using vector quantization," </title> <booktitle> Proc. Intl. Conf. on Acoust. Speech, and Signal Processing, </booktitle> <volume> Vol. 1, </volume> <pages> pp. 428-431, </pages> <address> Paris, </address> <month> April </month> <year> 1982. </year>
Reference: [151] <author> J.D. Gibson and K. Sayood, </author> <title> "Lattice quantizatio," </title> <journal> Advances in Electronics and Electron Phys., </journal> <volume> Vol. 72, </volume> <pages> pp. 259-330, </pages> <year> 1988. </year>
Reference-contexts: Fast search algorithms have been developed for unstructured reproduction codebooks [71, 335, 32, 67, 149]. Even faster searches are possible when the reproduction codebook is constrained to have a simple structure, for example to be a subset of points of a regular lattice as in a lattice vector quantizer <ref> [147, 81, 82, 83, 84, 151, 116] </ref>. Additional structure can be imposed for faster searches with virtually no loss of performance, as in Fisher's pyramid VQ [121] which takes advantage of the asymptotic equipartition property to choose a structured support region for the quantizer. <p> near optimality in the fixed-rate case, as well. (These assume that the Gersho's conjecture holds and that the best lattice quantizer is approximately as good as the best tesselation.) Especially important is the fact that their highly structured nature has lead to encoding algorithms with very low complexity and storage <ref> [81, 82, 83, 84, 151] </ref>. Conway and Sloane [82, 84] have reported the best known lattices for several dimensions, as well as fast quantizing and decoding algorithms.
Reference: [152] <author> N. Gilchrist and C. Grewin, </author> <title> Collected Papers on Digital Audio Bit-Rate Reduction, </title> <booktitle> Audio Engineering Society, </booktitle> <address> New York, </address> <year> 1996. </year>
Reference-contexts: For discussions of transform coding for images see [377, 297, 267, 197, 77, 268, 192, 298, 149, 160, 293, 317]. More recently transform coding has also been widely used in high fidelity audio coding <ref> [202, 152] </ref>.
Reference: [153] <author> J. D. Gibson, </author> <title> "Adaptive Prediction in Speech Differential Encoding Systems," </title> <journal> Proc. IEEE, </journal> <volume> Vol. 68, </volume> <pages> pp. 488-525, </pages> <month> April </month> <year> 1980. </year> <title> [154] "Optimum quantization of random sequences," </title> <type> Ph.D. Dissertation, </type> <institution> Harvard University, </institution> <month> March </month> <year> 1967. </year>
Reference-contexts: In speech coding they form the basis of ITU-G.721, 722, 723, and 726, and in video coding they form the basis of the interframe coding schemes standardized in the MPEG and H.26X series. Comprehensive discussions may be found in books [197, 268, 149, 317] and survey papers <ref> [196, 153] </ref>. Though decorrelation was an early motivation for predictive quantization, the most common view at present is that the primary role of the predictor is to reduce the variance of the variable to be scalar quantized.
Reference: [155] <author> H. Gish and J. N. Pierce, </author> <title> "Asymptotically efficient quantizing," </title> <journal> IEEE Trans. on Information theory, </journal> <volume> Vol. 14, </volume> <pages> pp. 676-683, </pages> <month> Sep. </month> <year> 1968. </year>
Reference-contexts: As Goblick and Holsinger's paper appeared in the mainstream literature, its immediate impact was larger than that of Zador. In 1968 Gish and Pierce <ref> [155] </ref> applied the Bennett high resolution method to provide approximations for quantizer output entropy and to study the rate-distortion tradeoffs with rate measured by encoder output entropy as well as by log of the codebook size. <p> More generally for a k-dimensional vector quantizer with mostly small cells, negligible overload distortion and adjacent cells having similar sizes, Gish and Pierce (1968) <ref> [155] </ref> showed that the entropy of L successive outputs is H (q (X 1 ); q (X 2 ); : : : ; q (X L )) = h (X 1 ; : : : ; X kL ) + L f k (x) log fl (x) dx (28) where X <p> , which, as before, will probably be just a little larger than one, and which in any case will converge to one as k ! 1. 37 In the case of scalar quantization, the uniform point density and the operational distortion-rate function were first published by Gish and Pierce (1968) <ref> [155] </ref>. The above derivation of (30) is due to Gersho (1979) [147]. <p> Comparing the above with (30), one sees that entropy coding can reduce distortion by the factor fi k =fi, which is the shape loss for k-dimensional quantizers. For scalar quantization (35) and the fact that uniform quantization is best were first derived by Gish and Pierce (1968) <ref> [155] </ref>. However, the existence of their result in is still widely overlooked, as credit is more often given to Ziv (1985) [398]. Now let us fix the order of the entropy coder to be one and consider arbitrary quantizer dimension. <p> Thus, the loss due to k-dimensional quantization is only the space filling loss M k =M , which explains what Gish and Pierce found for scalar quantizers in 1968 <ref> [155] </ref>. We emphasize that there is no point density, oblongits loss or memory loss, even for sources with memory. In effect, the entropy code has eliminated the need to shape the point density, and as a result, there is no need to compromise cell shapes. <p> The most common extension of distortion measures for scalars is the rth power distortion, d (x; y) = jx yj r . For example, Roe [311] generalized Max's formulation to distortion measures of this form. Gish and Pierce <ref> [155] </ref> considered a more general distortion measure of the form d (x; y) = L (x y), where L is a monotone increasing function of the magnitude of its argument and L (0) = 0 with the added property that M (v) j (1=v) R v=2 v=2 L (u) du has <p> Gish and Pierce (1968) <ref> [155] </ref>, who discovered that uniform is the asymptotically best type of scalar quantizer for variable-rate coding, presented both informal and rigorous derivations the latter being the first to appear in these transactions. <p> Both theories have been extended to to continuous-time random processes. However, the high-resolution results are somewhat sketchy <ref> [35, 237, 155] </ref>. Both can be applied to two or higher dimensional sources such as images or video. Both have have been developed the most for Gaussian sources in the context of squared error distortion, which is not surprising in view of the tractability of squared error and Gaussianity.
Reference: [156] <author> B. Girod, </author> <title> "Rate-constrained Motion Estimation", in Visual Communication and Image Processing VCIP'94, </title> <editor> A.K.Katsaggelos (ed.), </editor> <booktitle> Proc. SPIE vol. </booktitle> <volume> 2308, </volume> <pages> pp. 1026-1034, </pages> <month> September </month> <year> 1994. </year>
Reference-contexts: The influence of the Shannon formulation can be seen in video coding where many current techniques use the Lagrangian distortion to jointly minimize distortion and bit rate <ref> [156, 157] </ref>.
Reference: [157] <author> B. Girod, R.M. Gray, J. Kovacevic. and M. Vetterli. </author> <title> "Image and Video Coding," </title> <journal> Signal Proc. Magazine, </journal> <month> March </month> <year> 1998. </year>
Reference-contexts: the signal description if an analog link is used. 24 Modern video coding schemes often incorporate the Lagrangian distortion viewpoint for ac-complishing rate control, while using predictive quantization in a general sense through motion compensation and uniform quantizers with optimized lossless coding of transform coefficients for the intraframe coding. (cf. <ref> [157] </ref>). 3 Quantization Basics: Encoding, Rate, Distortion, and Optimality The mathematical model for a quantizer, scalar or vector, is equivalent to Shannon's block source code subject to a fidelity criterion or, more simply put, a lossy block source code. <p> The influence of the Shannon formulation can be seen in video coding where many current techniques use the Lagrangian distortion to jointly minimize distortion and bit rate <ref> [156, 157] </ref>.
Reference: [158] <author> T.J. Goblick and J.L. Holsinger, </author> <title> "Analog source digitization: A comparison of theory and practice," </title> <journal> IEEE Trans. on Information theory, </journal> <volume> Vol. 13, </volume> <pages> pp. 323-326, </pages> <month> April </month> <year> 1967. </year>
Reference-contexts: In 1967 Goblick and Holsinger <ref> [158] </ref> measured the distortion and entropy produced by uniform scalar quantizers applied to an IID Gaussian source, and by comparing these to what is called the Shannon lower bound to ffi (R), discovered empirically that for larger rates, variable-rate uniform scalar quantization achieves performance roughly only 1/4 bit (equivalently about 1.5 <p> The first to explicitly apply Shannon's source coding theory to the problem of analog-to-digital conversion combined with digital transmission appear to be Goblick and Holsinger <ref> [158] </ref> in 1967, and the first to make explicit comparisons of scalar and vector quantizer performance to Shannon's rate-distortion function was Dunn [106] in 1965.
Reference: [159] <author> A.J. Goldstein, </author> <title> "Quantization noise in P.C.M.," </title> <institution> Bell Telephone Laboratories Technical Memorandum, </institution> <month> 18 October </month> <year> 1957. </year>
Reference-contexts: Lloyd's introduction of the point density function was a critically important step for subsequent generalizations of Bennett's approximations to vector sources. Later in the same year in another Bell Telephone Laboratories Technical Memorandum, Goldstein <ref> [159] </ref> used variational methods to derive conditions for global optimality of a scalar quantizer in terms of second order partial derivatives with respect to the quantizer levels and thresholds.
Reference: [160] <author> R. C. Gonzales and R. E. Wood, </author> <title> Digital Image Processing, </title> <publisher> Addison-Wesley, </publisher> <address> Reading, MA, </address> <year> 1992. </year>
Reference: [161] <author> R. M. Gray and L.D. Davission, </author> <title> "Source coding theorems without the ergodic assumption," </title> <journal> IEEE Trans. Information Theory, </journal> <volume> Vol. 20, </volume> <pages> pp. 502-516, </pages> <month> July </month> <year> 1974. </year>
Reference-contexts: As such, it applies to sources that are stationary in either the strict sense or some weaker sense, such as asymptotic mean stationarity (c.f. [165], p. 16). Though originally derived for ergodic sources, it has been extended to nonergodic sources <ref> [161, 331, 97] </ref>. In contrast, high resolution theory applies, fundamentally, to finite dimensional random vectors. However, for stationary (or asymptotically stationary) sources, taking limits yields results for random processes. For example, the operational distortion-rate function ffi (R) was found to equal Z (R) in this way; see (26).
Reference: [162] <author> R. M. Gray, </author> <title> Vector Quantization, </title> <journal> IEEE ASSP Magazine, </journal> <volume> Vol. 1, Number =2, </volume> <pages> pp. 4-29, </pages> <month> April </month> <year> 1984. </year>
Reference-contexts: Towards the middle of the 1980's, several tutorial articles on vector quantization appeared, which greatly increased the accessibility of the subject <ref> [148, 162, 244, 265] </ref>. The Mid 1980's to the Present In the late 80's a wide variety of vector quantizer design algorithms were developed and tested for speech, images, video, and other signal sources.
Reference: [163] <author> R. M. Gray, </author> <title> "Quantization Noise Spectra," </title> <journal> IEEE Trans. on Information theory, </journal> <volume> Vol. 36, </volume> <pages> pp. 1220-1244, </pages> <month> Nov. </month> <year> 1990. </year>
Reference-contexts: stationarity, to analyses of distortion via Hermite polynomial expansions for Gaussian processes [95, 323, 15, 246, 179, 180, 193, 118, 143, 144, 262, 263, 264, 210], and to exact results for constant and sinusoidal signals using Rice's method, extensions of Panter, Clavier, and Grieg to quantizers inside a feedback loop <ref> [191, 60, 163] </ref>. <p> Schuchman's conditions are satisfied, for example, if the dither signal has a uniform probability density function on (=2; =2]. It follows from the work of Jayant and Rabiner [195] and Sripad and Snyder [336] (see also <ref> [163] </ref>) that Schuchman's condition implies that the sequence of quantization errors fe n g is independent. The case of uniform dither remains by far the most widely studied in the literature.
Reference: [164] <author> R. M. Gray, </author> <title> Source Coding Theory, </title> <publisher> Kluwer Academic Publishers, </publisher> <address> Boston, </address> <year> 1990. </year>
Reference-contexts: The Turing complexity of higher dimensional quantizers is as yet unknown. Computability First-order Shannon distortion-rate functions can be computed analytically for squared error and several source densites, such as Gaussian, Laplacian and uniform (c.f. <ref> [37, 164] </ref>). For sources with memory, higher order Shannon distortion-rate functions are known only for Gaussian sources and for some discrete Markov sources at small distortions. <p> For other cases, the Blahut algorithm [42] can be used to compute D k (R), though its computational complexity becomes overwhelming unless k is small. Due to the difficulty of computing it, many (mostly lower) bounds to the Shannon distortion-rate function have been developed (c.f. <ref> [327, 235, 37, 164] </ref>). One important upper bound derives from the fact that with respect to squared error, the Gaussian source has the largest Shannon distortion-rate function (kth-order or in the limit) of any source with the same covariance function.
Reference: [165] <author> R. M. Gray, </author> <title> Entropy and Information Theory, </title> <publisher> Springer-Verlag, </publisher> <address> New York, </address> <year> 1990. </year>
Reference-contexts: This is not usually the case for other sources. Shannon's approach was subsequently generalized to sources with memory. (See, e.g., <ref> [136, 37, 165, 40] </ref>.) The general definitions of distortion-rate and rate-distortion functions resemble those for operational distortion-rate and rate-distortion functions in that they are infima of kth order functions. <p> l ); (y 1 ; : : : ; y l )) + d kl ((x l+1 ; : : : ; x k ); (y l+1 ; : : : ; y k )) (39) and the subadditive ergodic theorem will still lead to positive and negative coding theorems <ref> [243, 165] </ref>. 9 An example of a subadditive distortion measure is the Levenshtein distance [221] which counts the number of insertions and deletions along with the number of changes that it takes to convert one sequence into another. <p> As such, it applies to sources that are stationary in either the strict sense or some weaker sense, such as asymptotic mean stationarity (c.f. <ref> [165] </ref>, p. 16). Though originally derived for ergodic sources, it has been extended to nonergodic sources [161, 331, 97]. In contrast, high resolution theory applies, fundamentally, to finite dimensional random vectors. However, for stationary (or asymptotically stationary) sources, taking limits yields results for random processes. <p> Applicability: Distortion Measures Shannon rate-distortion theory applies primarily to additive distortion measures; i.e. distortion measures of the form d (x; y) = P k i=1 d 1 (x; y) (or a normalized version),though there are some results for subadditive distortion measures <ref> [243, 165] </ref> and some for distortion measures such as (xy) t B x (x y) [234].
Reference: [166] <author> R.M. Gray, </author> <title> "Combined compression and segmentation of images," </title> <booktitle> Proceedings of the 1997 International Workshop on Mobile Multimedia Communications (MoMuC97), </booktitle> <address> Seoul, Korea, </address> <month> Sept - Oct , </month> <year> 1997. </year>
Reference-contexts: Due to the fact that not every bucket contains one codevector, such techniques, which may be found in <ref> [69, 255, 254, 62, 166] </ref>, do not do a perfect full search. Some quantitative analysis of the increased distortion is given in [253] for a case where the prequantization is a lattice quantizer. <p> The codevectors in C should then be the centroids of these cells. Such techniques have been exploited in [69, 255]. One technique worth particular mention is called hierarchical table lookup VQ <ref> [69, 166] </ref>. In this case, the prequantizer is itself an unstructured codebook that is searched with a fine prequantizer that is in turn searched with an even finer prequantizer, and so on. Specifically, the first prequantizer uses a high rate scalar quantizer k times.
Reference: [167] <author> R. M. Gray, A. Buzo, Y. Matsuyama, A. H. Gray, Jr., and J. D. Markel, </author> <title> "Source coding and speech compression," </title> <booktitle> Proceedings of the International Telemetering Conference, </booktitle> <volume> Vol. XIV, </volume> <pages> pp. 871-878, </pages> <address> Los Angeles, CA, </address> <month> Nov. </month> <year> 1978. </year>
Reference-contexts: Also in 1977, Chen used an algorithm equivalent to a 2-dimensional Lloyd algorithm to design 2-dimensional vector quantizers [70]. In 1978 and 1979 a vector extension of Lloyd's Method I was applied to linear predictive coded (LPC) speech parameters by Buzo and others <ref> [167, 56, 57, 169] </ref> with a weighted quadratic distortion measure on parameter vectors closely related to the Itakura-Saito spectral distortion measure. Also in 1978, Adoul, Collin, and Dalle [3] used clustering ideas to design two-dimensional vector quantizers for speech coding.
Reference: [168] <author> R. M. Gray and A. H. Gray, Jr., </author> <title> "Asymptotically optimal quantizers," </title> <journal> IEEE Trans. on Info. Theory, </journal> <volume> Vol. IT-23, </volume> <pages> pp. 143-144, </pages> <month> Feb. </month> <year> 1977. </year>
Reference-contexts: It can also be derived without using variational methods by application of Holder's inequality to Bennett's integral <ref> [168] </ref>, demonstrating that the minimum is global. <p> The simple derivations combined the vector quantizer point density approximations with the use of Holder's and Jensen's inequalities, generalizing a scalar quantizer technique introduced in 1977 <ref> [168] </ref>. One step of the development rested on a yet unproved conjecture regarding the asymptotically optimal quantizer cell shapes and Zador's constants, a conjecture which since has borne Gersho's name and which will be considered at some length in Section 4. <p> In this case, the optimum inertial profile is a constant and Bennett's integral can be minimized by variational techniques or Holder's inequality <ref> [168, 147] </ref>, resulting in the optimal point density fl f k (x) f k (x 0 ) dx 0 and the following approximation to the operational distortion-rate function: for large R ffi (R) = M k fi k oe 2 2 2R j Z (R) (26) where M k j M
Reference: [169] <author> R. M. Gray, A. H. Gray, Jr., and G. Rebolledo, </author> <title> "Optimal speech compression," </title> <booktitle> Proceedings of the 13th Asilomar Conference on Circuits Systems and computers, </booktitle> <address> Pacific Grove, CA, </address> <year> 1979. </year>
Reference-contexts: Also in 1977, Chen used an algorithm equivalent to a 2-dimensional Lloyd algorithm to design 2-dimensional vector quantizers [70]. In 1978 and 1979 a vector extension of Lloyd's Method I was applied to linear predictive coded (LPC) speech parameters by Buzo and others <ref> [167, 56, 57, 169] </ref> with a weighted quadratic distortion measure on parameter vectors closely related to the Itakura-Saito spectral distortion measure. Also in 1978, Adoul, Collin, and Dalle [3] used clustering ideas to design two-dimensional vector quantizers for speech coding.
Reference: [170] <author> R.M. Gray and E. Karnin, </author> <title> "Multiple local optima in vector quantizers," </title> <journal> IEEE Transactions on Information Theory, </journal> <volume> Vol. 28, </volume> <pages> pp. 708-721, </pages> <month> November </month> <year> 1981. </year>
Reference-contexts: If the distortion is squared error, the reproduction decoder is simply the conditional expectation of X given it was encoded into i, centroid (S i ) = E [XjX 2 S i ]: If the distortion measure is the input-weighted squared error of (18), then <ref> [170] </ref> centroid (S i ) = E [B X jX 2 S] 1 E [B X XjX 2 S]: * For a fixed lossy encoder ff , regardless of the reproduction decoder fi, the optimal lossless encoder fl is the optimal lossless code for the discrete source ff (X), e.g., a
Reference: [171] <author> R.M. Gray and Y. Linde, </author> <title> "Vector quantizers and predictive quantizers for Gauss-Markov sources," </title> <journal> IEEE Trans. Communications, </journal> <volume> Vol. 30, </volume> <pages> pp. 381-389, </pages> <month> Feb. </month> <year> 1982. </year>
Reference-contexts: This reduces the arithmetic complexity and storage roughly in half to approximately kR operations per sample and 2 kR vectors. Further reductions in storage are possible, as described in [190] The usual (but not necessarily optimal) greedy method for designing a balanced TSVQ <ref> [58, 171] </ref> is to design the test vectors stemming from the root node using the Lloyd algorithm on a training set.
Reference: [172] <author> R. M. Gray and T. G. Stockham, </author> <title> Jr. </title> <journal> "Dithered quantizers,"IEEE Transactions on Information Theory, </journal> <volume> Volume 39, No. 3, </volume> <month> May </month> <year> 1993, </year> <pages> pp. 805-812. </pages>
Reference-contexts: The properties of nonsubtractive dither were originally developed in unpublished work by Wright [382] in 1979 and Brinton [44] in 1984 and subsequently extended and refined with a variety of proofs <ref> [363, 362, 236, 172] </ref>.
Reference: [173] <author> L. Guan and M. Kamel, </author> <title> "Equal-average hyperplane partitioning method for vector quantization of image data," </title> <journal> Pattern Recognition Letters, </journal> <volume> Vol. 13, </volume> <pages> pp. 605-609, </pages> <month> Oct. </month> <year> 1992. </year>
Reference-contexts: In this way the set of potential codevectors is gradually narrowed. Techniques in this category, with different ways of narrowing the search, may be found in <ref> [257, 366, 335, 258, 300, 286, 173] </ref>. A number of other fast search techniques begin with a "coarse" prequantization with some very low complexity technique. It is called "coarse" because it typically has larger cells than the Voronoi regions of the codebook C that is being searched.
Reference: [174] <author> D.J. Hall and G.B. Ball, </author> <title> "ISODATA: A novel method of data analysis and pattern classification," </title> <type> Technical Report, </type> <institution> Stanford Research Institute, </institution> <address> Menlo Park, CA. </address> <month> 81 </month>
Reference-contexts: This completed what he and Schutzenberger had begun. In the mid-1960s the optimality properties described by Steinhaus, Lloyd, and Zador and the design algorithm of Steinhaus and Lloyd were rediscovered in the statistical clustering literature. A similar algorithm was introduced in 1965 by Forgey 1965 [128], Ball and Hall <ref> [26, 174] </ref>, Jancey [194], and in 1969 by MacQueen's "k-means" algorithm [245].
Reference: [175] <author> H.-M. Hang and J. W. Woods, </author> <title> "Predictive vector quantization of images," </title> <journal> IEEE Trans. on Communications, </journal> <volume> Vol. 33, </volume> <pages> pp. </pages> <address> 1208--1219, </address> <month> Nov. </month> <year> 1985. </year>
Reference-contexts: This approach was further developed by Swaszek in [343]. Feedback Vector Quantization Just as with scalar quantizers, a vector quantizer can be made predictive by a straightforward generalization of predictive quantization to vectors <ref> [175, 91, 68, 295] </ref> as depicted in Figure 3 if one replaces the scalars by vectors. The encoder and decoder can have a finite set of states, each with a quantizer custom designed for the state.
Reference: [176] <author> A. Haoui and D. G. Messerschmitt, </author> <title> "Predictive vector quantization," </title> <booktitle> Proc. Intl. Conf. on Acoust. Speech, and Signal Processing, </booktitle> <volume> Vol. 1, </volume> <pages> pp. </pages> <address> 10.10.1-10.10.4, San Diego, CA, </address> <month> March </month> <year> 1984. </year>
Reference-contexts: The result is a finite-state version of a predictive quantizer, referred to as a finite-state vector quantizer and depicted in Figure 10. Although little theory has been developed for finite-state quantizers [120, 134, 135], a variety of design methods exist <ref> [130, 131, 105, 176, 13, 14, 207, 149] </ref>. Lloyd's optimal decoder extends in a natural way to finite-state vector quantizers, the optimal reproduction decoder is a conditional expection of the input vector given the binary codeword and the state.
Reference: [177] <author> C. W. Harrison, </author> <title> "Experiments with linear prediction in television," </title> <journal> Bell Systems Technical Journal, </journal> <volume> Vol. 31, </volume> <pages> pp. 764-783, </pages> <month> July </month> <year> 1952. </year>
Reference-contexts: In 1950 Elias [108] provided an information theoretic development of the benefits of predictive coding, but the work was not published until 1955 [109]. Other early references include <ref> [283, 216, 177, 361, 397] </ref>.
Reference: [178] <author> J.A. Hartigan, </author> <title> Clustering Algorithms, </title> <address> New York:Wiley, </address> <year> 1975. </year>
Reference-contexts: These algorithms were developed for statistical clustering applications, the selection of a finite collection of templates that well represented a large collection of data in the MSE sense, i.e., a fixed-rate VQ with an MSE distortion measure in quantization terminology. (See, e.g., Anderberg [7], Hartigan <ref> [178] </ref>, or Diday and Simon [103].) MacQueen used an incremental incorporation of successive samples of a training set to design the codes, each vector being first mapped into a minimum distortion reproduction level representing a cluster, and then the level for that cluster being replaced by an adjusted centroid.
Reference: [179] <author> A. Hayashi, </author> <title> "Differential pulse code modulation of the Wiener process," </title> <journal> IEEE Trans. on Communications, </journal> <volume> Vol. 26, </volume> <pages> pp. 881-887, </pages> <month> June </month> <year> 1978. </year>
Reference: [180] <author> A. Hayashi, </author> <title> "Differential pulse code modulation of stationary Gaussian inputs," </title> <journal> IEEE Trans. on Communications, </journal> <volume> Vol. 26, </volume> <pages> pp. 1137-1147, </pages> <month> August </month> <year> 1978. </year>
Reference: [181] <author> E. E. </author> <title> Hilbert, "Cluster compression algorithm: a joint clustering/data compression concept," Publication No. </title> <type> 77-43, </type> <institution> Jet Propulsion Lab, Pasadena, </institution> <address> CA, </address> <month> Dec. </month> <year> 1977. </year>
Reference-contexts: In 1974-1975 Chaffee [63] and Chafee and Omura [64] used clustering ideas to design a vector quantizer for very low rate speech vocoding. In 1977 Hilbert used clustering algorithms for joint image compression and image classification <ref> [181] </ref>. These papers appear to be the first applications of direct vector quantization for speech and image coding applications. Also in 1977, Chen used an algorithm equivalent to a 2-dimensional Lloyd algorithm to design 2-dimensional vector quantizers [70].
Reference: [182] <author> Y.-S. Ho and A. Gersho. </author> <title> Variable-rate multi-stage vector quantization for image coding. </title> <booktitle> In Proceedings ICASSP, </booktitle> <pages> pages 1156-1159, </pages> <year> 1988. </year>
Reference-contexts: Tree-structured VQ uses a tree-structured reproduction codebook with a matched tree-structured search algorithm [58, 308, 74]. A tree-structured VQ with far less memory is provided by a multistage or residual VQ <ref> [203, 182, 29, 223, 133, 66, 29, 289, 309, 30] </ref>. A variety of product vector quantizers use a cartesian product reproduction codebook, which often can be rapidly searched. <p> In other words, multistage quantization can be used (and often is) with very different kinds of quantizers in its stages (different dimensions and much different structures, e.g. DPCM or wavelet coding). For example, structuring the stage quantizers leads to good performance further substantial reductions in complexity, e.g. <ref> [182, 65] </ref>. Of course, the multistage structuring leads to a suboptimal VQ for its given dimension. <p> And more sophisticated design algorithms (than the greedy one) can also have benefits [29, 133, 28, 30]. Variable-rate multistage quantizers have been developed <ref> [182, 213, 214, 309] </ref>.
Reference: [183] <author> J. Huang, </author> <title> "Quantization of correlated random variables," </title> <type> Ph.D. dissertation, </type> <institution> School of Engineering, Yale University, </institution> <address> New Haven, CT 1962. </address> . 
Reference-contexts: The operation is depicted in Figure 4. This style of code was introduced in 1956 by Kramer and Mathews [215] and analyzed and popularized in 1962-3 by Huang and Schultheiss <ref> [183, 184] </ref>. Kramer and Mathews simply assumed that the goal of the transform was to decorrelate the symbols, but Huang and Schultheis proved that this was true for Gaussian sources with the high resolution assumption. <p> More recently transform coding has also been widely used in high fidelity audio coding [202, 152]. Unlike predictive quantizers, the transform coding approach lent itself quite well to the Ben-nett high resolution approximations, the classical analysis being Huang's and Schultheiss' development <ref> [183, 184] </ref> of the performance of optimized transform codes for fixed-rate scalar quantizers for Gaussian sources, a result which demonstrated that the Karhunen-Loeve decorrelating transform was optimum for this application for the given assumptions.
Reference: [184] <author> J.-Y. Huang and P. M. Schultheiss, </author> <title> "Block quantization of correlated Gaussian random variables," </title> <journal> IEEE Trans. on Communications, </journal> <volume> Vol. 11, </volume> <pages> pp. 289-296, </pages> <month> Sep. </month> <year> 1963. </year>
Reference-contexts: The operation is depicted in Figure 4. This style of code was introduced in 1956 by Kramer and Mathews [215] and analyzed and popularized in 1962-3 by Huang and Schultheiss <ref> [183, 184] </ref>. Kramer and Mathews simply assumed that the goal of the transform was to decorrelate the symbols, but Huang and Schultheis proved that this was true for Gaussian sources with the high resolution assumption. <p> More recently transform coding has also been widely used in high fidelity audio coding [202, 152]. Unlike predictive quantizers, the transform coding approach lent itself quite well to the Ben-nett high resolution approximations, the classical analysis being Huang's and Schultheiss' development <ref> [183, 184] </ref> of the performance of optimized transform codes for fixed-rate scalar quantizers for Gaussian sources, a result which demonstrated that the Karhunen-Loeve decorrelating transform was optimum for this application for the given assumptions. <p> The scalar version of Bennett's integral has been used to analyze and optimize transform coding <ref> [184] </ref>. Examples of the use of the vector extension of Bennett's will be given later in the paper.
Reference: [185] <author> D. A. Huffman, </author> <title> "A method for the construction of minimum redundancy codes," </title> <journal> Proc. IRE, </journal> <volume> Vol. 40, </volume> <pages> pp. 1098-1101, </pages> <month> Sept. </month> <year> 1952. </year>
Reference-contexts: Huffman's algorithm <ref> [185] </ref> provides a method of designing binary codes with the smallest possible average length, but there is no simple formula determining the length of each binary codeword and Huffman's algorithm can be quite complex numerically and in terms of memory requirements if the number of levels is large, e.g., in the
Reference: [186] <author> D. Hui and D. L. Neuhoff, </author> <title> "Asymptotic analysis of optimum uniform scalar quantizers for generalized Gaussian distributions," </title> <booktitle> Proc. 1994 IEEE Int'l Symp. on Inform. Thy, </booktitle> <address> Trondheim, Norway, p. 461, </address> <month> June </month> <year> 1994. </year>
Reference-contexts: Specifically, Hui and Neuhoff <ref> [186, 187, 188] </ref> have found that for a Gaussian density with variance oe 2 N!1 4oeN 1 ln N N!1 (4=3)oe 2 N 2 ln N This result was independently found by Eriksson and Agrell [114].
Reference: [187] <author> D. Hui and D.L. Neuhoff, </author> <title> "When is overload distortion negligible in uniform scalar quantization," </title> <booktitle> 1997 IEEE Int'l Symp Inform. Thy., </booktitle> <address> Ulm, Germany, p. 517, </address> <month> July </month> <year> 1997. </year>
Reference-contexts: Specifically, Hui and Neuhoff <ref> [186, 187, 188] </ref> have found that for a Gaussian density with variance oe 2 N!1 4oeN 1 ln N N!1 (4=3)oe 2 N 2 ln N This result was independently found by Eriksson and Agrell [114].
Reference: [188] <author> D. Hui and D. L. Neuhoff, </author> <title> "Asymptotic analysis of optimal fixed-rate uniform scalar quantization," </title> <note> Submitted to IEEE Trans. Information Theory. </note>
Reference-contexts: Specifically, Hui and Neuhoff <ref> [186, 187, 188] </ref> have found that for a Gaussian density with variance oe 2 N!1 4oeN 1 ln N N!1 (4=3)oe 2 N 2 ln N This result was independently found by Eriksson and Agrell [114].
Reference: [189] <author> D. Hui and D.L. Neuhoff, </author> <title> "On the complexity of scalar quantization," </title> <booktitle> Proc. 1995 IEEE Int'l Symp. on Inform. Thy," </booktitle> <address> Whistler, B.C., p. 372, </address> <month> Sept. </month> <year> 1995. </year>
Reference-contexts: On the other hand, because high resolution theory can analyze the performance of families of quantizers with complexity reducing structure, one can learn much from it about how complexity relates to performance. Moreover, the recent work of Hui and Neuhoff <ref> [189] </ref> has used the high resolution approach to explore fundamental issues of complexity in quantization. Using a Turing formulation, they showed that the complexity of optimal scalar quantization is polynomial (in rate) for most common source densities. The Turing complexity of higher dimensional quantizers is as yet unknown.
Reference: [190] <author> D. Hui, D.F. Lyons and D.L. Neuhoff, </author> <title> "Reduced storage VQ via secondary quantization," </title> <note> To appear in IEEE Trans. Image Processing. </note>
Reference-contexts: For example, a recent study of VQ codebook storage has shown that in routine cases one needs to store codevector components with only about R + 4 bits per component, where R is the rate of the quantizer <ref> [190] </ref>. Though this study did not assess the required arithmetic precision, one would guess that it need not be more than a little larger than that of the storage; e.g. R plus 5 or 6 bit arithmetic should suffice. <p> This reduces the arithmetic complexity and storage roughly in half to approximately kR operations per sample and 2 kR vectors. Further reductions in storage are possible, as described in <ref> [190] </ref> The usual (but not necessarily optimal) greedy method for designing a balanced TSVQ [58, 171] is to design the test vectors stemming from the root node using the Lloyd algorithm on a training set.
Reference: [191] <author> J. E. Iwersen, </author> <title> "Calculated quantizing noise of single-integration delta-modulation coders," </title> <journal> Bell Syst. Tech. J., </journal> <pages> pp. 2359-2389, </pages> <month> September </month> <year> 1969. </year>
Reference-contexts: stationarity, to analyses of distortion via Hermite polynomial expansions for Gaussian processes [95, 323, 15, 246, 179, 180, 193, 118, 143, 144, 262, 263, 264, 210], and to exact results for constant and sinusoidal signals using Rice's method, extensions of Panter, Clavier, and Grieg to quantizers inside a feedback loop <ref> [191, 60, 163] </ref>.
Reference: [192] <author> A. K. Jain, </author> <title> Fundamentals of Digital Image Processing, </title> <publisher> Prentice-Hall, </publisher> <address> Englewood Cliffs, NJ, </address> <year> 1989. </year>
Reference: [193] <author> E. Janardhanan, </author> <title> "Differentical PCM systems," </title> <journal> IEEE Trans. on Communications, </journal> <volume> Vol. 27, </volume> <pages> pp. 82-93, </pages> <month> Jan. </month> <year> 1979. </year>
Reference: [194] <author> R.C. Jancey, </author> <title> "Multidimensional group analysis," </title> <journal> Austrailian Journal of Botany, </journal> <volume> Vol. 14, </volume> <pages> pp. 127-130, </pages> <year> 1966. </year>
Reference-contexts: In the mid-1960s the optimality properties described by Steinhaus, Lloyd, and Zador and the design algorithm of Steinhaus and Lloyd were rediscovered in the statistical clustering literature. A similar algorithm was introduced in 1965 by Forgey 1965 [128], Ball and Hall [26, 174], Jancey <ref> [194] </ref>, and in 1969 by MacQueen's "k-means" algorithm [245].
Reference: [195] <author> N. S. Jayant and L. R. Rabiner, </author> <title> "The application of dither to the quantization of speech signals," </title> <journal> Bell Systems Technical Journal, </journal> <volume> Vol. 51, No. 6, </volume> <pages> pp. 1293-1304, </pages> <address> July-Aug., </address> <year> 1972. </year>
Reference-contexts: Schuchman's conditions are satisfied, for example, if the dither signal has a uniform probability density function on (=2; =2]. It follows from the work of Jayant and Rabiner <ref> [195] </ref> and Sripad and Snyder [336] (see also [163]) that Schuchman's condition implies that the sequence of quantization errors fe n g is independent. The case of uniform dither remains by far the most widely studied in the literature.
Reference: [196] <author> N. S. Jayant, </author> <title> "Digital coding of speech waveforms: PCM, DPCM and DM quantizers," </title> <journal> Proc. IEEE, </journal> <volume> Vol. 62, No. 5, </volume> <pages> pp. 611-632, </pages> <month> May </month> <year> 1974 </year>
Reference-contexts: In speech coding they form the basis of ITU-G.721, 722, 723, and 726, and in video coding they form the basis of the interframe coding schemes standardized in the MPEG and H.26X series. Comprehensive discussions may be found in books [197, 268, 149, 317] and survey papers <ref> [196, 153] </ref>. Though decorrelation was an early motivation for predictive quantization, the most common view at present is that the primary role of the predictor is to reduce the variance of the variable to be scalar quantized.
Reference: [197] <author> N. S. Jayant and P. Noll, </author> <title> Digital Coding of Waveforms: Principles and Applications to Speech and Video, </title> <address> Prentice-Hall,Englewood Cliffs, NJ, </address> <year> 1971. </year>
Reference-contexts: In speech coding they form the basis of ITU-G.721, 722, 723, and 726, and in video coding they form the basis of the interframe coding schemes standardized in the MPEG and H.26X series. Comprehensive discussions may be found in books <ref> [197, 268, 149, 317] </ref> and survey papers [196, 153]. Though decorrelation was an early motivation for predictive quantization, the most common view at present is that the primary role of the predictor is to reduce the variance of the variable to be scalar quantized. <p> prediction error is sufficiently similar in form to that of the source that its operational distortion-rate function is smaller than that of the original source by, approximately, the ratio of the variance of the source to that of the prediction error, a quantity that is often called a prediction gain <ref> [251, 284, 338, 197] </ref>. Analyses of this form usually claim that under high resolution conditions the distribution of the prediction error approaches that of the error when predictions are based on past source samples rather than past reproductions. <p> Indeed, it is still an open question whether this type of analysis, which typically uses Bennett and Panter-Dite formulas, is asymptotically correct. Nevertheless, the results of such high resolution approximations are widely accepted and often compare well with experimental results <ref> [197, 118] </ref>.
Reference: [198] <author> F. Jelinek, </author> <title> "Tree encoding of memoryless time-discrete sources with a fidelith criterion," </title> <journal> IEEE Trans. on Information theory, </journal> <volume> Vol. 15, </volume> <pages> pp. 584-590, </pages> <month> Sept. </month> <year> 1969. </year> <month> 82 </month>
Reference-contexts: In the early 1970s the algorithms for tree decoding channel codes were inverted to form tree-encoding algorithms for sources by Jelinek, Anderson, and others <ref> [198, 199, 9, 8, 102] </ref>. Later trellis channel decoding algorithms were modified to trellis-encoding algorithms for sources by Viterbi and Omura [368].
Reference: [199] <author> F. Jelinek and J. B. Anderson, </author> <title> "Instrumentable tree encoding of information sources," </title> <journal> IEEE Trans. on Information theory, </journal> <volume> Vol. 17, </volume> <pages> pp. 118-119, </pages> <month> Jan. </month> <year> 1971. </year>
Reference-contexts: In the early 1970s the algorithms for tree decoding channel codes were inverted to form tree-encoding algorithms for sources by Jelinek, Anderson, and others <ref> [198, 199, 9, 8, 102] </ref>. Later trellis channel decoding algorithms were modified to trellis-encoding algorithms for sources by Viterbi and Omura [368].
Reference: [200] <author> D.G. Jeong and J.D. Gibson, </author> <title> "Uniform and piecewise uniform lattice vector quantization for memoryless Gaussian and Laplacian sources," </title> <journal> IEEE Trans. on Information theory, </journal> <volume> Vol. 39, No. 3, </volume> <pages> pp. 786-804, </pages> <month> May </month> <year> 1993. </year>
Reference-contexts: Good schemes of this sort have even been developed for low to moderate rates by Gibson <ref> [200, 201] </ref> and Pan and Fischer [288, 289]. Cell-conditioned two-stage quantizers can be viewed as having a piecewise constant point density of the sort proposed earlier by Kuhlmann and Bucklew [218] as a means of circumventing the fact that optimal vector quantizers cannot be implemented with companders.
Reference: [201] <author> D.G. Jeong and J.D. Gibson, </author> <title> "Image coding with uniform and piecewise-uniform vector quantizers," </title> <journal> IEEE Trans. on Information theory, vol.4, no.2, </journal> <pages> pp. 140-6, </pages> <month> Feb. </month> <year> 1995. </year>
Reference-contexts: Good schemes of this sort have even been developed for low to moderate rates by Gibson <ref> [200, 201] </ref> and Pan and Fischer [288, 289]. Cell-conditioned two-stage quantizers can be viewed as having a piecewise constant point density of the sort proposed earlier by Kuhlmann and Bucklew [218] as a means of circumventing the fact that optimal vector quantizers cannot be implemented with companders.
Reference: [202] <author> J. D. Johnston, </author> <title> "Transform Coding of audio signals using perceptual noise criteria," </title> <journal> IEEE Journal of Selected Areas in Communications, </journal> <volume> Vol. 6, </volume> <pages> pp. 314-323, </pages> <month> Feb. </month> <year> 1988. </year>
Reference-contexts: For discussions of transform coding for images see [377, 297, 267, 197, 77, 268, 192, 298, 149, 160, 293, 317]. More recently transform coding has also been widely used in high fidelity audio coding <ref> [202, 152] </ref>.
Reference: [203] <author> B.-H. Juang and A. H. Gray, Jr., </author> <title> "Multiple stage vector quantization for speech coding," </title> <booktitle> Proc. Intl. Conf. on Acoust. Speech, and Signal Processing, </booktitle> <volume> Vol. 1, </volume> <pages> pp. 597-600, </pages> <address> Paris, </address> <month> April </month> <year> 1982. </year>
Reference-contexts: Tree-structured VQ uses a tree-structured reproduction codebook with a matched tree-structured search algorithm [58, 308, 74]. A tree-structured VQ with far less memory is provided by a multistage or residual VQ <ref> [203, 182, 29, 223, 133, 66, 29, 289, 309, 30] </ref>. A variety of product vector quantizers use a cartesian product reproduction codebook, which often can be rapidly searched. <p> See for example the work of Nobel and Olshen [277, 278, 279]. Multistage Vector Quantization Multistage (or multistep or cascade or residual) vector quantization was introduced by Juang and A.H. Gray <ref> [203] </ref> as a form of tree structured quantization with much reduced arithmetic complexity and storage.
Reference: [204] <author> J. C. Kieffer, </author> <title> "Exponential rate of convergence for Lloyd's method I," </title> <journal> IEEE Trans. on Information theory, </journal> <volume> Vol. 28, </volume> <pages> pp. 205-210, </pages> <month> March </month> <year> 1982. </year>
Reference-contexts: theory for vector quantizers and rth power distortion measures by Bucklew and Wise [51], Kieffer's demonstration of stochastic stability for a general class of feedback quantizers including the historic class of predictive quantizers and delta modulators along with adaptive generalizations [205], Kieffer's study of the convergence rate of Lloyd's algorithm <ref> [204] </ref>, and the demonstration by Garey, Johnson, and Witsenhausen that the Lloyd-Max optimization was NP-hard [142]. Towards the middle of the 1980's, several tutorial articles on vector quantization appeared, which greatly increased the accessibility of the subject [148, 162, 244, 265].
Reference: [205] <author> J. C. Kieffer, </author> <title> "Stochastic stability for feedback quantization schemes," </title> <journal> IEEE Trans. Inform. Theory, </journal> <volume> vol. IT-28, </volume> <pages> pp. 248-254, </pages> <month> March </month> <year> 1982. </year>
Reference-contexts: Because it has not been rigorously shown that one may apply Bennett's integral or the Panter-Dite formula directly to the prediction error, the analysis of such feedback quantization systems has proved to be notoriously difficult, with results limited to proofs of stability <ref> [145, 205, 206] </ref>, i.e. asymptotic stationarity, to analyses of distortion via Hermite polynomial expansions for Gaussian processes [95, 323, 15, 246, 179, 180, 193, 118, 143, 144, 262, 263, 264, 210], and to exact results for constant and sinusoidal signals using Rice's method, extensions of Panter, Clavier, and Grieg to quantizers <p> by Conway and Sloane [81], rigorous developments of the Bennett theory for vector quantizers and rth power distortion measures by Bucklew and Wise [51], Kieffer's demonstration of stochastic stability for a general class of feedback quantizers including the historic class of predictive quantizers and delta modulators along with adaptive generalizations <ref> [205] </ref>, Kieffer's study of the convergence rate of Lloyd's algorithm [204], and the demonstration by Garey, Johnson, and Witsenhausen that the Lloyd-Max optimization was NP-hard [142].
Reference: [206] <author> J. C. Kieffer and J. G. Dunham, </author> <title> "On a type of stochastic stability for a class of encoding schemes," </title> <journal> IEEE Trans. Inform. Theory, </journal> <volume> Vol. IT-29, </volume> <pages> pp. 703-797, </pages> <month> November </month> <year> 1983. </year>
Reference-contexts: Because it has not been rigorously shown that one may apply Bennett's integral or the Panter-Dite formula directly to the prediction error, the analysis of such feedback quantization systems has proved to be notoriously difficult, with results limited to proofs of stability <ref> [145, 205, 206] </ref>, i.e. asymptotic stationarity, to analyses of distortion via Hermite polynomial expansions for Gaussian processes [95, 323, 15, 246, 179, 180, 193, 118, 143, 144, 262, 263, 264, 210], and to exact results for constant and sinusoidal signals using Rice's method, extensions of Panter, Clavier, and Grieg to quantizers
Reference: [207] <author> T. Kim, </author> <title> "New Finite State Vector Quantizers for Images," </title> <booktitle> Proceedings of the Intl. Conf. on Acoustics, Speech, and Signal Processing, </booktitle> <pages> pp. 1180-1183, </pages> <address> New York, NY, </address> <month> Apr. </month> <year> 1988. </year> <title> [208] "A construction of optimum vector quantizers by simulated annealing," </title> <editor> H. Kodama, K. Wakasugi, and M. </editor> <title> Kasahara, </title> <journal> Transactions of the Institute of Electronics, Information and Communication Engineers B-I, </journal> <volume> vol.J74B-I, </volume> <pages> no.1 pp. 58-65, </pages> <month> Jan. </month> <year> 1991. </year>
Reference-contexts: The result is a finite-state version of a predictive quantizer, referred to as a finite-state vector quantizer and depicted in Figure 10. Although little theory has been developed for finite-state quantizers [120, 134, 135], a variety of design methods exist <ref> [130, 131, 105, 176, 13, 14, 207, 149] </ref>. Lloyd's optimal decoder extends in a natural way to finite-state vector quantizers, the optimal reproduction decoder is a conditional expection of the input vector given the binary codeword and the state.
Reference: [209] <author> T. Kohonen, </author> <title> Self-organization and Associative Memory, third edition, </title> <publisher> Springer-Verlag, </publisher> <address> Berlin, </address> <year> 1989. </year>
Reference-contexts: Some of the quantizer design algorithms developed as alternatives to Lloyd's algorithm include simulated annealing [107, 360, 125, 208], deterministic annealing [312], pairwise nearest neighbor [112] (which had its origins in earlier clustering techniques [370]), stochastic relaxation [394, 396], self organizing feature maps <ref> [209, 384, 385] </ref> and other neural nets [348, 217, 347, 241, 54].
Reference: [210] <author> T. Koski and S. </author> <title> Cambanis "On the statistics of the error in predictive coding for stationary Ornstein-Uhlenbeck processes," </title> <journal> IEEE Trans. Information Theory, Vol.38, </journal> <volume> pp.1029-40, </volume> <month> May </month> <year> 1992. </year>
Reference: [211] <author> T. Koski and L.-E. Persson, </author> <title> "On quantizer distortion and the upper bound for exponential entropy," </title> <journal> IEEE Trans. Information Theory, </journal> <volume> Vol. 37, pp.1168-1172, </volume> <month> July </month> <year> 1991. </year>
Reference-contexts: In some cases, analytical formulas can be found (e.g. Gaussian, Laplacian, gamma densities). In other cases numerical integration can be used. Upper bounds to fi 1 are given in <ref> [211] </ref>. To the authors' knowledge, for sources with memory, the Zador factors have been determined analytically only for Gaussian sources , where they depend on the covariance matrix. It is clearly of interest to have methods for estimating the Zador factors directly from training data from a source.
Reference: [212] <author> F. Kossentini, W.C. Chung, and M.J.T. Smith, </author> <title> "Subband image coding using entropy-constrained residual vector quantization," </title> <booktitle> Information Processing and Management, </booktitle> <volume> vol. 30, no. 6, </volume> <pages> pp. 887-896, </pages> <year> 1994. </year>
Reference-contexts: Early wavelet coding techniques emphasized scalar or lattice vector quantization [10, 11, 100, 321, 12, 27, 140] and other vector quantization techniques have also been applied to wavelet coefficients, including tree encoding [261], residual vector quantization <ref> [212] </ref>, and other methods [85]. A major breakthrough in performance and complexity came with the introduction of zerotrees [220, 328, 316], which provided an extremely efficient embedded representation of scalar quantized wavelet coefficients, called embedded zerotree wavelet (EZW) coding.
Reference: [213] <author> F. Kossentini, M.J.T. Smith and C.F. Barnes, </author> <title> "Image coding using entropy-constrained residual vector quantizationm" IEEE Trans. </title> <booktitle> Image Processing, </booktitle> <volume> Vol. 4, </volume> <pages> pp. 1349-1357, </pages> <month> Oct. </month> <year> 1995. </year>
Reference-contexts: And more sophisticated design algorithms (than the greedy one) can also have benefits [29, 133, 28, 30]. Variable-rate multistage quantizers have been developed <ref> [182, 213, 214, 309] </ref>.
Reference: [214] <author> F. Kossentini, M.J.T. Smith and C.F. Barnes, </author> <title> "Necessary conditions for the optimality of variable-rate residual vector quantizers," </title> <journal> IEEE Trans. Information Theory, </journal> <volume> Vol. 41, </volume> <pages> pp 1903-1914, </pages> <month> Nov </month> <year> 1995. </year>
Reference-contexts: And more sophisticated design algorithms (than the greedy one) can also have benefits [29, 133, 28, 30]. Variable-rate multistage quantizers have been developed <ref> [182, 213, 214, 309] </ref>.
Reference: [215] <author> H. P. Kramer and M. V. Mathews, </author> <title> "A linear coding for transmitting a set of correlated signals, </title> <journal> IRE Trans. Inform. Theory, </journal> <volume> Vol. 23, </volume> <pages> pp. 41-46, </pages> <month> Sept. </month> <year> 1956. </year>
Reference-contexts: The operation is depicted in Figure 4. This style of code was introduced in 1956 by Kramer and Mathews <ref> [215] </ref> and analyzed and popularized in 1962-3 by Huang and Schultheiss [183, 184]. Kramer and Mathews simply assumed that the goal of the transform was to decorrelate the symbols, but Huang and Schultheis proved that this was true for Gaussian sources with the high resolution assumption.
Reference: [216] <author> E. R. Kretzmer, </author> <title> "Statistics of television signals," </title> <journal> Bell Systems Technical Journal, </journal> <volume> Vol. 31, </volume> <pages> pp. 751-763, </pages> <month> July </month> <year> 1952. </year>
Reference-contexts: In 1950 Elias [108] provided an information theoretic development of the benefits of predictive coding, but the work was not published until 1955 [109]. Other early references include <ref> [283, 216, 177, 361, 397] </ref>. <p> The possibility of applying variable-length coding to quantization may well have occurred to any number of people who were familiar with both quantization and Shannon's 1948 paper. The earliest references to such that we have found are in the 1952 papers by Oliver [283] and Kretzmer <ref> [216] </ref>. 14 The first paper to analyze variable-rate quantization was Schutzenberger (1958) [320] who showed that the distortion of optimized variable-rate quantization (both scalar and vector) decreases with rate as 2 2R , just as with fixed-rate quantization.
Reference: [217] <author> A. K. Krishnamurthy, S. C. Ahalt, D. E. Melton, and P. Chen, </author> <title> "Neural networks for vector quantization of speech and images," </title> <journal> IEEE Journal on Selected Areas in Communications, </journal> <volume> Vol. 8, No. 8, </volume> <pages> pp. 1449-1457, </pages> <month> Oct. </month> <year> 1990. </year>
Reference-contexts: Some of the quantizer design algorithms developed as alternatives to Lloyd's algorithm include simulated annealing [107, 360, 125, 208], deterministic annealing [312], pairwise nearest neighbor [112] (which had its origins in earlier clustering techniques [370]), stochastic relaxation [394, 396], self organizing feature maps [209, 384, 385] and other neural nets <ref> [348, 217, 347, 241, 54] </ref>.
Reference: [218] <author> F. Kuhlmann and J. A. Bucklew, </author> <title> "Piecewise uniform vector quantizers," </title> <journal> IEEE Trans. Inform. Theory, vol.34, no.5, </journal> <volume> pt.2, </volume> <pages> pp. 1259-63, </pages> <month> Sept. </month> <year> 1988. </year>
Reference-contexts: Good schemes of this sort have even been developed for low to moderate rates by Gibson [200, 201] and Pan and Fischer [288, 289]. Cell-conditioned two-stage quantizers can be viewed as having a piecewise constant point density of the sort proposed earlier by Kuhlmann and Bucklew <ref> [218] </ref> as a means of circumventing the fact that optimal vector quantizers cannot be implemented with companders. This approach was further developed by Swaszek in [343].
Reference: [219] <author> R. Laroia and N. Farvardin, </author> " <title> A structured fixed-rate vector quantizer derived from a variable-length scalar quantizer. I. Memoryless sources, II Vector sources" IEEE Transactions on Information Theory, </title> <journal> vol.39, </journal> <volume> no.3, </volume> <pages> pp. 851-876, </pages> <month> May </month> <year> 1993. </year>
Reference-contexts: While wavelet advocates may credit the decomposition itself for the gains in compression, the theory suggests that rather it is the fact that vector entropy coding for very 65 large vectors is feasible. Scalar-vector Quantization Like permutation vector quantization and Fischer's pyramid vector quantizer, Laroia and Far-vardin's <ref> [219] </ref> scalar-vector quantization attempts to match the performance of an optimal entropy constrained scalar quantizer with a low complexity fixed-rate structured vector quantizer. A derivative technique called block constrained quantization [21, 24, 20, 25] is simpler and easier to describe. <p> A high resolution analysis is given in [23, 20]. The scalar-vector method extends to sources with memory by combining it with transform coding using a decorrelating or approximately decorrelating transform <ref> [219] </ref>. Tree-Structured Quantization In its original and simplest form, a k dimensional tree-structured vector quantizer (TSVQ) [58] is a fixed-rate quantizer with, say, rate R whose encoding is guided by a balanced (fixed-depth) binary tree of depth kR.
Reference: [220] <author> A. S. Lewis and G. Knowles, </author> <title> "Image compression using the 2-D wavelet transform," </title> <journal> IEEE Trans. Image Processing, </journal> <volume> vol. 1, no. 2, </volume> <pages> pp. 244-250, </pages> <month> April </month> <year> 1992. </year>
Reference-contexts: A major breakthrough in performance and complexity came with the introduction of zerotrees <ref> [220, 328, 316] </ref>, which provided an extremely efficient embedded representation of scalar quantized wavelet coefficients, called embedded zerotree wavelet (EZW) coding. The zerotree approach has been extended to vector quantization [86], but the slight improvement comes at a significant cost in added complexity.
Reference: [221] <author> V.I. Levenshtein, </author> <title> "Binary codes capable of correcting deletions, </title> <journal> insertions, and reversals," Sov. Phys.-Dokl., </journal> <volume> Vol. 10, </volume> <pages> pp. 707-710, </pages> <year> 1966. </year> <title> [222] "Asymptotic Quantization Error and Cell-Conditioned Two-Stage Vector," </title> <type> Ph.D. Dissertation, </type> <institution> University of Michigan, </institution> <month> Dec. </month> <year> 1990. </year>
Reference-contexts: kl ((x l+1 ; : : : ; x k ); (y l+1 ; : : : ; y k )) (39) and the subadditive ergodic theorem will still lead to positive and negative coding theorems [243, 165]. 9 An example of a subadditive distortion measure is the Levenshtein distance <ref> [221] </ref> which counts the number of insertions and deletions along with the number of changes that it takes to convert one sequence into another.
Reference: [223] <author> D.H. Lee and D.L. Neuhoff, </author> <title> "Conditionally corrected two-stage vector quantization," </title> <booktitle> Conf. on Information Sciences and Systems, </booktitle> <publisher> Princeton, </publisher> <pages> pp. 802-806, </pages> <month> March </month> <year> 1990. </year>
Reference-contexts: Tree-structured VQ uses a tree-structured reproduction codebook with a matched tree-structured search algorithm [58, 308, 74]. A tree-structured VQ with far less memory is provided by a multistage or residual VQ <ref> [203, 182, 29, 223, 133, 66, 29, 289, 309, 30] </ref>. A variety of product vector quantizers use a cartesian product reproduction codebook, which often can be rapidly searched. <p> And more sophisticated design algorithms (than the greedy one) can also have benefits [29, 133, 28, 30]. Variable-rate multistage quantizers have been developed [182, 213, 214, 309]. Finally, we mention that it has been observed by Lee and Neuhoff <ref> [223, 222] </ref> that if the first stage quantizer has high rate, say R 1 , then by Gersho's conjecture the first stage cells all have approximately the same shape and the source density is approximately constant on them. <p> Direct implementation of such cell-conditioned two stage VQ <ref> [223, 226] </ref> requires the storing of a scale factor and a rotation for each first stage cell, which are used to scale and rotate the first stage residual before quantization by the second stage. Their inverses are applied subsequently.
Reference: [224] <author> D.H. Lee and D.L. Neuhoff, </author> <title> "An asymptotic analysis of two-stage vector quantization," </title> <booktitle> 1991 IEEE Int'l Symp. on Inform. Thy, </booktitle> <address> Budapest, p. 316, </address> <month> June </month> <year> 1991. </year>
Reference-contexts: On the one hand, a rate-distortion theory analysis [113] has shown that there are situations where successive approximation can be done without loss of optimality. On the other hand, high resolution analyses of TSVQ [273] and two-stage VQ <ref> [224] </ref> have quantified the loss of these particular codes, and in the latter case shown ways of modifying the quantizer to eliminate the loss. <p> However, two-stage VQ's designed in this way work fairly well. A high resolution analysis of two-stage VQ using Bennett's integral on the second stage can be found in <ref> [224, 222] </ref>. In order to apply Bennett's integral, it was necessary to find the form of the probability density of the quantization error produced by the first stage.
Reference: [225] <author> D.H. Lee and D.L. Neuhoff, </author> <title> "Asymptotic Distribution of the Errors in Scalar and Vector Quantizers," </title> <journal> IEEE Trans. Information Theory, </journal> <volume> Vol. 42, </volume> <pages> pp. 446-460, </pages> <month> March </month> <year> 1996. </year>
Reference-contexts: In order to apply Bennett's integral, it was necessary to find the form of the probability density of the quantization error produced by the first stage. This motivated the asymptotic error density analysis of vector quantization in <ref> [225, 271] </ref>. (An interesting sidelight of this work is that it turns out that one can learn much about the point density and cell shapes of a high resolution quantizer from the histogram of the lengths of the quantization errors.) Multistage quantizes have been improved in a number of ways.
Reference: [226] <author> D.H. Lee, D.L. Neuhoff and K.K. Paliwal, </author> <title> "Cell-conditioned two-stage vector quantization of speech," </title> <booktitle> Proc. IEEE ICASSP, </booktitle> <volume> Toronto Vol. 4, </volume> <pages> pp. 653-656, </pages> <month> May </month> <year> 1991. </year>
Reference-contexts: Direct implementation of such cell-conditioned two stage VQ <ref> [223, 226] </ref> requires the storing of a scale factor and a rotation for each first stage cell, which are used to scale and rotate the first stage residual before quantization by the second stage. Their inverses are applied subsequently.
Reference: [227] <author> J. Li, N. Chaddha, and R.M. Gray, </author> <title> "Asymptotic performance of vector quantizers with a perceptual distortion measure," </title> <booktitle> 1997 IEEE International Symposium on Information Theory, </booktitle> <address> Ulm, Germany, </address> <month> June </month> <year> 1997. </year> <note> (Full paper submitted for possible publication. Preprint available at http://www-isl.stanford.edu/~gray/compression.html.) </note>
Reference-contexts: to a class of distortion measures which have proved useful in perceptual coding, the input weighted quadratic distortion measures of the form d (x; ^x) = (x ^x) t B x (x ^x); (18) where B x is a nonnegative definite matrix that depends on the input. (See, for example, <ref> [276, 275, 115, 141, 227, 234, 233] </ref>.) Most of the theory and design techniques considered here extend to such measures. <p> Gardner and Rao [141] used a similar d (x; y) to model perceptual distortion measure for speech. The matrix B (y) is the `sensitivity matrix' in [141]. We follow <ref> [227] </ref> and place additional regularity constraints in 9 This differs slightly from the previous definition of subadditive because the d k are not assumed to be normalized. The previous definition applied to d k =k is equivalent to this definition. 45 order to ensure the desired asymptotics work. <p> These distortion measures have been found useful for modeling perceptual masking effects in speech and image coding. The Bennett integral has been extended to this type of distortion and approximations for both fixed-rate and variable-rate operational distortion-rate functions have been developed <ref> [141, 227] </ref>. Parallel results for Shannon lower bounds to the rate-distortion function have been developed by Linder and Zamir [234] and similar results for multidimensional companding with lattice codes for similar distortion measures have been developed by Linder, Zamir, and Zeger [233]. <p> High resolution theory has the most results for rth power difference distortion measures, and as mentioned previously, some of its results have recently extended to nondifference distortion measures such as (x y) t B x (x y) <ref> [141, 227, 233] </ref>. In any event both theories are the most fully developed for squared error distortion measure, expecially for Gaussian sources. In addition, both theories require a finite moment condition, specific to the distortion measure.
Reference: [228] <author> Y. Linde, A. Buzo and R. M. Gray, </author> <title> "An algorithm for vector quantizer design," </title> <journal> IEEE Trans. on Comm., </journal> <volume> Vol. COM-28, </volume> <pages> pp. 84-95, </pages> <month> Jan. </month> <year> 1980. </year>
Reference-contexts: In 1980 Linde, Buzo, and Gray explicitly extended Lloyd's algorithm to vector quantizer design <ref> [228] </ref>.
Reference: [229] <author> Y. Linde and R. M. Gray, </author> <title> "A fake process approach to data compression," </title> <journal> IEEE Trans. on Comm., </journal> <volume> Vol. COM-26, </volume> <pages> pp. 840-847, </pages> <month> June </month> <year> 1978. </year>
Reference-contexts: While linear encoders sufficed for channel coding, nonlinear decoders were required for the source coding application, and a variety of design algorithms were developed for designing the decoder to populate the trellis searched by the encoder <ref> [229, 375, 337, 16, 33] </ref>.
Reference: [230] <author> T. Linder, </author> <title> "On asymptotically optimal companding quantization," </title> <journal> Problems of Control and Information Theory, </journal> <volume> Vol. 20, No. 6, </volume> <pages> pp. 465-484, </pages> <year> 1991. </year>
Reference-contexts: It also contained a generalization to random vectors without probability densities, i.e. with distributions that are not absolutely continuous or even continuous. The paper also gave the first rigorous approach to the derivation of Bennett's integral for scalar quantization via companding. However, as pointed out by Linder (1991) <ref> [230] </ref>, there was "a gap in the proof concerning the convergence of Riemann sums with increasing support to a Riemann integral." Linder fixed this and presented a correct derivation with weaker assumptions.
Reference: [231] <author> T. Linder and K. Zeger, </author> <title> "Asymptotic Entropy-Constrained Performance of Tesselating and Universal Randomized Lattice Quantization," </title> <journal> IEEE Trans. Information Theory, </journal> <volume> Vol. 40, </volume> <pages> pp. 575-579, </pages> <month> Mar. </month> <year> 1994. </year> <title> [232] "On the asymptotic tightness of the Shannon lower bound," </title> <journal> IEEE Trans. Information Theory, </journal> <volume> Vol. 40, </volume> <pages> pp. 2026-2031, </pages> <month> Nov. </month> <year> 1994. </year>
Reference-contexts: This result was independently rederived in a simpler fashion by Newman (1964) [274]. The lower bound is asymptotically achievable by a lattice with hexagonal cells; see <ref> [231] </ref> for a rigorous proof. It follows then that the ratio of ffi 2 (R) to M (hexagon)oe 2 2 2R tends to one, and also, that Gersho's conjecture holds for dimension two. Zador's thesis (1963) [389] was the next rigorous work. As mentioned earlier, it contains two principal results. <p> This is like Bennett's integral in that f (1) (x), and consequently (x), can be arbitrary, but like Zador's result (or Gersho's generalization of Bennett's integral [147]) in that, in essence, it is assumed that the quantizers have optimal cell shapes. 50 In 1994 Linder and Zeger <ref> [231] </ref> generalized (36) to quantizers generated by tesselations by showing that the quantizer q ff based formed by tesselating with some basic cell shape S scaled by a postive number ff, has average (narrow sense) rth-power distortion D ff satisfying lim D ff = 1 They then combined the above with
Reference: [233] <author> T. Linder, R. Zamir, K. </author> <title> Zeger "High resolution source coding for non-differencedistortion measures: </title> <journal> multidimensionalcompanding," </journal> <note> submitted to IEEE Trans. Information Theory. </note>
Reference-contexts: to a class of distortion measures which have proved useful in perceptual coding, the input weighted quadratic distortion measures of the form d (x; ^x) = (x ^x) t B x (x ^x); (18) where B x is a nonnegative definite matrix that depends on the input. (See, for example, <ref> [276, 275, 115, 141, 227, 234, 233] </ref>.) Most of the theory and design techniques considered here extend to such measures. <p> Parallel results for Shannon lower bounds to the rate-distortion function have been developed by Linder and Zamir [234] and similar results for multidimensional companding with lattice codes for similar distortion measures have been developed by Linder, Zamir, and Zeger <ref> [233] </ref>. <p> High resolution theory has the most results for rth power difference distortion measures, and as mentioned previously, some of its results have recently extended to nondifference distortion measures such as (x y) t B x (x y) <ref> [141, 227, 233] </ref>. In any event both theories are the most fully developed for squared error distortion measure, expecially for Gaussian sources. In addition, both theories require a finite moment condition, specific to the distortion measure.
Reference: [234] <author> T. Linder and R. Zamir, </author> <title> "High-resolution source coding for non-difference distortion measures: The rate distortion function," </title> <booktitle> Proc. 1997 IEEE International Symposium on Information Theory, </booktitle> <address> Ulm, Germany, p. 187, </address> <month> June </month> <year> 1997. </year> <note> Also submitted to IEEE Trans. Information Theory. </note>
Reference-contexts: to a class of distortion measures which have proved useful in perceptual coding, the input weighted quadratic distortion measures of the form d (x; ^x) = (x ^x) t B x (x ^x); (18) where B x is a nonnegative definite matrix that depends on the input. (See, for example, <ref> [276, 275, 115, 141, 227, 234, 233] </ref>.) Most of the theory and design techniques considered here extend to such measures. <p> The Bennett integral has been extended to this type of distortion and approximations for both fixed-rate and variable-rate operational distortion-rate functions have been developed [141, 227]. Parallel results for Shannon lower bounds to the rate-distortion function have been developed by Linder and Zamir <ref> [234] </ref> and similar results for multidimensional companding with lattice codes for similar distortion measures have been developed by Linder, Zamir, and Zeger [233]. <p> applies primarily to additive distortion measures; i.e. distortion measures of the form d (x; y) = P k i=1 d 1 (x; y) (or a normalized version),though there are some results for subadditive distortion measures [243, 165] and some for distortion measures such as (xy) t B x (x y) <ref> [234] </ref>. High resolution theory has the most results for rth power difference distortion measures, and as mentioned previously, some of its results have recently extended to nondifference distortion measures such as (x y) t B x (x y) [141, 227, 233].
Reference: [235] <author> Yu. N. Linkov, </author> <title> "Evaluation of epsilon entropy of random variables for small epsilon," </title> <journal> Problems of Information Transmission, </journal> <volume> Vol. 1, </volume> <pages> pp. 12-18, </pages> <year> 1965. </year> <journal> (Translated from Problemy Peredachi Informatsii, </journal> <volume> Vol. 1, </volume> <pages> 18-26.) </pages>
Reference-contexts: For other cases, the Blahut algorithm [42] can be used to compute D k (R), though its computational complexity becomes overwhelming unless k is small. Due to the difficulty of computing it, many (mostly lower) bounds to the Shannon distortion-rate function have been developed (c.f. <ref> [327, 235, 37, 164] </ref>). One important upper bound derives from the fact that with respect to squared error, the Gaussian source has the largest Shannon distortion-rate function (kth-order or in the limit) of any source with the same covariance function.
Reference: [236] <author> S.P. Lipshitz, R.A. Wannamaker, and J. Vanderkooy, </author> <title> "Quantization and dither: a theoretical survey," </title> <journal> Journal of the Audio Engineering Society, vol.40, </journal> <volume> no.5, </volume> <pages> pp. 355-75, </pages> <month> May </month> <year> 1992. </year>
Reference-contexts: The properties of nonsubtractive dither were originally developed in unpublished work by Wright [382] in 1979 and Brinton [44] in 1984 and subsequently extended and refined with a variety of proofs <ref> [363, 362, 236, 172] </ref>.
Reference: [237] <author> S. P. Lloyd. </author> <title> Least squares quantizationin PCM. </title> <institution> Unpublished Bell Laboratories Technical Note. Portions presented at the Institute of Mathematical Statistics Meeting Atlantic City New Jersey September 1957. </institution> <note> Published in the March 1982 special issue on quantization of the IEEE Transactions on Information Theory, </note> <year> 1957. </year>
Reference-contexts: (or high rate or asymptotic) quantization theory, which had its origins in the 1948 paper on PCM by Oliver, Pierce, and Shannon [282] and the 1948 paper on quantization error spectra by Bennett [35] and its subsequent development in 1951 by Panter and Dite [290] and in 1957 by Lloyd <ref> [237] </ref>. Much of the history and state of the art of quantization derives from these seminal works, half of which first appeared in 1948. In contrast to these two asymptotic theories, there is also a small but important collection of results that are not asymptotic in nature. <p> The most important nonasymptotic results, however, are the basic optimality conditions and iterative descent algorithms for quantizer design, such as first developed by Steinhaus (1956) [325] and Lloyd (1957) <ref> [237] </ref>, and later popularized by Max (1960) [250]. <p> It is useful to jump ahead and point out that g can be interpreted, as Lloyd would explicitly point out in 1957 <ref> [237] </ref>, as a constant times a "quantizer point density function (x)," that is, a function with the property that for any region S number of quantizer levels in S N Z (x) dx: (5) Integrating (x) over a region gives the fraction of quantizer reproduction levels in the region so that <p> Aigrain. For example, if the input density is Gaussian with variance oe 2 , then [146] ffi (R) = 12 p ffi (R) can also be derived directly from Bennett's integral using variational methods (as did Lloyd (1957) <ref> [237] </ref> and Smith (1957) [334] and, much later without apparent knowledge of these early works, by Roe (1964) [311]). It can also be derived without using variational methods by application of Holder's inequality to Bennett's integral [168], demonstrating that the minimum is global. <p> Motivated by the asymptotic work of Panter and Dite, Lloyd (1957) <ref> [237] </ref> found necessary and sufficient conditions for a fixed-rate quantizer to be locally optimal; i.e. conditions that if satisfied implied that small perturbations to the levels or thresholds would increase distortion. <p> One such iteration of T must decrease or leave unchanged the average distortion. Iterate until convergence or the improvement falls beneath some threshold. This algorithm is an extension and variation on the algorithm for optimal scalar quantizer design introduced for fixed-rate scalar quantization by Lloyd <ref> [237] </ref>. The algorithm is a fixed-point algorithm since if the algorithm converges to a code, the code must be a fixed point with respect to T . <p> Though the notion of point density would no doubt have been recognizable to the earliest contributors such as Bennett, Panter and Dite, as mentioned earlier, it was not explicitly introduced until Lloyd's work <ref> [237] </ref>. In nonuniform scalar and vector quantization, there is the additional issue of codevector placement within cells and, in the latter case, of cell shape. <p> f k (x) dx: (23) For scalar quantizers (k = 1) with points in the middle of the cells, this reduces to D = 12 N 2 1 f 1 (x) dx (24) which is what Bennett [35] found for companders, as restated in terms of point densities by Lloyd <ref> [237] </ref>. Both (24) and the more general formula (23) are called Bennett's integral. <p> When k = 1, Z (R) reduces to the formula for scalar quantization first found by Panter and Dite [290]. Cleaner derivations were given by Smith (1957) [334] and Lloyd (1957) <ref> [237] </ref>. From the form of fl k (x) one may straightforwardly deduce that cells are smaller and have higher probability where f k (x) is larger, and that all cells contribute roughly the same to the distortion; i.e. D i in (22) is approximately the same for all i. <p> The earliest quantizer distortion analyses to appear in the open literature [35, 290, 334] assumed finite range and used the density-approximately-constant-in-cells assumption. Several papers avoided the latter by using a Taylor series expansion of the source density. For example, Lloyd <ref> [237] </ref> used this approach to show that, ignoring overload distortion, the approximation error in the Panter-Dite formula is o (1=N 2 ), which means that it tends to zero, even when divided by 1=N 2 . Roe [311], Algazi [6] and Wood [379] also used Taylor series. <p> such that any k-dimensional quantizer with finitely or infinitely many cells, and output entropy H, has distortion at least K k;r 2 (r=k)H . (kxk j kxk 2 ) Moreover, there exists K k;r &gt; K 0 k;r and a sequence of quantizers with increasing output entropies 10 Though Lloyd <ref> [237] </ref> gave a fairly rigorous analysis of distortion, we do not include his paper in the this category because it ignored overload distortion. 47 H and distortion no more than K 0 k;r 2 (r=k)H . <p> Both theories have been extended to to continuous-time random processes. However, the high-resolution results are somewhat sketchy <ref> [35, 237, 155] </ref>. Both can be applied to two or higher dimensional sources such as images or video. Both have have been developed the most for Gaussian sources in the context of squared error distortion, which is not surprising in view of the tractability of squared error and Gaussianity.
Reference: [238] <author> T. D. Lookabaugh and R. M. Gray, </author> <title> "High-resolution quantization theory and the vector quantizer advantage," </title> <journal> IEEE Trans. on Information theory, </journal> <volume> Vol. 35, </volume> <pages> pp. 1020-1033, </pages> <month> Sep. </month> <year> 1989. </year>
Reference-contexts: Motivated by several prior explanations <ref> [244, 238, 260] </ref>, we offer the following, which is an extension of these. We wish to compare an optimal quantizer, q k , with dimension k to an optimal k 0 -dimensional quantizer, q k 0 with k 0 &gt;> k. <p> k 0 j 1=k 0 f k 0 (x) dx R 1 Q k 0 sq (x i ) R 1 pr;k 0 (x) = L sp fi L ob fi L pt (34) where the cell shape loss has been factored into the product of a space filling loss <ref> [238] </ref> 6 , L sp , which is the ratio of the normalized moment of inertia of a cube to that of a high dimensional sphere, and an oblongitis loss, L ob , which is the factor by which the rectangularity of the cells makes the cell shape loss larger than <p> The sum of these, 2.81 dB, which equals 10 log 10 fi 1 =fi, has been called the "shape loss" <ref> [238] </ref> because it is determined by the shape of the density the more uniform the density the less need for compromise because the scalar point densities leading to best product cell shapes and best point density are more similar. For a uniform source density, there is no shape loss. <p> For sources with memory, scalar quantization (k = 1) engenders an additional loss due to its inability to exploit the dependence between source samples. More specifically, when there is dependence/correlation between source samples, the product point density cannot match the ideal point density, not even approximately. See <ref> [238, 260] </ref> for a definition of memory loss. (One can factor both the point density and oblongitis losses into two terms, one of which is due to the quantizer's inability to exploit memory.) There is also a memory loss for k-dimensional quantization, which decreases to 1 as k increases.
Reference: [239] <author> A. Lowry, S. Hossain and W. Millar, </author> <title> "Binary search trees for vector quantization," </title> <booktitle> Proc. ICASSP, Dallas, </booktitle> <pages> pp. 2206-2208, </pages> <year> 1987. </year>
Reference-contexts: Then to encode a source vector x, one applies the prequantization, finds the index of the prequantization cell in which x is contained, and performs a full search on the corresponding bucket for the closest codevector to x. Techniques of this type may be found in <ref> [36, 132, 71, 72, 239, 112] </ref>. Another class of techniques is like the previous except that the low complexity prequantization has much smaller cells than the Voronoi cells of C, i.e. it is finer.
Reference: [240] <author> J. Lukaszewicz and H. Steinhaus, </author> <title> "On measuring by comparison," </title> <journal> Zastos. Mat., </journal> <volume> Vol. 2, </volume> <pages> pp. 225-231, </pages> <year> 1955. </year> <note> (In Polish.) </note>
Reference-contexts: Specifically, in 1950-1951 Dalenius et al. [93, 94] used variational techniques to consider optimal grouping of Gaussian data with respect to average squared error. Lukaszewicz and H. Steinhaus <ref> [240] </ref> (1955) developed what we now consider to be the Lloyd optimality conditions using variational techniques in a study of optimum go/no-go gauge sets (as acknowledged by Lloyd). Cox in 1957 [88] also derived similar conditions.
Reference: [241] <author> S.P. Luttrell, </author> <title> "Self-supervised training of hierarchical vector quantizers," </title> <booktitle> II Int'l. Conf. Artificial Neural Networks, Conf. Publ. </booktitle> <volume> No. 349, London:IEE, </volume> <pages> pp. 5-9, </pages> <year> 1991. </year>
Reference-contexts: Some of the quantizer design algorithms developed as alternatives to Lloyd's algorithm include simulated annealing [107, 360, 125, 208], deterministic annealing [312], pairwise nearest neighbor [112] (which had its origins in earlier clustering techniques [370]), stochastic relaxation [394, 396], self organizing feature maps [209, 384, 385] and other neural nets <ref> [348, 217, 347, 241, 54] </ref>.
Reference: [242] <author> D.F. Lyons and D.L. Neuhoff, </author> <title> "A coding theorem for low-rate transform codes," </title> <booktitle> Proc. IEEE Int'l Symp. on Inform. Thy, </booktitle> <address> San Antonio, TX, p. 333, </address> <month> Jan. </month> <year> 1993. </year>
Reference: [243] <author> K. M. Mackenthun and M. B. Pursley, </author> <title> "Strongly and weakly universal source coding," </title> <editor> K. M. Mackenthun and M. B. Pursley," </editor> <booktitle> Proceedings of the 1977 Conference on Information Science and Systems, </booktitle> <publisher> Johns Hopkins University, </publisher> <pages> pp. 286-291, </pages> <year> 1977. </year>
Reference-contexts: l ); (y 1 ; : : : ; y l )) + d kl ((x l+1 ; : : : ; x k ); (y l+1 ; : : : ; y k )) (39) and the subadditive ergodic theorem will still lead to positive and negative coding theorems <ref> [243, 165] </ref>. 9 An example of a subadditive distortion measure is the Levenshtein distance [221] which counts the number of insertions and deletions along with the number of changes that it takes to convert one sequence into another. <p> Applicability: Distortion Measures Shannon rate-distortion theory applies primarily to additive distortion measures; i.e. distortion measures of the form d (x; y) = P k i=1 d 1 (x; y) (or a normalized version),though there are some results for subadditive distortion measures <ref> [243, 165] </ref> and some for distortion measures such as (xy) t B x (x y) [234].
Reference: [244] <author> J. Makhoul, S. Roucos, and H. Gish, </author> <title> "Vector quantization in speech coding," </title> <journal> Proc. IEEE, </journal> <volume> Vol. 73, </volume> <pages> pp. 1551-1588, </pages> <month> Nov. </month> <year> 1985. </year> <month> 84 </month>
Reference-contexts: Towards the middle of the 1980's, several tutorial articles on vector quantization appeared, which greatly increased the accessibility of the subject <ref> [148, 162, 244, 265] </ref>. The Mid 1980's to the Present In the late 80's a wide variety of vector quantizer design algorithms were developed and tested for speech, images, video, and other signal sources. <p> Motivated by several prior explanations <ref> [244, 238, 260] </ref>, we offer the following, which is an extension of these. We wish to compare an optimal quantizer, q k , with dimension k to an optimal k 0 -dimensional quantizer, q k 0 with k 0 &gt;> k. <p> The TSVQ will still be competitive in terms of throughput, however, as the tree-structured search is amenable to pipelining. TSVQ's can be generalized to unbalanced trees (with variable depth as opposed to the fixed depth discussed above) <ref> [244, 74, 306, 149] </ref> and with larger branching factors than two or even variable branching factors [318]. <p> One can grow a balanced tree by splitting all nodes in each level of the tree, or by splitting one node at a time, e.g., by splitting the node with the largest contribution to the distortion <ref> [244] </ref> 67 or in a greedy fashion to to maximize the decrease in distortion for the increase in rate [306]. Once grown, the tree can be pruned by removing all descendents of any internal node, thereby making it a leaf.
Reference: [245] <author> J. MacQueen, </author> <title> "Some methods for classification and analysis of multivariate observations," </title> <booktitle> Proc. of the Fifth Berkeley Symposium on Math. Stat. and Prob., </booktitle> <volume> Vol. 1, </volume> <pages> pp. 281-296, </pages> <year> 1967. </year>
Reference-contexts: A similar algorithm was introduced in 1965 by Forgey 1965 [128], Ball and Hall [26, 174], Jancey [194], and in 1969 by MacQueen's "k-means" algorithm <ref> [245] </ref>.
Reference: [246] <author> E. Masry and S. Cambanis, </author> <title> "Delta modulation of the Wiener Process," </title> <journal> IEEE Trans. on Information Theory, </journal> <volume> Vol. 23, </volume> <month> pp.1297-1300 Nov. </month> <year> 1975. </year>
Reference: [247] <author> M. W. Marcellin and T. R. Fischer, </author> <title> "Trellis coded quantization of memoryless and Gauss-Markov sources," </title> <journal> IEEE Trans. on Communications, </journal> <volume> Vol. 38, </volume> <pages> pp. 82-93, </pages> <year> 1990. </year>
Reference-contexts: A variety of product vector quantizers use a cartesian product reproduction codebook, which often can be rapidly searched. Examples include polar vector quantizers [291, 292, 47, 376, 345, 346, 339, 342, 344], mean-removed vector quantizers [18, 19], and shape-gain vector quantizers [313, 281]. Trellis-coded quantizers <ref> [249, 247, 123, 124, 248, 324] </ref> use a Viterbi algorithm encoder matched to a reproduction codebook with a trellis structure. Hierarchical table-lookup vector quantizers provide fixed-rate vector quantizers with minimal computational complexity, both lossy encoder and reproduction decoder being accomplished by table-lookups [69, 255, 367, 62]. <p> Trellis Coded Quantization Trellis coded quantization, both scalar and vector, improve upon traditional encoded systems by labeling the trellis branches with entire subcodebooks (or "subsets") rather than with individual reproduction levels <ref> [249, 247, 123, 124, 369, 248, 324] </ref>. The primary gain resulting is a reduction in encoder complexity for a given level of performance.
Reference: [248] <author> M. W. Marcellin, </author> <title> "On entropy-constrained trellis-coded quantization," </title> <journal> IEEE Trans. on Communications, </journal> <volume> Vol. 42, </volume> <pages> pp. 14-16, </pages> <month> Jan. </month> <year> 1994. </year>
Reference-contexts: A variety of product vector quantizers use a cartesian product reproduction codebook, which often can be rapidly searched. Examples include polar vector quantizers [291, 292, 47, 376, 345, 346, 339, 342, 344], mean-removed vector quantizers [18, 19], and shape-gain vector quantizers [313, 281]. Trellis-coded quantizers <ref> [249, 247, 123, 124, 248, 324] </ref> use a Viterbi algorithm encoder matched to a reproduction codebook with a trellis structure. Hierarchical table-lookup vector quantizers provide fixed-rate vector quantizers with minimal computational complexity, both lossy encoder and reproduction decoder being accomplished by table-lookups [69, 255, 367, 62]. <p> Trellis Coded Quantization Trellis coded quantization, both scalar and vector, improve upon traditional encoded systems by labeling the trellis branches with entire subcodebooks (or "subsets") rather than with individual reproduction levels <ref> [249, 247, 123, 124, 369, 248, 324] </ref>. The primary gain resulting is a reduction in encoder complexity for a given level of performance.
Reference: [249] <author> M. W. Marcellin, T. R. Fischer, and J. D. Gibson, </author> <title> "Predictive trellis coded quantization of speech," </title> <journal> IEEE Trans. Acoustics, Speech, and Signal Processing Vol. </journal> <volume> 38, </volume> <pages> pp. 46-55, </pages> <month> Jan. </month> <year> 1990. </year>
Reference-contexts: A variety of product vector quantizers use a cartesian product reproduction codebook, which often can be rapidly searched. Examples include polar vector quantizers [291, 292, 47, 376, 345, 346, 339, 342, 344], mean-removed vector quantizers [18, 19], and shape-gain vector quantizers [313, 281]. Trellis-coded quantizers <ref> [249, 247, 123, 124, 248, 324] </ref> use a Viterbi algorithm encoder matched to a reproduction codebook with a trellis structure. Hierarchical table-lookup vector quantizers provide fixed-rate vector quantizers with minimal computational complexity, both lossy encoder and reproduction decoder being accomplished by table-lookups [69, 255, 367, 62]. <p> Trellis Coded Quantization Trellis coded quantization, both scalar and vector, improve upon traditional encoded systems by labeling the trellis branches with entire subcodebooks (or "subsets") rather than with individual reproduction levels <ref> [249, 247, 123, 124, 369, 248, 324] </ref>. The primary gain resulting is a reduction in encoder complexity for a given level of performance.
Reference: [250] <author> J. Max, </author> <title> "Quantizing for minimum distortion," </title> <journal> IEEE Trans. on Information theory, </journal> <pages> pp. 7-12, </pages> <month> March </month> <year> 1960. </year>
Reference-contexts: The most important nonasymptotic results, however, are the basic optimality conditions and iterative descent algorithms for quantizer design, such as first developed by Steinhaus (1956) [325] and Lloyd (1957) [237], and later popularized by Max (1960) <ref> [250] </ref>. <p> Lloyd provided design examples for uniform, Gaussian, and Laplacian random variables and showed that the results were consistent with the high resolution approximations. Although Method II would initially gain more popularity when rediscovered in 1960 by Max <ref> [250] </ref>, it is Method I that easily extends to vector quantizers and quantizers with structural constraints. Unfortunately Lloyd's work was not published in an archival journal at the time. Instead, it appeared as a Bell Laboratories Technical Memorandum and was presented at the 1957 Institute of Mathematical Statistics (IMS) meeting. <p> In 1959 Shteyn [333] added terms representing overload distortion to the 2 =12 formula and to Bennett's integral and used them optimize uniform and nonuniform quantizers. Unaware of prior work except for Bennett, he rederived the optimal compressor characteristic and the Panter-Dite formula. In 1960 Max <ref> [250] </ref> published a variational proof of the Lloyd optimality properties for rth power distortion measures, rediscovered Lloyd's Method II, and numerically investigated the design of fixed-rate quantizers for a varity of input densities.
Reference: [251] <author> R. A. McDonald, </author> <title> "Signal-to-noise and idle channel performance of DPCM systems with particular application to voice signals," </title> <journal> Bell Systems Technical Journal, </journal> <volume> Vol. 45, </volume> <pages> pp. 1123-1151, </pages> <month> Sept. </month> <year> 1966. </year>
Reference-contexts: prediction error is sufficiently similar in form to that of the source that its operational distortion-rate function is smaller than that of the original source by, approximately, the ratio of the variance of the source to that of the prediction error, a quantity that is often called a prediction gain <ref> [251, 284, 338, 197] </ref>. Analyses of this form usually claim that under high resolution conditions the distribution of the prediction error approaches that of the error when predictions are based on past source samples rather than past reproductions.
Reference: [252] <author> J. Menez, F. Boeri, </author> <title> and D.J. Esteban, "Optimum quantizer algorithm for real-time block quantizing," </title> <booktitle> Proc. of the 1979 IEEE Intl Conf. on Acoustics, Speech, and Signal Processing, </booktitle> <pages> pp. 980-984, </pages> <year> 1979. </year>
Reference-contexts: Also in 1978, Adoul, Collin, and Dalle [3] used clustering ideas to design two-dimensional vector quantizers for speech coding. Caprio, Westin, and Esposito in 1978 [61] and Menez, Boeri, and Esteban in 1979 <ref> [252] </ref> also considered clustering algorithms for the design of vector quantizers with squared error and magnitude error distortion measures. The most important paper on quantization during the 1970s was without a doubt Gersho's pa 21 per on "Asymptotically optimal block quantization" [147].
Reference: [253] <author> N. Moayeri and D.L. Neuhoff, </author> <title> "Theory of Lattice-based fine-coarse vector quantization," </title> <journal> IEEE Trans. Information Theory, </journal> <volume> Vol. 37, </volume> <pages> pp. 1072-1084, </pages> <month> July </month> <year> 1991. </year>
Reference-contexts: Due to the fact that not every bucket contains one codevector, such techniques, which may be found in [69, 255, 254, 62, 166], do not do a perfect full search. Some quantitative analysis of the increased distortion is given in <ref> [253] </ref> for a case where the prequantization is a lattice quantizer. Other fast search methods include the partial distortion method of [71, 32] and the transform subspace domain approach of [67]. Consideration of methods based on prequantization leads to the question of how fine should the prequantization cells be.
Reference: [254] <author> N. Moayeri and D.L. Neuhoff, </author> <title> "Time-memory tradeoffs in vector quantizer codebook searching based on Decision Trees," </title> <journal> IEEE Trans. Speech and Audio Processing, </journal> <volume> Vol. 2, </volume> <pages> pp. 490-506, </pages> <month> Oct. </month> <year> 1994. </year>
Reference-contexts: Due to the fact that not every bucket contains one codevector, such techniques, which may be found in <ref> [69, 255, 254, 62, 166] </ref>, do not do a perfect full search. Some quantitative analysis of the increased distortion is given in [253] for a case where the prequantization is a lattice quantizer.
Reference: [255] <author> N. Moayeri, D.L. Neuhoff, </author> <title> ad W.E. Stark, "Fine-coarse vector quantization," </title> <journal> IEEE Trans. Signal Processing, </journal> <volume> Vol. 39, No. 7, </volume> <pages> pp. 1503-1515, </pages> <month> July </month> <year> 1991. </year>
Reference-contexts: Trellis-coded quantizers [249, 247, 123, 124, 248, 324] use a Viterbi algorithm encoder matched to a reproduction codebook with a trellis structure. Hierarchical table-lookup vector quantizers provide fixed-rate vector quantizers with minimal computational complexity, both lossy encoder and reproduction decoder being accomplished by table-lookups <ref> [69, 255, 367, 62] </ref>. Many of the quantization techniques, results, and applications can be found in original form in Swaszek's 1985 reprint collection on quantization [340] and Abut's 1990 IEEE Reprint Collection on Vector Quantization [2]. <p> Due to the fact that not every bucket contains one codevector, such techniques, which may be found in <ref> [69, 255, 254, 62, 166] </ref>, do not do a perfect full search. Some quantitative analysis of the increased distortion is given in [253] for a case where the prequantization is a lattice quantizer. <p> Thus the question becomes: what is the best partition into N cells, each of which is the union of some number of fine cells. The codevectors in C should then be the centroids of these cells. Such techniques have been exploited in <ref> [69, 255] </ref>. One technique worth particular mention is called hierarchical table lookup VQ [69, 166]. In this case, the prequantizer is itself an unstructured codebook that is searched with a fine prequantizer that is in turn searched with an even finer prequantizer, and so on.
Reference: [256] <author> P.W. Moo and D.L. Neuhoff, </author> <title> "An asymptotic analysis of fixed-rate lattice vector quantization," </title> <booktitle> Proc. Intern'l Symp. Inform. Thy. and Its Applications, </booktitle> <address> Victoria, B.C., </address> <pages> pp. 409-412, </pages> <month> Sept. </month> <year> 1996. </year>
Reference-contexts: There are even densities with tails so heavy that o = 2 and the granular distortion becomes negligible in comparison to the overload distortion. For lattice quantizers the asymptotic form of the optimal scaling of a lattice quantizer for an IID Gaussian source was found recently in <ref> [256] </ref> and also in [114]. We conclude this subsection by mentioning some gaps in rigorous high resolution theory. One, of course, is a proof or counterproof of Gersho's conjecture in dimensions three and higher. <p> The theory becomes more difficult if, as is usually the case, only a bounded portion of the lattice is used as the codebook and one must separately consider granular and overload distortion. There are a variety of ways of considering the tradeoffs involved (see, e.g., <ref> [116, 256, 114] </ref>). In any case, the essence of a lattice code is its uniform point density and nicely shaped cells with low normalized moment of inertia. For fixed-rate coding, they work well for uniform sources or other sources with bounded support.
Reference: [257] <author> K. Motoishi and T. Misumi, </author> <title> "On a fast vector quantization algorithm," </title> <booktitle> Proc. VIIth Symp. on Information Theory and Its Applications, </booktitle> <year> 1984. </year>
Reference-contexts: In this way the set of potential codevectors is gradually narrowed. Techniques in this category, with different ways of narrowing the search, may be found in <ref> [257, 366, 335, 258, 300, 286, 173] </ref>. A number of other fast search techniques begin with a "coarse" prequantization with some very low complexity technique. It is called "coarse" because it typically has larger cells than the Voronoi regions of the codebook C that is being searched.
Reference: [258] <author> K. Motoishi and T. Misumi, </author> <title> "Fast vector quantization algorithm by using an adaptive searching technique," </title> <booktitle> Abstracts of the IEEE International Symposium on Information Theory San Diego, </booktitle> <address> CA Jan. </address> <year> 1990. </year>
Reference-contexts: In this way the set of potential codevectors is gradually narrowed. Techniques in this category, with different ways of narrowing the search, may be found in <ref> [257, 366, 335, 258, 300, 286, 173] </ref>. A number of other fast search techniques begin with a "coarse" prequantization with some very low complexity technique. It is called "coarse" because it typically has larger cells than the Voronoi regions of the codebook C that is being searched.
Reference: [259] <author> T. Murakami and K. Asai and E. Yamazaki, </author> <title> "Vector quantizer of video signals," </title> <journal> Electronics Letters, </journal> <volume> Vol. 7, </volume> <pages> pp. 1005-1006, </pages> <month> Nov. </month> <year> 1982. </year>
Reference: [260] <author> S. Na and D. Neuhoff, </author> " <title> Bennett's integral for vector quantizers," </title> <journal> IEEE Transactions on Information Theory, vol.41, </journal> <volume> no.4, </volume> <pages> pp. 886-900, </pages> <month> July </month> <year> 1995. </year>
Reference-contexts: The extension of Bennett's integral to vector quantizers was first made by Gersho (1979) [147] for quantizers with congruent cells for which the concept of inertial profile was not needed, and then to vector quantizers with varying cell shapes (and codevector placements) by Na and Neuhoff (1995) <ref> [260] </ref>. <p> Motivated by several prior explanations <ref> [244, 238, 260] </ref>, we offer the following, which is an extension of these. We wish to compare an optimal quantizer, q k , with dimension k to an optimal k 0 -dimensional quantizer, q k 0 with k 0 &gt;> k. <p> Since the widths of the cells are, approximately, determined by sq (x 1 ), the point density and inertial profile of q pr;k 0 are functions of sq . Specifically, from the rectangular nature of the product cells one obtains <ref> [260, 270] </ref>. pr;k 0 (x) = Y sq (x i ) and m pr;k 0 (x) = 12 k 0 i=1 2 sq (x i ) Q k 0 1 1=k 0 (32) Note that along the diagonal of the first "quadrant" (where x 1 = x 2 = : : <p> To quantify the suboptimality of the product quantizer's principal feature, we factor the ratio of the distortions of q pr;k 0 (x) and q k 0 , which is a kind of loss, into terms that reflect the loss due to the inertial profile and point density <ref> [260, 271] </ref> 5 L = ffi k 0 (R) 2 2R R m pr;k 0 (x) pr;k 0 (x) 2 2R R M k 0 k 0 (x) = 2=k 0 f k 0 (x) dx 2=k 0 f k 0 (x) dx R 1 pr;k 0 (x) R 1 pr;k <p> For sources with memory, scalar quantization (k = 1) engenders an additional loss due to its inability to exploit the dependence between source samples. More specifically, when there is dependence/correlation between source samples, the product point density cannot match the ideal point density, not even approximately. See <ref> [238, 260] </ref> for a definition of memory loss. (One can factor both the point density and oblongitis losses into two terms, one of which is due to the quantizer's inability to exploit memory.) There is also a memory loss for k-dimensional quantization, which decreases to 1 as k increases. <p> The generalization of Bennett's integral to fixed-rate vector quantizers with rather arbitrary cell shapes was accomplished by Na and Neuhoff (1995) <ref> [260] </ref>, who presented both rigorous and informal derivations. <p> Even assuming Gersho's conjecture is correct, there is no rigorous proof of the Zador-Gersho formulas (26) and (30) along the lines of the informal derivations that start with Bennett's integral. We mention that the tail conditions given in some of the rigorous results (e.g. <ref> [53, 260] </ref>) are very difficult to check. Simpler ones are needed. Finally, as discussed in Section 2 there are no convincing (let alone rigorous) asymptotic analyses of the operational distortion-rate function of DPCM.
Reference: [261] <author> S. Nanda and W. A. Pearlman, </author> <title> "Tree Coding of Image Subbands", </title> <journal> IEEE Trans. on Image Proc., </journal> <volume> vol. 1, no. 2, </volume> <pages> pp. 133-147, </pages> <month> April </month> <year> 1992. </year>
Reference-contexts: Early wavelet coding techniques emphasized scalar or lattice vector quantization [10, 11, 100, 321, 12, 27, 140] and other vector quantization techniques have also been applied to wavelet coefficients, including tree encoding <ref> [261] </ref>, residual vector quantization [212], and other methods [85]. A major breakthrough in performance and complexity came with the introduction of zerotrees [220, 328, 316], which provided an extremely efficient embedded representation of scalar quantized wavelet coefficients, called embedded zerotree wavelet (EZW) coding.
Reference: [262] <author> M. Naraghi-Pour and D.L. Neuhoff, </author> <title> "Mismatched DPCM encoding of autoregressive processes," </title> <journal> IEEE Trans. Information Theory, </journal> <volume> Vol. 36, </volume> <pages> pp. 296-304, </pages> <month> March </month> <year> 1990. </year>
Reference: [263] <author> M. Naraghi-Pour and D.L. Neuhoff, </author> <title> "On the continuity of the stationary state distribution of DPCM," </title> <journal> IEEE Trans. Information Theory, </journal> <volume> Vol. 36, </volume> <pages> pp. 305-311, </pages> <month> March </month> <year> 1990. </year>
Reference: [264] <author> M. Naraghi-Pour and D.L. Neuhoff, </author> <title> "Convergence of the projection method for an autoregressive process and a matched DPCM code," </title> <journal> IEEE Trans. Information Theory, </journal> <volume> Vol. 36, </volume> <pages> pp. 1255-1264, </pages> <month> Nov. </month> <year> 1990. </year>
Reference: [265] <author> N. M. Nasrabadi and R. A. King, </author> <title> "Image coding using vector quantization: A review," </title> <journal> IEEE Trans. on Communications, </journal> <volume> Vol. 36, </volume> <pages> pp. 957-971, </pages> <month> Aug. </month> <year> 1988. </year>
Reference-contexts: Towards the middle of the 1980's, several tutorial articles on vector quantization appeared, which greatly increased the accessibility of the subject <ref> [148, 162, 244, 265] </ref>. The Mid 1980's to the Present In the late 80's a wide variety of vector quantizer design algorithms were developed and tested for speech, images, video, and other signal sources.
Reference: [266] <author> A.N. Netravali and R. Saigal, </author> <title> "Optimal quantizer design using a fixed-point algorithm," </title> <journal> Bell Systems Technical Journal, </journal> <volume> Vol. 55, No. 11, </volume> <pages> pp. 1423-1435, </pages> <year> 1976. </year>
Reference-contexts: In 16 1976 Netravali and Saigal introduced a fixed-point algorithm with the same goal of minimizing average distortion for a scalar quantizer with an entropy constraint <ref> [266] </ref>. Yet another approach was taken by Noll and Zelinski (1978) [280]. Variable-rate quantization was also extended to DPCM and transform coding where high resolution analysis shows that it gains the same relative to fixed-rate quantization as it does when applied to direct scalar quantizing [285, 119].
Reference: [267] <author> A. N. Netravali and J. O. </author> <title> Limb, "Picture coding: a review," </title> <journal> Proc. IEEE, </journal> <volume> Vol. 68, </volume> <pages> pp. 366-406, </pages> <month> March </month> <year> 1980. </year> <month> 85 </month>
Reference: [268] <author> A. N. Netravali and B. G. </author> <title> Haskell, Digital Pictures: Representation and Compression, </title> <publisher> Plenum, </publisher> <address> New York, </address> <note> 1988 and 199?. </note>
Reference-contexts: In speech coding they form the basis of ITU-G.721, 722, 723, and 726, and in video coding they form the basis of the interframe coding schemes standardized in the MPEG and H.26X series. Comprehensive discussions may be found in books <ref> [197, 268, 149, 317] </ref> and survey papers [196, 153]. Though decorrelation was an early motivation for predictive quantization, the most common view at present is that the primary role of the predictor is to reduce the variance of the variable to be scalar quantized.
Reference: [269] <author> D. L. Neuhoff, </author> <title> "Source coding strategies: simple quantizers vs. simple noiseless codes," </title> <booktitle> Proceedings 1986 Conf. on Information Sciences and Systems, </booktitle> <volume> Vol. 1, </volume> <pages> pp. 267-271, </pages> <month> March </month> <year> 1986. </year>
Reference-contexts: Taking into account all of the previous discussion, one sees that good performance can be obtained with two rather different strategies <ref> [269] </ref>. On the one hand, fixed-rate quantization with large dimension achieves the best possible performance with a complex quantizer and, simply, a fixed-rate lossless coder.
Reference: [270] <author> D.L. </author> <title> Neuhoff "Why Vector Quantizers Outperform Scalar Quantizers on Stationary Memoryless Sources," </title> <booktitle> IEEE Int'l Symposium on Information Theory, </booktitle> <address> Whistler, B.C., p. 438, </address> <month> Sept. </month> <year> 1995. </year>
Reference-contexts: Since the widths of the cells are, approximately, determined by sq (x 1 ), the point density and inertial profile of q pr;k 0 are functions of sq . Specifically, from the rectangular nature of the product cells one obtains <ref> [260, 270] </ref>. pr;k 0 (x) = Y sq (x i ) and m pr;k 0 (x) = 12 k 0 i=1 2 sq (x i ) Q k 0 1 1=k 0 (32) Note that along the diagonal of the first "quadrant" (where x 1 = x 2 = : :
Reference: [271] <author> D.L. Neuhoff, </author> <title> "On the asymptotic distribution of the errors in vector quantization," </title> <journal> IEEE Trans. Information Theory, </journal> <volume> Vol. 42, </volume> <pages> pp. 461-468, </pages> <month> March </month> <year> 1996. </year>
Reference-contexts: To quantify the suboptimality of the product quantizer's principal feature, we factor the ratio of the distortions of q pr;k 0 (x) and q k 0 , which is a kind of loss, into terms that reflect the loss due to the inertial profile and point density <ref> [260, 271] </ref> 5 L = ffi k 0 (R) 2 2R R m pr;k 0 (x) pr;k 0 (x) 2 2R R M k 0 k 0 (x) = 2=k 0 f k 0 (x) dx 2=k 0 f k 0 (x) dx R 1 pr;k 0 (x) R 1 pr;k <p> In order to apply Bennett's integral, it was necessary to find the form of the probability density of the quantization error produced by the first stage. This motivated the asymptotic error density analysis of vector quantization in <ref> [225, 271] </ref>. (An interesting sidelight of this work is that it turns out that one can learn much about the point density and cell shapes of a high resolution quantizer from the histogram of the lengths of the quantization errors.) Multistage quantizes have been improved in a number of ways.
Reference: [272] <author> D.L. Neuhoff, </author> <title> "Polar quantization revisited," </title> <booktitle> Proc. IEEE International Symposium on Information Theory, </booktitle> <address> Ulm, Germany, p. 60, </address> <month> July </month> <year> 1997. </year>
Reference-contexts: Originally, methods were developed specifically for polar quantizers but recently it has been shown that Ben-nett's integral can be applied to analyze polar quantization in a straightforward way <ref> [272] </ref>. It turns 63 out that for an IID Gaussian source optimized convential polar quantization gains about .41 dB over direct scalar quantization, and optimized unrestricted polar quantization gains another .73 dB.
Reference: [273] <author> D.L. Neuhoff and D.H. Lee, </author> <title> "On the performance of tree-structured vector quantization," </title> <booktitle> Proc. IEEE ICAASP, Toronto, </booktitle> <volume> Vol. 4, </volume> <pages> pp. 2277-2280, </pages> <month> May </month> <year> 1991. </year>
Reference-contexts: On the one hand, a rate-distortion theory analysis [113] has shown that there are situations where successive approximation can be done without loss of optimality. On the other hand, high resolution analyses of TSVQ <ref> [273] </ref> and two-stage VQ [224] have quantified the loss of these particular codes, and in the latter case shown ways of modifying the quantizer to eliminate the loss. <p> k , the ratio of the normalized moment of inertia of a cube to that of the best k dimensional cell shape, which approaches 1.53 dB for large k, and the remainder, about .5 to .7 dB, is due to the oblongitis caused by the cubes being cut into pieces. <ref> [273] </ref>. A paper investigating the nature of TSVQ cells is [395]. Our experience has been that when taking both performance and complexity into account, TSVQ is a very competitive VQ method.
Reference: [274] <author> D. J. Newman, </author> <title> "The hexagon theorem," </title> <institution> Bell Laboratories Technical Memorandum, </institution> <year> 1964. </year> <note> Published in the March 1982 special issue on Quantization of the IEEE Transactions on Information Theory. </note>
Reference-contexts: Gish and Pierce also observed that when coding vectors, performance could be improved by using quantizer cells other than the cube implicitly used by uniform scalar quantizers and noted that the hexagonal cell was superior in two dimensions, as originally demonstrated by Toth [353] and Newman <ref> [274] </ref>.) A serious shortcoming of this result is that the lossless code capable of approaching the kth order entropy of the quantized source can be quite complicated. <p> This was the first evaluation of the performance of a genuinely multidimensional quantizer. The result appeared first in English in a 1964 Bell Laboratories Technical Memorandum by Newman <ref> [274] </ref>. <p> The latter statement has been proven for k = 1 (c.f. [84], p. 59) and for k = 2 by Toth (1959) [354], see also <ref> [274] </ref>). For k = 3, it is known that best lattice tesselation is the body-centered cubic lattice, which is generated by a truncated octahedron. But it has not been proven that this is the best tesselation, though one would suspect that it is. <p> This result was independently rederived in a simpler fashion by Newman (1964) <ref> [274] </ref>. The lower bound is asymptotically achievable by a lattice with hexagonal cells; see [231] for a rigorous proof. It follows then that the ratio of ffi 2 (R) to M (hexagon)oe 2 2 2R tends to one, and also, that Gersho's conjecture holds for dimension two.
Reference: [275] <author> N. B. Nill, </author> <title> "A Visual Model Weighted Cosine Transform for Image Compression and Quality Assessment," </title> <journal> IEEE Transactions on Communications, </journal> <volume> Vol COM-33, </volume> <pages> pp. 551-557, </pages> <month> June, </month> <year> 1985. </year>
Reference-contexts: to a class of distortion measures which have proved useful in perceptual coding, the input weighted quadratic distortion measures of the form d (x; ^x) = (x ^x) t B x (x ^x); (18) where B x is a nonnegative definite matrix that depends on the input. (See, for example, <ref> [276, 275, 115, 141, 227, 234, 233] </ref>.) Most of the theory and design techniques considered here extend to such measures. <p> Nevertheless, these conditions are usually satisfied by perceptual distortion measures. Examples can be found in Eskicioglu and Fisher [115]. Also Nill's <ref> [275] </ref> definition of quality measure analyzed in detail later in this paper satisfies these conditions.
Reference: [276] <author> N.B. Nill and B.H. Bouxas, </author> <title> "Objective image quality measure derived from digital image power spectra," </title> <journal> Optical Engineering, </journal> <volume> Vol 31, </volume> <pages> pp. 813-825, </pages> <month> April </month> <year> 1992. </year>
Reference-contexts: to a class of distortion measures which have proved useful in perceptual coding, the input weighted quadratic distortion measures of the form d (x; ^x) = (x ^x) t B x (x ^x); (18) where B x is a nonnegative definite matrix that depends on the input. (See, for example, <ref> [276, 275, 115, 141, 227, 234, 233] </ref>.) Most of the theory and design techniques considered here extend to such measures.
Reference: [277] <editor> A.B. Nobel and R.A. Olshen, </editor> <title> "Termination and continuity of greedy growing for tree-structured vector quantizers," </title> <journal> IEEE Transactions on Information Theory, vol.42, </journal> <volume> no.1, </volume> <pages> pp. 191-205, </pages> <month> Jan. </month> <year> 1996. </year>
Reference-contexts: There has been a flurry of recent work on the theory of tree growing algorithms for vector quantizers, which are a form of recursive partitioning. See for example the work of Nobel and Olshen <ref> [277, 278, 279] </ref>. Multistage Vector Quantization Multistage (or multistep or cascade or residual) vector quantization was introduced by Juang and A.H. Gray [203] as a form of tree structured quantization with much reduced arithmetic complexity and storage.
Reference: [278] <author> A.B. Nobel, </author> <title> "Vanishing distortion and shrinking cells," </title> <journal> IEEE Transactions on Information Theory, vol.42, </journal> <volume> no.4, </volume> <pages> pp. 1303-5, </pages> <month> July </month> <year> 1996. </year>
Reference-contexts: There has been a flurry of recent work on the theory of tree growing algorithms for vector quantizers, which are a form of recursive partitioning. See for example the work of Nobel and Olshen <ref> [277, 278, 279] </ref>. Multistage Vector Quantization Multistage (or multistep or cascade or residual) vector quantization was introduced by Juang and A.H. Gray [203] as a form of tree structured quantization with much reduced arithmetic complexity and storage.
Reference: [279] <author> A.B. Nobel, </author> <title> "Recursive partitioning to reduce distortion," </title> <journal> IEEE Transactions on Information Theory, vol.43, </journal> <volume> no.4, </volume> <pages> pp. 1122-33, </pages> <month> July </month> <year> 1997. </year>
Reference-contexts: There has been a flurry of recent work on the theory of tree growing algorithms for vector quantizers, which are a form of recursive partitioning. See for example the work of Nobel and Olshen <ref> [277, 278, 279] </ref>. Multistage Vector Quantization Multistage (or multistep or cascade or residual) vector quantization was introduced by Juang and A.H. Gray [203] as a form of tree structured quantization with much reduced arithmetic complexity and storage.
Reference: [280] <author> P. Noll and R. Zelinski, </author> <title> "Bounds on quantizer performance in the low bit-rate region," </title> <journal> IEEE Transactions on Communications, </journal> <volume> Vol. 26, </volume> <pages> pp. 300-305, </pages> <month> Feb. </month> <year> 1978. </year>
Reference-contexts: In 16 1976 Netravali and Saigal introduced a fixed-point algorithm with the same goal of minimizing average distortion for a scalar quantizer with an entropy constraint [266]. Yet another approach was taken by Noll and Zelinski (1978) <ref> [280] </ref>. Variable-rate quantization was also extended to DPCM and transform coding where high resolution analysis shows that it gains the same relative to fixed-rate quantization as it does when applied to direct scalar quantizing [285, 119].
Reference: [281] <author> K.L. Oehler and R.M. Gray, </author> <title> "Mean-Gain-Shape Vector Quantization," </title> <booktitle> Proceedings of the Intl. Conf. on Acoustics, Speech, and Signal Processing, </booktitle> <address> Minneapolis, Minnesota, </address> <month> April </month> <year> 1993. </year>
Reference-contexts: A variety of product vector quantizers use a cartesian product reproduction codebook, which often can be rapidly searched. Examples include polar vector quantizers [291, 292, 47, 376, 345, 346, 339, 342, 344], mean-removed vector quantizers [18, 19], and shape-gain vector quantizers <ref> [313, 281] </ref>. Trellis-coded quantizers [249, 247, 123, 124, 248, 324] use a Viterbi algorithm encoder matched to a reproduction codebook with a trellis structure. Hierarchical table-lookup vector quantizers provide fixed-rate vector quantizers with minimal computational complexity, both lossy encoder and reproduction decoder being accomplished by table-lookups [69, 255, 367, 62]. <p> In the case of the shape-gain VQ, the optimal lossy encoder is happily a simple sequential operation, where the gain quantizer is scalar, but the selection depends on the result of another quantizer, the shape quantizer. Similar ideas can be used for mean-removed VQ [18, 19] and mean/gain/shape VQ <ref> [281] </ref>. Fischer's pyramid VQ [121] is also a kind of shape-gainVQ. In this case, the codevectors of the shape codebook are constrained to lie on the surface of a k dimensional pyramid, namely, the set of all vectors whose components have magnitudes summing to one.
Reference: [282] <author> B. M. Oliver, J. Pierce, and C. E. Shannon, </author> <title> "The philosopy of PCM," </title> <booktitle> Proceedings of the IRE, </booktitle> <volume> Vol. 36, </volume> <pages> pp. 1324-1331, </pages> <month> Nov. </month> <year> 1948. </year>
Reference-contexts: The second approach is that of high resolution (or high rate or asymptotic) quantization theory, which had its origins in the 1948 paper on PCM by Oliver, Pierce, and Shannon <ref> [282] </ref> and the 1948 paper on quantization error spectra by Bennett [35] and its subsequent development in 1951 by Panter and Dite [290] and in 1957 by Lloyd [237]. <p> Fixed-Rate Scalar Quantization: PCM and the Origins of Quantization Theory Both quantization and source coding have their origins in pulse code modulation (PCM), a technique patented by Reeves [304] in 1938 and later both analyzed and popularized by Oliver, Pierce, and Shannon in 1948 <ref> [282] </ref>. A PCM system consists fundamentally of three components: a sampler, a quantizer, and a coder. Sampling converts a continuous-time waveform x (t) into a discrete time waveform x n = x (n=f s ), where f s is the sampling frequency. <p> The complicated sums of Bessel functions resembled the early analyses of another nonlinear modulation technique, FM, and left little hope for general closed forms solutions for interesting signals. The first general contributions to quantization theory came in 1948 with the papers of Oliver, Pierce, and Shannon <ref> [282] </ref> and Bennett [35]. As part of their analysis of PCM for communications, they developed the oft-quoted result that for large rate or resolution, a uniform quantizer with cell width yields average distortion D (q) = 2 =12. <p> Asymptotic Distortion As mentioned earlier, the first and most elementary result in high resolution theory is the 2 =12 approximation to the mean squared error of a uniform scalar quantizer with step size <ref> [329, 282, 35] </ref>, which we now derive. Consider an N -level uniform quantizer q whose levels are y 1 ; : : : ; y N , with y i = y i1 + .
Reference: [283] <author> B. M. Oliver, </author> <title> "Efficient Coding," </title> <journal> Bell Systems Technical Journal, </journal> <volume> Vol. 31, </volume> <pages> pp. 724-750, </pages> <month> July </month> <year> 1952. </year>
Reference-contexts: In 1950 Elias [108] provided an information theoretic development of the benefits of predictive coding, but the work was not published until 1955 [109]. Other early references include <ref> [283, 216, 177, 361, 397] </ref>. <p> The possibility of applying variable-length coding to quantization may well have occurred to any number of people who were familiar with both quantization and Shannon's 1948 paper. The earliest references to such that we have found are in the 1952 papers by Oliver <ref> [283] </ref> and Kretzmer [216]. 14 The first paper to analyze variable-rate quantization was Schutzenberger (1958) [320] who showed that the distortion of optimized variable-rate quantization (both scalar and vector) decreases with rate as 2 2R , just as with fixed-rate quantization.
Reference: [284] <author> J. B. O'Neal, Jr., </author> <title> "A bound on signal-to-quantizing noise ratios for digital encoding systems," </title> <journal> Proc. IEEE, </journal> <volume> Vol. 55, </volume> <pages> pp. 287-292, </pages> <month> Mar. </month> <year> 1967. </year>
Reference-contexts: prediction error is sufficiently similar in form to that of the source that its operational distortion-rate function is smaller than that of the original source by, approximately, the ratio of the variance of the source to that of the prediction error, a quantity that is often called a prediction gain <ref> [251, 284, 338, 197] </ref>. Analyses of this form usually claim that under high resolution conditions the distribution of the prediction error approaches that of the error when predictions are based on past source samples rather than past reproductions.
Reference: [285] <author> J. B. O'Neal, Jr., </author> <title> "Entropy coding in speech and television differential PCM systems," </title> <journal> IEEE Trans. on Information Theory, </journal> <volume> Vol. 17, </volume> <pages> pp. 7580761, </pages> <month> Nov. </month> <year> 1971. </year>
Reference-contexts: Yet another approach was taken by Noll and Zelinski (1978) [280]. Variable-rate quantization was also extended to DPCM and transform coding where high resolution analysis shows that it gains the same relative to fixed-rate quantization as it does when applied to direct scalar quantizing <ref> [285, 119] </ref>. We note, however, that the derivation of this result for DPCM suffers from the same flaws as for fixed-rate quantization.
Reference: [286] <author> M. D. Orchard, </author> <title> "A fast nearest neighbor search algorithm," </title> <booktitle> Proc. IEEE ICASSP, Toronto, </booktitle> <pages> pp. 2297-2300, </pages> <month> May </month> <year> 1991. </year>
Reference-contexts: In this way the set of potential codevectors is gradually narrowed. Techniques in this category, with different ways of narrowing the search, may be found in <ref> [257, 366, 335, 258, 300, 286, 173] </ref>. A number of other fast search techniques begin with a "coarse" prequantization with some very low complexity technique. It is called "coarse" because it typically has larger cells than the Voronoi regions of the codebook C that is being searched.
Reference: [287] <institution> M.T. Orchard and C.A. </institution> <month> Bouman, </month> <title> "Color Quantization of Images," </title> <journal> IEEE Trans. on Signal Processing, </journal> <volume> Vol. 39, </volume> <pages> pp. 2677-2690, </pages> <month> Dec. </month> <year> 1991. </year>
Reference: [288] <author> J. Pan and T.R. Fischer, </author> <title> "Vector quantization-lattice vector quantization of speech LPC coefficients," </title> <booktitle> Proc. IEEE ICASSP, Adelaide, Aust. Part 1, </booktitle> <year> 1994. </year>
Reference-contexts: Good schemes of this sort have even been developed for low to moderate rates by Gibson [200, 201] and Pan and Fischer <ref> [288, 289] </ref>. Cell-conditioned two-stage quantizers can be viewed as having a piecewise constant point density of the sort proposed earlier by Kuhlmann and Bucklew [218] as a means of circumventing the fact that optimal vector quantizers cannot be implemented with companders. This approach was further developed by Swaszek in [343].
Reference: [289] <author> J. Pan and T. R. Fischer, </author> <title> "Two-stage vector quantization-lattice vector quantization," </title> <journal> IEEE Transactions on Information Theory Vol. </journal> <volume> 41, </volume> <pages> pp. </pages> <month> 155-63 Jan </month> <year> 1995. </year>
Reference-contexts: Tree-structured VQ uses a tree-structured reproduction codebook with a matched tree-structured search algorithm [58, 308, 74]. A tree-structured VQ with far less memory is provided by a multistage or residual VQ <ref> [203, 182, 29, 223, 133, 66, 29, 289, 309, 30] </ref>. A variety of product vector quantizers use a cartesian product reproduction codebook, which often can be rapidly searched. <p> Good schemes of this sort have even been developed for low to moderate rates by Gibson [200, 201] and Pan and Fischer <ref> [288, 289] </ref>. Cell-conditioned two-stage quantizers can be viewed as having a piecewise constant point density of the sort proposed earlier by Kuhlmann and Bucklew [218] as a means of circumventing the fact that optimal vector quantizers cannot be implemented with companders. This approach was further developed by Swaszek in [343].
Reference: [290] <author> P.F. Panter and W. </author> <title> Dite "Quantizing distortion in pulse-count modulation with nonuniform spacing of levels," </title> <journal> Proc. IRE, </journal> <volume> Vol. 39, </volume> <pages> pp. 44-48, </pages> <month> Jan. </month> <year> 1951. </year>
Reference-contexts: approach is that of high resolution (or high rate or asymptotic) quantization theory, which had its origins in the 1948 paper on PCM by Oliver, Pierce, and Shannon [282] and the 1948 paper on quantization error spectra by Bennett [35] and its subsequent development in 1951 by Panter and Dite <ref> [290] </ref> and in 1957 by Lloyd [237]. Much of the history and state of the art of quantization derives from these seminal works, half of which first appeared in 1948. <p> Bennett also used a variation on Rice's method to derive an exact computation of the spectrum of quantizer noise when a Gaussian process is uniformly quantized, providing one of the very few exact computations of quantization error spectra. In 1951 Panter and Dite <ref> [290] </ref> used high resolution approximations similar to Bennett's (but without reference to Bennett) to investigate the best possible quantizer performance for a given rate, that is, the minimum average distortion achievable by any scalar quantizer with a fixed-rate constraint: ffi (R) = inf D (q): (7) Henceforth, ffi (R) will be <p> When k = 1, Z (R) reduces to the formula for scalar quantization first found by Panter and Dite <ref> [290] </ref>. Cleaner derivations were given by Smith (1957) [334] and Lloyd (1957) [237]. From the form of fl k (x) one may straightforwardly deduce that cells are smaller and have higher probability where f k (x) is larger, and that all cells contribute roughly the same to the distortion; i.e. <p> Similar comments apply to informal vs. rigorous analyses of asymptotic entropy. In the following we review the development of rigorous theory. Many analyses informal and rigorous explicitly assume the source has finite range (i.e. a probability distribution with bounded support); so there is no overload distortion to be ignored <ref> [35, 290, 334] </ref>. In some cases the source has finite range. In others, for example speech and images, the source samples have infinite range, but the measurement device has finite range. <p> We view that this differs only stylistically from an explicit assumption of finite support, for both approaches ignore overload distortion. However, assuming finite support is, arguably, humbler and mathematically more honest. The earliest quantizer distortion analyses to appear in the open literature <ref> [35, 290, 334] </ref> assumed finite range and used the density-approximately-constant-in-cells assumption. Several papers avoided the latter by using a Taylor series expansion of the source density. <p> In the scalar case, one can use this philosophy directly to construct a good quantizer, by designing a compander whose nonlinearity c (x) has derivative fl 1 (x), and extracting the resulting reconstruction levels and thresholds to obtain an approximately optimal point quantizer. This was first mentioned in Panter-Dite <ref> [290] </ref> and rediscovered several times. Unfortunately, at higher dimensions, companders cannot implement an optimal point density without creating large oblongitis [147, 50, 52]. So there is no direct way to construct vector quantizers with the high resolution philosophy.
Reference: [291] <author> W.A. Pearlman and R.M. Gray, </author> <title> "Source coding of the discrete Fourier transform," </title> <journal> IEEE Trans. on Information theory, </journal> <volume> Vol. 24, </volume> <pages> pp. 683-692, </pages> <month> Nov. </month> <year> 1978. </year> <month> 86 </month>
Reference-contexts: A tree-structured VQ with far less memory is provided by a multistage or residual VQ [203, 182, 29, 223, 133, 66, 29, 289, 309, 30]. A variety of product vector quantizers use a cartesian product reproduction codebook, which often can be rapidly searched. Examples include polar vector quantizers <ref> [291, 292, 47, 376, 345, 346, 339, 342, 344] </ref>, mean-removed vector quantizers [18, 19], and shape-gain vector quantizers [313, 281]. Trellis-coded quantizers [249, 247, 123, 124, 248, 324] use a Viterbi algorithm encoder matched to a reproduction codebook with a trellis structure.
Reference: [292] <author> W.A. Pearlman, </author> <title> "Polar quantization of a complex Gaussian random variable," </title> <journal> IEEE Trans. on Communications, </journal> <volume> Vol. 27, No. 6, </volume> <pages> pp. 892-899, </pages> <year> 1979. </year>
Reference-contexts: A tree-structured VQ with far less memory is provided by a multistage or residual VQ [203, 182, 29, 223, 133, 66, 29, 289, 309, 30]. A variety of product vector quantizers use a cartesian product reproduction codebook, which often can be rapidly searched. Examples include polar vector quantizers <ref> [291, 292, 47, 376, 345, 346, 339, 342, 344] </ref>, mean-removed vector quantizers [18, 19], and shape-gain vector quantizers [313, 281]. Trellis-coded quantizers [249, 247, 123, 124, 248, 324] use a Viterbi algorithm encoder matched to a reproduction codebook with a trellis structure.
Reference: [293] <author> W. B. Pennebaker and J. L. Mitchell, </author> <title> JPEG Still Image Compression Standard, </title> <publisher> Van Nostrand Reinhold, </publisher> <address> New York, </address> <year> 1993. </year>
Reference: [294] <author> C. Pepin, J.-C. Belfiore and J. </author> <title> Boutros "Quantization of both stationary and nonstationary Gaussian sources with Voronoi constellations," </title> <booktitle> Proc. IEEE Inter'l Symposium on Information Theory, </booktitle> <editor> Ulm, Ger. p. </editor> <volume> 59, </volume> <year> 1997. </year>
Reference-contexts: Similar results were reported by Pepin <ref> [294] </ref>. On the other hand, as mentioned earlier, a quantizer with dimension 12 can achieve this same distortion.
Reference: [295] <author> Poggi, G. and Olshen, R.A. </author> <title> "Pruned tree-structured vector quantization of medical images with segmentation and improved prediction," </title> <booktitle> IEEE Transactions on Image Processing 4 (1995), </booktitle> <pages> 734-742. </pages>
Reference-contexts: This approach was further developed by Swaszek in [343]. Feedback Vector Quantization Just as with scalar quantizers, a vector quantizer can be made predictive by a straightforward generalization of predictive quantization to vectors <ref> [175, 91, 68, 295] </ref> as depicted in Figure 3 if one replaces the scalars by vectors. The encoder and decoder can have a finite set of states, each with a quantizer custom designed for the state.
Reference: [296] <author> D. Pollard, </author> <title> "Quantization and the method of $k$-means," </title> <journal> IEEE Trans. on Information theory, </journal> <volume> Vol. 28, </volume> <pages> pp. 199-205, </pages> <month> Mar. </month> <year> 1982. </year>
Reference-contexts: conditions for uniqueness of local optima [356], results on the asymptotic behavior 3 The idea for a special issue on Quantization was first proposed by by Neil Sloane at the 1980 Allerton Conference. 22 of Lloyd's algorithm with training sequence size based on the theory of k-means consistency by Pollard <ref> [296] </ref>, two seminal papers on lattice quantization by Conway and Sloane [81], rigorous developments of the Bennett theory for vector quantizers and rth power distortion measures by Bucklew and Wise [51], Kieffer's demonstration of stochastic stability for a general class of feedback quantizers including the historic class of predictive quantizers and <p> We focus on the Lloyd algorithm because of its simplicity, its proven merit at designing codes, and because of the wealth of theoretical results regarding its convergence properties <ref> [315, 296, 101] </ref>.
Reference: [297] <author> W. K. Pratt, </author> <title> Image Transmission Techniques, </title> <publisher> Academic Press, </publisher> <address> New York, </address> <year> 1979. </year>
Reference: [298] <author> M. Rabbani and P. W. Jones, </author> <title> Digital Image Compression Techniques, Vol. TT7 of Tutorial Texts in Optical Engineering. </title> <publisher> SPIE Optical Engineering Press, Bellingham, </publisher> <address> WA, </address> <year> 1991. </year>
Reference: [299] <author> V. Ramasubramanian and K. K. Paliwal, </author> <title> "An optimized k-d tree algorithm for fast vector quantization of speech," </title> <booktitle> Proc. European Signal Processing Conf., Grenoble, </booktitle> <pages> pp. 875-878, </pages> <year> 1988. </year>
Reference: [300] <author> V. Ramasubramanian and K. K. Paliwal, </author> <title> "An efficient approximation-elimination algorithm for fast nearest- neighbor search based on a spherical distance coordinate formulation," </title> <booktitle> Proc. European Signal Processing Conf., </booktitle> <address> Barcelona, </address> <month> Sept. </month> <year> 1990. </year>
Reference-contexts: In this way the set of potential codevectors is gradually narrowed. Techniques in this category, with different ways of narrowing the search, may be found in <ref> [257, 366, 335, 258, 300, 286, 173] </ref>. A number of other fast search techniques begin with a "coarse" prequantization with some very low complexity technique. It is called "coarse" because it typically has larger cells than the Voronoi regions of the codebook C that is being searched.
Reference: [301] <author> K. Ramchandran and M. Vetterli, </author> <title> "Best wavelet packet bases in a rate-distortion sense," </title> <journal> IEEE Trans. Image Processing, </journal> <volume> vol. 2, no. 2, </volume> <pages> pp. 160-176, </pages> <month> April </month> <year> 1993. </year>
Reference-contexts: The zerotree approach has been extended to vector quantization [86], but the slight improvement comes at a significant cost in added complexity. Rate-distortion ideas have been used to optimize the rate-distortion tradeoffs using wavelet packets by minimizing a Lagrangian distortion over code trees and bit assignments <ref> [301] </ref>. Recently competitive schemes have demonstrated that separate scalar quantization of individual subbands coupled with a sophisticated but low complexity lossless coding algorithm called stack-run coding can provide performace nearly as good as EZW [357].
Reference: [302] <author> C. J. Read, D. M. Chabries, R. W. Christiansen and J. K. Flanagan, </author> <title> "A Method for Computing the DFT of Vector Quantized Data," </title> <journal> pp. </journal> <pages> 1015-1018, </pages> <booktitle> Proceedings of the Intl. Conf. on Acoustics, Speech, and Signal Processing, </booktitle> <address> Glasgow, Scotland, </address> <month> May </month> <year> 1989. </year>
Reference: [303] <author> G. Rebolledo and R. M. Gray and J. P. </author> <title> Burg, "A multirate voice digitizer based upon vector quantization," </title> <journal> IEEE Trans. on Communications, </journal> <volume> Vol. 30, </volume> <pages> pp. 721-727, </pages> <month> April </month> <year> 1982. </year>
Reference: [304] <author> A. H. Reeves, </author> <title> French Patent No. </title> <address> 852,183, Oct. 3, </address> <year> 1938. </year>
Reference-contexts: Finally, we sketch briefly developments from the mid 80's to the present. Fixed-Rate Scalar Quantization: PCM and the Origins of Quantization Theory Both quantization and source coding have their origins in pulse code modulation (PCM), a technique patented by Reeves <ref> [304] </ref> in 1938 and later both analyzed and popularized by Oliver, Pierce, and Shannon in 1948 [282]. A PCM system consists fundamentally of three components: a sampler, a quantizer, and a coder.
Reference: [305] <author> S. O. Rice, </author> <title> "Mathematical analysis of random noise," </title> <journal> Bell Systems Technical Journal, </journal> <volume> Vol. </volume> <pages> 23 pp. 282-332, </pages> <year> 1944, </year> <journal> and Vol. </journal> <volume> 24, </volume> <pages> pp. 46-156, </pages> <year> 1945. </year> <title> Reprinted in Selected papers on noise and stochastic processes (N. </title> <editor> Wax and N. Wax, </editor> <booktitle> eds.), </booktitle> <pages> pp. 133-294, </pages> <address> New York, NY: </address> <publisher> Dover, </publisher> <year> 1954. </year>
Reference-contexts: In an early contribution to the theory of quantization, Clavier, Panter, and Grieg (1947) [78] applied Rice's characteristic function or transform method <ref> [305] </ref> to provide exact expressions for the quantization error and its moments resulting from uniform quantization for certain specific inputs, including constants and sinusoids.
Reference: [306] <author> E. A. Riskin and R. M. Gray, </author> <title> "A Greedy Tree Growing Algorithm for the Design of Variable Rate Vector Quantizers," </title> <journal> IEEE Trans. on Signal Processing, </journal> <volume> Vol =39, </volume> <pages> pp. 2500-2507, </pages> <month> Nov. </month> <year> 1991. </year>
Reference-contexts: We will not delve into the large literature of transforms, but will observe that bit allocation becomes an important issue, and one can either use the high resolution approximations or a variety of non-asymptotic allocation algorithms such as the "fixed slope" or Pareto-optimality considered in <ref> [372, 332, 74, 306, 307, 321] </ref> . The method involves operating all quantizers at points on their operational distortion-rate curves of equal slopes. For a survey of some of these methods, see [85] or Chapter 10 of [149]. A combinatorial optimization method is given in [383]. <p> The TSVQ will still be competitive in terms of throughput, however, as the tree-structured search is amenable to pipelining. TSVQ's can be generalized to unbalanced trees (with variable depth as opposed to the fixed depth discussed above) <ref> [244, 74, 306, 149] </ref> and with larger branching factors than two or even variable branching factors [318]. <p> The most well known is the CART algorithm of Breiman, Friedman, Olshen, and Stone [43], and the variation of CART for designing TSVQs bears their initials: the BFOS algorithm <ref> [74, 306, 149] </ref>. In this method, a balanced or unbalanced tree with more leaves than needed is first grown and then pruned. <p> tree by splitting all nodes in each level of the tree, or by splitting one node at a time, e.g., by splitting the node with the largest contribution to the distortion [244] 67 or in a greedy fashion to to maximize the decrease in distortion for the increase in rate <ref> [306] </ref>. Once grown, the tree can be pruned by removing all descendents of any internal node, thereby making it a leaf. This will increase average distortion, but will also decrease the rate.
Reference: [307] <author> E. A. Riskin, </author> <title> "Optimal bit allocation via the generalized BFOS algorithm," </title> <journal> IEEE Trans. on Information theory, pp. </journal> <volume> 400-402, Vol 37, </volume> <month> March </month> <year> 1991. </year>
Reference-contexts: We will not delve into the large literature of transforms, but will observe that bit allocation becomes an important issue, and one can either use the high resolution approximations or a variety of non-asymptotic allocation algorithms such as the "fixed slope" or Pareto-optimality considered in <ref> [372, 332, 74, 306, 307, 321] </ref> . The method involves operating all quantizers at points on their operational distortion-rate curves of equal slopes. For a survey of some of these methods, see [85] or Chapter 10 of [149]. A combinatorial optimization method is given in [383].
Reference: [308] <author> E.A. Riskin, R. Ladner, R. Wang, and L.E. </author> <title> Atlas, "Index assignment for progressive transmission of full-search vector quantization," </title> <journal> IEEE Transactions on Image Processing, vol.3, </journal> <volume> no.3, </volume> <pages> pp. 307-12, </pages> <month> May </month> <year> 1994. </year>
Reference-contexts: Tree-structured VQ uses a tree-structured reproduction codebook with a matched tree-structured search algorithm <ref> [58, 308, 74] </ref>. A tree-structured VQ with far less memory is provided by a multistage or residual VQ [203, 182, 29, 223, 133, 66, 29, 289, 309, 30]. A variety of product vector quantizers use a cartesian product reproduction codebook, which often can be rapidly searched.
Reference: [309] <author> S.A. Rizvi, N.M. Nasrabadi, and W.L. Cheng, </author> <title> "Entropy-constrained predictive residual vector quantizatio," </title> <journal> Optical Engineering, </journal> <volume> Vol. 35, </volume> <month> Jan. </month> <year> 1996. </year>
Reference-contexts: Tree-structured VQ uses a tree-structured reproduction codebook with a matched tree-structured search algorithm [58, 308, 74]. A tree-structured VQ with far less memory is provided by a multistage or residual VQ <ref> [203, 182, 29, 223, 133, 66, 29, 289, 309, 30] </ref>. A variety of product vector quantizers use a cartesian product reproduction codebook, which often can be rapidly searched. <p> And more sophisticated design algorithms (than the greedy one) can also have benefits [29, 133, 28, 30]. Variable-rate multistage quantizers have been developed <ref> [182, 213, 214, 309] </ref>.
Reference: [310] <author> L. G. Roberts, </author> <title> "Picture coding using pseudo-random noise," </title> <journal> IRE Trans. on Information Theory, </journal> <volume> Vol. 8, No. 2, </volume> <pages> pp. 145-154, </pages> <month> Feb. </month> <year> 1962. </year>
Reference-contexts: This was the motivation for randomizing the action by the addition of a dither signal, a method introduced by Roberts <ref> [310] </ref>, as a means of making quantized images look better by replacing the artifacts resulting from deterministic errors by random noise. We shall return to dithering in Section 5, where it will be seen that dithering can indeed make the Bennett approximation exact. <p> Recent combinations of TCQ to coding wavelet coefficients [324] have yielding excellent performance in image coding applications, winning the JPEG 2000 contest of 1997 and thereby a position as a serious contender for the new standard. Dithering Dithered quantization was introduced by Roberts <ref> [310] </ref> in 1962 as a means of randomizing the effects of uniform quantization so as to minimize visual artifacts. Intuitively, the goal was to cause the reconstruction error to look more like signal-independent additive white noise. It turns out that for one type of dithering, this intuition is true.
Reference: [311] <author> G.M. Roe, </author> <title> "Quantizing for minimum distortion," </title> <journal> IEEE Trans. on Information Theory, </journal> <volume> Vol. 10, </volume> <pages> pp. 384-385, </pages> <year> 1964. </year>
Reference-contexts: density is Gaussian with variance oe 2 , then [146] ffi (R) = 12 p ffi (R) can also be derived directly from Bennett's integral using variational methods (as did Lloyd (1957) [237] and Smith (1957) [334] and, much later without apparent knowledge of these early works, by Roe (1964) <ref> [311] </ref>). It can also be derived without using variational methods by application of Holder's inequality to Bennett's integral [168], demonstrating that the minimum is global. <p> Intuitively, the average squared error is the average energy or power in the quantization noise. The most common extension of distortion measures for scalars is the rth power distortion, d (x; y) = jx yj r . For example, Roe <ref> [311] </ref> generalized Max's formulation to distortion measures of this form. <p> For example, Lloyd [237] used this approach to show that, ignoring overload distortion, the approximation error in the Panter-Dite formula is o (1=N 2 ), which means that it tends to zero, even when divided by 1=N 2 . Roe <ref> [311] </ref>, Algazi [6] and Wood [379] also used Taylor series.
Reference: [312] <author> K. Rose, E. Gurewitz an,d G. C. Fox, </author> <title> "A deterministic annealing approach to clustering," </title> <journal> Pattern Recognition Letters, </journal> <volume> Vol. 11, </volume> <pages> pp. 589-594, </pages> <year> 1990. </year>
Reference-contexts: Some of the quantizer design algorithms developed as alternatives to Lloyd's algorithm include simulated annealing [107, 360, 125, 208], deterministic annealing <ref> [312] </ref>, pairwise nearest neighbor [112] (which had its origins in earlier clustering techniques [370]), stochastic relaxation [394, 396], self organizing feature maps [209, 384, 385] and other neural nets [348, 217, 347, 241, 54].
Reference: [313] <author> M. J. Sabin and R. M. Gray, </author> <title> Product code vector quantizers for speech waveform coding, </title> <booktitle> Conference Record Globecom '82, </booktitle> <pages> pp. 1087-1091, </pages> <month> Dec. </month> <year> 1982. </year>
Reference-contexts: A variety of product vector quantizers use a cartesian product reproduction codebook, which often can be rapidly searched. Examples include polar vector quantizers [291, 292, 47, 376, 345, 346, 339, 342, 344], mean-removed vector quantizers [18, 19], and shape-gain vector quantizers <ref> [313, 281] </ref>. Trellis-coded quantizers [249, 247, 123, 124, 248, 324] use a Viterbi algorithm encoder matched to a reproduction codebook with a trellis structure. Hierarchical table-lookup vector quantizers provide fixed-rate vector quantizers with minimal computational complexity, both lossy encoder and reproduction decoder being accomplished by table-lookups [69, 255, 367, 62]. <p> As such they are ordinarily much less than the complexities of an unstructured quantizer with the same number of codevectors, whose complexities equal the product of those of the components of a product quantizer. A shape-gain vector quantizer <ref> [313, 314] </ref> is an example of a product quantizer.
Reference: [314] <author> M. J. Sabin and R. M. Gray, </author> <title> "Product code vector quantizers for waveform and voice coding," </title> <journal> IEEE Trans. Acoustics, Speech, and Signal Processing, </journal> <volume> Vol. 32, </volume> <pages> pp. 474-488, </pages> <month> June </month> <year> 1984. </year>
Reference-contexts: As such they are ordinarily much less than the complexities of an unstructured quantizer with the same number of codevectors, whose complexities equal the product of those of the components of a product quantizer. A shape-gain vector quantizer <ref> [313, 314] </ref> is an example of a product quantizer.
Reference: [315] <author> M. J. Sabin and R. M. Gray, </author> <title> "Global convergence and empirical consistency of the generalized Lloyd algorithm," </title> <journal> IEEE Trans. on Information theory, </journal> <volume> Vol. 32, </volume> <pages> pp. 148-155, </pages> <month> March </month> <year> 1986. </year> <month> 87 </month>
Reference-contexts: We focus on the Lloyd algorithm because of its simplicity, its proven merit at designing codes, and because of the wealth of theoretical results regarding its convergence properties <ref> [315, 296, 101] </ref>.
Reference: [316] <author> A. Said and W. Pearlman, </author> <title> "A new, fast, and efficient image codec based on set partitioning in hierarchical trees," </title> <journal> IEEE Transactions on Circuits and Systems for Video Technology, vol.6, </journal> <volume> no.3, </volume> <pages> p. 243-50, </pages> <month> June </month> <year> 1996. </year>
Reference-contexts: A major breakthrough in performance and complexity came with the introduction of zerotrees <ref> [220, 328, 316] </ref>, which provided an extremely efficient embedded representation of scalar quantized wavelet coefficients, called embedded zerotree wavelet (EZW) coding. The zerotree approach has been extended to vector quantization [86], but the slight improvement comes at a significant cost in added complexity.
Reference: [317] <author> Sayood, Khalid, </author> <title> Introduction to data compression, </title> <publisher> Morgan Kaufmann Publishers, </publisher> <address> San Francisco, Calif., </address> <year> 1996. </year>
Reference-contexts: In speech coding they form the basis of ITU-G.721, 722, 723, and 726, and in video coding they form the basis of the interframe coding schemes standardized in the MPEG and H.26X series. Comprehensive discussions may be found in books <ref> [197, 268, 149, 317] </ref> and survey papers [196, 153]. Though decorrelation was an early motivation for predictive quantization, the most common view at present is that the primary role of the predictor is to reduce the variance of the variable to be scalar quantized. <p> It has been successively used for video coding. Structured Quantizers We now turn to quantizers with structured partitions or reproduction codebooks, which in turn lend themselves to fast searching techniques and, in some cases, to greatly reduced storage. Many of these techniques are discussed in <ref> [149, 317] </ref>. Lattice Quantizers Lattice quantization can be viewed as a vector generalization of uniform scalar quantization.
Reference: [318] <author> T. Schmidl, </author> <title> P.C. Cosman, and R.M.Gray, "Unbalanced non-binary tree-structured vector quantization," </title> <booktitle> Proceedings of the Twenty-seventh Asilomar conference on Signals, Systems, & Computers, </booktitle> <month> 31 October - 3 November, </month> <title> 1993, </title> <address> Pacific Grove, CA, </address> <pages> pp. 1519-1523. </pages>
Reference-contexts: TSVQ's can be generalized to unbalanced trees (with variable depth as opposed to the fixed depth discussed above) [244, 74, 306, 149] and with larger branching factors than two or even variable branching factors <ref> [318] </ref>.
Reference: [319] <author> L. Schuchman, </author> <title> "Dither signals and their effects on quantization noise," </title> <journal> IEEE Trans. Commun. Technology, </journal> <volume> Vol.12, </volume> <pages> pp. 162-165, </pages> <month> Dec. </month> <year> 1964. </year>
Reference-contexts: An obvious problem is the need for the decoder to possess a copy of the dither signal. Nonsubtractive dithering forms the reproduction as ^ X = q (X n + W n ). The principal theoretical property of nonsubtractive dithering was developed by Schuchman <ref> [319] </ref>, who showed that the quantizer error e n = q (X n + W n ) W n = ~ X n X n is uniformly distributed on (=2; =2] and is independent of the original input signal X n if and only if the quantizer does not overload and
Reference: [320] <author> M. P. Schutzenberger, </author> <title> "On the quantization of finite dimensional messages," </title> <journal> Information and Control, </journal> <volume> Vol. 1, </volume> <pages> pp. 153-158, </pages> <year> 1958. </year>
Reference-contexts: The earliest references to such that we have found are in the 1952 papers by Oliver [283] and Kretzmer [216]. 14 The first paper to analyze variable-rate quantization was Schutzenberger (1958) <ref> [320] </ref> who showed that the distortion of optimized variable-rate quantization (both scalar and vector) decreases with rate as 2 2R , just as with fixed-rate quantization. <p> The first high resolution approximations for vector quantization were published by Schutzen-berger in 1958 <ref> [320] </ref>, who found upper and lower bounds to the least distortion of k-dimensional variable-rate vector quantizers, both of the form K2 2R . Unfortunately, the upper and lower bounds diverged. <p> The earliest rigorous analysis 10 is contained in Schutzenberger's 1958 paper <ref> [320] </ref>, which showed that for k-dimensional variable-rate quantization (L = 1), rth power distortion (kx yk r ), and a source with finite differential entropy and EkXk r 0 &lt; 1 for some r 0 &gt; r, there is a K k;r &gt; 0, depending on the source and the dimension, <p> We will not describe the converse. Zador's 1966 Bell Labs Memorandum [390] reproves these two main results under weaker conditions. The distortion measure is rth power in the general sense, which includes as special cases the narrow sense of the rth power of the Euclidean norm considered by Schutzenberger <ref> [320] </ref>. The requirement on the source density is only that each of its marginals has the property that it is bounded from above by jxj r+ffi , for some ffi &gt; 0 and all x of sufficiently large magnitude. <p> Note also that it no longer requires that kf k k k=(k+r) be finite. As indicated earlier, Zador's memorandum also derives the asymptotic form of the operational distortion-rate function of variable-rate quantization. In other words, it finishes what his thesis and Schutzenberger <ref> [320] </ref> started, though he was apparently unaware of the latter.
Reference: [321] <author> T. Senoo and B. Girod, </author> <title> "Vector quantization for entropy coding of image subbands, </title> <journal> IEEE Transactions on Image Processing, </journal> <volume> Vol. 1, No. 4, </volume> <pages> pp. 526-532, </pages> <month> October, </month> <year> 1992. </year>
Reference-contexts: We will not delve into the large literature of transforms, but will observe that bit allocation becomes an important issue, and one can either use the high resolution approximations or a variety of non-asymptotic allocation algorithms such as the "fixed slope" or Pareto-optimality considered in <ref> [372, 332, 74, 306, 307, 321] </ref> . The method involves operating all quantizers at points on their operational distortion-rate curves of equal slopes. For a survey of some of these methods, see [85] or Chapter 10 of [149]. A combinatorial optimization method is given in [383]. <p> The extension of subband filtering from 1-D to 2-D was made by Vetterli [364] and 2-D subband filtering was first applied to image coding by Woods et al. [380, 371, 381]. Early wavelet coding techniques emphasized scalar or lattice vector quantization <ref> [10, 11, 100, 321, 12, 27, 140] </ref> and other vector quantization techniques have also been applied to wavelet coefficients, including tree encoding [261], residual vector quantization [212], and other methods [85].
Reference: [322] <author> D. Slepian, </author> <title> "A class of binary signaling alphabets," </title> <journal> Bell Systems Technical Journal, </journal> <volume> Vol. 35, </volume> <pages> pp. </pages> <year> 203-234,1956. </year>
Reference-contexts: In 1965 Dunn [106] introduced a form of vector quantization for quantizing multidimensional IID Gaussian vectors and argued that his code was effectively a permutation code as earlier used by Slepian <ref> [322] </ref> for channel coding.
Reference: [323] <author> D. Slepian, </author> <title> "On delta modulation," </title> <journal> Bell Syst. Tech. J., </journal> <volume> vol. 51, </volume> <pages> pp. 2101-2136, </pages> <year> 1972. </year>
Reference: [324] <author> P. Sriram and M. Marcellin, </author> <title> "Image coding using wavelet transforms and entropy-constrained trellis-coded quantization," </title> <journal> IEEE Transactions on Image Processing, </journal> <volume> Vol. 4, No. 6, </volume> <pages> pp. 725-733, </pages> <month> June </month> <year> 1995. </year>
Reference-contexts: A variety of product vector quantizers use a cartesian product reproduction codebook, which often can be rapidly searched. Examples include polar vector quantizers [291, 292, 47, 376, 345, 346, 339, 342, 344], mean-removed vector quantizers [18, 19], and shape-gain vector quantizers [313, 281]. Trellis-coded quantizers <ref> [249, 247, 123, 124, 248, 324] </ref> use a Viterbi algorithm encoder matched to a reproduction codebook with a trellis structure. Hierarchical table-lookup vector quantizers provide fixed-rate vector quantizers with minimal computational complexity, both lossy encoder and reproduction decoder being accomplished by table-lookups [69, 255, 367, 62]. <p> Trellis Coded Quantization Trellis coded quantization, both scalar and vector, improve upon traditional encoded systems by labeling the trellis branches with entire subcodebooks (or "subsets") rather than with individual reproduction levels <ref> [249, 247, 123, 124, 369, 248, 324] </ref>. The primary gain resulting is a reduction in encoder complexity for a given level of performance. <p> As the original trellis encoding systems were motivated by convolutional channel codes with Viterbi decoders, trellis coded quantization was motivated by Ungerboeck's enormously successful coded modulation approach to channel coding for narrowband channels [358, 359]. Recent combinations of TCQ to coding wavelet coefficients <ref> [324] </ref> have yielding excellent performance in image coding applications, winning the JPEG 2000 contest of 1997 and thereby a position as a serious contender for the new standard.
Reference: [325] <author> H. Steinhaus, </author> <title> "Sur la division des corp materiels en parties," </title> <journal> Bull. Acad. Polon. Sci., C1. </journal> <volume> III, vol IV, </volume> <pages> 801-804, </pages> <year> 1956. </year>
Reference-contexts: The most important nonasymptotic results, however, are the basic optimality conditions and iterative descent algorithms for quantizer design, such as first developed by Steinhaus (1956) <ref> [325] </ref> and Lloyd (1957) [237], and later popularized by Max (1960) [250]. <p> Cox in 1957 [88] also derived similar conditions. Some additional early work, which can now be seen as relating to vector quantization, will be reviewed later <ref> [325, 354, 389] </ref>. Scalar Quantization with Memory It was recognized early that common sources such as speech and images had considerable redundancy that scalar quantization could not exploit. In other words, there is statistical correlation or dependence between the samples of such sources, which are said to have memory. <p> The Earliest Vector Quantization Work Outside of Shannon's sketch of rate-distortion theory in 1948, the earliest work with a definite vector quantization flavor appeared in the mathematical and statistical literature. Most important was the remarkable work of Steinhaus in 1956 <ref> [325] </ref>, who considered a problem equivalent to a three dimensional generalization of scalar quantization with a squared error distortion measure. Suppose that a mass density m (x) is defined on Euclidean space.
Reference: [326] <author> C.E. Shannon, </author> <title> "A mathematical theory of communication," </title> <journal> Bell Syst. Tech. J., </journal> <volume> Vol. 27, </volume> <pages> pp. 379-423, 623-656, </pages> <year> 1948. </year>
Reference-contexts: Likely the approach best known to the readers of these Transactions is that of rate-distortion theory or source coding theory | Shannon's information theoretic approach to source coding | which was first suggested in in his 1948 paper <ref> [326] </ref> providing the foundations of information theory, but which was not fully developed until his 1959 source coding paper [327]. <p> the design and analysis of practical quantization techniques in three paths: fixed-rate scalar quantization, which leads directly from the discussion of Section 1, predictive and transform coding, which adds linear processing to scalar quantization in order to exploit source redundancy, and variable-rate quantization, which uses Shannon's lossless source coding techniques <ref> [326] </ref> to reduce rate. (Lossless codes were originally called noiseless.) Next we follow early forward looking work on vector quantization, including the seminal work of Shannon and Zador, in which vector quantization appears more to be a paradigm for analyzing the fundamental limits of quantizer performance than a practical coding technique. <p> Hence, like DPCM, transform coding does a good job of exploiting source memory for a system based on scalar quantization. Variable-Rate Quantization Shannon's lossless source coding theorem (1948) <ref> [326] </ref> made it clear that assigning equal numbers of bits to all quantization cells is wasteful if the cells have unequal probabilities. Instead, the number of bits produced by the quantizer will, on the average, be reduced if shorter binary codewords are assigned to higher probability cells. <p> Indeed, no quantization technique can perform better than the best vector quantizers. Since in these early decades it was not evident whether vector quantization would become a practical technique, it primarily served as a paradigm for exploring fundamental limits. Shannon's Source Coding Theory In his classic 1948 paper, Shannon <ref> [326] </ref> sketched the idea of the rate of a source as the minimum bit rate required to reconstruct the source to some degree of accuracy as measured by a fidelity criterion such as mean squared error.
Reference: [327] <author> C.E. Shannon, </author> <title> "Coding theorems for a discrete source with a fidelity criterion," </title> <booktitle> IRE National Convention Record, </booktitle> <volume> Part 4, </volume> <pages> pages 142-163, </pages> <year> 1959. </year>
Reference-contexts: readers of these Transactions is that of rate-distortion theory or source coding theory | Shannon's information theoretic approach to source coding | which was first suggested in in his 1948 paper [326] providing the foundations of information theory, but which was not fully developed until his 1959 source coding paper <ref> [327] </ref>. <p> it turns out that for a Gaussian source with independent and identical (IID) samples, the operational distortion-rate function given above is p 3=2 = 2:72 times larger than ffi (R), the least distortion achievable by any quantization technique with rate R or less. (It was not until Shannon's 1959 paper <ref> [327] </ref> that ffi (R) was known.) Equivalently, the induced signal-to-noise ratio is 4.35 dB less than the best possible, or for a fixed distortion D this implies the rate is .72 bits/sample larger than that achievable by the best quantizers. <p> The sketch was fully developed in his 1959 paper <ref> [327] </ref> for IID sources, simple measures of distortion, and block source codes, which we now call vector quantizers. <p> For other cases, the Blahut algorithm [42] can be used to compute D k (R), though its computational complexity becomes overwhelming unless k is small. Due to the difficulty of computing it, many (mostly lower) bounds to the Shannon distortion-rate function have been developed (c.f. <ref> [327, 235, 37, 164] </ref>). One important upper bound derives from the fact that with respect to squared error, the Gaussian source has the largest Shannon distortion-rate function (kth-order or in the limit) of any source with the same covariance function.
Reference: [328] <author> J. Shapiro, </author> <title> "Embedded image coding using zerotrees of wavelet coefficients," </title> <journal> IEEE Transactions on Signal Processing, </journal> <volume> vol. 41, no. 12, </volume> <pages> pp. 3445-3462, </pages> <month> December </month> <year> 1993. </year>
Reference-contexts: A major breakthrough in performance and complexity came with the introduction of zerotrees <ref> [220, 328, 316] </ref>, which provided an extremely efficient embedded representation of scalar quantized wavelet coefficients, called embedded zerotree wavelet (EZW) coding. The zerotree approach has been extended to vector quantization [86], but the slight improvement comes at a significant cost in added complexity.
Reference: [329] <author> W.F. Sheppard, </author> <title> "On the calculation of the most probable values of frequency constants for data arranged according to equidistant divisions of a scale," </title> <journal> Proc. London Math. Soc., </journal> <volume> Vol. 24, Pt. 2, </volume> <pages> pp. 353-380, 1898. </pages>
Reference-contexts: 1 Introduction The dictionary meaning of quantization is the division of a quantity into a discrete number of small parts, often assumed to be integral multiples of a common quantity. The oldest example of quantization is rounding off, which was first analyzed by Sheppard <ref> [329] </ref> for the application of estimating densities by histograms. Any real number x can be rounded off to the nearest integer, say q (x), with a resulting quantization error e = q (x) x so that q (x) = x + e. <p> The 2 =12 result also appeared (in albeit somewhat disguised form) in Sheppard's 1898 treatment <ref> [329] </ref>. Bennett also developed several other fundamental results in quantization theory. <p> Asymptotic Distortion As mentioned earlier, the first and most elementary result in high resolution theory is the 2 =12 approximation to the mean squared error of a uniform scalar quantizer with step size <ref> [329, 282, 35] </ref>, which we now derive. Consider an N -level uniform quantizer q whose levels are y 1 ; : : : ; y N , with y i = y i1 + .
Reference: [330] <author> H.N. Shaver, </author> <title> "Topics in statistical quantization,' </title> <type> Technical Report No. 7050-5, </type> <institution> Systems Theory Laboratory, Stanford Electronics Laboratories, Stanford University, Stanford, </institution> <address> CA, </address> <month> May </month> <year> 1965. </year>
Reference-contexts: Numerous extensions of the Bennett-style asymptotic approximations and the approximation of r (D) or ffi (R) and the characterizations of properties of optimal high resolution quantization for both fixed- and variable-rate quantization for squared error and other error moments appeared during the 1960s, e.g., <ref> [350, 351, 45, 330, 6] </ref>. An excellent summary of the early work is contained in a 1970 paper by Elias [110].
Reference: [331] <author> P.C. Shields, D.L. Neuhoff, L.D. Davisson and F. Ledrappier, </author> <title> "The distortion-rate function for nonergodic sources, </title> " <journal> Annals of Probability, </journal> <volume> Vol. 6, No. 1, </volume> <pages> pp. 138-143, </pages> <year> 1978. </year>
Reference-contexts: As such, it applies to sources that are stationary in either the strict sense or some weaker sense, such as asymptotic mean stationarity (c.f. [165], p. 16). Though originally derived for ergodic sources, it has been extended to nonergodic sources <ref> [161, 331, 97] </ref>. In contrast, high resolution theory applies, fundamentally, to finite dimensional random vectors. However, for stationary (or asymptotically stationary) sources, taking limits yields results for random processes. For example, the operational distortion-rate function ffi (R) was found to equal Z (R) in this way; see (26).
Reference: [332] <author> Y. Shoham and A. Gersho, </author> <title> "Efficient bit allocation for an arbitrary set of quantizers," </title> <journal> IEEE Trans. Acoustics, Speech, and Signal Processing Vol. </journal> <volume> 36, No. 9, </volume> <pages> pp. 1445-1453, </pages> <month> Sep. </month> <year> 1988. </year>
Reference-contexts: We will not delve into the large literature of transforms, but will observe that bit allocation becomes an important issue, and one can either use the high resolution approximations or a variety of non-asymptotic allocation algorithms such as the "fixed slope" or Pareto-optimality considered in <ref> [372, 332, 74, 306, 307, 321] </ref> . The method involves operating all quantizers at points on their operational distortion-rate curves of equal slopes. For a survey of some of these methods, see [85] or Chapter 10 of [149]. A combinatorial optimization method is given in [383].
Reference: [333] <author> V.M. Schteyn, </author> <title> "On group transmission with frequency division of channels by the pulse-code modulation method," </title> <journal> Telecommunications pp. </journal> <pages> 169-184, </pages> <year> 1959, </year> <title> a translation from Elektrosvyaz, </title> <journal> No. </journal> <volume> 2, </volume> <pages> pp 43-54, </pages> <year> 1959. </year>
Reference-contexts: In 1959 Shteyn <ref> [333] </ref> added terms representing overload distortion to the 2 =12 formula and to Bennett's integral and used them optimize uniform and nonuniform quantizers. Unaware of prior work except for Bennett, he rederived the optimal compressor characteristic and the Panter-Dite formula. <p> Roe [311], Algazi [6] and Wood [379] also used Taylor series. Overload distortion was first explcitly considered in the work of Shteyn (1959) <ref> [333] </ref>, who optimized the cell size of uniform scalar quantization using an explicit formula for the overload distortion (as well as 2 =12 for the graunular distortion) and while rederiving the Panter-Dite formula, added an overload distortion term.
Reference: [334] <author> B. Smith, </author> <title> "Instantaneous companding of quantized signals," </title> <journal> Bell Syst. Tech. J., </journal> <volume> Vol. 36, </volume> <pages> pp. 653-709, </pages> <year> 1957. </year>
Reference-contexts: Aigrain. For example, if the input density is Gaussian with variance oe 2 , then [146] ffi (R) = 12 p ffi (R) can also be derived directly from Bennett's integral using variational methods (as did Lloyd (1957) [237] and Smith (1957) <ref> [334] </ref> and, much later without apparent knowledge of these early works, by Roe (1964) [311]). It can also be derived without using variational methods by application of Holder's inequality to Bennett's integral [168], demonstrating that the minimum is global. <p> When k = 1, Z (R) reduces to the formula for scalar quantization first found by Panter and Dite [290]. Cleaner derivations were given by Smith (1957) <ref> [334] </ref> and Lloyd (1957) [237]. From the form of fl k (x) one may straightforwardly deduce that cells are smaller and have higher probability where f k (x) is larger, and that all cells contribute roughly the same to the distortion; i.e. <p> Similar comments apply to informal vs. rigorous analyses of asymptotic entropy. In the following we review the development of rigorous theory. Many analyses informal and rigorous explicitly assume the source has finite range (i.e. a probability distribution with bounded support); so there is no overload distortion to be ignored <ref> [35, 290, 334] </ref>. In some cases the source has finite range. In others, for example speech and images, the source samples have infinite range, but the measurement device has finite range. <p> We view that this differs only stylistically from an explicit assumption of finite support, for both approaches ignore overload distortion. However, assuming finite support is, arguably, humbler and mathematically more honest. The earliest quantizer distortion analyses to appear in the open literature <ref> [35, 290, 334] </ref> assumed finite range and used the density-approximately-constant-in-cells assumption. Several papers avoided the latter by using a Taylor series expansion of the source density.
Reference: [335] <author> M. R. Soleymani and S. D. Morgera, </author> <title> "An efficient nearest neighbor search method," </title> <journal> IEEE Trans. on Communications, </journal> <volume> Vol. 35, </volume> <pages> pp. 677-679, </pages> <year> 1987. </year>
Reference-contexts: We here only mention several examples with references and leave further discussion to Section 5. Fast search algorithms have been developed for unstructured reproduction codebooks <ref> [71, 335, 32, 67, 149] </ref>. Even faster searches are possible when the reproduction codebook is constrained to have a simple structure, for example to be a subset of points of a regular lattice as in a lattice vector quantizer [147, 81, 82, 83, 84, 151, 116]. <p> In this way the set of potential codevectors is gradually narrowed. Techniques in this category, with different ways of narrowing the search, may be found in <ref> [257, 366, 335, 258, 300, 286, 173] </ref>. A number of other fast search techniques begin with a "coarse" prequantization with some very low complexity technique. It is called "coarse" because it typically has larger cells than the Voronoi regions of the codebook C that is being searched.
Reference: [336] <author> A. B. Sripad and D. L. Snyder, </author> <title> "A necessary and sufficient condition for quantization errors to be uniform and white," </title> <journal> IEEE Trans. Acoustics, Speech, and Signal Processing, </journal> <volume> Vol. 25, </volume> <pages> pp. 442-448, </pages> <month> Oct. </month> <year> 1977. </year>
Reference-contexts: Schuchman's conditions are satisfied, for example, if the dither signal has a uniform probability density function on (=2; =2]. It follows from the work of Jayant and Rabiner [195] and Sripad and Snyder <ref> [336] </ref> (see also [163]) that Schuchman's condition implies that the sequence of quantization errors fe n g is independent. The case of uniform dither remains by far the most widely studied in the literature.
Reference: [337] <author> L. C. Stewart, R. M. Gray, and Y. Linde, </author> <title> "The design of trellis waveform coders," </title> <journal> IEEE Trans. on Communications, </journal> <volume> Vol. 30, </volume> <pages> pp. 702-710, </pages> <month> April </month> <year> 1982. </year>
Reference-contexts: While linear encoders sufficed for channel coding, nonlinear decoders were required for the source coding application, and a variety of design algorithms were developed for designing the decoder to populate the trellis searched by the encoder <ref> [229, 375, 337, 16, 33] </ref>.
Reference: [338] <author> R. W. Stroh, </author> <title> "Optimum and adaptive differential pulse code modulation," </title> <type> Ph.D. Dissertation, </type> <institution> Polytechnic Inst. Brooklyn, </institution> <address> N.Y. </address> <year> 1970. </year>
Reference-contexts: prediction error is sufficiently similar in form to that of the source that its operational distortion-rate function is smaller than that of the original source by, approximately, the ratio of the variance of the source to that of the prediction error, a quantity that is often called a prediction gain <ref> [251, 284, 338, 197] </ref>. Analyses of this form usually claim that under high resolution conditions the distribution of the prediction error approaches that of the error when predictions are based on past source samples rather than past reproductions.
Reference: [339] <author> P. F. Swaszek, </author> <title> "Uniform spherical coordinate quantization of spherically symmetric sources," </title> <journal> IEEE Transactions on Communications, </journal> <volume> Vol. 33, </volume> <pages> pp. 518-521, </pages> <year> 1985. </year>
Reference-contexts: A tree-structured VQ with far less memory is provided by a multistage or residual VQ [203, 182, 29, 223, 133, 66, 29, 289, 309, 30]. A variety of product vector quantizers use a cartesian product reproduction codebook, which often can be rapidly searched. Examples include polar vector quantizers <ref> [291, 292, 47, 376, 345, 346, 339, 342, 344] </ref>, mean-removed vector quantizers [18, 19], and shape-gain vector quantizers [313, 281]. Trellis-coded quantizers [249, 247, 123, 124, 248, 324] use a Viterbi algorithm encoder matched to a reproduction codebook with a trellis structure. <p> Such quantizers are called "unrestricted" polar quantization [376, 344]. High resolution analysis can be used to study the rate-distortion performance of these quantiz-ers <ref> [47, 48, 339, 341, 344] </ref>. Among other things such analyses find the optimal point density for the magnitude quantizer and the optimal bit allocation between magnitude and phase.
Reference: [340] <editor> P. F. Swaszek, Ed. Quantization, </editor> <volume> Vol. 29, </volume> <booktitle> Benchmark Papers in Electrical Engineering and Computer Science, </booktitle> <publisher> Van Nostrand Reinhold Company,Inc., </publisher> <address> New York, NY, </address> <year> 1985. </year> <month> 88 </month>
Reference-contexts: Hierarchical table-lookup vector quantizers provide fixed-rate vector quantizers with minimal computational complexity, both lossy encoder and reproduction decoder being accomplished by table-lookups [69, 255, 367, 62]. Many of the quantization techniques, results, and applications can be found in original form in Swaszek's 1985 reprint collection on quantization <ref> [340] </ref> and Abut's 1990 IEEE Reprint Collection on Vector Quantization [2]. We close this section with a brief discussion of two specific works which deal with optimizing variable-rate scalar quantizers without additional structure, the problem that leads to the general 23 formulation of optimal quantization in the next section.
Reference: [341] <author> P. Swaszek, </author> <title> "Asymptotic Performance of Dirichlet Rotated Polar Quantizers," </title> <journal> IEEE Trans. Inform. Thy., </journal> <volume> vol. IT-31, </volume> <pages> pp. 537-540, </pages> <month> July </month> <year> 1985. </year>
Reference-contexts: Such quantizers are called "unrestricted" polar quantization [376, 344]. High resolution analysis can be used to study the rate-distortion performance of these quantiz-ers <ref> [47, 48, 339, 341, 344] </ref>. Among other things such analyses find the optimal point density for the magnitude quantizer and the optimal bit allocation between magnitude and phase.
Reference: [342] <author> P. F. Swaszek, </author> <title> "A vector quantizer for the Laplace source," </title> <journal> IEEE Trans. on Information Theory, </journal> <volume> vol. 37, </volume> <pages> pp. 1355-1365, </pages> <month> Sept. </month> <year> 1991. </year>
Reference-contexts: A tree-structured VQ with far less memory is provided by a multistage or residual VQ [203, 182, 29, 223, 133, 66, 29, 289, 309, 30]. A variety of product vector quantizers use a cartesian product reproduction codebook, which often can be rapidly searched. Examples include polar vector quantizers <ref> [291, 292, 47, 376, 345, 346, 339, 342, 344] </ref>, mean-removed vector quantizers [18, 19], and shape-gain vector quantizers [313, 281]. Trellis-coded quantizers [249, 247, 123, 124, 248, 324] use a Viterbi algorithm encoder matched to a reproduction codebook with a trellis structure.
Reference: [343] <author> P.F. Swaszek, </author> <title> "Unrestricted multistage vector quantizers," </title> <journal> IEEE Trans. on Information Theory, </journal> <volume> vol. 38, no.3, </volume> <pages> pp. 1169-74, </pages> <month> May </month> <year> 1992. </year>
Reference-contexts: Cell-conditioned two-stage quantizers can be viewed as having a piecewise constant point density of the sort proposed earlier by Kuhlmann and Bucklew [218] as a means of circumventing the fact that optimal vector quantizers cannot be implemented with companders. This approach was further developed by Swaszek in <ref> [343] </ref>. Feedback Vector Quantization Just as with scalar quantizers, a vector quantizer can be made predictive by a straightforward generalization of predictive quantization to vectors [175, 91, 68, 295] as depicted in Figure 3 if one replaces the scalars by vectors.
Reference: [344] <author> P.F. Swaszek and T.W. Ku, </author> <title> "Asymptotic performance of unrestricted polar quantizers," </title> <journal> IEEE Trans. on Information Theory, </journal> <volume> vol. 32, </volume> <pages> pp. 330-333, </pages> <month> Mar. </month> <year> 1986. </year>
Reference-contexts: A tree-structured VQ with far less memory is provided by a multistage or residual VQ [203, 182, 29, 223, 133, 66, 29, 289, 309, 30]. A variety of product vector quantizers use a cartesian product reproduction codebook, which often can be rapidly searched. Examples include polar vector quantizers <ref> [291, 292, 47, 376, 345, 346, 339, 342, 344] </ref>, mean-removed vector quantizers [18, 19], and shape-gain vector quantizers [313, 281]. Trellis-coded quantizers [249, 247, 123, 124, 248, 324] use a Viterbi algorithm encoder matched to a reproduction codebook with a trellis structure. <p> Such quantizers are called "unrestricted" polar quantization <ref> [376, 344] </ref>. High resolution analysis can be used to study the rate-distortion performance of these quantiz-ers [47, 48, 339, 341, 344]. Among other things such analyses find the optimal point density for the magnitude quantizer and the optimal bit allocation between magnitude and phase. <p> Such quantizers are called "unrestricted" polar quantization [376, 344]. High resolution analysis can be used to study the rate-distortion performance of these quantiz-ers <ref> [47, 48, 339, 341, 344] </ref>. Among other things such analyses find the optimal point density for the magnitude quantizer and the optimal bit allocation between magnitude and phase.
Reference: [345] <author> P. F. Swaszek and J. B. Thomas, </author> <title> "Optimal cirularly symmetric quantizers," </title> <journal> Franklin Inst. J., </journal> <volume> Vol. 313, No. 6, </volume> <pages> pp. 373-384, </pages> <year> 1982. </year>
Reference-contexts: A tree-structured VQ with far less memory is provided by a multistage or residual VQ [203, 182, 29, 223, 133, 66, 29, 289, 309, 30]. A variety of product vector quantizers use a cartesian product reproduction codebook, which often can be rapidly searched. Examples include polar vector quantizers <ref> [291, 292, 47, 376, 345, 346, 339, 342, 344] </ref>, mean-removed vector quantizers [18, 19], and shape-gain vector quantizers [313, 281]. Trellis-coded quantizers [249, 247, 123, 124, 248, 324] use a Viterbi algorithm encoder matched to a reproduction codebook with a trellis structure.
Reference: [346] <author> P. F. Swaszek and J. B. Thomas, </author> <title> "Multidimensional spherical coordinates quantization," </title> <journal> IEEE Trans. on Information theory, </journal> <volume> Vol. 29, </volume> <pages> pp. 570-576, </pages> <year> 1983. </year>
Reference-contexts: A tree-structured VQ with far less memory is provided by a multistage or residual VQ [203, 182, 29, 223, 133, 66, 29, 289, 309, 30]. A variety of product vector quantizers use a cartesian product reproduction codebook, which often can be rapidly searched. Examples include polar vector quantizers <ref> [291, 292, 47, 376, 345, 346, 339, 342, 344] </ref>, mean-removed vector quantizers [18, 19], and shape-gain vector quantizers [313, 281]. Trellis-coded quantizers [249, 247, 123, 124, 248, 324] use a Viterbi algorithm encoder matched to a reproduction codebook with a trellis structure.
Reference: [347] <author> N. Ta, Y. Attikiouzel, and C. Crebbin, </author> <title> "Vector quantization of images using the competitive networks," </title> <booktitle> Proceedings of the second Austrailian conference on neural networks, </booktitle> <volume> ACNN `91, </volume> <pages> pp. 258-262, </pages> <year> 1991. </year>
Reference-contexts: Some of the quantizer design algorithms developed as alternatives to Lloyd's algorithm include simulated annealing [107, 360, 125, 208], deterministic annealing [312], pairwise nearest neighbor [112] (which had its origins in earlier clustering techniques [370]), stochastic relaxation [394, 396], self organizing feature maps [209, 384, 385] and other neural nets <ref> [348, 217, 347, 241, 54] </ref>.
Reference: [348] <author> D. W. Tank and J. J. </author> <title> Hopfield, "Simple `neural' optimization networks: an A/D converter, signal decision circuit, and a linear programming circuit," </title> <journal> Vol. </journal> <volume> 33, </volume> <pages> pp. 533-541, </pages> <month> May </month> <year> 1986. </year>
Reference-contexts: Some of the quantizer design algorithms developed as alternatives to Lloyd's algorithm include simulated annealing [107, 360, 125, 208], deterministic annealing [312], pairwise nearest neighbor [112] (which had its origins in earlier clustering techniques [370]), stochastic relaxation [394, 396], self organizing feature maps [209, 384, 385] and other neural nets <ref> [348, 217, 347, 241, 54] </ref>.
Reference: [349] <author> T. Tarpey, L. Li and B.D. Flury, </author> <title> "Principal points and self-consistent points of elliptical distributions," </title> <journal> Annals of Statistics, </journal> <volume> Vol 23, No. 1, </volume> <pages> pp 103-112, </pages> <year> 1995. </year>
Reference-contexts: It is difficult to resist pointing out, however, that in 1990 Lloyd's algorithm was rediscovered in the statistical literature under the name of "principal points," which are distinguished from traditional k-means by the assumption of an absolutely continuous distribution instead of an empirical distribution <ref> [127, 349] </ref>, a formulation included in the VQ formulation for a general distribution. Unfortunately, these works reflect no awareness of the rich quantization literature. Most quantizers today are indeed uniform and scalar, but are combined with prediction or transforms.
Reference: [350] <author> R.C. Titsworth, </author> <title> "Optimal threshold and level selection for quantizing data," JPL Space Programs Summary 37-23, </title> <booktitle> IV, </booktitle> <pages> pp. 196-200, </pages> <institution> California Institute of Technology, Pasadena, </institution> <address> CA, </address> <month> October </month> <year> 1963. </year>
Reference-contexts: Numerous extensions of the Bennett-style asymptotic approximations and the approximation of r (D) or ffi (R) and the characterizations of properties of optimal high resolution quantization for both fixed- and variable-rate quantization for squared error and other error moments appeared during the 1960s, e.g., <ref> [350, 351, 45, 330, 6] </ref>. An excellent summary of the early work is contained in a 1970 paper by Elias [110].
Reference: [351] <author> R.C. Titsworth, </author> <title> "Asymptotic results for optimum equally spaced quantization of Gaussian data," JPL Space Programs Summary 37-29, </title> <booktitle> IV, </booktitle> <pages> pp. </pages> <institution> 242,244, California Institute of Technology, Pasadena, </institution> <address> CA, </address> <month> October </month> <year> 1964. </year>
Reference-contexts: Numerous extensions of the Bennett-style asymptotic approximations and the approximation of r (D) or ffi (R) and the characterizations of properties of optimal high resolution quantization for both fixed- and variable-rate quantization for squared error and other error moments appeared during the 1960s, e.g., <ref> [350, 351, 45, 330, 6] </ref>. An excellent summary of the early work is contained in a 1970 paper by Elias [110].
Reference: [352] <author> I. Tokaji and C. W. Barnes, </author> <title> "Roundoff error statistics for a continuous range of multiplier coefficients," </title> <journal> Vol. </journal> <volume> 34, </volume> <pages> pp. 52-59, </pages> <month> Jan. </month> <year> 1987. </year>
Reference: [353] <editor> L. Fejes Toth, Lagerungen in der Ebene, auf der Kugel und im Raum, </editor> <publisher> Springer Verlag, </publisher> <address> Berlin, </address> <year> 1953. </year>
Reference-contexts: As will be discussed later, this belief is contradicted both by Shannon's work, which demonstrated strictly improved performance using vector quantizers even for memoryless sources and by the early work of Toth (1953) <ref> [353] </ref>. Nevertheless, removing redundancy leads to much improved codes. <p> Gish and Pierce also observed that when coding vectors, performance could be improved by using quantizer cells other than the cube implicitly used by uniform scalar quantizers and noted that the hexagonal cell was superior in two dimensions, as originally demonstrated by Toth <ref> [353] </ref> and Newman [274].) A serious shortcoming of this result is that the lossless code capable of approaching the kth order entropy of the quantized source can be quite complicated. <p> In 1959 Toth described the specific application of Steinhaus' problem in two dimensions with a uniform density on a bounded support region and an asymptotically large number of points [354]. Using an earlier inequality of his <ref> [353] </ref>, he showed that the optimal two-dimensional quantizer under these assumptions tesselated the support region with hexagons. This was the first evaluation of the performance of a genuinely multidimensional quantizer. The result appeared first in English in a 1964 Bell Laboratories Technical Memorandum by Newman [274].
Reference: [354] <author> L. Fejes Toth, </author> <title> "Sur la representation d'ue population infinie par un nombre fini d'elements," </title> <journal> Acta Math. Acad. Sci. Hung., </journal> <volume> vol. 10, </volume> <pages> pp. 76-81, </pages> <year> 1959. </year>
Reference-contexts: Cox in 1957 [88] also derived similar conditions. Some additional early work, which can now be seen as relating to vector quantization, will be reviewed later <ref> [325, 354, 389] </ref>. Scalar Quantization with Memory It was recognized early that common sources such as speech and images had considerable redundancy that scalar quantization could not exploit. In other words, there is statistical correlation or dependence between the samples of such sources, which are said to have memory. <p> In 1959 Toth described the specific application of Steinhaus' problem in two dimensions with a uniform density on a bounded support region and an asymptotically large number of points <ref> [354] </ref>. Using an earlier inequality of his [353], he showed that the optimal two-dimensional quantizer under these assumptions tesselated the support region with hexagons. This was the first evaluation of the performance of a genuinely multidimensional quantizer. <p> The latter statement has been proven for k = 1 (c.f. [84], p. 59) and for k = 2 by Toth (1959) <ref> [354] </ref>, see also [274]). For k = 3, it is known that best lattice tesselation is the body-centered cubic lattice, which is generated by a truncated octahedron. But it has not been proven that this is the best tesselation, though one would suspect that it is. <p> If instead one were to bound the distortion by the moment of inertia of the cell times the maximum value of the density within it, then K 0 k;r =K k;r would not tend to infinity. The second rigorous analysis appears to be that of Toth (1959) <ref> [354] </ref>, who showed that for a two-dimensional random vector that is uniformly distributed on the unit square, the mean squared error of any N point quantizer is bounded from below by M (hexagon)=N . This result was independently rederived in a simpler fashion by Newman (1964) [274].
Reference: [355] <author> A. V. Trushkin, </author> <title> "Optimal bit allocation algorithm for quantizing a random vector," Probl. </title> <journal> Inf. Transmission, </journal> <volume> Vol. 17, No. 3, </volume> <pages> pp. 156-161, </pages> <month> July-Sept., </month> <year> 1981. </year> <note> (Translated from Russian.) </note>
Reference: [356] <author> A. V. Trushkin, </author> <title> "Sufficient conditions for uniqueness of a locally optimal quantizer for a class of convex error weighting functions," </title> <journal> IEEE Trans. on Information theory, </journal> <volume> Vol. 28, </volume> <pages> pp. 187-198, </pages> <month> March </month> <year> 1982. </year>
Reference-contexts: Bell Laboratories Technical Memos of Lloyd, Newman, and Zador along with Berger's extension of the optimality properties of entropy-constrained scalar quantization to rth power distortion measures and his extensive comparison of minimum entropy quantizers and fixed-rate permutation codes [39], generalizations by Trushkin of Fleischer's conditions for uniqueness of local optima <ref> [356] </ref>, results on the asymptotic behavior 3 The idea for a special issue on Quantization was first proposed by by Neil Sloane at the 1980 Allerton Conference. 22 of Lloyd's algorithm with training sequence size based on the theory of k-means consistency by Pollard [296], two seminal papers on lattice quantization
Reference: [357] <author> M.J. Tsai, J.D. Villasenor, and F. Chen, </author> <title> "Stack-run image coding," </title> <journal> IEEE Transactions on Circuits and Systems for Video Technology, vol.6, </journal> <volume> no.5, </volume> <pages> pp. 519-21, </pages> <month> Oct. </month> <year> 1996. </year>
Reference-contexts: Recently competitive schemes have demonstrated that separate scalar quantization of individual subbands coupled with a sophisticated but low complexity lossless coding algorithm called stack-run coding can provide performace nearly as good as EZW <ref> [357] </ref>. The best wavelet codes tend to use very smart lossless codes, lossless codes which effectively code very large vectors.
Reference: [358] <author> G. Ungerboeck, </author> <title> "Channel coding with multilevel/phase signas," </title> <journal> IEEE Trans. Inform. Theory, </journal> <volume> Vol. 28, </volume> <pages> pp. 55-67, </pages> <month> Jan. </month> <year> 1982. </year>
Reference-contexts: The primary gain resulting is a reduction in encoder complexity for a given level of performance. As the original trellis encoding systems were motivated by convolutional channel codes with Viterbi decoders, trellis coded quantization was motivated by Ungerboeck's enormously successful coded modulation approach to channel coding for narrowband channels <ref> [358, 359] </ref>. Recent combinations of TCQ to coding wavelet coefficients [324] have yielding excellent performance in image coding applications, winning the JPEG 2000 contest of 1997 and thereby a position as a serious contender for the new standard.
Reference: [359] <author> G. Ungerboeck, </author> <title> "Trellis-coded modulation with redundant signal sets," Parts I and II," </title> <journal> IEEE Communications Magazine, </journal> <volume> Vol. 25, </volume> <pages> pp. 5-21, </pages> <month> Feb. </month> <year> 1987. </year>
Reference-contexts: The primary gain resulting is a reduction in encoder complexity for a given level of performance. As the original trellis encoding systems were motivated by convolutional channel codes with Viterbi decoders, trellis coded quantization was motivated by Ungerboeck's enormously successful coded modulation approach to channel coding for narrowband channels <ref> [358, 359] </ref>. Recent combinations of TCQ to coding wavelet coefficients [324] have yielding excellent performance in image coding applications, winning the JPEG 2000 contest of 1997 and thereby a position as a serious contender for the new standard.
Reference: [360] <editor> J. Vaisey and A. Gersho, </editor> <booktitle> "Simulated annealing and codebook design," </booktitle> <pages> pp. 1176-1179, </pages> <booktitle> Proceedings of the Intl. Conf. on Acoustics, Speech, and Signal Processing, </booktitle> <address> New York, </address> <month> April </month> <year> 1988. </year>
Reference-contexts: The Mid 1980's to the Present In the late 80's a wide variety of vector quantizer design algorithms were developed and tested for speech, images, video, and other signal sources. Some of the quantizer design algorithms developed as alternatives to Lloyd's algorithm include simulated annealing <ref> [107, 360, 125, 208] </ref>, deterministic annealing [312], pairwise nearest neighbor [112] (which had its origins in earlier clustering techniques [370]), stochastic relaxation [394, 396], self organizing feature maps [209, 384, 385] and other neural nets [348, 217, 347, 241, 54].
Reference: [361] <author> H. Van de Weg, </author> <title> "Quantization noise of a single integration delta modulation system with an N-digit code," </title> <journal> Phillips Res. Rep., </journal> <volume> Vol. 8, </volume> <pages> pp. 568-569, </pages> <month> Aug. </month> <year> 1971. </year>
Reference-contexts: In 1950 Elias [108] provided an information theoretic development of the benefits of predictive coding, but the work was not published until 1955 [109]. Other early references include <ref> [283, 216, 177, 361, 397] </ref>.
Reference: [362] <author> J. Vanderkooy and S. P. Lipshitz, </author> <title> "Dither in Digital Audio," </title> <journal> J. Audio Eng. Soc., </journal> <volume> Vol. 35, </volume> <pages> pp. 966-975, </pages> <month> Dec. </month> <year> 1987. </year>
Reference-contexts: The properties of nonsubtractive dither were originally developed in unpublished work by Wright [382] in 1979 and Brinton [44] in 1984 and subsequently extended and refined with a variety of proofs <ref> [363, 362, 236, 172] </ref>.
Reference: [363] <author> J. Vanderkooy and S. P. Lipshitz, </author> <title> "Resolution below the least significant bit in digital systems with dither," </title> <journal> J. Audio Eng. Soc., </journal> <volume> Vol. 32, </volume> <pages> pp. 106-113, </pages> <month> Nov. </month> <year> 1984. </year> <title> Correction Ibid., </title> <editor> p. </editor> <volume> 889. </volume>
Reference-contexts: The properties of nonsubtractive dither were originally developed in unpublished work by Wright [382] in 1979 and Brinton [44] in 1984 and subsequently extended and refined with a variety of proofs <ref> [363, 362, 236, 172] </ref>.
Reference: [364] <author> M. Vetterli. </author> <title> Multi-dimensional sub-band coding: some theory and algorithms. </title> <booktitle> Signal Processing, </booktitle> <volume> 6 </volume> <pages> 97-112, </pages> <month> April </month> <year> 1984. </year> <month> 89 </month>
Reference-contexts: The interested reader is referred to the book by Vetterli and Kovacevic on wavelets and subband coding [365]. Subband coding was introduced in the context of speech coding in 1976 by Crochiere et al. [89]. The extension of subband filtering from 1-D to 2-D was made by Vetterli <ref> [364] </ref> and 2-D subband filtering was first applied to image coding by Woods et al. [380, 371, 381].
Reference: [365] <author> M. Vetterli and J. Kovacevic. </author> <title> Wavelets and Subband Coding. Signal Processing. </title> <publisher> Prentice-Hall, </publisher> <address> Englewood Cliffs, NJ, </address> <year> 1995. </year> <title> [366] "An algorithm for finding nearest neighbors in (approximately) constant average time complexity," </title> <journal> Pattern Recognition Letters, </journal> <volume> Vol. 4, </volume> <pages> pp. 145-157, </pages> <year> 1986. </year>
Reference-contexts: Hence we content ourselves with the mention of a few highlights. The interested reader is referred to the book by Vetterli and Kovacevic on wavelets and subband coding <ref> [365] </ref>. Subband coding was introduced in the context of speech coding in 1976 by Crochiere et al. [89]. The extension of subband filtering from 1-D to 2-D was made by Vetterli [364] and 2-D subband filtering was first applied to image coding by Woods et al. [380, 371, 381].
Reference: [367] <author> M. Vishwanath and P. Chou, </author> <title> "An efficient algorithm for hierarchical compression of video," </title> <journal> pp. </journal> <pages> 275-279, </pages> <booktitle> Proceedings ICIP-94, Vol. III, </booktitle> <publisher> IEEE Computer Society Press, </publisher> <address> Austin, Texas, </address> <month> November, </month> <year> 1994. </year>
Reference-contexts: Trellis-coded quantizers [249, 247, 123, 124, 248, 324] use a Viterbi algorithm encoder matched to a reproduction codebook with a trellis structure. Hierarchical table-lookup vector quantizers provide fixed-rate vector quantizers with minimal computational complexity, both lossy encoder and reproduction decoder being accomplished by table-lookups <ref> [69, 255, 367, 62] </ref>. Many of the quantization techniques, results, and applications can be found in original form in Swaszek's 1985 reprint collection on quantization [340] and Abut's 1990 IEEE Reprint Collection on Vector Quantization [2].
Reference: [368] <author> A. J. Viterbi and J. K. Omura, </author> <title> "Trellis encoding of memoryless discrete-time sources with a fidelity criterion," </title> <journal> IEEE Trans. on Information theory, </journal> <volume> Vol. 20, </volume> <pages> pp. 325-332, </pages> <month> May </month> <year> 1974. </year>
Reference-contexts: In the early 1970s the algorithms for tree decoding channel codes were inverted to form tree-encoding algorithms for sources by Jelinek, Anderson, and others [198, 199, 9, 8, 102]. Later trellis channel decoding algorithms were modified to trellis-encoding algorithms for sources by Viterbi and Omura <ref> [368] </ref>. While linear encoders sufficed for channel coding, nonlinear decoders were required for the source coding application, and a variety of design algorithms were developed for designing the decoder to populate the trellis searched by the encoder [229, 375, 337, 16, 33].
Reference: [369] <author> H.S. Wang and N. Moayeri, </author> <title> "Trellis coded vector quantization," </title> <journal> IEEE Trans Commun, </journal> <volume> Vol. 40, </volume> <pages> pp. 1273-1276, </pages> <month> Aug </month> <year> 1992. </year>
Reference-contexts: Trellis Coded Quantization Trellis coded quantization, both scalar and vector, improve upon traditional encoded systems by labeling the trellis branches with entire subcodebooks (or "subsets") rather than with individual reproduction levels <ref> [249, 247, 123, 124, 369, 248, 324] </ref>. The primary gain resulting is a reduction in encoder complexity for a given level of performance.
Reference: [370] <author> J. Ward, </author> <title> "Hierarchical grouping to optimize an objective function," </title> <journal> J. Amer. Stat. Assoc., </journal> <volume> Vol. 37, </volume> <pages> pp. 236-244, </pages> <month> March </month> <year> 1963. </year>
Reference-contexts: Some of the quantizer design algorithms developed as alternatives to Lloyd's algorithm include simulated annealing [107, 360, 125, 208], deterministic annealing [312], pairwise nearest neighbor [112] (which had its origins in earlier clustering techniques <ref> [370] </ref>), stochastic relaxation [394, 396], self organizing feature maps [209, 384, 385] and other neural nets [348, 217, 347, 241, 54].
Reference: [371] <author> P. H. Westerink, D. E. Boekee, J. Biemond, and J. W. Woods, </author> <title> "Subband coding of images using vector quantization," </title> <journal> IEEE Trans. Comm., </journal> <volume> vol. COM-36, </volume> <pages> pp. 713-719, </pages> <month> June </month> <year> 1988. </year>
Reference-contexts: Subband coding was introduced in the context of speech coding in 1976 by Crochiere et al. [89]. The extension of subband filtering from 1-D to 2-D was made by Vetterli [364] and 2-D subband filtering was first applied to image coding by Woods et al. <ref> [380, 371, 381] </ref>. Early wavelet coding techniques emphasized scalar or lattice vector quantization [10, 11, 100, 321, 12, 27, 140] and other vector quantization techniques have also been applied to wavelet coefficients, including tree encoding [261], residual vector quantization [212], and other methods [85].
Reference: [372] <author> P. H. Westerink and J. Biemond and D. E. Boekee, </author> <title> "An optimal bit allocation algorithm for sub-band coding," </title> <booktitle> Proc. Intl. Conf. on Acoust. Speech, and Signal Processing, </booktitle> <pages> pp. 757-760, </pages> <year> 1988. </year>
Reference-contexts: We will not delve into the large literature of transforms, but will observe that bit allocation becomes an important issue, and one can either use the high resolution approximations or a variety of non-asymptotic allocation algorithms such as the "fixed slope" or Pareto-optimality considered in <ref> [372, 332, 74, 306, 307, 321] </ref> . The method involves operating all quantizers at points on their operational distortion-rate curves of equal slopes. For a survey of some of these methods, see [85] or Chapter 10 of [149]. A combinatorial optimization method is given in [383].
Reference: [373] <author> B. Widrow, </author> <title> "A study of rough amplitude quantization by means of Nyquist sampling theory," </title> <journal> IRE Trans. Circuit Theory, </journal> <volume> Vol. 3, </volume> <pages> pp. 266-276, </pages> <year> 1956. </year>
Reference-contexts: This led to an "additive noise" model of quantizer error, since with these properties the formula q (X) = X + [q (X) X] could be interpreted as representing the quantizer output as the sum of a signal and white noise. This model was later popularized by Widrow <ref> [373, 374] </ref>, but the viewpoint avoids the fact that the "noise" is in fact dependent on the signal and the approximations are valid only under certain conditions.
Reference: [374] <author> B. Widrow, </author> <title> "Statistical analysis of amplitude quantized sampled data systems," </title> <journal> Trans. Amer. Inst. Elec. Eng., </journal> <volume> Pt. </volume> <booktitle> II: Applications and Industry, </booktitle> <volume> Vol. 79, </volume> <pages> pp. 555-568, </pages> <year> 1960. </year>
Reference-contexts: This led to an "additive noise" model of quantizer error, since with these properties the formula q (X) = X + [q (X) X] could be interpreted as representing the quantizer output as the sum of a signal and white noise. This model was later popularized by Widrow <ref> [373, 374] </ref>, but the viewpoint avoids the fact that the "noise" is in fact dependent on the signal and the approximations are valid only under certain conditions. <p> In 1960 Max [250] published a variational proof of the Lloyd optimality properties for rth power distortion measures, rediscovered Lloyd's Method II, and numerically investigated the design of fixed-rate quantizers for a varity of input densities. Also in 1960, Widrow <ref> [374] </ref> derived an exact formula for the characteristic function of a uniformly quantized signal when the quantizer has an infinite number of levels.
Reference: [375] <author> S. G. Wilson and D. W. Lytle, </author> <title> "Trellis encoding of continuous-amplitude memoryless sources," </title> <journal> IEEE Trans. on Information theory, </journal> <volume> Vol. 28, </volume> <pages> pp. 211-226, </pages> <month> March </month> <year> 1982. </year>
Reference-contexts: While linear encoders sufficed for channel coding, nonlinear decoders were required for the source coding application, and a variety of design algorithms were developed for designing the decoder to populate the trellis searched by the encoder <ref> [229, 375, 337, 16, 33] </ref>.
Reference: [376] <author> S.G. Wilson, </author> <title> "Magnitude/phase quantization of independent Gaussian variates," </title> <journal> IEEE Trans. Commun., </journal> <volume> Vol. 28, </volume> <pages> pp. 1924-1929, </pages> <month> Nov. </month> <year> 1990. </year>
Reference-contexts: A tree-structured VQ with far less memory is provided by a multistage or residual VQ [203, 182, 29, 223, 133, 66, 29, 289, 309, 30]. A variety of product vector quantizers use a cartesian product reproduction codebook, which often can be rapidly searched. Examples include polar vector quantizers <ref> [291, 292, 47, 376, 345, 346, 339, 342, 344] </ref>, mean-removed vector quantizers [18, 19], and shape-gain vector quantizers [313, 281]. Trellis-coded quantizers [249, 247, 123, 124, 248, 324] use a Viterbi algorithm encoder matched to a reproduction codebook with a trellis structure. <p> Such quantizers are called "unrestricted" polar quantization <ref> [376, 344] </ref>. High resolution analysis can be used to study the rate-distortion performance of these quantiz-ers [47, 48, 339, 341, 344]. Among other things such analyses find the optimal point density for the magnitude quantizer and the optimal bit allocation between magnitude and phase.
Reference: [377] <author> P. A. Wintz, </author> <title> "Transform picture coding," </title> <journal> Proc. IEEE, </journal> <volume> Vol. 60, </volume> <pages> pp. 809-820, </pages> <month> July, </month> <year> 1972. </year>
Reference: [378] <author> D. Wong, B.-H. Juang, A. H. Gray, Jr., </author> <title> "An 800 bit/s vector quantization LPC vocoder," </title> <journal> IEEE Trans. Acoustics, Speech, and Signal Processing, </journal> <volume> Vol. 30, </volume> <pages> pp. 770-779, </pages> <month> Oct. </month> <year> 1982. </year>
Reference-contexts: The result was an 800 bits per second LPC speech coder with intelligible quality comparable to that scalar quantized LPC speech coders of four times the rate. (See also <ref> [378] </ref>.) In the same year Adoul, Debray, and Dalle [4] also used a spectral distance measure to optimize predictors for DPCM and the first thorough study of vector quantization for image compression was published by Yamada, Fujita, and Tazaki [386].
Reference: [379] <author> R.C. </author> <title> Wood "On optimal quantization," </title> <journal> IEEE Trans. Information Theory, </journal> <volume> Vol. 5, No. 2, </volume> <pages> pp. 248-252, </pages> <year> 1969. </year>
Reference-contexts: In 1969 Wood <ref> [379] </ref> provided a numerical descent algorithm for designing an entropy constrained scalar quantizer, and showed, as predicted by Goblick and Holsinger and by Gish and Pierce, that the performance was only slightly superior to a uniform scalar quantizer followed by a lossless code. <p> For example, Lloyd [237] used this approach to show that, ignoring overload distortion, the approximation error in the Panter-Dite formula is o (1=N 2 ), which means that it tends to zero, even when divided by 1=N 2 . Roe [311], Algazi [6] and Wood <ref> [379] </ref> also used Taylor series.
Reference: [380] <author> J. W. Woods and S. D. O'Neil, </author> <title> "Subband coding of images," </title> <journal> IEEE Trans. Acoustics, Speech, and Signal Processing, </journal> <volume> Vol. 34, </volume> <pages> pp. 1278-1288, </pages> <month> Oct. </month> <year> 1986. </year>
Reference-contexts: Subband coding was introduced in the context of speech coding in 1976 by Crochiere et al. [89]. The extension of subband filtering from 1-D to 2-D was made by Vetterli [364] and 2-D subband filtering was first applied to image coding by Woods et al. <ref> [380, 371, 381] </ref>. Early wavelet coding techniques emphasized scalar or lattice vector quantization [10, 11, 100, 321, 12, 27, 140] and other vector quantization techniques have also been applied to wavelet coefficients, including tree encoding [261], residual vector quantization [212], and other methods [85].
Reference: [381] <author> John W. Woods, Ed., </author> <title> Subband Image Coding, </title> <publisher> Kluwer Academic Publishers, </publisher> <address> Boston, </address> <year> 1991. </year>
Reference-contexts: Subband coding was introduced in the context of speech coding in 1976 by Crochiere et al. [89]. The extension of subband filtering from 1-D to 2-D was made by Vetterli [364] and 2-D subband filtering was first applied to image coding by Woods et al. <ref> [380, 371, 381] </ref>. Early wavelet coding techniques emphasized scalar or lattice vector quantization [10, 11, 100, 321, 12, 27, 140] and other vector quantization techniques have also been applied to wavelet coefficients, including tree encoding [261], residual vector quantization [212], and other methods [85].
Reference: [382] <author> N. Wright, </author> <title> Unpublished work. </title>
Reference-contexts: For example, it can make the perceived quantization noise energy constant as an input signal fades from high intensity to low intensity, where otherwise it can (and does) exhibit strongly signal-dependent behavior. The properties of nonsubtractive dither were originally developed in unpublished work by Wright <ref> [382] </ref> in 1979 and Brinton [44] in 1984 and subsequently extended and refined with a variety of proofs [363, 362, 236, 172].
Reference: [383] <author> X. Wu, </author> <title> "Globally optimum bit allocation," </title> <booktitle> Proc. Data Compression Conference, Snowbird, Utah, </booktitle> <pages> pp. 22-31, </pages> <year> 1993. </year>
Reference-contexts: The method involves operating all quantizers at points on their operational distortion-rate curves of equal slopes. For a survey of some of these methods, see [85] or Chapter 10 of [149]. A combinatorial optimization method is given in <ref> [383] </ref>. As a final comment on traditional transform coding, the code can be considered as being suboptimal as a k dimensional quantizer because of the constrained structure (transform and product code).
Reference: [384] <author> L. Wu and F. Fallside, </author> <title> "On the design of connectionist vector quantizers," </title> <booktitle> Computer Speech and Language, </booktitle> <volume> vol. 5, </volume> <pages> 207-229, </pages> <year> 1991. </year>
Reference-contexts: Some of the quantizer design algorithms developed as alternatives to Lloyd's algorithm include simulated annealing [107, 360, 125, 208], deterministic annealing [312], pairwise nearest neighbor [112] (which had its origins in earlier clustering techniques [370]), stochastic relaxation [394, 396], self organizing feature maps <ref> [209, 384, 385] </ref> and other neural nets [348, 217, 347, 241, 54].
Reference: [385] <author> L. Wu and F. Fallside, </author> <title> "Source coding and vector quantization with codebook-excited neural networks," </title> <booktitle> Computer Speech and Language, </booktitle> <volume> vol. 6, </volume> <pages> 43-276, </pages> <year> 1992. </year>
Reference-contexts: Some of the quantizer design algorithms developed as alternatives to Lloyd's algorithm include simulated annealing [107, 360, 125, 208], deterministic annealing [312], pairwise nearest neighbor [112] (which had its origins in earlier clustering techniques [370]), stochastic relaxation [394, 396], self organizing feature maps <ref> [209, 384, 385] </ref> and other neural nets [348, 217, 347, 241, 54].
Reference: [386] <author> Y. Yamada and K. Fujita and S. Tazaki, </author> <title> "Vector quantization of video signals," </title> <booktitle> Proceedings of Annual Conference of IECE, </booktitle> <pages> pp. 1031, </pages> <year> 1980. </year>
Reference-contexts: quantized LPC speech coders of four times the rate. (See also [378].) In the same year Adoul, Debray, and Dalle [4] also used a spectral distance measure to optimize predictors for DPCM and the first thorough study of vector quantization for image compression was published by Yamada, Fujita, and Tazaki <ref> [386] </ref>. There followed an active period for all facets of quantization theory and design.
Reference: [387] <author> Y. Yamada and S. Tazaki and R. M. Gray, </author> <title> "Asymptotic performance of block quantizers with a difference distortion measure," </title> <journal> IEEE Trans. on Information theory, </journal> <volume> Vol. 26, </volume> <pages> pp. 6-14, </pages> <month> Jan. </month> <year> 1980. </year>
Reference-contexts: Portions of this work were extended to nondecreasing functions of norms in <ref> [387] </ref>.
Reference: [388] <author> Y. Yamada and S. Tazaki, </author> <title> "Vector quantizer design for video signals," </title> <journal> IECE Trans., </journal> <volume> Vol. J66-B, </volume> <pages> pp. 965-972, </pages> <year> 1983. </year>
Reference: [389] <author> P.L. Zador, </author> <title> "Development and evaluation of procedures for quantizing multivariate distributions," </title> <type> Ph.D. Dissertation, </type> <institution> Stanford University, 1963. (Also Stanford University Department of Statistics Technical Report.) </institution>
Reference-contexts: a sufficient condition (namely, that the log of the source density be concave) in order that the optimal quantizer be the only local optimal quantizer, and consequently, that Lloyd's Method I will yield a globally optimal quantizer. (The condition is satisfied for common densities such as Gaussian and Laplacian.) Zador <ref> [389] </ref> referred to Lloyd a year earlier in his Ph.D. thesis and Stanford University Department of Statistics Technical Report. In 1959 Shteyn [333] added terms representing overload distortion to the 2 =12 formula and to Bennett's integral and used them optimize uniform and nonuniform quantizers. <p> Cox in 1957 [88] also derived similar conditions. Some additional early work, which can now be seen as relating to vector quantization, will be reviewed later <ref> [325, 354, 389] </ref>. Scalar Quantization with Memory It was recognized early that common sources such as speech and images had considerable redundancy that scalar quantization could not exploit. In other words, there is statistical correlation or dependence between the samples of such sources, which are said to have memory. <p> The first high resolution approximations for vector quantization were published by Schutzen-berger in 1958 [320], who found upper and lower bounds to the least distortion of k-dimensional variable-rate vector quantizers, both of the form K2 2R . Unfortunately, the upper and lower bounds diverged. In 1963 Zador <ref> [389] </ref> made a very large and important advance by using high resolution methods to show that for large rates, the operational distortion-rate function of fixed-rate quantization has the form ffi k (R) = b k jjf k jj k=(k+2) 2 2R ; (16) where b k is a term that is <p> Zador's factor fi k tends to be smaller for source densities that are more "compact" (lighter tails and more uniform) and have more dependence among the source variables. Fortunately, high resolution theory need not rely solely on Gersho's conjecture, because Zador's dissertation <ref> [389] </ref> and subsequent memo [390] showed that for large rate ffi (R) has the form b k fi k oe 2 2 2R , where b k is independent of the source distribution. Thus Gersho's conjecture is really just a conjecture about b k . <p> This operational distortion-rate function was also derived by Zador <ref> [389] </ref>, who showed that his unknown factors b k and c k converged to (2e) 1 . But the argument given here is due to Gersho [147]. Notice that in this limiting case, there is no doubt about the constant M . <p> The lower bound is asymptotically achievable by a lattice with hexagonal cells; see [231] for a rigorous proof. It follows then that the ratio of ffi 2 (R) to M (hexagon)oe 2 2 2R tends to one, and also, that Gersho's conjecture holds for dimension two. Zador's thesis (1963) <ref> [389] </ref> was the next rigorous work. As mentioned earlier, it contains two principal results. <p> Finally, it is interesting to note that high resolution theory actually contains some analyses of the Shannon random coding approach. For example, Zador's thesis <ref> [389] </ref> gives an upper bound on the distortion of a randomly generated vector quantizer. Succesive Approximation Many vector quantizers operate in a successive approximation or progressive fashion, whereby a low rate coarse quantization is followed by a sequence of finer and finer quantizations, which add to the rate.
Reference: [390] <author> P. L. Zador, </author> <title> "Topics in the asymptotic quantization of continuous random variables," </title> <institution> Bell Laboratories Technical Memorandum, </institution> <year> 1966. </year>
Reference-contexts: Max merely computed the entropy of nonuniform and uniform quantizers designed to minimize distortion with a given number of levels. For a Gaussian source his results show potential rate reductions of about .5 bits/sample. In an unpublished 1966 Bell Telephone Laboratories Technical Memo <ref> [390] </ref>, Zador also envisioned variable-rate (as well as fixed-rate) quantization. As his focus was on vector quantization his work will be detailed later. <p> Zador's remarkable dissertation also dealt with the analysis of variable-rate vector quantization, but the asymptotic formula given there is not the correct one. Rather it was left to his subsequent unpublished 1966 memo <ref> [390] </ref> to derive the correct formula. (Curiously, his 1982 paper [391] reports the formula from the thesis rather than the memo.) Again using high resolution methods, he showed that for large rates, the operational distortion-rate function of variable-rate vector quantization has the form ffi k (R) = c k 2 2h <p> Zador's factor fi k tends to be smaller for source densities that are more "compact" (lighter tails and more uniform) and have more dependence among the source variables. Fortunately, high resolution theory need not rely solely on Gersho's conjecture, because Zador's dissertation [389] and subsequent memo <ref> [390] </ref> showed that for large rate ffi (R) has the form b k fi k oe 2 2 2R , where b k is independent of the source distribution. Thus Gersho's conjecture is really just a conjecture about b k . <p> This can be directly verified using Jensen's inequality [147]. As mentioned in Section 2, Zador's seminal thesis also dealt with the analysis of variable-rate quantization, but the asymptotic formula given there is not the correct one. Rather it was left to his subsequent unpublished 1966 memo <ref> [390] </ref> to derive the correct formula. <p> Zador <ref> [390] </ref> defined a very general "rth power distortion measure" as any distortion measure of the form d (x; y) = ae (x y) where for any a &gt; 0, ae (ax) = a r ae (jx 1 j; : : : ; jx k j), some r &gt; 0. <p> One can easily see how this construction creates codes with essentially optimal point density and cell shape. We will not describe the converse. Zador's 1966 Bell Labs Memorandum <ref> [390] </ref> reproves these two main results under weaker conditions. The distortion measure is rth power in the general sense, which includes as special cases the narrow sense of the rth power of the Euclidean norm considered by Schutzenberger [320].
Reference: [391] <author> P.L. Zador, </author> <title> "Asymptotic quantization error of continuous signals and the quantization dimension," </title> <journal> IEEE Transactions on Infor--mation Theory, </journal> <volume> Vol. 28, </volume> <pages> pp. 139-148. </pages> <month> March </month> <year> 1982. </year> <note> (Revised version of [390].) </note>
Reference-contexts: Unfortunately, the results of Zador's thesis were not published until 1982 <ref> [391] </ref> and were little known outside of Bell Laboratories until, probably, Gersho's important paper of 1979 [147], to be described later. Zador's remarkable dissertation also dealt with the analysis of variable-rate vector quantization, but the asymptotic formula given there is not the correct one. <p> Zador's remarkable dissertation also dealt with the analysis of variable-rate vector quantization, but the asymptotic formula given there is not the correct one. Rather it was left to his subsequent unpublished 1966 memo [390] to derive the correct formula. (Curiously, his 1982 paper <ref> [391] </ref> reports the formula from the thesis rather than the memo.) Again using high resolution methods, he showed that for large rates, the operational distortion-rate function of variable-rate vector quantization has the form ffi k (R) = c k 2 2h k (X) 2 2R ; (17) where c k is
Reference: [392] <author> R. Zamir and M. Feder, </author> <title> "On lattice quantization noise," </title> <journal> IEEE Transactions on Information Theory, vol.42, </journal> <volume> no.4, </volume> <pages> pp. 1152-9, </pages> <month> July </month> <year> 1996. </year>
Reference: [393] <author> R. Zamir and M. Feder, </author> <title> "Information rates of pre/post-filtered dithered quantizers," </title> <journal> IEEE Transactions on Information Theory, vol.42, </journal> <volume> no.5, </volume> <pages> pp. 1340-53, </pages> <month> Sept. </month> <year> 1996. </year>
Reference-contexts: Though it has long been presumed, only recently has it been directly shown that the M k 's tend to (2e) 1 as k increases (Zamir and Feder <ref> [393] </ref>), which is the limit of the normalized moment of inertia of k-dimensional spheres as k tends to infinity. Previously, the assertion that the M k 's tend (2e) 1 depended on Gersho's conjecture. <p> For example, Ziv [398] showed that even without high resolution theory, uniform scalar quantization combined with dithering and vector lossless coding could yield perfor 73 mance within .75 bits/symbol of the rate-distortion function. Extensions to lattice quantization and variations of this result have been developed by Zamir and Feder <ref> [393] </ref> Other Applications We have not treated many interesting variations and applications of quantization, several of which have been successfully analyzed or designed using the tools described here.
Reference: [394] <author> K. Zeger and A. Gersho, </author> <title> "A stochastic relaxation algorithm for improved vector quantiser design," </title> <journal> Electronics Letters, </journal> <volume> Vol 25, </volume> <pages> pp. 896-898, </pages> <month> July </month> <year> 1989. </year>
Reference-contexts: Some of the quantizer design algorithms developed as alternatives to Lloyd's algorithm include simulated annealing [107, 360, 125, 208], deterministic annealing [312], pairwise nearest neighbor [112] (which had its origins in earlier clustering techniques [370]), stochastic relaxation <ref> [394, 396] </ref>, self organizing feature maps [209, 384, 385] and other neural nets [348, 217, 347, 241, 54].
Reference: [395] <author> K. Zeger and M. R. Kantorovitz, </author> <title> "Average Number of Facets per Cell in Tree-Structured Vector Quantizer Partitions," </title> <journal> IEEE Trans. on Information Theory, </journal> <volume> Vol. 39, </volume> <pages> pp. 1053-1055, </pages> <month> May </month> <year> 1993. </year>
Reference-contexts: A paper investigating the nature of TSVQ cells is <ref> [395] </ref>. Our experience has been that when taking both performance and complexity into account, TSVQ is a very competitive VQ method.
Reference: [396] <author> K. Zeger and J. Vaisey and A. Gersho, </author> <title> "Globally optimal vector quantizer design by stochastic relaxation," </title> <journal> IEEE Trans. on Signal Processing, 1992, </journal> <volume> Vol. 40, No. 2, </volume> <pages> pp. 310-322, </pages> <month> Feb. </month>
Reference-contexts: Some of the quantizer design algorithms developed as alternatives to Lloyd's algorithm include simulated annealing [107, 360, 125, 208], deterministic annealing [312], pairwise nearest neighbor [112] (which had its origins in earlier clustering techniques [370]), stochastic relaxation <ref> [394, 396] </ref>, self organizing feature maps [209, 384, 385] and other neural nets [348, 217, 347, 241, 54].
Reference: [397] <author> L. H. Zetterberg, </author> <title> "A comparison between delta and pulse code modulation," </title> <journal> Ericsson Techics, </journal> <volume> Vol. 11, No. 1, </volume> <pages> pp. 95-154, </pages> <year> 1955. </year>
Reference-contexts: In 1950 Elias [108] provided an information theoretic development of the benefits of predictive coding, but the work was not published until 1955 [109]. Other early references include <ref> [283, 216, 177, 361, 397] </ref>.
Reference: [398] <author> J. Ziv, </author> <title> "Universal Quantization," </title> <journal> IEEE Trans. on Information theory, </journal> <volume> Vol. 31, </volume> <pages> pp. 344-347, </pages> <year> 1985. </year> <month> 91 </month>
Reference-contexts: scalar quantization with variable-length coding of k successive quantizer outputs (block entropy coding) achieves performance that is 1.53 dB (.255 bits/sample) from ffi (R), even for sources with memory. (They accomplished this by comparison to Shannon lower bounds.) This surprising result was not widely appreciated until rediscovered by Ziv (1985)) <ref> [398] </ref>. <p> Berger [38] showed that permutation codes achieved roughly the same performance with a fixed-rate vector quantizer. Ziv <ref> [398] </ref> showed in 1985 that if subtractive dithering is allowed, dithered uniform quantization followed by block lossless encoding will be at most .754 bits worse than the optimal entropy constrained vector quantizer with the same block size, even if the rate is not high. <p> For scalar quantization (35) and the fact that uniform quantization is best were first derived by Gish and Pierce (1968) [155]. However, the existence of their result in is still widely overlooked, as credit is more often given to Ziv (1985) <ref> [398] </ref>. Now let us fix the order of the entropy coder to be one and consider arbitrary quantizer dimension. <p> In addition to its role in whitening quantization noise and making the noise or its moments independent of the input, dithering has played a role in proofs of "universal quantization" results in information theory. For example, Ziv <ref> [398] </ref> showed that even without high resolution theory, uniform scalar quantization combined with dithering and vector lossless coding could yield perfor 73 mance within .75 bits/symbol of the rate-distortion function.
References-found: 391

