URL: http://www.isi.edu/~shen/URL.ps
Refering-URL: http://www.isi.edu/~shen/papers_by_date.html
Root-URL: 
Title: Unsupervised Learning of Relational Patterns  
Keyword: Content Areas: machine learning Word Count: 5160 Tracking Number: A355  
Abstract: Learning relational patterns without supervision is a challenging and open problem for machine learning, yet a very desirable feature for real-world applications. In this paper, we present a new algorithm for unsupervised learning of relational patterns. Given a relational database, this algorithm can find function-free Horn clauses without requiring users to label the data as positive or negative examples, and specify what target concepts to learn. The algorithm is a heuristic search through the relational pattern space. It starts with general patterns derived from a given database schema, and then iteratively generates new promising patterns using the knowledge dynamically collected in the learning process, such as previously learned patterns. To demonstrate the capability of the algorithm, we show some abstracted database examples, and we also show that some of the well-known examples in the relational concept learning literature can be learned without supervision. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Aho, A.V., J.E. Hopcroft, and J.D. Ullman, </author> <title> "The Design and Analysis of Computer Algorithms", </title> <publisher> Addison-Wesley Publishing Company, </publisher> <year> 1974. </year>
Reference-contexts: Finding circles in a graph can be accomplished by using a standard transitive closure algorithm (see for example <ref> [1] </ref>) with some simple augmentations to enforce the alternating edge constraint. A graph G cannot have more than jGj! circles because a circle, without duplicated nodes, cannot be longer than the size of the graph.
Reference: [2] <author> Bisson, G., "KBG, </author> <title> A Knowledge Based Generalizer", </title> <booktitle> The Proceedings of the 7th International Conference on Machine Learning, </booktitle> <publisher> Morgan Kaufmann, </publisher> <pages> pp. 9-15, </pages> <year> 1990. </year>
Reference: [3] <author> Bisson, G., </author> <title> "Conceptual clustering in a first order logic representation", </title> <booktitle> The Proceedings of the 10th European Conference on Artificial Intelligence, </booktitle> <pages> pp. 458-462, </pages> <year> 1992. </year>
Reference: [4] <author> Hinton, G.E., </author> <title> "Learning distributed representations of concepts", </title> <booktitle> Proceedings of the 8th Annual Conference of the Cognitive Science Society, </booktitle> <year> 1986. </year>
Reference-contexts: It was first used by Hinton in his neural network system <ref> [4] </ref>, and then used by Quinlan in his FOIL system. 10 Again, using this example, Quinlan shown that given a concept and its positive and negative examples, FOIL can learn its relational definition. Here, we show that the concept can be learned without supervision 4 .
Reference: [5] <author> Kietz, J.U. and K. Morik, </author> <title> "A Polynomial Approach to the Constructive Induction of Structural Knowledge", </title> <journal> Machine Learning, </journal> <volume> 14(2) </volume> <pages> 193-218, </pages> <year> 1994. </year>
Reference-contexts: Most existing systems such as FOIL [10], FOCL [9], CIGOL [7], Progol [8] need pre-specified concepts and pre-labeled examples. On the other hand, unsupervised learning systems mainly focus on attribute-value concepts. Only a few of them, such as CLUSTER/S [13], MOBAL [15], KLUSTER <ref> [5] </ref>, LABYRINTH [14], and KBG [2,3], can learn more expressive concepts. However, they focused on different aspects than ours (a detailed analysis is given in the related work section).
Reference: [6] <author> Minton, S. and I. Underwood, </author> <title> "Small is Beautiful: A Brute-Force Approach to Learning First-Order Formulas", </title> <booktitle> Proceedings of the 12th National Conference on Artificial Intel 13 ligence, </booktitle> <pages> pp. 168-174, </pages> <year> 1994. </year>
Reference-contexts: The second option is useful when the size of the graph G is very large and users are interested in seeing shorter patterns first (this bears the same philosophy as <ref> [6] </ref>). In the following sections, we shall explain each phase in detail. 2.1 Find Connections Between Database Tables In order to generate relational patterns, we first identify sets of data fields (also known as "columns" in the database literature) that are significantly connected.
Reference: [7] <author> Muggleton, S. and W. Buntine, </author> <title> "Machine invention of first order predicates by inverting resolution", </title> <booktitle> Proceedings of the 5th international Machine Learning Workshop, </booktitle> <publisher> Morgan Kaufmann, </publisher> <pages> pp. 339-352, </pages> <year> 1988. </year>
Reference-contexts: Moreover, sometimes we even don't know a priori what patterns to learn. 1 It is unfortunate that current machine learning systems have limitations for this type of learning. On the one hand, relational concept learning systems need supervision. Most existing systems such as FOIL [10], FOCL [9], CIGOL <ref> [7] </ref>, Progol [8] need pre-specified concepts and pre-labeled examples. On the other hand, unsupervised learning systems mainly focus on attribute-value concepts. Only a few of them, such as CLUSTER/S [13], MOBAL [15], KLUSTER [5], LABYRINTH [14], and KBG [2,3], can learn more expressive concepts.
Reference: [8] <author> Muggleton, S., </author> <title> "Inverse entailment and Progol", </title> <journal> New Generation Computing Journal, </journal> <volume> Vol. 13, </volume> <pages> pp. 245-286, </pages> <year> 1995. </year>
Reference-contexts: On the one hand, relational concept learning systems need supervision. Most existing systems such as FOIL [10], FOCL [9], CIGOL [7], Progol <ref> [8] </ref> need pre-specified concepts and pre-labeled examples. On the other hand, unsupervised learning systems mainly focus on attribute-value concepts. Only a few of them, such as CLUSTER/S [13], MOBAL [15], KLUSTER [5], LABYRINTH [14], and KBG [2,3], can learn more expressive concepts.
Reference: [9] <author> Pazzani, M. and D. Kibler, </author> <title> "The Utility of Knowledge in Inductive Learning", </title> <journal> Machine Learning, </journal> <volume> 9 </volume> <pages> 57-94, </pages> <year> 1992. </year>
Reference-contexts: Moreover, sometimes we even don't know a priori what patterns to learn. 1 It is unfortunate that current machine learning systems have limitations for this type of learning. On the one hand, relational concept learning systems need supervision. Most existing systems such as FOIL [10], FOCL <ref> [9] </ref>, CIGOL [7], Progol [8] need pre-specified concepts and pre-labeled examples. On the other hand, unsupervised learning systems mainly focus on attribute-value concepts. Only a few of them, such as CLUSTER/S [13], MOBAL [15], KLUSTER [5], LABYRINTH [14], and KBG [2,3], can learn more expressive concepts.
Reference: [10] <author> Quinlan, J.R., </author> <title> "Learning Logical Definitions from Relations", </title> <journal> Machine Learning, </journal> <volume> 5 </volume> <pages> 239-266, </pages> <year> 1990. </year>
Reference-contexts: Moreover, sometimes we even don't know a priori what patterns to learn. 1 It is unfortunate that current machine learning systems have limitations for this type of learning. On the one hand, relational concept learning systems need supervision. Most existing systems such as FOIL <ref> [10] </ref>, FOCL [9], CIGOL [7], Progol [8] need pre-specified concepts and pre-labeled examples. On the other hand, unsupervised learning systems mainly focus on attribute-value concepts. Only a few of them, such as CLUSTER/S [13], MOBAL [15], KLUSTER [5], LABYRINTH [14], and KBG [2,3], can learn more expressive concepts.
Reference: [11] <author> Shen, W.-M., </author> <title> "Discovering regularities from Knowledge Bases", </title> <journal> International Journal of Intelligent Systems, </journal> <volume> 7(7) </volume> <pages> 623-636, </pages> <year> 1992. </year>
Reference-contexts: The reason we choose transitivity patterns as a focal point here is because these patterns are highly structured and they are general enough to derive other types of pattern, such as implication, inheritance, transfer-through, and function dependency (see <ref> [11] </ref> for details).
Reference: [12] <author> Shen, W.-M., K. Ong, B. Mitbander, and C. Zaniolo, </author> <title> "Metaqueries for Data Mining", chapter 15. </title> <publisher> MIT Press, </publisher> <year> 1995. </year>
Reference-contexts: One way to ease this problem is to start with high thresholds and gradually decrease them if too few patterns are found. The fourth is to integrate the -algorithm with mining systems such as <ref> [12] </ref> to automatically generate metapatterns to increase the efficiency and effectiveness of today's knowlege discovery systems. Actually, this work is developed in that context. The last, and also the ultimate goal, is to apply the algorithm on real databases.
Reference: [13] <author> Stepp, R.E. and R.S. Michalski, </author> <title> "Inventing Goal-Oriented Classifications of Structured Objects", </title> <booktitle> Machine Learning an Artificial Intelligence Approach, </booktitle> <volume> vol. II, </volume> <pages> pp. 471-498, </pages> <year> 1986. </year>
Reference-contexts: On the one hand, relational concept learning systems need supervision. Most existing systems such as FOIL [10], FOCL [9], CIGOL [7], Progol [8] need pre-specified concepts and pre-labeled examples. On the other hand, unsupervised learning systems mainly focus on attribute-value concepts. Only a few of them, such as CLUSTER/S <ref> [13] </ref>, MOBAL [15], KLUSTER [5], LABYRINTH [14], and KBG [2,3], can learn more expressive concepts. However, they focused on different aspects than ours (a detailed analysis is given in the related work section).
Reference: [14] <author> Thompson, K. and P. Langley, </author> <title> "Incremental concept formation with composite objects", </title> <booktitle> Proceedings of the 6th International Workshop on Machine Learning, </booktitle> <publisher> Morgan Kauf-mann, </publisher> <pages> pp. 373-374, </pages> <year> 1989. </year>
Reference-contexts: Most existing systems such as FOIL [10], FOCL [9], CIGOL [7], Progol [8] need pre-specified concepts and pre-labeled examples. On the other hand, unsupervised learning systems mainly focus on attribute-value concepts. Only a few of them, such as CLUSTER/S [13], MOBAL [15], KLUSTER [5], LABYRINTH <ref> [14] </ref>, and KBG [2,3], can learn more expressive concepts. However, they focused on different aspects than ours (a detailed analysis is given in the related work section).
Reference: [15] <author> Wrobe, S., </author> <title> "Concept Formation During Interactive Theory Revision", </title> <journal> Machine Learning, </journal> <volume> 14(2) </volume> <pages> 169-192, </pages> <year> 1994. </year> <month> 14 </month>
Reference-contexts: Most existing systems such as FOIL [10], FOCL [9], CIGOL [7], Progol [8] need pre-specified concepts and pre-labeled examples. On the other hand, unsupervised learning systems mainly focus on attribute-value concepts. Only a few of them, such as CLUSTER/S [13], MOBAL <ref> [15] </ref>, KLUSTER [5], LABYRINTH [14], and KBG [2,3], can learn more expressive concepts. However, they focused on different aspects than ours (a detailed analysis is given in the related work section).
References-found: 15

