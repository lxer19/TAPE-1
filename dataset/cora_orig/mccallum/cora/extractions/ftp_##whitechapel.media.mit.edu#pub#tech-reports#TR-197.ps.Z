URL: ftp://whitechapel.media.mit.edu/pub/tech-reports/TR-197.ps.Z
Refering-URL: http://www-white.media.mit.edu/cgi-bin/tr_pagemaker/
Root-URL: http://www.media.mit.edu
Title: Recognition of Space-Time Gestures using a Distributed Representation  
Author: Trevor J. Darrell and Alex P. Pentland 
Abstract: M.I.T. Media Laboratory Vision and Modeling Group Technical Report No. 197 ABSTRACT This paper presents a method for learning, tracking, and recognizing human gestures using a view-based approach to model both object and behavior. Object views are represented using sets of view models, rather than single templates. Stereotypical space-time patterns, i.e. gestures, are then matched to stored gesture patterns using dynamic time warping. Real-time performance is achieved by using special-purpose correlation hardware and view prediction to prune as much of the search space as possible. Both view models and view predictions are learned from examples. We present results showing tracking and recognition of human hand gestures at over 10Hz. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Bellman, R. E., </author> <title> (1957) Dynamic Programming. </title> <publisher> Princeton, </publisher> <address> NJ: </address> <publisher> Princeton Univ. Press. </publisher>
Reference-contexts: Each example is time-warped to be the same length as the the longest example before computing statistics on the correlation scores. 5.1 Dynamic Time Warping The dynamic time warping method involves the use of dynamic programming techniques <ref> [1] </ref> to solve an elastic pattern matching task. This method was developed to solve the time alignment problem in the speech and signal processing literature [8], and is easily applied to our task.
Reference: [2] <author> Bolt, R. </author> <title> (1984) "The Human Interface, where people and computers meet," Lifetime Learning Publications, </title> <address> Belmont, California. </address>
Reference-contexts: 1 Introduction The location and orientation of head, hand, and eyes is a critical element of all human dialog. The ability to follow objects moving through space and recognize particular motions as meaningful gestures is therefore essential if computer systems are to interact naturally with human users <ref> [2, 6] </ref>. Currently, however, this information is largely unavailable to computing machines. An important new application of machine vision, therefore, is to extend the interface between man and machine, allowing the machine to directly perceive what its user is doing [5, 10, 12, 3].
Reference: [3] <author> Ishibuchi, K., Takemura, H., and Kishino, F., </author> <title> "Real Time Hand Shape Recognition using Pipe-line Image Processor", </title> <booktitle> (1992) IEEE Workshop on Robot and Human Communication, </booktitle> <pages> pp. 111-116 </pages>
Reference-contexts: Currently, however, this information is largely unavailable to computing machines. An important new application of machine vision, therefore, is to extend the interface between man and machine, allowing the machine to directly perceive what its user is doing <ref> [5, 10, 12, 3] </ref>. This paper reports a system that can recover such information in real time, so that it can help mediate human-machine interaction.
Reference: [4] <author> Makhoul, J., Roucos, S., and Gish, H., </author> <title> (1985) "Vector Quantization in Speech Coding" Proc. </title> <journal> IEEE, </journal> <volume> Vol. 73, No. 11, </volume> <pages> pp. 1551-1587 </pages>
Reference-contexts: Using a nearest-neighbor update rule, the models can be modified so that they optimally cover the view space in terms of RMS error <ref> [4] </ref>. The refinement stage functions by first classifying new images as nearest to one of the existing models, and then modifying the pixel statistics for that model.
Reference: [5] <author> Mase, K., </author> <title> (1991) "Recognition of Facial Expression from Optical Flow", </title> <journal> ICICE Transactions, </journal> <volume> Vol. E 74, No. 10, </volume> <pages> pp. 3474-3483 </pages>
Reference-contexts: Currently, however, this information is largely unavailable to computing machines. An important new application of machine vision, therefore, is to extend the interface between man and machine, allowing the machine to directly perceive what its user is doing <ref> [5, 10, 12, 3] </ref>. This paper reports a system that can recover such information in real time, so that it can help mediate human-machine interaction.
Reference: [6] <author> Negroponte, </author> <title> N.,(1980) "The Architecture Machine," </title> <publisher> M.I.T. Press, </publisher> <address> Cambridge. </address>
Reference-contexts: 1 Introduction The location and orientation of head, hand, and eyes is a critical element of all human dialog. The ability to follow objects moving through space and recognize particular motions as meaningful gestures is therefore essential if computer systems are to interact naturally with human users <ref> [2, 6] </ref>. Currently, however, this information is largely unavailable to computing machines. An important new application of machine vision, therefore, is to extend the interface between man and machine, allowing the machine to directly perceive what its user is doing [5, 10, 12, 3].
Reference: [7] <author> Poggio, T., and Edelman, S., </author> <title> (1990) "A Network that Learns to Recognize Three Dimensional Objects," </title> <journal> Nature, </journal> <volume> Vol. 343, No. 6255, </volume> <pages> pp. 263-266 </pages>
Reference-contexts: We have therefore adopted a representation based on interpolation of appearance from a relatively small number of views. This is similar to the ideas of Ullman [11] and Poggio <ref> [7] </ref> for representing 3-D objects by interpolating between a small set of 2-D views.
Reference: [8] <author> Sakoe, H., and Chiba, S., </author> <title> (1980) "Dynamic Programming optimization for spoken word recognition", </title> <journal> IEEE Trans. Acoust., Speech, Signal Processing, </journal> <volume> Vol. 26, </volume> <pages> pp. 623-625 </pages>
Reference-contexts: This method was developed to solve the time alignment problem in the speech and signal processing literature <ref> [8] </ref>, and is easily applied to our task.
Reference: [9] <author> Sperling, G., Landy, M. S., Cohen, Y. and Pavel, M. </author> <year> (1985). </year> <title> "Intelligible encoding of ASL image sequences at extremely low information rates," Computer Vision, </title> <journal> Graphics, and Image Processing, </journal> <volume> Vol. 31, </volume> <pages> pp. 335-391. </pages>
Reference-contexts: The domain of human gesture recognition seems particularly promising for this approach. It is known, for instance, that the human ability to interpret American Sign Language remains accurate even for 32 x 32 pixel high-contrast images with a frame rate of 16 images per second <ref> [9] </ref>. The ability to "read" sign language from very low-resolution, poor quality imagery indicates that humans do not require precise contours, shading, texture, or 3-D properties. What they require is a coarse 2-D description of hand appearance and an accurate representation of the hand's 2-D trajectory.
Reference: [10] <author> Torige, A., and Kono, T., </author> <title> "Human-Interface by Recognition of Human Gesture with Image Processing Recognition of Gesture to Specify Moving Direction", </title> <booktitle> (1992) IEEE Workshop on Robot and Human Communication, </booktitle> <pages> pp. 105-110 </pages>
Reference-contexts: Currently, however, this information is largely unavailable to computing machines. An important new application of machine vision, therefore, is to extend the interface between man and machine, allowing the machine to directly perceive what its user is doing <ref> [5, 10, 12, 3] </ref>. This paper reports a system that can recover such information in real time, so that it can help mediate human-machine interaction.
Reference: [11] <author> Ullman, S., and Basri, R., </author> <title> (1991)"Recognition by Linear Combinations of Models," </title> <journal> IEEE PAMI, </journal> <volume> Vol. 13, No. 10, </volume> <pages> pp. 992-1007 </pages>
Reference-contexts: We have therefore adopted a representation based on interpolation of appearance from a relatively small number of views. This is similar to the ideas of Ullman <ref> [11] </ref> and Poggio [7] for representing 3-D objects by interpolating between a small set of 2-D views.
Reference: [12] <author> Yamato, J., Ohya, J., Ishii, K., </author> <title> (1992) "Recognizing Human Action in Time-Sequential Images using Hidden Markov Model", </title> <booktitle> IEEE Proc. CVPR-92, </booktitle> <pages> pp. 379-385. 19 </pages>
Reference-contexts: Currently, however, this information is largely unavailable to computing machines. An important new application of machine vision, therefore, is to extend the interface between man and machine, allowing the machine to directly perceive what its user is doing <ref> [5, 10, 12, 3] </ref>. This paper reports a system that can recover such information in real time, so that it can help mediate human-machine interaction.
References-found: 12

