URL: http://www.cs.cmu.edu/afs/cs.cmu.edu/user/tcm/www/tcm_papers/io_pf_osdi96.ps.gz
Refering-URL: http://www.cs.cmu.edu/afs/cs.cmu.edu/user/tcm/www/Papers.html
Root-URL: 
Email: ftcm,demke,okriegg@eecg.toronto.edu  
Title: Automatic Compiler-Inserted I/O Prefetching for Out-of-Core Applications  
Author: Todd C. Mowry, Angela K. Demke and Orran Krieger 
Note: 0 To appear in the Second Symposium on Operating Systems Design and Implementations (OSDI '96), Oc-tober 28-31, 1996,  
Address: Toronto, Ontario, Canada M5S 3G4  Seattle, Washington, USA.  
Affiliation: Department of Electrical and Computer Engineering Department of Computer Science University of Toronto  
Abstract: Current operating systems offer poor performance when a numeric application's working set does not fit in main memory. As a result, programmers who wish to solve "out-of-core" problems efficiently are typically faced with the onerous task of rewriting an application to use explicit I/O operations (e.g., read/write). In this paper, we propose and evaluate a fully-automatic technique which liberates the programmer from this task, provides high performance, and requires only minimal changes to current operating systems. In our scheme, the compiler provides the crucial information on future access patterns without burdening the programmer, the operating system supports non-binding prefetch and release hints for managing I/O, and the operating system cooperates with a run-time layer to accelerate performance by adapting to dynamic behavior and minimizing prefetch overhead. This approach maintains the abstraction of unlimited virtual memory for the programmer, gives the compiler the flexibility to aggressively move prefetches back ahead of references, and gives the operating system the flexibility to arbitrate between the competing resource demands of multiple applications. We have implemented our scheme using the SUIF compiler and the Hurricane operating system. Our experimental results demonstrate that our fully-automatic scheme effectively hides the I/O latency in out-of-core versions of the entire NAS Parallel benchmark suite, thus resulting in speedups of roughly twofold for five of the eight applications, with two applications speeding up by threefold or more. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> M. Arunachalam, A. Choudhary, and B. Rullman. </author> <title> A prefetching prototype for the parallel file system on the Paragon. </title> <booktitle> In Proceedings of the 1995 ACM Sigmetrics Conference on Measurement and Modeling of Computer Systems, </booktitle> <pages> pages 321-323, </pages> <month> May </month> <year> 1995. </year> <note> Extended Abstract. </note>
Reference-contexts: On the OS side, this work includes the automatic detection of file access patterns in the file system <ref> [1, 10, 11, 12, 14, 15, 17] </ref>, as well as the use of access patterns supplied directly by the application using an I/O type of interface [22, 26, 30, 4]. For compilers it involves analysis to move explicit I/O calls back and change them to asynchronous I/O calls instead [25].
Reference: [2] <author> D. Bailey, J. Barton, T. Lasinski, and H. Simon. </author> <title> The NAS Parallel Benchmarks. </title> <type> Technical Report RNR-91-002, </type> <institution> NASA Ames Research Center, </institution> <month> August </month> <year> 1991. </year>
Reference-contexts: Our experimental results demonstrate that our scheme effectively hides the I/O latency in out-of-core versions of the entire NAS Parallel benchmark suite <ref> [2] </ref>, thus resulting in speedups of roughly twofold for the majority of these applications, and over threefold in one case. This paper is organized as follows. We begin in Section 2 by discussing how the compiler and the operating system can cooperate to automatically prefetch disk accesses for out-of-core applications. <p> an un-cached state; and (iii) processor speeds have increased more rapidly than disk speeds, and hence the importance of tolerating I/O latency has increased in modern systems. 3.2 Applications To evaluate the effectiveness of our approach, we measured its impact on the performance of the entire NAS Parallel benchmark suite <ref> [2] </ref>. We chose these applications because they represent a variety of different scientific workloads, their data sets can easily be scaled up to out-of-core sizes, and they have not been written to manage I/O explicitly.
Reference: [3] <author> R. Bordawekar, A. Choudhary, and J. Ramanu-jam. </author> <title> Automatic optimization of communication in out-of-core stencil codes. </title> <booktitle> In Proceedings of the 10th ACM International Conference on Supercomputing, </booktitle> <month> May </month> <year> 1996. </year>
Reference-contexts: Compiling for out-of-core array codes tends to focus on two areas. The first is reordering computation to improve data reuse and reduce the total I/O required <ref> [3] </ref>. The second area is inserting explicit I/O calls into array codes [6, 13, 21, 29]. In general, the compilers are aided by extensions to the source code that indicate particular structures are out-of-core. In addition, some of the work specifically targets I/O performance for parallel applications [3], while we have <p> total I/O required <ref> [3] </ref>. The second area is inserting explicit I/O calls into array codes [6, 13, 21, 29]. In general, the compilers are aided by extensions to the source code that indicate particular structures are out-of-core. In addition, some of the work specifically targets I/O performance for parallel applications [3], while we have achieved impressive speedups for even single-threaded applications. 6 For BUK, it is more realistic to cold-start the application, since it must always read its input data set from disk.
Reference: [4] <author> Pei Cao, E. W. Felten, A. R. Karlin, and K. Li. </author> <title> A study of integrated prefetching and caching strategies. </title> <type> Technical report, </type> <month> November </month> <year> 1995. </year>
Reference-contexts: On the OS side, this work includes the automatic detection of file access patterns in the file system [1, 10, 11, 12, 14, 15, 17], as well as the use of access patterns supplied directly by the application using an I/O type of interface <ref> [22, 26, 30, 4] </ref>. For compilers it involves analysis to move explicit I/O calls back and change them to asynchronous I/O calls instead [25]. While some of the OS policies developed may be useful in our environment, our goal is to avoid the use of explicit I/O entirely.
Reference: [5] <author> P. M. Chen, E. K. Lee, G. A. Gibson, R. H. Katz, and D. A. Patterson. </author> <title> RAID: high-performance, reliable secondary storage. </title> <journal> ACM Computing Surveys, </journal> <volume> 26(2) </volume> <pages> 145-185, </pages> <month> June </month> <year> 1994. </year>
Reference-contexts: Fortunately, we can construct cost-effective, high-bandwidth I/O systems by harnessing the aggregate bandwidth of multiple disks <ref> [5, 16, 28] </ref>.
Reference: [6] <author> T. H. Cormen and A. Colvin. </author> <title> ViC*: A preprocessor for virtual-memory C*. </title> <type> Technical Report PCS-TR94-243, </type> <institution> Dept. of Computer Science, Dart-mouth College, </institution> <month> November </month> <year> 1994. </year>
Reference-contexts: Compiling for out-of-core array codes tends to focus on two areas. The first is reordering computation to improve data reuse and reduce the total I/O required [3]. The second area is inserting explicit I/O calls into array codes <ref> [6, 13, 21, 29] </ref>. In general, the compilers are aided by extensions to the source code that indicate particular structures are out-of-core.
Reference: [7] <author> P. E. Crandall, R. A. Aydt, A. A. Chien, and D. A. Reed. </author> <title> Input/output characteristics of scalable parallel applications. </title> <booktitle> In Proceedings of Supercomputing '95, </booktitle> <month> December </month> <year> 1995. </year>
Reference-contexts: For example, global climate modeling, computational physics and chemistry, and many engineering problems (e.g., aircraft simulation) can easily involve data sets that are too large to fit in main memory <ref> [7, 9, 23] </ref>. For such applications (which are commonly referred to as "out-of-core" applications), main memory simply constitutes an intermediate stage in the memory hierarchy, and the bulk of the data must reside on disk or other secondary storage.
Reference: [8] <author> K. M. Curewitz, P. Krishnan, and J. S. Vitter. </author> <title> Practical prefetching via data compression. </title> <booktitle> In Proceedings of the 1993 ACM-SIGMOD Conference on Management of Data, </booktitle> <pages> pages 257-266, </pages> <month> May </month> <year> 1993. </year>
Reference-contexts: Other work has also been done in the area of prefetching for paged virtual memory systems. As for file systems, some of the work depends on the OS detecting patterns to initiate prefetching <ref> [8, 27] </ref>. These techniques suffer from the fact that some number of faults are required to establish patterns before prefetching can begin, and when the patterns change unnecessary prefetches will occur.
Reference: [9] <author> J. M. del Rosario and A. Choudhary. </author> <title> High performance I/O for massively parallel computers: Problems and prospects. </title> <journal> IEEE Computer, </journal> <volume> 27(3) </volume> <pages> 59-68, </pages> <month> March </month> <year> 1994. </year>
Reference-contexts: For example, global climate modeling, computational physics and chemistry, and many engineering problems (e.g., aircraft simulation) can easily involve data sets that are too large to fit in main memory <ref> [7, 9, 23] </ref>. For such applications (which are commonly referred to as "out-of-core" applications), main memory simply constitutes an intermediate stage in the memory hierarchy, and the bulk of the data must reside on disk or other secondary storage.
Reference: [10] <author> J. Griffioen and R. Appleton. </author> <title> Reducing file system latency using a predictive approach. </title> <booktitle> In Conference Proceedings of the USENIX Summer 1994 Technical Conference, </booktitle> <pages> pages 197-208, </pages> <month> June </month> <year> 1994. </year>
Reference-contexts: On the OS side, this work includes the automatic detection of file access patterns in the file system <ref> [1, 10, 11, 12, 14, 15, 17] </ref>, as well as the use of access patterns supplied directly by the application using an I/O type of interface [22, 26, 30, 4]. For compilers it involves analysis to move explicit I/O calls back and change them to asynchronous I/O calls instead [25].
Reference: [11] <author> A. S. Grimshaw and Edmond C. Loyot, Jr. </author> <title> ELFS: object-oriented extensible file systems. </title> <booktitle> In Proceedings of the First International Conference on Par allel and Distributed Information Systems, </booktitle> <pages> page 177, </pages> <month> December </month> <year> 1991. </year>
Reference-contexts: On the OS side, this work includes the automatic detection of file access patterns in the file system <ref> [1, 10, 11, 12, 14, 15, 17] </ref>, as well as the use of access patterns supplied directly by the application using an I/O type of interface [22, 26, 30, 4]. For compilers it involves analysis to move explicit I/O calls back and change them to asynchronous I/O calls instead [25].
Reference: [12] <author> J. Huber, C. L. Elford, D. A. Reed, A. A. Chien, and D. S. Blumenthal. </author> <title> PPFS: A high performance portable parallel file system. </title> <booktitle> In Proceedings of the 9th ACM International Conference on Supercomputing, </booktitle> <pages> pages 385-394, </pages> <address> Barcelona, </address> <month> July </month> <year> 1995. </year>
Reference-contexts: On the OS side, this work includes the automatic detection of file access patterns in the file system <ref> [1, 10, 11, 12, 14, 15, 17] </ref>, as well as the use of access patterns supplied directly by the application using an I/O type of interface [22, 26, 30, 4]. For compilers it involves analysis to move explicit I/O calls back and change them to asynchronous I/O calls instead [25].
Reference: [13] <author> K. Kennedy, C. Koelbel, and M. Paleczny. </author> <title> Scalable I/O for out-of-core structures. </title> <type> Technical Report CRPC-TR93357-S, </type> <institution> Center for Research on Parallel Computation, Rice University, </institution> <month> November </month> <year> 1993. </year> <note> Updated August, </note> <year> 1994. </year>
Reference-contexts: Compiling for out-of-core array codes tends to focus on two areas. The first is reordering computation to improve data reuse and reduce the total I/O required [3]. The second area is inserting explicit I/O calls into array codes <ref> [6, 13, 21, 29] </ref>. In general, the compilers are aided by extensions to the source code that indicate particular structures are out-of-core.
Reference: [14] <author> D. Kotz and C. Schlatter Ellis. </author> <title> Practical prefetch-ing techniques for multiprocessor file systems. </title> <journal> Journal of Distributed and Parallel Databases, </journal> <volume> 1(1) </volume> <pages> 33-51, </pages> <month> January </month> <year> 1993. </year>
Reference-contexts: On the OS side, this work includes the automatic detection of file access patterns in the file system <ref> [1, 10, 11, 12, 14, 15, 17] </ref>, as well as the use of access patterns supplied directly by the application using an I/O type of interface [22, 26, 30, 4]. For compilers it involves analysis to move explicit I/O calls back and change them to asynchronous I/O calls instead [25].
Reference: [15] <author> David Kotz and Carla Schlatter Ellis. </author> <title> Prefetching in file systems for MIMD multiprocessors. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 1(2) </volume> <pages> 218-230, </pages> <month> April </month> <year> 1990. </year>
Reference-contexts: On the OS side, this work includes the automatic detection of file access patterns in the file system <ref> [1, 10, 11, 12, 14, 15, 17] </ref>, as well as the use of access patterns supplied directly by the application using an I/O type of interface [22, 26, 30, 4]. For compilers it involves analysis to move explicit I/O calls back and change them to asynchronous I/O calls instead [25].
Reference: [16] <author> O. Krieger and M. Stumm. </author> <title> HFS: A performance-oriented flexible file system based on building-block compositions. </title> <booktitle> In Fourth Workshop on Input/Output in Parallel and Distributed Systems, </booktitle> <pages> pages 95-108, </pages> <address> Philadelphia, </address> <month> May </month> <year> 1996. </year>
Reference-contexts: Fortunately, we can construct cost-effective, high-bandwidth I/O systems by harnessing the aggregate bandwidth of multiple disks <ref> [5, 16, 28] </ref>. <p> In this way, at most one system call is required for a block prefetch. 3 Experimental Framework We now describe our experimental platform, and the applications which we study in our experiments. 3.1 Experimental Platform The experimental platform used to evaluate our scheme is the Hurricane file system <ref> [16] </ref> and Hurri Table 1: Experimental platform characteristics. <p> The basic characteristics of our experimental platform (with the instrumentation disabled) are shown in Table 1, and more detailed descriptions of the platform can be found in earlier publications <ref> [16, 33, 34] </ref>. We believe that our experimental results are conservative for the following reasons: (i) instrumentation is enabled for all the experiments, and Table 2: Description of applications.
Reference: [17] <author> T. M. Kroeger and D. D. E. </author> <title> Long. Predicting file system actions from prior events. </title> <booktitle> In Proceedings of the USENIX 1996 Annual Technical Conference, </booktitle> <pages> pages 319-328, </pages> <month> January </month> <year> 1996. </year>
Reference-contexts: On the OS side, this work includes the automatic detection of file access patterns in the file system <ref> [1, 10, 11, 12, 14, 15, 17] </ref>, as well as the use of access patterns supplied directly by the application using an I/O type of interface [22, 26, 30, 4]. For compilers it involves analysis to move explicit I/O calls back and change them to asynchronous I/O calls instead [25].
Reference: [18] <author> M. Malkawi and J. Patel. </author> <title> Compiler directed management policy for numerical programs. </title> <booktitle> In Proceedings of the Tenth ACM Symposium on Operating Systems Principles, </booktitle> <pages> pages 97-106, </pages> <month> December </month> <year> 1985. </year>
Reference-contexts: These techniques suffer from the fact that some number of faults are required to establish patterns before prefetching can begin, and when the patterns change unnecessary prefetches will occur. Using application-specific knowledge to assist memory management policies was studied by Malkawi and Patel <ref> [18] </ref>, however they only considered retaining needed pages in memory and did not consider prefetching. The most relevant work to our study was conducted nearly twenty years ago by Trivedi [32], who looked at the use application access patterns extracted by a compiler to implement "prepag-ing".
Reference: [19] <author> T. C. Mowry. </author> <title> Tolerating Latency Through Software-Controlled Data Prefetching. </title> <type> PhD thesis, </type> <institution> Stanford University, </institution> <month> March </month> <year> 1994. </year>
Reference-contexts: There are two reasons why existing read/write I/O interfaces are unacceptable for our purposes. First, for the compiler to successfully move prefetches back far enough to hide the large latency of I/O, it is essential that prefetches be non-binding <ref> [19] </ref>. The non-binding property means that when a given reference is prefetched, the data value seen by that reference is bound at reference time; in contrast, with a binding prefetch, the value is bound at prefetch time. <p> "MADV DONTNEED" hints to the madvise () interface can potentially be used to implement prefetch and release in UNIX.) 2.2.2 Minimizing Prefetch Overhead Earlier studies on compiler-based prefetching to hide cache-to-memory latency have demonstrated the importance of avoiding the overhead of unnecessarily prefetching data that already resides in the cache <ref> [19, 20] </ref>. To address this problem, compiler algorithms have been developed for inserting prefetches only for those references that are likely to suffer misses. <p> introduced the three layers of our system|the compiler, the OS, and the run-time layer|we now discuss each layer in more detail. 2.3 Compiler Support The bulk of our compiler algorithm is a straightforward extension of an algorithm that was developed earlier for prefetching cache-to-memory misses in dense-matrix and sparse-matrix codes <ref> [19, 20] </ref>. Roughly speaking, we changed the input parameters that describe the cache size, line size, and miss latency to correspond to main memory size, the page size, and the page fault latency, respectively. <p> Since space limitations prevent us from describing the compiler algorithm in detail, we focus mainly on the major changes to the original algorithm <ref> [19] </ref>. Two of our modifications to support I/O prefetching are related to spatial locality|i.e. when strided accesses fall within the same page|in which case page faults only occur on iterations that cross page boundaries.
Reference: [20] <author> T. C. Mowry, M. S. Lam, and A. Gupta. </author> <title> Design and evaluation of a compiler algorithm for prefetching. </title> <booktitle> In Proceedings of the Fifth International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <volume> volume 27, </volume> <pages> pages 62-73, </pages> <month> October </month> <year> 1992. </year>
Reference-contexts: "MADV DONTNEED" hints to the madvise () interface can potentially be used to implement prefetch and release in UNIX.) 2.2.2 Minimizing Prefetch Overhead Earlier studies on compiler-based prefetching to hide cache-to-memory latency have demonstrated the importance of avoiding the overhead of unnecessarily prefetching data that already resides in the cache <ref> [19, 20] </ref>. To address this problem, compiler algorithms have been developed for inserting prefetches only for those references that are likely to suffer misses. <p> introduced the three layers of our system|the compiler, the OS, and the run-time layer|we now discuss each layer in more detail. 2.3 Compiler Support The bulk of our compiler algorithm is a straightforward extension of an algorithm that was developed earlier for prefetching cache-to-memory misses in dense-matrix and sparse-matrix codes <ref> [19, 20] </ref>. Roughly speaking, we changed the input parameters that describe the cache size, line size, and miss latency to correspond to main memory size, the page size, and the page fault latency, respectively. <p> Although the interface to the OS is nearly identical, there are some significant differences. First, Trivedi's compiler analysis was restricted to programs in which blocking could be performed whereas previous studies on prefetching for caches have shown that many programs which can be prefetched cannot be blocked <ref> [20] </ref>. Thus, our approach is much more widely applicable. Second, improvements in compiler analysis enable us to be much more aggressive, allowing the prefetching of indirect references and other interesting structures.
Reference: [21] <author> M. Paleczny, K. Kennedy, and C. Koelbel. </author> <title> Compiler support for out-of-core arrays on data parallel machines. </title> <booktitle> In Proceedings of the Fifth Symposium on the Frontiers of Massively Parallel Computation, </booktitle> <pages> pages 110-118, </pages> <address> McLean, VA, </address> <month> February </month> <year> 1995. </year>
Reference-contexts: Compiling for out-of-core array codes tends to focus on two areas. The first is reordering computation to improve data reuse and reduce the total I/O required [3]. The second area is inserting explicit I/O calls into array codes <ref> [6, 13, 21, 29] </ref>. In general, the compilers are aided by extensions to the source code that indicate particular structures are out-of-core.
Reference: [22] <author> R. H. Patterson, G. A. Gibson, E. Ginting, D. Stodolsky, and J. Zelenka. </author> <title> Informed prefetch-ing and caching. </title> <booktitle> In Proceedings of the Fifteenth ACM Symposium on Operating Systems Principles, </booktitle> <pages> pages 79-95, </pages> <month> December </month> <year> 1995. </year>
Reference-contexts: On the OS side, this work includes the automatic detection of file access patterns in the file system [1, 10, 11, 12, 14, 15, 17], as well as the use of access patterns supplied directly by the application using an I/O type of interface <ref> [22, 26, 30, 4] </ref>. For compilers it involves analysis to move explicit I/O calls back and change them to asynchronous I/O calls instead [25]. While some of the OS policies developed may be useful in our environment, our goal is to avoid the use of explicit I/O entirely. <p> While some of the OS policies developed may be useful in our environment, our goal is to avoid the use of explicit I/O entirely. Of the file system prefetching techniques mentioned above, the work on Transparent Informed Prefetching (TIP) by Patterson et. al <ref> [22] </ref> is most relevant to our work in that hints provided by the application level are used by the operating system to optimize file prefetching and replacement. In fact, the cost model employed by TIP might be very useful for our memory manager.
Reference: [23] <author> J. T. Poole. </author> <title> Preliminary survey of I/O intensive applications. </title> <type> Technical Report CCSF-38, </type> <institution> Scalable I/O Initiative, Caltech Concurrent Supercomputing Facilities, Caltech, </institution> <year> 1994. </year>
Reference-contexts: For example, global climate modeling, computational physics and chemistry, and many engineering problems (e.g., aircraft simulation) can easily involve data sets that are too large to fit in main memory <ref> [7, 9, 23] </ref>. For such applications (which are commonly referred to as "out-of-core" applications), main memory simply constitutes an intermediate stage in the memory hierarchy, and the bulk of the data must reside on disk or other secondary storage.
Reference: [24] <author> A. K. Porterfield. </author> <title> Software Methods for Improvement of Cache Performance on Supercomputer Applications. </title> <type> PhD thesis, </type> <institution> Department of Computer Science, Rice University, </institution> <month> May </month> <year> 1989. </year>
Reference-contexts: Two of our modifications to support I/O prefetching are related to spatial locality|i.e. when strided accesses fall within the same page|in which case page faults only occur on iterations that cross page boundaries. First, we use strip mining <ref> [24] </ref> rather than loop unrolling to isolate these faulting iterations, since replicating a loop body 1000 times or more is clearly infeasible.
Reference: [25] <author> A. L. Narasimha Reddy, P. Banerjee, and D. K. Chen. </author> <title> Compiler support for parallel I/O operations. </title> <booktitle> In ICPP91, </booktitle> <pages> pages II:290-II:291, </pages> <year> 1991. </year>
Reference-contexts: For compilers it involves analysis to move explicit I/O calls back and change them to asynchronous I/O calls instead <ref> [25] </ref>. While some of the OS policies developed may be useful in our environment, our goal is to avoid the use of explicit I/O entirely.
Reference: [26] <author> T.P. Singh and A. Choudhary. ADOPT: </author> <title> A dynamic scheme for optimal prefetching in parallel file systems. </title> <type> Technical report, </type> <institution> NPAC, </institution> <month> June </month> <year> 1994. </year>
Reference-contexts: On the OS side, this work includes the automatic detection of file access patterns in the file system [1, 10, 11, 12, 14, 15, 17], as well as the use of access patterns supplied directly by the application using an I/O type of interface <ref> [22, 26, 30, 4] </ref>. For compilers it involves analysis to move explicit I/O calls back and change them to asynchronous I/O calls instead [25]. While some of the OS policies developed may be useful in our environment, our goal is to avoid the use of explicit I/O entirely.
Reference: [27] <author> I. Song and Y. Cho. </author> <title> Page prefetching based on fault history. </title> <booktitle> In USENIX Mach III symposium proceedings, </booktitle> <pages> pages 203-213, </pages> <month> April </month> <year> 1993. </year>
Reference-contexts: Other work has also been done in the area of prefetching for paged virtual memory systems. As for file systems, some of the work depends on the OS detecting patterns to initiate prefetching <ref> [8, 27] </ref>. These techniques suffer from the fact that some number of faults are required to establish patterns before prefetching can begin, and when the patterns change unnecessary prefetches will occur.
Reference: [28] <author> A. Sweeney, D. Doucette, W. Hu, C. Anderson, M. Nishimoto, , and G. Peck. </author> <title> Scalability in the XFS file system. </title> <booktitle> In USENIX Technical Conference, </booktitle> <pages> pages 1-14. </pages> <publisher> Usenix, </publisher> <month> January </month> <year> 1996. </year>
Reference-contexts: Fortunately, we can construct cost-effective, high-bandwidth I/O systems by harnessing the aggregate bandwidth of multiple disks <ref> [5, 16, 28] </ref>.
Reference: [29] <author> R. Thakur, R. Bordawekar, and A. Choudhary. </author> <title> Compilation of out-of-core data parallel programs for distributed memory machines. </title> <booktitle> In IPPS '94 Workshop on Input/Output in Parallel Computer Systems, </booktitle> <pages> pages 54-72. </pages> <institution> Syracuse University, </institution> <month> April </month> <year> 1994. </year>
Reference-contexts: Compiling for out-of-core array codes tends to focus on two areas. The first is reordering computation to improve data reuse and reduce the total I/O required [3]. The second area is inserting explicit I/O calls into array codes <ref> [6, 13, 21, 29] </ref>. In general, the compilers are aided by extensions to the source code that indicate particular structures are out-of-core.
Reference: [30] <author> R. Thakur, R. Bordawekar, A. Choudhary, R. Pon-nusamy, and T. Singh. </author> <title> PASSION runtime library for parallel I/O. </title> <booktitle> In Proceedings of the Scalable Parallel Libraries Conference, </booktitle> <pages> pages 119-128, </pages> <month> October </month> <year> 1994. </year>
Reference-contexts: On the OS side, this work includes the automatic detection of file access patterns in the file system [1, 10, 11, 12, 14, 15, 17], as well as the use of access patterns supplied directly by the application using an I/O type of interface <ref> [22, 26, 30, 4] </ref>. For compilers it involves analysis to move explicit I/O calls back and change them to asynchronous I/O calls instead [25]. While some of the OS policies developed may be useful in our environment, our goal is to avoid the use of explicit I/O entirely.
Reference: [31] <author> S. W. K. Tjiang and J. L. Hennessy. Sharlit: </author> <title> A tool for building optimizers. </title> <booktitle> In SIGPLAN Conference on Programming Language Design and Implementation, </booktitle> <year> 1992. </year>
Reference-contexts: A brief description of each of the benchmarks and the data set used is given in Table 2. We implemented our prefetching algorithm as a pass in the SUIF (Stanford University Intermediate Format) compiler <ref> [31] </ref>, which we used to convert the original Fortran source code of each application into C code containing prefetch and release calls (as illustrated earlier in Figure 2 (b)).
Reference: [32] <author> K.S. Trivedi. </author> <title> On the paging performance of array algorithms. </title> <journal> IEEE Transactions on Computers, </journal> <volume> C-26(10):938-947, </volume> <month> October </month> <year> 1977. </year>
Reference-contexts: Using application-specific knowledge to assist memory management policies was studied by Malkawi and Patel [18], however they only considered retaining needed pages in memory and did not consider prefetching. The most relevant work to our study was conducted nearly twenty years ago by Trivedi <ref> [32] </ref>, who looked at the use application access patterns extracted by a compiler to implement "prepag-ing". Although the interface to the OS is nearly identical, there are some significant differences.
Reference: [33] <author> R. C. Unrau, O. Krieger, B. Gamsa, and M. Stumm. </author> <title> Hierarchical clustering: A structure for scalable multiprocessor operating system design. </title> <journal> Journal of Supercomputing, </journal> 9(1/2):105-134, 1995. 
Reference-contexts: IPC request: 70 sec In-core fault: 200 sec Out-of-core fault: 800 sec Base prefetch: 60 sec + per out-of-core page: 200 sec + per in-core page: 30 sec + per in-page table page: 10 sec File System Operation Overhead Prefetch (per-page): 70 sec Read/Write (per-page): 70 sec cane operating system <ref> [33] </ref> running on the Hector shared-memory multiprocessor [34]. Hurricane is a hierarchically clustered, micro-kernel based operating system that is mostly POSIX compliant. <p> The basic characteristics of our experimental platform (with the instrumentation disabled) are shown in Table 1, and more detailed descriptions of the platform can be found in earlier publications <ref> [16, 33, 34] </ref>. We believe that our experimental results are conservative for the following reasons: (i) instrumentation is enabled for all the experiments, and Table 2: Description of applications.
Reference: [34] <author> Z. G. Vranesic, M. Stumm, R. White, and D. Lewis. </author> <title> The Hector Multiprocessor. </title> <journal> IEEE Computer, </journal> <volume> 24(1), </volume> <month> January </month> <year> 1991. </year>
Reference-contexts: sec Out-of-core fault: 800 sec Base prefetch: 60 sec + per out-of-core page: 200 sec + per in-core page: 30 sec + per in-page table page: 10 sec File System Operation Overhead Prefetch (per-page): 70 sec Read/Write (per-page): 70 sec cane operating system [33] running on the Hector shared-memory multiprocessor <ref> [34] </ref>. Hurricane is a hierarchically clustered, micro-kernel based operating system that is mostly POSIX compliant. <p> The basic characteristics of our experimental platform (with the instrumentation disabled) are shown in Table 1, and more detailed descriptions of the platform can be found in earlier publications <ref> [16, 33, 34] </ref>. We believe that our experimental results are conservative for the following reasons: (i) instrumentation is enabled for all the experiments, and Table 2: Description of applications.
Reference: [35] <author> D. Womble, D. Greenberg, R. Riesen, and S. Wheat. </author> <title> Out of core, out of mind: Practical parallel I/O. </title> <booktitle> In Proceedings of the Scalable Parallel Libraries Conference, </booktitle> <pages> pages 10-16, </pages> <institution> Mississippi State University, </institution> <month> October </month> <year> 1993. </year>
Reference-contexts: While this approach does yield a logically correct answer, the resulting performance is typically so poor that it is not considered a viable technique for solving out-of-core problems <ref> [35] </ref>. In practice, scientific programmers who wish to solve out-of-core problems typically write a separate version of the program with explicit I/O calls for the sake of achieving reasonable performance. <p> Writing an out-of-core version of a program is a formidable task|it is not simply a matter of inserting a few I/O read or write statements, but often involves significant restructuring of the code, and in some cases can have a negative impact on the numerical stability of the algorithm <ref> [35] </ref>.
References-found: 35

