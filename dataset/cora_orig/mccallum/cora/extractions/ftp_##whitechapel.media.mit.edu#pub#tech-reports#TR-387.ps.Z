URL: ftp://whitechapel.media.mit.edu/pub/tech-reports/TR-387.ps.Z
Refering-URL: http://www-white.media.mit.edu/cgi-bin/tr_pagemaker/
Root-URL: http://www.media.mit.edu
Title: Motion Recognition of Human Actions  
Author: by James William Davis Supervisor: Aaron F. Bobick 
Degree: in partial fulfillment of the requirements for the degree of Master of Science in  Title: Assistant Professor  
Date: July 25, 1996  
Affiliation: Media Arts and Sciences, School of  Media Arts and Sciences  of Computational Vision  
Note: Appearance-Based  Submitted to the Program in  Architecture and Planning on  Thesis  
Abstract: M.I.T. Media Lab Perceptual Computing Group Technical Report No. 387 Abstract A new view-based approach to the representation and recognition of action is presented. The work is motivated by the observation that a human observer can easily and instantly recognize action in extremely low resolution imagery with no strong features or information about the three-dimensional structure of the scene. Our underlying representations for action are view-based descriptions of the coarse image motion. Using these descriptions, we propose an appearance-based recognition strategy embedded within a hypothesize-and-test paradigm. A binary motion region (BMR) image is initially computed to act as an index into the action library. The BMR grossly describes the spatial distribution of motion energy for a given view of a given action. Any stored BMRs that plausibly match the unknown input BMR are then tested for a coarse, categorical agreement with a known motion model of the action. We have developed two motion-based methods for the verification of the hypothesized actions. The first approach collapses the temporal variations of region-based motion parameters into a single, low-order coefficient vector. A statistical acceptance region generated around the coefficients is used for classification into the training instances. In the second approach, a motion history image (MHI) is the basis of the representation. The MHI is a static image where pixel intensity is a function of the recency of motion in a sequence. Recognition is accomplished in a feature-based statistical framework. Results employing multiple cameras show reasonable recognition within a MHI verification method which automatically performs temporal segmentation, is invariant to linear changes in speed, and runs in real-time on a standard platform. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Akita, K. </author> <title> Image sequence analysis of real world human motion. </title> <journal> Pattern Recognition, </journal> <volume> 17, </volume> <year> 1984. </year>
Reference-contexts: Some believe that a 3-D description is necessary and sufficient for understanding action (e.g. [19, 4, 29, 15, 28, 16]), while others choose to analyze the 2-D appearance as a means of interpretation (e.g. <ref> [6, 7, 1, 35] </ref>). We now take a closer look at these approaches. The most common method for attaining the 3-D information in the action is to recover the pose of the object at each time instant using a 3-D model of the object. <p> In contrast to the 3-D reconstruction and recognition approaches, others attempt to use only the 2-D appearance of the action (e.g. <ref> [1, 7, 6, 35] </ref>). View-based representations of 2-D statics are used in a multitude of frameworks, where an action is described by a sequence of 2-D instances/poses of the object. Many methods require a normalized image of the object (usually with no background) for representation. <p> As opposed to using the actual raw grayscale image, Yamato et al. [35] examines body silhouettes, and Akita <ref> [1] </ref> employs body contours/edges. Yamato utilizes low-level silhouettes of human actions in a Hidden Markov Model (HMM) framework, where binary silhouettes of background-subtracted images are vector quantized and used as input to the HMMs. In Akita's work [1], the use of edges and some simple 2-D body configuration knowledge (e.g. the <p> actual raw grayscale image, Yamato et al. [35] examines body silhouettes, and Akita <ref> [1] </ref> employs body contours/edges. Yamato utilizes low-level silhouettes of human actions in a Hidden Markov Model (HMM) framework, where binary silhouettes of background-subtracted images are vector quantized and used as input to the HMMs. In Akita's work [1], the use of edges and some simple 2-D body configuration knowledge (e.g. the arm is a protrusion out from the torso) are used to determine the body parts in a hierarchical manner (first find legs, then head, arms, trunk) based on stability.
Reference: [2] <author> Bergen, J., P. Anadan, K. Hanna, and R. Hingorami. </author> <title> Hierarchical model-based motion estimation. </title> <booktitle> In European Conference on Computer Vision, </booktitle> <pages> pages 237-252, </pages> <year> 1992. </year>
Reference-contexts: An obvious approach for constructing a BMR image is to first compute the optic flow field between each pair of frames using a local, gradient-based technique similar to Lucas 17 Frame 0 13 20 30 40 motion region images starting from Frame 0. and Kanade <ref> [2] </ref> yielding a vector image ~ D (x; y; t) for each sequential pair at time t. <p> The motion parameters would be determined by tracking the patches using a region-based parametric optic flow algorithm. One example of tracked patches is shown in Figure 4-1. Three polygonal patches were created and placed manually but tracked automatically using an affine model of optic flow <ref> [2] </ref>. We have not yet achieved a robust enough model placement and tracking algorithm to test our recognition method using patches. Unlike the face images of [3], full-body action sequences can have quite a variety of image textures, shadows, and occlusions which make motion estimation a non-trivial operation. <p> Unfortunately the dilation and erosion operators used in morphology can be computation-ally expensive, and we wish to keep real-time performance a concern. Another technique, the one used here, is the use of pyramids (as used in motion estimation <ref> [2] </ref> and stereo matching [27]) in a hierarchical differencing technique. Though real-time pyramid hardware solutions exist, we used sub-sampled versions of the original image (not low-pass filtered) to generate an approximation to the pyramid.
Reference: [3] <author> Black, M. and Y. Yacoob. </author> <title> Tracking and recognizing rigid and non-rigid facial motion using local parametric models of image motion. </title> <booktitle> In ICCV, </booktitle> <year> 1995. </year>
Reference-contexts: Hence, motion understanding is really accomplished by recognizing a sequence of static configurations. This understanding generally requires previous recognition and segmentation of the person [26]. We now consider recognition of action within a motion-based framework. 2.2 Motion-based recognition Directional motion recognition <ref> [26, 30, 24, 3, 34, 31, 11] </ref> approaches attempt to characterize the motion itself without reference to the underlying static poses of the body. <p> There is a group of work which focuses on motions associated with facial expressions (e.g. characteristic motion of the mouth, eyes, and eyebrows) using region-based motion properties <ref> [34, 3, 11] </ref>. The goal of this research is to recognize human facial expressions as a dynamic system, where the motion of interest regions (locations known a priori) is relevant. <p> Their approaches characterize the expressions using the underlying motion properties rather than represent the action as a sequence of poses or configurations. For Black and Yacoob <ref> [3] </ref>, and also Yacoob and Davis [34], optical flow measurements are used to help track predefined polygonal patches placed on interest regions (e.g. mouth). The parameterization and location relative to the face of each patch was given a priori. <p> These methods demonstrate the plausibility of using motion as a means of recognizing action. 4.1 Motion Model 1: Region-based motion parameteriza tion Our work in this section seeks to extend the facial-expression recognition work of Black and Yacoob <ref> [3] </ref>. In their work, the temporal trajectories of region-based motion parameters were qualitatively labeled to recognize facially expressive emotions (e.g. anger, happiness). They 23 use only a single motion-tracking model consisting of five polygonal patches corresponding to the eyebrows, eyes, and mouth. <p> Three polygonal patches were created and placed manually but tracked automatically using an affine model of optic flow [2]. We have not yet achieved a robust enough model placement and tracking algorithm to test our recognition method using patches. Unlike the face images of <ref> [3] </ref>, full-body action sequences can have quite a variety of image textures, shadows, and occlusions which make motion estimation a non-trivial operation. Recent work by Ju, Black, and Yacoob [22] further investigate this patch tracking paradigm applied to the human body and have found similar shortcomings.
Reference: [4] <author> Campbell, L. and A. Bobick. </author> <title> Recognition of human body motion using phase space constraints. </title> <booktitle> In ICCV, </booktitle> <year> 1995. </year>
Reference-contexts: Many approaches have been proposed with the presumption that 3-D information would be useful and perhaps even necessary to understand actions (e.g. <ref> [28, 16, 19, 4, 29, 15] </ref>), but the blurred sequence example leads us to believe that much information about the action is present in the underlying motion of the person. When viewing the motion in a blurred sequence, two distinct patterns are apparent. <p> Some believe that a 3-D description is necessary and sufficient for understanding action (e.g. <ref> [19, 4, 29, 15, 28, 16] </ref>), while others choose to analyze the 2-D appearance as a means of interpretation (e.g. [6, 7, 1, 35]). We now take a closer look at these approaches. <p> Acquiring the 3-D information from image sequences is currently a complicated process, many times necessitating human intervention or contrived imaging environments. 2.1.2 Recognition As for action recognition, Campbell and Bobick <ref> [4] </ref> used a commercially available system to obtain 3-D data of human body limb positions. Their system removes redundancies that 12 exist for particular actions and performs recognition using only the information that varies between actions.
Reference: [5] <author> Cedras, C., and Shah, M. </author> <title> Motion-based recognition: A survey. </title> <journal> Image and Vision Computing, </journal> <volume> 13(2) </volume> <pages> 129-155, </pages> <month> March </month> <year> 1995. </year>
Reference-contexts: For an excellent survey on the machine understanding of motion (particularly human motion) see the work of Cedras and Shah <ref> [5] </ref>. A detailed description of methods for extracting motion information (e.g. optical flow, motion correspondence, trajectory parameterization) and a brief discussion on matching is reviewed. Also, recent work in motion recognition in terms of generic motion (trajectory and cyclic), human movements, and specialized motion (lip-reading, gestures) is discussed.
Reference: [6] <author> Cui, Y., D. Swets, and J. Weng. </author> <title> Learning-based hand sign recognition using shoslif-m. </title> <booktitle> In ICCV, </booktitle> <year> 1995. </year>
Reference-contexts: Some believe that a 3-D description is necessary and sufficient for understanding action (e.g. [19, 4, 29, 15, 28, 16]), while others choose to analyze the 2-D appearance as a means of interpretation (e.g. <ref> [6, 7, 1, 35] </ref>). We now take a closer look at these approaches. The most common method for attaining the 3-D information in the action is to recover the pose of the object at each time instant using a 3-D model of the object. <p> In contrast to the 3-D reconstruction and recognition approaches, others attempt to use only the 2-D appearance of the action (e.g. <ref> [1, 7, 6, 35] </ref>). View-based representations of 2-D statics are used in a multitude of frameworks, where an action is described by a sequence of 2-D instances/poses of the object. Many methods require a normalized image of the object (usually with no background) for representation. <p> View-based representations of 2-D statics are used in a multitude of frameworks, where an action is described by a sequence of 2-D instances/poses of the object. Many methods require a normalized image of the object (usually with no background) for representation. For example, Cui et al. <ref> [6] </ref>, Darrell and Pentland [7], and also Wilson and Bobick [33] present results using actions (mostly hand gestures), where the actual grayscale images (with no background) are used in the representation for the action.
Reference: [7] <author> Darrell, T. and A. Pentland. </author> <title> Space-time gestures. </title> <booktitle> In CVPR, </booktitle> <year> 1993. </year>
Reference-contexts: Some believe that a 3-D description is necessary and sufficient for understanding action (e.g. [19, 4, 29, 15, 28, 16]), while others choose to analyze the 2-D appearance as a means of interpretation (e.g. <ref> [6, 7, 1, 35] </ref>). We now take a closer look at these approaches. The most common method for attaining the 3-D information in the action is to recover the pose of the object at each time instant using a 3-D model of the object. <p> In contrast to the 3-D reconstruction and recognition approaches, others attempt to use only the 2-D appearance of the action (e.g. <ref> [1, 7, 6, 35] </ref>). View-based representations of 2-D statics are used in a multitude of frameworks, where an action is described by a sequence of 2-D instances/poses of the object. Many methods require a normalized image of the object (usually with no background) for representation. <p> Many methods require a normalized image of the object (usually with no background) for representation. For example, Cui et al. [6], Darrell and Pentland <ref> [7] </ref>, and also Wilson and Bobick [33] present results using actions (mostly hand gestures), where the actual grayscale images (with no background) are used in the representation for the action.
Reference: [8] <author> Darrell, T., P. Maes, B. Blumberg, and A. Pentland. </author> <title> A novel environment for situated vision and behavior. In IEEE Wkshp. for Visual Behaviors (CVPR-94), </title> <year> 1994. </year>
Reference-contexts: Much focus is currently being placed on the understanding and interpretation of "action" (e.g. sitting, throwing, walking, etc). Understanding action is particularly attractive to those developing wireless interfaces [12, 13, 14] and interactive environments <ref> [8] </ref>. Such systems need to recognize specific actions of the person (e.g. hand gestures, body movements) while not appearing intrusive to the user.
Reference: [9] <author> Edgerton, H. and J. Killian. </author> <title> Moments of vision: the stroboscopic revolution in photography. </title> <publisher> MIT Press, </publisher> <year> 1979. </year>
Reference-contexts: The result is analogous to a picture taken of an object moving faster than the camera's shutter speed. There are also many interesting photographs in which strobes or moving cameras were used to produce very similar "motion-blurred" effects <ref> [9] </ref>. In these images the most recent location of motion in the image is shown more prominently than the remaining motion region, much 29 like a viewing the tail of a comet. This "fading" of intensity gives a rough indication of the moving object's path.
Reference: [10] <author> Eggert, D., K. Bowyer, C. Dyer, H. Christensen, and D. Goldgof. </author> <title> The scale space aspect graph. </title> <journal> IEEE Trans. PAMI, </journal> <volume> 15(11), </volume> <year> 1993. </year>
Reference-contexts: To extend this paradigm to such conditions requires some mechanism to automatically mask away regions of undesired motion. In keeping with the notion of aspects (or aspect graphs) <ref> [23, 21, 10] </ref>, we desire the generation of view-based motion models which can be made to accommodate discrete view regions. In general, the term "aspect recognition" has come to include any recognition scheme that partitions the view sphere into distinct models.
Reference: [11] <author> Essa, I. and A. Pentland. </author> <title> Facial expression recognition using a dynamic model and motion energy. </title> <booktitle> In ICCV, </booktitle> <month> June </month> <year> 1995. </year> <month> 49 </month>
Reference-contexts: Hence, motion understanding is really accomplished by recognizing a sequence of static configurations. This understanding generally requires previous recognition and segmentation of the person [26]. We now consider recognition of action within a motion-based framework. 2.2 Motion-based recognition Directional motion recognition <ref> [26, 30, 24, 3, 34, 31, 11] </ref> approaches attempt to characterize the motion itself without reference to the underlying static poses of the body. <p> There is a group of work which focuses on motions associated with facial expressions (e.g. characteristic motion of the mouth, eyes, and eyebrows) using region-based motion properties <ref> [34, 3, 11] </ref>. The goal of this research is to recognize human facial expressions as a dynamic system, where the motion of interest regions (locations known a priori) is relevant. <p> We extend this expression recognition approach in our work by applying a similar framework to the domain of full-body motion. Optical flow, rather than patches, was used by Essa <ref> [11] </ref> to estimate muscle activation on a detailed, physically-based model of the face. One recognition approach classifies expressions by a similarity measure to the typical patterns of muscle activation. Another recognition method matches motion energy templates derived from the muscle activations. <p> We seek to extend the parameterized approach in our first motion modeling method by developing a framework incorporating multiple models and performing statistical recognition on the motion trajectories. In our second approach, we generate motion-based templates (similar to <ref> [11] </ref>) for representing actions. 16 Chapter 3 Spatial distribution of motion Given a rich vocabulary of motions that are recognizable, an exhaustive matching search is not feasible, especially if real-time performance is desired.
Reference: [12] <author> Freeman, W. and C. Weissman. </author> <title> Television control by hand gestures. </title> <booktitle> In Int'l Workshop on Automatic Face- and Gesture-Recognition, </booktitle> <year> 1995. </year>
Reference-contexts: Introduction Recently in computer vision, a growing interest in video sequences, rather than single images, has emerged. Much focus is currently being placed on the understanding and interpretation of "action" (e.g. sitting, throwing, walking, etc). Understanding action is particularly attractive to those developing wireless interfaces <ref> [12, 13, 14] </ref> and interactive environments [8]. Such systems need to recognize specific actions of the person (e.g. hand gestures, body movements) while not appearing intrusive to the user.
Reference: [13] <author> Freeman, W., and M. Roth. </author> <title> Orientation histogram for hand gesture recognition. </title> <booktitle> In Int'l Workshop on Automatic Face- and Gesture-Recognition, </booktitle> <year> 1995. </year>
Reference-contexts: Introduction Recently in computer vision, a growing interest in video sequences, rather than single images, has emerged. Much focus is currently being placed on the understanding and interpretation of "action" (e.g. sitting, throwing, walking, etc). Understanding action is particularly attractive to those developing wireless interfaces <ref> [12, 13, 14] </ref> and interactive environments [8]. Such systems need to recognize specific actions of the person (e.g. hand gestures, body movements) while not appearing intrusive to the user.
Reference: [14] <author> Fukumoto, M, Mase, K., and Y. Suenaga. </author> <title> Real-time detection of pointing actions for a glove-free interface. </title> <booktitle> In IAPR Workshop on Machine Vision Applications, </booktitle> <month> December </month> <year> 1992. </year>
Reference-contexts: Introduction Recently in computer vision, a growing interest in video sequences, rather than single images, has emerged. Much focus is currently being placed on the understanding and interpretation of "action" (e.g. sitting, throwing, walking, etc). Understanding action is particularly attractive to those developing wireless interfaces <ref> [12, 13, 14] </ref> and interactive environments [8]. Such systems need to recognize specific actions of the person (e.g. hand gestures, body movements) while not appearing intrusive to the user.
Reference: [15] <author> Gavrila, D. and L. Davis. </author> <title> Tracking of humans in actions: a 3-d model-based approach. </title> <booktitle> In ARPA Image Understanding Workshop, </booktitle> <month> Feb </month> <year> 1996. </year>
Reference-contexts: Many approaches have been proposed with the presumption that 3-D information would be useful and perhaps even necessary to understand actions (e.g. <ref> [28, 16, 19, 4, 29, 15] </ref>), but the blurred sequence example leads us to believe that much information about the action is present in the underlying motion of the person. When viewing the motion in a blurred sequence, two distinct patterns are apparent. <p> Some believe that a 3-D description is necessary and sufficient for understanding action (e.g. <ref> [19, 4, 29, 15, 28, 16] </ref>), while others choose to analyze the 2-D appearance as a means of interpretation (e.g. [6, 7, 1, 35]). We now take a closer look at these approaches. <p> Rohr incorporates a 1 DOF pose parameter to aid in the model fitting. All the poses in a walking action are indexed by a single number. Here there is only a small subset of poses which can exist. Gavrila and Davis <ref> [15] </ref> also used a full-body model (22 DOF, tapered super-quadrics) for tracking human motion against a complex background. For simplifying the edge detection in cases of self-occlusion, the user is required to wear a tight-fitting body suit with contrasting limb colors. <p> One advantage of having the recovered model is the ability to estimate and predict the feature locations, for instance edges, in the following frames. Given the past history of the model configurations, prediction is commonly attained using Kalman filtering [29, 28, 16] and velocity constraints <ref> [26, 15] </ref>. Because of the self-occlusions that frequently occur in articulated objects, some employ multiple cameras and restrict the motion to small regions [28, 15] to help with projective model occlusion constraints. <p> Given the past history of the model configurations, prediction is commonly attained using Kalman filtering [29, 28, 16] and velocity constraints [26, 15]. Because of the self-occlusions that frequently occur in articulated objects, some employ multiple cameras and restrict the motion to small regions <ref> [28, 15] </ref> to help with projective model occlusion constraints. A single camera is used in [19, 16, 29], but the actions tracked in these works had little deviation in the depth of motion.
Reference: [16] <author> Goncalves, L., E. DiBernardo, E. Ursella, P. Perona. </author> <title> Monocular tracking of the human arm in 3d. </title> <booktitle> In ICCV, </booktitle> <month> June </month> <year> 1995. </year>
Reference-contexts: Many approaches have been proposed with the presumption that 3-D information would be useful and perhaps even necessary to understand actions (e.g. <ref> [28, 16, 19, 4, 29, 15] </ref>), but the blurred sequence example leads us to believe that much information about the action is present in the underlying motion of the person. When viewing the motion in a blurred sequence, two distinct patterns are apparent. <p> Some believe that a 3-D description is necessary and sufficient for understanding action (e.g. <ref> [19, 4, 29, 15, 28, 16] </ref>), while others choose to analyze the 2-D appearance as a means of interpretation (e.g. [6, 7, 1, 35]). We now take a closer look at these approaches. <p> For example, Rehg and Kanade [28] used a 27 degree-of-freedom (DOF) model of a human hand in their system called "Digiteyes". Local image-based trackers are employed to align the projected model lines to the finger edges against a solid background. The work of Goncalves et al. <ref> [16] </ref> promoted 3-D tracking of the human arm against a uniform background using a two cone arm model and a single camera. Though it may be possible to extend their approach to the whole body as claimed, it seems unlikely that it is appropriate for non-constrained human motion with self-occlusion. <p> One advantage of having the recovered model is the ability to estimate and predict the feature locations, for instance edges, in the following frames. Given the past history of the model configurations, prediction is commonly attained using Kalman filtering <ref> [29, 28, 16] </ref> and velocity constraints [26, 15]. Because of the self-occlusions that frequently occur in articulated objects, some employ multiple cameras and restrict the motion to small regions [28, 15] to help with projective model occlusion constraints. <p> Because of the self-occlusions that frequently occur in articulated objects, some employ multiple cameras and restrict the motion to small regions [28, 15] to help with projective model occlusion constraints. A single camera is used in <ref> [19, 16, 29] </ref>, but the actions tracked in these works had little deviation in the depth of motion.
Reference: [17] <author> Grimson, W. E. </author> <title> Object Recognition By Computer: The Role of Geometric Constraints. </title> <publisher> MIT Press, </publisher> <year> 1990. </year>
Reference-contexts: Since these motion patterns are different for various views of actions, a view-based, model-based technique is required. The model, however, is of the body's motion and not of the body's configuration. The basic components of the theory presented are embedded in a hypothesize-and-test paradigm <ref> [17] </ref> to eliminate exhaustive searching for possible matches. First, a simple feature-based characterization of the spatial ("where") motion region is used as the initial filter into 1 In this work, we have begun to embrace aerobics, including many exercise movements in some of the testing procedures. <p> In our second approach, we generate motion-based templates (similar to [11]) for representing actions. 16 Chapter 3 Spatial distribution of motion Given a rich vocabulary of motions that are recognizable, an exhaustive matching search is not feasible, especially if real-time performance is desired. In keeping with the hypothesize-and-test paradigm <ref> [17] </ref>, the first step is to construct an initial index into the known motion library. Calculating the index requires a data-driven, bottom up computation that can suggest a small number of plausible motions to test further.
Reference: [18] <author> Hoffman, D. and B. Flinchbaugh. </author> <title> The interpretation of biological motion. </title> <journal> Biological Cybernetics, </journal> <volume> 45, </volume> <year> 1982. </year>
Reference-contexts: This distinction is important. Although individual frames of moving light displays also contain insufficient information to directly recover pose, they do contain features that allow for the structural recovery of the limbs <ref> [18] </ref> without a priori knowledge of the semantic assignments (e.g. "light 1 is the left hip").
Reference: [19] <author> Hogg, D. </author> <title> Model-based vision: a paradigm to see a walking person. </title> <journal> Image and Vision Computing, </journal> <volume> 1(1), </volume> <year> 1983. </year>
Reference-contexts: Many approaches have been proposed with the presumption that 3-D information would be useful and perhaps even necessary to understand actions (e.g. <ref> [28, 16, 19, 4, 29, 15] </ref>), but the blurred sequence example leads us to believe that much information about the action is present in the underlying motion of the person. When viewing the motion in a blurred sequence, two distinct patterns are apparent. <p> Some believe that a 3-D description is necessary and sufficient for understanding action (e.g. <ref> [19, 4, 29, 15, 28, 16] </ref>), while others choose to analyze the 2-D appearance as a means of interpretation (e.g. [6, 7, 1, 35]). We now take a closer look at these approaches. <p> Though it may be possible to extend their approach to the whole body as claimed, it seems unlikely that it is appropriate for non-constrained human motion with self-occlusion. Hogg <ref> [19] </ref> and Rohr [29] used a full-body cylindrical model for tracking walking humans in natural scenes. Rohr incorporates a 1 DOF pose parameter to aid in the model fitting. All the poses in a walking action are indexed by a single number. <p> Because of the self-occlusions that frequently occur in articulated objects, some employ multiple cameras and restrict the motion to small regions [28, 15] to help with projective model occlusion constraints. A single camera is used in <ref> [19, 16, 29] </ref>, but the actions tracked in these works had little deviation in the depth of motion.
Reference: [20] <author> Hu, M. </author> <title> Visual pattern recognition by moment invariants. </title> <journal> IRE Trans. Information Theory, </journal> <volume> IT-8(2), </volume> <year> 1962. </year>
Reference-contexts: Because the BMRs are blob-like in appearance, it seems reasonable to employ a set of moments-based descriptions for the characterization. The first seven parameters &lt; -1 ; : : : ; -7 &gt; are the Hu moments 18 0 ffi 10 ffi 20 ffi 30 ffi 40 ffi <ref> [20] </ref> (See Appendix A.3) which are known to yield reasonable shape discrimination in a translation, scale, and rotation invariant manner. We augment the feature vector to include terms sensitive to orientation and the correlation between the x and y locations: -8 = [E (xy) E (x)E (y)]=[ x y ]. <p> Instead of using a pooled covariance for computing the Mahalanobis distance, we compute a mean and covariance for the moments of each view of each action. We found that using only seven translation- and scale-invariant moments (x p ; y q with order p + q = 2; 3) <ref> [20] </ref> (See Appendix A.4) offered reasonable discrimination. With these seven moments, the action performed can be translated in the image and placed at various depths (scale) from the novel position while retaining the same moment set (assuming no major camera distortions). <p> Lastly, I would like to express my appreciation to my friends and family for all their support. 45 Appendix A Moment generation In this appendix, we define two-dimensional moments that are invariant under translation, scale, and orientation. The more complete derivation can be found in <ref> [20] </ref>. A.1 General moments The two-dimensional (p + q)th order moments of a density distribution function (x; y) (e.g. image intensity) are defined in terms of Riemann integrals as m pq = 1 1 x p y q (x; y)dxdy ; (A:1) for p; q = 0; 1; 2; .
Reference: [21] <author> Ikeuchi, K. and K. S. Hong. </author> <title> Determining linear shape change: Toward automatic generation of object recognition programs. CVGIP, Image Understanding, </title> <type> 53(2), </type> <year> 1991. </year>
Reference-contexts: To extend this paradigm to such conditions requires some mechanism to automatically mask away regions of undesired motion. In keeping with the notion of aspects (or aspect graphs) <ref> [23, 21, 10] </ref>, we desire the generation of view-based motion models which can be made to accommodate discrete view regions. In general, the term "aspect recognition" has come to include any recognition scheme that partitions the view sphere into distinct models.
Reference: [22] <author> Ju, S., Black, M., and Y. Yacoob. </author> <title> Cardboard people: a parameterized model of articulated image motion. </title> <booktitle> In Submitted to the Second International Conference on Automatic Face and Gesture Recognition, </booktitle> <year> 1996. </year>
Reference-contexts: The temporal trajectories of the motion parameters were qualitatively described according to positive or negative intervals. Then these qualitative labels were used in a rule-based, temporal model for recognition to determine emotions such as anger or happiness. Recently, Ju, Black, and Yacoob <ref> [22] </ref> have extended this work with faces to include tracking the legs of a person walking. As opposed to the simple, independent patches used for faces, an articulated three-patch model was needed for tracking the legs. <p> Unlike the face images of [3], full-body action sequences can have quite a variety of image textures, shadows, and occlusions which make motion estimation a non-trivial operation. Recent work by Ju, Black, and Yacoob <ref> [22] </ref> further investigate this patch tracking paradigm applied to the human body and have found similar shortcomings. To decouple the nature of the representation from our current ability to do patch tracking, we employ a simplified patch model, which uses manually placed and tracked sticks (See Figure 4-2).
Reference: [23] <author> Koenderink, and A. van Doorn. </author> <title> The internal representation of solid shape with respect to vision. </title> <journal> Biological Cybernetics, </journal> <volume> 32, </volume> <year> 1979. </year> <month> 50 </month>
Reference-contexts: To extend this paradigm to such conditions requires some mechanism to automatically mask away regions of undesired motion. In keeping with the notion of aspects (or aspect graphs) <ref> [23, 21, 10] </ref>, we desire the generation of view-based motion models which can be made to accommodate discrete view regions. In general, the term "aspect recognition" has come to include any recognition scheme that partitions the view sphere into distinct models.
Reference: [24] <author> Little, J., and J. Boyd. </author> <title> Describing motion for recognition. </title> <booktitle> In International Symposium on Computer Vision, </booktitle> <pages> pages 235-240, </pages> <month> November </month> <year> 1995. </year>
Reference-contexts: Hence, motion understanding is really accomplished by recognizing a sequence of static configurations. This understanding generally requires previous recognition and segmentation of the person [26]. We now consider recognition of action within a motion-based framework. 2.2 Motion-based recognition Directional motion recognition <ref> [26, 30, 24, 3, 34, 31, 11] </ref> approaches attempt to characterize the motion itself without reference to the underlying static poses of the body. <p> Of the "blob-analysis" approaches, the work of Polana and Nelson [26], Shavit and Jepson [30], and also Little and Boyd <ref> [24] </ref> are most applicable. Polana and Nelson use repetitive motion as a strong cue to recognize cyclic walking motions. They track and recognize people walking in outdoor scenes by gathering a feature vector, over the entire body, of low-level motion characteristics (optical-flow magnitudes) and periodicity measurements.
Reference: [25] <author> McCloud, S. </author> <title> Understanding Comics: The invisible art. </title> <publisher> Kitchen Sink Press, </publisher> <year> 1993. </year>
Reference-contexts: This single image appears to contain the necessary information for determining how a person has moved during the action. Artists have explored this idea of depicting motion using a static image for quite some time <ref> [25] </ref>. A technique frequently used by artists for representing motion in a single frame is streaking, where the motion region in the image is blurred. The result is analogous to a picture taken of an object moving faster than the camera's shutter speed.
Reference: [26] <author> Polana, R. and R. Nelson. </author> <title> Low level recognition of human motion. </title> <booktitle> In IEEE Workshop on Non-rigid and Articulated Motion, </booktitle> <year> 1994. </year>
Reference-contexts: One advantage of having the recovered model is the ability to estimate and predict the feature locations, for instance edges, in the following frames. Given the past history of the model configurations, prediction is commonly attained using Kalman filtering [29, 28, 16] and velocity constraints <ref> [26, 15] </ref>. Because of the self-occlusions that frequently occur in articulated objects, some employ multiple cameras and restrict the motion to small regions [28, 15] to help with projective model occlusion constraints. <p> Hence, motion understanding is really accomplished by recognizing a sequence of static configurations. This understanding generally requires previous recognition and segmentation of the person <ref> [26] </ref>. We now consider recognition of action within a motion-based framework. 2.2 Motion-based recognition Directional motion recognition [26, 30, 24, 3, 34, 31, 11] approaches attempt to characterize the motion itself without reference to the underlying static poses of the body. <p> Hence, motion understanding is really accomplished by recognizing a sequence of static configurations. This understanding generally requires previous recognition and segmentation of the person [26]. We now consider recognition of action within a motion-based framework. 2.2 Motion-based recognition Directional motion recognition <ref> [26, 30, 24, 3, 34, 31, 11] </ref> approaches attempt to characterize the motion itself without reference to the underlying static poses of the body. <p> Two main approaches include the analysis of the body region as a single "blob-like" entity and the tracking of predefined regions (e.g. legs, head, mouth) using motion instead of structural features. Of the "blob-analysis" approaches, the work of Polana and Nelson <ref> [26] </ref>, Shavit and Jepson [30], and also Little and Boyd [24] are most applicable. Polana and Nelson use repetitive motion as a strong cue to recognize cyclic walking motions.
Reference: [27] <author> Quam, </author> <title> L.H. Hierarchical warp stereo. </title> <journal> IUW, </journal> <volume> 84 </volume> <pages> 137-148, 84. </pages>
Reference-contexts: Unfortunately the dilation and erosion operators used in morphology can be computation-ally expensive, and we wish to keep real-time performance a concern. Another technique, the one used here, is the use of pyramids (as used in motion estimation [2] and stereo matching <ref> [27] </ref>) in a hierarchical differencing technique. Though real-time pyramid hardware solutions exist, we used sub-sampled versions of the original image (not low-pass filtered) to generate an approximation to the pyramid. First, the top-level images of the two pyramids (corresponding to two consecutive images) are differenced and thresholded.
Reference: [28] <author> Rehg, J. and T. Kanade. </author> <title> Model-based tracking of self-occluding articulated objects. </title> <booktitle> In ICCV, </booktitle> <year> 1995. </year>
Reference-contexts: Many approaches have been proposed with the presumption that 3-D information would be useful and perhaps even necessary to understand actions (e.g. <ref> [28, 16, 19, 4, 29, 15] </ref>), but the blurred sequence example leads us to believe that much information about the action is present in the underlying motion of the person. When viewing the motion in a blurred sequence, two distinct patterns are apparent. <p> Some believe that a 3-D description is necessary and sufficient for understanding action (e.g. <ref> [19, 4, 29, 15, 28, 16] </ref>), while others choose to analyze the 2-D appearance as a means of interpretation (e.g. [6, 7, 1, 35]). We now take a closer look at these approaches. <p> This generally requires a strong segmentation of foreground/background and also of the individual body parts to aid the model alignment process. It is difficult to imagine such techniques could be extended to the blurred sequence of Figure 1-1. For example, Rehg and Kanade <ref> [28] </ref> used a 27 degree-of-freedom (DOF) model of a human hand in their system called "Digiteyes". Local image-based trackers are employed to align the projected model lines to the finger edges against a solid background. <p> One advantage of having the recovered model is the ability to estimate and predict the feature locations, for instance edges, in the following frames. Given the past history of the model configurations, prediction is commonly attained using Kalman filtering <ref> [29, 28, 16] </ref> and velocity constraints [26, 15]. Because of the self-occlusions that frequently occur in articulated objects, some employ multiple cameras and restrict the motion to small regions [28, 15] to help with projective model occlusion constraints. <p> Given the past history of the model configurations, prediction is commonly attained using Kalman filtering [29, 28, 16] and velocity constraints [26, 15]. Because of the self-occlusions that frequently occur in articulated objects, some employ multiple cameras and restrict the motion to small regions <ref> [28, 15] </ref> to help with projective model occlusion constraints. A single camera is used in [19, 16, 29], but the actions tracked in these works had little deviation in the depth of motion.
Reference: [29] <author> Rohr, K. </author> <title> Towards model-based recognition of human movements in image sequences. CVGIP, Image Understanding, </title> <type> 59(1), </type> <year> 1994. </year>
Reference-contexts: Many approaches have been proposed with the presumption that 3-D information would be useful and perhaps even necessary to understand actions (e.g. <ref> [28, 16, 19, 4, 29, 15] </ref>), but the blurred sequence example leads us to believe that much information about the action is present in the underlying motion of the person. When viewing the motion in a blurred sequence, two distinct patterns are apparent. <p> Some believe that a 3-D description is necessary and sufficient for understanding action (e.g. <ref> [19, 4, 29, 15, 28, 16] </ref>), while others choose to analyze the 2-D appearance as a means of interpretation (e.g. [6, 7, 1, 35]). We now take a closer look at these approaches. <p> Though it may be possible to extend their approach to the whole body as claimed, it seems unlikely that it is appropriate for non-constrained human motion with self-occlusion. Hogg [19] and Rohr <ref> [29] </ref> used a full-body cylindrical model for tracking walking humans in natural scenes. Rohr incorporates a 1 DOF pose parameter to aid in the model fitting. All the poses in a walking action are indexed by a single number. <p> One advantage of having the recovered model is the ability to estimate and predict the feature locations, for instance edges, in the following frames. Given the past history of the model configurations, prediction is commonly attained using Kalman filtering <ref> [29, 28, 16] </ref> and velocity constraints [26, 15]. Because of the self-occlusions that frequently occur in articulated objects, some employ multiple cameras and restrict the motion to small regions [28, 15] to help with projective model occlusion constraints. <p> Because of the self-occlusions that frequently occur in articulated objects, some employ multiple cameras and restrict the motion to small regions [28, 15] to help with projective model occlusion constraints. A single camera is used in <ref> [19, 16, 29] </ref>, but the actions tracked in these works had little deviation in the depth of motion.
Reference: [30] <author> Shavit, E. and A. Jepson. </author> <title> Motion understanding using phase portraits. </title> <booktitle> In IJCAI Workshop: Looking at People, </booktitle> <year> 1995. </year>
Reference-contexts: Hence, motion understanding is really accomplished by recognizing a sequence of static configurations. This understanding generally requires previous recognition and segmentation of the person [26]. We now consider recognition of action within a motion-based framework. 2.2 Motion-based recognition Directional motion recognition <ref> [26, 30, 24, 3, 34, 31, 11] </ref> approaches attempt to characterize the motion itself without reference to the underlying static poses of the body. <p> Two main approaches include the analysis of the body region as a single "blob-like" entity and the tracking of predefined regions (e.g. legs, head, mouth) using motion instead of structural features. Of the "blob-analysis" approaches, the work of Polana and Nelson [26], Shavit and Jepson <ref> [30] </ref>, and also Little and Boyd [24] are most applicable. Polana and Nelson use repetitive motion as a strong cue to recognize cyclic walking motions.
Reference: [31] <author> Siskind, J. M. </author> <title> Grounding language in perception. </title> <booktitle> In SPIE, </booktitle> <month> September </month> <year> 1993. </year>
Reference-contexts: Their system removes redundancies that 12 exist for particular actions and performs recognition using only the information that varies between actions. This method examines the relevant parts of the body, as opposed to the entire body data. Siskind <ref> [31] </ref> similarly used known object configurations. The input to his system consisted of line-drawings of a person, table, and ball. The positions, orientations, shapes, and sizes of the objects are known at all times. <p> Hence, motion understanding is really accomplished by recognizing a sequence of static configurations. This understanding generally requires previous recognition and segmentation of the person [26]. We now consider recognition of action within a motion-based framework. 2.2 Motion-based recognition Directional motion recognition <ref> [26, 30, 24, 3, 34, 31, 11] </ref> approaches attempt to characterize the motion itself without reference to the underlying static poses of the body.
Reference: [32] <author> Ullman, S. </author> <title> Analysis of visual motion by biological and computer systems. </title> <booktitle> Computer, </booktitle> <month> August </month> <year> 1981. </year>
Reference-contexts: Even if a system knew that the images were that of a person, no particular pose could be reasonably assigned due to the lack of features present in the imagery. A more subtle observation is that no good features exist upon which to base a structure-from-motion algorithm <ref> [32] </ref>. This distinction is important. Although individual frames of moving light displays also contain insufficient information to directly recover pose, they do contain features that allow for the structural recovery of the limbs [18] without a priori knowledge of the semantic assignments (e.g. "light 1 is the left hip").
Reference: [33] <author> Wilson, A. and A. Bobick. </author> <title> Learning visual behavior for gesture analysis. </title> <booktitle> In Proc. IEEE Int'l. Symp. on Comp. Vis., Coral Gables, </booktitle> <address> Florida, </address> <month> November </month> <year> 1995. </year>
Reference-contexts: Many methods require a normalized image of the object (usually with no background) for representation. For example, Cui et al. [6], Darrell and Pentland [7], and also Wilson and Bobick <ref> [33] </ref> present results using actions (mostly hand gestures), where the actual grayscale images (with no background) are used in the representation for the action.
Reference: [34] <author> Yacoob, Y. and L. Davis. </author> <title> Computing spatio-temporal representations of human faces. </title> <booktitle> In CVPR, </booktitle> <year> 1994. </year>
Reference-contexts: Hence, motion understanding is really accomplished by recognizing a sequence of static configurations. This understanding generally requires previous recognition and segmentation of the person [26]. We now consider recognition of action within a motion-based framework. 2.2 Motion-based recognition Directional motion recognition <ref> [26, 30, 24, 3, 34, 31, 11] </ref> approaches attempt to characterize the motion itself without reference to the underlying static poses of the body. <p> There is a group of work which focuses on motions associated with facial expressions (e.g. characteristic motion of the mouth, eyes, and eyebrows) using region-based motion properties <ref> [34, 3, 11] </ref>. The goal of this research is to recognize human facial expressions as a dynamic system, where the motion of interest regions (locations known a priori) is relevant. <p> Their approaches characterize the expressions using the underlying motion properties rather than represent the action as a sequence of poses or configurations. For Black and Yacoob [3], and also Yacoob and Davis <ref> [34] </ref>, optical flow measurements are used to help track predefined polygonal patches placed on interest regions (e.g. mouth). The parameterization and location relative to the face of each patch was given a priori. The temporal trajectories of the motion parameters were qualitatively described according to positive or negative intervals.
Reference: [35] <author> Yamato, J., J. Ohya, and K. Ishii. </author> <title> Recognizing human action in time sequential images using hidden markov models. </title> <booktitle> In CVPR, </booktitle> <year> 1992. </year> <month> 51 </month>
Reference-contexts: Some believe that a 3-D description is necessary and sufficient for understanding action (e.g. [19, 4, 29, 15, 28, 16]), while others choose to analyze the 2-D appearance as a means of interpretation (e.g. <ref> [6, 7, 1, 35] </ref>). We now take a closer look at these approaches. The most common method for attaining the 3-D information in the action is to recover the pose of the object at each time instant using a 3-D model of the object. <p> In contrast to the 3-D reconstruction and recognition approaches, others attempt to use only the 2-D appearance of the action (e.g. <ref> [1, 7, 6, 35] </ref>). View-based representations of 2-D statics are used in a multitude of frameworks, where an action is described by a sequence of 2-D instances/poses of the object. Many methods require a normalized image of the object (usually with no background) for representation. <p> As opposed to using the actual raw grayscale image, Yamato et al. <ref> [35] </ref> examines body silhouettes, and Akita [1] employs body contours/edges. Yamato utilizes low-level silhouettes of human actions in a Hidden Markov Model (HMM) framework, where binary silhouettes of background-subtracted images are vector quantized and used as input to the HMMs.
References-found: 35

