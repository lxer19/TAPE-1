URL: http://ftp.cs.yale.edu/pub/hager/papers/icpaa98.ps.gz
Refering-URL: http://ftp.cs.yale.edu/pub/hager/papers/
Root-URL: http://www.cs.yale.edu
Title: IMAGE VARIABILITY DECOMPOSITION FOR RECOVERING POSE AND ILLUMINATION FROM OBJECT TRACKING  
Author: Peter N. Belhumeur Gregory D. Hager 
Address: P.O. Box 208285 New Haven, CT 06520-8285  
Affiliation: Center for Computational Vision and Control Yale University  
Abstract: As an object moves through space, it changes its orientation relative to the viewing camera and relative to light sources which illuminate it. As a consequence, the images of the object produced by the viewing camera may change dramatically. Thus to successfully track a moving object using computer vision, image changes due to varying pose and illumination must be accounted for. In this paper, we develop a method for object tracking that can not only accommodate large changes in object pose and illumination, but can recover these parameters as well. To do this, we separately model the image variation of the object produced by changes in pose and illumination. To track the object through each image in the sequence, we then locally search the models to find the best match, recovering the object's orientation and illumination in the process. Throughout, we present experimental results, achieved in real-time, demonstrating the effectiveness of our methods.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> P. Belhumeur. </author> <title> A Bayesian approach to binocular stereopsis. </title> <journal> Int. Journal of Computer Vision, </journal> <volume> 19, </volume> <year> 1996. </year>
Reference-contexts: The basis vectors spanning the pose subspace were crudely determined by first determining the correspondence using the optical flow techniques of [19]. (The methods for establishing a dense correspondence can be improved by borrowing from work in binocular stereopsis <ref> [15, 8, 1] </ref>.) We then randomly sampled the pose manifold, recreating the images of the object at the new pose. The pictures at the bottom of the figure are artificially generated images of the human face.
Reference: [2] <author> P. Belhumeur and D. Kriegman. </author> <title> What is the set of images of an object under all possible lighting conditions? In IEEE Proc. </title> <booktitle> Conf. Computer Vision and Pattern Recognition, </booktitle> <year> 1996. </year>
Reference-contexts: We call this approach to handling image changes "Image Variability Decomposition." In the case of illumination variation, we exploit the results in <ref> [2] </ref> which show that the set of images of an object seen from a fixed viewpoint, under all possible illumination conditions, is a convex cone in the space of images, and, this cone (termed the "illumination cone") can often be constructed from as few as three images. <p> However, the variability due to illumination may be much larger as the set of possible lighting conditions is infinite dimensional. In particular, in previous work we have shown that the set of npixel images of the object seen under all possible combinations of light sources can have dimension n <ref> [2] </ref>. Thus, the set of images of an object seen from a fixed viewpoint, but under varying illumination, in general, spans the image space. <p> The set of images of an object with arbitrary reflectance functions seen under arbitrary illumination conditions is a convex cone in IR n where n is the number of pixels in each image, see again <ref> [2] </ref>. And, if the object has a convex shape and a Lambertian reflectance function, the set of images under an arbitrary number of point light sources at infinity is a convex polyhedral cone in IR n , which can be determined exactly from as few as three images. <p> If the object's surface has a Lambertian reflectance function and the shape of the object is roughly convex, then we have shown <ref> [2] </ref> that the extreme rays defining the illumination cone can 5 under variable illumination including shadows is a convex cone in IR n (the image space). When the object surface reflectance is Lambertian, the cone can be exactly determined from as few as three images. <p> None of the original images had such shadowing, yet they appear here. be generated from a 3-D linear subspace in the ndimensional image space we call this subspace the "illumination subspace." Furthermore, the illumination subspace can be generated from as few as three images <ref> [25, 6, 2] </ref>. Even if the surface reflectance function is more general than Lambertian and the object is non-convex in shape, this method still seems to work quite well. Figure 2 gives a demonstration of such an approximation for varying illumination of a face.
Reference: [3] <author> M. Black and A. Jepson. Eigentracking: </author> <title> Robust matching and tracking of articulated objects using a view-based representation. </title> <booktitle> In European Conf. on Computer Vision, </booktitle> <pages> pages I:329-342, </pages> <year> 1996. </year>
Reference-contexts: This differs from earlier work on handling pose variation by ourselves and others <ref> [4, 10, 3, 26] </ref> in that image coordinate deformations are not restricted to be affine. For planar or nearly planar targets, affine models work quite well. Yet, most targets are not planar, and, consequently, as the target rotates in space affine models quickly break down. <p> These methods have demonstrated the advantages of using appearance-based descriptions for recognition [17, 28, 22] or for tracking <ref> [21, 3] </ref>. From these methods, it is even possible to recover the pose of the tracked object [21].
Reference: [4] <author> A. Blake, R. Curwen, and A. Zisserman. </author> <title> Affine-invariant contour tracking with automatic control of spatiotemporal scale. </title> <booktitle> In Int. Conf. on Computer Vision, </booktitle> <pages> pages 421-430. </pages> <publisher> IEEE Computer Society Press, </publisher> <year> 1993. </year>
Reference-contexts: This differs from earlier work on handling pose variation by ourselves and others <ref> [4, 10, 3, 26] </ref> in that image coordinate deformations are not restricted to be affine. For planar or nearly planar targets, affine models work quite well. Yet, most targets are not planar, and, consequently, as the target rotates in space affine models quickly break down. <p> These systems overcome the deleterious effects of changes due to illumination by concentrating on a sparse set of edges or corners in the images, i.e. discontinuities in the image intensity <ref> [4, 13, 23, 5, 18, 7] </ref>. Since these methods focus only on image edges or contours, they are impressively effecient and can easily track objects in real-time. Yet, one might argue that models using only local features throw out a great deal of information present in the original image. <p> Yet, one might argue that models using only local features throw out a great deal of information present in the original image. As a possible consequence, feature-based tracking methods require good dynamical models and effective search methods to avoid feature matching ambiguity, see again <ref> [4, 13, 23, 18] </ref>. And, if the objects do not have piecewise constant albedo patterns, the detection and localization of the edge or corner points are sensitive to changes in illumination.
Reference: [5] <author> R. Deriche and O. Faugeras. </author> <title> Tracking line segments. </title> <editor> In O. Faugeras, editor, </editor> <booktitle> Proc. 1st European Conference on Computer Vision, </booktitle> <pages> pages 259-268. </pages> <publisher> Springer-Verlag, </publisher> <year> 1990. </year> <month> 9 </month>
Reference-contexts: These systems overcome the deleterious effects of changes due to illumination by concentrating on a sparse set of edges or corners in the images, i.e. discontinuities in the image intensity <ref> [4, 13, 23, 5, 18, 7] </ref>. Since these methods focus only on image edges or contours, they are impressively effecient and can easily track objects in real-time. Yet, one might argue that models using only local features throw out a great deal of information present in the original image.
Reference: [6] <author> R. Epstein, A. Yuille, and P. N. Belhumeur. </author> <title> Learning and recognizing objects using illumination subspaces. </title> <booktitle> In Proc. of the Int. Workshop on Object Representation for Computer Vision, </booktitle> <year> 1996. </year>
Reference-contexts: None of the original images had such shadowing, yet they appear here. be generated from a 3-D linear subspace in the ndimensional image space we call this subspace the "illumination subspace." Furthermore, the illumination subspace can be generated from as few as three images <ref> [25, 6, 2] </ref>. Even if the surface reflectance function is more general than Lambertian and the object is non-convex in shape, this method still seems to work quite well. Figure 2 gives a demonstration of such an approximation for varying illumination of a face.
Reference: [7] <author> D. Gavrila and L. Davis. </author> <title> Tracking humans in action: A 3D model-based approach. </title> <booktitle> In Proc. Image Understanding Workshop, </booktitle> <pages> pages 737-746, </pages> <year> 1996. </year>
Reference-contexts: These systems overcome the deleterious effects of changes due to illumination by concentrating on a sparse set of edges or corners in the images, i.e. discontinuities in the image intensity <ref> [4, 13, 23, 5, 18, 7] </ref>. Since these methods focus only on image edges or contours, they are impressively effecient and can easily track objects in real-time. Yet, one might argue that models using only local features throw out a great deal of information present in the original image.
Reference: [8] <author> D. Geiger, B. Ladendorf, and A. Yuille. </author> <title> Occlusions in binocular stereo. </title> <booktitle> In Proc. European Conf. on Computer Vision, </booktitle> <address> Santa Margherita Ligure, Italy, </address> <year> 1992. </year>
Reference-contexts: The basis vectors spanning the pose subspace were crudely determined by first determining the correspondence using the optical flow techniques of [19]. (The methods for establishing a dense correspondence can be improved by borrowing from work in binocular stereopsis <ref> [15, 8, 1] </ref>.) We then randomly sampled the pose manifold, recreating the images of the object at the new pose. The pictures at the bottom of the figure are artificially generated images of the human face.
Reference: [9] <author> M. Goresky and R. Macpherson. </author> <title> Stratified Morse Theory. </title> <publisher> Springer-Verlag, </publisher> <address> New York, </address> <year> 1980. </year>
Reference-contexts: This 4-D 1 Due to possible symmetries in the object, the set of coordinates may not be a manifold, but a stratified set <ref> [9] </ref> composed of manifolds and singular sets of dimension six or less. For simplicity we will refer to these sets as manifolds. 3 subspace, which we call the "pose subspace," defines the 8-D linear subspace to which the pose manifold is restricted.
Reference: [10] <author> G. D. Hager and P. N. Belhumeur. </author> <title> Real-time tracking of image regions with changes in geometry and illumination. </title> <booktitle> In Proc. IEEE Conf. Computer Vision and Pattern Recognition, </booktitle> <pages> pages 403-410. </pages> <publisher> IEEE Computer Society Press, </publisher> <year> 1996. </year>
Reference-contexts: This differs from earlier work on handling pose variation by ourselves and others <ref> [4, 10, 3, 26] </ref> in that image coordinate deformations are not restricted to be affine. For planar or nearly planar targets, affine models work quite well. Yet, most targets are not planar, and, consequently, as the target rotates in space affine models quickly break down. <p> follows that the temporal correspondence of the target region across an image sequence can be determined by finding the pose parameters and the illumination parameters that minimize O (a; b; c) = kI (f (b); g (c); t) M (x; y; a)k 2 : (4) As we have recently shown <ref> [10] </ref>, for problems of this form it is possible to efficiently compute both the illumination and pose parameters of a problem of this form. Briefly, the procedure is to linearize the above expression and to solve for the pose and illumination parameters incrementally in each frame. <p> In fact, the results in the next section use the geometry of the first author's face to track the face of the second author. 5 Results A tracking algorithm based on these ideas has been implemented in the XVision tracking environment <ref> [10, 11] </ref>. In this section, we present three runs of the system applied to the problem of human face tracking. We note that the pose and illumination models for the face have been described in Sections 3.1 and 3.2, respectively.
Reference: [11] <author> G. D. Hager and K. Toyama. XVision: </author> <title> A portable substrate for real-time vision applications. </title> <booktitle> Computer Vision and Image Understanding, </booktitle> <year> 1996. </year> <note> In Press. </note>
Reference-contexts: In fact, the results in the next section use the geometry of the first author's face to track the face of the second author. 5 Results A tracking algorithm based on these ideas has been implemented in the XVision tracking environment <ref> [10, 11] </ref>. In this section, we present three runs of the system applied to the problem of human face tracking. We note that the pose and illumination models for the face have been described in Sections 3.1 and 3.2, respectively.
Reference: [12] <author> P. Hallinan. </author> <title> A Deformable Model for Face Recognition Under Arbitrary Lighting Conditions. </title> <type> PhD thesis, </type> <institution> Harvard University, </institution> <year> 1995. </year>
Reference-contexts: Yet, the same object seen from the same pose can appear drastically different depending the directions and strengths of the light sources <ref> [12, 20] </ref>. This fact has profound implications not only for object recognition, but also for object tracking. The image formation process for a rigid object can be viewed as a function of pose and lighting.
Reference: [13] <author> M. Isard and A. Blake. </author> <title> Contour tracking by stochastic propogation of conditional density. </title> <booktitle> In Computer Vision-ECCV '96, </booktitle> <volume> volume 1, </volume> <pages> pages 343-356. </pages> <publisher> Springer Verlag, </publisher> <year> 1996. </year>
Reference-contexts: These systems overcome the deleterious effects of changes due to illumination by concentrating on a sparse set of edges or corners in the images, i.e. discontinuities in the image intensity <ref> [4, 13, 23, 5, 18, 7] </ref>. Since these methods focus only on image edges or contours, they are impressively effecient and can easily track objects in real-time. Yet, one might argue that models using only local features throw out a great deal of information present in the original image. <p> Yet, one might argue that models using only local features throw out a great deal of information present in the original image. As a possible consequence, feature-based tracking methods require good dynamical models and effective search methods to avoid feature matching ambiguity, see again <ref> [4, 13, 23, 18] </ref>. And, if the objects do not have piecewise constant albedo patterns, the detection and localization of the edge or corner points are sensitive to changes in illumination.
Reference: [14] <author> D. W. Jacobs. </author> <title> Space efficient 3D model indexing. </title> <booktitle> In Proc. IEEE Conf. on Comp. Vision and Patt. Recog., </booktitle> <pages> pages 439-444, </pages> <year> 1992. </year>
Reference-contexts: In the case of pose variation, we exploit the results of Ullman and Basri [29], Jacobs <ref> [14] </ref>, and Tomasi and Kanade [27] which show that, under weak perspective, the set of image coordinates of an object lies in low-dimensional linear subspace of the image coordinate space.
Reference: [15] <author> D. Jones and J. Malik. </author> <title> A computational framework for determining stereo correspondence from a set of linear spatial filters. </title> <booktitle> In Proc. European Conf. on Computer Vision, </booktitle> <address> Santa Margherita Ligure, Italy, </address> <year> 1992. </year>
Reference-contexts: The basis vectors spanning the pose subspace were crudely determined by first determining the correspondence using the optical flow techniques of [19]. (The methods for establishing a dense correspondence can be improved by borrowing from work in binocular stereopsis <ref> [15, 8, 1] </ref>.) We then randomly sampled the pose manifold, recreating the images of the object at the new pose. The pictures at the bottom of the figure are artificially generated images of the human face.
Reference: [16] <author> J. Koenderink and A. Van Doorn. </author> <title> Affine structure from motion. </title> <journal> J. Opt. Soc. Am., </journal> <volume> 8(2) </volume> <pages> 337-385, </pages> <year> 1991. </year>
Reference-contexts: From the correspondences of each pixel in the first image with those in the second image, we can determine the pose subspace [27]. Note that determining the pose subspace only fixes the affine structure of the object <ref> [16] </ref>.
Reference: [17] <author> L. Sirovitch and M. Kirby. </author> <title> Low-dimensional procedure for the characterization of human faces. </title> <journal> J. Optical Soc. of America A, </journal> <volume> 2 </volume> <pages> 519-524, </pages> <year> 1987. </year>
Reference-contexts: Only recently have "appearance-based" approaches been developed in an effort to use intensity information to model or learn a representation that captures a large set of the possible images of an object under pose and/or illumination variation <ref> [21, 22, 17, 28] </ref>. These methods have demonstrated the advantages of using appearance-based descriptions for recognition [17, 28, 22] or for tracking [21, 3]. From these methods, it is even possible to recover the pose of the tracked object [21]. <p> These methods have demonstrated the advantages of using appearance-based descriptions for recognition <ref> [17, 28, 22] </ref> or for tracking [21, 3]. From these methods, it is even possible to recover the pose of the tracked object [21].
Reference: [18] <author> D. Lowe. </author> <title> Robust model-based motion tracking through the integration of search and estimation. </title> <journal> Int. Journal of Computer Vision, </journal> <volume> 8(2) </volume> <pages> 113-122, </pages> <year> 1992. </year>
Reference-contexts: These systems overcome the deleterious effects of changes due to illumination by concentrating on a sparse set of edges or corners in the images, i.e. discontinuities in the image intensity <ref> [4, 13, 23, 5, 18, 7] </ref>. Since these methods focus only on image edges or contours, they are impressively effecient and can easily track objects in real-time. Yet, one might argue that models using only local features throw out a great deal of information present in the original image. <p> Yet, one might argue that models using only local features throw out a great deal of information present in the original image. As a possible consequence, feature-based tracking methods require good dynamical models and effective search methods to avoid feature matching ambiguity, see again <ref> [4, 13, 23, 18] </ref>. And, if the objects do not have piecewise constant albedo patterns, the detection and localization of the edge or corner points are sensitive to changes in illumination.
Reference: [19] <author> B. Lucas and T. Kanade. </author> <title> An iterative image registration technique with an application to stereo vision. </title> <booktitle> In Proc. of the 7th International Joint Conference on Artificial Intelligence, </booktitle> <pages> pages 674-679, </pages> <year> 1981. </year>
Reference-contexts: Yet, what is resulting set of image coordinates? to generate the pose subspace in which the pose manifold is embedded. The basis vectors spanning the pose subspace were crudely determined by first determining the correspondence using the optical flow techniques of <ref> [19] </ref>. We then randomly sampled the pose manifold, recreating the images of the object at the new pose. The pictures at the bottom of the figure are artificially generated images of the human face. <p> Here we have used twenty images, of which four are shown, to generate the pose subspace in which the pose manifold is embedded. The basis vectors spanning the pose subspace were crudely determined by first determining the correspondence using the optical flow techniques of <ref> [19] </ref>. (The methods for establishing a dense correspondence can be improved by borrowing from work in binocular stereopsis [15, 8, 1].) We then randomly sampled the pose manifold, recreating the images of the object at the new pose.
Reference: [20] <author> Y. Moses, Y. Adini, and S. Ullman. </author> <title> Face recognition: The problem of compensating for changes in illumination direction. </title> <booktitle> In European Conf. on Computer Vision, </booktitle> <pages> pages 286-296, </pages> <year> 1994. </year>
Reference-contexts: Yet, the same object seen from the same pose can appear drastically different depending the directions and strengths of the light sources <ref> [12, 20] </ref>. This fact has profound implications not only for object recognition, but also for object tracking. The image formation process for a rigid object can be viewed as a function of pose and lighting.
Reference: [21] <author> H. Murase and S. Nayar. </author> <title> Visual learning and recognition of 3-D objects from appearence. </title> <journal> Int. J. Computer Vision, </journal> <pages> 14(5-24), </pages> <year> 1995. </year>
Reference-contexts: Only recently have "appearance-based" approaches been developed in an effort to use intensity information to model or learn a representation that captures a large set of the possible images of an object under pose and/or illumination variation <ref> [21, 22, 17, 28] </ref>. These methods have demonstrated the advantages of using appearance-based descriptions for recognition [17, 28, 22] or for tracking [21, 3]. From these methods, it is even possible to recover the pose of the tracked object [21]. <p> These methods have demonstrated the advantages of using appearance-based descriptions for recognition [17, 28, 22] or for tracking <ref> [21, 3] </ref>. From these methods, it is even possible to recover the pose of the tracked object [21]. <p> These methods have demonstrated the advantages of using appearance-based descriptions for recognition [17, 28, 22] or for tracking [21, 3]. From these methods, it is even possible to recover the pose of the tracked object <ref> [21] </ref>. Still, a drawback of these approaches is that in order to recognize (or track) an object seen from a particular pose and under a particular illumination, the object must have been previously seen under similar conditions. <p> First, we can simply gather a collection of images of the target regions illuminated by point light sources of different directions and use these to define the extreme rays. This method is similar to that of <ref> [21] </ref>, except the illumination cone representation implicitly models multiple light sources since it allows for convex combinations of the extreme rays (images). A drawback of this method (and that of [21]) is that many images are needed to construct the model. <p> This method is similar to that of <ref> [21] </ref>, except the illumination cone representation implicitly models multiple light sources since it allows for convex combinations of the extreme rays (images). A drawback of this method (and that of [21]) is that many images are needed to construct the model. A second method for determining the extreme rays is to gather only a small number of images of the object under varying illumination and use these to generate the complete set of extreme rays. <p> Likewise, the illumination coefficients each correspond to different light source directions, with the magnitudes of the coefficients proportional to the light source strengths. We should add that, while the method in <ref> [21] </ref> could determine pose in the same fashion, an advantage of this method is that for objects with similar geometry, but different albedo patterns, all that is required is that the illumination cone be transformed. There is no need to relearn the pose manifold or illumination subspace.
Reference: [22] <author> T. Poggio and K. Sung. </author> <title> Example-based learning for view-based human face detection. </title> <booktitle> In Proc. Image Understanding Workshop, </booktitle> <pages> pages II:843-850, </pages> <year> 1994. </year>
Reference-contexts: Only recently have "appearance-based" approaches been developed in an effort to use intensity information to model or learn a representation that captures a large set of the possible images of an object under pose and/or illumination variation <ref> [21, 22, 17, 28] </ref>. These methods have demonstrated the advantages of using appearance-based descriptions for recognition [17, 28, 22] or for tracking [21, 3]. From these methods, it is even possible to recover the pose of the tracked object [21]. <p> These methods have demonstrated the advantages of using appearance-based descriptions for recognition <ref> [17, 28, 22] </ref> or for tracking [21, 3]. From these methods, it is even possible to recover the pose of the tracked object [21].
Reference: [23] <author> D. Reynard, A. Wildenberg, A. Blake, and J. Marchant. </author> <title> Learning dynamics of complex motions from image sequences. </title> <booktitle> In European Conf. on Computer Vision, </booktitle> <pages> pages I:357-368, </pages> <year> 1996. </year>
Reference-contexts: These systems overcome the deleterious effects of changes due to illumination by concentrating on a sparse set of edges or corners in the images, i.e. discontinuities in the image intensity <ref> [4, 13, 23, 5, 18, 7] </ref>. Since these methods focus only on image edges or contours, they are impressively effecient and can easily track objects in real-time. Yet, one might argue that models using only local features throw out a great deal of information present in the original image. <p> Yet, one might argue that models using only local features throw out a great deal of information present in the original image. As a possible consequence, feature-based tracking methods require good dynamical models and effective search methods to avoid feature matching ambiguity, see again <ref> [4, 13, 23, 18] </ref>. And, if the objects do not have piecewise constant albedo patterns, the detection and localization of the edge or corner points are sensitive to changes in illumination.
Reference: [24] <author> L. Shapiro, A. Zisserman, and M. Brady. </author> <title> 3D motion recovery via affine epipolar geometry. </title> <journal> Int. J. Computer Vision, </journal> <volume> 16(2) </volume> <pages> 147-182, </pages> <month> October </month> <year> 1995. </year>
Reference-contexts: Thus, to determine the pose subspace, which in turn determines a superset of all possible 2ndimensional image coordinates, we need only x and y coordinates of the n points in two images. The above is simply a restatement of <ref> [29, 27, 24] </ref>; no new result is claimed here. However, we will argue that this approach can be used as the foundation for predicting novel images of an image, not simply the new locations of a sparse set of image feature points.
Reference: [25] <author> A. Shashua. </author> <title> Geometry and Photometry in 3D Visual Recognition. </title> <type> PhD thesis, </type> <institution> MIT, </institution> <year> 1992. </year>
Reference-contexts: None of the original images had such shadowing, yet they appear here. be generated from a 3-D linear subspace in the ndimensional image space we call this subspace the "illumination subspace." Furthermore, the illumination subspace can be generated from as few as three images <ref> [25, 6, 2] </ref>. Even if the surface reflectance function is more general than Lambertian and the object is non-convex in shape, this method still seems to work quite well. Figure 2 gives a demonstration of such an approximation for varying illumination of a face.
Reference: [26] <author> J. Shi and C. Tomasi. </author> <title> Good features to track. </title> <booktitle> In Proc. IEEE Computer Society Conference on Computer Vision and Pattern Recognition, </booktitle> <pages> pages 593-600. </pages> <publisher> IEEE Computer Society Press, </publisher> <year> 1994. </year>
Reference-contexts: This differs from earlier work on handling pose variation by ourselves and others <ref> [4, 10, 3, 26] </ref> in that image coordinate deformations are not restricted to be affine. For planar or nearly planar targets, affine models work quite well. Yet, most targets are not planar, and, consequently, as the target rotates in space affine models quickly break down.
Reference: [27] <author> C. Tomasi and T. Kanade. </author> <title> Shape and motion from image streams under orthography: A factorization method. </title> <journal> Int. J. Computer Vision, </journal> <volume> 9(2) </volume> <pages> 137-154, </pages> <year> 1992. </year>
Reference-contexts: In the case of pose variation, we exploit the results of Ullman and Basri [29], Jacobs [14], and Tomasi and Kanade <ref> [27] </ref> which show that, under weak perspective, the set of image coordinates of an object lies in low-dimensional linear subspace of the image coordinate space. <p> Thus, to determine the pose subspace, which in turn determines a superset of all possible 2ndimensional image coordinates, we need only x and y coordinates of the n points in two images. The above is simply a restatement of <ref> [29, 27, 24] </ref>; no new result is claimed here. However, we will argue that this approach can be used as the foundation for predicting novel images of an image, not simply the new locations of a sparse set of image feature points. <p> From the correspondences of each pixel in the first image with those in the second image, we can determine the pose subspace <ref> [27] </ref>. Note that determining the pose subspace only fixes the affine structure of the object [16]. To determine the 3-D Euclidean structure of the object or, equivalently, to determine the pose manifold, a third image is needed. (This, of course, was implicit in work of [27], which demonstrated how these subspaces <p> can determine the pose subspace <ref> [27] </ref>. Note that determining the pose subspace only fixes the affine structure of the object [16]. To determine the 3-D Euclidean structure of the object or, equivalently, to determine the pose manifold, a third image is needed. (This, of course, was implicit in work of [27], which demonstrated how these subspaces could be computed efficiently and, from them, the 3-D coordinates of the points determined.) Yet, we stress that the goal of Pose Decomposition is not to necessarily determine the precise 3-D structure of objects.
Reference: [28] <author> M. Turk and A. Pentland. </author> <title> Eigenfaces for recognition. </title> <journal> J. of Cognitive Neuroscience, </journal> <volume> 3(1), </volume> <year> 1991. </year>
Reference-contexts: Only recently have "appearance-based" approaches been developed in an effort to use intensity information to model or learn a representation that captures a large set of the possible images of an object under pose and/or illumination variation <ref> [21, 22, 17, 28] </ref>. These methods have demonstrated the advantages of using appearance-based descriptions for recognition [17, 28, 22] or for tracking [21, 3]. From these methods, it is even possible to recover the pose of the tracked object [21]. <p> These methods have demonstrated the advantages of using appearance-based descriptions for recognition <ref> [17, 28, 22] </ref> or for tracking [21, 3]. From these methods, it is even possible to recover the pose of the tracked object [21].
Reference: [29] <author> S. Ullman and R. Basri. </author> <title> Recognition by linear combinations of models. </title> <journal> IEEE Trans. Pattern Analysis and Machine Intelligence, </journal> <volume> 13(10) </volume> <pages> 992-1006, </pages> <year> 1991. </year> <month> 10 </month>
Reference-contexts: In the case of pose variation, we exploit the results of Ullman and Basri <ref> [29] </ref>, Jacobs [14], and Tomasi and Kanade [27] which show that, under weak perspective, the set of image coordinates of an object lies in low-dimensional linear subspace of the image coordinate space. <p> We call this 6-D manifold the "pose manifold." However, this manifold lies within an 8-D linear subspace. As stated in the work of Ullman and Basri <ref> [29] </ref>, because of the symmetry between the x and y image coordinates, both the x and y coordinates lie in the same 4-D linear subspace of an ndimensional coordinate space. <p> Thus, to determine the pose subspace, which in turn determines a superset of all possible 2ndimensional image coordinates, we need only x and y coordinates of the n points in two images. The above is simply a restatement of <ref> [29, 27, 24] </ref>; no new result is claimed here. However, we will argue that this approach can be used as the foundation for predicting novel images of an image, not simply the new locations of a sparse set of image feature points.
References-found: 29

