URL: http://www.eecs.umich.edu/techreports/cse/1995/CSE-TR-273-95.ps.gz
Refering-URL: http://www.eecs.umich.edu/home/techreports/cse95.html
Root-URL: http://www.eecs.umich.edu
Email: -weeteck,gurur,caycock,pmchen-@eecs.umich.edu  
Title: Measuring and Improving Memorys Resistance to Operating System Crashes 1 Measuring and Improving Memorys Resistance
Author: Wee Teck Ng, Gurushankar Rajamani, Christopher M. Aycock, Peter M. Chen 
Address: Michigan  
Affiliation: Computer Science and Engineering Division Department of Electrical Engineering and Computer Science University of  
Abstract: Memory is commonly viewed as an unreliable place to store permanent data because it is perceived to be vulnerable to system crashes. 1 Yet despite all the negative implications of memorys unreliability, no data exists that quantifies how vulnerable memory actually is to system crashes. The goals of this paper are to quantify the vulnerability of memory to operating system crashes and to propose a method for protecting memory from these crashes. We use software fault injection to induce a wide variety of operating system crashes in DEC Alpha workstations running Digital Unix, ranging from bit errors in the kernel stack to deleting branch instructions to C-level allocation management errors. We show that memory is remarkably resistant to operating system crashes. Out of the 996 crashes we observed, only 17 corrupted file cache data. Excluding direct corruption from copy overruns, only 2 out of 820 corrupted file cache data. This data contradicts the common assumption that operating system crashes often corrupt files in memory. For users who need even greater protection against operating system crashes, we propose a simple, low-overhead software scheme that controls access to file cache buffers using virtual memory protection and code patching. 
Abstract-found: 1
Intro-found: 1
Reference: [Abbott94] <author> M. Abbott, D. Har, L. Herger, M. Kauffmann, K. Mak, J. Murdock, C. Schulz, T. B. Smith, B. Tremaine, D. Yeh, and L. Wong. </author> <title> Durable Memory RS/6000 System Design. </title> <booktitle> In Proceedings of the 1994 International Symposium on Fault-Tolerant Computing, </booktitle> <pages> pages 414423, </pages> <year> 1994. </year>
Reference-contexts: The Durable Memory RS/6000 uses batteries, replicated processors, memory ECC, and alternate paths to tolerate a wide variety of hardware failures <ref> [Abbott94] </ref>. Finally, several papers have examined the performance advantages and management of reliable memory [Copeland89, Baker92a, Biswas93, Akyurek95]. 3 Experimental Environment and Mechanisms Our experiments were run on DEC Alpha 3000/600 workstations (Table 1) running the Digital Unix V3.0 operating system. <p> Hardware faults are usually specific and relatively easy to model [Lee93b], and various techniques such as ECC and redundancy have been successfully used to protect against these errors <ref> [Abbott94, Banatre93] </ref>. We focus primarily on software faults because: Kernel programming errors are the errors most likely to circumvent hardware error correction schemes and corrupt memory. Software errors (like most design aws) are difficult to model and understand.
Reference: [Akyurek95] <author> Sedat Akyurek and Kenneth Salem. </author> <title> Management of partially safe buffers. </title> <journal> IEEE Transactions on Computers, </journal> <volume> 44(3):394407, </volume> <month> March </month> <year> 1995. </year>
Reference-contexts: The Durable Memory RS/6000 uses batteries, replicated processors, memory ECC, and alternate paths to tolerate a wide variety of hardware failures [Abbott94]. Finally, several papers have examined the performance advantages and management of reliable memory <ref> [Copeland89, Baker92a, Biswas93, Akyurek95] </ref>. 3 Experimental Environment and Mechanisms Our experiments were run on DEC Alpha 3000/600 workstations (Table 1) running the Digital Unix V3.0 operating system. Digital Unix is a monolithic kernel derived from Mach 2.5 and OSF/1.
Reference: [Baker91] <author> Mary G. Baker, John H. Hartman, Michael D. Kupfer, Ken W. Shirriff, and John K. Ousterhout. </author> <title> Measurements of a Distributed File System. </title> <booktitle> In Proceedings of the 13th ACM Symposium on Operating Systems Principles, </booktitle> <pages> pages 198212, </pages> <month> October </month> <year> 1991. </year>
Reference-contexts: In addition, 1/3 to 2/3 of newly written data lives longer than 30 seconds <ref> [Baker91, Hartman93] </ref>, so a large 1. It is also vulnerable to power loss, but this paper will not discuss this aspect of reliability. It is possible to make memory non-volatile by using an uninterruptible power supply or by using Flash RAM.
Reference: [Baker92a] <author> Mary Baker, Satoshi Asami, Etienne Deprit, John Ousterhout, and Margo Seltzer. </author> <title> Non-Volatile Memory for Fast Reliable File Systems. </title> <booktitle> In Fifth International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS-V), </booktitle> <pages> pages 1022, </pages> <month> October </month> <year> 1992. </year>
Reference-contexts: It is hence relatively easy for many simple software errors (such as de-referencing an uninitialized pointer) to accidentally corrupt the contents of memory <ref> [Baker92a] </ref>. The assumption that memory is unreliable hurts system performance, reliability, simplicity, semantics, and cost. Because memory is unreliable, systems that require high reliability, such as databases, write new data through to disk, but this slows performance to that of disks. <p> The Durable Memory RS/6000 uses batteries, replicated processors, memory ECC, and alternate paths to tolerate a wide variety of hardware failures [Abbott94]. Finally, several papers have examined the performance advantages and management of reliable memory <ref> [Copeland89, Baker92a, Biswas93, Akyurek95] </ref>. 3 Experimental Environment and Mechanisms Our experiments were run on DEC Alpha 3000/600 workstations (Table 1) running the Digital Unix V3.0 operating system. Digital Unix is a monolithic kernel derived from Mach 2.5 and OSF/1.
Reference: [Baker92b] <author> Mary Baker and Mark Sullivan. </author> <title> The Recovery Box: Using Fast Recovery to Provide High Availability in the UNIX Environment. </title> <booktitle> In Proceedings USENIX Summer Conference, </booktitle> <month> June </month> <year> 1992. </year>
Reference-contexts: The Harp file system protects a log of recent modifications by replicating it in volatile, battery-backed memory across several server nodes [Liskov91]. The Recovery Box keeps special system state in a region of memory accessed only through a rigid interface <ref> [Baker92b] </ref>. No attempt is made to prevent other functions from accidentally modifying the recovery box, although the system detects corruption by maintaining checksums. <p> We use two strategies to detect corruption of the file cache: checksums and a synthetic workload called memTest. 3.1 Checksum Detection of Corruption Our primary method to detect corruption is to maintain a checksum of each memory block in the file cache <ref> [Baker92b] </ref>. We update the checksum in all functions that write the file cache; unintentional changes to file cache buffers will result in an inconsistent checksum. <p> We believe memory-resident files can be protected from crashes in much the same way by strictly controlling the way memory can be written <ref> [Baker92b] </ref>. To accomplish this protection, we propose adding a memory device driver to check for errors and prevent misbehaving software from corrupting memory.
Reference: [Banatre86] <author> Jean-Pierre Banatre, Michel Banatre, Guy LaPalme, and Florimond Ployette. </author> <title> The design and building of Enchere, a distributed electronic marketing system. </title> <journal> Communications of the ACM, </journal> <volume> 29(1):1929, </volume> <month> January </month> <year> 1986. </year>
Reference-contexts: Banatre, et. al. implement stable transactional memory, which protects memory contents with dual memory banks, a special memory controller, and explicit calls to allow write access to specified memory blocks <ref> [Banatre86, Banatre88, Banatre91] </ref>. Our work seeks to make main memory reliable without needing special-purpose hardware or dual memory banks.
Reference: [Banatre88] <author> Michel Banatre, Gilles Muller, and Jean-Pierre Banatre. </author> <title> Ensuring data security and integrity with a fast stable storage. </title> <booktitle> In Proceedings of the 1988 International Conference on Data Engineering, </booktitle> <pages> pages 285293, </pages> <month> February </month> <year> 1988. </year>
Reference-contexts: Banatre, et. al. implement stable transactional memory, which protects memory contents with dual memory banks, a special memory controller, and explicit calls to allow write access to specified memory blocks <ref> [Banatre86, Banatre88, Banatre91] </ref>. Our work seeks to make main memory reliable without needing special-purpose hardware or dual memory banks.

Reference: [Banatre93] <author> Michel Banatre, Pack Heng, Gilles Muller, Nadine Peyrouze, and Bruno Rochat. </author> <title> An experience in the design of a reliable object based system. </title> <booktitle> In Proceedings of the 1993 International Conference on Parallel and Distributed Information Systems, </booktitle> <pages> pages 187190, </pages> <month> January </month> <year> 1993. </year>
Reference-contexts: Hardware faults are usually specific and relatively easy to model [Lee93b], and various techniques such as ECC and redundancy have been successfully used to protect against these errors <ref> [Abbott94, Banatre93] </ref>. We focus primarily on software faults because: Kernel programming errors are the errors most likely to circumvent hardware error correction schemes and corrupt memory. Software errors (like most design aws) are difficult to model and understand.
Reference: [Barton90] <author> James H. Barton, Edward W. Czeck, Zary Z. Segall, and Daniel P. Siewiorek. </author> <title> Fault injection experiments using FIAT. </title> <journal> IEEE Transactions on Computers, </journal> <volume> 39(4):575582, </volume> <month> April </month> <year> 1990. </year>
Reference-contexts: FINE uses software to emulate hardware and software bugs and monitors the effect of the fault on the Unix operating system. Another tool, FIAT, uses software to inject memory bit faults into various code and data segments <ref> [Segall88, Barton90] </ref> of an application program. FERRARI also uses software to inject various hardware faults [Kanawati95, Kanawati92]. FERRARI is extremely exible: it can emulate a large number of data, address, and control faults, and it can inject transient or permanent faults into user programs or the operating system. <p> We classify the faults we inject into three categories: random bit ips, low-level software faults, and high-level software faults. Each succeeding fault category is progressively more realistic. 4.1 Random Bit Flips The first category of faults ips randomly chosen bits in the kernels address space <ref> [Barton90, Kanawati95] </ref>. We target three areas of the kernels address space: the kernel text, heap, and stack. For kernel text tests, we corrupt ten randomly chosen instructions in memory after the system is up and running.
Reference: [Biswas93] <author> Prabuddha Biswas, K. K. Ramakrishnan, Don Towsley, and C. M. Krishna. </author> <title> Performance Analysis of Distributed File Systems with Non-Volatile Caches. </title> <booktitle> In Proceedings of the 1993 International Symposium on High Performance Distributed Computing (HPDC-2), </booktitle> <pages> pages 252262, </pages> <month> July </month> <year> 1993. </year>
Reference-contexts: The Durable Memory RS/6000 uses batteries, replicated processors, memory ECC, and alternate paths to tolerate a wide variety of hardware failures [Abbott94]. Finally, several papers have examined the performance advantages and management of reliable memory <ref> [Copeland89, Baker92a, Biswas93, Akyurek95] </ref>. 3 Experimental Environment and Mechanisms Our experiments were run on DEC Alpha 3000/600 workstations (Table 1) running the Digital Unix V3.0 operating system. Digital Unix is a monolithic kernel derived from Mach 2.5 and OSF/1.
Reference: [Chapin95] <author> John Chapin, Mendel Rosenblum, Scott Devine, Tirthankar Lahiri, Dan Teodosiu, and Anoop Gupta. Hive: </author> <title> Fault Containment for Shared-Memory Multiprocessors. </title> <booktitle> In Proceedings of the 1995 Symposium on Operating Systems Principles, </booktitle> <month> December </month> <year> 1995. </year>
Reference-contexts: This is similar to modifying the memory controller to enforce protection, as are Johnsons and Wahbes suggestions for various hardware mechanisms to trap the updates of certain memory locations [Johnson82, Wahbe92]. Hive uses the Flash firewall to protect memory against wild writes by other processors in a multiprocessor <ref> [Chapin95] </ref>. Hive preemptively discards pages that are writable by failed processors, an option not available when storing permanent data in memory. Finally, object code modification has been suggested as a way to provide data breakpoints [Kessler90, Wahbe92] and fault isolation between software modules [Wahbe93]. <p> The memory device driver is the only module in the operating system allowed to change files in memoryany write to the file cache that does not use the memory device driver should cause an exception <ref> [Chapin95] </ref>.
Reference: [Chen95] <author> Peter M. Chen, Christopher M. Aycock, Wee Teck Ng, Gurushankar Rajamani, and Rajagopalan Si-varamakrishnan. </author> <title> Rio: Storing Files Reliably in Memory. </title> <type> Technical Report CSE-TR-250-95, </type> <institution> University of Michigan, </institution> <month> July </month> <year> 1995. </year>
Reference-contexts: To examine the file cache image in memory, we perform a warm reboot during which all files in memory at the time of the crash are restored to disk <ref> [Chen95] </ref>. We have two other goals in designing the workload. First, we want a general-purpose workload that calls many different programs. Second, we want to stress the file system with real programs that expanded the file cache to include most of main memory. <p> For example, functions such as bcopy modify sequential blocks of data; these blocks can be checked once rather than checking every individual store. Initial performance data for Digital Unix indicates that the overhead of code patching is only 2-10% <ref> [Chen95] </ref>. We are currently completing an initial implementation of this protection mechanism and plan to evaluate how effectively it lowers the risk of memory corruption for the faults described in Section 3. 8 Conclusions and Future Work We have shown that memory is remarkably resistant to operating system crashes.
Reference: [Copeland89] <author> George Copeland, Tom Keller, Ravi Krishnamurthy, and Marc Smith. </author> <title> The Case for Safe RAM. </title> <booktitle> In Proceedings of the Fifteenth International Conference on Very Large Data Bases, </booktitle> <pages> pages 327335, </pages> <month> August </month> <year> 1989. </year>
Reference-contexts: However, no paper on fault injection have specifically measured the effects of faults on permanent data in memory. Measuring and Improving Memorys Resistance to Operating System Crashes 3 2.3 Protecting Memory Several researchers have proposed ways to protect memory from software failures <ref> [Copeland89] </ref>, though to our knowledge none have evaluated how effectively memory withstood these failures. The only file system we are aware of that attempts to make all permanent files reliable while in memory is Phoenix [Gait90]. Phoenix keeps two versions of an in-memory file system. <p> The Durable Memory RS/6000 uses batteries, replicated processors, memory ECC, and alternate paths to tolerate a wide variety of hardware failures [Abbott94]. Finally, several papers have examined the performance advantages and management of reliable memory <ref> [Copeland89, Baker92a, Biswas93, Akyurek95] </ref>. 3 Experimental Environment and Mechanisms Our experiments were run on DEC Alpha 3000/600 workstations (Table 1) running the Digital Unix V3.0 operating system. Digital Unix is a monolithic kernel derived from Mach 2.5 and OSF/1. <p> In particular, avoiding custom hardware would enable us to modify the system more quickly and make our results more widely applicable. At first glance, the virtual memory protection of a system seems ideally suited to protect the file cache from unauthorized stores <ref> [Copeland89] </ref>. By keeping the write-permission bits in the page table entries turned off for the file cache pages, the system will cause most unauthorized stores to encounter a protection violation.
Reference: [DEC95] <author> August 1995. </author> <title> Digital Unix development team, </title> <type> Personal Communication. </type>
Reference-contexts: We do not consider power outages further in this paper. Memorys vulnerability to OS crashes is less concrete. Most people would feel nervous if their system crashed while the sole copy of important data was in memory, even if the power stayed on <ref> [DEC95, Tanenbaum95 page 146, Silberschatz94 page 200] </ref>. As evidence of this view, most systems periodically write file data to disk, and transaction processing applications view transactions as committed only when the changes are made to the disk copy of the database.
Reference: [Dutton92] <author> Todd A. Dutton, Daniel Eiref, Hugh R. Kurth, James J. Reisert, and Robin L. Stewart. </author> <title> The Design of the DEC 3000 AXP Systems, Two High-Performance Workstations. </title> <journal> Digital Technical Journal, </journal> <volume> 4(4):6681, </volume> <year> 1992. </year>
Reference-contexts: Digital Unix is a monolithic kernel derived from Mach 2.5 and OSF/1. Table 1: Specifications of Experimental Platform <ref> [Dutton92] </ref>. machine type DEC 3000 model 600 CPU chip Alpha 21064, 175 MHz SPECint92 114 SPECfp92 165 memory bandwidth 207 MB/s memory capacity 128 MB (512 MB max capacity) system bus Turbochannel system bus bandwidth 100 MB/s Measuring and Improving Memorys Resistance to Operating System Crashes 4 What data does the
Reference: [Eich87] <author> Margaret H. Eich. </author> <title> A classification and comparison of main memory database recovery techniques. </title> <booktitle> In Proceedings of the IEEE International Conference on Data Engineering, </booktitle> <pages> pages 332339, </pages> <month> February </month> <year> 1987. </year>
Reference-contexts: Increased disk traffic due to extra write backs forces the use of extra disk optimizations such as disk scheduling, disk reorganization, and group commit. Much of the research in main-memory databases deals with checkpointing and recover ing data in case the system crashes <ref> [GM92, Eich87] </ref>. Ideal semantics, such as atomicity for every transaction, are also sacrificed because disk accesses are slow and memory is unreliable. Finally, memorys unreliability forces systems to keep a copy of perma nent memory data on disk; this shrinks the available storage capacity.
Reference: [Gait90] <author> Jason Gait. </author> <title> Phoenix: A Safe In-Memory File System. </title> <journal> Communications of the ACM, </journal> <volume> 33(1):8186, </volume> <month> Jan-uary </month> <year> 1990. </year>
Reference-contexts: The only file system we are aware of that attempts to make all permanent files reliable while in memory is Phoenix <ref> [Gait90] </ref>. Phoenix keeps two versions of an in-memory file system. One of these versions is kept write-protected; the other version is unprotected and evolves from the write-protected one via copy-on-write. At periodic checkpoints, the system write-protects the unprotected version and deletes obsolete pages in the original version.
Reference: [GM92] <author> Hector Garcia-Molina and Kenneth Salem. </author> <title> Main Memory Database Systems: An Overview. </title> <journal> IEEE Transactions on Knowledge and Data Engineering, </journal> <volume> 4(6):509516, </volume> <month> December </month> <year> 1992. </year>
Reference-contexts: Increased disk traffic due to extra write backs forces the use of extra disk optimizations such as disk scheduling, disk reorganization, and group commit. Much of the research in main-memory databases deals with checkpointing and recover ing data in case the system crashes <ref> [GM92, Eich87] </ref>. Ideal semantics, such as atomicity for every transaction, are also sacrificed because disk accesses are slow and memory is unreliable. Finally, memorys unreliability forces systems to keep a copy of perma nent memory data on disk; this shrinks the available storage capacity.
Reference: [Gray90] <author> Jim Gray. </author> <title> A Census of Tandem System Availability between 1985 and 1990. </title> <journal> IEEE Transactions on Reliability, </journal> <volume> 39(4), </volume> <month> October </month> <year> 1990. </year>
Reference-contexts: proposes a software mechanism that protects memory from system crashes. 2 Related Work We divide the research related to this paper into three areas: field studies, fault injection, and protection schemes. 2.1 Field Studies of System Crashes Studies have shown that software has become the dominant cause of system outages <ref> [Gray90] </ref>. Many studies have investigated system software errors. The studies most relevant to this paper investigate operating system errors on production IBM and Tandem systems.
Reference: [Hartman93] <author> John H. Hartman and John K. Ousterhout. </author> <title> The Zebra Striped Network File System. </title> <booktitle> In Proceedings of the 1993 Symposium on Operating System Principles, </booktitle> <pages> pages 2943, </pages> <month> December </month> <year> 1993. </year>
Reference-contexts: In addition, 1/3 to 2/3 of newly written data lives longer than 30 seconds <ref> [Baker91, Hartman93] </ref>, so a large 1. It is also vulnerable to power loss, but this paper will not discuss this aspect of reliability. It is possible to make memory non-volatile by using an uninterruptible power supply or by using Flash RAM.
Reference: [Howard88] <author> John H. Howard, Michael L. Kazar, Sherri G. Menees, David A. Nichols, M. Satyanarayanan, Robert N. Sidebotham, and Michael J. West. </author> <title> Scale and Performance in a Distributed File System. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 6(1):5181, </volume> <month> February </month> <year> 1988. </year>
Reference-contexts: First, we want a general-purpose workload that calls many different programs. Second, we want to stress the file system with real programs that expanded the file cache to include most of main memory. To create a general-purpose workload, we run four copies of the Andrew benchmark <ref> [Howard88, Ousterhout90] </ref>. Andrew creates and copies a source hierarchy; examines the hierarchy using find, ls, du, grep, and wc; and compiles the source hierarchy.
Reference: [Iyer95] <author> Ravishankar K. Iyer. </author> <title> Experimental Evaluation. </title> <booktitle> In Proceedings of the 1995 International Symposium on Fault-Tolerant Computing, </booktitle> <pages> pages 115132, </pages> <month> July </month> <year> 1995. </year>
Reference-contexts: We review some of the most relevant prior work; see <ref> [Iyer95] </ref> for an excellent introduction to the overall area and a summary of much of the past fault injection techniques. The most relevant work to this paper is the FINE fault injector and monitoring environment [Kao93].
Reference: [Johnson82] <author> Mark Scott Johnson. </author> <title> Some Requirements for Architectural Support of Software Debugging. </title> <booktitle> In Proceedings of the 1982 International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS), </booktitle> <pages> pages 140148, </pages> <month> April </month> <year> 1982. </year>
Reference-contexts: This is similar to modifying the memory controller to enforce protection, as are Johnsons and Wahbes suggestions for various hardware mechanisms to trap the updates of certain memory locations <ref> [Johnson82, Wahbe92] </ref>. Hive uses the Flash firewall to protect memory against wild writes by other processors in a multiprocessor [Chapin95]. Hive preemptively discards pages that are writable by failed processors, an option not available when storing permanent data in memory.
Reference: [Kanawati92] <author> Ghani A. Kanawati, Nasser A. Kanawati, and Jacob A. Abraham. FERRARI: </author> <title> a tool for the validation of system dependability properties. </title> <booktitle> In Proceedings of the 1992 International Symposium on Fault-Tolerant Computing, </booktitle> <pages> pages 336344, </pages> <month> July </month> <year> 1992. </year>
Reference-contexts: Another tool, FIAT, uses software to inject memory bit faults into various code and data segments [Segall88, Barton90] of an application program. FERRARI also uses software to inject various hardware faults <ref> [Kanawati95, Kanawati92] </ref>. FERRARI is extremely exible: it can emulate a large number of data, address, and control faults, and it can inject transient or permanent faults into user programs or the operating system.
Reference: [Kanawati95] <author> Ghani A. Kanawati, Nasser A. Kanawati, and Jacob A. Abraham. FERRARI: </author> <title> A Flexible Software-Based Fault and Error Injection System. </title> <journal> IEEE Transactions on Computers, </journal> <volume> 44(2):248260, </volume> <month> February </month> <year> 1995. </year>
Reference-contexts: Another tool, FIAT, uses software to inject memory bit faults into various code and data segments [Segall88, Barton90] of an application program. FERRARI also uses software to inject various hardware faults <ref> [Kanawati95, Kanawati92] </ref>. FERRARI is extremely exible: it can emulate a large number of data, address, and control faults, and it can inject transient or permanent faults into user programs or the operating system. <p> Our primary goal in designing these experiments is to generate a wide variety of system crashes. We use software to emulate both software and hardware faults because software fault injection has proven to be an easy and effective injection mechanism <ref> [Kanawati95] </ref>. The faults we inject range from low-level hardware faults such as ipping bits in memory to high-level software faults such as memory allocation errors. <p> We classify the faults we inject into three categories: random bit ips, low-level software faults, and high-level software faults. Each succeeding fault category is progressively more realistic. 4.1 Random Bit Flips The first category of faults ips randomly chosen bits in the kernels address space <ref> [Barton90, Kanawati95] </ref>. We target three areas of the kernels address space: the kernel text, heap, and stack. For kernel text tests, we corrupt ten randomly chosen instructions in memory after the system is up and running.
Reference: [Kane92] <author> Gerry Kane and Joe Heinrich. </author> <title> MIPS RISC Architecture. </title> <publisher> Prentice Hall, </publisher> <year> 1992. </year>
Reference-contexts: Or the memory device driver can create a shadow copy and implement atomic writes. Unfortunately, many systems allow certain kernel accesses to bypass the virtual memory protection mechanism and directly access physical memory <ref> [Kane92, Sites92] </ref>. For example, addresses in the DEC Alpha processor with the two most significant bits equal to 10 2 bypass the TLB. 7 To protect against these physical addresses, we can modify the kernel object code, inserting a check before every kernel store; this is called code patching [Wahbe93].
Reference: [Kao93] <author> Wei-Lun Kao, Ravishankar K. Iyer, and Dong Tang. </author> <title> FINE: A Fault Injection and Monitoring Environment for Tracing the UNIX System Behavior under Faults. IEEE Transactions on Software Engi Measuring and Improving Memorys Resistance to Operating System Crashes 12 neering, </title> <address> 19(11):11051118, </address> <month> November </month> <year> 1993. </year>
Reference-contexts: We review some of the most relevant prior work; see [Iyer95] for an excellent introduction to the overall area and a summary of much of the past fault injection techniques. The most relevant work to this paper is the FINE fault injector and monitoring environment <ref> [Kao93] </ref>. FINE uses software to emulate hardware and software bugs and monitors the effect of the fault on the Unix operating system. Another tool, FIAT, uses software to inject memory bit faults into various code and data segments [Segall88, Barton90] of an application program. <p> These faults are intended to approximate the assembly-level manifestation of real C-level programming errors <ref> [Kao93] </ref>. The first fault in this category is an assignment faults. One type of assignment fault changes the destination register used by an instruction; the other changes a source register. <p> These are more targeted at specific programming errors than low-level software faults are. We inject an initialization fault by deleting (turning into noops) the instructions in the kernel text responsible for initializing a variable at the start of a function <ref> [Kao93, Lee93a] </ref>. We inject pointer corruption by 1) finding a register that is used as a base register of a load or store and 2) deleting the most recent instruction before the load/store that modifies that register [Sullivan91, Lee93a]. <p> We observed 76 unique crash error messages and used these error messages to divide the crashes into six categories. The first category is kernel memory fault/unaligned access. These accounted for the largest fraction of crashes (78%), which is consistent with prior results in the field <ref> [Lee93a, Kao93] </ref>. The second largest category is kernel consistency check (11%), followed by user process failure (4%), hardware error (2%), illegal instruction (2%), and unknown (3%). Table 2 shows the distribution of crashes for each type of fault. <p> This is consistent with results from FINE that show most faults do not propagate to other kernel modules <ref> [Kao93] </ref>. Overall, only 1.7% of all crashes corrupted the file cache. Excluding direct corruption from copy overruns, only 0.2% of crashes corrupted the file cache. 6 Discussion In this section, we discuss why the difference between disk and memory reliability is not as great as one might think. <p> The first reason is that the huge majority of operating system crashes do not corrupt files in memory. Systems tend to fail quickly due to virtual memory protection and kernel consistency checks, and errors do not tend to propagate between different kernel modules <ref> [Kao93] </ref>. To illustrate memorys reliability, consider a system that crashes once per month (a pessimistic estimate for production-quality operating systems).
Reference: [Kessler90] <author> Peter B. Kessler. </author> <title> Fast breakpoints: </title> <booktitle> Design and implementation. In Proceedings of the 1990 Conference on Programming Language Design and Implementation (PLDI), </booktitle> <pages> pages 7884, </pages> <month> June </month> <year> 1990. </year>
Reference-contexts: Hive preemptively discards pages that are writable by failed processors, an option not available when storing permanent data in memory. Finally, object code modification has been suggested as a way to provide data breakpoints <ref> [Kessler90, Wahbe92] </ref> and fault isolation between software modules [Wahbe93].
Reference: [Lee93a] <author> Inhwan Lee and Ravishankar K. Iyer. </author> <title> Faults, Symptoms, and Software Fault Tolerance in the Tandem GUARDIAN Operating System. </title> <booktitle> In International Symposium on Fault-Tolerant Computing (FTCS), pages 2029, </booktitle> <year> 1993. </year>
Reference-contexts: Sullivan and Chillarege classify software faults in the MVS operating system and DB2 and IMS database systems; in particular, they analyze faults that corrupt program memory (overlays) [Sullivan91, Sullivan92]. Lee and Iyer study and classify software failures in Tandems Guardian operating system <ref> [Lee93a, Lee95] </ref>. These studies provide valuable information about failures in production environments; in fact many of the fault types in Section 3 were inspired by the major error categories from [Sullivan91] and [Lee95]. <p> These are more targeted at specific programming errors than low-level software faults are. We inject an initialization fault by deleting (turning into noops) the instructions in the kernel text responsible for initializing a variable at the start of a function <ref> [Kao93, Lee93a] </ref>. We inject pointer corruption by 1) finding a register that is used as a base register of a load or store and 2) deleting the most recent instruction before the load/store that modifies that register [Sullivan91, Lee93a]. <p> We inject pointer corruption by 1) finding a register that is used as a base register of a load or store and 2) deleting the most recent instruction before the load/store that modifies that register <ref> [Sullivan91, Lee93a] </ref>. We do not corrupt the stack pointer register, as this is used to access local variables instead of as a pointer variable. For both initialization and pointer corruption fault tests, we inject ten faults and halt the system if it stays up for more than ten minutes. <p> We observed 76 unique crash error messages and used these error messages to divide the crashes into six categories. The first category is kernel memory fault/unaligned access. These accounted for the largest fraction of crashes (78%), which is consistent with prior results in the field <ref> [Lee93a, Kao93] </ref>. The second largest category is kernel consistency check (11%), followed by user process failure (4%), hardware error (2%), illegal instruction (2%), and unknown (3%). Table 2 shows the distribution of crashes for each type of fault.
Reference: [Lee93b] <author> Inhwan Lee, Dong Tang, Ravishankar K. Iyer, and Mei-Chen Hsueh. </author> <title> Measurement-Based Evaluation of Operating System Fault Tolerance. </title> <journal> IEEE Transactions on Reliability, </journal> <volume> 42(2):238249, </volume> <month> June </month> <year> 1993. </year>
Reference-contexts: The faults we inject range from low-level hardware faults such as ipping bits in memory to high-level software faults such as memory allocation errors. Hardware faults are usually specific and relatively easy to model <ref> [Lee93b] </ref>, and various techniques such as ECC and redundancy have been successfully used to protect against these errors [Abbott94, Banatre93]. We focus primarily on software faults because: Kernel programming errors are the errors most likely to circumvent hardware error correction schemes and corrupt memory.
Reference: [Lee95] <author> Edward K. Lee. </author> <title> Highly-Available, Scalable Network Storage. </title> <booktitle> In Proceedings of the 1995 IEEE Computer Society International Conference (COMPCON), </booktitle> <year> 1995. </year>
Reference-contexts: Sullivan and Chillarege classify software faults in the MVS operating system and DB2 and IMS database systems; in particular, they analyze faults that corrupt program memory (overlays) [Sullivan91, Sullivan92]. Lee and Iyer study and classify software failures in Tandems Guardian operating system <ref> [Lee93a, Lee95] </ref>. These studies provide valuable information about failures in production environments; in fact many of the fault types in Section 3 were inspired by the major error categories from [Sullivan91] and [Lee95]. <p> Lee and Iyer study and classify software failures in Tandems Guardian operating system [Lee93a, Lee95]. These studies provide valuable information about failures in production environments; in fact many of the fault types in Section 3 were inspired by the major error categories from [Sullivan91] and <ref> [Lee95] </ref>. However, they do not provide specific information about how often system crashes corrupt the permanent data in memory. 2.2 Using Software to Inject Faults Software fault injection is a popular technique for evaluating how prototype systems behave in the presence of hardware and software faults.
Reference: [Leffler89] <author> Samuel J. Leffler, Marshall Kirk McKusick, Michael J. Karels, and John S. Quarterman. </author> <title> The Design and Implementation of the 4.3BSD Unix Operating System. </title> <publisher> Addison-Wesley Publishing Company, </publisher> <year> 1989. </year>
Reference: [Liskov91] <author> Barbara Liskov, Sanjay Ghemawat, Robert Gruber, Paul Johnson, Liuba Shrira, and Michael Will-iams. </author> <title> Replication in the Harp File System. </title> <booktitle> In Proceedings of the 1991 Symposium on Operating System Principles, </booktitle> <pages> pages 226238, </pages> <month> October </month> <year> 1991. </year>
Reference-contexts: The Harp file system protects a log of recent modifications by replicating it in volatile, battery-backed memory across several server nodes <ref> [Liskov91] </ref>. The Recovery Box keeps special system state in a region of memory accessed only through a rigid interface [Baker92b]. No attempt is made to prevent other functions from accidentally modifying the recovery box, although the system detects corruption by maintaining checksums.
Reference: [Needham83] <author> R. M. Needham, A. J. Herbert, and J. G. Mitchell. </author> <title> How to Connect Stable Memory to a Computer. Operating System Review, </title> <address> 17(1):16, </address> <month> January </month> <year> 1983. </year>
Reference-contexts: Our work seeks to make main memory reliable without needing special-purpose hardware or dual memory banks. General mechanisms may be used to help protect memory from software faults. <ref> [Needham83] </ref> suggests changing a machines microcode to check certain conditions when writing a memory word; the condition they suggest is that a certain register has been pre-loaded with the memory words previous content. <p> an exception when other modules try to change the file cache without using the memory device driver? An ideal protection mechanism would have the following characteristics: Lightweight: the protection mechanism should add little or no overhead to file cache accesses: it should not need to be invoked on memory reads <ref> [Needham83] </ref> and should have minimal overhead on writes. Enforced: it should be extremely unlikely for a non-malicious kernel function to accidentally bypass the protection mechanism. The vast majority of errors should be trapped. Simple: the protection mechanism should require little change to the existing system.
Reference: [Ousterhout85] <author> John K. Ousterhout, Herve Da Costa, et al. </author> <title> A Trace-Driven Analysis of the UNIX 4.2 BSD File System. </title> <booktitle> In Proceedings of the 1985 Symposium on Operating System Principles, </booktitle> <pages> pages 1524, </pages> <month> December </month> <year> 1985. </year>
Reference-contexts: Many systems, such as Unix file systems, mitigate the performance loss caused by extra disk writes by only writing new data to disk every 30 seconds or so, but this ensures the loss of data written within 30 seconds of a crash <ref> [Ousterhout85] </ref>. In addition, 1/3 to 2/3 of newly written data lives longer than 30 seconds [Baker91, Hartman93], so a large 1. It is also vulnerable to power loss, but this paper will not discuss this aspect of reliability.
Reference: [Ousterhout90] <author> John K. Ousterhout. </author> <title> Why arent operating systems getting faster as fast as hardware? In Proceedings USENIX Summer Conference, </title> <booktitle> pages 247256, </booktitle> <month> June </month> <year> 1990. </year>
Reference-contexts: First, we want a general-purpose workload that calls many different programs. Second, we want to stress the file system with real programs that expanded the file cache to include most of main memory. To create a general-purpose workload, we run four copies of the Andrew benchmark <ref> [Howard88, Ousterhout90] </ref>. Andrew creates and copies a source hierarchy; examines the hierarchy using find, ls, du, grep, and wc; and compiles the source hierarchy.
Reference: [Rahm92] <author> Erhard Rahm. </author> <title> Performance Evaluation of Extended Storage Architectures for Transaction Processing. </title> <booktitle> In Proceedings of the 1992 ACM SIGMOD International Conference on Management of Data, </booktitle> <pages> pages 308317, </pages> <month> June </month> <year> 1992. </year>
Reference-contexts: The extreme approach is to use a pure write-back scheme where data is only written to disk when the memory is full. This is only an option for applications where reliability is not an issue, such as compiler-generated temporary files. Memorys unreliability also increases system complexity <ref> [Rahm92] </ref>. Increased disk traffic due to extra write backs forces the use of extra disk optimizations such as disk scheduling, disk reorganization, and group commit. Much of the research in main-memory databases deals with checkpointing and recover ing data in case the system crashes [GM92, Eich87].
Reference: [Segall88] <author> Z. Segall, D. Vrsalovic, D. Siewiorek, D. Yaskin, J. Kownacki, J. Barton, R. Dancey, A. Robinson, and T. Lin. </author> <title> FIATFault Injection Based Automated Testing Environment. </title> <booktitle> In 18th Annual International Symposium on Fault-Tolerant Computing, </booktitle> <pages> pages 102107, </pages> <year> 1988. </year>
Reference-contexts: FINE uses software to emulate hardware and software bugs and monitors the effect of the fault on the Unix operating system. Another tool, FIAT, uses software to inject memory bit faults into various code and data segments <ref> [Segall88, Barton90] </ref> of an application program. FERRARI also uses software to inject various hardware faults [Kanawati95, Kanawati92]. FERRARI is extremely exible: it can emulate a large number of data, address, and control faults, and it can inject transient or permanent faults into user programs or the operating system.
Reference: [Silberschatz94] <author> Abraham Silberschatz and Peter B. Galvin. </author> <title> Operating System Concepts. </title> <publisher> Addison-Wesley, </publisher> <year> 1994. </year>
Reference: [Sites92] <author> Richard L. </author> <title> Sites, editor. Alpha Architecture Reference Manual. </title> <publisher> Digital Press, </publisher> <year> 1992. </year>
Reference-contexts: Or the memory device driver can create a shadow copy and implement atomic writes. Unfortunately, many systems allow certain kernel accesses to bypass the virtual memory protection mechanism and directly access physical memory <ref> [Kane92, Sites92] </ref>. For example, addresses in the DEC Alpha processor with the two most significant bits equal to 10 2 bypass the TLB. 7 To protect against these physical addresses, we can modify the kernel object code, inserting a check before every kernel store; this is called code patching [Wahbe93].
Reference: [Sullivan91] <author> Mark Sullivan and R. Chillarege. </author> <title> Software Defects and Their Impact on System AvailabilityA Study of Field Failures in Operating Systems. </title> <booktitle> In Proceedings of the 1991 International Symposium on Fault-Tolerant Computing, </booktitle> <month> June </month> <year> 1991. </year>
Reference-contexts: The studies most relevant to this paper investigate operating system errors on production IBM and Tandem systems. Sullivan and Chillarege classify software faults in the MVS operating system and DB2 and IMS database systems; in particular, they analyze faults that corrupt program memory (overlays) <ref> [Sullivan91, Sullivan92] </ref>. Lee and Iyer study and classify software failures in Tandems Guardian operating system [Lee93a, Lee95]. These studies provide valuable information about failures in production environments; in fact many of the fault types in Section 3 were inspired by the major error categories from [Sullivan91] and [Lee95]. <p> Lee and Iyer study and classify software failures in Tandems Guardian operating system [Lee93a, Lee95]. These studies provide valuable information about failures in production environments; in fact many of the fault types in Section 3 were inspired by the major error categories from <ref> [Sullivan91] </ref> and [Lee95]. However, they do not provide specific information about how often system crashes corrupt the permanent data in memory. 2.2 Using Software to Inject Faults Software fault injection is a popular technique for evaluating how prototype systems behave in the presence of hardware and software faults. <p> We inject pointer corruption by 1) finding a register that is used as a base register of a load or store and 2) deleting the most recent instruction before the load/store that modifies that register <ref> [Sullivan91, Lee93a] </ref>. We do not corrupt the stack pointer register, as this is used to access local variables instead of as a pointer variable. For both initialization and pointer corruption fault tests, we inject ten faults and halt the system if it stays up for more than ten minutes. <p> For both initialization and pointer corruption fault tests, we inject ten faults and halt the system if it stays up for more than ten minutes. We also inject two of the common, high-level programming errors described by <ref> [Sullivan91] </ref>: allocation management and copy overruns. In an allocation management fault, a module continues to use a region of memory after it has deallocated it. We inject this fault by modifying the kernel malloc function to occasionally prematurely free the newly allocated memory. <p> The length of the overrun was distributed as follows: 50% corrupt one byte; 44% corrupt 2-1024 bytes; 6% corrupt 2-4 KB. This distribution was chosen by starting with the data gathered in <ref> [Sullivan91] </ref> and modifying it somewhat according to our specific platform and experience. bcopy is set to inject this error every 1000-4000 times it is called; this occurs approximately every 15 seconds. 5 Results This section focuses on two major results from the fault-injection experiments: the variety of crashes and their effect
Reference: [Sullivan92] <author> Mark Sullivan and Ram Chillarege. </author> <title> A Comparison of Software Defects in Database Management Systems and Operating Systems. </title> <booktitle> In Proceedings of the 1992 International Symposium on Fault-Tolerant Computing, </booktitle> <pages> pages 475484, </pages> <month> July </month> <year> 1992. </year>
Reference-contexts: The studies most relevant to this paper investigate operating system errors on production IBM and Tandem systems. Sullivan and Chillarege classify software faults in the MVS operating system and DB2 and IMS database systems; in particular, they analyze faults that corrupt program memory (overlays) <ref> [Sullivan91, Sullivan92] </ref>. Lee and Iyer study and classify software failures in Tandems Guardian operating system [Lee93a, Lee95]. These studies provide valuable information about failures in production environments; in fact many of the fault types in Section 3 were inspired by the major error categories from [Sullivan91] and [Lee95].
Reference: [Tanenbaum95] <author> Andrew S. Tanenbaum. </author> <title> Distributed Operating Systems. </title> <publisher> Prentice-Hall, </publisher> <year> 1995. </year>
Reference-contexts: I/O devices such as disks and tapes are considered fairly reliable places to store long-term data such as files. However, random-access memory is commonly viewed as an unreliable place to store permanent data (files) because it is perceived to be vulnerable to power outages and operating system crashes <ref> [Tanenbaum95, page 146] </ref>. Memorys vulnerability to power outages is straightforward to understand and fix. A simple solution is to add an uninterruptible power supply to the system. Another solution is to switch to a non-volatile memory technology such as Flash RAM [Wu94].
Reference: [Wahbe92] <author> Robert Wahbe. </author> <title> Efficient Data Breakpoints. </title> <booktitle> In Proceedings of the 1992 International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS), </booktitle> <month> October </month> <year> 1992. </year>
Reference-contexts: This is similar to modifying the memory controller to enforce protection, as are Johnsons and Wahbes suggestions for various hardware mechanisms to trap the updates of certain memory locations <ref> [Johnson82, Wahbe92] </ref>. Hive uses the Flash firewall to protect memory against wild writes by other processors in a multiprocessor [Chapin95]. Hive preemptively discards pages that are writable by failed processors, an option not available when storing permanent data in memory. <p> Hive preemptively discards pages that are writable by failed processors, an option not available when storing permanent data in memory. Finally, object code modification has been suggested as a way to provide data breakpoints <ref> [Kessler90, Wahbe92] </ref> and fault isolation between software modules [Wahbe93].
Reference: [Wahbe93] <author> Robert Wahbe, Steven Lucco, Thomas E. Anderson, and Susan L. Graham. </author> <title> Efficient Software-Based Fault Isolation. </title> <booktitle> In Proceedings of the 14th ACM Symposium on Operating Systems Principles, </booktitle> <pages> pages 203216, </pages> <month> December </month> <year> 1993. </year>
Reference-contexts: Hive preemptively discards pages that are writable by failed processors, an option not available when storing permanent data in memory. Finally, object code modification has been suggested as a way to provide data breakpoints [Kessler90, Wahbe92] and fault isolation between software modules <ref> [Wahbe93] </ref>. <p> For example, addresses in the DEC Alpha processor with the two most significant bits equal to 10 2 bypass the TLB. 7 To protect against these physical addresses, we can modify the kernel object code, inserting a check before every kernel store; this is called code patching <ref> [Wahbe93] </ref>. If the address is a physical address, the system checks to make sure the address is not in the file cache, or that the file cache has explicitly registered the address as writable. 8 Initially, the idea of inserting code before every store instruction sounds prohibitively slow. <p> In addition, the stack pointer is almost always modified in small increments, and these small increments cannot change a virtual address to a physical address. We can therefore decrease the number of checks by replacing the checks on local, stack variables with a few checks on the stack pointer <ref> [Wahbe93] </ref>. Another method to lower the checking overhead is to replace individual checks in loops with a few higher-level checks. For example, functions such as bcopy modify sequential blocks of data; these blocks can be checked once rather than checking every individual store.
Reference: [Wu94] <author> Michael Wu and Willy Zwaenepoel. eNVy: </author> <title> A Non-Volatile, Main Memory Storage System. </title> <booktitle> In Proceedings of the 1994 International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS), </booktitle> <month> October </month> <year> 1994. </year>
Reference-contexts: Memorys vulnerability to power outages is straightforward to understand and fix. A simple solution is to add an uninterruptible power supply to the system. Another solution is to switch to a non-volatile memory technology such as Flash RAM <ref> [Wu94] </ref>. We do not consider power outages further in this paper. Memorys vulnerability to OS crashes is less concrete. <p> Other projects seek to improve the reliability of memory against hardware faults such as power outages and board failures. eNVy implements a memory board based on ash RAM, which is non-volatile <ref> [Wu94] </ref>. eNVy uses copy-on-write, page remapping, and a small, battery-backed, SRAM buffer to hide ash RAMs slow writes and bulk erases. The Durable Memory RS/6000 uses batteries, replicated processors, memory ECC, and alternate paths to tolerate a wide variety of hardware failures [Abbott94].
References-found: 46

