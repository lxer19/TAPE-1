URL: http://www.cs.ubc.ca/spider/cebly/Papers/_download_/decomposition.ps
Refering-URL: http://www.cs.ubc.ca/spider/cebly/papers.html
Root-URL: 
Email: email: fcebly,brafman,geibg@cs.ubc.ca  
Title: Prioritized Goal Decomposition of Markov Decision Processes: Toward a Synthesis of Classical and Decision Theoretic Planning  
Author: Craig Boutilier, Ronen I. Brafman, and Christopher Geib 
Address: Vancouver, BC, CANADA, V6T 1Z4  
Affiliation: Department of Computer Science University of British Columbia  
Date: August, 1997  
Note: To appear, Proc. 15th Intl. Joint Conf. on AI (IJCAI-97),Nagoya,  
Abstract: We describe an approach to goal decomposition for a certain class of Markov decision processes (MDPs). An abstraction mechanism is used to generate abstract MDPs associated with different objectives, and several methods for merging the policies for these different objectives are considered. In one technique, causal (least-commitment) structures are generated for abstract policies and plan merging techniques, exploiting the relaxation of policy commitments reflected in this structure, are used to piece the results into a single policy. Abstract value functions provide guidance if plan repair is needed. This work makes some first steps toward the synthesis of classical and decision theoretic planning methods. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> R. E. Bellman. </author> <title> Dynamic Programming. </title> <publisher> Princeton, </publisher> <year> 1957. </year>
Reference-contexts: To appear, Proc. 15th Intl. Joint Conf. on AI (IJCAI-97),Nagoya, August, 1997 2 MDPs and Least-Commitment Plans 2.1 Markov Decision Processes We assume that the system to be controlled can be described as a fully-observable, discrete state Markov decision process <ref> [1, 7, 12] </ref>, with a finite set of system states S. The controlling agent has available a finite set of actions A which cause stochastic state transitions: we write Pr (s; a; t) to denote the probability action a causes a transition to state t when executed in state s. <p> A policy is optimal if V (s) V 0 (s) for all s 2 S and policies 0 . The optimal value function V fl is simply the value function for any optimal policy. We refer to <ref> [1, 7, 12] </ref> for a description of policy construction techniques. 2.2 Action and Reward Representation Several good representations for MDPs, suitable for DTP, have been proposed. These include stochastic STRIPS operators [9, 6] and dynamic Bayes nets [5, 2] for representing actions, and related methods for describing reward functions.
Reference: [2] <author> C. Boutilier, R. Dearden, and M. Goldszmidt. </author> <title> Exploiting structure in policy construction. </title> <booktitle> In IJCAI-95, </booktitle> <address> pp.1104-1111, Montreal, </address> <year> 1995. </year>
Reference-contexts: These generally require explicit state space enumeration. Recent work in DTP has focused on techniques that exploit problem structure to solve MDPs in efficient ways. Such methods include problem decomposition based on reachabil-ity analysis [4] and exact and approximate abstraction algorithms <ref> [6, 2] </ref> in which states are (implicitly) aggregated depending on whether they agree on the values of certain problem variables. Indeed, these abstraction methods apply an insight fundamental to classical planning: feature-based search methods are usually much more tractable than state-based methods. <p> We refer to [1, 7, 12] for a description of policy construction techniques. 2.2 Action and Reward Representation Several good representations for MDPs, suitable for DTP, have been proposed. These include stochastic STRIPS operators [9, 6] and dynamic Bayes nets <ref> [5, 2] </ref> for representing actions, and related methods for describing reward functions. We will adopt the STRIPS-style specification used in [6]. We assume a set of atomic propositions used to describe the planning problem of interest. <p> We assume reward 0 is associated with all unmentioned features. 1 This representation reflects the same kinds of independence assumptions one finds in Bayes net action representations <ref> [5, 2] </ref>, and offers the same conciseness of representation when there are multiple uncorrelated effects. To appear, Proc. 15th Intl.
Reference: [3] <author> M. E. Bratman. </author> <title> Intentions, Plans, and Practical Reason. </title> <address> Har-vard, </address> <year> 1987. </year>
Reference-contexts: The plan-based merging approach shares much of the motivation underlying work on intentions and resource-bounded reasoning in planning <ref> [3] </ref>. Although it may result in inferior policies as compared to the first, less constrained search method, the commitment embodied by the plan-based approach is suitable when, due to uncertainty and resource constraints, we prefer an anytime approach in which we first generate a plan for the most important objectives.
Reference: [4] <author> T. Dean, L. P. Kaelbling, J. Kirman, and A. Nicholson. </author> <title> Planning with deadlines in stochastic domains. </title> <booktitle> In AAAI-93, </booktitle> <address> pp.574-579, Washington, D.C., </address> <year> 1993. </year>
Reference-contexts: These generally require explicit state space enumeration. Recent work in DTP has focused on techniques that exploit problem structure to solve MDPs in efficient ways. Such methods include problem decomposition based on reachabil-ity analysis <ref> [4] </ref> and exact and approximate abstraction algorithms [6, 2] in which states are (implicitly) aggregated depending on whether they agree on the values of certain problem variables. Indeed, these abstraction methods apply an insight fundamental to classical planning: feature-based search methods are usually much more tractable than state-based methods. <p> 3.2 Extracting Causal Structure from Policies Once the reward function has been decomposed, the abstract MDPs associated with feature sets R 1 ; R 2 ; : : : can be formulated and solved, and optimal policies 1 ; 2 ; : : : and value 3 We refer to <ref> [4] </ref> to see how such objectives can be encoded in an MDP using absorbing states. For example, we can have a distinguished stop action that precludes further actions. functions V 1 ; V 2 ; : : : determined.
Reference: [5] <author> T. Dean and K. </author> <title> Kanazawa. A model for reasoning about persistence and causation. </title> <journal> Comp. Intel., </journal> <volume> 5(3) </volume> <pages> 142-150, </pages> <year> 1989. </year>
Reference-contexts: We refer to [1, 7, 12] for a description of policy construction techniques. 2.2 Action and Reward Representation Several good representations for MDPs, suitable for DTP, have been proposed. These include stochastic STRIPS operators [9, 6] and dynamic Bayes nets <ref> [5, 2] </ref> for representing actions, and related methods for describing reward functions. We will adopt the STRIPS-style specification used in [6]. We assume a set of atomic propositions used to describe the planning problem of interest. <p> We assume reward 0 is associated with all unmentioned features. 1 This representation reflects the same kinds of independence assumptions one finds in Bayes net action representations <ref> [5, 2] </ref>, and offers the same conciseness of representation when there are multiple uncorrelated effects. To appear, Proc. 15th Intl.
Reference: [6] <author> R. Dearden and C. Boutilier. </author> <title> Abstraction and approximate decision theoretic planning. </title> <journal> Artif. Intel., </journal> <volume> 89 </volume> <pages> 219-283, </pages> <year> 1997. </year>
Reference-contexts: These generally require explicit state space enumeration. Recent work in DTP has focused on techniques that exploit problem structure to solve MDPs in efficient ways. Such methods include problem decomposition based on reachabil-ity analysis [4] and exact and approximate abstraction algorithms <ref> [6, 2] </ref> in which states are (implicitly) aggregated depending on whether they agree on the values of certain problem variables. Indeed, these abstraction methods apply an insight fundamental to classical planning: feature-based search methods are usually much more tractable than state-based methods. <p> Since merging is not guaranteed to succeed, we also give priority to abstract policies associated with more important objectives. This prioritization of goals approximates the workings of optimal policy construction. In Section 2 we overview MDPs and their representation, nonlinear plans, and the MDP abstraction technique of <ref> [6] </ref>. Section 3 outlines the major components of our framework we describe: the construction of prioritized goals or objectives; the extraction of a nonlinear plan from a given policy; and methods for merging policies, including the use of least-commitment structure. <p> The optimal value function V fl is simply the value function for any optimal policy. We refer to [1, 7, 12] for a description of policy construction techniques. 2.2 Action and Reward Representation Several good representations for MDPs, suitable for DTP, have been proposed. These include stochastic STRIPS operators <ref> [9, 6] </ref> and dynamic Bayes nets [5, 2] for representing actions, and related methods for describing reward functions. We will adopt the STRIPS-style specification used in [6]. We assume a set of atomic propositions used to describe the planning problem of interest. <p> These include stochastic STRIPS operators [9, 6] and dynamic Bayes nets [5, 2] for representing actions, and related methods for describing reward functions. We will adopt the STRIPS-style specification used in <ref> [6] </ref>. We assume a set of atomic propositions used to describe the planning problem of interest. Each action a is represented by two components, a precondition and a probabilistic effect set. <p> To concisely represent actions that have multiple, uncorrelated stochastic effects, we use action aspects <ref> [6] </ref>. Each aspect for action a is represented exactly as a probabilistic effect set, with its own set of contexts and effects. <p> To appear, Proc. 15th Intl. Joint Conf. on AI (IJCAI-97),Nagoya, August, 1997 2.3 Approximate Abstraction The abstraction technique developed in <ref> [6] </ref> can reduce the size of the MDP one needs to solve to produce an approximately optimal policy. Using regression-like operations, we can discover which atoms influence the probability of other propositions becoming true when certain action sequences are executed. <p> The abstract MDP over this reduced space has only 8 states, and can be more easily solved to produce an optimal policy. More importantly, one can ignore certain reward propositions to produce an approximate abstraction (see <ref> [6] </ref>). For instance, in Figure 2, atoms hole (P1), pressed (P1), and painted (P1) have a larger influence on reward than hole (P2). <p> Unlike the abstraction method of <ref> [6] </ref>, however, we do not solve a single abstract MDP (e.g., for P1-objectives) and ignore other objectives completely. <p> Of course, larger MDPs usually result and the degree of potential interaction should be balanced against the increased size of the MDP and the potential loss in value if one of the objective sets cannot be accommodated by merging. We refer to <ref> [6] </ref> for further discussion of reward attribute selection for abstraction and the tradeoff with abstract MDP size. 3.2 Extracting Causal Structure from Policies Once the reward function has been decomposed, the abstract MDPs associated with feature sets R 1 ; R 2 ; : : : can be formulated and solved,
Reference: [7] <author> R. A. Howard. </author> <title> Dynamic Programming and Markov Processes. </title> <publisher> MIT Press, </publisher> <address> Cambridge, </address> <year> 1960. </year>
Reference-contexts: To appear, Proc. 15th Intl. Joint Conf. on AI (IJCAI-97),Nagoya, August, 1997 2 MDPs and Least-Commitment Plans 2.1 Markov Decision Processes We assume that the system to be controlled can be described as a fully-observable, discrete state Markov decision process <ref> [1, 7, 12] </ref>, with a finite set of system states S. The controlling agent has available a finite set of actions A which cause stochastic state transitions: we write Pr (s; a; t) to denote the probability action a causes a transition to state t when executed in state s. <p> In order to compare policies, we adopt expected total discounted reward as our optimality criterion; future rewards are discounted by rate 0 fi &lt; 1. The value of a policy can be shown to satisfy <ref> [7] </ref>: V (s) = R (s) + fi t2S The value of at any initial state s can be computed by solving this system of linear equations. A policy is optimal if V (s) V 0 (s) for all s 2 S and policies 0 . <p> A policy is optimal if V (s) V 0 (s) for all s 2 S and policies 0 . The optimal value function V fl is simply the value function for any optimal policy. We refer to <ref> [1, 7, 12] </ref> for a description of policy construction techniques. 2.2 Action and Reward Representation Several good representations for MDPs, suitable for DTP, have been proposed. These include stochastic STRIPS operators [9, 6] and dynamic Bayes nets [5, 2] for representing actions, and related methods for describing reward functions.
Reference: [8] <author> R. L. Keeney and H. Raiffa. </author> <title> Decisions with Multiple Objectives: Preferences and Value Trade-offs. </title> <publisher> Wiley, </publisher> <address> New York, </address> <year> 1978. </year>
Reference-contexts: This underlies, for instance, regression and causal link planning algorithms [13]. In this paper, we investigate another way of decomposing MDPs for efficient solution, namely, goal decomposition. Specifically, if utility is assigned to the achievement of certain features (as in multi-attribute utility theory <ref> [8] </ref>), we will consider components of the utility function separately. <p> No preconditions are shown in the table, but we assume that the drilling action has the precondition :hot (the drill bit is not hot). The reward function R can also be represented compactly in many circumstances. We assume the reward function associates additive, independent rewards with different domain propositions <ref> [8] </ref>: the reward associated with a state is simply the sum of the rewards given for the propositions it satisfies. on the features of two distinct parts P1 and P2.
Reference: [9] <author> N. Kushmerick, S. Hanks, and D. Weld. </author> <title> An algorithm for probabilistic least-commitment planning. </title> <booktitle> In AAAI-94, </booktitle> <address> pp.1073-1078, Seattle, </address> <year> 1994. </year>
Reference-contexts: The optimal value function V fl is simply the value function for any optimal policy. We refer to [1, 7, 12] for a description of policy construction techniques. 2.2 Action and Reward Representation Several good representations for MDPs, suitable for DTP, have been proposed. These include stochastic STRIPS operators <ref> [9, 6] </ref> and dynamic Bayes nets [5, 2] for representing actions, and related methods for describing reward functions. We will adopt the STRIPS-style specification used in [6]. We assume a set of atomic propositions used to describe the planning problem of interest. <p> If E i occurs, the resulting state change is such that the literals mentioned in E i become true and any literal whose atom does not occur in E i retains its truth value <ref> [9] </ref> (i.e., E i is a STRIPS-style change list). To concisely represent actions that have multiple, uncorrelated stochastic effects, we use action aspects [6]. Each aspect for action a is represented exactly as a probabilistic effect set, with its own set of contexts and effects.
Reference: [10] <author> D. McAllester and D. Rosenblitt. </author> <title> Systematic nonlinear planning. </title> <booktitle> In AAAI-91, </booktitle> <address> pp.634-639, Anaheim, </address> <year> 1991. </year>
Reference-contexts: We can think of an LCP as representing its set of linearizations: the sequences of action instances in A consistent with the ordering constraints. We do not discuss here algorithms for producing LCPs, but refer to [13] for a survey and to some of the key developments presented in <ref> [10, 11] </ref>. 3 Goal Decomposition The aim of goal decomposition is similar to that of abstraction: to solve smaller MDPs by ignoring variables. Unlike the abstraction method of [6], however, we do not solve a single abstract MDP (e.g., for P1-objectives) and ignore other objectives completely.
Reference: [11] <author> J. S. Penberthy and D. S. Weld. UCPOP: </author> <title> A sound, complete, partial order planner for adl. </title> <booktitle> In KR-92, </booktitle> <year> 1992. </year>
Reference-contexts: We can think of an LCP as representing its set of linearizations: the sequences of action instances in A consistent with the ordering constraints. We do not discuss here algorithms for producing LCPs, but refer to [13] for a survey and to some of the key developments presented in <ref> [10, 11] </ref>. 3 Goal Decomposition The aim of goal decomposition is similar to that of abstraction: to solve smaller MDPs by ignoring variables. Unlike the abstraction method of [6], however, we do not solve a single abstract MDP (e.g., for P1-objectives) and ignore other objectives completely.
Reference: [12] <author> M. L. Puterman. </author> <title> Markov Decision Processes: Discrete Stochastic Dynamic Programming. </title> <publisher> Wiley, </publisher> <address> NY, </address> <year> 1994. </year>
Reference-contexts: To appear, Proc. 15th Intl. Joint Conf. on AI (IJCAI-97),Nagoya, August, 1997 2 MDPs and Least-Commitment Plans 2.1 Markov Decision Processes We assume that the system to be controlled can be described as a fully-observable, discrete state Markov decision process <ref> [1, 7, 12] </ref>, with a finite set of system states S. The controlling agent has available a finite set of actions A which cause stochastic state transitions: we write Pr (s; a; t) to denote the probability action a causes a transition to state t when executed in state s. <p> A policy is optimal if V (s) V 0 (s) for all s 2 S and policies 0 . The optimal value function V fl is simply the value function for any optimal policy. We refer to <ref> [1, 7, 12] </ref> for a description of policy construction techniques. 2.2 Action and Reward Representation Several good representations for MDPs, suitable for DTP, have been proposed. These include stochastic STRIPS operators [9, 6] and dynamic Bayes nets [5, 2] for representing actions, and related methods for describing reward functions.
Reference: [13] <author> D. S. Weld. </author> <title> An introduction to least commitment planning. </title> <journal> AI Magazine, </journal> <volume> Winter </volume> 1994 27-61, 1994. 
Reference-contexts: Indeed, these abstraction methods apply an insight fundamental to classical planning: feature-based search methods are usually much more tractable than state-based methods. This underlies, for instance, regression and causal link planning algorithms <ref> [13] </ref>. In this paper, we investigate another way of decomposing MDPs for efficient solution, namely, goal decomposition. Specifically, if utility is assigned to the achievement of certain features (as in multi-attribute utility theory [8]), we will consider components of the utility function separately. <p> We briefly review the relevant concepts here, but refer to <ref> [13] </ref> for more details. A least-commitment plan (LCP) consists of a set of action instances A, a set of causal links involving these actions and a set of ordering constraints (i.e., using the relations &lt; and &gt;) over these actions. <p> Such constraints restrict the legal sequences of action instances. We can think of an LCP as representing its set of linearizations: the sequences of action instances in A consistent with the ordering constraints. We do not discuss here algorithms for producing LCPs, but refer to <ref> [13] </ref> for a survey and to some of the key developments presented in [10, 11]. 3 Goal Decomposition The aim of goal decomposition is similar to that of abstraction: to solve smaller MDPs by ignoring variables. <p> The details of the various algorithms described in this paper are based on several assumptions. We assume that our problems are goal-oriented; that is, we are only concerned with the achievement of propositions in the final state that 2 We refer to <ref> [13] </ref> for a discussion of the complexities introduced by the existence of conditional effects. To appear, Proc. 15th Intl. <p> LCP extraction for proceeds as follows. We first generate the MLEP for given initial state I to goal state G. 4 We then call a least-commitment planning algorithm, such as Weld's <ref> [13] </ref> UCPOP, with initial state I and goals consisting of those reward propositions in the current R set satisfied by G.
Reference: [14] <author> Q. Yang, D. S. Nau, and J. Hendler. </author> <title> Merging seperately generated plans in limited domains. </title> <journal> Comp. Intel., </journal> <volume> 8(4), </volume> <year> 1992. </year>
Reference-contexts: We note that it may be possible to use and modify existing plan merging techniques (e.g., see <ref> [14] </ref>) in this re gard. However, strong commitment to earlier (higher priority) plans remains essential if plans prove unmergeable. 4 Concluding Remarks We have suggested a method of goal decomposition for MDPs based on abstraction and least-commitment planning.
References-found: 14

