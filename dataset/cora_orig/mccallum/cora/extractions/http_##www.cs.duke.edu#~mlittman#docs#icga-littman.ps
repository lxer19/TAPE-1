URL: http://www.cs.duke.edu/~mlittman/docs/icga-littman.ps
Refering-URL: 
Root-URL: 
Title: Adaptation in constant utility non-stationary environments  
Author: Michael L. Littman David H. Ackley 
Date: December 20, 1995  
Address: Morristown, NJ 07960  
Affiliation: Cognitive Science Research Group Bell Communications Research  
Abstract: Environments that vary over time present a fundamental problem to adaptive systems. Although in the worst case there is no hope of effective adaptation, some forms environmental variability do provide adaptive opportunities. We consider a broad class of non-stationary environments, those which combine a variable result function with an invariant utility function, and demonstrate via simulation that an adaptive strategy employing both evolution and learning can tolerate a much higher rate of environmental variation than an evolution-only strategy. We suggest that in many cases where stability has previously been assumed, the constant utility non-stationary environment may in fact be a more powerful viewpoint.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Ackley, D.H. & Littman, </author> <title> M.L. (1990). Learning from natural selection in an artificial environment. </title> <editor> In M. Caudhill, (ed.) </editor> <booktitle> Proceedings of the International Joint Conference on Neural Networks, IJCNN-90-WASH-DC, </booktitle> <volume> Volume I, </volume> <pages> 189-193, </pages> <publisher> Lawrence Erlbaum Associates: </publisher> <address> Hillsdale, NJ. </address>
Reference-contexts: In our scenario, the only information available is the lifetime average value of the utility function (i.e., the fitness), and that information is not available until end of the organism's lifespan, rendering moot the possibility of using it directly for learning. The essential idea of evolutionary reinforcement learning (ERL) <ref> [3, 1] </ref> is that an evolutionary algorithm can supply learning with such an oracle. We can include in the genetic material a representation of a goodness function, and let learning proceed during an organism's lifetime under the assumption that the goodness function is an accurate representation of the utility function. <p> Let u fl = 0. At time 1 t n: 1. (Situation) Pick random v s ; compute output probabilities v o (i) = 1= 1 + e (v s (i)w ii w 0i ) : 2. (Action) Given a uniform random variable ~ 2 <ref> [0; 1] </ref>, v a (i) = 1 if v o (i) ~ and 0 otherwise. 3. (Result) Compute result vector v r (i) = r (R (2i1);R (2i)) v g = v r go to 4, else go to 5. 4. (Reward) Update weights and biases: w ji = 10 (v
Reference: [2] <author> Ackley, D.H. & Littman, </author> <title> M.L. (1990). Generalization and scaling in reinforcement learning. </title> <editor> In D. Touretzky, (ed.) </editor> <booktitle> Advances in Neural Information Processing Systems - 2, </booktitle> <pages> 550-557, </pages> <publisher> Morgan Kaufmann: </publisher> <address> San Mateo, CA. </address>
Reference-contexts: Here, the GENESIS genetic algorithm system [9] provides the evolutionary component, and a simplified form of the CRBP (complementary reinforcement back-propagation) algorithm <ref> [2] </ref> provides the learning component.
Reference: [3] <author> Ackley, D.H. & Littman, </author> <title> M.L. (1991, in press). Interactions between evolution and learning. In C.G. </title> <editor> Langton, (ed.) </editor> <booktitle> Artificial Life II, </booktitle> <publisher> Morgan Kaufmann: </publisher> <address> San Mateo, CA. </address>
Reference-contexts: In our scenario, the only information available is the lifetime average value of the utility function (i.e., the fitness), and that information is not available until end of the organism's lifespan, rendering moot the possibility of using it directly for learning. The essential idea of evolutionary reinforcement learning (ERL) <ref> [3, 1] </ref> is that an evolutionary algorithm can supply learning with such an oracle. We can include in the genetic material a representation of a goodness function, and let learning proceed during an organism's lifetime under the assumption that the goodness function is an accurate representation of the utility function.
Reference: [4] <author> Baldwin, J. M. </author> <title> (1896). A new factor in evolution. </title> <journal> American Naturalist, </journal> <volume> 30: </volume> <pages> 441-451, 536-553. </pages>
Reference: [5] <author> Belew, R. </author> <year> (1989). </year> <title> Evolution, learning and culture: Computational metaphors for adaptive algorithms. </title> <institution> University of California at San Diego, </institution> <type> CSE Technical report CS89-156, </type> <address> La Jolla, CA. </address>
Reference-contexts: One might have expected that learning would be a drawback in that case, since each generation would waste time relearning the same action function. Actually, learning can serve as a "scout" to help evolution find good directions in which to evolve <ref> [10, 15, 5, 7] </ref>. Such "Bald-win effects" may account for evolutionary reinforcement learning's edge in the stationary environment. 3 Analysis At least in this abstract and simplified case, we have seen that evolution combined with learning can outperform evolution alone across a wide range of environmental variation rates.
Reference: [6] <author> DeJong, K.A. </author> <year> (1980). </year> <title> Adaptive system design: a genetic approach. </title> <journal> IEEE Trans. Syst., Man, and Cyber., </journal> <volume> SMC-10(9), </volume> <pages> 566-574. </pages>
Reference-contexts: As a consequence, if averaged over many environmental changes, the fitness produced by any single mapping will be at the chance level. The adaptive challenge is to produce organisms that yield high online fitness (i.e., fitness averaged over successive generations of organisms, see <ref> [6] </ref>), in the face of the changing environment. Although this scenario is familiar enough to genetic algorithm researchers in some ways, it also has certain unusual aspects that are central to the present endeavor.
Reference: [7] <author> Fontanari, J.F., & Meir, R. </author> <year> (1990). </year> <title> The effect of learning on the evolution of asexual populations. </title> <booktitle> Complex Systems 4, </booktitle> <pages> 401-414. </pages>
Reference-contexts: One might have expected that learning would be a drawback in that case, since each generation would waste time relearning the same action function. Actually, learning can serve as a "scout" to help evolution find good directions in which to evolve <ref> [10, 15, 5, 7] </ref>. Such "Bald-win effects" may account for evolutionary reinforcement learning's edge in the stationary environment. 3 Analysis At least in this abstract and simplified case, we have seen that evolution combined with learning can outperform evolution alone across a wide range of environmental variation rates. <p> In Hinton & Nowlan's model [10], learning guides an evolutionary search toward a well-hidden maximum. Smith [15] points out that learning can also help compensate for the disruptive effects of sexual recombination. Fontanari & Meir <ref> [7] </ref> provide simulations and analytic results of a model that shows how learning allows an evolutionary search to function successfully under much higher mutation rates. In Miller & Todd's model [12], learning speeds evolutionary search given noisy environmental cues.
Reference: [8] <author> Goldberg, D. </author> <year> (1989). </year> <title> Genetic algorithms in search, optimization, and machine learning. </title> <publisher> Addison-Wesley, </publisher> <address> MA. </address>
Reference-contexts: such a view can gain advantage | even though the environment will always have some surprises in store | by finding, representing, and exploiting those component regularities. 1.1 Constant utility In this paper we demonstrate one way this can occur within the context of evolutionary adaptation via the genetic algorithm <ref> [11, 8] </ref>. Consider this scenario in which an organism is faced with a series of interactions with an environment: In each interaction, the environment presents the organism with a situation, and then the organism responds with an action, and then the environment responds with a result.
Reference: [9] <author> Grefenstette, J.J. </author> <year> (1987). </year> <title> A user's guide to GENESIS. (Available with the GENESIS system via electronic mail to gref@NRL-AIC.ARPA). </title> <type> 13 </type>
Reference-contexts: Here, the GENESIS genetic algorithm system <ref> [9] </ref> provides the evolutionary component, and a simplified form of the CRBP (complementary reinforcement back-propagation) algorithm [2] provides the learning component. <p> We adopted the default values for all the parameters in the GENESIS system (population size = 50, crossover rate = 0.6, mutation rate = 0.001, generation gap = 1.0, scaling window = 5, elitist strategy; see <ref> [9] </ref>), except for the "total trials" parameter (raised to 2; 500), the "structure length" parameter (raised to 72 bits), and the use of the "a" option (since fitness is stochastic). <p> The genetic representation of the initial action function a 0 occupies 64 bits, representing the 16 weights and biases in a range of 4, scaled into four bits each and gray-coded (see <ref> [9] </ref>). The reinforcement function compares each result vector to a genetically specified 8 bit goodness vector v g , returns 6 evolution-learning. success if and only if they match exactly. 1 The overall length of the genetic material is 64 + 8 = 72 bits.
Reference: [10] <author> Hinton, G.E. & Nowlan, S.J. </author> <year> (1987). </year> <title> How learning can guide evolution. </title> <journal> Complex Systems, </journal> <volume> 1, </volume> <pages> 495-502. </pages>
Reference-contexts: One might have expected that learning would be a drawback in that case, since each generation would waste time relearning the same action function. Actually, learning can serve as a "scout" to help evolution find good directions in which to evolve <ref> [10, 15, 5, 7] </ref>. Such "Bald-win effects" may account for evolutionary reinforcement learning's edge in the stationary environment. 3 Analysis At least in this abstract and simplified case, we have seen that evolution combined with learning can outperform evolution alone across a wide range of environmental variation rates. <p> In these models, according to various criteria, learning provides an improvement compared to evolution alone. In Hinton & Nowlan's model <ref> [10] </ref>, learning guides an evolutionary search toward a well-hidden maximum. Smith [15] points out that learning can also help compensate for the disruptive effects of sexual recombination.
Reference: [11] <author> Holland, J.H. </author> <year> (1975). </year> <title> Adaptation in Natural and Artificial Systems. </title> <publisher> University of Michigan Press, </publisher> <address> Ann Arbor, MI. </address>
Reference-contexts: such a view can gain advantage | even though the environment will always have some surprises in store | by finding, representing, and exploiting those component regularities. 1.1 Constant utility In this paper we demonstrate one way this can occur within the context of evolutionary adaptation via the genetic algorithm <ref> [11, 8] </ref>. Consider this scenario in which an organism is faced with a series of interactions with an environment: In each interaction, the environment presents the organism with a situation, and then the organism responds with an action, and then the environment responds with a result.
Reference: [12] <author> Miller, G.F. & Todd, P.M. </author> <year> (1990). </year> <title> Exploring adaptive agency I: Theory and methods for simulating the evolution of learning. </title> <editor> In D.S. Touretzky, J.L. Elman, T.J. Sejnowski, & G.E. Hinton (Eds.), </editor> <booktitle> Proceedings of the 1990 Connectionist Models Summer School. </booktitle> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann, </publisher> <pages> 65-80. </pages>
Reference-contexts: Figure 4 displays the data from the lower ff values in the form of fitness ratios of the two strategies, illustrating the relative advantage provided by learning. As in other studies of evolution and learning (e.g., <ref> [12] </ref>), we observe an "inverted-U"-shape relating the independent parameter and the advantage 8 lowest six tested values of ff. of learning. In a stationary environment, at the left of the graph, both algorithms perform almost equally well. <p> Fontanari & Meir [7] provide simulations and analytic results of a model that shows how learning allows an evolutionary search to function successfully under much higher mutation rates. In Miller & Todd's model <ref> [12] </ref>, learning speeds evolutionary search given noisy environmental cues. In the present model, the addition of learning allows evolution to track a basically deterministic but rapidly changing environment.
Reference: [13] <author> Rumelhart, D.E., Hinton, G.E., & Williams, R.J. </author> <year> (1986). </year> <title> Learning internal representations by error propagation. Chapter 8 of D.E. </title> <editor> Rumelhart & J.L. McClelland (Eds.) </editor> <booktitle> Parallel Distributed Processing: Explorations in the microstructures of cognition. Volume 1: Foundations. </booktitle>
Reference-contexts: CRBP performs reinforcement learning using a back-propagation-style neural network <ref> [13] </ref>, and a reinforcement function that returns a success or failure signal for each generated output. In this simulation, we used the simplest network | containing 8 input-output weights and 8 output biases | sufficient to represent all possibly optimal action functions.
Reference: [14] <author> Schull, J. </author> <year> (1990). </year> <title> Are species intelligent? Behavioral and Brain Sciences, </title> <booktitle> 13:1, </booktitle> <pages> 61-73. </pages>
Reference: [15] <author> Smith, J.M. </author> <year> (1987). </year> <title> When learning guides evolution. </title> <journal> Nature, </journal> <volume> 32, </volume> <pages> 761-762. </pages>
Reference-contexts: One might have expected that learning would be a drawback in that case, since each generation would waste time relearning the same action function. Actually, learning can serve as a "scout" to help evolution find good directions in which to evolve <ref> [10, 15, 5, 7] </ref>. Such "Bald-win effects" may account for evolutionary reinforcement learning's edge in the stationary environment. 3 Analysis At least in this abstract and simplified case, we have seen that evolution combined with learning can outperform evolution alone across a wide range of environmental variation rates. <p> In these models, according to various criteria, learning provides an improvement compared to evolution alone. In Hinton & Nowlan's model [10], learning guides an evolutionary search toward a well-hidden maximum. Smith <ref> [15] </ref> points out that learning can also help compensate for the disruptive effects of sexual recombination. Fontanari & Meir [7] provide simulations and analytic results of a model that shows how learning allows an evolutionary search to function successfully under much higher mutation rates.
References-found: 15

