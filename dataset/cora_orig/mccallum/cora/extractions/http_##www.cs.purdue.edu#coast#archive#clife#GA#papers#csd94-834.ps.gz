URL: http://www.cs.purdue.edu/coast/archive/clife/GA/papers/csd94-834.ps.gz
Refering-URL: http://www.cs.purdue.edu/coast/archive/clife/GA/papers/
Root-URL: http://www.cs.purdue.edu
Title: Stochastic Hillclimbing as a Baseline Method for Evaluating Genetic Algorithms  
Author: Ari Juels Martin Wattenberg 
Date: September 28, 1994  
Abstract: We investigate the effectiveness of stochastic hillclimbing as a baseline for evaluating the performance of genetic algorithms (GAs) as combinatorial function optimizers. In particular, we address four problems to which GAs have been applied in the literature: the maximum cut problem, Koza's 11-multiplexer problem, MDAP (the Multiprocessor Document Allocation Problem), and the jobshop problem. We demonstrate that simple stochastic hillclimbing methods are able to achieve results comparable or superior to those obtained by the GAs designed to address these four problems. We further illustrate, in the case of the jobshop problem, how insights obtained in the formulation of a stochastic hillclimbing algorithm can lead to improvements in the encoding used by a GA. fl Department of Computer Science, University of California at Berkeley. Supported by a NASA Graduate Fellowship. This paper was written while the author was a visiting researcher at the Ecole Normale Superieure-rue d'Ulm, Groupe de BioInformatique, France. E-mail: juels@cs.berkeley.edu y Department of Mathematics, University of California at Berkeley. Supported by an NDSEG Graduate Fellowship. E-mail: wattenbe@math.berkeley.edu 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> D. Ackley. </author> <title> A Connectionist Machine for Genetic Hillclimbing. </title> <publisher> Kluwer Academic Publishers, </publisher> <year> 1987. </year>
Reference-contexts: A number of researchers in the GA community have already addressed the issue of how various versions of hillclimbing on the space of bitstrings, f0; 1g n , compare with GAs. An early effort in this direction is to be found in the thesis of Ackley <ref> [1] </ref>, which considers several varieties of SH on bitstrings in comparison with a method called "stochastic iterated genetic hillclimbing" (SIGH). Ackley performs his experiments on a suite of test functions of his own devising and a collection of instances of the minimum-cut graph partition problem.
Reference: [2] <author> D. Applegate and W. Cook. </author> <title> A computational study of the job-shop problem. </title> <journal> ORSA Journal of Computing, </journal> <volume> 3(2), </volume> <year> 1991. </year>
Reference-contexts: The 6x6 instance is now known to have an optimal makespan of 55. This is very easy to achieve. While the optimum value for the 10x10 problem is known to be 930, this is a difficult problem which remained unsolved for over 20 years <ref> [2] </ref>. A great deal of research has also been invested in the similarly challenging 20x5 problem, for which an optimal value of 1165 has been achieved, and a lower bound of 1164 [6]. A number of papers have considered the application of GAs to scheduling problems.
Reference: [3] <author> T. </author> <title> Back. The interaction of mutation rate, selection, and self-adaptation within a genetic algorithm. </title> <editor> In R. Manner and B. Manderick, editors, </editor> <booktitle> Parallel Problem Solving from Nature 2, </booktitle> <pages> pages 85-94. </pages> <publisher> Elsevier, </publisher> <year> 1992. </year>
Reference-contexts: For this probability, referred to generally as the crossover rate, a value of 0.6 is fairly standard in the literature. It is, for instance, the default value proposed in Greffenstette's GENESIS software package [16]. In accordance with results in <ref> [3] </ref> and [26] regarding the optimal mutation rate, we set this parameter to 1 n . More precisely, the number of mutations performed on a given permutation in a single iteration is binomial with parameter p = 1 n .
Reference: [4] <author> E. Balas. </author> <title> Machine sequencing via disjunctive graphs: An implicit enumeration algorithm. </title> <journal> Operations research, </journal> <volume> 17 </volume> <pages> 941-957, </pages> <year> 1969. </year>
Reference-contexts: It is a notoriously difficult NP-complete problem [13] that is hard to solve even for small instances. A great deal of effort over the course of thirty years has gone into finding efficient approximation algorithms for it. See, for example, <ref> [4, 5, 7, 24, 6, 28, 9, 27] </ref>. In this problem, a collection of J jobs are to be scheduled on M machines (or processors), each of which can process only one task at a time. Each job is a list of M tasks which must be performed in order.
Reference: [5] <author> J. Barker and G. McMahon. </author> <title> Scheduling the general jobshop. </title> <journal> Management Science, </journal> <volume> 31(5) </volume> <pages> 594-598, </pages> <year> 1985. </year>
Reference-contexts: It is a notoriously difficult NP-complete problem [13] that is hard to solve even for small instances. A great deal of effort over the course of thirty years has gone into finding efficient approximation algorithms for it. See, for example, <ref> [4, 5, 7, 24, 6, 28, 9, 27] </ref>. In this problem, a collection of J jobs are to be scheduled on M machines (or processors), each of which can process only one task at a time. Each job is a list of M tasks which must be performed in order.
Reference: [6] <author> J. Carlier and E. Pinson. </author> <title> An algorithm for solving the jobshop problem. </title> <booktitle> Management Science, </booktitle> <address> 35:(2):164-176, </address> <year> 1989. </year>
Reference-contexts: It is a notoriously difficult NP-complete problem [13] that is hard to solve even for small instances. A great deal of effort over the course of thirty years has gone into finding efficient approximation algorithms for it. See, for example, <ref> [4, 5, 7, 24, 6, 28, 9, 27] </ref>. In this problem, a collection of J jobs are to be scheduled on M machines (or processors), each of which can process only one task at a time. Each job is a list of M tasks which must be performed in order. <p> A great deal of research has also been invested in the similarly challenging 20x5 problem, for which an optimal value of 1165 has been achieved, and a lower bound of 1164 <ref> [6] </ref>. A number of papers have considered the application of GAs to scheduling problems. In particular, Nakano and Yamada [28], Davidor et al. [7], and Fang et al. [9] have described 15 GAs designed to address the three benchmark instances for the jobshop problem.
Reference: [7] <author> Y. Davidor, T. Yamada, and R. Nakano. </author> <title> The ECOlogical framework II: Improving GA performance at virtually zero cost. </title> <editor> In Forrest, editor, </editor> <booktitle> Proceedings of the Fifth International Conference on Genetic Algorithms, </booktitle> <pages> pages 171-176, </pages> <address> San Mateo, CA, 1993. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: It is a notoriously difficult NP-complete problem [13] that is hard to solve even for small instances. A great deal of effort over the course of thirty years has gone into finding efficient approximation algorithms for it. See, for example, <ref> [4, 5, 7, 24, 6, 28, 9, 27] </ref>. In this problem, a collection of J jobs are to be scheduled on M machines (or processors), each of which can process only one task at a time. Each job is a list of M tasks which must be performed in order. <p> A number of papers have considered the application of GAs to scheduling problems. In particular, Nakano and Yamada [28], Davidor et al. <ref> [7] </ref>, and Fang et al. [9] have described 15 GAs designed to address the three benchmark instances for the jobshop problem. We compare our results with those obtained in Fang et al., one of the more recent of these articles.
Reference: [8] <author> L. Davis. </author> <title> Bit-climbing, representational bias, and test suite design. </title> <editor> In Belew and Booker, editors, </editor> <booktitle> Proceedings of the Fourth International Conference on Genetic Algorithms, </booktitle> <pages> pages 18-23, </pages> <address> San Mateo, CA, 1991. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Ackley performs his experiments on a suite of test functions of his own devising and a collection of instances of the minimum-cut graph partition problem. Davis <ref> [8] </ref> makes the surprising discovery that hillclimbing on bitstrings outperforms standard GAs on 2 the widely referenced De Jong suite of functions [19], and proposes a modification of these functions to make them more difficult for the simpler algorithm.
Reference: [9] <author> H. Fang, P. Ross, and D. Corne. </author> <title> A promising genetic algorithm approach to job-shop scheduling, rescheduling, and open-shop scheduling problems. </title> <editor> In Forrest, editor, </editor> <booktitle> Proceedings of the Fifth International Conference on Genetic Algorithms, </booktitle> <address> San Mateo, CA, 1993. </address> <publisher> Morgan Kaufmann. </publisher> <pages> 19 </pages>
Reference-contexts: It is a notoriously difficult NP-complete problem [13] that is hard to solve even for small instances. A great deal of effort over the course of thirty years has gone into finding efficient approximation algorithms for it. See, for example, <ref> [4, 5, 7, 24, 6, 28, 9, 27] </ref>. In this problem, a collection of J jobs are to be scheduled on M machines (or processors), each of which can process only one task at a time. Each job is a list of M tasks which must be performed in order. <p> A number of papers have considered the application of GAs to scheduling problems. In particular, Nakano and Yamada [28], Davidor et al. [7], and Fang et al. <ref> [9] </ref> have described 15 GAs designed to address the three benchmark instances for the jobshop problem. We compare our results with those obtained in Fang et al., one of the more recent of these articles. <p> When a job is complete, it is removed from C. Fang et al. also develop a highly specialized GA for this problem in which they use a scheme of increasing mutation rates and a technique known as GVOT (Gene-Variance based Operator Targeting). For the details see <ref> [9] </ref>. The SH Algorithm In our SH algorithm for this problem, a schedule is encoded in the form of an ordering 1 ; 2 ; : : : ; JM of JM markers.
Reference: [10] <author> O. Frieder. </author> <type> Personal communication, </type> <year> 1993. </year>
Reference-contexts: Our results, however, were consistently inferior to those given by the authors of that article. This situation was aggravated by the fact that their code was unavailable for distribution due to a computer crash <ref> [10] </ref>. We found, however, that by implementing the mating and mutation operators in [30] in the context of a different formulation of the GA we were able to achieve results superior to those presented by the authors. <p> We present their results, indicated by "ga", with those obtained by our own GA, indicated by "GA", and with those of our SH algorithm, indicated 12 by "SH". The results for the GA in [30] are the average results over 5 trials <ref> [10] </ref>. The results for our GA are the average results over 100 runs on a population size of 30 for 1000 iterations. The results for the SH algorithm are the average of 100 runs, each executed for 30,000 iterations.
Reference: [11] <author> C. Fujiki. </author> <title> An evaluation of Holland's genetic operators applied to a program generator. </title> <type> Master's thesis, </type> <institution> University of Idaho, </institution> <year> 1986. </year>
Reference-contexts: These programs take the form of Lisp symbolic 6 expressions, called S-expressions. The idea of applying GAs to S-expressions rather than combinatorial structures is due originally to Fujiki and Dickinson [12] <ref> [11] </ref>, and was brought to prominence through the work of Koza [22]. The S-expressions in GP correspond to programs which a user seeks to adapt to perform some pre-specified task. The fitness of an S-expression may therefore be evaluated in terms of how effectively its corresponding program performs this task.
Reference: [12] <author> C. Fujiki and J. Dickinson. </author> <title> Using the genetic algorithm to generate Lisp source code to solve the prisoner's dilemma. </title> <editor> In J. Grefenstette, editor, </editor> <booktitle> Proceedings of the Second International Conference on Genetic Algorithms, </booktitle> <pages> pages 236-240, </pages> <address> San Mateo, CA, 1989. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: These programs take the form of Lisp symbolic 6 expressions, called S-expressions. The idea of applying GAs to S-expressions rather than combinatorial structures is due originally to Fujiki and Dickinson <ref> [12] </ref> [11], and was brought to prominence through the work of Koza [22]. The S-expressions in GP correspond to programs which a user seeks to adapt to perform some pre-specified task.
Reference: [13] <author> M. Garey and D. Johnson. </author> <title> Computers and Intractability. W.H. </title> <publisher> Freeman and Co., </publisher> <year> 1979. </year>
Reference-contexts: Maximum cut is NP-complete <ref> [13] </ref>. The GA Khuri et al. encode a partition for this problem in the form of a bitstring B = (x 1 ; x 2 ; : : : ; x n ), where n is the number of vertices in G. <p> This leads us to conceive a new neighborhood structure for jobshop which we shall employ in the following section to construct a new, more effective GA for the problem. The jobshop problem is widely studied in the field of management science. It is a notoriously difficult NP-complete problem <ref> [13] </ref> that is hard to solve even for small instances. A great deal of effort over the course of thirty years has gone into finding efficient approximation algorithms for it. See, for example, [4, 5, 7, 24, 6, 28, 9, 27].
Reference: [14] <author> D. Goldberg. </author> <title> Genetic Algorithms in Search, Optimization, and Machine Learning. </title> <publisher> Addison Wesley, </publisher> <year> 1989. </year>
Reference-contexts: function f : S ! R over a combinatorial space S, i.e., to find some state s 2 S for which f (s) is as large as possible. (The case in which f is to be minimized is clearly symmetrical.) For a detailed description of the algorithm see, for example, <ref> [14] </ref>, which constitutes a standard text on the subject. GAs belong to a larger class of methods known as black-box algorithms - i.e., algorithms which attempt to optimize a function using a strategy essentially independent of the problem at hand. <p> The GA applied to this encoding employs one-point crossover with a crossover rate of 0.6 and a mutation rate of 1 n , where n is the length of the bitstrings on which the algorithm operates. Selection follows a method known as linear-dynamical scaling (see <ref> [14] </ref> pp. 123-4). Except in the case of the mutation rate, for which the authors adhere to a different standard, all parameters in this GA are identical to those in Greffenstette's GENESIS software package [16].
Reference: [15] <author> D. Goldberg and K. Deb. </author> <title> A comparative analysis of selection schemes used in genetic algorithms. </title> <booktitle> In Foundations of Genetic Algorithms 2, </booktitle> <pages> pages 69-93, </pages> <address> San Mateo, CA, 1991. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: The GA proposed in [30] employs unscaled fitness-proportionate selection. The performance of this variety of selection on function optimization tasks is often found to be somewhat poor, so we chose instead to use binary stochastic tournament selection <ref> [15] </ref>. In this type of selection, P pairs, where P is the size of the population, are selected uniformly at random with replacement from the population. A new population of size P is constituted by selecting the fitter permutation from each of these pairs (with ties broken randomly).
Reference: [16] <author> J. Grenfenstette. </author> <title> A User's Guide to GENESIS. </title> <booktitle> Navy Center for Applied Research in Artificial Intelligence, </booktitle> <year> 1987. </year>
Reference-contexts: Selection follows a method known as linear-dynamical scaling (see [14] pp. 123-4). Except in the case of the mutation rate, for which the authors adhere to a different standard, all parameters in this GA are identical to those in Greffenstette's GENESIS software package <ref> [16] </ref>. The SH Algorithm In this problem, changing a single bit corresponds to the combi-natorially natural operation of moving a single vertex from one set in the partition to the other. <p> For this probability, referred to generally as the crossover rate, a value of 0.6 is fairly standard in the literature. It is, for instance, the default value proposed in Greffenstette's GENESIS software package <ref> [16] </ref>. In accordance with results in [3] and [26] regarding the optimal mutation rate, we set this parameter to 1 n . More precisely, the number of mutations performed on a given permutation in a single iteration is binomial with parameter p = 1 n .
Reference: [17] <author> D. Johnson, C. Aragon, L. McGeoch, and C. Schevon. </author> <title> Optimization by simulated annealing: An experimental evaluation; part I, graph partitioning. </title> <journal> Operations Research, </journal> <volume> 37(6), </volume> <year> 1989. </year>
Reference-contexts: This apparently trifling but in fact significant type of variation often goes unconsidered in implementations of hillclimbing, even in detailed and comprehensive studies of function optimization heuristics such as <ref> [17] </ref> and [18]. In the formulation in this paper, the hillclimbing algorithm is able to explore "level surfaces," i.e., regions in the search space in which the fitnesses of neighboring states are equal. In practice, this appears to lead to more rapid discovery of high fitness solutions.
Reference: [18] <author> D. Johnson, C. Aragon, L. McGeoch, and C. Schevon. </author> <title> Optimization by simulated annealing: An experimental evaluation; part II, graph coloring and number partitioning. </title> <journal> Operations Research, </journal> <volume> 39(3), </volume> <year> 1991. </year>
Reference-contexts: This apparently trifling but in fact significant type of variation often goes unconsidered in implementations of hillclimbing, even in detailed and comprehensive studies of function optimization heuristics such as [17] and <ref> [18] </ref>. In the formulation in this paper, the hillclimbing algorithm is able to explore "level surfaces," i.e., regions in the search space in which the fitnesses of neighboring states are equal. In practice, this appears to lead to more rapid discovery of high fitness solutions.
Reference: [19] <author> K. De Jong. </author> <title> An Analysis of the Behavior of a Class of Genetic Adaptive Systems. </title> <type> PhD thesis, </type> <institution> University of Michigan, </institution> <year> 1975. </year>
Reference-contexts: Ackley performs his experiments on a suite of test functions of his own devising and a collection of instances of the minimum-cut graph partition problem. Davis [8] makes the surprising discovery that hillclimbing on bitstrings outperforms standard GAs on 2 the widely referenced De Jong suite of functions <ref> [19] </ref>, and proposes a modification of these functions to make them more difficult for the simpler algorithm. Wilson [31] points out the existence of some very elementary functions on which GAs can outperform a particular type of hillclimbing called steepest-ascent.
Reference: [20] <author> S. Khuri, T. Back, and J. Heitkotter. </author> <title> An evolutionary approach to combinatorial optimization problems. </title> <booktitle> In Proceedings of CSC 1994, </booktitle> <year> 1994. </year>
Reference-contexts: evaluation of the fitness function generally constitutes the most substantial portion of the execution time of the optimization algorithm, and accords with standard practice in the GA community. 3.2 Maximum Cut In order to demonstrate the effectiveness of the GA as a general combinatorial optimization method, Khuri, Back, and Heitkotter <ref> [20] </ref> apply the algorithm to several NP-complete problems. We compare their results on one of these, the maximum cut problem, with results obtained by an SH approach. <p> The authors construct this graph as a chain of four-vertex components in such a way that it is easy to demonstrate a unique, optimal cut of weight 1077. For details, see <ref> [20] </ref>. Khuri et al. run a GA with a population size of 50 for 1000 iterations, so that their GA executes a total 50,000 function evaluations. They perform 100 experiments. We compare their results with those of our SH algorithm run for 50,000 iterations, likewise over 100 experiments.
Reference: [21] <author> J. Koza. </author> <title> Foundations of Genetic Algorithms, chapter A Hierarchical Approach to Learning the Boolean Multiplexer Function, </title> <address> pages 171-192. </address> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, CA, </address> <year> 1991. </year>
Reference-contexts: The fitness of an S-expression may therefore be evaluated in terms of how effectively its corresponding program performs this task. Details on GP, an increasingly common GA application, and on the 11-multiplexer problem which we address in this section, may be found, for example, in [22] <ref> [21] </ref> [23]. The boolean 11-multiplexer problem entails the generation of a program to perform the following task. <p> In <ref> [21] </ref>, where Koza performs a series of 21 runs with a slightly different selection scheme from the one described above, he finds that the average number of function evaluations required to find a correct S-expression is 46,667.
Reference: [22] <editor> J. Koza. </editor> <booktitle> Genetic Programming. </booktitle> <publisher> MIT Press, </publisher> <address> Cambridge, Massachusetts, </address> <year> 1991. </year>
Reference-contexts: These programs take the form of Lisp symbolic 6 expressions, called S-expressions. The idea of applying GAs to S-expressions rather than combinatorial structures is due originally to Fujiki and Dickinson [12] [11], and was brought to prominence through the work of Koza <ref> [22] </ref>. The S-expressions in GP correspond to programs which a user seeks to adapt to perform some pre-specified task. The fitness of an S-expression may therefore be evaluated in terms of how effectively its corresponding program performs this task. <p> The fitness of an S-expression may therefore be evaluated in terms of how effectively its corresponding program performs this task. Details on GP, an increasingly common GA application, and on the 11-multiplexer problem which we address in this section, may be found, for example, in <ref> [22] </ref> [21] [23]. The boolean 11-multiplexer problem entails the generation of a program to perform the following task.
Reference: [23] <author> J. Koza. </author> <title> The genetic programming paradigm: Breeding computer programs. In Branko Soucek and the IRIS Group, editors, Dynamic, </title> <booktitle> Genetic, and Chaotic Programming, </booktitle> <pages> pages 203-221. </pages> <publisher> John Wiley and Sons, Inc., </publisher> <year> 1992. </year> <month> 20 </month>
Reference-contexts: The fitness of an S-expression may therefore be evaluated in terms of how effectively its corresponding program performs this task. Details on GP, an increasingly common GA application, and on the 11-multiplexer problem which we address in this section, may be found, for example, in [22] [21] <ref> [23] </ref>. The boolean 11-multiplexer problem entails the generation of a program to perform the following task. <p> Experimental Results In the implementation described in <ref> [23] </ref>, Koza initializes the GA with a pool of 4000 expressions.
Reference: [24] <author> G. McMahon and M. Florian. </author> <title> On scheduling with ready times and due dates to minimize maximum lateness. </title> <journal> Operations research, </journal> <volume> 23(3) </volume> <pages> 475-482, </pages> <year> 1975. </year>
Reference-contexts: It is a notoriously difficult NP-complete problem [13] that is hard to solve even for small instances. A great deal of effort over the course of thirty years has gone into finding efficient approximation algorithms for it. See, for example, <ref> [4, 5, 7, 24, 6, 28, 9, 27] </ref>. In this problem, a collection of J jobs are to be scheduled on M machines (or processors), each of which can process only one task at a time. Each job is a list of M tasks which must be performed in order.
Reference: [25] <author> M. Mitchell, J. Holland, and S. Forrest. </author> <title> When will a genetic algorithm outperform hill-climbing? In J.D. </title> <editor> Cowen, G. Tesauro, and J. Alspector, editors, </editor> <booktitle> Advances in Neural Information Processing Systems 6, </booktitle> <address> San Mateo, CA, 1994. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Wilson [31] points out the existence of some very elementary functions on which GAs can outperform a particular type of hillclimbing called steepest-ascent. Mitchell and Holland <ref> [25] </ref> present an extensive analysis of the relative performances of bitstring-based hillclimbing and GAs on a synthetic function known as the "Royal Road". Our investigations in this paper differ in two important respects from these previous ones.
Reference: [26] <author> H. Muhlenbein. </author> <title> How genetic algorithms really work: </title> <editor> I. mutation and hillclimbing. In R. Manner and B. Manderick, editors, </editor> <booktitle> Parallel Problem Solving from Nature 2, </booktitle> <pages> pages 15-25. </pages> <publisher> Elsevier, </publisher> <year> 1992. </year>
Reference-contexts: For this probability, referred to generally as the crossover rate, a value of 0.6 is fairly standard in the literature. It is, for instance, the default value proposed in Greffenstette's GENESIS software package [16]. In accordance with results in [3] and <ref> [26] </ref> regarding the optimal mutation rate, we set this parameter to 1 n . More precisely, the number of mutations performed on a given permutation in a single iteration is binomial with parameter p = 1 n .
Reference: [27] <author> J. Muth and G. Thompson. </author> <title> Industrial Scheduling. </title> <publisher> Prentice Hall, </publisher> <address> Englewood Cliffs, New Jersey, </address> <year> 1963. </year>
Reference-contexts: It is a notoriously difficult NP-complete problem [13] that is hard to solve even for small instances. A great deal of effort over the course of thirty years has gone into finding efficient approximation algorithms for it. See, for example, <ref> [4, 5, 7, 24, 6, 28, 9, 27] </ref>. In this problem, a collection of J jobs are to be scheduled on M machines (or processors), each of which can process only one task at a time. Each job is a list of M tasks which must be performed in order. <p> Every task has a fixed (integer) processing time. The problem is to schedule the jobs on the machines so that all jobs are completed in the shortest overall time. This time is referred to as the makespan. Three instances formulated in <ref> [27] </ref> constitute a standard benchmark for this problem: a 6 job, 6 machine instance, a 10 job, 10 machine instance, and a 20 job, 5 machine instance. The 6x6 instance is now known to have an optimal makespan of 55. This is very easy to achieve.
Reference: [28] <author> R. Nakano and T. Yamada. </author> <title> Conventional genetic algorithm for job shop problems. </title> <editor> In Belew and Booker, editors, </editor> <booktitle> Proceedings of the Fourth International Conference on Genetic Algorithms, </booktitle> <pages> pages 474-479, </pages> <address> San Mateo, CA, 1991. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: It is a notoriously difficult NP-complete problem [13] that is hard to solve even for small instances. A great deal of effort over the course of thirty years has gone into finding efficient approximation algorithms for it. See, for example, <ref> [4, 5, 7, 24, 6, 28, 9, 27] </ref>. In this problem, a collection of J jobs are to be scheduled on M machines (or processors), each of which can process only one task at a time. Each job is a list of M tasks which must be performed in order. <p> A number of papers have considered the application of GAs to scheduling problems. In particular, Nakano and Yamada <ref> [28] </ref>, Davidor et al. [7], and Fang et al. [9] have described 15 GAs designed to address the three benchmark instances for the jobshop problem. We compare our results with those obtained in Fang et al., one of the more recent of these articles.
Reference: [29] <author> C. Shaefer and S. Smith. </author> <title> The Argot strategy II combinatorial optimizations. </title> <type> Technical Report RL90-1, </type> <institution> Thinking Machines, </institution> <year> 1990. </year>
Reference: [30] <author> H. Siegelmann and O. Frieder. </author> <title> Document allocation in multiprocessor information retrieval systems. </title> <editor> In N. Adam and N. Bhargava, editors, </editor> <booktitle> Lecture note series in Computer Science: Advanced Database Systems. </booktitle> <publisher> Springer Verlag, </publisher> <year> 1994. </year>
Reference-contexts: In consequence of this observation, a simple inductive proof suffices to show that the search space contains no strict local minima. 3.4 MDAP The authors of <ref> [30] </ref> formulate an NP-complete document allocation problem which they refer to as MDAP (Multiprocessor Document Allocation Problem). Their efforts to solve this problem using a GA, which they show to be superior to certain more traditional methods, have received considerable attention. <p> In this section we will perform two experiments comparing the performance of a GA developed in that paper to an SH algorithm for MDAP. We will conduct the first of these experiments on a suite of problems drawn from <ref> [30] </ref>, and the second on a new suite of problems developed for the purposes of the current paper. The inputs to the MDAP problem are a multiprocessor architecture, a collection of documents, and a collection of subsets of these documents, or clusters. <p> Significant overlap among clusters can result in complex tradeoffs in the minimization of cluster radii. MDAP is NP-complete for general architectures <ref> [30] </ref>. The GA In the GA developed in [30] specifically for this problem, feasible solutions take the form of permutations. The permutation : f1 : : :ng ! f1 : : : ng specifies the document allocation A (d i ) = (i) (mod jXj). <p> Significant overlap among clusters can result in complex tradeoffs in the minimization of cluster radii. MDAP is NP-complete for general architectures <ref> [30] </ref>. The GA In the GA developed in [30] specifically for this problem, feasible solutions take the form of permutations. The permutation : f1 : : :ng ! f1 : : : ng specifies the document allocation A (d i ) = (i) (mod jXj). This scheme ensures an even distribution of documents across processors. <p> For the purposes of the experiments to come in the latter half of this section, we attempted to recode the GA in <ref> [30] </ref> according to the description presented in the paper. Our results, however, were consistently inferior to those given by the authors of that article. This situation was aggravated by the fact that their code was unavailable for distribution due to a computer crash [10]. <p> Our results, however, were consistently inferior to those given by the authors of that article. This situation was aggravated by the fact that their code was unavailable for distribution due to a computer crash [10]. We found, however, that by implementing the mating and mutation operators in <ref> [30] </ref> in the context of a different formulation of the GA we were able to achieve results superior to those presented by the authors. Our GA includes, in order, the following phases: evaluation, elitist replacement, selection, crossover, and mutation. <p> Elitist replacement substitutes the fittest permutation from the evaluation phase of the previous iteration for the least fit permutation in the current 11 population (except, of course, in the first iteration, in which there is no replacement). The GA proposed in <ref> [30] </ref> employs unscaled fitness-proportionate selection. The performance of this variety of selection on function optimization tasks is often found to be somewhat poor, so we chose instead to use binary stochastic tournament selection [15]. <p> A neighbor is selected by transposing two elements chosen uniformly at random from the permutation. In other words, to select a neighbor t , a single mutation of the sort employed in the GA described above is applied. Four architectures are considered in <ref> [30] </ref>: a 1x16 mesh, a 2x8 mesh, a 4x4 mesh, and a hypercube of 16 processors. The authors consider two types of document distribution, an "even" distribution and a "(64, 8, 25, 50)" distribution. In the "even" distribution, 64 documents are assigned to 8 clusters, each of size 8. <p> As the other distribution considered in their paper, the "(64, 8, 25, 50)" in which half of the documents occupy one quarter of the clusters appears to have a similar triviality, we consider it sufficient just to examine the "even" distribution. The authors of <ref> [30] </ref> tabulate their average results for a GA on a population of size 30 executed for 1000 iterations. We present their results, indicated by "ga", with those obtained by our own GA, indicated by "GA", and with those of our SH algorithm, indicated 12 by "SH". <p> We present their results, indicated by "ga", with those obtained by our own GA, indicated by "GA", and with those of our SH algorithm, indicated 12 by "SH". The results for the GA in <ref> [30] </ref> are the average results over 5 trials [10]. The results for our GA are the average results over 100 runs on a population size of 30 for 1000 iterations. The results for the SH algorithm are the average of 100 runs, each executed for 30,000 iterations.
Reference: [31] <author> S. Wilson. </author> <title> GA-easy does not imply steepest-ascent optimizable. </title> <editor> In Belew and Booker, editors, </editor> <booktitle> Proceedings of the Fourth International Conference on Genetic Algorithms, </booktitle> <pages> pages 85-89, </pages> <address> San Mateo, CA, 1991. </address> <publisher> Morgan Kaufmann. </publisher> <pages> 21 </pages>
Reference-contexts: Davis [8] makes the surprising discovery that hillclimbing on bitstrings outperforms standard GAs on 2 the widely referenced De Jong suite of functions [19], and proposes a modification of these functions to make them more difficult for the simpler algorithm. Wilson <ref> [31] </ref> points out the existence of some very elementary functions on which GAs can outperform a particular type of hillclimbing called steepest-ascent. Mitchell and Holland [25] present an extensive analysis of the relative performances of bitstring-based hillclimbing and GAs on a synthetic function known as the "Royal Road".
References-found: 31

