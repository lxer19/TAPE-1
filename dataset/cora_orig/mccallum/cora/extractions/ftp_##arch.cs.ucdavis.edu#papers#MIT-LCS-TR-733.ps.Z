URL: ftp://arch.cs.ucdavis.edu/papers/MIT-LCS-TR-733.ps.Z
Refering-URL: http://arch.cs.ucdavis.edu/~chong/pubs.html
Root-URL: http://www.cs.ucdavis.edu
Title: Parallel Communication Mechanisms for Sparse, Irregular Applications  
Author: Frederic T. Chong 
Abstract-found: 0
Intro-found: 1
Reference: [A + 95] <author> R. Arpaci et al. </author> <title> Empirical evaluation of the Cray T3D: A compiler perspective. </title> <booktitle> In Proceedings of the 22nd Annual Symposium on Computer Architecture, </booktitle> <year> 1995. </year>
Reference-contexts: fi 8 Mesh 3200 16.0 62 352 40 [HKO + 94] Wisconsin T0 #200.0 none simulated N/A N/A 200 1461 40 [RPW96][Rei96] Wisconsin T1 #200.0 none simulated N/A N/A 200 401 40 [RPW96][Rei96] Cray T3D 150.0 4 fi 2 fi 2 Torus 2-proc clusters 4800 32.0 15 100 23 [T3D93] <ref> [A + 95] </ref> Cray T3E 300.0 4 fi 4 fi 2 Torus 19200 64.0 110 300-600 80 [Sco96b][Sco96a] SGI Origin 200.0 Hypercube 4-proc clusters 10800 54.0 60 150 61 [?] [Gal96] * projected, # simulated, latencies given in processor cycles. Table 6.1: Parameter estimates for various 32-processor multiprocessors.
Reference: [ABC + 95] <author> Anant Agarwal, Ricardo Bianchini, David Chaiken, Kirk Johnson, David Kranz, John Kubiatowicz, Beng-Hong Lim, Ken Mackenzie, and Donald Yeung. </author> <title> The MIT Alewife machine: Architecture and performance. </title> <booktitle> In Proc. 22nd Annual International Symposium on Computer Architecture, </booktitle> <month> June </month> <year> 1995. </year>
Reference-contexts: These parts generally communicate via the memory bus, although some architectures also use the cache and I/O buses. All the multiprocessors discussed in this thesis have this general organization. These multiprocessors include the MIT Alewife Multiprocessor <ref> [ABC + 95] </ref>, the Stanford DASH [LLG + 92], the CM5 [Thi93a], the Intel Paragon [Int91], the Cray T3D [T3D93], and many others. Because of its flexibility, the Alewife machine will be the focus of this thesis. All the machines will be discussed in detail in Chapter 3. <p> We refer the reader to [Thi93a] [T3D93] [Int91] for further information Table 3.1: Multiprocessors discussed in this thesis. 42 CHAPTER 3. PLATFORMS 43 on the other machines. 3.1 The Alewife Multiprocessor We conduct the majority of our experiments on the MIT Alewife multiprocessor <ref> [ABC + 95] </ref>, shown in Figure 3-2. The Alewife machine is ideal for our experiments because it efficiently supports distributed shared memory, message passing, and DMA (Direct Memory Access). <p> Chapter 4 Shared Memory versus Message Passing Distributed shared memory and message passing are two dominant communication mechanisms in parallel systems. Most machines incorporate either message passing [Thi93a] [Int91] [Che93] or shared memory [LT88] [LLG + 92] [BFKR92] [SGI94]. A few machines support both mechanisms <ref> [ABC + 95] </ref> [HKO + 94]. The precise benefit of shared memory versus message passing has remained an open question, largely due to the difficulty of finding experimental platforms supporting an apples to apples comparison and the difficulty of writing applications in both styles without bias. <p> This chapter addresses this open question using the MIT Alewife machine <ref> [ABC + 95] </ref> and sparse, irregular applications. Alewife integrates distributed, cache-coherent shared memory and message passing, providing an ideal platform for our experiments. <p> SENSITIVITY TO BANDWIDTH AND LATENCY 99 Machine Proc Topology Bisection Bandwidth Network Remote Local Refs (32 Processors) MHz Mbytes/s bytes/ Latency Miss Miss cycle Latency Latency MIT Alewife 20.0 4 fi 8 Mesh 360 18.0 15 50 11 <ref> [ABC + 95] </ref> TMC CM5 33.0 4-ary Fat-Tree 640 19.4 50 N/A 16 [Thi93a][L + 92] KSR-2 20.0 Ring 1000 50.0 ? 126 18 [BFKR92] [Dun92b] MIT J-Machine 12.5 4 fi 4 fi 2 Mesh 3200 256.0 7 N/A 7 [NDD93] MIT M-Machine #100.0 4 fi 4 fi 2 Mesh 12800
Reference: [ACM88] <author> Arvind, David E. Culler, and Gino K. Maa. </author> <title> Assessing the benefits of fine-grained parallelism in dataflow programs. </title> <booktitle> In Supercomputing `88. IEEE, </booktitle> <year> 1988. </year>
Reference-contexts: Many of these dependencies involve communication. This communication interspersed with computation makes ICCG our most challenging application. 4.3.1 ICCG with Message Passing As described in Chapter 2, ICCG is essentially a dataflow computation <ref> [ACM88] </ref>, and is easily implemented via active messages. From the example in Figure 2-2, let us assume that DAG node 0 is on processor 0 and node 4 is on processor 4. This arrangement is shown in Figure 4-7.
Reference: [AK89] <author> J. M. C. Aarden and K.-E. Karlsson. </author> <title> Preconditioned cg-type methods for solving the coupled system of fundamental semiconductor equations. BIT, </title> <address> 29:916937, </address> <year> 1989. </year>
Reference-contexts: The more fill that occurs, the more computation required to solve the system and often the less parallelism in that computation. In general, the larger the matrix A, the more severe the fill. To avoid fill, iterative techniques are often used [Gus79] [Mv77] [Axe85] <ref> [AK89] </ref>. Instead of computing the true factors of A, incomplete factors L 0 and U 0 are used. L 0 and U 0 are generally computed by factoring A, but discarding elements that introduce fill.
Reference: [APS93] <author> Fernando Alvarado, Alex Pothen, and Robert Schreiber. </author> <title> Highly parallel sparse triangular solution, pages 141157. </title> <publisher> Springer-Verlag, </publisher> <address> New York, </address> <year> 1993. </year> <note> The IMA Volumes in Mathematics and Its Applications, Volume 56. </note>
Reference-contexts: Element i of the right-hand-side vector, b, is also kept with DAG node i, as is element i of the result vector x. A.4 Partitioned Inverses Partitioned inverses is an alternative method of sparse triangular solution that has been shown to greatly enhance parallelism [AS93] <ref> [APS93] </ref> [CS95]. In general, the system Lx = b can be solved through the computation x = L 1 b. Unfortunately, as Figure A-2 shows, the calculation of L 1 can lead to fill, non-zeros that were not in the original sparse structure of L.
Reference: [AS93] <author> Fernando Alvarado and Robert Schreiber. </author> <title> Optimal parallel solution of sparse triangular systems. </title> <journal> SIAM Journal of Scientific and Statistical Computation, </journal> <volume> 14:446460, </volume> <year> 1993. </year>
Reference-contexts: Element i of the right-hand-side vector, b, is also kept with DAG node i, as is element i of the result vector x. A.4 Partitioned Inverses Partitioned inverses is an alternative method of sparse triangular solution that has been shown to greatly enhance parallelism <ref> [AS93] </ref> [APS93] [CS95]. In general, the system Lx = b can be solved through the computation x = L 1 b. Unfortunately, as Figure A-2 shows, the calculation of L 1 can lead to fill, non-zeros that were not in the original sparse structure of L. <p> Figure A-4 shows how we represent this calculation as a DAG. In general, Lx = b may be solved by computing: x = i i b where the L i 's are chosen to have transitively closed column subgraphs. We use the rp2 algorithm given in <ref> [AS93] </ref> to choose our partitions. Note that rp2 may also reorder L into other lower triangular matrices in an attempt to find better partitions. A.5 Substitution vs. Partitioned Inverses The key issue in choosing a technique is fill.
Reference: [Axe85] <author> O. Axelsson. </author> <title> A survey of preconditioned iterative methods for linear systems of algebraic equations. </title> <journal> BIT, 25:166187, 1985. </journal> <volume> 130 BIBLIOGRAPHY 131 </volume>
Reference-contexts: The more fill that occurs, the more computation required to solve the system and often the less parallelism in that computation. In general, the larger the matrix A, the more severe the fill. To avoid fill, iterative techniques are often used [Gus79] [Mv77] <ref> [Axe85] </ref> [AK89]. Instead of computing the true factors of A, incomplete factors L 0 and U 0 are used. L 0 and U 0 are generally computed by factoring A, but discarding elements that introduce fill.
Reference: [BB85] <author> M. J. Berger and S. H. Bokhari. </author> <title> A partitioning strategy for PDEs across multiprocessors. </title> <booktitle> In International Conference on Parallel Processing, </booktitle> <month> August </month> <year> 1985. </year>
Reference-contexts: We used a MESH2K as an input dataset, a 2000 node irregular mesh provided with the code. To minimize communication, nodes are partitioned into groups to minimize edges between groups. This partitioning is done with a recursive coordinate bisection (RCB) algorithm taken from <ref> [BB85] </ref>. Groups are then mapped to processors without considering network topology. Edges and faces are assigned to the processor with the most associated nodes. Ties are broken by assigning to the processor with the least edges or faces.
Reference: [BCL + 95] <author> Eric A. Brewer, Frederic T. Chong, Lok T. Liu, Shamik D. Sharma, and John Kubiatowicz. </author> <title> Remote queues: Exposing message queues for optimization and atomicity. </title> <booktitle> In 1995 Symposium on Parallel Architectures and Algorithms, </booktitle> <address> Santa Barbara, California, </address> <month> July </month> <year> 1995. </year>
Reference-contexts: Unfortunately, it is difficult to avoid programmer and system bias in such studies. Our study copes with bias in two ways. First, our conclusion is counter to our programmer bias. Our results favor shared memory even though this study evolved from research [CSBS95] <ref> [BCL + 95] </ref> [CS95] and researchers 1 with a clear message-passing bias. Second, we perform all experiments on the same hardware. The MIT Alewife multiprocessor provides a uniquely integrated 1 For example, one of the original developers of the CMMD message-passing library [Thi93b] on the CM5. CHAPTER 1. <p> We were able, however, to overcome the synchronization difficulties of shared memory by shifting from an owner-computes model of computation to a producer-computes model. This shift led us to propose a general communication model, Remote Queues (RQ) <ref> [BCL + 95] </ref>, which provides the se CHAPTER 1. INTRODUCTION 24 mantics of polling active messages on a broad range of platforms. By exposing and naming message queues, RQ integrates polling into complex environments that can include interrupts, DMA, multiprogramming, and global memory. <p> This chapter addresses this open question using the MIT Alewife machine [ABC + 95] and sparse, irregular applications. Alewife integrates distributed, cache-coherent shared memory and message passing, providing an ideal platform for our experiments. Although this study evolved from research [CSBS95] <ref> [BCL + 95] </ref> [CS95] and researchers 1 with a clear message-passing bias, our results point to shared memory as the most useful communication mechanism for a multiprocessor. <p> Second, our transformation from an owner-computes, active-message model to a producer-computes, shared-memory model is only practical when the active-message handler modifies very little data. In this chapter, we introduce a mechanism, called Remote Queues, which addresses these issues. Much of the material from this chapter is taken from <ref> [BCL + 95] </ref>. 5.1 Polling versus Interrupts Active messages come in two flavors, those received via interrupt and those received via polling. In Figure 5-1, the left side shows an active message received via interrupt. The receiving processor is computing along its main thread of computation. <p> Finally, these properties of integration and synthesis allow RQ to provide a high degree of portability for codes written with RQ communication primitives. RQ has been implemented on the CM5, Alewife, the Cray T3D, and the Intel Paragon <ref> [BCL + 95] </ref>. CHAPTER 5. REMOTE QUEUES 79 5.4 Advantages of Polling and Interrupts The motivation for Remote Queues comes from polling, and so do many of RQ's advantages. The power of RQ, however, is that it also achieves many of the benefits of message interrupts.
Reference: [BFKR92] <author> H. Burkhardt, S. Frank, B. Knobe, and J. Rothnie. </author> <title> Overview of the KSR1 Computer System. </title> <type> Technical Report KSR-TR-9202001, </type> <institution> Kendall Square Research, </institution> <month> February </month> <year> 1992. </year>
Reference-contexts: Chapter 4 Shared Memory versus Message Passing Distributed shared memory and message passing are two dominant communication mechanisms in parallel systems. Most machines incorporate either message passing [Thi93a] [Int91] [Che93] or shared memory [LT88] [LLG + 92] <ref> [BFKR92] </ref> [SGI94]. A few machines support both mechanisms [ABC + 95] [HKO + 94]. <p> Remote Local Refs (32 Processors) MHz Mbytes/s bytes/ Latency Miss Miss cycle Latency Latency MIT Alewife 20.0 4 fi 8 Mesh 360 18.0 15 50 11 [ABC + 95] TMC CM5 33.0 4-ary Fat-Tree 640 19.4 50 N/A 16 [Thi93a][L + 92] KSR-2 20.0 Ring 1000 50.0 ? 126 18 <ref> [BFKR92] </ref> [Dun92b] MIT J-Machine 12.5 4 fi 4 fi 2 Mesh 3200 256.0 7 N/A 7 [NDD93] MIT M-Machine #100.0 4 fi 4 fi 2 Mesh 12800 128.0 10 154 21 [FKD + 95][Kec96] Intel Delta 40.0 4 fi 8 Mesh 216 5.4 15 N/A 10 [Dun92a] [Moy91] Intel Paragon 50.0
Reference: [BK94] <author> Eric A. Brewer and Bradley C. Kuszmaul. </author> <title> How to get good performance from the CM-5 data network. </title> <booktitle> In International Parallel Processing Symposium, </booktitle> <address> Can-cun, Mexico, </address> <month> April </month> <year> 1994. </year>
Reference-contexts: Not only would this increase message latency, it would substantially increase network congestion. Networks have finite buffering and can not perform well when an arbitrary number of messages are sent without a comparable number of receptions <ref> [BK94] </ref>. Step 2 addresses this issue. Polling may, of course, be inserted at other points in the code, but communicaton functions are an obvious place for both the user and the compiler.
Reference: [CB96] <author> David Conroy and Lance Berc. </author> <type> Personal communication, </type> <institution> Digitial Systems Research Center, </institution> <month> March </month> <year> 1996. </year>
Reference-contexts: While a quad-issue, 300 MHz Alpha is more than 30 times faster than Alewife's 20 MHz Sparcle processor, many current memory systems are no faster than Alewife's 500 ns miss time. Even the fastest current prototypes, such as Digital's cache-less workstation <ref> [CB96] </ref>, still require 90 ns to reference main memory. Memory latency limitations will remain severe. Synchronous DRAMs and wide datapaths will not help with irregular accesses. The key is that network technology scales at least as well as DRAM speeds.
Reference: [CDG + 93] <author> David E. Culler, Andrea Dusseau, Seth Copen Goldstein, Arvind Krishna-murthy, Steven Lumetta, Thorsten von Eicken, and Katherine Yelick. </author> <title> Parallel programming in Split-C. </title> <booktitle> In Supercomputing, </booktitle> <month> November </month> <year> 1993. </year>
Reference-contexts: Second, UNSTRUC is a computational fluid dynamics application which uses real datasets. Third, ICCG is a general iterative solver which is the most challenging of our applications. 2.1 EM3D EM3D is a small benchmark code originally developed at UC Berkeley to exercise the Split-C parallel language <ref> [CDG + 93] </ref>. It models the propagation of electromagnetic waves through three-dimensional objects using algorithms described in [Mad92]. <p> The only data dependences are between phases, and these are satisfied by the communication step at the beginning of each phase. Pre-communicating all the data simplifies the computation step. It requires, however, that non-local data be stored in buffers until they are needed in the computation. Berkeley Split-C study <ref> [CDG + 93] </ref> calls these buffered copies ghost nodes, and they are equivalent to a software managed cache. Figure 4-1 gives a simple example of ghost nodes. Figure 4-2 illustrates the inter-processor communication necessary to update the ghost nodes in between computation phases.
Reference: [Che93] <author> Chesney. </author> <title> The Meiko CS-2 system architecture. </title> <booktitle> In Annual ACM Symposium on Parallel Algorithms and Architectures, </booktitle> <year> 1993. </year>
Reference-contexts: Chapter 4 Shared Memory versus Message Passing Distributed shared memory and message passing are two dominant communication mechanisms in parallel systems. Most machines incorporate either message passing [Thi93a] [Int91] <ref> [Che93] </ref> or shared memory [LT88] [LLG + 92] [BFKR92] [SGI94]. A few machines support both mechanisms [ABC + 95] [HKO + 94]. <p> They do not show how message traffic impacts runtime. 7.2 Remote Queues With respect to Remote Queues, there are many machines that have explicit queues in the network interface or communication coprocessor. Some of these are FLASH [HKO + 94], *T [PBGB93], Typhoon [RLW94] and Meiko CS-2 <ref> [Che93] </ref>. RQ maps well onto all of these machines. Scheiman and Schauser [SS95] independently considered some of the basic issues of RQ in their Meiko study. *T provides a fairly direct implementation of RQ.
Reference: [CKA91] <author> David Chaiken, John Kubiatowicz, and Anant Agarwal. </author> <title> LimitLESS directories: A scalable cache coherence scheme. </title> <booktitle> In Fourth International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS IV), </booktitle> <pages> pages 224234. </pages> <publisher> ACM, </publisher> <month> April </month> <year> 1991. </year>
Reference-contexts: Each directory may need as many entries as processors in the system. This makes implementing the directory in hardware a problem when scaling to large numbers of processors. Alewife addresses this issue with the LimitLESS <ref> [CKA91] </ref> software-extended directory-based protocol. Up to five copies are supported for each global memory location in hardware. If more than five processors share a location, directory operations trap to software. Studies [CKA91] [CLB + 96] have demonstrated that this software-extended approach provides scalability to large numbers of processors without significant loss <p> Alewife addresses this issue with the LimitLESS <ref> [CKA91] </ref> software-extended directory-based protocol. Up to five copies are supported for each global memory location in hardware. If more than five processors share a location, directory operations trap to software. Studies [CKA91] [CLB + 96] have demonstrated that this software-extended approach provides scalability to large numbers of processors without significant loss in performance relative to all-hardware approaches. The efficiency of Alewife's communication mechanisms is essential to exposing the tradeoffs and results of this study.
Reference: [CLB + 96] <author> Frederic T. Chong, Beng-Hong Lim, Ricardo Bianchini, John Kubiatowicz, and Anant Agarwal. </author> <title> Application performance on the mit alewife multiprocessor. BIBLIOGRAPHY 132 IEEE Computer: Special Issue on Emerging Applications for Shared-Memory Multiprocessors, </title> <month> December </month> <year> 1996. </year>
Reference-contexts: Alewife addresses this issue with the LimitLESS [CKA91] software-extended directory-based protocol. Up to five copies are supported for each global memory location in hardware. If more than five processors share a location, directory operations trap to software. Studies [CKA91] <ref> [CLB + 96] </ref> have demonstrated that this software-extended approach provides scalability to large numbers of processors without significant loss in performance relative to all-hardware approaches. The efficiency of Alewife's communication mechanisms is essential to exposing the tradeoffs and results of this study.
Reference: [CLR94] <author> Satish Chandra, James R. Larus, and Anne Rogers. </author> <title> Where is time spent in message-passing and shared-memory programs. </title> <booktitle> In ASPLOS VI, </booktitle> <pages> pages 6173, </pages> <address> San Jose, California, </address> <year> 1994. </year>
Reference-contexts: To investigate locality and scaling effects, we also took advantage of the flexibility of the synthetic dataset to vary some parameters. These experiments will be discussed in Chapter 4. We started with shared-memory and bulk-transfer codes from the University of Wisconsin at Madison <ref> [CLR94] </ref>. The fine-grained and polling message-passing versions were developed from the shared-memory code. The bulk-transfer code contained extensive message aggregation and software caching which was not necessary in the fine-grained or polling versions. CHAPTER 2. <p> Chapter 7 Related Work 7.1 Mechanisms Not only has our work has been strongly influenced by studies from Wisconsin, Stanford, and Maryland, but these previous results help confirm the general context established by our emulations. Our comparison of communication mechanisms is similar to Chandra, Larus and Rogers <ref> [CLR94] </ref>, but we have available a larger set of mechanisms and we generalize to a range of system parameters. This generalization is similar to the study of latency, occupancy, and bandwidth by Holt et. al [HHS + 95], which focuses exclusively upon shared-memory mechanisms.
Reference: [CS95] <author> Frederic T. Chong and Robert Schreiber. </author> <title> Parallel sparse triangular solution with partitioned inverses and prescheduled DAGs. </title> <booktitle> In 1995 Workshop on Solving Irregular Problems on Distributed Memory Machines, </booktitle> <address> Santa Barbara, California, </address> <month> April </month> <year> 1995. </year>
Reference-contexts: Unfortunately, it is difficult to avoid programmer and system bias in such studies. Our study copes with bias in two ways. First, our conclusion is counter to our programmer bias. Our results favor shared memory even though this study evolved from research [CSBS95] [BCL + 95] <ref> [CS95] </ref> and researchers 1 with a clear message-passing bias. Second, we perform all experiments on the same hardware. The MIT Alewife multiprocessor provides a uniquely integrated 1 For example, one of the original developers of the CMMD message-passing library [Thi93b] on the CM5. CHAPTER 1. <p> This chapter addresses this open question using the MIT Alewife machine [ABC + 95] and sparse, irregular applications. Alewife integrates distributed, cache-coherent shared memory and message passing, providing an ideal platform for our experiments. Although this study evolved from research [CSBS95] [BCL + 95] <ref> [CS95] </ref> and researchers 1 with a clear message-passing bias, our results point to shared memory as the most useful communication mechanism for a multiprocessor. We chose especially challenging fine-grain, irregular benchmarks such as the solution of sparse linear systems via ICCG (conjugate gradient preconditioned with incomplete Cholesky factors) [GvL83]. <p> Element i of the right-hand-side vector, b, is also kept with DAG node i, as is element i of the result vector x. A.4 Partitioned Inverses Partitioned inverses is an alternative method of sparse triangular solution that has been shown to greatly enhance parallelism [AS93] [APS93] <ref> [CS95] </ref>. In general, the system Lx = b can be solved through the computation x = L 1 b. Unfortunately, as Figure A-2 shows, the calculation of L 1 can lead to fill, non-zeros that were not in the original sparse structure of L.
Reference: [CSBS95] <author> Frederic T. Chong, Shamik D. Sharma, Eric A. Brewer, and Joel Saltz. </author> <title> Multiprocessor runtime support for irregular DAGs. </title> <journal> Parallel Processing Letters: Special Issue on Partitioning and Scheduling for Parallel and Distributed Systems, </journal> <pages> pages 671683, </pages> <month> December </month> <year> 1995. </year>
Reference-contexts: Unfortunately, it is difficult to avoid programmer and system bias in such studies. Our study copes with bias in two ways. First, our conclusion is counter to our programmer bias. Our results favor shared memory even though this study evolved from research <ref> [CSBS95] </ref> [BCL + 95] [CS95] and researchers 1 with a clear message-passing bias. Second, we perform all experiments on the same hardware. The MIT Alewife multiprocessor provides a uniquely integrated 1 For example, one of the original developers of the CMMD message-passing library [Thi93b] on the CM5. CHAPTER 1. <p> Each incoming arc results in 2 floating point operations in the kernel. Colors indicates the number of colors required in the multicolor ordering described in Section 2.3.5. The larger the ratio between Order and Colors, the better the parallelism and message aggregation. 2.3.3 Overview of ICCG Unlike previous studies <ref> [CSBS95] </ref>, we have extensively studied the entire ICCG algorithm to ensure that our results are consistent with a complete, best-effort implementation. <p> CHAPTER 2. APPLICATIONS 38 vious DAG-oriented approaches <ref> [CSBS95] </ref>. For data placement, we use Chaco, a well-established sequential code from Sandia National Laboratories which provides a range of mapping algorithms. We use a multilevel algorithm which uses a coarsening heuristic to condense the graph, and then partitions the condensed graph with recursive spectral bisection. <p> The increased parallelism allows our triangular solve speedups scale to much higher number of processors than with other orderings in previous studies <ref> [CSBS95] </ref>. 2 Recall that, depending upon our mechanism-dependent implementations, a processor may perform varying amounts of the computation of a node that it owns, but it will always complete every owned node by checking input dependencies and passing results along outgoing edges. CHAPTER 2. <p> This chapter addresses this open question using the MIT Alewife machine [ABC + 95] and sparse, irregular applications. Alewife integrates distributed, cache-coherent shared memory and message passing, providing an ideal platform for our experiments. Although this study evolved from research <ref> [CSBS95] </ref> [BCL + 95] [CS95] and researchers 1 with a clear message-passing bias, our results point to shared memory as the most useful communication mechanism for a multiprocessor. <p> The traditional method would be to schedule the nodes into phases (levels of a topological sort) and use barrier synchronization between levels. However, this has been found to be less efficient than finer-grain scheduling and synchronization <ref> [CSBS95] </ref>. The problem with shared memory is that processor 0 can perform a remote write to shared memory on processor 4 to communicate value v, but there is no synchronization event that tells processor 4 to subtract v from x 4 . CHAPTER 4. <p> Unfortunately, there is no notification to the receiver that the data has arrived. Applications face the awkward situation of having to poll for incoming data at all possible locations. As shown in polling point for the receiver to look for incoming data. An implementation of RQ on the T3D <ref> [CSBS95] </ref> provided active message communication which was up to ten times faster than the heavyweight T3D interrupting message (which requires kernel crossings). To get CHAPTER 5. <p> As demonstrated in Section 4.3.3, these computations are not only irregular, they often require a fine computation and communication grain. A.3 Substitution Substitution is the basic method of solving a triangular system, such as Lx = b, where L is lower triangular. Preliminary work <ref> [CSBS95] </ref> used a row-based distribution and represented the substitution computation as a directed acyclic graph (DAG). Figure A-1 illustrates how the DAG is derived from the triangular system. Each row i is represented by a DAG node i.
Reference: [DCB + 94] <author> Andre DeHon, Frederic Chong, Matthew Becker, Eran Egozy, Henry Minsky, Samuel Peretz, and Thomas F. Knight, Jr. METRO: </author> <title> A router architecture for high-performance, short-haul routing networks. </title> <booktitle> In Proceedings of the International Symposium on Computer Architecture, </booktitle> <pages> pages 266277, </pages> <month> May </month> <year> 1994. </year>
Reference-contexts: It takes one local-miss-time to service a remote miss at the receiver. It takes much less than one local-miss-time to get the request off the sender. That leaves more than three local-miss-times for the network transit time. With current VLSI switch technology <ref> [DCB + 94] </ref> keeping pace with processor cycle times, future networks should have no trouble keeping up with local memory speeds. Fast remote misses are necessary to make shared-memory machines easy to use and viable for a wider range of applications.
Reference: [DGL92] <author> Ian S. Duff, Roger G. Grimes, and John G. Lewis. </author> <title> User's guide for the Harwell-Boeing sparse matrix collection. </title> <type> Technical Report TR/PA/92/86, CERFACS, 42 Ave G. Coriolis, </type> <address> 31057 Toulouse Cedex, France, </address> <month> October </month> <year> 1992. </year>
Reference-contexts: This assumption avoids super-linear speedups which can result from sequential times based upon a workstation paging to disk or a single multiprocessor node using the memory from multiple nodes. Most of our benchmarks are from the Harwell-Boeing benchmark suite <ref> [DGL92] </ref>. Most of the matrices, especially the large ones, only come with the pattern of the nonzero entries, 1 The current Alewife runs at 20 MHz, but a bug fix in the CMMU memory controller will increase performance by 50 percent to 33 MHz. CHAPTER 2.
Reference: [DM89] <author> Iain S. Duff and Gerard A. Meurant. </author> <title> The effect of ordering on preconditioned conjugate gradients. BIT, </title> <address> 29:635657, </address> <year> 1989. </year>
Reference-contexts: This division causes even our largest benchmarks to benefit little from message aggregation. How do multicolor orderings affect convergence and what are the benefits of increased parallelism? Studies <ref> [DM89] </ref> on similar data-sets to ours indicate that the convergence rate of multicolor orderings is within roughly a factor of two of the best reorderings optimized for convergence.
Reference: [Dun92a] <author> Thomas H. Dunigan. </author> <title> Communication performance of the Intel Tochston Delta mesh. </title> <type> ORNL Report ORNL/TM-11983, </type> <institution> Oak Ridge National Laboratory, </institution> <note> Jan-uary 1992. BIBLIOGRAPHY 133 </note>
Reference-contexts: 50.0 ? 126 18 [BFKR92] [Dun92b] MIT J-Machine 12.5 4 fi 4 fi 2 Mesh 3200 256.0 7 N/A 7 [NDD93] MIT M-Machine #100.0 4 fi 4 fi 2 Mesh 12800 128.0 10 154 21 [FKD + 95][Kec96] Intel Delta 40.0 4 fi 8 Mesh 216 5.4 15 N/A 10 <ref> [Dun92a] </ref> [Moy91] Intel Paragon 50.0 4 fi 8 Mesh 2800 56.0 12 N/A 10 [Int91] [Moy91] Stanford DASH 33.0 2 fi 4 4-proc clusters 480 14.5 31 120 30 [LLG + 92] Stanford FLASH *200.0 4 fi 8 Mesh 3200 16.0 62 352 40 [HKO + 94] Wisconsin T0 #200.0 none
Reference: [Dun92b] <author> Thomas H. Dunigan. </author> <title> Kendall Square multiprocessor: Early experiences and performance. </title> <type> ORNL Report ORNL/TM-12065, </type> <institution> Oak Ridge National Laboratory, </institution> <month> March </month> <year> 1992. </year>
Reference-contexts: Local Refs (32 Processors) MHz Mbytes/s bytes/ Latency Miss Miss cycle Latency Latency MIT Alewife 20.0 4 fi 8 Mesh 360 18.0 15 50 11 [ABC + 95] TMC CM5 33.0 4-ary Fat-Tree 640 19.4 50 N/A 16 [Thi93a][L + 92] KSR-2 20.0 Ring 1000 50.0 ? 126 18 [BFKR92] <ref> [Dun92b] </ref> MIT J-Machine 12.5 4 fi 4 fi 2 Mesh 3200 256.0 7 N/A 7 [NDD93] MIT M-Machine #100.0 4 fi 4 fi 2 Mesh 12800 128.0 10 154 21 [FKD + 95][Kec96] Intel Delta 40.0 4 fi 8 Mesh 216 5.4 15 N/A 10 [Dun92a] [Moy91] Intel Paragon 50.0 4
Reference: [E + 92] <author> Thorsten von Eicken et al. </author> <title> Active messages: a mechanism for integrated communication and computation. </title> <booktitle> In Proceedings of the 19th Annual Symposium on Computer Architecture, </booktitle> <address> Queensland, Australia, </address> <month> May </month> <year> 1992. </year>
Reference-contexts: While a shared memory reference involves transferring an address and data across the bus, sending a message requires transferring a destination processor, a data address, the data, and, in the case of Active Messages <ref> [E + 92] </ref>, a message handler address. The overhead of such messaging is traditionally reduced by increasing the amount of data in each message. In sparse, irregular applications, however, data must be gathered into memory before it can be sent as a large message. <p> SHARED MEMORY VERSUS MESSAGE PASSING 49 Although we expected clear gains from the superior synchronization semantics of message-passing with active messages <ref> [E + 92] </ref>, we were able to overcome the synchronization difficulties of shared memory by shifting from an owner-computes model of computation to a producer-computes model (see Section 2.3.1).
Reference: [ETM93] <institution> Analysis of performance accelerator running ETMSP. </institution> <type> Technical Report TR-102856, </type> <institution> Performance Processors, Inc., </institution> <address> Palo Alto, California 94301, </address> <month> October </month> <year> 1993. </year> <note> Research Project 8010-31. </note>
Reference-contexts: For example, the time spent performing solves is equal to or double that of the time spent factoring in the sequential execution of the ETMSP power grid code <ref> [ETM93] </ref>. A.2 Direct vs. Iterative Solution Sparse matrix systems can be solved either through direct or iterative methods. Both methods use sparse triangular solution.
Reference: [FKD + 95] <author> Marco Fillo, Stephen W. Keckler, W.J. Dally, Nicholas P. Carter, Andrew Chang, Yevgeny Gurevich, and Whay S. Lee. </author> <title> The M-Machine Multicomputer. </title> <booktitle> In Proceedings of the 28th Annual International Symposium on Microarchitecture, </booktitle> <pages> pages 146156, </pages> <address> Ann Arbor, MI, </address> <month> November </month> <year> 1995. </year> <journal> IEEE Computer Society. </journal>
Reference-contexts: 33.0 4-ary Fat-Tree 640 19.4 50 N/A 16 [Thi93a][L + 92] KSR-2 20.0 Ring 1000 50.0 ? 126 18 [BFKR92] [Dun92b] MIT J-Machine 12.5 4 fi 4 fi 2 Mesh 3200 256.0 7 N/A 7 [NDD93] MIT M-Machine #100.0 4 fi 4 fi 2 Mesh 12800 128.0 10 154 21 <ref> [FKD + 95] </ref>[Kec96] Intel Delta 40.0 4 fi 8 Mesh 216 5.4 15 N/A 10 [Dun92a] [Moy91] Intel Paragon 50.0 4 fi 8 Mesh 2800 56.0 12 N/A 10 [Int91] [Moy91] Stanford DASH 33.0 2 fi 4 4-proc clusters 480 14.5 31 120 30 [LLG + 92] Stanford FLASH *200.0 4
Reference: [FLR + 94] <author> Babak Falsafi, Alvin R. Lebeck, Steven K. Reinhardt, Ioannis Schoinas, Mark D. Hill James R. Larus, Anne Rogers, and David A. Wood. </author> <title> Application-specific protocols for user-level shared memory. </title> <booktitle> In Supercomputing 94, </booktitle> <year> 1994. </year>
Reference-contexts: They also found that node-to-network bandwidth was not critical in modern multiprocessors. Our study shows, however, that bandwidth across the bisection of the machine may become a critical cost in supporting shared memory on modern machines. Such costs will make message passing and specialized user-level protocols <ref> [FLR + 94] </ref> increasingly important as processor speeds increase. Woo et al. [WSH94] compared bulk transfer with shared memory on simulations of the FLASH multiprocessor [HKO + 94] running the SPLASH [SWG92] suite.
Reference: [Gal96] <author> Mike Galles. </author> <title> The SGI SPIDER chip. </title> <note> Available from http://www.sgi.com/Technology/spider paper/spider paper.html, </note> <year> 1996. </year>
Reference-contexts: 401 40 [RPW96][Rei96] Cray T3D 150.0 4 fi 2 fi 2 Torus 2-proc clusters 4800 32.0 15 100 23 [T3D93] [A + 95] Cray T3E 300.0 4 fi 4 fi 2 Torus 19200 64.0 110 300-600 80 [Sco96b][Sco96a] SGI Origin 200.0 Hypercube 4-proc clusters 10800 54.0 60 150 61 [?] <ref> [Gal96] </ref> * projected, # simulated, latencies given in processor cycles. Table 6.1: Parameter estimates for various 32-processor multiprocessors. Network Latency is for one-way network transit time of a 24-byte packet.
Reference: [Gus79] <author> Ivar Gustafsson. </author> <title> On modified incomplete cholesky factorization methods for the solution of problems with mixed boundary conditions and problems with discontinuous material coefficients. </title> <journal> Int. J. Numer. Meth. </journal> <volume> Engng., </volume> <pages> pages 1127 1140, </pages> <year> 1979. </year>
Reference-contexts: The more fill that occurs, the more computation required to solve the system and often the less parallelism in that computation. In general, the larger the matrix A, the more severe the fill. To avoid fill, iterative techniques are often used <ref> [Gus79] </ref> [Mv77] [Axe85] [AK89]. Instead of computing the true factors of A, incomplete factors L 0 and U 0 are used. L 0 and U 0 are generally computed by factoring A, but discarding elements that introduce fill.
Reference: [GvL83] <author> G. Golub and C. F. van Loan. </author> <title> Matrix Computations. </title> <publisher> John Hopkins University Press, </publisher> <address> Baltimore, </address> <year> 1983. </year> <note> BIBLIOGRAPHY 134 </note>
Reference-contexts: We chose especially challenging fine-grain, irregular benchmarks such as the solution of sparse linear systems via ICCG (conjugate gradient preconditioned with incomplete Cholesky factors) <ref> [GvL83] </ref>. We expected our iterative applications to favor message passing because they had little data reuse.
Reference: [HHS + 95] <author> C. Holt, M. Heinrich, J. P. Singh, E. Rothberg, and J. Hennessy. </author> <title> The effects of latency, occupancy and bandwidth on the performance of cache-coherent mult-processors. </title> <type> Technical report, </type> <institution> Stanford University, Stanford, California, </institution> <month> January </month> <year> 1995. </year>
Reference-contexts: Our comparison of communication mechanisms is similar to Chandra, Larus and Rogers [CLR94], but we have available a larger set of mechanisms and we generalize to a range of system parameters. This generalization is similar to the study of latency, occupancy, and bandwidth by Holt et. al <ref> [HHS + 95] </ref>, which focuses exclusively upon shared-memory mechanisms. Although the Alewife machine provides an excellent starting point for the comparison of a large number of communication mechanisms, our results are greatly enhanced by our use of emulation, an approach inspired by the work at Wisconsin [RLW94].
Reference: [HKO + 94] <author> Mark Heinrich, Jeffrey Kuskin, David Ofelt, John Heinlein, Joel Baxter, Jaswinder Pal Singh, Richard Simoni, Kourosh Gharachorloo, David Nakahira, Mark Horowitz, Anoop Gupta, Mendel Rosenblum, and John Hennessy. </author> <title> The performance impact of flexibility in the Stanford FLASH multiprocessor. </title> <booktitle> In ASPLOS VI, </booktitle> <pages> pages 274285, </pages> <address> San Jose, California, </address> <year> 1994. </year>
Reference-contexts: Chapter 4 Shared Memory versus Message Passing Distributed shared memory and message passing are two dominant communication mechanisms in parallel systems. Most machines incorporate either message passing [Thi93a] [Int91] [Che93] or shared memory [LT88] [LLG + 92] [BFKR92] [SGI94]. A few machines support both mechanisms [ABC + 95] <ref> [HKO + 94] </ref>. The precise benefit of shared memory versus message passing has remained an open question, largely due to the difficulty of finding experimental platforms supporting an apples to apples comparison and the difficulty of writing applications in both styles without bias. <p> Mesh 216 5.4 15 N/A 10 [Dun92a] [Moy91] Intel Paragon 50.0 4 fi 8 Mesh 2800 56.0 12 N/A 10 [Int91] [Moy91] Stanford DASH 33.0 2 fi 4 4-proc clusters 480 14.5 31 120 30 [LLG + 92] Stanford FLASH *200.0 4 fi 8 Mesh 3200 16.0 62 352 40 <ref> [HKO + 94] </ref> Wisconsin T0 #200.0 none simulated N/A N/A 200 1461 40 [RPW96][Rei96] Wisconsin T1 #200.0 none simulated N/A N/A 200 401 40 [RPW96][Rei96] Cray T3D 150.0 4 fi 2 fi 2 Torus 2-proc clusters 4800 32.0 15 100 23 [T3D93] [A + 95] Cray T3E 300.0 4 fi 4 <p> Such costs will make message passing and specialized user-level protocols [FLR + 94] increasingly important as processor speeds increase. Woo et al. [WSH94] compared bulk transfer with shared memory on simulations of the FLASH multiprocessor <ref> [HKO + 94] </ref> running the SPLASH [SWG92] suite. They found bulk transfer performance to be disappointing due to the high cost of initiating transfer and the difficulty in finding computation to overlap with the transfer. <p> Finally they report only message traffic, not execution time numbers. They do not show how message traffic impacts runtime. 7.2 Remote Queues With respect to Remote Queues, there are many machines that have explicit queues in the network interface or communication coprocessor. Some of these are FLASH <ref> [HKO + 94] </ref>, *T [PBGB93], Typhoon [RLW94] and Meiko CS-2 [Che93]. RQ maps well onto all of these machines. Scheiman and Schauser [SS95] independently considered some of the basic issues of RQ in their Meiko study. *T provides a fairly direct implementation of RQ.
Reference: [HL95] <author> Bruce Hendrickson and Robert Leland. </author> <title> The Chaco user's guide. </title> <type> Technical Report SAND94-2692, </type> <institution> Sandia National Laboratories, </institution> <month> July </month> <year> 1995. </year>
Reference-contexts: We obtain supporting data by instrumenting two state-of-the-art scientific packages: Chaco <ref> [HL95] </ref>, a graph partitioner and mapper from Sandia National Laboratories; and BlockSolve [JP94], a conjugate gradient linear systems solver from Argonne National Laboratory.
Reference: [HS92] <author> S. W. Hammond and R. Schreiber. </author> <title> Mapping unstructured grid computations to the Connection Machine. </title> <editor> In J. Saltz P. Mehrotra and R. Voigt, editors, </editor> <booktitle> Unstructured Scientific Computation on Scalable Multiprocessors, </booktitle> <pages> pages 1130. </pages> <publisher> MIT Press, </publisher> <year> 1992. </year>
Reference-contexts: For our input dataset on 32 processors, MESH2K, about 56 percent of edges are internal. For a good overview of mapping methods on similar problems, see the study by Hammond and Schreiber on the CM2 <ref> [HS92] </ref>. The shared memory version of UNSTRUC was developed from the sequential C-code. The bulk transfer version was ported from the CM5 CMMD version of the code, which used large transfers.
Reference: [Int91] <institution> Paragon XP/S product overview. Intel Corporation, </institution> <year> 1991. </year>
Reference-contexts: All the multiprocessors discussed in this thesis have this general organization. These multiprocessors include the MIT Alewife Multiprocessor [ABC + 95], the Stanford DASH [LLG + 92], the CM5 [Thi93a], the Intel Paragon <ref> [Int91] </ref>, the Cray T3D [T3D93], and many others. Because of its flexibility, the Alewife machine will be the focus of this thesis. All the machines will be discussed in detail in Chapter 3. What mechanisms should the processing nodes of these architectures use to communicate CHAPTER 1. <p> The T3D and the Paragon provide test the flexibility and portability of the Remote Queues portion of our work (see Chapter 5). The bulk of our experiments are conducted on the MIT Alewife multiprocessor, discussed in detail in the next section. We refer the reader to [Thi93a] [T3D93] <ref> [Int91] </ref> for further information Table 3.1: Multiprocessors discussed in this thesis. 42 CHAPTER 3. PLATFORMS 43 on the other machines. 3.1 The Alewife Multiprocessor We conduct the majority of our experiments on the MIT Alewife multiprocessor [ABC + 95], shown in Figure 3-2. <p> Chapter 4 Shared Memory versus Message Passing Distributed shared memory and message passing are two dominant communication mechanisms in parallel systems. Most machines incorporate either message passing [Thi93a] <ref> [Int91] </ref> [Che93] or shared memory [LT88] [LLG + 92] [BFKR92] [SGI94]. A few machines support both mechanisms [ABC + 95] [HKO + 94]. <p> Mesh 3200 256.0 7 N/A 7 [NDD93] MIT M-Machine #100.0 4 fi 4 fi 2 Mesh 12800 128.0 10 154 21 [FKD + 95][Kec96] Intel Delta 40.0 4 fi 8 Mesh 216 5.4 15 N/A 10 [Dun92a] [Moy91] Intel Paragon 50.0 4 fi 8 Mesh 2800 56.0 12 N/A 10 <ref> [Int91] </ref> [Moy91] Stanford DASH 33.0 2 fi 4 4-proc clusters 480 14.5 31 120 30 [LLG + 92] Stanford FLASH *200.0 4 fi 8 Mesh 3200 16.0 62 352 40 [HKO + 94] Wisconsin T0 #200.0 none simulated N/A N/A 200 1461 40 [RPW96][Rei96] Wisconsin T1 #200.0 none simulated N/A N/A
Reference: [JP94] <author> Mark T. Jones and Paul E. Plassman. BlockSolve v2.0: </author> <title> Scalable library software for the parallel solution of sparse linear systems. </title> <type> ANL Report (updated draft) 92-46, </type> <institution> Argonne National Laboratory, </institution> <month> October </month> <year> 1994. </year>
Reference-contexts: We obtain supporting data by instrumenting two state-of-the-art scientific packages: Chaco [HL95], a graph partitioner and mapper from Sandia National Laboratories; and BlockSolve <ref> [JP94] </ref>, a conjugate gradient linear systems solver from Argonne National Laboratory.
Reference: [KA93] <author> John Kubiatowicz and Anant Agarwal. </author> <title> Anatomy of a message in the alewife multiprocessor. </title> <booktitle> In Proceedings of the International Supercomputing Conference (ISC) 1993, </booktitle> <address> Tokyo, Japan, </address> <month> July </month> <year> 1993. </year> <note> IEEE. Also as MIT/LCS TM-498, </note> <month> December </month> <year> 1992. </year>
Reference-contexts: An atomic launching mechanism in Alewife permits user code to compose outgoing messages directly in the hardware network queues without disabling interrupts or preventing system code from using the network interface <ref> [KA93] </ref>. Figure 5-8 illustrates code for sending a message complete with DMA. The store instructions illustrated here (stio) transfer data directly into the network message descriptor array in the CMMU. Message reception is similarly inexpensive. Under normal circumstances, an arriving message invokes a message-arrival interrupt.
Reference: [Kec96] <author> Steve Keckler. </author> <type> Personal communication, </type> <month> November </month> <year> 1996. </year> <note> BIBLIOGRAPHY 135 </note>
Reference: [KL70] <author> B. W. Kernighan and S. Lin. </author> <title> An efficient heuristic procedure for partitioning graphs. </title> <journal> The Bell System Technical Journal, </journal> <pages> pages 291307, </pages> <month> February </month> <year> 1970. </year>
Reference-contexts: We use a multilevel algorithm which uses a coarsening heuristic to condense the graph, and then partitions the condensed graph with recursive spectral bisection. Chaco assigns a mapping from partitions to processors on the parallel system and then refines it with the Kernighan-Lin algorithm <ref> [KL70] </ref>. 2.3.5 Multicolor Reordering and Message Aggregation After the data is mapped to processors, we hold the mapping fixed but we can renumber the nodes. The mapping works on the undirected graph, which corresponds to treating the A-matrix as an adjacency matrix.
Reference: [KL94] <author> A. Klaiber and H. Levy. </author> <title> A comparison of message passing and shared memory for data-parallel programs. </title> <booktitle> In Proceedings of the 21st Annual Symposium on Computer Architecture, </booktitle> <month> April </month> <year> 1994. </year>
Reference-contexts: This class includes papers by Lin and Snyder [LS90], Martonosi and Gupta [MG89], and LeBlanc and Markatos [LM92]. These studies however do not compare machine implementations, as all programs ultimately used shared memory only. Klaiber and Levy <ref> [KL94] </ref> study the performance of programs which accesses shared memory or message passing runtime libraries. These libraries generated traces for shared memory and message passing simulators, to generate statistics on message traffic. However, CHAPTER 7.
Reference: [L + 92] <author> Charles E. Leiserson et al. </author> <title> The network architecture of the connection machine CM-5. </title> <booktitle> In Symposium on Parallel Architectures and Algorithms, </booktitle> <pages> pages 272285, </pages> <address> San Diego, California, </address> <month> June </month> <year> 1992. </year> <note> ACM. </note>
Reference: [Lam79] <author> Leslie Lamport. </author> <title> How to make a multiprocessor computer that correctly executes multiprocess programs. </title> <journal> IEEE Transactions on Computers, </journal> <volume> C-28(9):690691, </volume> <month> September </month> <year> 1979. </year>
Reference-contexts: Alewife supports a sequentially consistent cache-coherent, distributed shared memory with a directory-based invalidation protocol which uses both hardware and software. Sequential consistency is formally defined by Lamport <ref> [Lam79] </ref> as follows: CHAPTER 3. PLATFORMS 44 CHAPTER 3.
Reference: [Lau96] <author> Jim Laudon. </author> <type> Personal communication, </type> <month> November </month> <year> 1996. </year>
Reference: [LLG + 92] <author> Daniel Lenoski, James Laudon, Kourosh Gharachorloo, Wolf-Dietrich Weber, Anoop Gupta, John Hennessy, Mark Horowitz, and Monica S. Lam. </author> <title> The Stan-ford Dash multiprocessor. </title> <booktitle> Computer, </booktitle> <address> 25(3):6380, </address> <month> March </month> <year> 1992. </year>
Reference-contexts: These parts generally communicate via the memory bus, although some architectures also use the cache and I/O buses. All the multiprocessors discussed in this thesis have this general organization. These multiprocessors include the MIT Alewife Multiprocessor [ABC + 95], the Stanford DASH <ref> [LLG + 92] </ref>, the CM5 [Thi93a], the Intel Paragon [Int91], the Cray T3D [T3D93], and many others. Because of its flexibility, the Alewife machine will be the focus of this thesis. All the machines will be discussed in detail in Chapter 3. <p> In particular, the high communication volume of shared memory threatens to become difficult to support on future machines without expensive, high-dimensional networks. If Alewife had the bisection bandwidth to processor clock ratio of the Stanford DASH <ref> [LLG + 92] </ref>, an architecture with a 2-dimensional mesh network, our results indicate that shared memory could perform up to 5 percent worse than message passing on our applications rather than slightly better. The Cray T3E [Sco96b], with its 3-dimensional torus network, avoids this problem. <p> Chapter 4 Shared Memory versus Message Passing Distributed shared memory and message passing are two dominant communication mechanisms in parallel systems. Most machines incorporate either message passing [Thi93a] [Int91] [Che93] or shared memory [LT88] <ref> [LLG + 92] </ref> [BFKR92] [SGI94]. A few machines support both mechanisms [ABC + 95] [HKO + 94]. <p> Mesh 12800 128.0 10 154 21 [FKD + 95][Kec96] Intel Delta 40.0 4 fi 8 Mesh 216 5.4 15 N/A 10 [Dun92a] [Moy91] Intel Paragon 50.0 4 fi 8 Mesh 2800 56.0 12 N/A 10 [Int91] [Moy91] Stanford DASH 33.0 2 fi 4 4-proc clusters 480 14.5 31 120 30 <ref> [LLG + 92] </ref> Stanford FLASH *200.0 4 fi 8 Mesh 3200 16.0 62 352 40 [HKO + 94] Wisconsin T0 #200.0 none simulated N/A N/A 200 1461 40 [RPW96][Rei96] Wisconsin T1 #200.0 none simulated N/A N/A 200 401 40 [RPW96][Rei96] Cray T3D 150.0 4 fi 2 fi 2 Torus 2-proc clusters
Reference: [LM92] <author> T. LeBlanc and E. Markatos. </author> <title> Shared memory vs. message passing in shared-memory multiprocesors. </title> <booktitle> In Fourth IEEE Symposium on Parallel and Distributed Processing, </booktitle> <month> December </month> <year> 1992. </year>
Reference-contexts: Another group of studies simulated message passing on a shared memory machine, and compared the performance of message passing programs using this simulation, against programs using shared memory directly. This class includes papers by Lin and Snyder [LS90], Martonosi and Gupta [MG89], and LeBlanc and Markatos <ref> [LM92] </ref>. These studies however do not compare machine implementations, as all programs ultimately used shared memory only. Klaiber and Levy [KL94] study the performance of programs which accesses shared memory or message passing runtime libraries.
Reference: [LS90] <author> C. Lin and L. Snyder. </author> <title> A comparison of programming models for shared-memory multiprocessors. </title> <booktitle> In ICPP, </booktitle> <month> August </month> <year> 1990. </year>
Reference-contexts: Another group of studies simulated message passing on a shared memory machine, and compared the performance of message passing programs using this simulation, against programs using shared memory directly. This class includes papers by Lin and Snyder <ref> [LS90] </ref>, Martonosi and Gupta [MG89], and LeBlanc and Markatos [LM92]. These studies however do not compare machine implementations, as all programs ultimately used shared memory only. Klaiber and Levy [KL94] study the performance of programs which accesses shared memory or message passing runtime libraries.
Reference: [LT88] <author> T. Lovett and S. Thakkar. </author> <title> The Symmetry multiprocessor system. </title> <booktitle> In Proceedings of the 1988 International Conference on Parallel Processing, </booktitle> <volume> Vol. I Architecture, </volume> <pages> pages 303310. </pages> , <institution> University Park, Pennsylvania, </institution> <note> [8] 1988. BIBLIOGRAPHY 136 </note>
Reference-contexts: Chapter 4 Shared Memory versus Message Passing Distributed shared memory and message passing are two dominant communication mechanisms in parallel systems. Most machines incorporate either message passing [Thi93a] [Int91] [Che93] or shared memory <ref> [LT88] </ref> [LLG + 92] [BFKR92] [SGI94]. A few machines support both mechanisms [ABC + 95] [HKO + 94].
Reference: [Mad92] <author> N. K. Madsen. </author> <title> Divergence preserving discrete surface integral methods for Maxwell's curl equations using non-orthogonal unstructured grids. </title> <type> Technical Report 92.04, </type> <institution> RIACS, </institution> <month> February </month> <year> 1992. </year>
Reference-contexts: It models the propagation of electromagnetic waves through three-dimensional objects using algorithms described in <ref> [Mad92] </ref>. As shown in Figure 2-1, the code operates on an irregular bipartite graph which consists of E nodes, representing electric field value at that point, and H nodes, representing magnetic field value at that point.
Reference: [MC95] <author> A. Mainwaring and D. Culler. </author> <title> Active Messages: Organization and Applications Programming Interface (API V2.0). </title> <institution> University of California at Berkeley, Network of Workstations Project White Paper, </institution> <month> September </month> <year> 1995. </year>
Reference: [Mes93] <author> Message Passing Interface Forum. </author> <title> MPI: A message passing interface. </title> <booktitle> In Supercomputing `93, </booktitle> <pages> pages 878883. </pages> <publisher> IEEE, </publisher> <year> 1993. </year>
Reference-contexts: In particular, it shows that many iterations are required to solve these linear systems and that triangular solve takes at least 40 percent of runtime out of a total runtime which includes incomplete Cholesky and iterative solution. Iteration counts and timings are from BlockSolve running on top of MPI <ref> [Mes93] </ref> on a collection of Sun workstations connected by Ethernet. This system runs the same computation as would a full implementation of ICCG on Alewife. The data, partitions, and convergence would be the same. However, the overheads and latencies are substantially higher in the cluster than on Alewife.
Reference: [MFHW96] <author> Shubhendu S. Mukherjee, Babak Falsafi, Mark D. Hill, and David A. Wood. </author> <title> Coherent network interfaces for fine-grain communication. </title> <booktitle> In Proceedings of the 23rd Annual Symposium on Computer Architecture, </booktitle> <year> 1996. </year>
Reference: [MG89] <author> M. Martonosi and A. Gupta. </author> <title> Tradeoffs in message passing and shared memory implementations of a standard cell router. </title> <booktitle> In ICPP, </booktitle> <month> August </month> <year> 1989. </year>
Reference-contexts: Another group of studies simulated message passing on a shared memory machine, and compared the performance of message passing programs using this simulation, against programs using shared memory directly. This class includes papers by Lin and Snyder [LS90], Martonosi and Gupta <ref> [MG89] </ref>, and LeBlanc and Markatos [LM92]. These studies however do not compare machine implementations, as all programs ultimately used shared memory only. Klaiber and Levy [KL94] study the performance of programs which accesses shared memory or message passing runtime libraries.
Reference: [MKAK94] <author> Kenneth Mackenzie, John Kubiatowicz, Anant Agarwal, and M. Frans Kaashoek. FUGU: </author> <title> Implementing Translation and Protection in a Multiuser, Multimodel Multiprocessor. </title> <booktitle> In In Proceedings of the 1994 Workshop on Shared Memory Multiprocessors, </booktitle> <address> Chicago, IL, </address> <month> April </month> <year> 1994. </year> <pages> USC. </pages>
Reference-contexts: Competitive algorithms may provide promising answers to these questions. Alewife does not fully implement Remote Queues. Selective interrupts are implemented at user-level rather than system-level, relying upon the user to implement the correct policy. FUGU <ref> [MKAK94] </ref>, a multiprogrammed follow-on to the Alewife machine, will implement Remote Queues at a hardware level. This will allow us to more fully explore the implications of Remote Queues on issues such as network congestion and multiprogrammed workloads. Chapter 9 Conclusion We conclude by summarizing the key insights gained.
Reference: [Moy91] <author> Steven A. Moyer. </author> <title> Performance of the iPSC/860 node architecture. </title> <type> Technical Report IPC-TR-91-007, </type> <institution> Institute for Parallel Computation, School of Engineering and Applied Science, University of Virginia, </institution> <address> Charlottesville, VA 22903, </address> <month> May </month> <year> 1991. </year>
Reference-contexts: ? 126 18 [BFKR92] [Dun92b] MIT J-Machine 12.5 4 fi 4 fi 2 Mesh 3200 256.0 7 N/A 7 [NDD93] MIT M-Machine #100.0 4 fi 4 fi 2 Mesh 12800 128.0 10 154 21 [FKD + 95][Kec96] Intel Delta 40.0 4 fi 8 Mesh 216 5.4 15 N/A 10 [Dun92a] <ref> [Moy91] </ref> Intel Paragon 50.0 4 fi 8 Mesh 2800 56.0 12 N/A 10 [Int91] [Moy91] Stanford DASH 33.0 2 fi 4 4-proc clusters 480 14.5 31 120 30 [LLG + 92] Stanford FLASH *200.0 4 fi 8 Mesh 3200 16.0 62 352 40 [HKO + 94] Wisconsin T0 #200.0 none simulated <p> 3200 256.0 7 N/A 7 [NDD93] MIT M-Machine #100.0 4 fi 4 fi 2 Mesh 12800 128.0 10 154 21 [FKD + 95][Kec96] Intel Delta 40.0 4 fi 8 Mesh 216 5.4 15 N/A 10 [Dun92a] <ref> [Moy91] </ref> Intel Paragon 50.0 4 fi 8 Mesh 2800 56.0 12 N/A 10 [Int91] [Moy91] Stanford DASH 33.0 2 fi 4 4-proc clusters 480 14.5 31 120 30 [LLG + 92] Stanford FLASH *200.0 4 fi 8 Mesh 3200 16.0 62 352 40 [HKO + 94] Wisconsin T0 #200.0 none simulated N/A N/A 200 1461 40 [RPW96][Rei96] Wisconsin T1 #200.0 none simulated N/A N/A 200
Reference: [MSH + 95] <author> Shubhendu S. Mukherjee, Shamik D. Sharma, Mark D. Hill, James R. Larus, Anne Rogers, and Joel Saltz. </author> <title> Efficient support for irregular applications on distributed-memory machines. </title> <booktitle> In Proc. 5th ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming (PPoPP'95), </booktitle> <pages> pages 6879, </pages> <address> Santa Barbara, California, </address> <month> July </month> <year> 1995. </year> <note> BIBLIOGRAPHY 137 </note>
Reference-contexts: CHAPTER 2. APPLICATIONS 31 2.2 UNSTRUC For more realistic datasets, we turn to UNSTRUC, a computational fluid dynamics code developed by the University of Maryland and the University of Wisconsin at Madison <ref> [MSH + 95] </ref>. UNSTRUC simulates fluid flows over three-dimensional physical objects, represented by an unstructured mesh. The code operates upon nodes, edges between nodes, and faces that connect three or four nodes. We used a MESH2K as an input dataset, a 2000 node irregular mesh provided with the code.
Reference: [Mv77] <author> J. A. Meijerink and H. A. van der Vorst. </author> <title> An iterative solution method for linear systems of which the coefficient matrix is a symmetric m-matrix. </title> <booktitle> Mathematics of Computation, </booktitle> <address> 31(137):148162, </address> <year> 1977. </year>
Reference-contexts: The more fill that occurs, the more computation required to solve the system and often the less parallelism in that computation. In general, the larger the matrix A, the more severe the fill. To avoid fill, iterative techniques are often used [Gus79] <ref> [Mv77] </ref> [Axe85] [AK89]. Instead of computing the true factors of A, incomplete factors L 0 and U 0 are used. L 0 and U 0 are generally computed by factoring A, but discarding elements that introduce fill.
Reference: [NDD93] <author> M.D. Noakes, D.A.Wallach, and W.J. Dally. </author> <title> The J-Machine multicomputer: An architectural evaluation. </title> <booktitle> In In Proceedings of the 20th Annual International Symposium on Computer Architecture 1993, </booktitle> <pages> pages 224235, </pages> <address> San Diego, CA, </address> <month> May </month> <year> 1993. </year> <note> ACM. </note>
Reference-contexts: 20.0 4 fi 8 Mesh 360 18.0 15 50 11 [ABC + 95] TMC CM5 33.0 4-ary Fat-Tree 640 19.4 50 N/A 16 [Thi93a][L + 92] KSR-2 20.0 Ring 1000 50.0 ? 126 18 [BFKR92] [Dun92b] MIT J-Machine 12.5 4 fi 4 fi 2 Mesh 3200 256.0 7 N/A 7 <ref> [NDD93] </ref> MIT M-Machine #100.0 4 fi 4 fi 2 Mesh 12800 128.0 10 154 21 [FKD + 95][Kec96] Intel Delta 40.0 4 fi 8 Mesh 216 5.4 15 N/A 10 [Dun92a] [Moy91] Intel Paragon 50.0 4 fi 8 Mesh 2800 56.0 12 N/A 10 [Int91] [Moy91] Stanford DASH 33.0 2 fi
Reference: [PBGB93] <author> G. M. Papadopoulos, G. A. Boughton, R. Greiner, and M. J. Berkerle. </author> <title> *t: Integrated building blocks for parallel computing. </title> <booktitle> In Supercomputing, </booktitle> <year> 1993. </year>
Reference-contexts: They do not show how message traffic impacts runtime. 7.2 Remote Queues With respect to Remote Queues, there are many machines that have explicit queues in the network interface or communication coprocessor. Some of these are FLASH [HKO + 94], *T <ref> [PBGB93] </ref>, Typhoon [RLW94] and Meiko CS-2 [Che93]. RQ maps well onto all of these machines. Scheiman and Schauser [SS95] independently considered some of the basic issues of RQ in their Meiko study. *T provides a fairly direct implementation of RQ.
Reference: [Rei96] <author> Steve Reinhardt. </author> <type> Personal communication, </type> <month> November </month> <year> 1996. </year>
Reference: [RG93] <author> Edward Rothberg and Anoop Gupta. </author> <title> An efficient block-oriented approach to parallel sparse Cholesky factorization. </title> <booktitle> In Supercomputing `93, </booktitle> <pages> pages 503512. </pages> <publisher> IEEE, </publisher> <year> 1993. </year>
Reference-contexts: ICCG only works on A-matrices which are symmetric, positive, and definite. Ax = b can be solved directly by factoring A into A = LU , via Cholesky factorization, and performing two triangular solves: Ly = b (forward substitution) and U x = y (backward substitution). Coarse-grain blocking approaches <ref> [RG93] </ref> perform well for this approach. Unfortunately, L and U can be expensive to compute and contain significantly more nonzeros than the original A. For this reason, an approximate factorization is often used.
Reference: [RLW94] <author> S. K. Reinhart, J. R. Larus, and D. A. Wood. Tempest and Typhoon: </author> <title> User-level shared memory. </title> <booktitle> In Proceedings of the International Symposium on Computer Architecture, </booktitle> <year> 1994. </year>
Reference-contexts: Although the Alewife machine provides an excellent starting point for the comparison of a large number of communication mechanisms, our results are greatly enhanced by our use of emulation, an approach inspired by the work at Wisconsin <ref> [RLW94] </ref>. Chandra, Larus and Rogers compare four applications on a simulation of a message-passing machine similar to a CM-5 multiprocessor against a simulation of a hypothetical machine also similar to a CM-5, but extended by shared-memory hardware. <p> They do not show how message traffic impacts runtime. 7.2 Remote Queues With respect to Remote Queues, there are many machines that have explicit queues in the network interface or communication coprocessor. Some of these are FLASH [HKO + 94], *T [PBGB93], Typhoon <ref> [RLW94] </ref> and Meiko CS-2 [Che93]. RQ maps well onto all of these machines. Scheiman and Schauser [SS95] independently considered some of the basic issues of RQ in their Meiko study. *T provides a fairly direct implementation of RQ.
Reference: [Rot93] <author> Edward Rothberg. </author> <title> Performance of panel and block approaches to sparse Cholesky factorization on iPSC/860 and Paragon multiprocessors. </title> <type> Technical report, </type> <institution> Intel SSD, </institution> <address> 14924 N.W. Greenbrier Parkway, Beaverton OR 97006, </address> <year> 1993. </year>
Reference-contexts: Most parallel computing involving sparse matrices has focused on factorization rather than solution for two reasons. First, both the unit of computation and the messages sizes are larger. Block factorization algorithms have performed well on parallel machines with relatively large communications overhead <ref> [Rot93] </ref>. We have specifically chosen sparse triangular solve as a challenging application for emerging parallel systems with finer grain capabilities. The second reason for the emphasis on factorization is that factorization takes much more computation than sparse solution. However, there are many applications where the solve time 123 APPENDIX A. <p> To solve this system directly, upper (U ) and lower (L) triangular factors must be computed: A = LU This is the common LU -factorization problem which has been extensively studied, resulting in the relatively successful block factorization algorithms mentioned earlier <ref> [Rot93] </ref>. Once the matrix A is factored, two sparse triangular solutions are necessary to solve for the unknowns ~x.
Reference: [RPW96] <author> Steven K. Reinhardt, Robert W. Pfile, and David A. Wood. </author> <title> Decoupled hardware support for distributed shared memory. </title> <booktitle> In Proceedings of the 23rd Annual Symposium on Computer Architecture, </booktitle> <year> 1996. </year>
Reference: [S + 93] <author> Ellen Spertus et al. </author> <title> Evaluation of mechanisms for fine-grained parallel programs in the J-machine and the CM-5. </title> <booktitle> In Proceedings of the International Symposium on Computer Architecture, </booktitle> <year> 1993. </year> <note> BIBLIOGRAPHY 138 </note>
Reference-contexts: The application then decides whether to process the message with the code within the loop or to enable interrupts (using user atomicity release) and allow the message to generate an interrupt. This is similar to the pseudo-polling from Spertus et al. <ref> [S + 93] </ref>, except that most messages are processed via true polling. Once all message CHAPTER 5.
Reference: [Sco96a] <author> Steve Scott. </author> <type> Personal communication, </type> <month> November </month> <year> 1996. </year>
Reference: [Sco96b] <author> Steven L. Scott. </author> <title> Synchronization and communication in the T3E multiprocessor. </title> <booktitle> In ASPLOS VII, </booktitle> <address> Cambridge, Massachusetts, </address> <year> 1996. </year>
Reference-contexts: The Cray T3E <ref> [Sco96b] </ref>, with its 3-dimensional torus network, avoids this problem. More importantly, the round-trip nature of shared memory may not be able to tolerate the latencies of future networks.
Reference: [SGI94] <institution> Power Challenge technical report. </institution> <type> Technical report, </type> <institution> Silicon Graphics Inc., </institution> <address> 2011 N. Shoreline Blvd., Mountain View, CA 94043, </address> <year> 1994. </year>
Reference-contexts: Chapter 4 Shared Memory versus Message Passing Distributed shared memory and message passing are two dominant communication mechanisms in parallel systems. Most machines incorporate either message passing [Thi93a] [Int91] [Che93] or shared memory [LT88] [LLG + 92] [BFKR92] <ref> [SGI94] </ref>. A few machines support both mechanisms [ABC + 95] [HKO + 94].
Reference: [SS95] <author> K. E. Schauser and C. J. Scheiman. </author> <title> Experience with active messages on the Meiko CS-2. </title> <booktitle> In International Parallel Processing Symposium, </booktitle> <year> 1995. </year>
Reference-contexts: Some of these are FLASH [HKO + 94], *T [PBGB93], Typhoon [RLW94] and Meiko CS-2 [Che93]. RQ maps well onto all of these machines. Scheiman and Schauser <ref> [SS95] </ref> independently considered some of the basic issues of RQ in their Meiko study. *T provides a fairly direct implementation of RQ. The importance of user-level handlers remains up for debate, with FLASH arguing that prohibiting such handlers simplifies the coprocessor and allows multiprogramming without gang scheduling.
Reference: [ST82] <author> R. Schreiber and W. Tang. </author> <title> Vectorizing the conjugate gradient method. </title> <booktitle> In Proceedings Symposium CYBER 205 Applications, </booktitle> <address> Ft. Collins, CO, </address> <year> 1982. </year>
Reference-contexts: Renumbering can also result in greater slackness (the amount of computation able to proceed between communications) which creates more data available to be aggregated into long messages. Long messages are often used to try to amortize communication overhead. We use the multicolor reordering algorithm <ref> [ST82] </ref>, described later in this section, which is one of the best known algorithms for enhancing parallelism and message aggregation. As shown in Figure 2-4, the goal of message aggregation is to delay communication of DAG edges until several edges can be CHAPTER 2.
Reference: [SWG92] <author> Jaswinder Pal Singh, Wolf-Dietrich Weber, and Anoop Gupta. </author> <title> SPLASH: Stan-ford parallel applications for shared-memory. </title> <booktitle> Computer Architecture News, </booktitle> <pages> pages 544, </pages> <month> March </month> <year> 1992. </year>
Reference-contexts: Such costs will make message passing and specialized user-level protocols [FLR + 94] increasingly important as processor speeds increase. Woo et al. [WSH94] compared bulk transfer with shared memory on simulations of the FLASH multiprocessor [HKO + 94] running the SPLASH <ref> [SWG92] </ref> suite. They found bulk transfer performance to be disappointing due to the high cost of initiating transfer and the difficulty in finding computation to overlap with the transfer. Although, Alewife's DMA mechanism is cheaper to initiate than theirs, we also found bulk transfer to have performance problems.
Reference: [T3D93] <institution> CRAY T3D system architecture overview. </institution> <type> Technical report, </type> <institution> Cray Research, Inc., 900 Lowater Road, Chippewa Falls, WI 54729, </institution> <month> September </month> <year> 1993. </year>
Reference-contexts: All the multiprocessors discussed in this thesis have this general organization. These multiprocessors include the MIT Alewife Multiprocessor [ABC + 95], the Stanford DASH [LLG + 92], the CM5 [Thi93a], the Intel Paragon [Int91], the Cray T3D <ref> [T3D93] </ref>, and many others. Because of its flexibility, the Alewife machine will be the focus of this thesis. All the machines will be discussed in detail in Chapter 3. What mechanisms should the processing nodes of these architectures use to communicate CHAPTER 1. <p> The T3D and the Paragon provide test the flexibility and portability of the Remote Queues portion of our work (see Chapter 5). The bulk of our experiments are conducted on the MIT Alewife multiprocessor, discussed in detail in the next section. We refer the reader to [Thi93a] <ref> [T3D93] </ref> [Int91] for further information Table 3.1: Multiprocessors discussed in this thesis. 42 CHAPTER 3. PLATFORMS 43 on the other machines. 3.1 The Alewife Multiprocessor We conduct the majority of our experiments on the MIT Alewife multiprocessor [ABC + 95], shown in Figure 3-2. <p> RQ can provide an efficient implementation of polling active message semantics on any memory-based system. By memory-based system, we refer to systems that use shared or global memory for communication. The Cray T3D <ref> [T3D93] </ref>, for example, exclusively uses remote reads and writes for fast communication. Memory-based communication is an extremely efficient way to move data. Unfortunately, there is no notification to the receiver that the data has arrived. <p> 4 fi 8 Mesh 3200 16.0 62 352 40 [HKO + 94] Wisconsin T0 #200.0 none simulated N/A N/A 200 1461 40 [RPW96][Rei96] Wisconsin T1 #200.0 none simulated N/A N/A 200 401 40 [RPW96][Rei96] Cray T3D 150.0 4 fi 2 fi 2 Torus 2-proc clusters 4800 32.0 15 100 23 <ref> [T3D93] </ref> [A + 95] Cray T3E 300.0 4 fi 4 fi 2 Torus 19200 64.0 110 300-600 80 [Sco96b][Sco96a] SGI Origin 200.0 Hypercube 4-proc clusters 10800 54.0 60 150 61 [?] [Gal96] * projected, # simulated, latencies given in processor cycles. Table 6.1: Parameter estimates for various 32-processor multiprocessors.
Reference: [Thi93a] <institution> Thinking Machines Corporation, Cambridge, MA. </institution> <type> CM-5 Technical Summary, </type> <month> November </month> <year> 1993. </year>
Reference-contexts: These parts generally communicate via the memory bus, although some architectures also use the cache and I/O buses. All the multiprocessors discussed in this thesis have this general organization. These multiprocessors include the MIT Alewife Multiprocessor [ABC + 95], the Stanford DASH [LLG + 92], the CM5 <ref> [Thi93a] </ref>, the Intel Paragon [Int91], the Cray T3D [T3D93], and many others. Because of its flexibility, the Alewife machine will be the focus of this thesis. All the machines will be discussed in detail in Chapter 3. <p> The T3D and the Paragon provide test the flexibility and portability of the Remote Queues portion of our work (see Chapter 5). The bulk of our experiments are conducted on the MIT Alewife multiprocessor, discussed in detail in the next section. We refer the reader to <ref> [Thi93a] </ref> [T3D93] [Int91] for further information Table 3.1: Multiprocessors discussed in this thesis. 42 CHAPTER 3. PLATFORMS 43 on the other machines. 3.1 The Alewife Multiprocessor We conduct the majority of our experiments on the MIT Alewife multiprocessor [ABC + 95], shown in Figure 3-2. <p> Chapter 4 Shared Memory versus Message Passing Distributed shared memory and message passing are two dominant communication mechanisms in parallel systems. Most machines incorporate either message passing <ref> [Thi93a] </ref> [Int91] [Che93] or shared memory [LT88] [LLG + 92] [BFKR92] [SGI94]. A few machines support both mechanisms [ABC + 95] [HKO + 94]. <p> When a message arrives, the receiving processor is interrupted and it runs the handler associated with the message. This interrupt-driven approach is the most intuitive notion of active messages, but processor interrupts can be very expensive. In fact, on systems such as the Thinking Machines CM5 <ref> [Thi93a] </ref>, the expense of interrupts led to the predominant use of another flavor of active messages based upon polling. The right side of Figure 5-1 illustrates active-message reception via polling. Once again, 73 CHAPTER 5. REMOTE QUEUES 74 the receiving processor is computing along its main thread of computation.
Reference: [Thi93b] <institution> Thinking Machines Corporation, </institution> <address> Cambridge, MA. </address> <note> CMMD Reference Manual (Version 3.0), </note> <month> May </month> <year> 1993. </year>
Reference-contexts: Second, we perform all experiments on the same hardware. The MIT Alewife multiprocessor provides a uniquely integrated 1 For example, one of the original developers of the CMMD message-passing library <ref> [Thi93b] </ref> on the CM5. CHAPTER 1. INTRODUCTION 23 environment for comparing cache-coherent distributed shared memory, message passing, and DMA. 1.5 Data Transfer Even when an application receives no benefit from caching, cache-coherent shared memory hardware is still an extremely efficient communication mechanism. <p> We expected our iterative applications to favor message passing because they had little data reuse. However, we found shared memory to be an extremely efficient communication mechanism even without the benefits of caching. 1 For example, one of the original developers of the CMMD message-passing library <ref> [Thi93b] </ref> on the CM5. 48 CHAPTER 4. <p> The semantics of such a polling point are very flexible. The receiver may poll to receive messages only if there are messages pending, receive no more than one message, and/or loop until a particular condition is satisfied. The Thinking Machines CMMD message-passing library manual <ref> [Thi93b] </ref> provides a good reference for polling options. As we shall see, not only does polling provide significant performance advantages over expensive processor interrupts, polling also provides desirable properties of atomicity. Unfortunately, traditional methods of polling can also lead to some problems.
Reference: [WHJ + 95] <author> D. A. Wallach, W. C. Hsieh, K. L. Johnson, M. F. Kaashoek, and W. E. Weihl. </author> <title> Optimistic active messages: A mechanism for scheduling communication with computation. </title> <booktitle> In Proc. 5th ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming (PPoPP'95), </booktitle> <year> 1995. </year>
Reference-contexts: Specialized handlers mitigate this performance hit substantially. Optimistic Active Messages <ref> [WHJ + 95] </ref> solve the deadlock problem for user handlers by creating and suspending a thread when a lock or other synchronization event is encountered in the handler.
Reference: [WSH94] <author> Steven Cameron Woo, Jaswinder Pal Singh, and John L. Hennessy. </author> <title> The performance advantages of integrating block data transfer in cache-coherent multiprocessors. </title> <booktitle> In Asplos VI, </booktitle> <pages> pages 219229, </pages> <address> San Jose, California, </address> <year> 1994. </year> <note> BIBLIOGRAPHY 139 </note>
Reference-contexts: Our study shows, however, that bandwidth across the bisection of the machine may become a critical cost in supporting shared memory on modern machines. Such costs will make message passing and specialized user-level protocols [FLR + 94] increasingly important as processor speeds increase. Woo et al. <ref> [WSH94] </ref> compared bulk transfer with shared memory on simulations of the FLASH multiprocessor [HKO + 94] running the SPLASH [SWG92] suite. They found bulk transfer performance to be disappointing due to the high cost of initiating transfer and the difficulty in finding computation to overlap with the transfer.
Reference: [YA93] <author> Donald Yeung and Anant Agarwal. </author> <title> Experience with fine-grain synchronization in MIMD machines for preconditioned conjugate gradient. </title> <booktitle> In Proceedings of the Fourth ACM SIGPLAN Symposium on Principles and Practices of Parallel Programming (PPoPP'93), </booktitle> <pages> pages 187197, </pages> <address> San Diego, California, </address> <month> May </month> <year> 1993. </year>
Reference-contexts: U 0 is similarly defined. In some Modified ICCG algorithms, the non-zero values in the incomplete factor are adjusted in an attempt to compensate for the omitted values <ref> [YA93] </ref>. L 0 and U 0 are CHAPTER 2. APPLICATIONS 35 exactly as sparse as A and we use them to iteratively arrive at a solution. Note that the sparsity of L 0 and U 0 make coarse-grain approaches impractical. The resulting iterative computation is inherently fine-grain and irregular.
References-found: 77

