URL: http://www.cs.gatech.edu/people/home/rosu/papers-uncompressed/CUCS-TR-95-1538.ps
Refering-URL: http://www.cs.gatech.edu/people/home/rosu/cv/cv.html
Root-URL: 
Email: rosu@cs.cornell.edu  
Title: Processor Controlled Off-Processor I/O  
Author: Marcel-Catalin Ro~su 
Date: August 27, 1995  
Address: Upson Hall  Ithaca, New York 14853-7501  
Affiliation: Computer Science Department  Cornell University  
Abstract: We took the approach of removing the kernel layer from the cross-machine communication path while still providing protection. The presence of a programmable communication processor on the network adapter made this experiment possible. The firmware running on the communication processor implements a Virtual Communication Machine (VCM); applications communicate with the VCM through shared memory without having to switch to kernel mode. Data is transferred directly between application buffers and the network without any intermediate buffering in the user or kernel spaces. The VCM architecture makes this possible; in particular, the VCM can be programmed to access any location in the address space of an application. The main processor controls the communication but it is not directly involved with it; as a consequence, the overhead on the main processor is very low. The design not only provides very low latencies, but also minimizes the effect of communication on the main processor data caches. We implemented the datagram subset of the Berkeley sockets interface on top of the VCM interface and integrated it with a user-level thread package. Multicast capabilities were added to the interface. Performance measured at both the VCM and socket layers is presented.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> T.E. Anderson, H. M. Levy, B. N. Bershad, and E. D. Lazowska, </author> <title> "The Interaction of Architecture and Operating System Design", </title> <booktitle> In Proceedings of the 4th Int'l Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <month> April </month> <year> 1991. </year>
Reference-contexts: In [5] the effect of context switches on caches is analyzed; costs range from 10 to 400 microseconds per switch. Performance of modern RISC processors on operating system code is studied in <ref> [1] </ref>. The authors conclude that operating system performance is well below application code performance. A communication model based on remote memory access is described in the context of more general work in [6].
Reference: [2] <author> J.B. Carter and W. Zwaenepoel, </author> <title> "Optimistic Implementation of Bulk Data Transfer Protocols" In Performance Evaluation Review, </title> <journal> Vol. </journal> <volume> 17, No. 1, </volume> <month> May </month> <year> 1989, </year> <pages> Pages 61-69. </pages>
Reference-contexts: The performance of our implementation is presented in the fifth section. The last section contains our conclusions and acknowledgments. 2 Related Work The first 0-copy system we are aware of is presented in <ref> [2] </ref> and uses Ethernet adapters with scatter-gather capabilities. [4] describes the advantages of user-level protocol implementations. In [5] the effect of context switches on caches is analyzed; costs range from 10 to 400 microseconds per switch. Performance of modern RISC processors on operating system code is studied in [1].
Reference: [3] <author> FORE Systems Inc., </author> <title> "Programmer's Reference Manual", </title> <month> August </month> <year> 1994. </year>
Reference-contexts: The advantage of executing as few CPU instructions as possible for an I/O operation is evident, especially when I/Os are frequent. The protection mechanism allows safe use in a multiuser environment. A new communication architecture was designed to support these characteristics. We used the communication mechanisms in <ref> [3] </ref> (and their implementation) as the starting point of our design. In a multitasking operating system, the network is accessed through a kernel layer that includes the protocol processing code and the device driver of the network adapter. <p> Furthermore, using faster CPUs does not help significantly because processing of individual packets is dominated by memory accesses. A communication processor (CP) is often added to the network adapter card to reduce the load on the CPU. In the architecture described in <ref> [3] </ref>, the network device driver is partially implemented in the firmware running on the CP and on the host CPU. The network adapter appears to the host as a (software) device handling much larger network packets. The host and the adapter communicate through shared (host or adapter) memory. <p> However, the VCM design allows the complete elimination of any buffering and provides the means to extend this feature to upper layers, while the existing U-net interface uses one level of buffering. In the FORE architecture (see <ref> [3] </ref>), the firmware running on the CP communicates with the host driver through a set of shared queues. The host supplies the firmware with free buffers through the free queue. <p> While we implement "fly-by" DMA transfers and pipeline the outgoing data too, these were merely extensions of the FORE implementation. We handle protection in an entirely different way than <ref> [3] </ref>. 3 3 The System Architecture The main components of our architecture are: the VCM implemented in the firmware running on the CP, a user-level library used by applications to issue commands to the VCM, and an extension to the host kernel.
Reference: [4] <author> J.C. Mogul, R.F. Rashid, and M.J. Accetta, </author> <title> "The Packet Filter: An Efficient Mechanism for User-level Network Code", </title> <booktitle> In Proceedings of the 11th Symposium on Operating System Principles, ACM SIGOPS, </booktitle> <address> Austin, Texas, </address> <month> November </month> <year> 1987. </year>
Reference-contexts: The performance of our implementation is presented in the fifth section. The last section contains our conclusions and acknowledgments. 2 Related Work The first 0-copy system we are aware of is presented in [2] and uses Ethernet adapters with scatter-gather capabilities. <ref> [4] </ref> describes the advantages of user-level protocol implementations. In [5] the effect of context switches on caches is analyzed; costs range from 10 to 400 microseconds per switch. Performance of modern RISC processors on operating system code is studied in [1].
Reference: [5] <author> J. C. Mogul and A. Borg, </author> <title> "The Effect of Context Switches on Cache Performance", </title> <booktitle> In Proceedings of the 4th Int'l Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <month> April </month> <year> 1991. </year> <month> 12 </month>
Reference-contexts: The last section contains our conclusions and acknowledgments. 2 Related Work The first 0-copy system we are aware of is presented in [2] and uses Ethernet adapters with scatter-gather capabilities. [4] describes the advantages of user-level protocol implementations. In <ref> [5] </ref> the effect of context switches on caches is analyzed; costs range from 10 to 400 microseconds per switch. Performance of modern RISC processors on operating system code is studied in [1]. The authors conclude that operating system performance is well below application code performance.
Reference: [6] <author> C. A. Thekkath, H. M. Levy, E. D. Lazowska, </author> <title> "Separating Data and Control Transfer in Distributed Operating Systems", </title> <booktitle> In Proceedings of the 6th Int'l Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <month> October </month> <year> 1994. </year>
Reference-contexts: Performance of modern RISC processors on operating system code is studied in [1]. The authors conclude that operating system performance is well below application code performance. A communication model based on remote memory access is described in the context of more general work in <ref> [6] </ref>. The communication primitives we have implemented are based on the message passing paradigm but share a characteristic: both are designed for transferring contiguous segments to/from the virtual address spaces of applications on different machines. We implement protection in an entirely different way.
Reference: [7] <author> R. van Renesse, K. Birman, B. Glade, K. Guo, M. Hayden, T. Hickey, D. Malki, A. Vaysburd, and W. Vogels "Horus: </author> <title> A Flexible Group Communications System", </title> <institution> Cornell University's Computer Science Technical Report 95-1500. </institution>
Reference-contexts: This is not an implementation of the IP and UDP protocols. It is only a user-level library designed to hide the details of the VCM from the application programmer. Multicast groups are implemented as virtual hosts. The interface was integrated with the user-level thread package of the Horus system <ref> [7] </ref>. The next section mentions some of the work related to our experiment. The third section describes the architecture and functions of the VCM and kernel extension. The socket interface is 1 In the following, an application buffer is any data structure in the application address space. <p> As a consequence, there can be at most one posted accept per application. To fix this we integrated our socket library with the user-level thread package of HORUS (see <ref> [7] </ref>). In this new environment, a thread still blocks in the receive call until the expected message arrives but another ready thread (if any) is scheduled for execution.
Reference: [8] <author> T. von Eicken, D. E. Culler, S. C. Goldstein, and K. E. Schauser, </author> <title> "Active Messages: A Mechanism for Integrated Communication and Computation", </title> <booktitle> In Proceedings of the 19th ISCA, </booktitle> <month> May </month> <year> 1992. </year>
References-found: 8

