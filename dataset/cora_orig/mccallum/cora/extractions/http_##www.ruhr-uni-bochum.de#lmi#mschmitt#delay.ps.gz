URL: http://www.ruhr-uni-bochum.de/lmi/mschmitt/delay.ps.gz
Refering-URL: http://www.ruhr-uni-bochum.de/lmi/mschmitt/
Root-URL: http://www.ruhr-uni-bochum.de/lmi/mschmitt/
Email: mschmitt@lmi.ruhr-uni-bochum.de  
Title: On the Implications of Delay Adaptability for Learning in Pulsed Neural Networks  
Author: Michael Schmitt 
Address: Ruhr-Universitat Bochum, D-44780 Bochum, Germany  
Affiliation: Lehrstuhl Mathematik und Informatik, Fakultat fur Mathematik  
Abstract: We consider a model for networks of neurons that compute and communicate in terms of pulses. In addition to weights and thresholds, which are commonly the parameters of artificial neural networks, these pulsed neural networks have adaptable delays. We present and discuss the most recent and prominent results on the complexity of computing and learning using this new type of parameter. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> W. Gerstner. </author> <title> Spiking neurons. </title> <editor> In W. Maass and C. M. Bishop, editors, </editor> <booktitle> Pulsed Neural Networks, chapter 1, </booktitle> <pages> pages 3-54. </pages> <publisher> MIT Press, </publisher> <address> Cam-bridge, Mass., </address> <year> 1999. </year>
Reference-contexts: Further discussions of this and more general types of neuron models can be found in the surveys by Gerstner <ref> [1] </ref> and Maass [3]. 2 Computational Power In this section, we restrict the inputs to binary values and ask how many and which Boolean functions can be computed by a pulsed neuron.
Reference: [2] <author> W. Maass. </author> <title> Networks of spiking neurons: </title> <booktitle> The third generation of neural network models. Neural Networks, </booktitle> <volume> 10 </volume> <pages> 1659-1671, </pages> <year> 1997. </year>
Reference-contexts: For analog coding the firing of a neuron at time t v represents the real number t v . Neuron models of the type considered here are also known as spiking neurons, or leaky integrate-and-fire neurons. In particular, the model defined above has first been introduced by Maass <ref> [2] </ref>. For simplicity, we call it a pulsed neuron throughout the paper. <p> We show that a pulsed neuron is more powerful than both of these function classes. (In particular, the fact of comprising a threshold gate implies the lower bound in Theorem 1.) Theorem 2 <ref> [2, 5] </ref> A pulsed neuron with binary coding of the inputs can compute any linearly separable Boolean function. Further, it can compute any Boolean function that can be written as a DNF formula. Both inclusions are strict.
Reference: [3] <author> W. Maass. </author> <title> Computing with spiking neurons. </title> <editor> In W. Maass and C. M. Bishop, editors, </editor> <booktitle> Pulsed Neural Networks, chapter 2, </booktitle> <pages> pages 55-85. </pages> <publisher> MIT Press, </publisher> <address> Cam-bridge, Mass., </address> <year> 1999. </year>
Reference-contexts: Further discussions of this and more general types of neuron models can be found in the surveys by Gerstner [1] and Maass <ref> [3] </ref>. 2 Computational Power In this section, we restrict the inputs to binary values and ask how many and which Boolean functions can be computed by a pulsed neuron.
Reference: [4] <author> W. Maass and C. M. Bishop, </author> <title> editors. Pulsed Neural Networks. </title> <publisher> MIT Press, </publisher> <address> Cambridge, Mass., </address> <year> 1999. </year>
Reference-contexts: Attention from both of these areas has manifested itself, for instance, in a recent book <ref> [4] </ref>. In this paper we consider a simple model for pulsed neural networks that has connections parameterized by transmission delays. We present and discuss recent theoretical results on the complexity of computing and learning using adaptable delays.
Reference: [5] <author> W. Maass and M. Schmitt. </author> <title> On the complexity of learning for a spiking neuron. </title> <booktitle> In Proceedings of the Tenth Annual Conference on Computational Learning Theory, </booktitle> <pages> pages 54-61, </pages> <publisher> ACM, </publisher> <address> New York, </address> <year> 1997. </year>
Reference-contexts: The upper bound in the following counting theorem is a result that turns out as special case of the analysis of the learning capabilities of this neuron model, which are discussed in the next section. Theorem 1 <ref> [5] </ref> There are 2 fi (n 2 ) many Boolean functions computable by a pulsed neuron with binary coding of the inputs. 2 We now compare the pulsed neuron with two well-established function classes. <p> We show that a pulsed neuron is more powerful than both of these function classes. (In particular, the fact of comprising a threshold gate implies the lower bound in Theorem 1.) Theorem 2 <ref> [2, 5] </ref> A pulsed neuron with binary coding of the inputs can compute any linearly separable Boolean function. Further, it can compute any Boolean function that can be written as a DNF formula. Both inclusions are strict. <p> It is well known that the VC dimension of a threshold gate is equal to the number of programmable parameters (i.e. weights and threshold). The following result shows that there is a significant increase in this dimension when delays instead of weights become adjustable. Theorem 6 <ref> [5] </ref> The VC dimension of a pulsed neuron with n adaptable delays is (n log n). This holds even if the inputs are restricted to binary values and all weights are kept fixed. <p> This holds even if the inputs are restricted to binary values and all weights are kept fixed. There is a corresponding upper bound that holds even if we allow input values to be from the reals. Theorem 7 <ref> [5] </ref> The VC dimension of a pulsed neuron with n analog coded inputs and binary coded output is O (n log n). Thus we can establish a tight bound for the VC dimension of a pulsed neuron. Corollary 8 [5] A pulsed neuron has VC dimension fi (n log n). <p> Theorem 7 <ref> [5] </ref> The VC dimension of a pulsed neuron with n analog coded inputs and binary coded output is O (n log n). Thus we can establish a tight bound for the VC dimension of a pulsed neuron. Corollary 8 [5] A pulsed neuron has VC dimension fi (n log n). This holds for binary as well as for analog coding of the inputs. A sigmoidal gate is the real-valued analogue of a threshold gate. <p> As in the case of the VC dimension for the threshold gate, the pseudo dimension of the sigmoidal gate is equal to the number of parameters. Again, for a pulsed neuron computing real-valued functions we have a super-linear bound. Theorem 9 <ref> [5] </ref> The pseudo dimension of a pulsed neuron with n analog coded inputs and analog coded output is fi (n log n). 4 Sample Complexity for Networks We consider feedforward neural networks which is the type of neural architecture most widely used in practice. <p> This holds also for a pulsed neuron where only the delays are adaptable. For simplicity, this result was shown in <ref> [5] </ref> for a pulsed neuron which may use natural numbers 0; : : : ; k 1 as delay values.
Reference: [6] <author> W. Maass and M. Schmitt. </author> <title> On the complexity of computing and learning with networks of spiking neurons. </title> <booktitle> In Electronic Proceedings of the Fifth International Symposium on Artificial Intelligence and Mathematics, </booktitle> <address> http://rutcor.rutgers.edu/~amai/, 1998. </address>
Reference-contexts: The following result shows that there is a significantly larger lower bound for the case of adaptable delays in pulsed neural networks. Theorem 10 <ref> [6] </ref> For each n a pulsed neural network with O (C) connections can be constructed that has VC dimension (C 2 ). This even holds if only the delays are adaptable and all weights and thresholds remain fixed. In [6] this result is shown for inputs from the Boolean domain and <p> Theorem 10 <ref> [6] </ref> For each n a pulsed neural network with O (C) connections can be constructed that has VC dimension (C 2 ). This even holds if only the delays are adaptable and all weights and thresholds remain fixed. In [6] this result is shown for inputs from the Boolean domain and for a network using binary coding of the inputs. Interestingly, this quadratic lower bound equals the best known lower bound for sigmoidal neural networks. <p> The following result, which holds even if we allow delay and weight adaptability and even if the inputs are real numbers, shows that the 4 quadratic lower bound is asymptotically tight in the case of pulsed neural networks. Theorem 11 <ref> [6] </ref> Consider a pulsed neural network where all delays, weights, and thresholds are adaptable, and let C be the number of connections. The VC dimension of this network is O (C 2 ) even for analog coding of the inputs. <p> However, it can also be shown that the VC dimension increases linearly with the number of layers and the number of connections. Theorem 13 <ref> [6, 9] </ref> For each L; C 1, where C kL for some constant k, there exists a multi-layer pulsed neural network with L layers, C connections and binary coding of the inputs that has VC dimension (CL). <p> The proof of Theorem 13, however, shows that no such bound can exist in the case of delay adaptability. Moreover, the adaptable delay parameters that are used in this construction can even be restricted to a single layer. Corollary 14 <ref> [6, 9] </ref> It is impossible to give an upper bound for the VC dimension of a multi-layer pulsed neural network with fixed weights solely in terms of the number of connections and layers that have adaptable delays. 5 On the Computational Complexity of Learning with Adaptable Delays The issues of learning
Reference: [7] <author> A. F. Murray. </author> <title> Pulse-based computation in VLSI neural networks. </title> <editor> In W. Maass and C. M. Bishop, editors, </editor> <booktitle> Pulsed Neural Networks, chapter 3, </booktitle> <pages> pages 87-109. </pages> <publisher> MIT Press, </publisher> <address> Cambridge, Mass., </address> <year> 1999. </year>
Reference-contexts: Compared to conventional neuron models, the pulsed neurons is not only a further step towards a more biologically plausible description of neural systems, it is also suitable for implementations of pulsed neural networks in analog VLSI such as described by Murray <ref> [7] </ref>.
Reference: [8] <author> M. Schmitt. </author> <title> On computing Boolean functions by a spiking neuron. </title> <journal> Annals of Mathematics and Artificial Intelligence, </journal> <volume> 24 </volume> <pages> 181-191, </pages> <year> 1998. </year> <booktitle> Extended abstract in Proceedings of the 8th International Conference on Artificial Neural Networks ICANN 98. </booktitle>
Reference-contexts: It provides a way to characterize the complexity of a Boolean function in terms of linear separability. (A linearly separable Boolean function has threshold number 1.) We define the threshold number of a pulsed neuron as the largest threshold number of a Boolean function it can compute. Theorem 3 <ref> [8] </ref> The threshold number of a pulsed neuron with n inputs is fi (n). Another characterization of the complexity of a Boolean function is in terms of its threshold order. <p> This number can range from 0 to n, obviously being 1 for linearly separable Boolean functions. The following result gives a lower bound on the largest threshold order of a Boolean function computable by a pulsed neuron. Theorem 4 <ref> [8] </ref> The threshold order of a pulsed neuron with n inputs is (n 1=3 ). Neuron models that compute polynomially separable Boolean functions are also known as polynomial threshold gates, sigma-pi units, or higher-order neurons. <p> The result just stated implies in particular that for the simulation of a pulsed neuron by a higher-order neuron the degree cannot be bounded by a constant but must grow with the number of inputs. Corollary 5 <ref> [8] </ref> There is no higher-order neuron with fixed degree that can simulate a pulsed neuron. 3 Sample Complexity for Pulsed Neurons An essential characterization for the complexity of learning using a specific hypothesis class is the sample complexity of the class, that is, the number of examples that a learning algorithm
Reference: [9] <author> M. Schmitt. </author> <title> VC dimension bounds for networks of spiking neurons. </title> <booktitle> In Proceedings of the 7th European Symposium on Artificial Neural Networks ESANN 99, to appear. </booktitle>
Reference-contexts: The following result shows that the VC dimension of such a multi-layered network of pulsed neurons is almost linear in terms of the number of connections and the number of layers. Theorem 12 <ref> [9] </ref> A multi-layer pulsed neural network with C connections and L layers has VC dimension O (CL log (CL)). If the number of layers is considered a constant then this upper bound appears to be asymptotically optimal. <p> However, it can also be shown that the VC dimension increases linearly with the number of layers and the number of connections. Theorem 13 <ref> [6, 9] </ref> For each L; C 1, where C kL for some constant k, there exists a multi-layer pulsed neural network with L layers, C connections and binary coding of the inputs that has VC dimension (CL). <p> The proof of Theorem 13, however, shows that no such bound can exist in the case of delay adaptability. Moreover, the adaptable delay parameters that are used in this construction can even be restricted to a single layer. Corollary 14 <ref> [6, 9] </ref> It is impossible to give an upper bound for the VC dimension of a multi-layer pulsed neural network with fixed weights solely in terms of the number of connections and layers that have adaptable delays. 5 On the Computational Complexity of Learning with Adaptable Delays The issues of learning
Reference: [10] <author> M. Vidyasagar. </author> <title> A Theory of Learning and Generalization. Communications and Control Engineering Series. </title> <publisher> Springer, </publisher> <address> London, </address> <year> 1997. </year> <month> 6 </month>
Reference-contexts: These bounds are given in terms of a combinatorial parameter known as the Vapnik-Chervonenkis dimension. (For details concerning the relationship between this dimension and learnability we refer the reader to Vidyasagar <ref> [10] </ref>.) Finally, Section 5 deals with the computational complexity of learning with adaptable delays presenting a hardness result for pulsed neurons. <p> Further, by exhibiting neural networks with superlinear VC dimension this bound has been shown to be asymptotically tight (see, e.g., <ref> [10] </ref> for these and other results concerning neural networks with adaptable weights). The following result shows that there is a significantly larger lower bound for the case of adaptable delays in pulsed neural networks.
References-found: 10

