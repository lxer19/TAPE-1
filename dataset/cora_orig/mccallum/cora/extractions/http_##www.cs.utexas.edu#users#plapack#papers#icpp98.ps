URL: http://www.cs.utexas.edu/users/plapack/papers/icpp98.ps
Refering-URL: http://www.cs.utexas.edu/users/plapack/icpp98/index.html
Root-URL: 
Title: PLAPACK: High Performance through High Level Abstraction  
Author: Greg Baker John Gunnels Greg Morrow Beatrice Riviere Robert van de Geijn 
Date: February 10, 1998  
Note: A Technical Paper Submitted to ICPP'98  
Address: Austin, TX 78712  
Affiliation: The University of Texas at Austin  
Abstract: Coding parallel algorithms is generally regarded as a formidable task. To make this task manageable in the arena of linear algebra algorithms, we have developed the Parallel Linear Algebra Package (PLA-PACK), an infrastructure for coding such algorithms at a high level of abstraction. It is often believed that by raising the level of abstraction in this fashion, performance is sacrificed. Throughout, we have maintained that indeed there is a performance penalty, but that by coding at a higher level of abstraction, more sophisticated algorithms can be implemented, which allows high levels of performance to be regained. In this paper, we show this to indeed be the case for the parallel solver package implemented using PLAPACK, which includes Cholesky, LU, and QR factorization based solvers for symmetric positive definite, general, and overdetermined systems of equations, respectively. Performance comparison with ScaLAPACK shows better performance is attained by our solvers. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Philip Alpatov, Greg Baker, Carter Edwards, John Gunnels, Greg Morrow, James Overfelt, Robert van de Geijn, and Yuan-Jye J. Wu. Plapack: </author> <title> Parallel linear algebra package design overview. </title> <booktitle> In Proceedings of SC97, </booktitle> <year> 1997. </year>
Reference-contexts: The PLAPACK infrastructure attempts to show that by adopting an object based coding style, already popularized by the Message-Passing Infrastructure (MPI) [10, 14], the coding of parallel linear algebra algorithms is simplified compared to the more traditional sequential coding approaches. We contrast the coding styles in <ref> [1] </ref>. It is generally believed that by coding at a higher level of abstraction, one pays a price of higher overhead and thus reduced performance.
Reference: [2] <author> E. Anderson, Z. Bai, J. Demmel, J. E. Dongarra, J. DuCroz, A. Greenbaum, S. Hammarling, A. E. McKenney, S. Ostrouchov, and D. Sorensen. </author> <title> LAPACK Users' Guide. </title> <publisher> SIAM, </publisher> <address> Philadelphia, </address> <year> 1992. </year>
Reference-contexts: High performance libraries for matrix operations like the various factorizations first became available with the arrival of the LAPACK <ref> [2] </ref> library in the early 90's. This library recognized the necessity to code in terms of Basic Linear Algebra Subprograms (BLAS) [12, 7, 6] for modularity and portable performance.
Reference: [3] <author> Christian Bischof and Charles Van Loan. </author> <title> The WY representation for products of Householder matrices. </title> <note> SIAM J. </note> <institution> Sci. Stat. Comput., 8(1):s2-s13, </institution> <month> Jan. </month> <year> 1987. </year>
Reference-contexts: Store v in the now zero part of a 1 . 4. A 2 (I + fivv T )A 2 = A 2 + fivy T where y T = v T A 2 5. Repartition A = 12 6. Continue recursively with A 22 4.1.1 Blocked algorithm In <ref> [3, 9] </ref>, it is shown how a blocked (matrix-matrix multiply) based algorithm can be derived , by creating a WY transform, which, when applied, yield the same result as a number of successive applications of Householder transforms: I + W Y T = H 1 H 2 H k where W
Reference: [4] <author> L. S. Blackford, J. Choi, A. Cleary, E. D'Azevedo, J. Demmel, I. Dhillon, J. Dongarra, S. Hammar-ling, G. Henry, A. Petitet, K. Stanley, D. Walker, and R. C. Whaley. </author> <title> ScaLAPACK Users' Guide. </title> <publisher> SIAM, </publisher> <year> 1997. </year> <month> 15 </month>
Reference-contexts: Frederick Avenue, Gaithersburg, MD 20879-3328 y Corresponding Author. Department of Computer Sciences, The University of Texas, Austin, TX 78712, (512) 471-9720, (512) 471-8885 (fax), rvdg@cs.utexas.edu 1 Extensions of LAPACK to distributed memory architectures like the Cray T3E, IBM SP-2, and Intel Paragon are explored by the ScaLAPACK project <ref> [4] </ref>. While this package does manage to provide a subset of the functionality of LAPACK, it is our belief that it has also clearly demonstrated that mimicking the coding styles that were effective on sequential and shared memory computers does not create maintainable and flexible code for distributed memory architectures. <p> We compare performance of our implementations to performance attained by the routines with the same functionality provided by ScaLAPACK. The presented curves for ScaLAPACK were created with data from the ScaLAPACK Users' Guide <ref> [4] </ref>. While it is highly likely that that data was obtained with different versions of the OS, MPI, compilers, and BLAS, we did perform some measurements of ScaLAPACK routines on the same system on which our implementations were measured.
Reference: [5] <author> Jack Dongarra, Robert van de Geijn, and David Walker. </author> <title> Scalability issues affecting the design of a dense linear algebra library. </title> <journal> J. Parallel Distrib. Comput., </journal> <volume> 22(3), </volume> <month> Sept. </month> <year> 1994. </year>
Reference-contexts: Unlike ScaLAPACK, PLAPACK has a distinct approach to distributing vectors. For scalability reasons, parallel dense linear algebra algorithms must be implemented using a logical two-dimensional mesh of nodes <ref> [5, 11, 13, 15] </ref>. However, for the distribution of vectors, that two-dimensional mesh is linearized by ordering the nodes in column-major order.
Reference: [6] <author> Jack J. Dongarra, Jeremy Du Croz, Sven Hammarling, and Iain Duff. </author> <title> A set of level 3 basic linear algebra subprograms. </title> <journal> ACM Trans. Math. Soft., </journal> <volume> 16(1) </volume> <pages> 1-17, </pages> <month> March </month> <year> 1990. </year>
Reference-contexts: High performance libraries for matrix operations like the various factorizations first became available with the arrival of the LAPACK [2] library in the early 90's. This library recognized the necessity to code in terms of Basic Linear Algebra Subprograms (BLAS) <ref> [12, 7, 6] </ref> for modularity and portable performance. In particular, it explores the benefits of recoding such algorithms in terms of matrix-matrix kernels like matrix-matrix multiplication in order to improve utilization of the cache of a microprocessor. fl Currently with Lockheed Martin Federal Systems, Inc., 700 N.
Reference: [7] <author> Jack J. Dongarra, Jeremy Du Croz, Sven Hammarling, and Richard J. Hanson. </author> <title> An extended set of FORTRAN basic linear algebra subprograms. </title> <journal> ACM Trans. Math. Soft., </journal> <volume> 14(1) </volume> <pages> 1-17, </pages> <month> March </month> <year> 1988. </year>
Reference-contexts: High performance libraries for matrix operations like the various factorizations first became available with the arrival of the LAPACK [2] library in the early 90's. This library recognized the necessity to code in terms of Basic Linear Algebra Subprograms (BLAS) <ref> [12, 7, 6] </ref> for modularity and portable performance. In particular, it explores the benefits of recoding such algorithms in terms of matrix-matrix kernels like matrix-matrix multiplication in order to improve utilization of the cache of a microprocessor. fl Currently with Lockheed Martin Federal Systems, Inc., 700 N.
Reference: [8] <author> C. Edwards, P. Geng, A. Patra, and R. van de Geijn. </author> <title> Parallel matrix distributions: have we been doing it all wrong? Technical Report TR-95-40, </title> <institution> Department of Computer Sciences, The University of Texas at Austin, </institution> <year> 1995. </year>
Reference-contexts: In <ref> [8, 16] </ref> we call this approach to generating a matrix distribution from a vector distribution physically based matrix distribution. A consequence of this assignment strategy is that a column of a matrix can be redistributed like a vector by scattering the elements appropriately within rows of nodes.
Reference: [9] <author> Gene H. Golub and Charles F. Van Loan. </author> <title> Matrix Computations. </title> <publisher> The Johns Hopkins University Press, </publisher> <address> Baltimore, </address> <note> second edition, </note> <year> 1989. </year>
Reference-contexts: Store v in the now zero part of a 1 . 4. A 2 (I + fivv T )A 2 = A 2 + fivy T where y T = v T A 2 5. Repartition A = 12 6. Continue recursively with A 22 4.1.1 Blocked algorithm In <ref> [3, 9] </ref>, it is shown how a blocked (matrix-matrix multiply) based algorithm can be derived , by creating a WY transform, which, when applied, yield the same result as a number of successive applications of Householder transforms: I + W Y T = H 1 H 2 H k where W
Reference: [10] <author> W. Gropp, E. Lusk, and A. Skjellum. </author> <title> Using MPI. </title> <publisher> The MIT Press, </publisher> <year> 1994. </year>
Reference-contexts: The PLAPACK infrastructure attempts to show that by adopting an object based coding style, already popularized by the Message-Passing Infrastructure (MPI) <ref> [10, 14] </ref>, the coding of parallel linear algebra algorithms is simplified compared to the more traditional sequential coding approaches. We contrast the coding styles in [1]. It is generally believed that by coding at a higher level of abstraction, one pays a price of higher overhead and thus reduced performance.
Reference: [11] <author> B. A. Hendrickson and D. E. Womble. </author> <title> The torus-wrap mapping for dense matrix calculations on massively parallel computers. </title> <journal> SIAM J. Sci. Stat. Comput., </journal> <volume> 15(5) </volume> <pages> 1201-1226, </pages> <year> 1994. </year>
Reference-contexts: Unlike ScaLAPACK, PLAPACK has a distinct approach to distributing vectors. For scalability reasons, parallel dense linear algebra algorithms must be implemented using a logical two-dimensional mesh of nodes <ref> [5, 11, 13, 15] </ref>. However, for the distribution of vectors, that two-dimensional mesh is linearized by ordering the nodes in column-major order.
Reference: [12] <author> C. L. Lawson, R. J. Hanson, D. R. Kincaid, and F. T. Krogh. </author> <title> Basic linear algebra subprograms for Fortran usage. </title> <journal> ACM Trans. Math. Soft., </journal> <volume> 5(3) </volume> <pages> 308-323, </pages> <month> Sept. </month> <year> 1979. </year>
Reference-contexts: High performance libraries for matrix operations like the various factorizations first became available with the arrival of the LAPACK [2] library in the early 90's. This library recognized the necessity to code in terms of Basic Linear Algebra Subprograms (BLAS) <ref> [12, 7, 6] </ref> for modularity and portable performance. In particular, it explores the benefits of recoding such algorithms in terms of matrix-matrix kernels like matrix-matrix multiplication in order to improve utilization of the cache of a microprocessor. fl Currently with Lockheed Martin Federal Systems, Inc., 700 N.
Reference: [13] <author> W. Lichtenstein and S. L. Johnsson. </author> <title> Block-cyclic dense linear algebra. </title> <type> Technical Report TR-04-92, </type> <institution> Harvard University, Center for Research in Computing Technology, </institution> <month> Jan. </month> <year> 1992. </year>
Reference-contexts: Unlike ScaLAPACK, PLAPACK has a distinct approach to distributing vectors. For scalability reasons, parallel dense linear algebra algorithms must be implemented using a logical two-dimensional mesh of nodes <ref> [5, 11, 13, 15] </ref>. However, for the distribution of vectors, that two-dimensional mesh is linearized by ordering the nodes in column-major order.
Reference: [14] <author> Marc Snir, Steve W. Otto, Steven Huss-Lederman, David W. Walker, and Jack Dongarra. </author> <title> MPI: The Complete Reference. </title> <publisher> The MIT Press, </publisher> <year> 1996. </year>
Reference-contexts: The PLAPACK infrastructure attempts to show that by adopting an object based coding style, already popularized by the Message-Passing Infrastructure (MPI) <ref> [10, 14] </ref>, the coding of parallel linear algebra algorithms is simplified compared to the more traditional sequential coding approaches. We contrast the coding styles in [1]. It is generally believed that by coding at a higher level of abstraction, one pays a price of higher overhead and thus reduced performance.
Reference: [15] <author> G.W. Stewart. </author> <title> Communication and matrix computations on large message passing systems. </title> <journal> Parallel Computing, </journal> <volume> 16 </volume> <pages> 27-40, </pages> <year> 1990. </year>
Reference-contexts: Unlike ScaLAPACK, PLAPACK has a distinct approach to distributing vectors. For scalability reasons, parallel dense linear algebra algorithms must be implemented using a logical two-dimensional mesh of nodes <ref> [5, 11, 13, 15] </ref>. However, for the distribution of vectors, that two-dimensional mesh is linearized by ordering the nodes in column-major order.
Reference: [16] <author> Robert A. van de Geijn. </author> <title> Using PLAPACK: Parallel Linear Algebra Package. </title> <publisher> The MIT Press, </publisher> <year> 1997. </year> <month> 16 </month>
Reference-contexts: In <ref> [8, 16] </ref> we call this approach to generating a matrix distribution from a vector distribution physically based matrix distribution. A consequence of this assignment strategy is that a column of a matrix can be redistributed like a vector by scattering the elements appropriately within rows of nodes. <p> Thus, if the block size b is relatively large, there is an advantage to redistributing the panel. Furthermore, the multivector distribution is a natural intermediate distribution for the data movement that subsequently must be performed to duplicate the data as part of the parallel symmetric rank-k update <ref> [16] </ref>. A further optimization of the parallel Cholesky factorization, for which we do not present code in Fig. 2, takes advantage of this fact. Naturally, one may get the impression that we have merely hidden all complexity and ugliness in routines like PLA Chol mv.
References-found: 16

