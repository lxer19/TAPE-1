URL: ftp://ftp.cs.rochester.edu/pub/u/jag/cvpr97final.ps.gz
Refering-URL: http://www.cs.rochester.edu/u/jag/publications.html
Root-URL: 
Title: Image Based View Synthesis of Articulated Agents  
Author: Martin Jagersand 
Web: http://www.cs.rochester.edu/u/jag  
Address: Rochester, Rochester, NY 14627  
Affiliation: Department of Computer Science, University of  
Note: In Proc. of Computer Vision and Pattern Recognition, 1997, to appear  
Abstract: Using a combination of techniques from visual representations, view synthesis, and visual-motor model estimation, we present a method for animating movements of an articulated agent (e.g. human or robot arm), without the use of any prior models or explicit 3D information. The information needed to generate simulated images can be acquired either on or off line, by watching the agent doing an arbitrary, possibly unrelated task. We present experimental results synthesizing image sequences of the simulated movement of a human arm and a PUMA 760 robot arm. Control is in either image (camera), motor (joint), or Cartesian world coordinates. We have created a user interface, where a user can input a movement program, and then upon execution, view movies of the (simulated) agent executing the program, along with the instantaneous values of the dynamics variables 
Abstract-found: 1
Intro-found: 1
Reference: [1] <institution> On-line m-peg movies of the experiments are availi-ble. </institution> <note> To see [www-video- ] references use the menu in: http://www.cs.rochester.edu/u/jag/SimAct/videos.html </note>
Reference: [2] <author> Nayar S. Nene S. Murase H. </author> <title> Subspace Methods for Robot Vision TR CUCS-06-95 CS, </title> <address> Columbia, </address> <year> 1995. </year>
Reference-contexts: In our case we will be looking at the same agent, in different poses, and all the images we want to represent are fairly similar. In this case it is advantageous to use a basis specifically for the particular agent. In summary (see also <ref> [2, 4] </ref>) this can be done by acquiring a (large) number p of size q fi q images I k ; k = 1 : : : p; I 2 &lt; (q 2 ) of the agent in dif ferent poses.
Reference: [3] <author> Rao R. Ballard D. </author> <title> An Active Vision Architecture based on Iconic Representations TR 548, </title> <type> CS, </type> <institution> University of Rochester, </institution> <year> 1995 </year>
Reference: [4] <author> Turk M. Pentland A. </author> <title> "Eigenfaces for recognition" In J of Cognitive Neuroscience v3 nr1, </title> <address> p71-86, </address> <year> 1991. </year>
Reference-contexts: In our case we will be looking at the same agent, in different poses, and all the images we want to represent are fairly similar. In this case it is advantageous to use a basis specifically for the particular agent. In summary (see also <ref> [2, 4] </ref>) this can be done by acquiring a (large) number p of size q fi q images I k ; k = 1 : : : p; I 2 &lt; (q 2 ) of the agent in dif ferent poses. <p> In previously published research <ref> [4] </ref> on linear sub space methods for recognition eigen-images U in (1) look like the objects they represent (hence the terms "eigen objects" and "eigen faces" in face recognition).
Reference: [5] <author> Cox I. J. Hinggorani S. Maggs B. M. Rao S. B. </author> <title> "Stereo without Disparity Gradient Smoothing: A Bayesian sensor fusion solution" In Proc. </title> <booktitle> of British Machine Vision p337-346 1992. </booktitle>
Reference-contexts: An advantage of using disparity is that considerable research effort has been spent on developing good "stereo" vision algorithms, in which finding the disparity between the images in the two cameras is usually a crucial step. We base our method on an algorithm by Cox et al <ref> [5] </ref>, which uses a Bayesian ML approach to do an image intensity based matching of image features searching along the epipolar lines in two images. We cannot directly apply these disparity based methods.
Reference: [6] <author> Werner T. Hersch R.D. Hlavac V. </author> <title> "Rendering Real-World Objects Using View Interpolation" In Proc. </title> <booktitle> of ICCV p957-962, </booktitle> <year> 1995. </year>
Reference-contexts: Creating the models needed for this kind of view synthesis is however a tedious, time consuming, and often manual task. Recently some results have been published on image based view synthesis, not requiring a-priori models or calibration, e.g. <ref> [6, 7, 8] </ref>. Basically these methods do image interpolation, and in some cases also extrapolation between reference views of a scene taken from different viewpoints. Viewing geometry models (e.g. affine [7], or projective [8]), and visibility constraints are used to ensure the rendering of physically possible scenes. <p> The objective has been to synthesize different viewpoints on a 1D viewing circle, or possibly a 2D viewing sphere <ref> [6, 7, 8] </ref>. An advantage of using disparity is that considerable research effort has been spent on developing good "stereo" vision algorithms, in which finding the disparity between the images in the two cameras is usually a crucial step.
Reference: [7] <author> Seitz S. Dyer C. </author> <title> "Physically-Valid View Synthesis by Image-Interpolation" In Proc. Workshop on Representations of Visual Scenes in conj. </title> <address> ICCV, </address> <year> 1995. </year>
Reference-contexts: Creating the models needed for this kind of view synthesis is however a tedious, time consuming, and often manual task. Recently some results have been published on image based view synthesis, not requiring a-priori models or calibration, e.g. <ref> [6, 7, 8] </ref>. Basically these methods do image interpolation, and in some cases also extrapolation between reference views of a scene taken from different viewpoints. Viewing geometry models (e.g. affine [7], or projective [8]), and visibility constraints are used to ensure the rendering of physically possible scenes. <p> Recently some results have been published on image based view synthesis, not requiring a-priori models or calibration, e.g. [6, 7, 8]. Basically these methods do image interpolation, and in some cases also extrapolation between reference views of a scene taken from different viewpoints. Viewing geometry models (e.g. affine <ref> [7] </ref>, or projective [8]), and visibility constraints are used to ensure the rendering of physically possible scenes. <p> The objective has been to synthesize different viewpoints on a 1D viewing circle, or possibly a 2D viewing sphere <ref> [6, 7, 8] </ref>. An advantage of using disparity is that considerable research effort has been spent on developing good "stereo" vision algorithms, in which finding the disparity between the images in the two cameras is usually a crucial step. <p> We have instead tried an approximate method, based on image rectification <ref> [7] </ref>, aligning the scan lines in the rectified images with the major direction of motion. Disparity is then measured along this line only with the 1D disparity algorithm.
Reference: [8] <author> Laveau S. Faugeras O. </author> <title> 3-D Scene Representation as a Collection of Images and Fundamental Matrices TR #2205 INRIA 1994. </title>
Reference-contexts: Creating the models needed for this kind of view synthesis is however a tedious, time consuming, and often manual task. Recently some results have been published on image based view synthesis, not requiring a-priori models or calibration, e.g. <ref> [6, 7, 8] </ref>. Basically these methods do image interpolation, and in some cases also extrapolation between reference views of a scene taken from different viewpoints. Viewing geometry models (e.g. affine [7], or projective [8]), and visibility constraints are used to ensure the rendering of physically possible scenes. <p> Basically these methods do image interpolation, and in some cases also extrapolation between reference views of a scene taken from different viewpoints. Viewing geometry models (e.g. affine [7], or projective <ref> [8] </ref>), and visibility constraints are used to ensure the rendering of physically possible scenes. <p> The objective has been to synthesize different viewpoints on a 1D viewing circle, or possibly a 2D viewing sphere <ref> [6, 7, 8] </ref>. An advantage of using disparity is that considerable research effort has been spent on developing good "stereo" vision algorithms, in which finding the disparity between the images in the two cameras is usually a crucial step.
Reference: [9] <author> Beymer D. Shashua A. Poggio T. </author> <title> Example Based Image Analysis and Synthesis AI Memo # 1431, </title> <publisher> MIT 1993. </publisher>
Reference-contexts: The goals of our work are perhaps closest to those of Beymer, Shashua and Poggio in <ref> [9] </ref>. They synthesize (and analyze) different facial expressions, based on a learned model between image feature movements and expression (pose) parameters, such as degree of smile and view point rotation. The main differences between Beymer et al's work and ours are: (1) The underlying mechanism of learning is different.
Reference: [10] <author> Jagersand M. Nelson R. </author> <title> Adaptive Differential Visual Feedback for uncalibrated hand-eye coordination and motor control TR# 579, </title> <type> U. </type> <institution> of Rochester 1994. </institution>
Reference-contexts: Our work draws from previous experience in two fields of computer vision. We combine a visual front end, based on model free image representation methods, with a visual-motor estimation or "learning" method used in optimization [13] and more recently in uncalibrated visual servoing <ref> [10, 12] </ref>. We have experimented with two kinds of visual front ends. The first is the "subspace"[2] or "eigen image"[4] approach of reducing the dimensionality of the visual input by projection onto a different basis. In the second we use a disparity ("stereo") algorithm, to measure image motion. <p> can both be generated from the representation, in the synthesis stage, and described in what we call appearance vector-space, in the analysis/learning stage. 1 A sparse representation, based on hand picked features would work also, and in fact that is what we use when solving the inverse problem, visual servoing <ref> [10, 12] </ref>. 1 In Proc. of Computer Vision and Pattern Recognition, 1997, to appear We show the view synthesis system on two example agents: a human arm and a PUMA robot arm. The system can however quite easily be trained to visually simulate any articulated agent. <p> x k ), valid around the current system configuration x k , and described by the "image"[11] or visual-motor Jacobian defined as (J j;i )(x k ) = @x i The image Jacobian not only relates visual changes to motor changes, as has been previously exploited in visual feedback control <ref> [10] </ref>, but also locally highly constrains the possible visual changes to the set of possible solutions y k+1 = J x + y k . <p> In our system the limiting factor in scaling it to high DOF agents is the visual front end. We have in previous work successfully done the visual-motor model estimation in up to 12 DOF even with non-rigidly changing agents <ref> [10] </ref>, and used that estimate for control of the agent (which typically is a harder problem than the view synthesis). With the eigen image visual front end we have done the PUMA robot simulation in up to 3 DOF.
Reference: [11] <author> Corke P. I. </author> <title> High-Performance Visual Closed-Loop Robot Control PhD thesis U of Melbourne 1994. </title>
Reference: [12] <author> Jagersand M. Nelson R. </author> <title> "Visual Space Task Specification, </title> <booktitle> Planning and Control" In Proc on IEEE Int. Symp. on Computer Vision, </booktitle> <year> 1995. </year>
Reference-contexts: Our work draws from previous experience in two fields of computer vision. We combine a visual front end, based on model free image representation methods, with a visual-motor estimation or "learning" method used in optimization [13] and more recently in uncalibrated visual servoing <ref> [10, 12] </ref>. We have experimented with two kinds of visual front ends. The first is the "subspace"[2] or "eigen image"[4] approach of reducing the dimensionality of the visual input by projection onto a different basis. In the second we use a disparity ("stereo") algorithm, to measure image motion. <p> can both be generated from the representation, in the synthesis stage, and described in what we call appearance vector-space, in the analysis/learning stage. 1 A sparse representation, based on hand picked features would work also, and in fact that is what we use when solving the inverse problem, visual servoing <ref> [10, 12] </ref>. 1 In Proc. of Computer Vision and Pattern Recognition, 1997, to appear We show the view synthesis system on two example agents: a human arm and a PUMA robot arm. The system can however quite easily be trained to visually simulate any articulated agent.
Reference: [13] <author> Fletcher R. </author> <title> Practical Methods of Optimization Chich-ester, second ed. </title> <year> 1987 </year>
Reference-contexts: Our work draws from previous experience in two fields of computer vision. We combine a visual front end, based on model free image representation methods, with a visual-motor estimation or "learning" method used in optimization <ref> [13] </ref> and more recently in uncalibrated visual servoing [10, 12]. We have experimented with two kinds of visual front ends. The first is the "subspace"[2] or "eigen image"[4] approach of reducing the dimensionality of the visual input by projection onto a different basis. <p> The on-line case is to gener ate arbitrary simulated views, representing (reasonably 3 Similar symmetric formulas (ffi i = ~ i ~ T i ) are used in optimization <ref> [13] </ref> and preserve symmetric positive definiteness of ^ J, Our problem has no symmetry constraint In Proc. of Computer Vision and Pattern Recognition, 1997, to appear small) deviations x from the current state of the real physical agent, while that same agent is executing some task and while learning and refining
Reference: [14] <author> Dahlquist G. Bjorck A. </author> <title> Numerical Methods Second Ed, </title> <publisher> Prentice Hall, </publisher> <month> 199x, </month> <type> preprint. </type>
Reference-contexts: Over a course of time our estimation method will generate a piecewise linear estimate of the visual-motor model. This is illustrated in fig.2. The size of the mesh element on which a particular Jacobian is used is determined using a trust region method <ref> [14] </ref>.
References-found: 14

