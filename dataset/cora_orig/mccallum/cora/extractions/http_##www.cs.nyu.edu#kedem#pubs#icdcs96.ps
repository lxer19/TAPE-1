URL: http://www.cs.nyu.edu/kedem/pubs/icdcs96.ps
Refering-URL: http://www.cs.nyu.edu/kedem/pubs.html
Root-URL: http://www.cs.nyu.edu
Email: schuang@cs.nyu.edu kedem@cs.nyu.edu  
Title: Supporting a Flexible Parallel Programming Model on a Network of Workstations  
Author: Shih-Chen Huang Zvi M. Kedem 
Address: New York University  
Affiliation: Department of Computer Science  
Abstract: We introduce a shared memory software prototype system for executing programs with nested parallelism on a network of workstations. This programming model exhibits a very convenient and natural programming style and provides functionality similar to a subset of Compositional C++. Such programming model is especially suitable for computations whose complexity and parallelism emerges only during their execution, as in divide and conquer problems. To both support and take advantage of the flexibility inherent in the programming model, we develop an architecture, which distributes both the shared memory management and the computation, removing bottlenecks inherent in centralization, thus also providing scalability and dependability. The system supports also dynamic load balancing, and fault toleranceboth transparently to the programmer. The prototype performs well using the realistic platforms of non-dedicated network of workstation. We describe encouraging performance experiments on a network in which some of the machines became slow unpredictably (to the application program). The system coped well with such dynamic behavior. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> J. Arrabe, A. Beguilin, B. Lowekamp, E. Seligman, M. Starkey, and P. Stephan. Dome: </author> <title> Parallel programming in a distributed computing environment. </title> <booktitle> To appear in Proc. 10th Intl. Parallel Processing Symp., </booktitle> <year> 1996. </year>
Reference-contexts: means that any shared variable can be read and written by all the siblings, however all the updates produced by the ? ? ? ? ? J 1 J 2 J 3 J 7 J 8 1: main ()// job J1 2: ... // sequential statements 3: parbegin 4: routine <ref> [1] </ref> - // job J2 5: ... // sequential statements 6: parbegin // m "happened to be" 3 in this run 7: routine [m](int w, int i) - // jobs J4, J5, J6 8: ... // sequential statements 9: - 10: parend; 11: ... // sequential statements 12: - 13: routine <p> - // job J2 5: ... // sequential statements 6: parbegin // m "happened to be" 3 in this run 7: routine [m](int w, int i) - // jobs J4, J5, J6 8: ... // sequential statements 9: - 10: parend; 11: ... // sequential statements 12: - 13: routine <ref> [1] </ref> - ... - // job J3 14: parend; 15: ... // sequential statements 16: parbegin 17: routine [1] - ... - // job J7 18: routine [1] - ... - // job J8 19: parend; 20: ... - // sequential statements sibling tasks to the shared variable must be the <p> this run 7: routine [m](int w, int i) - // jobs J4, J5, J6 8: ... // sequential statements 9: - 10: parend; 11: ... // sequential statements 12: - 13: routine <ref> [1] </ref> - ... - // job J3 14: parend; 15: ... // sequential statements 16: parbegin 17: routine [1] - ... - // job J7 18: routine [1] - ... - // job J8 19: parend; 20: ... - // sequential statements sibling tasks to the shared variable must be the same when they report to the parent. <p> // jobs J4, J5, J6 8: ... // sequential statements 9: - 10: parend; 11: ... // sequential statements 12: - 13: routine <ref> [1] </ref> - ... - // job J3 14: parend; 15: ... // sequential statements 16: parbegin 17: routine [1] - ... - // job J7 18: routine [1] - ... - // job J8 19: parend; 20: ... - // sequential statements sibling tasks to the shared variable must be the same when they report to the parent. Also, the updates to the shared variables are not visible to the siblings. <p> The effect of parallel for loop can be achieved by using the width and id arguments in the routine statement. Implementations of CC++ (e.g. as in HPC++) do not support fault tolerance, or automatic load balancing as we do. Dome <ref> [6, 1] </ref> is based on C++, and supports data parallelism. It is relevant to our work as it provides load balancing. Dome provides a library of classes and runs on top of PVM. <p> In Dome, tailor-made classes must be written to enable load balancing, requiring specialized effort for the development of new applications, including additional programming complexity, testing, and debugging. Also, the initial performance results indicate that less available machines can hold up fully available machines. For instance, according to <ref> [6, 1] </ref> an experiment was conducted to evaluate load balancing. When one slow machine is added to a set of fast machines, the overall speed dropped significantly. In contrast, in our system, when a slow machine was added to a set of fast machines, the overall speed increased.
Reference: [2] <author> Y. Aumann, Z. Kedem, K. Palem, and M. Rabin. </author> <title> Highly efficient asynchronous execution of large-grained parallel programs. </title> <booktitle> In Proc. IEEE Symp. on Foundations of Computer Science, </booktitle> <year> 1993. </year>
Reference-contexts: We start with a sketch of the execution of some specific program. The system will view it as a a set of jobs, with some dynamically maintained subset of them executing at any time. As described in <ref> [2, 14, 11, 3] </ref>, thanks to the idempotent execution strategy, a job can have several identical executing copies in the system, while maintaining exactly-once semantics. We refer to each of such copy as a task. By extending the parent/child relation among jobs, we can define a parent/child relationship among tasks.
Reference: [3] <author> A. Baratloo, P. Dasgupta, and Z. Kedem. Calypso: </author> <title> A novel software system for fault-tolerant parallel processing on distributed platforms. </title> <booktitle> In Proc. 5th IEEE Intl. Symp. on High Performance Distributed Computing, </booktitle> <year> 1995. </year>
Reference-contexts: 1. Background and key characteristics In this paper, we present a system that utilize non-dedicated networks of workstations for dependable parallel computation. Relying on some of the mechanisms in [11] and <ref> [3] </ref>, we developed a prototype software environment with additional properties beyond those in [3]: * Enhanced programmer's model, supporting a very flexible syntax and semantics for parallelism, equivalent in its fundamental aspects to a subset of the Compositional fl This research was partially supported by the National Science Foun dation under <p> 1. Background and key characteristics In this paper, we present a system that utilize non-dedicated networks of workstations for dependable parallel computation. Relying on some of the mechanisms in [11] and <ref> [3] </ref>, we developed a prototype software environment with additional properties beyond those in [3]: * Enhanced programmer's model, supporting a very flexible syntax and semantics for parallelism, equivalent in its fundamental aspects to a subset of the Compositional fl This research was partially supported by the National Science Foun dation under grant numbers CCR-94-11590 and CCR-94-21935. C++ (CC++) model [10]. <p> Then, the parent starts running again. The computation terminates when there are no running or waiting jobs. 2.2. Syntax and semantics Our formal syntax extends that of Calypso prototype <ref> [3, 4] </ref> and supports functionality similar to that of the parallel part of CC++; in fact, as we elaborate elsewhere, one of the goals of our system is the enabling of the CC++ parallel programming style on distributed platforms. (See related work in Section 5) We found it more convenient to <p> We start with a sketch of the execution of some specific program. The system will view it as a a set of jobs, with some dynamically maintained subset of them executing at any time. As described in <ref> [2, 14, 11, 3] </ref>, thanks to the idempotent execution strategy, a job can have several identical executing copies in the system, while maintaining exactly-once semantics. We refer to each of such copy as a task. By extending the parent/child relation among jobs, we can define a parent/child relationship among tasks. <p> When one slow machine is added to a set of fast machines, the overall speed dropped significantly. In contrast, in our system, when a slow machine was added to a set of fast machines, the overall speed increased. Our direct debt is to the Calypso prototype <ref> [3] </ref>. We have employed some of the mechanisms used there, mainly those supporting once-only execution semantics in the presence of load balancing and fault tolerance. Our system, provides a richer programming model, especially for computations whose complexity emerges only during the execution.
Reference: [4] <author> A. Baratloo, P. Dasgupta, M. Karaul, Z. Kedem, and F. Monrose. </author> <title> Calypso 0.8 Manual. </title> <address> New York University, </address> <year> 1994. </year>
Reference-contexts: Then, the parent starts running again. The computation terminates when there are no running or waiting jobs. 2.2. Syntax and semantics Our formal syntax extends that of Calypso prototype <ref> [3, 4] </ref> and supports functionality similar to that of the parallel part of CC++; in fact, as we elaborate elsewhere, one of the goals of our system is the enabling of the CC++ parallel programming style on distributed platforms. (See related work in Section 5) We found it more convenient to
Reference: [5] <author> D. Bakken and R. Schlichting. </author> <title> Supporting fault-tolerant parallel programming in Linda. </title> <institution> The University of Arizona, </institution> <year> 1993. </year>
Reference-contexts: Extensions to handle fault tolerance are proposed [16]. Tuple-space-based systems such as Linda [8] provide a higher level programming model, but it is unconventional and requires programmers to marshal/unmarshal data. Limited support for load balancing exists in Linda's derivative, Piranha [12]. Extensions to handle fault tolerance are proposed <ref> [5, 13] </ref>. Distributed shared memory systems employ a high level programming model, but do not provide architectural support for load balancing and fault tolerance. CC++ [10] addresses task parallelism at a high level and is close to our programming.
Reference: [6] <author> A. Beguilin, E. Seligman, and M. Starkey. Dome: </author> <title> Distributed object migration environment. </title> <address> CMU, </address> <year> 1994. </year>
Reference-contexts: The effect of parallel for loop can be achieved by using the width and id arguments in the routine statement. Implementations of CC++ (e.g. as in HPC++) do not support fault tolerance, or automatic load balancing as we do. Dome <ref> [6, 1] </ref> is based on C++, and supports data parallelism. It is relevant to our work as it provides load balancing. Dome provides a library of classes and runs on top of PVM. <p> In Dome, tailor-made classes must be written to enable load balancing, requiring specialized effort for the development of new applications, including additional programming complexity, testing, and debugging. Also, the initial performance results indicate that less available machines can hold up fully available machines. For instance, according to <ref> [6, 1] </ref> an experiment was conducted to evaluate load balancing. When one slow machine is added to a set of fast machines, the overall speed dropped significantly. In contrast, in our system, when a slow machine was added to a set of fast machines, the overall speed increased.
Reference: [7] <author> J. Bennett, J. Carter, and W. Zwaenepoel. </author> <title> Adaptive software cache management for distributed shared memory architectures. </title> <booktitle> In Proc. Intl. Symp. on Computer Architecture, </booktitle> <year> 1990. </year>
Reference-contexts: Indeed, in [15], an upper bound of approximately 3.5 for the optimal speedup for fine-grained Quicksort execution utilizing 5 processors of a parallel multiprocessor was theoretically derived. This was also confirmed by an experiment on CM5. Similar behavior was also observed in <ref> [7] </ref>. In our experiment, we used five machines in public laboratories.
Reference: [8] <author> N. Carriero and D. Gelernter. </author> <title> Linda in context. </title> <journal> Communications of the ACM, </journal> <volume> 32, </volume> <year> 1989. </year>
Reference-contexts: We do not consider message passing and remote procedure calls. These are generally efficient systems, but employ a relatively low level programming model and do not provide architectural support for load balancing and fault tolerance. Extensions to handle fault tolerance are proposed [16]. Tuple-space-based systems such as Linda <ref> [8] </ref> provide a higher level programming model, but it is unconventional and requires programmers to marshal/unmarshal data. Limited support for load balancing exists in Linda's derivative, Piranha [12]. Extensions to handle fault tolerance are proposed [5, 13].
Reference: [9] <author> J. Carter. </author> <title> Efficient Distributed Shared Memory Based On Multi-Protocal Release Consistency. </title> <type> PhD thesis, </type> <institution> Computer Science Department, Rice University, </institution> <year> 1993. </year>
Reference-contexts: We now elaborate on the memory coherence model. As stated above, a job can access the two parameters passed to it, its own local variables, and the variables declared as shared. A popular model is release consistency <ref> [9] </ref>, in which the global shared memory is not continuously updated, but the updates are applied only at well-defined specific points during the execution. Our memory consistency model is similar to release consistency, but is especially adapted to support nested parallelism in a natural manner.
Reference: [10] <author> K. Chandy and C. Kesselman. </author> <title> CC++: A declarative concurrent object-oriented programming notation. </title> <editor> In G. Agha, P. Wegner, and A. Yonezawa, editors, </editor> <booktitle> Research Directions in Concurrent Object Oriented Programming. </booktitle> <publisher> MIT Press, </publisher> <year> 1993. </year>
Reference-contexts: C++ (CC++) model <ref> [10] </ref>. Arbitrary parallelism depth is provided (by explicit definition and/or by recursion). * Execution environment in which in the computational and memory management functionality is almost fully distributed. Thus, the execution environment supports: high degree of scalability, more flexible and adaptive load balancing, and higher degree of fault masking. <p> Limited support for load balancing exists in Linda's derivative, Piranha [12]. Extensions to handle fault tolerance are proposed [5, 13]. Distributed shared memory systems employ a high level programming model, but do not provide architectural support for load balancing and fault tolerance. CC++ <ref> [10] </ref> addresses task parallelism at a high level and is close to our programming. However, CC++ divides the C++ extension into two parts, constructs for parallel machines, and constructs for distributed environments.
Reference: [11] <author> P. Dasgupta, Z. Kedem, and M. Rabin. </author> <title> Parallel processing on networks of workstations: A fault-tolerant, high performance approach. </title> <booktitle> In Proc. 15th IEEE Intl. Conf. on Distributed Computing Systems, </booktitle> <year> 1995. </year>
Reference-contexts: 1. Background and key characteristics In this paper, we present a system that utilize non-dedicated networks of workstations for dependable parallel computation. Relying on some of the mechanisms in <ref> [11] </ref> and [3], we developed a prototype software environment with additional properties beyond those in [3]: * Enhanced programmer's model, supporting a very flexible syntax and semantics for parallelism, equivalent in its fundamental aspects to a subset of the Compositional fl This research was partially supported by the National Science Foun <p> We start with a sketch of the execution of some specific program. The system will view it as a a set of jobs, with some dynamically maintained subset of them executing at any time. As described in <ref> [2, 14, 11, 3] </ref>, thanks to the idempotent execution strategy, a job can have several identical executing copies in the system, while maintaining exactly-once semantics. We refer to each of such copy as a task. By extending the parent/child relation among jobs, we can define a parent/child relationship among tasks.
Reference: [12] <author> D. Gelernter, M. Jourdenais, and D. Kaminsky. </author> <title> Piranha scheduling: Strategies and their implementation. </title> <institution> Yale University, </institution> <year> 1993. </year>
Reference-contexts: Extensions to handle fault tolerance are proposed [16]. Tuple-space-based systems such as Linda [8] provide a higher level programming model, but it is unconventional and requires programmers to marshal/unmarshal data. Limited support for load balancing exists in Linda's derivative, Piranha <ref> [12] </ref>. Extensions to handle fault tolerance are proposed [5, 13]. Distributed shared memory systems employ a high level programming model, but do not provide architectural support for load balancing and fault tolerance. CC++ [10] addresses task parallelism at a high level and is close to our programming.
Reference: [13] <author> K. Jeong and D. Shasha. Plinda 2.0: </author> <title> A transactional/checkpointing approach to fault tolerant linda. </title> <booktitle> In Proc. 13th Symp. on Reliable Distributed Systems, </booktitle> <year> 1994. </year>
Reference-contexts: Extensions to handle fault tolerance are proposed [16]. Tuple-space-based systems such as Linda [8] provide a higher level programming model, but it is unconventional and requires programmers to marshal/unmarshal data. Limited support for load balancing exists in Linda's derivative, Piranha [12]. Extensions to handle fault tolerance are proposed <ref> [5, 13] </ref>. Distributed shared memory systems employ a high level programming model, but do not provide architectural support for load balancing and fault tolerance. CC++ [10] addresses task parallelism at a high level and is close to our programming.
Reference: [14] <author> Z. Kedem, K. Palem, and P. Spirakis. </author> <title> Efficient robust parallel computations. </title> <booktitle> In Proc. 22nd ACM Symp. on Theory of Computing, </booktitle> <year> 1990. </year>
Reference-contexts: We start with a sketch of the execution of some specific program. The system will view it as a a set of jobs, with some dynamically maintained subset of them executing at any time. As described in <ref> [2, 14, 11, 3] </ref>, thanks to the idempotent execution strategy, a job can have several identical executing copies in the system, while maintaining exactly-once semantics. We refer to each of such copy as a task. By extending the parent/child relation among jobs, we can define a parent/child relationship among tasks.
Reference: [15] <author> M. Quinn. </author> <title> Designing Efficient Algorithms for Parallel Computers. </title> <publisher> McGraw-Hill, </publisher> <year> 1987. </year>
Reference-contexts: Then, two jobs work on the two subproblem. Assuming the best case with one task per job, and given p machines, for the first log p iterations, there are more machines than subproblems, limiting the possible speedup. Indeed, in <ref> [15] </ref>, an upper bound of approximately 3.5 for the optimal speedup for fine-grained Quicksort execution utilizing 5 processors of a parallel multiprocessor was theoretically derived. This was also confirmed by an experiment on CM5. Similar behavior was also observed in [7].
Reference: [16] <author> V. Sunderam, G. Geist, J. Dongarra, and R. Manchek. </author> <title> The PVM concurrent computing system: evolution, experiences, and trends. </title> <journal> Parallel Computing, </journal> <volume> 20, </volume> <year> 1994. </year>
Reference-contexts: We do not consider message passing and remote procedure calls. These are generally efficient systems, but employ a relatively low level programming model and do not provide architectural support for load balancing and fault tolerance. Extensions to handle fault tolerance are proposed <ref> [16] </ref>. Tuple-space-based systems such as Linda [8] provide a higher level programming model, but it is unconventional and requires programmers to marshal/unmarshal data. Limited support for load balancing exists in Linda's derivative, Piranha [12]. Extensions to handle fault tolerance are proposed [5, 13].
References-found: 16

