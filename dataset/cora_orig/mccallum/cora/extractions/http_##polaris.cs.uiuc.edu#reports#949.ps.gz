URL: http://polaris.cs.uiuc.edu/reports/949.ps.gz
Refering-URL: http://polaris.cs.uiuc.edu/tech_reports.html
Root-URL: http://www.cs.uiuc.edu
Title: Compiler-directed Data Prefetching in Multiprocessors with Memory Hierarchies  
Author: Edward H. Gornish Elana D. Granston Alexander V. Veidenbaum 
Address: Urbana, Illinois, 61801  
Affiliation: Center for Supercomputing Research and Development University of Illinois at Urbana-Champaign  
Abstract: Memory hierarchies are used by multiprocessor systems to reduce large memory access times. It is necessary to automatically manage such a hierarchy, to obtain effective memory utilization. In this paper, we discuss the various issues involved in obtaining an optimal memory management strategy for a memory hierarchy. We present an algorithm for finding the earliest point in a program that a block of data can be prefetched. This determination is based on the control and data dependences in the program. Such a method is an integral part of more general memory management algorithms. We demonstrate our method's potential by using static analysis to estimate the performance improvement afforded by our prefetching strategy and to analyze the reference patterns in a set of Fortran benchmarks. We also study the effectiveness of prefetching in a realistic shared-memory system using an RTL-level simulator and real codes. This differs from previous studies by considering prefetching benefits in the presence of network contention. 
Abstract-found: 1
Intro-found: 1
Reference: [CCK87] <author> David Callahan, John Cocke, and Ken Kennedy. </author> <title> Estimating interlock and improving balance for pipelined architectures. </title> <booktitle> In International Conference on Parallel Processing, </booktitle> <pages> pages 295-304, </pages> <month> August </month> <year> 1987. </year>
Reference-contexts: As is evidenced by these results, more needs to done to overcome the problem of network contention and more fully realize the potential of data prefetching. In larger codes, loop transformations can sometimes be applied to improve the balance between memory accesses and computation <ref> [CCK87] </ref>, thereby affording more opportunity to overlap memory accesses with computation. However, in simple codes, such as VEC-SUM, there is relatively little computation with which to overlap memory accesses, so it may not be possible to completely eliminate the time the processor spends waiting on data. <p> However, it can still be beneficial to overlap memory accesses with each other. If loop unrolling <ref> [CCK87] </ref> is applied before applying our prefetching/scheduling heuristic, the number of memory accesses which can be pulled out of a loop and overlapped may be increased. 13 Latency Interarrival Time Routine no-overlap/ overlap/ overlap/ no-overlap/ overlap/ overlap/ Name no-pre no-pre- prefetching no-pre no-pre- prefetching fetching fetching fetching fetching VECSUM 141 136
Reference: [DBMS79] <author> J. J. Dongarra, J. R. Bunch, C. B. Moler, and G. W. Stewart. </author> <title> Linpack User's Guide. </title> <publisher> Siam Press, </publisher> <address> Philadelphia, </address> <year> 1979. </year>
Reference-contexts: We refer to this as the reverse order. 4 Static Analysis To determine the potential of prefetching, we tested our algorithm on a set of 35 subroutines. This set is comprised of a subset of EISPACK [SBD + 76] subroutines, a subset of LINPACK <ref> [DBMS79] </ref> subroutines, and RE-ALSET, a collection of benchmark subroutines. The subsets of EISPACK and LINPACK are chosen to be representative of all the algorithms in their respective packages.
Reference: [GGK + 82] <author> Allan Gottlieb, Ralph Grishman, Clyde P. Kruskal, Kevin P. McAuliffe, Larry 14 Rudolph, and Marc Snir. </author> <title> The NYU Ul--tracomputer designing an MIMD shared memory parallel machine. </title> <booktitle> In Tenth Annual Symposium on Computer Architecture, </booktitle> <year> 1982. </year>
Reference-contexts: In addition, a pipelined asynchronous memory access mode has been provided on high-speed machines, such as Cray computers [Inc82], to further decrease the access times. Hierarchical memory systems are used in multiprocessors such as Cedar [KDLS86], the RP3 [PBG + 85] and the Ultracomputer <ref> [GGK + 82] </ref>. Unfortunately, no effective techniques currently exist for managing such hierarchical systems. There are many issues involved, and this paper attempts to address some of these issues, focusing on optimization of memory accesses in multiprocessors with multiple levels of memory. Our target architecture is a large-scale multiprocessor system.
Reference: [GJG87] <author> Dennis Gannon, William Jalby, and Kyle Gallivan. </author> <title> Strategies for cache and local memory management by global program transformation. </title> <booktitle> In 1987 International Conference on Supercomputing, </booktitle> <year> 1987. </year>
Reference-contexts: However, their methods only discuss prefetching data before the execution of the innermost loop using the data, not at the earliest possible point in the program. In <ref> [GJG87] </ref> and [Por89], the finite cache problem is addressed. Gannon et al. and Porterfield use data dependence analysis to determine the number of array elements that need to be kept in fast memory to achieve good machine performance.
Reference: [GJG88] <author> Kyle Gallivan, William Jalby, and Den-nis Gannon. </author> <title> On the problem of optimizing data transfers for complex memory systems. </title> <booktitle> In 1988 International Conference on Supercomputing, </booktitle> <month> July </month> <year> 1988. </year>
Reference-contexts: They show how program transformations, such as loop interchanging, can reduce this necessary number of elements. These techniques can also be used to reduce the amount of data that needs to be prefetched. In <ref> [GJG88] </ref>, Gallivan, Jalby and Gannon discuss issues associated with moving data between global and local memories. They present a method for deciding which data should be moved from global to local memory, and when to move it.
Reference: [Gor89] <author> Edward H. Gornish. </author> <title> Compile time analysis for data prefetching. </title> <type> Master's thesis, </type> <institution> University of Illinois at Urbana-Champaign, </institution> <month> December </month> <year> 1989. </year>
Reference-contexts: A more complete description of these two models can be found in <ref> [Gor89] </ref>. <p> The data in this section is important as it helps us identify the causes of suppression. This information can then be used to augment our algorithm to overcome some of these limitations (see <ref> [Gor89] </ref>). The data presented in Figure 6 and summarized in Table 4 show the effects of control dependence and data dependence suppression. The histograms show the number of subroutines that have a given percentage range of their references suppressed for a particular reason. <p> Only 3% of all candidates can be pulled out from an entire subroutine. This indicates that better intraprocedural results need to be achieved before interprocedural memory management becomes feasible. <ref> [Gor89] </ref> discusses ways of overcoming prefetch suppression.
Reference: [GTV90] <author> Elana D. Granston, Stephen W. Turner, and Alexander V. Veidenbaum. </author> <title> Designing a scalable shared-memory system with support for burst traffic. </title> <note> To be published, </note> <year> 1990. </year>
Reference-contexts: NASA NCC 2-559, the Department of Energy under Grant No. DE-FG02-85ER25001, the National Science Foundation under Grant No. MIP-8410110, Mips, and Cray. from network and memory conflicts can increase memory access times dramatically <ref> [Tur89, GTV90] </ref>. Memory hierarchies have traditionally been used to decrease average memory access times. These include registers, caches, additional levels of memory and disks. In addition, a pipelined asynchronous memory access mode has been provided on high-speed machines, such as Cray computers [Inc82], to further decrease the access times. <p> The best way to move data is by using the asynchronous block transfer mechanism. This pipelining of accesses allows a much higher bandwidth to be obtained from a MIN-based shared-memory subsystem, as compared to individual word transfers <ref> [GTV90] </ref>, and 1 makes it possible to overlap memory accesses and com-putation. Thus, ideally, we would like to convert each array load or store in a program into a sequence of block moves to utilize the asynchronous block transfer mechanism. <p> For simplicity, all arithmetic operations are assumed to take optime units. Table 1 shows the values we used for the various timing parameters mentioned above. These values are chosen based on MIN-based shared-memory subsystem simulations described in [Tur89] and <ref> [GTV90] </ref>. 4.1.1.2 Overlap Model In the overlap model, our static timing analysis is similar to the no-overlap model, except that we assumed that the computation of a loop body begins after the elements for the first iteration have been prefetched. <p> This section presents the results of this work. 5.1 System Configuration The system configuration is based on a class of generalized Cedar-like architectures [KDLS86]. The values of some of the parameters have been determined experimentally to provide the best cost/performance ratio as determined by random traffic studies <ref> [GTV90] </ref>. The remaining parameters are based on Cedar itself. The system is a shared-memory machine with thirty-two processors connected to an equal number of interleaved memory modules via a pair of Omega networks [Law75]. Each processor has an associated prefetch unit with connections to the forward and reverse networks.
Reference: [Hus86] <author> Harlan Husmann. </author> <title> Compiler Memory Management and Compound Function Definition for Multiprocessors. </title> <type> PhD thesis, </type> <institution> University of Illinois at Urbana-Champaign, </institution> <month> August </month> <year> 1986. </year>
Reference-contexts: Porterfield's work differs from that presented here because he only deals with prefetching from vector loops and does not attempt to determine the earliest point at which a datum can be fetched. Furthermore, he does not address multiprocessor issues. Husmann <ref> [Hus86] </ref>, Jackson [Jac85] and Lee [Lee83] discuss dependence analysis to overlap memory fetches, computation and memory stores.
Reference: [Inc82] <institution> Cray Research Inc. The Cray X-MP series of computers, </institution> <year> 1982. </year>
Reference-contexts: Memory hierarchies have traditionally been used to decrease average memory access times. These include registers, caches, additional levels of memory and disks. In addition, a pipelined asynchronous memory access mode has been provided on high-speed machines, such as Cray computers <ref> [Inc82] </ref>, to further decrease the access times. Hierarchical memory systems are used in multiprocessors such as Cedar [KDLS86], the RP3 [PBG + 85] and the Ultracomputer [GGK + 82]. Unfortunately, no effective techniques currently exist for managing such hierarchical systems.
Reference: [Jac85] <author> Daniel Thomas Jackson. </author> <title> Data movement in doall loops. </title> <type> Master's thesis, </type> <institution> University of Illinois at Urbana-Champaign, </institution> <month> May </month> <year> 1985. </year>
Reference-contexts: Porterfield's work differs from that presented here because he only deals with prefetching from vector loops and does not attempt to determine the earliest point at which a datum can be fetched. Furthermore, he does not address multiprocessor issues. Husmann [Hus86], Jackson <ref> [Jac85] </ref> and Lee [Lee83] discuss dependence analysis to overlap memory fetches, computation and memory stores. <p> This is an important concern, since we would be doing unnecessary work. If we were to prefetch too much data that would not be used, we might offset the gains derived from prefetching. Even worse, we might increase the program's execution time, instead of decreasing it. Jackson <ref> [Jac85] </ref> also discusses this issue. 2. A more serious problem might exist. If b were declared to contain only elements 1 through 5, we would be prefetching data that did not exist and would encounter an error. These two problems cause prefetches to be suppressed by control dependences.
Reference: [KDLS86] <author> David. J. Kuck, E. S. Davidson, D. H. Lawrie, and A. H. Sameh. </author> <title> Parallel supercomputing today and the Cedar approach. </title> <journal> Science, </journal> <volume> 231 </volume> <pages> 967-974, </pages> <month> February </month> <year> 1986. </year>
Reference-contexts: These include registers, caches, additional levels of memory and disks. In addition, a pipelined asynchronous memory access mode has been provided on high-speed machines, such as Cray computers [Inc82], to further decrease the access times. Hierarchical memory systems are used in multiprocessors such as Cedar <ref> [KDLS86] </ref>, the RP3 [PBG + 85] and the Ultracomputer [GGK + 82]. Unfortunately, no effective techniques currently exist for managing such hierarchical systems. <p> This section presents the results of this work. 5.1 System Configuration The system configuration is based on a class of generalized Cedar-like architectures <ref> [KDLS86] </ref>. The values of some of the parameters have been determined experimentally to provide the best cost/performance ratio as determined by random traffic studies [GTV90]. The remaining parameters are based on Cedar itself.
Reference: [KKLW80] <author> David J. Kuck, R. H. Kuhn, B. Leasure, and M. Wolfe. </author> <title> The structure of an advanced vectorizer for pipelined processors. </title> <booktitle> In Fourth International Computer Software and Applications Conference, </booktitle> <month> Octo-ber </month> <year> 1980. </year>
Reference-contexts: In this case, we might be able to prefetch the elements that would not be modified before they are used. 3.3 Algorithm We have implemented an algorithm to execute the method of prefetching, described in Section 3, using PARAFRASE <ref> [KKLW80, Wol82] </ref>. PARAFRASE is a source-to-source Fortran translator for parallel machines. It automatically detects parallel loops in a program and performs transformations that aid in the detection of parallelism. In the previous sections, we discussed pulling references out from loops.
Reference: [Law75] <author> Duncan H. Lawrie. </author> <title> Access and alignment of data in an array processor. </title> <journal> IEEE Transactions on Computers, </journal> <volume> C-24(12):173-183, </volume> <month> December </month> <year> 1975. </year>
Reference-contexts: The remaining parameters are based on Cedar itself. The system is a shared-memory machine with thirty-two processors connected to an equal number of interleaved memory modules via a pair of Omega networks <ref> [Law75] </ref>. Each processor has an associated prefetch unit with connections to the forward and reverse networks. The prefetch unit can both produce and consume a single word on each clock cycle. Data paths in the network are one word wide.
Reference: [Lee83] <author> Kyungsook Yoon Lee. </author> <title> Interconnection Networks and Compiler Algorithms for Multiprocessors. </title> <type> PhD thesis, </type> <institution> University of Illi-nois at Urbana-Champaign, </institution> <year> 1983. </year>
Reference-contexts: Porterfield's work differs from that presented here because he only deals with prefetching from vector loops and does not attempt to determine the earliest point at which a datum can be fetched. Furthermore, he does not address multiprocessor issues. Husmann [Hus86], Jackson [Jac85] and Lee <ref> [Lee83] </ref> discuss dependence analysis to overlap memory fetches, computation and memory stores.
Reference: [Lee87] <author> Roland Lun Lee. </author> <title> The Effectiveness of Caches and Data Prefetch Buffers in Large-Scale Shared Memory Multiprocessors. </title> <type> PhD thesis, </type> <institution> University of Illinois at Urbana-Champaign, </institution> <month> May </month> <year> 1987. </year>
Reference-contexts: Some of these studies have dealt specifically with data prefetching, while others have not; however, the techniques involved are related, in either case. Furthermore, the results generated by some of the studies that do not explicitly examine data prefetching can still be applied to data prefetching. Lee <ref> [Lee87] </ref> and Marcovitz [Mar88] use a run-time lookahead scheme to prefetch data that has been marked cacheable. A reference is non-cacheable if it is accessed by more than one processor and written to by at least one processor; otherwise it is cacheable.
Reference: [Lov77] <author> D. B. Loveman. </author> <title> Program improvement by source-to-source transformation. </title> <journal> Journal of the ACM, </journal> <volume> 24(1) </volume> <pages> 121-145, </pages> <month> January </month> <year> 1977. </year>
Reference-contexts: Example 13 prefetch (a (1:n)) do i = 1 to n = a (i) + b (j*j) enddo enddo To ensure that prefetches do not exceed the maximum block length, all loops are stripmined <ref> [Lov77] </ref> into thirty-two word blocks. 5.4 Experimental Results The PARAFRASE Fortran compiler has been extended to generate pseudo-assembly level output. This output is then used to drive our simulator. Each benchmark routine has been compiled with our prefetch-ing/scheduling heuristic (prefetch) and without (no-prefetch).
Reference: [Mar88] <author> David M. Marcovitz. </author> <title> A multiprocessor cache performance metric. </title> <type> Master's thesis, </type> <institution> University of Illinois at Urbana-Champaign, </institution> <month> October </month> <year> 1988. </year>
Reference-contexts: Furthermore, the results generated by some of the studies that do not explicitly examine data prefetching can still be applied to data prefetching. Lee [Lee87] and Marcovitz <ref> [Mar88] </ref> use a run-time lookahead scheme to prefetch data that has been marked cacheable. A reference is non-cacheable if it is accessed by more than one processor and written to by at least one processor; otherwise it is cacheable.
Reference: [PBG + 85] <author> Gregory F. Pfister, W. C. Brantley, D. A. George, S. L. Harvey, W. J. Kleinfeider, K. P. McAuliffe, E. A. Melton, V. A. Nor-ton, and J. Weiss. </author> <title> The IBM Research Parallel Processor Prototype (RP3): Introduction and architecture. </title> <booktitle> In International Conference on Parallel Processing, </booktitle> <month> August </month> <year> 1985. </year>
Reference-contexts: These include registers, caches, additional levels of memory and disks. In addition, a pipelined asynchronous memory access mode has been provided on high-speed machines, such as Cray computers [Inc82], to further decrease the access times. Hierarchical memory systems are used in multiprocessors such as Cedar [KDLS86], the RP3 <ref> [PBG + 85] </ref> and the Ultracomputer [GGK + 82]. Unfortunately, no effective techniques currently exist for managing such hierarchical systems. There are many issues involved, and this paper attempts to address some of these issues, focusing on optimization of memory accesses in multiprocessors with multiple levels of memory.
Reference: [Por89] <author> Allan K. Porterfield. </author> <title> Software Methods for Improvement of Cache Cache Performance on Supercomputer Applications. </title> <type> PhD thesis, </type> <institution> Rice University, </institution> <month> May </month> <year> 1989. </year>
Reference-contexts: Furthermore, they do not determine their prefetching window at compile time as we do. Rather, the prefetching unit determines at run-time when a particular datum can be prefetched. This requires very complex hardware. Porterfield <ref> [Por89] </ref> compares hardware lookahead with the compile-time prefetching technique of fetching vector data one iteration before it is used. <p> However, their methods only discuss prefetching data before the execution of the innermost loop using the data, not at the earliest possible point in the program. In [GJG87] and <ref> [Por89] </ref>, the finite cache problem is addressed. Gannon et al. and Porterfield use data dependence analysis to determine the number of array elements that need to be kept in fast memory to achieve good machine performance.
Reference: [SBD + 76] <author> B. T. Smith, J. M. Boyle, J. J. Dongarra, B. S. Garbow, Y. Ikebe, V. C. Klema, and C. B. Moler. </author> <title> Matrix Eigensystem Routines|Eispack Guide. </title> <publisher> Springer-Verlag, </publisher> <address> Heidelberg, </address> <year> 1976. </year>
Reference-contexts: We refer to this as the reverse order. 4 Static Analysis To determine the potential of prefetching, we tested our algorithm on a set of 35 subroutines. This set is comprised of a subset of EISPACK <ref> [SBD + 76] </ref> subroutines, a subset of LINPACK [DBMS79] subroutines, and RE-ALSET, a collection of benchmark subroutines. The subsets of EISPACK and LINPACK are chosen to be representative of all the algorithms in their respective packages.
Reference: [Tur89] <author> Stephen Wilson Turner. </author> <title> Shared memory and interconnection network performance for vector multiprocessors. </title> <type> Master's thesis, </type> <institution> University of Illinois at Urbana-Champaign, </institution> <month> May </month> <year> 1989. </year>
Reference-contexts: NASA NCC 2-559, the Department of Energy under Grant No. DE-FG02-85ER25001, the National Science Foundation under Grant No. MIP-8410110, Mips, and Cray. from network and memory conflicts can increase memory access times dramatically <ref> [Tur89, GTV90] </ref>. Memory hierarchies have traditionally been used to decrease average memory access times. These include registers, caches, additional levels of memory and disks. In addition, a pipelined asynchronous memory access mode has been provided on high-speed machines, such as Cray computers [Inc82], to further decrease the access times. <p> For simplicity, all arithmetic operations are assumed to take optime units. Table 1 shows the values we used for the various timing parameters mentioned above. These values are chosen based on MIN-based shared-memory subsystem simulations described in <ref> [Tur89] </ref> and [GTV90]. 4.1.1.2 Overlap Model In the overlap model, our static timing analysis is similar to the no-overlap model, except that we assumed that the computation of a loop body begins after the elements for the first iteration have been prefetched.
Reference: [Wol82] <author> Michael Joseph Wolfe. </author> <title> Optimizing Compilers for Supercomputers. </title> <type> PhD thesis, </type> <institution> University of Illinois at Urbana-Champaign, </institution> <month> October </month> <year> 1982. </year> <month> 15 </month>
Reference-contexts: The organization of the memory hierarchy also plays a role in determining the optimal compilation strategy. Thus, the following issues determine when data moves can both legally and efficiently take place: 1. control dependences <ref> [Wol82] </ref> 2. data dependences [Wol82] 3. memory coherence 4. data reuse 5. interference with other data moves 6. overlap between data moves and computation 7. sizes of the different memories in the hierarchy 8. organization of the memory hierarchy. <p> The organization of the memory hierarchy also plays a role in determining the optimal compilation strategy. Thus, the following issues determine when data moves can both legally and efficiently take place: 1. control dependences <ref> [Wol82] </ref> 2. data dependences [Wol82] 3. memory coherence 4. data reuse 5. interference with other data moves 6. overlap between data moves and computation 7. sizes of the different memories in the hierarchy 8. organization of the memory hierarchy. In this paper, we will concentrate on the problem of data prefetching. <p> More complicated analysis can be used to determine if a reference can be pulled out from an exit loop. This involves splitting out from the rest of the loop those parts of the loop upon which the exit test is dependent <ref> [Wol82] </ref>. However, since only 4% of the loops we have encountered in our experiments are exit loops, we simply treat all references in exit loops as being suppressed. If a statement branches out from multiple loops, then all the references in those loops are suppressed. <p> In this case, we might be able to prefetch the elements that would not be modified before they are used. 3.3 Algorithm We have implemented an algorithm to execute the method of prefetching, described in Section 3, using PARAFRASE <ref> [KKLW80, Wol82] </ref>. PARAFRASE is a source-to-source Fortran translator for parallel machines. It automatically detects parallel loops in a program and performs transformations that aid in the detection of parallelism. In the previous sections, we discussed pulling references out from loops.
References-found: 22

