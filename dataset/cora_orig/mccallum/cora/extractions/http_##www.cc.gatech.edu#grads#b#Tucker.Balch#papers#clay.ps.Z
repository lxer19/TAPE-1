URL: http://www.cc.gatech.edu/grads/b/Tucker.Balch/papers/clay.ps.Z
Refering-URL: http://www.cc.gatech.edu/grads/b/Tucker.Balch/papers/
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Email: tucker@cc.gatech.edu  
Title: Clay: Integrating Motor Schemas and Reinforcement Learning  
Author: Tucker Balch 
Web: GIT-CC-97-11  
Address: Atlanta, Georgia 30332-0280  
Affiliation: Mobile Robot Laboratory College of Computing Georgia Institute of Technology  
Abstract: Clay is an evolutionary architecture for autonomous robots that integrates motor schema-based control and reinforcement learning. Robots utilizing Clay benefit from the real-time performance of motor schemas in continuous and dynamic environments while taking advantage of adaptive reinforcement learning. Clay coordinates assemblages (groups of motor schemas) using embedded reinforcement learning modules. The coordination modules activate specific assemblages based on the presently perceived situation. Learning occurs as the robot selects assemblages and samples a reinforcement signal over time. Experiments in a robot soccer simulation illustrate the performance and utility of the system.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> R.C. Arkin. </author> <title> Motor schema based mobile robot navigation. </title> <journal> International Journal of Robotics Research, </journal> <volume> 8(4) </volume> <pages> 92-112, </pages> <year> 1989. </year>
Reference-contexts: It is important to note that the entire field is never computed, only the vectors for the robot's current location. Individual motor schemas, or primitive behaviors, express separate goals or constraints for a task <ref> [1] </ref>. As an example, important schemas for a navigational task would include avoid obstacles and move to goal. Since schemas are independent, they can run concurrently, providing parallelism and speed. Sensor input is processed by perceptual schemas embedded in the motor behaviors. <p> In the example figure an entire field is shown, but this is only for visualization purposes. Problems with local minima, maxima, and cyclic behavior which are endemic to many potential fields strategies are handled by several methods including: the injection of noise into the system <ref> [1] </ref>; resorting to high-level planning; repulsion from previously visited locales [4]; continuous adaptation [5]; and other learning strategies [16, 17]. <p> This is illustrated in the declaration move to backfield. move to backfield is a weighted combination of move to ball and a new schema, stay near goal: defended_goal = new PerceptSchemaDefendedGoal (m); stay_near_goal = new MotorSchemaLinearAttraction (1.0, 0.25, defended_goal); move_to_backfield_schemas [0] = move_to_ball; move_to_backfield_gains [0] = 0.5; move_to_backfield_schemas <ref> [1] </ref> = stay_near_goal; move_to_backfield_gains [1] = 1.5; move_to_backfield = new CoordinateSum (move_to_backfield_schemas, move_to_backfield_gains); The first two lines declare stay near goal as a linear attraction motor schema configured to move the robot towards its defended goal. <p> illustrated in the declaration move to backfield. move to backfield is a weighted combination of move to ball and a new schema, stay near goal: defended_goal = new PerceptSchemaDefendedGoal (m); stay_near_goal = new MotorSchemaLinearAttraction (1.0, 0.25, defended_goal); move_to_backfield_schemas [0] = move_to_ball; move_to_backfield_gains [0] = 0.5; move_to_backfield_schemas <ref> [1] </ref> = stay_near_goal; move_to_backfield_gains [1] = 1.5; move_to_backfield = new CoordinateSum (move_to_backfield_schemas, move_to_backfield_gains); The first two lines declare stay near goal as a linear attraction motor schema configured to move the robot towards its defended goal. The parameters 1.0 and 0.25 specify ranges at which the schema's magnitude is maximized and minimized, respectively. <p> Finally, the component schemas are coordinated by gain multiplication and summation. The get behind ball is declared similarly. Once the primitive behaviors have been combined as assemblages, they are coordinated as follows. In the goalie configuration: behind_ball = new PerceptFeatureBehindBall (m); assemblages [0] = get_behind_ball; assemblages <ref> [1] </ref> = move_to_backfield; top_level = new CoordinateSelection (assemblages, behind_ball); top level is the output of a selection operator which chooses between get behind ball and move to backfield depending on whether the robot is behind the ball. assemblages [0], or get behind ball, is selected when behind ball == 0. assemblages <p> = move_to_backfield; top_level = new CoordinateSelection (assemblages, behind_ball); top level is the output of a selection operator which chooses between get behind ball and move to backfield depending on whether the robot is behind the ball. assemblages [0], or get behind ball, is selected when behind ball == 0. assemblages <ref> [1] </ref>, or move to backfield is selected when behind ball == 1. This completes the specification of a goalie robot's behavior. <p> Here is how a Q-learning soccer robot might be configured: learner = new LearnerQ (states, actions, alpha, gamma, randomrate, randomdecay); reward = new RewardOnScore (m); assemblages [0] = move_to_ball; assemblages <ref> [1] </ref> = get_behind_ball; assemblages [2] = move_to_backfield; top_level = new CoordinateLearner (assemblages, behind_ball, reward, learner); First, a Q-learning module is instantiated (the parameters aren't important for this discussion). Next a reward schema is instantiated. RewardOnScore, is one of several potentially useful reward functions for soccer.
Reference: [2] <author> R.C. Arkin and T.R. Balch. Aura: </author> <title> principles and practice in review. </title> <journal> Journal of Experimental and Theoretical Artificial Intelligence, </journal> <note> in press, </note> <year> 1997. </year>
Reference-contexts: The motor schema paradigm is the central method in use at the Georgia Tech Mobile Robot Laboratory, and is the platform for this research. Motor schemas are the reactive component of Arkin's Autonomous Robot Architecture (AuRA) <ref> [2] </ref>. AuRA's design integrates deliberative planning at a top level with behavior-based motor control at the bottom. The lower levels, concerned with executing the reactive behaviors are incorporated in this research. 1 corresponding to a move-to-goal schema, pulling the robot to a location on the right. <p> Second, Clay's primitive, the motor schema, provides a rich repertoire for behavioral design <ref> [2] </ref>. Motor schemas take full advantage of continuous sensor values and can generate an infinite range of actuator output; most of the other approaches only select from a discrete list of actions. <p> Here is how a Q-learning soccer robot might be configured: learner = new LearnerQ (states, actions, alpha, gamma, randomrate, randomdecay); reward = new RewardOnScore (m); assemblages [0] = move_to_ball; assemblages [1] = get_behind_ball; assemblages <ref> [2] </ref> = move_to_backfield; top_level = new CoordinateLearner (assemblages, behind_ball, reward, learner); First, a Q-learning module is instantiated (the parameters aren't important for this discussion). Next a reward schema is instantiated. RewardOnScore, is one of several potentially useful reward functions for soccer.
Reference: [3] <author> R.C. </author> <title> Arkin and D.C. MacKenzie. Temporal coordination of perceptual algorithms for mobile robot navigation. </title> <journal> IEEE Transactions on Robotics and Automation, </journal> <volume> 10(3) </volume> <pages> 276-286, </pages> <year> 1994. </year>
Reference-contexts: The steps in the sequence are separate behavioral states. Perceptual events that cause transitions from one behavioral state to another are called perceptual triggers. The resulting task-solving strategy can be represented as a Finite State Automaton (FSA). The technique is referred to as temporal sequencing <ref> [3] </ref>. As an example use of temporal sequencing, consider the strategy for a robot soccer player. The salient issue for now is that points are scored by bumping the ball across the opponent's goal.
Reference: [4] <author> T. </author> <title> Balch and R.C. Arkin. Avoiding the past: a simple but effective strategy for reactive navigation. </title> <booktitle> In IEEE Conference on Robotics and Automation, </booktitle> <pages> pages 678-685. </pages> <publisher> IEEE, </publisher> <month> May </month> <year> 1993. </year> <institution> Atlanta, Georgia. </institution>
Reference-contexts: Problems with local minima, maxima, and cyclic behavior which are endemic to many potential fields strategies are handled by several methods including: the injection of noise into the system [1]; resorting to high-level planning; repulsion from previously visited locales <ref> [4] </ref>; continuous adaptation [5]; and other learning strategies [16, 17]. Schema-based robot control has been demonstrated to provide robust navigation in complex and dynamic worlds. 1.2 Temporal Sequencing As illustrated above for navigation, motor schemas may be grouped to form more complex, emergent behaviors.
Reference: [5] <author> R.J. Clark, R.C. Arkin, and A. Ram. </author> <title> Learning momentum: On-line performance enhancement for reactive systems. </title> <booktitle> In IEEE Conf. on Robotics and Automation, </booktitle> <pages> pages 111-116. </pages> <publisher> IEEE, </publisher> <month> May </month> <year> 1992. </year> <institution> Nice, France. </institution>
Reference-contexts: The importance of motor schemas relative to each other is indicated by a gain value for each one. The gain is usually set by a human designer, but may also be determined through automatic means, including on-line learning <ref> [5] </ref>, case-based reasoning [17] or genetic algorithms [16]. Each motor vector is multiplied by the associated gain value and the results are summed and normalized. The resultant vector is sent to the robot hardware for execution. An example of this process is illustrated in Figure 1. <p> Problems with local minima, maxima, and cyclic behavior which are endemic to many potential fields strategies are handled by several methods including: the injection of noise into the system [1]; resorting to high-level planning; repulsion from previously visited locales [4]; continuous adaptation <ref> [5] </ref>; and other learning strategies [16, 17]. Schema-based robot control has been demonstrated to provide robust navigation in complex and dynamic worlds. 1.2 Temporal Sequencing As illustrated above for navigation, motor schemas may be grouped to form more complex, emergent behaviors. Groups of behaviors are referred to as behavioral assemblages.
Reference: [6] <author> C. Connolly and R. Grupen. </author> <title> On the applications of harmonic functions to robotics. </title> <journal> Journal of Robotic Systems, </journal> <volume> 10(7) </volume> <pages> 931-936, </pages> <year> 1993. </year>
Reference-contexts: Each motor vector is multiplied by the associated gain value and the results are summed and normalized. The resultant vector is sent to the robot hardware for execution. An example of this process is illustrated in Figure 1. The approach bears a strong resemblance to potential field methods <ref> [6, 9, 8] </ref>, but with an important difference: the entire field is never computed, only the robot's reaction to its current perception of the world at its present location. In the example figure an entire field is shown, but this is only for visualization purposes.
Reference: [7] <author> L.P. Kaelbling, M.L. Littman, and A.W. Moore. </author> <title> Reinforcement learning: A survey. </title> <journal> Journal of Artificial Intelligence Research, </journal> <volume> 4 </volume> <pages> 237-285, </pages> <year> 1996. </year>
Reference-contexts: The guarantees are tempered by rather strong conditions for convergence; Q-learning for example, requires all actions to be repeatedly sampled in all states. The reader is referred to Kaelbling <ref> [7] </ref> for an excellent survey of reinforcement learning in robotics. Reinforcement learning methods fall into two broad groups: model-based and model-free. Model-free systems like Q-learning are computationally simple, but require many experience steps to converge.
Reference: [8] <author> O. Khatib. </author> <title> Real-time obstacle avoidance for manipulators and mobile robots. </title> <booktitle> In Proceedings 1985 IEEE Conference on Robotics and Automation, </booktitle> <pages> page 500, </pages> <address> St. Louis, </address> <year> 1985. </year>
Reference-contexts: Each motor vector is multiplied by the associated gain value and the results are summed and normalized. The resultant vector is sent to the robot hardware for execution. An example of this process is illustrated in Figure 1. The approach bears a strong resemblance to potential field methods <ref> [6, 9, 8] </ref>, but with an important difference: the entire field is never computed, only the robot's reaction to its current perception of the world at its present location. In the example figure an entire field is shown, but this is only for visualization purposes.
Reference: [9] <author> B. Krogh. </author> <title> A generalized potential field approach to obstacle avoidance control. </title> <type> Sme - ri technical paper, </type> <year> 1984. </year> <month> MS84-484. </month>
Reference-contexts: Each motor vector is multiplied by the associated gain value and the results are summed and normalized. The resultant vector is sent to the robot hardware for execution. An example of this process is illustrated in Figure 1. The approach bears a strong resemblance to potential field methods <ref> [6, 9, 8] </ref>, but with an important difference: the entire field is never computed, only the robot's reaction to its current perception of the world at its present location. In the example figure an entire field is shown, but this is only for visualization purposes.
Reference: [10] <author> Long-Ji Lin. </author> <title> Hierachical learning of robot skills by reinforcement. </title> <booktitle> In International Conference on Neural Networks, </booktitle> <year> 1993. </year>
Reference-contexts: Since the state space is rather large, Mahadevan sought ways to reduce it, including weighted Hamming distance and statistical clustering to group similar states. Using this approach, their robot, OBELIX was able to learn to perform better than hand-coded behaviors for box-pushing. In research at Carnegie Mellon University <ref> [10] </ref>, Lin developed a method for Q-learning to be applied hierarchically, so that complex tasks are learned at several levels.
Reference: [11] <author> D.C. MacKenzie, J.M. Cameron, </author> <title> and R.C. Arkin. Specification and execution of multiagent missions. </title> <note> 1995. To appear IROS-1995. </note>
Reference-contexts: Georgia Tech's Mobile Robot Laboratory has developed MissionLab to support the design and test of sequenced behaviors on robots and in simulation <ref> [11] </ref>. MissionLab includes a set of tools for recursively expressing sequenced behaviors. Behaviors can be designed graphically, using a the cfgedit tool, or textually in the Configuration Network Language (CNL). Clay draws from CNL but extends it in several important ways.
Reference: [12] <author> P. Maes and R. Brooks. </author> <title> Learning to coordinate behaviors. </title> <booktitle> In Proceedings, Eighth National Conference on Artificial Intelligence (AAAI-90), </booktitle> <pages> pages 796-802. </pages> <publisher> AAAI, </publisher> <year> 1990. </year>
Reference-contexts: The reinforcement learning approaches outlined so far use a centralized scheme for learning when particular sub-behaviors should be activated. Maes and Brooks <ref> [12] </ref> propose a alternative, distributed mechanism. In their approach, each behavior learns for itself when it ought to be applied. They pre-define a set of behaviors and a set of binary perceptual conditions. Each behavior learns when it should be "on" or "off" based the perceptual conditions.
Reference: [13] <author> Sridhar Mahadevan and Jonathan Connell. </author> <title> Automatic programming of behavior-based robots using reinforcement learning. </title> <booktitle> Artificial Intelligence, </booktitle> <pages> pages 311-365, </pages> <year> 1992. </year>
Reference-contexts: Q-learning then, is one way for a robot to learn appropriate sequences of action to attain a goal. Mahadevan and Connell <ref> [13] </ref> have applied it in a slightly different manner: to learn the component behaviors within a pre-defined sequence. The particular task they investigate is for a robot to find, then push a box across a room.
Reference: [14] <author> M. Mataric. </author> <title> Designing emergent behaviors: From local interactions to collective intelligence. </title> <booktitle> In Proceedings of the International Conference on Simulation of Adaptive Behavior: From Animals to Animats 2, </booktitle> <pages> pages 432-441, </pages> <year> 1992. </year>
Reference-contexts: Each behavior learns when it should be "on" or "off" based the perceptual conditions. Positive and negative feedback are provided to guide the learning. The approach was demonstrated on a learning robotic hexapod. Mataric's research has focused on developing specialized reinforcement functions for social learning <ref> [14, 15] </ref>. The overall reinforcement, R (t), for each robot is composed of separate components, D; O and V . D indicates progress towards the agent's present goal. O provides a reinforcement if the present action is a repetition of another agent's behavior.
Reference: [15] <author> M. Mataric. </author> <title> Learning to behave socially. </title> <booktitle> In Proceedings of the International Conference on Simulation of Adaptive Behavior: From Animals to Animats 3, </booktitle> <year> 1994. </year>
Reference-contexts: Each behavior learns when it should be "on" or "off" based the perceptual conditions. Positive and negative feedback are provided to guide the learning. The approach was demonstrated on a learning robotic hexapod. Mataric's research has focused on developing specialized reinforcement functions for social learning <ref> [14, 15] </ref>. The overall reinforcement, R (t), for each robot is composed of separate components, D; O and V . D indicates progress towards the agent's present goal. O provides a reinforcement if the present action is a repetition of another agent's behavior.
Reference: [16] <author> M. Pearce, R.C. Arkin, and A. Ram. </author> <title> The learning of reactive control parameters through genetic alg orithms. </title> <booktitle> In IEEE/RSJ Int. Conf. on Intelligent Robots and Systems, </booktitle> <pages> pages 130-137. </pages> <booktitle> IEEE/RSJ, 1992. </booktitle> <address> Raleigh, NC. </address>
Reference-contexts: The importance of motor schemas relative to each other is indicated by a gain value for each one. The gain is usually set by a human designer, but may also be determined through automatic means, including on-line learning [5], case-based reasoning [17] or genetic algorithms <ref> [16] </ref>. Each motor vector is multiplied by the associated gain value and the results are summed and normalized. The resultant vector is sent to the robot hardware for execution. An example of this process is illustrated in Figure 1. <p> Problems with local minima, maxima, and cyclic behavior which are endemic to many potential fields strategies are handled by several methods including: the injection of noise into the system [1]; resorting to high-level planning; repulsion from previously visited locales [4]; continuous adaptation [5]; and other learning strategies <ref> [16, 17] </ref>. Schema-based robot control has been demonstrated to provide robust navigation in complex and dynamic worlds. 1.2 Temporal Sequencing As illustrated above for navigation, motor schemas may be grouped to form more complex, emergent behaviors. Groups of behaviors are referred to as behavioral assemblages.
Reference: [17] <author> A. Ram and J.C. Santamara. </author> <title> Multistrategy learning in reactive control. </title> <journal> Infor-matica, </journal> <volume> 17(4) </volume> <pages> 347-369, </pages> <year> 1993. </year>
Reference-contexts: The importance of motor schemas relative to each other is indicated by a gain value for each one. The gain is usually set by a human designer, but may also be determined through automatic means, including on-line learning [5], case-based reasoning <ref> [17] </ref> or genetic algorithms [16]. Each motor vector is multiplied by the associated gain value and the results are summed and normalized. The resultant vector is sent to the robot hardware for execution. An example of this process is illustrated in Figure 1. <p> Problems with local minima, maxima, and cyclic behavior which are endemic to many potential fields strategies are handled by several methods including: the injection of noise into the system [1]; resorting to high-level planning; repulsion from previously visited locales [4]; continuous adaptation [5]; and other learning strategies <ref> [16, 17] </ref>. Schema-based robot control has been demonstrated to provide robust navigation in complex and dynamic worlds. 1.2 Temporal Sequencing As illustrated above for navigation, motor schemas may be grouped to form more complex, emergent behaviors. Groups of behaviors are referred to as behavioral assemblages.
Reference: [18] <author> Richard S. Sutton. DYNA, </author> <title> an Integrated Architecture for Learning, Planning and Reacting. </title> <booktitle> In Working Notes of the AAAI Spring Symposium on Integrated Intelligent Architectures, </booktitle> <month> March </month> <year> 1991. </year>
Reference-contexts: The reader is referred to Kaelbling [7] for an excellent survey of reinforcement learning in robotics. Reinforcement learning methods fall into two broad groups: model-based and model-free. Model-free systems like Q-learning are computationally simple, but require many experience steps to converge. Model-based systems like Dyna <ref> [18] </ref> seek to reduce the cost of experience in the real-world (as in risk of damage perceptual assemblage feature mtb gbb mtb f not behind ball 0 1 0 behind ball 1 0 0 Control Team Forward perceptual assemblage feature mtb gbb mtb f not behind ball 0 1 0 behind
Reference: [19] <author> J. Tsitsiklis and B Van Roy. </author> <title> An analysis of temporal-difference learning with function approximation. </title> <type> Technical report, </type> <institution> M.I.T. Laboratory for Infomation and Decision Systems, </institution> <year> 1996. </year> <note> Available at http://donald-duck.mit.edu/lids. </note>
Reference-contexts: If the task is feasible, and feedback regarding how well the agent is doing is provided, several reinforcement learning techniques are guaranteed to converge (within an arbitrary *) to the optimal solution <ref> [20, 19] </ref>. The guarantees are tempered by rather strong conditions for convergence; Q-learning for example, requires all actions to be repeatedly sampled in all states. The reader is referred to Kaelbling [7] for an excellent survey of reinforcement learning in robotics.
Reference: [20] <author> Christopher J. C. H. Watkins and Peter Dayan. </author> <title> Technical note: Q learning. </title> <journal> Machine Learning, </journal> <volume> 8 </volume> <pages> 279-292, </pages> <year> 1992. </year>
Reference-contexts: If the task is feasible, and feedback regarding how well the agent is doing is provided, several reinforcement learning techniques are guaranteed to converge (within an arbitrary *) to the optimal solution <ref> [20, 19] </ref>. The guarantees are tempered by rather strong conditions for convergence; Q-learning for example, requires all actions to be repeatedly sampled in all states. The reader is referred to Kaelbling [7] for an excellent survey of reinforcement learning in robotics. <p> If the function is properly computed, an agent can act optimally simply by looking up the best-valued action for any situation. The problem is to find the Q (s; a) that provides an optimal policy. Watkins <ref> [20] </ref> has developed an algorithm for determining Q (s; a) that converges to optimal. He prefers to represent Q (s; a) as a table, Q [s; a], and asserts in [20] that the algorithm is not guaranteed to converge otherwise. <p> The problem is to find the Q (s; a) that provides an optimal policy. Watkins <ref> [20] </ref> has developed an algorithm for determining Q (s; a) that converges to optimal. He prefers to represent Q (s; a) as a table, Q [s; a], and asserts in [20] that the algorithm is not guaranteed to converge otherwise.
References-found: 20

