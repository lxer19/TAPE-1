URL: http://cobar.cs.umass.edu/pubfiles/mm-19.ps
Refering-URL: http://cobar.cs.umass.edu/pubfiles/
Root-URL: 
Email: Email:fvwu,manmathag@cs.umass.edu  
Title: DOCUMENT IMAGE CLEAN-UP AND  
Author: BINARIZATION Victor Wu and R. Manmatha 
Keyword: binarization, background removal, OCR, text extraction  
Date: December 18, 1997  
Address: Amherst, MA 01003-4610  
Affiliation: Multimedia Indexing And Retrieval Group Computer Science Department University of Massachusetts,  
Abstract: Image binarization is a difficult task for documents with text over textured or shaded backgrounds, poor contrast, and/or considerable noise. Current optical character recognition (OCR) and document analysis technology do not handle such documents well. We have developed a simple yet effective algorithm for document image clean-up and bina-rization. The algorithm consists of two basic steps. In the first step, the input image is smoothed using a low-pass (Gaussian) filter. The smoothing operation enhances the text relative to any background texture. This is because background texture normally has higher frequency than text does. The smoothing operation also removes speckle noise. In the second step, the intensity histogram of the smoothed image is computed and a threshold automatically selected as follows. For black text, the first peak of the histogram corresponds to text. Thresholding the image at the value of the valley between the first and second peaks of the histogram binarizes the image well. In order to reliably identify the valley, the histogram is smoothed by a low-pass filter before the threshold is computed. The algorithm has been applied to some 50 images from a wide variety of sources: digitized video frames, photos, newspapers, advertisements in magazines or sales flyers, personal checks, etc. There are 21820 characters and 4406 words in these images. 91% of the characters and 86% of the words are successfully cleaned up and binarized. A commercial OCR was applied to the binarized text when it consisted of fonts which were OCR recognizable. The recognition rate was 84% for the characters and 77% for the words. fl This material is based on work supported in part by the National Science Foundation, Library of Congress and Department of Commerce under cooperative agreement number EEC-9209623, in part by the United States Patent and Trademark Office and Defense Advanced Research Projects Agency/ITO under ARPA order number D468, issued by ESC/AXS contract number F19628-95-C-0235, in part by the National Science Foundation under grant number IRI-9619117 and in part by NSF Multimedia CDA-9502639. Any opinions, findings and conclusions or recommendations expressed in this material are the author(s) and do not necessarily reflect those of the sponsors. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> M. Bokser, "Omnidocument Technoligies," </author> <booktitle> Proceedings of The IEEE 80, </booktitle> <pages> pp. 1066-1078, </pages> <month> July </month> <year> 1992. </year>
Reference-contexts: 1 Introduction Current optical character recognition (OCR) technology <ref> [1, 2] </ref> is largely restricted to recognizing text printed against clean backgrounds. Most OCR engines require that the input image be binarized before the characters can be processed.
Reference: [2] <author> S. Mori, C. Y. Suen, and K. Yamamoto, </author> <title> "Historical Review of OCR Research and Development," </title> <booktitle> Proceedings of The IEEE 80, </booktitle> <pages> pp. 1029-1058, </pages> <month> July </month> <year> 1992. </year>
Reference-contexts: 1 Introduction Current optical character recognition (OCR) technology <ref> [1, 2] </ref> is largely restricted to recognizing text printed against clean backgrounds. Most OCR engines require that the input image be binarized before the characters can be processed.
Reference: [3] <author> C. A. Glasbey, </author> <title> "An Analysis of Histogram-Based Thresholding Algorithms," CVGIP: </title> <booktitle> Graphical Models and Image Processing 55, </booktitle> <pages> pp. 532-537, </pages> <month> Nov. </month> <year> 1993. </year>
Reference: [4] <author> L. O'Gorman, </author> <title> "Binarization and Multithresholding of Document Images Using Connectivity," </title> <booktitle> Computer Vision, Graphics and Image Processing 56, </booktitle> <pages> pp. 494-506, </pages> <month> Nov. </month> <year> 1994. </year>
Reference: [5] <author> A. Rosenfeld and P. D. L. Torre, </author> <title> "Histogram Concavity Analysis as an Aid in Threshold Selection," </title> <journal> IEEE Trans. Systems, Man, and Cybernetics SMC-13, </journal> <pages> pp. 231-235, </pages> <year> 1983. </year>
Reference-contexts: Methods have also been proposed to facilitate more robust valley picking <ref> [5] </ref>. There are problems with the above global thresholding paradigm. First, due to noise and poor contrast, many images do not have well-differentiated foreground and background intensities. Second, the bimodal histogram assumption is often not valid for images of complicated documents such as advertisements and photographs.
Reference: [6] <author> J. S. Weszka and A. Rosenfeld, </author> <title> "Histogram Modification for Threshold Selection," </title> <journal> IEEE Trans. on Systems, Man, and Cybernetics SMC-9, </journal> <pages> pp. 38-52, </pages> <month> Jan. </month> <year> 1979. </year>
Reference-contexts: Third, the foreground peak is often overshadowed by other peaks which makes the valley detection difficult or impossible. Some research has been carried out to overcome some of these problems. For example, weighted histograms <ref> [6] </ref> are used to balance the size difference between the foreground and background, and/or convert the valley-finding into maximum peak detection. Minimum-error thresholding [7, 8] models the foreground and background intensity distributions as Gaussian distributions and the threshold is selected to minimize the misclassification error.
Reference: [7] <author> J. Kittler and J. Illingworth, </author> <title> "Minimum Error Thresholding," </title> <booktitle> Pattern Recognition 19(1), </booktitle> <pages> pp. 41-47, </pages> <year> 1986. </year>
Reference-contexts: Some research has been carried out to overcome some of these problems. For example, weighted histograms [6] are used to balance the size difference between the foreground and background, and/or convert the valley-finding into maximum peak detection. Minimum-error thresholding <ref> [7, 8] </ref> models the foreground and background intensity distributions as Gaussian distributions and the threshold is selected to minimize the misclassification error. Otsu [9] models the intensity histogram as a probability distribution and the threshold is chosen to maximize the separability of the resultant foreground and background classes.
Reference: [8] <author> Q. Z. Ye and P. E. Danielson, </author> <title> "On Minimum Error Thresholding and Its Implementation," </title> <journal> Pattern Recognition Letters 7, </journal> <pages> pp. 201-206, </pages> <month> Apr. </month> <year> 1988. </year>
Reference-contexts: Some research has been carried out to overcome some of these problems. For example, weighted histograms [6] are used to balance the size difference between the foreground and background, and/or convert the valley-finding into maximum peak detection. Minimum-error thresholding <ref> [7, 8] </ref> models the foreground and background intensity distributions as Gaussian distributions and the threshold is selected to minimize the misclassification error. Otsu [9] models the intensity histogram as a probability distribution and the threshold is chosen to maximize the separability of the resultant foreground and background classes.
Reference: [9] <author> N. Otsu, </author> <title> "A Threshold Selection Method from Gray-Level Histogram," </title> <journal> IEEE Trans. on Systems, Man, and Cybernetics SMC-9, </journal> <pages> pp. 62-66, </pages> <month> Jan. </month> <year> 1979. </year>
Reference-contexts: Minimum-error thresholding [7, 8] models the foreground and background intensity distributions as Gaussian distributions and the threshold is selected to minimize the misclassification error. Otsu <ref> [9] </ref> models the intensity histogram as a probability distribution and the threshold is chosen to maximize the separability of the resultant foreground and background classes. Similarly, entropy measures have been used [10, 11, 12] to select the threshold which maximizes the sum of foreground and background entropies. <p> In addition, Tsai [13] uses the threshold which best preserves the moment statistics of the binarized image as compared with the original grey-scale image. Liu and Srihari [14] uses Otsu's algorithm <ref> [9] </ref> to obtain candidate thresholds, each of which is used to produce an intermediate binarized image. Then, text features are measured from each binarized image. These features are used to pick the best threshold among the candidates. <p> However, a unique feature of this algorithm is that it works well even if the text is printed against shaded or hatched background as shown in Figure 1 (a) and (b). Figure 1 also demonstrates that our new algorithm performs better than Tsai's moment-preserving method [13], Otsu's histogram-based algorithm <ref> [9] </ref>, and Kamel & Zhao's adaptive binarization algorithm [16]. <p> It should be pointed out that the clean-up output looks fine to a person in the places where the rest of the OCR errors occurred. 3.3 Comparison With Other Thresholding Methods In this section, we compare the performance of our algorithm with those of Tsai [13], Otsu <ref> [9] </ref> and Kamel and Zhao's adaptive method [16]. 12 image chips cropped from our test image set are used for this experiment. The total number of characters/words are counted as shown in Table 2. Each of these image chips is then binarized using the four algorithms.
Reference: [10] <author> A. S. Abutaleb, </author> <title> "Automatic Thresholding of Gray-Level Picture Using Two-Dimensional Entropy," CVGIP: </title> <booktitle> Graphical Models and Image Processing 47, </booktitle> <pages> pp. 22-32, </pages> <month> July </month> <year> 1989. </year>
Reference-contexts: Otsu [9] models the intensity histogram as a probability distribution and the threshold is chosen to maximize the separability of the resultant foreground and background classes. Similarly, entropy measures have been used <ref> [10, 11, 12] </ref> to select the threshold which maximizes the sum of foreground and background entropies. In addition, Tsai [13] uses the threshold which best preserves the moment statistics of the binarized image as compared with the original grey-scale image.
Reference: [11] <author> J. N. Kapur, P. K. Sahoo, and A. K. C. Wong, </author> <title> "A New Method for Gray-Level Picture Thresholding Using the Entropy of the Histogram," </title> <booktitle> Computer Vision, Graphics and Image Processing 29, </booktitle> <pages> pp. 273-285, </pages> <month> Mar. </month> <year> 1985. </year>
Reference-contexts: Otsu [9] models the intensity histogram as a probability distribution and the threshold is chosen to maximize the separability of the resultant foreground and background classes. Similarly, entropy measures have been used <ref> [10, 11, 12] </ref> to select the threshold which maximizes the sum of foreground and background entropies. In addition, Tsai [13] uses the threshold which best preserves the moment statistics of the binarized image as compared with the original grey-scale image.
Reference: [12] <author> T. Pun, </author> <title> "Entropic Thresholding: A New Approach," CVGIP: </title> <booktitle> Graphical Models and Image Processing 16, </booktitle> <pages> pp. 210-239, </pages> <month> July </month> <year> 1981. </year>
Reference-contexts: Otsu [9] models the intensity histogram as a probability distribution and the threshold is chosen to maximize the separability of the resultant foreground and background classes. Similarly, entropy measures have been used <ref> [10, 11, 12] </ref> to select the threshold which maximizes the sum of foreground and background entropies. In addition, Tsai [13] uses the threshold which best preserves the moment statistics of the binarized image as compared with the original grey-scale image.
Reference: [13] <author> W. H. Tsai, </author> <title> "Moment-Preserving Thresholding: A New Approach," Computer Vision, Graphics, </title> <booktitle> and Image Processing 29, </booktitle> <pages> pp. 377-393, </pages> <month> Mar. </month> <year> 1985. </year>
Reference-contexts: Similarly, entropy measures have been used [10, 11, 12] to select the threshold which maximizes the sum of foreground and background entropies. In addition, Tsai <ref> [13] </ref> uses the threshold which best preserves the moment statistics of the binarized image as compared with the original grey-scale image. Liu and Srihari [14] uses Otsu's algorithm [9] to obtain candidate thresholds, each of which is used to produce an intermediate binarized image. <p> However, a unique feature of this algorithm is that it works well even if the text is printed against shaded or hatched background as shown in Figure 1 (a) and (b). Figure 1 also demonstrates that our new algorithm performs better than Tsai's moment-preserving method <ref> [13] </ref>, Otsu's histogram-based algorithm [9], and Kamel & Zhao's adaptive binarization algorithm [16]. <p> It should be pointed out that the clean-up output looks fine to a person in the places where the rest of the OCR errors occurred. 3.3 Comparison With Other Thresholding Methods In this section, we compare the performance of our algorithm with those of Tsai <ref> [13] </ref>, Otsu [9] and Kamel and Zhao's adaptive method [16]. 12 image chips cropped from our test image set are used for this experiment. The total number of characters/words are counted as shown in Table 2. Each of these image chips is then binarized using the four algorithms.
Reference: [14] <author> Y. Liu and S. N. Srihari, </author> <title> "Document Image Binarization Based on Texture Features," </title> <journal> IEEE Trans. on Pattern Analysis and Machine Intelligence 19, </journal> <pages> pp. 540-544, </pages> <month> May </month> <year> 1997. </year>
Reference-contexts: Similarly, entropy measures have been used [10, 11, 12] to select the threshold which maximizes the sum of foreground and background entropies. In addition, Tsai [13] uses the threshold which best preserves the moment statistics of the binarized image as compared with the original grey-scale image. Liu and Srihari <ref> [14] </ref> uses Otsu's algorithm [9] to obtain candidate thresholds, each of which is used to produce an intermediate binarized image. Then, text features are measured from each binarized image. These features are used to pick the best threshold among the candidates.
Reference: [15] <author> R. G. Casey and K. Y. Wong, </author> <title> "Document Analysis System and Techniques," in Image Analysis Applications, </title> <editor> R. Kasturi and M. M. Trivedi eds. </editor> <publisher> Marcel Dekker, </publisher> <address> New York, N.Y. </address> , <pages> pp. 1-36, </pages> <year> 1990. </year>
Reference-contexts: Then, text features are measured from each binarized image. These features are used to pick the best threshold among the candidates. In contrast, adaptive algorithms compute a threshold for each pixel based on information extracted from its neighborhood (local window) <ref> [15, 16] </ref>. For images in which the intensity ranges of the foreground objects and backgrounds entangle, different thresholds must be used for different regions. Domain dependent infomation can also be coded in the algorithm to get the work done [16].
Reference: [16] <author> M. Kamel and A. Zhao, </author> <title> "Extraction of Binary Character/Graphics Images from Grayscale Document Images," </title> <booktitle> Computer Vision, Graphics and Image Processing 55, </booktitle> <pages> pp. 203-217, </pages> <month> May. </month> <year> 1993. </year> <note> [17] . D. </note> <author> Trier and T. Taxt, </author> <title> "Evaluation of Binarization Methods for Document Images," </title> <journal> IEEE Transactions on Pattern Analysis And Machine Intelligence 17, </journal> <pages> pp. 312-315, </pages> <month> March </month> <year> 1995. </year>
Reference-contexts: Then, text features are measured from each binarized image. These features are used to pick the best threshold among the candidates. In contrast, adaptive algorithms compute a threshold for each pixel based on information extracted from its neighborhood (local window) <ref> [15, 16] </ref>. For images in which the intensity ranges of the foreground objects and backgrounds entangle, different thresholds must be used for different regions. Domain dependent infomation can also be coded in the algorithm to get the work done [16]. <p> For images in which the intensity ranges of the foreground objects and backgrounds entangle, different thresholds must be used for different regions. Domain dependent infomation can also be coded in the algorithm to get the work done <ref> [16] </ref>. <p> Figure 1 also demonstrates that our new algorithm performs better than Tsai's moment-preserving method [13], Otsu's histogram-based algorithm [9], and Kamel & Zhao's adaptive binarization algorithm <ref> [16] </ref>. <p> out that the clean-up output looks fine to a person in the places where the rest of the OCR errors occurred. 3.3 Comparison With Other Thresholding Methods In this section, we compare the performance of our algorithm with those of Tsai [13], Otsu [9] and Kamel and Zhao's adaptive method <ref> [16] </ref>. 12 image chips cropped from our test image set are used for this experiment. The total number of characters/words are counted as shown in Table 2. Each of these image chips is then binarized using the four algorithms. The binarized images are then examined by one of the authors.
Reference: [18] <author> V. Wu, R. Manmatha, and E. M. Riseman, </author> <title> "Finding Text In Images," </title> <booktitle> Proc. of the 2nd intl. conf. on Digital Libraries. Philadaphia, </booktitle> <pages> PA , pp. 1-10, </pages> <month> July </month> <year> 1997. </year> <month> 11 </month>
Reference-contexts: To extract text against darker background, a threshold at the last valley is picked instead. The thresholded image is shown at bottom left in Figure 2. This has been successfully recognized by an OCR system. Wu, Manmatha and Riseman <ref> [18] </ref> have developed a system called TextFinder which detects and generates text chips automatically. Thus, even though one threshold is used for 4 Table 1: Summary of the system's performance. 48 images were used for detection and clean-up. Out of these, 35 binarized images were used for the OCR process. <p> In fact, no assumptions are made about the resolution of the input images, since such information is normally not available for the images from outside sources, such as those downloaded from the Internet. 3.1 Text Clean-up Characters and words (as perceived by one of the authors) detected by the TextFinder <ref> [18] </ref> were counted in each image (the ground truth). The total numbers over the whole test set are shown in the "Total Detected" column in Table 1. Then, characters and words which are clearly readable by a person after the binarization operation were counted for each image.
References-found: 17

