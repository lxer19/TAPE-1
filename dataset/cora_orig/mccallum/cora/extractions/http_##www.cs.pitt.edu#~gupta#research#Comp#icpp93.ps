URL: http://www.cs.pitt.edu/~gupta/research/Comp/icpp93.ps
Refering-URL: http://www.cs.pitt.edu/~gupta/research/dm.html
Root-URL: http://www.cs.pitt.edu
Title: "Compiling Programs for Distributed-Memory Multiprocessors," "Dataflow Analysis of Array and Scalar "On the Problem of
Author: D. Callahan, and K. Kennedy, P. Feautrier, [] K. Gallivan, and W. Jalby, [] H. M. Gerndt, and H. P. Zima, [] G. Goff, K. Kennedy, and C. Tseng, [] R. Gupta, [] P. Havlak, and K. Kennedy, [] S. Hiranandani, K. Kennedy, C. Koelbel, U. Kre-mer, and C. W. Tseng, [] S. Hiranandani, K. Kennedy, and C. W. Tseng, [] C. Koelbel, P. Mehrotra, and J. V. Rosendale, [] J. K. Li, and M. Chen, [] D. E. Maydan, J. L. Hennessy, and M. S. Lam, [] W. Pugh, and D. Wonnacott, . [] M. J. Quinn, and P. J. Hatcher, 
Date: 20, January, 1991.  2, July, 1991.  35, August, 1992.  2, July, 1991.  
Note: 6 Related Work 7 Summary References [1]  The Journal of Supercomputing, Vol. 2, 1988. [2]  References," International Journal of Parallel Programming, Vol.  Proc. ACM International Conference on  IEEE Trans. Parallel and Distributed Sys., Vol.  ACM, Vol.  IEEE trans. on Parallel and Distributed Sys., Vol.  Proceedings of International Conference on Computer Languages, 1990. [16] "Fortran 90 Standard," 1991.  
Abstract: Many approaches have been proposed to optimize the communication in SPMD execution. These include: Code reordering, Loop interchange, Loop reversal, Loop elimination and Communication combining [1] [6] [4] [10]. The purpose of these approaches is to extract as much parallelism as possible. Instead of inserting the communication at the earliest possible point, these approaches usually insert communication immediately before the computation. Moreover, all of these approaches assume that the send and receive instructions are inserted at the same point in the program. This does not allow effective overlapping of communication and computation. Gallivan and Jalby [3] were one of the first to formally treat the problem of optimizing data transfers in distributed-memory systems by avoiding redundant communication and avoiding sequentialization. In [11], the authors defined some array reference patterns and use pre-implemented routines to match those patterns and achieve communication optimization. However, only restricted classes of communication patterns can be optimized in this way. In [9], the authors proposed optimizations similar to the ones presented in this paper. Our unified framework, however, allows us to perform all the optimizations together and enables us to decide the tradeoff between competing optimizations. Data flow analysis has been successfully applied to allow code optimizations involving scalar variables. Only recently has the technique been applied to array variables. In [2], the author proposed a data flow analysis algorithm to collect information about array and scalar references and suggested that this information could be used for program verification and parallel program construction. We use data flow analysis for communication optimizations for which we need to collect the communication pattern information. The purpose of the optimizations is to reduce communication and to increase parallelism. The contribution of this paper is twofold. First, we exploit the idea of separating the send and receive instruction to overlap communication with execution. Second, we present the data flow analysis technique for solving data communication optimization problems. We are currently investigating how to expand the framework to include interprocedural analysis. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> D. Callahan, and K. Kennedy, </author> <title> "Compiling Programs for Distributed-Memory Multiprocessors," </title> <journal> The Journal of Supercomputing, </journal> <volume> Vol. 2, </volume> <year> 1988. </year>
Reference: [2] <author> P. Feautrier, </author> <title> "Dataflow Analysis of Array and Scalar References," </title> <journal> International Journal of Parallel Programming, </journal> <volume> Vol. 20, </volume> <month> January, </month> <year> 1991. </year>
Reference: [3] <author> K. Gallivan, and W. Jalby, </author> <title> "On the Problem of Optimizing Data Transfers for Complex Memory System," </title> <booktitle> Proc. ACM International Conference on Supercomputing, </booktitle> <year> 1988. </year>
Reference: [4] <author> H. M. Gerndt, and H. P. Zima, </author> <title> "Optimizing Communication in SUPERB," </title> <address> CONPAR 90, </address> <year> 1990. </year>
Reference: [5] <author> G. Goff, K. Kennedy, and C. Tseng, </author> <title> "Practical Dependence Testing," </title> <booktitle> ACM SIGPLAN '91 Conference on Programming Language Design and Implementation, </booktitle> <year> 1991. </year>
Reference: [6] <author> R. Gupta, </author> <title> "Compiler Optimizations for Distributed Memory Programs," </title> <booktitle> Proc. of the Scalable High Performance Computing Conference, </booktitle> <year> 1992. </year>
Reference: [7] <author> P. Havlak, and K. Kennedy, </author> <title> "An Implementation of Interprocedural Bounded Regular Section Analysis," </title> <journal> IEEE Trans. Parallel and Distributed Sys., </journal> <volume> Vol. 2, </volume> <month> July, </month> <year> 1991. </year>
Reference: [8] <author> S. Hiranandani, K. Kennedy, C. Koelbel, U. Kre-mer, and C. W. Tseng, </author> <title> "An Overview of the Fortran D Programming System," </title> <booktitle> Proc. of the Fourth Workshop on Languages and Compilers for Parallel Computing, </booktitle> <year> 1991. </year>
Reference: [9] <author> S. Hiranandani, K. Kennedy, and C. W. Tseng, </author> <title> "Compiling Fortran D for MIMD Distributed-Memory Machines," </title> <journal> Communication of ACM, </journal> <volume> Vol. 35, </volume> <month> August, </month> <year> 1992. </year>
Reference: [10] <author> C. Koelbel, P. Mehrotra, and J. V. Rosendale, </author> <title> "Supporting Shared Data Structure on Distributed Memory Architectures," </title> <booktitle> Proceedings of the Second ACM SIGPLAN Symposium on Principles & Practice of Parallel Programming, </booktitle> <year> 1990. </year>
Reference: [11] <author> J. K. Li, and M. Chen, </author> <title> "Compiling Communication-Efficient Programs for Massively Parallel Machines," </title> <journal> IEEE trans. on Parallel and Distributed Sys., </journal> <volume> Vol. 2, </volume> <month> July, </month> <year> 1991. </year>
Reference: [12] <author> D. E. Maydan, J. L. Hennessy, and M. S. Lam, </author> <title> "Efficient and Exact Data Dependence Analysis," </title> <booktitle> ACM SIGPLAN '91 Conference on Programming Language Design and Implementation, </booktitle> <year> 1991. </year>
Reference: [13] <institution> Thinking Machines Corporation, </institution> <note> "CM Fortran Reference Manual," </note> <year> 1991. </year>
Reference: [14] <author> W. Pugh, and D. Wonnacott, </author> <title> "Eliminating False Data Dependences Using the Omega Test," </title> <booktitle> Proceedings of the ACM SIGPLAN conference on Programming Language Design and Implementation, </booktitle> <year> 1992. </year>

References-found: 14

