URL: http://www.cs.unc.edu/~olano/papers/succ_map.ps.gz
Refering-URL: http://www.cs.unc.edu/~olano/papers/
Root-URL: http://www.cs.unc.edu
Email: fcohenj,manocha,olanog@cs.unc.edu  
Title: Simplifying Polygonal Models Using Successive Mappings  
Author: Jonathan Cohen Dinesh Manocha Marc Olano 
Address: Chapel Hill, NC 27599-3175  
Affiliation: Department of Computer Science University of North Carolina  
Abstract: We present the use of mapping functions to automatically generate levels of detail with known error bounds for polygonal models. We develop a piece-wise linear mapping function for each simplification operation and use this function to measure deviation of the new current surface from both the previous level of detail and from the original surface. In addition, we use the mapping function to compute appropriate texture coordinates if the original map has texture coordinates at its vertices. Our overall algorithm uses edge collapse operations. We present rigorous procedures for the generation of local planar projections as well as for the selection of a new vertex position for the edge collapse operation. As compared to earlier methods, our algorithm is able to compute tight error bounds on surface deviation and other attributes and produce an entire continuum of levels of details with successive mapping. We demonstrate the effectiveness of our algorithm on several models: a Ford Bronco consisting of over 300 parts and 70; 000 triangles, a textured lion model consisting of 49 parts and 86; 000 triangles, and a textured, wrinkled torus consisting of 79; 000 triangles. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Chandrajit Bajaj and Daniel Schikore. </author> <title> Error-bounded reduction of triangle meshes with multivariate data. </title> <booktitle> SPIE, </booktitle> <volume> 2656 </volume> <pages> 34-45, </pages> <year> 1996. </year>
Reference-contexts: But they do not produce an entire spectrum of levels of detail. Gueziec [9] has presented an algorithm for computing local error bounds inside the simplification process by maintaining tolerance volumes. However, it does not produce a suitable mapping between levels of detail. Bajaj and Schikore <ref> [1, 16] </ref> have presented algorithm for producing a mapping between approximations and measure the error of scalar fields across the surface based on vertex-removals. Some of the results presented in this paper extend this work non-trivially to edge collapse operation. <p> Such surfaces are quite common in practice. Borders create some complications for the creation of a mapping in the plane. The problem is that the total shape of the neighborhood projected into the plane changes as a result of the edge collapse. Bajaj and Schikore <ref> [1] </ref> also address this problem. Since they employ a vertex-removal approach, their projected vertex neighborhood is somewhat less complex than our projected edge neighborhood. They deal with this problem by mapping the removed vertex to a length-parameterized position along the border. <p> Also, the tolerance volume approach does not generate mappings between the surfaces for use with other attributes. We have made several significant improvements over the error bounded reduction algorithm presented by Bajaj and Schikore <ref> [1, 16] </ref>. First, we have switched from their vertex removal approach to an edge collapse approach. This is better suited to an emphasis in rendering levels of detail with smooth transitions (say using geomorphs). This extension is non-trivial.
Reference: [2] <author> M.P. </author> <title> Do Carmo. Differential Geometry of Curves and Surfaces. </title> <publisher> Prentice Hall, </publisher> <year> 1976. </year>
Reference-contexts: However, to achieve greater efficiency and to reduce the complexity of the software system we choose to find only a single valid direction, which is typically all we require. Consider the Gaussian sphere <ref> [2] </ref>, the unit sphere on which each point corresponds to a unit normal vector of the same coordinates. We can now consider the plane through the origin determined by the normal vector of one of the triangles. <p> We can now consider the plane through the origin determined by the normal vector of one of the triangles. For a direction of projection to be valid with respect to this triangle, its point on the Gaussian (normal) sphere <ref> [2] </ref> must lie on the correct side of this plane (i.e. within the correct hemisphere). If we now consider two triangles simultaneously, the direction of projection must lie on the correct side of the planes determined by the normal vectors of both these triangles.
Reference: [3] <author> A. Certain, J. Popovic, T. Derose, T. Duchamp, D. Salesi n, and W. Stuetzle. </author> <title> Interactive multiresolution surface viewing. </title> <booktitle> In Proc. of ACM Siggraph, </booktitle> <pages> pages 91-98, </pages> <year> 1996. </year>
Reference-contexts: These methods preserve global topology, give error bounds on the simplified object and provide a mapping between levels of detail. In <ref> [3] </ref> they have been further extended to handle colored meshes. However, the initial mesh is not contained in the level of detail hierarchy, but can only be recovered to within an *-tolerance. In some cases this is undesirable.
Reference: [4] <author> J. Cohen, A. Varshney, D. Manocha, and G. Turk et al. </author> <title> Simplification envelopes. </title> <booktitle> In Proc. of ACM Sig-graph'96, </booktitle> <pages> pages 119-128, </pages> <year> 1996. </year>
Reference-contexts: Tight Error Bounds: Our approach can measure and minimize the error for surface deviation and other attributes. These error bounds give guarantees on the shape of the simplified object and screen-space deviation. 3. Generality: Our approach can be easily combined with other topology preserving algorithms, such as simplification envelopes <ref> [4] </ref>, or topology modifying algorithms. Furthermore, the algorithm for collapsing an edge into a vertex is rather general and does not restrict the vertex to lie on the original edge. 4. <p> However, the algorithm in [11] gives no local error bounds or guarantees on the shape of the simplified model. There is considerable literature on model simplification using error bounds. Cohen and Varshney et al. <ref> [4, 22] </ref> have used envelopes to preserve the model topology and obtain tight error bounds for a simple simplification. But they do not produce an entire spectrum of levels of detail. Gueziec [9] has presented an algorithm for computing local error bounds inside the simplification process by maintaining tolerance volumes. <p> In some cases this is undesirable. Furthermore, the wavelet based approach can be somewhat conservative and for a given error bound, algorithms based on vertex removal and edge collapses <ref> [4, 11] </ref> have been empirically able to simplify more (in terms of reducing the polygon count). 3 Overview Our simplification approach may be seen as a high-level algorithm which controls the simplification process with a lower-level cost function based on local mappings. <p> The resulting set of triangles will contain no self-intersections, so long as the projection is one-to-one. Many other simplification algorithms, such as those by Turk [21], Schroeder [18] and Cohen, Varshney et al. <ref> [4] </ref>, also used such projections for vertex removal. However, they would choose a likely direction, such as the average of the normal vectors of the triangles of interest. <p> The lions, however, are not rendered at their appropriate viewing distances, so certain discrepancies will appear as fuzzy areas. 7.1 Application of Projection Algorithm We have also applied the technique of finding a one-to-one planar projection to the simplification envelopes algorithm <ref> [4] </ref>. The simplification envelopes method requires the calculation of a vertex normal at each vertex that may be used as a direction to offset the vertex. <p> The criterion for being able to move a vertex without creating a local self-intersection is the same as the criterion for being able to project to a plane. The algorithm presented in <ref> [4] </ref> used a heuristic based on averaging the face normals. 14 By applying the projection algorithm based on linear programming (presented in Section 3) to the computation of the offset directions, we were able to perform more drastic simplifications. <p> The efficient and complete algorithms for computing the planar projection and placing the generated vertex after edge collapse should improve the performance of all the earlier algorithms that use vertex removals or edge collapses. We compared our implementation with that of simplification envelope approach <ref> [4] </ref>. We generated levels of detail of the Stanford bunny model (70,000 triangles) using the simplification envelope method, then generated levels of detail with the same number of triangles using the successive mapping approach. Visually, the models were totally comparable.
Reference: [5] <author> M.J. Dehaemer and M.J. Zyda. </author> <title> Simplification of objects rendered by polygonal approximations. </title> <journal> Computer and Graphics, </journal> <volume> 15(2) </volume> <pages> 175-184, </pages> <year> 1981. </year>
Reference: [6] <author> T. Derose, M. Lounsbery, and J. Warren. </author> <title> Multiresolution analysis for surfaces of arbitrary topology type. </title> <type> Technical Report TR 93-10-05, </type> <institution> Department of Computer Science, University of Washington, </institution> <year> 1993. </year>
Reference-contexts: Some of the results presented in this paper extend this work non-trivially to edge collapse operation. A detailed comparison with these approaches is presented in Section 6. An elegant solution to the polygon simplification problem has been presented in <ref> [6, 7] </ref> where arbitrary polygonal meshes are first subdivided into patches with subdivision connectivity and then multiresolu-tion wavelet analysis is used over each patch. These methods preserve global topology, give error bounds on the simplified object and provide a mapping between levels of detail. <p> However, there are no strict local error bounds (except for the global optimizing function), so there is no simple way to automatically choose switching distances that guarantee some visual quality. The multi-resolution analysis approach to simplification does <ref> [7, 6] </ref>, in fact, provide strict error bounds as well as a mapping between surfaces. However, the requirements of its subdivision topology and the coarse granularity of its simplification operation do not provide the local control of the edge collapse. In particular, it does not deal well with sharp edges.
Reference: [7] <author> M. Eck, T. DeRose, T. Duchamp, H. Hoppe, M. Lounsbery, and W. Stuetzle. </author> <title> Multiresolution analysis of arbitrary meshes. </title> <booktitle> In Proc. of ACM Siggraph, </booktitle> <pages> pages 173-182, </pages> <year> 1995. </year>
Reference-contexts: Some of the results presented in this paper extend this work non-trivially to edge collapse operation. A detailed comparison with these approaches is presented in Section 6. An elegant solution to the polygon simplification problem has been presented in <ref> [6, 7] </ref> where arbitrary polygonal meshes are first subdivided into patches with subdivision connectivity and then multiresolu-tion wavelet analysis is used over each patch. These methods preserve global topology, give error bounds on the simplified object and provide a mapping between levels of detail. <p> However, there are no strict local error bounds (except for the global optimizing function), so there is no simple way to automatically choose switching distances that guarantee some visual quality. The multi-resolution analysis approach to simplification does <ref> [7, 6] </ref>, in fact, provide strict error bounds as well as a mapping between surfaces. However, the requirements of its subdivision topology and the coarse granularity of its simplification operation do not provide the local control of the edge collapse. In particular, it does not deal well with sharp edges.
Reference: [8] <author> Michael Garland and Paul S. Heckbert. </author> <title> Fast polygonal approximation of terrains and height fields. </title> <type> Technical report, </type> <institution> CS Dept., Carnegie Mellon U., </institution> <month> Sept. </month> <year> 1995. </year>
Reference: [9] <author> Andre Gueziec. </author> <title> Surface simplification with variable tolerance. </title> <booktitle> In Second Annual Intl. Symp. on Medical Robotics and Computer Assisted Surgery (MRCAS '95), </booktitle> <pages> pages 132-139, </pages> <month> November </month> <year> 1995. </year>
Reference-contexts: There is considerable literature on model simplification using error bounds. Cohen and Varshney et al. [4, 22] have used envelopes to preserve the model topology and obtain tight error bounds for a simple simplification. But they do not produce an entire spectrum of levels of detail. Gueziec <ref> [9] </ref> has presented an algorithm for computing local error bounds inside the simplification process by maintaining tolerance volumes. However, it does not produce a suitable mapping between levels of detail. <p> For a given error bound, we expect our mapping algorithm to be able to simplify more as compared to the multi-resolution approach. 15 Gueziec's tolerance volume approach <ref> [9] </ref> uses edge collapses with local error bounds. However, it is difficult to compare our approach to it.
Reference: [10] <author> P. Heckbert and M. </author> <title> Garland. Multiresolution modeling for fast rendering. </title> <booktitle> Proceedings of Graphics Interface, </booktitle> <year> 1994. </year>
Reference: [11] <author> H. Hoppe. </author> <title> Progressive meshes. </title> <booktitle> In Proc. of ACM Siggraph, </booktitle> <year> 1996. </year>
Reference-contexts: Continuum of Levels of Details: The algorithm incrementally produces an entire spectrum of levels-of-details as opposed to a few discrete levels. Furthermore, the algorithm incrementally stores the error bounds for each level. The simplified model can be stored as a progressive mesh <ref> [11] </ref>. The algorithm has been successfully applied to a number of models. These models consist of hundreds of parts and tens of thousands of polygons, including a Ford Bronco with 300 parts, a textured lion model and a textured wrinkled torus. <p> This algorithm has been used in the Brush walkthrough system [17]. A dynamic view-dependent simplification algorithm has been presented in [23]. Hoppe et al. <ref> [11, 12] </ref> posed the model simplification problem into a global optimization framework, minimizing the least-squares error from a set of point-samples on the original surface. Later, Hoppe extended this framework to handle other scalar attributes, explicitly recognizing the distinction between smooth gradients and sharp discontinuities. <p> Later, Hoppe extended this framework to handle other scalar attributes, explicitly recognizing the distinction between smooth gradients and sharp discontinuities. He also introduced the term progressive mesh <ref> [11] </ref>, which is essentially a stored sequence of simplification operations, allowing quick construction of any desired level of detail along the continuum of simplifications. However, the algorithm in [11] gives no local error bounds or guarantees on the shape of the simplified model. <p> He also introduced the term progressive mesh <ref> [11] </ref>, which is essentially a stored sequence of simplification operations, allowing quick construction of any desired level of detail along the continuum of simplifications. However, the algorithm in [11] gives no local error bounds or guarantees on the shape of the simplified model. There is considerable literature on model simplification using error bounds. Cohen and Varshney et al. [4, 22] have used envelopes to preserve the model topology and obtain tight error bounds for a simple simplification. <p> In some cases this is undesirable. Furthermore, the wavelet based approach can be somewhat conservative and for a given error bound, algorithms based on vertex removal and edge collapses <ref> [4, 11] </ref> have been empirically able to simplify more (in terms of reducing the polygon count). 3 Overview Our simplification approach may be seen as a high-level algorithm which controls the simplification process with a lower-level cost function based on local mappings. <p> This process continues until none of the remaining edges may be collapsed. The output of our algorithm is the original model plus an ordered list of edge collapses and their associated cost functions. This progressive mesh <ref> [11] </ref> represents an entire continuum of levels of detail for the surface. <p> The successive mapping approach discourages these creases by its use of planar projections. At the same time, the performance of simplification envelope approach (in terms of performing drastic simplification for a given error bound) has been improved by our new projecting algorithm. Hoppe's progressive mesh <ref> [11] </ref> implementation goes much farther than we have so far to incorporate colors and textures into the simplification framework. However, there are no strict local error bounds (except for the global optimizing function), so there is no simple way to automatically choose switching distances that guarantee some visual quality. <p> However, the requirements of its subdivision topology and the coarse granularity of its simplification operation do not provide the local control of the edge collapse. In particular, it does not deal well with sharp edges. Hoppe <ref> [11] </ref> has provided a visual comparison between the quality of his progressive meshes vs. the multi-resolution analysis meshes for a given number of triangles, and his progressive meshes provide much higher visual quality.
Reference: [12] <author> H. Hoppe, T. Derose, T. Duchamp, J. Mcdonald, and W. Stuetzle. </author> <title> Mesh optimization. </title> <booktitle> In Proc. of ACM Siggraph, </booktitle> <pages> pages 19-26, </pages> <year> 1993. </year>
Reference-contexts: This algorithm has been used in the Brush walkthrough system [17]. A dynamic view-dependent simplification algorithm has been presented in [23]. Hoppe et al. <ref> [11, 12] </ref> posed the model simplification problem into a global optimization framework, minimizing the least-squares error from a set of point-samples on the original surface. Later, Hoppe extended this framework to handle other scalar attributes, explicitly recognizing the distinction between smooth gradients and sharp discontinuities.
Reference: [13] <author> J. O'Rourke. </author> <title> Computational Geometry in C. </title> <publisher> Cambridge University Press, </publisher> <year> 1994. </year>
Reference-contexts: For the neighborhood of the generated vertex to have a one-to-one mapping with the plane, its edges must lie entirely within the polygon, ensuring that no edge crossings occur. This 2D visibility problem has been well-studied in the computational geometry literature <ref> [13] </ref>. The generated vertex must have an unobstructed line of sight to each of the surrounding polygon vertices (unlike the vertex shown in Figure 5). This condition holds if and only if the generated vertex lies within the polygon's kernel, shown in Figure 6.
Reference: [14] <author> Remi Ronfard and Jarek Rossignac. </author> <title> Full-range approximation of triangulated polyhedra. </title> <journal> Computer Graphics Forum, </journal> <volume> 15(3), </volume> <month> Aug. </month> <year> 1996. </year> <note> Proc. Eurographics '96. </note>
Reference: [15] <author> J. Rossignac and P. Borrel. </author> <title> Multi-resolution 3D approximations for rendering. </title> <booktitle> In Modeling in Computer Graphics, </booktitle> <pages> pages 455-465. </pages> <publisher> Springer-Verlag, </publisher> <month> June-July </month> <year> 1993. </year>
Reference-contexts: Some of the earlier work by Turk [21] and Schroeder [18] employed heuristics based on curvature to determine which parts of the surface to simplify to achieve a model with the desired polygon count. Other work include that 2 of Rossignac and Borrel <ref> [15] </ref> where vertices close to each other are clustered and a vertex is generated to represent them. This algorithm has been used in the Brush walkthrough system [17]. A dynamic view-dependent simplification algorithm has been presented in [23].
Reference: [16] <author> D. Schikore and C. Bajaj. </author> <title> Decimation of 2d scalar data with error control. </title> <type> Technical report, </type> <institution> Computer Science Report CSD-TR-95-004, Purdue University, </institution> <year> 1995. </year>
Reference-contexts: But they do not produce an entire spectrum of levels of detail. Gueziec [9] has presented an algorithm for computing local error bounds inside the simplification process by maintaining tolerance volumes. However, it does not produce a suitable mapping between levels of detail. Bajaj and Schikore <ref> [1, 16] </ref> have presented algorithm for producing a mapping between approximations and measure the error of scalar fields across the surface based on vertex-removals. Some of the results presented in this paper extend this work non-trivially to edge collapse operation. <p> Also, the tolerance volume approach does not generate mappings between the surfaces for use with other attributes. We have made several significant improvements over the error bounded reduction algorithm presented by Bajaj and Schikore <ref> [1, 16] </ref>. First, we have switched from their vertex removal approach to an edge collapse approach. This is better suited to an emphasis in rendering levels of detail with smooth transitions (say using geomorphs). This extension is non-trivial.
Reference: [17] <author> B. Schneider, P. Borrel, J. Menon, J. Mittleman, and J. Rossignac. </author> <title> Brush as a walkthrough system for architectural models. </title> <booktitle> In Fifth Eurographics Workshop on Rendering, </booktitle> <pages> pages 389-399, </pages> <month> July </month> <year> 1994. </year>
Reference-contexts: Other work include that 2 of Rossignac and Borrel [15] where vertices close to each other are clustered and a vertex is generated to represent them. This algorithm has been used in the Brush walkthrough system <ref> [17] </ref>. A dynamic view-dependent simplification algorithm has been presented in [23]. Hoppe et al. [11, 12] posed the model simplification problem into a global optimization framework, minimizing the least-squares error from a set of point-samples on the original surface.
Reference: [18] <author> W.J. Schroeder, J.A. Zarge, and W.E. Lorensen. </author> <title> Decimation of triangle meshes. </title> <booktitle> In Proc. of ACM Siggraph, </booktitle> <pages> pages 65-70, </pages> <year> 1992. </year>
Reference-contexts: Some of the earlier work by Turk [21] and Schroeder <ref> [18] </ref> employed heuristics based on curvature to determine which parts of the surface to simplify to achieve a model with the desired polygon count. Other work include that 2 of Rossignac and Borrel [15] where vertices close to each other are clustered and a vertex is generated to represent them. <p> For example, suppose we project a connected set of triangles onto a plane and then re-triangulate the polygon described by their boundary. The resulting set of triangles will contain no self-intersections, so long as the projection is one-to-one. Many other simplification algorithms, such as those by Turk [21], Schroeder <ref> [18] </ref> and Cohen, Varshney et al. [4], also used such projections for vertex removal. However, they would choose a likely direction, such as the average of the normal vectors of the triangles of interest.
Reference: [19] <author> R. Seidel. </author> <title> Linear programming and convex hulls made easy. </title> <booktitle> In Proc. 6th Ann. ACM Conf. on Computational Geometry, </booktitle> <pages> pages 211-215, </pages> <address> Berkeley, California, </address> <year> 1990. </year>
Reference-contexts: These optimization functions are like maximize x; maximize x + y etc. We used Seidel's linear time randomized algorithm <ref> [19] </ref> to solve each linear programming problem. A public domain implementation of this algorithm by Hohmeyer is available. It is very fast in practice. 4.2 Placing the Vertex in the Plane In the previous section, we presented an algorithm to compute a valid plane.
Reference: [20] <author> David C. Taylor and William A. Barrett. </author> <title> An algorithm for continuous resolution polygonalizations of a discrete surface. </title> <booktitle> In Proc. Graphics Interface '94, </booktitle> <pages> pages 33-42, </pages> <address> Banff, Canada, </address> <month> May </month> <year> 1994. </year> <journal> Canadian Inf. Proc. Soc. </journal>
Reference: [21] <author> G. Turk. </author> <title> Re-tiling polygonal surfaces. </title> <booktitle> In Proc. of ACM Siggraph, </booktitle> <pages> pages 55-64, </pages> <year> 1992. </year>
Reference-contexts: Some of the earlier work by Turk <ref> [21] </ref> and Schroeder [18] employed heuristics based on curvature to determine which parts of the surface to simplify to achieve a model with the desired polygon count. <p> For example, suppose we project a connected set of triangles onto a plane and then re-triangulate the polygon described by their boundary. The resulting set of triangles will contain no self-intersections, so long as the projection is one-to-one. Many other simplification algorithms, such as those by Turk <ref> [21] </ref>, Schroeder [18] and Cohen, Varshney et al. [4], also used such projections for vertex removal. However, they would choose a likely direction, such as the average of the normal vectors of the triangles of interest.
Reference: [22] <author> A. Varshney. </author> <title> Hierarchical Geometric Approximations. </title> <type> PhD thesis, </type> <institution> University of N. Carolina, </institution> <year> 1994. </year>
Reference-contexts: However, the algorithm in [11] gives no local error bounds or guarantees on the shape of the simplified model. There is considerable literature on model simplification using error bounds. Cohen and Varshney et al. <ref> [4, 22] </ref> have used envelopes to preserve the model topology and obtain tight error bounds for a simple simplification. But they do not produce an entire spectrum of levels of detail. Gueziec [9] has presented an algorithm for computing local error bounds inside the simplification process by maintaining tolerance volumes.
Reference: [23] <author> J. Xia, J. El-Sana, and A. Varshney. </author> <title> Adaptive real-time level-of-detail-based rendering for polygonal models. </title> <journal> IEEE Transactions on Visualization and Computer Graphics, </journal> <month> June </month> <year> 1997. </year> <note> (to appear). 17 18 19 </note>
Reference-contexts: Other work include that 2 of Rossignac and Borrel [15] where vertices close to each other are clustered and a vertex is generated to represent them. This algorithm has been used in the Brush walkthrough system [17]. A dynamic view-dependent simplification algorithm has been presented in <ref> [23] </ref>. Hoppe et al. [11, 12] posed the model simplification problem into a global optimization framework, minimizing the least-squares error from a set of point-samples on the original surface. Later, Hoppe extended this framework to handle other scalar attributes, explicitly recognizing the distinction between smooth gradients and sharp discontinuities.
References-found: 23

