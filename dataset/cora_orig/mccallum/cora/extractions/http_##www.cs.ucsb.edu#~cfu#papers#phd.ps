URL: http://www.cs.ucsb.edu/~cfu/papers/phd.ps
Refering-URL: http://www.cs.ucsb.edu/Research/rapid_sweb/RAPID.html
Root-URL: http://www.cs.ucsb.edu
Title: Scheduling and Run-time Support for Parallel Irregular Computations  
Author: Cong Fu 
Degree: A dissertation submitted in partial satisfaction of the requirements for the degree of Doctor of Philosophy in Computer Science by  Committee in charge: Professor Tao Yang, Chair Professor Shiv Chandrasekaran Professor Oscar Ibarra Professor Klaus Schauser Professor Yuan-fang Wang  
Date: October 1997  
Affiliation: UNIVERSITY of CALIFORNIA Santa Barbara  
Abstract-found: 0
Intro-found: 1
Reference: [1] <author> C. Amza, A. Cox, S. Dwarkadas, P. Keleher, H. Lu, R. Rajamony, W. Yu, and W. Zwaenepoel. TreadMarks: </author> <title> Shared Memory Computing on Networks of Workstations. </title> <journal> IEEE Computer, </journal> <volume> 29(2) </volume> <pages> 18-28, </pages> <month> February </month> <year> 1996. </year>
Reference-contexts: LPARX [44] is a run-time system designed for adaptive mesh methods. CHAPTER 1. 5 Consistency models. The issue of executing a program correctly has been studied in the context of distributed shared memory systems (DSM), e.g. Tread-Marks <ref> [1] </ref>. The main difference between our work and DSM is that a DSM system does not have any knowledge of a program to be executed. A DSM system maintains the consistency of shared objects for all processors without knowing if those objects will be further accessed or not.
Reference: [2] <author> C. Ashcraft and R. Grimes. </author> <title> The Influence of Relaxed Supernode Partitions on the Multifrontal Method . ACM Transactions on Mathematical Software, </title> <booktitle> 15(4) </booktitle> <pages> 291-309, </pages> <year> 1989. </year>
Reference-contexts: The parallel performance is shown in Figure 3.8 and all the speedups are obtained with consideration of commutativity. In average, we have obtained speedup 12:2 on 32 processors and 19:8 on 64 processors. All the parallel performance data have been obtained by incorporating supernode amalgamation technique <ref> [50, 2] </ref>, which essentially increases average supernode sizes, and eventually increases task granularity. To the best of our knowledge, this is the highest performance obtained for Cholesky factorization by automatically scheduled code. We will analyze and discuss the overhead of run-time execution in Section 3.4.5. <p> These two issues make timing among tasks different from what is expected at the static time and tend to increase the processor idle time. The performance becomes even more sensitive when task granularities are small [32]. That is why techniques of increasing average task granularity, e.g., supernode amalgamation <ref> [2, 50] </ref>, are very valuable. 3.4.6 Inspecting cost: case study In this section we examine how much time is spent at the inspector stage, i.e., the overhead for deriving, transforming and scheduling task dependence graph. The rationale of the inspector/executor approach is to amortize the inspecting cost over many iterations. <p> We also incorporate a supernode amalgamation <ref> [2, 20] </ref> technique to increase the granularity of the computation. We have used RAPID system to exploit irregular parallelism in the re-designed sparse LU algorithm with 1-D data mapping scheme. Usually 2-D data mapping CHAPTER 4. 67 can expose more parallelism, but it is very difficult to model the parallelism.
Reference: [3] <author> C. Ashcraft, R. Grimes, J. Lewis, B. Peyton, and H. Simon. </author> <title> Progress in Sparse Matrix Methods for Large Sparse Linear Systems on Vector Supercomputers. </title> <journal> International Journal of Supercomputer Applications, </journal> <volume> 1 </volume> <pages> 10-30, </pages> <year> 1987. </year>
Reference-contexts: af23560 23560 460598 1.054 30.4 44.4 57.4 1.46 3.16 vavasis3 41092 1683902 1.999 29.2 32.0 38.8 1.10 1.43 Table 4.1: Testing matrices and their statistics. 4.3.2 2-D L/U supernode partitioning and dense struc ture identification Supernode partitioning is a commonly used technique to improve the caching performance of sparse code <ref> [3] </ref>. For a symmetric sparse matrix, a supernode is defined as a group of consecutive columns that have nested structure in the L fac tor of the matrix. Excellent performance has been achieved in [39, 50, 51] using supernode partitioning for Cholesky factorization. <p> For example, Figure 5.1 (b) is a schedule produced by RCP while (c) is a schedule produced by MPO. The ordering difference between Figure 5.1 (b) and (c) is that on processor 1, T [7; 8] is executed at time 6 by RCP while T <ref> [3; 10] </ref> is chosen instead at time 6 by MPO. <p> For MPO, T <ref> [3; 10] </ref> has a higher memory priority 1 because data d 3 and d 10 are all available locally at time 6, and T [7; 8]'s memory priority is 0.5 because the space for d 7 has not been allocated before time 6.
Reference: [4] <author> G. E. Blelloch, P. B. Gibbons, and Y. Matias. </author> <title> Provably Efficient Scheduling for Languages with Fine-Grained Parallelism. </title> <booktitle> In Proceedings of 7th ACM Symposium on Parallel Algorithms and Architectures, </booktitle> <pages> pages 1-12, </pages> <month> July </month> <year> 1995. </year> <note> 151 BIBLIOGRAPHY 152 </note>
Reference-contexts: Memory efficient scheduling and memory optimizations for irregular parallel applications. Most of previous research on scheduling [53, 62, 65] does CHAPTER 1. 6 not address memory issues. In <ref> [4] </ref>, a dynamic scheduling algorithm for directed acyclic graphs is proposed. This work provides a solid theoretical ground for space-efficient scheduling, and it is still an open research problem how to integrate their techniques in practical systems. <p> Our work assumes that each processor has a maximum space limit and the goal is to make the data space cost to be close to optimal. In addition, the scheduling scheme used in RAPID is static in the run-time pre-processing stage while the scheduling schemes used in <ref> [4] </ref> and [6] are dynamic. This is mainly because in practice it is difficult to minimize the run-time control overhead of dynamic scheduling in parallelizing sparse code with mixed granularities. 1.3 Organization of the dissertation The rest of the dissertation is organized as follows. <p> Most of previous research on scheduling [53, 62, 65] does not address memory issues. In <ref> [4] </ref>, a dynamic scheduling algorithm for directed acyclic graphs is proposed with memory space usage S 1 =p + O (D) on each processor, where S 1 is the sequential space requirement, p is the total number of processors and D is the depth of a DAG. <p> The scheduling scheme CHAPTER 5. 108 we use is static in the run-time pre-processing stage while <ref> [4] </ref> and [6] use dynamic scheduling. This is mainly because in practice it is difficult to minimize the run-time control overhead of dynamic scheduling in parallelizing sparse code with mixed granularities.
Reference: [5] <author> R. Blumofe, M. Frigo, C. Joerg, C. Leiserson, and K. Randall. </author> <title> Dag-Consistent Distributed Shared Memory. </title> <booktitle> In Proceeding of 10th International Parallel Processing Symposium, </booktitle> <pages> pages 132-141, </pages> <month> April </month> <year> 1996. </year>
Reference-contexts: Our work provides effective solutions to a more general class of irregular applications. Run-time support for parallel computations. The Charm [57] system adopts a message driven approach for general asynchronous computation using dynamic scheduling. The Cilk <ref> [5] </ref> multi-threaded system aims at applications that have "strict" dependence structures. Randomized load balancing and work stealing techniques are used to execute a dynamic DAG. <p> If the available amount of memory is 8 for each processor, then there are 2 units of memory for volatile objects on P 1 . In addition to the MAPs at the beginning of two task chains, there is another MAP right after task T <ref> [5; 10] </ref> on P 1 at which space for d 3 and d 5 will be freed and space for d 7 is allocated. The address for d 7 on P 1 is then notified to P 0 .
Reference: [6] <author> R. Blumofe, C. Joerg, B. Kuszmaul, C. Leiserson, K. Randall, and Y. Zhou. Cilk: </author> <title> An Efficient Multithreaded Runtime System. </title> <booktitle> In Proceedings of Fifth ACM Symposium on Principles and Practice of Parallel Programming, </booktitle> <pages> pages 207-216, </pages> <month> July </month> <year> 1995. </year>
Reference-contexts: This work provides a solid theoretical ground for space-efficient scheduling, and it is still an open research problem how to integrate their techniques in practical systems. Their space model is different from ours and assumes a globally shared memory pool. The Cilk <ref> [6] </ref> run-time system addresses the space efficiency issue, but with a rather generous memory bound on each processor. Our work assumes that each processor has a maximum space limit and the goal is to make the data space cost to be close to optimal. <p> Our work assumes that each processor has a maximum space limit and the goal is to make the data space cost to be close to optimal. In addition, the scheduling scheme used in RAPID is static in the run-time pre-processing stage while the scheduling schemes used in [4] and <ref> [6] </ref> are dynamic. This is mainly because in practice it is difficult to minimize the run-time control overhead of dynamic scheduling in parallelizing sparse code with mixed granularities. 1.3 Organization of the dissertation The rest of the dissertation is organized as follows. <p> This work provides a solid theoretical ground for space-efficient scheduling, and it is still an open research problem how to integrate their techniques in practical systems as indicated by the authors. Their space model is different from ours and assumes a globally shared memory pool. The Cilk <ref> [6] </ref> run-time system addresses the space efficiency issue and its space complexity is O (S 1 ) per processor. The RAPID design presented in Chapter 3 uses at most S 1 space per processor. <p> The scheduling scheme CHAPTER 5. 108 we use is static in the run-time pre-processing stage while [4] and <ref> [6] </ref> use dynamic scheduling. This is mainly because in practice it is difficult to minimize the run-time control overhead of dynamic scheduling in parallelizing sparse code with mixed granularities.
Reference: [7] <author> Eric A. Brewer, Frederic T. Chong, Lok T. Liu, Shamik D. Sharma, and John Kubiatowicz. </author> <title> Remote Queues: Exposing Message Queues for Optimization and Atomicity. </title> <booktitle> In 7th Annual ACM Symposium on Parallel Algorithms and Architectures, </booktitle> <pages> pages 42-53, </pages> <year> 1995. </year>
Reference-contexts: In the situation of task parallelism with mixed granularities, it is not easy to decide where to insert the polling code <ref> [7] </ref>. Memory efficient scheduling and memory optimizations for irregular parallel applications. Most of previous research on scheduling [53, 62, 65] does CHAPTER 1. 6 not address memory issues. In [4], a dynamic scheduling algorithm for directed acyclic graphs is proposed. <p> For example, Figure 5.1 (b) is a schedule produced by RCP while (c) is a schedule produced by MPO. The ordering difference between Figure 5.1 (b) and (c) is that on processor 1, T <ref> [7; 8] </ref> is executed at time 6 by RCP while T [3; 10] is chosen instead at time 6 by MPO. The reason is that for RCP, T [7; 8] has a longer path from this task to an exit task (the path is T [7; 8]; T [8]; T [8; <p> The ordering difference between Figure 5.1 (b) and (c) is that on processor 1, T <ref> [7; 8] </ref> is executed at time 6 by RCP while T [3; 10] is chosen instead at time 6 by MPO. The reason is that for RCP, T [7; 8] has a longer path from this task to an exit task (the path is T [7; 8]; T [8]; T [8; 9] with length 4 because communication delay is also included) than other unscheduled tasks on P 1 . <p> that on processor 1, T <ref> [7; 8] </ref> is executed at time 6 by RCP while T [3; 10] is chosen instead at time 6 by MPO. The reason is that for RCP, T [7; 8] has a longer path from this task to an exit task (the path is T [7; 8]; T [8]; T [8; 9] with length 4 because communication delay is also included) than other unscheduled tasks on P 1 . <p> For MPO, T [3; 10] has a higher memory priority 1 because data d 3 and d 10 are all available locally at time 6, and T <ref> [7; 8] </ref>'s memory priority is 0.5 because the space for d 7 has not been allocated before time 6. As a result, the MPO schedule has a less memory requirement but leads to a longer parallel time.
Reference: [8] <author> S. Chakrabarti, J. Demmel, and K. Yelick. </author> <title> Modeling the Benefits of Mixed Data and Task Parallelism. </title> <booktitle> In Proceedings of 7th ACM Symposium on Parallel Algorithms and Architectures, </booktitle> <pages> pages 74-83, </pages> <month> July </month> <year> 1995. </year>
Reference-contexts: For example, Figure 5.1 (b) is a schedule produced by RCP while (c) is a schedule produced by MPO. The ordering difference between Figure 5.1 (b) and (c) is that on processor 1, T <ref> [7; 8] </ref> is executed at time 6 by RCP while T [3; 10] is chosen instead at time 6 by MPO. The reason is that for RCP, T [7; 8] has a longer path from this task to an exit task (the path is T [7; 8]; T [8]; T [8; <p> The ordering difference between Figure 5.1 (b) and (c) is that on processor 1, T <ref> [7; 8] </ref> is executed at time 6 by RCP while T [3; 10] is chosen instead at time 6 by MPO. The reason is that for RCP, T [7; 8] has a longer path from this task to an exit task (the path is T [7; 8]; T [8]; T [8; 9] with length 4 because communication delay is also included) than other unscheduled tasks on P 1 . <p> that on processor 1, T <ref> [7; 8] </ref> is executed at time 6 by RCP while T [3; 10] is chosen instead at time 6 by MPO. The reason is that for RCP, T [7; 8] has a longer path from this task to an exit task (the path is T [7; 8]; T [8]; T [8; 9] with length 4 because communication delay is also included) than other unscheduled tasks on P 1 . <p> The reason is that for RCP, T [7; 8] has a longer path from this task to an exit task (the path is T [7; 8]; T <ref> [8] </ref>; T [8; 9] with length 4 because communication delay is also included) than other unscheduled tasks on P 1 . <p> The reason is that for RCP, T [7; 8] has a longer path from this task to an exit task (the path is T [7; 8]; T [8]; T <ref> [8; 9] </ref> with length 4 because communication delay is also included) than other unscheduled tasks on P 1 . <p> For MPO, T [3; 10] has a higher memory priority 1 because data d 3 and d 10 are all available locally at time 6, and T <ref> [7; 8] </ref>'s memory priority is 0.5 because the space for d 7 has not been allocated before time 6. As a result, the MPO schedule has a less memory requirement but leads to a longer parallel time. <p> Therefore, one future work is to extend our results for more complicated task dependence structures, such as hierarchical graphs <ref> [8, 34, 48] </ref>. CHAPTER 6. 150 * It will improve the easiness of using the RAPID system if we can provide a preprocessor that takes a higher level application code and generates a C code containing calls to RAPID API.
Reference: [9] <author> F. T. Chong, S. D. Sharma, E. A. Brewer, and J. Saltz. </author> <title> Multiprocessor Run-time Support for Fine-Grained Irregular DAGs. </title> <editor> In Rajiv K. Kalia and Priya Vashishta, editors, </editor> <title> Toward Teraflop Computing and New Grand Challenge Applications., </title> <address> New York, 1995. </address> <publisher> Nova Science Publishers. </publisher>
Reference-contexts: In [59], it is found that pre-scheduling improves the CHAPTER 1. 4 performance of distributed sparse Cholesky factorization by 30% to 40% and there is still a lot of room for obtaining better absolute performance. Recent work by <ref> [9] </ref> demonstrates that using both effective DAG scheduling and low-overhead communication mechanisms, scalable performance can be obtained for solving fine-grained sparse triangular systems. Our work provides effective solutions to a more general class of irregular applications. Run-time support for parallel computations. <p> Low overhead communication mechanism. We have used low-level communication mechanisms (e.g., DMA) for designing the run-time support in RAPID system. We could use active messages [60] which have been used in <ref> [9] </ref> for executing fine-grained triangular solving DAGs. But we find that it is not easy to integrate active messages with a general task graph execution scheme because careful network polling is required as demonstrated in [9]. <p> We could use active messages [60] which have been used in <ref> [9] </ref> for executing fine-grained triangular solving DAGs. But we find that it is not easy to integrate active messages with a general task graph execution scheme because careful network polling is required as demonstrated in [9]. In the situation of task parallelism with mixed granularities, it is not easy to decide where to insert the polling code [7]. Memory efficient scheduling and memory optimizations for irregular parallel applications. Most of previous research on scheduling [53, 62, 65] does CHAPTER 1. 6 not address memory issues. <p> The reason is that for RCP, T [7; 8] has a longer path from this task to an exit task (the path is T [7; 8]; T [8]; T <ref> [8; 9] </ref> with length 4 because communication delay is also included) than other unscheduled tasks on P 1 .
Reference: [10] <author> M. Cosnard and M. Loi. </author> <title> Automatic Task Graph Generation Techniques. </title> <journal> Parallel Processing Letters, </journal> <volume> 5(4) </volume> <pages> 527-538, </pages> <month> December </month> <year> 1995. </year> <note> BIBLIOGRAPHY 153 </note>
Reference-contexts: Such codes are hard to write using existing parallel languages or libraries. The goal of this work is to deliver high performance irregular codes on distributed memory machines. Our future work will address the automatic generation of inspector specification code and automatic task partitioning <ref> [10, 47, 53] </ref>. CHAPTER 2. 14 2.2.2 An example Cholesky factorization is performed on a symmetric positive definite matrix A of size n fi n. In a block sparse Cholesky algorithm as shown in Figure 2.3, matrix A is partitioned into N fi N submatrices. <p> If the available amount of memory is 8 for each processor, then there are 2 units of memory for volatile objects on P 1 . In addition to the MAPs at the beginning of two task chains, there is another MAP right after task T <ref> [5; 10] </ref> on P 1 at which space for d 3 and d 5 will be freed and space for d 7 is allocated. The address for d 7 on P 1 is then notified to P 0 . <p> For example, Figure 5.1 (b) is a schedule produced by RCP while (c) is a schedule produced by MPO. The ordering difference between Figure 5.1 (b) and (c) is that on processor 1, T [7; 8] is executed at time 6 by RCP while T <ref> [3; 10] </ref> is chosen instead at time 6 by MPO. <p> For MPO, T <ref> [3; 10] </ref> has a higher memory priority 1 because data d 3 and d 10 are all available locally at time 6, and T [7; 8]'s memory priority is 0.5 because the space for d 7 has not been allocated before time 6.
Reference: [11] <author> R. Cytron and J. Ferrante. </author> <title> What's in a name? The Value of Renaming for Parallelism Detection and Storage Allocation. </title> <booktitle> In Proceedings of International Conference on Parallel Processing, </booktitle> <pages> pages 19-27, </pages> <month> February </month> <year> 1987. </year>
Reference-contexts: The true dependence can be enforced easily since one task cannot start to execute until the required data objects produced by its predecessors arrive at the local processor. With the presence of anti and output dependencies, run-time synchronization becomes more complicated. We can use renaming techniques <ref> [11] </ref> to remove these dependencies, however it needs additional memory optimization. We use the following simple strategy to remove output and anti dependence. 1. First we delete all the redundant edges for output and anti dependencies if they are subsumed by a dependence path in the graph. <p> The address then must be invalidated on other processors since this data object may have a new address at that processor. Data renaming would avoid this problem <ref> [11] </ref>, but it creates more complexity in indexing CHAPTER 5. 113 data objects and memory optimization. * Address buffering. We will also use the RMA feature to transfer addresses. Without providing address buffering, a processor cannot re-send address information unless the destination processor has read the previous address package.
Reference: [12] <author> R. Das, M. Uysal, J. Saltz, and Y.-S. Hwang. </author> <title> Communication Optimizations for Irregular Scientific Computations on Distributed Memory Architectures . Journal of Parallel and Distributed Computing, </title> <booktitle> 22(3) </booktitle> <pages> 462-479, </pages> <month> September </month> <year> 1994. </year>
Reference-contexts: Comparing to our work, Charm and Cilk are suitable for relatively coarser grain computation because dynamic load balancing on distributed memory machines has relatively high control overhead. There are also other run-time systems developed for irregular computations. The CHAOS system <ref> [12] </ref> has used the inspector/executor approach to exploit irregular parallelism at each iteration of the computation phase. The problems addressed in CHAOS have no loop-carried dependency at the computation phase and processors can run independently before next communication phase.
Reference: [13] <author> T. Davis. </author> <title> User's guide for the Unsymmetric-pattern Multifrontal Package (UMFPACK). </title> <type> Technical Report TR-93-020, </type> <institution> Computer and Information Sciences Department, University of Florida, </institution> <month> June </month> <year> 1993. </year>
Reference-contexts: SuperLU performs symbolic factorization and generates supernodes on the fly as the factorization proceeds. UMFPACK is another competitive sequential code for this problem and neither SuperLU nor UMFPACK is always better than the other <ref> [13, 14, 17] </ref>. MA41 is a code for sparse matrices with symmetric patterns. All of them are regarded as of high quality and deliver excellent megaflops performance.
Reference: [14] <author> T. Davis and I. Duff. </author> <title> A Combined Unifrontal/Multifrontal Method for Un-symmetric Sparse Matrices. </title> <type> Technical Report TR-95-020, </type> <institution> Computer and Information Science and Engineering Department, Univ. of Florida, </institution> <month> September </month> <year> 1997. </year>
Reference-contexts: SuperLU performs symbolic factorization and generates supernodes on the fly as the factorization proceeds. UMFPACK is another competitive sequential code for this problem and neither SuperLU nor UMFPACK is always better than the other <ref> [13, 14, 17] </ref>. MA41 is a code for sparse matrices with symmetric patterns. All of them are regarded as of high quality and deliver excellent megaflops performance.
Reference: [15] <author> T. Davis and I. S. Duff. </author> <title> An Unsymmetric-pattern Multifrontal Method for Sparse LU factorization. </title> <journal> SIAM Matrix Analysis & Applications, </journal> <month> January </month> <year> 1997. </year>
Reference-contexts: The adaptive and irregular nature of sparse LU data structures makes an efficient implementation of this algorithm very hard even on a modern sequential machine with memory hierarchies. There are several approaches that can be used for solving nonsymmetric systems. One approach is the unsymmetric-pattern multi-frontal method <ref> [15, 38] </ref> that uses elimination graphs to model irregular parallelism and guide the parallel computation. Another approach [28] is to restructure a sparse matrix into a bordered block upper triangular form and use a special pivoting technique which preserves the structure and maintains numerical stability at acceptable levels. <p> For sparse LU, an elimination tree of A T A does not directly reflect the available parallelism. Dynamically created DAGs have been used for modeling parallelism and guiding run-time execution in a nonsymmetric multi-frontal method <ref> [15, 38] </ref>.
Reference: [16] <author> J. Demmel. </author> <title> Numerical Linear Algebra on Parallel Processors. </title> <booktitle> Lecture Notes for NSF-CBMS Regional Conference in the Mathematical Sciences, </booktitle> <month> June </month> <year> 1995. </year> <note> To be published as a book by SIAM. BIBLIOGRAPHY 154 </note>
Reference-contexts: However in many applications, the associated equation systems involve nonsymmetric matrices and pivoting may be required to maintain numerical stability for such nonsymmetric linear systems <ref> [16, 35] </ref>. Because pivoting operations interchange rows based 63 CHAPTER 4. 64 on the numerical values of matrix elements during the elimination process, it is impossible to predict the precise structures of L and U factors without actually performing the numerical factorization. <p> Apply RAPID system to schedule and execute the re-structured sparse LU computation. From Table 3.1 we observe that on most current commodity processors with memory hierarchies, a highly optimized BLAS-3 subroutine usually outperforms a BLAS-2 subroutine in implementing the same numerical operations <ref> [16, 19] </ref>. We can afford to introduce some extra BLAS-3 operations in re-designing the LU algorithm so that the new algorithm is easy to be parallelized but the sequential performance of this code is still competitive to the current best sequential code. <p> It is also a fact that BLAS-3 routine DGEMM (matrix-matrix multiplication) is usually much faster than BLAS-1 and BLAS-2 routines <ref> [16] </ref>. Thus the key idea of our approach is that if we could find a way to maximize the use of DGEMM after using static symbolic factorization, even with overestimated nonzeros and extra numerical operations, the overall code performance could still be competitive to SuperLU which mainly uses DGEMV. <p> Instead of performing the row interchange to the right part of the matrix right after each pivoting search, a technique called "delayed-pivoting" is used <ref> [16] </ref>. In this technique, the pivoting sequence is held until the factorization of the k-th column block is completed. Then the pivoting sequence is applied to the rest of the matrix, i.e., interchange rows.
Reference: [17] <author> J. Demmel, S. Eisenstat, J. Gilbert, X. Li, and J. Liu. </author> <title> A Supernodal Approach to Sparse Partial Pivoting. </title> <type> Technical Report CSD-95-883, </type> <institution> UC Berke-ley, </institution> <month> September </month> <year> 1995. </year>
Reference-contexts: It is easy to get speedups by comparing a parallel code to a sequential code which does not fully exploit the uniprocessor capability, but it is not as easy to parallelize a highly optimized sequential code. One such sequential code is SuperLU <ref> [17] </ref> which uses a supernode approach to conduct sequential sparse LU with partial pivoting. The supernode partitioning makes it possible to perform most of the numerical updates using BLAS-2 level dense matrix-vector multiplications, and therefore to better exploit memory hierarchies. <p> SuperLU performs symbolic factorization and generates supernodes on the fly as the factorization proceeds. UMFPACK is another competitive sequential code for this problem and neither SuperLU nor UMFPACK is always better than the other <ref> [13, 14, 17] </ref>. MA41 is a code for sparse matrices with symmetric patterns. All of them are regarded as of high quality and deliver excellent megaflops performance. <p> We will use this static strategy in our S fl approach. But the overestimation does introduce extra fill-ins and some unnecessary operations in the numerical factorization. We observe that in SuperLU <ref> [17] </ref> the DGEMV routine (the BLAS-2 level dense matrix vector multiplication) accounts for 78% to 98% of the floating point operations (excluding the symbolic factorization part). It is also a fact that BLAS-3 routine DGEMM (matrix-matrix multiplication) is usually much faster than BLAS-1 and BLAS-2 routines [16]. <p> Thus it is necessary and beneficial to identify dense structures in a sparse matrix after the static symbolic factorization. It should be noted that there are some cases that static symbolic factorization leads to excessive overestimation. For example, memplus matrix <ref> [17] </ref> is such a case. The static scheme produces 119 times as many nonzeros as SuperLU does. In such situation, we have to resort to an alternative approach. <p> Excellent performance has been achieved in [39, 50, 51] using supernode partitioning for Cholesky factorization. However, the above definition is not directly applicable to sparse LU with nonsymmetric matrices. A good analysis for defining unsymmetric supernodes in an L factor is available in <ref> [17] </ref>. CHAPTER 4. 75 Notice that supernodes may need to be further broken into smaller ones to fit into cache and to expose more parallelism. <p> This results in very fine grained tasks. Amalgamating small supernodes can lead to great performance improvement for both parallel and sequential sparse codes because it can improve caching performance and reduce interprocessor communication overhead. There could be many ways to amalgamate supernodes <ref> [17, 50] </ref>. The basic idea is to relax the restriction that all the columns in a supernode must have exactly the same nonzero structure below diagonal. The amalgamation is usually guided by a supernode elimination tree. <p> Let be the ratio of symbolic factorization time to numerical factorization time in SuperLU, then we simplify Equation (4.5.1) to the following: T SuperLU = (1 + ) ! 2 C: (4.5.3) We estimate that 0:82 for the tested matrices based on the results in <ref> [17] </ref>. In [26], we have also measured as approximately 0:67. The ratios of the number of floating point operations performed in S fl and SuperLU for the tested matrices are available in Table 4.1. In average, the value of C 0 C is 3:98.
Reference: [18] <author> J. Demmel, J. Gilbert, and X. Li. </author> <title> An Asynchronous Parallel Supernodal Algorithm for Sparse Gaussian Elimination. </title> <type> Technical Report CSD-97-943, </type> <institution> UC Berkeley, </institution> <month> February </month> <year> 1997. </year>
Reference-contexts: It has achieved up to 6.88 GFLOPS on 128 nodes, which breaks the previous performance record <ref> [18] </ref> on sparse LU with partial pivoting.
Reference: [19] <author> J. Dongarra, J. Du Croz, S. Hammarling, and R. Hanson. </author> <title> An Extended Set of Basic Linear Algebra Subroutines. </title> <journal> ACM Trans. on Mathematical Software, </journal> <volume> 14 </volume> <pages> 18-32, </pages> <year> 1988. </year>
Reference-contexts: We have summarized typical numbers for the above architectures in Table 3.1. Note that in this table we list the computation performance achieved by the Basic Linear Algebra Subroutines (BLAS) <ref> [19] </ref> instead of the theoretical peak CHAPTER 3. 39 performance. This is simply because the BLAS library is optimized for each individual architecture and frequently used in the scientific applications such as sparse matrix computations. <p> Apply RAPID system to schedule and execute the re-structured sparse LU computation. From Table 3.1 we observe that on most current commodity processors with memory hierarchies, a highly optimized BLAS-3 subroutine usually outperforms a BLAS-2 subroutine in implementing the same numerical operations <ref> [16, 19] </ref>. We can afford to introduce some extra BLAS-3 operations in re-designing the LU algorithm so that the new algorithm is easy to be parallelized but the sequential performance of this code is still competitive to the current best sequential code.
Reference: [20] <author> I. Duff and J. Reid. </author> <title> The Multifrontal Solution of Indefinite Sparse Symmetric Systems of Equations. </title> <journal> ACM Transactions on Mathematical Software, </journal> <volume> 9 </volume> <pages> 302-325, </pages> <year> 1983. </year>
Reference-contexts: We also incorporate a supernode amalgamation <ref> [2, 20] </ref> technique to increase the granularity of the computation. We have used RAPID system to exploit irregular parallelism in the re-designed sparse LU algorithm with 1-D data mapping scheme. Usually 2-D data mapping CHAPTER 4. 67 can expose more parallelism, but it is very difficult to model the parallelism.
Reference: [21] <author> I. S. Duff. </author> <title> On Algorithms for Obtaining a Maximum Transversal. </title> <journal> ACM Transactions on Mathematical Software, </journal> <volume> 7(3) </volume> <pages> 315-330, </pages> <month> September </month> <year> 1981. </year>
Reference-contexts: For any nonsingular matrix which does not have a zero-free diagonal, it is always possible to permute the rows of the matrix so that the permuted matrix has a zero-free diagonal <ref> [21] </ref>. Though the symbolic factorization does work on a matrix that contains zero entries in the diagonal, it is not preferable because it makes the overestimation too generous. <p> This symbolic factorization is applied after an ordering is performed on the matrix A to reduce fill-ins. The ordering we are currently using is the multiple minimum degree ordering for A T A. We also permute the rows of the matrix using a transversal obtained from Duff's algorithm <ref> [21] </ref> to make A have a zero-free diagonal. The transversal can often help reduce fill-ins [22]. We have tested the storage impact of overestimation for a number of nonsymmetric testing matrices from various sources. The results are listed in Table 4.1.
Reference: [22] <author> I. S. Duff. </author> <type> Personal Communication, </type> <year> 1996. </year>
Reference-contexts: The ordering we are currently using is the multiple minimum degree ordering for A T A. We also permute the rows of the matrix using a transversal obtained from Duff's algorithm [21] to make A have a zero-free diagonal. The transversal can often help reduce fill-ins <ref> [22] </ref>. We have tested the storage impact of overestimation for a number of nonsymmetric testing matrices from various sources. The results are listed in Table 4.1.
Reference: [23] <author> C. Fu, X. Jiao, and T. Yang. </author> <title> Efficient Sparse LU Factorization with Partial Pivoting on Distributed Memory Architectures. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <note> 1997. Accepted for publication. Also as UCSB technical report TRCS97-11. BIBLIOGRAPHY 155 </note>
Reference-contexts: The 2-D data mapping is considered more scalable than 1-D data mapping because it enables parallelization of a single F actor (k) or U pdate (k; j) task on p r processors. For details of using asynchronous schedule executions to exploit 2-D parallelism, please refer to <ref> [23] </ref>. We also compare the RAPID code to another 1-D code which uses a block-cyclic mapping of tasks with a compute-ahead scheduling strategy instead of sophisticated graph scheduling algorithms used in RAPID. The idea of compute-ahead scheduling is to factor a column block as soon as all its updates finish.
Reference: [24] <author> C. Fu and T. Yang. </author> <title> Efficient Run-time Support for Irregular Task Computations with Mixed Granularities. </title> <booktitle> In Proceedings of IEEE International Parallel Processing Symposium, </booktitle> <pages> pages 823-830, </pages> <address> Hawaii, </address> <month> April </month> <year> 1996. </year>
Reference-contexts: The idea of compute-ahead scheduling is to factor a column block as soon as all its updates finish. This is demonstrated in Figure 4.10. It has been used to speed up parallel dense factorizations [35]. Graph scheduling has been shown effective in exploiting irregular parallelism for other applications (e.g., <ref> [24, 25] </ref>). Graph scheduling should outperform the CA scheduling for sparse LU because it does not have a constraint in ordering F actor () tasks. We demonstrate this point using the LU task graph in Figure 4.9.
Reference: [25] <author> C. Fu and T. Yang. </author> <title> Run-time Compilation for Parallel Sparse Matrix Computations. </title> <booktitle> In Proceedings of ACM International Conference on Supercomputing, </booktitle> <pages> pages 237-244, </pages> <address> Philadelphia, </address> <month> May </month> <year> 1996. </year>
Reference-contexts: The performance numbers reported in this dissertation are all obtained on Cray-T3E in San Diego CHAPTER 3. 41 Supercomputing Center, unless otherwise indicated. For performance results on other platforms, please refer to <ref> [25, 26, 27, 68] </ref>. 3.4.2 Sparse Cholesky factorization Sequential and parallel sparse Cholesky factorization algorithms have been studied extensively in literature. Rothberg and Schreiber [50, 51] show that the supernode-based approach can deliver good performance on both shared and distributed memory machines. <p> The idea of compute-ahead scheduling is to factor a column block as soon as all its updates finish. This is demonstrated in Figure 4.10. It has been used to speed up parallel dense factorizations [35]. Graph scheduling has been shown effective in exploiting irregular parallelism for other applications (e.g., <ref> [24, 25] </ref>). Graph scheduling should outperform the CA scheduling for sparse LU because it does not have a constraint in ordering F actor () tasks. We demonstrate this point using the LU task graph in Figure 4.9.
Reference: [26] <author> C. Fu and T. Yang. </author> <title> Sparse LU Factorization with Partial Pivoting on Distributed Memory Machines. </title> <booktitle> In Proceedings of ACM/IEEE Supercomputing'96, </booktitle> <address> Pittsburgh, </address> <month> November </month> <year> 1996. </year>
Reference-contexts: The performance numbers reported in this dissertation are all obtained on Cray-T3E in San Diego CHAPTER 3. 41 Supercomputing Center, unless otherwise indicated. For performance results on other platforms, please refer to <ref> [25, 26, 27, 68] </ref>. 3.4.2 Sparse Cholesky factorization Sequential and parallel sparse Cholesky factorization algorithms have been studied extensively in literature. Rothberg and Schreiber [50, 51] show that the supernode-based approach can deliver good performance on both shared and distributed memory machines. <p> Let be the ratio of symbolic factorization time to numerical factorization time in SuperLU, then we simplify Equation (4.5.1) to the following: T SuperLU = (1 + ) ! 2 C: (4.5.3) We estimate that 0:82 for the tested matrices based on the results in [17]. In <ref> [26] </ref>, we have also measured as approximately 0:67. The ratios of the number of floating point operations performed in S fl and SuperLU for the tested matrices are available in Table 4.1. In average, the value of C 0 C is 3:98. <p> The parallel time improvement ratios (1 P T a =P T ) on T3E for several testing matrices are listed in Table 4.4 and similar results on T3D are observed in <ref> [26] </ref>. Apparently the supernode amalgamation has brought significant improvement due to the increase of supernode size which implies an increase of the task granularities. This is important to obtaining good parallel performance [33].
Reference: [27] <author> C. Fu and T. Yang. </author> <title> Run-time Techniques for Exploiting Irregular Task Parallelism on Distributed Memory Architectures. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 42(2) </volume> <pages> 143-156, </pages> <month> May </month> <year> 1997. </year>
Reference-contexts: The performance numbers reported in this dissertation are all obtained on Cray-T3E in San Diego CHAPTER 3. 41 Supercomputing Center, unless otherwise indicated. For performance results on other platforms, please refer to <ref> [25, 26, 27, 68] </ref>. 3.4.2 Sparse Cholesky factorization Sequential and parallel sparse Cholesky factorization algorithms have been studied extensively in literature. Rothberg and Schreiber [50, 51] show that the supernode-based approach can deliver good performance on both shared and distributed memory machines.
Reference: [28] <author> K. Gallivan, B. Marsolf, and H. Wijshoff. </author> <title> The Parallel Solution of Nonsymmetric Sparse Linear Systems using H* Reordering and an Associated Factorization. </title> <booktitle> In Proc. of ACM International Conference on Supercomputing, </booktitle> <pages> pages 419-430, </pages> <address> Manchester, </address> <month> July </month> <year> 1994. </year>
Reference-contexts: There are several approaches that can be used for solving nonsymmetric systems. One approach is the unsymmetric-pattern multi-frontal method [15, 38] that uses elimination graphs to model irregular parallelism and guide the parallel computation. Another approach <ref> [28] </ref> is to restructure a sparse matrix into a bordered block upper triangular form and use a special pivoting technique which preserves the structure and maintains numerical stability at acceptable levels. This method has been implemented on Illinois Cedar multi-processors based on Aliant shared memory clusters.
Reference: [29] <author> G. A. Geist and E. Ng. </author> <title> Task Scheduling for Parallel Sparse Cholesky Factorization . International Journal of Parallel Programming, </title> <booktitle> 18(4) </booktitle> <pages> 291-314, </pages> <year> 1989. </year> <note> BIBLIOGRAPHY 156 </note>
Reference-contexts: Rothberg and Schreiber [50, 51] show that the supernode-based approach can deliver good performance on both shared and distributed memory machines. Specialized scheduling techniques have been studied for sparse Cholesky factorization in <ref> [29] </ref>. We will examine the performance of applying general task scheduling and executing techniques to this problem. Given a sparse matrix, we first use the multiple minimum degree algorithm to decide the ordering and then calculate fill-ins.
Reference: [30] <author> A. George and E. Ng. </author> <title> Symbolic Factorization for Sparse Gaussian Elimination with Partial Pivoting. </title> <journal> SIAM J. Scientific and Statistical Computing, </journal> <volume> 8(6) </volume> <pages> 877-898, </pages> <month> November </month> <year> 1987. </year>
Reference-contexts: We use the static symbolic factorization technique first proposed in <ref> [30, 31] </ref> to predict the worst possible structures of L and U factors without knowing the actual numerical values, then we develop a 2-D L/U supernode partitioning technique to identify dense structures in both L and U factors, and maximize the use of BLAS-3 level subroutines for these dense structures. <p> Using the precise pivoting information at each elimination step can certainly optimize data space usage, reduce communication and improve load balance, but such benefits could be offset by high run-time control and communication overhead. The strategy of static data structure prediction in <ref> [30] </ref> is valuable in avoiding dynamic symbolic factorization, identifying the maximum data dependence patterns and minimizing dynamic control overhead. We will use this static strategy in our S fl approach. But the overestimation does introduce extra fill-ins and some unnecessary operations in the numerical factorization. <p> It has been shown that the structure of L c can be used as an upper bound for the structures of L and U factors regardless of the choice of the pivot row at each step <ref> [30] </ref>. But it turns out that this bound is not very tight. It often substantially overestimates the structures of the L and U factors (refer to Table 4.1). Instead we consider another method from [30]. The basic idea is to statically consider all possible pivoting choices at each step. <p> of L and U factors regardless of the choice of the pivot row at each step <ref> [30] </ref>. But it turns out that this bound is not very tight. It often substantially overestimates the structures of the L and U factors (refer to Table 4.1). Instead we consider another method from [30]. The basic idea is to statically consider all possible pivoting choices at each step. The space is allocated for all the possible nonzeros that would be introduced by any pivoting sequence that could occur during the numerical factorization. We summarize the symbolic factorization method briefly as follows.
Reference: [31] <author> A. George and E. Ng. </author> <title> Parallel Sparse Gaussian Elimination with Partial Pivoting. </title> <journal> Annals of Operations Research, </journal> <volume> 22 </volume> <pages> 219-240, </pages> <year> 1990. </year>
Reference-contexts: This method has been implemented on Illinois Cedar multi-processors based on Aliant shared memory clusters. This chapter focuses on parallelization issues for a given column ordering with row interchanges to maintain numerical stability. Parallelization of sparse LU with partial pivoting is also studied in <ref> [31] </ref> on a shared memory machine. Their approaches overestimate the nonzero fill-ins by using a static symbolic LU factorization so that the dynamic variation of LU data structures is avoided. <p> We use the static symbolic factorization technique first proposed in <ref> [30, 31] </ref> to predict the worst possible structures of L and U factors without knowing the actual numerical values, then we develop a 2-D L/U supernode partitioning technique to identify dense structures in both L and U factors, and maximize the use of BLAS-3 level subroutines for these dense structures.
Reference: [32] <author> A. Gerasoulis, J. Jiao, and T. Yang. </author> <title> Scheduling of Structured and Unstructured Computation . In D. </title> <editor> Hsu, A. Rosenberg, and D. Sotteau, editors, </editor> <booktitle> Interconnections Networks and Mappings and Scheduling Parallel Computation, </booktitle> <pages> pages 139-172. </pages> <publisher> American Math. Society, </publisher> <year> 1995. </year>
Reference-contexts: The automatic parallelization of such problems on distributed memory machines is extremely difficult and presents a great challenge. Automatic scheduling and load balancing techniques are useful in exploiting irregular parallelism in unstructured computations <ref> [32, 45, 61] </ref>. Most of previous work in DAG scheduling has mainly focused on the algorithmic research for task mapping, e.g., [53, 63], and little research has been conducted on efficient run-time support for executing task schedules. <p> Application of graph scheduling for solving scientific problems. As we discussed before, application of graph scheduling has been used in large N-body simulations <ref> [32] </ref>. In [59], it is found that pre-scheduling improves the CHAPTER 1. 4 performance of distributed sparse Cholesky factorization by 30% to 40% and there is still a lot of room for obtaining better absolute performance. <p> Thus the weight variation is small and the degradation of the performance because of using the same initial schedule is also small based on the analysis in <ref> [32] </ref>. Of course after enough number of iterations, the particle distribution could have changed significantly. <p> These two issues make timing among tasks different from what is expected at the static time and tend to increase the processor idle time. The performance becomes even more sensitive when task granularities are small <ref> [32] </ref>. That is why techniques of increasing average task granularity, e.g., supernode amalgamation [2, 50], are very valuable. 3.4.6 Inspecting cost: case study In this section we examine how much time is spent at the inspector stage, i.e., the overhead for deriving, transforming and scheduling task dependence graph.
Reference: [33] <author> A. Gerasoulis and T. Yang. </author> <title> On the Granularity and Clustering of Directed Acyclic Task Graphs . IEEE Transactions on Parallel and Distributed Systems, </title> <booktitle> 4(6) </booktitle> <pages> 686-701, </pages> <month> June </month> <year> 1993. </year>
Reference-contexts: Apparently the supernode amalgamation has brought significant improvement due to the increase of supernode size which implies an increase of the task granularities. This is important to obtaining good parallel performance <ref> [33] </ref>.
Reference: [34] <author> M. Girkar and C. Polychronopoulos. </author> <title> Automatic Extraction of Functional Parallelism from Ordinary Programs. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 3(2) </volume> <pages> 166-178, </pages> <year> 1992. </year>
Reference-contexts: Therefore, one future work is to extend our results for more complicated task dependence structures, such as hierarchical graphs <ref> [8, 34, 48] </ref>. CHAPTER 6. 150 * It will improve the easiness of using the RAPID system if we can provide a preprocessor that takes a higher level application code and generates a C code containing calls to RAPID API.
Reference: [35] <author> G. Golub and J. M. Ortega. </author> <title> Scientific Computing: An Introduction with Parallel Computing Compilers . Academic Press, </title> <year> 1993. </year>
Reference-contexts: Generally a triangular solver is much less time consuming than factorization. A triangular system with multiple right-hand sides is useful in many scientific applications <ref> [35] </ref>. An interesting property with the triangular solver is that it is very easy to adjust the granularity of tasks by using different number of right-hand sides. <p> However in many applications, the associated equation systems involve nonsymmetric matrices and pivoting may be required to maintain numerical stability for such nonsymmetric linear systems <ref> [16, 35] </ref>. Because pivoting operations interchange rows based 63 CHAPTER 4. 64 on the numerical values of matrix elements during the elimination process, it is impossible to predict the precise structures of L and U factors without actually performing the numerical factorization. <p> The idea of compute-ahead scheduling is to factor a column block as soon as all its updates finish. This is demonstrated in Figure 4.10. It has been used to speed up parallel dense factorizations <ref> [35] </ref>. Graph scheduling has been shown effective in exploiting irregular parallelism for other applications (e.g., [24, 25]). Graph scheduling should outperform the CA scheduling for sparse LU because it does not have a constraint in ordering F actor () tasks.
Reference: [36] <author> L. Greengard. </author> <title> The Rapid Evaluation of Potential Fields in Particle Systems. </title> <type> PhD thesis, </type> <institution> Dept. of Computer Science, Yale University, </institution> <year> 1987. </year> <note> BIBLIOGRAPHY 157 </note>
Reference-contexts: For the definitions of these lists and details of the FMM algorithm, please refer to <ref> [36] </ref>. Here we only describe the task structures of the FMM algorithm so that we have a good idea how to apply the RAPID system to this problem. The computation of FMM consists of two passes: upward pass and downward pass. <p> The rationale of the inspector/executor approach is to amortize the inspecting cost over many iterations. For example, solving partial differential equations involves several thousand iterations of execution on the same task graph structure [42], and a typical N-body simulation involves hundreds of thousands of time steps <ref> [36] </ref>. We list the inspecting time and execution time for the three tested applications in Table 3.9. For each application, we choose the largest input data set we have tested, i.e., BCSSTK29 for sparse Cholesky, BCSSTK15 with NR=50 for sparse triangular solver, and 64 K particles for FMM.
Reference: [37] <author> A. Gupta and V. Kumar. </author> <title> Highly Scalable Parallel Algorithms for Sparse Matrix Factorization. </title> <type> Technical Report TR 94-63, </type> <institution> Computer Science, Univ. of Minnesota, </institution> <month> August </month> <year> 1994. </year>
Reference-contexts: If a matrix is symmetric and positive definite, Cholesky factorization can be used, for which fast sequential and parallel algorithms have been developed in the previous chapter and <ref> [37, 50, 51] </ref>. However in many applications, the associated equation systems involve nonsymmetric matrices and pivoting may be required to maintain numerical stability for such nonsymmetric linear systems [16, 35].
Reference: [38] <author> S. Hadfield and T. Davis. </author> <title> A Parallel Unsymmetric-pattern Multifrontal Method. </title> <type> Technical Report TR-94-028, </type> <institution> Computer and Information Sciences Departmenmt, University of Florida, </institution> <month> August </month> <year> 1994. </year>
Reference-contexts: The adaptive and irregular nature of sparse LU data structures makes an efficient implementation of this algorithm very hard even on a modern sequential machine with memory hierarchies. There are several approaches that can be used for solving nonsymmetric systems. One approach is the unsymmetric-pattern multi-frontal method <ref> [15, 38] </ref> that uses elimination graphs to model irregular parallelism and guide the parallel computation. Another approach [28] is to restructure a sparse matrix into a bordered block upper triangular form and use a special pivoting technique which preserves the structure and maintains numerical stability at acceptable levels. <p> For sparse LU, an elimination tree of A T A does not directly reflect the available parallelism. Dynamically created DAGs have been used for modeling parallelism and guiding run-time execution in a nonsymmetric multi-frontal method <ref> [15, 38] </ref>.
Reference: [39] <author> M. Heath, E. Ng, and B. Peyton. </author> <title> Parallel Algorithms for Sparse Linear Systems . SIAM Review, </title> <booktitle> 33(3) </booktitle> <pages> 420-460, </pages> <month> September </month> <year> 1991. </year>
Reference-contexts: In a block sparse Cholesky algorithm as shown in Figure 2.3, matrix A is partitioned into N fi N submatrices. A partitioning example is shown in Figure 2.4. Notice that this submatrix partitioning is not uniform due to supernode partitioning <ref> [39, 52] </ref>. We assume that the nonzero structure information is available after symbolic factorization and supernode partitioning. These operations are performed before task specification. Each data object is defined as a non-zero sub-matrix of A. The declaration of irregular data objects is shown in Figure 2.5. <p> Given a sparse matrix, we first use the multiple minimum degree algorithm to decide the ordering and then calculate fill-ins. For instance, Figure 3.6 is a 21fi21 CHAPTER 3. 42 sparse matrix with fill-ins. After adding fill-ins we use supernode partitioning <ref> [39] </ref> technique to partition the sparse matrix. Based on this data partitioning scheme, every basic task in the resulting program partitioning only involves dense matrix or vector operations which can be implemented using BLAS and LAPACK routines. We also split supernodes into smaller blocks. <p> To better exploit memory hierarchy in modern architectures, supernode partitioning is an important technique to exploit the regularity of sparse matrix computations and utilize BLAS routines to speed up the computation. It has been successfully applied to Cholesky factorization <ref> [39, 50, 51] </ref>. CHAPTER 4. 69 The difficulty for the nonsymmetric factorization is that supernode structure depends on pivoting choices during the factorization thus cannot be determined in advance. SuperLU performs symbolic factorization and identifies supernodes on the fly. <p> For a symmetric sparse matrix, a supernode is defined as a group of consecutive columns that have nested structure in the L fac tor of the matrix. Excellent performance has been achieved in <ref> [39, 50, 51] </ref> using supernode partitioning for Cholesky factorization. However, the above definition is not directly applicable to sparse LU with nonsymmetric matrices. A good analysis for defining unsymmetric supernodes in an L factor is available in [17].
Reference: [40] <author> M. Ibel, K. E. Schauser, C. J. Scheiman, and M. Weis. </author> <title> Implementing Active Messages and Split-C for SCI Clusters and Some Architectural Implications. </title> <booktitle> In Sixth International Workshop on SCI-based Low-cost/High-performance Computing, </booktitle> <month> September </month> <year> 1996. </year>
Reference-contexts: RMA is available on modern multi-processor architectures such as Cray-T3D/T3E (SHMEM) [58, 56], Meiko CS-2 (DMA) [54] and workstation clusters <ref> [40] </ref>. With RMA, a processor can write to memory of any other processor if a remote address is given. RMA allows passing data directly from one source location to a destination location, without any copying, packing/unpacking and buffering. The model of RMA is defined as follows. <p> For instance, the effective communication bandwidth of Meiko CS-2 is about 1 5MBytes/sec for a double-precision matrix of size less than 6 fi 6 under the assumption that the message is sent only once. For more detailed information on the communication performance of these platforms, please refer to <ref> [40, 54, 55, 58] </ref>. comput. (MFLOPS) communication 1st-level platform BLAS-3 BLAS-2 o.h. (s) b.w. (MB/s) cache (KB) Cray-T3E 388 255 0.5-2 500 8 Cray-T3D 103 85 2.7 126 8 SCI W.C. 121 85 7.9 30 16 Meiko CS-2 31 27 9 40 16 Table 3.1: Typical performance numbers for several distributed
Reference: [41] <author> X. Jiao. </author> <title> Parallel Sparse Gaussian Elimination with Partial Pivoting and 2-D Data Mapping. </title> <type> Master's thesis, </type> <institution> Dept. of Computer Science, University of California at Santa Barbara, </institution> <month> August </month> <year> 1997. </year>
Reference-contexts: We have used a simpler approach that does not require any permutation. This approach only amalgamates consecutive supernodes if their nonzero structures only differ by a small number of entries and it can be performed in a very efficient manner which only has a time complexity of O (n) <ref> [41] </ref>. We can control the maximum allowed differences by an amalgamation factor r. Our experiments show that when r is in the range of 4 6, it gives the best performance for the tested matrices and leads to 10 55% improvement on the execution times of the sequential code.
Reference: [42] <author> N. Karmarkar. </author> <title> A New Parallel Architecture for Sparse Matrix Computation Based on Finite Project Geometries. </title> <booktitle> In Proc. of Supercomputing '91, </booktitle> <pages> pages 358-369, </pages> <month> November </month> <year> 1991. </year>
Reference-contexts: The rationale of the inspector/executor approach is to amortize the inspecting cost over many iterations. For example, solving partial differential equations involves several thousand iterations of execution on the same task graph structure <ref> [42] </ref>, and a typical N-body simulation involves hundreds of thousands of time steps [36]. We list the inspecting time and execution time for the three tested applications in Table 3.9.
Reference: [43] <author> S.J. Kim and J.C. Browne. </author> <title> A General Approach to Mapping of Parallel Computation upon Multiprocessor Architectures, </title> . <booktitle> In Proceedings of International Conference on Parallel Processing, </booktitle> <pages> pages 318-328, </pages> <year> 1988. </year> <note> BIBLIOGRAPHY 158 </note>
Reference-contexts: We use a graph scheduling algorithm to exploit task parallelism and also determine the execution order of commuting operations so as to minimize parallel time. CHAPTER 2. 21 Algorithms for static scheduling of DAGs have been extensively studied in the literatures, e.g., <ref> [43, 53, 66] </ref>. The main optimizations are eliminating unnecessary communication to exploit data locality, overlapping communication with computation to hide communication latency, and exploiting task concurrency to balance loads among processors. A global performance monitoring for minimizing the overall execution time is needed to guide these optimizations.
Reference: [44] <author> S. Kohn and S. Baden. </author> <title> A Parallel Software Infrastructure for Structured Adaptive Mesh Methods. </title> <booktitle> In Proceedings of Supercomputing'95, </booktitle> <month> December </month> <year> 1995. </year>
Reference-contexts: The problems addressed in CHAOS have no loop-carried dependency at the computation phase and processors can run independently before next communication phase. Multipol [61] is a run-time library system which supports distributed data structures for several kinds of scientific applications. LPARX <ref> [44] </ref> is a run-time system designed for adaptive mesh methods. CHAPTER 1. 5 Consistency models. The issue of executing a program correctly has been studied in the context of distributed shared memory systems (DSM), e.g. Tread-Marks [1].
Reference: [45] <author> A. Lain and P. Banerjee. </author> <title> Techniques to Overlap Computation and Communication in Irregular Iterative Applications. </title> <booktitle> In Proc. of ACM Inter. Conf. on Supercomputing, </booktitle> <pages> pages 236-245, </pages> <month> July </month> <year> 1994. </year>
Reference-contexts: The automatic parallelization of such problems on distributed memory machines is extremely difficult and presents a great challenge. Automatic scheduling and load balancing techniques are useful in exploiting irregular parallelism in unstructured computations <ref> [32, 45, 61] </ref>. Most of previous work in DAG scheduling has mainly focused on the algorithmic research for task mapping, e.g., [53, 63], and little research has been conducted on efficient run-time support for executing task schedules.
Reference: [46] <author> J. W. H. Liu. </author> <title> Computational Models and Task Scheduling for Parallel Sparse Cholesky Factorization . Parallel Computing, </title> <booktitle> 18 </booktitle> <pages> 327-342, </pages> <year> 1986. </year>
Reference-contexts: The DAGs are constructed statically before numerical factorization. Previous work on exploiting task parallelism for sparse Cholesky factorization has used elimination trees (e.g. <ref> [46, 50] </ref>), which is a good way to expose the available parallelism because pivoting is not required. For sparse LU, an elimination tree of A T A does not directly reflect the available parallelism.
Reference: [47] <author> C. D. Polychronopoulos. </author> <title> Parallel Programming and Compilers. </title> <publisher> Kluwer Academic Publishers, </publisher> <year> 1988. </year>
Reference-contexts: Such codes are hard to write using existing parallel languages or libraries. The goal of this work is to deliver high performance irregular codes on distributed memory machines. Our future work will address the automatic generation of inspector specification code and automatic task partitioning <ref> [10, 47, 53] </ref>. CHAPTER 2. 14 2.2.2 An example Cholesky factorization is performed on a symmetric positive definite matrix A of size n fi n. In a block sparse Cholesky algorithm as shown in Figure 2.3, matrix A is partitioned into N fi N submatrices.
Reference: [48] <author> S. Ramaswamy, S. Sapatnekar, and P. Banerjee. </author> <title> A Convex Programming Approach for Exploiting Data and Functional Parallelism. </title> <booktitle> In Proceedings of International Conference on Parallel Processing, </booktitle> <pages> pages 116-125, </pages> <year> 1994. </year>
Reference-contexts: Therefore, one future work is to extend our results for more complicated task dependence structures, such as hierarchical graphs <ref> [8, 34, 48] </ref>. CHAPTER 6. 150 * It will improve the easiness of using the RAPID system if we can provide a preprocessor that takes a higher level application code and generates a C code containing calls to RAPID API.
Reference: [49] <author> M. Rinard. </author> <title> Communication Optimizations for Parallel Computing Using Data Access Information . In Proceedings of Supercomputing, </title> <address> San Diego, </address> <month> December </month> <year> 1995. </year>
Reference-contexts: We will also present the RAPID Application Programming Interface (API) which comprises a set of library functions. The design of library functions is based on three concepts: distributed shared data objects, tasks and access specifications. Similar concepts have been proposed in JADE <ref> [49] </ref> which extracts task dependence and schedules tasks dynamically. Such an approach has a flexibility to handle problems with adaptive structures; however, it is still an open problem to balance between the benefits of such flexibility and the run-time control overhead in parallelizing applications such as sparse matrix factorization [49]. <p> JADE <ref> [49] </ref> which extracts task dependence and schedules tasks dynamically. Such an approach has a flexibility to handle problems with adaptive structures; however, it is still an open problem to balance between the benefits of such flexibility and the run-time control overhead in parallelizing applications such as sparse matrix factorization [49]. Our approach extracts dependence and schedules tasks at the inspector stage to trade flexibility for performance. The rest of the chapter is organized as follows. Section 2.1 briefly 8 CHAPTER 2. 9 discusses the computation model used through this work.
Reference: [50] <author> E. Rothberg. </author> <title> Exploiting the Memory Hierarchy in Sequential and Parallel Sparse Cholesky Factorization. </title> <type> PhD thesis, </type> <institution> Dept. of Computer Science, Stanford, </institution> <month> December </month> <year> 1992. </year> <note> BIBLIOGRAPHY 159 </note>
Reference-contexts: For performance results on other platforms, please refer to [25, 26, 27, 68]. 3.4.2 Sparse Cholesky factorization Sequential and parallel sparse Cholesky factorization algorithms have been studied extensively in literature. Rothberg and Schreiber <ref> [50, 51] </ref> show that the supernode-based approach can deliver good performance on both shared and distributed memory machines. Specialized scheduling techniques have been studied for sparse Cholesky factorization in [29]. We will examine the performance of applying general task scheduling and executing techniques to this problem. <p> The parallel performance is shown in Figure 3.8 and all the speedups are obtained with consideration of commutativity. In average, we have obtained speedup 12:2 on 32 processors and 19:8 on 64 processors. All the parallel performance data have been obtained by incorporating supernode amalgamation technique <ref> [50, 2] </ref>, which essentially increases average supernode sizes, and eventually increases task granularity. To the best of our knowledge, this is the highest performance obtained for Cholesky factorization by automatically scheduled code. We will analyze and discuss the overhead of run-time execution in Section 3.4.5. <p> These two issues make timing among tasks different from what is expected at the static time and tend to increase the processor idle time. The performance becomes even more sensitive when task granularities are small [32]. That is why techniques of increasing average task granularity, e.g., supernode amalgamation <ref> [2, 50] </ref>, are very valuable. 3.4.6 Inspecting cost: case study In this section we examine how much time is spent at the inspector stage, i.e., the overhead for deriving, transforming and scheduling task dependence graph. The rationale of the inspector/executor approach is to amortize the inspecting cost over many iterations. <p> If a matrix is symmetric and positive definite, Cholesky factorization can be used, for which fast sequential and parallel algorithms have been developed in the previous chapter and <ref> [37, 50, 51] </ref>. However in many applications, the associated equation systems involve nonsymmetric matrices and pivoting may be required to maintain numerical stability for such nonsymmetric linear systems [16, 35]. <p> To better exploit memory hierarchy in modern architectures, supernode partitioning is an important technique to exploit the regularity of sparse matrix computations and utilize BLAS routines to speed up the computation. It has been successfully applied to Cholesky factorization <ref> [39, 50, 51] </ref>. CHAPTER 4. 69 The difficulty for the nonsymmetric factorization is that supernode structure depends on pivoting choices during the factorization thus cannot be determined in advance. SuperLU performs symbolic factorization and identifies supernodes on the fly. <p> For a symmetric sparse matrix, a supernode is defined as a group of consecutive columns that have nested structure in the L fac tor of the matrix. Excellent performance has been achieved in <ref> [39, 50, 51] </ref> using supernode partitioning for Cholesky factorization. However, the above definition is not directly applicable to sparse LU with nonsymmetric matrices. A good analysis for defining unsymmetric supernodes in an L factor is available in [17]. <p> This results in very fine grained tasks. Amalgamating small supernodes can lead to great performance improvement for both parallel and sequential sparse codes because it can improve caching performance and reduce interprocessor communication overhead. There could be many ways to amalgamate supernodes <ref> [17, 50] </ref>. The basic idea is to relax the restriction that all the columns in a supernode must have exactly the same nonzero structure below diagonal. The amalgamation is usually guided by a supernode elimination tree. <p> The DAGs are constructed statically before numerical factorization. Previous work on exploiting task parallelism for sparse Cholesky factorization has used elimination trees (e.g. <ref> [46, 50] </ref>), which is a good way to expose the available parallelism because pivoting is not required. For sparse LU, an elimination tree of A T A does not directly reflect the available parallelism. <p> Another advantage is that parallelism modeled by the above dependence structure can be effectively exploited using graph scheduling techniques in the RAPID system. 2-D data mapping. In the literature 2-D mapping has been shown more scalable than 1-D for sparse Cholesky <ref> [50, 51] </ref>. However there are several difficulties to apply the 2-D block-oriented mapping to the case of sparse LU factorization even the static structure is predicted. Firstly, pivoting operations and row interchanges require frequent and well-synchronized interprocessor communication when submatrices in the same column block are assigned to different processors.
Reference: [51] <author> E. Rothberg and R. Schreiber. </author> <title> Improved Load Distribution in Parallel Sparse Cholesky Factorization. </title> <booktitle> In Proceedings of ACM/IEEE Supercomputing, </booktitle> <pages> pages 783-792, </pages> <month> November </month> <year> 1994. </year>
Reference-contexts: For performance results on other platforms, please refer to [25, 26, 27, 68]. 3.4.2 Sparse Cholesky factorization Sequential and parallel sparse Cholesky factorization algorithms have been studied extensively in literature. Rothberg and Schreiber <ref> [50, 51] </ref> show that the supernode-based approach can deliver good performance on both shared and distributed memory machines. Specialized scheduling techniques have been studied for sparse Cholesky factorization in [29]. We will examine the performance of applying general task scheduling and executing techniques to this problem. <p> If a matrix is symmetric and positive definite, Cholesky factorization can be used, for which fast sequential and parallel algorithms have been developed in the previous chapter and <ref> [37, 50, 51] </ref>. However in many applications, the associated equation systems involve nonsymmetric matrices and pivoting may be required to maintain numerical stability for such nonsymmetric linear systems [16, 35]. <p> To better exploit memory hierarchy in modern architectures, supernode partitioning is an important technique to exploit the regularity of sparse matrix computations and utilize BLAS routines to speed up the computation. It has been successfully applied to Cholesky factorization <ref> [39, 50, 51] </ref>. CHAPTER 4. 69 The difficulty for the nonsymmetric factorization is that supernode structure depends on pivoting choices during the factorization thus cannot be determined in advance. SuperLU performs symbolic factorization and identifies supernodes on the fly. <p> For a symmetric sparse matrix, a supernode is defined as a group of consecutive columns that have nested structure in the L fac tor of the matrix. Excellent performance has been achieved in <ref> [39, 50, 51] </ref> using supernode partitioning for Cholesky factorization. However, the above definition is not directly applicable to sparse LU with nonsymmetric matrices. A good analysis for defining unsymmetric supernodes in an L factor is available in [17]. <p> Another advantage is that parallelism modeled by the above dependence structure can be effectively exploited using graph scheduling techniques in the RAPID system. 2-D data mapping. In the literature 2-D mapping has been shown more scalable than 1-D for sparse Cholesky <ref> [50, 51] </ref>. However there are several difficulties to apply the 2-D block-oriented mapping to the case of sparse LU factorization even the static structure is predicted. Firstly, pivoting operations and row interchanges require frequent and well-synchronized interprocessor communication when submatrices in the same column block are assigned to different processors. <p> We partially explain the reason by analyzing load balance factors of the 1-D RAPID code and the 2-D code in Figure 4.15. The load balance factor is defined as work total =(P work max ) <ref> [51] </ref>. Here we only count the work from the updating part because it is the major part of the computation. The 2-D code has better load balance, which can make up for the impact of lacking of efficient task scheduling.
Reference: [52] <author> E. Rothberg and R. Schreiber. </author> <title> Efficient Parallel Sparse Cholesky Factorization . In Proc. </title> <booktitle> of Seventh SIAM Conference on Parallel Processing for Scientific Computing, </booktitle> <pages> pages 407-412, </pages> <month> February </month> <year> 1995. </year>
Reference-contexts: In a block sparse Cholesky algorithm as shown in Figure 2.3, matrix A is partitioned into N fi N submatrices. A partitioning example is shown in Figure 2.4. Notice that this submatrix partitioning is not uniform due to supernode partitioning <ref> [39, 52] </ref>. We assume that the nonzero structure information is available after symbolic factorization and supernode partitioning. These operations are performed before task specification. Each data object is defined as a non-zero sub-matrix of A. The declaration of irregular data objects is shown in Figure 2.5.
Reference: [53] <author> V. Sarkar. </author> <title> Partitioning and Scheduling Parallel Programs for Execution on Multiprocessors. </title> <publisher> MIT Press, </publisher> <year> 1989. </year>
Reference-contexts: Automatic scheduling and load balancing techniques are useful in exploiting irregular parallelism in unstructured computations [32, 45, 61]. Most of previous work in DAG scheduling has mainly focused on the algorithmic research for task mapping, e.g., <ref> [53, 63] </ref>, and little research has been conducted on efficient run-time support for executing task schedules. <p> In the situation of task parallelism with mixed granularities, it is not easy to decide where to insert the polling code [7]. Memory efficient scheduling and memory optimizations for irregular parallel applications. Most of previous research on scheduling <ref> [53, 62, 65] </ref> does CHAPTER 1. 6 not address memory issues. In [4], a dynamic scheduling algorithm for directed acyclic graphs is proposed. This work provides a solid theoretical ground for space-efficient scheduling, and it is still an open research problem how to integrate their techniques in practical systems. <p> Such codes are hard to write using existing parallel languages or libraries. The goal of this work is to deliver high performance irregular codes on distributed memory machines. Our future work will address the automatic generation of inspector specification code and automatic task partitioning <ref> [10, 47, 53] </ref>. CHAPTER 2. 14 2.2.2 An example Cholesky factorization is performed on a symmetric positive definite matrix A of size n fi n. In a block sparse Cholesky algorithm as shown in Figure 2.3, matrix A is partitioned into N fi N submatrices. <p> We use a graph scheduling algorithm to exploit task parallelism and also determine the execution order of commuting operations so as to minimize parallel time. CHAPTER 2. 21 Algorithms for static scheduling of DAGs have been extensively studied in the literatures, e.g., <ref> [43, 53, 66] </ref>. The main optimizations are eliminating unnecessary communication to exploit data locality, overlapping communication with computation to hide communication latency, and exploiting task concurrency to balance loads among processors. A global performance monitoring for minimizing the overall execution time is needed to guide these optimizations. <p> The proposed techniques are incorporated into the RAPID system, and experiments with incorporating the proposed memory optimizing techniques show that solvable problem sizes with limited memory space can be increased substantially without paying too much extra control overhead. Most of previous research on scheduling <ref> [53, 62, 65] </ref> does not address memory issues.
Reference: [54] <author> K. E. Schauser and C. J. Scheiman. </author> <title> Experience with Active Messages on the Meiko CS-2. </title> <booktitle> In International Parallel Processing Symposium, </booktitle> <pages> pages 140-149, </pages> <year> 1995. </year>
Reference-contexts: RMA is available on modern multi-processor architectures such as Cray-T3D/T3E (SHMEM) [58, 56], Meiko CS-2 (DMA) <ref> [54] </ref> and workstation clusters [40]. With RMA, a processor can write to memory of any other processor if a remote address is given. RMA allows passing data directly from one source location to a destination location, without any copying, packing/unpacking and buffering. The model of RMA is defined as follows. <p> For example, we have implemented our system on Meiko CS-2 which provides Direct CHAPTER 3. 30 Memory Access (DMA) as the major way to access non-local memory. Each node of Meiko CS-2 is also equipped with a DMA co-processor for handling communication. It takes the main processor 9 microseconds <ref> [54] </ref> to dispatch the remote memory access descriptor to the co-processor. The co-processor afterwards will be responsible for sending data without interrupting the main processor so that the opportunity of overlapping communication and computation is maximized. <p> For instance, the effective communication bandwidth of Meiko CS-2 is about 1 5MBytes/sec for a double-precision matrix of size less than 6 fi 6 under the assumption that the message is sent only once. For more detailed information on the communication performance of these platforms, please refer to <ref> [40, 54, 55, 58] </ref>. comput. (MFLOPS) communication 1st-level platform BLAS-3 BLAS-2 o.h. (s) b.w. (MB/s) cache (KB) Cray-T3E 388 255 0.5-2 500 8 Cray-T3D 103 85 2.7 126 8 SCI W.C. 121 85 7.9 30 16 Meiko CS-2 31 27 9 40 16 Table 3.1: Typical performance numbers for several distributed
Reference: [55] <author> S. L. Scott. </author> <title> Synchronization and Communication in the T3E Multiprocess. </title> <booktitle> In ASPLOS-VII, </booktitle> <month> October </month> <year> 1996. </year>
Reference-contexts: For instance, the effective communication bandwidth of Meiko CS-2 is about 1 5MBytes/sec for a double-precision matrix of size less than 6 fi 6 under the assumption that the message is sent only once. For more detailed information on the communication performance of these platforms, please refer to <ref> [40, 54, 55, 58] </ref>. comput. (MFLOPS) communication 1st-level platform BLAS-3 BLAS-2 o.h. (s) b.w. (MB/s) cache (KB) Cray-T3E 388 255 0.5-2 500 8 Cray-T3D 103 85 2.7 126 8 SCI W.C. 121 85 7.9 30 16 Meiko CS-2 31 27 9 40 16 Table 3.1: Typical performance numbers for several distributed
Reference: [56] <author> S. L. Scott and G. M. Thorson. </author> <title> The Cray T3E Network: Adaptive Routing in a High Performance 3D Torus. </title> <booktitle> In Proceedings of HOT Interconnects IV, </booktitle> <month> August </month> <year> 1996. </year>
Reference-contexts: RMA is available on modern multi-processor architectures such as Cray-T3D/T3E (SHMEM) <ref> [58, 56] </ref>, Meiko CS-2 (DMA) [54] and workstation clusters [40]. With RMA, a processor can write to memory of any other processor if a remote address is given. RMA allows passing data directly from one source location to a destination location, without any copying, packing/unpacking and buffering.
Reference: [57] <author> W. Shu and L. Kale. </author> <title> Chare Kernel A Runtime Support System for Parallel Computations. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 11(3) </volume> <pages> 198-211, </pages> <month> March </month> <year> 1991. </year>
Reference-contexts: Recent work by [9] demonstrates that using both effective DAG scheduling and low-overhead communication mechanisms, scalable performance can be obtained for solving fine-grained sparse triangular systems. Our work provides effective solutions to a more general class of irregular applications. Run-time support for parallel computations. The Charm <ref> [57] </ref> system adopts a message driven approach for general asynchronous computation using dynamic scheduling. The Cilk [5] multi-threaded system aims at applications that have "strict" dependence structures. Randomized load balancing and work stealing techniques are used to execute a dynamic DAG.
Reference: [58] <author> T. Stricker, J. Stichnoth, D. O'Hallaron, S. Hinrichs, and T. Gross. </author> <title> De-coupling Synchronization and Data Transfer in Message Passing Systems of BIBLIOGRAPHY 160 Parallel Computers. </title> <booktitle> In Proceedings of ACM International Conference on Supercomputing, </booktitle> <pages> pages 1-10, </pages> <address> Barcelona, </address> <month> July </month> <year> 1995. </year>
Reference-contexts: RMA is available on modern multi-processor architectures such as Cray-T3D/T3E (SHMEM) <ref> [58, 56] </ref>, Meiko CS-2 (DMA) [54] and workstation clusters [40]. With RMA, a processor can write to memory of any other processor if a remote address is given. RMA allows passing data directly from one source location to a destination location, without any copying, packing/unpacking and buffering. <p> For instance, the effective communication bandwidth of Meiko CS-2 is about 1 5MBytes/sec for a double-precision matrix of size less than 6 fi 6 under the assumption that the message is sent only once. For more detailed information on the communication performance of these platforms, please refer to <ref> [40, 54, 55, 58] </ref>. comput. (MFLOPS) communication 1st-level platform BLAS-3 BLAS-2 o.h. (s) b.w. (MB/s) cache (KB) Cray-T3E 388 255 0.5-2 500 8 Cray-T3D 103 85 2.7 126 8 SCI W.C. 121 85 7.9 30 16 Meiko CS-2 31 27 9 40 16 Table 3.1: Typical performance numbers for several distributed
Reference: [59] <author> S. Venugopal, V. Naik, and J. Saltz. </author> <title> Performance of Distributed Sparse Cholesky Factorization with Pre-scheduling. </title> <booktitle> In Proc. of Supercomputing'92, </booktitle> <pages> pages 52-61, </pages> <address> Minneapolis, </address> <month> November </month> <year> 1992. </year>
Reference-contexts: Application of graph scheduling for solving scientific problems. As we discussed before, application of graph scheduling has been used in large N-body simulations [32]. In <ref> [59] </ref>, it is found that pre-scheduling improves the CHAPTER 1. 4 performance of distributed sparse Cholesky factorization by 30% to 40% and there is still a lot of room for obtaining better absolute performance.
Reference: [60] <author> T. von Eicken, D. E. Culler, S. C. Goldstein, and K. E. Schauser. </author> <title> Active Messages: a Mechanism for Integrated Communication and Computation. </title> <booktitle> In Proc. of the 19th Int'l Symposium on Computer Architecture, </booktitle> <address> Gold Coast, Australia, </address> <month> May </month> <year> 1992. </year>
Reference-contexts: Hence DSM normally has higher overhead than our scheme and has a difficulty for obtaining good performance for sparse matrix problems. Low overhead communication mechanism. We have used low-level communication mechanisms (e.g., DMA) for designing the run-time support in RAPID system. We could use active messages <ref> [60] </ref> which have been used in [9] for executing fine-grained triangular solving DAGs. But we find that it is not easy to integrate active messages with a general task graph execution scheme because careful network polling is required as demonstrated in [9]. <p> Usually in order to support asynchronous communication, a message passing system such as Intel NX/2 uses system buffer space to manage incoming and outgoing messages. It is well known that message-buffering imposes a significant amount of overheads for copying and space management on source and destination processors <ref> [60] </ref>. Secondly, effort is required to maintain data dependencies among tasks. Each task has to receive the correct copies of desired data objects. Figure 3.1 shows a situation in which a task could receive a wrong copy of the desired data object.
Reference: [61] <author> C.-P. Wen, S. Chakrabarti, E. Deprit, A. Krishnamurthy, and K. Yelick. </author> <title> Runtime Support for Portable Distributed Data Structures, chapter 9. Languages, Compilers, and Runtime Systems for Scalable Computers. </title> <publisher> Kluwer Academic Publishers, </publisher> <month> May </month> <year> 1995. </year>
Reference-contexts: The automatic parallelization of such problems on distributed memory machines is extremely difficult and presents a great challenge. Automatic scheduling and load balancing techniques are useful in exploiting irregular parallelism in unstructured computations <ref> [32, 45, 61] </ref>. Most of previous work in DAG scheduling has mainly focused on the algorithmic research for task mapping, e.g., [53, 63], and little research has been conducted on efficient run-time support for executing task schedules. <p> The CHAOS system [12] has used the inspector/executor approach to exploit irregular parallelism at each iteration of the computation phase. The problems addressed in CHAOS have no loop-carried dependency at the computation phase and processors can run independently before next communication phase. Multipol <ref> [61] </ref> is a run-time library system which supports distributed data structures for several kinds of scientific applications. LPARX [44] is a run-time system designed for adaptive mesh methods. CHAPTER 1. 5 Consistency models.
Reference: [62] <author> R. Wolski and J. Feo. </author> <title> Program Partitioning for NUMA Multiprocessor Computer Systems. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <year> 1993. </year>
Reference-contexts: In the situation of task parallelism with mixed granularities, it is not easy to decide where to insert the polling code [7]. Memory efficient scheduling and memory optimizations for irregular parallel applications. Most of previous research on scheduling <ref> [53, 62, 65] </ref> does CHAPTER 1. 6 not address memory issues. In [4], a dynamic scheduling algorithm for directed acyclic graphs is proposed. This work provides a solid theoretical ground for space-efficient scheduling, and it is still an open research problem how to integrate their techniques in practical systems. <p> The proposed techniques are incorporated into the RAPID system, and experiments with incorporating the proposed memory optimizing techniques show that solvable problem sizes with limited memory space can be increased substantially without paying too much extra control overhead. Most of previous research on scheduling <ref> [53, 62, 65] </ref> does not address memory issues.
Reference: [63] <author> M. Y. Wu and D. Gajski. Hypertool: </author> <title> A Programming Aid for Message-passing Systems . IEEE Transactions on Parallel and Distributed Systems, </title> <booktitle> 1(3) </booktitle> <pages> 330-343, </pages> <year> 1990. </year>
Reference-contexts: Automatic scheduling and load balancing techniques are useful in exploiting irregular parallelism in unstructured computations [32, 45, 61]. Most of previous work in DAG scheduling has mainly focused on the algorithmic research for task mapping, e.g., <ref> [53, 63] </ref>, and little research has been conducted on efficient run-time support for executing task schedules.
Reference: [64] <author> T. Yang. </author> <title> Scheduling and Code Generation for Parallel Architectures. </title> <type> PhD thesis, </type> <institution> Dept. of Computer Science, Rutgers University, </institution> <address> New Brunswick, </address> <month> May </month> <year> 1993. </year> <note> BIBLIOGRAPHY 161 </note>
Reference-contexts: Most of previous work in DAG scheduling has mainly focused on the algorithmic research for task mapping, e.g., [53, 63], and little research has been conducted on efficient run-time support for executing task schedules. The early work in 1 CHAPTER 1. 2 the PYRROS system <ref> [64, 66] </ref> provides a complete framework for general task computation; however, its run-time support system uses the NX/2 level communication interface and overhead for message management is high, which prevents PYRROS from obtaining good performance for problems such as sparse matrix factorizations.
Reference: [65] <author> T. Yang and A. Gerasoulis. </author> <title> List Scheduling with and without Communication Delays. </title> <journal> Parallel Computing, </journal> <volume> 19 </volume> <pages> 1321-1344, </pages> <year> 1992. </year>
Reference-contexts: In the situation of task parallelism with mixed granularities, it is not easy to decide where to insert the polling code [7]. Memory efficient scheduling and memory optimizations for irregular parallel applications. Most of previous research on scheduling <ref> [53, 62, 65] </ref> does CHAPTER 1. 6 not address memory issues. In [4], a dynamic scheduling algorithm for directed acyclic graphs is proposed. This work provides a solid theoretical ground for space-efficient scheduling, and it is still an open research problem how to integrate their techniques in practical systems. <p> The proposed techniques are incorporated into the RAPID system, and experiments with incorporating the proposed memory optimizing techniques show that solvable problem sizes with limited memory space can be increased substantially without paying too much extra control overhead. Most of previous research on scheduling <ref> [53, 62, 65] </ref> does not address memory issues. <p> In the second stage tasks assigned to each processor are also ordered to overlap communication with computation so that maximum inter-processor parallelism CHAPTER 5. 123 is explored. This ordering algorithm is called RCP <ref> [65] </ref>. A RCP schedule is time efficient, but may not be space-efficient because it executes tasks in the order of importance based on the critical path information, which may require more memory to hold volatile objects.
Reference: [66] <author> T. Yang and A. Gerasoulis. </author> <title> PYRROS: Static Task Scheduling and Code Generation for Message-Passing Multiprocessors . In Proc. </title> <booktitle> of 6th ACM International Conference on Supercomputing, </booktitle> <pages> pages 428-437, </pages> <year> 1992. </year>
Reference-contexts: Most of previous work in DAG scheduling has mainly focused on the algorithmic research for task mapping, e.g., [53, 63], and little research has been conducted on efficient run-time support for executing task schedules. The early work in 1 CHAPTER 1. 2 the PYRROS system <ref> [64, 66] </ref> provides a complete framework for general task computation; however, its run-time support system uses the NX/2 level communication interface and overhead for message management is high, which prevents PYRROS from obtaining good performance for problems such as sparse matrix factorizations. <p> We use a graph scheduling algorithm to exploit task parallelism and also determine the execution order of commuting operations so as to minimize parallel time. CHAPTER 2. 21 Algorithms for static scheduling of DAGs have been extensively studied in the literatures, e.g., <ref> [43, 53, 66] </ref>. The main optimizations are eliminating unnecessary communication to exploit data locality, overlapping communication with computation to hide communication latency, and exploiting task concurrency to balance loads among processors. A global performance monitoring for minimizing the overall execution time is needed to guide these optimizations. <p> At the second stage, clusters are mapped to a fixed number of physical processors CHAPTER 2. 22 available at the run-time by using the PYRROS algorithm <ref> [66] </ref> to balance loads and overlap computation with communication. scheduled task graph. T 3 has to wait for the arrival of data object x sent from T 1 at processor 0. Computation of T 2 can be overlapped with this sending. <p> Optimizations have to be performed to eliminate redundant messages. CHAPTER 3. 28 Correspondingly, in the destination only one receiving operation is needed to pull out the data from the network. Subsequent receiving operations for this data object should be re-directed to read from the local memory instead. PYRROS <ref> [66] </ref> uses a buffered message-passing mechanism and aggregates communication as much as possible.
Reference: [67] <author> T. Yang and A. Gerasoulis. </author> <title> DSC: Scheduling Parallel Tasks on An Unbounded Number of Processors. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 5(9) </volume> <pages> 951-967, </pages> <year> 1994. </year> <note> A short version is in Proceedings of Supercomputing'91. </note>
Reference-contexts: The scheduling algorithm using weight and dependence information has two stages. At the first stage, we cluster tasks to a set of threads (or directly call them clusters) to reduce communication and exploit data locality. Two clustering strategies are used: 1) Use the DSC algorithm <ref> [67] </ref>. 2) Form clusters based on the data accessing patterns. If tasks write or modify the same data object, they will be assigned into one cluster. This data-driven approach is essentially following the owner-compute rule. Currently we use this strategy when a task graph contains commuting operations. <p> At the first stage tasks are clustered to exploit data locality using DSC <ref> [67] </ref> or the owner-compute rule, i.e., all the tasks that modify the same object are assigned to the same cluster. Clusters are then mapped to physical processors using a load balancing criterion. For simplicity of the description, we assume that each task modifies only one object in this section.
Reference: [68] <author> H. Zhu, C. Fu, and T. Yang. </author> <title> An Efficient Communication Mechanism on SCI and Its Application in Parallelizing Irregular Computations. </title> <note> In Proceedings of SCIzzL-8 workshop, to appear, </note> <month> November </month> <year> 1997. </year>
Reference-contexts: The performance numbers reported in this dissertation are all obtained on Cray-T3E in San Diego CHAPTER 3. 41 Supercomputing Center, unless otherwise indicated. For performance results on other platforms, please refer to <ref> [25, 26, 27, 68] </ref>. 3.4.2 Sparse Cholesky factorization Sequential and parallel sparse Cholesky factorization algorithms have been studied extensively in literature. Rothberg and Schreiber [50, 51] show that the supernode-based approach can deliver good performance on both shared and distributed memory machines.
References-found: 68

