URL: http://www-robotics.usc.edu/~maja/publications/ijcai95ws.ps.gz
Refering-URL: http://www-robotics.usc.edu/~maja/publications.html
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Email: maja@cs.brandeis.edu  
Title: Learning in Multi-Robot Systems  
Author: Maja J Mataric 
Address: Waltham, MA 02254  
Affiliation: Volen Center for Complex Systems Computer Science Department Brandeis University  
Abstract: This paper 1 discusses why traditional reinforcement learning methods, and algorithms applied to those models, result in poor performance in dynamic, situated multi-agent domains characterized by multiple goals, noisy perception and action, and inconsistent reinforcement. We propose a methodology for designing the representation and the forcement functions that take advantage of implicit domain knowledge in order to accelerate learning in such domains, and demonstrate it experimentally in two different mobile robot domains.
Abstract-found: 1
Intro-found: 1
Reference: <author> Agre, P. E. & Chapman, D. </author> <year> (1990), </year> <title> What are plans for?, </title> <editor> in P. Maes, ed., </editor> <title> `Designing Autonomous Agents', </title> <publisher> MIT Press, </publisher> <pages> pp. 17-34. </pages>
Reference-contexts: Much of simulation work attempts to hide continuous state, such as the inputs from complex sensors, by presuming higher-level filters most of which have proven unrealistic in physical systems <ref> (Agre & Chapman 1990) </ref>. In general, in situated domains, which are dynamic and noisy, the agent cannot sense world state or even its own state correctly or completely. 1.2 Transitions vs.
Reference: <author> Altenburg, K. </author> <year> (1994), </year> <title> Adaptive Resource Allocation for a Multiple Mobile Robot System using Communication, </title> <type> Technical Report NDSU-CSOR-TR-9404, </type> <institution> North Dakota State Univeristy. </institution>
Reference: <author> Arkin, R. C., Balch, T. & Nitz, E. </author> <year> (1993), </year> <title> Communication of Behavioral State in Multi-agent Retrieval Tasks, </title> <booktitle> in `IEEE International Conference on Robotics and Automation', </booktitle> <publisher> IEEE Computer Society Press, Los Alamitos, CA, </publisher> <pages> pp. 588-594. </pages>
Reference: <author> Atkeson, C. G. </author> <year> (1990), </year> <title> Memory-Based Approaches to Approximating Continuous Functions, </title> <booktitle> in `Proceedings, Sixth Yale Workshop on Adaptive and Learning Systems'. </booktitle>
Reference-contexts: One more direct way to utilize implicit domain knowledge is to convert reward functions into error signals, akin to those used in learning control <ref> (Atkeson 1990) </ref>. Immediate reinforcement in RL is a weak version of error signals, using only the sign of the error but not the magnitude. Intermittent reinforcement can be used similarly, by weighting the reward according to the accomplished progress.
Reference: <author> Beckers, R., Holland, O. E. & Deneubourg, J. L. </author> <year> (1994), </year> <title> From Local Actions to Global Tasks: Stigmergy and Collective Robotics, </title> <editor> in R. Brooks & P. Maes, eds, </editor> <booktitle> `Artificial Life IV, Proceedings of the Fourth International Workshop on the Synthesis and Simulation of Living Systems', </booktitle> <publisher> MIT Press. </publisher>
Reference: <author> Donald, B. R., Jennings, J. & Rus, D. </author> <year> (1993), </year> <title> Information Invariants for Cooperating Autonomous Mobile Robots, </title> <booktitle> in `Proc. International Symposium on Robotics Research', </booktitle> <address> Hidden Valley, PA. </address>
Reference: <author> Fukuda, T., Nadagawa, S., Kawauchi, Y. & Buss, M. </author> <year> (1989), </year> <title> Structure Decision for Self Organizing Robots Based on Cell Structures - CEBOT, </title> <booktitle> in `IEEE International Conference on Robotics and Automation', </booktitle> <publisher> IEEE Computer Society Press, Los Alamitos, </publisher> <address> CA, Scottsdale, Arizona, </address> <pages> pp. 695-700. </pages>
Reference: <author> Jaakkola, T. & Jordan, M. I. </author> <year> (1993), </year> <title> `On the Convergence of Stochastic Iterative Dynamic Programming Algorithms', </title> <note> Submitted to Neural Computation. </note>
Reference-contexts: All of these properties which make modeling difficult make learning world models even more challenging (Whitehead 1992). 1.3 Learning Trials Traditional RL models allow for proving convergence properties of various forms of temporal differencing (TD) applied to deterministic MDP environments but require infinite trials for asymptotic convergence <ref> (Jaakkola & Jordan 1993) </ref>. Thus, even in ideal Markovian worlds the required number of trials is prohibitive for all but the smallest state spaces.
Reference: <author> Kaelbling, L. P. </author> <year> (1990), </year> <title> Learning in Embedded Systems, </title> <type> PhD thesis, </type> <institution> Stanford University. </institution>
Reference: <author> Maes, P. & Brooks, R. A. </author> <year> (1990), </year> <title> Learning to Coordinate Behaviors, </title> <booktitle> in `Proceedings, AAAI-90', </booktitle> <address> Boston, MA, </address> <pages> pp. 796-802. </pages>
Reference-contexts: Feedback Design of reinforcement functions is not often discussed, although it is perhaps the most difficult aspect of setting up an RL system. Immediate reinforcement, when available, is the most effective <ref> (Maes & Brooks 1990) </ref>. More delayed reinforcement requires the introduction of subgoals in order to make the task learnable under realistic time constraints (Mahadevan & Connell 1991).
Reference: <author> Mahadevan, S. & Connell, J. </author> <year> (1991), </year> <title> Automatic Programming of Behavior-Based Robots Using Reinforcement Learning, </title> <booktitle> in `Proceedings, AAAI-91', </booktitle> <address> Pitts-burgh, PA, </address> <pages> pp. 8-14. </pages>
Reference-contexts: Immediate reinforcement, when available, is the most effective (Maes & Brooks 1990). More delayed reinforcement requires the introduction of subgoals in order to make the task learnable under realistic time constraints <ref> (Mahadevan & Connell 1991) </ref>. Situated domains tend to fall in between the two popular reinforcement extremes, providing some immediate rewards, plenty of intermittent delayed ones, and only few very delayed ones.
Reference: <author> Mataric, M. J. </author> <year> (1992), </year> <title> Designing Emergent Behaviors: From Local Interactions to Collective Intelligence, </title> <editor> in J.-A. Meyer, H. Roitblat & S. Wilson, eds, </editor> <booktitle> `From Animals to Animats: International Conference on Simulation of Adaptive Behavior'. </booktitle>
Reference-contexts: Individually, each robot learns to select the best behavior for each condition, in order to find and take home the most pucks. Foraging was chosen because it is a nontrivial and biologically inspired task, and because our previous group behavior work <ref> (Mataric 1992) </ref> provided the basic behavior repertoire from which to learn behavior selection, consisting of avoiding, dispersing, searching, homing, and resting. Utility behaviors for grasping and dropping were hard-wired and their conditions were not learned.
Reference: <author> Mataric, M. J. </author> <year> (1994), </year> <title> Interaction and Intelligent Behavior, </title> <type> Technical Report AI-TR-1495, </type> <institution> MIT Artificial Intelligence Lab. </institution>
Reference: <author> Noreils, F. R. </author> <year> (1993), </year> <title> `Toward a Robot Architecture Integrating Cooperation between Mobile Robots: Application to Indoor Environment', </title> <journal> The International Journal of Robotics Research 12(1), </journal> <pages> 79-98. </pages>
Reference: <author> Parker, L. E. </author> <year> (1994), </year> <title> Heterogeneous Multi-Robot Cooperation, </title> <type> PhD thesis, </type> <institution> MIT. </institution>
Reference: <author> Simsarian, K. T. & Mataric, M. J. </author> <year> (1995), </year> <title> Learning to Cooperate Using Two Six-Legged Mobile Robots, </title> <booktitle> in `Proceedings, Third European Workshop of Learning Robots', </booktitle> <address> Heraklion, Crete, Greece. </address>
Reference-contexts: We successfully applied the above-described learning paradigm to this task and explored different communication and turn-taking strategies. The details of the experiments and the data are described in <ref> (Simsarian & Mataric 1995) </ref>. 6 Related Work Numerous simulations of multi-agent systems have been developed in the Artificial Intelligence, Distributed Artificial Intelligence, and Artificial Life communities.
Reference: <author> Singh, S. P. </author> <year> (1991), </year> <title> Transfer of Learning Across Compositions of Sequential Tasks, </title> <booktitle> in `Proceedings, Eighth International Conference on Machine Learning', </booktitle> <publisher> Morgan Kaufmann, </publisher> <address> Evanston, </address> <publisher> Illinois, </publisher> <pages> pp. 348-352. </pages>
Reference-contexts: In traditional RL, learning a multi-goal policy requires that the goals must be formulated as sequential and consistent sub-goals of a monolithic reward function <ref> (Singh 1991) </ref>, or that separate state spaces and reinforcement functions are used for each of the goals (Whitehead, Karlsson & Tenenberg 1993).
Reference: <author> Sutton, R. </author> <year> (1988), </year> <title> `Learning to Predict by Method of Temporal Differences', </title> <booktitle> Machine Learning 3(1), </booktitle> <pages> 9-44. </pages>
Reference-contexts: 1 Learning in Situated Domains Successful applications of RL methodologies to well-behaved domains <ref> (Sutton 1988, Watkins 1989, Kaelbling 1990) </ref> have encouraged researchers to hypothesize about its value for learning on situated agents such as mobile robots. However, while simulation results are encouraging, work on physical robots has not yet repeated that success.
Reference: <author> Tan, M. </author> <year> (1993), </year> <title> Multi-Agent Reinforcement Learning: Independent vs. </title> <booktitle> Cooperative Agents, in `Proceedings, Tenth International Conference on Machine Learning', </booktitle> <address> Amherst, MA, </address> <pages> pp. 330-337. </pages>
Reference: <author> Watkins, C. J. C. H. </author> <year> (1989), </year> <title> Learning from Delayed Rewards, </title> <type> PhD thesis, </type> <institution> King's College, </institution> <address> Cambridge. </address>
Reference: <author> Whitehead, S. D. </author> <year> (1992), </year> <title> Reinforcement Learning for the Adaptive Control of Perception and Action, </title> <type> PhD thesis, </type> <institution> University of Rochester. </institution>
Reference-contexts: All of these properties which make modeling difficult make learning world models even more challenging <ref> (Whitehead 1992) </ref>. 1.3 Learning Trials Traditional RL models allow for proving convergence properties of various forms of temporal differencing (TD) applied to deterministic MDP environments but require infinite trials for asymptotic convergence (Jaakkola & Jordan 1993). <p> The proposed subgoals are directly tied to behaviors which are used as the basis of learning. Similarly, progress estimators are also mapped to one or more behaviors, and expedite learning of the associated goals, unlike a single complete external critic used with a monolithic reinforcement function <ref> (Whitehead 1992) </ref>. Multi-agent learning has been largely treated in simulation. For example, Tan (1993) has applied RL to a simulated multi-agent domain whose simplicity allows for retaining the MDP assumption that fails to hold for physical robots.
Reference: <author> Whitehead, S. D., Karlsson, J. & Tenenberg, J. </author> <year> (1993), </year> <title> Learning Multiple Goal Behavior via Task Decomposition and Dynamic Policy Merging, </title> <editor> in J. H. Con-nell & S. Mahadevan, eds, </editor> <title> `Robot Learning', </title> <publisher> Kluwer Academic Publishers, </publisher> <pages> pp. 45-78. </pages>
Reference-contexts: In traditional RL, learning a multi-goal policy requires that the goals must be formulated as sequential and consistent sub-goals of a monolithic reward function (Singh 1991), or that separate state spaces and reinforcement functions are used for each of the goals <ref> (Whitehead, Karlsson & Tenenberg 1993) </ref>.
References-found: 22

