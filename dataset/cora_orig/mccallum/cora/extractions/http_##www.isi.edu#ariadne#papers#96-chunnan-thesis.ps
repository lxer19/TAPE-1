URL: http://www.isi.edu/ariadne/papers/96-chunnan-thesis.ps
Refering-URL: http://www.isi.edu/~knoblock/
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Title: LEARNING EFFECTIVE AND ROBUST KNOWLEDGE FOR SEMANTIC QUERY OPTIMIZATION  
Author: by Chun-Nan Hsu 
Degree: A Dissertation Presented to the FACULTY OF THE GRADUATE SCHOOL  In Partial Fulfillment of the Requirements for the Degree DOCTOR OF PHILOSOPHY (Computer Science)  
Note: Copyright 1997 Chun-Nan Hsu  
Date: December 1996  
Affiliation: UNIVERSITY OF SOUTHERN CALIFORNIA  
Abstract-found: 0
Intro-found: 1
Reference: [ Agrawal et al., 1993 ] <author> Rakesh Agrawal, Tomasz Imielinski, and Arun Swami. </author> <title> Mining association rules between sets of items in large databases. </title> <booktitle> In Proceedings of ACM SIGMOD, </booktitle> <pages> pages 207-216, </pages> <address> Washington, DC, </address> <year> 1993. </year>
Reference-contexts: Our approach learns conjunctive queries which must cover all positive instances but no negative instances. Another difference is their search heuristics. foil uses an information-gain heuristic while our approach uses a set-covering heuristic for learning a low-cost specification. Research work on data mining for association rules <ref> [ Agrawal et al., 1993, Man-nila et al., 1994 ] </ref> is related to our work in that they also generate rules from large databases. Their approach generates a set of data patterns from a table, and then converts those patterns into association rules.
Reference: [ Ambite et al., 1995 ] <author> Jose-Luis Ambite, Yigal Arens, Naveen Ashish, Chin Y. Chee, Chun-Nan Hsu, Craig A. Knoblock, Wei-Min Shen, and Sheila Tejada. </author> <title> The SIMS manual: Version 1.0. </title> <type> Technical Report ISI/TM-95-428, </type> <institution> University of Southern California, Information Sciences Institute, </institution> <year> 1995. </year>
Reference-contexts: The queries and rules are expressed in the loom knowledge representation language [ MacGregor, 1990 ] . Please refer to the sims manual <ref> [ Ambite et al., 1995 ] </ref> for a formal definition of the syntax of the loom language. ;;;;;; Input query: (214 "List all wharves at Long Beach with RORO ramps, by pier name and berth ID where METEOR ships can dock" (sims-retrieve (?PNAME ?BERTHID) (:AND (SHIP CLASS ?SHIP) (SHIP CLASS.SH CLASS
Reference: [ Apers et al., 1983 ] <author> Peter M.G. Apers, Alan R. Hevner, and S.Bing Yao. </author> <title> Optimizing algorithms for distributed queries. </title> <journal> IEEE Trans. on Software Engineering, </journal> <volume> 9 </volume> <pages> 57-68, </pages> <year> 1983. </year>
Reference: [ Arens et al., 1993 ] <author> Yigal Arens, Chin Y. Chee, Chun-Nan Hsu, and Craig A. Knoblock. </author> <title> Retrieving and integrating data from multiple information sources. </title> <journal> International Journal on Intelligent and Cooperative Information Systems, </journal> <volume> 2(2) </volume> <pages> 127-159, </pages> <year> 1993. </year>
Reference: [ Arens et al., 1996 ] <author> Yigal Arens, Craig A. Knoblock, and Wei-Min Shen. </author> <title> Query reformulation for dynamic information integration. </title> <journal> Journal of Intelligent Information Systems, Special Issue on Intelligent Information Integration, </journal> <year> 1996. </year>
Reference: [ Bacchus et al., 1992 ] <author> Fahiem Bacchus, Adam Grove, Joseph Y. Halpern, and Daphne Koller. </author> <title> From statistics to beliefs. </title> <booktitle> In Proceedings of the Tenth National Conference on Artificial Intelligence(AAAI-92), </booktitle> <pages> pages 602-608, </pages> <address> San Jose, CA, </address> <year> 1992. </year>
Reference-contexts: The formalism proposed by [ Bacchus, 1988 ] , [ Halpern, 1990 ] , and [ Bacchus et al., 1992, Bacchus et al., 1993, Bacchus et al., 1994 ] for uncertain reasoning, in spite of the different motivation, is quite similar to robustness. <ref> [ Bacchus et al., 1992 ] </ref> defines the degree of belief in a given logic sentence ' as the probability of the set of worlds where ' is true. They further define this probability as the ratio between the number of all possible worlds and worlds where ' is true. <p> They further define this probability as the ratio between the number of all possible worlds and worlds where ' is true. This is the same as Definition 2.1, if 30 we consider a database as a model of "worlds." <ref> [ Bacchus et al., 1992 ] </ref> also surveys early philosophical work on probability that discuss related uncertainty measures.
Reference: [ Bacchus et al., 1993 ] <author> Fahiem Bacchus, Adam Grove, Joseph Y. Halpern, and Daphne Koller. </author> <title> Statistical foundations for default reasoning. </title> <booktitle> In Proceedings of the 13th International Joint Conference on Artificial Intelligence(IJCAI-93), </booktitle> <pages> pages 563-569, </pages> <address> Chambery, France, </address> <year> 1993. </year>
Reference: [ Bacchus et al., 1994 ] <author> Fahiem Bacchus, Adam Grove, Joseph Y. Halpern, and Daphne Koller. </author> <title> Forming beliefs about a changing world. </title> <booktitle> In Proceedings of the Twelfth National Conference on Artificial Intelligence(AAAI-94), </booktitle> <pages> pages 222-229, </pages> <address> Seattle, WA, </address> <year> 1994. </year> <month> 130 </month>
Reference: [ Bacchus, 1988 ] <author> Fahiem Bacchus. </author> <title> Representing and Reasoning with Probabilistic Knowledge. </title> <type> PhD thesis, </type> <institution> University of Alberta, Edmonton, Alta., Canada, </institution> <year> 1988. </year> <note> Also available from MIT Press, </note> <year> 1990. </year>
Reference-contexts: Our emphasis on transactions in our definition of robustness is analogous in spirit to the notion of accessibility in the possible worlds semantics of modal logic [ Ramsay, 1988 ] . The formalism proposed by <ref> [ Bacchus, 1988 ] </ref> , [ Halpern, 1990 ] , and [ Bacchus et al., 1992, Bacchus et al., 1993, Bacchus et al., 1994 ] for uncertain reasoning, in spite of the different motivation, is quite similar to robustness. [ Bacchus et al., 1992 ] defines the degree of belief in
Reference: [ Cestnik and Bratko, 1991 ] <author> Bojan Cestnik and Ivan Bratko. </author> <title> On estimating probabilities in tree pruning. </title> <booktitle> In Machine Learning - EWSL-91, European Working Session on Learning, </booktitle> <pages> pages 138-150. </pages> <publisher> Springer-Verlag, </publisher> <address> Berlin, Germany, </address> <year> 1991. </year>
Reference-contexts: A detailed description and a proof of the Laplace law can be found in [ Howson and Urbach, 1988 ] . The Laplace law applies to any repeatable experiments (e.g., tossing a coin). The Laplace law is a special case of a modified estimate called m-Probability <ref> [ Cestnik and Bratko, 1991 ] </ref> . A prior probability of outcomes can be brought to bear in this more general estimate. m-Probability Let r, n, and C be as in the description of the Laplace law. <p> The Laplace law is a special case of the m-probability estimate with Pr (C) = 1=k, and m = k. The prior probability used here is that k outcomes are equally probable. The m-probability estimate has been used in many machine learning systems for different purposes <ref> [ Cestnik and Bratko, 1991, Lavrac and Dzeroski, 1994 ] </ref> . Convincing results in handling noisy data and pruning decision trees have been achieved. The advantage of the Laplace estimate is that it takes both known relative frequency and prior probability into account.
Reference: [ Chakravarthy et al., 1990 ] <author> Upen S. Chakravarthy, John Grant, and Jack Minker. </author> <title> Logic-based approach to semantic query optimization. </title> <journal> ACM Transactions on Database Systems, </journal> <volume> 15(2) </volume> <pages> 162-207, </pages> <year> 1990. </year>
Reference: [ Chu and Chen, 1994 ] <author> Wesley W. Chu and Qiming Chen. </author> <title> A pattern-based data and knowledge integration for intelligent query answering. </title> <journal> Journal of Integrated Computer-Aided Engineering, </journal> <volume> 1(5), </volume> <year> 1994. </year>
Reference-contexts: Imprecise Query Answering Imprecise query answering retrieves data that does not precisely satisfy a given query. Examples of imprecise query answering include intentional query answering <ref> [ Chu and Chen, 1994 ] </ref> , which assumes that an input query only specifies a subset of the data that a user intends to retrieve, and query answering with uncertainty, which retrieves data that satisfies a given query along some measure of uncertainty.
Reference: [ Clark and Boswell, 1991 ] <author> Peter Clark and Robin Boswell. </author> <title> Rule induction with CN2: Some recent improvements. </title> <booktitle> In Machine Learning - EWSL-91, European Working Session on Learning, </booktitle> <pages> pages 151-163. </pages> <publisher> Springer-Verlag, </publisher> <address> Berlin, Germany, </address> <year> 1991. </year>
Reference: [ Clark and Niblett, 1989 ] <author> Peter Clark and Tim Niblett. </author> <title> The CN2 induction algorithm. </title> <journal> Machine Learning, </journal> <volume> 3(4) </volume> <pages> 261-283, </pages> <year> 1989. </year>
Reference-contexts: This is not enough for dynamic closed-world databases where updates and deletions may affect the validity of a rule, as we discussed earlier. In addition to predictive accuracy, <ref> [ Clark and Niblett, 1989 ] </ref> proposed using significance in rule induction to measure the correlation between the antecedents and consequent of a rule by computing the likelihood ratio of the data coverage of a rule.
Reference: [ Cohen, 1993 ] <author> William W. Cohen. </author> <title> Efficient pruning methods for separate-and-conquer rule learning systems. </title> <booktitle> In Proceedings of the 13th International Joint Conference on Artificial Intelligence(IJCAI-93), </booktitle> <address> Chambery, France, </address> <year> 1993. </year>
Reference: [ Cohen, 1995a ] <author> Paul R. Cohen. </author> <title> Empirical methods for artificial intelligence. </title> <publisher> The MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1995. </year>
Reference-contexts: This experiment applies a k-fold cross validation <ref> [ Cohen, 1995a ] </ref> to test the effectiveness of the learned rules. The 23 training queries are randomly divided into four sets, three of them contain 6 queries, and one contains 5 queries.
Reference: [ Cohen, 1995b ] <author> William W. Cohen. </author> <title> Fast effective rule induction. </title> <booktitle> In Machine Learning, Proceedings of the 12th International Conference(ML-95), </booktitle> <address> San Mateo, CA, 1995. </address> <publisher> Morgan Kaufmann. </publisher>
Reference: [ Cormen et al., 1989 ] <author> Thomas H. Cormen, Charles E. Leiserson, and Ronald L. Rivest. </author> <title> Introduction To Algorithms. </title> <publisher> The MIT Press/McGraw-Hill Book Co., </publisher> <address> Cambridge, MA, </address> <year> 1989. </year>
Reference-contexts: The motivation of this formula is also from the generalized minimum set covering problem. The gain/cost heuristic has been proved to generate a set cover within a small ratio bound (ln n + 1) of the optimal set covering cost <ref> [ Cormen et al., 1989 ] </ref> , where n is the number of input sets. In this problem, the cost of a set is a constant and the total cost of the entire set cover is the sum of the cost of each set. <p> As we have discussed earlier, the cost of a set cover is the sum of the cost of each set and the cost of each member set is constant. <ref> [ Cormen et al., 1989 ] </ref> shows that the gain/cost heuristic allows an algorithm to generate a set cover within a small ratio bound (ln n + 1) of the optimal set covering cost, where n is the number of input sets.
Reference: [ Cussens, 1993 ] <author> James Cussens. </author> <title> Bayes and pesudo-Bayes estimates of conditional probabilities and their reliability. </title> <booktitle> In Machine Learning: ECML-93, </booktitle> <pages> pages 136-152, </pages> <address> Berlin, Germany, 1993. </address> <publisher> Springer-Verlag. </publisher>
Reference: [ DeJong and Mooney, 1986 ] <author> Gerald DeJong and Raymond Mooney. </author> <title> Explanation-based learning: An alternative view. </title> <journal> Machine Learning, </journal> <volume> 1(2) </volume> <pages> 145-176, </pages> <year> 1986. </year> <month> 131 </month>
Reference: [ desJardins, 1992 ] <author> Marie E. desJardins. PAGODA: </author> <title> A model for autonomous learn-ing in probabilistic domains. </title> <type> PhD thesis, </type> <institution> University of California, Berkeley, Computer Science Division, </institution> <year> 1992. </year>
Reference: [ Doorenbos et al., 1992 ] <author> Bob Doorenbos, Milind Tambe, and Allen Newell. </author> <booktitle> Learning 10,000 chunks: What's it like out there? In Proceedings of the Tenth National Conference on Artificial Intelligence(AAAI-92), </booktitle> <pages> pages 830-836, </pages> <address> San Jose, CA, </address> <year> 1992. </year>
Reference-contexts: They can be classified into two general approaches. One approach is to organize rules so that an applicable rule can be efficiently matched. For example, the rete algorithm [ Forgy, 1982 ] and its more advanced descendants <ref> [ Tambe, 1991, Doorenbos et al., 1992 ] </ref> are in this category. The other approach is to use a rule maintainer to remove redundant and low utility rules so as to decrease the number of rules.
Reference: [ Etzioni, 1992 ] <author> Oren Etzioni. </author> <title> An asymptotic analysis of speedup learning. </title> <booktitle> In Proceedings of the Ninth International Workshop on Machine Learning, </booktitle> <pages> pages 129-136, </pages> <address> San Mateo, CA, 1992. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: The other approach is to use a rule maintainer to remove redundant and low utility rules so as to decrease the number of rules. Redundant rules are logical consequences of other rules in a rule set. [ Greiner and Likuski, 1989 ] and <ref> [ Etzioni, 1992 ] </ref> show that redundant macro-operators are guaranteed to slow down a problem solver. [ Yu and Sun, 1989 ] extends the SQO optimizer to identify logically redundant rules. Low utility rules are those rarely applicable or yield small saving.
Reference: [ Fayyad et al., 1996 ] <editor> Usama M. Fayyad, Gregory Piatetsky-Shapiro, Padhraic Smyth, and Ramasamy Uthurusamy, editors. </editor> <booktitle> Advances in Knowledge Discovery and Data Mining. </booktitle> <publisher> AAAI Press/MIT press, </publisher> <address> Cambridge, MA, </address> <year> 1996. </year>
Reference-contexts: The results confirm our assumption about effectiveness and robustness, and show that the learned rules are both effective and robust. Chapter 6 reviews the contributions and describes some future work. 14 Chapter 2 Robustness of Knowledge Many applications of machine learning and data mining <ref> [ Piatetsky-Shapiro and Frawley, 1991, Fayyad et al., 1996 ] </ref> such as learning for semantic query optimization require the knowledge to be consistent with data. However, databases usually change over time and make machine-learned knowledge inconsistent with data.
Reference: [ Forgy, 1982 ] <author> Charles L. Forgy. </author> <title> RETE: A fast algorithm for the many pattern/many object pattern matching problem. </title> <booktitle> Artificial Intelligence, </booktitle> <pages> pages 17-37, </pages> <year> 1982. </year>
Reference-contexts: Many approaches have been developed to decrease the match cost and address the utility problem. They can be classified into two general approaches. One approach is to organize rules so that an applicable rule can be efficiently matched. For example, the rete algorithm <ref> [ Forgy, 1982 ] </ref> and its more advanced descendants [ Tambe, 1991, Doorenbos et al., 1992 ] are in this category. The other approach is to use a rule maintainer to remove redundant and low utility rules so as to decrease the number of rules.
Reference: [ Furnkranz and Widmer, 1994 ] <author> Johannes Furnkranz and Gerhard Widmer. </author> <title> Incremental reduced error prunning. </title> <booktitle> In Machine Learning, Proceedings of the 11th International Conference(ML-94), </booktitle> <address> San Mateo, CA, 1994. </address> <publisher> Morgan Kaufmann. </publisher>
Reference: [ Gil, 1992 ] <author> Yolanda Gil. </author> <title> Acquiring Domain Knowledge for Planning by Experimentation. </title> <type> PhD thesis, </type> <institution> Carnegie Mellon University, </institution> <address> Pittsburgh, PA, </address> <year> 1992. </year>
Reference: [ Ginsberg, 1987 ] <author> Matthew L. Ginsberg. </author> <title> Readings in Nonmonotonic Reasoning. </title> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, CA, </address> <year> 1987. </year>
Reference-contexts: Reasoning about the consistency of beliefs and knowledge after changes to closed-world relational data is an important research subject in nonmonotonic and uncertain reasoning <ref> [ Ginsberg, 1987, Shafer and Pearl, 1990 ] </ref> . Our emphasis on transactions in our definition of robustness is analogous in spirit to the notion of accessibility in the possible worlds semantics of modal logic [ Ramsay, 1988 ] .
Reference: [ Goodman, 1946 ] <author> Nielson Goodman. </author> <title> A query on confirmation. </title> <journal> Journal of Philosophy, </journal> <volume> 43 </volume> <pages> 383-385, </pages> <year> 1946. </year>
Reference-contexts: This way, a system can achieve desired behavior of query answering depending on the application domain. 6.2.2 Justification of Induction Robustness of knowledge in its general sense provides a partial solution to the justification of induction raised in <ref> [ Goodman, 1946 ] </ref> . The problem is to evaluate the two induced rules: 1. All emeralds are green. 2. All emeralds are grue, which means green if observed before tomorrow, and blue if observed then after.
Reference: [ Greiner and Likuski, 1989 ] <author> Russel Greiner and Joseph Likuski. </author> <title> Incorporating redundant learned rules: A preliminary formal analysis of EBL. </title> <booktitle> In Proceedings of the Eleventh International Joint Conference on Artificial Intelligence, </booktitle> <pages> pages 744-749, </pages> <address> Detroit, MI, </address> <year> 1989. </year>
Reference-contexts: The other approach is to use a rule maintainer to remove redundant and low utility rules so as to decrease the number of rules. Redundant rules are logical consequences of other rules in a rule set. <ref> [ Greiner and Likuski, 1989 ] </ref> and [ Etzioni, 1992 ] show that redundant macro-operators are guaranteed to slow down a problem solver. [ Yu and Sun, 1989 ] extends the SQO optimizer to identify logically redundant rules. Low utility rules are those rarely applicable or yield small saving.
Reference: [ Halpern, 1990 ] <author> Joseph Y. Halpern. </author> <title> An analysis of first-order logics of probability. </title> <journal> Artificial Intelligence, </journal> <volume> 46(3), </volume> <month> December </month> <year> 1990. </year>
Reference-contexts: Our emphasis on transactions in our definition of robustness is analogous in spirit to the notion of accessibility in the possible worlds semantics of modal logic [ Ramsay, 1988 ] . The formalism proposed by [ Bacchus, 1988 ] , <ref> [ Halpern, 1990 ] </ref> , and [ Bacchus et al., 1992, Bacchus et al., 1993, Bacchus et al., 1994 ] for uncertain reasoning, in spite of the different motivation, is quite similar to robustness. [ Bacchus et al., 1992 ] defines the degree of belief in a given logic sentence '
Reference: [ Hammer and Zdonik, 1980 ] <author> Michael Hammer and Stanley B. Zdonik. </author> <title> Knowledge-based query processing. </title> <booktitle> In Proceedings of the Sixth VLDB Conference, </booktitle> <pages> pages 137-146, </pages> <address> Washington, DC, </address> <year> 1980. </year> <month> 132 </month>
Reference: [ Hammer et al., 1995 ] <author> Joachim Hammer, Hector Garcia-Molina, Kelly Ireland, Yan--nis Papakonstantinou, Jeffrey Ullman, and Jennifer Widom. </author> <title> Information translation, mediation, and mosaic-based browsing in the tsimmis system. </title> <booktitle> In Proceedings of the ACM SIGMOD Internation Conference on Management of Data, </booktitle> <address> San Jose, CA, </address> <year> 1995. </year>
Reference: [ Haussler, 1988 ] <author> David Haussler. </author> <title> Quantifying inductive bias: AI learning algorithms and Valiant's learning framework. </title> <journal> Artificial Intelligence, </journal> <volume> 36 </volume> <pages> 177-221, </pages> <year> 1988. </year>
Reference: [ Helmbold and Long, 1994 ] <author> David P. Helmbold and Philip M. </author> <title> Long. Tracking drifting concepts by minimizing disagreement. </title> <journal> Machine Learning, </journal> <volume> 14 </volume> <pages> 27-45, </pages> <year> 1994. </year>
Reference-contexts: Both views do not capture all aspects of uncertainty but since database transactions are indeed deterministic, our assumption is more appropriate for database applications. The research work described in [ Widmer and Kubat, 1993 ] and <ref> [ Helmbold and Long, 1994 ] </ref> attempts to solve the problem of learning drifting concepts from dynamic environments where a target concept may gradually change over time. Their solution is to incrementally modify a learned concept description to minimize its disagreement with most recently observed examples. <p> ] and <ref> [ Helmbold and Long, 1994 ] </ref> attempts to solve the problem of learning drifting concepts from dynamic environments where a target concept may gradually change over time. Their solution is to incrementally modify a learned concept description to minimize its disagreement with most recently observed examples. In particular, [ Helmbold and Long, 1994 ] shows that the error rate of a learned description is guaranteed to be smaller than a constant if the number of recent examples that are taken into account is larger than a polynomial of the error rate.
Reference: [ Howson and Urbach, 1988 ] <author> Colin Howson and Peter Urbach. </author> <title> Scientific Reasoning: The Bayesian Approach. </title> <publisher> Open Court, </publisher> <year> 1988. </year>
Reference-contexts: The probability that the outcome of the next experiment will be C can be estimated as r + 1 . A detailed description and a proof of the Laplace law can be found in <ref> [ Howson and Urbach, 1988 ] </ref> . The Laplace law applies to any repeatable experiments (e.g., tossing a coin). The Laplace law is a special case of a modified estimate called m-Probability [ Cestnik and Bratko, 1991 ] .
Reference: [ Hsu and Knoblock, 1993a ] <author> Chun-Nan Hsu and Craig A. Knoblock. </author> <title> Learning database abstractions for query reformulation. </title> <editor> In Gregory Piatetsky-Shapiro, editor, </editor> <booktitle> Proceedings of KDD-93, AAAI-93 Workshop on Knowledge Discovery in Databases, </booktitle> <address> Washington, D.C., </address> <year> 1993. </year>
Reference-contexts: In our problem, however, the learning algorithm is also required to induce a low-cost description, that is, a low-cost alternative query that can be evaluated by the DBMS efficiently. Previously, we have developed an inductive learning algorithm that learns low-cost queries from single-table 34 databases <ref> [ Hsu and Knoblock, 1993a ] </ref> . Section 3.2 will describe in detail a more advanced algorithm that learns conjunctive Datalog queries from relational databases. This algorithm allows the learner to learn relational rules, a very important class of rules for detecting redundant relational joins in the optimization.
Reference: [ Hsu and Knoblock, 1993b ] <author> Chun-Nan Hsu and Craig A. Knoblock. </author> <title> Reformulating query plans for multidatabase systems. </title> <booktitle> In Proceedings of the Second International Conference on Information and Knowledge Management(CIKM-93), </booktitle> <address> Washington, D.C., </address> <year> 1993. </year>
Reference-contexts: The query plan optimization approach presented here is elaborated from our prototype approach described previously in <ref> [ Hsu and Knoblock, 1993b ] </ref> .
Reference: [ Hsu and Knoblock, 1994 ] <author> Chun-Nan Hsu and Craig A. Knoblock. </author> <title> Rule induction for semantic query optimization. </title> <booktitle> In Machine Learning, Proceedings of the 11th International Conference(ML-94), </booktitle> <address> San Mateo, CA, 1994. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Another important feature of this inductive learning component is that it can form an alternative query with relational joins from relational data <ref> [ Hsu and Knoblock, 1994, Hsu and Knoblock, 1996b ] </ref> . This feature allows the learner to generate relational rules for the optimizer to detect redundant relational joins. The semantic rules derived from the alternative query may not be robust and could be overly-specific to the given query.
Reference: [ Hsu and Knoblock, 1995 ] <author> Chun-Nan Hsu and Craig A. Knoblock. </author> <title> Estimating the robustness of discovered knowledge. </title> <booktitle> In Proceedings of the First International Conference on Knowledge Discovery and Data Mining(KDD-95), </booktitle> <address> Menlo Park, CA, 1995. </address> <publisher> AAAI Press. </publisher>
Reference-contexts: The goal of this research is to develop a general learning approach that maximizes the net utility of semantic rules throughout the life span of a information system. To achieve this goal, I have developed an approach to the estimation of the robustness of semantic rules <ref> [ Hsu and Knoblock, 1995 ] </ref> . This estimation approach allows the learner to 8 determine the degree of the robustness of a semantic rule and use the results to guide the search for robust rules. This estimation approach will be described in detail in Chapter 2.
Reference: [ Hsu and Knoblock, 1996a ] <author> Chun-Nan Hsu and Craig A. Knoblock. </author> <title> Discovering robust knowledge from dynamic closed-world data. </title> <booktitle> In Proceedings of the Thirteenth National Conference on Artificial Intelligence (AAAI-96), </booktitle> <address> Portland, Oregon, 1996. </address> <publisher> AAAI Press. </publisher>
Reference-contexts: This feature allows the learner to generate relational rules for the optimizer to detect redundant relational joins. The semantic rules derived from the alternative query may not be robust and could be overly-specific to the given query. To address this problem, the learner uses a rule pruner <ref> [ Hsu and Knoblock, 1996a ] </ref> to prune the literals in the derived rules to increase their applicability to a wide range of queries.
Reference: [ Hsu and Knoblock, 1996b ] <author> Chun-Nan Hsu and Craig A. Knoblock. </author> <title> Using inductive learning to generate rules for semantic query optimization. </title> <editor> In Usama M. Fayyad, Gregory Piatetsky-Shapiro, Padhraic Smyth, and Ramasamy Uthurusamy, editors, </editor> <booktitle> Advances in Knowledge Discovery and Data Mining, chapter 17. </booktitle> <publisher> AAAI Press/MIT Press, </publisher> <year> 1996. </year> <month> 133 </month>
Reference-contexts: Another important feature of this inductive learning component is that it can form an alternative query with relational joins from relational data <ref> [ Hsu and Knoblock, 1994, Hsu and Knoblock, 1996b ] </ref> . This feature allows the learner to generate relational rules for the optimizer to detect redundant relational joins. The semantic rules derived from the alternative query may not be robust and could be overly-specific to the given query.
Reference: [ Jarke and Koch, 1984 ] <author> Matthias Jarke and Jurgen Koch. </author> <title> Query optimization in database systems. </title> <journal> ACM Computer Surveys, </journal> <volume> 16 </volume> <pages> 111-152, </pages> <year> 1984. </year>
Reference: [ Kaelbling, 1993 ] <author> Leslie. P. Kaelbling. </author> <title> Hierarchical learning in stochastic domains: Preliminary results. </title> <booktitle> In Proceedings of the Tenth International Conference on Machine Learning (ML-93), </booktitle> <pages> pages 167-173, </pages> <address> Amherst, MA, </address> <year> 1993. </year>
Reference: [ King, 1981 ] <author> Jonathan J. King. </author> <title> Query Optimization by Semantic Reasoning. </title> <type> PhD thesis, </type> <institution> Stanford University, Department of Computer Science, </institution> <year> 1981. </year>
Reference-contexts: Executing the optimized query is less expensive than executing the original query because the system saves the time for the redundant comparisons. Though semantic query optimization is an effective and promising technique, it requires sufficient semantic knowledge to yield high cost reduction. Previous work in SQO, such as <ref> [ King, 1981 ] </ref> , assume that the optimizer can use semantic integrity constraints given by users for the optimization. Semantic integrity constraints express rules that must be followed by the data in any state of a database. <p> Some of the reformulated equivalent queries of Q1 are shown in Table 1.1. Q1.1 is reformulated from Q1 by applying R1.1. In this reformulation, the constraint on the status ``Active'' is deleted. This is an example of constraint elimination <ref> [ King, 1981 ] </ref> reformulation. From Q1.1 and R1.2, we can infer that the literal on a database relation ship is implied by other literals in Q1.1 and thus is redundant. Therefore, it can be deleted and the resulting query Q1.2 is still equivalent to Q1. <p> From Q1.1 and R1.2, we can infer that the literal on a database relation ship is implied by other literals in Q1.1 and thus is redundant. Therefore, it can be deleted and the resulting query Q1.2 is still equivalent to Q1. This is an example of join elimination <ref> [ King, 1981 ] </ref> reformulation where the relational join from ship class to ship is eliminated. In addition to deletions, we may also add new constraints to a query based on the semantic rules. <p> Siegel's system learns simple rules, a limited form of rules that allows exactly one literal on each side of implication. His system uses a set of predefined heuristics combined with example queries to guide the learning for effective rules. The heuristics are identical to those proposed by <ref> [ King, 1981 ] </ref> to guide semantic query optimization.
Reference: [ Knoblock et al., 1994 ] <author> Craig A. Knoblock, Yigal Arens, and Chun-Nan Hsu. </author> <title> Cooperating agents for information retrieval. </title> <booktitle> In Proceedings of the Second International Conference on Intelligent and Cooperative Information Systems, </booktitle> <address> Toronto, Ontario, Canada, </address> <year> 1994. </year>
Reference: [ Laird and Rosenbloom, 1990 ] <author> John E. Laird and Paul S. Rosenbloom. </author> <title> Integrating execution, planning, and learning in SOAR for external envirnoments. </title> <booktitle> In Proceedings of the Eighth National Conference on Artificial Intelligence(AAAI-90), </booktitle> <pages> pages 1022-1029, </pages> <address> Boston, MA, </address> <year> 1990. </year>
Reference: [ Lavrac and Dzeroski, 1994 ] <author> Nada Lavrac and Saso Dzeroski. </author> <title> Inductive Logic Programming: Techniques and Applications. </title> <publisher> Ellis Horwood, </publisher> <year> 1994. </year>
Reference-contexts: The Laplace law is a special case of the m-probability estimate with Pr (C) = 1=k, and m = k. The prior probability used here is that k outcomes are equally probable. The m-probability estimate has been used in many machine learning systems for different purposes <ref> [ Cestnik and Bratko, 1991, Lavrac and Dzeroski, 1994 ] </ref> . Convincing results in handling noisy data and pruning decision trees have been achieved. The advantage of the Laplace estimate is that it takes both known relative frequency and prior probability into account.
Reference: [ Levy et al., 1994 ] <author> Alon Y. Levy, Inderpal Singh Mumick, and Yehoshua Sagiv. </author> <title> Query optimization by predicate move-around. </title> <booktitle> In Proceedings of the 20th VLDB Conference, </booktitle> <address> Santiago, Chile, </address> <year> 1994. </year>
Reference-contexts: We can also establish that moving literals backward will not change the semantics of a query access plan. The proof is similar to the proof for the correctness of the predicate push-down techniques <ref> [ Ullman, 1988, Levy et al., 1994 ] </ref> . Consequently, the resulting query plan of Algorithm 4.2 will return the same answer as an input query plan, as long as the given database state is consistent with the semantic knowledge. <p> Optimizing a query plan by moving literals is previously studied in the work on predicate push-down [ Ullman, 1988 ] , and predicate move-around <ref> [ Levy et al., 1994 ] </ref> . Predicate push-down is a commonly used query optimization technique. By pushing data selection predicates down the hierarchical access graph of a query, predicate push-down allows the selections to be applied as early as possible during query execution.
Reference: [ Levy et al., 1995 ] <author> Alon Y. Levy, Divesh Srivastava, and Thomas Kirk. </author> <title> Data model and query evaluation in global information systems. </title> <journal> Journal of Intelligent Information Systems, </journal> <note> Special Issue on Networked Information Discovery and Retrieval, 5(2), </note> <year> 1995. </year>
Reference: [ Levy et al., 1996 ] <author> Alon Y. Levy, Anand Rajaraman, and Joann J. Ordille. </author> <title> Query-answering algorithms for information agents. </title> <booktitle> In Proceedings of the Thirteenth National Conference on Artificial Intelligence(AAAI-96), </booktitle> <address> Portland, OR, </address> <year> 1996. </year>
Reference-contexts: The alternative view would be used repeatedly, so even a small improvement could provide significant savings. Similarly, we can apply the inductive learning algorithm to learn less expensive definitions of information sources <ref> [ Levy et al., 1996 ] </ref> , or less expensive concept definitions in an integrated domain model in an information mediator [ Arens et al., 1993, Knoblock et al., 1994, Arens et al., 1996 ] . In both cases, definitions are expressed as queries.
Reference: [ Lloyd, 1987 ] <author> John W. Lloyd. </author> <title> Foundations of Logic Programming. </title> <publisher> Springer-Verlag, </publisher> <address> Berlin, Germany, </address> <year> 1987. </year>
Reference-contexts: Proposing reformulations involves locating applicable semantic rules from the set of given semantic knowledge. A rule is considered applicable to a query if the antecedent part of the rule is a logical consequence of the query literals. This can be determined by applying SLD-refutation <ref> [ Lloyd, 1987 ] </ref> to a target rule and the query. The applicable semantic knowledge for this query is given in Table 1.2. <p> In contrast, database changes in our learning problem includes updates or deletions. Under the closed-world assumption <ref> [ Lloyd, 1987 ] </ref> , they have the same semantic effect as to assert that the old data are now false and cause the learned rules to become inconsistent. Chapter 2 will compare the difference between robustness and predictive accuracy in detail. <p> When a transaction changes the data in a database, the system can identify an inconsistent rule by check whether the transaction implies an invalidating transaction of a rule. We note that a similar approach to identifying inconsistent rules has been proposed and studied substantially for deductive databases <ref> [ Lloyd, 1987 ] </ref> . The difference is that previous work requires computing the information that allows the system to identify affected rules each time the system performs a transaction. In our case, the information for robustness estimation is ready and can be reused directly for inconsistent rule identification. <p> A.3 Semantic Knowledge Our approach uses two forms of semantic knowledge: semantic rules and range facts. Semantic rules, represented in terms of Horn-clause rules, express the regularity of data in an individual database. We adopt standard Prolog terminology and semantics as defined in <ref> [ Lloyd, 1987 ] </ref> in our discussion of rules. Semantic knowledge is interpreted under the closed-world assumption. That is, a database literal is true with regard to a database if and only if in the database there exists an instance that satisfies the literal. <p> Generally speaking, a rule is applicable to a query if the antecedent part of the rule is a logical consequence of the literals of Q. This can be determined by applying SLD-refutation <ref> [ Lloyd, 1987 ] </ref> to a target rule and query, where the goal comprises the antecedents of R and the body of the query corresponds to the program clauses. Unlike general theorem proving, the program clauses only contains query literals.
Reference: [ MacGregor, 1990 ] <author> Robert MacGregor. </author> <title> The evolving technology of classification-based knowledge representation systems. </title> <editor> In John Sowa, editor, </editor> <booktitle> Principles of Semantic Networks: Explorations in the Representation of Knowledge. </booktitle> <publisher> Morgan Kaufmann, </publisher> <year> 1990. </year>
Reference-contexts: Figure 5.1 shows the organization of sims with the query plan optimizer and the learner. sims takes as input a query expressed in the loom knowledge representation language <ref> [ MacGregor, 1990 ] </ref> , which is also used as the representation language to build an integrated model of databases. <p> To distinguish a rule from a query, we show queries using Datalog syntax and semantic rules in a standard logic notation. Table A.3 shows some example semantic rules. 1 In our implementation, however, queries are expressed in the loom knowledge representation language <ref> [ MacGregor, 1990 ] </ref> . It is also used as the representation language to build an integrated model of databases. <p> The queries and rules are expressed in the loom knowledge representation language <ref> [ MacGregor, 1990 ] </ref> .
Reference: [ Mannila et al., 1994 ] <author> Heikki Mannila, Hannu Toivonen, and A. Inkeri Verkamo. </author> <title> Efficient algorithms for discovering association rules. </title> <editor> In Usama M. Fayyad and 134 Ramasamy Uthurusamy, editors, </editor> <booktitle> Proceedings of KDD-94, AAAI-94 Workshop on Knowledge Discovery in Databases, </booktitle> <address> Seattle, WA, </address> <year> 1994. </year>
Reference: [ Michalski, 1983 ] <author> Ryszard S. Michalski. </author> <title> A theory and methodology of inductive learning. </title> <booktitle> In Machine Learning: An Artificial Intelligence Approach, </booktitle> <volume> volume I, </volume> <pages> pages 83-134. </pages> <publisher> Morgan Kaufmann Publishers, Inc., </publisher> <address> Los Altos, CA, </address> <year> 1983. </year>
Reference: [ Minton et al., 1989 ] <author> Steven Minton, Jaime G. Carbonell, Craig A. Knoblock, Daniel R. Kuokka, Oren Etzioni, and Yolanda Gil. </author> <title> Explanation-based learning: A problem solving perspective. </title> <journal> Artificial Intelligence, </journal> <volume> 40(1-3):63-118, </volume> <year> 1989. </year>
Reference: [ Minton, 1988 ] <author> Steven Minton. </author> <title> Learning Effective Search Control Knowledge: An Explanation-Based Approach. </title> <type> PhD thesis, </type> <institution> Carnegie Mellon University, School of Computer Science, </institution> <year> 1988. </year>
Reference-contexts: The rationale is that rule construction algorithms tend to generate overly-specific rules, but taking the length and robustness of rules into account in rule construction could be too expensive. This is because the search space of rule construction is already huge and evaluating robustness is not trivial. prodigy-ebl <ref> [ Minton, 1988 ] </ref> , a speedup learning approach for problem solving, includes a compressor to prune learned rules. <p> This example shows the utility of the rule pruning with the robustness estimation. 54 3.5 Utility Problem and Rule Maintenance This section discusses the utility problem that may arise when we consider the collective effect of learned semantic rules in query optimization. The utility problem, originally identified in <ref> [ Minton, 1988 ] </ref> , refers to situations when learning degrades the performance of a system. <p> The utility problem may arise in a rule-based system because the utility of a rule is determined not only by its saving and applicability, but also by the cost for the system to locate it. This observation leads to the following formula of utility: Utility of Search Control Rules <ref> [ Minton, 1988 ] </ref> U tility = Average Saving fi Application F requency M atch Cost (3:1) According to this formula, we need to decrease the match cost to increase the utility of a rule, otherwise, the utility problem may arise. <p> Low utility rules are those rarely applicable or yield small saving. A simple approach to removing this class of rules is to monitor the saving yielded by rules and remove those with small saving <ref> [ Minton, 1988 ] </ref> . The utility problem in SQO can be alleviated by an alert query optimizer that can give up its attempt to match more rules and send the original query to the database system.
Reference: [ Mitchell et al., 1983 ] <author> Tom M. Mitchell, Paul E. Utgoff, and Ranan Banerji. </author> <title> Learning problem solving heuristics by experimentation. </title> <booktitle> In Machine Learning: An Artificial Intelligence Approach, </booktitle> <pages> pages 163-190. </pages> <publisher> Morgan Kaufmann, </publisher> <address> Palo Alto, CA, </address> <year> 1983. </year>
Reference-contexts: This way, the system will gradually collect sufficient rules for query optimization. This model of learning is similar to that in lex <ref> [ Mitchell et al., 1983 ] </ref> , which uses example problems to trigger the learning of heuristics for solving mathematical integration. pruning component. An expensive query is given to the learner as the training query to trigger the learning.
Reference: [ Mitchell et al., 1986 ] <author> Tom. M. Mitchell, Richard. M. Keller, and Smadar T. Kedar-Cabelli. </author> <title> Explanation-based generalization: A unifying view. </title> <journal> Machine Learning, </journal> <volume> 1(1) </volume> <pages> 47-80, </pages> <year> 1986. </year>
Reference: [ Muggleton and Feng, 1990 ] <author> Stephen Muggleton and C. Feng. </author> <title> Efficient induction of logic programs. </title> <booktitle> In Proceedings of the first conference on Algorithmic Learning Theory, </booktitle> <address> Tokyo, Japan, 1990. </address> <publisher> Morgan Kaufmann. </publisher>
Reference: [ Parr and Russell, 1995 ] <author> Ronald Parr and Stuart J. Russell. </author> <title> Approximating optimal policies for partially observable stochastic domains. </title> <booktitle> In Proceedings of the 14th International Joint Conference on Artificial Intelligence(IJCAI-93), </booktitle> <address> Mon-treal, Canada, </address> <year> 1995. </year>
Reference: [ Pawlak, 1991 ] <author> Zdzislaw Pawlak. </author> <title> Rough Sets: Theoretical aspects of Reasoning about Data. </title> <publisher> Kluwer, </publisher> <address> Boston, MA, </address> <year> 1991. </year>
Reference-contexts: In addition to predictive accuracy, [ Clark and Niblett, 1989 ] proposed using significance in rule induction to measure the correlation between the antecedents and consequent of a rule by computing the likelihood ratio of the data coverage of a rule. Rough set theory <ref> [ Pawlak, 1991 ] </ref> has been applied in many knowledge discovery applications [ Ziarko, 1995 ] and is useful for measuring whether a given set of attributes is sufficient to represent a target concept.
Reference: [ Pazzani and Kibler, 1992 ] <author> Micheal J. Pazzani and Dennis Kibler. </author> <title> The utility of knowledge in inductive learning. </title> <journal> Machine Learning, </journal> <volume> 9 </volume> <pages> 57-94, </pages> <year> 1992. </year>
Reference: [ Piatetsky-Shapiro and Frawley, 1991 ] <author> Gregory Piatetsky-Shapiro and William J. Frawley. </author> <title> Knowledge Discovery in Databases. </title> <publisher> MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1991. </year>
Reference-contexts: The results confirm our assumption about effectiveness and robustness, and show that the learned rules are both effective and robust. Chapter 6 reviews the contributions and describes some future work. 14 Chapter 2 Robustness of Knowledge Many applications of machine learning and data mining <ref> [ Piatetsky-Shapiro and Frawley, 1991, Fayyad et al., 1996 ] </ref> such as learning for semantic query optimization require the knowledge to be consistent with data. However, databases usually change over time and make machine-learned knowledge inconsistent with data.
Reference: [ Piatetsky-Shapiro, 1984 ] <author> Gregory Piatetsky-Shapiro. </author> <title> A Self-Organizing Database System A Different Approach To Query Optimization. </title> <type> PhD thesis, </type> <institution> Department of Computer Science, </institution> <address> New York University, </address> <year> 1984. </year> <month> 135 </month>
Reference-contexts: The number of tuples that satisfy a literal can be retrieved from the database. If this is too expensive for large databases, we can use the estimation approaches used for conventional query optimization <ref> [ Piatetsky-Shapiro, 1984, Ullman, 1988 ] </ref> to estimate this number. * The value of latitude is updated, given that the tuple that is updated is a tuple of geoloc with its ?country ="Malta": Pr (x 4 jx 2 ^ x 1 ) = t u;geoloc;latitude + 1 t u;geoloc + A
Reference: [ Quinlan, 1990 ] <author> J. Ross Quinlan. </author> <title> Learning logical definitions from relations. </title> <journal> Ma--chine Learning, </journal> <volume> 5 </volume> <pages> 239-266, </pages> <year> 1990. </year>
Reference-contexts: Both problems learn definitions from databases with multiple relations. Our inductive learning approach uses a top-down algorithm similar to foil <ref> [ Quinlan, 1990 ] </ref> to build an alternative query. A difference between our approach and foil is that they learn descriptions in a different language. foil learns Horn-clause definitions where each clause covers a subset of positive instances but no negative instances.
Reference: [ Quinlan, 1993 ] <author> J. Ross Quinlan. C4.5: </author> <title> Programs for Machine Learning. </title> <publisher> Morgan Kaufmann, </publisher> <year> 1993. </year>
Reference: [ Raedt and Bruynooghe, 1993 ] <author> Luc De Raedt and Maurice Bruynooghe. </author> <title> A theory of clausal discovery. </title> <booktitle> In Proceedings of the 13th International Joint Conference on Artificial Intelligence(IJCAI-93), </booktitle> <address> Chambery, France, </address> <year> 1993. </year>
Reference: [ Ramsay, 1988 ] <author> Allan Ramsay. </author> <booktitle> Formal Methods in Artificial Intelligence. </booktitle> <publisher> Cam-bridge University Press, </publisher> <address> Cambridge, U.K., </address> <year> 1988. </year>
Reference-contexts: The robustness of r in accessible states from the current state d is defined as Robust (rjd) = Pr (:tjd) = 1 Pr (tjd). This definition of robustness is analogous in spirit to the notion of accessibility and the possible worlds semantics in modal logic <ref> [ Ramsay, 1988 ] </ref> . Definition 2.2 retains our intuitive notion of robustness, but allows us to estimate robustness without 18 counting the intractably large number of possible database states. <p> Our emphasis on transactions in our definition of robustness is analogous in spirit to the notion of accessibility in the possible worlds semantics of modal logic <ref> [ Ramsay, 1988 ] </ref> .
Reference: [ Rosenbloom and Laird, 1986 ] <author> Paul S. Rosenbloom and John E. Laird. </author> <title> Mapping explanation-based generalization onto SOAR. </title> <booktitle> In Proceedings of the Fifth National Conference on Artificial Intelligence(AAAI-86), </booktitle> <address> Philadelphia, PA, </address> <year> 1986. </year>
Reference: [ Russell, 1986 ] <author> Stuart J. Russell. </author> <title> Preliminary steps toward the automation of induction. </title> <booktitle> In Proceedings of the Fifth National Conference on Artificial Intelligence(AAAI-86), </booktitle> <address> Philadelphia, PA, </address> <year> 1986. </year>
Reference: [ Russell, 1989 ] <author> Stuart J. Russell. </author> <title> The Use of Knowledge in Analogy and Induction. </title> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, CA, </address> <year> 1989. </year>
Reference-contexts: This is very likely, and statement 2 is thus not robust. Therefore, we should induce statement 1. Certainly, to draw this conclusion we will need to accumulate enough knowledge about actions that change the color of an emerald. 98 Previously, <ref> [ Russell, 1989 ] </ref> provided a survey of solutions to the problem and proposes a solution of his own using determinations. He argues that since there is a determination between the type of an object and its color, we should prefer statement 1.
Reference: [ Shafer and Pearl, 1990 ] <author> Glenn Shafer and Judea Pearl. </author> <title> Readings in Uncertain Reasoning. </title> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, CA, </address> <year> 1990. </year>
Reference-contexts: Reasoning about the consistency of beliefs and knowledge after changes to closed-world relational data is an important research subject in nonmonotonic and uncertain reasoning <ref> [ Ginsberg, 1987, Shafer and Pearl, 1990 ] </ref> . Our emphasis on transactions in our definition of robustness is analogous in spirit to the notion of accessibility in the possible worlds semantics of modal logic [ Ramsay, 1988 ] .
Reference: [ Shavlik and Dietterich, 1990 ] <author> Jude Shavlik and Thomas A. Dietterich. </author> <booktitle> Readings in Machine Learning. </booktitle> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, CA, </address> <year> 1990. </year>
Reference-contexts: Given a set of data instances classified as positive or negative, the problem of inducing a description that covers all positive data instances but no negatives is known as supervised inductive learning in machine learning <ref> [ Shavlik and Dietterich, 1990 ] </ref> . Since a query is a description of the data to be retrieved, inductive learning algorithms that learn descriptions expressed in the query language can potentially be used in our approach.
Reference: [ Shekhar et al., 1988 ] <author> Shashi Shekhar, Jaideep Srivastava, and Soumitra Dutta. </author> <title> A formal model of trade-off between optimization and execution costs in semantic query optimization. </title> <booktitle> In Proceedings of the 14th VLDB Conference, </booktitle> <address> Los Angeles, CA, </address> <year> 1988. </year>
Reference-contexts: The utility problem in SQO can be alleviated by an alert query optimizer that can give up its attempt to match more rules and send the original query to the database system. This strategy is adopted in <ref> [ Shekhar et al., 1988 ] </ref> so that the query execution cost is always at most as high as or slightly higher than the cost without optimization. Therefore, even in the worst case an SQO optimizer will not slow down the query execution significantly.
Reference: [ Shekhar et al., 1993 ] <author> Shashi Shekhar, Babak Hamidzadeh, Ashim Kohli, and Mark Coyle. </author> <title> Learning transformation rules for semantic query optimization: A data-driven approach. </title> <journal> IEEE Transactions on Knowledge and Data Engineering, </journal> <volume> 5(6) </volume> <pages> 950-964, </pages> <year> 1993. </year>
Reference-contexts: This way, the learner will be able to learn effective and robust rules for semantic query optimization. 1.4 Closely Related Work Previously, three approaches to automating the knowledge acquisition for semantic query optimization were proposed in [ Siegel, 1988 ] , <ref> [ Shekhar et al., 1993 ] </ref> and [ Yu and Sun, 1989 ] . The first approach is a query-driven approach due to [ Siegel, 1988, Siegel, 1989 ] . Siegel's system learns simple rules, a limited form of rules that allows exactly one literal on each side of implication. <p> The fundamental limitation of this approach is that the heuristics depend only on the query specification but do not take properties of data into account, and thus may miss learning many high utility rules. Shekhar et al. develop a data-driven approach to the learning problem of semantic query optimization <ref> [ Shekhar et al., 1993 ] </ref> . Their system is based on an assumption that useful semantic rules can be derived from the nonuniform distributions of attribute values. <p> Their system is based on an assumption that useful semantic rules can be derived from the nonuniform distributions of attribute values. To detect nonuniform data distributions, their system constructs a set of data distribution grids, such as the one appears in Table 1.3, adopted from <ref> [ Shekhar et al., 1993 ] </ref> . The numbers in the cells of a grid are the tuples satisfying the conditions specified on the coordinates.
Reference: [ Shen, 1989 ] <author> Wei-Min Shen. </author> <title> Learning from the Environment Based on Percepts and Actions. </title> <type> PhD thesis, </type> <institution> Carnegie Mellon University, </institution> <address> Pittsburgh, PA, </address> <year> 1989. </year>
Reference: [ Shenoy and Ozsoyoglu, 1989 ] <author> Sreekumar T. Shenoy and Zehra M. Ozsoyoglu. </author> <title> Design and implementation of a semantic query optimizer. </title> <journal> IEEE Trans. Knowledge and Data Engineering, </journal> <volume> I(3):344-361, </volume> <year> 1989. </year> <month> 136 </month>
Reference: [ Siegel, 1988 ] <author> Michael D. Siegel. </author> <title> Automatic rule derivation for semantic query op-timization. </title> <editor> In Larry Kerschberg, editor, </editor> <booktitle> Proceedings of the Second International Conference on Expert Database Systems, </booktitle> <pages> pages 371-385. </pages> <institution> George Mason Foundation, Fairfax, VA, </institution> <year> 1988. </year>
Reference-contexts: This way, the learner will be able to learn effective and robust rules for semantic query optimization. 1.4 Closely Related Work Previously, three approaches to automating the knowledge acquisition for semantic query optimization were proposed in <ref> [ Siegel, 1988 ] </ref> , [ Shekhar et al., 1993 ] and [ Yu and Sun, 1989 ] . The first approach is a query-driven approach due to [ Siegel, 1988, Siegel, 1989 ] . <p> The first approach is a query-driven approach due to <ref> [ Siegel, 1988, Siegel, 1989 ] </ref> . Siegel's system learns simple rules, a limited form of rules that allows exactly one literal on each side of implication. His system uses a set of predefined heuristics combined with example queries to guide the learning for effective rules.
Reference: [ Siegel, 1989 ] <author> Michael D. Siegel. </author> <title> Automatic Rule Derivation for Semantic Query Optimization. </title> <type> PhD thesis, </type> <institution> Department of Computer Science, Boston University, </institution> <year> 1989. </year>
Reference-contexts: The first approach is a query-driven approach due to <ref> [ Siegel, 1988, Siegel, 1989 ] </ref> . Siegel's system learns simple rules, a limited form of rules that allows exactly one literal on each side of implication. His system uses a set of predefined heuristics combined with example queries to guide the learning for effective rules.
Reference: [ Silberschatz et al., 1996 ] <author> Avi Silberschatz, Mike Stonebraker, and Jeff Ullman. </author> <title> Database research: Achievements and opportunities into the 21st century. </title> <booktitle> SIG-MOD Record, </booktitle> <month> March </month> <year> 1996. </year>
Reference-contexts: An effective query optimizer can automatically search for efficient procedures to retrieve data, which allows us to query an information system without the need to understand its internal mechanism. Query optimization is increasingly important in the 1990s and, according to a report on the trends of database research <ref> [ Silberschatz et al., 1996 ] </ref> , it will be even more important in the twenty-first century due to the increasing complexity of queries and the heterogeneity of information sources. However, it is difficult for conventional query optimization techniques to solve all the problems for the next generation information systems.
Reference: [ Sun and Yu, 1994 ] <author> Wei Sun and Clement T. Yu. </author> <title> Semantic query optimization for tree and chain queries. </title> <journal> IEEE Trans. Knowledge and Data Engineering, </journal> <volume> 6(1) </volume> <pages> 136-151, </pages> <year> 1994. </year>
Reference-contexts: That is, the join over the relations ship class and ship on the variable ?class is redundant and can be eliminated 1 . The optimizer also derives a new 1 See <ref> [ Sun and Yu, 1994 ] </ref> for a detailed discussion on detecting redundant relational joins in semantic query optimization. 62 Q4: assets (?ship class,?draft):- ship class (?ship class, ,?draft, ,?container), ship ( ,?ship class,?status, , ), ?status = "Active", ?container = "Y", ?draft &lt; 50. stage 1: Q4.1: assets (?ship class,?draft):- <p> The optimizer then searches for a subset of the literals in the implication closure to retain and deletes the rest so that the resulting query is the least expensive (line 10). Since it could be difficult to identify the optimal combination in all cases (an NP-complete problem <ref> [ Sun and Yu, 1994 ] </ref> ), the optimizer needs to apply heuristics to guide the search. Usually, the heuristics are derived from a cost model of query execution.
Reference: [ Sutton, 1990 ] <author> Richard S. Sutton. </author> <title> Integrated architectures for learning, planning, and reacting based on approximating dynamic programming. </title> <booktitle> In Proceedings of the Seventh International Conference on Machine Learning (ML-90), </booktitle> <pages> pages 216-224, </pages> <address> Austin, TX, </address> <year> 1990. </year>
Reference: [ Tambe and Rosenbloom, 1993 ] <author> Milind Tambe and Paul Rosenbloom. </author> <title> On masking effect. </title> <booktitle> In Proceedings of the Eleventh National Conference on Artificial Intelligence(AAAI-93), </booktitle> <pages> pages 526-533, </pages> <address> Washington, D.C., </address> <year> 1993. </year>
Reference-contexts: Roughly speaking, the match cost may increase either because there are too many rules (average growth 55 effect <ref> [ Tambe and Rosenbloom, 1993 ] </ref> ) or because individual rules are too complex to be matched efficiently. Many approaches have been developed to decrease the match cost and address the utility problem. They can be classified into two general approaches.
Reference: [ Tambe, 1991 ] <author> Milind Tambe. </author> <title> Eliminating combinatorics from production match. </title> <type> PhD thesis, </type> <institution> Carnegie Mellon University, School of Computer Science, </institution> <year> 1991. </year>
Reference-contexts: They can be classified into two general approaches. One approach is to organize rules so that an applicable rule can be efficiently matched. For example, the rete algorithm [ Forgy, 1982 ] and its more advanced descendants <ref> [ Tambe, 1991, Doorenbos et al., 1992 ] </ref> are in this category. The other approach is to use a rule maintainer to remove redundant and low utility rules so as to decrease the number of rules.
Reference: [ Ullman, 1988 ] <author> Jeffrey D. Ullman. </author> <title> Principles of Database and Knowledge-base Systems, volume I,II. </title> <publisher> Computer Science Press, </publisher> <address> Palo Alto, CA, </address> <year> 1988. </year>
Reference-contexts: To detect redundant joins, they use referential integrity constraints <ref> [ Ullman, 1988 ] </ref> , a restrictive form of relational rules that allow only one literal as the antecedent. One of the new features of the SQO approach presented in Chapter 4 is that it can apply relational rules. <p> The number of tuples that satisfy a literal can be retrieved from the database. If this is too expensive for large databases, we can use the estimation approaches used for conventional query optimization <ref> [ Piatetsky-Shapiro, 1984, Ullman, 1988 ] </ref> to estimate this number. * The value of latitude is updated, given that the tuple that is updated is a tuple of geoloc with its ?country ="Malta": Pr (x 4 jx 2 ^ x 1 ) = t u;geoloc;latitude + 1 t u;geoloc + A <p> To estimate the actual cost of a constraint is expensive. We therefore use an approximation here. The execution cost of individual constraints can be estimated using standard query size estimation techniques <ref> [ Ullman, 1988 ] </ref> . A set of simple estimates is shown in Table 3.4. For an internal disjunction on a non-indexed attribute of a relation D, the query execution system has to scan the entire relation to find all satisfying instances. <p> If the join is over indexed attributes, the execution cost is proportional to the number of instance pairs returned from the join, jD 1 jjD 2 j max (I 1 ;I 2 ) <ref> [ Ullman, 1988 ] </ref> . This estimate assumes that distinct attribute values distribute uniformly in instances of joined relations. If possible, the learner may sample the database for more accurate estimation. For the example at hand, two candidate constraints are the most promising: C3:geoloc (?name, ,"Malta", , ). <p> We can also establish that moving literals backward will not change the semantics of a query access plan. The proof is similar to the proof for the correctness of the predicate push-down techniques <ref> [ Ullman, 1988, Levy et al., 1994 ] </ref> . Consequently, the resulting query plan of Algorithm 4.2 will return the same answer as an input query plan, as long as the given database state is consistent with the semantic knowledge. <p> As a result, our approach can optimize complex queries that includes complex data operations, as long as a complex query is decomposed into a query plan of conjunctive subqueries. Optimizing a query plan by moving literals is previously studied in the work on predicate push-down <ref> [ Ullman, 1988 ] </ref> , and predicate move-around [ Levy et al., 1994 ] . Predicate push-down is a commonly used query optimization technique. <p> Learning View Definitions A view in a relational database <ref> [ Ullman, 1988 ] </ref> is a virtual relation that is not present physically in the database. Data tuples of a view will be extracted from physical relations during query evaluation time.
Reference: [ Widmer and Kubat, 1993 ] <author> Gerhard Widmer and Miroslav Kubat. </author> <title> Effective learning in dynamic environments by explicit context tracking. </title> <booktitle> In Machine Learning: </booktitle> <address> ECML-93, Berlin, Germany, 1993. </address> <publisher> Springer-Verlag. </publisher>
Reference-contexts: Both views do not capture all aspects of uncertainty but since database transactions are indeed deterministic, our assumption is more appropriate for database applications. The research work described in <ref> [ Widmer and Kubat, 1993 ] </ref> and [ Helmbold and Long, 1994 ] attempts to solve the problem of learning drifting concepts from dynamic environments where a target concept may gradually change over time.
Reference: [ Wiederhold, 1992 ] <author> Gio Wiederhold. </author> <booktitle> Mediators in the architecture of future information systems. IEEE Computer, </booktitle> <month> March </month> <year> 1992. </year>
Reference: [ Yu and Sun, 1989 ] <author> Clement T. Yu and Wei Sun. </author> <title> Automatic knowledge acquisition and maintenance for semantic query optimization. </title> <journal> IEEE Trans. Knowledge and Data Engineering, </journal> <volume> I(3):362-375, </volume> <year> 1989. </year>
Reference-contexts: This way, the learner will be able to learn effective and robust rules for semantic query optimization. 1.4 Closely Related Work Previously, three approaches to automating the knowledge acquisition for semantic query optimization were proposed in [ Siegel, 1988 ] , [ Shekhar et al., 1993 ] and <ref> [ Yu and Sun, 1989 ] </ref> . The first approach is a query-driven approach due to [ Siegel, 1988, Siegel, 1989 ] . Siegel's system learns simple rules, a limited form of rules that allows exactly one literal on each side of implication. <p> Again, the issue of database change is totally neglected in this approach. In brief, this approach requires careful hand-tailoring and does not address important issues of the learning problem. The knowledge acquisition approach presented in <ref> [ Yu and Sun, 1989 ] </ref> generates semantic rules from two equivalent or subsumable queries, where the answer of one query is a subset of the others. Unlike our approach, they do not describe any systematic approach to generating two equivalent queries. <p> This identification process could be expensive but futile, because the system needs to cache the answers of all queries, while the system may end up generating rules from two expensive queries and the learned rules are not useful in cost reduction. In addition to a rule generation approach, <ref> [ Yu and Sun, 1989 ] </ref> also presents an approach to removing logically redundant rules. That approach could be useful to maintain the number of rules and partly address the utility problem. <p> Receiving a training query, the learner applies an inductive learning algorithm to induce a low-cost alternative query equivalent to 33 the training query. The pruning component then takes the training query and the learned alternative query to derive a set of semantic rules. Previously, <ref> [ Yu and Sun, 1989 ] </ref> showed that semantic rules for SQO can be derived from two equivalent queries. However, they do not show how to automatically generate equivalent queries. Our approach can automatically induce a low-cost equivalent query for an expensive training query. <p> We will explain why this is the case in Section 3.3 (see also <ref> [ Yu and Sun, 1989 ] </ref> ). <p> Redundant rules are logical consequences of other rules in a rule set. [ Greiner and Likuski, 1989 ] and [ Etzioni, 1992 ] show that redundant macro-operators are guaranteed to slow down a problem solver. <ref> [ Yu and Sun, 1989 ] </ref> extends the SQO optimizer to identify logically redundant rules. Low utility rules are those rarely applicable or yield small saving.
Reference: [ Ziarko, 1995 ] <author> Wojciech Ziarko. </author> <title> The special issue on rough sets and knowledge discovery. </title> <journal> Computational Intelligence, </journal> <volume> 11(2), </volume> <year> 1995. </year> <month> 137 </month>
Reference-contexts: Rough set theory [ Pawlak, 1991 ] has been applied in many knowledge discovery applications <ref> [ Ziarko, 1995 ] </ref> and is useful for measuring whether a given set of attributes is sufficient to represent a target concept.
References-found: 90

