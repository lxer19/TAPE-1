URL: ftp://ftp.cis.ufl.edu/cis/tech-reports/tr91/tr91-023.ps
Refering-URL: http://www.cis.ufl.edu/tech-reports/tech-reports/tr91-abstracts.html
Root-URL: http://www.cis.ufl.edu
Title: Unsymmetric-pattern multifrontal methods for parallel sparse LU factorization  
Author: T. A. Davis I. S. Duff 
Date: September 1991  
Note: European Center for Research and Advanced Training in Scientific  
Address: Gainesville, Florida, USA  Toulouse, France  
Affiliation: Computer and Information Sciences Department University of Florida,  Computation (CERFACS)  
Abstract: Technical Report TR-91-23 Computer and Information Sciences Department University of Florida Gainesville, FL, 32611 USA fl phone: (904) 392-1481, email: davis@cis.ufl.edu. Supported by a post-doctoral grant from CERFACS, September 1989 to December 1990. Continuing research supported by the National Science Foundation (ASC-9111263, 8/91). This report is available in postscript form via anonymous ftp (in binary mode) to cis.ufl.edu as the compressed file /cis/tech-reports/tr91/tr91-023.ps.Z. y Also Rutherford Appleton Laboratory, Chilton, Didcot, Oxon. 0X11 0QX England. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> A. V. Aho, M. R. Garey, and J. D. Ullman, </author> <title> The transitive reduction of a directed graph, </title> <journal> SIAM J. Comput., </journal> <volume> 1 (1972), </volume> <pages> pp. 131-137. </pages>
Reference-contexts: Edge (r; t) is redundant to describe the control flow dependences between tasks r, s, and t and can be removed. The edge (r; t) would also be removed if transitive reduction <ref> [1] </ref> is applied to this fragment. The new graph, after edge removal via all four cases, will be referred to as the reduced control flow graph, and denoted as G rcontrol = (V; E rcontrol ). <p> These two graphs are not necessarily minimal. For example, the redundant edge (1; 4) in the graph shown in Figure 6 will be present in the reduced graphs (where all edges are in E L ). Transitive reduction <ref> [1] </ref> could be applied to G complete control to obtain a minimal G rcontrol . Transitive reduction cannot be applied to G complete data .
Reference: [2] <author> G. Alaghband and H. F. Jordan, </author> <title> Sparse Gaussian elimination with controlled fill-in on a shared memory multiprocessor, </title> <journal> IEEE Trans. Comput., </journal> <volume> 38 (1989), </volume> <pages> pp. 1539-1557. </pages>
Reference-contexts: It will be described in Section 2 to contrast it with the unsymmetric-pattern multifrontal method proposed here. Other parallel algorithms for unsymmetric sparse matrices include the D2 algorithm [5], partial pivoting methods [20, 21, 22, 24], the PSolve algorithm [4], and others <ref> [2, 27] </ref>. The D2 algorithm is based on a nondeterministic parallel pivot search that constructs a set of independent pivots (m, say) followed by a parallel rank-m update of the active submatrix.
Reference: [3] <author> P. R. Amestoy and I. S. Duff, </author> <title> Vectorization of a multiprocessor multi--frontal code, </title> <journal> Internat. J. Supercomputer Appl., </journal> <volume> 3 (1989), </volume> <pages> pp. 41-59. </pages>
Reference-contexts: Thus the algorithm uses dense matrix kernels in its innermost loops, potentially giving it high performance on parallel-vector supercomputers. 1.1 Previous work The multifrontal method of Duff and Reid (MA37) <ref> [3, 9, 14] </ref> will be referred to as the classical multifrontal method. It will be described in Section 2 to contrast it with the unsymmetric-pattern multifrontal method proposed here. <p> [k] [k] t , respectively. 9 2.1 Example matrix Consider the matrix A A = 6 6 6 6 6 6 p 1 fi fi fi fi p 3 fi fi fi fi fi fi fi fi fi 7 7 7 7 7 7 with a partially factorized matrix L <ref> [3] </ref> A [3] U [3] 2 6 6 6 6 6 4 fi p 2 fi fi fi fi fi p 4 fi fi fi 3 7 7 7 7 7 5 after three steps of LU factorization. <p> t , respectively. 9 2.1 Example matrix Consider the matrix A A = 6 6 6 6 6 6 p 1 fi fi fi fi p 3 fi fi fi fi fi fi fi fi fi 7 7 7 7 7 7 with a partially factorized matrix L <ref> [3] </ref> A [3] U [3] 2 6 6 6 6 6 4 fi p 2 fi fi fi fi fi p 4 fi fi fi 3 7 7 7 7 7 5 after three steps of LU factorization. <p> respectively. 9 2.1 Example matrix Consider the matrix A A = 6 6 6 6 6 6 p 1 fi fi fi fi p 3 fi fi fi fi fi fi fi fi fi 7 7 7 7 7 7 with a partially factorized matrix L <ref> [3] </ref> A [3] U [3] 2 6 6 6 6 6 4 fi p 2 fi fi fi fi fi p 4 fi fi fi 3 7 7 7 7 7 5 after three steps of LU factorization. <p> A single scatter operation assembles a row of E [k1] s into E k . Without this offset, both a scatter and a gather would be needed. This approach is taken in the classical mul tifrontal method <ref> [3] </ref>. <p> Since a fully parallel version is not yet developed, only one processor is used for the performance comparisons, even though both D2 and the classical multifrontal method are parallel algorithms. Amestoy and Duff's version of the classical multifrontal method is used for the comparisons <ref> [3] </ref>. Each algorithm was used to factorize a set of 39 test matrices from the Harwell/Boeing sparse matrix collection [11], and the results for five typical matrices are shown in Table 1. <p> An unmatched entry is an entry a ij for which a ji is zero. The next section of the table compares the performance of MA28 [12], the D2 algorithm (with and without the switch to dense code) [5], the classical multifrontal method (Amestoy-Duff) <ref> [3] </ref>, and the analysis-factor algorithm based on the unsymmetric-pattern multifrontal method. The D2 algorithm switches to dense code when a pivot set of less than four pivots is found and when the density of A k is greater than 20%.
Reference: [4] <author> T. A. Davis and E. S. Davidson, </author> <title> Pairwise reduction for the direct, parallel solution of sparse unsymmetric sets of linear equations, </title> <journal> IEEE Trans. Comput., </journal> <volume> 37 (1988), </volume> <pages> pp. 1648-1654. </pages>
Reference-contexts: It will be described in Section 2 to contrast it with the unsymmetric-pattern multifrontal method proposed here. Other parallel algorithms for unsymmetric sparse matrices include the D2 algorithm [5], partial pivoting methods [20, 21, 22, 24], the PSolve algorithm <ref> [4] </ref>, and others [2, 27]. The D2 algorithm is based on a nondeterministic parallel pivot search that constructs a set of independent pivots (m, say) followed by a parallel rank-m update of the active submatrix.
Reference: [5] <author> T. A. Davis and P.-C. Yew, </author> <title> A nondeterministic parallel algorithm for general unsymmetric sparse LU factorization, </title> <journal> SIAM J. Matrix Anal. Appl., </journal> <volume> 11 (1990), </volume> <pages> pp. 383-402. </pages>
Reference-contexts: It will be described in Section 2 to contrast it with the unsymmetric-pattern multifrontal method proposed here. Other parallel algorithms for unsymmetric sparse matrices include the D2 algorithm <ref> [5] </ref>, partial pivoting methods [20, 21, 22, 24], the PSolve algorithm [4], and others [2, 27]. The D2 algorithm is based on a nondeterministic parallel pivot search that constructs a set of independent pivots (m, say) followed by a parallel rank-m update of the active submatrix. <p> An unmatched entry is an entry a ij for which a ji is zero. The next section of the table compares the performance of MA28 [12], the D2 algorithm (with and without the switch to dense code) <ref> [5] </ref>, the classical multifrontal method (Amestoy-Duff) [3], and the analysis-factor algorithm based on the unsymmetric-pattern multifrontal method. The D2 algorithm switches to dense code when a pivot set of less than four pivots is found and when the density of A k is greater than 20%.
Reference: [6] <author> J. J. Dongarra, J. Du Croz, I. S. Duff, and S. Hammarling, </author> <title> A set of level 3 basic linear algebra subprograms, </title> <journal> ACM Trans. Math. Softw., </journal> <volume> 16 (1990), </volume> <pages> pp. 1-17. </pages>
Reference-contexts: These steps of LU factorization compute a submatrix of update terms that are held within the frontal matrix until they are assembled (added) into the frontal matrix of its father in the elimination tree. The elimination tree controls parallelism across multiple frontal matrices, while dense matrix operations <ref> [6] </ref> provide parallelism and vectorization within each frontal matrix. These concepts are discussed by Duff and Reid in [14]. However, this method is based on an assumption of a symmetric nonzero pattern, and so has a poor performance on matrices whose patterns are very unsymmetric. <p> The elimination tree describes the large-grain parallelism between the nodes. For both the classical and unsymmetric-pattern multifrontal methods, the work at a node s will be referred to as task s, although additional medium-grain parallelism can occur within each task, using the Level-3 BLAS <ref> [6] </ref> for supernodes or Level-2 BLAS [7] for simple nodes. <p> The entire contribution block D s of a son can always be assembled into its father, since all the rows and columns that are affected by D s are present in E t . The method takes advantage of the dense matrix kernels <ref> [6, 7] </ref> to factorize E t : the Level-2 BLAS (outer-product) if g t = 1 or the Level-3 BLAS if g t &gt; 1. The classical multifrontal method is not the only method based on the elimination tree.
Reference: [7] <author> J. J. Dongarra, J. Du Croz, S. Hammarling, and R. J. Hanson, </author> <title> An extended set of Fortran Basic Linear Algebra Subprograms, </title> <journal> ACM Trans. Math. Softw., </journal> <volume> 14 (1988), </volume> <pages> pp. 1-17. </pages>
Reference-contexts: For both the classical and unsymmetric-pattern multifrontal methods, the work at a node s will be referred to as task s, although additional medium-grain parallelism can occur within each task, using the Level-3 BLAS [6] for supernodes or Level-2 BLAS <ref> [7] </ref> for simple nodes. The column pattern U t of the frontal matrix E t is given by the union of the column pattern U [t1] s of each son s of node t and the structure of row t of the upper triangular part of A. <p> The entire contribution block D s of a son can always be assembled into its father, since all the rows and columns that are affected by D s are present in E t . The method takes advantage of the dense matrix kernels <ref> [6, 7] </ref> to factorize E t : the Level-2 BLAS (outer-product) if g t = 1 or the Level-3 BLAS if g t &gt; 1. The classical multifrontal method is not the only method based on the elimination tree.
Reference: [8] <author> I. S. Duff, </author> <title> On permutations to block triangular form, </title> <journal> J. Inst. Math. Applic., </journal> <volume> 19 (1977), </volume> <pages> pp. 339-342. </pages>
Reference-contexts: The lower bound on the number of edges that transitive reduction could obtain for a data flow or control flow graph with jVj nodes is jVj b (where b is the number of irreducible blocks in A <ref> [8] </ref>, although the edge counts were computed without permuting the matrix A to block upper triangular form). 9 Summary The factor-only algorithm is "fragile" with respect to disruptions in the graphs and patterns of L and U caused by numerical pivoting. This problem is addressed by the analysis-factor algorithm.
Reference: [9] <author> I. S. Duff, A. M. Erisman, and J. K. Reid, </author> <title> Direct Methods for Sparse Matrices, </title> <publisher> Oxford University Press, Oxford, </publisher> <year> 1986. </year>
Reference-contexts: Thus the algorithm uses dense matrix kernels in its innermost loops, potentially giving it high performance on parallel-vector supercomputers. 1.1 Previous work The multifrontal method of Duff and Reid (MA37) <ref> [3, 9, 14] </ref> will be referred to as the classical multifrontal method. It will be described in Section 2 to contrast it with the unsymmetric-pattern multifrontal method proposed here. <p> The pivot search is assisted by a set of n linked lists. The d-th linked list holds those columns with upper bound degree d. The pivot a [k1] kk at step k must satisfy the threshold partial-pivoting cri terion <ref> [9] </ref>: ja kk j u max ja ik j; 0 &lt; u 1: (9) The candidate pivot is the numerically acceptable entry a [k1] ij with lowest approximate Markowitz cost using the true column degree and the upper bound row degree. 7.2.2 Assembly and dynamic amalgamation The frontal matrix E [k1]
Reference: [10] <author> I. S. Duff, N. I. M. Gould, J. K. Reid, J. A. Scott, and K. Turner, </author> <title> Factorization of sparse symmetric indefinite matrices, </title> <journal> IMA J. Numer. Anal., </journal> <volume> 11 (1991), </volume> <pages> pp. 181-204. </pages>
Reference-contexts: However, we can apply recent work of Duff and Reid [15] based on algorithms developed by Duff, Gould, Reid, Scott, and Turner <ref> [10] </ref> to obtain a suitable analysis. We discuss the use of their algorithms in this section. Methods based on the elimination dag are presented in [16, 23]. Duff et al. [10] design algorithms for factorizing symmetric indefinite matrices which use block pivots of order 1 or 2, chosen from the diagonal <p> can apply recent work of Duff and Reid [15] based on algorithms developed by Duff, Gould, Reid, Scott, and Turner <ref> [10] </ref> to obtain a suitable analysis. We discuss the use of their algorithms in this section. Methods based on the elimination dag are presented in [16, 23]. Duff et al. [10] design algorithms for factorizing symmetric indefinite matrices which use block pivots of order 1 or 2, chosen from the diagonal to preserve symmetry, and are suitable even when there are zero entries on the diagonal. <p> the augmented system " B 0 x # " b (8) where B is a nonsingular unsymmetric matrix of order n, the first n components of the solution are just the solution of the set of unsymmetric linear equations Bx = b: Furthermore, if the algorithm of Duff et al. <ref> [10] </ref> is used to choose pivots from the coefficient matrix of Equation 8, then n oxo pivots will be chosen. <p> Thus we can use the symmetric minimum degree ordering of Duff et al. <ref> [10] </ref> to obtain a symbolic analysis of an unsymmetric matrix. The code of Duff and Reid [15] will also produce the equivalent of our data dependency directed acyclic graph which could, after suitable modification, be used as input to the factor-only algorithm of this paper.
Reference: [11] <author> I. S. Duff, R. G. Grimes, and J. K. Reid, </author> <title> Sparse matrix test problems, </title> <journal> ACM Trans. Math. Software, </journal> <volume> 15 (1989), </volume> <pages> pp. 1-14. </pages>
Reference-contexts: Amestoy and Duff's version of the classical multifrontal method is used for the comparisons [3]. Each algorithm was used to factorize a set of 39 test matrices from the Harwell/Boeing sparse matrix collection <ref> [11] </ref>, and the results for five typical matrices are shown in Table 1. The table describes the five matrices by name, discipline from which they are derived, size, number of nonzero entries, and asymmetry.
Reference: [12] <author> I. S. Duff and J. K. Reid, </author> <title> Some design features of a sparse matrix code, </title> <journal> ACM Trans. Math. Softw., </journal> <volume> 5 (1979), </volume> <pages> pp. </pages> <month> 18-35. </month> <title> [13] |, The multifrontal solution of indefinite sparse symmetric linear equations, </title> <journal> ACM Trans. Math. Softw., </journal> <note> 9 (1983), </note> <author> p. </author> <month> 302-325. </month> <title> [14] |, The multifrontal solution of unsymmetric sets of linear equations, </title> <journal> SIAM J. Sci. Statist. Comput., </journal> <volume> 5 (1984), </volume> <pages> pp. 633-641. </pages> <month> [15] |, MA47, </month> <title> a Fortran code for direct solution of indefinite sparse symmetric linear systems, </title> <note> To appear, 1991. 41 </note>
Reference-contexts: To avoid this, only upper and lower bounds 32 of the degree of each row and column are computed. When, and if, the true degree is calculated, the two bounds are set equal to the true degree. Only the first few columns with minimum upper bound degree are searched <ref> [12, 28] </ref>, and the true degrees of these columns are computed. The pivot search is assisted by a set of n linked lists. The d-th linked list holds those columns with upper bound degree d. <p> The asymmetry of a matrix A is the number of unmatched off-diagonal entries over the total number of off-diagonal entries. An unmatched entry is an entry a ij for which a ji is zero. The next section of the table compares the performance of MA28 <ref> [12] </ref>, the D2 algorithm (with and without the switch to dense code) [5], the classical multifrontal method (Amestoy-Duff) [3], and the analysis-factor algorithm based on the unsymmetric-pattern multifrontal method.
Reference: [16] <author> S. C. Eisenstat and J. W. H. Liu, </author> <title> Exploiting structural symmetry in unsymmetric sparse symbolic factorization, </title> <type> Report CS-90-12, </type> <institution> Dept. of Computer Science, York University, </institution> <address> North York, Ontario, Canada, </address> <month> Nov. </month> <year> 1990. </year>
Reference-contexts: Of all these algorithms for unsymmetric sparse matrices, the classical multifrontal method takes most advantage of dense matrix kernels, but is unsuitable when the pattern of the matrix is very unsymmetric. Most recently, Gilbert and Liu [23] and Eisenstat and Liu <ref> [16] </ref> have presented symbolic factorization algorithms for unsymmetric matrices, assuming that the pivot ordering is known a priori. The algorithms are based on the elimination directed acyclic graph (dag) and its reductions, which are similar to the reduced data flow and control flow graphs presented in this report. <p> 2s g E complete = E L complete [ E U complete E L complete control E L complete data E L complete E U complete control E U complete data E U complete : Similar directed acyclic graphs have been used for symbolic LU factorization of unsymmetric sparse matrices <ref> [16, 23] </ref>. <p> Experimental results showing the level of reduction achieved are given in Table 2 and the accompanying discussion in Section 8. Eisenstat and Liu <ref> [16] </ref> describe similar reductions, which they refer to as partial transitive reductions. These reductions improve the assembly process, decrease the amount of memory needed to represent the graphs, simplify amalgamation, and improve the memory requirements for unassembled contributions. <p> However, we can apply recent work of Duff and Reid [15] based on algorithms developed by Duff, Gould, Reid, Scott, and Turner [10] to obtain a suitable analysis. We discuss the use of their algorithms in this section. Methods based on the elimination dag are presented in <ref> [16, 23] </ref>. Duff et al. [10] design algorithms for factorizing symmetric indefinite matrices which use block pivots of order 1 or 2, chosen from the diagonal to preserve symmetry, and are suitable even when there are zero entries on the diagonal.
Reference: [17] <author> A. George, M. T. Heath, J. W. H. Liu, and E. Ng, </author> <title> Solution of sparse positive definite systems on a shared-memory multiprocessor, </title> <journal> Internat. J. Parallel Programming, </journal> <volume> 15 (1986), </volume> <pages> pp. 309-325. </pages>
Reference-contexts: The classical multifrontal method is not the only method based on the elimination tree. In the sparse column-Cholesky factorization of George et al. <ref> [17] </ref>, the work at node k in the elimination tree is the computation of column k of L. The work at node k modifies column k with columns corresponding to a subset of the descendants of node k in the elimination tree.
Reference: [18] <author> A. George and J. W. H. Liu, </author> <title> Computer Solution of Large Sparse Positive Definite Systems, </title> <publisher> Prentice-Hall, </publisher> <address> Englewood Cliffs, NJ, </address> <year> 1981. </year> <title> [19] |, The evolution of the minimum degree ordering algorithm, </title> <journal> SIAM Review, </journal> <volume> 31 (1989), </volume> <pages> pp. 1-19. </pages>
Reference: [20] <author> A. George and E. Ng, </author> <title> An implementation of Gaussian elimination with partial pivoting for sparse systems, </title> <journal> SIAM J. Sci. Statist. Comput. </journal> <volume> 6 (1985), </volume> <pages> pp. </pages> <month> 390-409. </month> <title> [21] |, Parallel sparse Gaussian elimination with partial pivoting, </title> <institution> Oak Ridge National Laboratory, Oak Ridge, TN, </institution> <year> 1988. </year>
Reference-contexts: It will be described in Section 2 to contrast it with the unsymmetric-pattern multifrontal method proposed here. Other parallel algorithms for unsymmetric sparse matrices include the D2 algorithm [5], partial pivoting methods <ref> [20, 21, 22, 24] </ref>, the PSolve algorithm [4], and others [2, 27]. The D2 algorithm is based on a nondeterministic parallel pivot search that constructs a set of independent pivots (m, say) followed by a parallel rank-m update of the active submatrix.
Reference: [22] <author> J. R. Gilbert, </author> <title> An efficient parallel sparse partial pivoting algorithm, </title> <type> Report 88/45052-1, </type> <institution> Center for Computer Science, Chr. Michelsen Institute, Bergen, Norway, </institution> <year> 1988. </year>
Reference-contexts: It will be described in Section 2 to contrast it with the unsymmetric-pattern multifrontal method proposed here. Other parallel algorithms for unsymmetric sparse matrices include the D2 algorithm [5], partial pivoting methods <ref> [20, 21, 22, 24] </ref>, the PSolve algorithm [4], and others [2, 27]. The D2 algorithm is based on a nondeterministic parallel pivot search that constructs a set of independent pivots (m, say) followed by a parallel rank-m update of the active submatrix. <p> The elimination tree is typical ly defined only for symmetric-patterned LU factors, with the exception of partial-pivoting methods <ref> [21, 22] </ref>. The classical multifrontal method [13] is one method based on the elimination tree. It has a similar formulation as the general frontal matrix formulation described in the previous section, except that the analysis is performed on the pattern of A+A T . Frontal matrices are square.
Reference: [23] <author> J. R. Gilbert and J. W. H. Liu, </author> <title> Elimination structures for unsymmet-ric sparse LU factors, </title> <type> Report CS-90-11, </type> <institution> Dept. of Computer Science, York University, </institution> <address> North York, Ontario, Canada, </address> <month> Feb. </month> <year> 1991. </year>
Reference-contexts: Of all these algorithms for unsymmetric sparse matrices, the classical multifrontal method takes most advantage of dense matrix kernels, but is unsuitable when the pattern of the matrix is very unsymmetric. Most recently, Gilbert and Liu <ref> [23] </ref> and Eisenstat and Liu [16] have presented symbolic factorization algorithms for unsymmetric matrices, assuming that the pivot ordering is known a priori. <p> 2s g E complete = E L complete [ E U complete E L complete control E L complete data E L complete E U complete control E U complete data E U complete : Similar directed acyclic graphs have been used for symbolic LU factorization of unsymmetric sparse matrices <ref> [16, 23] </ref>. <p> However, we can apply recent work of Duff and Reid [15] based on algorithms developed by Duff, Gould, Reid, Scott, and Turner [10] to obtain a suitable analysis. We discuss the use of their algorithms in this section. Methods based on the elimination dag are presented in <ref> [16, 23] </ref>. Duff et al. [10] design algorithms for factorizing symmetric indefinite matrices which use block pivots of order 1 or 2, chosen from the diagonal to preserve symmetry, and are suitable even when there are zero entries on the diagonal.
Reference: [24] <author> J. R. Gilbert and T. Peierls, </author> <title> Sparse partial pivoting in time proportional to arithmetic operations, </title> <journal> SIAM J. Sci. Statist. Comput., </journal> <note> 9 (1988) pp. 862-874. </note>
Reference-contexts: It will be described in Section 2 to contrast it with the unsymmetric-pattern multifrontal method proposed here. Other parallel algorithms for unsymmetric sparse matrices include the D2 algorithm [5], partial pivoting methods <ref> [20, 21, 22, 24] </ref>, the PSolve algorithm [4], and others [2, 27]. The D2 algorithm is based on a nondeterministic parallel pivot search that constructs a set of independent pivots (m, say) followed by a parallel rank-m update of the active submatrix.
Reference: [25] <author> J. W. H. Liu, </author> <title> The role of elimination trees in sparse factorization, </title> <journal> SIAM J. Matrix Anal. Appl., </journal> <volume> 11 (1990), </volume> <pages> pp. 134-172. </pages>
Reference-contexts: This would not occur if the pattern of A was symmetric. 3 Elimination tree and elimination directed acyclic graphs The elimination tree forms the basis of many sparse matrix algorithms, describing either the data flow graph or the control flow graph, or both <ref> [25] </ref>. A more general pair of graphs is needed for the unsymmetric-pattern multi frontal method.
Reference: [26] <author> H. M. Markowitz, </author> <title> The elimination form of the inverse and its application to linear programming, </title> <booktitle> Management Science, 3 (1957), </booktitle> <pages> pp. 255-269. </pages>
Reference-contexts: Set g k to the number of pivots actually found. 5. Perform degree update and update the Lson and Uson lists. Increment k by g k and repeat until the matrix is factorized. 7.2.1 Pivot search The pivot search is based on Markowitz' strategy <ref> [26] </ref>, which selects the pivot a ij with minimum upper bound on fill-in (or cost), (r i 1)(c j 1): Scanning a row i of A [k1] involves scanning row i of the active part of A and unmarked tuples in the Lson list i.
Reference: [27] <author> O. Wing and J. W. Huang, </author> <title> A computational model of parallel solution of linear equations, </title> <journal> IEEE Trans. Comput., </journal> <volume> 29 (1980), </volume> <pages> pp. 632-638. 42 </pages>
Reference-contexts: It will be described in Section 2 to contrast it with the unsymmetric-pattern multifrontal method proposed here. Other parallel algorithms for unsymmetric sparse matrices include the D2 algorithm [5], partial pivoting methods [20, 21, 22, 24], the PSolve algorithm [4], and others <ref> [2, 27] </ref>. The D2 algorithm is based on a nondeterministic parallel pivot search that constructs a set of independent pivots (m, say) followed by a parallel rank-m update of the active submatrix.
Reference: [28] <author> Z. Zlatev, J. Wasniewski, and K. Schaumburg, Y12M: </author> <title> Solution of large and sparse systems of linear algebraic equations, </title> <booktitle> Lecture Notes in Computer Science, </booktitle> <volume> 121, </volume> <publisher> Springer-Verlag, </publisher> <address> Berlin, </address> <year> 1981. </year> <month> 43 </month>
Reference-contexts: To avoid this, only upper and lower bounds 32 of the degree of each row and column are computed. When, and if, the true degree is calculated, the two bounds are set equal to the true degree. Only the first few columns with minimum upper bound degree are searched <ref> [12, 28] </ref>, and the true degrees of these columns are computed. The pivot search is assisted by a set of n linked lists. The d-th linked list holds those columns with upper bound degree d.
References-found: 23

