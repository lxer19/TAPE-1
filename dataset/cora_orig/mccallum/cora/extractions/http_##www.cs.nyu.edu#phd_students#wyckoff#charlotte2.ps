URL: http://www.cs.nyu.edu/phd_students/wyckoff/charlotte2.ps
Refering-URL: http://www.cs.nyu.edu/phd_students/wyckoff/index.html
Root-URL: http://www.cs.nyu.edu
Title: Charlotte: Metacomputing on the Web  
Author: A. Baratloo a M. Karaul b Z. M. Kedem a and P. Wyckoff c 
Keyword: Metacomputing, Parallel Computing, Fault Tolerance, Load Balancing, World Wide Web  
Note: c Bellcore, 445  
Address: New York University, New York, NY 10012-1185, USA  Murray Hill, NJ, USA  South Street, Morristown, NJ 07960-6438, USA  
Affiliation: a Department of Computer Science, Courant Institute of Mathematical Sciences,  b Bell Labs,  
Abstract: Parallel computing on local area networks is generally based on mechanisms that specifically target the properties of the local area network environment. However, those mechanisms do not effectively extend to wide area networks due to issues such as heterogeneity, security, and administrative boundaries. We present a system which enables application programmers to write parallel programs in Java and allows Java-capable browsers to execute parallel tasks. It comprises a virtual machine model which isolates the program from the execution environment, and a runtime system realizing this virtual machine on the Web. Load balancing and fault masking are transparently provided by the runtime system. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Y. Aumann, Z. M. Kedem, K. Palem, and M. Rabin, </author> <title> Highly efficient asynchronous execution of large-grained parallel programs, </title> <booktitle> In Proc. 34th IEEE Annual Symp. on Foundations of Computer Science, </booktitle> <year> 1993. </year>
Reference: [2] <author> J. E. Baldeschwieler, R. D. Blumofe, and E. A. Brewer, </author> <title> ATLAS: An infrastructure for global computing, </title> <booktitle> In Proc. 7th ACM SIGOPS European Workshop: Systems Support for Worldwide Applications, </booktitle> <year> 1996. </year>
Reference-contexts: This is a very satisfying result, as the program managed the speedup of 3.52 while coping with four crashes and integrating four volunteers during the computation. 7 Related Work Many systems use Java to leverage its portability, heterogeneity, and code distribution via executable content. ATLAS <ref> [2] </ref> is a Java-based system which provides load-balancing by allowing underloaded workers to "steal" work from other workers.
Reference: [3] <author> A. Baratloo, </author> <title> Why not now? New York University, </title> <month> April </month> <year> 1996. </year> <month> 18 </month>
Reference-contexts: We have implemented Factoring [16] which computes the bunch size based on the number of remaining tasks and the number of currently available machines, which was shown effective for executing parallel programs on networks of workstations in Calypso <ref> [3] </ref>. Bunching has three 8 benefits. First, it reduces the number of task assignments, and hence, the as-sociated overhead. Second, it overlaps computation with communication by allowing machines to execute tasks while the manager is handling the shared memory updates of the previously computed task.
Reference: [4] <author> A. Baratloo, P. Dasgupta, and Z. M. Kedem, Calypso: </author> <title> A novel software system for fault-tolerant parallel processing on distributed platforms, </title> <booktitle> In Proc. IEEE International Symposium on High-Performance Distributed Computing, </booktitle> <year> 1995. </year>
Reference-contexts: The outline of the virtual machine interface to an actual system was proposed in [18]. Theoretical results were then interpreted in the context of networks of workstations in [11]. The above were significantly extended and validated in the Calypso <ref> [4] </ref> system, which provides a virtual machine interface and a run-time system targeting homogeneous networks of workstations. This separation 2 of programming and execution environment and the techniques employed in Calypso for load balancing and fault tolerance are also incorporated in Charlotte.
Reference: [5] <author> A. Baratloo, M. Karaul, H. Karl, and Z. M. Kedem, </author> <title> An infrastructure for network computing with Java applets, </title> <booktitle> In Proc. ACM Workshop on Java for High-Performanace Network Computing, </booktitle> <year> 1998. </year>
Reference-contexts: This is not a satisfactory solution if we wish to address metacomputing on the Web. To overcome the limitations of this solution, we have recently incorporated a directory service into Charlotte. The relevant techniques were developed during the research leading to KnittingFactory <ref> [5] </ref>. The approach does not require any specialized registry processes but instead uses standard HTTP servers which act as directory servers. In this presentation, we provide a brief summary. A directory maintains: (1) information about Charlotte applications looking for volunteers and (2) a list of other (neighbor) directories.
Reference: [6] <author> A. Baratloo, M. Karaul, Z. M. Kedem, and P. Wyckoff, </author> <title> Charlotte: Metacomputing on the Web, </title> <booktitle> In Proc. 9th Intl. Conf. on Parallel and Distributed Computing Systems, </booktitle> <year> 1996. </year>
Reference-contexts: These services together implement the virtual machine the program was written for. We assume that the user has one machine under her control and this machine is reliable. A manager (or a manager and a worker) executes on that machine. Other machines, volunteers (in <ref> [6] </ref> we refer to these as donaters) execute worker processes to help with the computation.
Reference: [7] <author> N. Biggs, </author> <title> Interaction models: </title> <institution> Course given at Royal Hollaway College, University of London. Cambridge University Press, </institution> <year> 1977. </year>
Reference-contexts: The state information 12 regarding which sites have and have not been visited is stored as part of the URL of the next directory. 6 Experiments To evaluate the performance of Charlotte, we chose a scientific application from statistical physics|computing the 3D Ising model <ref> [7] </ref> with a period of 23. This is a simplified model of magnets on a three dimensional lattice which can be used to describe qualitatively how small systems behave.
Reference: [8] <author> T. Brecht, H. Sandhu, M. Shan, and J. Talbot, ParaWeb: </author> <title> Towards world-wide supercomputing. </title> <booktitle> In Proc. 7th ACM SIGOPS European Workshop, </booktitle> <year> 1996. </year>
Reference-contexts: JavaParty [23], ParaWeb <ref> [8] </ref>, and NinFlit [25] are Java-based systems for distributed computing. JavaParty provides mechanisms (built on top of Java 16 RMI) for transparently distributing remote objects. ParaWeb is an imple-mentation of the JVM that allows Java threads to be transparently executed remotely.
Reference: [9] <author> P. Cappello, B. Christiansen, M. F. Ionescu, M. O. Neary, K. E. Schauser, and D. Wu, Javelin: </author> <title> Internet-based parallel computing using Java, </title> <journal> Concurrency: Practice and Experience, </journal> <year> 1997. </year>
Reference-contexts: The advantage of this approach is that problems such as load balancing and fault tolerance are handled by the runtime system; the disadvantage is that the programmer does not have explicit control over resource utilization. Javelin <ref> [9] </ref> and Bayanihan [24] are systems which address volunteer based computing. Javelin is an infrastructure for volunteer based computing. In Javelin a standalone application, called the broker, acts as the central task repository and scheduler.
Reference: [10] <author> N. Carriero and D. Gelernter, </author> <title> How to write parallel programs, </title> <publisher> MIT Press, </publisher> <year> 1991. </year>
Reference-contexts: It is easy to see that faster machines generally do more of the work. For this reason, the self-scheduling programming model has been used widely, and in the literature, it is called the master/slave [13], the manager/worker [14] and the bag-of-tasks <ref> [10] </ref> programming models. 7 The self-scheduling programming model is a good starting point, but does not solve all the problems associated with Web-based computing. First, it does not address machine and network failures.
Reference: [11] <author> P. Dasgupta, Z. M. Kedem, and M. Rabin, </author> <title> Parallel processing on networks of workstations: A fault-tolerant, high performance approach, </title> <booktitle> In Proc. of the 15th International Conference on Distributed Computing Systems, </booktitle> <year> 1995. </year>
Reference-contexts: The outline of the virtual machine interface to an actual system was proposed in [18]. Theoretical results were then interpreted in the context of networks of workstations in <ref> [11] </ref>. The above were significantly extended and validated in the Calypso [4] system, which provides a virtual machine interface and a run-time system targeting homogeneous networks of workstations.
Reference: [12] <author> A. Ferrari, </author> <title> JPVM The Java Parallel Virtual Machine, </title> <address> http://www.cs.virginia.edu/ajf2j/jpvm.html. </address>
Reference-contexts: In contrast to these systems, which provide more general distributed computing mechanisms, Charlotte provides a comprehensive environment specifically for volunteer based computing which transparently handles faults and load balancing for parallel applications. JPVM <ref> [12] </ref> and Java-MPI [26] provide a message passing interface to Java which should provide the same advantages in Java that PVM and MPI provide to C and Fortran, namely flexibility and performance. In Charlotte, we provide a more high level solution that decouples the programming environment from the execution environment.
Reference: [13] <author> A. Geist, A. Beguelin, J. Dongarra, W. Jiang, R. Manchek, and V. Sunderam, </author> <title> PVM: Parallel virtual machine, </title> <publisher> MIT Press, </publisher> <year> 1994. </year>
Reference-contexts: It is easy to see that faster machines generally do more of the work. For this reason, the self-scheduling programming model has been used widely, and in the literature, it is called the master/slave <ref> [13] </ref>, the manager/worker [14] and the bag-of-tasks [10] programming models. 7 The self-scheduling programming model is a good starting point, but does not solve all the problems associated with Web-based computing. First, it does not address machine and network failures.
Reference: [14] <author> W. Gropp, E. Lust, and A. Skjellum, </author> <title> Using MPI: Portable parallel programming with the message-passing interface, </title> <publisher> MIT Press, </publisher> <year> 1994. </year>
Reference-contexts: It is easy to see that faster machines generally do more of the work. For this reason, the self-scheduling programming model has been used widely, and in the literature, it is called the master/slave [13], the manager/worker <ref> [14] </ref> and the bag-of-tasks [10] programming models. 7 The self-scheduling programming model is a good starting point, but does not solve all the problems associated with Web-based computing. First, it does not address machine and network failures.
Reference: [15] <author> S. Hirano. Horb: </author> <title> Distributed execution of Java programs, worldwide computing and its applications, </title> <booktitle> In Springer Lecture Notes in Computer Science, </booktitle> <year> 1997. </year>
Reference-contexts: Bayanihan is similar to Charlotte in the services it provides for volunteer based computing. The two major components of the Bayanihan system are the communication module and the scheduler. Currently, Bayani-han provides communication in the form of migratable objects which are implemented on top of the HORB <ref> [15] </ref> system. The current implementation of Bayanihan uses eager scheduling, and thus, is able to provide transparent fault tolerance and load balancing like Charlotte. 8 Conclusions In this paper, we presented an environment for volunteer-based parallel computing on the World Wide Web.
Reference: [16] <author> S. F. Hummel, E. Schonberg, and L. E. Flynn, </author> <title> Factoring a method for scheduling parallel loops, </title> <journal> Communications of the ACM, </journal> <year> 1992. </year>
Reference-contexts: We employ dynamic granularity management (or bunching for short) to mask network latencies associated with the process of assigning tasks to machines. Bunching extends self-scheduling by assigning a set of tasks (a bunch) at once. We have implemented Factoring <ref> [16] </ref> which computes the bunch size based on the number of remaining tasks and the number of currently available machines, which was shown effective for executing parallel programs on networks of workstations in Calypso [3]. Bunching has three 8 benefits.
Reference: [17] <author> H. Karl, </author> <title> Bridging the gap between distributed shared memory and message passing, </title> <booktitle> In Proc. ACM Workshop on Java for High-Performanace Network Computing, </booktitle> <year> 1998. </year>
Reference-contexts: This can be achieved through the use of annotations, which allow the programmer to give the run-time system hints about data access patterns. This approach was studied and proved effective for Charlotte in <ref> [17] </ref>. As mentioned in Section 3 we employ bunching to execute fine-grain parallel tasks in a coarse-grain manner. The scheduling service dynamically assigns a bunch of identical routines specifying a range of id values. <p> Also as we have noted, the syntax of Charlotte does not even allow the programmer to specify the volunteers or how to distribute the data. However, Charlotte has recently been extended with "collocation" techniques using annotations <ref> [17] </ref> to allow the manager to assign a routine based on its expected data access patterns and the cached shared data of candidate volunteers.
Reference: [18] <author> Z. M. Kedem and K. Palem, </author> <title> Transformations for the automatic derivation of resilient parallel programs, </title> <booktitle> In Proc. IEEE Workshop on Fault-Tolerant Parallel and Distributed Systems, </booktitle> <year> 1992. </year>
Reference-contexts: Charlotte explicitly addresses these issues. The research leading to Charlotte started as theoretical work where provable methods for executing parallel computations on abstract asynchronous processors were developed [20,19,1]. The outline of the virtual machine interface to an actual system was proposed in <ref> [18] </ref>. Theoretical results were then interpreted in the context of networks of workstations in [11]. The above were significantly extended and validated in the Calypso [4] system, which provides a virtual machine interface and a run-time system targeting homogeneous networks of workstations.
Reference: [19] <author> Z. M. Kedem, K. Palem, M. Rabin, and A. Raghunathan, </author> <title> Efficient program transformations for resilient parallel computation via randomization, </title> <booktitle> In Proc. 24th ACM Symposium on Theory of Computing, </booktitle> <year> 1992. </year> <month> 19 </month>
Reference: [20] <author> Z. M. Kedem, K. Palem, and P. Spirakis, </author> <title> Efficient robust parallel computations, </title> <booktitle> In Proc. 22nd ACM Symposium on Theory of Computing, </booktitle> <year> 1990. </year>
Reference-contexts: Depending on the network, this overhead may be large, and in the case of the Web, unpredictable. Charlotte programs are self-scheduled. But, in addition, we extend the basic notion of self-scheduling with two mechanisms initially proposed in <ref> [20] </ref>: eager scheduling (though this term was coined later) and two-phase idempotent execution strategy (TIES).
Reference: [21] <author> A. Kleine, </author> <title> The TYA just in time compiler, </title> <address> http://www.dragon1.net/software/tya. </address>
Reference-contexts: All the experiments were conducted using Linux JDK 1.1.5 v7 with TYA version .07 <ref> [21] </ref>, a just-in-time compiler. A C program would run faster than a Java program; however, as Java compilers continue improving, they will provide better performance and we expect our results to carry over transparently.
Reference: [22] <author> D. Mosberger, </author> <title> Memory Consistency Models, </title> <type> Technical Report TR92/11, </type> <institution> University of Arizona, </institution> <year> 1992. </year>
Reference-contexts: The consistency and coherence of the distributed data is maintained by the runtime system. Since Java does not support operator overloading, Charlotte's distributed memory objects are read and written through member methods. Several DSM systems have introduced multiple memory-consistency semantics <ref> [22] </ref>. In the current Charlotte system, we provide a single and intuitive semantics: Concurrent Read, Concurrent Write Common (CRCW-Common). This means that one or more routines can read a variable, and one or more routines can write a variable as long as they write the same value.
Reference: [23] <author> M. Philippsen and M. Zenger, </author> <title> JavaParty Transparent remote objects in Java, </title> <booktitle> In Proc. ACM 1997 PPoPP Workshop on Java for Science and Engineering Computation, </booktitle> <year> 1997. </year>
Reference-contexts: JavaParty <ref> [23] </ref>, ParaWeb [8], and NinFlit [25] are Java-based systems for distributed computing. JavaParty provides mechanisms (built on top of Java 16 RMI) for transparently distributing remote objects. ParaWeb is an imple-mentation of the JVM that allows Java threads to be transparently executed remotely.
Reference: [24] <author> L. Sarmenta, </author> <title> Web-based volunteer computing using java, </title> <booktitle> In Proc. of the 2nd Intl. Conference on Worldwide Computing and its Applications, </booktitle> <year> 1998. </year>
Reference-contexts: The advantage of this approach is that problems such as load balancing and fault tolerance are handled by the runtime system; the disadvantage is that the programmer does not have explicit control over resource utilization. Javelin [9] and Bayanihan <ref> [24] </ref> are systems which address volunteer based computing. Javelin is an infrastructure for volunteer based computing. In Javelin a standalone application, called the broker, acts as the central task repository and scheduler.
Reference: [25] <author> H. Takagi, S. Matsuoka, H. Nakada, S. Sekiguchi, M. Satoh, and U. Nagashima, Ninflet: </author> <title> A migratable parallel objects framework using Java, </title> <booktitle> In Proc. ACM 1998 Workshop on Java for High-Performance Network Computing, </booktitle> <year> 1998. </year>
Reference-contexts: JavaParty [23], ParaWeb [8], and NinFlit <ref> [25] </ref> are Java-based systems for distributed computing. JavaParty provides mechanisms (built on top of Java 16 RMI) for transparently distributing remote objects. ParaWeb is an imple-mentation of the JVM that allows Java threads to be transparently executed remotely.

References-found: 25

