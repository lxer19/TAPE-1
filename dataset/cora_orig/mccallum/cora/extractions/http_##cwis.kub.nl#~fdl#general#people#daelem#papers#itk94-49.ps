URL: http://cwis.kub.nl/~fdl/general/people/daelem/papers/itk94-49.ps
Refering-URL: http://ilk.kub.nl/papers.html
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Email: Walter.Daelemans@kub.nl  
Title: Memory-Based Lexical Acquisition and Processing  
Author: Walter Daelemans 
Address: P.O.Box 90153, 5000 LE Tilburg, The Netherlands  
Affiliation: Institute for Language Technology and AI, Tilburg University  
Abstract: Current approaches to computational lexicology in language technology are knowledge-based (competence-oriented) and try to abstract away from specific formalisms, domains, and applications. This results in severe complexity, acquisition and reusability bottlenecks. As an alternative, we propose a particular performance-oriented approach to Natural Language Processing based on automatic memory-based learning of linguistic (lexical) tasks. The consequences of the approach for computational lexicology are discussed, and the application of the approach on a number of lexical acquisition and disambiguation tasks in phonology, morphology and syntax is described.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Aha, D.: </author> <title> A study of Instance-Based Algorithms for Supervised Learning Tasks. </title> <institution> University of California at Irvine technical report 90-42, </institution> <year> 1990. </year>
Reference-contexts: Many other methods to weigh the relative importance of features have been designed, both in statistical pattern recognition and in machine learning (e.g., <ref> [1] </ref>; [15]; etc.), but the one we used is extremely simple and produced excellent results. The main idea of information gain weighting is to interpret the training set as an information source capable of generating a number of messages (the different category labels) with a certain probability.
Reference: [2] <author> Aha, D., Kibler, D. and Albert, M.: </author> <title> Instance-Based Learning Algorithms. </title> <booktitle> Machine Learning 6, </booktitle> <year> (1991) </year> <month> 37-66. </month>
Reference: [3] <author> Van den Bosch, A. and Daelemans, W.: </author> <title> `Data-oriented methods for grapheme-to-phoneme conversion.' </title> <booktitle> Proceedings of the Sixth conference of the European chapter of the ACL, ACL, </booktitle> <year> (1993) </year> <month> 45-53. </month>
Reference-contexts: Again, in the knowledge-based approach, the lexical requirements for such a system are extensive. In a typical knowledge-based system solving the problem, morphological analysis (with lexicon), phonotactic knowledge, and syllable structure determination modules are designed and implemented. In a lazy learning approach ([9]; <ref> [3] </ref>), again a windowing approach was used to formulate the task as a classification problem (identification this time: given a set of possible phonemes, determine which phoneme should be used to translate a target spelling symbol taking into account its context).
Reference: [4] <author> Briscoe, T., de Paiva, V. and Copestake, A.: </author> <title> Inheritance, Defaults and the Lexicon. </title> <publisher> Cambridge: Cambridge University Press, </publisher> <year> 1993. </year>
Reference-contexts: These mappings tend to be many-to-many and complex because they can often only be described by conflicting regularities, sub-regularities, and exceptions. 9 In current NLP, these different levels of generalization have been the prime motivation for research into inheritance mechanisms and default reasoning ([6]; <ref> [4] </ref>), especially in research on the structure and organisation of the lexicon. To illustrate the difference between the traditional knowledge-based approach with the lazy learning approach, consider Fig. 3.
Reference: [5] <author> Cost, S. and Salzberg, S.: </author> <title> A weighted nearest neighbour algorithm for learning with symbolic features. </title> <booktitle> Machine Learning 10, </booktitle> <year> (1993) </year> <month> 57-78. </month>
Reference-contexts: Instances of this form of nearest neighbour method include instance-based learning ([2]), exemplar-based learning ([21], <ref> [5] </ref>), memory-based reasoning ([26]), and case-based reasoning ([17]). Advantages of the approach include an often surprisingly high classification accuracy, the capacity to learn polymorphous concepts, high speed of learning, and perspicuity of algorithm and classification (see e.g., [5]). <p> this form of nearest neighbour method include instance-based learning ([2]), exemplar-based learning ([21], <ref> [5] </ref>), memory-based reasoning ([26]), and case-based reasoning ([17]). Advantages of the approach include an often surprisingly high classification accuracy, the capacity to learn polymorphous concepts, high speed of learning, and perspicuity of algorithm and classification (see e.g., [5]).
Reference: [6] <editor> Daelemans, W. and Gazdar, G.: (guest eds.) </editor> <booktitle> Special Issue Computational Linguistics on Inheritance in Natural Language Processing, </booktitle> <volume> 18 (2) and 18 (3), </volume> <year> 1992. </year>
Reference: [7] <author> Daelemans, W. and van den Bosch, A.: </author> <title> Generalization Performance of Backpropagation Learning on a Syllabification Task. </title> <editor> In: M.F.J. Drossaers and A. Nijholt (eds.) </editor> <booktitle> Connectionism and Natural Language Processing. Proceedings Third Twente Workshop on Language Technology, </booktitle> <year> (1992) </year> <month> 27-38. </month>
Reference: [8] <author> Daelemans, W. and van den Bosch, A.: </author> <title> `A Neural Network for Hyphenation.' </title> <editor> In: I. Aleksander and J. Taylor (eds.) </editor> <booktitle> Artificial Neural Networks II: Proceedings of the International Conference on Artificial Neural Networks. </booktitle> <publisher> Elsevier Science Publishers, </publisher> <year> (1992) </year> <month> 1647-1650. </month>
Reference-contexts: This parser requires at least lexical knowledge about existing stems and affixes and the way they can be combined. In the lazy learning approach ([7]; <ref> [8] </ref>), we used the windowing approach referred to earlier to formulate the task as a classification problem (more specifically, a segmentation problem). For each letter or phoneme, a pattern was created with a target letter or phoneme, a left context and a right context.
Reference: [9] <author> Daelemans, W. and van den Bosch, A.: `TABTALK: </author> <title> Reusability in Data-oriented grapheme-to-phoneme conversion.' </title> <booktitle> Proceedings of Eu-rospeech, </booktitle> <address> Berlin, </address> <year> (1993) </year> <month> 1459-1466. </month>
Reference: [10] <author> Daelemans, W., Gillis, S., Durieux, G., van den Bosch, A.: </author> <title> Learnability and Markedness in Data-Driven Acquisition of Stress. </title> <editor> In: T. Mark Elli-son and James M. Scobbie (eds) Computational Phonology. </editor> <booktitle> Edinburgh Working Papers in Cognitive Science 8, </booktitle> <year> (1993) </year> <month> 157-178. 16 </month>
Reference-contexts: Elsewhere ([7]; <ref> [10] </ref>) we introduced the concept of information gain (also used in decision tree learning, [20]) into lazy learning to weigh the importance of different features in a domain-independent way.
Reference: [11] <author> Daelemans, W., Gillis, S., and Durieux, G.: </author> <title> `The Acquisition of Stress, a data-oriented approach.' </title> <booktitle> Computational Linguistics 20 (3), </booktitle> <year> (1994) </year> <month> forthcoming. </month>
Reference-contexts: The results were replicated for English, French, and Dutch, using the same lazy learning algorithm, which shows its reusability. 5.3 Word Stress Assignment Another task we applied the lazy learning algorithm to, was stress assignment in Dutch monomorphematic, polysyllabic words ([10], <ref> [11] </ref>). A word was coded by assigning one feature to each part of the syllable structure of the last three syllables (if present) of the word (see the description of the diminutive formation task described earlier). There were three categories: final stress, penultimate stress, and antepenultimate stress (an identification problem). <p> In a broader context, the results described here argue for an empiricist approach to language acquisition, and for exemplars rather than rules in linguistic knowledge representation (see <ref> [11] </ref> and Gillis et al. [14] for further discussion of these issues). There are also some limitations to the method. The most important of these is the sparse data problem.
Reference: [12] <author> Derwing, B. L. and Skousen, R.: </author> <title> Real Time Morphology: Symbolic Rules or Analogical Networks. </title> <booktitle> Berkeley Linguistic Society 15: </booktitle> <year> (1989) </year> <month> 48-62. </month>
Reference-contexts: Outside the linguistic mainstream, people like Skousen, Derwing, and Bybee stress that "the analogical approach (as opposed to the rule-based approach) should receive more attention in the light of psycholinguistic results and new formalizations of the notion of analogy" ([25]; <ref> [12] </ref>), In cognitive psychology (e.g., [24]), exemplar-based categorization has a long history as an alternative for probabilistic and classical rule-based classification, and finally, in statistical pattern recognition, there is a long tradition of research on nearest neighbour classification methods which has been a source of inspiration for the development of lazy
Reference: [13] <author> Friedman, J., Bentley, J., and Finkel, R., </author> <title> an algorithm for finding best matches in logarithmic expected time. </title> <journal> ACM Transactions on Mathematical Software, </journal> <volume> (1977) 3 (3). </volume>
Reference: [14] <author> Gillis, S., Daelemans, W., Durieux, G. and van den Bosch, A.: `Learn-ability and Markedness: </author> <title> Dutch Stress Assignment.' </title> <booktitle> In: Proceedings of the Fifteenth Annual Conference of the Cognitive Science Society, </booktitle> <address> Boulder Colorado, USA, Hillsdale: </address> <publisher> Lawrence Erlbaum Associates, </publisher> <year> (1993) </year> <month> 452-457. </month>
Reference-contexts: In a broader context, the results described here argue for an empiricist approach to language acquisition, and for exemplars rather than rules in linguistic knowledge representation (see [11] and Gillis et al. <ref> [14] </ref> for further discussion of these issues). There are also some limitations to the method. The most important of these is the sparse data problem.
Reference: [15] <author> Kira, K. and Rendell, L.: </author> <title> A practical approach to feature selection. </title> <booktitle> Proceedings International Conference on Machine Learning, </booktitle> <year> 1992. </year>
Reference-contexts: Many other methods to weigh the relative importance of features have been designed, both in statistical pattern recognition and in machine learning (e.g., [1]; <ref> [15] </ref>; etc.), but the one we used is extremely simple and produced excellent results. The main idea of information gain weighting is to interpret the training set as an information source capable of generating a number of messages (the different category labels) with a certain probability.
Reference: [16] <author> Kitano, H.: </author> <title> Challenges of massive parallelism. </title> <booktitle> Proceedings IJCAI 1993, </booktitle> <pages> 813-834. </pages>
Reference: [17] <author> Kolodner, J.: </author> <title> Case-Based Reasoning. </title> <publisher> San-Mateo: Morgan-Kaufmann. </publisher> <year> 1993. </year>
Reference: [18] <author> Ling, C.: </author> <title> Learning the past tense of English verbs: The symbolic Pattern Associator vs. Connectionist Models. </title> <journal> Journal of Artificial Intelligence Research 1, </journal> <year> (1994) </year> <month> 209-229. </month>
Reference-contexts: Given a target and a context, determine whether and which boundary is associated with this target. Examples include syllabification, morphological analysis, syntactic analysis (in combination with tagging), etc. 2 This restriction can be circumvented by having multiple classifiers predict a different part of the output pattern, see <ref> [18] </ref> for this approach in learning decision trees. 11 An approach often necessary to arrive at the context information needed is the windowing approach (as in [22] for text to speech), in which an imaginary window is moved one item at a time over an input string where one item in
Reference: [19] <editor> Pustejovsky, J.: Dictionary/Lexicon. In: Stuart C. Shapiro (ed.), </editor> <booktitle> Encyclopedia of artificial intelligence, </booktitle> <address> New York: </address> <publisher> Wiley, </publisher> <year> 1992, </year> <pages> 341-365. </pages>
Reference-contexts: Also uncontroversial, but apparently no priority issue for many researchers, is the fact that the question which knowledge should be represented (which morphological, syntactic, and semantic senses of lexical items should be distinguished, <ref> [19] </ref>) depends completely on the natural language processing task that is to be solved. Different tasks require different lexical information. Also, different theoretical formalisms, domains, and languages require different types of lexical information and therefore possibly also different types of lexical knowledge representation and different acquisition methods.
Reference: [20] <author> Quinlan, J. R.: </author> <title> Induction Of Decision Trees. </title> <booktitle> Machine Learning 1, </booktitle> <year> (1986) </year> <month> 81-106. </month>
Reference-contexts: Elsewhere ([7]; [10]) we introduced the concept of information gain (also used in decision tree learning, <ref> [20] </ref>) into lazy learning to weigh the importance of different features in a domain-independent way.
Reference: [21] <author> Salzberg, S.: </author> <title> A nearest hyperrectangle learning method. </title> <booktitle> Machine Learning 6, </booktitle> <year> (1990) </year> <month> 251-276. </month>
Reference: [22] <author> Sejnowski, T. and Rosenberg, C.: NETtalk: </author> <title> a parallel network that learns to read aloud. </title> <journal> Complex Systems 1, </journal> <year> (1986) </year> <month> 145-168. 17 </month>
Reference-contexts: (in combination with tagging), etc. 2 This restriction can be circumvented by having multiple classifiers predict a different part of the output pattern, see [18] for this approach in learning decision trees. 11 An approach often necessary to arrive at the context information needed is the windowing approach (as in <ref> [22] </ref> for text to speech), in which an imaginary window is moved one item at a time over an input string where one item in the window (usually the middle item or the last item) acts as a target item, and the rest as the context.
Reference: [23] <author> Simmons, R. and Yu, Y.: </author> <title> The acquisition and use of context-dependent grammars for English. </title> <booktitle> Computational Linguistics 18 (3) (1992), </booktitle> <pages> 391-418. </pages>
Reference-contexts: An alternative possibility is to use operators as categories, e.g., shift and different types of reduce as categories in a shift-reduce parser (see <ref> [23] </ref> for such an approach outside the context of Machine Learning). 5 Examples The approach proposed in this paper is fairly recent, and experiments have focused on phonological and morphological tasks rather than on tasks like term disambiguation.
Reference: [24] <author> Smith, E. and Medin, D.: </author> <title> Categories and Concepts. </title> <address> Cambridge, MA: </address> <publisher> Harvard University Press, </publisher> <year> 1981. </year>
Reference-contexts: Outside the linguistic mainstream, people like Skousen, Derwing, and Bybee stress that "the analogical approach (as opposed to the rule-based approach) should receive more attention in the light of psycholinguistic results and new formalizations of the notion of analogy" ([25]; [12]), In cognitive psychology (e.g., <ref> [24] </ref>), exemplar-based categorization has a long history as an alternative for probabilistic and classical rule-based classification, and finally, in statistical pattern recognition, there is a long tradition of research on nearest neighbour classification methods which has been a source of inspiration for the development of lazy learning algorithms. 3.2 Variants of
Reference: [25] <author> Skousen, R.: </author> <title> Analogical Modeling of Language. </title> <publisher> Dordrecht: Kluwer, </publisher> <year> 1989. </year>
Reference: [26] <author> Stanfill, C. and Waltz, </author> <title> D.L.: Toward Memory-based Reasoning. </title> <booktitle> Communications of the ACM (1986) 29: </booktitle> <pages> 1213-1228. </pages>
Reference: [27] <author> Weiss, S. and Kulikowski, C.: </author> <title> Computer systems that learn. </title> <publisher> San-Mateo: Morgan Kaufmann, </publisher> <year> 1991. </year>
Reference: [28] <author> Winston, P.: </author> <booktitle> Artificial Intelligence. </booktitle> <address> Reading Mass.: </address> <publisher> Addison-Wesley, </publisher> <year> 1992. </year>
References-found: 28

