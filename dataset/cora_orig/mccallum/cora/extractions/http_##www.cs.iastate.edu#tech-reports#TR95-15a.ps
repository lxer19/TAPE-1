URL: http://www.cs.iastate.edu/tech-reports/TR95-15a.ps
Refering-URL: http://www.cs.iastate.edu/tech-reports/catalog.html
Root-URL: 
Title: Constructive Neural Network Learning Algorithms for Multi-Category Pattern Classification  
Author: TR #-a Rajesh Parekh, Jihoon Yang, and Vasant Honavar 
Keyword: Learning connectionism and neural nets; I.5.1 [Pattern Recognition] Models Neural nets. Keywords: neural networks, constructive learning algorithms, multi-category pattern classification, machine learning, supervised learning  
Address: 226 Atanasoff Hall  Ames, Iowa 50011-1040, USA  
Affiliation: Artificial Intelligence Research Group Department of Computer Science  Iowa Sate University  
Note: This revised version supersedes ISU CS Technical Report TR 95-15. ACM Computing Classification System Categories (1991): I.2.6 [Artificial Intelligence]  
Date: December 5, 1995  
Abstract-found: 0
Intro-found: 1
Reference: [Burgess, 94] <author> Burgess, N. </author> <year> (1994). </year> <title> A Constructive Algorithm That Converges for Real-Valued Input Patterns. </title> <journal> International Journal of Neural Systems. </journal> <volume> Vol. 5, No 1. </volume> <pages> pp 59-66. </pages>
Reference-contexts: A number of constructive algorithms that incrementally construct networks of threshold neurons for 2-category pattern classification tasks have been proposed in the literature. These include the tower, pyramid [Gallant, 90], tiling [Mezard & Nadal, 89], upstart [Frean, 90], and perceptron cascade <ref> [Burgess, 94] </ref>. <p> Since the number of patterns in the training set is finite, the number of errors is guaranteed to eventually become zero. 2 2.5 Perceptron Cascade Algorithm The perceptron cascade algorithm 3 <ref> [Burgess, 94] </ref> draws on the ideas used in the upstart algorithm and constructs a neural network that is topologically similar to the one built by the cascade correlation algorithm [Fahlman & Lebiere, 90]. However, unlike the cascade correlation algorithm, the perceptron cascade algorithm uses TLUs.
Reference: [Chen et al, 95] <author> Chen, C-H., Parekh, R.G., Yang, J., Balakrishnan, K., and Honavar, V.G. </author> <year> (1995). </year> <title> Analysis of Decision Boundaries Generated by Constructive Neural Network Learning Algorithms. </title> <booktitle> Proceedings of the World Congress on Neural Networks. </booktitle> <address> Washington D.C. </address> <note> Vol. I, </note> <month> July </month> <year> 1995, </year> <pages> pp 628-635. </pages>
Reference-contexts: The interested reader is referred to <ref> [Chen et al, 95] </ref> for an analysis (in geometrical terms) of the decision boundaries generated by some of these constructive learning algorithms. Each of these algorithms can be shown to converge to networks which yield zero classification errors on any given training set in the 2-category case.
Reference: [Duda & Hart, 73] <author> Duda, R. O., and Hart, P. E. </author> <year> (1973). </year> <title> Pattern Classification and Scene Analysis. </title> <address> New York: </address> <publisher> Wiley. </publisher>
Reference-contexts: If such a weight vector ( ^ W) exists for the pattern set S then S is said to be linearly separable. Several iterative algorithms are available for finding such a ^ W if one exists <ref> [Nilsson, 65; Duda & Hart, 73] </ref>. Most of these are variants of the perceptron weight update rule: W W + (C p O p )X p (where &gt; 0 is the learning rate).
Reference: [Fahlman & Lebiere, 90] <author> Fahlman, S. E. and Lebiere, C. </author> <year> (1990). </year> <title> The Cascade Correlation Learning Architecture. In Neural Information Systems 2. </title> <editor> Touretzky, D. S. (ed). Morgan-Kauffman, </editor> <year> 1990. </year> <pages> pp 524-532. </pages>
Reference-contexts: finite, the number of errors is guaranteed to eventually become zero. 2 2.5 Perceptron Cascade Algorithm The perceptron cascade algorithm 3 [Burgess, 94] draws on the ideas used in the upstart algorithm and constructs a neural network that is topologically similar to the one built by the cascade correlation algorithm <ref> [Fahlman & Lebiere, 90] </ref>. However, unlike the cascade correlation algorithm, the perceptron cascade algorithm uses TLUs. Initially an output neuron is trained using the L W algorithm.
Reference: [Frean, 90] <author> Frean, M. </author> <year> (1990). </year> <title> Small nets and short paths: Optimizing neural computation. </title> <type> Ph.D. Thesis. </type> <institution> Center for Cognitive Science. University of Edinburgh, UK. </institution>
Reference-contexts: However when S is not linearly separable, such algorithms behave poorly (i.e., the classification accuracy on the training set can fluctuate wildly from iteration to iteration). Several extensions to the perceptron weight update rule e.g., pocket algorithm [Gallant, 93], thermal perceptron <ref> [Frean, 90] </ref>, loss minimization algorithm [Hrycej, 92], and the barycentric correction procedure [Poulard, 95] are designed to find a reasonably good weight vector that correctly classifies a large fraction of the training set S when S is not linearly separable and converge to zero classification errors when S is linearly separable. <p> A number of constructive algorithms that incrementally construct networks of threshold neurons for 2-category pattern classification tasks have been proposed in the literature. These include the tower, pyramid [Gallant, 90], tiling [Mezard & Nadal, 89], upstart <ref> [Frean, 90] </ref>, and perceptron cascade [Burgess, 94]. <p> The convergence proof follows directly from the convergence proof of the tower algorithm. 2 2.4 Upstart Algorithm The 2-category upstart algorithm <ref> [Frean, 90] </ref> constructs a a binary tree of threshold neurons. A simple extension of this idea to deal with M output categories would be to construct M independent binary trees (one for each output class). This approach fails to exploit the inter-relationships that may exist between the different M -outputs.
Reference: [Gallant, 90] <author> Gallant, S.I. </author> <year> (1990). </year> <title> Perceptron Based Learning Algorithms. </title> <journal> IEEE Transactions on Neural Networks. </journal> <volume> Vol. I, No. 2, </volume> <month> June </month> <year> 1990. </year> <pages> pp 179-191. </pages>
Reference-contexts: A number of constructive algorithms that incrementally construct networks of threshold neurons for 2-category pattern classification tasks have been proposed in the literature. These include the tower, pyramid <ref> [Gallant, 90] </ref>, tiling [Mezard & Nadal, 89], upstart [Frean, 90], and perceptron cascade [Burgess, 94]. <p> C p = O p Number of patterns wrongly classified at layer A: e A A function sgn is defined as sgn (x) = 1 if x &lt; 0 and sgn (x) = 1 if x 0 where x is a real number. 2.2 Tower Algorithm The 2-category Tower algorithm <ref> [Gallant, 90] </ref> constructs a tower of TLUs. The bottommost neuron in the tower receives N inputs, one for each component of the pattern vector. The tower is built by successively adding neurons to the network and training them using L W until the desired classification accuracy is achieved. <p> We rely on the algorithm L W to find such a weight setting. Since the training set is finite in size, eventual convergence to zero errors is guaranteed. 2 2.3 Pyramid Algorithm The 2-category pyramid algorithm <ref> [Gallant, 90] </ref> constructs a network in a manner similar to the tower algorithm, except that each newly added neuron receives input from each of the N input neurons as well as the outputs of all the neurons at each of the preceding layers.
Reference: [Gallant, 93] <author> Gallant, S. I. </author> <year> (1993). </year> <title> Neural Network Learning and Expert Systems. </title> <address> Cam-bridge, MA: </address> <publisher> MIT Press. </publisher>
Reference-contexts: (TLU) or multi-layer perceptrons (MLP) offer a particularly attractive framework for the design of pattern classification and inductive knowledge acquisition systems for a number of reasons including: potential for parallelism and fault tolerance; significant representational and computational efficiency that they offer over disjunctive normal form (DNF) functions and decision trees <ref> [Gallant, 93] </ref>; and simpler digital hardware realizations than their continuous counterparts. A single TLU, also known as perceptron, can be trained to classify a set of input patterns into one of two classes. <p> However when S is not linearly separable, such algorithms behave poorly (i.e., the classification accuracy on the training set can fluctuate wildly from iteration to iteration). Several extensions to the perceptron weight update rule e.g., pocket algorithm <ref> [Gallant, 93] </ref>, thermal perceptron [Frean, 90], loss minimization algorithm [Hrycej, 92], and the barycentric correction procedure [Poulard, 95] are designed to find a reasonably good weight vector that correctly classifies a large fraction of the training set S when S is not linearly separable and converge to zero classification errors when
Reference: [Garey & Johnson, 79] <author> Garey, M., and Johnson, D. </author> <year> (1979). </year> <title> Computers and Intractability. </title> <address> New York: </address> <publisher> W. H. Freeman. </publisher> <pages> 25 </pages>
Reference: [Honavar, 90] <author> Honavar, V. </author> <year> (1990). </year> <title> Generative Learning Structures and Processes for Generalized Connectionist Networks. </title> <type> Ph.D. Thesis. </type> <institution> University of Wisconsin, Madi-son, U.S.A. </institution>
Reference-contexts: The focus of this paper is on constructive or generative learning algorithms that incrementally construct networks of threshold neurons to correctly classify a given (typically non linearly separable) training set. Some of the motivations for studying such algorithms <ref> [Honavar, 90; Honavar & Uhr, 93] </ref> include: * Limitations of learning by weight modification alone within an otherwise a-priori fixed network topology: Weight modification algorithms typically search for a solution weight vector that satisfies some desired performance criterion (e.g., classification error).
Reference: [Honavar & Uhr, 93] <author> Honavar, V., and Uhr, L. </author> <year> (1993). </year> <title> Generative Learning Structures and Processes for Connectionist Networks. </title> <booktitle> Information Sciences 70, </booktitle> <pages> 75-108. </pages>
Reference-contexts: The focus of this paper is on constructive or generative learning algorithms that incrementally construct networks of threshold neurons to correctly classify a given (typically non linearly separable) training set. Some of the motivations for studying such algorithms <ref> [Honavar, 90; Honavar & Uhr, 93] </ref> include: * Limitations of learning by weight modification alone within an otherwise a-priori fixed network topology: Weight modification algorithms typically search for a solution weight vector that satisfies some desired performance criterion (e.g., classification error).
Reference: [Hrycej, 92] <author> Hrycej, T. </author> <year> (1992). </year> <title> Modular Neural Networks. </title> <address> New York: </address> <publisher> Wiley. </publisher>
Reference-contexts: However when S is not linearly separable, such algorithms behave poorly (i.e., the classification accuracy on the training set can fluctuate wildly from iteration to iteration). Several extensions to the perceptron weight update rule e.g., pocket algorithm [Gallant, 93], thermal perceptron [Frean, 90], loss minimization algorithm <ref> [Hrycej, 92] </ref>, and the barycentric correction procedure [Poulard, 95] are designed to find a reasonably good weight vector that correctly classifies a large fraction of the training set S when S is not linearly separable and converge to zero classification errors when S is linearly separable.
Reference: [Mezard & Nadal, 89] <author> Mezard, M., and Nadal, J. </author> <year> (1989). </year> <title> Learning in feed-forward networks: The tiling algorithm. </title> <journal> J. Phys. A: Math. and Gen. </journal> <volume> 22. </volume> <pages> 2191-2203. </pages>
Reference-contexts: A number of constructive algorithms that incrementally construct networks of threshold neurons for 2-category pattern classification tasks have been proposed in the literature. These include the tower, pyramid [Gallant, 90], tiling <ref> [Mezard & Nadal, 89] </ref>, upstart [Frean, 90], and perceptron cascade [Burgess, 94]. <p> The convergence proof for the perceptron cascade thus follows directly from the proof of the upstart algorithm. 2 2.6 Tiling Algorithm The tiling algorithm <ref> [Mezard & Nadal, 89] </ref> constructs a strictly layered network of threshold neurons. The bottom-most layer of neurons receives inputs from each of the N input neurons. The neurons in each subsequent layer receive inputs from the neurons in the layer immediatedly below itself. Each layer maintains a master neuron. <p> The faithfulness criterion simply ensures that no two training examples belonging to different classes produce identical output at any given layer. Faithfulness is clearly a necessary condition for convergence in strictly layered networks <ref> [Mezard & Nadal, 89] </ref>. The proposed extension to multiple output classes involves constructing layers with M master neurons (one for each of the output classes). Sets of one or more ancillary neurons are trained at a time in an attempt to make the current layer faithful.
Reference: [Nilsson, 65] <author> Nilsson, N. </author> <year> (1965). </year> <title> Learning Machines. </title> <address> New York: </address> <publisher> McGraw-Hill. </publisher>
Reference-contexts: If such a weight vector ( ^ W) exists for the pattern set S then S is said to be linearly separable. Several iterative algorithms are available for finding such a ^ W if one exists <ref> [Nilsson, 65; Duda & Hart, 73] </ref>. Most of these are variants of the perceptron weight update rule: W W + (C p O p )X p (where &gt; 0 is the learning rate).
Reference: [Poulard, 95] <author> Poulard, H. </author> <year> (1995). </year> <title> Barycentric Correction Procedure: A Fast Method of Learning Threshold Units. </title> <booktitle> In Proceedings of the World Congress on Neural Networks 95. </booktitle> <address> Washington D.C. </address> <month> July 95, </month> <title> Vol. </title> <booktitle> I. </booktitle> <pages> pp 710-713. </pages>
Reference-contexts: Several extensions to the perceptron weight update rule e.g., pocket algorithm [Gallant, 93], thermal perceptron [Frean, 90], loss minimization algorithm [Hrycej, 92], and the barycentric correction procedure <ref> [Poulard, 95] </ref> are designed to find a reasonably good weight vector that correctly classifies a large fraction of the training set S when S is not linearly separable and converge to zero classification errors when S is linearly separable.
Reference: [Siu et al., 95] <author> Siu, K-Y., Roychowdhury, V., and Kailath, T. </author> <year> (1995). </year> <title> Discrete Neural Computation ATheoretical Foundation. </title> <address> Englewood Cliffs, NJ: </address> <publisher> Prentice Hall. </publisher>
Reference-contexts: For a detailed comparison of the single TLU training algorithms see [Yang et al., 95]. Recently <ref> [Siu et al., 95] </ref> have established the necessary and sufficient conditions for a training set S to be non linearly separable.
Reference: [Yang et al., 95] <author> Yang, J., Parekh, R.G., and Honavar, V.G. </author> <year> (1995). </year> <title> Empirical Comparison of Graceful Variants of the Perceptron Learning Algorithm on Non-Separable Data Sets. </title> <note> In preparation. 26 </note>
Reference-contexts: For a detailed comparison of the single TLU training algorithms see <ref> [Yang et al., 95] </ref>. Recently [Siu et al., 95] have established the necessary and sufficient conditions for a training set S to be non linearly separable. <p> Detailed theoretical and experimental analysis of the performance of single threshold neuron training algorithms is in progress <ref> [Yang et al., 95] </ref>. We expect such analysis to be useful in suggesting near-optimal designs for threshold neuron training algorithms for each of the constructive learning algorithms. A few additional comments on the experimental results presented in section 3 are in order.
References-found: 16

