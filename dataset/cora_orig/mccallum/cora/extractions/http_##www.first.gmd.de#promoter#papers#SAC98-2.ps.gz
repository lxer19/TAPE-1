URL: http://www.first.gmd.de/promoter/papers/SAC98-2.ps.gz
Refering-URL: http://www.first.gmd.de/promoter/papers/index.html
Root-URL: 
Title: Keywords: data parallelism, parallel programming, parallel lan-guages, high-level programming languages  
Abstract: A key issue of problem-oriented parallel programming is an appropriate concept for representing the spatial structures of an application and modelling local or global interactions operating on them. This paper advocates for the use of so-called index spaces as a unified and powerful expression tool. It discusses the interface between application modelling and programming abstractions, and presents their embeddings in the high-level, data parallel language PROMOTER. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> M. Besch, G. Heber, B. Hua, P. Enskonatus, M. Wilhelmi: </author> <title> High-Level Data Parallel Programming in PROMOTER, </title> <booktitle> Proc. of the HIPS97, 11th Int. Parallel Processing Symp., </booktitle> <month> Apr. </month> <pages> 1-5, </pages> <address> Geneva, </address> <year> 1997. </year>
Reference: [2] <author> M. Besch, H. W. Pohl: </author> <title> Communication-Driven Alignment of Sparse Data Structures An Approach Towards Algebraic Mapping, </title> <type> RWCP Technical Report TR-96-014, </type> <address> Japan, </address> <year> 1996. </year>
Reference: [3] <author> M. Besch, H. Bi, G. Heber, M. Kessler, M. Wilhelmi: </author> <title> An Object-Oriented Approach to the Implementation of a High-Level Data Parallel Language,, </title> <booktitle> Proc. of the 1997 Int. Scientific Comp. in OO Parallel Environments Conference ISCOPE97, </booktitle> <address> Marina del Rey, </address> <month> Dec. </month> <year> 1997. </year>
Reference: [4] <author> H. Bi, P. Enskonatus., M. Kessler, M. Sander, M. Wilhelmi: </author> <title> An Object-Oriented Implementation Model for the PROMOTER Language, </title> <type> RWCP Technical Report, </type> <institution> RWC-TR-95-011, </institution> <year> 1996. </year>
Reference: [5] <author> G. E. Blelloch: NESL: </author> <title> A nested data-parallel language, </title> <type> Technical Report CMU-CS-93-129, </type> <institution> Carnegie-Mellon University, </institution> <address> Pittsburgh, PA, </address> <year> 1993. </year>
Reference-contexts: The data parallelism approach relies on some form of collection mechanism to group data on which replicated functions operate as a whole. The most prominent forms of collections in languages including HPF [10], HPC++ [9], NESL <ref> [5] </ref>, V [6], or 81/2 [11] are vectors, matrices, fields, and the like. However, although these structures abstract from low-level details to some extent, they already require a considerable translation of shapes to concrete data structures. In this sense, they are often not flexible enough and thus not problem-oriented.
Reference: [6] <author> M. M. T. Chakravarty, F. W. Schrer, M. Simons: </author> <title> V Nested Parallelism in C*, </title> <booktitle> Proc. of the Working Conference on Massively Parallel Programming Models (MPPM), </booktitle> <publisher> IEEE Computer Society Press, </publisher> <year> 1995. </year>
Reference-contexts: The data parallelism approach relies on some form of collection mechanism to group data on which replicated functions operate as a whole. The most prominent forms of collections in languages including HPF [10], HPC++ [9], NESL [5], V <ref> [6] </ref>, or 81/2 [11] are vectors, matrices, fields, and the like. However, although these structures abstract from low-level details to some extent, they already require a considerable translation of shapes to concrete data structures. In this sense, they are often not flexible enough and thus not problem-oriented.
Reference: [7] <author> J. Gerlach, G. Heber, A. Schramm: </author> <title> Programming Parallel Finite Element Methods in PROMOTER, </title> <booktitle> Proc. Int. EUROSIM Conference on HPCN Challenges in Telecomp and Telecom, </booktitle> <address> Delft, The Netherlands, </address> <year> 1996. </year>
Reference-contexts: From the point of view of an element (triangle), the refinement procedure itself is regular (there is a set of rules) and it is possible to provide an index space (in ) which is appropriate for modern multilevel algorithms <ref> [7] </ref>. Representation of Shapes The representation of a shape as an index space is nothing but a co-ordinization, an introduction of a coordinate system. The set of coordinates ( ) obeys rich algebraic and geometric structures which are closely related.
Reference: [8] <author> W. K. Giloi, M. Kessler, A. Schramm: </author> <title> A High-Level, Massively Parallel Programming Environment and Its Realization, </title> <booktitle> Proc. 1997 Real World Computing Symposium (RWC97), </booktitle> <address> Tokyo, Japan, </address> <month> Jan. </month> <pages> 29-31, </pages> <year> 1997 </year>
Reference: [9] <institution> The HPC++ Working Group: </institution> <type> HPC++ White Paper, Technical Report TR 95633, </type> <institution> Center for Res. on Parallel Computation, Rice University, </institution> <year> 1995. </year>
Reference-contexts: This is especially true if distributed and parallel target systems are to be supported. The data parallelism approach relies on some form of collection mechanism to group data on which replicated functions operate as a whole. The most prominent forms of collections in languages including HPF [10], HPC++ <ref> [9] </ref>, NESL [5], V [6], or 81/2 [11] are vectors, matrices, fields, and the like. However, although these structures abstract from low-level details to some extent, they already require a considerable translation of shapes to concrete data structures.
Reference: [10] <author> J. Merlin, A. Hey: </author> <title> An Introduction to High Performance Fortran, </title> <journal> Scientific Programming, </journal> <volume> vol. 4, </volume> <pages> pp. 87-113, </pages> <year> 1995. </year>
Reference-contexts: This is especially true if distributed and parallel target systems are to be supported. The data parallelism approach relies on some form of collection mechanism to group data on which replicated functions operate as a whole. The most prominent forms of collections in languages including HPF <ref> [10] </ref>, HPC++ [9], NESL [5], V [6], or 81/2 [11] are vectors, matrices, fields, and the like. However, although these structures abstract from low-level details to some extent, they already require a considerable translation of shapes to concrete data structures.
Reference: [11] <author> O. Michel: </author> <title> Design and implementation of 81/2, a declarative data-parallel language, </title> <type> Technical Report 1012, </type> <institution> Laboratoire de Recherche en Informatique, </institution> <month> Dec. </month> <year> 1995. </year>
Reference-contexts: The data parallelism approach relies on some form of collection mechanism to group data on which replicated functions operate as a whole. The most prominent forms of collections in languages including HPF [10], HPC++ [9], NESL [5], V [6], or 81/2 <ref> [11] </ref> are vectors, matrices, fields, and the like. However, although these structures abstract from low-level details to some extent, they already require a considerable translation of shapes to concrete data structures. In this sense, they are often not flexible enough and thus not problem-oriented.
Reference: [12] <institution> The NAS Parallel Benchmarks, </institution> <note> http://science.nas.nasa.gov/Software/NPB/. </note>
Reference-contexts: First experiences proved a good performance and scalability for different problem classes. backward) running on an IBM SP/2 (POWER2/AIX 4.1). To obtain more comparable results, we translated the conjugate gra dient benchmark from the NASPAR benchmark suite <ref> [12] </ref> to PRO MOTER. The kernel employs the multiplication of a sparse matrix with a vector, which is a typical example for unstructured grid com putations with irregular communication.
Reference: [13] <author> Y. Ishikawa, </author> <title> MPC++ Programming Language V1.0 Specification with Commentary Document Version 0.1, </title> <type> Technical Report TR-94014, </type> <institution> Real World Computing Partnership, </institution> <month> Jun. </month> <year> 1994. </year>
Reference-contexts: The system has been installed on our in-house testbed MANNA, on the IBM SP/2, the CRAY T3E, the HITACHI SR2201 and on SUN workstation clusters using either ethernet (MPICH) or the Myrinet hardware (MPC++, MTTL <ref> [13] </ref>) for communication. Several optimi zations concerning the mapping of aggregate objects, data parallel operations and communication scheduling were integrated.
References-found: 13

