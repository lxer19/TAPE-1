URL: http://www.cs.umn.edu/research/darpa/p_sparslib/psp/DOCS/manual.ps
Refering-URL: http://www.cs.umn.edu/research/darpa/p_sparslib/psp/DOCS/
Root-URL: http://www.cs.umn.edu
Title: PSPARSLIB Users Manual: A Portable Library of parallel Sparse Iterative Solvers  
Author: Yousef Saad Gen-Ching Lo Sergey Kuznetsov 
Date: January 18, 1998  
Abstract: PSPARSLIB is a library of parallel iterative solvers. Its primary goal is to provide modules which can be used to simplify the development and implementation of sparse iterative solvers for distributed memory computers. PSPARSLIB solves sparse linear systems which are distributed across processors. The linear system is first partitioned, then split according to the partitioning, a distributed data structure is constructed and, finally, a preconditioned Krylov solver is invoked for its solution. PSPARSLIB provides tools for helping in each of these tasks. PSPARSLIB is written in FORTRAN with a small number of modules in C. This documentation gives some details on the implementation. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> W. Gropp, E. Lusk, and A. Skjellum, </author> <title> Using MPI: Portable Parallel Programming with the Message Passing Interface, </title> <publisher> MIT press, </publisher> <year> 1994. </year>
Reference-contexts: This `distributed sparse linear system' is then solved by a parallel Preconditioned Krylov method. Portability across many machine architectures, and across different communication of networks, is achieved by using the MPI library (see <ref> [1] </ref>). The PSPARSLIB library was tested on various platforms such as the CRAY-T3D, the CRAY-T3E, the IBM SP2, an RS6000 cluster, and an SGI cluster. Most of the routines in this library have been written in FORTRAN 77 with a few C modules. Our test programs utilize dynamic memory allocation.
Reference: [2] <author> B. Hendrickson and R. Leland, </author> <title> An improved spectral graph partitioning algorithm for mapping parallel computations, </title> <type> Tech. Rep. </type> <institution> SAND92-1460, UC-405, Sandia National Laboratories, </institution> <address> Albuquerque, NM, </address> <year> 1992. </year>
Reference-contexts: Users are urged to use more sophisticated partitioners such as the Recursive Spectral Bisection [8], the Chaco package <ref> [2] </ref>, or the Metis package [5]. The output required by the partitioners is simply the mapping of unknowns to processors. All the functional routines (matrix setup, iterative solvers, preconditioners, etc.) are machine-independent and portable across a wide variety of computer architectures.
Reference: [3] <author> S. A. Hutchinson, J. N. Shadid, and R. S. Tuminaro, </author> <title> Aztec user's guide. version 1.0, </title> <type> Tech. Rep. </type> <institution> SAND95-1559, Sandia National Laboratories, </institution> <address> Albuquerque, NM, </address> <year> 1995. </year>
Reference-contexts: Among these are the PETSc project [15], the AZTEC project <ref> [3] </ref>, and the Block-Solve project [4]. All four projects do have some differences in the way they are designed. On the whole, however, it seems that all use the same general data-sctructure as the one used in PSPARSLIB [7, 13, 11, 12].
Reference: [4] <author> M. T. Jones and P. E. Plassmann, </author> <title> BlockSolve95 users manual: Scalable library software for the solution of sparse linear systems, </title> <type> Tech. Rep. </type> <institution> ANL-95/48, Argonne National Lab., Argonne, IL., </institution> <year> 1995. </year>
Reference-contexts: Among these are the PETSc project [15], the AZTEC project [3], and the Block-Solve project <ref> [4] </ref>. All four projects do have some differences in the way they are designed. On the whole, however, it seems that all use the same general data-sctructure as the one used in PSPARSLIB [7, 13, 11, 12].
Reference: [5] <author> G. Karypis, </author> <title> Graph Partitioning and its Applications to Scientific Computing, </title> <type> PhD thesis, </type> <institution> Department of Computer Science, University of Minnesota, Minneapolis, MN, </institution> <year> 1996. </year>
Reference-contexts: Users are urged to use more sophisticated partitioners such as the Recursive Spectral Bisection [8], the Chaco package [2], or the Metis package <ref> [5] </ref>. The output required by the partitioners is simply the mapping of unknowns to processors. All the functional routines (matrix setup, iterative solvers, preconditioners, etc.) are machine-independent and portable across a wide variety of computer architectures.
Reference: [6] <author> G.-C. Lo and Y. Saad, </author> <title> Iterative solution of general sparse linear systems on clusters of workstations, </title> <type> Tech. Rep. </type> <institution> UMSI 96/117 & UM-IBM 96/24, Minnesota Supercomputer Institute, University of Minnesota, Minneapolis, MN, </institution> <year> 1996. </year>
Reference-contexts: It is interesting to note that these routines are identical 2 with those in SPARSKIT [9]. Portability of these exact same codes from sequential to parallel platforms is made possible thanks to an implementation referred to as `Reverse Communication', see <ref> [6] </ref> for details. The test examples provided use mainly FGMRES, the flexible GMRES, as the Krylov subspace accelerator. FGMRES is a right-preconditioned variant of GMRES which allows the preconditioning to vary at each step. For further details on the algorithm, see [10]. <p> Preconditioned GMRES is used as a local solver. The action of the inverse of the Schur complement matrix on a vector is obtained at the expense of solving a system involving all unknowns in subdomain. The complete description of the Schur complement technique implemented in PSPARSLIB is given in <ref> [6] </ref>. All the test programs in GENERAL read some input data from the short data file 'inputs'.
Reference: [7] <author> S. Ma and Y. Saad, </author> <title> Distributed ILU(0) and SOR preconditioners for unstructured sparse linear systems, </title> <type> Tech. Rep. 94-027, </type> <institution> Army High Performance Computing Research Center, University of Minnesota, Minneapolis, MN, </institution> <year> 1994. </year>
Reference-contexts: Among these are the PETSc project [15], the AZTEC project [3], and the Block-Solve project [4]. All four projects do have some differences in the way they are designed. On the whole, however, it seems that all use the same general data-sctructure as the one used in PSPARSLIB <ref> [7, 13, 11, 12] </ref>. In addition, it seems that the general concepts in these new packages are the same or similar to those introduced in [11, 12]. One major difference seems to be the emphasis on objet oriented programming, particularly in PETSc.
Reference: [8] <author> A. Pothen, H. D. Simon, and K. P. Liou, </author> <title> Partitioning sparse matrices with eigenvectors of graphs, </title> <journal> SIAM Journal on Matrix Analysis and Applications, </journal> <volume> 11 (1990), </volume> <pages> pp. 430-452. </pages>
Reference-contexts: Users are urged to use more sophisticated partitioners such as the Recursive Spectral Bisection <ref> [8] </ref>, the Chaco package [2], or the Metis package [5]. The output required by the partitioners is simply the mapping of unknowns to processors. All the functional routines (matrix setup, iterative solvers, preconditioners, etc.) are machine-independent and portable across a wide variety of computer architectures.

Reference: [13] <author> Y. Saad and A. Malevsky, PSPARSLIB: </author> <title> A portable library of distributed memory sparse iterative solvers, </title> <booktitle> in Proceedings of Parallel Computing Technologies (PaCT-95), 3-rd international conference, </booktitle> <address> St. Petersburg, Russia, </address> <month> Sept. </month> <year> 1995, </year> <editor> V. E. M. et al., ed., </editor> <year> 1995. </year>
Reference-contexts: One of the most expensive operations in a parallel iterative solver is the matrix-vector product for distributed matrices. This operation is provided in the package. The detailed description of matrix-vectir operations along with the data structure for storing distributed sparse matrices are given in <ref> [13] </ref>. 2.2 Preconditioners On the preconditioning side, we have implemented a few of the existing techniques based on domain decomposition ideas. These include the block-Jacobi method (additive Schwarz), multi-color block SOR (multiplicative Schwarz), and Schur complement techniques. In particular, we have developed and implemented a two-level multi-color block SOR. <p> The complete description of the data structure associated with this boundary information is given in <ref> [13] </ref> along with additional implementation details. scale.f contains SPARSKIT routines for scaling of a matrix in CSR format. <p> Among these are the PETSc project [15], the AZTEC project [3], and the Block-Solve project [4]. All four projects do have some differences in the way they are designed. On the whole, however, it seems that all use the same general data-sctructure as the one used in PSPARSLIB <ref> [7, 13, 11, 12] </ref>. In addition, it seems that the general concepts in these new packages are the same or similar to those introduced in [11, 12]. One major difference seems to be the emphasis on objet oriented programming, particularly in PETSc.
Reference: [14] <author> Y. Saad and M. Sosonkina, </author> <title> Distributed Schur complement techniques for general sparse linear systems, </title> <type> Tech. Rep. </type> <institution> UMSI 97/159, Minnesota Supercomputer Institute, University of Minnesota, Minneapolis, MN, </institution> <year> 1997. </year>
Reference-contexts: Note that overlapping is allowed, i.e., the subsets are not necessarily disjoint. A description of the files in this directory follows. dd-jac.f Driver for a Block-Jacobi preconditioned FGMRES. dd-sor.f Driver for a Multi-color SOR preconditioned FGMRES. dd-sch.f Driver for a Schur-ILU preconditioned FGMRES <ref> [14] </ref>. schur.f Driver for a standard Schur complement technique. Implements an overlapping FGMRES iteration on the reduced system without preconditioning. Preconditioned GMRES is used as a local solver. <p> Even though not required the functions c and f must be defined (as dummy functions). A description of the files in this directory follows. dd-jac.f Driver for a Block-Jacobi preconditioned FGMRES. dd-sor.f Driver for a Multi-color SOR preconditioned FGMRES. dd-sch.f Driver for a Schur-ILU preconditioned FGMRES <ref> [14] </ref>. functs.f contains the functions and subroutines needed to define the elliptic PDE from which are generated the test matrices for the drivers in TESTS/GRIDS. Thus, the test drivers in this directory mirror those in TESTS/GENERAL except that the standard Schur complement method is omitted.
Reference: [15] <author> B. Smith, W. D. Gropp, and L. C. McInnes, </author> <title> PETSc 2.0 user's manual, </title> <type> Tech. Rep. </type> <institution> ANL-95/11, Argonne National Laboratory, Argonne, IL, </institution> <month> July </month> <year> 1995. </year> <month> 14 </month>
Reference-contexts: Among these are the PETSc project <ref> [15] </ref>, the AZTEC project [3], and the Block-Solve project [4]. All four projects do have some differences in the way they are designed. On the whole, however, it seems that all use the same general data-sctructure as the one used in PSPARSLIB [7, 13, 11, 12].
References-found: 11

