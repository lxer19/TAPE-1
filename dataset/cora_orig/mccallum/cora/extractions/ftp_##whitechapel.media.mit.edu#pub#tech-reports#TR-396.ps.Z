URL: ftp://whitechapel.media.mit.edu/pub/tech-reports/TR-396.ps.Z
Refering-URL: http://www.cs.gatech.edu/computing/classes/cs7322_98_spring/readings.html
Root-URL: 
Email: fnuria,sandyg@media.mit.edu  francois.berard@imag.fr  
Title: LAFTER: Lips and Face Real Time Tracker  
Author: Nuria Oliver, Alex P. Pentland Francois Berard 
Address: Cambridge, MA 02139, USA  BP 53 38041 Grenoble cedex 9 France  
Affiliation: Vision And Modeling Group MIT Media Laboratory  CLIPS-IMAG,  
Abstract: This paper describes an active-camera real-time system for tracking, shape description, and classification of the human face and mouth using only an SGI Indy computer. The system is based on use of 2-D blob features, which are spatially-compact clusters of pixels that are similar in terms of low-level image properties. Patterns of behavior (e.g., facial expressions and head movements) can be classified in real-time using Hidden Markov Model (HMM) methods. The system has been tested on hundreds of users and has demonstrated extremely reliable and accurate performance. Typical classification accuracies are near 100%. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> K. Aizawa and T. Huang. </author> <title> Model-based image-coding: Advanced video coding techniques for very-low bit-rate applications. </title> <journal> PIEEE, </journal> <volume> 83(2) </volume> <pages> 259-271, </pages> <month> February </month> <year> 1995. </year>
Reference-contexts: It is well-known that people are most sensitive to coding errors in facial features. Thus it makes sense to use a more accurate (and more expensive) coding algorithm for the facial features, and a less accurate (and cheaper) algorithm for the remaining image data <ref> [11, 24, 1] </ref>. Because the location of these features is detected by our system, we can make use of this coding scheme. 8.
Reference: [2] <author> S. Ayer and H. Sawhney. </author> <title> Layered representation of motion video using robust maximum-likelihood estimation of mixture models and mdl encoding. </title> <booktitle> In ICCV95, </booktitle> <pages> pages 777-784, </pages> <year> 1995. </year>
Reference-contexts: These are then clustered so that image properties such as color and spatial similarity combine to form coherent connected regions, or blobs, in which all the pixels have similar image properties. This blob description method is, in fact, a special case of recent Minimum Description Length (MDL) algorithms <ref> [9, 8, 2] </ref>. We have used essentially the same technique for real-time tracking of people in color video [34]. In that application the spatial coordinates are combined with color and brightness channels to form a four-element feature vector at each point (x; y; r r+g+b ). <p> This approach can easily be seen to be a special case of the MDL segmentation algorithms developed by Darrell and Pentland [9, 8] and later by Ayer and Sawhney <ref> [2] </ref>. 2.5. Kalman filtering To ensure stability of the MAP segmentation process, the spatial parameters for each blob model are filtered using a zero-order Kalman filter.
Reference: [3] <author> A. Azarbayejani, T. Starner, B. Horowitz, and A. Pentland. </author> <title> Visually controlled graphics. </title> <journal> PAMI, </journal> <volume> 15(6) </volume> <pages> 602-605, </pages> <month> June </month> <year> 1993. </year>
Reference: [4] <author> M. Black and Y. Yacoob. </author> <title> Tracking and recognizing rigid and non-rigid facial motions using local parametric models of image motion. </title> <booktitle> volume 1, </booktitle> <pages> pages 374-381. </pages> <publisher> IEEE, </publisher> <year> 1995. </year>
Reference: [5] <author> A. Bobick and R. Bolles. </author> <title> The representation space paradigm of concurrent evolving object descriptions. </title> <journal> PAMI, </journal> <volume> 14(2) </volume> <pages> 146-156, </pages> <month> February </month> <year> 1992. </year>
Reference-contexts: All of the experimental apparatus described here is real-time, at 20 to 30 frames per second, and runs on SGI Indy workstations without any special-purpose hardware. The notion of blobs as a representation for image features has a long history in computer vision <ref> [25, 19, 5, 34] </ref>, and has had many different mathematical definitions. In our usage it is a compact set of pixels that share a visual property that is not not shared by the surrounding pixels.
Reference: [6] <author> C. Bregler and S. Omohundro. </author> <title> Advances in Neural Information Processing Systems, chapter Surface Learning with Applications to Lipreading. </title> <publisher> Morgan Kaufmann, </publisher> <address> San Fran-cisco, CA, </address> <year> 1994. </year>
Reference: [7] <author> J. Crowley and F. Berard. </author> <title> Multi-modal tracking for video-communication. </title> <address> Puerto Rico, </address> <month> June </month> <year> 1997. </year> <month> CVPR. </month>
Reference-contexts: A generalized version of this technique is employed in <ref> [7] </ref> for fusing several concurrent observations. 2.6. Continuous real-time HMMs Our approach to temporal interpretation of facial expressions uses Hidden Markov Models (HMMs) [28] to recognize different patterns of mouth movement. HMMs are one of the basic probabilistic tools used for time series modeling.
Reference: [8] <author> T. Darrell and A. Pentland. </author> <title> Cooperative robust estimation using layers of support. </title> <journal> PAMI, </journal> <volume> 17(5) </volume> <pages> 474-487, </pages> <month> May </month> <year> 1995. </year>
Reference-contexts: These are then clustered so that image properties such as color and spatial similarity combine to form coherent connected regions, or blobs, in which all the pixels have similar image properties. This blob description method is, in fact, a special case of recent Minimum Description Length (MDL) algorithms <ref> [9, 8, 2] </ref>. We have used essentially the same technique for real-time tracking of people in color video [34]. In that application the spatial coordinates are combined with color and brightness channels to form a four-element feature vector at each point (x; y; r r+g+b ). <p> Once the class memberships have been determined, the statistics of each class are then updated via the EM algorithm, as described above. This approach can easily be seen to be a special case of the MDL segmentation algorithms developed by Darrell and Pentland <ref> [9, 8] </ref> and later by Ayer and Sawhney [2]. 2.5. Kalman filtering To ensure stability of the MAP segmentation process, the spatial parameters for each blob model are filtered using a zero-order Kalman filter.
Reference: [9] <author> T. Darrell, S. Sclaroff, and A. Pentland. </author> <title> Segmentation by minimal description. </title> <booktitle> In ICCV90, </booktitle> <pages> pages 173-177, </pages> <year> 1990. </year>
Reference-contexts: These are then clustered so that image properties such as color and spatial similarity combine to form coherent connected regions, or blobs, in which all the pixels have similar image properties. This blob description method is, in fact, a special case of recent Minimum Description Length (MDL) algorithms <ref> [9, 8, 2] </ref>. We have used essentially the same technique for real-time tracking of people in color video [34]. In that application the spatial coordinates are combined with color and brightness channels to form a four-element feature vector at each point (x; y; r r+g+b ). <p> Once the class memberships have been determined, the statistics of each class are then updated via the EM algorithm, as described above. This approach can easily be seen to be a special case of the MDL segmentation algorithms developed by Darrell and Pentland <ref> [9, 8] </ref> and later by Ayer and Sawhney [2]. 2.5. Kalman filtering To ensure stability of the MAP segmentation process, the spatial parameters for each blob model are filtered using a zero-order Kalman filter.
Reference: [10] <author> A. Dempster, N. Laird, and D. Rubin. </author> <title> Maximum likelihood from incomplete data via de em algorithm. </title> <journal> Journal of the Royal Statistical Society, </journal> <volume> 39-B:1-38, </volume> <year> 1977. </year>
Reference-contexts: Maximum Likelihood Estimation The blob features are modeled as a mixture of Gaussian distributions in the color (or texture, motion, etc.) space. The algorithm that is generally employed for learning the parameters of such a mixture model is the Expectation-Maximization (EM) algorithm of Dempster et al <ref> [10] </ref>, [29]. In our system the input data vector d is the normalized R,G,B content of the pixels in the image. The color distribution of each of our blobs is modeled as a mixture of Gaussian Probability Distribution Functions (PDF's) that are iteratively estimated using EM.
Reference: [11] <author> A. Eleftheriadis and A. Jacquin. </author> <title> Model-assisted coding of video teleconferencing sequences at low bit rates. </title> <booktitle> In ISCAS, </booktitle> <month> May-June </month> <year> 1994. </year>
Reference-contexts: It is well-known that people are most sensitive to coding errors in facial features. Thus it makes sense to use a more accurate (and more expensive) coding algorithm for the facial features, and a less accurate (and cheaper) algorithm for the remaining image data <ref> [11, 24, 1] </ref>. Because the location of these features is detected by our system, we can make use of this coding scheme. 8.
Reference: [12] <author> W. Ellis. </author> <title> A source book of gestalt psychology. In Harcourt, </title> <publisher> Brace and Co., </publisher> <year> 1939. </year>
Reference-contexts: Mathematical Framework The notion of grouping atomic parts of a scene together to form blob-like entities based on proximity and visual appearance is a natural one, and has been of interest to visual scientists since the Gestalt psychologists studied grouping criteria early in this century <ref> [12] </ref>. In modern computer vision processing we seek to group pixels of images together and to segment images based on visual coherence, but the features obtained from such efforts are usually taken to be the boundaries, or contours, of these regions rather than the regions themselves.
Reference: [13] <author> I. Essa and A. Pentland. </author> <title> Facial expression recognition using a dynamic model and motion energy. </title> <booktitle> In ICCV95, </booktitle> <pages> pages 360-367, </pages> <year> 1995. </year>
Reference: [14] <author> W. Gaver, G. Smets, and K. Overbeeke. </author> <title> A virtual window on media space. </title> <booktitle> CHI, </booktitle> <year> 1995. </year>
Reference-contexts: In informal teleconferencing testing, users have confirmed that this capability significantly improves the usability of the teleconferencing system. We can also use the system in a `Virtual Window' mode <ref> [14] </ref>: as the user moves in front of his local camera, the distant motorized camera is moved in the same way. In informal tests, users have said that the LAFTER-based virtual window system gives a good sense of the distant space. 7.2.
Reference: [15] <author> H. Hennecke, K. Venkatesh, and D. Stork. </author> <title> Using deformable templates to infer visual speech dynamics. </title> <type> Technical Report 9430, </type> <institution> California Research Center, </institution> <month> June </month> <year> 1994. </year>
Reference: [16] <author> H. Hunke. </author> <title> Locating and tracking of human faces with neural networks. </title> <type> Technical report, </type> <address> CMU, Pittsburgh PA, </address> <month> August </month> <year> 1994. </year>
Reference-contexts: This approach has advantages over correlation or eigenspace methods, such as speed and rotation invariance under constant illumination conditions. Moreover, our own work [34], or that of Schiele et al or Hunke et al <ref> [31, 16] </ref> have shown that use of normalized or chromatic color information ( r r+g+b ; g reliably used for finding 'flesh areas' present in the scene despite wide variations in lighting.
Reference: [17] <author> T. Jebara and A. Pentland. </author> <title> Parametrized structure from motion for 3d adaptive feedback tracking of faces. </title> <type> Technical Report 401, </type> <institution> MIT Media Lab, </institution> <address> Cambridge MA, </address> <year> 1996. </year> <note> To appear in Proceedings of IEEE CVPR'97. </note>
Reference-contexts: determine the mixture parameters of each of the blobs, the unsupervised EM clustering algorithm is computed off-line on hundreds of images of the different classes to be modeled (in our case, face, lips and interior of the mouth), in a similar way as is done for skin color modeling in <ref> [17] </ref>. When a new frame is available the likelihood of each pixel is computed using the learned mixture model and compared to a likelihood threshold. Only those pixels whose likelihood is above the threshold are classified as belonging to the model. 2.3.
Reference: [18] <author> M. Kass, A. Witkin, and D. Terzopolous. Snakes: </author> <title> active contour models. </title> <journal> International Journal of Computer Vision, </journal> <volume> 1 </volume> <pages> 321-331, </pages> <month> January </month> <year> 1988. </year>
Reference: [19] <author> R. Kauth, A. Pentland, and G. Thomas. Blob: </author> <title> An unsupervised clustering approach to spatial preprocessing of mss imagery. </title> <booktitle> In 11th Int'l Symp. on Remote Sensing of the Environment, </booktitle> <address> Ann Harbor MI, </address> <year> 1977. </year>
Reference-contexts: All of the experimental apparatus described here is real-time, at 20 to 30 frames per second, and runs on SGI Indy workstations without any special-purpose hardware. The notion of blobs as a representation for image features has a long history in computer vision <ref> [25, 19, 5, 34] </ref>, and has had many different mathematical definitions. In our usage it is a compact set of pixels that share a visual property that is not not shared by the surrounding pixels. <p> In very complex scenes, such as those containing people or natural objects, contour features often prove unreliable and difficult to find and use. The blob representation that we use was developed by Pentland and Kauth et al <ref> [25, 19] </ref> as a way of extracting an extremely compact, structurally meaningful description of multi-spectral satellite (MSS) imagery. In this method feature vectors at each pixel are formed by adding (x; y) spatial coordinates to the spectral (or textural) components of the imagery.
Reference: [20] <author> R. Magnolfi and P. Nesi. </author> <title> Analysis and synthesis of facial motions. </title> <booktitle> volume 1, </booktitle> <pages> pages 308-313, </pages> <address> Zurich, </address> <year> 1995. </year> <title> International Workshop on Automatic Face and Gesture Recognition. </title>
Reference: [21] <author> K. Matsuno, C. Lee, S. Kimura, and S. Tsuji. </author> <title> Automatic recognition of human facial expressions. </title> <booktitle> In CVPR, </booktitle> <volume> volume 1, </volume> <pages> pages 352-359. </pages> <publisher> IEEE, </publisher> <year> 1995. </year>
Reference: [22] <author> S. </author> <title> Morishima. </title> <booktitle> Emotion model. </booktitle> <pages> pages 284-289, </pages> <address> Zurich, </address> <year> 1995. </year> <title> International Workshop on Automatic Face and Gesture Recognition. </title>
Reference-contexts: Mouth shape The mouth shape is characterized by its area, its spatial eigenvalues (e.g., width and height) and its bounding box. The use of this feature vector to classify facial expressions has been suggested by psychological experiments <ref> [36, 22] </ref>, which examined the most important discriminative features for expression classification. Rotation invariance is achieved by computing the face's image-plane rotation angle and rotating the region of interest with the negative of this angle.
Reference: [23] <author> Y. Moses, D. Reynard, and A. Blake. </author> <title> Determining facial expressions in real time. </title> <booktitle> volume 1, </booktitle> <pages> pages 332-337, </pages> <address> Zurich, </address> <year> 1995. </year> <title> International Workshop on Automatic Face and Gesture Recognition. </title>
Reference-contexts: In recent years, much research has been done on machine recognition of human facial expressions. Feature points ([3]), physical skin and muscle activation models ([21], [33], [30]), optical flow models ([13] ), feature based models using manually selected features ([26]), local parametrized optical flow ([4]), deformable contours ([20], <ref> [23] </ref>), combined with optical flow ([35]) as well as deformable templates ([18],[37],[15],[6]) among several other techniques have been used for facial expression analysis. This paper extends these previous efforts to real-time analysis of the human face using our blob tracking methodology.
Reference: [24] <author> K. Ohzeki, T. Saito, M. Kaneko, and H. Harashima. </author> <title> Interactive model-based coding of facial image sequencewith a new motion detection algorithm. </title> <journal> IEICE, </journal> <volume> E79B(10):1474-1483, </volume> <month> October </month> <year> 1996. </year>
Reference-contexts: It is well-known that people are most sensitive to coding errors in facial features. Thus it makes sense to use a more accurate (and more expensive) coding algorithm for the facial features, and a less accurate (and cheaper) algorithm for the remaining image data <ref> [11, 24, 1] </ref>. Because the location of these features is detected by our system, we can make use of this coding scheme. 8.
Reference: [25] <author> A. Pentland. </author> <title> Classification by clustering. </title> <booktitle> In IEEE Symp. on Machine Processing and Remotely Sensed Data, Purdue, IN, </booktitle> <year> 1976. </year>
Reference-contexts: All of the experimental apparatus described here is real-time, at 20 to 30 frames per second, and runs on SGI Indy workstations without any special-purpose hardware. The notion of blobs as a representation for image features has a long history in computer vision <ref> [25, 19, 5, 34] </ref>, and has had many different mathematical definitions. In our usage it is a compact set of pixels that share a visual property that is not not shared by the surrounding pixels. <p> In very complex scenes, such as those containing people or natural objects, contour features often prove unreliable and difficult to find and use. The blob representation that we use was developed by Pentland and Kauth et al <ref> [25, 19] </ref> as a way of extracting an extremely compact, structurally meaningful description of multi-spectral satellite (MSS) imagery. In this method feature vectors at each pixel are formed by adding (x; y) spatial coordinates to the spectral (or textural) components of the imagery.
Reference: [26] <author> I. Pilowsky, M. Thornton, and B. </author> <title> Stokes. Aspects of face processing, chapter Towards the quantification of facial expressions with the use of a mathematics model of the face, </title> <address> pages 340-348. </address>
Reference: [27] <author> C. Priebe. </author> <title> Adaptive mixtures. </title> <journal> Journal of the American Statistical Association, </journal> <volume> 89(427), </volume> <year> 1994. </year>
Reference: [28] <author> L. Rabiner and B. Juang. </author> <title> An introduction to hidden markov models. </title> <journal> IEEE ASSP Magazine, </journal> <pages> pages 4-16, </pages> <month> January </month> <year> 1986. </year>
Reference-contexts: A generalized version of this technique is employed in [7] for fusing several concurrent observations. 2.6. Continuous real-time HMMs Our approach to temporal interpretation of facial expressions uses Hidden Markov Models (HMMs) <ref> [28] </ref> to recognize different patterns of mouth movement. HMMs are one of the basic probabilistic tools used for time series modeling. HMMs fall into our Bayesian framework with the addition of time in the feature vector. They offer dynamic time warping, an efficient learning algorithm and clear Bayesian semantics.
Reference: [29] <author> R. Redner and H. Walker. </author> <title> Mixture densities, maximum likelihood and the em algorithm. </title> <journal> SIAM Review, </journal> <volume> 26 </volume> <pages> 195-239, </pages> <year> 1984. </year>
Reference-contexts: Maximum Likelihood Estimation The blob features are modeled as a mixture of Gaussian distributions in the color (or texture, motion, etc.) space. The algorithm that is generally employed for learning the parameters of such a mixture model is the Expectation-Maximization (EM) algorithm of Dempster et al [10], <ref> [29] </ref>. In our system the input data vector d is the normalized R,G,B content of the pixels in the image. The color distribution of each of our blobs is modeled as a mixture of Gaussian Probability Distribution Functions (PDF's) that are iteratively estimated using EM.
Reference: [30] <author> M. Rydfalk. CANDIDE: </author> <title> A parametrized face. </title> <type> PhD thesis, </type> <institution> Link opnig University, EE Depart., </institution> <month> Oct </month> <year> 1987. </year>
Reference-contexts: In recent years, much research has been done on machine recognition of human facial expressions. Feature points ([3]), physical skin and muscle activation models ([21], [33], <ref> [30] </ref>), optical flow models ([13] ), feature based models using manually selected features ([26]), local parametrized optical flow ([4]), deformable contours ([20], [23]), combined with optical flow ([35]) as well as deformable templates ([18],[37],[15],[6]) among several other techniques have been used for facial expression analysis.
Reference: [31] <author> B. Schiele and A. Waibel. </author> <title> Gaze tracking based on face color. </title> <booktitle> In International Workshop on Automatic Face and Gesture Recognition, </booktitle> <pages> pages 344-349, </pages> <year> 1995. </year>
Reference-contexts: This approach has advantages over correlation or eigenspace methods, such as speed and rotation invariance under constant illumination conditions. Moreover, our own work [34], or that of Schiele et al or Hunke et al <ref> [31, 16] </ref> have shown that use of normalized or chromatic color information ( r r+g+b ; g reliably used for finding 'flesh areas' present in the scene despite wide variations in lighting.
Reference: [32] <author> T. Starner and A. Pentland. </author> <title> Real-time asl recognition from video using hmm's. </title> <type> Technical Report 375, </type> <institution> MIT, Media Laboratory, MIT, Media Laboratory, </institution> <address> Cambridge, MA 02139, </address> <year> 1996. </year>
Reference-contexts: These properties are particularly important in applications that require tracking people, and recently we have used 2-D blob tracking for real-time whole-body human interfaces [34] and real-time recognition of American Sign Language hand gestures <ref> [32] </ref>. In recent years, much research has been done on machine recognition of human facial expressions.
Reference: [33] <author> K. Waters. </author> <title> A muscle model for animating three-dimensional facial expression. </title> <booktitle> volume 21(4), </booktitle> <pages> pages 17-23. </pages> <booktitle> ACM SIG-GRAPH Conf. Proceedings, </booktitle> <year> 1987. </year>
Reference-contexts: In recent years, much research has been done on machine recognition of human facial expressions. Feature points ([3]), physical skin and muscle activation models ([21], <ref> [33] </ref>, [30]), optical flow models ([13] ), feature based models using manually selected features ([26]), local parametrized optical flow ([4]), deformable contours ([20], [23]), combined with optical flow ([35]) as well as deformable templates ([18],[37],[15],[6]) among several other techniques have been used for facial expression analysis.
Reference: [34] <author> C. Wren, A. Azarbayejani, T. Darrell, and A. Pentland. Pfinder: </author> <title> Real-time tracking of the human body. </title> <booktitle> In Photonics East, SPIE, </booktitle> <volume> volume 2615, </volume> <pages> Bellingham, </pages> <address> WA, </address> <year> 1995. </year>
Reference-contexts: All of the experimental apparatus described here is real-time, at 20 to 30 frames per second, and runs on SGI Indy workstations without any special-purpose hardware. The notion of blobs as a representation for image features has a long history in computer vision <ref> [25, 19, 5, 34] </ref>, and has had many different mathematical definitions. In our usage it is a compact set of pixels that share a visual property that is not not shared by the surrounding pixels. <p> These properties are particularly important in applications that require tracking people, and recently we have used 2-D blob tracking for real-time whole-body human interfaces <ref> [34] </ref> and real-time recognition of American Sign Language hand gestures [32]. In recent years, much research has been done on machine recognition of human facial expressions. <p> This blob description method is, in fact, a special case of recent Minimum Description Length (MDL) algorithms [9, 8, 2]. We have used essentially the same technique for real-time tracking of people in color video <ref> [34] </ref>. In that application the spatial coordinates are combined with color and brightness channels to form a four-element feature vector at each point (x; y; r r+g+b ). These were then clustered into blobs to drive a connected-blob representation of the person. <p> Automatic Face Detection and Tracking Our approach to the face finding problem uses coarse color and size/shape information. This approach has advantages over correlation or eigenspace methods, such as speed and rotation invariance under constant illumination conditions. Moreover, our own work <ref> [34] </ref>, or that of Schiele et al or Hunke et al [31, 16] have shown that use of normalized or chromatic color information ( r r+g+b ; g reliably used for finding 'flesh areas' present in the scene despite wide variations in lighting.
Reference: [35] <author> Y. Yacoob and L. Davis. </author> <title> Recognizing human facial expressions from long image sequences using optical-flow. </title> <journal> PAMI, </journal> <volume> 18(6) </volume> <pages> 636-642, </pages> <month> June </month> <year> 1996. </year>
Reference: [36] <author> H. Yamada. </author> <title> Dimensions of visual information for categorizing facial expressions of emotion. </title> <journal> Japanese Psychological Research, </journal> <volume> 35(4), </volume> <year> 1993. </year>
Reference-contexts: Mouth shape The mouth shape is characterized by its area, its spatial eigenvalues (e.g., width and height) and its bounding box. The use of this feature vector to classify facial expressions has been suggested by psychological experiments <ref> [36, 22] </ref>, which examined the most important discriminative features for expression classification. Rotation invariance is achieved by computing the face's image-plane rotation angle and rotating the region of interest with the negative of this angle.
Reference: [37] <author> A. Yuille, P. Hallinan, and D. Cohen. </author> <title> Feature extraction from faces using deformable templates. </title> <journal> Journal of Computer Vision, </journal> <pages> pages 99-111, </pages> <year> 1992. </year>
References-found: 37

