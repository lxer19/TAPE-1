URL: http://www.cs.huji.ac.il/~feit/parsched/p-97-7.ps.gz
Refering-URL: http://www.cs.huji.ac.il/~feit/parsched/parsched97.html
Root-URL: http://www.cs.huji.ac.il
Email: email: brecht@cs.yorku.ca  
Title: An Experimental Evaluation of Processor Pool-Based Scheduling for Shared-Memory NUMA Multiprocessors  
Author: Timothy B. Brecht 
Web: URL: http://www.cs.yorku.ca/~brecht  
Address: 4700 Keele Street, North York, Ontario, CANADA M3J 1P3  
Affiliation: Department of Computer Science, York University  
Abstract: The quest for scalable shared-memory multiprocessors has lead to the design and implementation of NUMA (Non-Uniform Memory Access) architectures, in which the time required to access memory depends on the relative locations of the processor requesting the access and the memory module being addressed. Access times are non-uniform because memory modules are distributed throughout the system. The effective and efficient execution of parallel applications on such systems depends upon the ability of the architecture, operating system, run-time system, compiler, and applications to reduce, hide and tolerate remote memory access latencies whenever possible. We believe that the operating system scheduler plays a key role in ensuring that latencies due to remote memory accesses are not unnecessarily introduced nor unnecessarily increased [51][6]. In this paper we describe the design, implementation and experimental evaluation of a technique for operating system schedulers, called processor pool-based scheduling [51], that is designed to assign processes (or kernel threads) of parallel applications to processors in multiprogrammed, shared-memory NUMA multiprocessors. Our implementation and experimental results demonstrate that: 1) Pool-based scheduling is an effective method for localizing application execution and reducing mean response times. 2) Although application parallelism should be considered, the optimal pool size is a function of the the system architecture. 3) The strategies of placing new applications in a pool with the largest potential for in-pool growth and of isolating applications from each other are desirable properties of algorithms for assigning threads of a parallel application to pools and the ``Worst-Fit'' policy we examine incorporates both of these properties. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> A. Agarwal, D. Chaiken, K. Johnson, D. Kranz, J. Kubiatowicz, K. Kurihara, B. Lim, G. Maa, and D. Nussbaum, </author> <title> ``The MIT Alewife Machine: A Large-Scale Distributed-Memory Multiprocessor'', Scalable Shared Memory Multiprocessors, </title> <editor> ed. M. Dubois and S. S. Thakkar, </editor> <publisher> Kluwer Academic Publishers, Norwell, Massachusetts, </publisher> <pages> pp. 239-261, </pages> <year> 1991. </year>
Reference-contexts: Such systems, called NUMA (Non-Uniform Memory Access) multiprocessors, are a departure from the more common single bus-based UMA (Uniform Memory Access) multiprocessors. Some examples of NUMA multiprocessors include the University of Toronto's Hector [48], MIT's Alewife <ref> [1] </ref>, Stanford's DASH and FLASH [23][21], Kendall Square Research's KSR1 [9], SUN Microsystem's S3.mp [31], the Convex Exemplar SPP1000/1200 [13] and the Sequent STiNG [25]. The proliferation of more scalable, shared-memory multiprocessors presents new opportunities for the users and new challenges for the designers of such systems. <p> In the case of the University of Toronto's Hector system [48] as well as in Stanford's DASH [23] and FLASH systems [21], pools may be formed by grouping the processors of an individual station or cluster. In the MIT Alewife multiprocessor <ref> [1] </ref>, pools might be chosen to consist of the four processors forming the smallest component of the mesh interconnect or a slightly larger set of nearest neighbour processors.
Reference: [2] <author> I. Ahmad and A. Ghafoor, </author> <title> ``Semi-Distributed Load Balancing for Massively Parallel Multicomputer Systems'', </title> <journal> IEEE Tr ansactions on Software Engineering, </journal> <volume> Vol. 17, No. 10, </volume> <pages> pp. 987-1004, </pages> <month> October, </month> <year> 1991. </year>
Reference: [3] <author> F. Bellosa, </author> <title> ``Locality-Information-Based Scheduling in Shared-Memory Multiprocessors'', </title> <booktitle> Proceedings of the 2nd IPPS Workshop on Job Scheduling Strategies for Parallel Processing, </booktitle> <pages> pp. 171-181, </pages> <address> Honolulu, Hawaii, </address> <month> April, </month> <year> 1996. </year>
Reference: [4] <author> D. L. Black, </author> <title> ``Scheduling Support for Concurrency and Parallelism in the Mach Operating System'', </title> <booktitle> IEEE Computer, </booktitle> <pages> pp. 35-43, </pages> <month> May, </month> <year> 1990. </year>
Reference-contexts: Using a sixteen processor DASH system they found that while a sequential workload benefited significantly from the improved locality, this approach did not improve execution times when compared with the baseline UNIX scheduler for parallel workloads. They also compare the use of gang scheduling [32], processor sets <ref> [4] </ref>, and process control [43] scheduling policies for executing parallel workloads. While they exploit cluster level locality in their implementations of each of these policies, they do not fully explore the strategies used in exploiting locality for parallel workloads nor how effective these strategies are at localization.
Reference: [5] <author> W. Bolosky, M. Scott, R. Fitzgerald, and A. Cox, </author> <title> ``NUMA Policies and their Relationship to Memory Architecture'', </title> <booktitle> Proceedings of the International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pp. 212-221, </pages> <month> April, </month> <year> 1991. </year>
Reference: [6] <author> T. Brecht, </author> <title> ``On the Importance of Parallel Application Placement in NUMA Multiprocessors'', </title> <booktitle> Proceedings of the Fourth Symposium on Experiences with Distributed and Multiprocessor Systems (SEDMS IV), </booktitle> <pages> pp. 1-18, </pages> <address> San Diego, CA, </address> <month> September, </month> <year> 1993. </year>
Reference-contexts: Note that the delay switches have no affect on local or on-station requests and that with the delay switches set to 16 cycles, the off-station access times for Hector are still below those of other systems that contain faster processors, more processors or mechanisms for hardware cache-coherence <ref> [6] </ref>. Off station latencies in DASH and remote node latencies on the KSR1 are 100 or more processors cycles and latencies to memory on a remote ring in the KSR1 are about a factor of six times slower [38]. <p> The affect that larger delays have on application performance and on the importance of application placement is explored more fully in <ref> [6] </ref>. The results of our experiments show that even for such systems the proper placement of the parallel threads of an application can significantly affect the execution time of the application. <p> Other processes of the job are placed within the same pool if possible. If there are no available processors in that pool then the pool with the largest number of available processors is chosen. This algorithm was devised using observations made while conducting experiments for a previous study <ref> [6] </ref> and is designed to isolate the execution of different jobs and to allow them ``room to grow''. This strategy corresponds to the ``Worst-Fit'' algorithm that is described in Section 6. We now conduct a series of experiments designed to further explore the inuences on the choice of processor pools. <p> The closer the pool size is to 4 the better the performance. The exception is the NEURAL application which, as described in detail in previous work <ref> [6] </ref>, suffers from an excessive number of system calls which overshadow the locality in the application. Although two pools of six processors may not seem appropriate for the current workload, it is included for completeness, since it will play a central role in a future experiment. <p> Application A is placed and executed in Pool 1 and application B is placed and executed in Pool 2. In previous work <ref> [6] </ref> we observed that if the first process of an application (the parent) is located on a station that is different from the rest of the processes of the application, the response time can be affected significantly because of the substantial memory context often associated with the first process of an <p> In previous work Zhou and Brecht [51] present the initial concept of processor pools and conduct a simulation study which demonstrates the potential benefits obtained from using processor pools for scheduling in NUMA multiprocessors. Since then <ref> [6] </ref> we have implemented and executed a number of parallel applications on a sixteen node multiprocessor to demonstrate the significant decreases in execution times that can be obtained by considering the architecture of NUMA systems when making application placement decisions. <p> The strategies of placing new applications in a pool with the largest potential for in-pool growth and of isolating applications from each other seem to be desirable properties of algorithms for using pools. The Worst-Fit policy incorporates both of these properties. An observation made in <ref> [6] </ref> that is also apparent when analyzing the experiments conducted in this work is that the proper placement of processes of an application is critical and localized placements are essential for the efficient execution of parallel applications.
Reference: [7] <author> T. Brecht, </author> <title> ``Multiprogrammed Parallel Application Scheduling in NUMA Multiprocessors'', </title> <type> Ph.D. Thesis, </type> <institution> University of Toronto, Toronto, Ontario, </institution> <type> Technical Report CSRI-303, </type> <month> June, </month> <year> 1994. </year>
Reference-contexts: This previous work has been conducted under the assumption that memory access latencies are uniform. Unfortunately, the problem of repartitioning becomes considerably more complex on clustered, NUMA architectures. We leave this important and interesting problem as a topic for future research (see <ref> [7] </ref> for a more detailed discussion of the problem). In this paper we do not consider the dynamic repartitioning of processors because of the issues related cache affinity in the assignment of user-level threads to kernel-level threads [27][28]. <p> In this paper we concentrate on obtaining a first-order understanding of the issues involved in making placement decisions and in the performance benefits that can result from making good placement decisions. The relationship between these problems is discussed in more detail in <ref> [7] </ref> and is an interesting topic of future research. 5.4. Architectural Inuences on Pool Size While the experiments shown in Figure 2 suggest that there is a relationship between pool size and application parallelism, these experiments do not fully explore the relationship between pool size and the system architecture. <p> However, under closer inspection we determined in that the same is not always true when the pool size is six. The results of this experiment, the details of which can be found in <ref> [7] </ref>, show that there is a difference between using three pools of size four and two pools of size six when allocating six processors to each application. <p> An important and difficult problem is how to repartition the processors while maintaining the locality of the executing applications. The topic of repartitioning with pools is discussed in more detail in <ref> [7] </ref> and is an interesting topic for further research. In this paper we examine the first two decision points more carefully, present algorithms for making these decisions and, when possible, evaluate their performance. We begin by examining the problem of application expansion. 6.1. <p> Although we've previously discussed the relationship between the problems of allocation (how many processor to allocate) and placement (which processor to allocate) <ref> [7] </ref>, the work in this paper concentrates on first gaining an understanding of the issues related to the placement problem before concerning ourselves with the interplay between the allocation and placement problems. Chandra et al. [10] add cache-affinity and cluster-affinity to a UNIX scheduler by modifying the traditional priority mechanisms.
Reference: [8] <author> T. Brecht and K. Guha, </author> <title> ``Using Parallel Program Characteristics in Dynamic Processor Allocation Policies'', </title> <journal> Performance Evaluation, </journal> <volume> Vol. 27 & 28, </volume> <pages> pp. 519-539, </pages> <month> October, </month> <year> 1996. </year>
Reference: [9] <author> H. Burkhardt, S. Frank, B. Knobe, and J. Rothnie, </author> <title> ``Overview of the KSR1 Computer System'', Kendall Square Research, Boston, </title> <type> Technical Report KSR-TR-9202001, </type> <month> February, </month> <year> 1992. </year>
Reference-contexts: Such systems, called NUMA (Non-Uniform Memory Access) multiprocessors, are a departure from the more common single bus-based UMA (Uniform Memory Access) multiprocessors. Some examples of NUMA multiprocessors include the University of Toronto's Hector [48], MIT's Alewife [1], Stanford's DASH and FLASH [23][21], Kendall Square Research's KSR1 <ref> [9] </ref>, SUN Microsystem's S3.mp [31], the Convex Exemplar SPP1000/1200 [13] and the Sequent STiNG [25]. The proliferation of more scalable, shared-memory multiprocessors presents new opportunities for the users and new challenges for the designers of such systems. <p> In actually implementing processor pools on a specific system, the NUMA characteristics of that architecture should be identified and fully exploited. In most architectures, there are clusters of processors that are good candidates for pools. For example, in a large-scale KSR1 system <ref> [9] </ref> containing a number of RING:0 subsystems connected together with a RING:1 at the higher level, pools may be formed by grouping together the processors in each of the RING:0's.
Reference: [10] <author> R. Chandra, S. Devine, B. Verghese, A. Gupta, and M. Rosenblum, </author> <title> ``Scheduling and Page Migration for Multiprocessor Compute Servers'', </title> <booktitle> Proceedings of the International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pp. 12-24, </pages> <address> San Jose, CA, </address> <month> October, </month> <year> 1994. </year>
Reference-contexts: Chandra et al. <ref> [10] </ref> add cache-affinity and cluster-affinity to a UNIX scheduler by modifying the traditional priority mechanisms.
Reference: [11] <author> J. Chapin, S. Herrod, M. Rosenblum, and A. Gupta, </author> <title> ``Memory System Performance of UNIX on CC-NUMA Multiprocessors'', </title> <booktitle> Proceedings of the 1995 ACM SIGMETRICS Joint International Conference on Measurement and Modeling of Computer Systems, Ottawa, ON, </booktitle> <month> May, </month> <year> 1995. </year>
Reference: [12] <author> J. Chapin, M. Rosenblum, S. Devine, T. Lahiri, D. Teodosiu, and A. Gupta, ``Hive: </author> <title> Fault Containment for Shared-Memory Multiprocessors'', </title> <booktitle> Proceedings of the Fifteenth ACM Symposium on Operating Systems Principles, </booktitle> <pages> pp. 12-25, </pages> <month> December, </month> <year> 1995. </year>
Reference-contexts: The combination of these two properties is intended to reduce the cost of remote references to shared data and to reduce the likelihood of contention for the interconnection network. Other work has also used the concept of clustering for different purposes. For example, Chapin et al. <ref> [12] </ref> use their notion of clusters (called cells) to prevent faults that occur in one cell from propagating to other cells, thus containing or localizing hardware and software faults.
Reference: [13] <author> Convex, </author> <title> Convex: Exemplar SPP1000/1200 Architecture,, </title> <publisher> Convex Press,, </publisher> <month> May, </month> <year> 1995. </year>
Reference-contexts: Some examples of NUMA multiprocessors include the University of Toronto's Hector [48], MIT's Alewife [1], Stanford's DASH and FLASH [23][21], Kendall Square Research's KSR1 [9], SUN Microsystem's S3.mp [31], the Convex Exemplar SPP1000/1200 <ref> [13] </ref> and the Sequent STiNG [25]. The proliferation of more scalable, shared-memory multiprocessors presents new opportunities for the users and new challenges for the designers of such systems.
Reference: [14] <author> A. Cox and R. Fowler, </author> <title> ``The Implementation of a Coherent Memory Abstraction on a NUMA Multiprocessor: Experiences with Platinum'', </title> <booktitle> Proceedings of the Twelfth ACM Symposium on Operating Systems Principles, </booktitle> <pages> pp. 32-43, </pages> <month> December, </month> <year> 1989. </year>
Reference: [15] <author> D. Feitelson and L. Rudolph, </author> <title> ``Evaluation of Design Choices for Gang Scheduling using Distributed Hierarchical Control'', </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> Vol. 35, No. 1, </volume> <pages> pp. 18-34, </pages> <month> May, </month> <year> 1996. </year>
Reference: [16] <author> D. G. Feitelson and L. Rudolph, </author> <title> ``Mapping and Scheduling in a Shared Parallel Environment Using Distributed Hierarchical Control'', </title> <booktitle> 1990 International Conference on Parallel Processing, </booktitle> <pages> pp. </pages> <address> I1-I8, </address> <year> 1990. </year>
Reference: [17] <author> D. G. Feitelson and L. Rudolph, </author> <title> ``Distributed Hierarchical Control for Parallel Processing'', </title> <booktitle> IEEE Computer, </booktitle> <pages> pp. 65-77, </pages> <month> May, </month> <year> 1990. </year>
Reference: [18] <author> B. Gamsa, </author> <title> ``Region-Oriented Main Memory Management in Shared-Memory NUMA Multiprocessors'', M.Sc. </title> <type> Thesis, </type> <institution> University of Toronto, Toronto, </institution> <address> Ontario, </address> <month> September, </month> <year> 1992. </year>
Reference: [19] <author> A. Gupta, A. Tucker, and S. Urushibara, </author> <title> ``The Impact of Operating System Scheduling Policies and Synchronization Methods on the Performance of Parallel Applications'', </title> <booktitle> Proceedings of the 1991 ACM SIGMETRICS Conference on Measurement and Modeling of Computer Systems, </booktitle> <pages> pp. 120-132, </pages> <address> San Diego, CA, </address> <month> May, </month> <year> 1991. </year>
Reference: [20] <author> M. Holliday, </author> <title> ``Reference History, Page Size, and Migration Daemons in Local/Remote Architectures'', </title> <booktitle> Proceedings of the International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pp. 104-112, </pages> <month> April, </month> <year> 1989. </year>
Reference: [21] <author> J. Kuskin, D. Ofelt, M. Heinrich, J. Heinlein, R. Simoni, K. Gharachorloo, J. Chapin, D. Nakahira, J. Baxter, M. Horowitz, A. Gupta, M. Rosenblum, and J. Hennessy, </author> <title> ``The Stanford FLASH Multiprocessor'', </title> <booktitle> Proceedings of the 21st Annual International Symposium on Computer Architecture, </booktitle> <pages> pp. 302-313, </pages> <month> April, </month> <year> 1994. </year>
Reference-contexts: In the case of the University of Toronto's Hector system [48] as well as in Stanford's DASH [23] and FLASH systems <ref> [21] </ref>, pools may be formed by grouping the processors of an individual station or cluster. In the MIT Alewife multiprocessor [1], pools might be chosen to consist of the four processors forming the smallest component of the mesh interconnect or a slightly larger set of nearest neighbour processors.
Reference: [22] <author> R. LaRowe Jr., C. Ellis, and L. Kaplan, </author> <title> ``The Robustness of NUMA Memory Management'', </title> <booktitle> Proceedings of the Thirteenth ACM Symposium on Operating Systems Principles, </booktitle> <pages> pp. 137-151, </pages> <address> Pacific Grove, CA, </address> <month> October, </month> <year> 1991. </year>
Reference: [23] <author> D. Lenoski, J. Laudon, T. Joe, D. Nakahari, L. Stevens, A. Gupta, and J. Hennessy, </author> <title> ``The DASH Prototype: Implementation and Performance'', </title> <booktitle> The Proceedings of the 19th Annual International Symposium on Computer Architecture, </booktitle> <pages> pp. 92-103, </pages> <month> May, </month> <year> 1992. </year>
Reference-contexts: In the case of the University of Toronto's Hector system [48] as well as in Stanford's DASH <ref> [23] </ref> and FLASH systems [21], pools may be formed by grouping the processors of an individual station or cluster.
Reference: [24] <author> S. T. Leutenegger and M. K. Vernon, </author> <title> ``The Performance of Multiprogrammed Multiprocessor Scheduling Policies'', </title> <booktitle> Proceedings of the 1990 ACM SIGMETRICS Conference on Measurement and Modeling of Computer Systems, </booktitle> <pages> pp. 226-236, </pages> <address> Boulder, CO, </address> <month> May, </month> <year> 1990. </year>
Reference: [25] <author> T. Lovett and R. Clapp, ``STiNG: </author> <title> A CC-NUMA Computer System for the Commercial Marketplace'', </title> <booktitle> Proceedings of the 23rd Annual International Symposium on Computer Architecture, </booktitle> <pages> pp. 308-317, </pages> <month> May, </month> <year> 1996. </year>
Reference-contexts: Some examples of NUMA multiprocessors include the University of Toronto's Hector [48], MIT's Alewife [1], Stanford's DASH and FLASH [23][21], Kendall Square Research's KSR1 [9], SUN Microsystem's S3.mp [31], the Convex Exemplar SPP1000/1200 [13] and the Sequent STiNG <ref> [25] </ref>. The proliferation of more scalable, shared-memory multiprocessors presents new opportunities for the users and new challenges for the designers of such systems.
Reference: [26] <author> E. P. Markatos, </author> <title> ``Scheduling for Locality in Shared-Memory Multiprocessors'', </title> <type> Ph.D. Thesis, </type> <institution> Department of Computer Science, University of Rochester, Rochester, </institution> <address> New York, </address> <month> May, </month> <year> 1993. </year>
Reference: [27] <author> E. P. Markatos and T. J. LeBlanc, </author> <title> ``Load Balancing vs. Locality Management in Shared-Memory Multiprocessors'', </title> <booktitle> 1992 International Conference on Parallel Processing, </booktitle> <pages> pp. 258-267, </pages> <month> August, </month> <year> 1992. </year>
Reference: [28] <author> E. P. Markatos and T. J. LeBlanc, </author> <title> ``Using Processor Affinity in Loop Scheduling on Shared-Memory Multiprocessors'', </title> <booktitle> Proceedings of Supercomputing '92, </booktitle> <pages> pp. 104-113, </pages> <address> Minneapolis, MN, </address> <month> November, </month> <year> 1992. </year>
Reference: [29] <author> C. McCann, R. Vaswani, and J. Zahorjan, </author> <title> ``A Dynamic Processor Allocation Policy for Multiprogrammed, Shared Memory Multiprocessors'', </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> Vol. 11, No. 2, </volume> <pages> pp. 146-178, </pages> <month> May, </month> <year> 1993. </year>
Reference: [30] <author> C. McCann and J. Zahorjan, </author> <title> ``Scheduling Memory Constrained Jobs on Distributed Memory Parallel Computers'', </title> <booktitle> Proceedings of the 1995 ACM SIGMETRICS Joint International Conference on Measurement and Modeling of Computer Systems, </booktitle> <pages> pp. 208-219, </pages> <address> Ottawa, ON, </address> <month> May, </month> <year> 1995. </year>
Reference: [31] <author> A. Nowatzyk, G. Aybay, M. Browne, E. Kelly, M. Parkin, B. Radke, and S. Vishin, </author> <title> ``The S3.mp Scalable Shared Memory Multiprocessor'', </title> <booktitle> Proceedings of the International Conference on Parallel Processing, </booktitle> <year> 1995. </year>
Reference-contexts: Some examples of NUMA multiprocessors include the University of Toronto's Hector [48], MIT's Alewife [1], Stanford's DASH and FLASH [23][21], Kendall Square Research's KSR1 [9], SUN Microsystem's S3.mp <ref> [31] </ref>, the Convex Exemplar SPP1000/1200 [13] and the Sequent STiNG [25]. The proliferation of more scalable, shared-memory multiprocessors presents new opportunities for the users and new challenges for the designers of such systems.
Reference: [32] <author> J. K. Ousterhout, </author> <title> ``Scheduling Techniques for Concurrent Systems'', </title> <booktitle> Proceedings of the 3rd International Conference on Distributed Computing Systems, </booktitle> <pages> pp. 22-30, </pages> <month> October, </month> <year> 1982. </year>
Reference-contexts: Using a sixteen processor DASH system they found that while a sequential workload benefited significantly from the improved locality, this approach did not improve execution times when compared with the baseline UNIX scheduler for parallel workloads. They also compare the use of gang scheduling <ref> [32] </ref>, processor sets [4], and process control [43] scheduling policies for executing parallel workloads. While they exploit cluster level locality in their implementations of each of these policies, they do not fully explore the strategies used in exploiting locality for parallel workloads nor how effective these strategies are at localization.
Reference: [33] <author> E. Parsons and K. Sevcik, </author> <title> ``Coordinated Allocation of Memory and Processors in Multiprocessors'', </title> <booktitle> Proceedings of the 1996 ACM SIGMETRICS Conference on Measurement and Modeling of Computer Systems, </booktitle> <pages> pp. 57-67, </pages> <address> Philadelphia, PA, </address> <month> May, </month> <year> 1996. </year>
Reference: [34] <author> V. Peris, M. Squillante, and V. Naik, </author> <title> ``Analysis of the Impact of Memory in Distributed Parallel Processing Systems'', </title> <booktitle> Proceedings of the 1994 ACM SIGMETRICS Conference on Measurement and Modeling of Computer Systems, </booktitle> <pages> pp. 5-18, </pages> <address> Nashville, TN, </address> <month> May, </month> <year> 1994. </year>
Reference: [35] <author> J. Philbin, J. Edler, O. Anshus, C. Douglas, and K. Li, </author> <title> ``Thread Scheduling for Cache Locality'', </title> <booktitle> Proceedings of the International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <address> Cambridge, MA, </address> <month> October, </month> <year> 1996. </year>
Reference: [36] <author> S. Setia, </author> <title> ``The Interaction Between Memory Allocations and Adaptive Partitioning in Message-Passing Multiprocessors,'', Job Scheduling Strategies for Parallel Processing, </title> <booktitle> Lecture Notes in Computer Science, </booktitle> <volume> Vol. 949, </volume> <pages> pp. 146-164, </pages> <publisher> Springer-Verlag, </publisher> <year> 1995. </year>
Reference: [37] <author> A. Silberschatz and P. Galvin, </author> <title> Operating System Concepts, </title> <publisher> Addison-Wesley, </publisher> <address> Reading, Massachusetts, </address> <year> 1994. </year>
Reference-contexts: An especially notable similarity is the desire to avoid fragmentation, since fragmenting processes of an application across different pools will hurt localization. Because of these similarities, we briey consider a number of possible strategies for initial placement adapted from well known placement policies for non-paged memory systems <ref> [37] </ref>. First-Fit: Pools are listed in a predetermined order by simply numbering each pool. The first process of an application is then placed in the first pool with an available processor.
Reference: [38] <author> J. P. Singh, T. Joe, A. Gupta, and J. Hennessy, </author> <title> ``An Empirical Comparison of the Kendall Square Research KSR-1 and Stanford Dash Multiprocessors'', </title> <booktitle> Proceedings of Supercomputing '93, </booktitle> <pages> pp. 214-225, </pages> <address> Portland, OR, </address> <month> November, </month> <year> 1993. </year>
Reference-contexts: Off station latencies in DASH and remote node latencies on the KSR1 are 100 or more processors cycles and latencies to memory on a remote ring in the KSR1 are about a factor of six times slower <ref> [38] </ref>.
Reference: [39] <author> M. S. Squillante, </author> <title> ``Issues in Shared-Memory Multiprocessor Scheduling: A Performance Evaluation'', </title> <type> Ph.D. Thesis, </type> <institution> Department of Computer Science and Engineering, University of Washington, </institution> <address> Seattle, Washington, </address> <note> Technical Report 90-10-04, </note> <month> October, </month> <year> 1990. </year>
Reference: [40] <author> M. S. Squillante and E. D. Lazowska, </author> <title> ``Using Processor Cache Affinity Information in Shared-Memory Multiprocessor Scheduling'', </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> Vol. 4, No. 2, </volume> <pages> pp. 131-143, </pages> <month> February, </month> <year> 1993. </year>
Reference: [41] <author> M. Stumm, Z. Vranesic, R. White, R. Unrau, and K. Farkas, </author> <title> ``Experiences with the Hector Multiprocessor'', </title> <booktitle> Proceedings of the International Parallel Processing Symposium Parallel Processing Fair, </booktitle> <pages> pp. 9-16, </pages> <month> April, </month> <year> 1993. </year>
Reference: [42] <author> D. Thiebaut and H. S. Stone, </author> <title> ``Footprints in the Cache'', </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> Vol. 5, No. 4, </volume> <pages> pp. 305-329, </pages> <month> November, </month> <year> 1987. </year>
Reference-contexts: For example, Chapin et al. [12] use their notion of clusters (called cells) to prevent faults that occur in one cell from propagating to other cells, thus containing or localizing hardware and software faults. Recent work has recognized that applications can build considerable cache context, or footprints <ref> [42] </ref> and that it may be more efficient to execute a process or thread on a processor that already contains relevant data in the processor's cache. Much of this work is concerned with designing and evaluating techniques that consider the affinity a process has for a processor [39][46][19][27][28][40][26][3][35].
Reference: [43] <author> A. Tucker and A. Gupta, </author> <title> ``Process Control and Scheduling Issues for Multiprogrammed Shared-Memory Multiprocessors'', </title> <booktitle> Proceedings of the Twelfth ACM Symposium on Operating Systems Principles, </booktitle> <pages> pp. 159-166, </pages> <month> December, </month> <year> 1989. </year>
Reference-contexts: They also compare the use of gang scheduling [32], processor sets [4], and process control <ref> [43] </ref> scheduling policies for executing parallel workloads. While they exploit cluster level locality in their implementations of each of these policies, they do not fully explore the strategies used in exploiting locality for parallel workloads nor how effective these strategies are at localization.
Reference: [44] <author> R. Unrau, </author> <title> ``Scalable Memory Management through Hierarchical Symmetric Multiprocessing'', </title> <type> Ph.D. Thesis, </type> <institution> University of Toronto, Toronto, </institution> <address> Ontario, </address> <month> January, </month> <year> 1993. </year>
Reference: [45] <author> R. Unrau, M. Stumm, and O. Krieger, </author> <title> ``Hierarchical Clustering: A Structure for Scalable Multiprocessor Operating System Design'', </title> <booktitle> Proceedings of the USENIX Workshop on Micro-Kernels and Other Kernel Architectures, </booktitle> <pages> pp. 285-303, </pages> <address> Seattle, WA, </address> <month> April, </month> <year> 1992. </year>
Reference: [46] <author> R. Vaswani and J. Zahorjan, </author> <title> ``The Implications of Cache Affinity on Processor Scheduling for Multiprogrammed, Shared Memory Multiprocessors'', </title> <booktitle> Proceedings of the Thirteenth ACM Symposium on Operating Systems Principles, </booktitle> <pages> pp. 26-40, </pages> <address> Pacific Grove, CA, </address> <month> October, </month> <year> 1991. </year>
Reference: [47] <author> B. Verghese, S. Devine, A. Gupta, and M. Rosenblum, </author> <title> ``Operating System Support for Improving Data Locality on CC-NUMA Compute Servers'', </title> <booktitle> Proceedings of the International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <address> Cambridge, MA, </address> <month> October, </month> <year> 1996. </year>
Reference: [48] <author> Z. Vranesic, M. Stumm, D. Lewis, and R. White, ``Hector: </author> <title> A Hierarchically Structured Shared-Memory Multiprocessor'', </title> <journal> IEEE Computer, </journal> <volume> Vol. 24, No. 1, </volume> <pages> pp. 72-79, </pages> <month> January, </month> <year> 1991. </year>
Reference-contexts: Such systems, called NUMA (Non-Uniform Memory Access) multiprocessors, are a departure from the more common single bus-based UMA (Uniform Memory Access) multiprocessors. Some examples of NUMA multiprocessors include the University of Toronto's Hector <ref> [48] </ref>, MIT's Alewife [1], Stanford's DASH and FLASH [23][21], Kendall Square Research's KSR1 [9], SUN Microsystem's S3.mp [31], the Convex Exemplar SPP1000/1200 [13] and the Sequent STiNG [25]. The proliferation of more scalable, shared-memory multiprocessors presents new opportunities for the users and new challenges for the designers of such systems. <p> For example, in a large-scale KSR1 system [9] containing a number of RING:0 subsystems connected together with a RING:1 at the higher level, pools may be formed by grouping together the processors in each of the RING:0's. In the case of the University of Toronto's Hector system <ref> [48] </ref> as well as in Stanford's DASH [23] and FLASH systems [21], pools may be formed by grouping the processors of an individual station or cluster. <p> Experimental Environment The system used to conduct the experiments presented in this paper is a prototype shared-memory NUMA multiprocessor called Hector, dev eloped at the University of Toronto <ref> [48] </ref>. The prototype used contains a total of 16 processors grouped into four clusters, called stations. Stations are connected with a bit-parallel slotted ring and each station consists of four processor modules connected with a bus.
Reference: [49] <author> S. Woo, M. Ohara, E. Torrie, J.P. Singh, and A. Gupta, </author> <title> ``The SPLASH-2 Programs: Characterization and Methodological Considerations'', </title> <booktitle> Proceedings of the 22nd Annual International Symposium on Computer Architecture, </booktitle> <pages> pp. 24-36, </pages> <year> 1995. </year>
Reference-contexts: This multiprogramming of parallel applications is essential for the effective utilization of all of the processors because in larger systems not all applications will be capable of efficiently executing on all processors. <ref> [49] </ref>. A critical difference between processor scheduling in UMA and NUMA multiprocessors is that scheduling decisions in NUMA systems must also consider the time it takes to access different memory locations from different processors.
Reference: [50] <author> J. Zahorjan and C. McCann, </author> <title> ``Processor Scheduling in Shared Memory Multiprocessors'', </title> <booktitle> Proceedings of the 1990 ACM SIGMETRICS Conference on Measurement and Modeling of Computer Systems, </booktitle> <pages> pp. 214-225, </pages> <address> Boulder, CO, </address> <month> May, </month> <year> 1990. </year>
Reference: [51] <author> S. Zhou and T. Brecht, </author> <title> ``Processor Pool-Based Scheduling for Large-Scale NUMA Multiprocessors'', </title> <booktitle> Proceedings of the 1991 ACM SIGMETRICS Conference on Measurement and Modeling of Computer Systems, </booktitle> <pages> pp. 133-142, </pages> <address> San Diego, CA, </address> <month> May, </month> <year> 1991. </year>
Reference-contexts: In this paper we focus on developing guidelines and algorithms designed specifically to enforce localized placements and on evaluating the benefits of such algorithms. In previous work Zhou and Brecht <ref> [51] </ref> present the initial concept of processor pools and conduct a simulation study which demonstrates the potential benefits obtained from using processor pools for scheduling in NUMA multiprocessors.
References-found: 51

