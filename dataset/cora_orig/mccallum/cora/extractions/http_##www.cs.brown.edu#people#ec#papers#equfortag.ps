URL: http://www.cs.brown.edu/people/ec/papers/equfortag.ps
Refering-URL: http://www.cs.brown.edu/people/ec/
Root-URL: 
Title: Equations for Part-of-Speech Tagging  
Author: Eugene Charniak and Curtis Hendrickson and Neil Jacobson and Mike Perkowitz 
Address: Providence RI 02912  
Affiliation: Department of Computer Science Brown University  
Abstract: We derive from first principles the basic equations for a few of the basic hidden-Markov-model word taggers as well as equations for other models which may be novel (the descriptions in previous papers being too spare to be sure). We give performance results for all of the models. The results from our best model (96.45% on an unused test sample from the Brown corpus with 181 distinct tags) is on the upper edge of reported results. We also hope these results clear up some confusion in the literature about the best equations to use. However, the major purpose of this paper is to show how the equations for a variety of models may be derived and thus encourage future authors to give the equations for their model and the derivations thereof. 
Abstract-found: 1
Intro-found: 1
Reference: 1. <author> Boggess, L., Agarwal, R. and Davis, R. </author> <title> Dis ambiguation of prepositional phrases in automatically labelled technical text. </title> <booktitle> In Proceedings of the Ninth National Conference on Artificial Intelligence. </booktitle> <year> 1991, </year> <pages> 155-159. </pages>
Reference-contexts: Nevertheless, given the drastic Markov assumptions we made in the derivation of Equation 19 it is hard to be sure that its comparative theoretical purity translates into better performance. Indeed, the one paper we are acquainted with in which the comparison was made <ref> [1] </ref> found that the less pure Equation 20 gave the better performance. However, this was on a very small amount of training data, and thus the results may not be accurate. <p> Improving the model to include bigrams of tags increases the accuracy to the 95% level, with the more theoretically pure P (w i j t i ) performing better than P (t i j w i ), contrary to the results in <ref> [1] </ref>. Furthermore the improvement is much larger than the .1% required for the 95% significance level. Improvement beyond this level is possible but it gets much harder. In particular, the improvement from the more sophisticated smoothing equation, Equation 26 is minimal, only .05%. This is not statistically significant.
Reference: 2. <author> Brill, E. </author> <title> A simple rule-based part of speech tagger. </title> <booktitle> In Proceedings of the Third Conference on Applied Natural Language Processing. </booktitle> <year> 1992. </year>
Reference-contexts: the text as having been produced by a hidden Markov model (HMM), so that the tagging problem can be viewed as one of deciding which states the Markov process went through during its generation of the text. (For an example of a system which does not take this view, see <ref> [2] </ref>.) Unfortunately, despite the obvious mathematical formulation that HMM's provide, few of the papers bother to define the mathematical model they use. In one case this has resulted in a confusion which we address subsequently.
Reference: 3. <author> Brown, P. F., Della Pietra, S. A., Della Pietra, V. J., Lai, J. C. and Mercer, R. L. </author> <title> An estimate of an upper bound for the entropy of english. </title> <type> In IBM Technical Report. </type> <year> 1991. </year>
Reference-contexts: It would do so because the probability of the shorter root would be much higher than the longer ones. To model P (r i ) we have adopted a spelling model along the lines of the one used for the spelling of unknown words in <ref> [3] </ref>. This combines a Poisson distribution over word lengths with a maximum at 5, times a distribution over letters. We adopted a unigram model for letters. Here j r j j is the length of r j and l i is the ith letter of r j .
Reference: 4. <author> Church, K. W. </author> <title> A stochastic parts program and noun phrase parser for unrestricted text. </title> <booktitle> In Second Conference on Applied Natural Language Processing. </booktitle> <year> 1988, </year> <pages> 136-143. </pages>
Reference-contexts: Experimentation has shown that it offers a slight improvement, but not a great deal. We ignore it henceforth. Another modification conditions the tag probability on the tags following the word rather than those which preceded it <ref> [4] </ref>. However, it is easy to show that this has no effect on results. <p> This equation is found in [4,9] and is described in words in [5]. (However, while Church gives Equation 20 in <ref> [4] </ref>, the results cited there were based upon Equation 19 (Church, personal communication).) Equation 20 seems plausible except that it is virtually impossible to derive it from basic considerations (at least we have been unable to do so).
Reference: 5. <author> DeRose, S. J. </author> <title> Grammatical category disambigua tion by statistical optimization. </title> <booktitle> Computational Linguistics 14 (1988), </booktitle> <pages> 31-39. </pages>
Reference-contexts: This equation is found in [4,9] and is described in words in <ref> [5] </ref>. (However, while Church gives Equation 20 in [4], the results cited there were based upon Equation 19 (Church, personal communication).) Equation 20 seems plausible except that it is virtually impossible to derive it from basic considerations (at least we have been unable to do so).
Reference: 6. <author> Francis, W. N. and Ku cera, H. </author> <title> Frequency Analysis of English Usage: Lexicon and Grammar. </title> <publisher> Houghton Mi*in, </publisher> <address> Boston, </address> <year> 1982. </year>
Reference-contexts: For Equation 12 we need the probabilities of each possible tag for each possible word: P (t i j w j ). The most obvious way to get these is from a corpus which has been tagged by hand. Fortunately there is such a corpus, the Brown Corpus <ref> [6] </ref> and all of the statistical data we collect are from a subset of this corpus consisting of 90% of the sentences chosen at random. (The other 10% we reserve for testing our models.) So, let C (t i ; w j ) be the number of times the word w
Reference: 7. <author> Jelinek, F. </author> <title> Markov source modeling of text gener ation. IBM T.J. Watson Research Center, Continuous Speech Recognition Group. </title>
Reference-contexts: But this still means that P (t i j w ) is zero for all t i . We solve this problem by adding further terms to Equation 13. We model this after what is typically done in smoothing tri-gram models for English <ref> [7] </ref>. Thus we add a second term to the equation with weights attached to each term saying how heavily that term should be counted. <p> One way to get the new s is to use extra training data to train the HMM corresponding to Equations 22 and 26 to find a (locally) best set of -values as done in <ref> [7] </ref>. However, it is possible to provide an argument for what their values ought to be.
Reference: 8. <author> Kupiec, J. and Maxwell, J. </author> <title> Training stochas tic grammars from unlabelled text corpora. </title> <booktitle> In Workshop Notes, AAAI-92 Workshop on Statistically-Based NLP Techniques. </booktitle> <year> 1992, </year> <pages> 14-19. 9. </pages> <editor> deMarcken, C. G. </editor> <booktitle> Parsing the LOB corpus. In Proceedings of the 1990 Conference of the Association for Computational Linguistics. </booktitle> <year> 1990, </year> <pages> 243-259. </pages>
Reference: 10. <author> Zernik, U. </author> <title> Shipping departments vs. shipping pace makers: using thematic analysis to improve tagging accuracy. </title> <booktitle> In Proceedings of the Tenth National Conference on Artificial Intelligence. </booktitle> <year> 1992, </year> <pages> 335-342. </pages>
Reference-contexts: This improvement is statistically significant. We believe that fixing these problems would add another tenth of a percent or two, but better performance beyond this will require more lexical information, as that used in <ref> [10] </ref>. However, the point of this paper was to clarify the basic equations behind tagging models, rather than improving the models themselves. We hope this paper encourages tag modelers to think about the mathematics which underly their models and to present their models in terms of the equations.
References-found: 9

