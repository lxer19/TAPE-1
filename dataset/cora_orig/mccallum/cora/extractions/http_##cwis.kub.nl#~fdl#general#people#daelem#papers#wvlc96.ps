URL: http://cwis.kub.nl/~fdl/general/people/daelem/papers/wvlc96.ps
Refering-URL: http://ilk.kub.nl/~ilk/papers/abstracts.html
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Email: fwalter.daelemans,zavrelg@kub.nl  fpeter.berck,steven.gillisg@uia.ua.ac.be  
Title: MBT: A Memory-Based Part of Speech Tagger-Generator  
Author: Walter Daelemans, Jakub Zavrel Peter Berck, Steven Gillis 
Date: 1996, Copenhagen  
Note: In proceedings WVLC  
Address: P.O. Box 90153, NL-5000 LE Tilburg  Universiteitsplein 1, B-2610 Wilrijk  
Affiliation: Computational Linguistics and AI Tilburg University  Center for Dutch Language and Speech University of Antwerp  
Abstract: We introduce a memory-based approach to part of speech tagging. Memory-based learning is a form of supervised learning based on similarity-based reasoning. The part of speech tag of a word in a particular context is extrapolated from the most similar cases held in memory. Supervised learning approaches are useful when a tagged corpus is available as an example of the desired output of the tagger. Based on such a corpus, the tagger-generator automatically builds a tagger which is able to tag new text the same way, diminishing development time for the construction of a tagger considerably. Memory-based tagging shares this advantage with other statistical or machine learning approaches. Additional advantages specific to a memory-based approach include (i) the relatively small tagged corpus size sufficient for training, (ii) incremental learning, (iii) explanation capabilities, (iv) flexible integration of information in case representations, (v) its non-parametric nature, (vi) reasonably good results on unknown words without morphological analysis, and (vii) fast learning and tagging. In this paper we show that a large-scale application of the memory-based approach is feasible: we obtain a tagging accuracy that is on a par with that of known statistical approaches, and with attractive space and time complexity properties when using IGTree, a tree-based formalism for indexing and searching huge case bases. The use of IGTree has as additional advantage that optimal context size for disambiguation is dynamically computed. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Aha, D. W., Kibler, D., & Albert, M. </author> <year> (1991). </year> <title> `Instance-based learning algorithms'. </title> <journal> Machine Learning, </journal> <volume> 7, </volume> <pages> 37-66. </pages>
Reference-contexts: two values is measured using equation (2), an overlap metric, for symbolic features (we will have no numeric features in the tagging application). ffi (x i ; y i ) = 0 if x i = y i ; else 1 (2) We will refer to this approach as IB1 <ref> (Aha et al., 1991) </ref>. <p> In order to prove that IGTree is a suitable candidate for practical memory-based tagging, we compared three memory-based learning algorithms: (i) IB1, a slight extension (to cope with symbolic values and ambiguous training items) of the well-known k-nn algorithm in statistical pattern recognition <ref> (see Aha et al., 1991) </ref>, (ii) IB1-IG, an extension of IB1 which uses feature relevance weighting (described in Section 2), and (iii) IGTree, a memory- and processing time saving heuristic implementation of IB1-IG (see Section 3).
Reference: <author> Brill, E. </author> <title> (1992) `A simple rule-based part-of-speech tagger'. </title> <booktitle> Proceedings Third ACL Applied, Trento, Italy, </booktitle> <pages> 152-155. </pages>
Reference: <author> Cardie, C. </author> <year> (1993a). </year> <title> `A case-based approach to knowledge acquisition for domain-specific sentence analysis'. </title> <booktitle> In AAAI-93, </booktitle> <pages> 798-803. </pages>
Reference: <author> Cardie, C. </author> <year> (1993b). </year> <title> `Using Decision Trees to Improve Case-Based Learning'. </title> <booktitle> In Proceedings of the Tenth International Conference on Machine Learning, </booktitle> <pages> 25-32. </pages>
Reference-contexts: A decision-tree learning approach to feature selection is used in this experiment <ref> (Cardie, 1993b, 1994) </ref> to discard irrelevant features. Results are based on experiments with 120 randomly chosen sentences from the TIPSTER JV corpus (representing 2056 cases). Cardie (p.c.) reports 89.1% correct tagging for unknown words.
Reference: <author> Cardie, C. </author> <year> (1994). </year> <title> `Domain-Specific Knowledge Acquisition for Conceptual Sentence Analysis'. </title> <type> Ph.D. Thesis, </type> <institution> University of Massachusetts, </institution> <address> Amherst, MA. </address>
Reference-contexts: Compared to learning rule-based approaches such as the one by Brill (1992), a k-nn approach provides a uniform approach for all disambiguation tasks, more flexibility in the engineering of case representations, and a more elegant approach to handling of unknown words <ref> (see e.g. Cardie 1994) </ref>. 11 7 Conclusion We have shown that a memory-based approach to large-scale tagging is feasible both in terms of accuracy (comparable to other statistical approaches), and also in terms of computational efficiency (time and space requirements) when using IGTree to compress and index the case base.
Reference: <author> Chandler, S. </author> <year> (1992). </year> <title> `Are rules and modules really necessary for explaining language?' Journal of Psycholinguistic research, </title> <booktitle> 22(6): </booktitle> <pages> 593-606. </pages>
Reference: <author> Church, K. </author> <year> (1988). </year> <title> `A stochastic parts program and noun phrase parser for unrestricted text'. </title> <booktitle> Proceedings Second ACL Applied NLP, </booktitle> <address> Austin, Texas, </address> <pages> 136-143. </pages>
Reference: <author> Cost, S. and Salzberg, S. </author> <year> (1993). </year> <title> `A weighted nearest neighbour algorithm for learning with symbolic features.' </title> <journal> Machine Learning, </journal> <volume> 10, </volume> <pages> 57-78. </pages>
Reference: <author> Cutting, D., Kupiec, J., Pederson, J., Sibun, P. </author> <year> (1992). </year> <title> A practical part of speech tagger. </title> <booktitle> Proceedings Third ACL Applied NLP, Trento, Italy, </booktitle> <pages> 133-140. </pages>
Reference: <author> Daelemans, W. </author> <year> (1995). </year> <title> `Memory-based lexical acquisition and processing.' </title> <editor> In Steffens, P., editor, </editor> <booktitle> Machine Translation and the Lexicon, Lecture Notes in Artificial Intelligence 898. </booktitle> <address> Berlin: </address> <publisher> Springer, </publisher> <pages> 85-98. </pages>
Reference: <author> Daelemans, W., Van den Bosch, A. </author> <year> (1992). </year> <title> `Generalisation performance of backpropagation learning on a syllabification task.' </title> <editor> In M. Drossaers & A. Nijholt (Eds.), TWLT3: </editor> <booktitle> Connectionism and Natural Language Processing. </booktitle> <institution> Enschede: Twente University, </institution> <month> 27-38. </month>
Reference: <author> Daelemans, W., Van den Bosch, A., Weijters, T. </author> <year> (1996). </year> <title> `IGTree: Using Trees for Compression and Classification in Lazy Learning Algorithms.' </title> <editor> In Aha, D. (ed.). </editor> <title> AI Review Special Issue on Lazy Learning, </title> <publisher> forthcoming. </publisher>
Reference-contexts: The approach in its basic form is computationally expensive, however; each new word in context that has to be tagged, has to be compared to each pattern kept in memory. In this paper we show that a heuristic case base compression formalism <ref> (Daelemans et al., 1996) </ref>, makes the memory-based approach computationally attractive. 2 Memory-Based Learning Memory-based Learning is a form of supervised, inductive learning from examples. Examples are represented as a vector of feature values with an associated category label. <p> Without optimisation, it has an asymptotic retrieval complexity of O (N F ) (where N is the number of items in memory, and F the number of features). The same asymptotic complexity is of course found for memory storage in this approach. We use IGTrees <ref> (Daelemans et al. 1996) </ref> to compress the memory. IGTree is a heuristic approximation of the IB-IG algorithm. 3.1 The IGTree Algorithms IGTree combines two algorithms: one for compressing a case base into a trees, and one for retrieving classification information from these trees.
Reference: <author> DeRose, S. </author> <year> (1988). </year> <title> `Grammatical category disambiguation by statistical optimization. </title> ' <booktitle> Computational Linguistics 14, </booktitle> <pages> 31-39. </pages>
Reference: <author> Derwing, B. L. and Skousen, R. </author> <year> (1989). </year> <title> `Real Time Morphology: Symbolic Rules or Analogical Networks'. </title> <booktitle> Berkeley Linguistic Society 15: </booktitle> <pages> 48-62. </pages>
Reference: <author> Federici S. and V. Pirelli. </author> <year> (1996). </year> <title> `Analogy, Computation and Linguistic Theory.' </title> <editor> In Jones, D. (ed.) </editor> <booktitle> New Methods in Language Processing. </booktitle> <address> London: </address> <publisher> UCL Press, forthcoming. </publisher>
Reference: <author> Garside, R., Leech, G. and Sampson, G. </author> <year> (1987). </year> <title> The computational analysis of English: A corpus-based approach, </title> <publisher> London: Longman, </publisher> <year> 1987. </year>
Reference: <author> Greene, B.B. and Rubin, G.M. </author> <year> (1971). </year> <title> Automatic Grammatical Tagging of English. </title> <institution> Providence RI: Department of Linguistics, Brown University. </institution>
Reference: <author> Hindle, Donald. </author> <year> (1989). </year> <title> `Acquiring disambiguation rules from text.' </title> <booktitle> In Proceedings, 27th Annual Meeting of the Association for Computational Linguistics, </booktitle> <address> Vancouver, BC. </address>
Reference: <author> Hunt, E., J. Marin, P. Stone. </author> <year> (1966). </year> <title> Experiments in Induction. </title> <address> New York: </address> <publisher> Academic Press. </publisher>
Reference: <author> Jones, D. </author> <title> Analogical Natural Language Processing. </title> <publisher> London: UCL Press, </publisher> <year> 1996. </year> <editor> 13 Klein S. and Simmons, R. </editor> <year> (1963). </year> <title> `A grammatical approach to grammatical coding of English words.' </title> <journal> JACM 10, </journal> <pages> 334-347. </pages>
Reference: <author> Kolodner, J. </author> <year> (1993). </year> <title> Case-Based Reasoning. </title> <address> San Mateo: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Langley, P. and Sage, S. </author> <year> (1994). </year> <title> `Oblivious decision trees and abstract cases.' </title> <editor> In D. W. Aha (Ed.), </editor> <booktitle> Case-Based Reasoning: Papers from the 1994 Workshop (Technical Report WS-94-01). </booktitle> <address> Menlo Park, CA: </address> <publisher> AAAI Press. </publisher>
Reference: <author> Merialdo, B. </author> ( <year> 1994). </year> <title> `Tagging English Text with a Probabilistic Model.' </title> <booktitle> Computational Linguistics 20 (2), </booktitle> <pages> 155-172. </pages>
Reference: <author> Pereira, F., Y. Singer, N. Tishby. </author> <year> (1995). </year> <title> `Beyond Word N-grams.' </title> <booktitle> Proceedings Third Workshop on Very Large Corpora, </booktitle> <publisher> MIT, </publisher> <address> Cambridge Mass., </address> <pages> 95-106. </pages>
Reference: <author> Quinlan, J. </author> <year> (1993). </year> <title> C4.5: Programs for Machine Learning. </title> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Salzberg, S. </author> <title> (1990) `A nearest hyperrectangle learning method'. </title> <booktitle> Machine Learning 6, </booktitle> <pages> 251-276. </pages>
Reference: <author> Samuelsson, C. </author> <title> (1994) `Morphological Tagging Based Entirely on Bayesian Inference.' </title> <booktitle> In Proceedings of the 9th Nordic Conference on Computational Linguistics, </booktitle> <address> Stockholm University, Sweden, </address> <year> 1994. </year>
Reference: <author> Scha, R. </author> <booktitle> (1992) `Virtuele Grammatica's en Creatieve Algoritmen.' </booktitle> <volume> Gramma/TTT 1 (1), </volume> <pages> 57-77. </pages>
Reference: <author> Schmid, H. </author> <title> (1994) `Part-of-speech tagging with neural networks.' </title> <booktitle> In Proceedings of COLING, </booktitle> <address> Kyoto, Japan. </address>
Reference: <author> Schutze, H., and Y. Singer. </author> <title> (1994) `Part-of-speech Tagging Using a Variable Context Markov Model' Proceedings of ACL 1994, </title> <address> Las Cruces, New Mexico. </address>
Reference: <author> Skousen, R. </author> <year> (1989). </year> <title> Analogical Modeling of Language. </title> <publisher> Dordrecht: Kluwer. </publisher>
Reference: <author> Sejnowski, T. J., Rosenberg, C. S. </author> <year> (1987). </year> <title> Parallel networks that learn to pronounce English text. </title> <journal> Complex Systems, </journal> <volume> 1, </volume> <pages> 145-168. </pages>
Reference-contexts: Depending on whether or not they can be found there, a case representation is constructed for them, and they are retrieved from either the known words case base or the unknown words case base. 4.2 Known Words A windowing approach <ref> (Sejnowski & Rosenberg, 1987) </ref> was used to represent the tagging task as a classification problem. A case consists of information about a focus word to be tagged, its left and right context, and an associated category (tag) valid for the focus word in that context.
Reference: <author> Stanfill, C. and Waltz, D. </author> <year> (1986). </year> <title> `Toward memory-based reasoning.' </title> <journal> Communications of the ACM, </journal> <volume> 29, </volume> <pages> 1212-1228. </pages>
Reference-contexts: E.g. the similarity between the tags rb-in-nn and rb-in should be bigger than the similarity between rb-in and vb-nn. Apart from linguistic engineering refinements of the similarity metric, we are currently experimenting with statistical measures to compute such more fine-grained similarities <ref> (e.g. Stanfill & Waltz, 1986, Cost & Salzberg, 1994) </ref>. Acknowledgements Research of the first author was done while he was a visiting scholar at NIAS (Netherlands Institute for Advanced Studies) in Wassenaar.
Reference: <author> Weiss, S. and Kulikowski, C. </author> <year> (1991). </year> <title> Computer systems that learn. </title> <publisher> San-Mateo: Morgan Kaufmann. </publisher> <pages> 14 </pages>
Reference-contexts: In further experiments we studied the performance of our system on predicting the category of both known and unknown words. Experimental Set-up The experimental methodology was taken from Machine Learning practice <ref> (e.g. Weiss & Kulikowski, 1991) </ref>: independent training and test sets were selected from the original corpus, the system was trained on the training set, and the generalization accuracy (percentage of correct category assignments) was computed on the independent test set. Storage and time requirements were computed as well.
References-found: 34

