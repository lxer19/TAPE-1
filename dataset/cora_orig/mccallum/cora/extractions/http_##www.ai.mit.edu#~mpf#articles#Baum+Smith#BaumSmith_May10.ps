URL: http://www.ai.mit.edu/~mpf/articles/Baum+Smith/BaumSmith_May10.ps
Refering-URL: 
Root-URL: 
Email: Email: eric and wds @research.NJ.NEC.COM  
Title: Best Play for Imperfect Players and Game Tree Search  
Author: Eric B. Baum and Warren D. Smith 
Date: May 10, 1993  
Note: DRAFT  
Address: 4 Independence Way Princeton NJ 08540  
Affiliation: NEC Research Institute  
Abstract: We propose a new approach to game tree search. We train up an evaluation function which returns, rather than a single number estimating the `value' of a position, a probability distribution P L (x). P L (x) is the probability that if we expanded leaf L to some depth, the backed up value of leaf L would then be found to be x. We describe how to propagate these distributions efficiently up the tree so that at any node n we compute without approximation the probability node n's negamax value is x given that a value is assigned to each leaf from its distribution. After we are done expanding the tree, the best move is the child of the root whose distribution has highest mean. Note that we take means at the child of the root after propagating, whereas the normal (Shannon) approach takes the mean at the leaves before propagating, which throws away information. Now we model the expansion of a leaf as selection of one value from its distribution. The total utility of all possible expansion is then by definition the ensemble sum over those possible leaf configurations for which the current favorite move is inferior to some alternate move, weighted by the probability of the leaf configuration and the amount the current favorite move is inferior. We propose as the natural measure of the expansion importance of leaf L, the expected absolute change in this utility when we expand leaf L. We support this proposal with several arguments including an approximation theorem valid in the limit that one expands until the remaining utility of expansion becomes small. In summary, we gather distributions at the leaves, propagate exactly all this information to the root, and incrementally grow a tree expanding approximately the most interesting leaf at each step. Under reasonable conditions, we accomplish all of this in time O(N ), where N is the number of leaves in the tree when we are done expanding. That is, we pay only a small constant factor overhead for all of our bookkeeping. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> T. Anantharaman, M. Campbell, F. Hsu: </author> <title> Singular extensions; adding selectivity to brute force searching, </title> <booktitle> Artificial Intelligence 43 (1990) 99-109 </booktitle>
Reference: [2] <author> Thomas S. Anantharaman: </author> <title> A Statistical Study of Selective Min-Max Search in Computer Chess, </title> <type> (PhD thesis, </type> <institution> Carnegie Mellon University, Computer Science Dept.) </institution> <month> May </month> <year> 1990, </year> <month> CMU-CS-90-173 </month>

Reference: [4] <author> D.F. Beal: </author> <title> A generalized quiescence search algorithm, </title> <booktitle> Artificial Intelligence 43 (1990) 85-98 </booktitle>
Reference: [5] <author> M. Blum, R. Floyd, V. Pratt, R. Rivest, R. Tarjan: </author> <title> Time Bounds for selection, </title> <journal> J. Computer System Sci. </journal> <month> 7 </month> <year> (1973) </year> <month> 448-461 </month>
Reference-contexts: Step 7 takes O (L) time and storage by the use of a linear-time selection algorithm [8] <ref> [5] </ref>.
Reference: [6] <author> M. Campbell: </author> <title> The graph-history interaction; on ignoring position history, </title> <booktitle> Proc. ACM National Conf. </booktitle> <year> (1985) </year> <month> 278-280. </month>
Reference: [7] <author> Chi, P-C, D. S. Nau: </author> <title> Comparison of the Minimax and Product Back-up Rules in a Variety of Games, </title> <booktitle> in Search in Artificial Intelligence, </booktitle> <editor> eds. L. Kanal and V. Kumar, </editor> <publisher> Springer Verlag, </publisher> <address> New York,(1989) pp451-471. </address>
Reference-contexts: In essence one may empirically measure the likelihood of various opinion changes as a function of various features. We now assume that these distributions are independent. (Note: we are not assuming that the probabilities of winning at different nodes are independent, as have [16] <ref> [7] </ref>. We are assuming that the errors in our estimate of these probabilities are independent.) BPIP then forces the adoption of a certain formula (see x5) for propagating these distributions up the tree. <p> The same propagation formulas 3-5 are used as in BPIP, a fact which has led some unwary readers of previous drafts to conclude erroneously that BPIP is NPU. NPU is equivalent to the probability propagation proposal of Pearl [16], experimentally studied by Chi and Nau <ref> [7] </ref> and others. NPU yields at each interior node n of the tree, an estimate of the probability that n is a win with perfect play. You then move to the child of the root with the largest chance of being a win.
Reference: [8] <author> R. Floyd and R. Rivest: </author> <title> Expected time bounds for selection, </title> <journal> Commun. </journal> <note> ACM 18,3 (March 1975) 165-173 </note>
Reference-contexts: Step 7 takes O (L) time and storage by the use of a linear-time selection algorithm <ref> [8] </ref> [5].
Reference: [9] <author> Richard M. Karp, Michael Luby, and Neal Madras: </author> <title> Monte-Carlo Approximation Algorithms for Enumeration Problems, </title> <note> Journal of Algorithms 10 (1989) 429-448 </note>
Reference-contexts: QED. Note that we can approximate the exact CDFs at all nodes, taking full account of the DAG dependencies, by Monte Carlo evaluation. (See also <ref> [9] </ref>.) This may be useful in evaluating our move choice, but we believe is likely to be too slow in convergence to be useful in deciding how to expand the tree. The reason for this is the following. <p> value of a game DAG. 13 One might imagine a more sophisticated Monte Carlo procedure which evaluated the utility of leaf L by sampling uniformly directly from the region in which the value of leaf L is relevant and then computed the integral by some procedure analogous to that of <ref> [9] </ref>, but we have been unable to construct a procedure along these lines which is efficient and rapidly mixing. 14 Actually the situation is worse than this for two reasons. First we must sample from such regions many times because of the inherent noise in the Monte Carlo procedure.
Reference: [10] <author> Feng-hsiung Hsu: </author> <title> Large Scale Parallelization of Alpha Beta Search: An Algorithmic and Architectural Study with Computer Chess, </title> <type> (PhD thesis) Tech. </type> <institution> Rept. CMU-CS-90-108 (Feb 1990) School of Computer Science, Carnegie Mellon University, </institution> <address> Pittsburgh PA 15213 </address>
Reference-contexts: This allows one to explore a tree twice as deep as one might expect. Fast special purpose hardware has been developed which for example allows the Deep Thought machine to explore chess to a depth of about 10 ply <ref> [10] </ref>. This has proved effective in chess. Nonetheless it seems wasteful to explore the full width tree, since almost all lines are ludicrously bad. One might prefer an approach which preferentially explored more important lines. See x12 for a brief summary of some previous work along these lines.
Reference: [11] <author> Han La Poutre and Warren D. Smith: </author> <title> Approximation of staircases by staircases, Work in progress, NECI, 4 Independence Way, </title> <address> Princeton NJ 08540. </address>
Reference-contexts: Step 3. Compress the staircase-CDF probability distributions in each bin, into staircase CDFs optimally approximating the original staircases, but having only a small number of stairsteps <ref> [11] </ref>. 1 Lee and Mahajan proposed an elegant-sounding quadratic fitting procedure as an alternative to a linear least-squares fit [13].
Reference: [12] <author> Kai-Fu Lee and Sanjoy Mahajan: </author> <title> The development of a world class othello program, </title> <booktitle> Artificial Intelligence 43 (1990) 21-36 </booktitle>
Reference-contexts: This effect apparently happened in Lee and Mahajan's program "Bill," since two matrices given on page 32 of <ref> [12] </ref> have elements identical to within 1%... 5 By adding regurgitated compressed bin data to E 1 , one obtains an evaluation function E 2 which returns staircase probability distributions 2 . 4 Philosophy of Distributions at Nodes There are two alternative but mathematically equivalent ways of regarding our approach to
Reference: [13] <author> Kai-Fu Lee and Sanjoy Mahajan: </author> <title> A pattern classification approach to evaluation function learning, </title> <booktitle> Artificial Intelligence 36 (1988) 1-25 </booktitle>
Reference-contexts: Step 3. Compress the staircase-CDF probability distributions in each bin, into staircase CDFs optimally approximating the original staircases, but having only a small number of stairsteps [11]. 1 Lee and Mahajan proposed an elegant-sounding quadratic fitting procedure as an alternative to a linear least-squares fit <ref> [13] </ref>.
Reference: [14] <author> D.A. McAllester: </author> <title> Conspiracy numbers for min max search, </title> <booktitle> Artificial Intelligence 35 (1988) 287-310 </booktitle>
Reference-contexts: It turns out it is very important to consider the effects of expanding large sets of leaves, not just one. For one thing, "conspiracies" among many leaves are often the only way to affect the best move choice, since often no leaf acting alone is capable of doing it <ref> [14] </ref>. We can compute in linear time the expansion utility U of expanding the entire game tree. This gives a natural condition for termination of the search stop searching and make your move when the cost of computer time outweighs the utility of further search. <p> our algorithm and alpha-beta explore for the same amount of time, the tree we grow will be up to three times as deep, but contain about half as many leaves, as that of alpha-beta. 1.1 Previous work A number of tree growth algorithms have previously been proposed, e.g. by McAllester <ref> [14] </ref>, Palay [15], Rivest [17], and Russell and Wefald [18]. From our point of view these authors were all (either implicitly or explicitly) striving to calculate the decision theoretic optimal leaf to expand next. <p> In contrast 3 to the previous approaches, which were all greedy or ad hoc, our leaf importance measure accounts for the possible outcomes of future expansion. We review previous work in x12. We also remark there that the algorithms of <ref> [14] </ref> and [18] take time superlinear in the number of leaves. <p> We have not experimented with either transposition tables in BPIP, or this heuristic. 12 Tree-shaping algorithms by previous authors We now list some tree-shaping techniques, proposed by previous authors, which require storage of the whole tree S. 12.1 Conspiracies D. McAllester <ref> [14] </ref> suggested a method of "conspiracies" in the negamax tree-searching philosophy.
Reference: [15] <author> A.J. Palay: </author> <title> Searching with probabilities, </title> <publisher> Pitman 1985 </publisher>
Reference-contexts: and alpha-beta explore for the same amount of time, the tree we grow will be up to three times as deep, but contain about half as many leaves, as that of alpha-beta. 1.1 Previous work A number of tree growth algorithms have previously been proposed, e.g. by McAllester [14], Palay <ref> [15] </ref>, Rivest [17], and Russell and Wefald [18]. From our point of view these authors were all (either implicitly or explicitly) striving to calculate the decision theoretic optimal leaf to expand next. <p> MGSS* becomes completely helpless (terminates) if the move choice conspiracy number ever exceeds 1. Furthermore we have argued in x8.3 that the metagreedy approximation they make is a poor guide to the leaf importance even when the conspiracy number is 1. 12.4 Palay Andrew Palay <ref> [15] </ref> proposed an interesting scheme that uses probability distributions as node values. <p> Another drawback to Palay's approach is the following. Rather than determine which leaf of the tree was best to expand (under his criteria), Palay marched down from the root at each node choosing the most likely child until he reached a leaf. As he realized (p13 <ref> [15] </ref>) this greedy procedure need not pick the globally best leaf, nor even a good approximation. Palay was forced into this expedient, however, because examining all the leaves to find the best would have caused his runtime to grow superlinearly.
Reference: [16] <author> J. Pearl: </author> <title> Heuristics, </title> <publisher> Addison-Wesley 1984 </publisher>
Reference-contexts: We first argue that minimax, while producing best play in games between "perfect" players who can afford to search the entire game tree, is not the best way to utilize inexact leaf values in a given partial tree. Nor is another old idea <ref> [16] </ref> that we call "naive probability update." Instead, the best way, at least from the point of view of a Bayesian, is a propagation rule we call "best play for imperfect players," or BPIP. BPIP has a simple recursive definition. <p> In essence one may empirically measure the likelihood of various opinion changes as a function of various features. We now assume that these distributions are independent. (Note: we are not assuming that the probabilities of winning at different nodes are independent, as have <ref> [16] </ref> [7]. We are assuming that the errors in our estimate of these probabilities are independent.) BPIP then forces the adoption of a certain formula (see x5) for propagating these distributions up the tree. <p> If two sibling positions are assigned by an ordinary evaluation function a value .5 meaning that the probability of winning in that position is .5, it would be unwise to assume that these are independent and assign a probability of winning of .75 to their parent,as is sometimes advocated <ref> [16] </ref>. Our distributions, however, are over the error in the evaluation function, or to put it another way, are distributions over changes in our opinion about the position that would arise from future expansion. <p> Such a distribution of errors in the evaluation function, would seem to be inherently less correlated than the evaluation function itself. 4.2 BPIP-DFISA When it was first remarked that any evaluation function returns probabilistic information <ref> [16] </ref>, the question arose whether minimax is the correct way to propagate this information up a tree. One might instead propose a rule for combination of probabilities [16]. <p> be inherently less correlated than the evaluation function itself. 4.2 BPIP-DFISA When it was first remarked that any evaluation function returns probabilistic information <ref> [16] </ref>, the question arose whether minimax is the correct way to propagate this information up a tree. One might instead propose a rule for combination of probabilities [16]. This reasoning correctly asserts that a position with 100 alternative moves, each of which has independently .1 probability of leading to a won game, is almost certainly a won position. <p> The same propagation formulas 3-5 are used as in BPIP, a fact which has led some unwary readers of previous drafts to conclude erroneously that BPIP is NPU. NPU is equivalent to the probability propagation proposal of Pearl <ref> [16] </ref>, experimentally studied by Chi and Nau [7] and others. NPU yields at each interior node n of the tree, an estimate of the probability that n is a win with perfect play. You then move to the child of the root with the largest chance of being a win. <p> In negamaxing of single numerical values, the graph of the influence function consists of three line segments, the outer two being constant and the inner one being of slope 1. The t-values of the corners are the "ff-fi window." <ref> [16] </ref>. In this section we describe how we may associate to each jump in the CDF at each node in our search tree an influence coefficient.
Reference: [17] <author> R.L. Rivest: </author> <title> Game tree searching by min max approximation, </title> <booktitle> Artificial Intelligence 34 (1988) 77-96 </booktitle>
Reference-contexts: explore for the same amount of time, the tree we grow will be up to three times as deep, but contain about half as many leaves, as that of alpha-beta. 1.1 Previous work A number of tree growth algorithms have previously been proposed, e.g. by McAllester [14], Palay [15], Rivest <ref> [17] </ref>, and Russell and Wefald [18]. From our point of view these authors were all (either implicitly or explicitly) striving to calculate the decision theoretic optimal leaf to expand next. <p> Also we take account of the possibility of perturbations to leaves not in the conspiracy which might either counteract or reinforce this conspiracy. 12.2 Rivest's suggestion R. Rivest <ref> [17] </ref> suggested a method where the "max" in negamaxing is replaced by an L p mean. The root value then depends differentiably on the leaf values. One may find the gradient of the root value with respect to the leaf values, by applying the chain rule.
Reference: [18] <author> S. Russell and E. Wefald: </author> <title> Do the Right Thing, </title> <note> MIT Press 1991 (see especially chapter 4) </note>
Reference-contexts: Expanding according to immediate expansion utility (similar to the "metagreedy" idea of <ref> [18] </ref>) is not good enough because of its neglect of one's ability to expand other leaves before moving. We may 2 similarly define (although not efficiently compute) the expansion utility of any subset of the leaves. <p> of time, the tree we grow will be up to three times as deep, but contain about half as many leaves, as that of alpha-beta. 1.1 Previous work A number of tree growth algorithms have previously been proposed, e.g. by McAllester [14], Palay [15], Rivest [17], and Russell and Wefald <ref> [18] </ref>. From our point of view these authors were all (either implicitly or explicitly) striving to calculate the decision theoretic optimal leaf to expand next. <p> In contrast 3 to the previous approaches, which were all greedy or ad hoc, our leaf importance measure accounts for the possible outcomes of future expansion. We review previous work in x12. We also remark there that the algorithms of [14] and <ref> [18] </ref> take time superlinear in the number of leaves. Palay was the first author to propose the use of distribution valued evaluation functions and also proposed the same equations for propagation of probability distributions that we do, but his use of them was approximate, and his motivation and application different. <p> Of course when we actually expand this leaf U may increase or decrease, but the average change in it is negative. When we have expanded all leaves, the utility U of further 8 The procedure of expanding the leaf with optimal SLEU is akin to the procedure advocated by <ref> [18] </ref> in that leaves are valued as if there was no possibility for future expansion. They call this approach "metagreedy". 12 expansion will by definition be zero. This convergence to 0 of course shows that our assumption that the leaf "importances" are independent is not strictly true. <p> Russell and Wefald also experimented with MGSS2, a version permitting partial node expansion, and got good preliminary results (table p109 of <ref> [18] </ref>). The logic behind MGSS2 seems to us seriously flawed, however. We omit discussion here. Russell and Wefald's MGSS* results are dramatic evidence for the power of the ideas of "expansion utility" and the use of probability distributions at leaves to describe value changes after expansion.
Reference: [19] <author> J. Schaeffer: </author> <title> Conspiracy numbers, </title> <booktitle> Artificial Intelligence 43 (1990) 67-84 </booktitle>
Reference-contexts: McAllester's tree-shaping method was found experimentally to work well in tactical chess middlegames by J. Schaeffer <ref> [19] </ref>, but it worked badly in "positional" chess and in forced mates in which moves near the end of the forced mate line were "any"s. We now remark that large conspiracy number occurs even if one makes no special effort to shape the tree.
Reference: [20] <author> G. Schrufer: </author> <title> Presence and Absence of Pathology on Game Trees, </title> <editor> in D.F. Beal, ed., </editor> <booktitle> Advances in Computer Chess 4, </booktitle> <publisher> (Pergamon, Oxford, </publisher> <pages> 1986) pp 101-112. </pages>
Reference-contexts: A similar result holds in Schrufer's <ref> [20] </ref> more realistic probabilistic model of b-uniform depth-d game trees with Boolean leaf values. Indeed, Schrufer's theoretical results can be viewed as proving that, in his model, exponentially large conspiracy numbers occur if and only if negamax search is not pathological.
Reference: [21] <author> C.E. Shannon: </author> <title> Programming a computer for playing chess, </title> <journal> Philos. </journal> <note> Magazine 41,7 (1950) 256-275 </note>
Reference-contexts: 1 Introduction Shannon <ref> [21] </ref> proposed that computers should play games like chess by growing a full width game tree as deeply as time permits, heuristically assigning a numerical evaluation to each leaf, propagating these numbers up the tree by minimax, and choosing as the "best move" the child of the root with the largest <p> It is well known that if you have the computational resources to explore the whole tree, the optimal move is given by negamax. Shannon <ref> [21] </ref> proposed looking at the subtree of depth D, where D is as large as one can afford, and assigning a numerical value at the leaves of this subtree using an evaluation function.
Reference: [22] <author> Warren D. Smith: </author> <title> Fixed point for negamaxing probability distributions on regular trees, </title> <note> Submitted for publication </note>
Reference-contexts: Some justification for this assumption of drastic shrinking, and hence of DFISA, is provided by the results of <ref> [22] </ref>. In any case we are only using this assumption to decide which order to expand leaves in. If we continue to expand to some depth below a node, we will in practice achieve substantial sharpening of the distribution at that node.

References-found: 21

