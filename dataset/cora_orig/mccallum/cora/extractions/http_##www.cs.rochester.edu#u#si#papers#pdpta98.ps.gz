URL: http://www.cs.rochester.edu/u/si/papers/pdpta98.ps.gz
Refering-URL: http://www.cs.rochester.edu/stats/oldmonths/1998.06/docs-name.html
Root-URL: 
Email: si@cs.rochester.edu, fmarkatos|sevasg@ics.forth.gr  
Title: On Using Network Memory to Improve the Performance of Transaction-Based Systems  
Author: Sotiris Ioannidis Evangelos P. Markatos Julia Sevaslidou 
Address: Crete, GREECE  
Affiliation: Computer Architecture and VLSI Systems Group Institute of Computer Science (ICS) Foundation for Research Technology Hellas (FORTH),  
Abstract: Transactions have been valued for their atomicity and recoverability properties that are useful to several systems, ranging from CAD environment to large-scale databases. Unfortunately, the performance of transaction-based systems is usually limited by the magnetic disks that are used to hold the data. In this paper we describe how to use the collective main memory in a Network of Workstations (NOW) to improve the performance of transaction-based systems. We describe the design of our system and its implementation in two independent transaction-based systems, namely EXODUS, and RVM. We evaluate the performance of our prototype using several database benchmarks (like OO7 and TPC-A). Our experimental results indicate that our system delivers up to two orders of magnitude performance improvement compared to its predecessors. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> T.E. Anderson, D.E. Culler, and D.A. Patter-son. </author> <title> A case for NOW. </title> <booktitle> IEEE Micro, </booktitle> <month> February </month> <year> 1995. </year>
Reference-contexts: In this paper we describe a novel way to improve the performance of transaction management by using the collective main memory (hereafter called remote memory) in a Network of Workstations (NOW) <ref> [1] </ref>. fl This work was supported in part by PENED project "Exploitation of idle memory in a workstation cluster" (2041 2270/1-2-95). Several of the described experiments were performed at the Parallab High Performance Computing Centre and the University of Rochester.
Reference: [2] <author> M. Dahlin. </author> <title> Serverless Network File Systems. </title> <type> PhD thesis, </type> <institution> UC Berkeley, </institution> <month> December </month> <year> 1995. </year>
Reference-contexts: This cache is larger than any single workstation can provide, and thus can be used to hold large amounts of data. Reading data from remote main memory (over a high speed interconnection network), was shown to be significantly faster than reading data from a (local) magnetic disk <ref> [2] </ref>. Architecture trends suggest that this disparity between magnetic disks and interconnection networks will continue to increase with time [2]. Speeding up Synchronous Write Operations to Reliable Storage: Transaction-based systems frequently use synchronous write operations to force all modified data to disk at transaction commit time. <p> Reading data from remote main memory (over a high speed interconnection network), was shown to be significantly faster than reading data from a (local) magnetic disk <ref> [2] </ref>. Architecture trends suggest that this disparity between magnetic disks and interconnection networks will continue to increase with time [2]. Speeding up Synchronous Write Operations to Reliable Storage: Transaction-based systems frequently use synchronous write operations to force all modified data to disk at transaction commit time.
Reference: [3] <author> T. E. Anderson, M. D. Dahlin, J. M. Neefe, D. A. Patterson, D. S. Roselli, and R. Y. Wang. </author> <title> Serverless network file systems. </title> <journal> TOCS, </journal> <month> February </month> <year> 1996. </year>
Reference-contexts: The first of the above issues has been somewhat explored in the areas of file systems <ref> [3, 4, 5] </ref>, paging [6] and global memory databases for workstation clusters [7, 8]. The thrust of this paper is on exploring the second issue. <p> the system, not just transaction commit), our measurements suggest that REX results in noticeable performance improvement over the unmodified EXODUS storage manager. 4 Related Work Using Remote Main Memory to improve the performance and reliability of I/O in a NOW has been previously explored in the literature in file systems <ref> [3, 4, 5] </ref>, pagers [13, 14, 6], even Distributed Shared Memory systems [15]. The closest to our research is the Harp file system [5]. Harp uses replicated file servers to tolerate single server failure. <p> Systems like Rio may simplify the implementation of our approach significantly. However it requires operating system kernel changes not necessary in our approach. Network file systems like Sprite [17] and xfs <ref> [3, 18] </ref>, can also be used to store replicated data and build a reliable network main memory. However, our approach, would still result in better performance due to the minimum (block) size transfers that all file systems are forced to have.
Reference: [4] <author> J. Hartman and J. Ousterhout. </author> <title> The zebra striped network file system. </title> <booktitle> 14th SOSP, </booktitle> <month> De-cember </month> <year> 1993. </year>
Reference-contexts: The first of the above issues has been somewhat explored in the areas of file systems <ref> [3, 4, 5] </ref>, paging [6] and global memory databases for workstation clusters [7, 8]. The thrust of this paper is on exploring the second issue. <p> the system, not just transaction commit), our measurements suggest that REX results in noticeable performance improvement over the unmodified EXODUS storage manager. 4 Related Work Using Remote Main Memory to improve the performance and reliability of I/O in a NOW has been previously explored in the literature in file systems <ref> [3, 4, 5] </ref>, pagers [13, 14, 6], even Distributed Shared Memory systems [15]. The closest to our research is the Harp file system [5]. Harp uses replicated file servers to tolerate single server failure.
Reference: [5] <author> B. Liskov, S. Ghemawat, R. Gruber, P. John-son, L. Shrira, and M. Williams. </author> <title> Replication in the Harp file system. </title> <booktitle> 13th SOSP, </booktitle> <year> 1991. </year>
Reference-contexts: The first of the above issues has been somewhat explored in the areas of file systems <ref> [3, 4, 5] </ref>, paging [6] and global memory databases for workstation clusters [7, 8]. The thrust of this paper is on exploring the second issue. <p> the system, not just transaction commit), our measurements suggest that REX results in noticeable performance improvement over the unmodified EXODUS storage manager. 4 Related Work Using Remote Main Memory to improve the performance and reliability of I/O in a NOW has been previously explored in the literature in file systems <ref> [3, 4, 5] </ref>, pagers [13, 14, 6], even Distributed Shared Memory systems [15]. The closest to our research is the Harp file system [5]. Harp uses replicated file servers to tolerate single server failure. <p> The closest to our research is the Harp file system <ref> [5] </ref>. Harp uses replicated file servers to tolerate single server failure. Each file server is equipped with a UPS to tolerate power failures, and speedup synchronous write operations. Although we use similar techniques, there are several differences between our work and Harp.
Reference: [6] <author> E.P. Markatos and G. Dramitinos. </author> <title> Implementation of a reliable remote memory pager. </title> <booktitle> In Usenix Technical Conference, </booktitle> <year> 1996. </year>
Reference-contexts: The first of the above issues has been somewhat explored in the areas of file systems [3, 4, 5], paging <ref> [6] </ref> and global memory databases for workstation clusters [7, 8]. The thrust of this paper is on exploring the second issue. Transaction-based systems make many small synchronous write operations to stable storage, and thus they are going to benefit significantly from any improvements to synchronous disk write operations. <p> transaction commit), our measurements suggest that REX results in noticeable performance improvement over the unmodified EXODUS storage manager. 4 Related Work Using Remote Main Memory to improve the performance and reliability of I/O in a NOW has been previously explored in the literature in file systems [3, 4, 5], pagers <ref> [13, 14, 6] </ref>, even Distributed Shared Memory systems [15]. The closest to our research is the Harp file system [5]. Harp uses replicated file servers to tolerate single server failure. Each file server is equipped with a UPS to tolerate power failures, and speedup synchronous write operations.
Reference: [7] <author> M. Franklin, M. Carey, and M. Livny. </author> <title> Global memory management in client-server dbms architectures. </title> <booktitle> In 18th VLDB Conference, </booktitle> <month> August </month> <year> 1992. </year>
Reference-contexts: The first of the above issues has been somewhat explored in the areas of file systems [3, 4, 5], paging [6] and global memory databases for workstation clusters <ref> [7, 8] </ref>. The thrust of this paper is on exploring the second issue. Transaction-based systems make many small synchronous write operations to stable storage, and thus they are going to benefit significantly from any improvements to synchronous disk write operations. <p> Moreover, our approach would result in wider portability since, being user-level, it can run on top of any operating system. Franklin et al. have proposed the use of remote main memory in a NOW as a large database cache <ref> [7] </ref>. Feeley et al. proposed a generalized memory management system, where the collective main memory of all workstations in a cluster is handled by the operating system [19]. We believe that our approach complements this work in the sense that both [7] and [19] improve the performance of read accesses (by <p> memory in a NOW as a large database cache <ref> [7] </ref>. Feeley et al. proposed a generalized memory management system, where the collective main memory of all workstations in a cluster is handled by the operating system [19]. We believe that our approach complements this work in the sense that both [7] and [19] improve the performance of read accesses (by providing large caches), while our approach improves the performance of synchronous write accesses. Papathanasiou and Markatos [20] describe a system to improve performance of main memory databases on top of Networks of Workstations.
Reference: [8] <author> J. Griffioen, R. Vingralek, T. Anderson, and Y. Breitbart. Derby: </author> <title> A Memory Management System for Distributed Main Memory Databases. </title> <booktitle> In RIDE '96, </booktitle> <month> February </month> <year> 1996. </year>
Reference-contexts: The first of the above issues has been somewhat explored in the areas of file systems [3, 4, 5], paging [6] and global memory databases for workstation clusters <ref> [7, 8] </ref>. The thrust of this paper is on exploring the second issue. Transaction-based systems make many small synchronous write operations to stable storage, and thus they are going to benefit significantly from any improvements to synchronous disk write operations. <p> The approach described in this paper is applicable to all databases, whether they fit in main memory or not. Griffioen et al. proposed the DERBY storage manager, that exploits remote memory and UPSs to reliably store a transaction's data <ref> [8] </ref>. They simulate the performance of their system and provide encouraging results.
Reference: [9] <author> M. Carey et al. </author> <title> The EXODUS extensible DBMS project: An overview. </title> <booktitle> In Readings in Object-Oriented Database Systems. </booktitle> <year> 1990. </year>
Reference-contexts: To demonstrate our approach, we implemented it in two existing transaction-based systems: the EXODUS storage manager <ref> [9] </ref>, and the RVM (Recoverable Virtual Memory) System [10]. Section 2 describes the design and the implementation of our systems. We report our performance results in section 3. Section 4 places our work in context by surveying previous work and comparing it with our approach. <p> Thus, sensitive data that need to be synchronously written to disk, can be (synchronously) written to remote main memory with the same level of reliability. 2.2 EXODUS and RVM To illustrate our approach we have modified a lightweight transaction-based system called RVM [10] and the EXODUS storage manager <ref> [9] </ref> to use remote memory (instead of disks) for synchronous write operations. After studying the performance of the systems, we concluded that they spend a significant amount of their time, synchronously writing transaction data to their log file, which is used to implement a two-phase commit protocol. <p> The workstations are connected through EXODUS. a 100 Mbps FDDI, and a 10Mbps Ethernet interconnection network. 3.7.2 OO7 On top of EXODUS we ran a common database benchmark called OO7 [12]. We used three versions of EXODUS: * EXODUS: Is the unmodified EXODUS sys tem <ref> [9] </ref>. * REX-FDDI: Is our modified EXODUS (REX) system running on top of FDDI. * REX-ETHERNET: Is REX running on top of Ethernet. EXODUS. We see that in all cases REX has superior performance compared to the unmodified EXODUS systems.
Reference: [10] <author> M. Satyanarayanan, Henry H Mashburn, Puneet Kumar, David C. Steere, and James J. Kistler. </author> <title> Lightweight recoverable virtual memory. </title> <journal> TOCS, </journal> <year> 1994. </year>
Reference-contexts: To demonstrate our approach, we implemented it in two existing transaction-based systems: the EXODUS storage manager [9], and the RVM (Recoverable Virtual Memory) System <ref> [10] </ref>. Section 2 describes the design and the implementation of our systems. We report our performance results in section 3. Section 4 places our work in context by surveying previous work and comparing it with our approach. <p> Thus, sensitive data that need to be synchronously written to disk, can be (synchronously) written to remote main memory with the same level of reliability. 2.2 EXODUS and RVM To illustrate our approach we have modified a lightweight transaction-based system called RVM <ref> [10] </ref> and the EXODUS storage manager [9] to use remote memory (instead of disks) for synchronous write operations. <p> The workstations are connected through an Ethernet, an FDDI, and a Memory Channel Interconnection Network [11]. In our experiments we demonstrated the performance of RRVM and compared it with its predecessor RVM <ref> [10] </ref>. We have experimented with four system configurations: * RVM: Is the unmodified RVM system. * RRVM-ETHERNET: Is RRVM running on top of Ethernet. 2 Our modification were rather small. Out of about 30,000 lines of RVM code, we modified (or added) less than 600 lines (2%). <p> We see that the unmodified RVM system is able to sustain up to at most 40 transactions per second for small transactions, which agrees with previously reported results <ref> [10] </ref>. However, the performance of RRVM-ETHERNET, RRVM-FDDI and RRVM-MC manages to sustain close to 3,000 transactions per second: almost two orders of magnitude improvement over unmodified RVM. <p> as the number of participating workstations increases, but it is still significantly better than that of RVM. 3.6 TPC-A To place our RRVM system in the right perspective with previously published performance results, we ran the widely used TPC-A database benchmark (which was also used to evaluate the original system <ref> [10] </ref>) , on top of it. Our results, summarized in Figure 6, show the number of transactions per second that each system can achieve. The experiments present sequential, random and localized accesses.
Reference: [11] <author> R. Gillett. </author> <title> Memory channel network for pci. </title> <journal> IEEE Micro, </journal> <volume> 16(1) </volume> <pages> 12-18, </pages> <month> February </month> <year> 1996. </year>
Reference-contexts: The workstations are connected through an Ethernet, an FDDI, and a Memory Channel Interconnection Network <ref> [11] </ref>. In our experiments we demonstrated the performance of RRVM and compared it with its predecessor RVM [10]. We have experimented with four system configurations: * RVM: Is the unmodified RVM system. * RRVM-ETHERNET: Is RRVM running on top of Ethernet. 2 Our modification were rather small.
Reference: [12] <author> M. Carey, D. DeWitt, and J. Naughton. </author> <booktitle> The OO7 bechmark. In ACM SIGMOD, </booktitle> <year> 1993. </year>
Reference-contexts: The workstations are connected through EXODUS. a 100 Mbps FDDI, and a 10Mbps Ethernet interconnection network. 3.7.2 OO7 On top of EXODUS we ran a common database benchmark called OO7 <ref> [12] </ref>. We used three versions of EXODUS: * EXODUS: Is the unmodified EXODUS sys tem [9]. * REX-FDDI: Is our modified EXODUS (REX) system running on top of FDDI. * REX-ETHERNET: Is REX running on top of Ethernet. EXODUS.
Reference: [13] <author> L. Iftode, K. Li, and K. Petersen. </author> <title> Memory servers for multicomputers. </title> <booktitle> In COMPCON 93, </booktitle> <year> 1993. </year>
Reference-contexts: transaction commit), our measurements suggest that REX results in noticeable performance improvement over the unmodified EXODUS storage manager. 4 Related Work Using Remote Main Memory to improve the performance and reliability of I/O in a NOW has been previously explored in the literature in file systems [3, 4, 5], pagers <ref> [13, 14, 6] </ref>, even Distributed Shared Memory systems [15]. The closest to our research is the Harp file system [5]. Harp uses replicated file servers to tolerate single server failure. Each file server is equipped with a UPS to tolerate power failures, and speedup synchronous write operations.
Reference: [14] <author> K. Li and K. Petersen. </author> <title> Evaluation of memory system extensions. </title> <booktitle> In ISCA, </booktitle> <year> 1991. </year>
Reference-contexts: transaction commit), our measurements suggest that REX results in noticeable performance improvement over the unmodified EXODUS storage manager. 4 Related Work Using Remote Main Memory to improve the performance and reliability of I/O in a NOW has been previously explored in the literature in file systems [3, 4, 5], pagers <ref> [13, 14, 6] </ref>, even Distributed Shared Memory systems [15]. The closest to our research is the Harp file system [5]. Harp uses replicated file servers to tolerate single server failure. Each file server is equipped with a UPS to tolerate power failures, and speedup synchronous write operations.
Reference: [15] <author> M. Costa, P. Guedes, M. Sequeira, N. Neves, and M. Castro. </author> <title> Lightweight logging for lazy release consistent distributed shared memory. </title> <booktitle> In OSDI, </booktitle> <year> 1996. </year>
Reference-contexts: in noticeable performance improvement over the unmodified EXODUS storage manager. 4 Related Work Using Remote Main Memory to improve the performance and reliability of I/O in a NOW has been previously explored in the literature in file systems [3, 4, 5], pagers [13, 14, 6], even Distributed Shared Memory systems <ref> [15] </ref>. The closest to our research is the Harp file system [5]. Harp uses replicated file servers to tolerate single server failure. Each file server is equipped with a UPS to tolerate power failures, and speedup synchronous write operations.
Reference: [16] <author> Peter M. Chen, Wee Teck Ng, Subhachandra Chandra, Christopher Aycock, Gurushankar Rajamani, and David Lowell. </author> <title> The Rio file cache: Surviving operating system crashes. </title> <booktitle> In ASPLOS, </booktitle> <year> 1996. </year>
Reference-contexts: Open User Level Implementation: RRVM is linked with user applications as a library, outside the operating system kernel. Thus, it is portable and easily modifiable. The Rio file cache has been designed to survive operating system crashes by not destroying its main memory contents in case of a crash <ref> [16] </ref>. Systems like Rio may simplify the implementation of our approach significantly. However it requires operating system kernel changes not necessary in our approach. Network file systems like Sprite [17] and xfs [3, 18], can also be used to store replicated data and build a reliable network main memory.
Reference: [17] <author> M. Nelson, B. Welch, and J. Ousterhout. </author> <title> Caching in the Sprite network file system. </title> <journal> TOCS, </journal> <month> February </month> <year> 1988. </year>
Reference-contexts: Systems like Rio may simplify the implementation of our approach significantly. However it requires operating system kernel changes not necessary in our approach. Network file systems like Sprite <ref> [17] </ref> and xfs [3, 18], can also be used to store replicated data and build a reliable network main memory. However, our approach, would still result in better performance due to the minimum (block) size transfers that all file systems are forced to have.
Reference: [18] <author> M.D. Dahlin, R.Y. Wang, T.E. Anderson, and D.A. Patterson. </author> <title> Cooperative cahing: Using remote client memory to improve file system performance. </title> <booktitle> In OSDI, </booktitle> <year> 1994. </year>
Reference-contexts: Systems like Rio may simplify the implementation of our approach significantly. However it requires operating system kernel changes not necessary in our approach. Network file systems like Sprite [17] and xfs <ref> [3, 18] </ref>, can also be used to store replicated data and build a reliable network main memory. However, our approach, would still result in better performance due to the minimum (block) size transfers that all file systems are forced to have.
Reference: [19] <author> M. J. Feeley, W. E. Morgan, F. H. Pighin, A. R. Karlin, H. M. Levy, and C. A. Thekkath. </author> <title> Implementing global memory management in a workstation cluster. </title> <booktitle> In 15th SOSP, </booktitle> <year> 1995. </year>
Reference-contexts: Franklin et al. have proposed the use of remote main memory in a NOW as a large database cache [7]. Feeley et al. proposed a generalized memory management system, where the collective main memory of all workstations in a cluster is handled by the operating system <ref> [19] </ref>. We believe that our approach complements this work in the sense that both [7] and [19] improve the performance of read accesses (by providing large caches), while our approach improves the performance of synchronous write accesses. <p> Feeley et al. proposed a generalized memory management system, where the collective main memory of all workstations in a cluster is handled by the operating system <ref> [19] </ref>. We believe that our approach complements this work in the sense that both [7] and [19] improve the performance of read accesses (by providing large caches), while our approach improves the performance of synchronous write accesses. Papathanasiou and Markatos [20] describe a system to improve performance of main memory databases on top of Networks of Workstations.
Reference: [20] <author> A. E. Papathanassiou and E. P. Markatos. </author> <title> Lightweight transactions on networks of workstations. </title> <booktitle> In ICDCS, </booktitle> <year> 1998. </year>
Reference-contexts: We believe that our approach complements this work in the sense that both [7] and [19] improve the performance of read accesses (by providing large caches), while our approach improves the performance of synchronous write accesses. Papathanasiou and Markatos <ref> [20] </ref> describe a system to improve performance of main memory databases on top of Networks of Workstations. The approach described in this paper is applicable to all databases, whether they fit in main memory or not.
Reference: [21] <author> Michael Wu and Willy Zwaenepoel. eNVy: </author> <title> a non-volatile main memory storage system. </title> <booktitle> In ASPLOS, </booktitle> <year> 1994. </year>
Reference-contexts: To speed up database and file system write performance, several researchers have proposed to use special hardware. For example, Wu and Zwaenepoel have designed and simulated eNVy <ref> [21] </ref>, and Baker et al. have proposed the use of battery-backed SRAM [22]. Our approach has the advantage that is does not need any specialized hardware 5 Conclusions In this paper we described how to use several workstations in a NOW to provide fast and reliable access to stable storage.
Reference: [22] <author> M. Baker, S. Asami, E. Deprit, J. Ousterhout, and M. Seltzer. </author> <title> Non-volatile memory for fast, reliable file systems. </title> <booktitle> In ASPLOS, </booktitle> <year> 1992. </year>
Reference-contexts: To speed up database and file system write performance, several researchers have proposed to use special hardware. For example, Wu and Zwaenepoel have designed and simulated eNVy [21], and Baker et al. have proposed the use of battery-backed SRAM <ref> [22] </ref>. Our approach has the advantage that is does not need any specialized hardware 5 Conclusions In this paper we described how to use several workstations in a NOW to provide fast and reliable access to stable storage.
References-found: 22

