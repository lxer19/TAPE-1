URL: http://www.cs.berkeley.edu/~beymer/publications/iccv95.ps.Z
Refering-URL: http://www.cs.berkeley.edu/~beymer/publications.html
Root-URL: 
Email: email: beymer@ai.mit.edu, tp@ai.mit.edu  
Title: Face Recognition From One Example View  
Author: David Beymer and Tomaso Poggio 
Address: Cambridge, MA 01239, USA  
Affiliation: Artificial Intelligence Laboratory, and Center for Biological and Computational Learning Massachusetts Institute of Technology  
Abstract: To create a pose-invariant face recognizer, one strategy is the view-based approach, which uses a set of real example views at different poses. But what if we only have one real view available, such as a scanned passport photo - can we still recognize faces under different poses? Given one real view at a known pose, it is still possible to use the view-based approach by exploiting prior knowledge of faces to generate virtual views, or views of the face as seen from different poses. To represent prior knowledge, we use 2D example views of prototype faces under different rotations. We will develop example-based techniques for applying the rotation seen in the prototypes to essentially "rotate" the single real view which is available. Next, the combined set of one real and multiple virtual views is used as example views for a view-based, pose-invariant face recognizer. Our experiments suggest that among the techniques for expressing prior knowledge of faces, 2D example-based approaches should be considered alongside the more standard 3D modeling techniques. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Andrew C. Aitchison and Ian Craw. </author> <title> Synthetic images of faces - an approach to model-based face recognition. </title> <booktitle> In Proc. British Machine Vision Conference, </booktitle> <pages> pages 226-232, </pages> <year> 1991. </year>
Reference-contexts: Generic 3D models of the human face can be used to predict the appearance of a face under different pose-expression-lighting parameters. For synthesizing images of faces, 3D facial models have been explored in the computer graphics, computer vision, and model-based image coding communities (Aitchison and Craw <ref> [1] </ref>, Kang, Chen, and Hsu [13], Essa and Pentland [11], Akimoto, Suennaga, and Wallace [3], Waters and Terzopoulos [24], Aizawa, Harashima, and Saito [2]). A generic 3D model could also be applied to our scenario of face recognition from one example view.
Reference: [2] <author> K. Aizawa, H. Harashima, and T. Saito. </author> <title> Model-based analysis synthesis image coding (MBASIC) system for a person's face. Signal Processing: </title> <journal> Image Communication, </journal> <volume> 1 </volume> <pages> 139-152, </pages> <year> 1989. </year>
Reference-contexts: For synthesizing images of faces, 3D facial models have been explored in the computer graphics, computer vision, and model-based image coding communities (Aitchison and Craw [1], Kang, Chen, and Hsu [13], Essa and Pentland [11], Akimoto, Suennaga, and Wallace [3], Waters and Terzopoulos [24], Aizawa, Harashima, and Saito <ref> [2] </ref>). A generic 3D model could also be applied to our scenario of face recognition from one example view.
Reference: [3] <author> Takaaki Akimoto, Yasuhito Suenaga, and Richard S. Wallace. </author> <title> Automatic creation of 3D facial models. </title> <journal> IEEE Computer Graphics and Applications, </journal> <volume> 13(5) </volume> <pages> 16-22, </pages> <year> 1993. </year>
Reference-contexts: For synthesizing images of faces, 3D facial models have been explored in the computer graphics, computer vision, and model-based image coding communities (Aitchison and Craw [1], Kang, Chen, and Hsu [13], Essa and Pentland [11], Akimoto, Suennaga, and Wallace <ref> [3] </ref>, Waters and Terzopoulos [24], Aizawa, Harashima, and Saito [2]). A generic 3D model could also be applied to our scenario of face recognition from one example view.
Reference: [4] <author> Thaddeus Beier and Shawn Neely. </author> <title> Feature-based image metamorphosis. </title> <booktitle> In SIGGRAPH '92 Proceedings, </booktitle> <pages> pages 35-42, </pages> <address> Chicago, IL, </address> <year> 1992. </year>
Reference-contexts: Describing these techniques is beyond the scope of this paper; we only give references and sketch the techniques. The manual technique is borrowed from Beier and Neely's morphing technique in computer graphics <ref> [4] </ref>. In their technique, a manual, sparse set of corresponding line features are interpolated to derive correspondence everywhere on the pixel level. The second technique for computing interperson correspondence is based on our face "vectorizer" [6], which computes pixelwise correspondence between an input and the "average" face shape. <p> Fig. 4 shows a set of virtual views generated using this technique. Note that the prototype views must be of the same set of people across all nine views. We used a prototype set of 55 people, so we had to specify manual correspondence (again, using Beier and Neely <ref> [4] </ref>) for 9 views of each person to set up the shape free views. When generating the virtual views for a particular person, we would, of course, remove him from the prototype set if he were initially present, following a leave-one-out cross validation methodology.
Reference: [5] <author> J.R. Bergen and R. Hingorani. </author> <title> Hierarchical motion-based frame rate conversion. </title> <type> Technical report, </type> <institution> David Sarnoff Research Center, Princeton, </institution> <address> New Jersey, </address> <month> April </month> <year> 1990. </year>
Reference-contexts: In our scheme of pixelwise representation for shape, applying the parallel deformation approach boils down to mapping the 2D "rotation" deformation (y p;r y p ) onto the novel person's face. First, we compute the prototype deformation using a gradient-based optical flow algorithm <ref> [5] </ref>. Shown overlaid on the reference image on the left of Fig. 2, this 2D deformation specifies how to forward warp i p to i p;r and represents our "prior knowledge" of face rotation.
Reference: [6] <author> David Beymer. </author> <title> Vectorizing face images by interleaving shape and texture computations. </title> <journal> A.I. </journal> <volume> Memo No. </volume> <pages> 1537, </pages> <institution> Artificial Intelligence Laboratory, Massachusetts Institute of Technology, </institution> <year> 1995. </year>
Reference-contexts: The manual technique is borrowed from Beier and Neely's morphing technique in computer graphics [4]. In their technique, a manual, sparse set of corresponding line features are interpolated to derive correspondence everywhere on the pixel level. The second technique for computing interperson correspondence is based on our face "vectorizer" <ref> [6] </ref>, which computes pixelwise correspondence between an input and the "average" face shape. Correspondence between two arbitrary images can thus be found by vectorizing both, as now both images are in correspondence with the average shape. For both techniques virtual views will be shown and face recognition results given. <p> Since in our setup i n is an m4 pose of the face, this step means finding correspondence between i n and view m4's average face shape. Our image vectorizer, which we introduced in the previous subsection and explain in Beymer <ref> [6] </ref>, is used to simultaneously solve for both the correspondence and the prototype coefficients.
Reference: [7] <author> David Beymer and Tomaso Poggio. </author> <title> Face recognition from one example view. </title> <journal> A.I. </journal> <volume> Memo No. </volume> <pages> 1536, </pages> <institution> Artificial Intelligence Laboratory, Massachusetts Institute of Technology, </institution> <year> 1995. </year>
Reference-contexts: We next introduce notation for the prototype views and describe our representation for faces. Since we eventually wish to synthesize virtual views at different poses, the next two sections will focus on pose. Beymer and Poggio <ref> [7] </ref> suggest ways to apply these synthesis techniques for different lighting conditions and expressions. 2.1 Prototype views and face representation In our virtual views scenario, call the known pose of the real view the standard pose and the pose of the desired virtual view the transformed pose. <p> This was done for Fig. 3 (a) but not in 3 (b). Virtual shape was also processed slightly differently for the symmetric views in Fig. 3 (a); see <ref> [7] </ref> for the details. 3.1.2 Virtual texture We use the linear class idea to analyze the novel texture in terms of the prototypes at the standard view and m10 m9 m8 m7 m6 m15 m14 m13 m12 m11 real view, view m4, is available and we synthesize the remaining 14. reconstruct <p> For optimization purposes we process the set of 55 shape free prototype views using principal components and keep the top 28 eigenimages. Besides reducing the dimensionality N to 28, this orthogonalizes the prototype set, which should make the computed fi j more sta ble. Please refer to <ref> [7] </ref> for the details. 3.2 Experimental results 3.2.1 View-based recognizer In our view-based face recognizer [8], the 15 example views of Fig. 1 are stored for each person to handle pose invariance.
Reference: [8] <author> David J. Beymer. </author> <title> Face recognition under varying pose. </title> <booktitle> In Proceedings IEEE Conf. on Computer Vision and Pattern Recognition, </booktitle> <pages> pages 756-761, </pages> <address> Seattle, WA, </address> <year> 1994. </year>
Reference-contexts: One of the key remaining problems in face recognition is to handle the variability in appearance due to changes in pose, expression, and lighting conditions. There has been some recent work in this direction, such as view-based recog-nizers (Pentland, et al. [18], Beymer <ref> [8] </ref>) and deformable template approaches (Manjunath, et al. [16]). In addition to recognition, richer models for faces have been studied for analyzing varying illumination (Hallinan [12]) and expression (Yacoob and Davis [27], Essa and Pent-land [11]). <p> Besides reducing the dimensionality N to 28, this orthogonalizes the prototype set, which should make the computed fi j more sta ble. Please refer to [7] for the details. 3.2 Experimental results 3.2.1 View-based recognizer In our view-based face recognizer <ref> [8] </ref>, the 15 example views of Fig. 1 are stored for each person to handle pose invariance. To recognize an input view, our recognizer uses a strategy of registering the input with the example views followed by template matching. <p> The best match from the data base is reported as identified person. 3.2.2 Recognition results In this section we report the recognition rates obtained when virtual views were used in our view-based recog-nizer <ref> [8] </ref>. To test the recognizer, a set of 10 testing views per person were taken to randomly sample poses within the overall range of poses in Fig. 1. Roughly half of the test views include an image-plane rotation, so all three rotational degrees of freedom are tested. <p> An expected lower bound for recognition performance is when only view m4 plus its mirror reflection are used as example views. The best we could expect virtual views to perform is when the 15 views are actual real views, a case that has been documented in Beymer <ref> [8] </ref>. Virtual shape using manual in-terperson correspondences at 82% falls midway between the benchmark cases of 67% and 98%, so it shows that virtual views do benefit pose-invariant face recognition.
Reference: [9] <author> Vicki Bruce. </author> <title> Changing faces: Visual and non-visual coding processes in face recognition. </title> <journal> British Journal of Psychology, </journal> <volume> 73 </volume> <pages> 105-116, </pages> <year> 1982. </year>
Reference-contexts: Moses, Ull-man, and Edelman [17] have performed this experiment using testing views at a variety of poses and lighting conditions. While high recognition rates were observed in the subjects (97%), the subjects were only asked to discriminate between three different people. Bruce <ref> [9] </ref> performs a similar experiment where the subject is asked whether a face had appeared during training, and detection rates go down to either 76% or 60%, depending on the amount of pose/expression difference between the testing and training views.
Reference: [10] <author> Ian Craw and Peter Cameron. </author> <title> Parameterizing images for recognition and reconstruction. </title> <booktitle> In Proc. British Machine Vision Conference, </booktitle> <pages> pages 367-370, </pages> <year> 1991. </year>
Reference-contexts: The textural component is the original grey level image warped to a "shape-free" representation (Craw and Cameron <ref> [10] </ref>). That is, the geometrical differences among face images are factored out by warping the images to a common shape where all the features line up.
Reference: [11] <author> Irfan A. Essa and Alex Pentland. </author> <title> A vision system for observing and extracting facial action parameters. </title> <booktitle> In Proceedings IEEE Conf. on Computer Vision and Pattern Recognition, </booktitle> <pages> pages 76-83, </pages> <address> Seattle, WA, </address> <year> 1994. </year>
Reference-contexts: In addition to recognition, richer models for faces have been studied for analyzing varying illumination (Hallinan [12]) and expression (Yacoob and Davis [27], Essa and Pent-land <ref> [11] </ref>). In this paper, we address the problem of recognizing faces under varying pose when only one example view per person is available. For example, perhaps just a driver's license photograph is available for each person in the database. <p> For synthesizing images of faces, 3D facial models have been explored in the computer graphics, computer vision, and model-based image coding communities (Aitchison and Craw [1], Kang, Chen, and Hsu [13], Essa and Pentland <ref> [11] </ref>, Akimoto, Suennaga, and Wallace [3], Waters and Terzopoulos [24], Aizawa, Harashima, and Saito [2]). A generic 3D model could also be applied to our scenario of face recognition from one example view.
Reference: [12] <author> Peter W. Hallinan. </author> <title> A low-dimensional representation of human faces for arbitrary lighting conditions. </title> <booktitle> In Proceedings IEEE Conf. on Computer Vision and Pattern Recognition, </booktitle> <pages> pages 995-999, </pages> <address> Seattle, WA, </address> <year> 1994. </year>
Reference-contexts: There has been some recent work in this direction, such as view-based recog-nizers (Pentland, et al. [18], Beymer [8]) and deformable template approaches (Manjunath, et al. [16]). In addition to recognition, richer models for faces have been studied for analyzing varying illumination (Hallinan <ref> [12] </ref>) and expression (Yacoob and Davis [27], Essa and Pent-land [11]). In this paper, we address the problem of recognizing faces under varying pose when only one example view per person is available. For example, perhaps just a driver's license photograph is available for each person in the database.
Reference: [13] <author> Chii-Yuan Kang, Yung-Sheng Chen, and Wen-Hsing Hsu. </author> <title> Mapping a lifelike 2.5D human face via an automatic approach. </title> <booktitle> In Proceedings IEEE Conf. on Computer Vision and Pattern Recognition, </booktitle> <pages> pages 611-612, </pages> <address> New York, NY, </address> <month> June </month> <year> 1993. </year>
Reference-contexts: For synthesizing images of faces, 3D facial models have been explored in the computer graphics, computer vision, and model-based image coding communities (Aitchison and Craw [1], Kang, Chen, and Hsu <ref> [13] </ref>, Essa and Pentland [11], Akimoto, Suennaga, and Wallace [3], Waters and Terzopoulos [24], Aizawa, Harashima, and Saito [2]). A generic 3D model could also be applied to our scenario of face recognition from one example view.
Reference: [14] <author> Martin Lades, Jan C. Vorbruggen, Joachim Buhmann, Jorg Lange, Christoph v.d. Malsburg, Rolf P. Wurtz, and Wolfgang Ko-nen. </author> <title> Distortion invariant object recognition in the dynamic link architecture. </title> <journal> IEEE Transactions on Computers, </journal> <volume> 42(3), </volume> <month> March </month> <year> 1993. </year>
Reference: [15] <author> Maria Lando and Shimon Edelman. </author> <title> Generalization from a single view in face recognition. </title> <booktitle> In Proceedings, International Workshop on Automatic Face- and Gesture-Recognition, </booktitle> <pages> pages 80-85, </pages> <address> Zurich, </address> <year> 1995. </year>
Reference-contexts: Recognition performance will be reported on a separate test set of faces that cover a range of rotations both in and out of the image plane. Independent from our work, Lando and Edelman <ref> [15] </ref> have recently investigated the same overall question - generalization from a single view in face recognition - using a similar example-based technique for representing prior knowledge of faces. 2 Theory How can one generate new views of an object given just one view plus prior knowledge of the class of <p> Schyns and Bulthoff [22] obtain a low recognition rate, but their results are difficult to compare since their stimuli are Gouraud shaded 3D faces that exclude texture information. Lando and Edel-man <ref> [15] </ref> have recently performed computational experiments to replicate earlier psychophysical results in [17]. A recognition rate of only 76% was reported, but the authors suggest that this may be improved by using a two-stage classifier instead of a single-stage one. <p> Direct comparison of our results to related face recognition systems is difficult because of differences in example and testing views. Besides Lando and Edelman <ref> [15] </ref>, probably the two more comparable results are from Man-junath, et al. [16], who obtain 86% on a database of 86 people, and Pentland, et al. [18], whose extrapolation experiment with view-based eigenspaces yields 83% on a database of 21 people.
Reference: [16] <author> B.S. Manjunath, R. Chellappa, and C. von der Malsburg. </author> <title> A feature based approach to face recognition. </title> <booktitle> In Proceedings IEEE Conf. on Computer Vision and Pattern Recognition, </booktitle> <pages> pages 373-378, </pages> <year> 1992. </year>
Reference-contexts: There has been some recent work in this direction, such as view-based recog-nizers (Pentland, et al. [18], Beymer [8]) and deformable template approaches (Manjunath, et al. <ref> [16] </ref>). In addition to recognition, richer models for faces have been studied for analyzing varying illumination (Hallinan [12]) and expression (Yacoob and Davis [27], Essa and Pent-land [11]). In this paper, we address the problem of recognizing faces under varying pose when only one example view per person is available. <p> Direct comparison of our results to related face recognition systems is difficult because of differences in example and testing views. Besides Lando and Edelman [15], probably the two more comparable results are from Man-junath, et al. <ref> [16] </ref>, who obtain 86% on a database of 86 people, and Pentland, et al. [18], whose extrapolation experiment with view-based eigenspaces yields 83% on a database of 21 people.
Reference: [17] <author> Yael Moses, Shimon Ullman, and Shimon Edelman. </author> <title> Generalization to novel images in upright and inverted faces. </title> <type> Technical Report CC93-14, </type> <institution> The Weizmann Institute of Science, </institution> <year> 1993. </year>
Reference-contexts: After studying the training images, the subject would be asked to identify new images of the people under a variety of poses. Moses, Ull-man, and Edelman <ref> [17] </ref> have performed this experiment using testing views at a variety of poses and lighting conditions. While high recognition rates were observed in the subjects (97%), the subjects were only asked to discriminate between three different people. <p> Schyns and Bulthoff [22] obtain a low recognition rate, but their results are difficult to compare since their stimuli are Gouraud shaded 3D faces that exclude texture information. Lando and Edel-man [15] have recently performed computational experiments to replicate earlier psychophysical results in <ref> [17] </ref>. A recognition rate of only 76% was reported, but the authors suggest that this may be improved by using a two-stage classifier instead of a single-stage one. Direct comparison of our results to related face recognition systems is difficult because of differences in example and testing views.
Reference: [18] <author> Alex Pentland, Baback Moghaddam, and Thad Starner. </author> <title> View-based and modular eigenspaces for face recognition. </title> <booktitle> In Proceedings IEEE Conf. on Computer Vision and Pattern Recognition, </booktitle> <pages> pages 84-91, </pages> <address> Seattle, WA, </address> <year> 1994. </year>
Reference-contexts: One of the key remaining problems in face recognition is to handle the variability in appearance due to changes in pose, expression, and lighting conditions. There has been some recent work in this direction, such as view-based recog-nizers (Pentland, et al. <ref> [18] </ref>, Beymer [8]) and deformable template approaches (Manjunath, et al. [16]). In addition to recognition, richer models for faces have been studied for analyzing varying illumination (Hallinan [12]) and expression (Yacoob and Davis [27], Essa and Pent-land [11]). <p> Similarly to the shape case, this relies on the assumption that the space of grey level textures of faces is linearly spanned by a set of example images. The validity of this assumption is borne out by recent successful face recognition systems (e.g. eigenfaces, Pentland, et al. <ref> [18] </ref>). <p> Besides Lando and Edelman [15], probably the two more comparable results are from Man-junath, et al. [16], who obtain 86% on a database of 86 people, and Pentland, et al. <ref> [18] </ref>, whose extrapolation experiment with view-based eigenspaces yields 83% on a database of 21 people. In both cases, the system is trained on a set of views (vs. just one for ours) and recognition performance is tested on views from outside the pose-expression space of the training set.
Reference: [19] <author> T. Poggio. </author> <title> 3D object recognition: on a result by Basri and Ull-man. </title> <type> Technical Report # 9005-03, IRST, </type> <institution> Povo, Italy, </institution> <year> 1990. </year>
Reference-contexts: However, as the linear combination approach to 3D object recognition has recently demonstrated (Ullman and Basri [25], Pog-gio <ref> [19] </ref>), 3D models may be bypassed by simply interpolating between a small set of 2D views. Furthermore, if one includes prior information such as object symmetry, then Poggio and Vetter [21] have shown that only one view is required for unique recognition.
Reference: [20] <author> Tomaso Poggio and Roberto Brunelli. </author> <title> A novel approach to graphics. </title> <journal> A.I. </journal> <volume> Memo No. </volume> <pages> 1354, </pages> <institution> Artificial Intelligence Laboratory, Mas-sachusetts Institute of Technology, </institution> <year> 1992. </year>
Reference-contexts: Consider as a special case the deformation approach with just one prototype. In this case, the novel face is deformed in a manner that imitates the deformation seen in the prototype. This is similar to actor-based animation (Williams [26]), and Poggio and Brunelli <ref> [20] </ref>, who call it parallel deformation, have suggested it as a computer graphics tool for animating objects when provided with just one view.
Reference: [21] <author> Tomaso Poggio and Thomas Vetter. </author> <title> Recognition and structure from one 2D model view: Observations on prototypes, object classes, and symmetries. </title> <journal> A.I. </journal> <volume> Memo No. </volume> <pages> 1347, </pages> <institution> Artificial Intelligence Laboratory, Massachusetts Institute of Technology, </institution> <year> 1992. </year>
Reference-contexts: Given one view of a person, we will propose a method for using the information in the prototype views to synthesize new views of the person, views from different rotations in our case. Following Poggio and Vetter <ref> [21] </ref>, we call these synthesized views virtual views. After discussing methods for generating virtual views, we evaluate their usefulness in a view-based pose-invariant face recognizer. Given only one real example view per person, we will synthesize a set of rotated virtual views, views that cover up/down and left/right rotations. <p> However, as the linear combination approach to 3D object recognition has recently demonstrated (Ullman and Basri [25], Pog-gio [19]), 3D models may be bypassed by simply interpolating between a small set of 2D views. Furthermore, if one includes prior information such as object symmetry, then Poggio and Vetter <ref> [21] </ref> have shown that only one view is required for unique recognition. In this paper, we use views of prototype faces as our prior knowledge of faces, motivated by the potential for using a simple example-based approach as an alternative to the more expensive and complex 3D model-based approach. <p> the shape will suffice. 2.2 Generating rotated virtual views using prototype views Given the prototype views, we now introduce techniques for generating the shape and texture components of rotated virtual views. 2.2.1 Virtual shape The theory underlying our method for synthesizing virtual shapes, Poggio and Vetter's concept of linear classes <ref> [21] </ref>, relies on the assumption that the space of 3D face shapes for a given pose is spanned by a set of 3D example shapes, taken here to be the prototypes. <p> For the single real view, an off-center view was favored over, say, a frontal view because of the recognition results for bilaterally symmetric objects of Poggio and Vetter <ref> [21] </ref>. When the single real view is from a nondegenerate pose (i.e. mirror reflection is not equal to original view), then the mirror reflection immediately provides a second view that can be used for recognition.
Reference: [22] <author> Phillipe G. Schyns and Heinrich H. Bulthoff. </author> <title> Conditions for viewpoint invariant face recognition. </title> <journal> A.I. </journal> <volume> Memo No. </volume> <pages> 1432, </pages> <institution> Artificial Intelligence Laboratory, Massachusetts Institute of Technology, </institution> <year> 1993. </year>
Reference-contexts: The choice of an off-center view is also supported by the psychophysical experiments of Schyns and Bulthoff <ref> [22] </ref>. They found that when humans are trained on just one pose and tested on many, recognition performance is better when the single training view is an off-center one as opposed to a frontal pose. <p> Bruce [9] performs a similar experiment where the subject is asked whether a face had appeared during training, and detection rates go down to either 76% or 60%, depending on the amount of pose/expression difference between the testing and training views. Schyns and Bulthoff <ref> [22] </ref> obtain a low recognition rate, but their results are difficult to compare since their stimuli are Gouraud shaded 3D faces that exclude texture information. Lando and Edel-man [15] have recently performed computational experiments to replicate earlier psychophysical results in [17].
Reference: [23] <author> Pawan Sinha. </author> <title> Object recognition via image invariances. </title> <institution> Investigative Ophthalmology and Visual Science, 35(4):1626, </institution> <year> 1994. </year>
Reference-contexts: For example, the invariant features approach records features in the example view that do not change as pose-expression-lighting parameters change, features such as color or geometric invariants. While not yet applied to face recognition, this approach has been used for face detection under varying illumination (Sinha <ref> [23] </ref>). In the flexible matching approach (von der Malsburg and collaborators [16][14]), the input image is deformed in 2D to match the example view.
Reference: [24] <author> Demetri Terzopoulos and Keith Waters. </author> <title> Analysis of facial images using physical and anatomical models. </title> <booktitle> In Proceedings of the International Conference on Computer Vision, </booktitle> <pages> pages 727-732, </pages> <address> Osaka, Japan, </address> <month> December </month> <year> 1990. </year>
Reference-contexts: For synthesizing images of faces, 3D facial models have been explored in the computer graphics, computer vision, and model-based image coding communities (Aitchison and Craw [1], Kang, Chen, and Hsu [13], Essa and Pentland [11], Akimoto, Suennaga, and Wallace [3], Waters and Terzopoulos <ref> [24] </ref>, Aizawa, Harashima, and Saito [2]). A generic 3D model could also be applied to our scenario of face recognition from one example view.
Reference: [25] <author> Shimon Ullman and Ronen Basri. </author> <title> Recognition by linear combinations of models. </title> <journal> IEEE Transactions on Pattern Analysis and Machine Intelligence, </journal> <volume> 13(10) </volume> <pages> 992-1006, </pages> <year> 1991. </year>
Reference-contexts: However, as the linear combination approach to 3D object recognition has recently demonstrated (Ullman and Basri <ref> [25] </ref>, Pog-gio [19]), 3D models may be bypassed by simply interpolating between a small set of 2D views. Furthermore, if one includes prior information such as object symmetry, then Poggio and Vetter [21] have shown that only one view is required for unique recognition.
Reference: [26] <author> Lance Williams. </author> <title> Performance-driven facial animation. </title> <booktitle> In SIG-GRAPH '90 Proceedings, </booktitle> <pages> pages 235-242, </pages> <address> Dallas, TX, </address> <month> August </month> <year> 1990. </year>
Reference-contexts: This is the solution to the approximation equation (2). Consider as a special case the deformation approach with just one prototype. In this case, the novel face is deformed in a manner that imitates the deformation seen in the prototype. This is similar to actor-based animation (Williams <ref> [26] </ref>), and Poggio and Brunelli [20], who call it parallel deformation, have suggested it as a computer graphics tool for animating objects when provided with just one view.
Reference: [27] <author> Yaser Yacoob and Larry Davis. </author> <title> Computing spatio-temporal representations of human faces. </title> <booktitle> In Proceedings IEEE Conf. on Computer Vision and Pattern Recognition, </booktitle> <pages> pages 70-75, </pages> <address> Seattle, WA, </address> <year> 1994. </year>
Reference-contexts: There has been some recent work in this direction, such as view-based recog-nizers (Pentland, et al. [18], Beymer [8]) and deformable template approaches (Manjunath, et al. [16]). In addition to recognition, richer models for faces have been studied for analyzing varying illumination (Hallinan [12]) and expression (Yacoob and Davis <ref> [27] </ref>, Essa and Pent-land [11]). In this paper, we address the problem of recognizing faces under varying pose when only one example view per person is available. For example, perhaps just a driver's license photograph is available for each person in the database.
References-found: 27

