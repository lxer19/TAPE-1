URL: ftp://ftp.cc.gatech.edu/pub/gvu/tr/1992/92-09.ps.Z
Refering-URL: http://www.cs.gatech.edu/gvu/reports/1992/
Root-URL: 
Email: burgess@cc.gatech.edu  
Title: Techniques for Low Cost Spatial Audio  
Author: David A. Burgess 
Address: Atlanta, Georgia 30332  
Affiliation: Graphics Visualization and Usability Center Multimedia Group Georgia Institute of Technology  
Abstract: above or below the listener. This effect is achieved by using a better model of the human acoustic system, such as a dummy head with microphones embedded in the ears (Plenge, 1974). Because of the better model, the sound waves that arrive at the eardrums during playback are a close approximation of what would have actually arrived at a listeners eardrums during the original performance. Along with greater realism, binaural sound provides a number of other advantages over plain stereo. It conveys spatial information about each sound source to the listener. Furthermore, when sounds are spatially separated, a listener can easily distinguish different sources, and focus on those sources which are of interest while ignoring others. This is the so-called cocktail party effect (Cherry, 1953). If sounds can be recorded in this manner, an obvious next step is to convert monaural sounds to binaural sounds by artificially spatializing them. Given this ability, people who use sound in human-machine interfaces can gain the advantages that spatial sound offers. This goal has lead to research interest in the subject by the military, by NASA (Wenzel, et al, 1988), and by user interface designers (Ludwig, et al, 1990, 1991). This research is part of Mercator, a project to develop a nonvisual interface to X Window System applications for visually impaired software developers (Mynatt & Edwards, 1992). Until the proliferation of graphical user interfaces, blind professionals could excel in many fields that relied on the frequent use of computers. The all-text output of a TTY mapped reasonably well into a world of voice synthesizers and Braille devices. Today, however, as mice, icons, and pop-up menus invade their workplaces, these professionals are finding it progressively more difficult to use the applications they need. The goal of Mercator is to map the behaviors of generic, unmodified X applications into an auditory space. An important feature of Mercator is the use of spatial sound as the primary organizational cue, and for Mercator to be useful, this feature must be supported at modest cost. While a few commercial spatial sound systems exist, most are prohibitively expensive. One low-cost system, Focal Point tm , is presently available but does not provide an adequately open architecture for Mercator and is not available for most platforms which support the X Window Systems. This paper is organized in seven sections: Abstract There are a variety of potential uses for interactive spatial sound in human-computer interfaces, but hardware costs have made most of these applications impractical. Recently, however, single-chip digital signal processors have made real-time spatial audio an affordable possibility for many workstations. This paper describes an efficient spatialization technique and the associated computational requirements. Issues specific to the use of spatial audio in user interfaces are addressed. The paper also describes the design of a network server for spatial audio that can support a number of users at modest cost. 
Abstract-found: 1
Intro-found: 1
Reference: 1. <author> Blauert, J. </author> <title> (1983) Spatial Hearing: The Psychophysics of Human Sound Localization, </title> <publisher> MIT Press: </publisher> <address> Cambridge, MA. </address>
Reference-contexts: We will define azimuth as the component of theta in the horizontal plane and elevation as the component of theta in the vertical plane. (See Figure 1.) IDT (also called interaural group delay or interaural time 2. For a comprehensive reference on this subject, see Blauert <ref> (1983) </ref>. difference), the delay between a sound reaching the closer ear and the farther one, provides a primary cue for determining the lateral position of a sound. <p> The delay is zero for a source directly ahead, behind, or above the listener and roughly 0.63ms for a source to ones far left or far right. This delay is also dependent on both the frequency of the sound and the distance of the source <ref> (Blauert, 1983) </ref>. IDT manifests itself as a phase difference for signals below 1.6kHz and as an envelope delay for higher frequency sounds. A given IDT value constrains the position of a sound source to a hyperbola having an axis coincident with Z. <p> If a particular sound is put at a particular position in space, we call it an acoustic event <ref> (Blauert, 1983) </ref>. When spatial sound is used, the client workstation should not simply cache soundsit should cache acoustic events. If an acoustic event cache is used, sounds only need to be spatialized when they are moved to new positions.
Reference: 2. <author> Burgess, </author> <title> D.A (1992) Real-time audio spatialization with inexpensive hardware, </title> <type> GVU Tech. Report GIT-GVU-92-20. </type>
Reference-contexts: It is believed that the important features of the HRTF are consistent enough that one such set of filters may be suitable for a large portion of the population <ref> (Wenzel, 1992) </ref>. The spatialization technique we will focus on is the use of an empirical HRTF measured from the ears of a specific person (Wightman and Kistler, 1989a, 1989b). <p> In an informal test, it was found that for most listeners, monotonicity of perceived position with respect to target position can be maintained with FIRs as short as 1.45ms <ref> (Burgess, 1992) </ref>. Further study is needed to know the localization accuracy of such filters, but if found to perform adequately, they would allow as many as four 32kHz signals or eight 24kHz signals to be spatialized in real time.
Reference: 3. <author> Buxton, W., Gaver, W. & Bly, S. </author> <title> (1991) The Use of Non-Speech Audio at the Interface, Tutorial No. </title> <booktitle> 8, CHI91, ACM Conference on Human Factors in Com-puter Systems, </booktitle> <publisher> ACM Press: </publisher> <address> New York. </address>
Reference: 4. <author> Cherry, </author> <title> E.C. (1953) Some experiments on the recogni tion of speech with one or two ears, </title> <journal> J. Acoust. Soc. Am., </journal> <volume> 22, </volume> <pages> 61-62. </pages>
Reference: 5. <author> Gardner, </author> <title> M.B. (1968) Distance estimation of or apparent -oriented speech signals in anechoic space, 0 convolution engine acoustic event server filter tables audio library LAN acoustic event cache sound manager X application and X server Mercator user J. </title> <journal> Acoust. Soc. Am., </journal> <volume> 45, </volume> <pages> 47-53. </pages>
Reference: 6. <author> Gardner, </author> <title> M.B. (1973) Some monaural and binaural fac ets of median plane localization, </title> <journal> J. Acoust. Soc. Am., </journal> <volume> 54, </volume> <pages> 1489-1495. </pages>
Reference: 7. <author> Gaver, </author> <title> W.W. (1986) Auditory icons: Using sound in computer interfaces, </title> <journal> Human-Computer Interaction, </journal> <volume> 2, </volume> <pages> 167-177 </pages>
Reference: 8. <author> Gaver, </author> <title> W.W. (1989) The sonicfinder: An interface that uses auditory icons, </title> <journal> Human-Computer Interaction, </journal> <volume> 4, </volume> <pages> 67-94 </pages>
Reference: 9. <author> Laws, P. </author> <title> (1973) Entfernungshren und das Problem der Im-Kopf-Lokalisierheit von Hrereignissen [Auditory distance perception and the problem of in-head local-ization of sound images], </title> <journal> Acustica, </journal> <volume> 29, </volume> <pages> 243-259 </pages>
Reference-contexts: There are eight types of cues that are of particular importance in determining direction and distance. The four cues we will initially concern ourselves with are interaural delay time, or IDT (Rayleigh, 1907), head shadow (Mills, 1972), pinna response <ref> (Gardener, 1973) </ref>, and shoulder echoes (Searle, et al, 1976). Together, these form the head-related transfer function (HRTF) (Searle, et al, 1976, Blauert, 1983). To describe the HRTF, we must establish a coordinate system about the head. <p> For familiar sounds (a priori spectral information), the brain can estimate direction on the basis of pinna response from a single ear. Certain frequencies (roughly 1-3kHz) reect from the shoulders and upper body <ref> (Gardener, 1973) </ref>. The shoulder echoes reach the ear with a delay which is dependent on the elevation of the source. Additionally, the effects of the reection on the spectrum of the sound are direction-depen-dent.
Reference: 10. <author> Ludwig, L.F., Pincever, N. & Cohen, M. </author> <title> (1990) Extending the notion of a window system to audio, </title> <booktitle> Computer, </booktitle> <month> Aug. </month> <year> 1990, </year> <pages> 66-72. </pages>
Reference: 11. <author> Ludwig, L.F. & Cohen, M. </author> <title> (1991) Multidimensional audio window management, </title> <journal> International J. Man-Machine Studies, </journal> <volume> 34(3), </volume> <pages> 319-336 </pages>
Reference: 12. <author> Mills, A.W. </author> <title> (1972) Auditory localization, </title> <booktitle> Foundations of Modern Auditory Theory, </booktitle> <volume> Vol. </volume> <publisher> II/8, Academic: </publisher> <address> New York, NY. </address>
Reference: 13. <author> Moore, F.R. </author> <title> (1990) Elements of Computer Music, </title> <publisher> Pren tice Hall: </publisher> <address> Englewood Cliffs, NJ. </address>
Reference: 14. <author> Mynatt, E. & Edwards, </author> <title> W.K. (1992) The Mercator environment: A nonvisual interface to X Windows and Unix workstations, </title> <booktitle> ACM Symposium on User Interface Software and Technology, UIST 92. </booktitle>
Reference: 15. <author> Plenge, G. </author> <title> (1974) On the differences between localiza tion and lateralization, </title> <journal> J. Acoust. Soc. Am., </journal> <volume> 56, </volume> <pages> 944-951. </pages>
Reference: 16. <author> Lord Rayleigh [Strutt, J.W.] </author> <title> (1907) On our perception of sound direction, </title> <journal> Phil. Mag., </journal> <volume> 13, </volume> <pages> 214-232. </pages>
Reference: 17. <author> Searle, C.L., Braida, L.D., Davis, M.F. & Colburn, H.S. </author> <title> (1976) Model for auditory localization, </title> <journal> J. Acoust. Soc. Am., </journal> <volume> 60, </volume> <pages> 1164-1175. </pages>
Reference: 18. <author> Thomas, </author> <title> G.J (1940) Experimental study of the inu ence of vision on sound localization, </title> <journal> J. Exper. Psych., </journal> <volume> 28, </volume> <pages> 163-177 </pages>
Reference: 19. <author> Thurlow, W.R. </author> & <title> Runge, P.S. (1967) Effect of induced head movements on localization of direction of sounds, </title> <journal> J. Acoust. Soc. Am., </journal> <volume> 42, </volume> <pages> 480-488. </pages>
Reference-contexts: Additionally, the effects of the reection on the spectrum of the sound are direction-depen-dent. For familiar sounds, shoulder echoes may provide both elevation and azimuth information, although the previously mentioned cues are likely to be of greater importance (Searle, et al, 1976). The other four cues are head motion <ref> (Thurlow & Runge, 1967) </ref>, vision (Thomas, 1940), early echo response (Moore, 1990), and reverberation (Gardner, 1969). The lack of these cues can make spatial sound difficult to use. We tend to move our heads to get a better sense of a sounds direction (Thurlow, et al, 1967). <p> The lack of these cues can make spatial sound difficult to use. We tend to move our heads to get a better sense of a sounds direction <ref> (Thurlow, et al, 1967) </ref>. This closed-loop cue can be added Theta Azimuth Elevation Z Z Z to a spatial sound system through the use of a head-tracking device.
Reference: 20. <author> Thurlow, R.W., Mangels, J.W. </author> & <title> Runge P.S. (1967) Head movements during sound localization, </title> <journal> J. Acoust. Soc. Am., </journal> <volume> 42, </volume> <pages> 489-493. </pages>
Reference: 21. <author> Wenzel, </author> <title> E.M. (1992) Localization in virtual acoustic displays, </title> <journal> Presence, </journal> <volume> 1, </volume> <pages> 80-107. </pages>
Reference: 22. <author> Wenzel, E.M., Wightman, F.L. & Foster S.H. </author> <title> (1988) A virtual display system for conveying three-dimensional acoustic information, </title> <booktitle> Proceedings of the Human Fac-tors Society - 32nd Annual Meeting. </booktitle>
Reference: 23. <author> Wightman, F.L. & Kistler, </author> <title> D.J. (1989a) Headphone simulation of free-field listening I: stimulus synthesis, </title> <journal> J. Acoust. Soc. Am., </journal> <volume> 85, </volume> <pages> 858-867. </pages>
Reference: 24. <author> Wightman, F.L. & Kistler, </author> <title> D.J. (1989b) Headphone simulation of free-field listening II: psychophysical val-idation, </title> <journal> J. Acoust. Soc. Am., </journal> <volume> 85, </volume> <pages> 868-878. </pages>
References-found: 24

