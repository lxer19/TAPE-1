URL: http://www.cs.ucl.ac.uk/staff/B.Southall/papers/ICVS99.ps.gz
Refering-URL: http://www.cs.ucl.ac.uk/staff/B.Southall/pubs.html
Root-URL: http://www.cs.ucl.ac.uk
Email: B.Southall@cs.ucl.ac.uk  
Phone: 2  
Title: Vision-aided outdoor navigation of an autonomous horticultural vehicle  
Author: B Southall ; T Hague J A Marchant and B F Buxton 
Address: Wrest Park, Silsoe, Bedfordshire, MK45 4HS, UK  College London, Gower Street, London, WC1E 6BT, UK  
Affiliation: 1 Silsoe Research Institute,  Department of Computer Science, University  
Abstract: An autonomous outdoor vehicle has been developed at the Silsoe Research Institute as a testbed to investigate precise crop protection. The vehicle is able to navigate along rows of crop by using a Kalman filter to fuse information from proprioceptive sensing (odometry and inertial sensors) with data from an on-board computer vision system to generate estimates of its position and orientation. This paper describes a novel implementation of a previously proposed vision algorithm which uses a model of the crop planting pattern to extract vehicle position and orientation information from observations of many plants in each image. It is demonstrated that by implementing the vision system to compress the multiple plant observations into a single "pseudo observation" of vehicle position and orientation, it is possible to separate the vision system from the main body of the vehicle navigation Kalman filter, thus simplifying the task of fusing data from different sources. The algorithm is also used to segment the image sequences into areas of crop and weed, thus providing potential for targeting treatment. The implementation is tested on the vehicle, and results are shown from trials both in an indoor test area and outdoors on a field of real crop. Segmentation results are given for images captured from the vehicle. 
Abstract-found: 1
Intro-found: 1
Reference: 1. <author> Y Bar-Shalom and T Fortmann. </author> <title> Tracking and Data Association. </title> <publisher> Academic Press, </publisher> <address> New York, </address> <year> 1988. </year>
Reference-contexts: The standard update equations for the EKF <ref> [1] </ref> are given by: W (k+1) = P (k+1jk)H 0 (k+1)[H (k+1)P (k+1jk)H 0 (k+1) + R (k+1)] 1 (3) P (k+1jk+1) = P (k+1jk) W (k+1)[H (k+1)P (k+1jk)H 0 (k+1) + Where W is the Kalman gain, P (k+1jk) the predicted state covariance estimate, H the Jacobian of the state <p> If the probability of the nearest neighbour observation belonging to the same distribution as the prediction is less than 5%, the feature is rejected (this is the standard validation gate technique <ref> [1] </ref>). If the probability lies between 5 and 50%, then that observation is associated with the prediction point, and classified as plant. If several observations lie within the 50-100% range then these are merged by associating their mean position with the prediction, and each merged feature is classified as plant.
Reference: 2. <author> K S Chong and L Kleeman. </author> <title> Sonar based map building for a mobile robot. </title> <booktitle> In Proc. IEEE Int. Conf. Robotics and Automation, </booktitle> <pages> pages 1700-1705. </pages> <publisher> IEEE, </publisher> <month> April </month> <year> 1997. </year>
Reference-contexts: the vehicle both on an indoor test-bed and outside with real crop. 1.1 Related Work The Kalman filter has found numerous applications in robotics and machine vision, with examples from world geometry modelling both indoors [3] and outdoors [9], control of road vehicles [4] and mobile robot localization and map-building <ref> [11, 2] </ref>. In [6, 5], the visually-aided navigation of a small mobile platform is described, along with the Kalman filter based fusion of vision and odometric information.
Reference: 3. <author> R Deriche and O Faugeras. </author> <title> Tracking line segments. Image and Vision Computing, </title> <address> 8(4):261 - 270, </address> <month> November </month> <year> 1990. </year>
Reference-contexts: Finally, results are given which show the performance of the vehicle both on an indoor test-bed and outside with real crop. 1.1 Related Work The Kalman filter has found numerous applications in robotics and machine vision, with examples from world geometry modelling both indoors <ref> [3] </ref> and outdoors [9], control of road vehicles [4] and mobile robot localization and map-building [11, 2]. In [6, 5], the visually-aided navigation of a small mobile platform is described, along with the Kalman filter based fusion of vision and odometric information.
Reference: 4. <author> E D Dickmanns. </author> <title> Expectation-based dynamic scene understanding. In A Blake and </title>
Reference-contexts: are given which show the performance of the vehicle both on an indoor test-bed and outside with real crop. 1.1 Related Work The Kalman filter has found numerous applications in robotics and machine vision, with examples from world geometry modelling both indoors [3] and outdoors [9], control of road vehicles <ref> [4] </ref> and mobile robot localization and map-building [11, 2]. In [6, 5], the visually-aided navigation of a small mobile platform is described, along with the Kalman filter based fusion of vision and odometric information.
References-found: 4

