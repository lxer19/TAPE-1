URL: http://www.math.macalester.edu/~fox/old/cs62-97/ms.ps
Refering-URL: http://www.math.macalester.edu/~fox/old/cs62-97/page.html
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Title: Mutation Scheduling: A Unified Approach to Compiling for Fine-Grain Parallelism  
Author: Steven Novack and Alexandru Nicolau 
Address: Irvine, CA 92717  
Affiliation: Department of Information and Computer Science University of California  
Abstract: Trade-offs between code selection, register allocation, and instruction scheduling are inherently interdependent, especially when compiling for fine-grain parallel architectures. However, the conventional approach to compiling for such machines arbitrarily separates these phases so that decisions made during any one phase place unnecessary constraints on the remaining phases. Mutation Scheduling attempts to solve this problem by combining code selection, register allocation, and instruction scheduling into a unified framework in which trade-offs between the functional, register, and memory bandwidth resources of the target architecture are made "on the fly" in response to changing resource constraints and availability.
Abstract-found: 1
Intro-found: 1
Reference: 1. <author> A. Aho, R. Sethi, and J.D. Ullman. </author> <booktitle> Compilers: Principles, Techniques and Tools. </booktitle> <publisher> Addison-Wesley, </publisher> <address> Reading, MA, </address> <year> 1986. </year>
Reference-contexts: This representation of expressions is similar to value numbering <ref> [1] </ref> except that instead of representing multiple identical subexpressions as a single subexpression, referenced multiple times within a directed acyclic graph, we represent multiple identical equivalence classes of expressions (i.e.
Reference: 2. <author> D. Berson, R. Gupta, </author> <title> and M.L. Soffa. Resource spackling: A framework for integrating register allocation in local and global schedulers. </title> <booktitle> In Working Conf. on Par. Arch. and Compilation Techniques , August 1994. </booktitle>
Reference-contexts: For example, instruction scheduling and register allocation are integrated in <ref> [9, 14, 3, 17, 13, 19, 2] </ref>. Techniques like [14, 17, 13, 19] start with an initial register allocation and then during scheduling 2 Although, some compilers [14, 17, 13, 19] do partially mitigate the effect of early register allocation by removing spurious dependencies using dynamic register renaming [5]. <p> final locations of the STORE and LOAD will not generally be in adjacent instructions. 2 allocate unused registers using dynamic renaming [5] to remove false dependencies. 4 Unlike these techniques, which do not release allocated registers for use in other computations (e.g. by introducing spill code), the techniques presented in <ref> [9, 3, 2] </ref> do full register allocation and instruction scheduling "on the fly", including register spilling when appropriate. The re-materialization technique presented in [4] is a register allocation technique that, unlike the above methods, partially integrates the code selection pertaining to re-generating needed values. <p> MS differs most notably from the previous techniques in that it unifies all three aspects, code selection, register allocation, and instruction scheduling, into a single framework in which trade-offs among all three can be made, but MS does share some similarities with each technique: Like the techniques used in <ref> [9, 3, 2] </ref> MS attempts to do full register allocation in conjunction with instruction scheduling, but unlike these techniques, MS follows the same paradigm as used in [14, 17, 13, 19], of starting with an initial register allocation and then modifying it during scheduling in response to changing resource availability. <p> In Mutation Scheduling, whenever one of these dependencies is encountered 8 As opposed to techniques such as [21] that ignore resource-constraints during scheduling and <ref> [10, 2] </ref> that satisfy only resource estimates. 9 during scheduling, Mutate is used in an attempt to remove the dependence.
Reference: 3. <author> D. Bradlee, S. Eggers, and R. Henry. </author> <title> Integrating register allocation and instruction scheduling for riscs. </title> <booktitle> In ASPLOS , April 1991. </booktitle>
Reference-contexts: For example, instruction scheduling and register allocation are integrated in <ref> [9, 14, 3, 17, 13, 19, 2] </ref>. Techniques like [14, 17, 13, 19] start with an initial register allocation and then during scheduling 2 Although, some compilers [14, 17, 13, 19] do partially mitigate the effect of early register allocation by removing spurious dependencies using dynamic register renaming [5]. <p> final locations of the STORE and LOAD will not generally be in adjacent instructions. 2 allocate unused registers using dynamic renaming [5] to remove false dependencies. 4 Unlike these techniques, which do not release allocated registers for use in other computations (e.g. by introducing spill code), the techniques presented in <ref> [9, 3, 2] </ref> do full register allocation and instruction scheduling "on the fly", including register spilling when appropriate. The re-materialization technique presented in [4] is a register allocation technique that, unlike the above methods, partially integrates the code selection pertaining to re-generating needed values. <p> MS differs most notably from the previous techniques in that it unifies all three aspects, code selection, register allocation, and instruction scheduling, into a single framework in which trade-offs among all three can be made, but MS does share some similarities with each technique: Like the techniques used in <ref> [9, 3, 2] </ref> MS attempts to do full register allocation in conjunction with instruction scheduling, but unlike these techniques, MS follows the same paradigm as used in [14, 17, 13, 19], of starting with an initial register allocation and then modifying it during scheduling in response to changing resource availability.
Reference: 4. <author> P. Briggs, K. Cooper, and L. Torczon. </author> <note> Rematerialization. In PLDI , 1992. </note>
Reference-contexts: The re-materialization technique presented in <ref> [4] </ref> is a register allocation technique that, unlike the above methods, partially integrates the code selection pertaining to re-generating needed values.
Reference: 5. <author> R. Cytron and J. Ferrante. </author> <title> What's in a name? In ICPP , pages 19-27, </title> <month> August </month> <year> 1987. </year>
Reference-contexts: Techniques like [14, 17, 13, 19] start with an initial register allocation and then during scheduling 2 Although, some compilers [14, 17, 13, 19] do partially mitigate the effect of early register allocation by removing spurious dependencies using dynamic register renaming <ref> [5] </ref>. <p> is selected as a new mutation for Val, then it will be instantiated in its entirety, but nevertheless the STORE and LOAD will be scheduled separately so that the final locations of the STORE and LOAD will not generally be in adjacent instructions. 2 allocate unused registers using dynamic renaming <ref> [5] </ref> to remove false dependencies. 4 Unlike these techniques, which do not release allocated registers for use in other computations (e.g. by introducing spill code), the techniques presented in [9, 3, 2] do full register allocation and instruction scheduling "on the fly", including register spilling when appropriate.
Reference: 6. <author> R. Cytron, J. Ferrante, B. Rosen, M. Wegman, and K. Zadeck. </author> <title> Efficiently computing static single assignment form and the control dependence graph. </title> <journal> TOPLAS , 13(4) </journal> <pages> 451-490, </pages> <month> October </month> <year> 1991. </year>
Reference-contexts: To facilitate the "value-oriented" view used by MS, we augment the HTG structure with a slightly modified version of the Static Single Assignment (SSA) form of <ref> [6] </ref>. In the pure SSA form of a program, each use is reached by exactly one definition. When definitions from multiple control paths are needed by a use, a special -function is inserted at the confluence of the control paths that merges the multiple values into a single definition. <p> For this reason, SSA variables are usually referred to as "values" in this paper. For simplicity of exposition, we assume that LOAD's and STORE's of elements of arrays and structures are represented using the Access, Update, and HiddenUpdate operations defined in <ref> [6] </ref> which, in terms of dataflow, treat each element LOAD/STORE as a scalar read/write of the entire array or structure. The actual method used in our compiler is very similar to this, but allows for less conservative handling of ambiguous references and incremental updates of SSA form.
Reference: 7. <author> K. Ebcioglu. </author> <title> Some design ideas for a vliw architecture for sequential-natured software. </title> <booktitle> In IFIP Proceedings, </booktitle> <year> 1988. </year>
Reference-contexts: The specific scheduling approach, in conjunction with the desired application domain and cost vs. performance trade-offs would dictate the actual choices for the Mutate heuristics. 7 If the target architecture supports conditional write-back as in <ref> [7] </ref>, then it is possible that two operations from the same instruction produce the same value such that the write-back of one operation is predicated on some condition, and the write-back of the other on the negation of the condition.
Reference: 8. <author> M. Girkar and C.D. Polychronopoulos. </author> <title> Automatic extraction of functional parallelism from ordinary programs. </title> <booktitle> TOPADS , 3(2) </booktitle> <pages> 166-178, </pages> <month> March </month> <year> 1992. </year>
Reference: 9. <author> J. R. Goodman and W. Hsu. </author> <title> Code scheduling and register allocation in large basic blocks. </title> <note> In ICS , July 1988. </note>
Reference-contexts: For example, instruction scheduling and register allocation are integrated in <ref> [9, 14, 3, 17, 13, 19, 2] </ref>. Techniques like [14, 17, 13, 19] start with an initial register allocation and then during scheduling 2 Although, some compilers [14, 17, 13, 19] do partially mitigate the effect of early register allocation by removing spurious dependencies using dynamic register renaming [5]. <p> final locations of the STORE and LOAD will not generally be in adjacent instructions. 2 allocate unused registers using dynamic renaming [5] to remove false dependencies. 4 Unlike these techniques, which do not release allocated registers for use in other computations (e.g. by introducing spill code), the techniques presented in <ref> [9, 3, 2] </ref> do full register allocation and instruction scheduling "on the fly", including register spilling when appropriate. The re-materialization technique presented in [4] is a register allocation technique that, unlike the above methods, partially integrates the code selection pertaining to re-generating needed values. <p> MS differs most notably from the previous techniques in that it unifies all three aspects, code selection, register allocation, and instruction scheduling, into a single framework in which trade-offs among all three can be made, but MS does share some similarities with each technique: Like the techniques used in <ref> [9, 3, 2] </ref> MS attempts to do full register allocation in conjunction with instruction scheduling, but unlike these techniques, MS follows the same paradigm as used in [14, 17, 13, 19], of starting with an initial register allocation and then modifying it during scheduling in response to changing resource availability.
Reference: 10. <author> R. Gupta and M. L. Soffa. </author> <title> Region scheduling: An approach for detecting and redistributing parallelism. </title> <type> TOSE , 16(4), </type> <month> April </month> <year> 1990. </year>
Reference-contexts: In Mutation Scheduling, whenever one of these dependencies is encountered 8 As opposed to techniques such as [21] that ignore resource-constraints during scheduling and <ref> [10, 2] </ref> that satisfy only resource estimates. 9 during scheduling, Mutate is used in an attempt to remove the dependence.
Reference: 11. <author> D. J. Kuck, Y. Muraoka, and S. C. Chen. </author> <title> On the number of operations simultaneously executable in fortran-like programs and their resulting speedup. </title> <note> TOC , December 1972. </note>
Reference-contexts: Finally, Incremental Tree-Height Reduction (ITHR) partially integrates code selection into instruction scheduling. THR <ref> [11] </ref> is a well-known technique for changing the structure of expressions by exploiting the associative and distributive properties of most arithmetic and logical operators. Incremental THR [16] was used to change (on the fly) dependencies encountered during instruction-level scheduling when doing so would increase the degree of parallelism. <p> Tree Height Reduction and Constant Folding Tree Height Reduction (THR) is a well-known technique for changing the structure of an expression in tree form by exploiting the associative and distributive properties of most arithmetic and logical operations <ref> [11] </ref>. By "flattening out" tall thin expression trees into shorter, wider ones, THR increases the degree of parallelism in the expression, and thereby changes its resource requirements. Similarly, when the expression contains constant terms, it is often possible to group the constant terms together into the same operation using THR.
Reference: 12. <author> H. Massalin. Superoptimizer: </author> <title> A look at the smallest program. </title> <booktitle> In ASPLOS , 1987. </booktitle>
Reference-contexts: techniques also try to eliminate spill code that becomes unnecessary as a result of scheduling. 5 Including those derivable using CSE, re-materialization, and strength-reduction, as well as more complex sequences such as those that may be derived by a programmer with expert knowledge of the target architecture or a super-optimizer <ref> [12] </ref>. 3 formations so all three problems can easily be tuned by adjusting the heuristics, and without modifying the code transformation algorithms themselves. 3 Value-oriented TiPS MS schedules operations using a system of parallelizing program transformations called Trailblazing Percolation Scheduling (TiPS)[19]. <p> For instance, constant folding, composition/decomposition, and syn-onym mutations sometimes combine to yield an incremental form of strength reduction. Other types of functional equivalence might also be employed, including some of the more esoteric "tricks" that might be provided by an experienced assembly language programmer or a super-optimizer <ref> [12] </ref>. However, even the above "simple" types of functional equivalences are sufficient to yield a large number of functionally equivalent expressions. For instance, even an expression as simple (and common) as the address computation for the array reference, M [i][j], exhibits all three forms of functional equivalence.
Reference: 13. <author> S. Moon and K. Ebcioglu. </author> <title> An efficient resource constrained global scheduling technique for superscalar and vliw processors. </title> <booktitle> In MICRO , Portland, </booktitle> <address> OR, </address> <month> December </month> <year> 1992. </year>
Reference-contexts: For example, instruction scheduling and register allocation are integrated in <ref> [9, 14, 3, 17, 13, 19, 2] </ref>. Techniques like [14, 17, 13, 19] start with an initial register allocation and then during scheduling 2 Although, some compilers [14, 17, 13, 19] do partially mitigate the effect of early register allocation by removing spurious dependencies using dynamic register renaming [5]. <p> For example, instruction scheduling and register allocation are integrated in [9, 14, 3, 17, 13, 19, 2]. Techniques like <ref> [14, 17, 13, 19] </ref> start with an initial register allocation and then during scheduling 2 Although, some compilers [14, 17, 13, 19] do partially mitigate the effect of early register allocation by removing spurious dependencies using dynamic register renaming [5]. <p> For example, instruction scheduling and register allocation are integrated in [9, 14, 3, 17, 13, 19, 2]. Techniques like <ref> [14, 17, 13, 19] </ref> start with an initial register allocation and then during scheduling 2 Although, some compilers [14, 17, 13, 19] do partially mitigate the effect of early register allocation by removing spurious dependencies using dynamic register renaming [5]. <p> in which trade-offs among all three can be made, but MS does share some similarities with each technique: Like the techniques used in [9, 3, 2] MS attempts to do full register allocation in conjunction with instruction scheduling, but unlike these techniques, MS follows the same paradigm as used in <ref> [14, 17, 13, 19] </ref>, of starting with an initial register allocation and then modifying it during scheduling in response to changing resource availability. <p> In both compilers, renaming to remove false dependencies among register-to-register operations is accomplished by SSA form, so each version is capable of performing the same sort of register re-allocation as is used in <ref> [14, 17, 13, 19] </ref> by allowing code motion only if the number of live register values after each completed transformation does not exceed the register file size.
Reference: 14. <author> T. Nakatani and K. Ebcioglu. </author> <title> Using a lookahead window in a compaction-based paral-lelizing compiler. </title> <booktitle> In MICRO , 1990. </booktitle>
Reference-contexts: For example, instruction scheduling and register allocation are integrated in <ref> [9, 14, 3, 17, 13, 19, 2] </ref>. Techniques like [14, 17, 13, 19] start with an initial register allocation and then during scheduling 2 Although, some compilers [14, 17, 13, 19] do partially mitigate the effect of early register allocation by removing spurious dependencies using dynamic register renaming [5]. <p> For example, instruction scheduling and register allocation are integrated in [9, 14, 3, 17, 13, 19, 2]. Techniques like <ref> [14, 17, 13, 19] </ref> start with an initial register allocation and then during scheduling 2 Although, some compilers [14, 17, 13, 19] do partially mitigate the effect of early register allocation by removing spurious dependencies using dynamic register renaming [5]. <p> For example, instruction scheduling and register allocation are integrated in [9, 14, 3, 17, 13, 19, 2]. Techniques like <ref> [14, 17, 13, 19] </ref> start with an initial register allocation and then during scheduling 2 Although, some compilers [14, 17, 13, 19] do partially mitigate the effect of early register allocation by removing spurious dependencies using dynamic register renaming [5]. <p> in which trade-offs among all three can be made, but MS does share some similarities with each technique: Like the techniques used in [9, 3, 2] MS attempts to do full register allocation in conjunction with instruction scheduling, but unlike these techniques, MS follows the same paradigm as used in <ref> [14, 17, 13, 19] </ref>, of starting with an initial register allocation and then modifying it during scheduling in response to changing resource availability. <p> In both compilers, renaming to remove false dependencies among register-to-register operations is accomplished by SSA form, so each version is capable of performing the same sort of register re-allocation as is used in <ref> [14, 17, 13, 19] </ref> by allowing code motion only if the number of live register values after each completed transformation does not exceed the register file size.
Reference: 15. <author> A. Nicolau. </author> <title> Uniform parallelism exploitation in ordinary programs. </title> <booktitle> In ICPP , 1985. </booktitle>
Reference: 16. <author> A. Nicolau and R. Potasman. </author> <title> Incremental tree height reduction for high level synthesis. </title> <address> In DAC , San Francisco, CA, </address> <month> June </month> <year> 1991. </year>
Reference-contexts: Finally, Incremental Tree-Height Reduction (ITHR) partially integrates code selection into instruction scheduling. THR [11] is a well-known technique for changing the structure of expressions by exploiting the associative and distributive properties of most arithmetic and logical operators. Incremental THR <ref> [16] </ref> was used to change (on the fly) dependencies encountered during instruction-level scheduling when doing so would increase the degree of parallelism.
Reference: 17. <author> A. Nicolau, R. Potasman, and H. Wang. </author> <title> Register allocation, renaming and their impact on parallelism. In Lang. and Compilers for Par. Comp. </title> <publisher> Springer-Verlag, </publisher> <year> 1991. </year>
Reference-contexts: For example, instruction scheduling and register allocation are integrated in <ref> [9, 14, 3, 17, 13, 19, 2] </ref>. Techniques like [14, 17, 13, 19] start with an initial register allocation and then during scheduling 2 Although, some compilers [14, 17, 13, 19] do partially mitigate the effect of early register allocation by removing spurious dependencies using dynamic register renaming [5]. <p> For example, instruction scheduling and register allocation are integrated in [9, 14, 3, 17, 13, 19, 2]. Techniques like <ref> [14, 17, 13, 19] </ref> start with an initial register allocation and then during scheduling 2 Although, some compilers [14, 17, 13, 19] do partially mitigate the effect of early register allocation by removing spurious dependencies using dynamic register renaming [5]. <p> For example, instruction scheduling and register allocation are integrated in [9, 14, 3, 17, 13, 19, 2]. Techniques like <ref> [14, 17, 13, 19] </ref> start with an initial register allocation and then during scheduling 2 Although, some compilers [14, 17, 13, 19] do partially mitigate the effect of early register allocation by removing spurious dependencies using dynamic register renaming [5]. <p> in which trade-offs among all three can be made, but MS does share some similarities with each technique: Like the techniques used in [9, 3, 2] MS attempts to do full register allocation in conjunction with instruction scheduling, but unlike these techniques, MS follows the same paradigm as used in <ref> [14, 17, 13, 19] </ref>, of starting with an initial register allocation and then modifying it during scheduling in response to changing resource availability. <p> In both compilers, renaming to remove false dependencies among register-to-register operations is accomplished by SSA form, so each version is capable of performing the same sort of register re-allocation as is used in <ref> [14, 17, 13, 19] </ref> by allowing code motion only if the number of live register values after each completed transformation does not exceed the register file size.
Reference: 18. <author> S. Novack and A. Nicolau. </author> <title> An efficient global resource constrained technique for exploiting instruction level parallelism. </title> <booktitle> In ICPP , St. </booktitle> <address> Charles, IL, </address> <month> August </month> <year> 1992. </year>
Reference: 19. <author> S. Novack and A. Nicolau. Trailblazing: </author> <title> A hierarchical approach to percolation scheduling. </title> <type> Technical Report TR-92-56, </type> <institution> Univ. of Calif. </institution> <address> Irvine , 1992. </address> <booktitle> Also appears in the Proceedings of the 1993 International Conference on Parallel Processing, </booktitle> <address> St. Charles, IL., </address> <month> August </month> <year> 1993. </year>
Reference-contexts: For example, instruction scheduling and register allocation are integrated in <ref> [9, 14, 3, 17, 13, 19, 2] </ref>. Techniques like [14, 17, 13, 19] start with an initial register allocation and then during scheduling 2 Although, some compilers [14, 17, 13, 19] do partially mitigate the effect of early register allocation by removing spurious dependencies using dynamic register renaming [5]. <p> For example, instruction scheduling and register allocation are integrated in [9, 14, 3, 17, 13, 19, 2]. Techniques like <ref> [14, 17, 13, 19] </ref> start with an initial register allocation and then during scheduling 2 Although, some compilers [14, 17, 13, 19] do partially mitigate the effect of early register allocation by removing spurious dependencies using dynamic register renaming [5]. <p> For example, instruction scheduling and register allocation are integrated in [9, 14, 3, 17, 13, 19, 2]. Techniques like <ref> [14, 17, 13, 19] </ref> start with an initial register allocation and then during scheduling 2 Although, some compilers [14, 17, 13, 19] do partially mitigate the effect of early register allocation by removing spurious dependencies using dynamic register renaming [5]. <p> in which trade-offs among all three can be made, but MS does share some similarities with each technique: Like the techniques used in [9, 3, 2] MS attempts to do full register allocation in conjunction with instruction scheduling, but unlike these techniques, MS follows the same paradigm as used in <ref> [14, 17, 13, 19] </ref>, of starting with an initial register allocation and then modifying it during scheduling in response to changing resource availability. <p> In both compilers, renaming to remove false dependencies among register-to-register operations is accomplished by SSA form, so each version is capable of performing the same sort of register re-allocation as is used in <ref> [14, 17, 13, 19] </ref> by allowing code motion only if the number of live register values after each completed transformation does not exceed the register file size.
Reference: 20. <author> S. Novack and A. Nicolau. </author> <title> Resource-directed loop pipelining. </title> <type> Technical report, </type> <institution> Univ. of Calif., Irvine, Dept. of Information and Computer Science, </institution> <year> 1994. </year>
Reference-contexts: Trading resources for parallelism Speed-up bench MS No MS LL1 11.92 10.70 LL3 7.19 5.76 LL5 6.69 4.61 LL7 10.06 9.97 LL9 4.33 2.85 LL11 5.54 5.04 LL13 3.46 2.75 Avg 6.67 5.72 Table 3. Trade-offs among heterogeneous and special ized functional units <ref> [20] </ref>. RDLP works by unrolling and shifting 14 loops during scheduling to expose more operations to parallelize until resources are fully utilized and/or cost vs. performance constraints are satisfied.
Reference: 21. <author> R. Potasman. </author> <title> Percolation-Based Compiling for Evaluation of Parallelism and Hardware Design Trade-Offs. </title> <type> PhD thesis, </type> <institution> Univ. of Calif. </institution> <address> Irvine , 1991. </address>
Reference-contexts: In Mutation Scheduling, whenever one of these dependencies is encountered 8 As opposed to techniques such as <ref> [21] </ref> that ignore resource-constraints during scheduling and [10, 2] that satisfy only resource estimates. 9 during scheduling, Mutate is used in an attempt to remove the dependence.
Reference: 22. <author> B.R. Rau and C.D. Glaeser. </author> <title> Efficient code generation for horizontal architectures: </title> <booktitle> Compiler techniques and architectural support. In Symp. on Comp. Arch. </booktitle> , <month> April </month> <year> 1982. </year> <title> This article was processed using the L a T E X macro package with LLNCS style 15 </title>
Reference-contexts: Techniques like [14, 17, 13, 19] start with an initial register allocation and then during scheduling 2 Although, some compilers [14, 17, 13, 19] do partially mitigate the effect of early register allocation by removing spurious dependencies using dynamic register renaming [5]. Others <ref> [22] </ref> perform a potentially very expensive post-scheduling register re-allocation. 3 If this expression is selected as a new mutation for Val, then it will be instantiated in its entirety, but nevertheless the STORE and LOAD will be scheduled separately so that the final locations of the STORE and LOAD will not
References-found: 22

