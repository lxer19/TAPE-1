URL: http://http.icsi.berkeley.edu/ftp/global/pub/ai/stolcke/icassp96.ps.Z
Refering-URL: http://http.icsi.berkeley.edu/ftp/global/pub/ai/stolcke/
Root-URL: http://http.icsi.berkeley.edu
Email: stolcke@speech.sri.com ees@speech.sri.com  
Title: STATISTICAL LANGUAGE MODELING FOR SPEECH DISFLUENCIES  
Author: Andreas Stolcke Elizabeth Shriberg 
Address: Menlo Park, CA 94025  
Affiliation: Speech Technology and Research Laboratory SRI International,  
Abstract: Speech disfluencies (such as filled pauses, repetitions, restarts) are among the characteristics distinguishing spontaneous speech from planned or read speech. We introduce a language model that predicts disfluencies probabilistically and uses an edited, fluent context to predict following words. The model is based on a generalization of the standard N-gram language model. It uses dynamic programming to compute the probability of a word sequence, taking into account possible hidden disfluency events. We analyze the model's performance for various disfluency types on the Switchboard corpus. We find that the model reduces word perplexity in the neighborhood of disfluency events; however, overall differences are small and have no significant impact on recognition accuracy. We also note that for modeling of the most frequent type of dis-fluency, filled pauses, a segmentation of utterances into linguistic (rather than acoustic) units is required. Our analysis illustrates a generally useful technique for language model evaluation based on local perplexity comparisons. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> J. Bear, J. Dowding, and E. Shriberg. </author> <title> Integrating multiple knowledge sources for detection and correction of repairs in human-computer dialog. </title> <booktitle> In Proceedings of the 30th Annual Meeting of the Association for Computational Linguistics, </booktitle> <pages> pages 56-63, </pages> <institution> University of Delaware, Newark, Delaware, </institution> <month> June/July </month> <year> 1992. </year>
Reference-contexts: DFs are one of many potential factors contributing to the relatively poor performance of state-of-the-art recognizers on this type of speech, e.g., as found in the Switchboard [2] corpus. Past work on disfluent speech has focused on disfluency detection, using either acoustic features [7, 6] or recognized word sequences <ref> [1, 3] </ref>. Our goal in this work is to develop a statistical language model (LM) that can be used for speech decoding or rescoring, and that improves upon standard LMs by explicitly modeling the most frequent DF types.
Reference: [2] <author> J. J. Godfrey, E. C. Holliman, and J. McDaniel. </author> <title> SWITCHBOARD: Telephone speech corpus for research and development. </title> <booktitle> In Proceedings IEEE Conference on Acoustics, Speech and Signal Processing, </booktitle> <volume> volume I, </volume> <pages> pages 517-520, </pages> <address> San Francisco, </address> <month> March </month> <year> 1992. </year>
Reference-contexts: DFs are one of many potential factors contributing to the relatively poor performance of state-of-the-art recognizers on this type of speech, e.g., as found in the Switchboard <ref> [2] </ref> corpus. Past work on disfluent speech has focused on disfluency detection, using either acoustic features [7, 6] or recognized word sequences [1, 3].
Reference: [3] <author> P. Heeman and J. Allen. </author> <title> Detecting and correcting speech repairs. </title> <booktitle> In Proceedings of the 32th Annual Meeting of the Association for Computational Linguistics, </booktitle> <pages> pages 295-302, </pages> <address> New Mexico State University, Las Cruces, NM, </address> <month> June </month> <year> 1994. </year>
Reference-contexts: DFs are one of many potential factors contributing to the relatively poor performance of state-of-the-art recognizers on this type of speech, e.g., as found in the Switchboard [2] corpus. Past work on disfluent speech has focused on disfluency detection, using either acoustic features [7, 6] or recognized word sequences <ref> [1, 3] </ref>. Our goal in this work is to develop a statistical language model (LM) that can be used for speech decoding or rescoring, and that improves upon standard LMs by explicitly modeling the most frequent DF types.
Reference: [4] <author> R. Isotani and S. Matsunaga. </author> <title> A study of handling filled pauses in statistical language modeling. </title> <booktitle> In Proc. Acoustical Society of Japan, </booktitle> <pages> pages 81-82, </pages> <year> 1994. </year> <title> [In Japanese]. </title>
Reference-contexts: The main reason to expect that DF modeling can improve the LM is that standard N-gram models are based on word predictions from local contexts, which are rendered less uniform by intervening DFs. Other researchers have recently started exploring approaches to DF modeling based on similar assumptions <ref> [4, 8] </ref>. Section 2 describes a simple N-gram-style DF model, based on the intuition that DF events need to be predicted and edited from the context to improve the prediction of following words. <p> That is, the cleanup assumption holds for medial FPs if one models utterances based on linguistic, rather than acoustic, segments. 3.4. Results from related work We are aware of two other groups of researchers currently investigating similar approaches to DF language modeling. In <ref> [4] </ref> a cleanup-style model for filled pauses is described. Ries and Qui at CMU [8] have experimented with models for repetitions and certain types of sentence deletion that incorporate the cleanup assumption.
Reference: [5] <author> S. M. Katz. </author> <title> Estimation of probabilities from sparse data for the language model component of a speech recognizer. </title> <journal> IEEE Transactions on Acoustics, Speech, and Signal Processing, </journal> <volume> 35(3) </volume> <pages> 400-401, </pages> <month> March </month> <year> 1987. </year>
Reference-contexts: Trigram probabilities are denoted by p (j); these are obtained through the usual backoff procedure <ref> [5] </ref>. The total prefix probability is then computed as P (w 1 : : : w k ) = X where X ranges over the hidden states representing the disfluency types (including NODF). 2.4.
Reference: [6] <author> C. H. Nakatani and J. Hirschberg. </author> <title> A corpus-based study of repair cues in spontaneous speech. </title> <journal> Journal of the Acoustical Society of America, </journal> <volume> 95(3) </volume> <pages> 1603-1616, </pages> <year> 1994. </year>
Reference-contexts: DFs are one of many potential factors contributing to the relatively poor performance of state-of-the-art recognizers on this type of speech, e.g., as found in the Switchboard [2] corpus. Past work on disfluent speech has focused on disfluency detection, using either acoustic features <ref> [7, 6] </ref> or recognized word sequences [1, 3]. Our goal in this work is to develop a statistical language model (LM) that can be used for speech decoding or rescoring, and that improves upon standard LMs by explicitly modeling the most frequent DF types.
Reference: [7] <author> D. O'Shaughnessy. </author> <title> Correcting complex false starts in spontaneous speech. </title> <booktitle> In Proceedings IEEE Conference on Acoustics,Speech and Signal Processing, </booktitle> <volume> volume I, </volume> <pages> pages 349-352, </pages> <address> Adelaide, Australia, </address> <year> 1994. </year>
Reference-contexts: DFs are one of many potential factors contributing to the relatively poor performance of state-of-the-art recognizers on this type of speech, e.g., as found in the Switchboard [2] corpus. Past work on disfluent speech has focused on disfluency detection, using either acoustic features <ref> [7, 6] </ref> or recognized word sequences [1, 3]. Our goal in this work is to develop a statistical language model (LM) that can be used for speech decoding or rescoring, and that improves upon standard LMs by explicitly modeling the most frequent DF types.
Reference: [8] <author> K. Ries, </author> <year> 1995. </year> <type> Personal communication. </type>
Reference-contexts: The main reason to expect that DF modeling can improve the LM is that standard N-gram models are based on word predictions from local contexts, which are rendered less uniform by intervening DFs. Other researchers have recently started exploring approaches to DF modeling based on similar assumptions <ref> [4, 8] </ref>. Section 2 describes a simple N-gram-style DF model, based on the intuition that DF events need to be predicted and edited from the context to improve the prediction of following words. <p> Results from related work We are aware of two other groups of researchers currently investigating similar approaches to DF language modeling. In [4] a cleanup-style model for filled pauses is described. Ries and Qui at CMU <ref> [8] </ref> have experimented with models for repetitions and certain types of sentence deletion that incorporate the cleanup assumption. Overall, their results are consistent with ours (higher perplexity for filled pauses, lower perplexity for repetitions and deletions), but the overall effects are small, as in our case. 4.
Reference: [9] <author> E. E. Shriberg. </author> <title> Preliminaries to a Theory of Speech Disfluencies. </title> <type> Ph.D. thesis, </type> <institution> Department of Psychology, University of California, Berkeley, </institution> <address> CA, </address> <year> 1994. </year>
Reference-contexts: The emphasis is on a detailed analysis of the model at DF and following word positions. Section 4 provides a general discussion of the results. 2. THE MODEL 2.1. Disfluency types Following <ref> [9] </ref>, DFs can be classified based on how the actual utterance must be modified to obtain the intended fluent utterance, i.e., the utterance a speaker would produce if asked to repeat his or her utterance. The types can be characterized by the type of editing required. <p> IT'S A IT'S A FAIRLY LARGE COMMUNITY --&gt; IT'S A FAIRLY LARGE COMMUNITY Deletions (DEL) Words without correspondence in the repaired word sequence must be deleted. I DID YOU HAPPEN TO SEE ... --&gt; DID YOU HAPPEN TO SEE ... We know from prior work <ref> [9] </ref> that these three types of DF are the most frequent across a variety of spontaneous speech corpora, accounting for over 85% of DF tokens in the Switchboard corpus. 1 See [9] for a description of other, less frequent, types of DF that are not modeled explicitly in our LM. <p> We know from prior work <ref> [9] </ref> that these three types of DF are the most frequent across a variety of spontaneous speech corpora, accounting for over 85% of DF tokens in the Switchboard corpus. 1 See [9] for a description of other, less frequent, types of DF that are not modeled explicitly in our LM. For example, we are not modeling word substitutions or speech errors. 2.2. <p> We note with regard to these and later results that some types of disfluencies may contain word fragments (from speakers cutting themselves off in mid-word). According to <ref> [9] </ref>, 20 to 25% of repetitions and deletions in Switchboard contain word fragments; however, filled pauses, as classified here, never involve words fragments. Fragments are usually not part of the vocabulary of current recognizers, and are not modeled in our system. <p> This is largely attributable to poorer word probability estimates at locations immediately following a filled pause. In prior work 4 Due to differences in amount of training data and type of segmentation, the perplexities are not directly comparable to the previous two studies. Shriberg <ref> [9] </ref> observed that filled pauses tend to occur at linguistic segment (e.g., clause) boundaries. Since the standard LM test utterances are segmented according to acoustic criteria, filled pauses around linguistic boundaries can actually occur in the middle of acoustic utterance segments. <p> One potential source of improved DF modeling are correlations with speaker identity. For example, <ref> [9] </ref> found that speakers can be grouped into those preferring deletions over repetitions (`deleters'), and those with the opposite tendency (`repeaters'). Such cross-utterance effects could be modeled in the LM using standard techniques, e.g., using adaptive interpolation of specialized models.
References-found: 9

