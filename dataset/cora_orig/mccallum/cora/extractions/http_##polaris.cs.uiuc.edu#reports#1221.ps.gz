URL: http://polaris.cs.uiuc.edu/reports/1221.ps.gz
Refering-URL: http://polaris.cs.uiuc.edu/tech_reports.html
Root-URL: http://www.cs.uiuc.edu
Title: Estimating the Inherent Parallelism in Prolog Programs  
Author: David C. Sehr Laxmikant V. Kale 
Affiliation: University of Illinois at Urbana-Champaign  
Abstract: In this paper we describe a system for compile time instrumentation of Prolog programs to estimate the amount of inherent parallelism. Using this information we can determine the maximum speedup obtainable through OR- and AND/OR-parallel execution. We present the results of instrumenting a number of common benchmark programs, and draw some conclusions from their execution. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Khayri Ali. </author> <title> The muse or-parallel prolog model and its performance. </title> <booktitle> In Proceedings of the 1990 North American Logic Programming Conference, </booktitle> <pages> pages 757-776, </pages> <year> 1990. </year>
Reference-contexts: Sequential Prolog systems traverse this tree depth-first and left to right. 1 Which may have appeared in the source program, or may have been typed by the user at the read-evaluate-print prompt. 1 2 Sequential and OR time The most efficient OR parallel implementations of Prolog to date <ref> [14, 1] </ref> have been based upon the Warren Abstract Machine (WAM) [13]. Because of this, we compute critical path timings in number of WAM instructions executed. The number of instructions is an approximation to execution time, since each type of WAM instruction takes a slightly different time.
Reference: [2] <author> J. Chang, A. M. Despain, and D. </author> <title> DeGroot. And-parallelism of logic programs based on a static data dependency analysis. </title> <booktitle> In Proceedings of Compcon 85, </booktitle> <year> 1985. </year>
Reference-contexts: Another use of the method we propose is to compute exact dependences to test the effectiveness of dependence tests. There are a number of AND parallel execution models that differ in their treatment of the dynamic nature of dependences. The approaches range from dependence graphs that are static <ref> [7, 2, 15] </ref> to partly dynamic (conditional) [4, 5] to completely dynamic [3]. Kale [6] notes that in some rare situations it may be beneficial to evaluate dependent literals in parallel. His Reduce-Or Process Model allows for dependent AND parallelism, but his implementation [9] supports only independent AND parallelism.
Reference: [3] <author> J.S. Conery and D.F. Kibler. </author> <title> And parallelism and nondeterminism in logic programs. </title> <journal> New Generation Computing, </journal> <volume> 3 </volume> <pages> 43-70, </pages> <year> 1985. </year>
Reference-contexts: There are a number of AND parallel execution models that differ in their treatment of the dynamic nature of dependences. The approaches range from dependence graphs that are static [7, 2, 15] to partly dynamic (conditional) [4, 5] to completely dynamic <ref> [3] </ref>. Kale [6] notes that in some rare situations it may be beneficial to evaluate dependent literals in parallel. His Reduce-Or Process Model allows for dependent AND parallelism, but his implementation [9] supports only independent AND parallelism.
Reference: [4] <author> D. </author> <title> DeGroot. Restricted and-parallelism. </title> <booktitle> In Proceedings of the International Conference on Fifth Generation Computer Systems, </booktitle> <pages> pages 471-478. </pages> <publisher> North Holland, </publisher> <year> 1984. </year>
Reference-contexts: There are a number of AND parallel execution models that differ in their treatment of the dynamic nature of dependences. The approaches range from dependence graphs that are static [7, 2, 15] to partly dynamic (conditional) <ref> [4, 5] </ref> to completely dynamic [3]. Kale [6] notes that in some rare situations it may be beneficial to evaluate dependent literals in parallel. His Reduce-Or Process Model allows for dependent AND parallelism, but his implementation [9] supports only independent AND parallelism.
Reference: [5] <author> M. V. Hermenegildo. </author> <title> Independent AND-Parallel Prolog and its Architecture. </title> <publisher> Kluwer Academic Publishers, </publisher> <year> 1988. </year>
Reference-contexts: There are a number of AND parallel execution models that differ in their treatment of the dynamic nature of dependences. The approaches range from dependence graphs that are static [7, 2, 15] to partly dynamic (conditional) <ref> [4, 5] </ref> to completely dynamic [3]. Kale [6] notes that in some rare situations it may be beneficial to evaluate dependent literals in parallel. His Reduce-Or Process Model allows for dependent AND parallelism, but his implementation [9] supports only independent AND parallelism.
Reference: [6] <author> Laxmikant V. Kale. </author> <title> Parallel Architectures for Problem Solving. </title> <type> PhD thesis, </type> <institution> State University of New York at Stony Brook, </institution> <year> 1985. </year>
Reference-contexts: There are a number of AND parallel execution models that differ in their treatment of the dynamic nature of dependences. The approaches range from dependence graphs that are static [7, 2, 15] to partly dynamic (conditional) [4, 5] to completely dynamic [3]. Kale <ref> [6] </ref> notes that in some rare situations it may be beneficial to evaluate dependent literals in parallel. His Reduce-Or Process Model allows for dependent AND parallelism, but his implementation [9] supports only independent AND parallelism. Epilog [15] also permits dependent AND parallelism, but provides a primitive (CAND) to curtail it.
Reference: [7] <author> Laxmikant V. Kale. </author> <title> Parallel execution of logic programs: the reduce-or process model. </title> <booktitle> In Proceedings of the International Conference on Logic Programming, </booktitle> <pages> pages 616-632, </pages> <month> May </month> <year> 1987. </year>
Reference-contexts: Another use of the method we propose is to compute exact dependences to test the effectiveness of dependence tests. There are a number of AND parallel execution models that differ in their treatment of the dynamic nature of dependences. The approaches range from dependence graphs that are static <ref> [7, 2, 15] </ref> to partly dynamic (conditional) [4, 5] to completely dynamic [3]. Kale [6] notes that in some rare situations it may be beneficial to evaluate dependent literals in parallel. His Reduce-Or Process Model allows for dependent AND parallelism, but his implementation [9] supports only independent AND parallelism.
Reference: [8] <author> Manoj Kumar. </author> <title> Measuring parallelism in computation intensive scientific/engineering applications. </title> <journal> IEEE Transactions on Computers, </journal> <volume> 37(9), </volume> <month> September </month> <year> 1988. </year>
Reference-contexts: This method is then extended to give the best possible AND/OR parallel execution time. Our instrumentation does not drastically reduce efficiency, and we present the results of a number of programs. Our AND parallelism estimation method is based upon the work of Kumar <ref> [8] </ref> in estimating the inherent parallelism in Fortran programs. His method augments the source program with a timestamp for each data item d, which is updated each time d is written. <p> This is AND parallel execution, and unlike OR parallelism, it requires testing for dependences even in pure Prolog programs. In this section we describe the application of Kumar's <ref> [8] </ref> techniques for Fortran to estimate the best AND/OR parallel execution time. The method we describe extends his work to deal with the dynamic data structures and aliasing present in Prolog.
Reference: [9] <author> B. Ramkumar and L.V. Kale. </author> <title> Compiled execution of the reduce-or process model on multiprocessors. </title> <booktitle> In Proceedings of the 1989 North American Conference on Logic Programming, </booktitle> <pages> pages 313-331, </pages> <month> October </month> <year> 1989. </year>
Reference-contexts: Kale [6] notes that in some rare situations it may be beneficial to evaluate dependent literals in parallel. His Reduce-Or Process Model allows for dependent AND parallelism, but his implementation <ref> [9] </ref> supports only independent AND parallelism. Epilog [15] also permits dependent AND parallelism, but provides a primitive (CAND) to curtail it. The model we have developed includes dynamic, independent AND parallelism, with a strict sequential ordering on dependent literals.
Reference: [10] <author> Kish Shen. </author> <title> An investigation of the argonne model of or-parallel prolog. </title> <type> Master's thesis, </type> <institution> University of Manchester, </institution> <year> 1986. </year>
Reference-contexts: The method we describe extends his work to deal with the dynamic data structures and aliasing present in Prolog. We believe this framework has the advantage over other methods <ref> [10, 12] </ref> of allowing us to extend it to measure critical path times in programs with user parallelism. A program's dependences can only be exactly determined at execution time, since one execution may have a dependence while another does not.
Reference: [11] <author> Peter Szeredi. </author> <title> Performance analysis of the aurora or-parallel prolog system. </title> <booktitle> In Proceedings of the 1989 North American Conference on Logic Programming, </booktitle> <pages> pages 713-732, </pages> <year> 1989. </year>
Reference-contexts: A number of programs exhibited essentially no OR parallelism (e.g. divide10, log10, nreverse, times10). In general, independent AND parallel execution improved the performance of programs already speeded up by OR parallel execution by a small factor (1-6). These programs have all shown reasonable speedups in real OR parallel systems <ref> [11] </ref>. Our results show that there is plenty of parallelism in several of these programs to extend to much larger machines (e.g. consider chat parser, query and zebra). Those with smaller speedups may profit from the introduction of independent AND parallelism.
Reference: [12] <author> Evan Tick. </author> <title> Studies in Prolog Architectures. </title> <type> PhD thesis, </type> <institution> Stanford University, </institution> <month> June </month> <year> 1987. </year>
Reference-contexts: The method we describe extends his work to deal with the dynamic data structures and aliasing present in Prolog. We believe this framework has the advantage over other methods <ref> [10, 12] </ref> of allowing us to extend it to measure critical path times in programs with user parallelism. A program's dependences can only be exactly determined at execution time, since one execution may have a dependence while another does not.
Reference: [13] <author> David H. D. Warren. </author> <title> An abstract prolog instruction set. </title> <type> Technical report, </type> <institution> SRI International, </institution> <month> October </month> <year> 1983. </year> <note> Technical Note 309. </note>
Reference-contexts: to right. 1 Which may have appeared in the source program, or may have been typed by the user at the read-evaluate-print prompt. 1 2 Sequential and OR time The most efficient OR parallel implementations of Prolog to date [14, 1] have been based upon the Warren Abstract Machine (WAM) <ref> [13] </ref>. Because of this, we compute critical path timings in number of WAM instructions executed. The number of instructions is an approximation to execution time, since each type of WAM instruction takes a slightly different time. Variations in execution time come mainly from two sources: argument unification and backward execution.
Reference: [14] <author> David H.D. Warren. </author> <title> The sri model for or parallel execution of prolog | abstract design and implementation. </title> <booktitle> In Proceedings of the 1987 Symposium on Logic Programming, </booktitle> <pages> pages 92-103. </pages> <publisher> IEEE Computer Society Press, </publisher> <month> September </month> <year> 1987. </year>
Reference-contexts: Sequential Prolog systems traverse this tree depth-first and left to right. 1 Which may have appeared in the source program, or may have been typed by the user at the read-evaluate-print prompt. 1 2 Sequential and OR time The most efficient OR parallel implementations of Prolog to date <ref> [14, 1] </ref> have been based upon the Warren Abstract Machine (WAM) [13]. Because of this, we compute critical path timings in number of WAM instructions executed. The number of instructions is an approximation to execution time, since each type of WAM instruction takes a slightly different time. <p> These programs range over a variety of sizes and purposes. There are several interesting facts to observe from these programs. First, David H. D. Warren's assertion <ref> [14] </ref> that OR parallelism was likely to produce significant speedups on a range of programs appears to be borne out. Several programs achieved small speedups from OR parallelism, mostly due to shallow backtracking (e.g flatten, ops8, poly10, qsort, tak, unify). Improved indexing would probably eliminate most of this OR parallelism.
Reference: [15] <author> Michael Wise. </author> <title> Prolog Multiprocessors. </title> <publisher> Prentice-Hall International Publishers, </publisher> <year> 1986. </year>
Reference-contexts: Another use of the method we propose is to compute exact dependences to test the effectiveness of dependence tests. There are a number of AND parallel execution models that differ in their treatment of the dynamic nature of dependences. The approaches range from dependence graphs that are static <ref> [7, 2, 15] </ref> to partly dynamic (conditional) [4, 5] to completely dynamic [3]. Kale [6] notes that in some rare situations it may be beneficial to evaluate dependent literals in parallel. His Reduce-Or Process Model allows for dependent AND parallelism, but his implementation [9] supports only independent AND parallelism. <p> Kale [6] notes that in some rare situations it may be beneficial to evaluate dependent literals in parallel. His Reduce-Or Process Model allows for dependent AND parallelism, but his implementation [9] supports only independent AND parallelism. Epilog <ref> [15] </ref> also permits dependent AND parallelism, but provides a primitive (CAND) to curtail it. The model we have developed includes dynamic, independent AND parallelism, with a strict sequential ordering on dependent literals.
References-found: 15

