URL: http://www.hcrc.ed.ac.uk/publications/rp-73.ps.gz
Refering-URL: http://www.etse.urv.es/recerca/rgai/toni/
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Email: amoreno@lsi.upc.es, toni@cogsci.ed.ac.uk  
Title: Dynamic belief analysis  
Author: Antonio Moreno 
Keyword: rational inquiry, logics of knowledge and belief, logical omniscience, perfect rea soning, possible worlds, Kripke semantics, dynamic accessibility relation, analytic tableaux  
Address: Pau Gargallo, 5 08028-Barcelona  
Affiliation: Departament de Llenguatges i Sistemes Informatics Universitat Politecnica de Catalunya  
Abstract: The process of rational inquiry can be defined as the evolution of the beliefs of a rational agent as a consequence of its internal inference procedures or its interaction with the environment. These beliefs can be modelled in a formal way using doxastic logics. The possible worlds model and its associated Kripkean semantics provide an intuitive semantics for these logics, but they seem to model agents which are logically omniscient and perfect reasoners. These problems can be avoided with a syntactic view of possible worlds, defining them as arbitrary sets of sentences in a propositional doxastic logic. This approach does not account for any kind of evolution in the set of beliefs, though. In this work this syntactic view of possible worlds is taken, and a three-dimensional dynamic analysis of the beliefs of the agent is suggested in order to model the process of rational inquiry in which the agent is permanently engaged. The agent can make a logical analysis of the beliefs (using a modified version of the analytic tableaux method); the results of this analysis guide the experimental analysis, that allows the agent to perform certain tests in the environment, that corroborate or refute the results of the logical analysis. In a third (axiomatic) kind of analysis, the agent can transform its set of beliefs into an axiomatic system (a set of axioms and a set of inference rules). Most of the research described in this report was developed during the stay of the author at HCRC, University of Edinburgh, from March to August 1995. Partially funded by project VIM ERBCHRXCT930401. 
Abstract-found: 1
Intro-found: 1
Reference: [ALCH85] <author> Alchourron, C., Gardenfors, P., Makinson, D., </author> <title> "On the logic of theory change: partial meet functions for contraction and revision", </title> <journal> Journal of Symbolic Logic 50, </journal> <pages> pp. 510-530, </pages> <year> 1985. </year>
Reference-contexts: This means that this work will have connections with the AI field of belief revision, which has received much attention in recent years (especially from the logical approach taken by Gardenfors, Makinson and Alchourron; see e.g. <ref> [ALCH85] </ref>, [GARD88]).
Reference: [APPE85] <author> Appelt, D., </author> <title> "Planning English sentences", </title> <booktitle> Studies in Natural Language Processing, </booktitle> <publisher> Cambridge University Press, </publisher> <year> 1985. </year>
Reference-contexts: Appelt describes a similar approach in <ref> [APPE85] </ref>, where he argues that actions can generate knowledge by restricting the possible worlds that are consistent with the agents' knowledge after performance of the action (following ideas from Moore, [MOOR83], [MOOR85]).
Reference: [BARW83] <author> Barwise, J., Perry, J., </author> <title> "Situations and attitudes", </title> <publisher> Bradford Books, </publisher> <address> Cambridge, MA, </address> <year> 1983. </year>
Reference-contexts: The main motivation of this dimension is a concept that could be called cognitive economy. There are several advantages of an axiomatic system vs. a set of beliefs, e.g. the following: 17 Although the idea of "successful belief" in <ref> [BARW83] </ref> is of a probabilistic nature. 19 * ease of manipulation and representation * representation of different levels of acceptance of the beliefs (e.g. the easier it is to obtain a proposition from the axioms, the easier it is for the agent to believe it) * compactness of the axiomatic system
Reference: [BELL77] <author> Bell, J., Machover, M., </author> <title> "A course in Mathematical Logic", </title> <publisher> North Holland, </publisher> <year> 1977. </year>
Reference-contexts: In fact, the possibility of adding this kind of tautologies in the analytic tableaux is a classical idea in the tradition of model theory, as described e.g. in <ref> [BELL77] </ref>. * The introduction of instances of AEM is also a way to control the set of formulas that can be believed by the agent using the analytic tableaux method.
Reference: [BELN77] <author> Belnap, N., </author> <title> "A useful four-valued logic", in Modern uses of multiple-valued logic, </title> <editor> Epstein,G., Dunn, J. </editor> <booktitle> (Eds.), </booktitle> <pages> pp. 5-37, </pages> <publisher> Reidel, </publisher> <year> 1977. </year> <title> 18 These questions about the logical analysis have partly been considered, </title> <editor> see appendixes A and B. </editor> <volume> 21 </volume>
Reference-contexts: Rescher and Brandom in [RESC79], and it is (see [FAGI95]). Others have taken into account in their logical approaches the time that the agent must spend in the course of its reasoning (step logic, see [ELGO88], [NIRK94]). 3 also implicit in the development of the so-called paraconsistent logics (Belnap <ref> [BELN77] </ref>, Fitting [FITT90])). It is obvious that e.g. humans are both incomplete and inconsistent. Humans often have to say "I don't know" when asked if they believe a certain proposition or its negation, showing their incompleteness (they do not believe or disbelief every imaginable proposition).
Reference: [BETH55] <author> Beth, </author> <title> E.W., "Semantic entailment and formal derivability", from Mededelingen van de Koninklijke Nederlandse Akademie van Wetenschappen, </title> <journal> Afdeling Letterkunde, </journal> <volume> Vol. 18, no. 13, </volume> <pages> pp. 309-342, </pages> <year> 1955. </year>
Reference-contexts: substitution instance of a subformula of a formula in the subtableaux with respect to names appearing in the subtableaux). 14 The third subtableau would be immediately closed because it would contain both p and :p in its left column. 15 Hintikka uses the classical Beth-style analytic tableaux, as described in <ref> [BETH55] </ref>. 16 4 General view of the belief system This section describes briefly the different parts of the belief system, shown in figure 6. ? - ? ? - ? 6 USER USER USER PROCS. REVISION BELIEF EVALUATOR (TABLEAUX) ANALYSIS LOGICAL TAUTS.
Reference: [BRAT88] <author> Bratman, M., Israel, D., Pollack, M., </author> <title> "Plans and resource-bounded practical reasoning", </title> <booktitle> Computational Intelligence 4, </booktitle> <pages> pp. 349-355, </pages> <year> 1988. </year>
Reference-contexts: If the agent is to display a rational behaviour, it probably needs to have a module capable of handling its beliefs (see e.g. the Belief-Desire-Intention architectures considered in <ref> [BRAT88] </ref>, [RAOG91]). These beliefs can be used by the agent in different tasks, e.g. to choose between alternative courses of action.
Reference: [CRES72] <author> Cresswell, M.J., </author> <title> "Intensional logics and logical truth", </title> <journal> Journal of Philosophical Logic 1, </journal> <pages> pp. 2-15, </pages> <year> 1972. </year>
Reference-contexts: fact, many people have proposed to consider impossible possible worlds, in the sense of having possible worlds where the usual logical connectives do not behave in the usual way, or tautologies may not be true, or inconsistent formulas may hold (see e.g. impossible possible worlds in [HINT75], non-classical worlds in <ref> [CRES72] </ref>, non-standard worlds in [RESC79] or situations in [LEVE84]). The drawback of these approaches is that, although they alleviate the problems of logical omniscience and perfect reasoning, they cause different problems.
Reference: [DELG95] <author> Delgrande, J., </author> <title> "A framework for logics of explicit belief", </title> <booktitle> Computational Intelligence 11, </booktitle> <pages> pp. 47-86, </pages> <year> 1995. </year>
Reference-contexts: subset of them (call that a context), it can draw conclusions which are consistent within the context but inconsistent if all the beliefs are considered (as remarked by Shoham in [SHOH91]; Fagin and Halpern modelled this situation in their logic of local reasoning, [FAGI85]; Delgrande also considered this idea in <ref> [DELG95] </ref>). * There is no theoretical problem in defining interesting procedures of inquiry over inconsis tent sets of beliefs (as shown e.g. in [RESC79]). * These kinds of world are perfectly conceivable (and even depictable in pictures, as Escher (see e.g. [HOFS80]) proved so many times). * Some theories have been
Reference: [DEVL91] <author> Devlin, K., </author> <title> "Logic and information", </title> <publisher> Cambridge University Press, </publisher> <year> 1991. </year>
Reference-contexts: It could be misleading, though, due to the inherent consistency of situations in that framework (incoherent abstract situations being the exception, as defined by Devlin in <ref> [DEVL91] </ref>). 8 BF (w)=f j B * wg * Negated belief formulas (w): formulas that are prefixed by the negation and the modal operator in w. N BF (w)=f j :B * wg * Propositional formulas (w): standard propositional formulas of w.
Reference: [ELGO88] <author> Elgot-Drapkin, J., "Step-logic: </author> <title> reasoning situated in time", </title> <type> Ph.D. thesis, </type> <institution> University of Maryland, </institution> <year> 1988. </year>
Reference-contexts: Rescher and Brandom in [RESC79], and it is (see [FAGI95]). Others have taken into account in their logical approaches the time that the agent must spend in the course of its reasoning (step logic, see <ref> [ELGO88] </ref>, [NIRK94]). 3 also implicit in the development of the so-called paraconsistent logics (Belnap [BELN77], Fitting [FITT90])). It is obvious that e.g. humans are both incomplete and inconsistent.
Reference: [FAGI85] <author> Fagin, R., Halpern, J., </author> <title> "Belief, awareness and limited reasoning", Procs. </title> <booktitle> of the Ninth IJCAI, </booktitle> <pages> pp. 491-501, </pages> <year> 1985. </year>
Reference-contexts: The partiality or incompleteness of possible worlds has been traditionally accepted in the AI literature, the most common justifications being the following: * The agent can be unaware of certain facts (Fagin and Halpern tried to take this fact into account in their logic of general awareness, <ref> [FAGI85] </ref>). * The agent can have limited resources (e.g. the time required or the space needed to perform a given inference). <p> every inference; if it focuses in a subset of them (call that a context), it can draw conclusions which are consistent within the context but inconsistent if all the beliefs are considered (as remarked by Shoham in [SHOH91]; Fagin and Halpern modelled this situation in their logic of local reasoning, <ref> [FAGI85] </ref>; Delgrande also considered this idea in [DELG95]). * There is no theoretical problem in defining interesting procedures of inquiry over inconsis tent sets of beliefs (as shown e.g. in [RESC79]). * These kinds of world are perfectly conceivable (and even depictable in pictures, as Escher (see e.g. [HOFS80]) proved so
Reference: [FAGI95] <author> Fagin, R., Halpern, J., Moses, Y., Vardi, M., </author> <title> "Reasoning about knowledge", </title> <publisher> MIT Press, </publisher> <year> 1995. </year>
Reference-contexts: These logics are useful in some circumstances (e.g. modelling states of knowledge in communication protocols in distributed environments, or solving classical puzzles (the wise men, the muddy children, the cheating wives), as described in <ref> [FAGI95] </ref>), but they are not appropriate for the purpose of this work (modelling rational inquiry) for two main reasons: * The set of possible worlds W and the accessibility relation R are fixed, so the set of beliefs is also fixed. <p> These properties are inherited by the beliefs modelled with the Kripke semantics, and thus the modelled agents are logically omniscient and perfect reasoners. These properties could be appropriate if one wished to model fairly ideal agents (e.g. the children in the muddy children puzzle, see <ref> [FAGI95] </ref>), but they are clearly inadequate if you consider more realistic agents (be they human or computational) which have resource limitations that do not allow them to even approach such incredible states of knowledge. <p> Several proposals that have tried to solve (or, at least, partially alleviate) them have been put forward in the AI and the philosophical literature (e.g. [McAR88], [REIC89] or <ref> [FAGI95] </ref> contain detailed reviews of the most relevant approaches). Hintikka made an interesting suggestion in [HINT75], where he proposed the idea of considering [logically] impossible [epistemically] possible worlds, but later he seems not to have pursued this idea. <p> Nevertheless, let's face this possibility anyway (some philosophers have indeed argued for the feasibility of this kind of worlds, e.g. Rescher and Brandom in [RESC79], and it is (see <ref> [FAGI95] </ref>). Others have taken into account in their logical approaches the time that the agent must spend in the course of its reasoning (step logic, see [ELGO88], [NIRK94]). 3 also implicit in the development of the so-called paraconsistent logics (Belnap [BELN77], Fitting [FITT90])).
Reference: [FITT90] <author> Fitting, M., </author> <title> "Modal/multiple-valued logics", </title> <type> Technical Report, </type> <institution> CUNY, </institution> <year> 1990. </year>
Reference-contexts: Others have taken into account in their logical approaches the time that the agent must spend in the course of its reasoning (step logic, see [ELGO88], [NIRK94]). 3 also implicit in the development of the so-called paraconsistent logics (Belnap [BELN77], Fitting <ref> [FITT90] </ref>)). It is obvious that e.g. humans are both incomplete and inconsistent. Humans often have to say "I don't know" when asked if they believe a certain proposition or its negation, showing their incompleteness (they do not believe or disbelief every imaginable proposition).
Reference: [GARD88] <author> Gardenfors, P., </author> <title> "Knowledge in flux", </title> <publisher> Cambridge University Press, </publisher> <year> 1988 </year>
Reference-contexts: This means that this work will have connections with the AI field of belief revision, which has received much attention in recent years (especially from the logical approach taken by Gardenfors, Makinson and Alchourron; see e.g. [ALCH85], <ref> [GARD88] </ref>).
Reference: [HALP92] <author> Halpern, J., Moses Y., </author> <title> "A guide to completeness and complexity for modal logics of knowledge and belief", </title> <booktitle> Artificial Intelligence 54, </booktitle> <pages> pp. 319-379, </pages> <year> 1992. </year>
Reference-contexts: These two components will find its counterparts in the analysis suggested later, namely in the logical and experimental analysis of the beliefs. In the field of mathematical logic this concern led to the development of the so-called logics of knowledge and belief (epistemic and doxastic logics, see e.g. <ref> [HALP92] </ref>). These logics are built on the language of standard propositional logic, adding the modal operators of necessity and possibility. The first operator is considered as belief/knowledge in doxastic/epistemic logics.
Reference: [HINT62] <author> Hintikka, J., </author> <title> "Knowledge and belief", </title> <publisher> Cornell University Press, </publisher> <address> Ithaca, N.Y., </address> <year> 1962. </year>
Reference: [HINT75] <author> Hintikka, J., </author> <title> "Impossible possible worlds vindicated", </title> <journal> Journal of Philosophical Logic 4, </journal> <pages> pp. 475-484, </pages> <year> 1975. </year>
Reference-contexts: Several proposals that have tried to solve (or, at least, partially alleviate) them have been put forward in the AI and the philosophical literature (e.g. [McAR88], [REIC89] or [FAGI95] contain detailed reviews of the most relevant approaches). Hintikka made an interesting suggestion in <ref> [HINT75] </ref>, where he proposed the idea of considering [logically] impossible [epistemically] possible worlds, but later he seems not to have pursued this idea. This is the path followed in this work, as will be apparent in the rest of this report. <p> In fact, many people have proposed to consider impossible possible worlds, in the sense of having possible worlds where the usual logical connectives do not behave in the usual way, or tautologies may not be true, or inconsistent formulas may hold (see e.g. impossible possible worlds in <ref> [HINT75] </ref>, non-classical worlds in [CRES72], non-standard worlds in [RESC79] or situations in [LEVE84]). The drawback of these approaches is that, although they alleviate the problems of logical omniscience and perfect reasoning, they cause different problems.
Reference: [HINT81] <author> Hintikka, J., </author> <title> "On the logic of an interrogative model of scientific inquiry", </title> <type> Synthese 47, </type> <pages> pp. 69-83, </pages> <year> 1981. </year>
Reference: [HINT86] <author> Hintikka, J., </author> <title> "Reasoning about knowledge in philosophy: the paradigm of epistemic logic", </title> <booktitle> Proceedings of the 1986 TARK Conference, </booktitle> <editor> Ed. </editor> <booktitle> J.Y.Halpern, </booktitle> <pages> pp. 63-80, </pages> <year> 1986. </year>
Reference-contexts: shown in section 4, the potential beliefs of the agent will be controlled by restricting the (instances of the) tautologies that the agent can use in the logical analysis. * This exception also creates a strong link between this approach and the interrogative model of inquiry proposed by Hintikka ([HINT81], <ref> [HINT86] </ref>, [HINT87], [HINT88], [HINT92]). He models the process of scientific inquiry with plays of the interrogative game, which is a game played by the Inquirer and Nature. <p> This means that this work will have connections with the AI field of belief revision, which has received much attention in recent years (especially from the logical approach taken by Gardenfors, Makinson and Alchourron; see e.g. [ALCH85], [GARD88]). Thus, as Hintikka suggested in <ref> [HINT86] </ref>, doubt can be understood as the beginning of a dynamic process that, by reducing the number of conceivable worlds that the agent considers, reinforces one side of the doubt over the other, sets the conditions to verify it (and falsify the other) and tendentially gives credence to it.
Reference: [HINT87] <author> Hintikka, J., </author> <title> "Knowledge representation and the interrogative model of inquiry", </title> <booktitle> International Philosophy Congress, </booktitle> <pages> pp. 1077-1084, </pages> <year> 1987. </year>
Reference-contexts: in section 4, the potential beliefs of the agent will be controlled by restricting the (instances of the) tautologies that the agent can use in the logical analysis. * This exception also creates a strong link between this approach and the interrogative model of inquiry proposed by Hintikka ([HINT81], [HINT86], <ref> [HINT87] </ref>, [HINT88], [HINT92]). He models the process of scientific inquiry with plays of the interrogative game, which is a game played by the Inquirer and Nature.
Reference: [HINT88] <author> Hintikka, J., </author> <title> "What is the logic of experimental inquiry?", </title> <type> Synthese 74, </type> <pages> pp. 173-190, </pages> <year> 1988. </year>
Reference-contexts: section 4, the potential beliefs of the agent will be controlled by restricting the (instances of the) tautologies that the agent can use in the logical analysis. * This exception also creates a strong link between this approach and the interrogative model of inquiry proposed by Hintikka ([HINT81], [HINT86], [HINT87], <ref> [HINT88] </ref>, [HINT92]). He models the process of scientific inquiry with plays of the interrogative game, which is a game played by the Inquirer and Nature. <p> Thus, this dimension models the agent's acquisition of data in the actual world (obtained via perception, experiences, tests, etc.). The root of this dimension can be traced (as Hintikka points out in <ref> [HINT88] </ref>) as far as Kant, who argued in his Critique of pure reason that reason has to take into account observations of the environment; he thinks that reason must not approach Nature as a student, that takes everything that its teacher chooses to say, but as a judge who formulates questions
Reference: [HINT92] <author> Hintikka, J., </author> <title> "The interrogative model of inquiry as a general theory of argumentation", </title> <journal> Communication and Cognition, </journal> <volume> Vol. 25, </volume> <pages> Nos. 2-3, pp. 221-242, </pages> <year> 1992. </year> <month> 22 </month>
Reference-contexts: 4, the potential beliefs of the agent will be controlled by restricting the (instances of the) tautologies that the agent can use in the logical analysis. * This exception also creates a strong link between this approach and the interrogative model of inquiry proposed by Hintikka ([HINT81], [HINT86], [HINT87], [HINT88], <ref> [HINT92] </ref>). He models the process of scientific inquiry with plays of the interrogative game, which is a game played by the Inquirer and Nature.
Reference: [HOFS80] <author> Hofstadter, D., </author> <title> "Godel, Escher, Bach: an Eternal Golden Braid", </title> <publisher> Basic Books Inc. Publishers, </publisher> <year> 1980. </year>
Reference-contexts: local reasoning, [FAGI85]; Delgrande also considered this idea in [DELG95]). * There is no theoretical problem in defining interesting procedures of inquiry over inconsis tent sets of beliefs (as shown e.g. in [RESC79]). * These kinds of world are perfectly conceivable (and even depictable in pictures, as Escher (see e.g. <ref> [HOFS80] </ref>) proved so many times). * Some theories have been considered acceptable for a certain period of time (e.g. Frege's set theory) for many people, but they have been proved to be inconsistent later (as Russell did with Frege's theory).
Reference: [KONO85] <author> Konolige, K., </author> <title> "Belief and incompleteness", </title> <booktitle> Formal theories of the commonsense world, </booktitle> <pages> pp. 359-403, </pages> <editor> Hobbs and Moore Eds., </editor> <year> 1985. </year>
Reference: [KONO86] <author> Konolige, K., </author> <title> "A Deduction Model of Belief", </title> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, CA, </address> <year> 1986. </year>
Reference: [KRIP63] <author> Kripke, S., </author> <title> "A semantical analysis of modal logic I: normal modal propositional calculi", </title> <journal> Zeitschrift fur Mathematische Logik und Grundlagen Mathematik 9, </journal> <pages> pp. 67-96, </pages> <year> 1963. </year>
Reference: [LEVE84] <author> Levesque, H.J., </author> <title> "A logic of implicit and explicit belief", </title> <booktitle> Proceedings of AAAI-84, </booktitle> <pages> pp. 198-202, </pages> <year> 1984. </year>
Reference-contexts: possible worlds, in the sense of having possible worlds where the usual logical connectives do not behave in the usual way, or tautologies may not be true, or inconsistent formulas may hold (see e.g. impossible possible worlds in [HINT75], non-classical worlds in [CRES72], non-standard worlds in [RESC79] or situations in <ref> [LEVE84] </ref>). The drawback of these approaches is that, although they alleviate the problems of logical omniscience and perfect reasoning, they cause different problems.
Reference: [McAR88] <author> McArthur, G., </author> <title> "Reasoning about knowledge and belief: a survey", </title> <booktitle> Computational Intelligence 4, </booktitle> <pages> pp. 223-243, </pages> <year> 1988. </year>
Reference-contexts: Several proposals that have tried to solve (or, at least, partially alleviate) them have been put forward in the AI and the philosophical literature (e.g. <ref> [McAR88] </ref>, [REIC89] or [FAGI95] contain detailed reviews of the most relevant approaches). Hintikka made an interesting suggestion in [HINT75], where he proposed the idea of considering [logically] impossible [epistemically] possible worlds, but later he seems not to have pursued this idea. <p> This is the path followed in this work, as will be apparent in the rest of this report. The main roots of these problems, as has been stated above, are the assumptions of completeness and consistency that underlie the possible worlds model. As McArthur notes in <ref> [McAR88] </ref>, since worlds are complete the agent is forced to have beliefs about the way that everything would be in all of the accessible worlds, and since worlds as consistent as well, everything that follows from the agent's beliefs must also be believed.
Reference: [MOOR83] <author> Moore, R., </author> <title> "Semantical considerations on nonmonotonic logic", </title> <booktitle> Artificial Intelligence Center Technical Note 284, SRI International, </booktitle> <address> Menlo Park, California, </address> <year> 1983. </year>
Reference-contexts: Appelt describes a similar approach in [APPE85], where he argues that actions can generate knowledge by restricting the possible worlds that are consistent with the agents' knowledge after performance of the action (following ideas from Moore, <ref> [MOOR83] </ref>, [MOOR85]). It could be argued that the evolution of the agent's beliefs can be modelled with just one kind of variability (be it in W or in R).
Reference: [MOOR85] <author> Moore, R., </author> <title> "A formal theory of knowledge and action", </title> <booktitle> in Formal theories of the commonsense world, </booktitle> <pages> pp. 319-358, </pages> <editor> Hobbs and Moore Eds., </editor> <year> 1985. </year>
Reference-contexts: Appelt describes a similar approach in [APPE85], where he argues that actions can generate knowledge by restricting the possible worlds that are consistent with the agents' knowledge after performance of the action (following ideas from Moore, [MOOR83], <ref> [MOOR85] </ref>). It could be argued that the evolution of the agent's beliefs can be modelled with just one kind of variability (be it in W or in R).
Reference: [NIRK94] <author> Nirkhe, M., Kraus, S., Perlis, D., </author> <title> "Thinking takes time: a modal active-logic for reasoning in time", </title> <institution> CS-TR-3249, Computer Science Dept., University of Maryland, </institution> <year> 1994. </year>
Reference-contexts: Rescher and Brandom in [RESC79], and it is (see [FAGI95]). Others have taken into account in their logical approaches the time that the agent must spend in the course of its reasoning (step logic, see [ELGO88], <ref> [NIRK94] </ref>). 3 also implicit in the development of the so-called paraconsistent logics (Belnap [BELN77], Fitting [FITT90])). It is obvious that e.g. humans are both incomplete and inconsistent.
Reference: [POPP34] <author> Popper, K., </author> <title> "The logic of scientific discovery", </title> <editor> Ed. Tecnos, </editor> <publisher> 1977 printing. </publisher>
Reference: [RAOG91] <author> Rao, A., Georgeff, M., </author> <title> "Modeling rational agents within a BDI-architecture", </title> <booktitle> Proceedings of the 2nd International Conference on principles of knowledge representation and reasoning, </booktitle> <pages> pp. 473-484, </pages> <year> 1991. </year>
Reference-contexts: If the agent is to display a rational behaviour, it probably needs to have a module capable of handling its beliefs (see e.g. the Belief-Desire-Intention architectures considered in [BRAT88], <ref> [RAOG91] </ref>). These beliefs can be used by the agent in different tasks, e.g. to choose between alternative courses of action.
Reference: [REIC89] <author> Reichgelt, H., </author> <title> "Logics for reasoning about knowledge and belief", </title> <journal> Knowledge Engineering Review, </journal> <volume> Vol. 4, </volume> <pages> No.2, pp. 119-139, </pages> <year> 1989. </year>
Reference-contexts: Several proposals that have tried to solve (or, at least, partially alleviate) them have been put forward in the AI and the philosophical literature (e.g. [McAR88], <ref> [REIC89] </ref> or [FAGI95] contain detailed reviews of the most relevant approaches). Hintikka made an interesting suggestion in [HINT75], where he proposed the idea of considering [logically] impossible [epistemically] possible worlds, but later he seems not to have pursued this idea.
Reference: [RESC79] <author> Rescher, N., Brandom, R., </author> <title> "The Logic of Inconsistency", </title> <editor> Rowman and Littlefield Eds., </editor> <year> 1979. </year>
Reference-contexts: This concern in AI is closely linked to a more general concern in philosophy, namely how to model the process of rational inquiry (defined by Brandom in <ref> [RESC79] </ref> as the [rationally controlled] transformation of beliefs over time). This will also be the main aim of this work. <p> Nevertheless, let's face this possibility anyway (some philosophers have indeed argued for the feasibility of this kind of worlds, e.g. Rescher and Brandom in <ref> [RESC79] </ref>, and it is (see [FAGI95]). Others have taken into account in their logical approaches the time that the agent must spend in the course of its reasoning (step logic, see [ELGO88], [NIRK94]). 3 also implicit in the development of the so-called paraconsistent logics (Belnap [BELN77], Fitting [FITT90])). <p> are considered (as remarked by Shoham in [SHOH91]; Fagin and Halpern modelled this situation in their logic of local reasoning, [FAGI85]; Delgrande also considered this idea in [DELG95]). * There is no theoretical problem in defining interesting procedures of inquiry over inconsis tent sets of beliefs (as shown e.g. in <ref> [RESC79] </ref>). * These kinds of world are perfectly conceivable (and even depictable in pictures, as Escher (see e.g. [HOFS80]) proved so many times). * Some theories have been considered acceptable for a certain period of time (e.g. <p> proposed to consider impossible possible worlds, in the sense of having possible worlds where the usual logical connectives do not behave in the usual way, or tautologies may not be true, or inconsistent formulas may hold (see e.g. impossible possible worlds in [HINT75], non-classical worlds in [CRES72], non-standard worlds in <ref> [RESC79] </ref> or situations in [LEVE84]). The drawback of these approaches is that, although they alleviate the problems of logical omniscience and perfect reasoning, they cause different problems.
Reference: [SHOH91] <author> Shoham, Y., </author> <title> "Varieties of context", </title> <booktitle> in Artificial Intelligence and Mathematical Theory of Computation, </booktitle> <pages> pp. 393-408, </pages> <editor> V. Lifschitz Ed., </editor> <publisher> Academic Press, </publisher> <address> N.Y., </address> <year> 1991. </year>
Reference-contexts: The agent can be unable to take all its beliefs into account in every inference; if it focuses in a subset of them (call that a context), it can draw conclusions which are consistent within the context but inconsistent if all the beliefs are considered (as remarked by Shoham in <ref> [SHOH91] </ref>; Fagin and Halpern modelled this situation in their logic of local reasoning, [FAGI85]; Delgrande also considered this idea in [DELG95]). * There is no theoretical problem in defining interesting procedures of inquiry over inconsis tent sets of beliefs (as shown e.g. in [RESC79]). * These kinds of world are perfectly
Reference: [SMUL68] <author> Smullyan, </author> <title> R.M., "First-order logic", </title> <publisher> Springer Verlag, </publisher> <year> 1968. </year>
Reference-contexts: 1 , w ff 2 , ..., w ff n ; furthermore, there is no other proposition in the intersection of all these worlds, so T The logical analysis of the beliefs of the agent is performed using (a modified version of) the method of the analytic tableaux (see, e.g. <ref> [SMUL68] </ref>). A tableau is divided into a left and a right column.
Reference: [SMUL86] <author> Smullyan, </author> <title> R.M., "Logicians who reason about themselves", </title> <booktitle> Proceedings of the 1986 Conference on Theoretical Aspects of Reasoning about Knowledge, </booktitle> <editor> Ed. </editor> <booktitle> J.Halpern, </booktitle> <pages> pp. 341-352, </pages> <year> 1986. </year>
Reference-contexts: Furthermore, if BF (w) and P F (w) were equal that would be an unstable situation, as suggested by Smullyan in <ref> [SMUL86] </ref> 7 .
Reference: [WOOL95] <author> Wooldridge, M., Jennings, N., </author> <title> "Intelligent agents: </title> <journal> theory and practice", Knowledge Engineering Review 10 (2), </journal> <year> 1995. </year> <month> 23 </month>
Reference-contexts: of such agents (typically) includes a knowledge base that stores relevant facts about the agent or its 1 environment, a module capable of making (deductive, inductive, abductive, : : :) inferences from those facts, modules that interact with the agent's environment, modules that create and evaluate different plans, etc. (see <ref> [WOOL95] </ref> for an extensive review of different approaches, including reactive architectures, that do not adhere to this classical picture). If the agent is to display a rational behaviour, it probably needs to have a module capable of handling its beliefs (see e.g. the Belief-Desire-Intention architectures considered in [BRAT88], [RAOG91]).
References-found: 40

