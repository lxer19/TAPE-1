URL: http://www.cs.brown.edu/people/dm/papers/nordic95.ps
Refering-URL: http://www.cs.brown.edu/people/pck/publications.html
Root-URL: http://www.cs.brown.edu/
Title: Controlling Memory Access Concurrency in Efficient Fault-Tolerant Parallel Algorithms  
Author: Paris C. Kanellakis Dimitrios Michailidis Alex A. Shvartsman 
Abstract: The CRCW PRAM under dynamic fail-stop (no restart) processor behavior is a fault-prone multiprocessor model for which it is possible to both guarantee reliability and preserve efficiency. To handle dynamic faults some redundancy is necessary in the form of many processors concurrently performing a common read or write task. In this paper we show how to significantly decrease this concurrency by bounding it in terms of the number of actual processor faults. We describe a low concurrency, efficient and fault-tolerant algorithm for the Write-All primitive: "using N processors, write 1's into N locations". This primitive can serve as the basis for efficient fault-tolerant simulations of algorithms written for fault-free PRAMs on fault-prone PRAMs. For any dynamic failure pattern F , our algorithm has total write concurrency jF j and total read concurrency 7 jF j log N , where jF j is the number of processor faults (for example, there is no concurrency in a run without failures); note that, previous algorithms used (N log N ) concurrency even in the absence of faults. We also describe a technique for limiting the per step concurrency and present an optimal fault-tolerant EREW PRAM algorithm for Write-All, when all processor faults are initial.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> M. Ajtai, J. Aspnes, C. Dwork and O. Waarts, </author> <title> A Theory of Competitive Analysis for Distributed Algorithms, </title> <booktitle> Proc. 36th IEEE FOCS (1994) 401-411. </booktitle> <pages> 31 </pages>
Reference-contexts: In this case the strongest lower bounds are in [25] and the tightest upper bounds are in [3]. More recently, the work on fault-tolerant and efficient parallel shared memory models has also been applied to the distributed message passing models; such results are reported in <ref> [1, 12, 13, 31] </ref>. A key primitive in much of the above mentioned work is the Write-All operation of [21]. The main technical insight in this paper is an efficient, deterministic fault-tolerant Write-All algorithm that significantly reduces memory access concurrency. <p> We use the notation W CR=W [s] to 20 indicate that input elements are organized into clusters of size s. For the case s = 1 we use the shorthand W CR=W for W CR=W <ref> [1] </ref>.
Reference: [2] <author> G.B. Adams III, D.P. Agrawal and H.J. Seigel, </author> <title> A Survey and Comparison of Fault-tolerant Multistage Interconnection Networks, </title> <note> IEEE Computer 20 6 (1987) 14-29. </note>
Reference-contexts: A combining interconnection network that is well suited for implementing synchronous concurrent reads and writes is studied in [26] (the combining properties are used in their simplest form only to implement concurrent access to memory). The network can be made more reliable by employing redundancy <ref> [2] </ref>. 5 2.2 Measures of Efficiency The complexity measure used throughout this work is the available processor steps of [21]. It generalizes the fault-free Parallel-time fiProcessors product and accounts for all steps performed by the active processors.
Reference: [3] <author> R. Anderson and H. Woll, </author> <title> Wait-Free Parallel Algorithms for the Union-Find Problem, </title> <booktitle> in: Proc. 23rd ACM STOC (1991) 370-380. </booktitle>
Reference-contexts: More general processor asynchrony has been examined in [3, 4, 5, 8, 9, 10, 16, 23, 24, 25, 28, 29, 30, 32]. Many of these analyses involve average processor behavior and use randomization. Some (e.g., <ref> [3, 8, 25] </ref>) deal with deterministic asynchronous computation (i.e., removing assumption (2) without using randomness). In this case the strongest lower bounds are in [25] and the tightest upper bounds are in [3]. <p> Many of these analyses involve average processor behavior and use randomization. Some (e.g., [3, 8, 25]) deal with deterministic asynchronous computation (i.e., removing assumption (2) without using randomness). In this case the strongest lower bounds are in [25] and the tightest upper bounds are in <ref> [3] </ref>. More recently, the work on fault-tolerant and efficient parallel shared memory models has also been applied to the distributed message passing models; such results are reported in [1, 12, 13, 31]. A key primitive in much of the above mentioned work is the Write-All operation of [21].
Reference: [4] <author> Y. Aumann and M.O. Rabin, </author> <title> Clock Construction in Fully Asynchronous Parallel Systems and PRAM Simulation, </title> <booktitle> in: Proc. 34th IEEE FOCS (1992) 147-156. </booktitle>
Reference: [5] <author> Y. Aumann, Z.M. Kedem, K.V. Palem and M.O. Rabin, </author> <title> Highly Efficient Asynchronous Execution of Large-Grained Parallel Programs, </title> <booktitle> in: Proc. 35th IEEE FOCS (1993) 271-280. </booktitle>
Reference: [6] <author> P. Beame, M. Kik and M. Kutylowski, </author> <title> Information Broadcasting by Exclusive Read PRAMs, </title> <type> manuscript 1992. </type>
Reference-contexts: For P = N , this solution is optimal (based on a result of <ref> [6] </ref>). For general simulations of PRAM algorithms our solution introduces a fi (P 0 log P ) additive overhead. It also handles static memory faults at no additional overhead. In summary, the static case has a simple optimal solution which eliminates memory access concurrency. <p> With the result of <ref> [6] </ref> it can be shown that this algorithm is optimal. Algorithm E can be used for simulations of PRAM algorithms on fail-stop PRAMs that are subject to static initial processor and memory faults. Simulations are much simpler for this case as compared to the dynamic failures case.
Reference: [7] <author> R.P. Brent, </author> <title> The Parallel Evaluation of General Arithmetic Expressions, </title> <journal> J. </journal> <note> ACM 21 (1974) 201-206. </note>
Reference-contexts: In Section 3.6 we describe how this algorithm can be used as a building block for efficient simulations of arbitrary PRAMs. This is a dynamic version of Brent's lemma <ref> [7] </ref>. In Section 4 we examine the worst-case concurrency per step. This is a different measure than the overall concurrency of Section 3. We present a variation of algorithm W whose maximum read/write concurrency per step is N=polylog (N ) instead of N . <p> In particular, only the enumeration phase E1 of algorithm E is needed. At the beginning of the simulation we use the enumeration algorithm to determine the number P 0 of live processors. We then use Brent's Lemma <ref> [7] </ref> to simulate the P processors of the original algorithm on the P 0 available processors. The time overhead of the simulation in this case is just O (log P ) for the enumeration and the work overhead is O (P 0 log P ). Both overheads are additive.
Reference: [8] <author> J. Buss, P.C. Kanellakis, P. Ragde and A.A. Shvartsman, </author> <title> Parallel Algorithms with Processor Failures and Delays, </title> <institution> Brown University TR CS-91-54, </institution> <year> 1991. </year> <note> (Prel. version appears as P.C. </note> <author> Kanellakis and A.A. Shvartsman, </author> <title> Efficient Parallel Algorithms On Restartable Fail-Stop Processors, </title> <booktitle> in: Proc. 10th ACM PODC (1991) 23-36.) </booktitle>
Reference-contexts: More general processor asynchrony has been examined in [3, 4, 5, 8, 9, 10, 16, 23, 24, 25, 28, 29, 30, 32]. Many of these analyses involve average processor behavior and use randomization. Some (e.g., <ref> [3, 8, 25] </ref>) deal with deterministic asynchronous computation (i.e., removing assumption (2) without using randomness). In this case the strongest lower bounds are in [25] and the tightest upper bounds are in [3].
Reference: [9] <author> R. Cole and O. Zajicek, </author> <title> The APRAM: Incorporating Asynchrony into the PRAM Model, </title> <booktitle> in: Proc. 1st ACM Symposium on Parallel Algorithms and Architectures (1989) 170-178. </booktitle>
Reference-contexts: algorithm is t N , then the available steps of the fault-tolerant simulation will be O (t S w (N; P )). 2.3.3 Static Initial Memory Errors The Write-All algorithms and simulations, e.g., [21, 23, 25, 36], or the algorithms that can serve as Write-All solutions, e.g., the algorithms in <ref> [9, 30] </ref>, invariably assume that a linear portion of shared memory is either cleared or is initialized to known values. Starting with a non-contaminated portion of memory, these algorithms perform their computation by "using up" the clear memory, and concurrently or subsequently clearing segments of memory needed for future iterations.
Reference: [10] <author> R. Cole and O. Zajicek, </author> <title> The Expected Advantage of Asynchrony, </title> <booktitle> in: Proc. 2nd ACM Symposium on Parallel Algorithms and Architectures (1990) 85-94. </booktitle>
Reference: [11] <author> F. Cristian, </author> <title> Understanding Fault-Tolerant Distributed Systems, </title> <note> Communications of the ACM 3 2 (1991) 56-78. </note>
Reference-contexts: Fault-tolerant technologies, such as those in surveys <ref> [11, 17, 18] </ref>, all contribute towards concrete realizations of its components. a. There are P fail-stop processors (see [34]), each with a unique address and some local memory. b. There are Q shared memory cells, the input of size N Q is stored in shared memory.
Reference: [12] <author> R. DePrisco, A. Mayer and M. Yung, </author> <title> Time-Optimal Message-Efficient Work Performance in the Presence of Faults, </title> <booktitle> Proc. 13th ACM PODC (1994) 161-172. </booktitle>
Reference-contexts: In this case the strongest lower bounds are in [25] and the tightest upper bounds are in [3]. More recently, the work on fault-tolerant and efficient parallel shared memory models has also been applied to the distributed message passing models; such results are reported in <ref> [1, 12, 13, 31] </ref>. A key primitive in much of the above mentioned work is the Write-All operation of [21]. The main technical insight in this paper is an efficient, deterministic fault-tolerant Write-All algorithm that significantly reduces memory access concurrency.
Reference: [13] <author> C. Dwork, J. Halpern and O. Waarts, </author> <title> Performing Work Efficiently in the Presence of Faults, </title> <booktitle> Proc. 11th ACM PODC (1992) 91-102. </booktitle>
Reference-contexts: In this case the strongest lower bounds are in [25] and the tightest upper bounds are in [3]. More recently, the work on fault-tolerant and efficient parallel shared memory models has also been applied to the distributed message passing models; such results are reported in <ref> [1, 12, 13, 31] </ref>. A key primitive in much of the above mentioned work is the Write-All operation of [21]. The main technical insight in this paper is an efficient, deterministic fault-tolerant Write-All algorithm that significantly reduces memory access concurrency.
Reference: [14] <author> D. Eppstein and Z. Galil, </author> <title> Parallel Techniques for Combinatorial Computation, </title> <note> Annual Computer Science Review 3 (1988) 233-283. </note>
Reference-contexts: We conclude with some open problems in Section 6. 2 The Model and Base Algorithms 2.1 Fail-Stop PRAM The parallel random access machine (PRAM) of Fortune and Wyllie [15] combines the simplicity of RAM with the power of parallelism, and a wealth of efficient algorithms exist for it; see surveys <ref> [14, 22] </ref> for the rationale behind this model and the fundamental algorithms. The main features of this model are as follows: 1. There are P initial processors with unique identifiers (PID) in the range 1; : : : ; P .
Reference: [15] <author> S. Fortune and J. Wyllie, </author> <title> Parallelism in Random Access Machines, </title> <booktitle> in: Proc. 10th ACM STOC (1978) 114-118. </booktitle>
Reference-contexts: In summary, the static case has a simple optimal solution which eliminates memory access concurrency. We conclude with some open problems in Section 6. 2 The Model and Base Algorithms 2.1 Fail-Stop PRAM The parallel random access machine (PRAM) of Fortune and Wyllie <ref> [15] </ref> combines the simplicity of RAM with the power of parallelism, and a wealth of efficient algorithms exist for it; see surveys [14, 22] for the rationale behind this model and the fundamental algorithms. The main features of this model are as follows: 1.
Reference: [16] <author> P. Gibbons, </author> <title> A More Practical PRAM Model, </title> <booktitle> in: Proc. 1st ACM Symposium on Parallel Algorithms and Architectures (1989) 158-168. </booktitle>
Reference: [17] <institution> Fault-Tolerant Computing, </institution> <note> IEEE Computer special issue 17 8 (1984). </note>
Reference-contexts: Fault-tolerant technologies, such as those in surveys <ref> [11, 17, 18] </ref>, all contribute towards concrete realizations of its components. a. There are P fail-stop processors (see [34]), each with a unique address and some local memory. b. There are Q shared memory cells, the input of size N Q is stored in shared memory.
Reference: [18] <institution> Fault-Tolerant Systems, </institution> <note> IEEE Computer special issue 23 7 (1990). </note>
Reference-contexts: Fault-tolerant technologies, such as those in surveys <ref> [11, 17, 18] </ref>, all contribute towards concrete realizations of its components. a. There are P fail-stop processors (see [34]), each with a unique address and some local memory. b. There are Q shared memory cells, the input of size N Q is stored in shared memory.
Reference: [19] <author> P.C. Kanellakis, D. Michailidis and A.A. Shvartsman, </author> <title> Concurrency = Fault-Tolerance in Parallel Computation, invited paper, </title> <booktitle> Proc. 5th International Conference on Concurrency Theory, Lecture Notes in Computer Science vol. </booktitle> <month> 836 </month> <year> (1994) </year> <month> 242-266. </month>
Reference-contexts: Such combinations of fault and computation models are interesting because they il lustrate good trade-offs between reliability and efficiency. To see that there is a trade-off fl Research supported by ONR grant N00014-91-J-1613 and by ONR contract N00014-91-J-4052 ARPA order 8225. Parts of this research were reported in <ref> [19] </ref> and [20]. y Department of Computer Science, Brown University, Box 1910, Providence, RI 02912-1910, USA. z Laboratory for Computer Science, Massachusetts Institute of Technology, 545 Technology Square NE43 340, Cambridge, MA 02139, USA. 1 observe that reliability usually requires adding redundancy to the computation in order to detect errors and
Reference: [20] <author> P.C. Kanellakis, D. Michailidis and A.A. Shvartsman, </author> <title> Controlling Memory Access Concur-rency in Efficient Fault-Tolerant Parallel Algorithms, </title> <booktitle> Proc. 7th WDAG, Lecture Notes in Computer Science vol. </booktitle> <month> 725 </month> <year> (1993) </year> <month> 99-114. </month>
Reference-contexts: To see that there is a trade-off fl Research supported by ONR grant N00014-91-J-1613 and by ONR contract N00014-91-J-4052 ARPA order 8225. Parts of this research were reported in [19] and <ref> [20] </ref>. y Department of Computer Science, Brown University, Box 1910, Providence, RI 02912-1910, USA. z Laboratory for Computer Science, Massachusetts Institute of Technology, 545 Technology Square NE43 340, Cambridge, MA 02139, USA. 1 observe that reliability usually requires adding redundancy to the computation in order to detect errors and reassign resources,
Reference: [21] <author> P.C. Kanellakis and A.A. Shvartsman, </author> <title> Efficient Parallel Algorithms Can Be Made Robust, </title> <note> Distributed Computing 5 (1992) 201-217. (Prel. version in Proc. 8th ACM PODC (1989) 138-148.) </note>
Reference-contexts: Even allowing for some abstraction in the model of parallel computation, it is not obvious that there are any non-trivial fault models that allow near-linear speed-ups. So it was somewhat surprising when in <ref> [21] </ref> we demonstrated that it is possible to combine efficiency and fault-tolerance for many basic algorithms that run on concurrent-read concurrent-write parallel random access machines (CRCW PRAMs). The fault model introduced in [21] allows any pattern of dynamic fail-stop no restart processor errors, as long as one processor remains alive. <p> So it was somewhat surprising when in <ref> [21] </ref> we demonstrated that it is possible to combine efficiency and fault-tolerance for many basic algorithms that run on concurrent-read concurrent-write parallel random access machines (CRCW PRAMs). The fault model introduced in [21] allows any pattern of dynamic fail-stop no restart processor errors, as long as one processor remains alive. Interestingly, our technique can be extended to all CRCW PRAMs. <p> The fault model was extended in [37] to include arbitrary static memory faults , i.e., arbitrary memory initialization. The CRCW PRAM computation model is a widely used abstraction of real massively parallel machines. As shown in <ref> [21] </ref>, it suffices to consider COMMON CRCW PRAMs (all concurrent writes are identical) in which the atomically written words need only contain a constant number of bits. All the above-mentioned algorithmic work makes two somewhat unrealistic, but critical, assumptions. <p> More recently, the work on fault-tolerant and efficient parallel shared memory models has also been applied to the distributed message passing models; such results are reported in [1, 12, 13, 31]. A key primitive in much of the above mentioned work is the Write-All operation of <ref> [21] </ref>. The main technical insight in this paper is an efficient, deterministic fault-tolerant Write-All algorithm that significantly reduces memory access concurrency. The Write-All problem is: using P processors write 1s into all locations of an array of size N , where P N . <p> In the fail-stop PRAM model the P processors share a global clock and are fail-stop no restart. Under dynamic failures, efficient deterministic solutions to Write-All, i.e., increasing the fault-free O (N ) work by small polylog (N ) factors, are non-obvious. The first such solution was algorithm W of <ref> [21] </ref> which has (to date) the best worst-case work bound O (N + P log 2 N= log log N ) for 1 P N , where P is the initial numebr of processors (this bound was shown in [25] for a variant of algorithm W and in [27] the same <p> To make the exposition self contained we present in Section 2.3 an outline of the main algorithmic ideas of algorithm W and sketch how it is utilized in PRAM simulation and memory clearing. Memory access concurrency is the main topic of this paper. In <ref> [21] </ref> it is shown that read and write concurrency are necessary for deterministic efficient solutions under dynamic processor faults. All Write-All solutions that have efficient work for any dynamic processor failures require concurrent-read concurrent-write (CRCW) PRAMs. <p> atomic with respect to failures: failures can occur before or after a shared write of fi (log maxfN; P g)-bit words, but not during the write. (This non-trivial assumption is made only for simplicity of presentation; algorithms that make it can be converted to use only single bit atomic writes <ref> [21] </ref>.) We also handle arbitrary initialization of the shared memory, which is assumed clear in the fault-free PRAM model. (This extends 3 above.) The abstract model that we are studying can be realized in the architecture of Fig. 1. <p> The network can be made more reliable by employing redundancy [2]. 5 2.2 Measures of Efficiency The complexity measure used throughout this work is the available processor steps of <ref> [21] </ref>. It generalizes the fault-free Parallel-time fiProcessors product and accounts for all steps performed by the active processors. <p> This does not imply that the algorithm itself is an EREW or CREW algorithm. However, an algorithm is EREW if its worst case and ! are 0 and it is CREW if its worst case ! is 0. 2.3 Background Algorithms 2.3.1 Algorithm W Algorithm W of <ref> [21] </ref> is a robust Write-All solution. It uses two complete binary trees as data structures: the processor enumeration and the progress trees. A high level view of algorithm W is in Fig. 2. <p> Algorithm W performs best when the cluster size s is chosen to be log N , resulting in a progress tree of N= log N leaves. A complete description of the algorithm can be found in <ref> [21] </ref>. <p> If the Parallel-time fiProcessors of an original N -processor algorithm is t N , then the available steps of the fault-tolerant simulation will be O (t S w (N; P )). 2.3.3 Static Initial Memory Errors The Write-All algorithms and simulations, e.g., <ref> [21, 23, 25, 36] </ref>, or the algorithms that can serve as Write-All solutions, e.g., the algorithms in [9, 30], invariably assume that a linear portion of shared memory is either cleared or is initialized to known values. <p> The same problem is shared by all robust deterministic Write-All algorithms that have been proposed to date. As shown in <ref> [21] </ref>, concurrent memory writes cannot be eliminated altogether. Any Write-All algorithm that does not use concurrent writes, e.g. <p> A timestamp in this context is just a sequence number that need not exceed N (see <ref> [21] </ref>). <p> The resulting algorithm W CR=W [log N log P ] turns out to be nonoptimal, so a further modification is needed. To obtain optimality we make use of the perfect allocation property of algorithm W (see <ref> [21] </ref>). This property guarantees that available processors are allocated evenly to the unvisited leaves in phase W2. Let U i and P i be the numbers of unvisited leaves and available processors, respectively, at the beginning of the ith iteration of the while loop of algorithm W CR=W . <p> Specifically, we show that there is no robust algorithm for the Write-All problem with concurrency ! jF j " for 0 " &lt; 1. For this we consider a simpler Write-One problem <ref> [21] </ref> which involves P processors and a single memory location. The objective is to write 1 into the memory location subject to faults determined by an on-line adversary.
Reference: [22] <author> R.M. Karp and V. Ramachandran, </author> <title> A Survey of Parallel Algorithms for Shared-Memory Ma--chines, </title> <editor> in: J. van Leeuwen, ed., </editor> <booktitle> Handbook of Theoretical Computer Science (North-Holland, </booktitle> <year> 1990) </year> <month> 869-941. </month>
Reference-contexts: We conclude with some open problems in Section 6. 2 The Model and Base Algorithms 2.1 Fail-Stop PRAM The parallel random access machine (PRAM) of Fortune and Wyllie [15] combines the simplicity of RAM with the power of parallelism, and a wealth of efficient algorithms exist for it; see surveys <ref> [14, 22] </ref> for the rationale behind this model and the fundamental algorithms. The main features of this model are as follows: 1. There are P initial processors with unique identifiers (PID) in the range 1; : : : ; P . <p> Alternatively, we can maintain the same concurrency bounds at the expense of increasing the work by a logarithmic factor by first converting the original algorithm into an equivalent EREW algorithm <ref> [22] </ref>. Similar results can be derived for contaminated initial memory using algorithm Z and Theorem 3.4 to initialize an appropriate amount of auxiliary shared memory required by the simulation.
Reference: [23] <author> Z.M. Kedem, K.V. Palem and P. Spirakis, </author> <title> Efficient Robust Parallel Computations, </title> <booktitle> in: Proc. 22nd ACM STOC (1990) 138-148. </booktitle>
Reference-contexts: The fault model introduced in [21] allows any pattern of dynamic fail-stop no restart processor errors, as long as one processor remains alive. Interestingly, our technique can be extended to all CRCW PRAMs. In <ref> [23] </ref> it is shown how to simulate any fault-free PRAM on a CRCW PRAM that executes in a fail-stop no-restart environment with comparable efficiency and in [36] it is shown how the simulation can be made optimal using processor slackness. <p> Algorithm W was extended to handle arbitrary initial memory in [37] and served as a building block for transforming fault-free PRAMs into fault-prone PRAMs of comparable efficiency in <ref> [23, 36] </ref>. We say that Write-All completes at the global clock tick at which all the processors that have not fail-stopped share the knowledge that 1's have been written into all N array locations. <p> Requiring completion of a Write-All algorithm is critical if one wishes to iterate it, as pointed out in <ref> [23] </ref> which uses a certification bit to separate the various iterations of (Certified) Write-All. Note that the Write-All completes when all processors halt in algorithm W. This is also the case for the algorithms presented here. In Section 2.1 we present our fault and computation models. <p> This intuition was made concrete when it was shown in <ref> [23, 36] </ref> how to use solutions for the Write-All problem in implementing general PRAM simulations. 8 01 forall processors PID=1..P parbegin P fail-stop processors simulate N fault-prone ones 02 The PRAM program for N processors is stored in shared memory (read-only) 03 Shared memory has two generations: current and future; the <p> This is done using two generations of shared memory, "current" and "future", and by executing each of these cycles in the Write-All style, e.g., using algorithm W (for details, see <ref> [23, 36] </ref>). Using such algorithm simulation techniques it was shown in [23, 36] that if S w (N; P ) denotes the available steps of solving a Write-All instance of size N using P processors, and if a linear in N amount of clear memory is available, then any N -processor <p> This is done using two generations of shared memory, "current" and "future", and by executing each of these cycles in the Write-All style, e.g., using algorithm W (for details, see <ref> [23, 36] </ref>). Using such algorithm simulation techniques it was shown in [23, 36] that if S w (N; P ) denotes the available steps of solving a Write-All instance of size N using P processors, and if a linear in N amount of clear memory is available, then any N -processor PRAM step can be deterministically simulated using P fail-stop processors and <p> If the Parallel-time fiProcessors of an original N -processor algorithm is t N , then the available steps of the fault-tolerant simulation will be O (t S w (N; P )). 2.3.3 Static Initial Memory Errors The Write-All algorithms and simulations, e.g., <ref> [21, 23, 25, 36] </ref>, or the algorithms that can serve as Write-All solutions, e.g., the algorithms in [9, 30], invariably assume that a linear portion of shared memory is either cleared or is initialized to known values.
Reference: [24] <author> Z.M. Kedem, K.V. Palem, M.O. Rabin and A. Raghunathan, </author> <title> Efficient Program Transformations for Resilient Parallel Computation via Randomization, </title> <booktitle> in: Proc. 24th ACM STOC (1992) 306-318. </booktitle>
Reference: [25] <author> Z.M. Kedem, K.V. Palem, A. Raghunathan and P. Spirakis, </author> <title> Combining Tentative and Definite Executions for Dependable Parallel Computing, </title> <booktitle> in: Proc. 23rd ACM STOC (1991) 381-390. </booktitle>
Reference-contexts: More general processor asynchrony has been examined in [3, 4, 5, 8, 9, 10, 16, 23, 24, 25, 28, 29, 30, 32]. Many of these analyses involve average processor behavior and use randomization. Some (e.g., <ref> [3, 8, 25] </ref>) deal with deterministic asynchronous computation (i.e., removing assumption (2) without using randomness). In this case the strongest lower bounds are in [25] and the tightest upper bounds are in [3]. <p> Many of these analyses involve average processor behavior and use randomization. Some (e.g., [3, 8, 25]) deal with deterministic asynchronous computation (i.e., removing assumption (2) without using randomness). In this case the strongest lower bounds are in <ref> [25] </ref> and the tightest upper bounds are in [3]. More recently, the work on fault-tolerant and efficient parallel shared memory models has also been applied to the distributed message passing models; such results are reported in [1, 12, 13, 31]. <p> The first such solution was algorithm W of [21] which has (to date) the best worst-case work bound O (N + P log 2 N= log log N ) for 1 P N , where P is the initial numebr of processors (this bound was shown in <ref> [25] </ref> for a variant of algorithm W and in [27] the same bound was shown for algorithm W). Algorithm W was extended to handle arbitrary initial memory in [37] and served as a building block for transforming fault-free PRAMs into fault-prone PRAMs of comparable efficiency in [23, 36]. <p> If the Parallel-time fiProcessors of an original N -processor algorithm is t N , then the available steps of the fault-tolerant simulation will be O (t S w (N; P )). 2.3.3 Static Initial Memory Errors The Write-All algorithms and simulations, e.g., <ref> [21, 23, 25, 36] </ref>, or the algorithms that can serve as Write-All solutions, e.g., the algorithms in [9, 30], invariably assume that a linear portion of shared memory is either cleared or is initialized to known values. <p> A second open problem is whether there is a faster algorithm than W for the Write-All and whether the methods presented here for controlling memory accesses can be applied to it. In <ref> [25] </ref> an (N log N ) lower bound was shown for the Write-All problem but no known deterministic algorithm attains it. A third open problem is whether the lower bound of [25] applies to the static case. <p> In <ref> [25] </ref> an (N log N ) lower bound was shown for the Write-All problem but no known deterministic algorithm attains it. A third open problem is whether the lower bound of [25] applies to the static case. It is interesting to consider whether there is a CRCW algorithm for the static Write-All that requires o (N log N ) work.
Reference: [26] <author> C. P. Kruskal, L. Rudolph, M. Snir, </author> <title> Efficient Synchronization on Multiprocessors with Shared Memory, </title> <journal> ACM TOPLAS 10 (1988) pp. </journal> <pages> 579-601. </pages>
Reference-contexts: Processors and memory are interconnected via a synchronous network (e.g., as in the Ultracomputer [35]). A combining interconnection network that is well suited for implementing synchronous concurrent reads and writes is studied in <ref> [26] </ref> (the combining properties are used in their simplest form only to implement concurrent access to memory). The network can be made more reliable by employing redundancy [2]. 5 2.2 Measures of Efficiency The complexity measure used throughout this work is the available processor steps of [21].
Reference: [27] <author> C. </author> <title> Martel, </title> <type> personal communication, </type> <month> March, </month> <year> 1991. </year>
Reference-contexts: W of [21] which has (to date) the best worst-case work bound O (N + P log 2 N= log log N ) for 1 P N , where P is the initial numebr of processors (this bound was shown in [25] for a variant of algorithm W and in <ref> [27] </ref> the same bound was shown for algorithm W). Algorithm W was extended to handle arbitrary initial memory in [37] and served as a building block for transforming fault-free PRAMs into fault-prone PRAMs of comparable efficiency in [23, 36]. <p> They start from the leaves they were at in phase W3. A version of the standard parallel summation algorithm is employed to compute underestimates of the progress for each subtree. The following lemma of <ref> [27] </ref> provides a bound on the total number of block steps executed by all processors (where a block step is an execution by one processor of the body of the while-loop in Fig. 2). It will be used in our analyses in Section 3. Lemma 2.1 ([27]) Let U be the
Reference: [28] <author> C. Martel, A. Park and R. Subramonian, </author> <title> Work-optimal Asynchronous Algorithms for Shared Memory Parallel Computers, </title> <note> SIAM Journal on Computing 21 (1992) 1070-1099. </note>
Reference: [29] <author> C. Martel and R. Subramonian, </author> <title> On the Complexity of Certified Write-All Algorithms, </title> <note> Journal of Algorithms 16 (1994) 361-387. </note>
Reference: [30] <author> C. Martel, R. Subramonian and A. Park, </author> <title> Asynchronous PRAMs are (Almost) as Good as Synchronous PRAMs, </title> <booktitle> in: Proc. 32nd IEEE FOCS (1990) 590-599. </booktitle>
Reference-contexts: algorithm is t N , then the available steps of the fault-tolerant simulation will be O (t S w (N; P )). 2.3.3 Static Initial Memory Errors The Write-All algorithms and simulations, e.g., [21, 23, 25, 36], or the algorithms that can serve as Write-All solutions, e.g., the algorithms in <ref> [9, 30] </ref>, invariably assume that a linear portion of shared memory is either cleared or is initialized to known values. Starting with a non-contaminated portion of memory, these algorithms perform their computation by "using up" the clear memory, and concurrently or subsequently clearing segments of memory needed for future iterations.
Reference: [31] <author> A. Mayer and M. Yung, </author> <title> Message Complexity of "Crash-failure Agreement" Resolved, </title> <type> Manuscript (1994). </type>
Reference-contexts: In this case the strongest lower bounds are in [25] and the tightest upper bounds are in [3]. More recently, the work on fault-tolerant and efficient parallel shared memory models has also been applied to the distributed message passing models; such results are reported in <ref> [1, 12, 13, 31] </ref>. A key primitive in much of the above mentioned work is the Write-All operation of [21]. The main technical insight in this paper is an efficient, deterministic fault-tolerant Write-All algorithm that significantly reduces memory access concurrency.
Reference: [32] <author> N. Nishimura, </author> <title> Asynchronous Shared Memory Parallel Computation, </title> <booktitle> in: Proc. 2nd ACM Symposium on Parallel Algorithms and Architectures (1990) 76-84. </booktitle>
Reference: [33] <author> D.B. Sarrazin and M. Malek, </author> <title> Fault-Tolerant Semiconductor Memories, </title> <note> IEEE Computer 17 8 (1984) 49-56. </note>
Reference-contexts: There are Q shared memory cells, the input of size N Q is stored in shared memory. These semiconductor memories can be manufactured with built-in fault tolerance using replication and coding techniques without appreciably degrading performance <ref> [33] </ref>. c. Processors and memory are interconnected via a synchronous network (e.g., as in the Ultracomputer [35]).
Reference: [34] <author> R.D. Schlichting and F.B. Schneider, </author> <title> Fail-Stop Processors: an Approach to Designing Fault-tolerant Computing Systems, </title> <journal> ACM Trans. on Comp. Sys. </journal> <month> 1 </month> <year> (1983) </year> <month> 222-238. </month>
Reference-contexts: Fault-tolerant technologies, such as those in surveys [11, 17, 18], all contribute towards concrete realizations of its components. a. There are P fail-stop processors (see <ref> [34] </ref>), each with a unique address and some local memory. b. There are Q shared memory cells, the input of size N Q is stored in shared memory. These semiconductor memories can be manufactured with built-in fault tolerance using replication and coding techniques without appreciably degrading performance [33]. c.
Reference: [35] <author> J.T. Schwartz, </author> <note> Ultracomputers, ACM TOPLAS 2 (1980) 484-521. </note>
Reference-contexts: These semiconductor memories can be manufactured with built-in fault tolerance using replication and coding techniques without appreciably degrading performance [33]. c. Processors and memory are interconnected via a synchronous network (e.g., as in the Ultracomputer <ref> [35] </ref>). A combining interconnection network that is well suited for implementing synchronous concurrent reads and writes is studied in [26] (the combining properties are used in their simplest form only to implement concurrent access to memory).
Reference: [36] <author> A.A. Shvartsman, </author> <title> Achieving Optimal CRCW PRAM Fault-Tolerance, </title> <note> Information Processing Letters 39 (1991) 59-66. </note>
Reference-contexts: Interestingly, our technique can be extended to all CRCW PRAMs. In [23] it is shown how to simulate any fault-free PRAM on a CRCW PRAM that executes in a fail-stop no-restart environment with comparable efficiency and in <ref> [36] </ref> it is shown how the simulation can be made optimal using processor slackness. The fault model was extended in [37] to include arbitrary static memory faults , i.e., arbitrary memory initialization. The CRCW PRAM computation model is a widely used abstraction of real massively parallel machines. <p> Algorithm W was extended to handle arbitrary initial memory in [37] and served as a building block for transforming fault-free PRAMs into fault-prone PRAMs of comparable efficiency in <ref> [23, 36] </ref>. We say that Write-All completes at the global clock tick at which all the processors that have not fail-stopped share the knowledge that 1's have been written into all N array locations. <p> This intuition was made concrete when it was shown in <ref> [23, 36] </ref> how to use solutions for the Write-All problem in implementing general PRAM simulations. 8 01 forall processors PID=1..P parbegin P fail-stop processors simulate N fault-prone ones 02 The PRAM program for N processors is stored in shared memory (read-only) 03 Shared memory has two generations: current and future; the <p> This is done using two generations of shared memory, "current" and "future", and by executing each of these cycles in the Write-All style, e.g., using algorithm W (for details, see <ref> [23, 36] </ref>). Using such algorithm simulation techniques it was shown in [23, 36] that if S w (N; P ) denotes the available steps of solving a Write-All instance of size N using P processors, and if a linear in N amount of clear memory is available, then any N -processor <p> This is done using two generations of shared memory, "current" and "future", and by executing each of these cycles in the Write-All style, e.g., using algorithm W (for details, see <ref> [23, 36] </ref>). Using such algorithm simulation techniques it was shown in [23, 36] that if S w (N; P ) denotes the available steps of solving a Write-All instance of size N using P processors, and if a linear in N amount of clear memory is available, then any N -processor PRAM step can be deterministically simulated using P fail-stop processors and <p> If the Parallel-time fiProcessors of an original N -processor algorithm is t N , then the available steps of the fault-tolerant simulation will be O (t S w (N; P )). 2.3.3 Static Initial Memory Errors The Write-All algorithms and simulations, e.g., <ref> [21, 23, 25, 36] </ref>, or the algorithms that can serve as Write-All solutions, e.g., the algorithms in [9, 30], invariably assume that a linear portion of shared memory is either cleared or is initialized to known values.
Reference: [37] <author> A.A. Shvartsman, </author> <title> An Efficient Write-All Algorithm for Fail-Stop PRAM without Initialized Memory, </title> <note> Information Processing Letters 44 (1992) 223-231. 33 </note>
Reference-contexts: In [23] it is shown how to simulate any fault-free PRAM on a CRCW PRAM that executes in a fail-stop no-restart environment with comparable efficiency and in [36] it is shown how the simulation can be made optimal using processor slackness. The fault model was extended in <ref> [37] </ref> to include arbitrary static memory faults , i.e., arbitrary memory initialization. The CRCW PRAM computation model is a widely used abstraction of real massively parallel machines. <p> Algorithm W was extended to handle arbitrary initial memory in <ref> [37] </ref> and served as a building block for transforming fault-free PRAMs into fault-prone PRAMs of comparable efficiency in [23, 36]. <p> Starting with a non-contaminated portion of memory, these algorithms perform their computation by "using up" the clear memory, and concurrently or subsequently clearing segments of memory needed for future iterations. An efficient Write-All solution that requires no clear shared memory has recently been defined using a bootstrap approach <ref> [37] </ref>. 9 The bootstrapping proceeds in stages: In stage 1 all P processors clear an initial segment of N 0 locations in the auxiliary memory. <p> If N i+1 &gt; N i and N 0 1, then the required N memory location will be cleared in at most N stages. The efficiency of the resulting algorithm depends on the particular Write-All solution (s) used and the parameters N i and G i . In <ref> [37] </ref>, a solution for Write-All (algorithm Z, see Fig. 4) is presented that for any failure pattern F (jF j &lt; P ) has work O (N +P log 3 N=(log log N ) 2 ) without any initialization assumptions. <p> The bound on is obtained as in Theorem 3.6. 2 3.4 Handling Unitialized Memory The algorithms considered so far require that memory be initially clear. Algorithm Z of <ref> [37] </ref> extends algorithm W to handle unitialized memory (see Section 2.3.3). It is possible to incorporate CR/W into Z to obtain a Write-All algorithm with limited concurrency that dispenses with the requirement that memory be clear. <p> This analysis is very similar to the analysis of <ref> [37] </ref> and the work bound follows by the same method; the only difference is the different value for the G i 's and that the work of each iteration is given by Theorem 3.7.
References-found: 37

