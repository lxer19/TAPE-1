URL: http://simon.cs.cornell.edu/Info/Projects/Bernoulli/papers/tocs93.ps
Refering-URL: 
Root-URL: 
Title: Access Normalization: Loop Restructuring for NUMA Computers  
Author: Wei Li Keshav Pingali 
Address: Ithaca, New York 14853  
Affiliation: Department of Computer Science Cornell University  
Abstract: In scalable parallel machines, processors can make local memory accesses much faster than they can make remote memory accesses. In addition, when a number of remote accesses must be made, it is usually more efficient to use block transfers of data rather than to use many small messages. To run well on such machines, software must exploit these features. We believe it is too onerous for a programmer to do this by hand, so we have been exploring the use of restructuring compiler technology for this purpose. In this paper, we start with a language like HPF-FORTRAN with user-specified data distribution and develop a systematic loop transformation strategy called access normalization that restructures loop nests to exploit locality and block transfers. We demonstrate the power of our techniques using routines from the BLAS (Basic Linear Algebra Subprograms) library. An important feature of our approach is that we model loop transformations using invertible matrices and integer lattice theory. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> A. Agarwal. </author> <title> Limits on interconnection network perfor mance. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 2(4) </volume> <pages> 398-412, </pages> <month> October </month> <year> 1991. </year>
Reference-contexts: Contention in the network has the effect of increasing the expected latency of non-local references; therefore, data management to avoid non-local references has the added benefit of reducing contention, thereby improving performance. Interestingly, some analytical studies show that long messages can increase the latency of non-local accesses <ref> [1] </ref>. This is an argument against long messages, but on current machines, this effect seems to be of secondary importance compared to the benefits of amortizing start-up time, as we show in Section 8. <p> This vector can be written as x = cZ (Z T Z) 1 Z T e k for some positive scaling integer c that makes all of the entries integers, where e T i = <ref> [0; 0; ::; 1; ::; 0] </ref>, with the 1 in the ith position, and Z is a column basis from D. j (0 0 1) For our example, the remaining dependence to be satisfied is e 3 . <p> The data access matrix is 1 1 0 0 0 1 1 0 0 . If we apply Algorithm BasisMatrix, we get a base matrix B consisting of the first three rows. However, the dependence matrix is <ref> [0; 0; 1] </ref> T . The legal base mapping is B legal = 1 1 0 0 1 1 , which is B with the second row negated. This matrix is invertible.
Reference: [2] <author> F. Allen, J. Cocke, and K. Kennedy. </author> <title> Reduction of Op erator Strength, </title> <address> pages 79-101. </address> <publisher> Prentice-Hall, </publisher> <year> 1981. </year>
Reference: [3] <author> R. Allen and K. Kennedy. </author> <title> Automatic translation of FORTRAN programs to vector form. </title> <journal> ACM Transactions on Progamming Languages and Systems, </journal> <volume> 9(4) </volume> <pages> 491-542, </pages> <month> October </month> <year> 1987. </year>
Reference-contexts: To reduce synchronization, transformations like loop interchange are carried out to move parallel loops outermost wherever possible <ref> [3, 7, 26, 37] </ref>. This approach does not perform any data management, so it is not suitable for generating good code on NUMA architectures.
Reference: [4] <author> C. Ancourt and F. Irigoin. </author> <title> Scanning polyhedra with DO loops. </title> <booktitle> In Third ACM Symposium on Principles and Practice of Parallel Programming, </booktitle> <pages> pages 39-50, </pages> <month> April </month> <year> 1991. </year>
Reference: [5] <author> V. Balasundaram, G. Fox, K. Kennedy, and U. Kremer. </author> <title> An interactive environment for data partitioning and distribution. </title> <booktitle> In Proc. 5th Distributed Memory Comput. Conf., </booktitle> <month> April </month> <year> 1990. </year>
Reference-contexts: The techniques in this paper can be used to accomplish this [23]. We require the programmer to specify data distributions. Automatic deduction of this information for special programs has been investigated by Balasundaram and others <ref> [5] </ref>, by Gannon et al [12] on CEDAR-like architectures, by Hudak and Abraham [15] for sequentially iterated parallel loops, by Knobe et al [18] for SIMD machines, by Li and Chen [22] for index domain alignment and by Ramanujam and Sadayappan [30] who find communication-free partitioning of arrays in fully parallel
Reference: [6] <author> U. Banerjee. </author> <title> Dependence Analysis for Supercomputing. </title> <publisher> Kluwer Academic, </publisher> <year> 1988. </year>
Reference-contexts: A dependence vector has the property that its leading non-zero is always positive; a legal transformation must preserve this property for each dependence, since the source of the dependence must be executed before its destination. More information on data dependences and techniques of dependence analysis can be found in <ref> [6] </ref>. To understand the legality of transformations, consider A = 0 1 1 , a basis matrix, and D A = 0 1 , the dependence matrix. Each column of the dependence matrix represents the distance vector of a dependence in the loop nest.
Reference: [7] <author> U. Banerjee. </author> <title> Unimodular transformations of double loops. </title> <booktitle> In Proceedings of the Workshop on Advances in Languages and Compilers for Parallel Processing, </booktitle> <pages> pages 192-219, </pages> <month> August </month> <year> 1990. </year>
Reference-contexts: To reduce synchronization, transformations like loop interchange are carried out to move parallel loops outermost wherever possible <ref> [3, 7, 26, 37] </ref>. This approach does not perform any data management, so it is not suitable for generating good code on NUMA architectures. <p> The use of invertible matrices to model loop transformations is a generalization of the unimodular approach which can be used to model loop interchange, skewing and reversal <ref> [7, 37] </ref>. Invertible matrices include unimodular matrices as a special case, and permit us to model loop scaling as well. <p> The techniques in this paper can be used to partition work and data among the processors; techniques to enhance data reuse can be used to optimize uniprocessor cache performance. Our use of matrix techniques generalizes the unimodular matrix approach <ref> [7, 37] </ref>. Unimodular matrices were used by Kumar, Kulkarni and Basu [20] to eliminate outermost loop-carried dependences in generating code for distributed memory machines. In our work, we use invertible matrices, which include unimodular matrices as a special case.
Reference: [8] <institution> BBN Advanced Computers Inc. Butterfly GP1000 Switch Tutorial, </institution> <year> 1989. </year>
Reference-contexts: For example, in the BBN Butterfly, accesses to local memory take 0.6 microseconds while accesses to remote memory take about 6.6 microseconds <ref> [8] </ref>. Distributed memory machines like the Intel iPSC/i860 have even greater non-uniformity in memory access times because access to remote data must be orchestrated through the exchange of messages. <p> For block transfers, the startup time is about 8 microseconds, and after that, a byte is transferred every 0.31 microseconds <ref> [8] </ref>. Our compiler takes as input FORTRAN-77 programs with data distribution information, and it generates C code for each processor; this node program is compiled into native code using the Green Hills C compiler (Release 1.8.4).
Reference: [9] <author> D. Callahan and K. Kennedy. </author> <title> Compiling programs for distributed memory multiprocessors. </title> <journal> The Journal of Supercomputing, </journal> <volume> 2(2), </volume> <month> October </month> <year> 1988. </year>
Reference-contexts: This is accomplished by placing these conditional tests in front of the statement, and having all the processors execute all iterations `looking for work to do' <ref> [32, 9] </ref>. In simple programs, these conditional tests can be optimized away, but in general they must be executed at runtime, which is inefficient. <p> Summary and Related Work This paper is a contribution to the state of the art of compiling programs in languages like HPF-FORTRAN that permit user-defined data decomposition for parallel machines with a memory hierarchy, which is the goal of a number of projects including FORTRAN-D, Id Nouveau, Superb and Crystal <ref> [32, 9, 14, 19, 22, 27, 34, 39] </ref>. The emphasis in these projects has been on code generation mechanisms (such as the ownership rule discussed in Section 2) and on recognizing and exploiting special patterns of computation and communication such as reductions.
Reference: [10] <author> T. Coleman and C. Van Loan. </author> <title> Handbook for Matrix Computations. </title> <journal> SIAM Publication, Phil, </journal> <year> 1988. </year>
Reference-contexts: In Section 7, we discuss how code can be generated after loops have been restructured according to our methods. We present experimental results in Section 8 that demonstrate that our methods work well on programs of practical interest such as routines from the BLAS (Basic Linear Algebra Subroutines) library <ref> [10] </ref>. Finally, we discuss related work in Section 9. 2 Overview of NUMA Compilation In this section, we give an overview of our compilation strategy for NUMA architectures. <p> Consider the rank 2k update SYR2K from BLAS (Basic Linear Algebra Subroutines) <ref> [10] </ref>. The subroutine computes C = ffA T B + ffB T A + C. Suppose A and B are banded matrices with band width b, then C is symmetric and banded with band width 2b 1.
Reference: [11] <institution> Digital Equipment Corporation. Alpha Architecture Handbook, </institution> <year> 1992. </year>
Reference: [12] <author> D. Gannon, W. Jalby, and K. Gallivan. </author> <title> Strategies for cache and local memory management by global program transformaions. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 5 </volume> <pages> 587-616, </pages> <year> 1988. </year> <month> 12 </month>
Reference-contexts: For a machine in which processors have a first-level cache, there is the obvious possibility of selecting the padding to improve cache performance by incorporating results on blocking of nested loops <ref> [12, 33] </ref>. We leave this for future work. 6.3 Direction Vectors Direction vectors provide a conservative approximation when the distance of dependences can not be detected at compile time. <p> The techniques in this paper can be used to accomplish this [23]. We require the programmer to specify data distributions. Automatic deduction of this information for special programs has been investigated by Balasundaram and others [5], by Gannon et al <ref> [12] </ref> on CEDAR-like architectures, by Hudak and Abraham [15] for sequentially iterated parallel loops, by Knobe et al [18] for SIMD machines, by Li and Chen [22] for index domain alignment and by Ramanujam and Sadayappan [30] who find communication-free partitioning of arrays in fully parallel loops.
Reference: [13] <author> H. M. Gerndt. </author> <title> Automatic Parallelization for Distributed-Memory Multiprocessing Systems. </title> <type> PhD thesis, </type> <institution> Bonn University, </institution> <address> FRG, </address> <year> 1989. </year>
Reference-contexts: Inserting block transfers is similar to message vectorization in distributed memory machines or block-invalidates for software cache coherent schemes. These steps are routine <ref> [13, 26, 31] </ref>, and are omitted from this paper. 9 for i = l, u for i = d P e fl P + p, step P for i = max (l, p fl S), (a) unit step (b) task p for wrapped distribution (c) task p for blocked distribution for
Reference: [14] <author> S. Hiranandani, K. Kennedy, and C. Tseng. </author> <title> Compiler optimizations for FORTRAN-D on MIMD distributed-memory machines. </title> <type> Technical Report TR91-156, </type> <institution> Rice University, </institution> <month> April </month> <year> 1991. </year>
Reference-contexts: This approach does not perform any data management, so it is not suitable for generating good code on NUMA architectures. An alternative approach, implemented by the Id Nouveau [32] and FORTRAN-D systems <ref> [14] </ref>, among others, is to give the programmer control over how data structures are distributed across the processors. The compiler uses this data decomposition information to determine how to assign work to processors. <p> In many of these cases, loop restructuring can improve code quality, but no general approach to loop transformation has been available in this context <ref> [14] </ref>. In this paper, we present a systematic approach to loop restructuring for parallel machines with a memory hierarchy. As in the ownership approach, our starting point is a language like HPF-FORTRAN with user-specified data decomposition. <p> Most of the examples in this paper use a wrapped column distribution. Blocked column distribution is similar, except that a processor gets a contiguous set of columns. We also support so-called 2-D blocks in which rectangular subblocks of the array are distributed to the processors <ref> [14] </ref>, but for lack of space, we will not consider them any further in this paper. Data distributions can be specified precisely using a distribution function. <p> Summary and Related Work This paper is a contribution to the state of the art of compiling programs in languages like HPF-FORTRAN that permit user-defined data decomposition for parallel machines with a memory hierarchy, which is the goal of a number of projects including FORTRAN-D, Id Nouveau, Superb and Crystal <ref> [32, 9, 14, 19, 22, 27, 34, 39] </ref>. The emphasis in these projects has been on code generation mechanisms (such as the ownership rule discussed in Section 2) and on recognizing and exploiting special patterns of computation and communication such as reductions.
Reference: [15] <author> D. Hudak and S. Abraham. </author> <title> Compiler techniques for data partitioning of sequentially iterated parallel loops. </title> <booktitle> In Proc. ACM Int. Conf. Supercomputing, </booktitle> <month> June </month> <year> 1990. </year>
Reference-contexts: The techniques in this paper can be used to accomplish this [23]. We require the programmer to specify data distributions. Automatic deduction of this information for special programs has been investigated by Balasundaram and others [5], by Gannon et al [12] on CEDAR-like architectures, by Hudak and Abraham <ref> [15] </ref> for sequentially iterated parallel loops, by Knobe et al [18] for SIMD machines, by Li and Chen [22] for index domain alignment and by Ramanujam and Sadayappan [30] who find communication-free partitioning of arrays in fully parallel loops.
Reference: [16] <author> F. Irigoin and R. Triolet. </author> <title> Supernode partitioning. </title> <booktitle> In Proc. 15th Annual ACM Symposium on Principles of Programming Languages, </booktitle> <month> January </month> <year> 1988. </year>
Reference: [17] <institution> Kendall Square Research Corporation, </institution> <address> 170 Tracer Lane, Waltham, Ma 02154. </address> <note> Parallel Programming Manual, </note> <year> 1991. </year>
Reference-contexts: We have attempted to exploit locality by matching code to the data distribution across the machine. This is a static notion of locality, and must be differentiated from the dynamic locality that must be exploited on parallel machines with coherent caches <ref> [17] </ref>. On such machines, the key to high performance is data reuse, and the code must be restructured to allow reuse of cached data wherever possible. Restructuring techniques for doing this have been explored by Wolf and Lam [36]. Their approach is complementary to the one described here.
Reference: [18] <author> K. Knobe, J. Lukas, and G. Steele. </author> <title> Data optimiza tion: Allocation of arrays to reduce communication on SIMD machines. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 8 </volume> <pages> 102-118, </pages> <month> February </month> <year> 1990. </year>
Reference-contexts: We require the programmer to specify data distributions. Automatic deduction of this information for special programs has been investigated by Balasundaram and others [5], by Gannon et al [12] on CEDAR-like architectures, by Hudak and Abraham [15] for sequentially iterated parallel loops, by Knobe et al <ref> [18] </ref> for SIMD machines, by Li and Chen [22] for index domain alignment and by Ramanujam and Sadayappan [30] who find communication-free partitioning of arrays in fully parallel loops.
Reference: [19] <author> C. Koelbel and P. Mehrotra. </author> <title> Compiling global names pace parallel loops for distributed execution. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 2, </volume> <month> October </month> <year> 1991. </year>
Reference-contexts: Summary and Related Work This paper is a contribution to the state of the art of compiling programs in languages like HPF-FORTRAN that permit user-defined data decomposition for parallel machines with a memory hierarchy, which is the goal of a number of projects including FORTRAN-D, Id Nouveau, Superb and Crystal <ref> [32, 9, 14, 19, 22, 27, 34, 39] </ref>. The emphasis in these projects has been on code generation mechanisms (such as the ownership rule discussed in Section 2) and on recognizing and exploiting special patterns of computation and communication such as reductions.
Reference: [20] <author> K. G. Kumar, D. Kulkarni, and A. Basu. </author> <title> General ized unimodular loop transformations for distributed memory multiprocessors. </title> <type> Technical Report FG-TR-014, </type> <institution> Center for Development of Advanced Computing, </institution> <address> Bangalore, INDIA, </address> <month> January </month> <year> 1991. </year>
Reference-contexts: Our use of matrix techniques generalizes the unimodular matrix approach [7, 37]. Unimodular matrices were used by Kumar, Kulkarni and Basu <ref> [20] </ref> to eliminate outermost loop-carried dependences in generating code for distributed memory machines. In our work, we use invertible matrices, which include unimodular matrices as a special case. This lets us model loop scaling as well, which is important in the NUMA context.
Reference: [21] <author> L. Lamport. </author> <title> The parallel execution of do loops. </title> <booktitle> Com munications of the ACM, </booktitle> <pages> pages 83-93, </pages> <month> February </month> <year> 1974. </year>
Reference: [22] <author> J. Li and M. Chen. </author> <title> Index domain alignment: Minimiz ing cost of cross-referencing between distributed arrays. </title> <type> Technical report, </type> <institution> Yale University, </institution> <year> 1989. </year>
Reference-contexts: Summary and Related Work This paper is a contribution to the state of the art of compiling programs in languages like HPF-FORTRAN that permit user-defined data decomposition for parallel machines with a memory hierarchy, which is the goal of a number of projects including FORTRAN-D, Id Nouveau, Superb and Crystal <ref> [32, 9, 14, 19, 22, 27, 34, 39] </ref>. The emphasis in these projects has been on code generation mechanisms (such as the ownership rule discussed in Section 2) and on recognizing and exploiting special patterns of computation and communication such as reductions. <p> Automatic deduction of this information for special programs has been investigated by Balasundaram and others [5], by Gannon et al [12] on CEDAR-like architectures, by Hudak and Abraham [15] for sequentially iterated parallel loops, by Knobe et al [18] for SIMD machines, by Li and Chen <ref> [22] </ref> for index domain alignment and by Ramanujam and Sadayappan [30] who find communication-free partitioning of arrays in fully parallel loops. These efforts focus on deducing good data distributions for particular kinds of programs such as fully parallel loops, and no general solution to this problem is known.
Reference: [23] <author> W. Li and K. Pingali. </author> <title> Access normalization: loop re structuring for NUMA compilers. </title> <type> Technical Report 92-1278, </type> <institution> Department of Computer Science, Cornell University, </institution> <year> 1992. </year>
Reference-contexts: The algorithm described informally above is simple, but it is expensive to keep checking rows for independence. A more efficient algorithm is obtained by using a variation of computing the Hermite normal form <ref> [23] </ref>. A detailed understanding of this algorithm is not important for reading the rest of the paper, so we give an informal description of what it does. <p> Then, we pad this matrix using Algorithm LegalInvt to yield the final transformation. In this paper, we discuss only the case when dependences are represented by distances; it is straight-forward to extend these results to dependence directions <ref> [23] </ref>. 6.1 Generating a Legal Basis Algorithm LegalBasis , shown in Figure 5, takes a basis matrix and checks each row against the dependences. For example, consider the product of the first row and D A . <p> A direction vector can be (&lt; &gt; =) or (= &lt; fl), as long as the leading nonzero is positive. The algorithms in this paper can be extended to handle direction vectors. For lack of space, details can be found in the associated technical report <ref> [23] </ref>. 7 NUMA Code Generation Once the program has been transformed by access normalization, we must generate the code that will run on each processor. <p> Even on machines such as the Fujitsu FACOM that support scatter and gather operations, it is more efficient to use constant stride accesses wherever possible since address generation for vector elements is faster. The techniques in this paper can be used to accomplish this <ref> [23] </ref>. We require the programmer to specify data distributions.
Reference: [24] <author> W. Li and K. Pingali. </author> <title> A singular loop transformation framework based on non-singular matrices. </title> <type> Technical Report 92-1294, </type> <institution> Department of Computer Science, Cornell University, </institution> <month> July </month> <year> 1992. </year>
Reference-contexts: The algorithm for generating a restructured program starting from a loop nest and an invertible mapping is given in the technical report <ref> [24] </ref>.
Reference: [25] <author> L. Lu. </author> <title> A unified framework for systematic loop transfor mations. </title> <booktitle> In 3rd ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming, </booktitle> <pages> pages 28-38, </pages> <month> April </month> <year> 1991. </year>
Reference: [26] <author> S. P. Midkiff and D. A. Padua. </author> <title> Compiler algorithms for synchronization. </title> <journal> IEEE Transactions on computers, </journal> <volume> C-36:1485-1495, </volume> <month> December </month> <year> 1987. </year>
Reference-contexts: To reduce synchronization, transformations like loop interchange are carried out to move parallel loops outermost wherever possible <ref> [3, 7, 26, 37] </ref>. This approach does not perform any data management, so it is not suitable for generating good code on NUMA architectures. <p> Inserting block transfers is similar to message vectorization in distributed memory machines or block-invalidates for software cache coherent schemes. These steps are routine <ref> [13, 26, 31] </ref>, and are omitted from this paper. 9 for i = l, u for i = d P e fl P + p, step P for i = max (l, p fl S), (a) unit step (b) task p for wrapped distribution (c) task p for blocked distribution for
Reference: [27] <author> R. Mirchandaney, J. Saltz, R. Smith, D. Nicol, and K. Crowley. </author> <title> Principles of runtime support for parallel processors. </title> <booktitle> In Proc. of the 2nd Int. Conf. on Supercomputing, </booktitle> <month> July </month> <year> 1988. </year>
Reference-contexts: Summary and Related Work This paper is a contribution to the state of the art of compiling programs in languages like HPF-FORTRAN that permit user-defined data decomposition for parallel machines with a memory hierarchy, which is the goal of a number of projects including FORTRAN-D, Id Nouveau, Superb and Crystal <ref> [32, 9, 14, 19, 22, 27, 34, 39] </ref>. The emphasis in these projects has been on code generation mechanisms (such as the ownership rule discussed in Section 2) and on recognizing and exploiting special patterns of computation and communication such as reductions.
Reference: [28] <author> D. Padua and M. Wolfe. </author> <title> Advanced compiler optimiza tions for supercomputers. </title> <journal> Communications of ACM, </journal> <volume> 29(12) </volume> <pages> 1184-1201, </pages> <month> December </month> <year> 1986. </year>
Reference: [29] <author> A. Porterfield. </author> <title> Software Methords for Improvement of Cache Performance on Supercomputer Applications. </title> <type> PhD thesis, </type> <institution> Rice University, </institution> <month> May </month> <year> 1989. </year>
Reference: [30] <author> J. Ramanujam and P. Sadayappan. </author> <title> Compile-time tech niques for data distribution in distributed memory machines. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 2, </volume> <month> October </month> <year> 1991. </year>
Reference-contexts: special programs has been investigated by Balasundaram and others [5], by Gannon et al [12] on CEDAR-like architectures, by Hudak and Abraham [15] for sequentially iterated parallel loops, by Knobe et al [18] for SIMD machines, by Li and Chen [22] for index domain alignment and by Ramanujam and Sadayappan <ref> [30] </ref> who find communication-free partitioning of arrays in fully parallel loops. These efforts focus on deducing good data distributions for particular kinds of programs such as fully parallel loops, and no general solution to this problem is known.
Reference: [31] <author> A. Rogers. </author> <title> Compiling for Locality of Reference. </title> <type> PhD thesis, </type> <institution> Cornell University, </institution> <year> 1990. </year>
Reference-contexts: Inserting block transfers is similar to message vectorization in distributed memory machines or block-invalidates for software cache coherent schemes. These steps are routine <ref> [13, 26, 31] </ref>, and are omitted from this paper. 9 for i = l, u for i = d P e fl P + p, step P for i = max (l, p fl S), (a) unit step (b) task p for wrapped distribution (c) task p for blocked distribution for
Reference: [32] <author> A. Rogers and K. Pingali. </author> <title> Process decomposition through locality of reference. </title> <booktitle> In Proc. of the 1989 SIG-PLAN Conference on Programming Language Design and Implementation, </booktitle> <year> 1989. </year>
Reference-contexts: To reduce synchronization, transformations like loop interchange are carried out to move parallel loops outermost wherever possible [3, 7, 26, 37]. This approach does not perform any data management, so it is not suitable for generating good code on NUMA architectures. An alternative approach, implemented by the Id Nouveau <ref> [32] </ref> and FORTRAN-D systems [14], among others, is to give the programmer control over how data structures are distributed across the processors. The compiler uses this data decomposition information to determine how to assign work to processors. <p> This is accomplished by placing these conditional tests in front of the statement, and having all the processors execute all iterations `looking for work to do' <ref> [32, 9] </ref>. In simple programs, these conditional tests can be optimized away, but in general they must be executed at runtime, which is inefficient. <p> Summary and Related Work This paper is a contribution to the state of the art of compiling programs in languages like HPF-FORTRAN that permit user-defined data decomposition for parallel machines with a memory hierarchy, which is the goal of a number of projects including FORTRAN-D, Id Nouveau, Superb and Crystal <ref> [32, 9, 14, 19, 22, 27, 34, 39] </ref>. The emphasis in these projects has been on code generation mechanisms (such as the ownership rule discussed in Section 2) and on recognizing and exploiting special patterns of computation and communication such as reductions.
Reference: [33] <author> R. Schreiber and J. Dongarra. </author> <title> Automatic blocking of nested loops. </title> <type> Technical Report 90.38, </type> <institution> NASA RIACS, </institution> <month> May </month> <year> 1990. </year>
Reference-contexts: For a machine in which processors have a first-level cache, there is the obvious possibility of selecting the padding to improve cache performance by incorporating results on blocking of nested loops <ref> [12, 33] </ref>. We leave this for future work. 6.3 Direction Vectors Direction vectors provide a conservative approximation when the distance of dependences can not be detected at compile time.
Reference: [34] <author> P. Tseng. </author> <title> A Parallelizing Compiler For Distributed Memory Parallel Computers. </title> <type> PhD thesis, </type> <institution> Carnegie Mellon University, </institution> <year> 1989. </year>
Reference-contexts: Summary and Related Work This paper is a contribution to the state of the art of compiling programs in languages like HPF-FORTRAN that permit user-defined data decomposition for parallel machines with a memory hierarchy, which is the goal of a number of projects including FORTRAN-D, Id Nouveau, Superb and Crystal <ref> [32, 9, 14, 19, 22, 27, 34, 39] </ref>. The emphasis in these projects has been on code generation mechanisms (such as the ownership rule discussed in Section 2) and on recognizing and exploiting special patterns of computation and communication such as reductions.
Reference: [35] <author> D. Whitfield and M. L. Soffa. </author> <title> Automatic generation of global optimizers. </title> <booktitle> In Proc. of the SIGPLAN '91 Conf. on Programming Language Design and Implementation, SIGPLAN Notices, </booktitle> <month> June </month> <year> 1991. </year>
Reference: [36] <author> M. Wolf and M. Lam. </author> <title> A data locality optimizing al gorithm. </title> <booktitle> In Proc. ACM SIGPLAN 91 Conference on Programming Language Design and Implementation, </booktitle> <pages> pages 30-44, </pages> <month> June </month> <year> 1991. </year>
Reference-contexts: On such machines, the key to high performance is data reuse, and the code must be restructured to allow reuse of cached data wherever possible. Restructuring techniques for doing this have been explored by Wolf and Lam <ref> [36] </ref>. Their approach is complementary to the one described here. It is likely that scalable parallel architectures 11 will be organized as networks of processor-memory pairs in which processors have an on-chip cache and perhaps a second level cache between the processor and its local memory.
Reference: [37] <author> M. Wolf and M. Lam. </author> <title> A loop transformation theory and an algorithm to maximize parallelism. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <month> October </month> <year> 1991. </year>
Reference-contexts: To reduce synchronization, transformations like loop interchange are carried out to move parallel loops outermost wherever possible <ref> [3, 7, 26, 37] </ref>. This approach does not perform any data management, so it is not suitable for generating good code on NUMA architectures. <p> The use of invertible matrices to model loop transformations is a generalization of the unimodular approach which can be used to model loop interchange, skewing and reversal <ref> [7, 37] </ref>. Invertible matrices include unimodular matrices as a special case, and permit us to model loop scaling as well. <p> The techniques in this paper can be used to partition work and data among the processors; techniques to enhance data reuse can be used to optimize uniprocessor cache performance. Our use of matrix techniques generalizes the unimodular matrix approach <ref> [7, 37] </ref>. Unimodular matrices were used by Kumar, Kulkarni and Basu [20] to eliminate outermost loop-carried dependences in generating code for distributed memory machines. In our work, we use invertible matrices, which include unimodular matrices as a special case.
Reference: [38] <author> M. Wolfe. </author> <title> Optimizing Supercompilers for Supercom puters. </title> <publisher> Pitman Publishing, </publisher> <address> London, </address> <year> 1989. </year>
Reference-contexts: In general, it is easier to work with invertible matrices since there are fewer constraints to be satisfied in generating invertible matrices, as opposed to unimodular matrices. There are a number of other loop transformations like distribution, jamming and alignment that are useful in generating code for parallel machines <ref> [38] </ref>. It would be useful to extend the matrix framework to incorporate these transformations. Related work on loop transformations can be found in [4, 3, 12, 16, 21, 25, 28, 29, 33, 35, 37, 38].
Reference: [39] <author> H. Zima and B. Chapman. </author> <title> Supercompilers for Parallel and Vector Computers. </title> <publisher> ACM Press Frontier Series, </publisher> <address> New York, New York, </address> <year> 1990. </year> <month> 13 </month>
Reference-contexts: Although this strategy takes data mappings into account, it can generate inefficient code, in which all processors execute all iterations `looking for work to do' if the structure of the loop nest does not match the data distribution <ref> [39] </ref>. In many of these cases, loop restructuring can improve code quality, but no general approach to loop transformation has been available in this context [14]. In this paper, we present a systematic approach to loop restructuring for parallel machines with a memory hierarchy. <p> Summary and Related Work This paper is a contribution to the state of the art of compiling programs in languages like HPF-FORTRAN that permit user-defined data decomposition for parallel machines with a memory hierarchy, which is the goal of a number of projects including FORTRAN-D, Id Nouveau, Superb and Crystal <ref> [32, 9, 14, 19, 22, 27, 34, 39] </ref>. The emphasis in these projects has been on code generation mechanisms (such as the ownership rule discussed in Section 2) and on recognizing and exploiting special patterns of computation and communication such as reductions.
References-found: 39

