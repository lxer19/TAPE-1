URL: http://www.daimi.aau.dk/~ulle/thesis.ps.gz
Refering-URL: http://www.daimi.aau.dk/~ulle/thesis.html
Root-URL: http://www.daimi.aau.dk
Title: Compiler-directed Memory Coherence for Parallel Computing M.Sc. Project Report an algorithm is developed for compiler-directed
Author: Ulrik Skyt 
Note: In this work,  
Date: 15 September 1998  
Affiliation: University of Edinburgh Department of Computer Science  
Abstract: Parallel programs run in a Virtual Shared Memory system are dependent on caching to make efficient use of the shared memory, and this raises the issue of keeping the memory coherent. Responsibility for ensuring coherence can be placed on hardware, on a run-time system, both of which must rely on dynamic communication or on the compiler, which must then make conservative compile-time decisions. A hybrid memory coherence model is one where the underlying system is responsible for coherence, but the compiler is left the opportunity to optimise coherence communication when the need can be statically determined. The advantage is that in this model static information can be exploited, but the compiler need not make conservative decisions to ensure coherence. Limited experimental results show very good performance in terms of prevented write-misses, and with few non-implemented improvements in place, the overhead of doing distributed invalidation is low as well. The generality of these results are discussed, and some suggestions are made for future work. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Alfred V. Aho, Ravi Sethi, and Jeffrey D. Ullman. </author> <booktitle> Compilers Principles, Techniques and Tools. </booktitle> <publisher> Addison-Wesley Publishing Company, </publisher> <year> 1986. </year>
Reference-contexts: The concepts introduced will be essential to be able to refer to high-level concepts in an unambiguous, compact and well-defined way when explaining the algorithms to come. Many texts on the subject include a variant of these definitions, for example <ref> [10, 18, 1] </ref>.
Reference: [2] <author> D. Barthou, A. Cohen, and Jean-Francois Collard. </author> <title> Maximal static expansion. </title> <booktitle> In ACM Symposium on Principles and Practice of Programming Languages (PoPL'98), </booktitle> <address> San Diego, CA, </address> <pages> pages 98-106. </pages> <publisher> ACM Press, </publisher> <address> New York, USA, </address> <month> January </month> <year> 1998. </year>
Reference-contexts: GSA may be relevant to consider for future DI algorithms or other future analyses in MARS. Collard [7] compares different variations of single assignment form applied to arrays and formally defines Array SA and Array SSA. The paper motivates Array (S)SA and discusses Array expansion <ref> [2] </ref> (as cited by [7]) and instance-wise reaching definitions analyses. Array (S)SA is to be understood as SA on the basis of instances of array entries, and that is what distinguishes this paper from most other work on SSA.
Reference: [3] <author> Francois Bodin, Zbigniew Chamski, Michael F. P. O'Boyle, and Yves Robin. </author> <title> MARS Documentation: Abstract Program Representation for Loop Transformations Version 1.1, </title> <year> 1995. </year>
Reference-contexts: Section 4.1 will start by giving a non-technical explanation of how loops and array accesses are represented in MARS. For technical details, see <ref> [3] </ref>. Section 4.2 will then introduce the Omega library, and explain how it can be used to represent and manipulate array sections.
Reference: [4] <author> Francois Bodin, D. Gannon, and J. G. S. Srinivas. Sage++: </author> <title> A class library for building fortran and c++ restructuring tools. </title> <booktitle> In Second Object Oriented Numerics Conference, </booktitle> <address> Oregon, USA. </address> <month> May </month> <year> 1994. </year>
Reference-contexts: The overall structure of the compiler can be seen in figure 2.1. The external interface, parsing and unparsing, is done by the Sage++ library <ref> [4] </ref>, incorporating submodules Octave and Polaris. At the heart of the compiler is the Augmented Linear Algebraic Framework. The Parallelisation Strategy, explained below, determines the actions taken in the compiler, based on results from the Analysis module and carried out by the Transformation ToolBox.
Reference: [5] <author> Francois Bodin and Michael F. P. O'Boyle. </author> <title> A compiler strategy for shared virtual memory. </title> <editor> In Boleslaw K. Szymanski and Balaram Sinharoy, editors, </editor> <booktitle> The Third Workshop on Languages, Compilers and Runtime Systems for Scalable Computing, chapter 5. </booktitle> <publisher> Kluwer Press, </publisher> <address> New York, </address> <month> May </month> <year> 1995. </year>
Reference-contexts: If it is done poorly, it can potentially thwart cache reuse and not benefit coherence traffic. 1.2 Goals of this Pro ject This project is centered around the experimental auto-parallelising compiler MARS <ref> [5, 21] </ref>, that compiles from standard sequential Fortran 77 to a variety of parallel Fortran dialects. An integral part of its compilation strategy is to minimise non-local accesses and maximise reuse of same, but until recently it did not otherwise address minimisation of coherence traffic. <p> Before we start examining the actual strategy and algorithm we will need to set the context of a compiler framework. In this project, the MARS compiler <ref> [5, 21] </ref> has been used. After giving a brief overview of the compiler's structure, section 2.1 will sketch the compilation and parallelisation strategy used, explain where our new Distributed Invalidation compiler phase will fit in, and sum up the conclusions we need. <p> In the following, the different phases of the compiler will be explained. Because of the limited amount of literature on this particular subject and the scope of this section, the following is in effect a summary of <ref> [5] </ref> with focus on what is relevant here. The goal of parallelisation is to minimise total execution time using multiple processors. To do so, different contributions to execution time are separately estimated and optimised in a fixed order, as dictated by a set of heuristics that constitute the parallelisation strategy.
Reference: [6] <author> Jong-Deok Choi, Ron K. Cytron, and Jeanne Ferrante. </author> <title> Automatic construction of sparse data flow evaluation graphs. </title> <booktitle> In Conference Record of the 18th Annual ACM Symposium on Principles of Programming Languages (POPL'91), </booktitle> <address> Orlando, Florida, USA, </address> <pages> pages 55-66. </pages> <publisher> ACM Press, </publisher> <address> New York, USA, </address> <month> January </month> <year> 1991. </year>
Reference-contexts: Computing OE-nodes is what potentially takes too long time in the current SSA algorithm since worst case behaviour is quadratic in the size of the CFG [10]. Choi et al. <ref> [6] </ref> suggested using SEG's to efficiently solve monotone data flow problems. They are essentially pruned copies of the CFG that only contain nodes relevant for the particular problem to be solved, so typically many successive SEG's are constructed. They are as effcient to calculate and use SSA but more general. <p> Or maybe the compiler could somehow statically determine directly what memory pages should be invalidated, instead of feeding information based on array entry addresses. The distributed invalidation algorithm is based on the novel Static Previous Access form, but I recently realised that the a Sparse Evaluation Graph <ref> [6] </ref>, which is more general, could have been used instead.
Reference: [7] <author> Jean-Francois Collard. </author> <title> Array SSA: Why? how? how much? In Peter Fritzson, editor, </title> <booktitle> Proceedings of the Seventh International Workshop on Compilers for Parallel Computers (CPC'98), </booktitle> <address> Linkoping, Sweden, </address> <month> June </month> <year> 1998. </year>
Reference-contexts: If the condition is evaluated dynamically it says exactly which other parameter is the resulting value of the "function call". But the condition provides extra information useful in static analyses as well. GSA may be relevant to consider for future DI algorithms or other future analyses in MARS. Collard <ref> [7] </ref> compares different variations of single assignment form applied to arrays and formally defines Array SA and Array SSA. The paper motivates Array (S)SA and discusses Array expansion [2] (as cited by [7]) and instance-wise reaching definitions analyses. <p> GSA may be relevant to consider for future DI algorithms or other future analyses in MARS. Collard <ref> [7] </ref> compares different variations of single assignment form applied to arrays and formally defines Array SA and Array SSA. The paper motivates Array (S)SA and discusses Array expansion [2] (as cited by [7]) and instance-wise reaching definitions analyses. Array (S)SA is to be understood as SA on the basis of instances of array entries, and that is what distinguishes this paper from most other work on SSA.
Reference: [8] <author> Thomas H. Cormen, Charles E. Leiserson, and Ronald L. Rivest. </author> <title> Introduction to Algorithms. </title> <publisher> MIT Press and McGraw-Hill Book Company, </publisher> <year> 1989. </year>
Reference-contexts: complex, but it turns out that the amortised running time is O (mff (m; n)), if m is the number of edges in the control flow graph and n is the number of nodes, and ff (; ) is a functional inverse of the very 1 fast growing Ackermann's function <ref> [8] </ref>. In [28] and [18] this running time is partially proved but they refer to [27] for the core of the proof. Before getting into how the eval-link-update principle is applied here, let us first outline the algorithm. Overall, it consists of four parts: 1.
Reference: [9] <author> Ron K. Cytron and Jeanne Ferrante. </author> <title> Efficiently computing OE-nodes on-the-fly. </title> <editor> In Utpal Banerjee, David Gelernter, Alex Nicolau, and David A. Padua, editors, </editor> <booktitle> LNCS 768: Proceedings of the 6th International Workshop on Languages and Compilers for Parallel Computing, </booktitle> <address> Portland, Oregon, USA, </address> <pages> pages 461-476. </pages> <publisher> Springer-Verlag, </publisher> <month> August </month> <year> 1993. </year>
Reference-contexts: For each node X, DF (X) will include all nodes in the lower row to the right of node X, which is of magnitude (N ). On realistic well-structured programs, DFs are normally quite small and are in consequence fairly efficient to calculate <ref> [9, 10] </ref>. 3.2 A Dominator Tree Algorithm In this section an algorithm is presented that computes the dominator tree of a control flow graph, which is a prerequisite for computing the dominance frontier and later static single assignment form, as will be explained later. <p> They are as effcient to calculate and use SSA but more general. They show how to change the standard SSA algotithm to contruct pruned SSA form which contains no dead OE-functions. Cytron & Ferrante <ref> [9] </ref> show a way to compute OE-nodes in almost linear worst case asymptotic time. Unfortunately they report that although better on some artificially generated examples, 3.6. RELATED WORK 32 their realistic test examples' median performance was degraded with a factor 3 compared to the usual algorithm [10].
Reference: [10] <author> Ron K. Cytron, Jeanne Ferrante, Barry K. Rosen, Mark N. Wegman, and F. Kenneth Zadeck. </author> <title> Efficiently computing static single assignment form and the control dependence graph. </title> <journal> ACM Transactions on Programming Language and Systems, </journal> <volume> 13(4) </volume> <pages> 451-490, </pages> <month> October </month> <year> 1991. </year> <note> BIBLIOGRAPHY 54 </note>
Reference-contexts: The concepts introduced will be essential to be able to refer to high-level concepts in an unambiguous, compact and well-defined way when explaining the algorithms to come. Many texts on the subject include a variant of these definitions, for example <ref> [10, 18, 1] </ref>. <p> For each node X, DF (X) will include all nodes in the lower row to the right of node X, which is of magnitude (N ). On realistic well-structured programs, DFs are normally quite small and are in consequence fairly efficient to calculate <ref> [9, 10] </ref>. 3.2 A Dominator Tree Algorithm In this section an algorithm is presented that computes the dominator tree of a control flow graph, which is a prerequisite for computing the dominance frontier and later static single assignment form, as will be explained later. <p> Material in this section is due to Cytron et al. <ref> [10] </ref>. <p> Material in this section is due to Cytron et al. <ref> [10] </ref>. By definition, a program is in single assignment form if every variable is the target of exactly one assignment [10]. <p> Material in this section is due to Cytron et al. <ref> [10] </ref>. By definition, a program is in single assignment form if every variable is the target of exactly one assignment [10]. Programs can be translated into single assignment form statically by introducing new pseudo-assignments in places making every use of the variable have a unique previous assignment, and afterwards renaming all occurrences of the variable so that each new variable name is assigned only once. <p> Computing OE-nodes is what potentially takes too long time in the current SSA algorithm since worst case behaviour is quadratic in the size of the CFG <ref> [10] </ref>. Choi et al. [6] suggested using SEG's to efficiently solve monotone data flow problems. They are essentially pruned copies of the CFG that only contain nodes relevant for the particular problem to be solved, so typically many successive SEG's are constructed. <p> Cytron & Ferrante [9] show a way to compute OE-nodes in almost linear worst case asymptotic time. Unfortunately they report that although better on some artificially generated examples, 3.6. RELATED WORK 32 their realistic test examples' median performance was degraded with a factor 3 compared to the usual algorithm <ref> [10] </ref>. Tu and Padua [29, 30] describe how to efficiently construct and use Gated SSA (GSA). In GSA OE-functions are replaced by -functions if in the head of a loop, fl-functions after conditionals and j-functions after loops. The three new functions each take an additional condition as parameter. <p> It is shown, among other things, how to use the PST and some theory developed in <ref> [10] </ref> to speed up calculation of OE-nodes for an SSA form, using the PST to facilitate divide-and-conquer style computation to any other SSA algorithm.
Reference: [11] <author> Ervan Darnell and Ken Kennedy. </author> <title> Cache coherence using local knowledge. </title> <booktitle> In Proceedings of the Supercomputing Conference, </booktitle> <address> Portland, Oregon, USA, </address> <pages> pages 720-729. </pages> <publisher> IEEE, Los Alamitos, </publisher> <address> California, USA, </address> <month> November </month> <year> 1993. </year>
Reference-contexts: Only on top of these frameworks has distributed invalidation been implemented. New algorithms should be easy to devise and implement with the frameworks in place, and there should be short way from idea to implementation. 1.3 Related Work Darnell et al. <ref> [12, 11] </ref> consider pure compiler-directed memory coherence (which places the entire burden of maintaining coherence on the compiler) for fork/join parallelism based on the write-update model. Having undivided responsibility for coherence, the compiler is forced to make conservative decisions, and therefore unnecessary read-misses are incurred.
Reference: [12] <author> Ervan Darnell, John M. Mellor-Crummery, and Ken Kennedy. </author> <title> Automatic software cache coherence through vectorization. </title> <booktitle> In Proceedings of the 1992 International Conference on Supercomputing, </booktitle> <address> Washington D.C., </address> <pages> pages 129-138. </pages> <publisher> ACM Press, </publisher> <address> New York, USA, </address> <month> July </month> <year> 1992. </year>
Reference-contexts: Only on top of these frameworks has distributed invalidation been implemented. New algorithms should be easy to devise and implement with the frameworks in place, and there should be short way from idea to implementation. 1.3 Related Work Darnell et al. <ref> [12, 11] </ref> consider pure compiler-directed memory coherence (which places the entire burden of maintaining coherence on the compiler) for fork/join parallelism based on the write-update model. Having undivided responsibility for coherence, the compiler is forced to make conservative decisions, and therefore unnecessary read-misses are incurred.
Reference: [13] <author> Rupert W. Ford, Andrew P. Nisbet, and Mark J. Bull. </author> <title> User-level vsm optimisation and its application. </title> <booktitle> In Proceedings of PARA'95. </booktitle> <publisher> Springer-Verlag, </publisher> <year> 1995. </year>
Reference-contexts: This contrasts an alternative model, fork/join, where processes are "forked" before each parallel loop-nest and "joined" immediately after. The resulting program is to be run in a Virtual Shared Memory (VSM) system <ref> [13] </ref> that creates the abstraction that, although physical memory is distributed among the processors, they all share the same virtual memory space.
Reference: [14] <author> Dov Harel. </author> <title> A linear time algorithm for finding dominators in flow graphs and related problems. </title> <booktitle> In Proceedings of the 17th Annual ACM Symposium on Theory of Computing, </booktitle> <address> Providence, Rhode Island, USA, </address> <pages> pages 185-194. </pages> <publisher> ACM Press, </publisher> <address> New York, USA, </address> <month> May </month> <year> 1985. </year>
Reference-contexts: What is still left to explain is how to summarise reads and writes in the three required ways. This will be explained in the next chapter, but before then the next section will present some related work on control flow algorithms. 3.6 Related Work Harel <ref> [14] </ref> shows how to solve a special case of the eval-link-update problem in linear time. A method is then outlined of how to apply this to find immediate dominators (i.e. dominator trees).
Reference: [15] <author> M. Hill, J. Larus, and S. Reinhardt. </author> <title> Cooperative shared memory: Software and hardware for scalable multiprocessors. </title> <journal> ACM Trans. on Comp. Sys., </journal> <volume> 11(4) </volume> <pages> 300-318, </pages> <month> November </month> <year> 1993. </year>
Reference-contexts: Having undivided responsibility for coherence, the compiler is forced to make conservative decisions, and therefore unnecessary read-misses are incurred. Using the fork/join model implies that each parallel loop is scheduled separately at run-time, and hence no scheduling information is available statically. Using a hybrid technique, CICO <ref> [15] </ref> makes it possible for a programmer or a tool to annotate hints to the underlying system as to what should be invalidated, but the underlying system is still completely responsible for the correctness of the actions taken. 1.4 Outline of the Thesis Chapter 2 will present the distributed invalidation algorithm
Reference: [16] <author> Richard Johnson, David Pearson, and Keshav Pingali. </author> <title> The program structure tree: Computing control regions in linear time. </title> <booktitle> In Proceedings of the ACM SIGPLAN '94 Conference on Programming Languages Design and Implementation (PLDI'94), </booktitle> <address> Orlando, Florida, USA, </address> <pages> pages 171-185. </pages> <publisher> ACM Press, </publisher> <month> June </month> <year> 1994. </year>
Reference-contexts: Array (S)SA is to be understood as SA on the basis of instances of array entries, and that is what distinguishes this paper from most other work on SSA. Johnson et al. <ref> [16] </ref> use Single Entry Single Exit (SESE) regions of a Control Flow Graph (CFG) to build a hierarchical Program Structure Tree (PST). SESE regions can relate in many ways, but canonical SESE regions are either disjoint or nested.
Reference: [17] <author> Wayne Kelly, Vadim Maslow, William Pugh, Evan Rosser, Tatiana Shpeisman, and David Wonnacott. </author> <title> The Omega Library Version 1.1.0 Interface Guide. </title> <note> http://www.cs.umd.edu/projects/omega, November 1996. </note>
Reference-contexts: 3 7 7 7 2 i k 5 6 6 6 4 1 n 100 7 7 7 5 A set of access functions Array section accessed a @ 4 0 1 0 3 2 i k 5 + 0 A = k 4.2 The Omega Library The Omega Library <ref> [17] </ref> is a set of C++ classes for manipulating integer tuple relations and sets. Although it uses the same data-structure (the "Relation") to represent both sets and relations, some operations can be done only on relations, others only on sets. <p> Presburger formulas are those that can be constructed by combining affine constraints on integer variables with the logical operations :, ^ and _, and the quantifiers 8 and 9 <ref> [17] </ref>. They can be expressed as trees, and indeed, that 4.3. TRANSLATING FROM MARS TO OMEGA 35 is the way Omega does it.
Reference: [18] <author> Thomas Lengauer and Robert Endre Tarjan. </author> <title> A fast algorithm for finding dominators in a flowgraph. </title> <journal> In ACM Transactions on Programming Languages & Systems, </journal> <volume> volume 1, </volume> <pages> pages 121-141. </pages> <publisher> ACM Press, </publisher> <year> 1979. </year>
Reference-contexts: The concepts introduced will be essential to be able to refer to high-level concepts in an unambiguous, compact and well-defined way when explaining the algorithms to come. Many texts on the subject include a variant of these definitions, for example <ref> [10, 18, 1] </ref>. <p> It is based on formulating the problem as an instance of the general eval-link-update problem for a balanced tree, on which path-compression can be applied to speed up execution time. The algorithm is due to Lengauer and Tarjan <ref> [18] </ref>. Details on application of path-compression on balanced trees can be found in [28]. This section is essentially a boiled-down version of those two papers. <p> In [28] and <ref> [18] </ref> this running time is partially proved but they refer to [27] for the core of the proof. Before getting into how the eval-link-update principle is applied here, let us first outline the algorithm. Overall, it consists of four parts: 1. Depth first numbering of nodes. 2. Finding semi-dominators. <p> So r is the node with minimal semi [r] also satisfying sdom (v) + fl since r is not a root, it has been processed, so semi [r] = sdom (r). Now, a theorem proved in <ref> [18] </ref> states the following. 3.2.
Reference: [19] <author> Sungdo Moon, Byoungro So, and Mary W. Hall. </author> <title> A case for combining compile-time and run-time parallelization. </title> <booktitle> In LCR98: Fourth Workshop on Languages, Compilers, and Run-time Systems for Scalable Computers, </booktitle> <month> May </month> <year> 1998. </year>
Reference-contexts: EVALUATION AND FUTURE WORK 52 to fewer write-misses prevented. When a target architecture for the compiler with support for distributed invalidation becomes available such examples should be investigated. Recent work <ref> [19] </ref> has shown that predicated array data-flow analysis can help dynamically improve the detected available parallelism in programs.
Reference: [20] <author> Michael F. P. O'Boyle. </author> <title> A data partitioning algorithm for distributed memory compilation. </title> <editor> In C. Halatsis, D. Maritsas, G. Philokyprov, and S. Theodoridis, editors, </editor> <booktitle> 6th International Parallel Architectures and Languages Europe Conference (PARLE'94), </booktitle> <address> Athens, Greece, </address> <pages> pages 62-72. </pages> <publisher> Springer-Verlag, </publisher> <address> Berlin, </address> <month> July </month> <year> 1994. </year>
Reference-contexts: The combination of global alignment and global data partitioning is in done in a way so that arrays of the same dimension have corresponding elements mapped to the same processor. This fact can exploited to reduce communication. For more details on the data partitioning, see <ref> [20] </ref>. The Post-Partitioning stage is divided into a number of phases that optimise aggressively given the global trade-offs, but while trying to minimise the possible introduced overhead of par-allelisation, new trade-offs must be made.
Reference: [21] <author> Michael F. P. O'Boyle. </author> <title> MARS: A distributed memory approach to shared memory compilation. </title> <booktitle> In LCR98: Fourth Workshop on Languages, Compilers, and Run-time Systems for Scalable Computers, </booktitle> <month> May </month> <year> 1998. </year>
Reference-contexts: If it is done poorly, it can potentially thwart cache reuse and not benefit coherence traffic. 1.2 Goals of this Pro ject This project is centered around the experimental auto-parallelising compiler MARS <ref> [5, 21] </ref>, that compiles from standard sequential Fortran 77 to a variety of parallel Fortran dialects. An integral part of its compilation strategy is to minimise non-local accesses and maximise reuse of same, but until recently it did not otherwise address minimisation of coherence traffic. <p> Before we start examining the actual strategy and algorithm we will need to set the context of a compiler framework. In this project, the MARS compiler <ref> [5, 21] </ref> has been used. After giving a brief overview of the compiler's structure, section 2.1 will sketch the compilation and parallelisation strategy used, explain where our new Distributed Invalidation compiler phase will fit in, and sum up the conclusions we need.
Reference: [22] <author> Michael F. P. O'Boyle and Francois Bodin. </author> <title> Compiler reduction of synchronisation in shared virtual memory systems. </title> <booktitle> In Conference Proceedings of the 1995 International Conference on Supercomputing, Barcelona, Spain, </booktitle> <pages> pages 318-327. </pages> <publisher> ACM Press, </publisher> <address> New York, USA, </address> <month> July </month> <year> 1995. </year> <note> BIBLIOGRAPHY 55 </note>
Reference-contexts: That is, writes are always scheduled to be executed at the owner of the memory being written. In the Synchronisation Placement phase, synchronisation points for local data dependences are removed, and the rest are placed to minimise the number executed at run-time. For all the details, see <ref> [23, 22] </ref>. The Hierarchical Locality Optimisation phase addresses optimisation of data reuse in the "shared memory cache" and the "local memory cache" 1 via a number of transformations. The Loop Optimisation phase rearranges loops and their bounds to minimise overhead.
Reference: [23] <author> Michael F. P. O'Boyle, Lionel Kervella, and Francois Bodin. </author> <title> Synchronization minimization in a SPMD execution model. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 29 </volume> <pages> 196-210, </pages> <month> September </month> <year> 1995. </year>
Reference-contexts: That is, writes are always scheduled to be executed at the owner of the memory being written. In the Synchronisation Placement phase, synchronisation points for local data dependences are removed, and the rest are placed to minimise the number executed at run-time. For all the details, see <ref> [23, 22] </ref>. The Hierarchical Locality Optimisation phase addresses optimisation of data reuse in the "shared memory cache" and the "local memory cache" 1 via a number of transformations. The Loop Optimisation phase rearranges loops and their bounds to minimise overhead.
Reference: [24] <author> Michael F. P. O'Boyle, Andrew P. Nisbeth, and Rupert W. Ford. </author> <title> A compiler algorithm to reduce invalidation latency in virtual shared memory systems. </title> <booktitle> In Proceedings of Parallel Architectures and Compilation Techniques, </booktitle> <pages> pages 248-257. </pages> <publisher> IEEE Press, </publisher> <month> October </month> <year> 1996. </year>
Reference-contexts: Next section will give some details of the program representation used in MARS, and pin down what structures are provided that are of help for our work. 2.2 What to set Invalid and Exclusive The following basically explains a slightly modified version of the framework from <ref> [24] </ref>. Given an array assignment, let W denote the array section globally written at this write, that is, it includes every array entry which at some execution of the assignment gets written.
Reference: [25] <author> Vugranam C. Sreedhar and Guang R. Gao. </author> <title> A linear time algorithm for placing OE-nodes. </title> <booktitle> In Proceedings of the ACM SIGPLAN Workshop on Intermediate Representations (IR'95), </booktitle> <address> San Francisco, </address> <pages> pages 1-12. </pages> <publisher> ACM Press, </publisher> <address> New York, USA, </address> <month> January </month> <year> 1995. </year>
Reference-contexts: A DJ-graph is a dominator tree augmented with join edges that potentially lead to merge points in the corresponding flow graph. They reference another paper <ref> [25] </ref> which gives a linear time and space algorithm for computing OE-nodes for an arbritary Sparse data flow Evaluation Graph (SEG). Computing OE-nodes is what potentially takes too long time in the current SSA algorithm since worst case behaviour is quadratic in the size of the CFG [10].
Reference: [26] <author> Vugranam C. Sreedhar, Guang R. Gao, and Yong-fong Lee. </author> <title> Incremental computation of dominator trees. </title> <booktitle> In ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages. </booktitle> <publisher> ACM Press, </publisher> <address> New York, USA, </address> <month> January </month> <year> 1995. </year>
Reference-contexts: A proof of the algorithm is based, among other things, on update operations being "permissible" with respect to a partitioning of nodes into "microsets" that satisfy a number of properties. Sreedhar et al. <ref> [26] </ref> show how to incrementally maintain efficiently a DJ-graph for an arbitrary control flow graph when edges of the flow graph are added and deleted. A DJ-graph is a dominator tree augmented with join edges that potentially lead to merge points in the corresponding flow graph.
Reference: [27] <author> Robert Endre Tarjan. </author> <title> Efficiency of a good but not linear set union algorithm. </title> <journal> Journal of the Association for Computing Machinery, </journal> <volume> 22(2) </volume> <pages> 215-225, </pages> <month> April </month> <year> 1975. </year>
Reference-contexts: In [28] and [18] this running time is partially proved but they refer to <ref> [27] </ref> for the core of the proof. Before getting into how the eval-link-update principle is applied here, let us first outline the algorithm. Overall, it consists of four parts: 1. Depth first numbering of nodes. 2. Finding semi-dominators.
Reference: [28] <author> Robert Endre Tarjan. </author> <title> Applications of path compression on balanced trees. </title> <journal> Journal of the Association for Computing Machinery, </journal> <volume> 26(4) </volume> <pages> 690-715, </pages> <month> October </month> <year> 1979. </year>
Reference-contexts: The algorithm is due to Lengauer and Tarjan [18]. Details on application of path-compression on balanced trees can be found in <ref> [28] </ref>. This section is essentially a boiled-down version of those two papers. <p> In <ref> [28] </ref> and [18] this running time is partially proved but they refer to [27] for the core of the proof. Before getting into how the eval-link-update principle is applied here, let us first outline the algorithm. Overall, it consists of four parts: 1. Depth first numbering of nodes. 2. <p> This is done using the fact that for every subtree, rooted in r, of a tree, its size, size [r] is known. Code for the Link function can be seen in figure 3.7. For further details, see <ref> [28] </ref>. The code for step 2 (and 3) can be seen in figure 3.6. In this section, although the eval-link-update principle is used, there is no Update function. This is because this otherwise non-recursive function is simply "in-lined" in the Link function.
Reference: [29] <author> Peng Tu and David A. Padua. </author> <title> Efficient building and placing of gating functions. </title> <booktitle> In Proceedings of ACM SIGPLAN '95 Conference on Programming Language Design and Implementation. </booktitle> <publisher> ACM Press, </publisher> <address> New York, </address> <month> June </month> <year> 1995. </year>
Reference-contexts: Unfortunately they report that although better on some artificially generated examples, 3.6. RELATED WORK 32 their realistic test examples' median performance was degraded with a factor 3 compared to the usual algorithm [10]. Tu and Padua <ref> [29, 30] </ref> describe how to efficiently construct and use Gated SSA (GSA). In GSA OE-functions are replaced by -functions if in the head of a loop, fl-functions after conditionals and j-functions after loops. The three new functions each take an additional condition as parameter.
Reference: [30] <author> Peng Tu and David A. Padua. </author> <title> Gated SSA-based demand-driven symbolic analysis for par-allelizing compilers. </title> <booktitle> In Proceedings of International Conference on Supercomputing, </booktitle> <pages> pages 414-423. </pages> <publisher> ACM Press, </publisher> <address> New York, </address> <month> July </month> <year> 1995. </year> <title> APPENDIX A. CODE FOR FINDING SSA AND SPA FORMS 56 </title>
Reference-contexts: Unfortunately they report that although better on some artificially generated examples, 3.6. RELATED WORK 32 their realistic test examples' median performance was degraded with a factor 3 compared to the usual algorithm [10]. Tu and Padua <ref> [29, 30] </ref> describe how to efficiently construct and use Gated SSA (GSA). In GSA OE-functions are replaced by -functions if in the head of a loop, fl-functions after conditionals and j-functions after loops. The three new functions each take an additional condition as parameter.
References-found: 30

