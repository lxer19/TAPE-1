URL: ftp://ftp.cs.wisc.edu/tech-reports/reports/93/tr1176.ps.Z
Refering-URL: 
Root-URL: 
Title: A Gauss-Newton Method for Convex Composite Optimization  
Author: J.V. Burke M.C. Ferris 
Address: Seattle, Washington 98195  Wisconsin, Madison, Wisconsin 53706  
Affiliation: Department of Mathematics, GN-50, University of Washington,  Computer Sciences Department, University of  
Note: This material is based on research supported by National Science Foundation Grants CCR-9157632 and DMS-9102059 and Air Force Office of Scientific Research Grant AFOSR-89-0410  
Date: August 1993  
Abstract: An extension of the Gauss-Newton method for nonlinear equations to convex composite optimization is described and analyzed. Local quadratic convergence is established for the minimization of h ffi F under two conditions, namely h has a set of weak sharp minima, C, and there is a regular point of the inclusion F (x) 2 C. This result extends a similar convergence result due to Womersley which employs the assumption of a strongly unique solution of the composite function h ffi F . A backtracking line-search is proposed as a globalization strategy. For this algorithm, a global convergence result is established, with a quadratic rate under the regularity assumption. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> J.M. </author> <title> Borwein Stability and regular points of inequality systems. </title> <journal> J. Optim. Theory Appl., </journal> <volume> 48 </volume> <pages> 9-52, </pages> <year> 1986. </year>
Reference-contexts: The regularity condition that we employ has been studied by many authors in various forms and contexts <ref> [1, 5, 13, 23, 27, 32, 37, 38, 39, 42, 46] </ref>. It is related to the stability of the set of solutions of the inclusion (5).
Reference: [2] <author> J.V. Burke. </author> <title> Algorithms for solving finite dimensional systems of nonlinear equations and inequalities that have both global and quadratic convergence properties. </title> <institution> Mathematics and Computer Science Division Report ANL/MCS-TM-54, Argonne National Laboratory, Argonne, Illinois, </institution> <year> 1985. </year>
Reference-contexts: An important distinction between these results is that we do not require that the solution set be a singleton. In particular, the solution set may be unbounded. The approach we take is based on that described in <ref> [2] </ref> which is an extension of a technique due to Garcia-Palomares and Restuccia [17]. However, since [2, 17] are concerned with the more specific problem of solving nonlinear systems of equations and inequalities, we are not able to capture the full flavor of these results. <p> In particular, the solution set may be unbounded. The approach we take is based on that described in [2] which is an extension of a technique due to Garcia-Palomares and Restuccia [17]. However, since <ref> [2, 17] </ref> are concerned with the more specific problem of solving nonlinear systems of equations and inequalities, we are not able to capture the full flavor of these results. In particular, [2, 17] contain results concerning active set strategies that are not considered in this article. <p> However, since <ref> [2, 17] </ref> are concerned with the more specific problem of solving nonlinear systems of equations and inequalities, we are not able to capture the full flavor of these results. In particular, [2, 17] contain results concerning active set strategies that are not considered in this article.
Reference: [3] <author> J.V. Burke. </author> <title> Descent methods for composite nondifferentiable optimization problems. </title> <journal> Mathematical Programming, </journal> <volume> 33 </volume> <pages> 260-279, </pages> <year> 1985. </year>
Reference-contexts: It is interesting to note that in their outline of the Gauss-Newton method, Ortega and Rheinboldt [29, page 267] used the notion of a composite function. A wide variety of applications of this formulation can be found throughout the mathematical programming literature <ref> [3, 14, 15, 22, 35, 42, 44, 45] </ref>, e.g. nonlinear inclusions, penalization methods, minimax, and goal programming. The convex composite model provides a unifying framework for the development and analysis of algorithmic solution techniques. <p> Algorithm 2 is an instance of the class of algorithms studied in <ref> [3] </ref>. Hence the global convergence properties of the method follow from Theorems 2.4 and 5.3 in [3]. These theorems specify the behavior of sequences generated by Algorithm 2 in terms of the first-order optimality conditions for the problem P. <p> Algorithm 2 is an instance of the class of algorithms studied in <ref> [3] </ref>. Hence the global convergence properties of the method follow from Theorems 2.4 and 5.3 in [3]. These theorems specify the behavior of sequences generated by Algorithm 2 in terms of the first-order optimality conditions for the problem P. <p> This condition can be equivalently stated in terms of the convex subdifferential of h as 0 2 F 0 (x) T @h (F (x)) : (30) Moreover, by <ref> [3, Lemma 4.5 and Theorem 3.6] </ref>, the conditions (29) and (30) are equivalent to the conditions h (F (x) + F 0 (x)d) h (F (x)) = 0 for all d 2 D (x) (31) 0 2 D (x) : (32) These latter conditions are particularly important in light of the <p> The key global properties established in <ref> [3] </ref> for Algorithm 2 are recalled in the following theorem. Theorem 5.1 Let x 0 2 IR n and let f = h ffi F be as in P.
Reference: [4] <author> J.V. Burke. </author> <title> A sequential quadratic programming method for potentially infeasible mathematical programs. </title> <journal> Journal of Mathematical Analysis and its Applications, </journal> <volume> 139(2) </volume> <pages> 319-351, </pages> <year> 1989. </year>
Reference-contexts: D 1 (x)) = maxfhy; F (x)i fl C (y) j y 2 F 0 (x) T 1 maxfhy; F (x)i fl C (y) j y 2 fi IB ffi g = fi dist (F (x) j C) for all x 2 N (x), where the last equality follows from <ref> [4, Lemma 2.9] </ref>. To complete the proof, we now establish (15). Let &gt; 0 be given. By (14), there is a neighborhood of x on which D 2 2 (x). By Parts (iii) and (v) of Lemma 3.2, there is a d 2 2 IR n satisfying (18).
Reference: [5] <author> J.V. Burke. </author> <title> An exact penalization viewpoint of constrained optimization. </title> <journal> SIAM Journal on Control and Optimization, </journal> <volume> 29 </volume> <pages> 968-998, </pages> <year> 1991. </year>
Reference-contexts: The convex composite model provides a unifying framework for the development and analysis of algorithmic solution techniques. Moreover, it is also a 2 convenient tool for the study of first- and second-order optimality conditions in constrained optimization <ref> [5, 7, 15, 18, 42] </ref>. Indeed, the deepest results on optimality conditions for a variety of problems have been obtained in this way. <p> The regularity condition that we employ has been studied by many authors in various forms and contexts <ref> [1, 5, 13, 23, 27, 32, 37, 38, 39, 42, 46] </ref>. It is related to the stability of the set of solutions of the inclusion (5).
Reference: [6] <author> J.V. Burke and M.C. Ferris. </author> <title> Weak sharp minima in mathematical programming. </title> <journal> SIAM Journal on Control and Optimization, </journal> <volume> 31(5), </volume> <year> 1993. </year> <month> 17 </month>
Reference-contexts: take requires two basic assumptions: (1) the set of minima for the function h, denoted by C, is a set of weak sharp minima for h, and (2) there is a regular point for the inclusion F (x) 2 C: (5) The notion of weak sharp minima was introduced in <ref> [6, 12] </ref> and is reviewed in the next section. The regularity condition that we employ has been studied by many authors in various forms and contexts [1, 5, 13, 23, 27, 32, 37, 38, 39, 42, 46]. <p> The prototypical example of a function h having a set of weak sharp minima is the distance function dist ( j C) itself. Other examples are explored in <ref> [6, 12] </ref>. For example, if h is polyhedral convex, then its set of minima is necessarily a set of weak sharp minima. <p> These concepts have a long history in the literature and have far reaching consequences for the convergence analysis of many iterative procedures [9, 19, 22, 33, 34, 44]. In <ref> [6] </ref>, it was shown that some of these convergence results can be extended to the case of weak sharp minima. This article continues this discussion in the context of convex composite optimization. <p> It is immediate from this fact that regularity of x for the inclusion F (x) 2 C coupled with an assumption that h is sharp on C implies that the composite function h ffi F is locally sharp <ref> [6] </ref> with respect to the set F 1 (C). This observation is summarized in the following proposition. Proposition 3.4 Let x 2 IR n be a regular point of the inclusion (5) where C is a set of weak sharp minima for h.
Reference: [7] <author> J.V. Burke and R.A. Poliquin. </author> <title> Optimality conditions for non finite-valued convex com-posite functions. </title> <journal> Mathematical Programming, </journal> <volume> 57 </volume> <pages> 103-120, </pages> <year> 1992. </year>
Reference-contexts: The convex composite model provides a unifying framework for the development and analysis of algorithmic solution techniques. Moreover, it is also a 2 convenient tool for the study of first- and second-order optimality conditions in constrained optimization <ref> [5, 7, 15, 18, 42] </ref>. Indeed, the deepest results on optimality conditions for a variety of problems have been obtained in this way. <p> Further discussion of this curvature component can be found [11, 29] for the classical Gauss-Newton method and in [15, 35] for convex composite optimization. The relationship of this component to second-order optimality conditions can be found in <ref> [7, 15, 18, 42] </ref>. In this article, we avoid the need for a curvature term by focusing on the local behavior of the algorithm in the neighborhood of a point x satisfying F (x) 2 C := arg min h.
Reference: [8] <author> F.H. Clarke. </author> <title> Optimization and Nonsmooth Analysis. </title> <publisher> John Wiley & Sons, </publisher> <address> New York, </address> <year> 1983. </year>
Reference: [9] <author> L. Cromme. </author> <title> Strong uniqueness. </title> <journal> Numerische Mathematik, </journal> <volume> 29 </volume> <pages> 179-193, </pages> <year> 1978. </year>
Reference-contexts: Numerical methods for obtaining least-norm solutions to linear programs have been developed in [10]. The notion of weak sharp minima generalizes the notion of a sharp [33, 34] or strongly unique <ref> [9, 22, 30, 31, 44] </ref> minimum. These concepts have a long history in the literature and have far reaching consequences for the convergence analysis of many iterative procedures [9, 19, 22, 33, 34, 44]. <p> The notion of weak sharp minima generalizes the notion of a sharp [33, 34] or strongly unique [9, 22, 30, 31, 44] minimum. These concepts have a long history in the literature and have far reaching consequences for the convergence analysis of many iterative procedures <ref> [9, 19, 22, 33, 34, 44] </ref>. In [6], it was shown that some of these convergence results can be extended to the case of weak sharp minima. This article continues this discussion in the context of convex composite optimization.
Reference: [10] <author> R. De Leone and O.L. Mangasarian. </author> <title> Serial and parallel solution of large scale linear programs by augmented Lagrangian successive overrelaxation. </title> <editor> In A. Kurzhanski, K. Neumann, and D. Pallaschke, editors, </editor> <booktitle> Optimization, Parallel Processing and Applications, </booktitle> <pages> pages 103-124, </pages> <address> Berlin, </address> <year> 1988. </year> <note> Springer-Verlag. Lecture Notes in Economics and Mathematical Systems 304. </note>
Reference-contexts: Numerical methods for obtaining least-norm solutions to linear programs have been developed in <ref> [10] </ref>. The notion of weak sharp minima generalizes the notion of a sharp [33, 34] or strongly unique [9, 22, 30, 31, 44] minimum.
Reference: [11] <author> J.E. Dennis and R.B. Schnabel. </author> <title> Numerical Methods for Unconstrained Optimizations and Nonlinear Equations. </title> <publisher> Prentice Hall, </publisher> <address> New Jersey, </address> <year> 1983. </year>
Reference-contexts: Other variations that enhance the robustness of the method are the addition of a quadratic term to the the objective in the step finding subproblem (see [20, 28]) or the inclusion of a trust-region constraint (see <ref> [11] </ref>). In this paper we discuss the extension of the Gauss-Newton methodology to finite-valued convex composite optimization. <p> Further discussion of this curvature component can be found <ref> [11, 29] </ref> for the classical Gauss-Newton method and in [15, 35] for convex composite optimization. The relationship of this component to second-order optimality conditions can be found in [7, 15, 18, 42].
Reference: [12] <author> M.C. Ferris. </author> <title> Weak Sharp Minima and Penalty Functions in Mathematical Programming. </title> <type> PhD thesis, </type> <institution> University of Cambridge, </institution> <year> 1988. </year>
Reference-contexts: take requires two basic assumptions: (1) the set of minima for the function h, denoted by C, is a set of weak sharp minima for h, and (2) there is a regular point for the inclusion F (x) 2 C: (5) The notion of weak sharp minima was introduced in <ref> [6, 12] </ref> and is reviewed in the next section. The regularity condition that we employ has been studied by many authors in various forms and contexts [1, 5, 13, 23, 27, 32, 37, 38, 39, 42, 46]. <p> Our analysis is based on two key assumptions; the set C is a set of weak sharp minima for the function h and the point x is a regular point (see Section 3) for the inclusion (5). The weak sharp minima concept was introduced in <ref> [12] </ref>. <p> The prototypical example of a function h having a set of weak sharp minima is the distance function dist ( j C) itself. Other examples are explored in <ref> [6, 12] </ref>. For example, if h is polyhedral convex, then its set of minima is necessarily a set of weak sharp minima.
Reference: [13] <author> A.V. Fiacco. </author> <title> Introduction to Sensitivity and Stability Analysis in Nonlinear Programming. </title> <publisher> Academic Press, </publisher> <address> New York, </address> <year> 1983. </year>
Reference-contexts: The regularity condition that we employ has been studied by many authors in various forms and contexts <ref> [1, 5, 13, 23, 27, 32, 37, 38, 39, 42, 46] </ref>. It is related to the stability of the set of solutions of the inclusion (5).
Reference: [14] <author> R. Fletcher. </author> <title> Second order correction for nondifferentiable optimization. In G.A. </title> <editor> Watson, editor, </editor> <booktitle> Numerical Analysis, </booktitle> <pages> pages 85-114. </pages> <publisher> Springer-Verlag, </publisher> <address> Berlin, </address> <year> 1982. </year>
Reference-contexts: It is interesting to note that in their outline of the Gauss-Newton method, Ortega and Rheinboldt [29, page 267] used the notion of a composite function. A wide variety of applications of this formulation can be found throughout the mathematical programming literature <ref> [3, 14, 15, 22, 35, 42, 44, 45] </ref>, e.g. nonlinear inclusions, penalization methods, minimax, and goal programming. The convex composite model provides a unifying framework for the development and analysis of algorithmic solution techniques.
Reference: [15] <author> R. Fletcher. </author> <title> Practical Methods of Optimization. </title> <publisher> John Wiley & Sons, </publisher> <address> New York, </address> <note> second edition, </note> <year> 1987. </year>
Reference-contexts: It is interesting to note that in their outline of the Gauss-Newton method, Ortega and Rheinboldt [29, page 267] used the notion of a composite function. A wide variety of applications of this formulation can be found throughout the mathematical programming literature <ref> [3, 14, 15, 22, 35, 42, 44, 45] </ref>, e.g. nonlinear inclusions, penalization methods, minimax, and goal programming. The convex composite model provides a unifying framework for the development and analysis of algorithmic solution techniques. <p> The convex composite model provides a unifying framework for the development and analysis of algorithmic solution techniques. Moreover, it is also a 2 convenient tool for the study of first- and second-order optimality conditions in constrained optimization <ref> [5, 7, 15, 18, 42] </ref>. Indeed, the deepest results on optimality conditions for a variety of problems have been obtained in this way. <p> Further discussion of this curvature component can be found [11, 29] for the classical Gauss-Newton method and in <ref> [15, 35] </ref> for convex composite optimization. The relationship of this component to second-order optimality conditions can be found in [7, 15, 18, 42]. <p> Further discussion of this curvature component can be found [11, 29] for the classical Gauss-Newton method and in [15, 35] for convex composite optimization. The relationship of this component to second-order optimality conditions can be found in <ref> [7, 15, 18, 42] </ref>. In this article, we avoid the need for a curvature term by focusing on the local behavior of the algorithm in the neighborhood of a point x satisfying F (x) 2 C := arg min h.
Reference: [16] <author> U.M. Garcia-Palomares. </author> <title> Superlinearly convergent algorithms for linearly constrained optimization. In R.R. </title> <editor> Meyer O.L. Mangasarian and S.M. Robinson, editors, </editor> <booktitle> Nonlinear Programming 2, </booktitle> <pages> pages 101-119, </pages> <address> New York, 1975. </address> <publisher> Academic Press. </publisher>
Reference-contexts: Our proof is based on <ref> [16, Lemma 3.7] </ref>.
Reference: [17] <author> U.M. Garcia-Palomares and A. Restuccia. </author> <title> A global quadratic algorithm for solving a system of mixed equations. </title> <journal> Mathematical Programming, </journal> <volume> 21 </volume> <pages> 290-300, </pages> <year> 1981. </year>
Reference-contexts: In particular, the solution set may be unbounded. The approach we take is based on that described in [2] which is an extension of a technique due to Garcia-Palomares and Restuccia <ref> [17] </ref>. However, since [2, 17] are concerned with the more specific problem of solving nonlinear systems of equations and inequalities, we are not able to capture the full flavor of these results. In particular, [2, 17] contain results concerning active set strategies that are not considered in this article. <p> In particular, the solution set may be unbounded. The approach we take is based on that described in [2] which is an extension of a technique due to Garcia-Palomares and Restuccia [17]. However, since <ref> [2, 17] </ref> are concerned with the more specific problem of solving nonlinear systems of equations and inequalities, we are not able to capture the full flavor of these results. In particular, [2, 17] contain results concerning active set strategies that are not considered in this article. <p> However, since <ref> [2, 17] </ref> are concerned with the more specific problem of solving nonlinear systems of equations and inequalities, we are not able to capture the full flavor of these results. In particular, [2, 17] contain results concerning active set strategies that are not considered in this article.
Reference: [18] <author> A.D. Ioffe. </author> <title> Variational analysis of a composite function: A formula for the lower second-order directional derivative. </title> <journal> Journal of Mathematical Analysis and its Applications, </journal> <year> 1993. </year> <month> Forthcoming. </month>
Reference-contexts: The convex composite model provides a unifying framework for the development and analysis of algorithmic solution techniques. Moreover, it is also a 2 convenient tool for the study of first- and second-order optimality conditions in constrained optimization <ref> [5, 7, 15, 18, 42] </ref>. Indeed, the deepest results on optimality conditions for a variety of problems have been obtained in this way. <p> Further discussion of this curvature component can be found [11, 29] for the classical Gauss-Newton method and in [15, 35] for convex composite optimization. The relationship of this component to second-order optimality conditions can be found in <ref> [7, 15, 18, 42] </ref>. In this article, we avoid the need for a curvature term by focusing on the local behavior of the algorithm in the neighborhood of a point x satisfying F (x) 2 C := arg min h.
Reference: [19] <author> K. Jittorntrum and M.R. Osborne. </author> <title> Strong uniqueness and second order convergence in nonlinear discrete approximation. </title> <journal> Numerische Mathematik 34 </journal> <pages> 439-455, </pages> <year> 1980. </year>
Reference-contexts: The notion of weak sharp minima generalizes the notion of a sharp [33, 34] or strongly unique [9, 22, 30, 31, 44] minimum. These concepts have a long history in the literature and have far reaching consequences for the convergence analysis of many iterative procedures <ref> [9, 19, 22, 33, 34, 44] </ref>. In [6], it was shown that some of these convergence results can be extended to the case of weak sharp minima. This article continues this discussion in the context of convex composite optimization. <p> In particular, this extends the quadratic convergence result under strong uniqueness due to Womersley [44]. Our result is local and guarantees a quadratic rate of convergence. Related results can be found in <ref> [24, 19, 30, 31] </ref>. Theorem 4.2 Let x 2 IR n be a regular point of the inclusion (5) where C is a set of weak sharp minima for h and suppose F 0 is locally Lipschitz at x.
Reference: [20] <author> K. Levenberg. </author> <title> A method for the solution of certain nonlinear problems in least squares. </title> <journal> Quarterly Applied Mathematics, </journal> <volume> 2 </volume> <pages> 164-168, </pages> <year> 1944. </year>
Reference-contexts: Nonetheless, the method is always implementable and can be made significantly more robust by the addition of a line-search. Other variations that enhance the robustness of the method are the addition of a quadratic term to the the objective in the step finding subproblem (see <ref> [20, 28] </ref>) or the inclusion of a trust-region constraint (see [11]). In this paper we discuss the extension of the Gauss-Newton methodology to finite-valued convex composite optimization.
Reference: [21] <author> D.G. Luenberger. </author> <title> Optimization by Vector Space Methods. </title> <publisher> John Wiley & Sons, </publisher> <address> New York, </address> <year> 1969. </year> <month> 18 </month>
Reference: [22] <author> K. Madsen. </author> <title> Minimization of Nonlinear Approximation Functions. </title> <type> PhD thesis, </type> <institution> Institute of Numerical Analysis, Technical University of Denmark, Lyngby, Denmark, </institution> <year> 1985. </year>
Reference-contexts: It is interesting to note that in their outline of the Gauss-Newton method, Ortega and Rheinboldt [29, page 267] used the notion of a composite function. A wide variety of applications of this formulation can be found throughout the mathematical programming literature <ref> [3, 14, 15, 22, 35, 42, 44, 45] </ref>, e.g. nonlinear inclusions, penalization methods, minimax, and goal programming. The convex composite model provides a unifying framework for the development and analysis of algorithmic solution techniques. <p> Numerical methods for obtaining least-norm solutions to linear programs have been developed in [10]. The notion of weak sharp minima generalizes the notion of a sharp [33, 34] or strongly unique <ref> [9, 22, 30, 31, 44] </ref> minimum. These concepts have a long history in the literature and have far reaching consequences for the convergence analysis of many iterative procedures [9, 19, 22, 33, 34, 44]. <p> The notion of weak sharp minima generalizes the notion of a sharp [33, 34] or strongly unique [9, 22, 30, 31, 44] minimum. These concepts have a long history in the literature and have far reaching consequences for the convergence analysis of many iterative procedures <ref> [9, 19, 22, 33, 34, 44] </ref>. In [6], it was shown that some of these convergence results can be extended to the case of weak sharp minima. This article continues this discussion in the context of convex composite optimization.
Reference: [23] <author> J. Maguregui. </author> <title> Regular Multivalued Functions and Algorithmic Applications. </title> <type> PhD thesis, </type> <institution> University of Wisconsin, Madison, Wisconsin, </institution> <year> 1977. </year>
Reference-contexts: The regularity condition that we employ has been studied by many authors in various forms and contexts <ref> [1, 5, 13, 23, 27, 32, 37, 38, 39, 42, 46] </ref>. It is related to the stability of the set of solutions of the inclusion (5). <p> It is related to the stability of the set of solutions of the inclusion (5). In particular, the regularity hypothesis implies that the local behavior of the Gauss-Newton method presented in Section 2 mimics that of the method proposed by Maguregui <ref> [23, 24] </ref>. Maguregui studies the procedure in the more general Banach space setting and obtains a convergence result that extends Kantorovich's convergence theory for Newton's method to smooth nonlinear convex inclusions. <p> Remarks: 1. The form of the regularity condition used by Maguregui <ref> [23, 24] </ref> most closely resembles condition (iii) above.
Reference: [24] <author> J. Maguregui. </author> <title> A modified Newton algorithm for functions over convex sets. In O.L. </title> <editor> Mangasarian, R.R. Meyer, and S.M. Robinson editors, </editor> <booktitle> Nonlinear Programming 3, </booktitle> <pages> pages 461-473, </pages> <publisher> Academic Press, </publisher> <pages> 461-473, </pages> <year> 1978 </year>
Reference-contexts: It is related to the stability of the set of solutions of the inclusion (5). In particular, the regularity hypothesis implies that the local behavior of the Gauss-Newton method presented in Section 2 mimics that of the method proposed by Maguregui <ref> [23, 24] </ref>. Maguregui studies the procedure in the more general Banach space setting and obtains a convergence result that extends Kantorovich's convergence theory for Newton's method to smooth nonlinear convex inclusions. <p> Remarks: 1. The form of the regularity condition used by Maguregui <ref> [23, 24] </ref> most closely resembles condition (iii) above. <p> In particular, this extends the quadratic convergence result under strong uniqueness due to Womersley [44]. Our result is local and guarantees a quadratic rate of convergence. Related results can be found in <ref> [24, 19, 30, 31] </ref>. Theorem 4.2 Let x 2 IR n be a regular point of the inclusion (5) where C is a set of weak sharp minima for h and suppose F 0 is locally Lipschitz at x.
Reference: [25] <author> O.L. Mangasarian. </author> <title> Least-norm linear programming solution as an unconstrained minimization problem. </title> <journal> Journal of Mathematical Analysis and Applications, </journal> <volume> 92(1) </volume> <pages> 240-251, </pages> <year> 1983. </year>
Reference-contexts: Moreover, in this case, if it is further assumed that the norm on IR n is polyhedral, then one can obtain a direction choice satisfying (7) of Step 1 in Algorithm 1 by computing a least-norm solution of a linear program in the sense of <ref> [25, 26] </ref>. Numerical methods for obtaining least-norm solutions to linear programs have been developed in [10]. The notion of weak sharp minima generalizes the notion of a sharp [33, 34] or strongly unique [9, 22, 30, 31, 44] minimum.
Reference: [26] <author> O.L. Mangasarian. </author> <title> Normal solutions of linear programs. </title> <journal> Mathematical Programming Study, </journal> <volume> 22 </volume> <pages> 206-216, </pages> <year> 1984. </year>
Reference-contexts: Moreover, in this case, if it is further assumed that the norm on IR n is polyhedral, then one can obtain a direction choice satisfying (7) of Step 1 in Algorithm 1 by computing a least-norm solution of a linear program in the sense of <ref> [25, 26] </ref>. Numerical methods for obtaining least-norm solutions to linear programs have been developed in [10]. The notion of weak sharp minima generalizes the notion of a sharp [33, 34] or strongly unique [9, 22, 30, 31, 44] minimum.
Reference: [27] <author> O.L. Mangasarian and S. Fromovitz. </author> <title> The Fritz John necessary optimality conditions in the presence of equality and inequality constraints. </title> <journal> J. Math. Anal. Appl., </journal> <volume> 17 </volume> <pages> 37-47, </pages> <year> 1967. </year>
Reference-contexts: The regularity condition that we employ has been studied by many authors in various forms and contexts <ref> [1, 5, 13, 23, 27, 32, 37, 38, 39, 42, 46] </ref>. It is related to the stability of the set of solutions of the inclusion (5).
Reference: [28] <author> D.W. Marquardt. </author> <title> An algorithm for least-squares estimation of nonlinear parameters. </title> <journal> SIAM Journal of Applied Mathematics, </journal> <volume> 11 </volume> <pages> 431-441, </pages> <year> 1963. </year>
Reference-contexts: Nonetheless, the method is always implementable and can be made significantly more robust by the addition of a line-search. Other variations that enhance the robustness of the method are the addition of a quadratic term to the the objective in the step finding subproblem (see <ref> [20, 28] </ref>) or the inclusion of a trust-region constraint (see [11]). In this paper we discuss the extension of the Gauss-Newton methodology to finite-valued convex composite optimization.
Reference: [29] <author> J.M. Ortega and W.C. Rheinboldt. </author> <title> Iterative Solution of Nonlinear Equations in Several Variables. </title> <publisher> Academic Press, </publisher> <year> 1970. </year>
Reference-contexts: In this article, we consider only the finite-valued case; h : IR m ! IR. Obviously the problem (4) is precisely of this form. It is interesting to note that in their outline of the Gauss-Newton method, Ortega and Rheinboldt <ref> [29, page 267] </ref> used the notion of a composite function. A wide variety of applications of this formulation can be found throughout the mathematical programming literature [3, 14, 15, 22, 35, 42, 44, 45], e.g. nonlinear inclusions, penalization methods, minimax, and goal programming. <p> Further discussion of this curvature component can be found <ref> [11, 29] </ref> for the classical Gauss-Newton method and in [15, 35] for convex composite optimization. The relationship of this component to second-order optimality conditions can be found in [7, 15, 18, 42]. <p> Since x 0 2 M (x) we have fl flx 1 x fl fl flx 0 x fl fl fld 0 fl ffi &lt; 2ffi: Hence x 1 2 x + 2ffi IB N (x). Furthermore, by the quadratic bound lemma <ref> [29, 3.2.12] </ref> fl flF (x 0 ) + F 0 (x 0 )d 0 F (x 1 ) fl L fl fld 0 fl 2 L ; so that F (x 0 ) + F 0 (x 0 )d 0 2 F (N (x)) + L 8 IB. <p> Let &gt; ffi &gt; 0 be chosen so that the conclusions of Proposition 3.3 hold for this choice of ffi. We suppose with no loss of generality that fx k g x + ffi IB. Then for all k we have from <ref> [29, 3.2.12] </ref> that h (F (x k + d k )) h min = fl fl fl fl flF (x k + d k ) F (x k ) F 0 (x k ) d k fl 2 fl fl fl : Therefore, by (34) c [h min h (F (x
Reference: [30] <author> M.R. Osborne. </author> <title> Strong uniqueness in nonlinear approximation. </title> <editor> in C.T.H. baker and C. Phillips, eds., </editor> <title> The numerical solution of nonlinear problems, </title> <publisher> Clarendon Press, Oxford, </publisher> <year> 1981. </year>
Reference-contexts: Numerical methods for obtaining least-norm solutions to linear programs have been developed in [10]. The notion of weak sharp minima generalizes the notion of a sharp [33, 34] or strongly unique <ref> [9, 22, 30, 31, 44] </ref> minimum. These concepts have a long history in the literature and have far reaching consequences for the convergence analysis of many iterative procedures [9, 19, 22, 33, 34, 44]. <p> In particular, this extends the quadratic convergence result under strong uniqueness due to Womersley [44]. Our result is local and guarantees a quadratic rate of convergence. Related results can be found in <ref> [24, 19, 30, 31] </ref>. Theorem 4.2 Let x 2 IR n be a regular point of the inclusion (5) where C is a set of weak sharp minima for h and suppose F 0 is locally Lipschitz at x.
Reference: [31] <author> M.R. </author> <title> Osborne and R.S. Womersley. Strong uniqueness in sequential linear programming. </title> <journal> Journal of the Australian Mathematical Society, </journal> <volume> Series B 31 </volume> <pages> 379-384, </pages> <year> 1990. </year>
Reference-contexts: Numerical methods for obtaining least-norm solutions to linear programs have been developed in [10]. The notion of weak sharp minima generalizes the notion of a sharp [33, 34] or strongly unique <ref> [9, 22, 30, 31, 44] </ref> minimum. These concepts have a long history in the literature and have far reaching consequences for the convergence analysis of many iterative procedures [9, 19, 22, 33, 34, 44]. <p> In particular, this extends the quadratic convergence result under strong uniqueness due to Womersley [44]. Our result is local and guarantees a quadratic rate of convergence. Related results can be found in <ref> [24, 19, 30, 31] </ref>. Theorem 4.2 Let x 2 IR n be a regular point of the inclusion (5) where C is a set of weak sharp minima for h and suppose F 0 is locally Lipschitz at x.
Reference: [32] <author> J.-P. </author> <title> Penot. On regularity conditions in mathematical programming. </title> <journal> Math. Prog. Studies, </journal> <volume> 17 </volume> <pages> 28-66, </pages> <year> 1982. </year>
Reference-contexts: The regularity condition that we employ has been studied by many authors in various forms and contexts <ref> [1, 5, 13, 23, 27, 32, 37, 38, 39, 42, 46] </ref>. It is related to the stability of the set of solutions of the inclusion (5).
Reference: [33] <author> B.T. Polyak. </author> <title> Sharp minima. A Talk given at the IIASA Workshop on Generalized Lagrangians and their Applications, </title> <address> IIASA, Laxenburg, Austria, </address> <year> 1979. </year>
Reference-contexts: Numerical methods for obtaining least-norm solutions to linear programs have been developed in [10]. The notion of weak sharp minima generalizes the notion of a sharp <ref> [33, 34] </ref> or strongly unique [9, 22, 30, 31, 44] minimum. These concepts have a long history in the literature and have far reaching consequences for the convergence analysis of many iterative procedures [9, 19, 22, 33, 34, 44]. <p> The notion of weak sharp minima generalizes the notion of a sharp [33, 34] or strongly unique [9, 22, 30, 31, 44] minimum. These concepts have a long history in the literature and have far reaching consequences for the convergence analysis of many iterative procedures <ref> [9, 19, 22, 33, 34, 44] </ref>. In [6], it was shown that some of these convergence results can be extended to the case of weak sharp minima. This article continues this discussion in the context of convex composite optimization.
Reference: [34] <author> B.T. Polyak. </author> <title> Introduction to Optimization. Optimization Software, </title> <publisher> Inc., Publications Division, </publisher> <address> New York, </address> <year> 1987. </year>
Reference-contexts: Numerical methods for obtaining least-norm solutions to linear programs have been developed in [10]. The notion of weak sharp minima generalizes the notion of a sharp <ref> [33, 34] </ref> or strongly unique [9, 22, 30, 31, 44] minimum. These concepts have a long history in the literature and have far reaching consequences for the convergence analysis of many iterative procedures [9, 19, 22, 33, 34, 44]. <p> The notion of weak sharp minima generalizes the notion of a sharp [33, 34] or strongly unique [9, 22, 30, 31, 44] minimum. These concepts have a long history in the literature and have far reaching consequences for the convergence analysis of many iterative procedures <ref> [9, 19, 22, 33, 34, 44] </ref>. In [6], it was shown that some of these convergence results can be extended to the case of weak sharp minima. This article continues this discussion in the context of convex composite optimization.
Reference: [35] <author> M.J.D. Powell. </author> <title> General algorithm for discrete nonlinear approximation calculations. In L.L. </title> <editor> Schumaker C.K. Chui and J.D. Ward, editors, </editor> <booktitle> Approximation Theory IV, </booktitle> <pages> pages 187-218. </pages> <publisher> Academic Press, </publisher> <address> New York, </address> <year> 1983. </year>
Reference-contexts: It is interesting to note that in their outline of the Gauss-Newton method, Ortega and Rheinboldt [29, page 267] used the notion of a composite function. A wide variety of applications of this formulation can be found throughout the mathematical programming literature <ref> [3, 14, 15, 22, 35, 42, 44, 45] </ref>, e.g. nonlinear inclusions, penalization methods, minimax, and goal programming. The convex composite model provides a unifying framework for the development and analysis of algorithmic solution techniques. <p> Further discussion of this curvature component can be found [11, 29] for the classical Gauss-Newton method and in <ref> [15, 35] </ref> for convex composite optimization. The relationship of this component to second-order optimality conditions can be found in [7, 15, 18, 42].
Reference: [36] <author> H. </author> <title> R-adstrom An embedding theorem for spaces of convex sets. </title> <journal> Proc. Amer. Math. Soc., </journal> <volume> 3 </volume> <pages> 165-169, </pages> <year> 1952. </year>
Reference-contexts: Hence IB m A IB n +(ri C z) + 2 whenever (z; A) 2 (z; A) + 2 IB. Therefore, by the R-adstrom Cancellation Lemma <ref> [36, Lemma 1] </ref>, IB m A IB n +(ri C z) A IB n + cone (C z) : Taking the polar of this last statement and setting * := 2 =: fi 1 , we find that (iii) implies the existence of * &gt; 0 and fi &gt; 0 such
Reference: [37] <author> S.M. </author> <title> Robinson Stability theory for systems of inequalities, Part I: linear systems. </title> <journal> SIAM J. Numer. Anal., </journal> <volume> 12 </volume> <pages> 754-769, </pages> <year> 1975. </year>
Reference-contexts: The regularity condition that we employ has been studied by many authors in various forms and contexts <ref> [1, 5, 13, 23, 27, 32, 37, 38, 39, 42, 46] </ref>. It is related to the stability of the set of solutions of the inclusion (5). <p> We show that under the regularity hypothesis the distance to the solution set of the linearized inclusion can be bounded locally by the distance of the linearization to the set C. This result is reminiscent of several similar results due to Robinson <ref> [37, 38, 39, 43] </ref>. Indeed, the bound (14) is easily derived from the Robinson-Ursescu Theorem. On the other hand, our proof of the result is a simple application of Fenchel's Duality Theorem [40, Corollary 31.2.1].
Reference: [38] <author> S.M. </author> <title> Robinson Stability theory for systems of inequalities, Part II: differentiable nonlinear systems. </title> <journal> SIAM J. Numer. Anal., </journal> <volume> 13 </volume> <pages> 497-513, </pages> <year> 1976. </year>
Reference-contexts: The regularity condition that we employ has been studied by many authors in various forms and contexts <ref> [1, 5, 13, 23, 27, 32, 37, 38, 39, 42, 46] </ref>. It is related to the stability of the set of solutions of the inclusion (5). <p> We show that under the regularity hypothesis the distance to the solution set of the linearized inclusion can be bounded locally by the distance of the linearization to the set C. This result is reminiscent of several similar results due to Robinson <ref> [37, 38, 39, 43] </ref>. Indeed, the bound (14) is easily derived from the Robinson-Ursescu Theorem. On the other hand, our proof of the result is a simple application of Fenchel's Duality Theorem [40, Corollary 31.2.1]. <p> We show that regularity combined with weak sharpness implies that the composite function is also weak sharp in a local sense (see Proposition 3.4). In the proposition above, we have obtained (14), essentially a linear result, by way of Fenchel duality. However, a stronger result is obtained by Robinson <ref> [38, Theorem 1] </ref>, where it is shown that the same regularity assumption implies dist dist (F (x) j C) ; for all x in a neighborhood of the regular point x and some &gt; 0.
Reference: [39] <author> S.M. </author> <title> Robinson Regularity and stability for convex multivalued functions. </title> <journal> Math. of O.R., </journal> <volume> 1 </volume> <pages> 130-143, </pages> <year> 1976. </year>
Reference-contexts: The regularity condition that we employ has been studied by many authors in various forms and contexts <ref> [1, 5, 13, 23, 27, 32, 37, 38, 39, 42, 46] </ref>. It is related to the stability of the set of solutions of the inclusion (5). <p> Maguregui studies the procedure in the more general Banach space setting and obtains a convergence result that extends Kantorovich's convergence theory for Newton's method to smooth nonlinear convex inclusions. The key ingredient in Maguregui's proof theory is the Robinson-Ursescu Theorem <ref> [39, Theorem 2] </ref>, [43] on the stability of convex multifunctions. In this article, we provide a self-contained and elementary proof theory in the finite dimensional case and link this theory to a glob-alization strategy via the Gauss-Newton methodology for convex composite optimization. <p> We show that under the regularity hypothesis the distance to the solution set of the linearized inclusion can be bounded locally by the distance of the linearization to the set C. This result is reminiscent of several similar results due to Robinson <ref> [37, 38, 39, 43] </ref>. Indeed, the bound (14) is easily derived from the Robinson-Ursescu Theorem. On the other hand, our proof of the result is a simple application of Fenchel's Duality Theorem [40, Corollary 31.2.1].
Reference: [40] <author> R.T. Rockafellar. </author> <title> Convex Analysis. </title> <publisher> Princeton University Press, </publisher> <address> Princeton, NJ, </address> <year> 1970. </year>
Reference-contexts: Furthermore, for a convex function f , @f signifies the subdifferential multifunction <ref> [40] </ref>. We denote a norm on IR by kk. The associated closed unit ball for the given norm is denoted by IB. <p> This result is reminiscent of several similar results due to Robinson [37, 38, 39, 43]. Indeed, the bound (14) is easily derived from the Robinson-Ursescu Theorem. On the other hand, our proof of the result is a simple application of Fenchel's Duality Theorem <ref> [40, Corollary 31.2.1] </ref>. Proposition 3.3 If x is a regular point of (5), then for all &gt; 0, there is some neighborhood N (x) of x and a fi &gt; 0 satisfying dist (0 j D (x)) fi dist (F (x) j C) ; (14) whenever x 2 N (x). <p> To complete the proof, we now establish (15). Let &gt; 0 be given. By (14), there is a neighborhood of x on which D 2 2 (x). By Parts (iii) and (v) of Lemma 3.2, there is a d 2 2 IR n satisfying (18). By <ref> [40, Theorem 6.1] </ref>, it follows that (1 t)[F (x) + F 0 (x)d 1 ] + t [F (x) + F 0 (x)d 2 ] 2 ri C; 8t 2 (0; 1] and hence that F (x) + F 0 (x)((1 t)d 1 + td 2 ) 2 ri C: The
Reference: [41] <author> R.T. Rockafellar. </author> <title> Conjugate Duality and Optimization, </title> <booktitle> volume 16, National Science Foundation CBMS Series. </booktitle> <publisher> SIAM, </publisher> <address> Philadelphia, PA, </address> <year> 1974. </year>
Reference-contexts: The actual condition he employs is z 2 C; 0 2 core (im A + C z); (13) where the core of a closed convex set in a Banach space is the same as the interior of the set in the norm topology <ref> [41, pg. 31] </ref>. Condition (iii) is equivalent to this regularity condition if it is further assumed in (iii) that z 2 C. Observe that the use of the multifunction C avoids the need to specify that z 2 C for results such as Lemma 3.2.
Reference: [42] <author> R.T. Rockafellar. </author> <title> First- and second-order epi-differentiabilty in nonlinear programming. </title> <journal> Transactions of the American Mathematical Society, </journal> <volume> 307 </volume> <pages> 75-108, </pages> <year> 1988. </year>
Reference-contexts: It is interesting to note that in their outline of the Gauss-Newton method, Ortega and Rheinboldt [29, page 267] used the notion of a composite function. A wide variety of applications of this formulation can be found throughout the mathematical programming literature <ref> [3, 14, 15, 22, 35, 42, 44, 45] </ref>, e.g. nonlinear inclusions, penalization methods, minimax, and goal programming. The convex composite model provides a unifying framework for the development and analysis of algorithmic solution techniques. <p> The convex composite model provides a unifying framework for the development and analysis of algorithmic solution techniques. Moreover, it is also a 2 convenient tool for the study of first- and second-order optimality conditions in constrained optimization <ref> [5, 7, 15, 18, 42] </ref>. Indeed, the deepest results on optimality conditions for a variety of problems have been obtained in this way. <p> The regularity condition that we employ has been studied by many authors in various forms and contexts <ref> [1, 5, 13, 23, 27, 32, 37, 38, 39, 42, 46] </ref>. It is related to the stability of the set of solutions of the inclusion (5). <p> Further discussion of this curvature component can be found [11, 29] for the classical Gauss-Newton method and in [15, 35] for convex composite optimization. The relationship of this component to second-order optimality conditions can be found in <ref> [7, 15, 18, 42] </ref>. In this article, we avoid the need for a curvature term by focusing on the local behavior of the algorithm in the neighborhood of a point x satisfying F (x) 2 C := arg min h.
Reference: [43] <author> C. Ursescu. </author> <title> Multifunctions with closed convex graph. Czech. </title> <journal> Math. J., </journal> <volume> 35 </volume> <pages> 438-441, </pages> <year> 1975. </year>
Reference-contexts: Maguregui studies the procedure in the more general Banach space setting and obtains a convergence result that extends Kantorovich's convergence theory for Newton's method to smooth nonlinear convex inclusions. The key ingredient in Maguregui's proof theory is the Robinson-Ursescu Theorem [39, Theorem 2], <ref> [43] </ref> on the stability of convex multifunctions. In this article, we provide a self-contained and elementary proof theory in the finite dimensional case and link this theory to a glob-alization strategy via the Gauss-Newton methodology for convex composite optimization. <p> We show that under the regularity hypothesis the distance to the solution set of the linearized inclusion can be bounded locally by the distance of the linearization to the set C. This result is reminiscent of several similar results due to Robinson <ref> [37, 38, 39, 43] </ref>. Indeed, the bound (14) is easily derived from the Robinson-Ursescu Theorem. On the other hand, our proof of the result is a simple application of Fenchel's Duality Theorem [40, Corollary 31.2.1].
Reference: [44] <author> R.S. Womersley. </author> <title> Local properties of algorithms for minimizing composite functions. </title> <journal> Mathematical Programming, </journal> <volume> 32 </volume> <pages> 69-89, </pages> <year> 1985. </year>
Reference-contexts: It is interesting to note that in their outline of the Gauss-Newton method, Ortega and Rheinboldt [29, page 267] used the notion of a composite function. A wide variety of applications of this formulation can be found throughout the mathematical programming literature <ref> [3, 14, 15, 22, 35, 42, 44, 45] </ref>, e.g. nonlinear inclusions, penalization methods, minimax, and goal programming. The convex composite model provides a unifying framework for the development and analysis of algorithmic solution techniques. <p> Indeed, the deepest results on optimality conditions for a variety of problems have been obtained in this way. Our extension of the Gauss-Newton methodology to finite-valued convex composite optimization generalizes a similar result due to Womersley <ref> [44] </ref> which uses the assumption of strong uniqueness on the composite function f . An important distinction between these results is that we do not require that the solution set be a singleton. In particular, the solution set may be unbounded. <p> Numerical methods for obtaining least-norm solutions to linear programs have been developed in [10]. The notion of weak sharp minima generalizes the notion of a sharp [33, 34] or strongly unique <ref> [9, 22, 30, 31, 44] </ref> minimum. These concepts have a long history in the literature and have far reaching consequences for the convergence analysis of many iterative procedures [9, 19, 22, 33, 34, 44]. <p> The notion of weak sharp minima generalizes the notion of a sharp [33, 34] or strongly unique [9, 22, 30, 31, 44] minimum. These concepts have a long history in the literature and have far reaching consequences for the convergence analysis of many iterative procedures <ref> [9, 19, 22, 33, 34, 44] </ref>. In [6], it was shown that some of these convergence results can be extended to the case of weak sharp minima. This article continues this discussion in the context of convex composite optimization. <p> This result allows us to relate the hypotheses used in the current paper to those used by Womersley <ref> [44] </ref>. Womersley assumes that the compostite function f has a strongly unique minimum, and uses this assumption to derive a quadratic convergence rate for his Gauss-Newton procedure. <p> In particular, this extends the quadratic convergence result under strong uniqueness due to Womersley <ref> [44] </ref>. Our result is local and guarantees a quadratic rate of convergence. Related results can be found in [24, 19, 30, 31].
Reference: [45] <author> Y. Yuan. </author> <title> On the superlinear convergence of a trust region algorithm for nonmooth optimization. </title> <journal> Mathematical Programming, </journal> <volume> 31 </volume> <pages> 269-285, </pages> <year> 1985. </year>
Reference-contexts: It is interesting to note that in their outline of the Gauss-Newton method, Ortega and Rheinboldt [29, page 267] used the notion of a composite function. A wide variety of applications of this formulation can be found throughout the mathematical programming literature <ref> [3, 14, 15, 22, 35, 42, 44, 45] </ref>, e.g. nonlinear inclusions, penalization methods, minimax, and goal programming. The convex composite model provides a unifying framework for the development and analysis of algorithmic solution techniques.
Reference: [46] <author> J. Zowe and S. Kurcyusz. </author> <title> Regularity and stability for mathematical programming in Banach spaces. </title> <journal> Appl. Math. Optim., </journal> <volume> 5 </volume> <pages> 49-62, </pages> <year> 1979. </year> <month> 20 </month>
Reference-contexts: The regularity condition that we employ has been studied by many authors in various forms and contexts <ref> [1, 5, 13, 23, 27, 32, 37, 38, 39, 42, 46] </ref>. It is related to the stability of the set of solutions of the inclusion (5).
References-found: 46

