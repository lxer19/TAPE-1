URL: ftp://ftp.cs.washington.edu/pub/orca/papers/tr-97-04-05.ps
Refering-URL: http://www.cs.washington.edu/research/projects/zpl/papers/abstracts/icpp97.html
Root-URL: 
Email: fsungeun,snyderg@cs.washington.edu  
Title: Quantifying the Effects of Communication Optimizations optimizations perform in practice. Further, since a compiler cannot
Author: Sung-Eun Choi Lawrence Snyder 
Note: Many papers describe compiler optimizations for communication, but most appear before any performance numbers are available, so it is not generally known how well these  
Address: Box 352350  Seattle, WA 98195-2350  
Affiliation: Department of Computer Science and Engineering  University of Washington  
Abstract: University of Washington Technical Report #97-04-05, 1997. Abstract Using a specially constructed machine independent communication optimizer that allows control over optimization selection, we quantify the performance of three well known communication optimizations: redundant communication removal, communication combination, and communication pipelining. The numbers are shown relative to the base performance of benchmark programs using the standard communication optimization of message vectorization. The effects on static and dynamic communication call counts are tabulated. Though we consider a variety of communication primitives including those found in Intel's NX library, PVM and the T3D's SHMEM library, the majority of the experiments are run on the T3D using PVM and SHMEM. The results show substantial improvement, with two combinations of optimizations being most effective. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Gagan Agrawal and Joel Saltz. </author> <title> Interprocedural communication optimizations for distributed memory compilation. </title> <booktitle> In Workshop on Languages and Compilers for Parallel Computing, </booktitle> <pages> pages 283-299, </pages> <month> August </month> <year> 1994. </year>
Reference-contexts: 1 Introduction There exists a rich body of work in optimizing communication for array languages and in parallelizing compilers <ref> [1, 2, 10, 15, 13, 21] </ref>. There are fewer studies empirically evaluating communication optimizations in the context of a specific compiler and target machine [4, 14, 19].
Reference: [2] <author> S. Amarasinghe and M. Lam. </author> <title> Communication optimization and code generation for distributed memory machines. </title> <booktitle> In Proceedings of the SIGPLAN'93 Conference on Program Language Design and Implementation, </booktitle> <month> June </month> <year> 1993. </year>
Reference-contexts: 1 Introduction There exists a rich body of work in optimizing communication for array languages and in parallelizing compilers <ref> [1, 2, 10, 15, 13, 21] </ref>. There are fewer studies empirically evaluating communication optimizations in the context of a specific compiler and target machine [4, 14, 19].
Reference: [3] <author> Ray Barriuso and Allan Knies. </author> <title> SHMEM user's guide for C. </title> <type> Technical report, </type> <institution> Cray Research Inc., </institution> <month> June </month> <year> 1994. </year>
Reference-contexts: We then investigate the influence of machine characteristics on the optimizations using a synthetic benchmark program. Finally, we empirically evaluate the optimizations for four benchmark programs. 3.1 Methodology and Framework Experiments were run on two platforms: the Intel Paragon [9] and the Cray T3D <ref> [3] </ref> (see Figure 3). On the Paragon, we use the native NX communication library routines. On the T3D we use a vendor optimized version of PVM [12] and the native SHMEM [3] library routines. The synthetic benchmark was run on two node dedicated partitions on the Paragon and the T3D. <p> programs. 3.1 Methodology and Framework Experiments were run on two platforms: the Intel Paragon [9] and the Cray T3D <ref> [3] </ref> (see Figure 3). On the Paragon, we use the native NX communication library routines. On the T3D we use a vendor optimized version of PVM [12] and the native SHMEM [3] library routines. The synthetic benchmark was run on two node dedicated partitions on the Paragon and the T3D. All benchmark programs were run on 64 node dedicated partitions on the T3D. Measured deviations were under 1% and therefore will not be reported with each experiment.
Reference: [4] <author> Soumen Chakrabarti, Manish Gupta, and Jong-Deok Choi. </author> <title> Global communication analysis and optimization. </title> <booktitle> In SIGPLAN'96 Conference on Programming Language Design and Implementation, </booktitle> <year> 1996. </year>
Reference-contexts: 1 Introduction There exists a rich body of work in optimizing communication for array languages and in parallelizing compilers [1, 2, 10, 15, 13, 21]. There are fewer studies empirically evaluating communication optimizations in the context of a specific compiler and target machine <ref> [4, 14, 19] </ref>. Moreover, detailed performance evaluations of communication optimizations for non-kernel applications are virtually non fl This research was supported by ARPA Grant N00014-92-J-1824 existent, particularly with respect to optimizations that are performed in a machine independent manner.
Reference: [5] <author> Bradford L. Chamberlain, Sung-Eun Choi, and Lawrence Snyder. </author> <title> Ironman: An architecture indepedent communication interface for parallel computers. </title> <note> submitted for publication, </note> <year> 1996. </year>
Reference-contexts: Programs written in ZPL may use any communication mechanism on any target machine with a single source compilation. This is achieved by compiling communication to the Ironman communication interface <ref> [5] </ref>. To make the paper self-contained, we will briefly discuss the interface here and refer the reader to the paper [5] for more details. <p> Programs written in ZPL may use any communication mechanism on any target machine with a single source compilation. This is achieved by compiling communication to the Ironman communication interface <ref> [5] </ref>. To make the paper self-contained, we will briefly discuss the interface here and refer the reader to the paper [5] for more details.
Reference: [6] <author> Braford L. Chamberlain, Sung-Eun Choi, E Christopher Lewis, Calvin Lin, Lawrence Snyder, and W. Derrick Weathersby. Factor-Join: </author> <title> A unique approach to compiling array languages for parallel machines. </title> <booktitle> In Workshop on Languages and Compilers for Parallel Computing, </booktitle> <month> August </month> <year> 1996. </year>
Reference-contexts: All respectable par-allelizing compilers perform message vectorization. Compilers for array languages, on the other hand, can directly use arrays and array slices as the unit of representation <ref> [6] </ref>, eliminating the need for message vectorization. Thus the baseline of comparison in our experiments will be optimization using only message vectorization. Before describing the optimizations, we will briefly discuss the notation and assumptions.
Reference: [7] <author> Sung-Eun Choi, Lawrence Snyder, Joachim Stadel, and Tom Quinn. </author> <title> A portable parallel planetary integrator. </title> <note> in preparation, Septem-ber 1996. </note>
Reference-contexts: to those suitable for Fortran 90 and has been successfully used for real scientific and engineering applications communication timer machine library granularity Intel Paragon NX (message passing) ~100 ns 50 MHz Cray T3D PVM (message passing) ~150 ns 150 MHz SHMEM (shared memory) libraries for the Paragon and the T3D. <ref> [7, 11, 16, 20] </ref>. ZPL provides reductions, parallel prefix operators and other parallel operations, but for the purposes of this paper, we will concentrate on nearest-neighbor communication introduced by the shift operator, @. The language's semantics guarantee static detection of communication and allow us to concentrate on optimizations.
Reference: [8] <author> Keith D. Cooper, Mary W. Hall, and Linda Torczon. </author> <title> Unexpected side effects of inline substitution: A case study. </title> <journal> ACM Letters on Programming Languages and Systems, </journal> <volume> 1(1) </volume> <pages> 22-32, </pages> <month> March </month> <year> 1992. </year>
Reference-contexts: Another simple method that may increase the opportunities for optimization is to use procedure inlining. Cooper et al. studied inlining in the context of scientific applications <ref> [8] </ref>. Though they found that it was almost always detrimental to performance, the presence of communication was not considered. In addition, we are currently investigating the interaction of communication optimizations with other array language optimizations, such as array contraction, and more traditional optimizations such as loop fusion.
Reference: [9] <author> Intel Corporation. </author> <title> Paragon User's Guide. </title> <year> 1993. </year>
Reference-contexts: First, we discuss our methodology and the framework for evaluation. We then investigate the influence of machine characteristics on the optimizations using a synthetic benchmark program. Finally, we empirically evaluate the optimizations for four benchmark programs. 3.1 Methodology and Framework Experiments were run on two platforms: the Intel Paragon <ref> [9] </ref> and the Cray T3D [3] (see Figure 3). On the Paragon, we use the native NX communication library routines. On the T3D we use a vendor optimized version of PVM [12] and the native SHMEM [3] library routines.
Reference: [10] <author> Raja Das, Mustafa Uysal, Joel Saltz, and Yuan-Shin Hwang. </author> <title> Communication optimizations for irregular scientific computations on distributed memory architectures. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 22(3) </volume> <pages> 462-478, </pages> <month> September </month> <year> 1994. </year>
Reference-contexts: 1 Introduction There exists a rich body of work in optimizing communication for array languages and in parallelizing compilers <ref> [1, 2, 10, 15, 13, 21] </ref>. There are fewer studies empirically evaluating communication optimizations in the context of a specific compiler and target machine [4, 14, 19].
Reference: [11] <author> Marios D. Dikaiakos, Calvin Lin, Daphne Manoussaki, and Diana E. Woodward. </author> <title> The portable parallel implementation of two novel mathematical biology algorithms in ZPL. </title> <booktitle> In 9 th International Conference on Supercomputing, </booktitle> <year> 1995. </year>
Reference-contexts: to those suitable for Fortran 90 and has been successfully used for real scientific and engineering applications communication timer machine library granularity Intel Paragon NX (message passing) ~100 ns 50 MHz Cray T3D PVM (message passing) ~150 ns 150 MHz SHMEM (shared memory) libraries for the Paragon and the T3D. <ref> [7, 11, 16, 20] </ref>. ZPL provides reductions, parallel prefix operators and other parallel operations, but for the purposes of this paper, we will concentrate on nearest-neighbor communication introduced by the shift operator, @. The language's semantics guarantee static detection of communication and allow us to concentrate on optimizations.
Reference: [12] <author> A. Belguelin et al. </author> <title> A user's guide to PVM. </title> <type> Technical report, </type> <institution> Oak Ridge National Laboratories, </institution> <year> 1991. </year>
Reference-contexts: On the Paragon, we use the native NX communication library routines. On the T3D we use a vendor optimized version of PVM <ref> [12] </ref> and the native SHMEM [3] library routines. The synthetic benchmark was run on two node dedicated partitions on the Paragon and the T3D. All benchmark programs were run on 64 node dedicated partitions on the T3D.
Reference: [13] <author> Manish Gupta, Edith Schonberg, and Harini Srinivasan. </author> <title> A unified data-flow framework for optimizing communication. </title> <booktitle> In Workshop on Languages and Compilers for Parallel Computing, </booktitle> <pages> pages 266-282, </pages> <month> August </month> <year> 1994. </year>
Reference-contexts: 1 Introduction There exists a rich body of work in optimizing communication for array languages and in parallelizing compilers <ref> [1, 2, 10, 15, 13, 21] </ref>. There are fewer studies empirically evaluating communication optimizations in the context of a specific compiler and target machine [4, 14, 19].
Reference: [14] <author> Seema Hiranandani, Ken Kennedy, and Chau-Wen Tseng. </author> <title> Evaluating compiler optimizations for Fortran D. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 21(1) </volume> <pages> 27-45, </pages> <month> April </month> <year> 1994. </year>
Reference-contexts: 1 Introduction There exists a rich body of work in optimizing communication for array languages and in parallelizing compilers [1, 2, 10, 15, 13, 21]. There are fewer studies empirically evaluating communication optimizations in the context of a specific compiler and target machine <ref> [4, 14, 19] </ref>. Moreover, detailed performance evaluations of communication optimizations for non-kernel applications are virtually non fl This research was supported by ARPA Grant N00014-92-J-1824 existent, particularly with respect to optimizations that are performed in a machine independent manner.
Reference: [15] <author> Ken Kennedy and Nenad Nedeljkovic. </author> <title> Combining dependence and data-flow analyses to optimize communication. </title> <booktitle> In Proceedings of the 9th International Parallel Processing Symposium, </booktitle> <pages> pages 340-346, </pages> <month> April </month> <year> 1995. </year>
Reference-contexts: 1 Introduction There exists a rich body of work in optimizing communication for array languages and in parallelizing compilers <ref> [1, 2, 10, 15, 13, 21] </ref>. There are fewer studies empirically evaluating communication optimizations in the context of a specific compiler and target machine [4, 14, 19].
Reference: [16] <author> E Christopher Lewis, Calvin Lin, Lawrence Snyder, and George Turkiyyah. </author> <title> A portable parallel n-body solver. </title> <editor> In D. Bailey, P. Bjorstad, J. Gilbert, M. Mascagni, R. Schreiber, H. Si-mon, V. Torczon, and L. Watson, editors, </editor> <booktitle> Proceedings of the Seventh SIAM Conference on Parallel Processing for Scientific Computing, </booktitle> <pages> pages 331-336. </pages> <publisher> SIAM, </publisher> <year> 1995. </year>
Reference-contexts: to those suitable for Fortran 90 and has been successfully used for real scientific and engineering applications communication timer machine library granularity Intel Paragon NX (message passing) ~100 ns 50 MHz Cray T3D PVM (message passing) ~150 ns 150 MHz SHMEM (shared memory) libraries for the Paragon and the T3D. <ref> [7, 11, 16, 20] </ref>. ZPL provides reductions, parallel prefix operators and other parallel operations, but for the purposes of this paper, we will concentrate on nearest-neighbor communication introduced by the shift operator, @. The language's semantics guarantee static detection of communication and allow us to concentrate on optimizations.
Reference: [17] <author> Calvin Lin. </author> <title> ZPL language reference manual. </title> <type> Technical Report 94-10-06, </type> <institution> Department of Computer Science and Engineering, University of Washington, </institution> <year> 1994. </year>
Reference-contexts: Consequently, we empirically evaluate these optimizations using four benchmark programs for two modern parallel machines, the Intel Paragon and the Cray T3D and two communication mechanisms, message passing and one-way communication (i.e., T3D's SHMEM libraries). These benchmark programs are written in ZPL <ref> [17, 18] </ref>, a portable data parallel array language similar to the array subset of Fortran 90.
Reference: [18] <author> Calvin Lin and Lawrence Snyder. ZPL: </author> <title> An array sublanguage. </title> <editor> In Uptal Baner-jee, David Gelernter, Alexandru Nicolau, and David Padua, editors, </editor> <booktitle> Workshop on Languages and Compilers for Parallel Computing, </booktitle> <pages> pages 96-114. </pages> <publisher> Springer-Verlag, </publisher> <year> 1993. </year>
Reference-contexts: Consequently, we empirically evaluate these optimizations using four benchmark programs for two modern parallel machines, the Intel Paragon and the Cray T3D and two communication mechanisms, message passing and one-way communication (i.e., T3D's SHMEM libraries). These benchmark programs are written in ZPL <ref> [17, 18] </ref>, a portable data parallel array language similar to the array subset of Fortran 90.
Reference: [19] <author> Daniel J. Palermo, Ernesto Su, John A. Chandy, and Prithviraj Banerjee. </author> <title> Communication optimizations used in the PARADIGM compiler for distributed-memory multicomput-ers. </title> <booktitle> In International Conference on Paralle Processing, </booktitle> <pages> pages II:1-10, </pages> <month> August </month> <year> 1994. </year>
Reference-contexts: 1 Introduction There exists a rich body of work in optimizing communication for array languages and in parallelizing compilers [1, 2, 10, 15, 13, 21]. There are fewer studies empirically evaluating communication optimizations in the context of a specific compiler and target machine <ref> [4, 14, 19] </ref>. Moreover, detailed performance evaluations of communication optimizations for non-kernel applications are virtually non fl This research was supported by ARPA Grant N00014-92-J-1824 existent, particularly with respect to optimizations that are performed in a machine independent manner.
Reference: [20] <author> George Wilkey Richardson. </author> <title> Evaluation of a parallel chaos router simulator. </title> <type> Master's thesis, </type> <institution> The University of Arizona, Department of Electrical and Computer Engineering, </institution> <year> 1995. </year>
Reference-contexts: to those suitable for Fortran 90 and has been successfully used for real scientific and engineering applications communication timer machine library granularity Intel Paragon NX (message passing) ~100 ns 50 MHz Cray T3D PVM (message passing) ~150 ns 150 MHz SHMEM (shared memory) libraries for the Paragon and the T3D. <ref> [7, 11, 16, 20] </ref>. ZPL provides reductions, parallel prefix operators and other parallel operations, but for the purposes of this paper, we will concentrate on nearest-neighbor communication introduced by the shift operator, @. The language's semantics guarantee static detection of communication and allow us to concentrate on optimizations.
Reference: [21] <author> Reinhard von Hanxleden and Ken Kennedy. </author> <title> Give-N-Take a balanced code placement framework. </title> <booktitle> In SIGPLAN'94 Conference on Programming Language Design and Implementation, </booktitle> <pages> pages 107-120, </pages> <month> June </month> <year> 1994. </year>
Reference-contexts: 1 Introduction There exists a rich body of work in optimizing communication for array languages and in parallelizing compilers <ref> [1, 2, 10, 15, 13, 21] </ref>. There are fewer studies empirically evaluating communication optimizations in the context of a specific compiler and target machine [4, 14, 19].
References-found: 21

