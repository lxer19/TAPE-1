URL: http://www.cs.iastate.edu/tech-reports/TR96-05.ps
Refering-URL: http://www.cs.iastate.edu/tech-reports/catalog.html
Root-URL: http://www.cs.iastate.edu
Title: Compression Depth and the Behavior of Cellular Automata 1  
Author: James I. Lathrop 
Address: Ames, IA 50011  
Affiliation: Department of Computer Science Iowa State University  
Abstract: A computable complexity measure analogous to computational depth is developed using the Lempel-Ziv compression algorithm. This complexity measure, which we call compression depth, is then applied to the computational output of cellular automata. We find that compression depth captures the complexity found in Wolfram Class III celluar automata, and is in good agreement with his classification scheme. We further investigate the rule space of cellular automata using Langton's parameter. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> L. Adleman. </author> <title> Time, space, and randomness. </title> <type> Technical Report MIT/LCS/79/TM-131, </type> <institution> Massachusettes Institute of Technology, Laboratory for Computer Science, </institution> <month> March </month> <year> 1979. </year>
Reference-contexts: shortest description. (Precise definitions appear below.) Since this shortest description contains all the information in the object, the depth thus represents the amount of "computational work" that has been "added" to this information and "stored in the organization" of the object. (Depth is closely related to Adleman's notion of "potential" <ref> [1] </ref> and Koppel's notion of "sophistication" [14].) Definition (Bennett [2, 3]) Let x 2 f0; 1g fl be a string, and let s 2 N be a significance parameter.
Reference: [2] <author> C. H. Bennett. </author> <title> Dissipation, information, computational complexity and the definition of organization. </title> <editor> In D. Pines, editor, </editor> <booktitle> Emerging Syntheses in Science, Proceedings of the Founding Workshops of the Santa Fe Institute, </booktitle> <pages> pages 297-313, </pages> <year> 1985. </year>
Reference-contexts: Many researchers have defined measures of complexity that attempt to capture specific types of organization or information. Typical proposals for such a measure are specific and designed to measure an object's complexity under a restricted model or domain. (See [8, 10, 11, 24, 22] for example.) However, Bennett <ref> [2, 3] </ref> has defined a complexity measure based on programs for universal Turing machines that does capture the desired complexity criteria and is universal for all objects that can be digitally encoded. Under Bennett's definition, the computational depth [2, 3] of a binary data string is roughly the amount of time <p> or domain. (See [8, 10, 11, 24, 22] for example.) However, Bennett <ref> [2, 3] </ref> has defined a complexity measure based on programs for universal Turing machines that does capture the desired complexity criteria and is universal for all objects that can be digitally encoded. Under Bennett's definition, the computational depth [2, 3] of a binary data string is roughly the amount of time required to generate the string from a description of the string of nearly minimal length. (A parameter s is used to define "nearly minimal" in section 3.) A description of an object contains all the essential information required <p> Thus, computational depth is the amount of organization embedded in a string by a computation. Computational depth appears to be an ideal complexity measure for determining whether an object contains intricate structure. For example, Bennett <ref> [2, 3] </ref> notes that objects with simple structures such as strings consisting of all zeros or strings composed of random bits are not deep. It is easy to show that strings that have 2 maximal information content, using entropy or Kolmogorov comlexity as a measure, are not strongly deep. <p> Roughly, the compression depth of a string x is the amount of resource required to compress x to within some number of bits of the smallest compression of x. (In the terminology of Bennett <ref> [2, 3] </ref>, a string with high compression depth is said to be cryptic.) Unlike Bennett's notion of computational depth, the resource is not required to be time, but may be any resource whose restriction impairs the performance of a compression algorithm, thereby parameterizing the amount of compression in terms of the <p> Motivated by Ben-nett's notion of computational depth <ref> [2, 3] </ref>, compression depth is based on well-known compression algorithms that quickly compress data. <p> Because compression depth is motivated by computational depth, we give a brief description of computational depth here. The interested reader may read the papers by Bennett <ref> [2, 3] </ref> or Juedes, Lathrop, and Lutz [12] for more in-depth and detailed analyses of computational depth and its properties. Roughly speaking, the computational depth (called "logical depth" by Bennett [2, 3]) of an object is the amount of time required for an algorithm to derive the object from its shortest <p> The interested reader may read the papers by Bennett <ref> [2, 3] </ref> or Juedes, Lathrop, and Lutz [12] for more in-depth and detailed analyses of computational depth and its properties. Roughly speaking, the computational depth (called "logical depth" by Bennett [2, 3]) of an object is the amount of time required for an algorithm to derive the object from its shortest description. (Precise definitions appear below.) Since this shortest description contains all the information in the object, the depth thus represents the amount of "computational work" that has been "added" to <p> description contains all the information in the object, the depth thus represents the amount of "computational work" that has been "added" to this information and "stored in the organization" of the object. (Depth is closely related to Adleman's notion of "potential" [1] and Koppel's notion of "sophistication" [14].) Definition (Bennett <ref> [2, 3] </ref>) Let x 2 f0; 1g fl be a string, and let s 2 N be a significance parameter. The depth of the string x at significance level s, is the number depth s (x) = max n o where we use the convention that max ; = 0. <p> In contrast with the two examples described above, the characteristic sequence of the halting language, denoted H , is an example of a sequence that has a high depth 8 measure. (This was proven by Bennett <ref> [2, 3] </ref> and generalized by Juedes, Lathrop, and Lutz [12].) Consider the first n bits of this sequence, namely the string H [0::n 1]. This string can be recovered exactly from a program that encodes the length of the string and the number of ones contained in the string. <p> One way to proceed is to consider the "reverse" of time-bounded Kolmogorov complexity by formulating a complexity measure based on the time required to compress x to its shortest description. (This is similar to Bennett's notion of cryptic <ref> [2, 3] </ref>.) Using compression as the basis for a depth-like measurement gives the following approach to defining the compression 9 depth of a string x. Definition.
Reference: [3] <author> C. H. Bennett. </author> <title> Logical depth and physical complexity. </title> <editor> In R. Herken, editor, </editor> <booktitle> The Universal Turing Machine: A Half-Century Survey, </booktitle> <pages> pages 227-257. </pages> <publisher> Oxford University Press, </publisher> <year> 1988. </year>
Reference-contexts: Many researchers have defined measures of complexity that attempt to capture specific types of organization or information. Typical proposals for such a measure are specific and designed to measure an object's complexity under a restricted model or domain. (See [8, 10, 11, 24, 22] for example.) However, Bennett <ref> [2, 3] </ref> has defined a complexity measure based on programs for universal Turing machines that does capture the desired complexity criteria and is universal for all objects that can be digitally encoded. Under Bennett's definition, the computational depth [2, 3] of a binary data string is roughly the amount of time <p> or domain. (See [8, 10, 11, 24, 22] for example.) However, Bennett <ref> [2, 3] </ref> has defined a complexity measure based on programs for universal Turing machines that does capture the desired complexity criteria and is universal for all objects that can be digitally encoded. Under Bennett's definition, the computational depth [2, 3] of a binary data string is roughly the amount of time required to generate the string from a description of the string of nearly minimal length. (A parameter s is used to define "nearly minimal" in section 3.) A description of an object contains all the essential information required <p> Thus, computational depth is the amount of organization embedded in a string by a computation. Computational depth appears to be an ideal complexity measure for determining whether an object contains intricate structure. For example, Bennett <ref> [2, 3] </ref> notes that objects with simple structures such as strings consisting of all zeros or strings composed of random bits are not deep. It is easy to show that strings that have 2 maximal information content, using entropy or Kolmogorov comlexity as a measure, are not strongly deep. <p> Roughly, the compression depth of a string x is the amount of resource required to compress x to within some number of bits of the smallest compression of x. (In the terminology of Bennett <ref> [2, 3] </ref>, a string with high compression depth is said to be cryptic.) Unlike Bennett's notion of computational depth, the resource is not required to be time, but may be any resource whose restriction impairs the performance of a compression algorithm, thereby parameterizing the amount of compression in terms of the <p> Motivated by Ben-nett's notion of computational depth <ref> [2, 3] </ref>, compression depth is based on well-known compression algorithms that quickly compress data. <p> Because compression depth is motivated by computational depth, we give a brief description of computational depth here. The interested reader may read the papers by Bennett <ref> [2, 3] </ref> or Juedes, Lathrop, and Lutz [12] for more in-depth and detailed analyses of computational depth and its properties. Roughly speaking, the computational depth (called "logical depth" by Bennett [2, 3]) of an object is the amount of time required for an algorithm to derive the object from its shortest <p> The interested reader may read the papers by Bennett <ref> [2, 3] </ref> or Juedes, Lathrop, and Lutz [12] for more in-depth and detailed analyses of computational depth and its properties. Roughly speaking, the computational depth (called "logical depth" by Bennett [2, 3]) of an object is the amount of time required for an algorithm to derive the object from its shortest description. (Precise definitions appear below.) Since this shortest description contains all the information in the object, the depth thus represents the amount of "computational work" that has been "added" to <p> description contains all the information in the object, the depth thus represents the amount of "computational work" that has been "added" to this information and "stored in the organization" of the object. (Depth is closely related to Adleman's notion of "potential" [1] and Koppel's notion of "sophistication" [14].) Definition (Bennett <ref> [2, 3] </ref>) Let x 2 f0; 1g fl be a string, and let s 2 N be a significance parameter. The depth of the string x at significance level s, is the number depth s (x) = max n o where we use the convention that max ; = 0. <p> In contrast with the two examples described above, the characteristic sequence of the halting language, denoted H , is an example of a sequence that has a high depth 8 measure. (This was proven by Bennett <ref> [2, 3] </ref> and generalized by Juedes, Lathrop, and Lutz [12].) Consider the first n bits of this sequence, namely the string H [0::n 1]. This string can be recovered exactly from a program that encodes the length of the string and the number of ones contained in the string. <p> One way to proceed is to consider the "reverse" of time-bounded Kolmogorov complexity by formulating a complexity measure based on the time required to compress x to its shortest description. (This is similar to Bennett's notion of cryptic <ref> [2, 3] </ref>.) Using compression as the basis for a depth-like measurement gives the following approach to defining the compression 9 depth of a string x. Definition.
Reference: [4] <author> G. J. Chaitin. </author> <title> On the length of programs for computing finite binary sequences. </title> <journal> Journal of the Association for Computing Machinery, </journal> <volume> 13 </volume> <pages> 547-569, </pages> <year> 1966. </year>
Reference-contexts: A string x is a proper prefix of y, denoted by x &lt; y, if and only if x v y and jxj &lt; jyj. Kolmogorov complexity, also called program-size complexity, was discovered independently by Solomonoff [23], Kolmogorov [13], and Chaitin <ref> [4] </ref>. Self-delimiting Kol-mogorov complexity is a technical improvement of the original formulation that was developed independently, in slightly different forms, by Levin [18, 19], Schnorr [20], and Chaitin [5]. The advantage of the self-delimiting version is that it gives precise characterizations of algorithmic probability and randomness.
Reference: [5] <author> G. J. Chaitin. </author> <title> A theory of program size formally identical to information theory. </title> <journal> Journal of the Association for Computing Machinery, </journal> <volume> 22 </volume> <pages> 329-340, </pages> <year> 1975. </year>
Reference-contexts: Kolmogorov complexity, also called program-size complexity, was discovered independently by Solomonoff [23], Kolmogorov [13], and Chaitin [4]. Self-delimiting Kol-mogorov complexity is a technical improvement of the original formulation that was developed independently, in slightly different forms, by Levin [18, 19], Schnorr [20], and Chaitin <ref> [5] </ref>. The advantage of the self-delimiting version is that it gives precise characterizations of algorithmic probability and randomness. In this paper, in order to simplify the presentation of compression depth, we very briefly develop the elements of Kolmogorov complexity and algorithmic information.
Reference: [6] <author> T. M. Cover and J. A. Thomas. </author> <title> Elements of Information Theory. </title> <publisher> John Wiley & Sons, Inc., </publisher> <address> New York, </address> <year> 1991. </year>
Reference: [7] <author> J. P. Crutchfield, P. T. Hraber, and M. Mitchell. </author> <title> Revisiting the edge of chaos: Evolving cellular automata to perform computations. </title> <journal> Complex Systems, </journal> <volume> 7 </volume> <pages> 89-130, </pages> <year> 1993. </year>
Reference-contexts: It is arguable that diversity is also a necessary condition for universality as well as arbitrarily long transient lengths. Even though the results here differ from Langton's, results by Mitchell, Hraber, and Crutchfield <ref> [7] </ref> agree with the results presented here. Crutchfield, Hraber, and Mitchel used genetic algorithms to evolve rules for cellular automata with fitness functions that encourage the attributes required for universal computation. The genetic algorithm evolved transition functions with parameters both above and below Langton's phase transition.
Reference: [8] <author> G. d'Alessandro and A. Politi. </author> <title> Hierarchical approach to complexity with applications to dynamic systems. </title> <journal> Physics Review Letters, </journal> <volume> 64 </volume> <pages> 1609-1612, </pages> <year> 1990. </year>
Reference-contexts: Many researchers have defined measures of complexity that attempt to capture specific types of organization or information. Typical proposals for such a measure are specific and designed to measure an object's complexity under a restricted model or domain. (See <ref> [8, 10, 11, 24, 22] </ref> for example.) However, Bennett [2, 3] has defined a complexity measure based on programs for universal Turing machines that does capture the desired complexity criteria and is universal for all objects that can be digitally encoded.
Reference: [9] <author> J. Gorodkin, A. Sorensen, and O. Winther. </author> <title> Neural networks and cellular automata complexity. </title> <journal> Complex Systems, </journal> <volume> 7 </volume> <pages> 1-23, </pages> <year> 1993. </year>
Reference-contexts: contained mostly conjectures, quali 23 tative observations, and few quantitative measurements on the behavior of cellular automata, his profound observation that cellular automata could be classified into four distinct types has provided the impetus for a significant body of work investigating the complexity and dynamical behavior of cellular automata. (See <ref> [26, 10, 9, 16] </ref> for example.) Wolfram divided the types of patterns that evolve in cellular automata into four basic classes. In Wolfram's own words, these four classes are described qualitatively as follows. I) Evolution leads to a homogeneous state.
Reference: [10] <author> P. Grassberger. </author> <title> Problems in quantifying self-organized complexity. </title> <journal> Helvetica Physica Acta, </journal> <volume> 62 </volume> <pages> 498-511, </pages> <year> 1989. </year>
Reference-contexts: Many researchers have defined measures of complexity that attempt to capture specific types of organization or information. Typical proposals for such a measure are specific and designed to measure an object's complexity under a restricted model or domain. (See <ref> [8, 10, 11, 24, 22] </ref> for example.) However, Bennett [2, 3] has defined a complexity measure based on programs for universal Turing machines that does capture the desired complexity criteria and is universal for all objects that can be digitally encoded. <p> contained mostly conjectures, quali 23 tative observations, and few quantitative measurements on the behavior of cellular automata, his profound observation that cellular automata could be classified into four distinct types has provided the impetus for a significant body of work investigating the complexity and dynamical behavior of cellular automata. (See <ref> [26, 10, 9, 16] </ref> for example.) Wolfram divided the types of patterns that evolve in cellular automata into four basic classes. In Wolfram's own words, these four classes are described qualitatively as follows. I) Evolution leads to a homogeneous state.
Reference: [11] <author> R. Gunther, B. Shapiro, and P. Wagner. </author> <title> Complex systems, complexity measures, grammars and model inferring. Chaos, </title> <booktitle> Solitons and Fractals, </booktitle> <volume> 4 </volume> <pages> 635-651, </pages> <year> 1994. </year>
Reference-contexts: Many researchers have defined measures of complexity that attempt to capture specific types of organization or information. Typical proposals for such a measure are specific and designed to measure an object's complexity under a restricted model or domain. (See <ref> [8, 10, 11, 24, 22] </ref> for example.) However, Bennett [2, 3] has defined a complexity measure based on programs for universal Turing machines that does capture the desired complexity criteria and is universal for all objects that can be digitally encoded.
Reference: [12] <author> D. W. Juedes, J. I. Lathrop, and J. H. Lutz. </author> <title> Computational depth and reducibility. </title> <journal> Theoretical Computer Science, </journal> <volume> 132 </volume> <pages> 37-70, </pages> <year> 1994. </year> <month> 35 </month>
Reference-contexts: Bennett also shows that the characteristic sequence of the halting problem is strongly deep, reflecting its very intricate structure. Further evidence that computational depth measures structural organization is given by Juedes, Lathrop, and Lutz <ref> [12] </ref> who have shown that, if an object can be used to speed up the computations of a significant collection of recursive sequences, then that object must be strongly deep. Unfortunately, an essential feature in the definition of computational depth is Kol-mogorov complexity, an uncomputable quantity. <p> Because compression depth is motivated by computational depth, we give a brief description of computational depth here. The interested reader may read the papers by Bennett [2, 3] or Juedes, Lathrop, and Lutz <ref> [12] </ref> for more in-depth and detailed analyses of computational depth and its properties. <p> In contrast with the two examples described above, the characteristic sequence of the halting language, denoted H , is an example of a sequence that has a high depth 8 measure. (This was proven by Bennett [2, 3] and generalized by Juedes, Lathrop, and Lutz <ref> [12] </ref>.) Consider the first n bits of this sequence, namely the string H [0::n 1]. This string can be recovered exactly from a program that encodes the length of the string and the number of ones contained in the string.
Reference: [13] <author> A. N. </author> <title> Kolmogorov. Three approaches to the quantitative definition of `informa-tion'. </title> <journal> Problems of Information Transmission, </journal> <volume> 1 </volume> <pages> 1-7, </pages> <year> 1965. </year>
Reference-contexts: A string x is a proper prefix of y, denoted by x &lt; y, if and only if x v y and jxj &lt; jyj. Kolmogorov complexity, also called program-size complexity, was discovered independently by Solomonoff [23], Kolmogorov <ref> [13] </ref>, and Chaitin [4]. Self-delimiting Kol-mogorov complexity is a technical improvement of the original formulation that was developed independently, in slightly different forms, by Levin [18, 19], Schnorr [20], and Chaitin [5]. The advantage of the self-delimiting version is that it gives precise characterizations of algorithmic probability and randomness.
Reference: [14] <author> M. Koppel. </author> <title> Structure. </title> <editor> In R. Herken, editor, </editor> <booktitle> The Universal Turing Machine: A Half-Century Survey, </booktitle> <pages> pages 435-452. </pages> <publisher> Oxford University Press, Oxford, </publisher> <year> 1988. </year>
Reference-contexts: Since this shortest description contains all the information in the object, the depth thus represents the amount of "computational work" that has been "added" to this information and "stored in the organization" of the object. (Depth is closely related to Adleman's notion of "potential" [1] and Koppel's notion of "sophistication" <ref> [14] </ref>.) Definition (Bennett [2, 3]) Let x 2 f0; 1g fl be a string, and let s 2 N be a significance parameter.
Reference: [15] <author> Martin Kummer. </author> <title> On the complexity of random strings. </title> <booktitle> In 13th Annual Symposium on Theoretical Aspects of Computer Science, </booktitle> <pages> pages 25-36. </pages> <publisher> Springer, </publisher> <year> 1996. </year>
Reference-contexts: For example, consider a string 7 x such that K (x) jxj. (A simple counting argument shows that, for all n, at least one string of length n has this property. In fact, a number of researchers <ref> [15] </ref> have independently shown that, for all sufficiently large n, at least 2 nc of the strings of length n have this property, where c is a constant that does not depend on n.) Since there is a very fast program of length jxj + 2 log jxj + C that
Reference: [16] <author> C. G. Langton. </author> <title> Computation at the edge of chaos. </title> <journal> Physica D, </journal> <volume> 42 </volume> <pages> 12-37, </pages> <year> 1990. </year>
Reference-contexts: We demonstrate this usefulness by investigating the compression depth of cellular automata in classes that have been defined and investigated by Wolfram [25] and Langton <ref> [16] </ref>. <p> Our experiments show that many Class IV automata also have large compression 3 depth, confirming that compression depth appears to measure some type of structure or complexity found in these types of cellular automata. Further experiments are performed on cellular automata using Langton's -parameter. Langton <ref> [16] </ref> defines a method that imposes a structure and ordering on the set of transition functions for all "legal" cellular automata. This allows Langton to define a single parameter, , which he uses to study the behavior of cellular automata. <p> contained mostly conjectures, quali 23 tative observations, and few quantitative measurements on the behavior of cellular automata, his profound observation that cellular automata could be classified into four distinct types has provided the impetus for a significant body of work investigating the complexity and dynamical behavior of cellular automata. (See <ref> [26, 10, 9, 16] </ref> for example.) Wolfram divided the types of patterns that evolve in cellular automata into four basic classes. In Wolfram's own words, these four classes are described qualitatively as follows. I) Evolution leads to a homogeneous state. <p> For S = 8 and N = 5 there are 2 32768 different transition functions. Langton <ref> [16] </ref> asked whether there was a way to partition this set so that transition functions in the same partition supported the same type of dynamic behavior in cellular automata. One obvious set of partitions would classify the four groups of dynamic behavior defined and observed by Wolfram. Langton [16] defined a <p> Langton <ref> [16] </ref> asked whether there was a way to partition this set so that transition functions in the same partition supported the same type of dynamic behavior in cellular automata. One obvious set of partitions would classify the four groups of dynamic behavior defined and observed by Wolfram. Langton [16] defined a parameter as one possible method for ordering the rule space of a large class of cellular automata. With this ordering, Langton found that transition functions with lower values of evolved patterns that belonged to Wolfram's Class I and Class II cellular automata. <p> The transistion function used for his experiment depended on of the cell itself, together with the two cells to the left and right, and S contained 4 states. The following observations describing Langton's experiment are paraphrased from his paper <ref> [16] </ref>. 0:00 &lt; 0:15 All dynamic activity dies after at most 7 time steps. The cells of the cellular automata all enter the same state. 0:20 &lt; 0:40 Cellular automata can support simple periodic structures. <p> Compute the estimated entropy for each state using these probability distribu tions. 5. Average the entropy values to compute the value of the average entropy per cell. It is assumed that Langton's method was similar, although it was not explicitly stated in his paper <ref> [16] </ref>. Experiments by the author to verify Langton's results were performed using one-dimensional cellular automata with 10; 000 cells. The average entropy per cell versus is shown in Figure 16 with 1; 000 simulations per value of for a total of 20; 000 simulations.
Reference: [17] <author> A. Lempel and J. Ziv. </author> <title> Compression of individual sequences via variable rate coding. </title> <journal> IEEE Transaction on Information Theory, </journal> <volume> 24 </volume> <pages> 530-536, </pages> <year> 1978. </year>
Reference-contexts: This can be very large, obscuring any compression of the string. 3.2 Lempel-Ziv Compression and LZ Depth Lempel-Ziv compression, first introduced by Lempel and Ziv <ref> [17] </ref>, provides a good and efficient compression algorithm that can be parameterized without suffering from the blocking effects associated with Huffman encoding. Many variations of this original algorithm have since been introduced that run faster and with better compression. <p> Many variations of this original algorithm have since been introduced that run faster and with better compression. However, these improvements are small and the asymptotic performance of these algorithms is no better than the original Lempel-Ziv algorithm <ref> [17] </ref>. There are many variations of the Lempel-Ziv algorithm and an even wider variety of implementations. However, this paper utilizes the original Lempel-Ziv (LZ) algorithm for simplicity. This section describes the original algorithm and gives two examples.
Reference: [18] <author> L. A. Levin. </author> <title> On the notion of a random sequence. </title> <journal> Soviet Mathematics Doklady, </journal> <volume> 14 </volume> <pages> 1413-1416, </pages> <year> 1973. </year>
Reference-contexts: Kolmogorov complexity, also called program-size complexity, was discovered independently by Solomonoff [23], Kolmogorov [13], and Chaitin [4]. Self-delimiting Kol-mogorov complexity is a technical improvement of the original formulation that was developed independently, in slightly different forms, by Levin <ref> [18, 19] </ref>, Schnorr [20], and Chaitin [5]. The advantage of the self-delimiting version is that it gives precise characterizations of algorithmic probability and randomness. In this paper, in order to simplify the presentation of compression depth, we very briefly develop the elements of Kolmogorov complexity and algorithmic information.
Reference: [19] <author> L. A. Levin. </author> <title> Laws of information conservation (nongrowth) and aspects of the foundation of probability theory. </title> <journal> Problems of Information Transmission, </journal> <volume> 10 </volume> <pages> 206-210, </pages> <year> 1974. </year>
Reference-contexts: Kolmogorov complexity, also called program-size complexity, was discovered independently by Solomonoff [23], Kolmogorov [13], and Chaitin [4]. Self-delimiting Kol-mogorov complexity is a technical improvement of the original formulation that was developed independently, in slightly different forms, by Levin <ref> [18, 19] </ref>, Schnorr [20], and Chaitin [5]. The advantage of the self-delimiting version is that it gives precise characterizations of algorithmic probability and randomness. In this paper, in order to simplify the presentation of compression depth, we very briefly develop the elements of Kolmogorov complexity and algorithmic information.
Reference: [20] <author> C. P. Schnorr. </author> <title> Process complexity and effective random tests. </title> <journal> Journal of Computer and System Sciences, </journal> <volume> 7 </volume> <pages> 376-388, </pages> <year> 1973. </year>
Reference-contexts: Kolmogorov complexity, also called program-size complexity, was discovered independently by Solomonoff [23], Kolmogorov [13], and Chaitin [4]. Self-delimiting Kol-mogorov complexity is a technical improvement of the original formulation that was developed independently, in slightly different forms, by Levin [18, 19], Schnorr <ref> [20] </ref>, and Chaitin [5]. The advantage of the self-delimiting version is that it gives precise characterizations of algorithmic probability and randomness. In this paper, in order to simplify the presentation of compression depth, we very briefly develop the elements of Kolmogorov complexity and algorithmic information.
Reference: [21] <author> C. E. Shannon. </author> <title> A mathematical theory of communication. </title> <journal> Bell System Technical Journal, </journal> <volume> 27 </volume> <pages> 379-423, 623-656, </pages> <year> 1948. </year>
Reference-contexts: The existence of a phase transition was shown by Langton for two-dimensional cellular automata. His results are verified in the present paper using one- dimensional cellular automata without any restrictions on the transition function. Definition The Shannon entropy <ref> [21] </ref>, or the information content, of a cell with state 28 set S is defined as H (S) = s2S where Pr (s) is the probability that at any time t, the cell is in state s.
Reference: [22] <author> M.C. Shea. </author> <title> Complexity and evolution: what everybody knows. </title> <journal> Biology and Philosophy, </journal> <volume> 6 </volume> <pages> 303-304, </pages> <year> 1991. </year>
Reference-contexts: Many researchers have defined measures of complexity that attempt to capture specific types of organization or information. Typical proposals for such a measure are specific and designed to measure an object's complexity under a restricted model or domain. (See <ref> [8, 10, 11, 24, 22] </ref> for example.) However, Bennett [2, 3] has defined a complexity measure based on programs for universal Turing machines that does capture the desired complexity criteria and is universal for all objects that can be digitally encoded.
Reference: [23] <author> R. J. Solomonoff. </author> <title> A formal theory of inductive inference. </title> <journal> Information and Control, </journal> <volume> 7 </volume> <pages> 1-22, 224-254, </pages> <year> 1964. </year>
Reference-contexts: A string x is a proper prefix of y, denoted by x &lt; y, if and only if x v y and jxj &lt; jyj. Kolmogorov complexity, also called program-size complexity, was discovered independently by Solomonoff <ref> [23] </ref>, Kolmogorov [13], and Chaitin [4]. Self-delimiting Kol-mogorov complexity is a technical improvement of the original formulation that was developed independently, in slightly different forms, by Levin [18, 19], Schnorr [20], and Chaitin [5].
Reference: [24] <author> B.L. Lipmanand S. Srivastava. </author> <title> Informational requirments and strategic complexity in repeated games. </title> <journal> Games and Economic Behaviour, </journal> <volume> 2 </volume> <pages> 273-290, </pages> <year> 1990. </year>
Reference-contexts: Many researchers have defined measures of complexity that attempt to capture specific types of organization or information. Typical proposals for such a measure are specific and designed to measure an object's complexity under a restricted model or domain. (See <ref> [8, 10, 11, 24, 22] </ref> for example.) However, Bennett [2, 3] has defined a complexity measure based on programs for universal Turing machines that does capture the desired complexity criteria and is universal for all objects that can be digitally encoded.
Reference: [25] <author> S. Wolfram. </author> <title> Universality and complexity in cellular automata. </title> <journal> Physica D, </journal> <volume> 10 </volume> <pages> 1-35, </pages> <year> 1984. </year>
Reference-contexts: We demonstrate this usefulness by investigating the compression depth of cellular automata in classes that have been defined and investigated by Wolfram <ref> [25] </ref> and Langton [16]. <p> These pictures are then easily viewed by assigning colors to the different states of the transition function. In this paper, we assign state 0 to be white and all other states to be black. Using these pictures, Wolfram <ref> [25] </ref> defined four classes of cellular automata based on the patterns they produced.
Reference: [26] <author> S. Wolfram. </author> <title> Complex systems theory. </title> <booktitle> In Emerging Syntheses in Science. </booktitle> <publisher> Addison-Wesley, </publisher> <year> 1987. </year> <month> 36 </month>
Reference-contexts: contained mostly conjectures, quali 23 tative observations, and few quantitative measurements on the behavior of cellular automata, his profound observation that cellular automata could be classified into four distinct types has provided the impetus for a significant body of work investigating the complexity and dynamical behavior of cellular automata. (See <ref> [26, 10, 9, 16] </ref> for example.) Wolfram divided the types of patterns that evolve in cellular automata into four basic classes. In Wolfram's own words, these four classes are described qualitatively as follows. I) Evolution leads to a homogeneous state.
References-found: 26

