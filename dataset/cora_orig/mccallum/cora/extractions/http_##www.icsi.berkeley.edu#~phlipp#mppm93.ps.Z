URL: http://www.icsi.berkeley.edu/~phlipp/mppm93.ps.Z
Refering-URL: http://www.icsi.berkeley.edu/~phlipp/phlipp.publ.html
Root-URL: http://www.icsi.berkeley.edu
Email: ira.uka.de  
Title: Proceedings of the Conference on Massively Parallel Programming Models  The Modula-2* Environment for Parallel Programming  
Author: Stefan U. Hangen, Ernst A. Heinz, Paul Lukowicz, Michael Philippsen, Walter F. Tichy (haensgenj heinzej lukowiczj phlippj tichy)@ 
Address: D-76128 Karlsruhe, F.R.G.  
Affiliation: Universitat Karlsruhe Fakultat fur Informatik  
Date: September 20-22, 1993  
Note: pages 43-52, Berlin, Germany,  
Abstract: This paper presents a portable parallel programming environment for Modula-2* an explicitly parallel machine-independent extension of Modula-2. Modula-2* offers synchronous and asynchronous parallelism, a global single address space, and automatic data and process distribution. The Modula-2* system consists of a compiler, a debugger, a cross-architecture make, a runtime systems for different machines, and a set of scalable parallel libraries. Implementations exist for the MasPar MP series of massively parallel processors (SIMD), the KSR-1 parallel computer (MIMD), heterogeneous LANs of workstations (MIMD), and single workstations (SISD). The paper presents the important components of the Modula-2* environment and discusses selected implementation issues. We focus on how we achieve a high degree of portability for our system while at the same time ensuring efficiency. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Selim G. Akl. </author> <title> The Design and Analysis of Parallel Algorithms. </title> <publisher> Prentice Hall, </publisher> <address> Englewood Cliffs, New Jersey, </address> <year> 1989. </year>
Reference-contexts: MasPar plural static struct testS__1 -CARDINAL A <ref> [1] </ref>-X; plural static dispatcher () - _ParProcStart_ (); _ParItem_ (Lwp_1_ID,Lwp_1); _ParProcEnd_ (); - void BEGIN_MODULE ()- /* begin main program */ _InitPEs_ (); - /* begin FORALL IN SYNC */ /* some declarations omitted here */ plural LONGCARD H_1_i [1]; /* FORALL initialization */ FLow_i = _thispe_ ()+1; FHi_i = p_0_MIN (_thispe_ ()+1,1024); _InitBounds_ (FHi_i,FLow_i,FNum_i); for (IR_i=0,i=FLow_i;i&lt;=FHi_i;i++,IR_i++) -Num_i = 0;AB_i [IR_i] = 0;-_Sync_ (0); /* virtualization loops */ for (IR_i=0,i=FLow_i;i&lt;=FHi_i;i++,IR_i++)- plural LONGCARD L_1; /* get X [i+1] */ _GetGlobalPT_ (L_1, _pl_ ((i + 1),16383,0),X.A [0]); H_1_i [IR_i] = L_1;- -_Sync_ <p> Another interesting feature of these libraries is their functional diversity. Wherever possible, normal, masked, segmented, and universal (masked plus segmented) versions of the parallel operations are provided. 4 Benchmark Results Our benchmark suite consists of 13 problems collected from literature <ref> [1, 2, 3, 5, 7] </ref>. For each problem we implemented the same algorithm in Modula-2*, in MPL 3 , and in sequential C. Then we measured the runtimes of our implementations on a 16K MasPar MP-1 and a Sparc-1 for widely ranging problem sizes.
Reference: [2] <author> John T. Feo, </author> <title> editor. A Comparative Study of Parallel Programming Languages: The Salishan Problems. </title> <publisher> Elsevier Science Publishers, Holland, </publisher> <year> 1992. </year>
Reference-contexts: Another interesting feature of these libraries is their functional diversity. Wherever possible, normal, masked, segmented, and universal (masked plus segmented) versions of the parallel operations are provided. 4 Benchmark Results Our benchmark suite consists of 13 problems collected from literature <ref> [1, 2, 3, 5, 7] </ref>. For each problem we implemented the same algorithm in Modula-2*, in MPL 3 , and in sequential C. Then we measured the runtimes of our implementations on a 16K MasPar MP-1 and a Sparc-1 for widely ranging problem sizes.
Reference: [3] <author> Alan Gibbons and Wojciech Rytter. </author> <title> Efficient Parallel Algorithms. </title> <publisher> Cambridge University Press, </publisher> <year> 1988. </year> <title> 0 20 40 60 u b r f e s r m n s 10 30 50 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0 relative performance cumulative, relative performance distribution t(c)/t(m2*) on seq. SUN-4 t(mpl)/t(m2*) on MasPar </title>
Reference-contexts: Another interesting feature of these libraries is their functional diversity. Wherever possible, normal, masked, segmented, and universal (masked plus segmented) versions of the parallel operations are provided. 4 Benchmark Results Our benchmark suite consists of 13 problems collected from literature <ref> [1, 2, 3, 5, 7] </ref>. For each problem we implemented the same algorithm in Modula-2*, in MPL 3 , and in sequential C. Then we measured the runtimes of our implementations on a 16K MasPar MP-1 and a Sparc-1 for widely ranging problem sizes.
Reference: [4] <author> Stefan U. Hangen. </author> <title> Ein symbolischer X-Windows Debugger fur Modula-2*. </title> <type> Master's thesis, </type> <institution> University of Karlsruhe, Department of Informatics, </institution> <month> December </month> <year> 1992. </year>
Reference-contexts: These are all taken care of by the compiler. This observation shifts the focus of debugging from machine dependent issues to more abstract problems such as visualization of parallel activities, data, and dynamic activation trees. For performance tuning, we additionally support profiling <ref> [4] </ref>. MSDB's display features. Concepts. The debugger provides the usual features that are also found in sequential debuggers, such as setting and examining variables, setting breakpoints, and stepping through the program. Additionally, MSDB supports different views of distributed data and parallel execution.
Reference: [5] <author> Philipp J. Hatcher and Michael J. Quinn. </author> <title> Data-Parallel Programming on MIMD Computers. </title> <publisher> MIT Press Cambridge, </publisher> <address> Massachusetts, London, England, </address> <year> 1991. </year>
Reference-contexts: Another interesting feature of these libraries is their functional diversity. Wherever possible, normal, masked, segmented, and universal (masked plus segmented) versions of the parallel operations are provided. 4 Benchmark Results Our benchmark suite consists of 13 problems collected from literature <ref> [1, 2, 3, 5, 7] </ref>. For each problem we implemented the same algorithm in Modula-2*, in MPL 3 , and in sequential C. Then we measured the runtimes of our implementations on a 16K MasPar MP-1 and a Sparc-1 for widely ranging problem sizes.
Reference: [6] <author> Ernst A. Heinz and Michael Philippsen. </author> <title> Synchronization barrier elimination in synchronous FORALLs. </title> <type> Technical Report No. 13/93, </type> <institution> University of Karlsruhe, Department of Informatics, </institution> <month> April </month> <year> 1993. </year>
Reference-contexts: The semantics of synchronous FORALL statements in [12] require a large number of synchronization barriers. Most real synchronous FORALLs, however, only need a fraction thereof to ensure correctness. We have shown that the automatic elimination of such redundant synchronization barriers is possible <ref> [6] </ref>. To statically detect redundant synchronization barriers, the compiler applies a variant of standard data dependence analysis modified for our specific needs.
Reference: [7] <author> Joseph JaJa. </author> <title> An Introduction to Parallel Algorithms. </title> <publisher> Addison-Wesley, </publisher> <address> Reading, Mass., </address> <year> 1992. </year>
Reference-contexts: Another interesting feature of these libraries is their functional diversity. Wherever possible, normal, masked, segmented, and universal (masked plus segmented) versions of the parallel operations are provided. 4 Benchmark Results Our benchmark suite consists of 13 problems collected from literature <ref> [1, 2, 3, 5, 7] </ref>. For each problem we implemented the same algorithm in Modula-2*, in MPL 3 , and in sequential C. Then we measured the runtimes of our implementations on a 16K MasPar MP-1 and a Sparc-1 for widely ranging problem sizes.
Reference: [8] <institution> Pawel Lukowicz. Code-Erzeugung fur Modula-2* fur verschiedene Maschinenarchitekturen. </institution> <type> Master's thesis, </type> <institution> University of Karlsruhe, Department of Informatics, </institution> <month> January </month> <year> 1992. </year>
Reference-contexts: To this end we use an abstract classification of parallel C dialects to control some structural aspects of the code generated by the compiler. This code is a general intermediate representation that we have chosen to be C augmented with machine-independent macros and keywords <ref> [8] </ref>. In the second stage we adapt our code to a specific target architecture by by combining the intermediate code with a machine-dependent macro package using a standard preprocessor.
Reference: [9] <author> Michael Philippsen. </author> <title> Automatic data distribution for nearest neighbor networks. </title> <booktitle> In Frontiers '92:The Fourth Symposium on the Frontiers of Massively Parallel Computation, </booktitle> <pages> pages 178-185, </pages> <address> Mc Lean, Virginia, </address> <month> October 19-21, </month> <year> 1992. </year>
Reference-contexts: Implemented as a source-to-source transformation, this optimization results in dramatically enhanced locality and normally improves performance on the MasPar MP-1 by at least 40% [11]. Furthermore, our compiler automatically maps arbitrary multi-dimensional arrays to the available processors. The scheme employed <ref> [9] </ref> enables nearest-neighbor communication and achieves ef ficient address calculations. Implementation Restrictions. There are two major restrictions in the current compiler. The first one is the lack of pointers to distributed data and distributed open array parameters (except for shared memory machines).
Reference: [10] <author> Michael Philippsen, Ernst A. Heinz, and Paul Lukowicz. </author> <title> Compiling machine-independent parallel programs. </title> <journal> ACM SIGPLAN Notices, </journal> <volume> 28(8) </volume> <pages> 99-108, </pages> <month> August </month> <year> 1993. </year>
Reference-contexts: Programs need to be written independently of the number of available processors and the actual machine topology. In this paper we present a parallel programming system that fulfills these requirements while achieving adequate performance of the compiled code <ref> [10] </ref>. It consists of a compiler for a problem-oriented language and a programming environment that provides a uniform problem view across different machine types (MIMD, SIMD and SISD). Section 2 describes the main features of Modula-2*. <p> Thus, the debugger can rely on existing source level debuggers such as dbx and gdb. However, it has to transparently map Modula-2* to C and vice versa while debugging a program. This is not simple due to the extensive restructuring of the code done by the compiler <ref> [10] </ref>. Thus, the compiler generates hints for the line and variable mapping and places them inside special comments which are parsed by the debugger when reading the source code. * Debug Functions. <p> For each problem we implemented the same algorithm in Modula-2*, in MPL 3 , and in sequential C. Then we measured the runtimes of our implementations on a 16K MasPar MP-1 and a Sparc-1 for widely ranging problem sizes. The detailed results can be found in <ref> [10] </ref>.
Reference: [11] <author> Michael Philippsen and Markus U. Mock. </author> <title> Data and process alignment in Modula-2*. </title> <editor> In Christoph W. Kessler, editor, </editor> <title> Automatic Par-allelization New Approaches to Code Generation, Data Distribution, </title> <booktitle> and Performance Prediction, </booktitle> <pages> pages 171-191, </pages> <address> AP'93 Saarbrucken, Ger-many, March 1-3, 1993, 1994. </address> <publisher> Verlag Vieweg, Wiesbaden, </publisher> <address> Germany, </address> <institution> Advanced Studies in Computer Science. </institution>
Reference-contexts: Implemented as a source-to-source transformation, this optimization results in dramatically enhanced locality and normally improves performance on the MasPar MP-1 by at least 40% <ref> [11] </ref>. Furthermore, our compiler automatically maps arbitrary multi-dimensional arrays to the available processors. The scheme employed [9] enables nearest-neighbor communication and achieves ef ficient address calculations. Implementation Restrictions. There are two major restrictions in the current compiler.
Reference: [12] <author> Walter F. Tichy and Christian G. Herter. </author> <title> Modula-2*: An extension of Modula-2 for highly parallel, portable programs. </title> <type> Technical Report No. 4/90, </type> <institution> University of Karlsruhe, Department of Informatics, </institution> <month> January </month> <year> 1990. </year>
Reference-contexts: Finally, we give some benchmark results demonstrating the performance of the compiled codes. 2 Modula-2* The programming language Modula-2* was developed to allow for high-level, problem-oriented, and machine-independent parallel programming. As described in <ref> [12] </ref> it embodies the following features: * An arbitrary number of processes operate on data in the same single address space (or virtual shared memory). <p> This means that Modula-2* does not require any synchronization between different branches of synchronous CASE or IF statements. The exact synchronous semantics of all Modula-2* statements, including nested FORALLs, and the effects of allocator declarations are described in <ref> [12] </ref>. 3 The Modula-2* System The Modula-2* system currently targets the MasPar MP series of massively parallel processors (SIMD), the KSR-1 parallel computer (MIMD), heterogeneous LANs of workstations (MIMD), and single workstations (SISD). <p> Optimizations. On parallel machines optimizations tend to improve program runtime dramatically. Therefore, our Modula-2* compiler performs various optimizations and code restructuring as summarized below. * Elimination of Synchronization Barriers. The semantics of synchronous FORALL statements in <ref> [12] </ref> require a large number of synchronization barriers. Most real synchronous FORALLs, however, only need a fraction thereof to ensure correctness. We have shown that the automatic elimination of such redundant synchronization barriers is possible [6].
References-found: 12

