URL: http://ftp.eecs.umich.edu/people/tyyeh/PhD-Diss.sglspc.ps
Refering-URL: http://ftp.eecs.umich.edu/people/tyyeh/
Root-URL: http://www.eecs.umich.edu
Title: ABSTRACT TWO-LEVEL ADAPTIVE BRANCH PREDICTION AND INSTRUCTION FETCH MECHANISMS FOR HIGH PERFORMANCE SUPERSCALAR PROCESSORS  
Degree: by Chairman: Professor  
Address: N. Patt  
Affiliation: Yale  
Abstract: As the issue width and depth of pipelining of high performance superscalar processors increase, the importance of an effective instruction fetch mechanism becomes vital to delivering the potential performance of a wide-issue, deep pipelined microarchitecture. In this thesis a new dynamic branch predictor (Two-Level Adaptive Branch Prediction) and a new instruction fetch mechanism suitable for superscalar processors are proposed to reduce the branch execution penalty in instruction fetch. The branch predictor uses two levels of branch history information to make predictions: the history of the last k branches encountered, and the branch behavior for the last s occurrences of the specific pattern of these k branches. Its nine variations are identified according to how finely the history information is gathered. The cost-effectiveness of the variations is compared. Simulation results show that the average misprediction rate for the Two-Level Adaptive branch predictor with a reasonable implementation cost is 3% over nine programs in the SPEC89 benchmark suite, while other known schemes achieve at least 5.6% average misprediction rate. The branch predictor is integrated into an instruction fetch mechanism that is able to fetch multiple instructions each cycle and change instruction flow without incurring any pipeline bubbles. Compared with designs which use other branch predictors, the proposed design significantly reduces the branch execution penalty. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> A.V. Aho, R. Sethi, and J. Ullman, </author> <booktitle> "Compilers: Principles, Techniques, and Tools", </booktitle> <publisher> Addison Wesley, </publisher> <month> June </month> <year> 1987. </year>
Reference-contexts: A more effective approach for superscalar processors is to store only the branch information in a branch history table and store the instructions in the instruction cache. Each branch is identified by the address of an instruction that dominates the branch <ref> [1] </ref>. During the fetch stage of the dominating instructions, the branch is identified and its target address is predicted. Intel's Pentium implementation accesses the branch history table (Intel calls it branch target buffer) with the address of the instruction in the first decode stage. <p> Section 9 contains the simulation results and the analysis of the designs. Section 10 summarizes the results and implications of this chapter. 6.1 Fetch Address Determination A basic block is by definition a sequence of consecutive instructions that does not contain any control flow change in the middle <ref> [1] </ref>. In a basic block, the flow of control enters at the beginning and leaves at the end without halt or possibility of branching except at the end. <p> In order to make the prediction sufficiently early before the fetch of branch target instructions, a branch should be identified by the address of an instruction which dominates <ref> [1] </ref> the branch. An instruction dominates a branch if it is always executed prior to the branch whenever the branch is executed. The address of an instruction should be used to identify only its first dominated branch; therefore, each address identifies a unique branch.
Reference: [2] <author> Advanced Micro Devices, </author> <title> "Am29000 Streamlined Instruction Processor User Manual", </title> <booktitle> Advanced Micro Devices, </booktitle> <year> 1988. </year>
Reference-contexts: A loop buffer was commonly used in processors which take several cycles to fetch instructions. This small and fast storage unit can hold the instructions of of a loop; hence, the fetch latency is greatly reduced if a loop iterates many times. The branch target cache <ref> [2] </ref> which is similar to the loop buffer stores the target instructions of branches. When a branch's address hits in the cache, the target instruction is available immediately. To reduce the penalty of incorrect branch prediction, the IBM RS/6000 prefetches the instructions from the alternate predicted path.
Reference: [3] <author> T. Ball and J.R. Larus, </author> <title> "Branch Prediction for Free", </title> <type> Technical Report No. 1137, </type> <institution> CS Department, University of Wisconsin- Madison, </institution> <month> Feb. </month> <year> 1993. </year>
Reference-contexts: Profiling [19, 43, 26, 20] is effective in predicting branch paths by measuring the tendency of a branch with sample data sets and presetting a static prediction bit in the opcode. In order to avoid lengthy compile-profile-compile cycles due to profiling, Ball and Larus <ref> [3] </ref> proposed a program-based prediction scheme which uses various heuristics for different opcodes, loop branches, and non-loop branches. The performance of static prediction schemes closely reflects the branch characteristics. The always-taken scheme achieves accuracy from 40 to 90% with an average around 60%.
Reference: [4] <author> B. Bray and M. Flynn, </author> <title> "Strategies for Branch Target Buffers", </title> <booktitle> Proceedings of the 24th ACM/IEEE International Symposium and Workshop on Microarchitecture, </booktitle> <pages> pp. 42-50, </pages> <month> Nov. </month> <year> 1991. </year>
Reference-contexts: The important factors are the size, set associativity, allocation policy, and index scheme. A high associativity or a larger BHT helps to improve the hit rate. Allocation policy is important in utilizing limited BHT entries. In branch target buffer designs <ref> [4, 52] </ref>, the entries can be conserved through caching only the targets of taken branches, because the target instructions of fall-through branches are available by fetching sequentially.
Reference: [5] <author> M. Butler and Y.N. Patt, </author> <title> "An Area-Efficient Register Alias Table for Implementing HPS", </title> <booktitle> Proceedings of the 1990 International Conference on Parallel Processing, </booktitle> <volume> vol. 1, </volume> <pages> pp. 611-612, </pages> <month> August </month> <year> 1990. </year>
Reference-contexts: Restoring the correct processor state quickly after an incorrect prediction is detected requires a good state maintenance mechanism. Three common mechanisms have been proposed: checkpointing <ref> [50, 24, 5] </ref>, reorder buffers [58, 32], and history buffers [58, 46]. The checkpoint mechanism in HPS [50] is able to restore the correct processor state immediately after an incorrect prediction is detected in the execution stage by discarding all information corresponding to the incorrectly-predicted paths.
Reference: [6] <author> M. Butler, T-Y Yeh, Y.N. Patt, M. Alsup, H. Scales, and M. Shebanow, </author> <title> "Instruction Level Parallelism is Greater Than Two", </title> <booktitle> Proceedings of the 18th International Symposium on Computer Architecture, </booktitle> <pages> pp. 276-286, </pages> <month> May </month> <year> 1991. </year>
Reference-contexts: This dramatic speedup comes from the large collection of instructions called the scheduling window from which instructions can be found to execute in parallel. Butler et al. <ref> [6] </ref>, Wall [60], and Lam [38] all have reported that there is significant parallelism (in the range of 15 to 3000 instructions each cycle) available when the results of branch executions are known in advance. However, in practical processors, the result of branch execution is not known in advance. <p> The average number of instruction issued per cycle is the optimal processor performance assuming no stalls caused by either the execution core or the memory system. Butler et. al. <ref> [6] </ref> show that the available instruction level parallelism is close to the average issue rate with an oracle branch predictor, an ideal memory system, and a sufficiently large scheduling window.
Reference: [7] <author> M. Butler and Y.N. Patt, </author> <title> "A Comparative Performance Evaluation of Various State Maintenance Mechanisms", </title> <booktitle> Proceedings of the 26th ACM/IEEE International Symposium and Workshop on Microarchi-tecture, </booktitle> <month> Dec. </month> <year> 1993. </year>
Reference-contexts: Using either the history buffer or the reorder buffer usually results in several cycles of delay between the detection of the incorrect prediction and the restoration of the correct processor state. Butler and Patt <ref> [7] </ref> reported that on average the reorder buffer technique incurs an additional 1.7 to 3.0 cycle misprediction penalty over the HPS checkpointing, which results in a decrease in average parallelism of between 6.2 and 7.7%.
Reference: [8] <author> P. Chang, S. Mahlke, W. Chen, N. Warter, and W. Hwu, </author> <title> "IMPACT: An Architectural Framework for Multiple-Instruction-Issue Processors", </title> <booktitle> Proceedings of the 18th International Symposium on Computer Architecture, </booktitle> <pages> pp. 266-275, </pages> <month> May </month> <year> 1991. </year>
Reference-contexts: In static compiler scheduling, profiling is often used to determine the scope of speculative scheduling to avoid executing instructions down incorrectly-predicted paths and instructions needed to repair the processor state. The IMPACT group reported that their advanced compiler <ref> [8] </ref> enabled an 8-wide issue processor with in-order execution to achieve a speedup of 2.5 over a single-issue processor. <p> Aligning basic blocks on cache line boundaries eliminates the need of a self-aligned cache; however, it may increase the code size by inserting nops between basic blocks. When basic blocks are enlarged <ref> [19, 8, 44] </ref> or the frequently-executed basic blocks are placed sequentially [25], fewer instructions need to be thrown away in each fetch. An effective instruction fetch mechanism needs to predict all types of branches. <p> If a superscalar processor can only fetch one basic block a time, the processor's ability to execute multiple instructions is greatly constrained. One solution would be to have compilers enlarge basic blocks through trace scheduling [19], superblock scheduling <ref> [8] </ref>, extended basic block [44], or hyperblock scheduling [42]. Another solution might be to implement hardware which can fetch multiple non-sequential basic blocks each cycle [65]. Such a hardware solution requires a branch predictor that can predict multiple branch paths and multiple fetch addresses in a single cycle.
Reference: [9] <author> R. Colwell, R. Nix, J. O'Donnell, D. Papworth, and P. Rodman, </author> <title> "A VLIW Architecture for a Trace Scheduling Compiler," </title> <booktitle> Proceedings of the 2nd International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pp. 180-192, </pages> <month> Oct. </month> <year> 1987. </year>
Reference-contexts: Compiler technologies that enlarge basic blocks such as loop unrolling and jamming reduce the number of branches executed by combining loop iterations. If an ISA supports predicated execution or conditional moves, conditional branches can be removed using the predicated (guarded) execution <ref> [15, 23, 9, 54, 42] </ref> or conditional moves [14]. In predicated execution, instructions from both paths of a branch are predicated with the condition of the original logic expression. In some implementations, only the instructions whose predicates match the resolution of the corresponding branch are executed. <p> In predicated execution, instructions from both paths of a branch are predicated with the condition of the original logic expression. In some implementations, only the instructions whose predicates match the resolution of the corresponding branch are executed. In others, as in the case of Multiflow <ref> [9] </ref>, all the instructions are executed, but only the results from the instructions with true predicates are valid. Multiflow also used 2 N -way branching [18] where the target is selected by multiple predicates to reduce the number of conditional branches.
Reference: [10] <author> H.G. Cragon, </author> <title> "Branch Strategy Taxonomy And Performance Models", </title> <publisher> IEEE Computer Society Press, Los Alamitos, </publisher> <address> CA, </address> <year> 1992. </year>
Reference-contexts: Delayed branching [21] fills the branch delay slots (pipeline bubbles) with useful instructions; thus, the processor does not stall waiting for the branch to be resolved. However, filling the delay slots with useful instructions becomes increasingly difficult as the number of function units increases and pipelines deepen. Branch prediction <ref> [57, 19, 39, 43, 26, 61, 10, 48, 62] </ref> predicts the targets of branches in order to continue executing useful instructions while a branch is being resolved, i.e. the subsequent instructions are prefetched and executed speculatively. <p> Delayed branching [21, 12, 53, 40], multiple-stream execution [37, 59], and branch prediction <ref> [57, 19, 39, 43, 26, 61, 10, 48, 62] </ref> attempt to exploit this idea. Since branch prediction is one of the major topics in this dissertation, it will be described in detail later. The other methods to hide the branch execution latency are described as follows. <p> They use static information, dynamic branch history, or both. Several pipeline designs have been studied to reduce the branch misprediction penalty. They are briefly described and compared below. 2.3.1 Prediction Accuracy The literature contains many suggested branch prediction schemes <ref> [57, 19, 39, 43, 26, 61, 10, 48, 62] </ref>. Some are static in that they use only static information such as opcodes and profiling statistics to make predictions. The prediction of a static branch is determined before the program is executed and does not change during execution.
Reference: [11] <author> Cypress Semiconductor, </author> <title> "SPARC RISC User Guide", </title> <institution> Cypress Semiconductor Corporation, </institution> <address> San Jose, CA. </address>
Reference-contexts: Since branch prediction is one of the major topics in this dissertation, it will be described in detail later. The other methods to hide the branch execution latency are described as follows. Delayed branching was a popular idea in many early RISC instruction set architectures (ISAs) <ref> [35, 30, 45, 11] </ref> because their early implementations had simple pipeline designs. This technique attempts to schedule instructions from the basic block in which the branch is located into the delay slots, allowing useful instructions to be executed while the branch is being resolved.
Reference: [12] <author> J. A. DeRosa and H. M. Levy, </author> <booktitle> "An Evaluation of Branch Architectures ", Proceedings of the 14th International Symposium on Computer Architecture, </booktitle> <pages> pp. 10-16, </pages> <month> June </month> <year> 1987. </year>
Reference-contexts: They solve the branch problem in four ways: hide branch execution latency, reduce branch resolution latency, reduce instruction fetch latency, and eliminate branches. 2.2.1 Hide Branch Execution Latency A processor can hide branch execution latency by doing useful work while a branch is being resolved. Delayed branching <ref> [21, 12, 53, 40] </ref>, multiple-stream execution [37, 59], and branch prediction [57, 19, 39, 43, 26, 61, 10, 48, 62] attempt to exploit this idea. Since branch prediction is one of the major topics in this dissertation, it will be described in detail later. <p> However, the number of instructions that can be scheduled from the same basic block into the delay slots is limited <ref> [12] </ref>. To overcome this problem, instructions from the predicted subsequent basic block are often considered as well. These instructions are executed speculatively. They are discarded when the actual branch path is resolved to be different from the predicted path.
Reference: [13] <author> A. Dollas and R.F. Krick, </author> <title> "The Case for the Sustained Performance Computer Architecture", </title> <journal> Computer Architecture News, </journal> <volume> vol. 17, no. 6, </volume> <pages> pp. 129-136, </pages> <month> Dec. </month> <year> 1989. </year>
Reference-contexts: With hardware support, fetching and executing both possible paths of a conditional branch is another approach to eliminate branches. The IBM 360/91 [28] and the IBM 3033 [29] use this scheme to avoid the penalty of incorrect branch predictions. Krick and Dollas proposed exhaustive instruction prefetching <ref> [13, 36] </ref> to keep the pipelines of a processor full to the greatest extent. This approach requires no change to the instruction set. Using a pure hardware approach, the results from the incorrect path are discarded when the branch is resolved. <p> This notion can be applied to all branch predictions so that the instructions from the alternative predicted path are prefetched. Additional cycles can be saved by decoding and executing the instructions from the alternative predicted path; however, this requires substantial hardware support <ref> [13, 36] </ref>. 2.3.3 Effects of Branch Prediction Branch prediction affects the performance of a deeply-pipelined superscalar processor in two ways. First, the utilization of the processor increases significantly as prediction accuracy increases.
Reference: [14] <institution> Digital Equipment Corporation, "DECCHIP T M 21064-AA RISC Microprocessor", Maynard, Mas-sachusetts, </institution> <year> 1992. </year>
Reference-contexts: Therefore, while waiting for the branch to be resolved, the processor can also perform useful work. Removing branches is another approach to solving the branch problem. Conditional branches 3 can be removed using conditional moves <ref> [14] </ref> or predicated execution [15, 23]. After a conditional branch is removed, a conditional branch structure is converted into straight-line instructions without branches. One of the above methods by itself or several methods together can be used to solve the branch problem. <p> Moreover, the difference in accuracy is even greater when branches behave differently during different periods of execution. Compared with other history-based dynamic branch predictors such as the two-bit counter [57, 39] and the last-time scheme <ref> [57, 39, 14] </ref>, Two-Level Adaptive Branch Prediction predicts better by distinguishing more branch execution states. Two-Level Adaptive Branch Prediction uses shift register (s) to keep the first-level branch history. <p> Compiler technologies that enlarge basic blocks such as loop unrolling and jamming reduce the number of branches executed by combining loop iterations. If an ISA supports predicated execution or conditional moves, conditional branches can be removed using the predicated (guarded) execution [15, 23, 9, 54, 42] or conditional moves <ref> [14] </ref>. In predicated execution, instructions from both paths of a branch are predicated with the condition of the original logic expression. In some implementations, only the instructions whose predicates match the resolution of the corresponding branch are executed. <p> However, as instruction cache line size increases, the number of branches existing in the same line increases. This complicates the logic to decide the next fetch address and increases the storage of branch prediction information. The DEC Alpha 21064 <ref> [14] </ref> implements this approach by storing an extra bit along with each instruction in the cache for branch prediction. The bit is set when a branch was taken in its last execution; thus, the next execution is predicted based on what happened in the previous execution.
Reference: [15] <author> E.W. Dijkstra, </author> <title> "Guarded Commands, Nondeterminacy, and Formal Derivation of Programs", </title> <journal> Communication of ACM, </journal> <volume> vol. 18, </volume> <pages> pp. 453-457, </pages> <month> August </month> <year> 1975. </year>
Reference-contexts: Therefore, while waiting for the branch to be resolved, the processor can also perform useful work. Removing branches is another approach to solving the branch problem. Conditional branches 3 can be removed using conditional moves [14] or predicated execution <ref> [15, 23] </ref>. After a conditional branch is removed, a conditional branch structure is converted into straight-line instructions without branches. One of the above methods by itself or several methods together can be used to solve the branch problem. <p> Compiler technologies that enlarge basic blocks such as loop unrolling and jamming reduce the number of branches executed by combining loop iterations. If an ISA supports predicated execution or conditional moves, conditional branches can be removed using the predicated (guarded) execution <ref> [15, 23, 9, 54, 42] </ref> or conditional moves [14]. In predicated execution, instructions from both paths of a branch are predicated with the condition of the original logic expression. In some implementations, only the instructions whose predicates match the resolution of the corresponding branch are executed.
Reference: [16] <author> D.R. Ditzel and H.R. McLellan, </author> <title> "Branch Folding in the CRISP Microprocessor: Reducing Branch Delay to Zero", </title> <booktitle> Proceedings of the 14th International Symposium on Computer Architecture, </booktitle> <pages> pp. 2-9, </pages> <month> June </month> <year> 1987. </year> <pages> 98 99 </pages>
Reference: [17] <author> P. G. Emma and E. S. Davidson, </author> <title> "Characterization of Branch and Data Dependencies in Programs for Evaluating Pipeline Performance" , IEEE Transactions on Computers, </title> <journal> pp. </journal> <pages> 859-876, </pages> <month> July </month> <year> 1987. </year>
Reference: [18] <author> J.A. Fisher, </author> <title> "2 N -Way Jump Microinstruction Hardware And An Effective Instruction Binding Method", </title> <booktitle> Proceedings of the 13th Annual Workshop on Microprogramming, </booktitle> <pages> pp. 64-75, </pages> <year> 1980. </year>
Reference-contexts: In others, as in the case of Multiflow [9], all the instructions are executed, but only the results from the instructions with true predicates are valid. Multiflow also used 2 N -way branching <ref> [18] </ref> where the target is selected by multiple predicates to reduce the number of conditional branches. In this scheme, the predicate of an instruction can also be a conjugate of several conditions, eliminating all but one of a sequence of conditional branches.
Reference: [19] <author> J.A. Fisher, </author> <title> "Trace Scheduling: A Technique for Global Microcode Compaction", </title> <journal> IEEE Transactions on Computers, </journal> <volume> vol. c-30, no. 7, </volume> <pages> pp. 478-490, </pages> <month> July </month> <year> 1981. </year>
Reference-contexts: Delayed branching [21] fills the branch delay slots (pipeline bubbles) with useful instructions; thus, the processor does not stall waiting for the branch to be resolved. However, filling the delay slots with useful instructions becomes increasingly difficult as the number of function units increases and pipelines deepen. Branch prediction <ref> [57, 19, 39, 43, 26, 61, 10, 48, 62] </ref> predicts the targets of branches in order to continue executing useful instructions while a branch is being resolved, i.e. the subsequent instructions are prefetched and executed speculatively. <p> Delayed branching [21, 12, 53, 40], multiple-stream execution [37, 59], and branch prediction <ref> [57, 19, 39, 43, 26, 61, 10, 48, 62] </ref> attempt to exploit this idea. Since branch prediction is one of the major topics in this dissertation, it will be described in detail later. The other methods to hide the branch execution latency are described as follows. <p> They use static information, dynamic branch history, or both. Several pipeline designs have been studied to reduce the branch misprediction penalty. They are briefly described and compared below. 2.3.1 Prediction Accuracy The literature contains many suggested branch prediction schemes <ref> [57, 19, 39, 43, 26, 61, 10, 48, 62] </ref>. Some are static in that they use only static information such as opcodes and profiling statistics to make predictions. The prediction of a static branch is determined before the program is executed and does not change during execution. <p> The latter scheme is effective for loop intensive code, but does not work well for the programs which contain many irregular non-loop branches. Profiling <ref> [19, 43, 26, 20] </ref> is effective in predicting branch paths by measuring the tendency of a branch with sample data sets and presetting a static prediction bit in the opcode. <p> Aligning basic blocks on cache line boundaries eliminates the need of a self-aligned cache; however, it may increase the code size by inserting nops between basic blocks. When basic blocks are enlarged <ref> [19, 8, 44] </ref> or the frequently-executed basic blocks are placed sequentially [25], fewer instructions need to be thrown away in each fetch. An effective instruction fetch mechanism needs to predict all types of branches. <p> If a superscalar processor can only fetch one basic block a time, the processor's ability to execute multiple instructions is greatly constrained. One solution would be to have compilers enlarge basic blocks through trace scheduling <ref> [19] </ref>, superblock scheduling [8], extended basic block [44], or hyperblock scheduling [42]. Another solution might be to implement hardware which can fetch multiple non-sequential basic blocks each cycle [65].
Reference: [20] <author> J.A. Fisher and S.M. Freudenberger, </author> <title> "Predicting Conditional Branch Directions From Previous Runs of a Program," </title> <booktitle> Proceedings of the 5th International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pp. 85-95, </pages> <month> Oct. </month> <year> 1992. </year>
Reference-contexts: The latter scheme is effective for loop intensive code, but does not work well for the programs which contain many irregular non-loop branches. Profiling <ref> [19, 43, 26, 20] </ref> is effective in predicting branch paths by measuring the tendency of a branch with sample data sets and presetting a static prediction bit in the opcode. <p> The profiling scheme achieves the highest average accuracy of about 90% among the static branch predictors by predicting well for branches which go consistently in the same path as suggested by the profiling. Fisher and Freudenberger <ref> [20] </ref> showed that using data sets other than the testing data set for profiling results in 0 to 50% degradation in prediction accuracy. They suggested that combining the results obtained by profiling multiple data sets makes a good profile. <p> for performance comparisons between different instruction fetch mechanisms, but it can be used as a direct metric for comparing the performance between conditional branch predictors. 30 Fisher and Freudenberger use the number of instructions between two incorrect branch predictions as the metric to evaluate the performance of conditional branch predictors <ref> [20] </ref>. They call the metric the run length. This metric is valid from the viewpoint of instruction scheduling, because the run length is the scope of effective instruction scheduling.
Reference: [21] <author> T.R. Gross and J. Hennessy, </author> <title> "Optimizing Delayed Branches", </title> <booktitle> Proceedings of the 15th Annual Workshop on Microprogramming, </booktitle> <pages> pp. 114-120, </pages> <month> Oct. </month> <year> 1982. </year>
Reference-contexts: Various approaches have been proposed to solve the branch problem such as delayed branching, branch prediction, multiple-stream execution, and branch removal. Delayed branching <ref> [21] </ref> fills the branch delay slots (pipeline bubbles) with useful instructions; thus, the processor does not stall waiting for the branch to be resolved. However, filling the delay slots with useful instructions becomes increasingly difficult as the number of function units increases and pipelines deepen. <p> They solve the branch problem in four ways: hide branch execution latency, reduce branch resolution latency, reduce instruction fetch latency, and eliminate branches. 2.2.1 Hide Branch Execution Latency A processor can hide branch execution latency by doing useful work while a branch is being resolved. Delayed branching <ref> [21, 12, 53, 40] </ref>, multiple-stream execution [37, 59], and branch prediction [57, 19, 39, 43, 26, 61, 10, 48, 62] attempt to exploit this idea. Since branch prediction is one of the major topics in this dissertation, it will be described in detail later.
Reference: [22] <author> D.W. Hammerstrom and E.S. Davidson, </author> <title> "Information Content of CPU Memory Referencing Behavior", </title> <booktitle> Proceedings of the 4th International Symposium on Computer Architecture, </booktitle> <pages> pp. 184-192, </pages> <month> March </month> <year> 1977. </year>
Reference-contexts: In addition, a significant percentage of the execution bandwidth is wasted in branch mispredictions. If the prediction accuracy could be increased to 97%, the size of the average uninterrupted instruction stream doubles to 33 basic blocks. To achieve high prediction accuracy, Hammerstrom and Davidson <ref> [22] </ref> found that there is significant amount of correlation between instruction addresses by measuring the entropy in instruction address traces. This correlation can be used to help predict the instruction flow of programs.
Reference: [23] <author> P.Y. Hsu and E.S. Davidson, </author> <title> "Highly Concurrent Scalar Processing", </title> <booktitle> Proceedings of the 13th International Symposium on Computer Architecture, </booktitle> <pages> pp. 386-395, </pages> <month> June </month> <year> 1986. </year>
Reference-contexts: Therefore, while waiting for the branch to be resolved, the processor can also perform useful work. Removing branches is another approach to solving the branch problem. Conditional branches 3 can be removed using conditional moves [14] or predicated execution <ref> [15, 23] </ref>. After a conditional branch is removed, a conditional branch structure is converted into straight-line instructions without branches. One of the above methods by itself or several methods together can be used to solve the branch problem. <p> Compiler technologies that enlarge basic blocks such as loop unrolling and jamming reduce the number of branches executed by combining loop iterations. If an ISA supports predicated execution or conditional moves, conditional branches can be removed using the predicated (guarded) execution <ref> [15, 23, 9, 54, 42] </ref> or conditional moves [14]. In predicated execution, instructions from both paths of a branch are predicated with the condition of the original logic expression. In some implementations, only the instructions whose predicates match the resolution of the corresponding branch are executed.
Reference: [24] <author> W.W. Hwu and Y.N. Patt, </author> <title> "Checkpoint Repair for Out-of-order Execution Machines", </title> <journal> IEEE Transactions on Computers, </journal> <pages> pp. 1496-1514, </pages> <month> Dec. </month> <year> 1987. </year>
Reference-contexts: Restoring the correct processor state quickly after an incorrect prediction is detected requires a good state maintenance mechanism. Three common mechanisms have been proposed: checkpointing <ref> [50, 24, 5] </ref>, reorder buffers [58, 32], and history buffers [58, 46]. The checkpoint mechanism in HPS [50] is able to restore the correct processor state immediately after an incorrect prediction is detected in the execution stage by discarding all information corresponding to the incorrectly-predicted paths. <p> This design can repair branch history correctly if branches are verified and repaired in program order. However, it may restore the speculative register to an older content when the branches can be verified and repaired out of the program order, as in HPS <ref> [49, 50, 24] </ref>. The correct branch history can actually be restored in the processors which verify and repair out of program order if we backup the contents of the history register when a prediction is made. However, restoring the BHT may take several cycles, thus degrading processor performance greatly. 2.
Reference: [25] <author> W-M Hwu and P.P.Chang, </author> <title> "Achieving High Instruction Cache Performance with an Optimizing Compiler", </title> <booktitle> Proceedings of the 16th International Symposium on Computer Architecture, </booktitle> <pages> pp. 242-251, </pages> <month> May </month> <year> 1989. </year>
Reference-contexts: Aligning basic blocks on cache line boundaries eliminates the need of a self-aligned cache; however, it may increase the code size by inserting nops between basic blocks. When basic blocks are enlarged [19, 8, 44] or the frequently-executed basic blocks are placed sequentially <ref> [25] </ref>, fewer instructions need to be thrown away in each fetch. An effective instruction fetch mechanism needs to predict all types of branches. Kaeli and Emma [34] proposed a branch history table design with subroutine return bits for identifying returns.
Reference: [26] <author> W-M Hwu, T.M.Conte, and P.P.Chang, </author> <title> "Comparing Software and Hardware Schemes for Reducing the Cost of Branches", </title> <booktitle> Proceedings of the 16th International Symposium on Computer Architecture, </booktitle> <pages> pp. 224-233, </pages> <month> May </month> <year> 1989. </year>
Reference-contexts: Delayed branching [21] fills the branch delay slots (pipeline bubbles) with useful instructions; thus, the processor does not stall waiting for the branch to be resolved. However, filling the delay slots with useful instructions becomes increasingly difficult as the number of function units increases and pipelines deepen. Branch prediction <ref> [57, 19, 39, 43, 26, 61, 10, 48, 62] </ref> predicts the targets of branches in order to continue executing useful instructions while a branch is being resolved, i.e. the subsequent instructions are prefetched and executed speculatively. <p> The first-level history is updated dynamically to show the state of the branch execution. The second-level history changes with the dynamic execution history to reflect the temporary behavior of branches with that history pattern. Compared to profile-based branch prediction schemes such as static profiling <ref> [26, 43] </ref> and static training [39], Two-Level Adaptive Branch Prediction achieves higher prediction accuracy because it can change predictions when the behavior of a branch changes with input data. Moreover, the difference in accuracy is even greater when branches behave differently during different periods of execution. <p> Delayed branching [21, 12, 53, 40], multiple-stream execution [37, 59], and branch prediction <ref> [57, 19, 39, 43, 26, 61, 10, 48, 62] </ref> attempt to exploit this idea. Since branch prediction is one of the major topics in this dissertation, it will be described in detail later. The other methods to hide the branch execution latency are described as follows. <p> They use static information, dynamic branch history, or both. Several pipeline designs have been studied to reduce the branch misprediction penalty. They are briefly described and compared below. 2.3.1 Prediction Accuracy The literature contains many suggested branch prediction schemes <ref> [57, 19, 39, 43, 26, 61, 10, 48, 62] </ref>. Some are static in that they use only static information such as opcodes and profiling statistics to make predictions. The prediction of a static branch is determined before the program is executed and does not change during execution. <p> The latter scheme is effective for loop intensive code, but does not work well for the programs which contain many irregular non-loop branches. Profiling <ref> [19, 43, 26, 20] </ref> is effective in predicting branch paths by measuring the tendency of a branch with sample data sets and presetting a static prediction bit in the opcode. <p> This section describes how to incorporate a global history predictor or a per-address history branch predictor into the instruction fetch mechanism. For comparison, this section also describes the mechanisms that use 2-bit counter [57] and static profiling <ref> [43, 26] </ref>. All the mechanisms use a BHT; however, to show the effect of using a BHT, the mechanism that uses static profiling was also simulated with no BHT. These mechanisms are described as follows: 76 * Per-address History Two-Level Adaptive Branch Predictor Branch Predictor.
Reference: [27] <author> W-M Hwu and P.P.Chang, </author> <title> "Inline Function Expansion for Compiling Realistic C Programs", </title> <booktitle> Proceedings of the ACM SIGPLAN '89 Conference on Programming Language Design and Implementation, </booktitle> <pages> pp. 246-257, </pages> <month> June </month> <year> 1989. </year>
Reference-contexts: Software, instruction set architecture (ISA), and hardware methods have been proposed to reduce the number of branch executions by converting instructions with conditional branches into straight-line code. Subroutine inlining, code replication, and basic block enlargement are software methods that reduce the number of branches. Subroutine inlining <ref> [27] </ref> eliminates subroutine calls by selectively expanding the subroutine body at the places where it is called. Code replication [47] reduces the number of unconditional branches by replacing branches with target instructions. Since these methods remove unconditional branches by replicating instructions, they require heuristics to avoid excessive code expansion.
Reference: [28] <author> IBM Corp., </author> <title> "IBM Maintenance Library System/370 Model 168, Theory of Operations/Diagrams Manual", IBM, </title> <journal> Poughkeepsie, NY, </journal> <volume> vol. 2, </volume> <year> 1973. </year>
Reference-contexts: Conditional move 10 instructions then move the results to the destination registers conditionally upon the result of the condition test. With hardware support, fetching and executing both possible paths of a conditional branch is another approach to eliminate branches. The IBM 360/91 <ref> [28] </ref> and the IBM 3033 [29] use this scheme to avoid the penalty of incorrect branch predictions. Krick and Dollas proposed exhaustive instruction prefetching [13, 36] to keep the pipelines of a processor full to the greatest extent. This approach requires no change to the instruction set.
Reference: [29] <author> IBM Corp., </author> <title> "IBM Maintenance Library 3033 Processor Complex, Theory of Operation/Diagrams Manual", IBM, </title> <journal> Poughkeepsie, NY, </journal> <volume> vol. </volume> <pages> 1-3, </pages> <month> Jan. </month> <year> 1978. </year>
Reference-contexts: Conditional move 10 instructions then move the results to the destination registers conditionally upon the result of the condition test. With hardware support, fetching and executing both possible paths of a conditional branch is another approach to eliminate branches. The IBM 360/91 [28] and the IBM 3033 <ref> [29] </ref> use this scheme to avoid the penalty of incorrect branch predictions. Krick and Dollas proposed exhaustive instruction prefetching [13, 36] to keep the pipelines of a processor full to the greatest extent. This approach requires no change to the instruction set.
Reference: [30] <institution> IBM Corp., </institution> <note> "POWER Processor Architecture Version 1.52", </note> <institution> IBM Corporation, Austin, TX, </institution> <year> 1990. </year>
Reference-contexts: Since branch prediction is one of the major topics in this dissertation, it will be described in detail later. The other methods to hide the branch execution latency are described as follows. Delayed branching was a popular idea in many early RISC instruction set architectures (ISAs) <ref> [35, 30, 45, 11] </ref> because their early implementations had simple pipeline designs. This technique attempts to schedule instructions from the basic block in which the branch is located into the delay slots, allowing useful instructions to be executed while the branch is being resolved. <p> As a result, the branch is resolved before it enters the execution 9 stage, reducing the number of delay slots needed to one. The IBM RS/6000 <ref> [30] </ref> and the WISQ [53] generate the target address and tests the condition in an even earlier stage. Once those processors detect a branch instruction in the instruction buffer, the target address of the branch is calculated and the condition is tested when the condition becomes available. <p> Branch execution latency can also be shortened by reducing the number of pipeline stages such as performing the condition test when the branch is being decoded [45] or when the branch is still in the instruction buffer <ref> [30] </ref>. Restoring the correct processor state quickly after an incorrect prediction is detected requires a good state maintenance mechanism. Three common mechanisms have been proposed: checkpointing [50, 24, 5], reorder buffers [58, 32], and history buffers [58, 46]. <p> The time to fetch the instructions from the correct path can be totally eliminated if they are prefetched while the branch is being resolved and spare bandwidth is available. For example, when a branch is predicted fall-through, the IBM RS/6000 <ref> [30] </ref> fetches the instructions from the taken path after the instructions from the fall-through path are fetched If the prediction is verified to be incorrect, the instructions from the taken path are available for decoding immediately, thus saving the time to fetch those instructions. <p> Since a basic block may not align with its cache line boundary, the instructions in the cache line before the beginning of the basic block need to be thrown away. In the worst case, there may be only one useful instruction in the entire cache line. The IBM RS/6000 <ref> [30] </ref> uses a self-aligned cache to overcome this problem. Its cache extracts and returns a cache line full of useful instructions by scanning two cache lines at a time. A similar approach is to fetch two cache lines together but discard useless instructions at the front of the decoders.
Reference: [31] <author> Intel Corporation, </author> <title> "Pentium T M Processor User's Manual, </title> <booktitle> Volume 1, 2, and 3", </booktitle> <address> Mt. Prospect, Illinois, </address> <year> 1993. </year>
Reference: [32] <author> W.M. Johnson, </author> <title> "Superscalar Processor Design", </title> <institution> Computer Systems Laboratory, Stanford University, </institution> <type> Technical Report No. </type> <institution> CSL-TR-89-383, </institution> <month> June </month> <year> 1989. </year>
Reference-contexts: Restoring the correct processor state quickly after an incorrect prediction is detected requires a good state maintenance mechanism. Three common mechanisms have been proposed: checkpointing [50, 24, 5], reorder buffers <ref> [58, 32] </ref>, and history buffers [58, 46]. The checkpoint mechanism in HPS [50] is able to restore the correct processor state immediately after an incorrect prediction is detected in the execution stage by discarding all information corresponding to the incorrectly-predicted paths. <p> This restriction is difficult to enforce when the issue width is large. Another method for identifying branches prior to decode is to store the branch information in the instruction cache. Johnson <ref> [32] </ref> suggests storing additional fields in the cache that indicate the addresses of the block's predicted successors and the locations of all branches in the block.
Reference: [33] <author> N.P. Jouppi and D. Wall, </author> <title> "Available Instruction-Level Parallelism for Superscalar and Superpipelined Machines.", </title> <booktitle> Proceedings of the Third International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pp. 272-282, </pages> <month> April </month> <year> 1989. </year>
Reference: [34] <author> D. R. Kaeli and P. G. Emma, </author> <title> "Branch History Table Prediction of Moving Target Branches Due to Subroutine Returns", </title> <booktitle> Proceedings of the 18th International Symposium on Computer Architecture, </booktitle> <pages> pp. 34-42, </pages> <month> May </month> <year> 1991. </year>
Reference-contexts: When basic blocks are enlarged [19, 8, 44] or the frequently-executed basic blocks are placed sequentially [25], fewer instructions need to be thrown away in each fetch. An effective instruction fetch mechanism needs to predict all types of branches. Kaeli and Emma <ref> [34] </ref> proposed a branch history table design with subroutine return bits for identifying returns. Since regular indirect jumps and returns use the same instructions in the IBM RS/6000, Kaeli and Emma used a pair of stacks to store correct return addresses and to identify returns.
Reference: [35] <author> G. Kane, </author> <title> "mips RISC Architecture", </title> <publisher> Prentice-Hall Inc., </publisher> <address> Englewood Cliffs, NJ, </address> <year> 1988. </year> <month> 100 </month>
Reference-contexts: Since branch prediction is one of the major topics in this dissertation, it will be described in detail later. The other methods to hide the branch execution latency are described as follows. Delayed branching was a popular idea in many early RISC instruction set architectures (ISAs) <ref> [35, 30, 45, 11] </ref> because their early implementations had simple pipeline designs. This technique attempts to schedule instructions from the basic block in which the branch is located into the delay slots, allowing useful instructions to be executed while the branch is being resolved.
Reference: [36] <author> R.F. Krick and A. Dollas, </author> <title> "The Evolution of Instruction Sequencing", </title> <journal> IEEE Computer, </journal> <volume> vol. 24 no. 4, </volume> <pages> pp. 5-16, </pages> <month> April </month> <year> 1991. </year>
Reference-contexts: With hardware support, fetching and executing both possible paths of a conditional branch is another approach to eliminate branches. The IBM 360/91 [28] and the IBM 3033 [29] use this scheme to avoid the penalty of incorrect branch predictions. Krick and Dollas proposed exhaustive instruction prefetching <ref> [13, 36] </ref> to keep the pipelines of a processor full to the greatest extent. This approach requires no change to the instruction set. Using a pure hardware approach, the results from the incorrect path are discarded when the branch is resolved. <p> This notion can be applied to all branch predictions so that the instructions from the alternative predicted path are prefetched. Additional cycles can be saved by decoding and executing the instructions from the alternative predicted path; however, this requires substantial hardware support <ref> [13, 36] </ref>. 2.3.3 Effects of Branch Prediction Branch prediction affects the performance of a deeply-pipelined superscalar processor in two ways. First, the utilization of the processor increases significantly as prediction accuracy increases.
Reference: [37] <author> J.T. Kuehn and B.J. Smith, </author> <title> "The Horizon Supercomputer System: </title> <booktitle> Architecture and Software", Proceedings of Supercomputing Conference, </booktitle> <address> Orlando, Florida, </address> <pages> pp. 28-34, </pages> <month> Nov. </month> <year> 1988. </year>
Reference-contexts: Branch prediction [57, 19, 39, 43, 26, 61, 10, 48, 62] predicts the targets of branches in order to continue executing useful instructions while a branch is being resolved, i.e. the subsequent instructions are prefetched and executed speculatively. Using multiple stream execution <ref> [37, 59] </ref> a processor switches to a different instruction stream when a branch is encountered in the current instruction stream. Therefore, while waiting for the branch to be resolved, the processor can also perform useful work. Removing branches is another approach to solving the branch problem. <p> Delayed branching [21, 12, 53, 40], multiple-stream execution <ref> [37, 59] </ref>, and branch prediction [57, 19, 39, 43, 26, 61, 10, 48, 62] attempt to exploit this idea. Since branch prediction is one of the major topics in this dissertation, it will be described in detail later. <p> Not enough instructions can be found to fill the delay slots, causing delayed branches to become ineffective in aggressive processor designs. For this reason, DEC Alpha [56] excludes delayed branching. Instruction latency can be hidden by switching between multiple instruction streams (I-streams). The HEP processor <ref> [37, 59] </ref> switches to another I-stream after a branch is encountered in its current I-stream. The same I-stream is switched back after the branch is resolved. As a result, while the branch is being executed, the processor continues to do useful work.
Reference: [38] <author> M.S. Lam and R.P. Wilson, </author> <title> "Limits of Control Flow on Parallelism", </title> <booktitle> Proceedings of the 19th International Symposium on Computer Architecture, </booktitle> <pages> pp. 46-57, </pages> <month> May </month> <year> 1992. </year>
Reference-contexts: This dramatic speedup comes from the large collection of instructions called the scheduling window from which instructions can be found to execute in parallel. Butler et al. [6], Wall [60], and Lam <ref> [38] </ref> all have reported that there is significant parallelism (in the range of 15 to 3000 instructions each cycle) available when the results of branch executions are known in advance. However, in practical processors, the result of branch execution is not known in advance.
Reference: [39] <author> J. Lee and A. J. Smith, </author> <title> "Branch Prediction Strategies and Branch Target Buffer Design", </title> <booktitle> IEEE Computer, </booktitle> <pages> pp. 6-22, </pages> <month> Jan. </month> <year> 1984. </year>
Reference-contexts: Delayed branching [21] fills the branch delay slots (pipeline bubbles) with useful instructions; thus, the processor does not stall waiting for the branch to be resolved. However, filling the delay slots with useful instructions becomes increasingly difficult as the number of function units increases and pipelines deepen. Branch prediction <ref> [57, 19, 39, 43, 26, 61, 10, 48, 62] </ref> predicts the targets of branches in order to continue executing useful instructions while a branch is being resolved, i.e. the subsequent instructions are prefetched and executed speculatively. <p> The first-level history is updated dynamically to show the state of the branch execution. The second-level history changes with the dynamic execution history to reflect the temporary behavior of branches with that history pattern. Compared to profile-based branch prediction schemes such as static profiling [26, 43] and static training <ref> [39] </ref>, Two-Level Adaptive Branch Prediction achieves higher prediction accuracy because it can change predictions when the behavior of a branch changes with input data. Moreover, the difference in accuracy is even greater when branches behave differently during different periods of execution. <p> Moreover, the difference in accuracy is even greater when branches behave differently during different periods of execution. Compared with other history-based dynamic branch predictors such as the two-bit counter <ref> [57, 39] </ref> and the last-time scheme [57, 39, 14], Two-Level Adaptive Branch Prediction predicts better by distinguishing more branch execution states. Two-Level Adaptive Branch Prediction uses shift register (s) to keep the first-level branch history. <p> Moreover, the difference in accuracy is even greater when branches behave differently during different periods of execution. Compared with other history-based dynamic branch predictors such as the two-bit counter [57, 39] and the last-time scheme <ref> [57, 39, 14] </ref>, Two-Level Adaptive Branch Prediction predicts better by distinguishing more branch execution states. Two-Level Adaptive Branch Prediction uses shift register (s) to keep the first-level branch history. <p> Delayed branching [21, 12, 53, 40], multiple-stream execution [37, 59], and branch prediction <ref> [57, 19, 39, 43, 26, 61, 10, 48, 62] </ref> attempt to exploit this idea. Since branch prediction is one of the major topics in this dissertation, it will be described in detail later. The other methods to hide the branch execution latency are described as follows. <p> They use static information, dynamic branch history, or both. Several pipeline designs have been studied to reduce the branch misprediction penalty. They are briefly described and compared below. 2.3.1 Prediction Accuracy The literature contains many suggested branch prediction schemes <ref> [57, 19, 39, 43, 26, 61, 10, 48, 62] </ref>. Some are static in that they use only static information such as opcodes and profiling statistics to make predictions. The prediction of a static branch is determined before the program is executed and does not change during execution. <p> The DEC Alpha 21064 implements this scheme by storing an extra bit with each instruction in the tag store of the instruction cache. The bit is set if the instruction is a branch and was taken in its last execution. * Two-bit counter scheme <ref> [57, 39] </ref> A branch can be predicted using the results of the last several executions collected by a two-bit saturating up-down counter. The counter is incremented by one whenever a branch is taken and is decremented whenever a branch is fall-through. <p> more history information, a change in branch execution has less effect on the Two-bit counter scheme than the Last-Time scheme. 3/T 2/T T T N T T 2-bit Counter (2-bit Saturating Up-down Counter) State Input Output 0 2 0 1 0 2 2 3 0 1 12 * Static Training <ref> [39] </ref> A branch can be predicted using the statistics gathered prior to execution coupled with the history pattern of the last k run-time executions of that branch. <p> The next time the branch has the same history register content which accesses the same pattern history table entry, the branch is predicted taken if the counter value is greater or equal to two; otherwise, the branch is predicted not taken. Both Static Training <ref> [39] </ref> and Two-Level Adaptive Branch Prediction are dynamic branch predictors, because their predictions are all based on dynamic branch history.
Reference: [40] <author> D. J. Lilja, </author> <title> "Reducing the Branch Penalty in Pipelined Processors ", IEEE Computer, </title> <journal> pp. </journal> <pages> 47-55, </pages> <month> July </month> <year> 1988. </year>
Reference-contexts: They solve the branch problem in four ways: hide branch execution latency, reduce branch resolution latency, reduce instruction fetch latency, and eliminate branches. 2.2.1 Hide Branch Execution Latency A processor can hide branch execution latency by doing useful work while a branch is being resolved. Delayed branching <ref> [21, 12, 53, 40] </ref>, multiple-stream execution [37, 59], and branch prediction [57, 19, 39, 43, 26, 61, 10, 48, 62] attempt to exploit this idea. Since branch prediction is one of the major topics in this dissertation, it will be described in detail later.
Reference: [41] <author> J. S. Liptay, </author> <title> "Design of the IBM Enterprise System/9000 High-End Processor", </title> <journal> IBM Journal of Research and Development, </journal> <volume> vol. 36, no. 4, </volume> <pages> pp. 713-732, </pages> <month> July </month> <year> 1992. </year>
Reference: [42] <author> S.A. Mahlke, D.C. Lin, W.Y. Chen, R.E. Hank, and R.A. Bringmann, </author> <title> "Effective Compiler Support for Predicated Execution Using the Hyperblock", </title> <booktitle> Proceedings of the 25th Annual ACM/IEEE International Symposium on Computer Microarchitecture, </booktitle> <pages> pp. 45-54, </pages> <month> Dec. </month> <year> 1992. </year>
Reference-contexts: Compiler technologies that enlarge basic blocks such as loop unrolling and jamming reduce the number of branches executed by combining loop iterations. If an ISA supports predicated execution or conditional moves, conditional branches can be removed using the predicated (guarded) execution <ref> [15, 23, 9, 54, 42] </ref> or conditional moves [14]. In predicated execution, instructions from both paths of a branch are predicated with the condition of the original logic expression. In some implementations, only the instructions whose predicates match the resolution of the corresponding branch are executed. <p> If a superscalar processor can only fetch one basic block a time, the processor's ability to execute multiple instructions is greatly constrained. One solution would be to have compilers enlarge basic blocks through trace scheduling [19], superblock scheduling [8], extended basic block [44], or hyperblock scheduling <ref> [42] </ref>. Another solution might be to implement hardware which can fetch multiple non-sequential basic blocks each cycle [65]. Such a hardware solution requires a branch predictor that can predict multiple branch paths and multiple fetch addresses in a single cycle.
Reference: [43] <author> S. McFarling and J. Hennessy, </author> <title> "Reducing the Cost of Branches", </title> <booktitle> Proceedings of the 13th International Symposium on Computer Architecture, </booktitle> <pages> pp. 396-403, </pages> <year> 1986. </year>
Reference-contexts: Delayed branching [21] fills the branch delay slots (pipeline bubbles) with useful instructions; thus, the processor does not stall waiting for the branch to be resolved. However, filling the delay slots with useful instructions becomes increasingly difficult as the number of function units increases and pipelines deepen. Branch prediction <ref> [57, 19, 39, 43, 26, 61, 10, 48, 62] </ref> predicts the targets of branches in order to continue executing useful instructions while a branch is being resolved, i.e. the subsequent instructions are prefetched and executed speculatively. <p> The first-level history is updated dynamically to show the state of the branch execution. The second-level history changes with the dynamic execution history to reflect the temporary behavior of branches with that history pattern. Compared to profile-based branch prediction schemes such as static profiling <ref> [26, 43] </ref> and static training [39], Two-Level Adaptive Branch Prediction achieves higher prediction accuracy because it can change predictions when the behavior of a branch changes with input data. Moreover, the difference in accuracy is even greater when branches behave differently during different periods of execution. <p> Delayed branching [21, 12, 53, 40], multiple-stream execution [37, 59], and branch prediction <ref> [57, 19, 39, 43, 26, 61, 10, 48, 62] </ref> attempt to exploit this idea. Since branch prediction is one of the major topics in this dissertation, it will be described in detail later. The other methods to hide the branch execution latency are described as follows. <p> They use static information, dynamic branch history, or both. Several pipeline designs have been studied to reduce the branch misprediction penalty. They are briefly described and compared below. 2.3.1 Prediction Accuracy The literature contains many suggested branch prediction schemes <ref> [57, 19, 39, 43, 26, 61, 10, 48, 62] </ref>. Some are static in that they use only static information such as opcodes and profiling statistics to make predictions. The prediction of a static branch is determined before the program is executed and does not change during execution. <p> The latter scheme is effective for loop intensive code, but does not work well for the programs which contain many irregular non-loop branches. Profiling <ref> [19, 43, 26, 20] </ref> is effective in predicting branch paths by measuring the tendency of a branch with sample data sets and presetting a static prediction bit in the opcode. <p> This section describes how to incorporate a global history predictor or a per-address history branch predictor into the instruction fetch mechanism. For comparison, this section also describes the mechanisms that use 2-bit counter [57] and static profiling <ref> [43, 26] </ref>. All the mechanisms use a BHT; however, to show the effect of using a BHT, the mechanism that uses static profiling was also simulated with no BHT. These mechanisms are described as follows: 76 * Per-address History Two-Level Adaptive Branch Predictor Branch Predictor.
Reference: [44] <author> S. Melvin and Y.N. Patt, </author> <title> "Exploiting Fine-Grained Parallelism Through a Combination of Hardware and Software Techniques", </title> <booktitle> Proceedings of the 18th International Symposium on Computer Architecture, </booktitle> <pages> pp. 287-296, </pages> <month> May </month> <year> 1991. </year>
Reference-contexts: Aligning basic blocks on cache line boundaries eliminates the need of a self-aligned cache; however, it may increase the code size by inserting nops between basic blocks. When basic blocks are enlarged <ref> [19, 8, 44] </ref> or the frequently-executed basic blocks are placed sequentially [25], fewer instructions need to be thrown away in each fetch. An effective instruction fetch mechanism needs to predict all types of branches. <p> If a superscalar processor can only fetch one basic block a time, the processor's ability to execute multiple instructions is greatly constrained. One solution would be to have compilers enlarge basic blocks through trace scheduling [19], superblock scheduling [8], extended basic block <ref> [44] </ref>, or hyperblock scheduling [42]. Another solution might be to implement hardware which can fetch multiple non-sequential basic blocks each cycle [65]. Such a hardware solution requires a branch predictor that can predict multiple branch paths and multiple fetch addresses in a single cycle.
Reference: [45] <author> Motorola Inc., </author> <title> "M88100 User's Manual", </title> <address> Phoenix, Arizona, March 13, </address> <year> 1989. </year>
Reference-contexts: Since branch prediction is one of the major topics in this dissertation, it will be described in detail later. The other methods to hide the branch execution latency are described as follows. Delayed branching was a popular idea in many early RISC instruction set architectures (ISAs) <ref> [35, 30, 45, 11] </ref> because their early implementations had simple pipeline designs. This technique attempts to schedule instructions from the basic block in which the branch is located into the delay slots, allowing useful instructions to be executed while the branch is being resolved. <p> This can be done by either scheduling the condition generation far enough ahead of the branch or executing branches early in the pipes <ref> [45, 53] </ref>. Scheduling conditional generation far ahead of branches reduces branch resolution latency because it ensures that the condition is available for deciding the branch path when the branches are executed. <p> This also often limits how far the condition generation can be scheduled ahead of the branches. The branch resolution latency can be further shortened by executing branches early in the pipes <ref> [45, 53] </ref>. In the MC88100, while a branch is being decoded, its taken target address is calculated and condition is tested if its source operand is available. As a result, the branch is resolved before it enters the execution 9 stage, reducing the number of delay slots needed to one. <p> Branch execution latency can also be shortened by reducing the number of pipeline stages such as performing the condition test when the branch is being decoded <ref> [45] </ref> or when the branch is still in the instruction buffer [30]. Restoring the correct processor state quickly after an incorrect prediction is detected requires a good state maintenance mechanism. Three common mechanisms have been proposed: checkpointing [50, 24, 5], reorder buffers [58, 32], and history buffers [58, 46]. <p> The C source code of the function is shown in Figure 6.4. Its translated Motorola 88100 assembly code <ref> [45] </ref> and assigned instruction addresses are shown in Figure 6.5. In this example, we assume that the instruction cache line size is 16 bytes, one cache line is fetched at a time, and the instruction cache is not self-aligned.
Reference: [46] <author> Motorola Inc., </author> <title> "MC88110 Second Generation RISC Microprocessor User's Manual", </title> <address> Phoenix, Arizona, </address> <year> 1991. </year>
Reference-contexts: Restoring the correct processor state quickly after an incorrect prediction is detected requires a good state maintenance mechanism. Three common mechanisms have been proposed: checkpointing [50, 24, 5], reorder buffers [58, 32], and history buffers <ref> [58, 46] </ref>. The checkpoint mechanism in HPS [50] is able to restore the correct processor state immediately after an incorrect prediction is detected in the execution stage by discarding all information corresponding to the incorrectly-predicted paths.
Reference: [47] <author> F. Mueller and D. Whalley, </author> <title> "Avoiding Unconditional Jumps by Code Replication", </title> <booktitle> Proceedings of the ACM SIGPLAN '92 Conference on Programming Language Design and Implementation, </booktitle> <pages> pp. 322-330, </pages> <month> June </month> <year> 1992. </year>
Reference-contexts: Subroutine inlining, code replication, and basic block enlargement are software methods that reduce the number of branches. Subroutine inlining [27] eliminates subroutine calls by selectively expanding the subroutine body at the places where it is called. Code replication <ref> [47] </ref> reduces the number of unconditional branches by replacing branches with target instructions. Since these methods remove unconditional branches by replicating instructions, they require heuristics to avoid excessive code expansion.
Reference: [48] <author> S-T Pan, K. So, and J.T. Rahmeh, </author> <title> "Improving the Accuracy of Dynamic Branch Prediction Using Branch Correlation," </title> <booktitle> Proceedings of the 5th International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pp. 76-84, </pages> <month> Oct. </month> <year> 1992. </year>
Reference-contexts: Delayed branching [21] fills the branch delay slots (pipeline bubbles) with useful instructions; thus, the processor does not stall waiting for the branch to be resolved. However, filling the delay slots with useful instructions becomes increasingly difficult as the number of function units increases and pipelines deepen. Branch prediction <ref> [57, 19, 39, 43, 26, 61, 10, 48, 62] </ref> predicts the targets of branches in order to continue executing useful instructions while a branch is being resolved, i.e. the subsequent instructions are prefetched and executed speculatively. <p> Delayed branching [21, 12, 53, 40], multiple-stream execution [37, 59], and branch prediction <ref> [57, 19, 39, 43, 26, 61, 10, 48, 62] </ref> attempt to exploit this idea. Since branch prediction is one of the major topics in this dissertation, it will be described in detail later. The other methods to hide the branch execution latency are described as follows. <p> They use static information, dynamic branch history, or both. Several pipeline designs have been studied to reduce the branch misprediction penalty. They are briefly described and compared below. 2.3.1 Prediction Accuracy The literature contains many suggested branch prediction schemes <ref> [57, 19, 39, 43, 26, 61, 10, 48, 62] </ref>. Some are static in that they use only static information such as opcodes and profiling statistics to make predictions. The prediction of a static branch is determined before the program is executed and does not change during execution. <p> This yields nine variations: GAg, GAp, GAs, PAg, PAp, PAs, SAg, SAp, and SAs, as shown in Table 4.1. Variation Reference Description GAg [62] Global Adaptive Branch Prediction using one global pattern history table. GAs <ref> [48] </ref> Global Adaptive Branch Prediction using per-set pattern history tables. GAp [48] Global Adaptive Branch Prediction using per-address pattern history tables. PAg [61] Per-address Adaptive Branch Prediction using one global pattern history table. PAs [64] Per-address Adaptive Branch Prediction using per-set pattern history tables. <p> This yields nine variations: GAg, GAp, GAs, PAg, PAp, PAs, SAg, SAp, and SAs, as shown in Table 4.1. Variation Reference Description GAg [62] Global Adaptive Branch Prediction using one global pattern history table. GAs <ref> [48] </ref> Global Adaptive Branch Prediction using per-set pattern history tables. GAp [48] Global Adaptive Branch Prediction using per-address pattern history tables. PAg [61] Per-address Adaptive Branch Prediction using one global pattern history table. PAs [64] Per-address Adaptive Branch Prediction using per-set pattern history tables. PAp [62] Per-address Adaptive Branch Prediction using per-address pattern history tables. <p> These three classes are shown in Figures 4.3, 4.4, and 4.5. They are characterized as follows: Global History Schemes In the global history schemes (shown in Figure 4.3) (also called Correlation Branch Prediction by Pan et al. <ref> [48] </ref>), the first-level branch history means the actual last k branches encountered; therefore, only a single global history register (GHR) is used.
Reference: [49] <author> Y.N. Patt, W-M Hwu, and M. Shebanow "HPS, </author> <title> A New Microarchitecture: Rationale and Introduction", </title> <booktitle> Proceedings of the 18th Annual ACM/IEEE International Symposium on Computer Microarchitecture, </booktitle> <pages> pp. 103-108, </pages> <month> Dec. </month> <year> 1985. </year>
Reference-contexts: This processor issues up to four instructions each cycle. A branch is not resolved until its first execution stage.) In an aggressive superscalar processor architecture like HPS <ref> [49, 50] </ref>, multiple deep pipelines (as shown in and issued in one cycle. If a branch takes on the average three cycles to be resolved after it is fetched, the processor stalls for three cycles out of every four cycles, wasting about 75 percent of the execution bandwidth. <p> Inputs to Archsim are a binary file generated for the Motorola 88100 and the input data file of the benchmark. Archsim interprets the binary file, instruction by instruction, and produces a trace. The produced trace includes only the user-space instructions actually executed and successfully retired <ref> [49] </ref>, and excludes the instructions executed in the incorrect speculative paths and the instructions executed within system calls. Each unit of the instruction trace contains the virtual address and the opcode of one instruction. <p> This design can repair branch history correctly if branches are verified and repaired in program order. However, it may restore the speculative register to an older content when the branches can be verified and repaired out of the program order, as in HPS <ref> [49, 50, 24] </ref>. The correct branch history can actually be restored in the processors which verify and repair out of program order if we backup the contents of the history register when a prediction is made. However, restoring the BHT may take several cycles, thus degrading processor performance greatly. 2.
Reference: [50] <author> Y.N. Patt, S. Melvin, W-M Hwu, and M. </author> <title> Shebanow "Critical Issues Regarding HPS", </title> <booktitle> Proceedings of the 18th Annual ACM/IEEE International Symposium on Computer Microarchitecture, </booktitle> <pages> pp. 109-117, </pages> <month> Dec. </month> <year> 1985. </year>
Reference-contexts: This processor issues up to four instructions each cycle. A branch is not resolved until its first execution stage.) In an aggressive superscalar processor architecture like HPS <ref> [49, 50] </ref>, multiple deep pipelines (as shown in and issued in one cycle. If a branch takes on the average three cycles to be resolved after it is fetched, the processor stalls for three cycles out of every four cycles, wasting about 75 percent of the execution bandwidth. <p> Restoring the correct processor state quickly after an incorrect prediction is detected requires a good state maintenance mechanism. Three common mechanisms have been proposed: checkpointing <ref> [50, 24, 5] </ref>, reorder buffers [58, 32], and history buffers [58, 46]. The checkpoint mechanism in HPS [50] is able to restore the correct processor state immediately after an incorrect prediction is detected in the execution stage by discarding all information corresponding to the incorrectly-predicted paths. <p> Restoring the correct processor state quickly after an incorrect prediction is detected requires a good state maintenance mechanism. Three common mechanisms have been proposed: checkpointing [50, 24, 5], reorder buffers [58, 32], and history buffers [58, 46]. The checkpoint mechanism in HPS <ref> [50] </ref> is able to restore the correct processor state immediately after an incorrect prediction is detected in the execution stage by discarding all information corresponding to the incorrectly-predicted paths. <p> This design can repair branch history correctly if branches are verified and repaired in program order. However, it may restore the speculative register to an older content when the branches can be verified and repaired out of the program order, as in HPS <ref> [49, 50, 24] </ref>. The correct branch history can actually be restored in the processors which verify and repair out of program order if we backup the contents of the history register when a prediction is made. However, restoring the BHT may take several cycles, thus degrading processor performance greatly. 2.
Reference: [51] <author> D.A. Patterson and C.H. Sequin, "RISC-I: </author> <title> A Reduced Instruction Set VLSI Computer", </title> <booktitle> Proceedings of the 8th International Symposium on Computer Architecture, </booktitle> <pages> pp. 443-458, </pages> <month> May. </month> <year> 1981. </year>
Reference: [52] <author> C.H. Perleberg and A.J. Smith, </author> <title> "Branch Target Buffer Design and Optimization", </title> <type> Technical Report No. </type> <institution> UCB/CSD 89/552, Department of EECS, University of California, Berkeley, </institution> <month> Dec. </month> <year> 1989. </year>
Reference-contexts: The important factors are the size, set associativity, allocation policy, and index scheme. A high associativity or a larger BHT helps to improve the hit rate. Allocation policy is important in utilizing limited BHT entries. In branch target buffer designs <ref> [4, 52] </ref>, the entries can be conserved through caching only the targets of taken branches, because the target instructions of fall-through branches are available by fetching sequentially.
Reference: [53] <author> A.R. Pleszkun, J.R. Goodman, W-C. Hsu, and R.T. Joersz, "WISQ: </author> <title> A Restartable Architecture Using Queues", </title> <booktitle> Proceedings of the 14th International Symposium on Computer Architecture, </booktitle> <pages> pp. 290-299, </pages> <month> June </month> <year> 1987. </year> <month> 101 </month>
Reference-contexts: They solve the branch problem in four ways: hide branch execution latency, reduce branch resolution latency, reduce instruction fetch latency, and eliminate branches. 2.2.1 Hide Branch Execution Latency A processor can hide branch execution latency by doing useful work while a branch is being resolved. Delayed branching <ref> [21, 12, 53, 40] </ref>, multiple-stream execution [37, 59], and branch prediction [57, 19, 39, 43, 26, 61, 10, 48, 62] attempt to exploit this idea. Since branch prediction is one of the major topics in this dissertation, it will be described in detail later. <p> This can be done by either scheduling the condition generation far enough ahead of the branch or executing branches early in the pipes <ref> [45, 53] </ref>. Scheduling conditional generation far ahead of branches reduces branch resolution latency because it ensures that the condition is available for deciding the branch path when the branches are executed. <p> This also often limits how far the condition generation can be scheduled ahead of the branches. The branch resolution latency can be further shortened by executing branches early in the pipes <ref> [45, 53] </ref>. In the MC88100, while a branch is being decoded, its taken target address is calculated and condition is tested if its source operand is available. As a result, the branch is resolved before it enters the execution 9 stage, reducing the number of delay slots needed to one. <p> As a result, the branch is resolved before it enters the execution 9 stage, reducing the number of delay slots needed to one. The IBM RS/6000 [30] and the WISQ <ref> [53] </ref> generate the target address and tests the condition in an even earlier stage. Once those processors detect a branch instruction in the instruction buffer, the target address of the branch is calculated and the condition is tested when the condition becomes available.
Reference: [54] <author> B.R. Rau, D. Yen, W. Yen, and R. Towle, </author> <title> "The Cydra 5 Departmental Supercomputer Design Philosophies, Decisions, and Trade-offs", </title> <booktitle> IEEE Computer, </booktitle> <pages> pp. 12-35, </pages> <month> Jan. </month> <year> 1989. </year>
Reference-contexts: Compiler technologies that enlarge basic blocks such as loop unrolling and jamming reduce the number of branches executed by combining loop iterations. If an ISA supports predicated execution or conditional moves, conditional branches can be removed using the predicated (guarded) execution <ref> [15, 23, 9, 54, 42] </ref> or conditional moves [14]. In predicated execution, instructions from both paths of a branch are predicated with the condition of the original logic expression. In some implementations, only the instructions whose predicates match the resolution of the corresponding branch are executed.
Reference: [55] <author> E.M. Riseman and C.C. Foster, </author> <title> "The Inhibition of Potential Parallelism by Conditional Jumps", </title> <journal> IEEE Transactions on Computers, </journal> <pages> pp. 1405-1415, </pages> <month> Dec. </month> <year> 1992. </year>
Reference-contexts: Since the parallelism within basic blocks is limited, parallelism across basic blocks must be used. Riseman and Foster <ref> [55] </ref> have shown that if the exploitation of parallelism is limited to inside basic blocks, even with an ideal processor, the speedup is only 1.72 times as fast as the conventional sequential processor. Finding available instruction level parallelism across basic blocks relies heavily on the performance of branch prediction.
Reference: [56] <author> R.L. </author> <title> Sites, "Alpha Architecture Reference Manual", </title> <publisher> Digital Press, </publisher> <address> Burlington, MA, </address> <year> 1992. </year>
Reference-contexts: The filling of the delay slots becomes increasingly difficult as their number increases, as in the case of deeply-pipelined superscalar processors. Not enough instructions can be found to fill the delay slots, causing delayed branches to become ineffective in aggressive processor designs. For this reason, DEC Alpha <ref> [56] </ref> excludes delayed branching. Instruction latency can be hidden by switching between multiple instruction streams (I-streams). The HEP processor [37, 59] switches to another I-stream after a branch is encountered in its current I-stream. The same I-stream is switched back after the branch is resolved.
Reference: [57] <author> J.E. Smith, </author> <title> "A Study of Branch Prediction Strategies", </title> <booktitle> Proceedings of the 8th International Symposium on Computer Architecture, </booktitle> <pages> pp. 135-148, </pages> <month> May. </month> <year> 1981. </year>
Reference-contexts: Delayed branching [21] fills the branch delay slots (pipeline bubbles) with useful instructions; thus, the processor does not stall waiting for the branch to be resolved. However, filling the delay slots with useful instructions becomes increasingly difficult as the number of function units increases and pipelines deepen. Branch prediction <ref> [57, 19, 39, 43, 26, 61, 10, 48, 62] </ref> predicts the targets of branches in order to continue executing useful instructions while a branch is being resolved, i.e. the subsequent instructions are prefetched and executed speculatively. <p> Moreover, the difference in accuracy is even greater when branches behave differently during different periods of execution. Compared with other history-based dynamic branch predictors such as the two-bit counter <ref> [57, 39] </ref> and the last-time scheme [57, 39, 14], Two-Level Adaptive Branch Prediction predicts better by distinguishing more branch execution states. Two-Level Adaptive Branch Prediction uses shift register (s) to keep the first-level branch history. <p> Moreover, the difference in accuracy is even greater when branches behave differently during different periods of execution. Compared with other history-based dynamic branch predictors such as the two-bit counter [57, 39] and the last-time scheme <ref> [57, 39, 14] </ref>, Two-Level Adaptive Branch Prediction predicts better by distinguishing more branch execution states. Two-Level Adaptive Branch Prediction uses shift register (s) to keep the first-level branch history. <p> Delayed branching [21, 12, 53, 40], multiple-stream execution [37, 59], and branch prediction <ref> [57, 19, 39, 43, 26, 61, 10, 48, 62] </ref> attempt to exploit this idea. Since branch prediction is one of the major topics in this dissertation, it will be described in detail later. The other methods to hide the branch execution latency are described as follows. <p> They use static information, dynamic branch history, or both. Several pipeline designs have been studied to reduce the branch misprediction penalty. They are briefly described and compared below. 2.3.1 Prediction Accuracy The literature contains many suggested branch prediction schemes <ref> [57, 19, 39, 43, 26, 61, 10, 48, 62] </ref>. Some are static in that they use only static information such as opcodes and profiling statistics to make predictions. The prediction of a static branch is determined before the program is executed and does not change during execution. <p> They can be as simple as predicting that the branch will always be taken, or they can be based on the opcode, or on the direction of the branch, as in "if the branch is backward, predict taken; if forward, predict not taken (BTFN)" <ref> [57] </ref>. The latter scheme is effective for loop intensive code, but does not work well for the programs which contain many irregular non-loop branches. <p> In all cases, the fact that dynamic prediction is made on the basis of run-time history information implies that additional hardware is required. * Last-Time <ref> [57] </ref> A branch can be predicted as always taking the same path as in its previous execution. This scheme requires only one bit for each branch to store the result of the previous execution. It results in an incorrect prediction whenever a conditional branch changes its branch path. <p> The DEC Alpha 21064 implements this scheme by storing an extra bit with each instruction in the tag store of the instruction cache. The bit is set if the instruction is a branch and was taken in its last execution. * Two-bit counter scheme <ref> [57, 39] </ref> A branch can be predicted using the results of the last several executions collected by a two-bit saturating up-down counter. The counter is incremented by one whenever a branch is taken and is decremented whenever a branch is fall-through. <p> Since the scheduling window size shows the scope in which instructions can be scheduled effectively, the branch predictor is one of the important components in extracting instruction level parallelism. Using J. Smith's two-bit counter <ref> [57] </ref>, Wall [60] has shown that the average available instruction level parallelism drops significantly to around 7 in an ideal processor with infinite hardware. Whenever an incorrect branch prediction is made, the scheduling window is interrupted. <p> Only when there is no taken branch recorded, the next prediction is not taken; otherwise, the next prediction is taken. The automaton 2-bit Counter is a 2-bit saturating up-down counter, similar to the automaton used in J. Smith's branch target buffer design for keeping branch history <ref> [57] </ref>. <p> This section describes how to incorporate a global history predictor or a per-address history branch predictor into the instruction fetch mechanism. For comparison, this section also describes the mechanisms that use 2-bit counter <ref> [57] </ref> and static profiling [43, 26]. All the mechanisms use a BHT; however, to show the effect of using a BHT, the mechanism that uses static profiling was also simulated with no BHT. These mechanisms are described as follows: 76 * Per-address History Two-Level Adaptive Branch Predictor Branch Predictor.
Reference: [58] <author> J.E. Smith and A.R. Pleszkun, </author> <title> "Implementation of Precise Interrupts in Pipelined Processors", </title> <booktitle> Proceedings of the 12th International Symposium on Computer Architecture, </booktitle> <pages> pp. 36-44, </pages> <month> June </month> <year> 1985. </year>
Reference-contexts: Restoring the correct processor state quickly after an incorrect prediction is detected requires a good state maintenance mechanism. Three common mechanisms have been proposed: checkpointing [50, 24, 5], reorder buffers <ref> [58, 32] </ref>, and history buffers [58, 46]. The checkpoint mechanism in HPS [50] is able to restore the correct processor state immediately after an incorrect prediction is detected in the execution stage by discarding all information corresponding to the incorrectly-predicted paths. <p> Restoring the correct processor state quickly after an incorrect prediction is detected requires a good state maintenance mechanism. Three common mechanisms have been proposed: checkpointing [50, 24, 5], reorder buffers [58, 32], and history buffers <ref> [58, 46] </ref>. The checkpoint mechanism in HPS [50] is able to restore the correct processor state immediately after an incorrect prediction is detected in the execution stage by discarding all information corresponding to the incorrectly-predicted paths.
Reference: [59] <author> M.R. Thistle and B.J. Smith, </author> <title> "A Processor Architecture for Horizon", </title> <booktitle> Proceedings of Supercomputing Conference, </booktitle> <address> Orlando, Florida, </address> <pages> pp. 35-41, </pages> <month> Nov. </month> <year> 1988. </year>
Reference-contexts: Branch prediction [57, 19, 39, 43, 26, 61, 10, 48, 62] predicts the targets of branches in order to continue executing useful instructions while a branch is being resolved, i.e. the subsequent instructions are prefetched and executed speculatively. Using multiple stream execution <ref> [37, 59] </ref> a processor switches to a different instruction stream when a branch is encountered in the current instruction stream. Therefore, while waiting for the branch to be resolved, the processor can also perform useful work. Removing branches is another approach to solving the branch problem. <p> Delayed branching [21, 12, 53, 40], multiple-stream execution <ref> [37, 59] </ref>, and branch prediction [57, 19, 39, 43, 26, 61, 10, 48, 62] attempt to exploit this idea. Since branch prediction is one of the major topics in this dissertation, it will be described in detail later. <p> Not enough instructions can be found to fill the delay slots, causing delayed branches to become ineffective in aggressive processor designs. For this reason, DEC Alpha [56] excludes delayed branching. Instruction latency can be hidden by switching between multiple instruction streams (I-streams). The HEP processor <ref> [37, 59] </ref> switches to another I-stream after a branch is encountered in its current I-stream. The same I-stream is switched back after the branch is resolved. As a result, while the branch is being executed, the processor continues to do useful work.
Reference: [60] <author> D.W. Wall, </author> <title> "Limits of Instruction-Level Parallelism", </title> <booktitle> Proceedings of the 4th International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pp. 176-188, </pages> <month> April </month> <year> 1991. </year>
Reference-contexts: This dramatic speedup comes from the large collection of instructions called the scheduling window from which instructions can be found to execute in parallel. Butler et al. [6], Wall <ref> [60] </ref>, and Lam [38] all have reported that there is significant parallelism (in the range of 15 to 3000 instructions each cycle) available when the results of branch executions are known in advance. However, in practical processors, the result of branch execution is not known in advance. <p> Since the scheduling window size shows the scope in which instructions can be scheduled effectively, the branch predictor is one of the important components in extracting instruction level parallelism. Using J. Smith's two-bit counter [57], Wall <ref> [60] </ref> has shown that the average available instruction level parallelism drops significantly to around 7 in an ideal processor with infinite hardware. Whenever an incorrect branch prediction is made, the scheduling window is interrupted.
Reference: [61] <author> T-Y Yeh and Y.N. Patt, </author> <title> "Two-Level Adaptive Branch Prediction", </title> <booktitle> Proceedings of the 24th ACM/IEEE International Symposium and Workshop on Microarchitecture, </booktitle> <pages> pp. 51-61, </pages> <month> Nov. </month> <year> 1991. </year>
Reference-contexts: Delayed branching [21] fills the branch delay slots (pipeline bubbles) with useful instructions; thus, the processor does not stall waiting for the branch to be resolved. However, filling the delay slots with useful instructions becomes increasingly difficult as the number of function units increases and pipelines deepen. Branch prediction <ref> [57, 19, 39, 43, 26, 61, 10, 48, 62] </ref> predicts the targets of branches in order to continue executing useful instructions while a branch is being resolved, i.e. the subsequent instructions are prefetched and executed speculatively. <p> Delayed branching [21, 12, 53, 40], multiple-stream execution [37, 59], and branch prediction <ref> [57, 19, 39, 43, 26, 61, 10, 48, 62] </ref> attempt to exploit this idea. Since branch prediction is one of the major topics in this dissertation, it will be described in detail later. The other methods to hide the branch execution latency are described as follows. <p> They use static information, dynamic branch history, or both. Several pipeline designs have been studied to reduce the branch misprediction penalty. They are briefly described and compared below. 2.3.1 Prediction Accuracy The literature contains many suggested branch prediction schemes <ref> [57, 19, 39, 43, 26, 61, 10, 48, 62] </ref>. Some are static in that they use only static information such as opcodes and profiling statistics to make predictions. The prediction of a static branch is determined before the program is executed and does not change during execution. <p> Variation Reference Description GAg [62] Global Adaptive Branch Prediction using one global pattern history table. GAs [48] Global Adaptive Branch Prediction using per-set pattern history tables. GAp [48] Global Adaptive Branch Prediction using per-address pattern history tables. PAg <ref> [61] </ref> Per-address Adaptive Branch Prediction using one global pattern history table. PAs [64] Per-address Adaptive Branch Prediction using per-set pattern history tables. PAp [62] Per-address Adaptive Branch Prediction using per-address pattern history tables. SAg [64] Per-Set Adaptive Branch Prediction using one global pattern history table. <p> However, the correct RAS may not be restored if the branches are verified out of the program order because some previous returns or subroutine calls have not yet retired. 6.6 Conditional Branch Instructions In our proposed instruction fetch mechanism, Two-Level Adaptive Branch Prediction <ref> [61, 62, 64] </ref> is used for making branch predictions because it achieves higher accuracy than other branch predictors. This section describes how to incorporate a global history predictor or a per-address history branch predictor into the instruction fetch mechanism.
Reference: [62] <author> T-Y Yeh and Y.N. </author> <title> Patt "Alternative Implementations of Two-Level Adaptive Branch Prediction," </title> <booktitle> Proceedings of the 19th International Symposium on Computer Architecture, </booktitle> <pages> pp. 124-134, </pages> <month> May </month> <year> 1992. </year>
Reference-contexts: Delayed branching [21] fills the branch delay slots (pipeline bubbles) with useful instructions; thus, the processor does not stall waiting for the branch to be resolved. However, filling the delay slots with useful instructions becomes increasingly difficult as the number of function units increases and pipelines deepen. Branch prediction <ref> [57, 19, 39, 43, 26, 61, 10, 48, 62] </ref> predicts the targets of branches in order to continue executing useful instructions while a branch is being resolved, i.e. the subsequent instructions are prefetched and executed speculatively. <p> Delayed branching [21, 12, 53, 40], multiple-stream execution [37, 59], and branch prediction <ref> [57, 19, 39, 43, 26, 61, 10, 48, 62] </ref> attempt to exploit this idea. Since branch prediction is one of the major topics in this dissertation, it will be described in detail later. The other methods to hide the branch execution latency are described as follows. <p> They use static information, dynamic branch history, or both. Several pipeline designs have been studied to reduce the branch misprediction penalty. They are briefly described and compared below. 2.3.1 Prediction Accuracy The literature contains many suggested branch prediction schemes <ref> [57, 19, 39, 43, 26, 61, 10, 48, 62] </ref>. Some are static in that they use only static information such as opcodes and profiling statistics to make predictions. The prediction of a static branch is determined before the program is executed and does not change during execution. <p> The change of data sets usually results in changing the behavior of the branches whose results depend on input data. Another disadvantage is that this scheme requires profiling and extra storage for the profiled information. * Static Training using Global History <ref> [62] </ref> Static Training uses the history collected for individual branches. The history used can also be global history, the history of all the conditional branches. <p> This yields nine variations: GAg, GAp, GAs, PAg, PAp, PAs, SAg, SAp, and SAs, as shown in Table 4.1. Variation Reference Description GAg <ref> [62] </ref> Global Adaptive Branch Prediction using one global pattern history table. GAs [48] Global Adaptive Branch Prediction using per-set pattern history tables. GAp [48] Global Adaptive Branch Prediction using per-address pattern history tables. PAg [61] Per-address Adaptive Branch Prediction using one global pattern history table. <p> GAs [48] Global Adaptive Branch Prediction using per-set pattern history tables. GAp [48] Global Adaptive Branch Prediction using per-address pattern history tables. PAg [61] Per-address Adaptive Branch Prediction using one global pattern history table. PAs [64] Per-address Adaptive Branch Prediction using per-set pattern history tables. PAp <ref> [62] </ref> Per-address Adaptive Branch Prediction using per-address pattern history tables. SAg [64] Per-Set Adaptive Branch Prediction using one global pattern history table. SAs [64] Per-Set Adaptive Branch Prediction using per-set pattern history tables. SAp [64] Per-Set Adaptive Branch Prediction using per-address pattern history tables. <p> However, the correct RAS may not be restored if the branches are verified out of the program order because some previous returns or subroutine calls have not yet retired. 6.6 Conditional Branch Instructions In our proposed instruction fetch mechanism, Two-Level Adaptive Branch Prediction <ref> [61, 62, 64] </ref> is used for making branch predictions because it achieves higher accuracy than other branch predictors. This section describes how to incorporate a global history predictor or a per-address history branch predictor into the instruction fetch mechanism. <p> costs for storing branch target and fall-through addresses but including the costs of golden history registers, GAs (11; 32) costs about 128K bits, P As (6; 16) costs about 8.5K bits, GAs (7; 32) costs about 8K bits, 2-bit Counter costs about 2K bits, and Profiling costs about 0.5K bits <ref> [62] </ref>. each column, the dark portion shows the prediction accuracy of the branches which miss in the BHT and the dotted portion shows the prediction accuracy of the branches which hit in the BHT. GAs (11; 32) achieves the highest average prediction accuracy of 97%.
Reference: [63] <author> T-Y Yeh and Y.N. </author> <title> Patt "A Comprehensive Instruction Fetch Mechanism for a Processor Supporting Speculative Execution," </title> <booktitle> Proceedings of the 25th Annual ACM/IEEE International Symposium on Computer Microarchitecture, </booktitle> <pages> pp. 129-139, </pages> <month> Dec. </month> <year> 1992. </year>
Reference-contexts: Since caching taken target and fall-through target addresses is necessary for making branch predictions prior to decode in order to eliminate the delay for target address generation <ref> [63] </ref>, all the variations in this research cache the target addresses in the BHT. However, some variations of Two-Level Adaptive Branch Prediction require storing additional information in the BHT. <p> Classifying branches using the opcode has also been investigated; however, the prediction accuracy is lower than using the branch address. * Every scheme is configured with a branch history table for storing the target addresses <ref> [63] </ref> of both conditional and unconditional branches. The branch history table is accessed using the low-order bits of the branch identifying address. For per-address history schemes, each branch history table entry also records the branch history.
Reference: [64] <author> T-Y Yeh and Y.N. </author> <title> Patt "A Comparison of Dynamic Branch Predictors that use Two Levels of Branch History," </title> <booktitle> Proceedings of the 20th International Symposium on Computer Architecture, </booktitle> <month> May </month> <year> 1993. </year>
Reference-contexts: GAs [48] Global Adaptive Branch Prediction using per-set pattern history tables. GAp [48] Global Adaptive Branch Prediction using per-address pattern history tables. PAg [61] Per-address Adaptive Branch Prediction using one global pattern history table. PAs <ref> [64] </ref> Per-address Adaptive Branch Prediction using per-set pattern history tables. PAp [62] Per-address Adaptive Branch Prediction using per-address pattern history tables. SAg [64] Per-Set Adaptive Branch Prediction using one global pattern history table. SAs [64] Per-Set Adaptive Branch Prediction using per-set pattern history tables. SAp [64] Per-Set Adaptive Branch Prediction using <p> GAp [48] Global Adaptive Branch Prediction using per-address pattern history tables. PAg [61] Per-address Adaptive Branch Prediction using one global pattern history table. PAs <ref> [64] </ref> Per-address Adaptive Branch Prediction using per-set pattern history tables. PAp [62] Per-address Adaptive Branch Prediction using per-address pattern history tables. SAg [64] Per-Set Adaptive Branch Prediction using one global pattern history table. SAs [64] Per-Set Adaptive Branch Prediction using per-set pattern history tables. SAp [64] Per-Set Adaptive Branch Prediction using per-address pattern history tables. Table 4.1: Variations of Two-Level Adaptive Branch Prediction. <p> PAg [61] Per-address Adaptive Branch Prediction using one global pattern history table. PAs <ref> [64] </ref> Per-address Adaptive Branch Prediction using per-set pattern history tables. PAp [62] Per-address Adaptive Branch Prediction using per-address pattern history tables. SAg [64] Per-Set Adaptive Branch Prediction using one global pattern history table. SAs [64] Per-Set Adaptive Branch Prediction using per-set pattern history tables. SAp [64] Per-Set Adaptive Branch Prediction using per-address pattern history tables. Table 4.1: Variations of Two-Level Adaptive Branch Prediction. <p> PAs <ref> [64] </ref> Per-address Adaptive Branch Prediction using per-set pattern history tables. PAp [62] Per-address Adaptive Branch Prediction using per-address pattern history tables. SAg [64] Per-Set Adaptive Branch Prediction using one global pattern history table. SAs [64] Per-Set Adaptive Branch Prediction using per-set pattern history tables. SAp [64] Per-Set Adaptive Branch Prediction using per-address pattern history tables. Table 4.1: Variations of Two-Level Adaptive Branch Prediction. Based on the source of the first level history, these nine variations of Two-Level Adaptive Branch Prediction can be classified into three classes. <p> However, the correct RAS may not be restored if the branches are verified out of the program order because some previous returns or subroutine calls have not yet retired. 6.6 Conditional Branch Instructions In our proposed instruction fetch mechanism, Two-Level Adaptive Branch Prediction <ref> [61, 62, 64] </ref> is used for making branch predictions because it achieves higher accuracy than other branch predictors. This section describes how to incorporate a global history predictor or a per-address history branch predictor into the instruction fetch mechanism.
Reference: [65] <author> T-Y Yeh, D. Marr, and Y.N. </author> <title> Patt "Increasing Instruction Fetch Rate via Multiple Branch Predictions and a Branch Address Cache," </title> <booktitle> Proceedings of the 7th ACM International Conference on Supercomputing, </booktitle> <pages> pp. 67-76, </pages> <month> July </month> <year> 1993. </year>
Reference-contexts: Another option is to fetch two sequential cache lines at a time. However, this requires modifications to the above index schemes. When basic blocks are small, fetching multiple basic blocks <ref> [65] </ref> is needed to supply enough instructions. <p> One solution would be to have compilers enlarge basic blocks through trace scheduling [19], superblock scheduling [8], extended basic block [44], or hyperblock scheduling [42]. Another solution might be to implement hardware which can fetch multiple non-sequential basic blocks each cycle <ref> [65] </ref>. Such a hardware solution requires a branch predictor that can predict multiple branch paths and multiple fetch addresses in a single cycle. Further work is needed to reduce the implementation cost of such a mechanism.
References-found: 65

