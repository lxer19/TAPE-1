URL: ftp://ftp.research.microsoft.com/pub/debull/dec98-letfinal.ps
Refering-URL: http://www.research.microsoft.com/research/db/debull/issues-list.htm
Root-URL: http://www.research.microsoft.com
Title: Special Issue on Data Replication  
Keyword: User Interfaces to Data Intensive Systems back cover  
Note: Bulletin of the Technical Committee on Data Engineering December 1998 Vol. 21 No. 4 IEEE Computer Society Letters Letter from the Editor-in-Chief. .David Lomet 1  Conference and Journal Notices ICDE'2000 Data Engineering Conference 44  
Abstract: Letter from the Special Issue Editors. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .Divyakant Agrawal and Amr El Abbadi 2 Quorum Systems in Replicated Databases: Science or Fiction? . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Avishai Wool 3 The Case for Non-transparent Replication: Examples from Bayou . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Douglas B. Terry, Karin Petersen, Mike J. Spreitzer, and Marvin M. Theimer 12 Issues in Web Content Replication . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Michael Rabinovich 21 Consensus-Based Management of Distributed and Replicated Data . . . . . . . . . . . . . . . . . . . . . . . . Michel Raynal 30 Replication Strategies for High Availability and Disaster Recovery . . . . . . . . . . . . . . . . . . . . . . . . . . Robert Breton 38 
Abstract-found: 1
Intro-found: 1
Reference: [ABKW98] <author> T. Anderson, Y. Breitbart, H. F. Korth, and A. Wool. </author> <title> Replication, consistency, and practicality: Are these mutually exclusive? In Proc. </title> <booktitle> ACM SIGMOD Inter. Conf. Management of Data, </booktitle> <pages> pages 484495, </pages> <address> Seattle, </address> <month> June </month> <year> 1998. </year>
Reference-contexts: These protocols have yet to be implemented in an actual database, but simulations of their performance over WANs <ref> [ABKW98] </ref> show that the network is not a bottleneck for either protocol. We can conclude that while the local read paradigm was a valid objection to quorum-based replication for many years, its validity is diminishing with time. Remote reads should no longer be considered to be prohibitive a priori.
Reference: [ADKM92] <author> Y. Amir, D. Dolev, S. Kramer, and D. Malki. Transis: </author> <title> A communication subsystem for high availability. </title> <booktitle> In Proc. 22nd IEEE Symp. Fault-Tolerant Computing (FTCS), </booktitle> <pages> pages 7684, </pages> <year> 1992. </year>
Reference-contexts: now owned by Oracle but originated on Digital's VMS systems [RDB98], uses the VAX Cluster's lock manager but does not use a quorum-based replication scheme. 7 3.2 Group Communication Systems Another successful application of quorum systems is in the context of reliable group communication services such as Isis [BvR94], Transis <ref> [ADKM92] </ref>, Totem [AMM + 95], Horus [vRBM96], and others. These are fault-tolerant communication middleware subsystems, that provide guarantees on the order of messages that are delivered to servers in a group.
Reference: [AE91] <author> D. Agrawal and A. El-Abbadi. </author> <title> An efficient and fault-tolerant solution for distributed mutual exclusion. </title> <journal> ACM Trans. Comp. Sys., </journal> <volume> 9(1):120, </volume> <year> 1991. </year>
Reference-contexts: Availability is the probability that the member servers of at least one quorum are functioning, assuming that the servers crash independently with a fixed probability, and that the complete database is replicated at every server. Many quorum systems, such as those of <ref> [KC91, Kum91, AE91, RST92, PW97b, PW97a, Baz96] </ref>, exhibit very high availability, and moreover, their availability tends to 1 very rapidly as the system scales up and more servers are added. Despite these features, quorum systems are not in wide-spread use within commercial replicated databases. <p> In general, r = w p n for this system, so the frequency of writes needs only to exceed 1= n for the quorum-based approach to be advantageous. Quorum systems with variable-sized quorums, such as those of <ref> [AE91, PW97a] </ref>, offer the possibility of outperforming ROWA for even lower write rates, by directing reads to smaller sized quorums, which can be as small as r log n for both systems. Clearly, as the number of replicas grows, quorum systems become advantageous for lower write rates.
Reference: [AMM + 95] <author> Y. Amir, L. E. Moser, P. M. Melliar-Smith, D. A. Agarwal, and P. Ciarfella. </author> <title> The Totem single-ring ordering and membership protocol. </title> <journal> ACM Trans. Comp. Sys., </journal> <volume> 13(4), </volume> <year> 1995. </year>
Reference-contexts: by Oracle but originated on Digital's VMS systems [RDB98], uses the VAX Cluster's lock manager but does not use a quorum-based replication scheme. 7 3.2 Group Communication Systems Another successful application of quorum systems is in the context of reliable group communication services such as Isis [BvR94], Transis [ADKM92], Totem <ref> [AMM + 95] </ref>, Horus [vRBM96], and others. These are fault-tolerant communication middleware subsystems, that provide guarantees on the order of messages that are delivered to servers in a group.
Reference: [AW96] <author> Y. Amir and A. Wool. </author> <title> Evaluating quorum systems over the Internet. </title> <booktitle> In Proc. 26'th IEEE Symp. Fault-Tolerant Computing (FTCS), </booktitle> <pages> pages 2635, </pages> <address> Sendai, Japan, </address> <year> 1996. </year>
Reference-contexts: Assumptions 1 and 2 are not so easily dealt with, as seen from the experiments of <ref> [AW96] </ref>. These experiments measured crashes and network connectivity over a period of 6 months, on a system that consisted of 14 machines on 3 LANs, split between two geographical locations 50 km apart, using Internet (UDP) communications. <p> Such a positive crash correlation has a dramatic effect on the availability: In the extreme case, if all the machines crash and revive at identical times, any quorum system will have precisely the same availability as that of a single server. In fact <ref> [AW96] </ref> observe that the most highly available machine on one of the LANs was down 1.25% of the time, yet using a majority-based quorum system with this machine and 5 others on the same LAN only reduced the unavailability to 1.12%. <p> This is a far smaller boost than that predicted under the independent crashes assumption. 6 As for WANs, the reverse situation occurs. The experiments of <ref> [AW96] </ref> show that crash independence be-tween geographically distant sites is a valid assumptionbut that the perfect communication assumption is not. The experimental system was partitioned into two or more disconnected components during 6.5% of the time. <p> In such a non-fully-connected network model, it is known that the optimal-availability quorum system is completely contained in a single bi-connected component [INK95]. And indeed the experiments of <ref> [AW96] </ref> showed that confining the quorum system to 6 machines on a single LAN gave better availability than using all 14 machines and allowing quorums to span the two sites.
Reference: [AW98] <author> Y. Amir and A. Wool. </author> <title> Optimal availability quorum systems: </title> <journal> Theory and practice. Inform. Process. Lett., </journal> <volume> 65:223228, </volume> <year> 1998. </year>
Reference-contexts: However, the quorum approach's promise of high availability was arguably its most desirable feature, and it certainly generated a substantial amount of research (e.g., <ref> [BG87, ET89, PW95, AW98] </ref>). Availability is the probability that the member servers of at least one quorum are functioning, assuming that the servers crash independently with a fixed probability, and that the complete database is replicated at every server. <p> One may hope that assuming perfect communication, for the purpose of availability analysis, would not change the qualitative predictions of the model. As we shall see, though, this modeling is problematic. Assumption 3 (uniform crash probability) simplifies the analysis but is not really crucial. It was shown by <ref> [SB94, AW98] </ref> that essentially the same properties hold with or without it, namely that the optimal availability quorum system is defined by weighted voting, where the optimal weights depend on the individual crash probabilities.
Reference: [Baz96] <author> R. A. Bazzi. </author> <title> Planar quorums. </title> <booktitle> In Proc. 10'th Inter. Workshop on Dist. Algorithms (WDAG), </booktitle> <address> Bologna, Italy, </address> <month> October </month> <year> 1996. </year>
Reference-contexts: Availability is the probability that the member servers of at least one quorum are functioning, assuming that the servers crash independently with a fixed probability, and that the complete database is replicated at every server. Many quorum systems, such as those of <ref> [KC91, Kum91, AE91, RST92, PW97b, PW97a, Baz96] </ref>, exhibit very high availability, and moreover, their availability tends to 1 very rapidly as the system scales up and more servers are added. Despite these features, quorum systems are not in wide-spread use within commercial replicated databases.
Reference: [BG86] <author> D. Barbara and H. Garcia-Molina. </author> <title> The vulnerability of vote assignments. </title> <journal> ACM Trans. Comp. Sys., </journal> <volume> 4(3):187 213, </volume> <year> 1986. </year>
Reference-contexts: Quorum systems were attractive to database researchers mainly because they offer a de-centralized approach that tolerates failures. For instance, quorum-based databases are able to maintain their consistency even in the presence of network partitions [DGS85] and server crashes (as quantified by their fault-tolerance <ref> [BG86] </ref>). However, the quorum approach's promise of high availability was arguably its most desirable feature, and it certainly generated a substantial amount of research (e.g., [BG87, ET89, PW95, AW98]).
Reference: [BG87] <author> D. Barbara and H. Garcia-Molina. </author> <title> The reliability of vote mechanisms. </title> <journal> IEEE Trans. Comput., </journal> <volume> C-36:1197 1208, </volume> <month> October </month> <year> 1987. </year>
Reference-contexts: However, the quorum approach's promise of high availability was arguably its most desirable feature, and it certainly generated a substantial amount of research (e.g., <ref> [BG87, ET89, PW95, AW98] </ref>). Availability is the probability that the member servers of at least one quorum are functioning, assuming that the servers crash independently with a fixed probability, and that the complete database is replicated at every server. <p> The reason is that several seemingly benign modeling assumptions are made in order to make the analysis of availability tractable. Unfortunately, these assumptions clash with the reality of network environments in crucial ways. The standard probabilistic assumptions which underly the definition of availability, as analyzed in <ref> [BG87, PW95] </ref> and others, are: 1. Crashes are independent events. 2. The communication network is fully connected and never fails. 3. All the servers have the same crash probability p. <p> If all the servers have the same crash probability p this voting system reduces to either a simple majority voting, which has optimal availability when p &lt; 1=2 <ref> [BG87] </ref>, or to a monarchy (single server) when p &gt; 1=2 [PW95]. Assumptions 1 and 2 are not so easily dealt with, as seen from the experiments of [AW96].
Reference: [Bir98] <author> K. P. </author> <type> Birman. </type> <institution> Talk at Bell Labs, </institution> <address> Murray Hill, NJ, </address> <month> October </month> <year> 1998. </year>
Reference-contexts: Group communication systems have made their way from the laboratories of academia where they were invented into the commercial world. Critical, real life applications such as the French air traffic control systems and the Swiss stock exchange now use them <ref> [Bir98] </ref>. Nevertheless, these systems do not provide transaction processing semantics. Typically they do not even make their internal quorum system structure visible through their application interface (API), e.g., they do not provide a multicast to a quorum service.
Reference: [BK97] <author> Y. Breitbart and H. F. Korth. </author> <title> Replication and consistency: Being lazy helps sometimes. </title> <booktitle> In Proc. 16th ACM SIGACT-SIGMOD Symp. Princip. of Database Systems (PODS), </booktitle> <pages> pages 173184, </pages> <address> Tucson, Arizona, </address> <month> May </month> <year> 1997. </year>
Reference-contexts: For instance, the protocol of [GHOS96] includes obtaining locks from remote machines even for read-only transactions, and the protocols of <ref> [BK97] </ref> include obtaining read, write, and commit permissions from a replication-graph-manager machine. These protocols have yet to be implemented in an actual database, but simulations of their performance over WANs [ABKW98] show that the network is not a bottleneck for either protocol.
Reference: [BvR94] <author> K. P. Birman and R. van Renesse. </author> <title> Reliable Distributed Computing with the Isis Toolkit. </title> <publisher> IEEE Computer Society Press, Los Alamitos, </publisher> <address> CA, </address> <year> 1994. </year>
Reference-contexts: which is now owned by Oracle but originated on Digital's VMS systems [RDB98], uses the VAX Cluster's lock manager but does not use a quorum-based replication scheme. 7 3.2 Group Communication Systems Another successful application of quorum systems is in the context of reliable group communication services such as Isis <ref> [BvR94] </ref>, Transis [ADKM92], Totem [AMM + 95], Horus [vRBM96], and others. These are fault-tolerant communication middleware subsystems, that provide guarantees on the order of messages that are delivered to servers in a group.
Reference: [CL98] <author> Credit Lyonnais: </author> <title> VMS clusters' trial by fire, </title> <month> April </month> <year> 1998. </year> <note> Available from http://www.success-stories.digital.com/css/cgi-bin/cssextusr/s=sublist? Customer+Name=Credit+Lyonnais. </note>
Reference-contexts: This technology is especially attractive to financial applications, for the following reasons: (i) strict data consistency is crucial; (ii) high reliability is required against disasters such as the 1993 World Trade Center explosion and the fire at the Credit Lyonnais bank <ref> [CL98] </ref>; and (iii) large financial institutions are able to bear the cost of the dedicated communication networks. Note that the VAX Cluster's quorum system does not provide a quorum-based replicated database per se. The system supports file-system semantics but does not provide transaction processing.
Reference: [CRR96] <author> P. Chundi, D. J. Rosenkrantz, and S. S. Ravi. </author> <title> Deferred updates and data placement in distributed databases. </title> <booktitle> In Proc. 12th IEEE Int. Conf. Data Engineering, </booktitle> <address> New Orleans, Louisiana, </address> <year> 1996. </year>
Reference-contexts: It is well known that such schemes do not guarantee serializability, and in fact serializability can only be guaranteed if the data placement graph is acyclic <ref> [CRR96] </ref>. There are some valid reasons why quorum systems have not yet been used in replicated databases. In the next section we shall list the more common criticisms, and discuss whether they are still valid today.
Reference: [DGS85] <author> S. B. Davidson, H. Garcia-Molina, and D. Skeen. </author> <title> Consistency in partitioned networks. </title> <journal> ACM Computing Surveys, </journal> <volume> 17(3):341370, </volume> <year> 1985. </year>
Reference-contexts: Quorum systems were attractive to database researchers mainly because they offer a de-centralized approach that tolerates failures. For instance, quorum-based databases are able to maintain their consistency even in the presence of network partitions <ref> [DGS85] </ref> and server crashes (as quantified by their fault-tolerance [BG86]). However, the quorum approach's promise of high availability was arguably its most desirable feature, and it certainly generated a substantial amount of research (e.g., [BG87, ET89, PW95, AW98]). <p> Note that assumption 2 does not in itself conflict with the fact that quorum systems guarantee strict consistency despite partitions <ref> [DGS85] </ref>. One may hope that assuming perfect communication, for the purpose of availability analysis, would not change the qualitative predictions of the model. As we shall see, though, this modeling is problematic. Assumption 3 (uniform crash probability) simplifies the analysis but is not really crucial.
Reference: [ET89] <author> A. El-Abbadi and S. Toueg. </author> <title> Maintaining availability in partitioned replicated databases. </title> <journal> ACM Trans. Database Sys., </journal> <volume> 14(2):264290, </volume> <month> June </month> <year> 1989. </year>
Reference-contexts: However, the quorum approach's promise of high availability was arguably its most desirable feature, and it certainly generated a substantial amount of research (e.g., <ref> [BG87, ET89, PW95, AW98] </ref>). Availability is the probability that the member servers of at least one quorum are functioning, assuming that the servers crash independently with a fixed probability, and that the complete database is replicated at every server.
Reference: [GB85] <author> H. Garcia-Molina and D. Barbara. </author> <title> How to assign votes in a distributed system. </title> <journal> J. ACM, </journal> <volume> 32(4):841860, </volume> <year> 1985. </year>
Reference-contexts: 1 Introduction A quorum system is a collection of subsets (quorums) of servers, every two of which intersect. During the late 70's and early 80's, quorum systems were proposed as a basic mechanism for concurrency control in replicated databases <ref> [Tho79, Gif79, GB85, Mae85, Her86] </ref>. Informally, a quorum-based replication scheme works as follows. Data items are timestamped and written to some quorum of servers.
Reference: [GHOS96] <author> J. Gray, P. Helland, P. O'Neil, and D. Shasha. </author> <title> The dangers of replication and a solution. </title> <booktitle> In Proc. ACM SIG-MOD Inter. Conf. Management of Data, </booktitle> <pages> pages 173182, </pages> <address> Montreal, Quebec, </address> <year> 1996. </year>
Reference-contexts: Over the last two years we have seen the emergence of several replication schemes in the database literature, schemes which do not adhere to the local read paradigm quite so strictly. For instance, the protocol of <ref> [GHOS96] </ref> includes obtaining locks from remote machines even for read-only transactions, and the protocols of [BK97] include obtaining read, write, and commit permissions from a replication-graph-manager machine.
Reference: [Gif79] <author> D. K. Gifford. </author> <title> Weighted voting for replicated data. </title> <booktitle> In Proc. 7th Symp. Oper. Sys. Princip., </booktitle> <pages> pages 150159, </pages> <year> 1979. </year>
Reference-contexts: 1 Introduction A quorum system is a collection of subsets (quorums) of servers, every two of which intersect. During the late 70's and early 80's, quorum systems were proposed as a basic mechanism for concurrency control in replicated databases <ref> [Tho79, Gif79, GB85, Mae85, Her86] </ref>. Informally, a quorum-based replication scheme works as follows. Data items are timestamped and written to some quorum of servers.
Reference: [GR93] <author> J. Gray and A. Reuter. </author> <title> Transaction Processing: Concepts and Techniques. </title> <publisher> Morgan Kaufmann Publishers, </publisher> <address> San Francisco, CA, </address> <year> 1993. </year>
Reference-contexts: As for latency, network latency is mostly due to protocol software which improves with CPU speed, whereas disk latency is limited by the mechanics of the seek time and the disk rotation speed <ref> [GR93, pp. 53,59] </ref>. In the setting of LANs there should be little doubt that the local read paradigm is no longer justified. <p> In many settings they are quite reasonable. 2.2 Reads vs. Writes Another criticism of quorum systems is that when read operations are much more frequent than write operations, which is a typical scenario, optimizing the read operations is more important than optimizing the writes <ref> [GR93] </ref>. In such scenarios the total amount of work incurred by a ROWA approach is claimed to be lower than that of a quorum-based approach (where total work is measured by the total number of disk accesses or total number of messages per operation).
Reference: [GS92] <author> H. Garcia-Molina and K. Salem. </author> <title> Main memory database systems: An overview. </title> <journal> IEEE Trans. Knowledge and Data Eng., </journal> <volume> 4(6):509516, </volume> <month> December </month> <year> 1992. </year> <month> 10 </month>
Reference-contexts: The multiple reads all execute in parallel, thus the reader would not even 4 suffer a substantially longer delay. To some extent, remote reads over a LAN may be reasonable even for main memory databases <ref> [GS92, JLR + 94] </ref>, or if the data is held in the cache; the incurred networking delay would be more than that of a pure memory access, but still much less than a disk access.
Reference: [Her86] <author> M. Herlihy. </author> <title> A quorum-consensus replication method for abstract data types. </title> <journal> ACM Trans. Comp. Sys., </journal> <volume> 4(1):32 53, </volume> <month> February </month> <year> 1986. </year>
Reference-contexts: 1 Introduction A quorum system is a collection of subsets (quorums) of servers, every two of which intersect. During the late 70's and early 80's, quorum systems were proposed as a basic mechanism for concurrency control in replicated databases <ref> [Tho79, Gif79, GB85, Mae85, Her86] </ref>. Informally, a quorum-based replication scheme works as follows. Data items are timestamped and written to some quorum of servers.
Reference: [HHB96] <author> A. A. Helal, A. A. Heddaya, and B. B. Bhargava. </author> <title> Replication Techniques in Distributed Systems. </title> <publisher> Kluwer Academic Publishers, </publisher> <year> 1996. </year>
Reference-contexts: Despite these features, quorum systems are not in wide-spread use within commercial replicated databases. As an example, we can consider the Oracle8 product [Ora97], which provides several modes of data replication. Among these, the only mode that guarantees serializability, called Synchronized Data Propagation, uses a Read-One-Write-All (ROWA) approach (cf. <ref> [HHB96] </ref>). Oracle's product documentation in fact recommends not to use this mode because : : : it can function only when all sites in the system are concurrently available [Ora97, p. 3026], i.e., it has very poor write-availability.
Reference: [INK95] <author> T. Ibaraki, H. Nagamochi, and T. Kameda. </author> <title> Optimal coteries for rings and related networks. </title> <booktitle> Distributed Computing, </booktitle> <address> 8:191201, </address> <year> 1995. </year>
Reference-contexts: In such a non-fully-connected network model, it is known that the optimal-availability quorum system is completely contained in a single bi-connected component <ref> [INK95] </ref>. And indeed the experiments of [AW96] showed that confining the quorum system to 6 machines on a single LAN gave better availability than using all 14 machines and allowing quorums to span the two sites.
Reference: [JLR + 94] <author> H. V. Jagadish, D. Lieuwen, R. Rastogi, A. Silberschatz, and S. Sudarshan. </author> <title> Dali: A high performance main-memory storage manager. </title> <booktitle> In Proc. 20th Inter. Conf. on Very Large Databases (VLDB), </booktitle> <year> 1994. </year>
Reference-contexts: The multiple reads all execute in parallel, thus the reader would not even 4 suffer a substantially longer delay. To some extent, remote reads over a LAN may be reasonable even for main memory databases <ref> [GS92, JLR + 94] </ref>, or if the data is held in the cache; the incurred networking delay would be more than that of a pure memory access, but still much less than a disk access.
Reference: [KC91] <author> A. Kumar and S. Y. Cheung. </author> <title> A high availability p n hierarchical grid algorithm for replicated data. </title> <journal> Inform. Process. Lett., </journal> <volume> 40:311316, </volume> <year> 1991. </year>
Reference-contexts: Availability is the probability that the member servers of at least one quorum are functioning, assuming that the servers crash independently with a fixed probability, and that the complete database is replicated at every server. Many quorum systems, such as those of <ref> [KC91, Kum91, AE91, RST92, PW97b, PW97a, Baz96] </ref>, exhibit very high availability, and moreover, their availability tends to 1 very rapidly as the system scales up and more servers are added. Despite these features, quorum systems are not in wide-spread use within commercial replicated databases.
Reference: [Kum91] <author> A. Kumar. </author> <title> Hierarchical quorum consensus: A new algorithm for managing replicated data. </title> <journal> IEEE Trans. Comput., </journal> <volume> 40(9):9961004, </volume> <year> 1991. </year>
Reference-contexts: Availability is the probability that the member servers of at least one quorum are functioning, assuming that the servers crash independently with a fixed probability, and that the complete database is replicated at every server. Many quorum systems, such as those of <ref> [KC91, Kum91, AE91, RST92, PW97b, PW97a, Baz96] </ref>, exhibit very high availability, and moreover, their availability tends to 1 very rapidly as the system scales up and more servers are added. Despite these features, quorum systems are not in wide-spread use within commercial replicated databases.
Reference: [Lyn96] <author> N. Lynch. </author> <title> Distributed Algorithms. </title> <publisher> Morgan Kaufmann Publishers, </publisher> <address> San Francisco, CA, </address> <year> 1996. </year>
Reference-contexts: A natural modeling of malicious servers is that of Byzantine-faulty processes: In a Byzantine failure model, a bounded number of servers is allowed to send arbitrary messages and not to adhere to the required protocol, yet the protocol needs to remain correct nonetheless (cf. <ref> [Lyn96] </ref>). Note that in a Byzantine environment, no single server can be trusted, so both the local read paradigm and the ROWA approach are unacceptable. Quorum systems have recently been adapted to mask Byzantine failures in [MR98a], and studied further in [MRW97].
Reference: [Mae85] <author> M. Maekawa. </author> <title> A p n algorithm for mutual exclusion in decentralized systems. </title> <journal> ACM Trans. Comp. Sys., </journal> <volume> 3(2):145159, </volume> <year> 1985. </year>
Reference-contexts: 1 Introduction A quorum system is a collection of subsets (quorums) of servers, every two of which intersect. During the late 70's and early 80's, quorum systems were proposed as a basic mechanism for concurrency control in replicated databases <ref> [Tho79, Gif79, GB85, Mae85, Her86] </ref>. Informally, a quorum-based replication scheme works as follows. Data items are timestamped and written to some quorum of servers. <p> The choice depends not only on the frequency of write operations, but also on the ratio of read quorum size to number of replicas. For instance, using Maekawa's quorum system <ref> [Mae85] </ref>, if n = 7 and r = w = 3, a quorum-based approach incurs less work than ROWA when 33% of the operations are writes. If n = 13 and r = w = 4 then the crossover 5 point is 25% writes.
Reference: [MR98a] <author> D. Malkhi and M. Reiter. </author> <title> Byzantine quorum systems. </title> <booktitle> Distributed Computing, </booktitle> <address> 11(4):203213, </address> <year> 1998. </year>
Reference-contexts: Note that in a Byzantine environment, no single server can be trusted, so both the local read paradigm and the ROWA approach are unacceptable. Quorum systems have recently been adapted to mask Byzantine failures in <ref> [MR98a] </ref>, and studied further in [MRW97]. These so called Byzantine quorum systems provide a mechanism for meeting security requirements, since they allow clients to mask out malicious or erroneous responses from servers, while still enjoying all the properties of normal quorum systems, such as strict consistency and excellent load-balancing capabilities.
Reference: [MR98b] <author> D. Malkhi and M. Reiter. </author> <title> Secure and scalable replication in Phalanx. </title> <booktitle> In Proc. 17th IEEE Symp. on Reliable Distributed Systems, </booktitle> <pages> pages 5158, </pages> <month> October </month> <year> 1998. </year>
Reference-contexts: Such applications include, for instance, public-key infrastructures (PKI), and national election systems (see <ref> [MR98b] </ref> for more details). These applications need to be able to tolerate malicious attacks on the servers; For instance, a vote-counting client needs to be able to ignore bogus votes that are reported by a compromised voting server. <p> These so called Byzantine quorum systems provide a mechanism for meeting security requirements, since they allow clients to mask out malicious or erroneous responses from servers, while still enjoying all the properties of normal quorum systems, such as strict consistency and excellent load-balancing capabilities. The Phalanx system of <ref> [MR98b] </ref> is a software system that uses Byzantine quorum systems to address the above-mentioned requirements. In its current state it only provides the semantics of shared variables, without transactional serializability.
Reference: [MRW97] <author> D. Malkhi, M. Reiter, and A. Wool. </author> <title> The load and availability of Byzantine quorum systems. </title> <booktitle> In Proc. 16th ACM Symp. Princip. of Distributed Computing (PODC), </booktitle> <pages> pages 249257, </pages> <month> August </month> <year> 1997. </year>
Reference-contexts: Note that in a Byzantine environment, no single server can be trusted, so both the local read paradigm and the ROWA approach are unacceptable. Quorum systems have recently been adapted to mask Byzantine failures in [MR98a], and studied further in <ref> [MRW97] </ref>. These so called Byzantine quorum systems provide a mechanism for meeting security requirements, since they allow clients to mask out malicious or erroneous responses from servers, while still enjoying all the properties of normal quorum systems, such as strict consistency and excellent load-balancing capabilities.
Reference: [NW98] <author> M. Naor and A. Wool. </author> <title> The load, capacity and availability of quorum systems. </title> <journal> SIAM J. Computing, </journal> <volume> 27(2):423 447, </volume> <month> April </month> <year> 1998. </year>
Reference-contexts: However it seems possible to architect a quorum-based replicated database using reliable group communication as a component. 4 Quorum Systems After All? 4.1 Load Balancing Rather Than Availability One feature of quorum systems, which has attracted some recent theoretical interest, deals with the notion of load <ref> [NW98] </ref>. The crucial observation is that clients can read or write from any quorum, and there is a great deal of freedom in choosing which access strategy to use. <p> If the quorum selection strategy is properly randomized, then each server only needs to handle a small fraction of the incoming operations, yet strict consistency is still guaranteed. Specifically, <ref> [NW98] </ref> shows that for many quorum systems the load on each server can be made as low as 1= n of the operations in an n-server system. This implies that as the system scales up and more servers are added, the load on each one of them actually decreases.
Reference: [Ora97] <institution> Oracle8 Server Concepts, </institution> <note> release 8.0, chapter 30: Database replication, June 1997. Available from http: //www.oracle.com/support/documentation/oracle8/SCN80.pdf. </note>
Reference-contexts: Despite these features, quorum systems are not in wide-spread use within commercial replicated databases. As an example, we can consider the Oracle8 product <ref> [Ora97] </ref>, which provides several modes of data replication. Among these, the only mode that guarantees serializability, called Synchronized Data Propagation, uses a Read-One-Write-All (ROWA) approach (cf. [HHB96]). <p> Among these, the only mode that guarantees serializability, called Synchronized Data Propagation, uses a Read-One-Write-All (ROWA) approach (cf. [HHB96]). Oracle's product documentation in fact recommends not to use this mode because : : : it can function only when all sites in the system are concurrently available <ref> [Ora97, p. 3026] </ref>, i.e., it has very poor write-availability. All the other modes of replication supported by Oracle8 essentially take a lazy replication, primary-copy approach, coupled with various heuristics which attempt to detect and resolve some common conflict scenarios.
Reference: [PW95] <author> D. Peleg and A. Wool. </author> <title> The availability of quorum systems. Information and Computation, </title> <address> 123(2):210223, </address> <year> 1995. </year>
Reference-contexts: However, the quorum approach's promise of high availability was arguably its most desirable feature, and it certainly generated a substantial amount of research (e.g., <ref> [BG87, ET89, PW95, AW98] </ref>). Availability is the probability that the member servers of at least one quorum are functioning, assuming that the servers crash independently with a fixed probability, and that the complete database is replicated at every server. <p> The reason is that several seemingly benign modeling assumptions are made in order to make the analysis of availability tractable. Unfortunately, these assumptions clash with the reality of network environments in crucial ways. The standard probabilistic assumptions which underly the definition of availability, as analyzed in <ref> [BG87, PW95] </ref> and others, are: 1. Crashes are independent events. 2. The communication network is fully connected and never fails. 3. All the servers have the same crash probability p. <p> If all the servers have the same crash probability p this voting system reduces to either a simple majority voting, which has optimal availability when p &lt; 1=2 [BG87], or to a monarchy (single server) when p &gt; 1=2 <ref> [PW95] </ref>. Assumptions 1 and 2 are not so easily dealt with, as seen from the experiments of [AW96].
Reference: [PW97a] <author> D. Peleg and A. Wool. </author> <title> The availability of crumbling wall quorum systems. </title> <journal> Discrete Applied Math., </journal> <volume> 74(1):69 83, </volume> <month> April </month> <year> 1997. </year>
Reference-contexts: Availability is the probability that the member servers of at least one quorum are functioning, assuming that the servers crash independently with a fixed probability, and that the complete database is replicated at every server. Many quorum systems, such as those of <ref> [KC91, Kum91, AE91, RST92, PW97b, PW97a, Baz96] </ref>, exhibit very high availability, and moreover, their availability tends to 1 very rapidly as the system scales up and more servers are added. Despite these features, quorum systems are not in wide-spread use within commercial replicated databases. <p> In general, r = w p n for this system, so the frequency of writes needs only to exceed 1= n for the quorum-based approach to be advantageous. Quorum systems with variable-sized quorums, such as those of <ref> [AE91, PW97a] </ref>, offer the possibility of outperforming ROWA for even lower write rates, by directing reads to smaller sized quorums, which can be as small as r log n for both systems. Clearly, as the number of replicas grows, quorum systems become advantageous for lower write rates.
Reference: [PW97b] <author> D. Peleg and A. Wool. Crumbling walls: </author> <title> A class of practical and efficient quorum systems. </title> <booktitle> Distributed Computing, </booktitle> <address> 10(2):8798, </address> <year> 1997. </year>
Reference-contexts: Availability is the probability that the member servers of at least one quorum are functioning, assuming that the servers crash independently with a fixed probability, and that the complete database is replicated at every server. Many quorum systems, such as those of <ref> [KC91, Kum91, AE91, RST92, PW97b, PW97a, Baz96] </ref>, exhibit very high availability, and moreover, their availability tends to 1 very rapidly as the system scales up and more servers are added. Despite these features, quorum systems are not in wide-spread use within commercial replicated databases.
Reference: [RDB98] <author> Oracle rdb information, </author> <year> 1998. </year> <note> Available from http://www.oracle.com/products/servers/rdb /index.html. </note>
Reference-contexts: Note that the VAX Cluster's quorum system does not provide a quorum-based replicated database per se. The system supports file-system semantics but does not provide transaction processing. And in fact the rdb database product, which is now owned by Oracle but originated on Digital's VMS systems <ref> [RDB98] </ref>, uses the VAX Cluster's lock manager but does not use a quorum-based replication scheme. 7 3.2 Group Communication Systems Another successful application of quorum systems is in the context of reliable group communication services such as Isis [BvR94], Transis [ADKM92], Totem [AMM + 95], Horus [vRBM96], and others.
Reference: [RST92] <author> S. Rangarajan, S. Setia, and S. K. Tripathi. </author> <title> A fault-tolerant algorithm for replicated data management. </title> <booktitle> In Proc. 8th IEEE Int. Conf. Data Engineering, </booktitle> <pages> pages 230237, </pages> <year> 1992. </year>
Reference-contexts: Availability is the probability that the member servers of at least one quorum are functioning, assuming that the servers crash independently with a fixed probability, and that the complete database is replicated at every server. Many quorum systems, such as those of <ref> [KC91, Kum91, AE91, RST92, PW97b, PW97a, Baz96] </ref>, exhibit very high availability, and moreover, their availability tends to 1 very rapidly as the system scales up and more servers are added. Despite these features, quorum systems are not in wide-spread use within commercial replicated databases.
Reference: [SB94] <author> M. Spasojevic and P. Berman. </author> <title> Voting as the optimal static pessimistic scheme for managing replicated data. </title> <journal> IEEE Trans. Parallel and Distributed Sys., </journal> <volume> 5(1):6473, </volume> <year> 1994. </year>
Reference-contexts: One may hope that assuming perfect communication, for the purpose of availability analysis, would not change the qualitative predictions of the model. As we shall see, though, this modeling is problematic. Assumption 3 (uniform crash probability) simplifies the analysis but is not really crucial. It was shown by <ref> [SB94, AW98] </ref> that essentially the same properties hold with or without it, namely that the optimal availability quorum system is defined by weighted voting, where the optimal weights depend on the individual crash probabilities.
Reference: [Tho79] <author> R. H. Thomas. </author> <title> A majority consensus approach to concurrency control for multiple copy databases. </title> <journal> ACM Trans. Database Sys., </journal> <volume> 4(2):180209, </volume> <year> 1979. </year>
Reference-contexts: 1 Introduction A quorum system is a collection of subsets (quorums) of servers, every two of which intersect. During the late 70's and early 80's, quorum systems were proposed as a basic mechanism for concurrency control in replicated databases <ref> [Tho79, Gif79, GB85, Mae85, Her86] </ref>. Informally, a quorum-based replication scheme works as follows. Data items are timestamped and written to some quorum of servers.
Reference: [VMS] <institution> DIGITAL OpenVMS systems. </institution> <note> http://www.openvms.digital.com/. </note>
Reference-contexts: of quorum systems to support extremely high load, in a scalable way, without weakening the database's consistency. 3 Some Success Stories 3.1 VAX Clusters and Financial Applications Quorum systems are used deep within the file system of Digital's OpenVMS operating system, as a basic component of the VAX Cluster technology <ref> [VMS] </ref>. A VAX Cluster of machines will function as long as a quorum (majority) of the machines are alive and able to communicate with each other.

Reference: [1] <author> P. A. Bernstein and N. Goodman. </author> <title> An algorithm for concurrency control and recovery in replicated distributed databases. </title> <journal> ACM Transactions on Database Systems 9(4) </journal> <pages> 596-615, </pages> <month> December </month> <year> 1984. </year>
Reference: [2] <author> A. Birrell, R. Levin, R. M. Needham, and M. D. Schroeder. Grapevine: </author> <title> An exercise in distributed computing. </title> <journal> Communications of the ACM 25(4) </journal> <pages> 260-274, </pages> <month> April </month> <year> 1982. </year>
Reference: [3] <author> S. Davidson, H. Garcia-Molina, and D. Skeen. </author> <title> Consistency in a partitioned network: A survey. </title> <journal> ACM Computing Surveys 17(3) </journal> <pages> 341-370, </pages> <month> September </month> <year> 1985. </year>
Reference: [4] <author> A. Demers, D. Greene, C. Hauser, W. Irish, J. Larson, S. Shenker, H. Sturgis, D. Swinehart, and D. Terry. </author> <title> Epidemic algorithms for replicated database maintenance. </title> <booktitle> Proceedings Sixth Symposium on Principles of Distributed Computing, </booktitle> <address> Vancouver, B.C., Canada, </address> <month> August </month> <year> 1987, </year> <pages> pages 1-12. </pages>
Reference: [5] <author> M. R. Ebling. </author> <title> Translucent cache management for mobile computing. </title> <institution> Carnegie Mellon University technical report CMU-CS-98-116, </institution> <month> March </month> <year> 1998. </year> <month> 19 </month>
Reference: [6] <author> W. K. Edwards, E. D. Mynatt, K. Petersen, M. J. Spreitzer, D. B. Terry, and M. M. Theimer. </author> <title> Designing and Implementing Asynchronous Collaborative Applications with Bayou. </title> <booktitle> Proceedings User Interface Systems and Technology, </booktitle> <address> Banff, Canada, </address> <month> October </month> <year> 1997, </year> <pages> pages 119-128. </pages>
Reference: [7] <author> R. A. Golding, </author> <title> A weak-consistency architecture for distributed information services, </title> <journal> Computing Systems, </journal> <volume> 5(4) </volume> <pages> 379-405, </pages> <month> Fall </month> <year> 1992. </year>
Reference: [8] <author> J. Gray, P. Helland, P. O'Neil, and D. Shasha. </author> <title> The dangers of replication and a solution. </title> <booktitle> Proceedings 1996 ACM SIGMOD Conference, </booktitle> <address> Montreal, Canada, </address> <month> June </month> <year> 1996, </year> <pages> pages 173-182. </pages>
Reference: [9] <author> R. G. Guy, J.S. Heidemann, W. Mak, T.W. Page, Jr., G.J. Popek, and D. Rothmeier. </author> <title> Implementation of the Ficus replicated file system. </title> <booktitle> Proceedings Summer USENIX Conference, </booktitle> <month> June </month> <year> 1990, </year> <pages> pages 63-71. </pages>
Reference: [10] <author> L. Kalwell Jr., S. Beckhardt, T. Halvorsen, R. Ozzie, and I. Greif. </author> <title> Replicated document management in a group communication system. In Groupware: Software for Computer-Supported Cooperative Work, </title> <editor> edited by D. Marca and G. Bock, </editor> <publisher> IEEE Computer Society Press, </publisher> <year> 1992, </year> <pages> pages 226-235. </pages>
Reference: [11] <author> P. Kumar and M. Satyanarayanan. </author> <title> Supporting application-specific resolution in an optimistically replicated file system. </title> <booktitle> Proceedings IEEE Workshop on Workstation Operating Systems, </booktitle> <address> Napa, California, </address> <month> October </month> <year> 1993, </year> <pages> pages 66-70. </pages>
Reference: [12] <author> R. Ladin, B. Liskov, L. Shrira, and S. Ghemawat. </author> <title> Providing high availability using lazy replication. </title> <journal> ACM Transactions on Computer Systems 10(4) </journal> <pages> 360-391, </pages> <month> November </month> <year> 1992. </year>
Reference: [13] <author> R. Larson-Hughes and H. J. Skalle. </author> <title> Lotus Notes Application Development. </title> <publisher> Prentice Hall, </publisher> <year> 1995. </year>
Reference: [14] <author> L. B. Mummert, M. R. Ebling, and M. Satyanarayanan. </author> <title> Exploiting weak connectivity for mobile file access. </title> <booktitle> Proceedings Fifteenth ACM Symposium on Operating Systems Principles, </booktitle> <address> Copper Mountain, Colorado, </address> <month> De-cember </month> <year> 1995, </year> <pages> pages 143-155. </pages>
Reference: [15] <author> Oracle Corporation. </author> <title> Oracle7 Server Distributed Systems: Replicated Data, Release 7.1. Part No. </title> <address> A21903-2, </address> <year> 1995. </year>
Reference: [16] <author> K. Petersen, M. J. Spreitzer, D. B. Terry, M. M. Theimer, and A. J. Demers. </author> <title> Flexible Update Propagation for Weakly Consistent Replication. </title> <booktitle> Proceedings 16th ACM Symposium on Operating Systems Principles, </booktitle> <address> Saint-Malo, France, </address> <month> October </month> <year> 1997, </year> <pages> pages 288-301. </pages>
Reference: [17] <author> P. Reiher, J. Heidemann, D. Ratner, G. Skinner, and G. Popek. </author> <title> Resolving file conflicts in the Ficus file system. </title> <booktitle> Proceedings Summer USENIX Conference, </booktitle> <month> June </month> <year> 1994, </year> <pages> pages 183-195. </pages>
Reference: [18] <author> D. B. Terry, A. J. Demers, K. Petersen, M. J. Spreitzer, M. M. Theimer and B. B. Welch. </author> <title> Session guarantees for weakly consistent replicated data. </title> <booktitle> Proceedings Third International Conference on Parallel and Distributed Information Systems, </booktitle> <address> Austin, Texas, </address> <month> September </month> <year> 1994, </year> <pages> pages 140-149. </pages>
Reference: [19] <author> D. B. Terry, M. M. Theimer, K. Petersen, A. J. Demers, M. J. Spreitzer, and C. H. Hauser. </author> <title> Managing update conflicts in Bayou, a weakly connected replicated storage system. </title> <booktitle> Proceedings Fifteenth ACM Symposium on Operating Systems Principles, </booktitle> <address> Copper Mountain, Colorado, </address> <month> December </month> <year> 1995, </year> <pages> pages 172-183. </pages>

Reference: [1] <author> A. Aggarwal and M. Rabinovich. </author> <title> Performance of replication schemes on the Internet. </title> <type> Technical Report HA6177000-981030-01-TM, </type> <institution> AT&T Labs, </institution> <month> October </month> <year> 1998. </year> <note> Also available as http://www.research.att.com/~misha/radar/tm-perf.ps.gz. </note>
Reference: [2] <author> D. Agrawal, A. El Abbadi, and R. C. Steinke. </author> <title> Epidemic algorithms in replicated databases. </title> <booktitle> In Proceedings of the 16th ACM SIGACT-SIGMOD-SIGART Symposium on Principles of Database Systems (PODS '97), </booktitle> <pages> pages 161172, </pages> <month> May </month> <year> 1997. </year> <month> 28 </month>
Reference: [3] <author> M. Baentsch, L. Baum, G. Molter, S. Rothkugel, and P. Sturm. </author> <title> Enhancing the web infrastructure from caching to replication. </title> <booktitle> IEEE Internet Computing, </booktitle> <pages> pages 1827, </pages> <month> Mar-Apr </month> <year> 1997. </year> <note> Also available at http://neumann.computer.org/ic/books/ic1997/pdf/w2018.pdf. </note>
Reference: [4] <author> Micah Beck and Terry Moore. </author> <title> The Internet-2 distributed storage infrastructure project: An architecture for internet content channels. </title> <booktitle> In 3rd Int. WWW Caching Workshop, </booktitle> <address> Manchester, UK, </address> <month> June </month> <year> 1998. </year> <note> Available at http://wwwcache.ja.net/events/workshop/18/mbeck2.html. </note>
Reference: [5] <author> S. Bhattacharjee, M. Ammar, E. Zegura, V. Shah, and Z. Fei. Application-layer anycasting. </author> <booktitle> In INFOCOM, </booktitle> <year> 1997. </year>
Reference: [6] <institution> Cisco Systems, Inc. DistributedDirector. </institution> <note> White paper. http://www.cisco.com/warp/public/734/distdir/dd wp.htm. </note>
Reference: [7] <author> Alan Demers, Dan Greene, Carl Hauser, Wes Irish, John Larson, Scott Shenker, Howard Sturgis, Dan Swinehart, and Doug Terry. </author> <title> Epidemic algorithms for replicated database maintenance. </title> <booktitle> In Proceedings of the 6th Annual ACM Symposium on Principles of Distributed Computing, </booktitle> <pages> pages 112, </pages> <month> August </month> <year> 1987. </year>
Reference: [8] <institution> The http distribution and replication protocol. A WWW Consortium submission, </institution> <month> August </month> <year> 1997. </year> <note> http://www.w3.org/TR/NOTE-drp. </note>
Reference: [9] <author> R. Farrell. </author> <title> Distributing the web load. </title> <booktitle> Network World, </booktitle> <pages> pages 5760, </pages> <month> September 22 </month> <year> 1997. </year>
Reference: [10] <author> Lewis Girod and Karen R. Sollins. </author> <title> Requirements for URN resolution systems. Internet Draft. </title> <note> http://ana-www.lcs.mit.edu/internet-drafts/draft-girod-urn-res-require-00.txt, June 1996. </note>
Reference: [11] <institution> IBM Interactive Network Dispatcher. </institution> <note> http://www.ics.raleigh.ibm.com/netdispatch/. </note>
Reference: [12] <institution> Large scale network caches provide more bandwidth for your money. Inktomi Corp. </institution> <note> White Paper, 1998. http://www.inktomi.com/products/traffic/tech/economics.html. </note>
Reference: [13] <author> C. Partridge, T. Mendez, and W. Milliken. </author> <type> RFC 1546: </type> <institution> Host anycasting service, </institution> <month> November </month> <year> 1993. </year>
Reference: [14] <institution> Probe Research, Inc. </institution> <note> http://www.proberesearch.com. </note>
Reference: [15] <author> M. Rabinovich, N. Gehani, and A. Kononov. </author> <title> Scalable update propagation in epidemic replicated databases. </title> <booktitle> Lecture Notes in Computer Science, 1057:207222, 1996. Proc. of EDBT'96. </booktitle>
Reference: [16] <author> M. Rabinovich, I. Rabinovich, and R. Rajaraman. </author> <title> Dynamic replication on the Internet. </title> <type> Technical Report HA6177000-980305-01-TM, </type> <institution> AT&T Labs, </institution> <month> March </month> <year> 1998. </year> <note> Also available as http://www.research.att.com/~misha/radar/tm.ps.gz. </note>
Reference: [17] <author> David L. Tennenhouse, Jonathan M. Smith, W. David Sincoskie, David J. Wetherall, and Gary J. Minden. </author> <title> A survey of active network research. </title> <journal> IEEE Communications Magazine, </journal> <volume> 35(1):8086, </volume> <month> January </month> <year> 1997. </year>
Reference: [18] <author> D. Terry, A. Demers, K. Peterson, M. Spreitzer, M, Theimer, and B. Welch. </author> <title> Session guarantees for weakly consistent replicated data. </title> <booktitle> In Int. Conf. on Parallel and Distributed Information Systems, </booktitle> <year> 1994. </year>
Reference: [19] <institution> WindDance Networks Corp. </institution> <address> Webchallenger. http://www.winddancenet.com/webchallenger/products/frame products.htm. </address>
Reference: [20] <author> O. Wolfson, S. Jajodia, and Y. Huang. </author> <title> An adaptive data replication algorithm. </title> <journal> ACM Transactions on Database Systems (TODS), </journal> <volume> 22(4):255314, </volume> <month> June </month> <year> 1997. </year>

References-found: 81

