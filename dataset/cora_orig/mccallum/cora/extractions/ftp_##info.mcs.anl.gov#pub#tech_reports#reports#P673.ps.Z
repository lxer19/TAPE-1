URL: ftp://info.mcs.anl.gov/pub/tech_reports/reports/P673.ps.Z
Refering-URL: http://www.mcs.anl.gov/publications/abstracts/abstracts97.htm
Root-URL: http://www.mcs.anl.gov
Title: Adaptive SOR: A case study in automatic differentiation of algorithm parameters  
Author: Paul Hovland Michael Heath 
Date: July 2, 1997  
Abstract: Many algorithms make use of one or more parameters to control the behavior of the algorithm. Examples include the damping factor ff in a damped Newton method or the relaxation parameter ! in a successive over-relaxation (SOR) iterative solver. The optimal value for such parameters is problem dependent and difficult to determine for most problems. We describe the use of automatic differentiation (AD) to adjust algorithm parameters toward optimal behavior. We demonstrate how AD can be used to transform an SOR solver with fixed ! into an adaptive SOR solver that adjusts ! toward its optimal value. We provide experimental evidence that for large problems with lax convergence criteria such an adaptive solver may converge faster than a solver using an optimal, but fixed, value for !. These are exactly the conditions that apply when SOR is used as a preconditioner, its most common use in modern scientific computing. 
Abstract-found: 1
Intro-found: 1
Reference: [2] <author> B. N. Datta. </author> <title> Numerical Linear Algebra and Applications. </title> <address> Brooks/Cole, Pacific Grove, CA, </address> <year> 1995. </year>
Reference-contexts: The parameter ! may be assigned any value in the interval <ref> [0; 2] </ref>, although typically values in the interval [1; 2] are used (hence the term over-relaxation). The value of ! can have a significant effect on the convergence rate of the algorithm. <p> The parameter ! may be assigned any value in the interval [0; 2], although typically values in the interval <ref> [1; 2] </ref> are used (hence the term over-relaxation). The value of ! can have a significant effect on the convergence rate of the algorithm. <p> The optimal fixed ! for the Poisson problem can be computed using an analytic formula given in <ref> [2] </ref> (see [7] for background theory): ! opt = 1 + 1 ae 2 where ae = 2 cos M + 1 We compared the standard SOR program using this optimal ! to the adaptive algorithm for various problem sizes.
Reference: [3] <author> P. Eberhard and C. Bischof. </author> <title> Automatic differentiation of numerical integration algorithms. </title> <type> Preprint ANL/MCS-621-1196, </type> <institution> Mathematics and Computer Science Division, Argonne National Laboratory, </institution> <year> 1996. </year>
Reference-contexts: AD is applied to algorithms, not some abstract notion of a function. Thus, the method used to compute a function can affect the values computed for the derivatives. In certain cases <ref> [3, 6] </ref>, this may affect the quality of the derivatives computed using AD if precautions are not taken. However, it is also possible to exploit this feature by computing derivatives with respect to algorithm parameters.
Reference: [4] <author> G. H. Golub and C. F. Van Loan. </author> <title> Matrix Computations. </title> <publisher> The Johns Hopkins University Press, </publisher> <address> Baltimore, MD, </address> <note> 3rd edition, </note> <year> 1996. </year>
Reference-contexts: 1 Introduction Many algorithms make use of one or more parameters to control the behavior of the algorithm. Consider, for example, the relaxation parameter ! in a successive over-relaxation (SOR) solver (see <ref> [4] </ref>, for example).
Reference: [5] <author> A. Griewank. </author> <title> On automatic differentiation. </title> <booktitle> In Mathematical Programming: Recent Developments and Applications, </booktitle> <pages> pages 83-108, </pages> <address> Amsterdam, 1989. </address> <publisher> Kluwer Academic Publishers. </publisher>
Reference-contexts: Thus, we can compute the partial derivatives of the elementary functions using formulas obtained via table lookup, then compute the overall derivatives using the chain rule. This process can be completely automated, and is thus termed automatic differentiation <ref> [5] </ref>. Consider the code for computing the function y = f (x), where f (x) = (sin (x) p x )=x, shown in Figure 1 (a). Using AD, we can generate code to compute both y and dy=dx, as shown in Figure 1 (b).
Reference: [6] <author> A. Griewank, C. Bischof, G. Corliss, A. Carle, and K. Williamson. </author> <title> Derivative convergence of iterative equation solvers. </title> <journal> Optimization Methods and Software, </journal> <volume> 2 </volume> <pages> 321-355, </pages> <year> 1993. </year>
Reference-contexts: AD is applied to algorithms, not some abstract notion of a function. Thus, the method used to compute a function can affect the values computed for the derivatives. In certain cases <ref> [3, 6] </ref>, this may affect the quality of the derivatives computed using AD if precautions are not taken. However, it is also possible to exploit this feature by computing derivatives with respect to algorithm parameters.
Reference: [7] <author> R. S. Varga. </author> <title> Matrix Iterative Analysis. </title> <publisher> Prentice Hall, </publisher> <address> Englewood Cliffs, NJ, </address> <year> 1962. </year> <month> 6 </month>
Reference-contexts: The optimal fixed ! for the Poisson problem can be computed using an analytic formula given in [2] (see <ref> [7] </ref> for background theory): ! opt = 1 + 1 ae 2 where ae = 2 cos M + 1 We compared the standard SOR program using this optimal ! to the adaptive algorithm for various problem sizes.
References-found: 6

