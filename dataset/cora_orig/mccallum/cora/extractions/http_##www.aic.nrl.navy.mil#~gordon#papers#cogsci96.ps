URL: http://www.aic.nrl.navy.mil/~gordon/papers/cogsci96.ps
Refering-URL: http://www.aic.nrl.navy.mil/~gordon/pubs.html
Root-URL: 
Email: gordon@aic.nrl.navy.mil  devika@cs.rice.edu  
Title: Cognitive Modeling of Action Selection Learning  
Author: Diana F. Gordon Devika Subramanian 
Address: Code 5510 4555 Overlook Avenue, S.W. Washington, D.C. 20375  Houston, TX 77005  
Affiliation: Naval Research Laboratory,  Department of Computer Science Rice University  
Abstract: Our goal is to develop a hybrid cognitive model of how humans acquire skills on complex cognitive tasks. We are pursuing this goal by designing hybrid computational architectures for the NRL Navigation task, which requires competent sensorimotor coordination. In this paper, we describe results of directly fitting human execution data on this task. We next present and then empirically compare two methods for modeling control knowledge acquisition (reinforcement learning and a novel variant of action models) with human learning on the task. The paper concludes with an experimental demonstration of the impact of background knowledge on system performance. Our results indicate that the performance of our action models approach more closely approximates the rate of human learning on this task than does reinforcement learning. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Arbib, M.A. </author> <year> (1972). </year> <title> The Metaphorical Brain. </title> <publisher> NY: Wi-ley and Sons Publishers. </publisher>
Reference-contexts: By far the most widely used machine learning method for tasks like ours is reinforcement learning. Reinforcement learning is mathematically sufficient for learning policies for our task, yet has no explicit world model. More common in the cognitive science literature are action models, e.g., <ref> (Arbib, 1972) </ref>, which require building explicit representations of the dynamics of the world to choose actions.
Reference: <author> Breiman, L., Friedman, J.H. Olshen, R.A.. & Stone, C.J. </author> <year> (1984). </year> <title> Classification and Regression Trees. </title> <address> Belmont, CA: </address> <publisher> Wadsworth International Group Publishers. </publisher>
Reference-contexts: We used 312 time-indexed execution trace snapshots collected from an expert subject. Each snapshot had the sensor values plus the corresponding action taken by the subject under these conditions. Four supervised inductive learning paradigms induced controllers from this data: CART <ref> (Breiman et al., 1984) </ref>, C4.5 (Quinlan, 1986), MDL (Ris-sanen, 1983), and backpropagation in neural networks (Rumelhart & McClelland, 1986).
Reference: <author> Chapman, D. & Kaelbling, L. </author> <year> (1991). </year> <title> Input generalization in delayed reinforcement learning: An algorithm and performance comparisons. </title> <booktitle> In Proceedings of Twelfth International Joint Conference on Artificial Intelligence (pp. </booktitle> <pages> 726-731). </pages> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann Publishers. </publisher>
Reference-contexts: Another likely reason for the dominance of action model-based methods is that neural networks with backpropagation tend to learn slowly <ref> (Chapman & Kaelbling, 1991) </ref>.
Reference: <author> Drescher, G.L. </author> <year> (1991). </year> <title> Made-Up Minds. </title> <address> Canbridge, MA: </address> <publisher> MIT Press. </publisher>
Reference: <author> Gordon, D., Schultz, A., Grefenstette, J., Ballas, J., & Perez, M. </author> <year> (1994). </year> <title> User's Guide to the Navigation and Collision Avoidance Task. </title> <institution> Naval Research Laboratory Technical Report AIC-94-013. </institution>
Reference: <author> Gordon, D. & Subramanian, D. </author> <year> (1993). </year> <title> A Multistrat-egy Learning Scheme for Agent Knowledge Acquisition. </title> <journal> Informatica, </journal> <volume> 17, </volume> <pages> 331-346. </pages>
Reference: <author> Jordan, M.I. & Rumelhart, D.E. </author> <year> (1992). </year> <title> Forward models: supervised learning with a distal teacher. </title> <journal> Cognitive Science, </journal> <volume> 16, </volume> <pages> 307-354. </pages>
Reference: <author> Lin, L. </author> <year> (1992). </year> <title> Self-Improving Reactive Agents Based on Reinforcement Learning, Planning and Teaching. </title> <journal> Machine Learning, </journal> <volume> 8, </volume> <pages> 293-321. </pages>
Reference-contexts: While q-learning with explicit state representations addresses the temporal credit assignment problem, it is standard practice to use input generalization and neural networks to also address the structural credit assignment problem, e.g., <ref> (Lin, 1992) </ref>. <p> The result is improved q-values at the output nodes. We selected q-learning as a benchmark algorithm with which to compare because the literature reports a wide range of successes with this algorithm, including on tasks with aspects similar to the NRL Navigation task, e.g., see <ref> (Lin, 1992) </ref>. Our implementation uses standard q-learning with neural networks. One network corresponds to each action (i.e., there are three turn networks corresponding to turn left, turn right, and go straight; speed is fixed at a level frequently found in the human execution traces, i.e., 20/40).
Reference: <author> Mahadevan, S. </author> <year> (1992). </year> <title> Enhancing transfer in reinforcement learning by building stochastic models of robot actions. </title> <booktitle> In Proceedings of the Ninth International Conference on Machine Learning (pp. </booktitle> <pages> 290-299). </pages> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann Publishers. </publisher>
Reference: <author> Moore, A. </author> <year> (1992). </year> <title> Fast, robust adaptive control by learning only forward models. </title> <booktitle> In Proceedings of the International Joint Conference on Neural Networks, </booktitle> <volume> 4, </volume> <pages> (pp. 571-578). </pages> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Arbib (1972) and Drescher (1991) provide examples in the psychological literature, STRIPS (Nilsson, 1980) is a classic example in the AI literature, and Sutton uses them in DYNA (Sutton, 1988). The learning of action models has been studied in the neural networks <ref> (Moore, 1992) </ref>, machine learning (Sutton, 1990; Mahadevan, 1992), and cognitive science (Munro, 1987; Jordan & Rumelhart, 1992) communities. Our algorithm uses two functions: A : sensors fi actions ! sensors P : sensors ! &lt; A is an action model, which our method represents as a decision tree.
Reference: <author> Munro, P. </author> <year> (1987). </year> <title> A dual back-propagation scheme for scalar reward learning. </title> <booktitle> In Proceedings of the Ninth Annual Conference of the Cognitive Science Society (pp. </booktitle> <pages> 165-176). </pages> <address> Hillsdale, NJ: </address> <publisher> Erlbaum. </publisher>
Reference: <author> Nilsson, N. </author> <year> (1980). </year> <booktitle> Principles of Artificial Intelligence. </booktitle> <address> Palo Alto, CA: </address> <publisher> Tioga Publishing Company. </publisher>
Reference-contexts: Action models (i.e., forward models) have appeared in multidisciplinary sources in the literature. Arbib (1972) and Drescher (1991) provide examples in the psychological literature, STRIPS <ref> (Nilsson, 1980) </ref> is a classic example in the AI literature, and Sutton uses them in DYNA (Sutton, 1988). The learning of action models has been studied in the neural networks (Moore, 1992), machine learning (Sutton, 1990; Mahadevan, 1992), and cognitive science (Munro, 1987; Jordan & Rumelhart, 1992) communities.
Reference: <author> Quinlan, J.R. </author> <year> (1986). </year> <title> Induction of decision trees. </title> <journal> Machine Learning, </journal> <volume> 1, </volume> <pages> 81-107. </pages>
Reference-contexts: We used 312 time-indexed execution trace snapshots collected from an expert subject. Each snapshot had the sensor values plus the corresponding action taken by the subject under these conditions. Four supervised inductive learning paradigms induced controllers from this data: CART (Breiman et al., 1984), C4.5 <ref> (Quinlan, 1986) </ref>, MDL (Ris-sanen, 1983), and backpropagation in neural networks (Rumelhart & McClelland, 1986). For all of these methods the 12 inputs were the sensor values plus the value of the last turn and last speed, and the output was an action chosen by our subject. <p> Our algorithm uses two functions: A : sensors fi actions ! sensors P : sensors ! &lt; A is an action model, which our method represents as a decision tree. The decision trees are learned using Quinlan's C4.5 system <ref> (Quinlan, 1986) </ref>. 4 P rates the desirability of various sensor configurations. P embodies background (relevance) knowledge about the task. For sonars, high utilities are associated with large values (no or distant mines), and for the bearing sensor high utilities are associated with values closer to the target being straight ahead.
Reference: <author> Rissanen, J. </author> <year> (1983). </year> <title> Minimum Description Length Principle. </title> <type> Report RJ 4131 (45769), </type> <institution> IBM Research Laboratory, </institution> <address> San Jose. </address>
Reference: <author> Rumelhart, D.E. & McClelland, J.L. </author> <year> (1986). </year> <title> Parallel Distributed Processing : Explorations in the Mi-crostructure of Cognition. </title> <address> Cambridge, MA: </address> <publisher> MIT Press. </publisher>
Reference-contexts: Each snapshot had the sensor values plus the corresponding action taken by the subject under these conditions. Four supervised inductive learning paradigms induced controllers from this data: CART (Breiman et al., 1984), C4.5 (Quinlan, 1986), MDL (Ris-sanen, 1983), and backpropagation in neural networks <ref> (Rumelhart & McClelland, 1986) </ref>. For all of these methods the 12 inputs were the sensor values plus the value of the last turn and last speed, and the output was an action chosen by our subject. One decision tree or neural net was constructed for predicting the subject's next turn.
Reference: <author> Skinner, B.F. </author> <year> (1984). </year> <title> Selection by consequences. </title> <journal> The Behavior and Brain Sciences, </journal> <volume> 7, </volume> <pages> 477-510. </pages>
Reference-contexts: More common in the cognitive science literature are action models, e.g., (Arbib, 1972), which require building explicit representations of the dynamics of the world to choose actions. Reinforcement learning Reinforcement learning has been studied extensively in the psychological literature, e.g., <ref> (Skinner, 1984) </ref>, and has recently become very popular in the machine learning literature, e.g., (Sutton, 1988; Lin, 1992; Gordon & Subramanian, 1993).
Reference: <author> Sutton, R. </author> <year> (1988). </year> <title> Learning to Predict by the Methods of Temporal Differences. </title> <journal> Machine Learning, </journal> <volume> 3, </volume> <pages> 9-44. </pages>
Reference-contexts: Action models (i.e., forward models) have appeared in multidisciplinary sources in the literature. Arbib (1972) and Drescher (1991) provide examples in the psychological literature, STRIPS (Nilsson, 1980) is a classic example in the AI literature, and Sutton uses them in DYNA <ref> (Sutton, 1988) </ref>. The learning of action models has been studied in the neural networks (Moore, 1992), machine learning (Sutton, 1990; Mahadevan, 1992), and cognitive science (Munro, 1987; Jordan & Rumelhart, 1992) communities.
Reference: <author> Sutton, R. </author> <year> (1990). </year> <title> Integrated architectures for learning, planning, and reacting based on approximating dynamic programming. </title> <booktitle> In Proceedings of the Seventh International Conference on Machine Learning (pp. </booktitle> <pages> 216-224). </pages> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Watkins, C. </author> <year> (1989). </year> <title> Learning from Delayed Rewards. Doctoral dissertation. </title> <address> Cambridge, England: Cam-bridge University. </address>
Reference-contexts: Reinforcement learning provides a method for modeling the acquisition of the function F , described above. Currently, the most popular type of reinforcement learning is q-learning, developed by Watkins, which is based on ideas from temporal difference learning, as well as conventional dynamic programming <ref> (Watkins, 1989) </ref>. It requires estimating the q-value of a sensor configuration s, i.e., q (s; a) is a prediction of the utility of taking action a in a world state represented by s. The q-values are updated during learning based on minimizing a temporal difference error.
References-found: 19

