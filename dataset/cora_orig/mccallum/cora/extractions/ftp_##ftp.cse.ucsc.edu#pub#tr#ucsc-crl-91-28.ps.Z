URL: ftp://ftp.cse.ucsc.edu/pub/tr/ucsc-crl-91-28.ps.Z
Refering-URL: http://www.research.att.com/~yoav/publications.html
Root-URL: 
Title: The Weighted Majority Algorithm  
Author: Nick Littlestone Manfred K. Warmuth 
Address: Santa Cruz, CA 95064 USA  
Affiliation: Baskin Center for Computer Engineering Information Sciences University of California, Santa Cruz  
Date: Revised October 26, 1992  
Pubnum: UCSC-CRL-91-28  
Abstract: fl This research was primarily conducted while this author was at the University of Calif. at Santa Cruz with support from ONR grant N00014-86-K-0454, and at Harvard University, supported by ONR grant N00014-85-K-0445 and DARPA grant AFOSR-89-0506. Current address: NEC Research Institute, 4 Independence Way, Princeton, NJ 08540. E-mail address: nickl@research.nj.nec.com. y Supported by ONR grants N00014-86-K-0454 and N00014-91-J-1162. Part of this research was done while this author was on sabbatical at Aiken Computation Laboratory, Harvard, with partial support from the ONR grants N00014-85-K-0445 and N00014-86-K-0454. Address: Department of Computer Science, University of California at Santa Cruz. E-mail address: manfred@cs.ucsc.edu. 
Abstract-found: 1
Intro-found: 1
Reference: [Ang88] <author> Dana Angluin. </author> <title> Queries and concept learning. </title> <journal> Machine Learning, </journal> <volume> 2 </volume> <pages> 319-342, </pages> <year> 1988. </year>
Reference: [Ass83] <author> Patrick Assouad. </author> <title> Densite et dimension. </title> <journal> Ann. Inst. Fourier, Grenoble, </journal> <volume> 33(3) </volume> <pages> 233-282, </pages> <year> 1983. </year> <note> 34 References </note>
Reference-contexts: This notion of shattering has been considered by Assouad <ref> [Ass83] </ref>. It is dual to the notion of shattering used to define the Vapnik-Chervonenkis dimension of a concept class [VC71,BEHW89].
Reference: [BEHW89] <author> Anselm Blumer, Andrzej Ehrenfeucht, David Haussler, and Manfred K. War-muth. </author> <title> Learnability and the Vapnik-Chervonenkis dimension. </title> <journal> JACM, </journal> <volume> 36(4) </volume> <pages> 929-965, </pages> <year> 1989. </year>
Reference: [BF72] <author> J. M. Barzdin and R. V. Freivalds. </author> <title> On the prediction of general recursive functions. </title> <journal> Sov. Math. Dokl., </journal> <volume> 13 </volume> <pages> 1224-1228, </pages> <year> 1972. </year>
Reference: [BF74] <author> J. M. Barzdin and R. V. Freivalds. </author> <title> Prognozirovanie i predel'nyi sintez effektivno perechislimykh klassov funktsii (prediction and limit synthesis of effectively enumerable classes of functions). </title> <editor> In J. M. Barzdin, editor, </editor> <booktitle> Theory of Algorithms and Programs, </booktitle> <volume> volume 1, </volume> <pages> pages 101-111. </pages> <institution> Latvian State University, </institution> <year> 1974. </year> <note> (in Russian). </note>
Reference-contexts: Instead, we construct an algorithm WMI 2 that uses an increasing initial segment 4. Selection From an Infinite Pool 11 of "active" weights 5 . This algorithm and its analysis is based on techniques introduced by Barzdin and Freivalds <ref> [BF74] </ref> (they consider only the fi = 0 case). These techniques, in addition to dealing with an infinite pool size, also allow one to work with finite-precision approximations to the weights in calculating the predictions of the algorithm. <p> Anomalies 29 the statement of the contrapositive of the final implication of the theorem that appears immediately following the theorem.) We also obtain the following result given in slightly weaker form in <ref> [BF74] </ref>. (Their lower bound is log 2 n 3.) Theorem 7.4: There exists a domain X and a computably indexed class of total recursive functions ' 1 ; ' 2 ; : : : such that for any n 1 and any on-line prediction algorithm A there exists a sequence of
Reference: [Bil86] <author> Patrick Billingsley. </author> <title> Probability and Measure. </title> <publisher> Wiley, </publisher> <address> New York, </address> <year> 1986. </year>
Reference: [DMW88] <author> Alfredo DeSantis, George Markowski, and Mark N. Wegman. </author> <title> Learning probabilistic prediction functions. </title> <booktitle> In Proceedings of the 1988 Workshop on Computational Learning Theory, </booktitle> <pages> pages 312-328, </pages> <publisher> Published by Morgan Kaufmann, </publisher> <address> San Mateo, CA, </address> <year> 1988. </year>
Reference-contexts: We make a similar comparison for the randomized algorithm WMR. The concluding section, Section 9 gives an overview of the various algorithms introduced here and mentions a number of directions for future research. DeSantis, Markowsky and Wegman <ref> [DMW88] </ref> applied an algorithm similar to WMC to a countably infinite pool (as in WMI 2 ) in a completely different setting.
Reference: [FSV89] <author> R. V. Freivalds, C. H. Smith, and M. Velauthapillai. </author> <title> Trade-off among parameters affecting inductive inference. </title> <journal> Information and Computation, </journal> <volume> 82 </volume> <pages> 323-349, </pages> <year> 1989. </year>
Reference: [HKS91] <author> David Haussler, Michael Kearns, and Robert Schapire. </author> <title> Bounds on the sample complexity of Bayesian learning using information theory and the VC dimension. </title> <booktitle> In Proceedings of the Fourth Annual Workshop on Computational Learning Theory, </booktitle> <pages> pages 61-74, </pages> <publisher> Published by Morgan Kaufmann, </publisher> <address> San Mateo, CA, </address> <year> 1991. </year>
Reference: [HO91] <author> David Haussler and Manfred Opper. </author> <title> Calculation of the learning curve of Bayes optimal classification algorithm for learning a perceptron with noise. </title> <booktitle> In Proceedings of the Fourth Annual Workshop on Computational Learning Theory, </booktitle> <pages> pages 75-87, </pages> <publisher> Published by Morgan Kaufmann, </publisher> <address> San Mateo, CA, </address> <year> 1991. </year>
Reference-contexts: The Bayes optimal classification algorithm that they consider is a special case of the Weighted Majority algorithm WM, and the randomized version WMR is the Gibbs algorithm of <ref> [HO91] </ref>. Furthermore our algorithm WMR is similar to a learning procedure studied in [LTS89]. However the analysis given there is very different from ours. Notations and assumptions: In this paper we design various master algorithms that use the predictions of the pool of algorithms to make their own predictions.
Reference: [HSW90] <author> David Helmbold, Robert Sloan, and Manfred K. Warmuth. </author> <title> Learning nested differences of intersection-closed concept classes. </title> <journal> Machine Learning, </journal> <volume> 5 </volume> <pages> 165-196, </pages> <year> 1990. </year> <booktitle> Special issue for the Second Annual Workshop on Computation Learning Theory, 1989, </booktitle> <address> Santa Cruz, California. </address>
Reference-contexts: The variant WMI 2 incorporates a method of Barzdin and Freivalds that lets one deal explicitly with computational imprecision. An application of these techniques to deal with an unknown parameter can be found in <ref> [HSW90] </ref>. The classes DIFF (C; B) discussed in that paper have the property that each function in such a class has a certain "depth," which is a non-negative integer. The basic algorithm given in that paper requires as input an estimate of the depth of the target. <p> The basic algorithm given in that paper requires as input an estimate of the depth of the target. Mistake bounds are derived <ref> [HSW90, Figure 10] </ref> under the assumption that this estimate 6 1. Introduction is at least as large as the actual depth; to obtain a good bound it must not be too much larger than the actual depth. <p> The following corollary gives a weight sequence that is useful when m i grows exponentially with i. A very similar version has been applied in <ref> [HSW90] </ref>. Corollary 4.3: Let S be any sequence of instances with binary labels on which the ith algorithm makes at most m i mistakes (i 1).
Reference: [Lit88] <author> Nick Littlestone. </author> <title> Learning quickly when irrelevant attributes abound: A new linear-threshold algorithm. </title> <journal> Machine Learning, </journal> <volume> 2 </volume> <pages> 285-318, </pages> <year> 1988. </year>
Reference: [Lit89a] <author> Nick Littlestone. </author> <title> From on-line to batch learning. </title> <booktitle> In Proceedings of the Second Annual Workshop on Computational Learning Theory, </booktitle> <pages> pages 269-284, </pages> <publisher> Published by Morgan Kaufmann, </publisher> <address> San Mateo, CA, </address> <year> 1989. </year>
Reference-contexts: Randomized Predictions 23 We will use a theorem regarding Chernoff bounds applied to supermartingales that is given in <ref> [Lit89a] </ref>. Let (X; A; P ) be a probability space and let G 1 ; : : : ; G n be -algebras contained in A and let S 1 ; : : :; S t be a sequence of random variables on this probability space. <p> Theorem 6.2: <ref> [Lit89a] </ref> Let (X; A; P ) be a probability space and let G G 0 G 1 G t be -algebras contained in A.
Reference: [Lit89b] <author> Nick Littlestone. </author> <title> Mistake Bounds and Logarithmic Linear-threshold Learning Algorithms. </title> <type> PhD thesis, Technical Report UCSC-CRL-89-11, </type> <institution> University of Calif., Santa Cruz, </institution> <year> 1989. </year>
Reference: [LTS89] <author> Esther Levin, Naftali Tishby, and Sarah A. Solla. </author> <title> A statistical approach to learning and generalization in layered neural networks. </title> <booktitle> In Proceedings of the Second Annual Workshop on Computational Learning Theory, </booktitle> <pages> pages 245-258, </pages> <publisher> Published by Morgan Kaufmann, </publisher> <address> San Mateo, CA, </address> <year> 1989. </year>
Reference-contexts: The Bayes optimal classification algorithm that they consider is a special case of the Weighted Majority algorithm WM, and the randomized version WMR is the Gibbs algorithm of [HO91]. Furthermore our algorithm WMR is similar to a learning procedure studied in <ref> [LTS89] </ref>. However the analysis given there is very different from ours. Notations and assumptions: In this paper we design various master algorithms that use the predictions of the pool of algorithms to make their own predictions.
Reference: [LW89a] <author> Nick Littlestone and Manfred K. Warmuth. </author> <title> The weighted majority algorithm. </title> <booktitle> In Proceedings of the 30th Annual Symposium on the Foundations of Computer Science, </booktitle> <pages> pages 256-261. </pages> <publisher> IEEE, </publisher> <year> 1989. </year>
References-found: 16

