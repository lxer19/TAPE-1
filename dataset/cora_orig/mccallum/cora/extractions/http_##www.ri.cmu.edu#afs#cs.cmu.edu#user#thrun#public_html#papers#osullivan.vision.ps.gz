URL: http://www.ri.cmu.edu/afs/cs.cmu.edu/user/thrun/public_html/papers/osullivan.vision.ps.gz
Refering-URL: http://www.ri.cmu.edu/afs/cs.cmu.edu/user/thrun/public_html/papers/full.html
Root-URL: 
Title: 1 Explanation Based Learning for Mobile Robot Perception To appear in: Symbolic Visual Learning  
Author: K. Ikeuchi and M. Veloso Joseph O'Sullivan and Tom M. Mitchell Sebastian Thrun 
Affiliation: Carnegie Mellon University  University of Bonn  
Note: (eds.), Oxford University Press  
Abstract: Although machine learning techniques have been applied with remarkable success to several problems of computer perception and vision, most of these problems have been fairly simple in nature. The difficulty with scaling up to more complex tasks is that inductive learning methods require a very large number of training examples in order to generalize correctly from complex sensor data. This chapter proposes an approach to overcoming this difficulty, by relying on previously learned information to augment the available training data. In particular, we consider the task faced by a mobile robot learning to recognize new objects within an already-familiar environment. Because the robot has previously operated within this environment (here the corridors of a particular building), it has already had the opportunity to learn certain regularities that can be useful in subsequent learning tasks. Given a new task, such as learning to recognize the distance to the next door in the corridor, knowledge of these regularities enables the system to learn the new task more accurately from a limited quantity of new training data. We describe the Explanation-Based Neural Network (EBNN) algorithm for utilizing previously learned knowledge, and examine its performance for the mobile robot perception task of door recognition in a familiar corridor based on color vision and sonar sensor data. Experimental results indicate that EBNN is able to generalize more accurately than purely inductive methods such as Backpropagation, even when its prior knowledge is only approxi mately correct. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Rich Caruana. </author> <title> Learning Many Related Tasks At the Same Time With Backpropagation. </title> <booktitle> In Advances in Neural Information Processing Systems 6. </booktitle> <publisher> Mor-gan Kaufmann, </publisher> <month> December </month> <year> 1994. </year> <title> 1. LEARNING FOR MOBILE ROBOT PERCEPTION 29 </title>
Reference-contexts: A number of approaches have been proposed for scaling up learning, by introducing constraints in addition to observed training data. These include, for example, engineering human knowledge into the system [29, 20], and using constraints from related learning tasks <ref> [21, 1] </ref>. This chapter considers methods by which a robot can use previously learned knowledge about its environment to augment the available training data when confronting new perceptual learning tasks.
Reference: [2] <author> Jonathan Connell and Sridhar Mahadevan. </author> <title> Rapid Task Learning for Real Robots. In Robot Learning, </title> <booktitle> chapter 4, </booktitle> <pages> pages 105-150. </pages> <publisher> Kluwer Academic Publishers, </publisher> <year> 1993. </year>
Reference-contexts: 1. Introduction Recent results in robot learning have demonstrated that robots can learn simple strategies from very little initial knowledge in restricted environments <ref> [9, 2, 17] </ref>. 1 2 O'SULLIVAN, MITCHELL AND THRUN While these results indicate the potential role of machine learning for robot perception and control, new approaches are needed to scale up to more complex problems and many realistic environments.
Reference: [3] <author> G. DeJong and R. Mooney. </author> <title> Explanation-Based Learning: An Alternative View. </title> <journal> Machine Learning, </journal> <volume> 1(2) </volume> <pages> 145-176, </pages> <year> 1986. </year>
Reference-contexts: Earlier explanation-based learning methods that used symbolic representations <ref> [3, 14] </ref> and the analytical learning component of EBNN are based on the same principle: given a target function, F , the domain theory is used as an alternative method for computing F , and the relevance of the various training example features is then extracted from the trace of this
Reference: [4] <author> B. Efron. </author> <title> The Jacknife, the Bootstrap, and Other Resampling Plans. </title> <publisher> SIAM, </publisher> <address> Philadelphia, </address> <year> 1982. </year>
Reference-contexts: In such cases, a more realistic approach is to use jacknifing <ref> [4] </ref>, or a leave-one-out strategy, in which the number of weight tuning epochs is determined as follows: Divide the N available examples into sets of N-1 train and 1 hold-out example, in each of the N possible ways.
Reference: [5] <author> Susan L. Epstein. </author> <title> For the Right Reasons: The FORR Architecture for Learning in a Skill Domain. </title> <journal> Cognitive Science, </journal> <volume> 18 </volume> <pages> 479-511, </pages> <year> 1994. </year>
Reference-contexts: in order to make relevant prior knowledge accessible and obvious when and where appropriate? Under what conditions should the agent rely on its learned target function to be more reliable than the domain theory? Some of these architectural issues have been explored in systems such as Soar [8] and FORR <ref> [5] </ref>, but many open issues remain. 28 O'SULLIVAN, MITCHELL AND THRUN 8. Conclusion The lesson of this chapter for computer vision learning is that when data is scarce, more accurate learning is possible by relying on available prior knowledge.
Reference: [6] <author> Li-Min Fu. </author> <title> Integration of neural heuristics into knowledge-based inference. </title> <journal> Connection Science, </journal> <volume> 1(3) </volume> <pages> 325-339, </pages> <year> 1989. </year>
Reference-contexts: This section considers related research on utilizing prior knowledge to guide learning. Three of the best known methods for inserting prior knowledge into neural network learning are Fu's method <ref> [6] </ref>, KBANN [23] and RAPTURE [12]. These 26 O'SULLIVAN, MITCHELL AND THRUN methods use prior knowledge in the form of hand-coded propositional rules to guide learning of neural networks.
Reference: [7] <author> D. Haussler. </author> <title> Decision Theoretic Generalizations of the PAC Model for Neural Net and other Learning Applications. </title> <journal> Inform. Comput., </journal> <volume> 100 </volume> <pages> 78-150, </pages> <year> 1992. </year>
Reference-contexts: Both experimental and theoretical research indicates that purely inductive learning methods, which basically rely on detecting statistical regularities among the training data, scale poorly with such increases in complexity <ref> [7, 30, 10] </ref>. A number of approaches have been proposed for scaling up learning, by introducing constraints in addition to observed training data. These include, for example, engineering human knowledge into the system [29, 20], and using constraints from related learning tasks [21, 1].
Reference: [8] <author> Laird, J.E. and Rosenbloom, P.S. and Newell, A. </author> <title> Chunking in Soar: The Anatomy of a General Learning Mechanism. </title> <journal> Machine Learning, </journal> <volume> 1(1) </volume> <pages> 11-46, </pages> <year> 1986. </year>
Reference-contexts: its long-term memory in order to make relevant prior knowledge accessible and obvious when and where appropriate? Under what conditions should the agent rely on its learned target function to be more reliable than the domain theory? Some of these architectural issues have been explored in systems such as Soar <ref> [8] </ref> and FORR [5], but many open issues remain. 28 O'SULLIVAN, MITCHELL AND THRUN 8. Conclusion The lesson of this chapter for computer vision learning is that when data is scarce, more accurate learning is possible by relying on available prior knowledge.
Reference: [9] <author> Long-Ji Lin. </author> <title> Reinforcement Learning for Robots using Neural Networks. </title> <type> PhD thesis, </type> <institution> Carnegie Mellon University, School of Computer Science, </institution> <address> Pittsburgh, PA 15232, </address> <year> 1993. </year> <note> also available as Technical Report CMU-CS-93-103. </note>
Reference-contexts: 1. Introduction Recent results in robot learning have demonstrated that robots can learn simple strategies from very little initial knowledge in restricted environments <ref> [9, 2, 17] </ref>. 1 2 O'SULLIVAN, MITCHELL AND THRUN While these results indicate the potential role of machine learning for robot perception and control, new approaches are needed to scale up to more complex problems and many realistic environments.
Reference: [10] <author> Nick Littlestone. </author> <title> Learning quickly when irrelevant attributes abound: A new linear-threshold algorithm. </title> <journal> Machine Learning, </journal> <volume> 2 </volume> <pages> 285-318, </pages> <year> 1987. </year>
Reference-contexts: Both experimental and theoretical research indicates that purely inductive learning methods, which basically rely on detecting statistical regularities among the training data, scale poorly with such increases in complexity <ref> [7, 30, 10] </ref>. A number of approaches have been proposed for scaling up learning, by introducing constraints in addition to observed training data. These include, for example, engineering human knowledge into the system [29, 20], and using constraints from related learning tasks [21, 1].
Reference: [11] <author> Sridhar Mahadevan. </author> <title> Enhancing Transfer in Reinforcement Learning by Building Stochastic Models of Robot Actions. </title> <editor> In Sleeman D. and Edwards P., editors, </editor> <booktitle> Proceedings of Ninth International Workshop on Machine Learning, </booktitle> <pages> pages 290-299. </pages> <publisher> Morgan Kaufmann, </publisher> <month> July </month> <year> 1992. </year>
Reference-contexts: EBNN differs from this approach in that this derivative knowledge is learned itself, represented by the domain theory networks. One technique for learning of domain knowledge to aid robot navigation has used a sonar-specific means of storing prior knowledge, rather than connectionist networks <ref> [11] </ref>. In this instance, occupancy grids were used to predict the effects of the actions for a sonar based robot.
Reference: [12] <author> J. Jeffrey Mahoney and Raymond J. Mooney. </author> <title> Combining neural and symbolic learning to revise probabilistic rule bases. </title> <editor> In J. E. Moody, S. J. Hanson, and R. P. Lippmann, editors, </editor> <booktitle> Advances in Neural Information Processing Systems 5, </booktitle> <address> San Mateo, CA, 1993. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: This section considers related research on utilizing prior knowledge to guide learning. Three of the best known methods for inserting prior knowledge into neural network learning are Fu's method [6], KBANN [23] and RAPTURE <ref> [12] </ref>. These 26 O'SULLIVAN, MITCHELL AND THRUN methods use prior knowledge in the form of hand-coded propositional rules to guide learning of neural networks.
Reference: [13] <author> Ryusuke Masuoka, Sebastian Thrun, and Tom M. Mitchell. </author> <title> Constraining Neural Networks to Fit Target Slopes. Internal Memo, Learning Robot Laboratory, </title> <institution> School of Computer Science, Carnegie Mellon University, </institution> <year> 1993. </year>
Reference-contexts: Refine the target function network: Update the target network weights to fit both the observed target value (inductive component), and the target derivatives extracted from the explanation (analytical component). version <ref> [13] </ref> of the Tangent-Prop algorithm, which takes advantage of the exact form of the EBNN derivative error (This error term adds up derivative errors with respect to all axial direction vectors, instead of just picking one direction in the input space). <p> This ratio, averaged over the various training set sizes, improves as the number of inputs increases. This also reflects our experience from prior research on synthetic learning tasks <ref> [13] </ref>. Representation Sonar Vision Sonar + Vision Input Size 24 128 152 Improvement Ratio 2.6 5.0 5.2 Table 2: We repeat the experiments of Section 4.2.1. using three different input representations. The improvement ratio, defined in the text, indicates the relative improvement of using EBNN versus using Backpropagation. <p> LEARNING FOR MOBILE ROBOT PERCEPTION 25 5.5. Computational Costs EBNN has a greater computational cost than Backpropagation, due to the additional constraints of training on derivative information. Theoretical research <ref> [13] </ref> has indicated that training with EBNN is about 2I + 7 2 times that of Backpropagation, where I is the number of network inputs. This assumes that all explanations have yielded correct predictions, and thus all derivative information is being used for training.
Reference: [14] <author> T.M. Mitchell, R.K. Keller, and S. Kedar-Cabelli. </author> <title> Explanation-Based Generalization: A Unifying View. </title> <journal> Machine Learning, </journal> <volume> 1(1), </volume> <year> 1986. </year> <note> 30 O'SULLIVAN, MITCHELL AND THRUN </note>
Reference-contexts: Earlier explanation-based learning methods that used symbolic representations <ref> [3, 14] </ref> and the analytical learning component of EBNN are based on the same principle: given a target function, F , the domain theory is used as an alternative method for computing F , and the relevance of the various training example features is then extracted from the trace of this
Reference: [15] <author> Tom M. Mitchell and Sebastian B. Thrun. </author> <title> Explanation-Based Neural Network Learning for Robot Control. </title> <editor> In J. E. Moody, S. J. Hanson, and R. P. Lipmann, editors, </editor> <booktitle> Advances in Neural Information Processing Systems 5. </booktitle> <publisher> Morgan Kaufmann, </publisher> <month> December </month> <year> 1993. </year>
Reference-contexts: Negative values of ff i , that is, values for which ffi i &gt; ffi threshold , are set to 0. This heuristic, LOB* <ref> [15] </ref>, for setting ff i is based on the assumption that the accuracy of the explanation's derivatives are correlated to the accuracy of the explanation's predictions. 3.5. <p> It is possible to construct an alternative domain theory in which the target function is itself used as a component of the domain theory, thereby eliminating the cost of gathering data to learn an additional object-specific network. In other work <ref> [15] </ref> this type of structure was found to be successful for learning to control a simulated robot.
Reference: [16] <author> Tom M. Mitchell and Sebastian B. Thrun. </author> <title> Learning Analytically and Inductively. </title> <editor> In Steier and Mitchell, editors, </editor> <title> Mind Matters: A Tribute to Allen Newell. </title> <publisher> Erlbaum, </publisher> <year> 1995. </year>
Reference-contexts: Previous work has demonstrated that EBNN performance degrades substantially if the LOB* heuristic is removed, and derivatives derived from explanations are simply used in all cases <ref> [16] </ref>. This heuristic is based on the assumption that domain theory accuracy is a good predictor of the quality of the derivatives used for generalization. Consider creating an antagonistic domain theory, which has perfect predictions over the training examples, but random derivatives at these points.
Reference: [17] <author> Andrew W. Moore. </author> <title> Efficent Memory-based Learning for Robot Control. </title> <type> PhD thesis, </type> <institution> Trinity Hall, University of Cambridge, </institution> <address> England, </address> <year> 1990, 1990. </year>
Reference-contexts: 1. Introduction Recent results in robot learning have demonstrated that robots can learn simple strategies from very little initial knowledge in restricted environments <ref> [9, 2, 17] </ref>. 1 2 O'SULLIVAN, MITCHELL AND THRUN While these results indicate the potential role of machine learning for robot perception and control, new approaches are needed to scale up to more complex problems and many realistic environments.
Reference: [18] <author> Joseph O'Sullivan, Tom M. Mitchell, and Sebastian B. Thrun. </author> <title> Explanation Based Learning for Mobile Robot Perception. </title> <booktitle> In Proceedings of MLC-Colt Workshop on Robot Learning, </booktitle> <institution> Rutgers University, </institution> <address> New Brunswick, N.J, </address> <month> July </month> <year> 1994. </year>
Reference-contexts: Section 5 discusses the main lessons of the experiments reported here. In section 6, we review other research on using prior knowledge in learning. Section 7 describes future research and Section 8 our conclusions. This chapter expands upon, and includes portions of, work previously presented in <ref> [18] </ref>. 2. Learning for Mobile Robot Perception Xavier was developed at CMU, and is built on a 4 wheeled omni-directional RWI base, 24 inches in diameter.
Reference: [19] <author> Micheal J. Pazzani and Dennis Kibler. </author> <title> The role of prior knowledge in inductive learning. </title> <journal> Machine Learning, </journal> <volume> 9 </volume> <pages> 54-97, </pages> <year> 1992. </year>
Reference-contexts: The original work on symbolic explanation-based learning algorithms also made the restrictive assumption that the domain theory was perfectly correct. This restriction is overcome by more recent work such as FOCL <ref> [19] </ref>. Simard and colleagues [24] TangentProg algorithm, which is used in EBNN to fit derivatives extracted from the domain theory networks, was originally applied to the task of learning hand-written digits.
Reference: [20] <author> Dean Pomerleau. </author> <title> Neural Network Perception for Mobile Robot Guidance. </title> <publisher> Kluwer Academic Publishers, </publisher> <year> 1993. </year>
Reference-contexts: A number of approaches have been proposed for scaling up learning, by introducing constraints in addition to observed training data. These include, for example, engineering human knowledge into the system <ref> [29, 20] </ref>, and using constraints from related learning tasks [21, 1]. This chapter considers methods by which a robot can use previously learned knowledge about its environment to augment the available training data when confronting new perceptual learning tasks. <p> A single output corresponds to the real value of the function Door-Distance. Alternative techniques for modeling real numbers in neural networks (such as activating one of many outputs or Gaussian weighting of outputs <ref> [20] </ref>) were not considered since using a single output turns out to be sufficient for solving the task. (a) Y V - - - to a reduced YUV image (b) in order to make experimentation practical. (The Y components roughly approximate intensity, while the U and V components approximate color).
Reference: [21] <author> Lorien Y. Pratt. </author> <title> Experiments on the Transfer of Knowledge between Neural Networks. In Computational Learning Theory and Natural Learning Systems, Constraints and Prospects, </title> <booktitle> chapter 14, </booktitle> <pages> pages 523-560. </pages> <publisher> MIT Press, </publisher> <year> 1994. </year>
Reference-contexts: A number of approaches have been proposed for scaling up learning, by introducing constraints in addition to observed training data. These include, for example, engineering human knowledge into the system [29, 20], and using constraints from related learning tasks <ref> [21, 1] </ref>. This chapter considers methods by which a robot can use previously learned knowledge about its environment to augment the available training data when confronting new perceptual learning tasks.
Reference: [22] <author> David E. Rumelhart, Geoffery E.Hinton, and Ronald J. Williams. </author> <title> Learning internal representations by error propagation. </title> <editor> In David E. Rymelhart and J. L. McClelland, editors, </editor> <booktitle> Parallel Distributed Processing. Vol I+II. </booktitle> <publisher> MIT Press, </publisher> <year> 1986. </year>
Reference-contexts: The Learning Approach The learning methods considered here use artificial neural networks to represent the learned target function. Neural networks provide a general mechanism for estimating, or learning, a large class of real-valued or discrete-valued functions. Given a set of training examples of the target function, the Backpropagation algorithm <ref> [22] </ref> can be used to tune the weights of the network to fit these training examples. Whether this learned network will generalize to perform accurately on examples outside the training set depends on many factors, notably the number of training examples provided.
Reference: [23] <author> Jude W. Shavlik. </author> <title> An approach for Combining Explanation-Based and Neural Learning algorithms. </title> <journal> Connection Science, </journal> <volume> 1 </volume> <pages> 231-253, </pages> <year> 1989. </year>
Reference-contexts: This section considers related research on utilizing prior knowledge to guide learning. Three of the best known methods for inserting prior knowledge into neural network learning are Fu's method [6], KBANN <ref> [23] </ref> and RAPTURE [12]. These 26 O'SULLIVAN, MITCHELL AND THRUN methods use prior knowledge in the form of hand-coded propositional rules to guide learning of neural networks.
Reference: [24] <author> Patrice Simard, Bernard Victorri, Yann LeCun, and John Denker. </author> <title> Tangent prop a formalism for specifying selected invariances in an adaptive network. </title> <editor> In J. E. Moody, S. J. Hanson, and R. P. Lipmann, editors, </editor> <booktitle> Advances in Neural Information Processing Systems 4, </booktitle> <pages> pages 895-903. </pages> <publisher> Morgan Kaufmann, </publisher> <month> December </month> <year> 1992. </year>
Reference-contexts: Gradient descent to minimize this error function is accomplished by the Tan-gentProp algorithm <ref> [24] </ref>, an extension to Backpropagation that iteratively adjusts network weights to minimize this combined error function. <p> The original work on symbolic explanation-based learning algorithms also made the restrictive assumption that the domain theory was perfectly correct. This restriction is overcome by more recent work such as FOCL [19]. Simard and colleagues <ref> [24] </ref> TangentProg algorithm, which is used in EBNN to fit derivatives extracted from the domain theory networks, was originally applied to the task of learning hand-written digits. In his case, the human designer provided certain directions along which the derivatives of the target network were trained to be zero.
Reference: [25] <author> Sebastian B. Thrun and Tom M. Mitchell. </author> <title> Integrating inductive neural network learning and explanation-based learning. </title> <booktitle> In Proceedings of IJCAI-93, </booktitle> <address> Chamberry, France, </address> <month> July </month> <year> 1993. </year> <pages> IJCAI. </pages>
Reference-contexts: These findings suggest that LOB* successfully prevents EBNN from suffering from misleading domain knowledge. Similar results have been reported for EBNN applied in the context of reinforcement learning <ref> [25] </ref>. 4.2.3. Scaling to High Dimensional Input Spaces Here, we were interested in characterizing the role of prior knowledge with varying dimension of the input space.
Reference: [26] <author> Sebastian B. Thrun and Tom M. Mitchell. </author> <title> Lifelong Robot Learning. </title> <booktitle> In Robotics and Autonomous Systems, </booktitle> <year> 1993. </year> <title> 1. LEARNING FOR MOBILE ROBOT PERCEPTION 31 </title>
Reference-contexts: EBNN is found to outperform or match the performance of Backpropagation over the entire range of situations considered. This model fits in with a lifelong learning approach to robotics <ref> [26] </ref>, in which we assume the robot must confront a variety of learning tasks over a long period of time.
Reference: [27] <author> Sebastian B. Thrun and Tom M. Mitchell. </author> <title> Learning One More Thing. </title> <type> Technical Report CMU-CS-94-184, </type> <institution> School of Computer Science, Carnegie Mellon University, </institution> <month> Sept </month> <year> 1994. </year>
Reference-contexts: For instance, if we included a "Turn right 30 degrees" action, we would have the additional means of explaining each example by "Turn right 30 degrees and perform recognition." Combining results from multiple explanationss has been found to produce increased generalization accuracy in an computer vision domain <ref> [27] </ref>. An alternative use for multiple explanations is in situations when two partially correct explanations are available. In the above situation of turning right, we would also expect the explanations produced to be accurate close to the door.
Reference: [28] <author> Sebastian B. Thrun and Knut Moller. </author> <title> Active Exploration in Dynamic Environments. </title> <editor> In J. E. Moody, S. J. Hanson, and R. P. Lippmann, editors, </editor> <booktitle> Advances in Neural Information Processing Systems 4, </booktitle> <pages> pages 531-538, </pages> <address> San Mateo, CA, 1992. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Although this heuristic is helpful in most cases, in some it may provide poor estimates of the quality of derivatives. One alternative to explore is learning additional networks trained to predict the quality of the domain theory for individual examples, similar to <ref> [28] </ref>. Whereas this paper considered only single explanations as a source of derivative information, it is possible in many cases to have multiple explanations.
Reference: [29] <author> Geoffry G. Towell, Jude W. Shavlik, and Michiel O. Noordewier. </author> <title> Refinement of approximate domain theories by knowledge-based neural networks. </title> <booktitle> In Proceedings of AAAI-90, </booktitle> <pages> pages 861-866. </pages> <publisher> Morgan Kaufmann, </publisher> <month> July </month> <year> 1990. </year>
Reference-contexts: A number of approaches have been proposed for scaling up learning, by introducing constraints in addition to observed training data. These include, for example, engineering human knowledge into the system <ref> [29, 20] </ref>, and using constraints from related learning tasks [21, 1]. This chapter considers methods by which a robot can use previously learned knowledge about its environment to augment the available training data when confronting new perceptual learning tasks.
Reference: [30] <author> L. G. Valiant. </author> <title> A Theory of the Learnable. </title> <journal> Comm. ACM, </journal> <volume> 27 </volume> <pages> 1134-1142, </pages> <year> 1984. </year>
Reference-contexts: Both experimental and theoretical research indicates that purely inductive learning methods, which basically rely on detecting statistical regularities among the training data, scale poorly with such increases in complexity <ref> [7, 30, 10] </ref>. A number of approaches have been proposed for scaling up learning, by introducing constraints in addition to observed training data. These include, for example, engineering human knowledge into the system [29, 20], and using constraints from related learning tasks [21, 1].
References-found: 30

