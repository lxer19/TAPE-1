URL: http://www.umiacs.umd.edu/users/lsd/papers/ugv-pi-report.ps
Refering-URL: http://www.umiacs.umd.edu/users/lsd/pubs.html
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Title: RSTA on the Move: Detection and Tracking of Moving Objects from an Autonomous Mobile Platform  
Author: Larry S. Davis Martin Herman Randal Nelson 
Address: College Park, MD 20742-3275  Philadelphia, PA 19104  Washington, DC 20234  Rochester, NY 14627  
Affiliation: Center for Automation Research University of Maryland  Ruzena Bajcsy Department of Computer and Information Science University of Pennsylvania  Sensory-Interactive Robotics National Institute of Standards and Technology  Department of Computer Science University of Rochester  
Abstract: This report describes progress made during the past year on the UGV RSTA project being conducted by a consortium led by the University of Maryland and including the University of Penn-sylvania, the University of Rochester, and the National Institute of Standards and Technology. We first review work done on the design, implementation and integration of real time vision algorithms for image stabilization, detection of moving objects from a moving platform and camera control. We then present brief descriptions of a number of supporting basic research projects being conducted by the members of the consortium. 
Abstract-found: 1
Intro-found: 1
Reference: [ Balakirsky and Chellappa, 1996 ] <author> S. Balakirsky and R. Chellappa. </author> <title> Performance characterization of image stabilization algorithms, </title> <institution> Center for Automation Research Technical Report, University of Maryland, College Park, MD, </institution> <note> to appear. </note>
Reference-contexts: On the other hand, when the target tracker was integrated with the third algorithm, it achieved a 0% false dismissal rate and a 1% false alarm rate on real IR sequences. Details of this study will be re ported in <ref> [ Balakirsky and Chellappa, 1996 ] </ref> . real FLIR sequences employed in the experiments. The target, near the top of the image, is outlined in a box. Table 1 compares the detection results of the three stabilization algorithms on one of the FLIR sequences.
Reference: [ Burt and Anandan, 1994 ] <author> P. Burt and P. Anandan. </author> <title> Image stabilization by registration to a reference mosaic. </title> <booktitle> In Proc. of ARPA Image Understanding Workshop, </booktitle> <address> Monterey, CA, </address> <month> November </month> <year> 1994, </year> <pages> pp. 425-434. </pages>
Reference-contexts: This system was developed to use a commercially available parallel pipeline image processing board (Datacube MV200) connected to a SUN SPARCstation 20/612, and is able to process 7 frames per second, using images of size 128 fi 128. A very similar scheme is described in <ref> [ Burt and Anandan, 1994 ] </ref> , where more specialized image processing hardware is used to stabilize images by registering frames using a hierarchical approach. 2.1.2 Detection of Independently Moving Objects Image stabilization renders the background of the image approximately stationary.
Reference: [ Cheong and Aloimonos, 1995 ] <author> L. Cheong and Y. Aloimonos. </author> <title> Isodistortion contours and ego-motion estimation. </title> <booktitle> In Proc. of IEEE International Symposium on Computer Vision, Coral Gables, </booktitle> <address> FL, </address> <month> November </month> <year> 1995, </year> <pages> pp. 70-76. </pages>
Reference-contexts: A recent technical development for the perception of the UGV's environment is the concept of iso-distortion surfaces, a framework for studying the relationship between the computation of 3D motion and depth from a sequence of im ages <ref> [ Cheong and Aloimonos, 1995 ] </ref> . The underlying conceptual theme is that motion errors (e.g., errors between retinal motion and perceived 3D motion) affect depth estimates systematically.
Reference: [ Davis et al., 1994 ] <editor> L.S. Davis, R. Bajcsy, R. Nelson, and M. Herman. </editor> <booktitle> RSTA on the move. In Proc. of ARPA Image Understanding Workshop, </booktitle> <address> Monterey, CA, </address> <month> November </month> <year> 1994, </year> <pages> pp. 435-456. </pages>
Reference-contexts: Details of the Maryland work are given below. 2.1.1 2D Image Stabilization The 2D image stabilization algorithm is de scribed in <ref> [ Davis et al., 1994 ] </ref> ; it uses the cam era model presented in [ Zheng and Chellappa, 1993 ] .
Reference: [ Fermuller and Aloimonos, 1995 ] <author> C. Fermuller and Y. Aloimonos. </author> <title> Qualitative egomotion. </title> <journal> International Journal of Computer Vision, </journal> <volume> 15 </volume> <pages> 7-29, </pages> <year> 1995. </year>
Reference-contexts: Figure 8 shows experiments using the approach devel oped in <ref> [ Fermuller and Aloimonos, 1995 ] </ref> with real data collected from the vehicle. <p> rough terrain in the countryside, thus undergoing continuously changing rigid motion. (a) shows one frame of the sequence with the normal flow field overlaid. (b), (d) and (f) show the positive (light color) and negative (dark color) vectors of the longitudinal patterns corresponding to the x-, y- and z-axes (see <ref> [ Fermuller and Aloimonos, 1995 ] </ref> ). (c), (e) and (g) show the corresponding fitted patterns. (i) shows, superimposed on the image, the boundaries of the patterns whose intersections provide the FOE and the AOR (the point where the rotation axis pierces the image plane). (j) Measurements are not everywhere available
Reference: [ Hosoda et al., 1995 ] <author> K. Hosoda, H. Moriyama, and M. Asada. </author> <title> Visual servoing utilizing zoom mechanism. </title> <booktitle> In Proc. of IEEE International Conference on Robotics and Automation, </booktitle> <address> Nagoya, Japan, </address> <month> May </month> <year> 1995, </year> <pages> pp. 178-183. </pages>
Reference-contexts: The influence of the velocity variance ensures that the camera does not zoom in too closely if an object's motion varies greatly, in order to safely maintain acquisition. In <ref> [ Hosoda et al., 1995 ] </ref> the authors use a robot arm and camera zoom to achieve a desired image feature configuration.
Reference: [ Hwang et al., 1993 ] <author> J. Hwang, Y. Ooi, and S. Ozawa. </author> <title> An adaptive sensing system with tracking and zooming a moving object. </title> <journal> IE-ICE Transactions on Information and Systems, </journal> <volume> E76-D:926-934, </volume> <year> 1993. </year>
Reference-contexts: The closer the camera zooms in on the target, the faster the target moves in the image, and the harder it becomes to maintain acquisition of it. To our knowledge, there exist only a few publications that deal with the control of zoom for tracking. In <ref> [ Hwang et al., 1993 ] </ref> the zoom is used to achieve a desired object image size. A fuzzy controller combines estimates of the diagonal extent of the object, the variance of the object velocity, and the confidence of the shape estimate to compute a suitable focal length.
Reference: [ Madden and von Seelen, 1995 ] <author> B.C. Madden and U.M.C. von Seelen. PennEyes. </author> <type> Technical Report, </type> <institution> GRASP Laboratory, Department of Computer and Information Science, University of Pennsylvania, </institution> <address> Philadelphia, PA, </address> <note> to appear. </note>
Reference: [ Morimoto et al., 1995 ] <author> C.H. Morimoto, D. De-Menthon, L. Davis, R. Chellappa, and R. Nel-son. </author> <title> Detection of independently moving objects in passive video. </title> <booktitle> In Proc. of IEEE Intelligent Vehicles Symposium, </booktitle> <address> Detroit, MI, </address> <year> 1995, </year> <pages> pp. 270-275. </pages>
Reference-contexts: A hardware implementa tion of the system is described in <ref> [ Morimoto et al., 1995 ] </ref> . This system was developed to use a commercially available parallel pipeline image processing board (Datacube MV200) connected to a SUN SPARCstation 20/612, and is able to process 7 frames per second, using images of size 128 fi 128. <p> This scheme can be extended to include more than two frames, since spots in frame f i2 should be 2p pixels away, and spots in frame f ij should be jp pixels away. Implementation issues regarding these filters are given in <ref> [ Morimoto et al., 1995 ] </ref> . 2.1.3 Experimental Results taken from a moving vehicle and Figure 3 shows the thresholded difference between the four-frame temporal median and the stabilized instance of that frame.
Reference: [ Nelson, 1991 ] <author> R.C. Nelson. </author> <title> Qualitative detection of motion by a moving observer. </title> <journal> International Journal of Computer Vision 7 </journal> <pages> 33-46, </pages> <year> 1991. </year>
Reference-contexts: This provides a uniformly applicable strategy by which small regions of the scene can be selected for more thorough inspection. In previous work, we produced real-time algorithms for detecting independently moving objects from a moving platform <ref> [ Nelson, 1991 ] </ref> . <p> In previous work, we have developed methods for detecting three qualitative types of motion. The first technique, which we term constraint ray filtering, provides a robust method of detecting independently moving objects from a moving platform when information is available about the platform motion <ref> [ Nelson, 1991 ] </ref> . <p> The second method, termed animate motion detection, allows rapid detection of animate objects with no information about the movement of the platform <ref> [ Nelson, 1991 ] </ref> . It is based on the observation that animate moving objects typically maneuver, that is, they or their component parts follow trajectories for which the projected velocity changes rapidly compared to the velocity change due to self-motion.
Reference: [ Nelson and Polana, 1992 ] <author> R.C. Nelson and R. Polana. </author> <title> Qualitative recognition of motion from temporal texture. CVGIP: </title> <booktitle> Image Understanding, </booktitle> <volume> 56 </volume> <pages> 78-89, </pages> <year> 1992. </year>
Reference-contexts: We are developing techniques based on temporal texture analysis, where we extract statistical spatial and temporal features from approximations to the motion field and use techniques analogous to those developed for grayscale texture analysis to classify regional activities. Some results in this area are described in <ref> [ Nelson and Polana, 1992 ] </ref> .
Reference: [ Parry et al., 1995 ] <author> H.S. Parry, A.D. Marshall, and K.C. Markham. </author> <title> Integration of segmentation information and correlation technique for tracking objects in sequences of images. </title> <booktitle> In Proc. of Videometrics IV (SPIE Proceedings vol. 2598), </booktitle> <address> Philadelphia, PA, </address> <month> October </month> <year> 1995, </year> <pages> pp. 208-219. </pages>
Reference-contexts: Changing scale poses increased demands on target identification and localization because commonly used approaches such as cross-correlation are not scale-invariant. Alternatives include feature-based tracking (e.g. [ Reid and Murray, 1993 ] ) or the use of adaptive correlation templates (e.g. <ref> [ Parry et al., 1995 ] </ref> ). We are currently investigating the latter approach. 2.4 NIST NIST is responsible for developing the vision processing platform, assisting in integrating University software onto the platform, and running the platform on vehicles at the NIST facility.
Reference: [ Polana and Nelson, 1994 ] <author> R. Polana and R.C. Nelson. </author> <title> Detecting activities. </title> <journal> Journal of Visual Communication and Image Representation, </journal> <volume> 5 </volume> <pages> 172-180, </pages> <year> 1994. </year>
Reference-contexts: We have also recently developed techniques for accomplishing this for objects that move in a complex manner, such as people or animals <ref> [ Polana and Nelson, 1994 ] </ref> . The identification step locates regions of interest via a more detailed analysis of motion. <p> The third method allows detection and tracking of objects whose motion has a periodic component, such as walking or running animals, oscillating machinery, etc. <ref> [ Polana and Nelson, 1994 ] </ref> . It is based on a Fourier transform technique. These techniques can be used to isolate motion for identification by later recognition processes.
Reference: [ Polana and Nelson, 1993 ] <author> R. Polana and R.C. Nelson. </author> <title> Detecting activities. </title> <booktitle> In Proc. of ARPA Image Understanding Workshop, </booktitle> <address> Washington, DC, </address> <month> April </month> <year> 1993, </year> <pages> pp. 569-574. </pages>
Reference-contexts: In a second approach, which we term activity recognition, we use the spatial and temporal arrangement of motion features in conjunction with simple geometric image analysis to identify complexly moving objects such as machinery and loco moting people and animals <ref> [ Polana and Nelson, 1993 ] </ref> . The remainder of this section concentrates on the motion detection processes.
Reference: [ Rao and Ballard, 1995a ] <author> R.P.N. Rao and D.H. Ballard. </author> <title> Dynamic model of visual memory predicts neural response properties in the visual cortex. </title> <type> Technical Report 95-1, </type> <institution> National Resource Laboratory for the Study of Brain and Behavior, </institution> <year> 1995. </year>
Reference-contexts: pierces the image plane). (j) Measurements are not everywhere available (strong intensity gradients are sparse), but a set of patterns can still be fitted, resulting in two bounded areas as solutions for the FOE and the AOR. 3.4 Dynamic Appearance-Based Recognition|University of Rochester We have recently developed a new method <ref> [ Rao and Ballard, 1995a ] </ref> of appearance-based recog nition; see [ Rao and Ballard, 1995b ] for a review. <p> The matrix P is the covariance matrix for prediction error (r ^ r) and is computed iteratively <ref> [ Rao and Ballard, 1995a ] </ref> . Intuitively, K can be interpreted as a form of signal-to-noise ratio. <p> The architecture embodied by the method has recently been used to model neural responses in the visual cortex (see <ref> [ Rao and Ballard, 1995a ] </ref> for further details). It possesses several properties essential for dynamic recognition such as the ability to perform pattern completions during occlusions and simultaneous top-down segmentation during recognition.
Reference: [ Rao and Ballard, 1995b ] <author> R.P.N. Rao and D.H. Ballard. </author> <title> An active vision architecture based on iconic representations. </title> <booktitle> Artificial Intelligence 78 </booktitle> <pages> 461-505, </pages> <year> 1995. </year>
Reference-contexts: intensity gradients are sparse), but a set of patterns can still be fitted, resulting in two bounded areas as solutions for the FOE and the AOR. 3.4 Dynamic Appearance-Based Recognition|University of Rochester We have recently developed a new method [ Rao and Ballard, 1995a ] of appearance-based recog nition; see <ref> [ Rao and Ballard, 1995b ] </ref> for a review. The method employs a hierarchically organized network in which input-driven bottom-up signals from one level are dynamically combined with expectation-driven top-down signals from a higher and more abstract level to achieve optimal estimation of current state.
Reference: [ Reid and Murray, 1993 ] <author> I.D. Reid and D.W. Murray. </author> <title> Tracking foveated corner clusters using affine structure. </title> <booktitle> In Proc. of International Conference on Computer Vision, </booktitle> <address> Berlin, Ger-many, </address> <month> May </month> <year> 1993, </year> <pages> pp. 76-83. </pages>
Reference-contexts: With the expertise gained from zooming for size constancy, we can approach the problem of zooming for scale change. Changing scale poses increased demands on target identification and localization because commonly used approaches such as cross-correlation are not scale-invariant. Alternatives include feature-based tracking (e.g. <ref> [ Reid and Murray, 1993 ] </ref> ) or the use of adaptive correlation templates (e.g. [ Parry et al., 1995 ] ).
Reference: [ Yao et al., 1996 ] <author> Y.S. Yao, P. Burlina, and R. Chellappa. </author> <title> Stabilization of images acquired by unmanned ground vehicles. </title> <booktitle> In these Proceedings. </booktitle>
Reference-contexts: by the three algorithms, the impact of the small differences on target acquisition and false alarm detection rates were quite significant. 3.2 3D Model-Based Image Stabilization|University of Maryland We have studied the use of combined visual cues and dynamic models for the stabilization of cal ibrated or uncalibrated image sequences <ref> [ Yao et al., 1996 ] </ref> . Parameters relevant to image warping are estimated by combining information from different tracked tokens, namely points and horizon lines. These parameters are simply the camera rotational velocity if intrinsic camera parameters are available, or the projectivity coefficients, in the uncalibrated case.
Reference: [ Zheng and Chellappa, 1993 ] <author> Q. Zheng, and R. Chellappa. </author> <title> A computational vision approach to image registration. </title> <booktitle> IEEE Transactions on Image Processing 2 </booktitle> <pages> 311-326, </pages> <year> 1993. </year>
Reference-contexts: Details of the Maryland work are given below. 2.1.1 2D Image Stabilization The 2D image stabilization algorithm is de scribed in [ Davis et al., 1994 ] ; it uses the cam era model presented in <ref> [ Zheng and Chellappa, 1993 ] </ref> .
References-found: 19

