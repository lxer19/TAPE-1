URL: http://www.cs.berkeley.edu/~culler/papers/sc95-connect.ps
Refering-URL: http://www.cs.berkeley.edu/~culler/papers/
Root-URL: 
Title: Towards Modeling the Performance of a Fast Connected Components Algorithm on Parallel Machines  
Author: Steven S. Lumetta, Arvind Krishnamurthy, and David E. Culler 
Date: April 4, 1995  
Abstract: We present and analyze a portable, high-performance algorithm for finding connected components on modern distributed memory multiprocessors. The graphs are probabilistic meshes used in cluster dynamics methods of computational physics. The algorithm is a hybrid of the classic DFS on the subgraph local to each processor and a variant of the Shiloach-Vishkin PRAM algorithm on the collection of subgraphs. On a 256-node Cray T3D, our algorithm demonstrates the fastest solution to date on graphs of practical importance. We analyze the performance characteristics of our solution across a range of graph types and machine performance parameters. The study sheds light on the performance impact of improvements in computational and/or communication performance on this challenging problem. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> T. E. Anderson, D. E. Culler, D. Patterson, </author> <title> "A Case for NOW," </title> <booktitle> IEEE Micro, </booktitle> <month> February </month> <year> 1995, </year> <pages> pp. 54-64. </pages>
Reference-contexts: These trade-offs are particularly important to quantify given that with microprocessor performance improving rapidly with time, greater computational performance can easily be obtained at the expense of communication performance by building less tightly integrated systems <ref> [1] </ref>. For our study, we use a class of graphs motivated by cluster dynamics problems in computational physics. Traditionally, numerical simulations of materials near critical temperatures utilize Monte Carlo algorithms with local methods of information propagation.
Reference: [2] <author> R. H. Arpaci, </author> <title> et al.,"Empirical Evaluation of the Cray T3D: a compiler perspective," </title> <booktitle> to appear in Proceedings of the International Symposium on Computer Architecture, </booktitle> <year> 1995. </year>
Reference-contexts: Using Split-C also gives our implementation portability. Versions of Split-C exist on the Cray T3D, the IBM SP-1 and SP-2, the Intel Paragon, the Thinking Machines Corp. CM-5, the Meiko CS-2, and networks of workstations <ref> [2, 20, 23, 29] </ref>. Although our algorithm accepts arbitrary graphs as input, obtaining optimal performance requires a reasonable partitioning of the graph across processors to enhance locality and load balancing. Partitioning techniques rely on the ability to determine properties of the graph structure. <p> A Split-C global read involves a short instruction sequence to gain addressability to the remote node and to load from remote memory, taking approximately 1 s <ref> [2] </ref>. The CS-2 is based on a 90 MHz, dual-issue Sparc microprocessor with a large cache. Communication is supported by a dedicated "ELAN" processor within the network interface, which can access remote memory via word-by-word or DMA network transactions.
Reference: [3] <author> B. Awerbuch, Y. Shiloach, </author> <title> "New Connectivity and MSF Algorithms for Ultracomputer and PRAM," </title> <booktitle> International Conference on Parallel Processing, </booktitle> <year> 1983, </year> <pages> pp. 175-179. </pages>
Reference-contexts: Parallel solutions have received a great deal of attention from both theorists and practical computer scientists, and have proven difficult. Theoretical work shows good results on the CRCW PRAM model <ref> [3, 10, 11, 24] </ref>, which assumes uniform memory access time and arbitrary bandwidth to any memory location. fl This material is based upon work supported under a National Science Foundation Presidential Faculty Fellow- ship Award, a Graduate Research Fellowship, and Infrastructure Grant number CDA-8722788, as well as Lawrence Livermore National Laboratories
Reference: [4] <author> M. Aydin, M. C. </author> <title> Yalabik, </title> <journal> Journal of Physics A 17, </journal> <volume> 2531, </volume> <year> 1984. </year>
Reference-contexts: The quality of statistics returned from these simulations depends heavily on the size of the sample, L. Although a larger sample size leads to better statistics, the number of steps needed to propagate information grows generally as O (L 2 ) or worse using the traditional methods <ref> [4, 16, 21, 27, 28] </ref>. More recent methods, such as the Swendsen-Wang algorithm, reduce correlation time for the simulations. For a two-dimensional Ising model, the correlation time using S-W grows as O (L 0:35 ) [25, 26], allowing much larger samples to be studied.
Reference: [5] <author> K. W. Chong, T. W. Lam, </author> <title> "Finding Connected Components in O(log n log log n) Time on EREW PRAM," </title> <booktitle> 4th ACM-SIAM Symposium on Discrete Algorithms, </booktitle> <year> 1993, </year> <pages> pp. 11-20. </pages>
Reference-contexts: Any opinions, findings, conclusions, or recommendations expressed in this publication are those of the authors and do not necessarily reflect the views of either organization. 1 The inherent contention in the algorithm has made even EREW solutions much more challenging <ref> [5, 14, 15, 17] </ref>. Practical application of the theoretical work to parallel machines has been largely restricted to small shared-memory machines and SIMD machines with very slow processors [11].
Reference: [6] <author> A. Choudhary, R. Thakur, </author> <title> "Connected Component Labeling on Coarse Grain Parallel Computers: An Experimental Study," </title> <journal> Journal of Parallel and Distributed Computing 20, </journal> <year> 1994, </year> <pages> pp. 78-83. </pages>
Reference-contexts: Practical application of the theoretical work to parallel machines has been largely restricted to small shared-memory machines and SIMD machines with very slow processors [11]. Many practical solutions have been developed for modern MIMD massively parallel platforms, or MPP's <ref> [6, 9, 13, 22, 18] </ref>, and vector machines [8, 22]. The practical solutions typically emphasize performance over issues of portability, scalability, and generality, but still rarely obtain good performance.
Reference: [7] <author> D. E. Culler, A. Dusseau, S. C. Goldstein, A. Krishnamurthy, S. Lumetta, T. von Eicken, K. Yelick, </author> <title> "Parallel Programming in Split-C," </title> <booktitle> Proceedings of Supercomputing '93, </booktitle> <address> Portland, Oregon, </address> <month> November </month> <year> 1993, </year> <pages> pp. 262-273. </pages>
Reference-contexts: Also, this model must be stable across a variety of platforms to make the algorithm truly portable|if the model changes dramatically on a new machine, we must optimize independently for that machine. The Split-C language <ref> [7] </ref> provides such a cost model, giving us a set of simple yet effective abstractions for programming a parallel machine and allowing us to implement the algorithm in a straightforward fashion and to optimize the global phase once for all platforms. In particular, 4 blocks for the processors.
Reference: [8] <author> H. G. Evertz, </author> <title> "Vectorized Cluster Search," Nuclear Physics B, </title> <booktitle> Proceedings Supplements, </booktitle> <year> 1992, </year> <pages> pp. 620-622. </pages>
Reference-contexts: Practical application of the theoretical work to parallel machines has been largely restricted to small shared-memory machines and SIMD machines with very slow processors [11]. Many practical solutions have been developed for modern MIMD massively parallel platforms, or MPP's [6, 9, 13, 22, 18], and vector machines <ref> [8, 22] </ref>. The practical solutions typically emphasize performance over issues of portability, scalability, and generality, but still rarely obtain good performance. In this paper, we present a portable, high-performance algorithm for finding the connected components of a general graph on distributed memory machines.
Reference: [9] <author> M. Flanigan, P. Tamayo, </author> <title> "A Parallel Cluster Labeling Method for Monte Carlo Dynamics," </title> <journal> International Journal of Modern Physics C, </journal> <volume> Vol. 3, No. 6, </volume> <year> 1992, </year> <pages> 1235-1249. </pages>
Reference-contexts: Practical application of the theoretical work to parallel machines has been largely restricted to small shared-memory machines and SIMD machines with very slow processors [11]. Many practical solutions have been developed for modern MIMD massively parallel platforms, or MPP's <ref> [6, 9, 13, 22, 18] </ref>, and vector machines [8, 22]. The practical solutions typically emphasize performance over issues of portability, scalability, and generality, but still rarely obtain good performance. <p> The rightmost column shows millions of nodes processed per second on a single node of a Cray C90 using an algorithm developed by Greiner. and Tamayo achieved 12.5 million nodes per second on much larger 2D graphs using a 256-node CM-5 <ref> [9] </ref>. Also, Hackl et. al. achieved nearly 7 Mn/s on 32 processors of an Intel IPSC/860, again on a very large 2D graph [12]. <p> <ref> [9] </ref>[12]. The global phases of these algorithms build new programming abstractions to allow for bulk communication between processors. The added complexity of design often results in worse performance, and forces the programmers to rewrite the implementation (if not the algorithm) to accept anything but a specific type of graph. Both [9] and [12], for example, work only with 2D graphs. Conversely, our hybrid implementation uses many of the sequential programming abstractions and can accept arbitrary graphs.
Reference: [10] <author> H. Gazit, </author> <title> "An Optimal Randomized Parallel Algorithm for Finding Connected Components in a Graph," </title> <journal> SIAM Journal of Computing 20(6), </journal> <month> December </month> <year> 1991. </year>
Reference-contexts: Parallel solutions have received a great deal of attention from both theorists and practical computer scientists, and have proven difficult. Theoretical work shows good results on the CRCW PRAM model <ref> [3, 10, 11, 24] </ref>, which assumes uniform memory access time and arbitrary bandwidth to any memory location. fl This material is based upon work supported under a National Science Foundation Presidential Faculty Fellow- ship Award, a Graduate Research Fellowship, and Infrastructure Grant number CDA-8722788, as well as Lawrence Livermore National Laboratories
Reference: [11] <author> J. Greiner, </author> <title> "A Comparison of Parallel Algorithms for Connected Components," </title> <booktitle> 6th ACM Symposium on Parallel Algorithms and Architectures, </booktitle> <year> 1994, </year> <pages> pp. 16-23. </pages>
Reference-contexts: Parallel solutions have received a great deal of attention from both theorists and practical computer scientists, and have proven difficult. Theoretical work shows good results on the CRCW PRAM model <ref> [3, 10, 11, 24] </ref>, which assumes uniform memory access time and arbitrary bandwidth to any memory location. fl This material is based upon work supported under a National Science Foundation Presidential Faculty Fellow- ship Award, a Graduate Research Fellowship, and Infrastructure Grant number CDA-8722788, as well as Lawrence Livermore National Laboratories <p> Practical application of the theoretical work to parallel machines has been largely restricted to small shared-memory machines and SIMD machines with very slow processors <ref> [11] </ref>. Many practical solutions have been developed for modern MIMD massively parallel platforms, or MPP's [6, 9, 13, 22, 18], and vector machines [8, 22]. The practical solutions typically emphasize performance over issues of portability, scalability, and generality, but still rarely obtain good performance. <p> For comparison, the second column shows millions of nodes processed per second on a Cray C90, as reported by Greiner. size of the graph. processing speeds for a single node of one of today's most powerful sequential supercomputers, the Cray C90, as reported by Greiner in <ref> [11] </ref>. Consider the relative performance of a T3D and a CM-5. The cycle time of the Alpha node in a T3D is about 4.5 times faster than that of the Sparc in a CM-5; on the other hand, the memory access latency is only about 3 times as fast.
Reference: [12] <author> R. Hackl, H.-G. Matuttis, J. M. Singer, T. H. Husslein, I. Morgenstern, </author> <title> "Parallelization of the 2D Swendsen-Wang Algorithm," </title> <journal> International Journal of Modern Physics C, </journal> <volume> Vol. 4, No. 6, </volume> <year> 1993, </year> <pages> pp. 59-72. </pages>
Reference-contexts: Also, Hackl et. al. achieved nearly 7 Mn/s on 32 processors of an Intel IPSC/860, again on a very large 2D graph <ref> [12] </ref>. Neither of the results can be compared directly, since they were applied to a particular problem and do not specifically state the edge presence probability for the graphs. <p> The added complexity of design often results in worse performance, and forces the programmers to rewrite the implementation (if not the algorithm) to accept anything but a specific type of graph. Both [9] and <ref> [12] </ref>, for example, work only with 2D graphs. Conversely, our hybrid implementation uses many of the sequential programming abstractions and can accept arbitrary graphs.
Reference: [13] <author> S. Hambrusch, L. TeWinkel, </author> <title> "A Study of Connected Component Labeling Algorithms on the MPP," </title> <booktitle> 3rd International Conference on Supercomputing, </booktitle> <volume> Vol. 1, </volume> <month> May </month> <year> 1988, </year> <pages> pp. 477-483. </pages>
Reference-contexts: Practical application of the theoretical work to parallel machines has been largely restricted to small shared-memory machines and SIMD machines with very slow processors [11]. Many practical solutions have been developed for modern MIMD massively parallel platforms, or MPP's <ref> [6, 9, 13, 22, 18] </ref>, and vector machines [8, 22]. The practical solutions typically emphasize performance over issues of portability, scalability, and generality, but still rarely obtain good performance.
Reference: [14] <author> D. S. Hirschberg, A. . Chandra, D. V. Sarwate, </author> <title> "Computing Connected Components on Parallel Computers," </title> <journal> Communications fo the ACM, </journal> <volume> 22(8), </volume> <year> 1979, </year> <pages> pp. 461-464. </pages>
Reference-contexts: Any opinions, findings, conclusions, or recommendations expressed in this publication are those of the authors and do not necessarily reflect the views of either organization. 1 The inherent contention in the algorithm has made even EREW solutions much more challenging <ref> [5, 14, 15, 17] </ref>. Practical application of the theoretical work to parallel machines has been largely restricted to small shared-memory machines and SIMD machines with very slow processors [11].
Reference: [15] <author> D. B. Johnson, P. Metaxas, </author> <title> "Connected Components in O(log 3 =2n) Parallel Time for the CREW PRAM," </title> <booktitle> 32nd Annual IEEE Symposium on Foundations of Computer Science, </booktitle> <year> 1991, </year> <pages> pp. 688-697. </pages>
Reference-contexts: Any opinions, findings, conclusions, or recommendations expressed in this publication are those of the authors and do not necessarily reflect the views of either organization. 1 The inherent contention in the algorithm has made even EREW solutions much more challenging <ref> [5, 14, 15, 17] </ref>. Practical application of the theoretical work to parallel machines has been largely restricted to small shared-memory machines and SIMD machines with very slow processors [11].
Reference: [16] <author> C. </author> <title> Kalle, </title> <journal> Journal of Physics A 17, </journal> <volume> L801, </volume> <year> 1984. </year>
Reference-contexts: The quality of statistics returned from these simulations depends heavily on the size of the sample, L. Although a larger sample size leads to better statistics, the number of steps needed to propagate information grows generally as O (L 2 ) or worse using the traditional methods <ref> [4, 16, 21, 27, 28] </ref>. More recent methods, such as the Swendsen-Wang algorithm, reduce correlation time for the simulations. For a two-dimensional Ising model, the correlation time using S-W grows as O (L 0:35 ) [25, 26], allowing much larger samples to be studied.
Reference: [17] <author> D. R. Karger, N. Nisan, M. Parnas, </author> <title> "Fast Connected Components Algorithm for the EREW PRAM," </title> <booktitle> 4th ACM Symposium on Parallel Algorithms and Architectures, </booktitle> <year> 1992, </year> <pages> pp. 373-381. </pages>
Reference-contexts: Any opinions, findings, conclusions, or recommendations expressed in this publication are those of the authors and do not necessarily reflect the views of either organization. 1 The inherent contention in the algorithm has made even EREW solutions much more challenging <ref> [5, 14, 15, 17] </ref>. Practical application of the theoretical work to parallel machines has been largely restricted to small shared-memory machines and SIMD machines with very slow processors [11].
Reference: [18] <author> J. Kertesz, D. Stauffer, </author> <title> "Swendsen-Wang Dynamics of Large 2D Critical Ising Models," </title> <journal> International Journal of Modern Physics C, </journal> <volume> Vol. 3, No. 6, </volume> <year> 1992, </year> <pages> pp. 1275-1279. </pages>
Reference-contexts: Practical application of the theoretical work to parallel machines has been largely restricted to small shared-memory machines and SIMD machines with very slow processors [11]. Many practical solutions have been developed for modern MIMD massively parallel platforms, or MPP's <ref> [6, 9, 13, 22, 18] </ref>, and vector machines [8, 22]. The practical solutions typically emphasize performance over issues of portability, scalability, and generality, but still rarely obtain good performance.
Reference: [19] <author> A. Krishnamurthy, S. Lumetta, D. Culler, K. Yelick, </author> <title> "Connected Components on Distributed Memory Machines," </title> <booktitle> to be published in the Proceedings of the 3rd Annual DIMACS Challenge, </booktitle> <year> 1995. </year>
Reference-contexts: Compared with purely global approaches, the algorithm performs an order of magnitude better. The optimized algorithm follows. For more detail on the data structures or on the process of optimization, see <ref> [19] </ref>. 1. Local Phase. Perform a local depth-first search (DFS) on each processor's portion of the graph, collapsing each local connected component into a representative node. The local phase results in a much smaller graph for the global phase.
Reference: [20] <author> S. Luna, </author> <title> "Implementing an Efficient Portable Global Memory Layer on Distributed Memory Multiprocessors," </title> <editor> U. </editor> <address> C. </address> <institution> Berkeley Technical Report #CSD-94-810, </institution> <month> May </month> <year> 1994. </year>
Reference-contexts: Using Split-C also gives our implementation portability. Versions of Split-C exist on the Cray T3D, the IBM SP-1 and SP-2, the Intel Paragon, the Thinking Machines Corp. CM-5, the Meiko CS-2, and networks of workstations <ref> [2, 20, 23, 29] </ref>. Although our algorithm accepts arbitrary graphs as input, obtaining optimal performance requires a reasonable partitioning of the graph across processors to enhance locality and load balancing. Partitioning techniques rely on the ability to determine properties of the graph structure.
Reference: [21] <author> G. F. Mazenko, M. J. Nolan, O. T. </author> <title> Valls, </title> <journal> Physical Review Letters 41, </journal> <volume> 500, </volume> <year> 1978. </year>
Reference-contexts: The quality of statistics returned from these simulations depends heavily on the size of the sample, L. Although a larger sample size leads to better statistics, the number of steps needed to propagate information grows generally as O (L 2 ) or worse using the traditional methods <ref> [4, 16, 21, 27, 28] </ref>. More recent methods, such as the Swendsen-Wang algorithm, reduce correlation time for the simulations. For a two-dimensional Ising model, the correlation time using S-W grows as O (L 0:35 ) [25, 26], allowing much larger samples to be studied.
Reference: [22] <author> H. Mino, </author> <title> "A Vectorized Algorithm for Cluster Formation in the Swendsen-Wang Dynamics," </title> <journal> Computer Physics Communications 66, </journal> <year> 1991, </year> <pages> pp. 25-30. 23 </pages>
Reference-contexts: Practical application of the theoretical work to parallel machines has been largely restricted to small shared-memory machines and SIMD machines with very slow processors [11]. Many practical solutions have been developed for modern MIMD massively parallel platforms, or MPP's <ref> [6, 9, 13, 22, 18] </ref>, and vector machines [8, 22]. The practical solutions typically emphasize performance over issues of portability, scalability, and generality, but still rarely obtain good performance. <p> Practical application of the theoretical work to parallel machines has been largely restricted to small shared-memory machines and SIMD machines with very slow processors [11]. Many practical solutions have been developed for modern MIMD massively parallel platforms, or MPP's [6, 9, 13, 22, 18], and vector machines <ref> [8, 22] </ref>. The practical solutions typically emphasize performance over issues of portability, scalability, and generality, but still rarely obtain good performance. In this paper, we present a portable, high-performance algorithm for finding the connected components of a general graph on distributed memory machines.
Reference: [23] <author> K. E. Schauser, C. J. Scheiman, </author> <title> "Experience with Active Messages on the Meiko CS-2," </title> <booktitle> to appear in Proceedings of the International Parallel Processing Symposium, </booktitle> <year> 1995. </year>
Reference-contexts: Using Split-C also gives our implementation portability. Versions of Split-C exist on the Cray T3D, the IBM SP-1 and SP-2, the Intel Paragon, the Thinking Machines Corp. CM-5, the Meiko CS-2, and networks of workstations <ref> [2, 20, 23, 29] </ref>. Although our algorithm accepts arbitrary graphs as input, obtaining optimal performance requires a reasonable partitioning of the graph across processors to enhance locality and load balancing. Partitioning techniques rely on the ability to determine properties of the graph structure. <p> The Split-C global read issues a command to the communications co-processor via an exchange instruction, which causes a waiting ELAN thread to either access remote memory or to begin a DMA transfer, depending on length. A remote read requires roughly 20 s <ref> [23, 29] </ref>. The CM-5 is based on the Cypress Sparc microprocessor, clocked at 33 MHz, with a 64 kB unified instruction and data cache. A Split-C global read involves issuing a CMAML active message to access the remote location and reply with the value, taking approximately 12 s.
Reference: [24] <author> Y. Shiloach, U. Vishkin, </author> <title> "An O(log n) Parallel Connectivity Algorithm," </title> <journal> Journal of Algorithms, </journal> <volume> No. 3, </volume> <year> 1982, </year> <pages> pp. 57-67. </pages>
Reference-contexts: Parallel solutions have received a great deal of attention from both theorists and practical computer scientists, and have proven difficult. Theoretical work shows good results on the CRCW PRAM model <ref> [3, 10, 11, 24] </ref>, which assumes uniform memory access time and arbitrary bandwidth to any memory location. fl This material is based upon work supported under a National Science Foundation Presidential Faculty Fellow- ship Award, a Graduate Research Fellowship, and Infrastructure Grant number CDA-8722788, as well as Lawrence Livermore National Laboratories <p> As a global algorithm grows in complexity, understanding and avoiding load imbalance and network contention becomes much more difficult. We followed a strategy of hybridization to create our algorithm, merging a simple and fast local algorithm (depth-first search, or DFS) with a powerful and efficient global algorithm (Shiloach- Vishkin <ref> [24] </ref>, or S-V). Neither the local nor the global algorithm is sufficient alone.
Reference: [25] <author> R. H. Swendsen, J.-S. Wang, </author> <title> "Nonuniversal Critical Dynamics in Monte Carlo Simulations," </title> <journal> Physical Review Letters, </journal> <volume> Vol. 58, No. 2, </volume> <month> January </month> <year> 1987, </year> <pages> pp. 86-88. </pages>
Reference-contexts: More recent methods, such as the Swendsen-Wang algorithm, reduce correlation time for the simulations. For a two-dimensional Ising model, the correlation time using S-W grows as O (L 0:35 ) <ref> [25, 26] </ref>, allowing much larger samples to be studied. At the heart of S-W is a connected components algorithm. The algorithm generates a random graph, finds the connected components of the graph, and applies a simple transformation to each component, then starts over [26].
Reference: [26] <author> J.-S. Wang, R. H. Swendsen, </author> <title> "Cluster Monte Carlo Algorithms," </title> <journal> Physica A, </journal> <volume> No. 167, </volume> <year> 1990, </year> <pages> pp. 565-579. </pages>
Reference-contexts: More recent methods, such as the Swendsen-Wang algorithm, reduce correlation time for the simulations. For a two-dimensional Ising model, the correlation time using S-W grows as O (L 0:35 ) <ref> [25, 26] </ref>, allowing much larger samples to be studied. At the heart of S-W is a connected components algorithm. The algorithm generates a random graph, finds the connected components of the graph, and applies a simple transformation to each component, then starts over [26]. <p> At the heart of S-W is a connected components algorithm. The algorithm generates a random graph, finds the connected components of the graph, and applies a simple transformation to each component, then starts over <ref> [26] </ref>. In the years since the introduction of the S-W algorithm, cluster dynamics, and hence the use of connected components, has also found widespread use in other areas including computational field theory, experimental high energy and particle physics, and statistical mechanics.
Reference: [27] <author> J. K. </author> <title> Williams, </title> <journal> Journal of Physics A 18, </journal> <volume> 49, </volume> <year> 1985. </year>
Reference-contexts: The quality of statistics returned from these simulations depends heavily on the size of the sample, L. Although a larger sample size leads to better statistics, the number of steps needed to propagate information grows generally as O (L 2 ) or worse using the traditional methods <ref> [4, 16, 21, 27, 28] </ref>. More recent methods, such as the Swendsen-Wang algorithm, reduce correlation time for the simulations. For a two-dimensional Ising model, the correlation time using S-W grows as O (L 0:35 ) [25, 26], allowing much larger samples to be studied.
Reference: [28] <author> H. Yahata, M. </author> <title> Suzuki, </title> <journal> Journal of the Physics Society of Japan 27, </journal> <volume> 1421, </volume> <year> 1969. </year>
Reference-contexts: The quality of statistics returned from these simulations depends heavily on the size of the sample, L. Although a larger sample size leads to better statistics, the number of steps needed to propagate information grows generally as O (L 2 ) or worse using the traditional methods <ref> [4, 16, 21, 27, 28] </ref>. More recent methods, such as the Swendsen-Wang algorithm, reduce correlation time for the simulations. For a two-dimensional Ising model, the correlation time using S-W grows as O (L 0:35 ) [25, 26], allowing much larger samples to be studied.
Reference: [29] <author> C. Yoshikawa, </author> <title> "Split-C on the Meiko CS-2," </title> <note> http://www.CS.Berkeley.EDU/~chad/meiko.ps, 1995. </note>
Reference-contexts: Using Split-C also gives our implementation portability. Versions of Split-C exist on the Cray T3D, the IBM SP-1 and SP-2, the Intel Paragon, the Thinking Machines Corp. CM-5, the Meiko CS-2, and networks of workstations <ref> [2, 20, 23, 29] </ref>. Although our algorithm accepts arbitrary graphs as input, obtaining optimal performance requires a reasonable partitioning of the graph across processors to enhance locality and load balancing. Partitioning techniques rely on the ability to determine properties of the graph structure. <p> The Split-C global read issues a command to the communications co-processor via an exchange instruction, which causes a waiting ELAN thread to either access remote memory or to begin a DMA transfer, depending on length. A remote read requires roughly 20 s <ref> [23, 29] </ref>. The CM-5 is based on the Cypress Sparc microprocessor, clocked at 33 MHz, with a 64 kB unified instruction and data cache. A Split-C global read involves issuing a CMAML active message to access the remote location and reply with the value, taking approximately 12 s.
References-found: 29

