URL: ftp://ftp.cs.dartmouth.edu/TR/TR98-337.ps.Z
Refering-URL: http://www.cs.dartmouth.edu/reports/abstracts/TR98-337/
Root-URL: http://www.cs.dartmouth.edu
Title: Applications of Parallel I/O  
Author: Ron Oldteld and David Kotz 
Date: August 19, 1998  
Address: Hanover, NH 03755-3510  
Affiliation: Department of Computer Science Dartmouth College  
Web: URL ftp://ftp.cs.dartmouth.edu/TR/TR98-337.ps.Z  
Note: Available at  
Abstract: Technical Report PCS-TR98-337 Supplement to PCS-TR96-297 Abstract Scientitc applications are increasingly being implemented on massively parallel supercomputers. Many of these applications have intense I/O demands, as well as massive computational requirements. This paper is essentially an annotated bibliography of papers and other sources of information about scientitc applications using parallel I/O. It will be updated periodically.
Abstract-found: 1
Intro-found: 1
Reference: [AVV98] <author> Lars Arge, Darren Erik Vengro, and Jerey Scott Vitter. </author> <title> External-memory algorithms for processing line segments in geographic information systems. </title> <journal> Algorithmica, </journal> <year> 1998. </year>
Reference-contexts: They are in the process of investigating parallel I/O to further increase their eciency. 3 Characterizations of parallel applications These papers are detailed characterizations of the I/O access pattern of one or more parallel applications. 2 ftp://ftp.ac.uma.es/pub/ots/pDNAml/ 4 * <ref> [AVV98] </ref> This paper discusses algorithms to reduce the number of I/O re-quests in out-of-core geographical information systems (GIS) applications. * [CHKM96] The authors presents a study of eight scientitc applications on three types of parallel architectures. <p> We also suspect that some scientists are just starting to become aware of the importance of ecient I/O. Unless technology trends change, I/O will become a bottleneck for many more scientitc applications. We were also surprised by the number of papers about out-of-core applications <ref> [AVV98, PSS96, SW97, BCD97, NFK98] </ref>. An interesting point made by 9 [SW97] is that the ratio of DRAM to disk pricing suggests the use of out-of-core techniques to overcome memory capacity limitations.
Reference: [BCD97] <author> P. Brezany, A. Choudhary, and M. Dang. </author> <title> Parallelization of irregular out-of-core applications for distributed-memory systems. </title> <booktitle> High-Performance Computing and Networking, </booktitle> <address> 1225:811820, </address> <year> 1997. </year>
Reference-contexts: Their design is much like 7 that of the Titan system [CMA + 97] . 4 Other papers on applications using parallel I/O There are a few other papers that do not discuss specitc applications, but still discuss issues relating to parallel I/O for scientitc applications. * <ref> [BCD97] </ref> The authors present techniques for implementing large scale irregular out-of-core applications. The techniques they describe can either be used by a parallel compiler (e.g., HPF and its extensions) or the programmer using message passing. <p> We also suspect that some scientists are just starting to become aware of the importance of ecient I/O. Unless technology trends change, I/O will become a bottleneck for many more scientitc applications. We were also surprised by the number of papers about out-of-core applications <ref> [AVV98, PSS96, SW97, BCD97, NFK98] </ref>. An interesting point made by 9 [SW97] is that the ratio of DRAM to disk pricing suggests the use of out-of-core techniques to overcome memory capacity limitations.
Reference: [CDZ + 97] <author> C. Ceron, J. Dopazo, E. L. Zapata, J.M. Carazo, and O. Trelles. </author> <title> Parallel implementation of DNAml program on message-passing architectures. </title> <booktitle> Parallel Computing, </booktitle> <address> 24(56):701716, </address> <month> June </month> <year> 1997. </year>
Reference-contexts: We do not include papers about scientitc kernels (LU factorization, matrix multiplication, sorting, FFT, and so forth). * <ref> [CDZ + 97] </ref> They discuss the parallelization on message-passing computers of the DNAml algorithm, a tool used to construct phylogenetic trees from DNA sequences.
Reference: [CHKM96] <author> Robert Cypher, Alex Ho, Smaragda Konstantinidou, and Paul Messina. </author> <title> A quantitative study of parallel scientitc applications with explicit communication. </title> <journal> Journal of Supercomputing, </journal> <volume> 10(1):5 24, </volume> <month> March </month> <year> 1996. </year>
Reference-contexts: to further increase their eciency. 3 Characterizations of parallel applications These papers are detailed characterizations of the I/O access pattern of one or more parallel applications. 2 ftp://ftp.ac.uma.es/pub/ots/pDNAml/ 4 * [AVV98] This paper discusses algorithms to reduce the number of I/O re-quests in out-of-core geographical information systems (GIS) applications. * <ref> [CHKM96] </ref> The authors presents a study of eight scientitc applications on three types of parallel architectures.
Reference: [CMA + 97] <author> Chialin Chang, Bongki Moon, Anurag Acharya, Carter Shock, Alan Sussman, and Joel Saltz. </author> <title> Titan: a high-performance remote-sensing database. </title> <booktitle> In Proceedings of the Thirteenth International Conference on Data Engineering, </booktitle> <pages> pages 375384, </pages> <address> Birmingham, U.K., </address> <month> April </month> <year> 1997. </year>
Reference-contexts: They use I/O buering (prefetching) to fetch tasks that need to be processed. The parallel code was written in C using PVM for message passing and is available via anonymous FTP. 2 * <ref> [CMA + 97] </ref> This paper is about a parallel database Titan, designed for handling remote-sensing data. Remotely-sensed data is acquired from satellite-based sensors and is commonly used for geographical, meteorological and environmental studies. The Titan system consists of a single front-end host and a multiprocessor back-end. <p> They use wavelet compression to reduce the size of each of the electronic slides and they use a parallel data server much like the parallel database server used for satellite images <ref> [CMA + 97] </ref> to service I/O requests. * [KBCH95] This paper describes the architecture and high-level design of the data management system for the Earth Observing System Data and Information System (EOSDIS). They do not discuss implementation details, but they do discuss the tremendous I/O requirements of the project. <p> Their design is much like 7 that of the Titan system <ref> [CMA + 97] </ref> . 4 Other papers on applications using parallel I/O There are a few other papers that do not discuss specitc applications, but still discuss issues relating to parallel I/O for scientitc applications. * [BCD97] The authors present techniques for implementing large scale irregular out-of-core applications.
Reference: [DIS96] <author> James Demmel, Melody Y. Ivory, and Sharon L. Smith. </author> <title> Model--ing and identifying bottlenecks in EOSDIS. </title> <booktitle> In Proceedings of the Sixth Symposium on the Frontiers of Massively Parallel Computation, </booktitle> <pages> pages 300308. </pages> <publisher> IEEE Computer Society Press, </publisher> <month> October </month> <year> 1996. </year>
Reference-contexts: They have one paragraph on I/O performance and a nice table showing total I/O as well as I/O per Megaop for each of the applications. * <ref> [DIS96] </ref> This paper uses timing models to analyze the performance of the proposed tertiary storage system of the Earth Observing System Distributed Information System (EOSDIS). They examine tertiary storage and network performance of typical user scenarios from climate-modeling applications to identify potential bottlenecks in the system.
Reference: [DLY + 98] <author> G. Davis, L. Lau, R. Young, F. Duncalfe, and L. Brebber. </author> <title> Parallel run-length encoding (RLE) compressionreducing I/O in dynamic environmental simulations. </title> <journal> The International Journal of High Performance Computing Applications, </journal> <volume> 12(4), </volume> <month> Winter </month> <year> 1998. </year> <note> To appear in a Special Issue on I/O in Parallel Applications. </note>
Reference-contexts: Finally, the back-end nodes overlap computation, I/O and communication by issuing multiple asynchronous requests for data blocks from both the network and the disk. As requests are pending, the back-end node processes requests that have already arrived. * <ref> [DLY + 98] </ref> This paper describes a climate-modeling application that in a single day can generate approximately 60 Tbytes of raw data. The authors argue that only reasonable way to keep data-sets of this size manageable is to use data compression.
Reference: [DR95] <author> E. F. D'Azevedo and C. H. Romine. EDONIO: </author> <title> Extended distributed object network I/O library. </title> <type> Technical Report ORNL/TM-12934, </type> <institution> Oak Ridge National Laboratory, </institution> <year> 1995. </year>
Reference-contexts: The three strategies they use for I/O are to let a single processor 5 collect data and perform sequential I/O, use Intel's parallel tle system (pfs) to collect data striped across an array of disks, and use their own Extended Distributed Object Network I/O library (EDONIO) <ref> [DR95] </ref>. EDONIO provides a fast direct access random I/O operation to a global shared tle by providing a large multi-gigabyte disk cache using the aggregate distributed memory.
Reference: [FMH + 97] <author> Renato Ferreira, Bongki Moon, Jim Humphries, Alan Sussman, Joel Saltz, Robert Miller, and Angelo Demarzo. </author> <title> The virtual microscope. </title> <booktitle> In American Medical Informatics Association, 1997 Annual Fall Symposium, </booktitle> <pages> pages 449453, </pages> <address> Nashville, TN, </address> <month> October </month> <year> 1997. </year>
Reference-contexts: They developed a run-length-encoding compression algorithm that uses the gather/scatter hardware available on the Cray parallel vector machines. The compression algorithm eciently exploits multiple processors and ensures that the basic operations within the inner loops of the algorithm are vectorizable. * <ref> [FMH + 97] </ref> This paper describes a client/server application that emulates a high-power light microscope.
Reference: [KBCH95] <author> Ben Kobler, John Berbert, Parris Caulk, and P. C. Hariharan. </author> <title> Architecture and design of storage and data management for the NASA Earth Observing System Data and Information System (EOSDIS). </title> <booktitle> In Proceedings of the Fourteenth IEEE Symposium on Mass Storage Systems, </booktitle> <pages> pages 6576. </pages> <publisher> IEEE Computer Society Press, </publisher> <month> September </month> <year> 1995. </year>
Reference-contexts: They use wavelet compression to reduce the size of each of the electronic slides and they use a parallel data server much like the parallel database server used for satellite images [CMA + 97] to service I/O requests. * <ref> [KBCH95] </ref> This paper describes the architecture and high-level design of the data management system for the Earth Observing System Data and Information System (EOSDIS). They do not discuss implementation details, but they do discuss the tremendous I/O requirements of the project.
Reference: [KKCB97] <author> Meenakshi A. Kandaswamy, Mahmut T. Kandemir, Alok N. Choud-hary, and David E. Bernholdt. </author> <title> Optimization and evaluation of Hartree-Fock application's I/O with PASSION. </title> <booktitle> In Proceedings of SC97: High Performance Networking and Computing, </booktitle> <address> San Jose, CA, November 1997. </address> <publisher> IEEE Computer Society Press. </publisher>
Reference-contexts: The timing models include the I/O time from tertiary storage to disk cache and the time to send the data across the network. The modeling also accounts for device contention (network or tertiary storage). * <ref> [KKCB97, KKCB98] </ref> They describe the I/O performance of a parallel computational chemistry package using the Hartree-Fock (HF) method and the Passion I/O library [TCB + 96]. Before using any I/O optimizations, the I/O phase of the HF method accounted for up to 62% of their total execution time.
Reference: [KKCB98] <author> Meenakshi Kandaswamy, Mahmut Kandemir, Alok Choudhary, and David Bernholdt. </author> <title> An experimental study to analyze and optimize Hartree-Fock application's I/O with PASSION. </title> <journal> The International Journal of High Performance Computing Applications, </journal> <volume> 12(4), </volume> <month> Winter </month> <year> 1998. </year> <note> To appear in a Special Issue on I/O in Parallel Applications. 11 </note>
Reference-contexts: The timing models include the I/O time from tertiary storage to disk cache and the time to send the data across the network. The modeling also accounts for device contention (network or tertiary storage). * <ref> [KKCB97, KKCB98] </ref> They describe the I/O performance of a parallel computational chemistry package using the Hartree-Fock (HF) method and the Passion I/O library [TCB + 96]. Before using any I/O optimizations, the I/O phase of the HF method accounted for up to 62% of their total execution time.
Reference: [Kot96] <author> David Kotz. </author> <title> Applications of parallel I/O. </title> <type> Technical Report PCS--TR96-297, </type> <institution> Dept. of Computer Science, Dartmouth College, </institution> <month> October </month> <year> 1996. </year> <note> see suplement in PCS-TR98-XXX. </note>
Reference-contexts: This paper is meant to be used as a supplement to the previously written paper of the same name <ref> [Kot96] </ref>. The earlier version contains many applications not listed in this paper as well as a section on workload characterizations. For a complete parallel I/O bibliography, see [Kot97].
Reference: [Kot97] <author> David Kotz. </author> <title> BibTeX bibliography tle: Parallel I/O. </title> <note> Available on the WWW at http://www.cs.dartmouth.edu/pario/bib/, February 1997. Ninth Edition. </note>
Reference-contexts: This paper is meant to be used as a supplement to the previously written paper of the same name [Kot96]. The earlier version contains many applications not listed in this paper as well as a section on workload characterizations. For a complete parallel I/O bibliography, see <ref> [Kot97] </ref>. We intend to update this technical report periodically; check its web page for updated versions. 1 At that page you can also tnd a link to an on-line copy of this bibliography, with links to many of the cited papers.
Reference: [LEG + 97] <author> P.M. Lyster, K. Ekers, J. Guo, M. Harber, D. Lamich, J.W. Lar-son, R. Lucchesi, R. Rood, S. Schubert, W. Sawyer, M. Sienkiewicz, A. da Silva, J. Stobie, L.L. Takacs, R. Todling, and J. </author> <title> Zero. Parallel computing at the NASA data assimilation oce (DAO). </title> <booktitle> In Proceedings of SC97: High Performance Networking and Computing, </booktitle> <address> San Jose, CA, November 1997. </address> <publisher> IEEE Computer Society Press. </publisher>
Reference-contexts: They briey discuss the I/O performance during the computations. * <ref> [LEG + 97] </ref> This paper is about a NASA project GEOS-DAS (Goddard Earth Observing System-Data Assimilation System). The goal of the project is to produce accurate gridded datasets of atmospheric telds.
Reference: [LPJ98] <author> P. Lockey, R. Proctor, and I. D. James. </author> <title> Characterization of I/O requirements in a massively parallel shelf sea model. </title> <journal> The International Journal of High Performance Computing Applications, </journal> <volume> 12(3):320332, </volume> <month> Fall </month> <year> 1998. </year>
Reference-contexts: They also ranked the optimizations based on the performance impact of the I/O phase of the HF method. All experiments were performed on an Intel Paragon. * <ref> [LPJ98] </ref> This paper describes the I/O requirements of a parallel application that models shelf sea regions. The authors developed high-level routines to hide the details of the parallel I/O from the application code.
Reference: [LSH98] <author> J. Lepper, U. Schnell, and K.R.G. Hein. </author> <title> Parallelization of a simulation code for reactive ows on the Intel Paragon. Computers and Mathematics with Applications, </title> <address> 35(7):101109, </address> <month> April </month> <year> 1998. </year>
Reference-contexts: The goal of EOSDIS is to maintain a large archive (petabytes) of scientitc data that will quickly and easily be available to a wide variety of users ranging from K-12 schools, to graduate schools, scientists, policy makers and public ocials. 3 * <ref> [LSH98] </ref> This paper describes the implementation of a 3D simulation code for turbulent ow and combustion processes in full-scale utility boilers on an Intel XP/S computer.
Reference: [MMD98] <author> David Mackay, G. Mahinthakumar, and Ed D'Azevedo. </author> <title> A study of I/O in a parallel tnite element groundwater transport code. </title> <journal> The International Journal of High Performance Computing Applications, </journal> <volume> 12(3):307319, </volume> <month> Fall </month> <year> 1998. </year>
Reference-contexts: The authors developed high-level routines to hide the details of the parallel I/O from the application code. They present analytical models of the I/O costs and show results on a Cray T3D. * <ref> [MMD98] </ref> They use a parallel tnite-element groundwater-transport code to analyze and compare three dierent strategies for parallel I/O. Each node in the application performs many writes of only a few kilobytes in length. <p> The techniques used by developers to relieve the intense I/O demands of scientitc applications varied from improving the I/O interface to implementing improved out-of-core techniques. As shown in <ref> [MMD98, OWO98, SR98b, NFK98, TLG98] </ref>, scientitc applications clearly benett from using an API that enables advanced parallel I/O techniques such as collective I/O, prefetching and data sieving; however, most application developers still prefer to use the standard inecient UNIX API.
Reference: [NFK98] <author> Jarek Nieplocha, Ian Foster, and Rick Kendall. </author> <title> ChemIO: High-performance parallel I/O for computational chemistry applications. </title> <journal> The International Journal of High Performance Computing Applications, </journal> <volume> 12(3):345363, </volume> <month> Fall </month> <year> 1998. </year>
Reference-contexts: The objectives of the proposed techniques are to minimize I/O accesses in all steps while maintaining load balance and minimal communication. They demonstrate the eective-ness of their techniques by showing results from a Computational Fluid Dynamics (CFD) code. * <ref> [NFK98] </ref> They describe an I/O project ChemIO, which detnes an interface designed specitcally for parallel out-of-core applications in computational chemistry. The ChemIO API supports three models: disk resident arrays, exclusive access tles and shared tles. Disk resident arrays support the transfer of data between global memory and secondary storage. <p> The techniques used by developers to relieve the intense I/O demands of scientitc applications varied from improving the I/O interface to implementing improved out-of-core techniques. As shown in <ref> [MMD98, OWO98, SR98b, NFK98, TLG98] </ref>, scientitc applications clearly benett from using an API that enables advanced parallel I/O techniques such as collective I/O, prefetching and data sieving; however, most application developers still prefer to use the standard inecient UNIX API. <p> We also suspect that some scientists are just starting to become aware of the importance of ecient I/O. Unless technology trends change, I/O will become a bottleneck for many more scientitc applications. We were also surprised by the number of papers about out-of-core applications <ref> [AVV98, PSS96, SW97, BCD97, NFK98] </ref>. An interesting point made by 9 [SW97] is that the ratio of DRAM to disk pricing suggests the use of out-of-core techniques to overcome memory capacity limitations.
Reference: [OOVW96] <author> Curtis Ober, Ron Oldteld, John VanDyke, and David Womble. </author> <title> Seismic imaging on massively parallel computers. </title> <type> Technical Report SAND96-1112, </type> <institution> Sandia National Laboratories, </institution> <month> April </month> <year> 1996. </year>
Reference-contexts: As expected, EDONIO showed signitcant improvement over the trst two techniques. * [OWO98] They describe the I/O performance for a seismic-imaging application called Salvo <ref> [OOVW96] </ref>. Salvo uses an I/O partition consisting of a portion of the compute nodes to perform all of the I/O. The I/O partition is used to perform asynchronous I/O requests, collective I/O, and data distribution.
Reference: [OWO98] <author> Ron A. Oldteld, David E. Womble, and Curtis C. Ober. </author> <title> Ecient parallel I/O in seismic imaging. </title> <journal> The International Journal of High Performance Computing Applications, </journal> <volume> 12(3):333344, </volume> <month> Fall </month> <year> 1998. </year> <month> 12 </month>
Reference-contexts: EDONIO provides a fast direct access random I/O operation to a global shared tle by providing a large multi-gigabyte disk cache using the aggregate distributed memory. As expected, EDONIO showed signitcant improvement over the trst two techniques. * <ref> [OWO98] </ref> They describe the I/O performance for a seismic-imaging application called Salvo [OOVW96]. Salvo uses an I/O partition consisting of a portion of the compute nodes to perform all of the I/O. The I/O partition is used to perform asynchronous I/O requests, collective I/O, and data distribution. <p> The techniques used by developers to relieve the intense I/O demands of scientitc applications varied from improving the I/O interface to implementing improved out-of-core techniques. As shown in <ref> [MMD98, OWO98, SR98b, NFK98, TLG98] </ref>, scientitc applications clearly benett from using an API that enables advanced parallel I/O techniques such as collective I/O, prefetching and data sieving; however, most application developers still prefer to use the standard inecient UNIX API.
Reference: [PSS96] <author> Yoonho Park, Ridgway Scott, and Stuart Sechrest. </author> <title> Virtual memory versus tle interfaces for large, </title> <booktitle> memory-intensive scientitc applications. In Proceedings of Supercomputing '96. </booktitle> <publisher> ACM Press and IEEE Computer Society Press, </publisher> <month> November </month> <year> 1996. </year> <note> Also available as UH Department of Computer Science Research Report UH-CH-96-7. </note>
Reference-contexts: Exclusive access tles allow each node in the computation to write to individually owned scratch tles. These tles are primarily for out-of-core computations. A shared tle allows multiple nodes to share access to a tle. The application must handle mutual exclusion. * <ref> [PSS96] </ref> They advocate the use of traditional demand-paged virtual memory systems in supporting out-of-core applications. They are implementing an operating system for the NEC Cenju-3/DE, a shared-nothing MIMD multiprocessor with a multistage interconnection network and disks on every node. <p> We also suspect that some scientists are just starting to become aware of the importance of ecient I/O. Unless technology trends change, I/O will become a bottleneck for many more scientitc applications. We were also surprised by the number of papers about out-of-core applications <ref> [AVV98, PSS96, SW97, BCD97, NFK98] </ref>. An interesting point made by 9 [SW97] is that the ratio of DRAM to disk pricing suggests the use of out-of-core techniques to overcome memory capacity limitations.
Reference: [RAN + 93] <author> D. A. Reed, R. A. Aydt, R. J. Noe, P. C. Roth, K. A. Shields, B. W. Schwartz, and L. F. Tavera. </author> <title> Scalable performance analysis: The Pablo performance analysis environment. </title> <editor> In A. Skjellum, editor, </editor> <booktitle> Proceedings of the Scalable Parallel Libraries Conference, </booktitle> <pages> pages 104113, </pages> <address> Silver Spring, </address> <year> 1993. </year>
Reference-contexts: Their goals were to collect detailed performance data on application characteristics and access patterns and to use that information to design and evaluate parallel tle system policies and parallel tle system APIs. The related work section gives a nice overview of recent I/O characterization studies. They used the Pablo <ref> [RAN + 93] </ref> performance analysis environment to analyze the performance of their tve applications.
Reference: [SCM + 98] <author> Carter T. Shock, Chialin Chang, Bongki Moon, Anurag Acharya, Larry Davis, Joel Saltz, and Alan Sussman. </author> <title> The design and evaluation of a high-performance earth science database. </title> <booktitle> Parallel Computing, </booktitle> <address> 24(1):6589, </address> <month> January </month> <year> 1998. </year>
Reference-contexts: They argue that we need a tle system in which the user can give hints to the tle system expressing expected access patterns or to have a tle system that automatically classites access patterns. The tle system can then chose policies to deal with the access patterns. * <ref> [SCM + 98] </ref> They describe the design of a database application to facilitate ecient access to and preprocessing of large volumes of satellite data. First, experiments were performed on a prototype parallel implementation that used a 16 node IBM SP2 with 12 GB of disk storage on each node.
Reference: [SR97] <author> E. Smirni and D.A. Reed. </author> <title> Workload characterization of input/output intensive parallel applications. </title> <booktitle> In Proceedings of the Conference on Modelling Techniques and Tools for Computer Performance Evaluation, volume 1245 of Lecture Notes in Computer Science, </booktitle> <pages> pages 169180. </pages> <publisher> Springer-Verlag, </publisher> <month> June </month> <year> 1997. </year>
Reference-contexts: The authors performed experiments on a computational chemistry application called MESSKIT. They concluded, physical input/output patterns induced by application requests are strongly aected by data striping mechanisms, tle system policies, and disk hardware attributes. * <ref> [SR97, SR98b] </ref> They compared the I/O performance of tve scientitc applications from the Scalable I/O Initiative (SIO) suite of applications running on a 512-node Intel Paragon XP/S.
Reference: [SR98a] <author> Huseyin Simitci and Daniel Reed. </author> <title> A comparison of logical and physical parallel I/O patterns. </title> <journal> The International Journal of High Performance Computing Applications, </journal> <volume> 12(3):364380, </volume> <month> Fall </month> <year> 1998. </year>
Reference-contexts: They derived an analytical model for estimating the I/O, computation and communication times for each of the operations and use the model to estimate the optimal ratio of compute nodes to I/O nodes for their application. Performance results are presented for the Intel Paragon. * <ref> [SR98a] </ref> This paper compares logical I/O performed by the application with the corresponding physical I/O that takes place at the disk.
Reference: [SR98b] <author> E. Smirni and D.A. Reed. </author> <title> Lessons from characterizing the input/output behavior of parallel scientitc applications. Performance Evaluation: </title> <note> An International Journal, 33(1):2744, </note> <month> June </month> <year> 1998. </year>
Reference-contexts: The authors performed experiments on a computational chemistry application called MESSKIT. They concluded, physical input/output patterns induced by application requests are strongly aected by data striping mechanisms, tle system policies, and disk hardware attributes. * <ref> [SR97, SR98b] </ref> They compared the I/O performance of tve scientitc applications from the Scalable I/O Initiative (SIO) suite of applications running on a 512-node Intel Paragon XP/S. <p> The techniques used by developers to relieve the intense I/O demands of scientitc applications varied from improving the I/O interface to implementing improved out-of-core techniques. As shown in <ref> [MMD98, OWO98, SR98b, NFK98, TLG98] </ref>, scientitc applications clearly benett from using an API that enables advanced parallel I/O techniques such as collective I/O, prefetching and data sieving; however, most application developers still prefer to use the standard inecient UNIX API.
Reference: [SW97] <author> John Salmon and Michael Warren. </author> <title> Parallel out-of-core methods for N-body simulation. </title> <booktitle> In Proceedings of the Eighth SIAM Conference on Parallel Processing for Scientitc Computing, </booktitle> <month> March </month> <year> 1997. </year>
Reference-contexts: The data will be used by meteorologists for weather analysis and forecasts as well as being a tool for climate research. This paper discusses their plans to parallelize the core code of the system. They include a section on parallel I/O. * <ref> [SW97] </ref> They describe a parallel, out-of-core treecode library used for N-body simulations. Their approach targets machines in which secondary storage is attached to each processor. The library manually pages temporary data to the local disk to improve spatial and temporal locality. <p> We also suspect that some scientists are just starting to become aware of the importance of ecient I/O. Unless technology trends change, I/O will become a bottleneck for many more scientitc applications. We were also surprised by the number of papers about out-of-core applications <ref> [AVV98, PSS96, SW97, BCD97, NFK98] </ref>. An interesting point made by 9 [SW97] is that the ratio of DRAM to disk pricing suggests the use of out-of-core techniques to overcome memory capacity limitations. <p> Unless technology trends change, I/O will become a bottleneck for many more scientitc applications. We were also surprised by the number of papers about out-of-core applications [AVV98, PSS96, SW97, BCD97, NFK98]. An interesting point made by 9 <ref> [SW97] </ref> is that the ratio of DRAM to disk pricing suggests the use of out-of-core techniques to overcome memory capacity limitations.
Reference: [TCB + 96] <author> Rajeev Thakur, Alok Choudhary, Rajesh Bordawekar, Sachin More, and Sivaramakrishna Kuditipudi. </author> <title> Passion: Optimized I/O for parallel applications. </title> <journal> IEEE Computer, </journal> <volume> 29(6):7078, </volume> <month> June </month> <year> 1996. </year>
Reference-contexts: The modeling also accounts for device contention (network or tertiary storage). * [KKCB97, KKCB98] They describe the I/O performance of a parallel computational chemistry package using the Hartree-Fock (HF) method and the Passion I/O library <ref> [TCB + 96] </ref>. Before using any I/O optimizations, the I/O phase of the HF method accounted for up to 62% of their total execution time. They studied the eect of replacing the FORTRAN I/O calls with calls to the Passion I/O library.
Reference: [TLG98] <author> Rajeev Thakur, Ewing Lusk, and William Gropp. </author> <title> I/O in parallel applications: The weakest link. </title> <journal> The International Journal of 13 High Performance Computing Applications, </journal> <volume> 12(4), </volume> <month> Winter </month> <year> 1998. </year> <note> To appear in a Special Issue on I/O in Parallel Applications. </note>
Reference-contexts: They show that they can get much better performance with some replacement policies than with others, but despite the paper's title they do not compare with the performance of an equivalent program using tle I/O. * <ref> [TLG98] </ref> This paper is an introduction to the IJSA Special issue on I/O in parallel applications. They argue the importance of the application program interface (API) in obtaining ecient parallel I/O and why the standard UNIX API is ineective. <p> The techniques used by developers to relieve the intense I/O demands of scientitc applications varied from improving the I/O interface to implementing improved out-of-core techniques. As shown in <ref> [MMD98, OWO98, SR98b, NFK98, TLG98] </ref>, scientitc applications clearly benett from using an API that enables advanced parallel I/O techniques such as collective I/O, prefetching and data sieving; however, most application developers still prefer to use the standard inecient UNIX API.
Reference: [TSF + 97] <author> Michael Tobis, Chad Schafer, Ian Foster, Robert Jacob, and John Anderson. FOAM: </author> <title> Expanding the horizons of climate modeling. </title> <booktitle> In Proceedings of SC97: High Performance Networking and Computing. </booktitle> <publisher> IEEE Computer Society Press, </publisher> <month> November </month> <year> 1997. </year> <month> 14 </month>
Reference-contexts: They used a single 100baseT ethernet switch with bi-directional bandwidth of 80MB/s and latency of 150us. The entire system cost less than $60,000! They showed overall performance and paging behavior of an 80 million body model, a 5 million body model and a 500000 body model. * <ref> [TSF + 97] </ref> This paper is about the Fast Ocean-Atmosphere Model (FOAM), a climate model that uses a combination of new model formulation and parallel computing to expand the time horizon that may be addressed by explicit uid dynamical representations of the climate system.
References-found: 31

