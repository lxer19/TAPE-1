URL: ftp://info.mcs.anl.gov/pub/tech_reports/reports/P180.ps.Z
Refering-URL: http://www.mcs.anl.gov/publications/preprints.htm
Root-URL: http://www.mcs.anl.gov
Title: ADOL-C: A Package for the Automatic Differentiation of Algorithms Written in C/C++  
Author: Andreas Griewank David Juedes Jean Utke 
Keyword: Automatic Differentiation, Chain Rule, Overloading, Taylor Coefficients, Gradients, Hessians, Reverse Propagation Abbreviated title: Automatic differentiation  
Date: 1.5, December 1993  
Note: Version  by overloading in C++  
Abstract: The C++ package ADOL-C described here facilitates the evaluation of first and higher derivatives of vector functions that are defined by computer programs written in C or C++. The resulting derivative evaluation routines may be called from C/C++, Fortran, or any other language that can be linked with C. The numerical values of derivative vectors are obtained free of truncation errors at a small multiple of the run time and randomly accessed memory of the given function evaluation program. Derivative matrices are obtained by columns or rows. For solution curves defined by ordinary differential equations, special routines are provided that evaluate the Taylor coefficient vectors and their Jacobians with respect to the current state vector. The derivative calculations involve a possibly substantial (but always a priori predictable) amount of data that are accessed strictly sequentially and are therefore automatically paged out to files on external mass storage devices. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> C. H. Bischof, A. Carle, G. F. Corliss, A. Griewank, and P. Hovland. ADIFOR: </author> <title> Generating derivative codes from Fortran programs. </title> <journal> Scientific Programming, </journal> <volume> 1(1) </volume> <pages> 1-29, </pages> <year> 1992. </year>
Reference-contexts: Since then, there have been numerous rediscoveries and various software implementations. Speelpenning himself wrote a Fortran precompiler called JAKE, which was upgraded at Argonne National Laboratory to JAKEF. Currently, there exist at least three other precompilers for automatic differentiation, namely, GRESS/ADGEN [11], 5 PADRE2 [13], 6 and ADIFOR <ref> [1] </ref>. 7 5 Contact: Jim Horwedel, ORNL, P.O. Box X, Oak Ridge, TN 37831, e-mail: jqh%ornlstc.bitnet 6 Contact: K. Kubota, Keio Univ., 3-14-1 Hiyoshi, Yokohama 223, Japan, e-mail: kubota@ae.keio.ac.jp 7 Contact: Ch. <p> The first argument tag specifies the particular tape of interest. The components of counts represent counts [0] the number of independents, i.e., calls to o= counts <ref> [1] </ref> the number of dependents, i.e., calls to AE= counts [2] the maximal number of live active variables counts [3] the number of deaths and overwrites counts [4] the buffer size (a multiple of eight) counts [5] the total number of operations recorded counts [6-10] other internal information about the tape <p> The new version can now be called from within any active section, as in the following main program. #include ....... as above main () - int i,tag=1; cout&lt;<"monomial degree=? "n"; // Input the desired degree. int n; cin &gt;> n; /*Allocations and Initializations*/ double* Y <ref> [1] </ref>; *Y = new double [n+2]; double* X [1]; // Allocate passive variables with *X = new double [n+4]; // extra dimension for derivatives X [0][0] = 0.5; // function value = 0. coefficient X [0][1] = 1.0; // first derivative = 1. coefficient for (i=0; i &lt; n+2; i++) X <p> can now be called from within any active section, as in the following main program. #include ....... as above main () - int i,tag=1; cout&lt;<"monomial degree=? "n"; // Input the desired degree. int n; cin &gt;> n; /*Allocations and Initializations*/ double* Y <ref> [1] </ref>; *Y = new double [n+2]; double* X [1]; // Allocate passive variables with *X = new double [n+4]; // extra dimension for derivatives X [0][0] = 0.5; // function value = 0. coefficient X [0][1] = 1.0; // first derivative = 1. coefficient for (i=0; i &lt; n+2; i++) X [0][i+2]=0; // further coefficients. double* Z [1]; // <p> X <ref> [1] </ref>; // Allocate passive variables with *X = new double [n+4]; // extra dimension for derivatives X [0][0] = 0.5; // function value = 0. coefficient X [0][1] = 1.0; // first derivative = 1. coefficient for (i=0; i &lt; n+2; i++) X [0][i+2]=0; // further coefficients. double* Z [1]; // used for checking consistency *Z = new double [n+2]; // between forward and reverse adouble y,x; // Declare active variables /*Beginning of Active Section*/ trace_on (1); // tag = 1 and keep = 0 x &lt;<= X [0][0]; // Only one independent var y = power (x,n); // Actual <p> (1); // tag = 1 and keep = 0 x &lt;<= X [0][0]; // Only one independent var y = power (x,n); // Actual function call y &gt;>= Y [0][0]; // Only one dependent adouble trace_off (); // No global adouble has died /*End of Active Section */ double u <ref> [1] </ref>; // weighting vector u [0]=1; // for reverse call for (i=0; i &lt; n+2; i++) - // Note that keep = i+1 in call forward (tag,1,1,i,i+1,X,Y); // Evaluate the i-the derivative if (i==0) cout &lt;< Y [0][i] &lt;< "=?" &lt;< value (y) &lt;< " should be the same "n"; else <p> (int j=0;j&lt;n; j++) A [i][j] &lt;<= j/(1.0+i); //make all elements of A independent diag += value (A [i][i]); //value (adouble) converts to double A [i][i] += 1.0; - det (n,m-1) &gt;>= detout; // Actual function call. printf (""n %f =? %f should be the same "n",detout,diag); trace_off (); double u <ref> [1] </ref>; u [0] = 1.0; double* B = new double [n*n]; reverse (tag,1,n*n,1,u,B); cout &lt;<" "n first base? : "; for (i=0;i&lt;n;i++) - adouble sum = 0; for (int j=0;j&lt;n;j++) // The matrix A times the first n sum += A [i][j]*B [j]; // components of the gradient B cout&lt;<value (sum)<<" <p> void tracerhs (short int tag, double* py, double* pyprime) - adoublev y (3); //This time we left the parameters adoublev yprime (3); // passive and use the vector types. trace_on (tag); y &lt;<= py; //Initialize and mark independents 33 yprime [0] = -sin (y [2]) + 1e8*y [2]*(1-1/y [0]); yprime <ref> [1] </ref> = -10*y [0] + 3e7*y [2]*(1-y [1]); yprime [2] = -yprime [0] - yprime [1]; yprime &gt;>= pyprime; //Mark and pass dependents trace_off (tag); - This function is a slight modification of Robertson test problem given in Hairer and Wanner's book on the numerical solution of ODEs [10]. <p> double* pyprime) - adoublev y (3); //This time we left the parameters adoublev yprime (3); // passive and use the vector types. trace_on (tag); y &lt;<= py; //Initialize and mark independents 33 yprime [0] = -sin (y [2]) + 1e8*y [2]*(1-1/y [0]); yprime <ref> [1] </ref> = -10*y [0] + 3e7*y [2]*(1-y [1]); yprime [2] = -yprime [0] - yprime [1]; yprime &gt;>= pyprime; //Mark and pass dependents trace_off (tag); - This function is a slight modification of Robertson test problem given in Hairer and Wanner's book on the numerical solution of ODEs [10]. <p> we left the parameters adoublev yprime (3); // passive and use the vector types. trace_on (tag); y &lt;<= py; //Initialize and mark independents 33 yprime [0] = -sin (y [2]) + 1e8*y [2]*(1-1/y [0]); yprime <ref> [1] </ref> = -10*y [0] + 3e7*y [2]*(1-y [1]); yprime [2] = -yprime [0] - yprime [1]; yprime &gt;>= pyprime; //Mark and pass dependents trace_off (tag); - This function is a slight modification of Robertson test problem given in Hairer and Wanner's book on the numerical solution of ODEs [10]. The Jacobian of the right-hand side has large negative eigenvalues, which make the ODE quite stiff.
Reference: [2] <author> Brett Averick, Jorge More, Christian Bischof, Alan Carle, and Andreas Griewank. </author> <title> Computing large sparse Jacobian matrices using automatic differentiation. </title> <type> Preprint MCS-P348-0193, </type> <institution> Argonne National Laboratory, </institution> <year> 1993. </year>
Reference-contexts: The first argument tag specifies the particular tape of interest. The components of counts represent counts [0] the number of independents, i.e., calls to o= counts [1] the number of dependents, i.e., calls to AE= counts <ref> [2] </ref> the maximal number of live active variables counts [3] the number of deaths and overwrites counts [4] the buffer size (a multiple of eight) counts [5] the total number of operations recorded counts [6-10] other internal information about the tape The values maxlive = counts [2] and deaths = counts <p> calls to AE= counts <ref> [2] </ref> the maximal number of live active variables counts [3] the number of deaths and overwrites counts [4] the buffer size (a multiple of eight) counts [5] the total number of operations recorded counts [6-10] other internal information about the tape The values maxlive = counts [2] and deaths = counts [3] determine the temporary storage requirements during calls to the workhorses forward and reverse. <p> For smaller p the interpretive overhead is not appropriately amortized and for larger p the p-fold increase in storage causes too many page faults. Therefore, large Jacobians that cannot be compressed via column coloring (as was done for example in <ref> [2] </ref>) should be strip-mined in that the above first-order-vector version of forward is called repeatedly with the successive n fi p matrices X forming a partition of the identity matrix of order n. 5.5 Drivers for Optimization and Nonlinear Equations For convenience one may use instead of forward and reverse the <p> differential equation. #include ..... as above void tracerhs (short int tag, double* py, double* pyprime) - adoublev y (3); //This time we left the parameters adoublev yprime (3); // passive and use the vector types. trace_on (tag); y &lt;<= py; //Initialize and mark independents 33 yprime [0] = -sin (y <ref> [2] </ref>) + 1e8*y [2]*(1-1/y [0]); yprime [1] = -10*y [0] + 3e7*y [2]*(1-y [1]); yprime [2] = -yprime [0] - yprime [1]; yprime &gt;>= pyprime; //Mark and pass dependents trace_off (tag); - This function is a slight modification of Robertson test problem given in Hairer and Wanner's book on the numerical <p> - adoublev y (3); //This time we left the parameters adoublev yprime (3); // passive and use the vector types. trace_on (tag); y &lt;<= py; //Initialize and mark independents 33 yprime [0] = -sin (y <ref> [2] </ref>) + 1e8*y [2]*(1-1/y [0]); yprime [1] = -10*y [0] + 3e7*y [2]*(1-y [1]); yprime [2] = -yprime [0] - yprime [1]; yprime &gt;>= pyprime; //Mark and pass dependents trace_off (tag); - This function is a slight modification of Robertson test problem given in Hairer and Wanner's book on the numerical solution of ODEs [10].
Reference: [3] <author> K. E. Brenan, S. L. Campbell, and L. R. Petzold. </author> <title> Numerical Solution of Initial Value Problems in Differential-Algebraic Equations. </title> <publisher> Elsevier (North Holland), </publisher> <year> 1989. </year>
Reference-contexts: The first argument tag specifies the particular tape of interest. The components of counts represent counts [0] the number of independents, i.e., calls to o= counts [1] the number of dependents, i.e., calls to AE= counts [2] the maximal number of live active variables counts <ref> [3] </ref> the number of deaths and overwrites counts [4] the buffer size (a multiple of eight) counts [5] the total number of operations recorded counts [6-10] other internal information about the tape The values maxlive = counts [2] and deaths = counts [3] determine the temporary storage requirements during calls to <p> the maximal number of live active variables counts <ref> [3] </ref> the number of deaths and overwrites counts [4] the buffer size (a multiple of eight) counts [5] the total number of operations recorded counts [6-10] other internal information about the tape The values maxlive = counts [2] and deaths = counts [3] determine the temporary storage requirements during calls to the workhorses forward and reverse.
Reference: [4] <author> D. G. Cacuci. </author> <title> Sensitivity theory for nonlinear systems. I. Nonlinear functional analysis approach. </title> <journal> J. Math. Phys., </journal> <volume> 22(12) </volume> <pages> 2794-2802, </pages> <year> 1981. </year>
Reference-contexts: The components of counts represent counts [0] the number of independents, i.e., calls to o= counts [1] the number of dependents, i.e., calls to AE= counts [2] the maximal number of live active variables counts [3] the number of deaths and overwrites counts <ref> [4] </ref> the buffer size (a multiple of eight) counts [5] the total number of operations recorded counts [6-10] other internal information about the tape The values maxlive = counts [2] and deaths = counts [3] determine the temporary storage requirements during calls to the workhorses forward and reverse.
Reference: [5] <author> D. G. Cacuci. </author> <title> Sensitivity theory for nonlinear systems. II. Extension to additional classes of responses. </title> <journal> J. Math. Phys., </journal> <volume> 22(12) </volume> <pages> 2803-2812, </pages> <year> 1981. </year>
Reference-contexts: of counts represent counts [0] the number of independents, i.e., calls to o= counts [1] the number of dependents, i.e., calls to AE= counts [2] the maximal number of live active variables counts [3] the number of deaths and overwrites counts [4] the buffer size (a multiple of eight) counts <ref> [5] </ref> the total number of operations recorded counts [6-10] other internal information about the tape The values maxlive = counts [2] and deaths = counts [3] determine the temporary storage requirements during calls to the workhorses forward and reverse.
Reference: [6] <author> Bruce Christianson. </author> <title> Reverse accumulation and accurate rounding error estimates for taylor series. </title> <booktitle> Optimization Methods and Software, </booktitle> <pages> pages 81-94, 1(92). </pages>
Reference-contexts: Provided F is analytic, this property is inherited by the functions y j = y j (x 0 ; x 1 ; : : : ; x j ) 2 R m ; and their derivatives satisfy the identity @y j = @x 0 which has been established in <ref> [6] </ref>. The m fi n matrices A k ; k = 0; : : :; d are in fact the Taylor coefficients of the Jacobian path F 0 (x (t)), a fact that is of interest primarily in the context of ordinary differential equations and differential algebraic equations.
Reference: [7] <author> Andreas Griewank. </author> <title> Direct calculation of Newton steps without accumulating Jaco-bians. </title> <editor> In T. F. Coleman and Yuying Li, editors, </editor> <booktitle> Large-Scale Numerical Optimization, </booktitle> <pages> pages 115 - 137. </pages> <publisher> SIAM, </publisher> <address> Philadelphia, Penna., </address> <year> 1990. </year>
Reference-contexts: When m, the number of component functions, is larger than n, Jacobians can be obtained more cheaply column by column through propagating gradients forward. This classical technique of automatic differentiation goes back at least to Wengert [20] and was later popularized by Rall [16]. It was noted in <ref> [7] </ref> that in general neither the row-by-row nor the column-by-column method is optimal for the calculation of Jacobians. The potentially more efficient alternatives, however, require some combinatorial optimization and involve large data structures that are not necessarily accessed sequentially [9].
Reference: [8] <author> Andreas Griewank. </author> <title> Achieving logarithmic growth of temporal and spatial complexity in reverse automatic differentiation. </title> <journal> Optimization Methods and Software, </journal> <volume> 1 </volume> <pages> 35-54, </pages> <year> 1992. </year> <month> 38 </month>
Reference-contexts: For the reverse propagation of derivatives, the whole execution trace of the original evaluation program must be recorded, unless it is recalculated in pieces as advocated in <ref> [8] </ref>. In ADOL-C, this potentially very large data set is written first into a buffer array and later into a file if the buffer is full or if the user wishes a permanent record of the execution trace.
Reference: [9] <author> Andreas Griewank and Shawn Reese. </author> <title> On the calculation of Jacobian matrices by the Markowitz rule. </title> <editor> In Andreas Griewank and George F. Corliss, editors, </editor> <title> Automatic Differentiation of Algorithms: Theory, Implementation, and Application. </title> <publisher> SIAM, </publisher> <address> Philadel-phia, Penna., </address> <year> 1991. </year>
Reference-contexts: It was noted in [7] that in general neither the row-by-row nor the column-by-column method is optimal for the calculation of Jacobians. The potentially more efficient alternatives, however, require some combinatorial optimization and involve large data structures that are not necessarily accessed sequentially <ref> [9] </ref>. Therefore, the package ADOL-C described here was written primarily for the evaluation of derivative vectors (e.g., rows or columns of Jacobians). This approach also simplifies parameter passing between subroutines and calls in different computer languages.
Reference: [10] <author> E. Hairer and G. Wanner, </author> <title> Solving Ordinary Differential Equations II, </title> <publisher> Springer-Verlag, </publisher> <address> Berlin, </address> <year> 1991. </year>
Reference-contexts: [0]); yprime [1] = -10*y [0] + 3e7*y [2]*(1-y [1]); yprime [2] = -yprime [0] - yprime [1]; yprime &gt;>= pyprime; //Mark and pass dependents trace_off (tag); - This function is a slight modification of Robertson test problem given in Hairer and Wanner's book on the numerical solution of ODEs <ref> [10] </ref>. The Jacobian of the right-hand side has large negative eigenvalues, which make the ODE quite stiff. We have added some numerically benign transcendentals to make the differentiation more interesting.
Reference: [11] <author> Jim E. Horwedel, Brian A. Worley, E. M. Oblow, and F. G. Pin. </author> <note> GRESS version 1.0 users manual. Technical Memorandum ORNL/TM 10835, </note> <institution> Oak Ridge National Laboratory, Oak Ridge, Tenn., </institution> <year> 1988. </year>
Reference-contexts: Since then, there have been numerous rediscoveries and various software implementations. Speelpenning himself wrote a Fortran precompiler called JAKE, which was upgraded at Argonne National Laboratory to JAKEF. Currently, there exist at least three other precompilers for automatic differentiation, namely, GRESS/ADGEN <ref> [11] </ref>, 5 PADRE2 [13], 6 and ADIFOR [1]. 7 5 Contact: Jim Horwedel, ORNL, P.O. Box X, Oak Ridge, TN 37831, e-mail: jqh%ornlstc.bitnet 6 Contact: K. Kubota, Keio Univ., 3-14-1 Hiyoshi, Yokohama 223, Japan, e-mail: kubota@ae.keio.ac.jp 7 Contact: Ch.
Reference: [12] <author> G. Kedem. </author> <title> Automatic differentiation of computer programs. </title> <journal> ACM Trans. Math. Software, </journal> <volume> 6(2) </volume> <pages> 150-165, </pages> <month> June </month> <year> 1980. </year>
Reference-contexts: Box X, Oak Ridge, TN 37831, e-mail: jqh%ornlstc.bitnet 6 Contact: K. Kubota, Keio Univ., 3-14-1 Hiyoshi, Yokohama 223, Japan, e-mail: kubota@ae.keio.ac.jp 7 Contact: Ch. Bischof, ANL-MCS, Argonne IL 60439-4844, e-mail: bischof@mcs.anl.gov 2 Following the work of Kedem <ref> [12] </ref> with the Fortran preprocessor AUGMENT, Rall [17] implemented in 1983 the forward propagation of gradients by overloading in PASCAL-SC. In contrast to precompilation, overloading requires only minor modifications of the user's evaluation program and does not generate intermediate source code. <p> and correctness checks, which are included in the full codes. 7.1 Product Example The first example evaluates the gradient and a Hessian-vector product for the function y = f (x) = i=0 using the appropriate drivers gradient and hessian. #include "adouble.h" #include "adutils.h" #include &lt;stream.h&gt; main () - int n,i,j,counts <ref> [12] </ref>; cout &lt;< "number of independent variables = ? "n"; cin &gt;> n; double* xp = new double [n]; // or: doublev xp (n); adouble* x = new adouble [n]; // or: adoublev x (n); for (i=0;i&lt;n;i++) xp [i] = (i+1.0)/(2.0+i); // some initializations trace_on (1); // tag =1, keep=0 by
Reference: [13] <author> Koichi Kubota. PADRE2, </author> <title> a FORTRAN precompiler yielding error estimates and second derivatives. </title> <editor> In Andreas Griewank and George F. Corliss, editors, </editor> <title> Automatic Differentiation of Algorithms: Theory, Implementation, and Application. </title> <journal> SIAM, Philadel-phia, </journal> <volume> Penna., </volume> <pages> 251-262, </pages> <year> 1991. </year>
Reference-contexts: Since then, there have been numerous rediscoveries and various software implementations. Speelpenning himself wrote a Fortran precompiler called JAKE, which was upgraded at Argonne National Laboratory to JAKEF. Currently, there exist at least three other precompilers for automatic differentiation, namely, GRESS/ADGEN [11], 5 PADRE2 <ref> [13] </ref>, 6 and ADIFOR [1]. 7 5 Contact: Jim Horwedel, ORNL, P.O. Box X, Oak Ridge, TN 37831, e-mail: jqh%ornlstc.bitnet 6 Contact: K. Kubota, Keio Univ., 3-14-1 Hiyoshi, Yokohama 223, Japan, e-mail: kubota@ae.keio.ac.jp 7 Contact: Ch.
Reference: [14] <author> S. Linnainmaa. </author> <title> Taylor expansion of the accumulated rounding error. </title> <journal> BIT (Nordisk Tidskrift for Informationsbehandling), </journal> <volume> 16(1) </volume> <pages> 146-160, </pages> <year> 1976. </year>
Reference-contexts: The discrete analog used here was apparently first discovered in the early seventies by Ostrovskii et al. [15] and Linnainmaa <ref> [14] </ref> in the context of rounding error estimates. Since then, there have been numerous rediscoveries and various software implementations. Speelpenning himself wrote a Fortran precompiler called JAKE, which was upgraded at Argonne National Laboratory to JAKEF.
Reference: [15] <author> G. M. Ostrovskii, Yu. M. Volin, and W. W. Borisov. </author> <title> Uber die Berechnung von Ableitungen. </title> <journal> Wissenschaftliche Zeitschrift der Technischen Hochschule fur Chemie, Leuna-Merseburg, </journal> <volume> 13(4) </volume> <pages> 382-384, </pages> <year> 1971. </year>
Reference-contexts: The discrete analog used here was apparently first discovered in the early seventies by Ostrovskii et al. <ref> [15] </ref> and Linnainmaa [14] in the context of rounding error estimates. Since then, there have been numerous rediscoveries and various software implementations. Speelpenning himself wrote a Fortran precompiler called JAKE, which was upgraded at Argonne National Laboratory to JAKEF.
Reference: [16] <author> Louis B. Rall. </author> <title> Automatic Differentiation: Techniques and Applications, </title> <booktitle> volume 120 of Lecture Notes in Computer Science. </booktitle> <publisher> Springer Verlag, </publisher> <address> Berlin, </address> <year> 1981. </year>
Reference-contexts: When m, the number of component functions, is larger than n, Jacobians can be obtained more cheaply column by column through propagating gradients forward. This classical technique of automatic differentiation goes back at least to Wengert [20] and was later popularized by Rall <ref> [16] </ref>. It was noted in [7] that in general neither the row-by-row nor the column-by-column method is optimal for the calculation of Jacobians. The potentially more efficient alternatives, however, require some combinatorial optimization and involve large data structures that are not necessarily accessed sequentially [9].
Reference: [17] <author> Louis B. Rall. </author> <title> Differentiation and generation of Taylor coefficients in Pascal-SC. </title> <editor> In Ulrich W. Kulisch and Willard L. Miranker, editors, </editor> <booktitle> A New Approach to Scientific Computation, </booktitle> <pages> pages 291-309. </pages> <publisher> Academic Press, </publisher> <address> New York, </address> <year> 1983. </year>
Reference-contexts: Box X, Oak Ridge, TN 37831, e-mail: jqh%ornlstc.bitnet 6 Contact: K. Kubota, Keio Univ., 3-14-1 Hiyoshi, Yokohama 223, Japan, e-mail: kubota@ae.keio.ac.jp 7 Contact: Ch. Bischof, ANL-MCS, Argonne IL 60439-4844, e-mail: bischof@mcs.anl.gov 2 Following the work of Kedem [12] with the Fortran preprocessor AUGMENT, Rall <ref> [17] </ref> implemented in 1983 the forward propagation of gradients by overloading in PASCAL-SC. In contrast to precompilation, overloading requires only minor modifications of the user's evaluation program and does not generate intermediate source code. Our package ADOL-C utilizes overloading in C++, but the user has to know only C.
Reference: [18] <author> B. Speelpenning. </author> <title> Compiling Fast Partial Derivatives of Functions Given by Algorithms. </title> <type> Ph.D. thesis, </type> <institution> Department of Computer Science, University of Illinois at Urbana-Champaign, Urbana-Champaign, Ill., </institution> <month> January </month> <year> 1980. </year>
Reference-contexts: An obvious way to avoid such redundant calculations is to apply an optimizing compiler to the source code that can be generated from the symbolic representation of the derivatives in question. Exactly this approach was investigated by Bert Speelpenning, a student of Bill Gear, during his Ph.D. research <ref> [18] </ref> at the University of Illinois from 1977 to 1980. Eventually he realized that at least in the cases n = 1 and m = 1, the most efficient code for the evaluation of derivatives can be obtained directly from that for the evaluation of the underlying vector function.
Reference: [19] <author> Olivier Talagrand and P. Courtier. </author> <title> Variational assimilation of meteorological observations with the adjoint vorticity equation Part I. Theory. </title> <editor> Q. J. R. </editor> <title> Meteorol. </title> <journal> Soc., </journal> <volume> 113 </volume> <pages> 1311-1328, </pages> <year> 1987. </year>
Reference-contexts: The reverse propagation of gradients employed by Speelpenning is closely related to the adjoint sensitivity analysis for differential equations, which has been used at least since the late sixties, especially in nuclear engineering [4],[5], weather forecasting <ref> [19] </ref>, and neural networks [21]. The discrete analog used here was apparently first discovered in the early seventies by Ostrovskii et al. [15] and Linnainmaa [14] in the context of rounding error estimates. Since then, there have been numerous rediscoveries and various software implementations.
Reference: [20] <author> R. E. Wengert. </author> <title> A simple automatic derivative evaluation program. </title> <journal> Comm. ACM, </journal> <volume> 7(8) </volume> <pages> 463-464, </pages> <year> 1964. </year>
Reference-contexts: When m, the number of component functions, is larger than n, Jacobians can be obtained more cheaply column by column through propagating gradients forward. This classical technique of automatic differentiation goes back at least to Wengert <ref> [20] </ref> and was later popularized by Rall [16]. It was noted in [7] that in general neither the row-by-row nor the column-by-column method is optimal for the calculation of Jacobians.
Reference: [21] <author> Paul Werbos. </author> <title> Beyond Regression: New Tools for Prediction and Analysis in the Behavioral Sciences. </title> <type> Ph.D/ thesis, </type> <institution> Committee on Applied Mathematics, Harvard University, </institution> <address> Cambridge, Mass., </address> <month> November </month> <year> 1974. </year> <month> 39 </month>
Reference-contexts: The reverse propagation of gradients employed by Speelpenning is closely related to the adjoint sensitivity analysis for differential equations, which has been used at least since the late sixties, especially in nuclear engineering [4],[5], weather forecasting [19], and neural networks <ref> [21] </ref>. The discrete analog used here was apparently first discovered in the early seventies by Ostrovskii et al. [15] and Linnainmaa [14] in the context of rounding error estimates. Since then, there have been numerous rediscoveries and various software implementations.
References-found: 21

