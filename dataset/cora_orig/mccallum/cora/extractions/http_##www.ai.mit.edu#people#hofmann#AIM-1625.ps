URL: http://www.ai.mit.edu/people/hofmann/AIM-1625.ps
Refering-URL: http://www.ai.mit.edu/people/hofmann/
Root-URL: 
Title: Statistical Models for Co-occurrence Data  
Author: Thomas Hofmann Jan Puzicha 
Note: This publication can be retrieved by anonymous ftp to publications.ai.mit.edu. Copyright c Massachusetts Institute of Technology, 1998  
Date: 1625 February, 1998  159  
Affiliation: MASSACHUSETTS INSTITUTE OF TECHNOLOGY ARTIFICIAL INTELLIGENCE LABORATORY  
Pubnum: A.I. Memo No.  C.B.C.L. Memo No.  
Abstract: Modeling and predicting co-occurrences of events is a fundamental problem of unsupervised learning. In this contribution we develop a statistical framework for analyzing co-occurrence data in a general setting where elementary observations are joint occurrences of pairs of abstract objects from two finite sets. The main challenge for statistical models in this context is to overcome the inherent data sparseness and to estimate the probabilities for pairs which were rarely observed or even unobserved in a given sample set. Moreover, it is often of considerable interest to extract grouping structure or to find a hierarchical data organization. A novel family of mixture models is proposed which explain the observed data by a finite number of shared aspects or clusters. This provides a common framework for statistical inference and structure discovery and also includes several recently proposed models as special cases. Adopting the maximum likelihood principle, EM algorithms are derived to fit the model parameters. We develop improved versions of EM which largely avoid overfitting problems and overcome the inherent locality of EM-based optimization. Among the broad variety of possible applications, e.g., in information retrieval, natural language processing, data mining, and computer vision, we have chosen document retrieval, the statistical analysis of noun/adjective co-occurrence and the unsupervised segmentation of textured images to test and evaluate the proposed algorithms. This report describes research accomplished at the Center for Biological and Computational Learning, the Artificial Intelligence Laboratory of the Massachusetts Institute of Technology, the University of Bonn and the beaches of Cape Cod. Thomas Hofmann was supported by a M.I.T. Faculty Sponser's Discretionary Fund. Jan Puzicha was supported by the German Research Foundation (DFG) under grant # BU 914/3-1. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> E. Bauer, D. Koller, and Y. Singer. </author> <title> Update rules for parameter estimation in Bayesian networks. </title> <booktitle> In Proceedings of the Thirteenth Annual Conference on Uncertainty in Artifical Intelligence, </booktitle> <pages> pages 3-13, </pages> <year> 1997. </year>
Reference-contexts: A simple way to accelerate EM algorithms is by overrelaxation in the M-step. This has been discussed early in the context of mixture models [44, 45] and was `rediscovered' more recently under the title of EM () in <ref> [1] </ref>. We found this method useful in accelerating the fitting procedure for all discussed models.
Reference: [2] <author> G. Bilbro and W. Snyder. </author> <title> Mean field approximation minimizes relative entropy. </title> <journal> Journal of the Optical Society of America, </journal> <volume> 8(2) </volume> <pages> 290-294, </pages> <year> 1991. </year>
Reference-contexts: We make use of the more suggestive notation hR rff i = p rff to stress that the variational parameters p rff can actually be thought of as an approximation of the posterior marginals. This variational technique is known as mean-field approximation <ref> [42, 2] </ref> and has been successfully applied for optimization problems [61, 23], in computer vision [15, 67, 24], and for inference in graphical models [55].
Reference: [3] <author> L. Breiman, J. H. Friedman, R. A. Olshen, and C. J. Stone. </author> <title> Classification and Regression Trees. </title> <publisher> Wadsworth Intern. Group, </publisher> <address> Belmont, California, </address> <year> 1984. </year>
Reference-contexts: it is straightforward to check that the predictive probabilities for s = (x i ; y j ; L + 1) are given by P (sjS; ) = p i - X hI iff it -jff;i : (59) 4.3 Hierarchies and Abstraction With other hierarchical mixture models proposed for supervised <ref> [3, 29] </ref> and unsupervised learning [37] the HACM shares the organization of clusters in a tree structure. It extracts hierarchical relations between clusters, i.e., it breaks the permutation-symmetry of the cluster labeling. Even more important, however, it is capable to perform statistical abstraction.
Reference: [4] <author> P.F. Brown, P.V. deSouza, R.L. Mercer, V.J. Della Pietra, and J.C. Lai. </author> <title> Class-based n-gram models of natural language. </title> <journal> Computational Lingus-tics, </journal> <volume> 18(4) </volume> <pages> 467-479, </pages> <year> 1992. </year>
Reference-contexts: Since the models are directly applicable to co-occurrence and histogram data, the necessity for pairwise comparisons is avoided altogether. Probabilistic models for COD have recently been investigated under the titles of class-based n-gram models <ref> [4] </ref>, distributional clustering [43], and aggregate Markov models [54] in natural language processing. All three approaches are recovered as special cases in our COD framework and we will clarify the relation to our approach in the following sections. <p> I and J thus maximizes the mutual information which is very satisfying, since it gives the SCM a precise interpretation in terms of an information theoretic concept. A similar criterion based on mutual information has been proposed by Brown et al. <ref> [4] </ref> in their class-based n-gram model. More precisely their model is a special case of the (hard clustering) SCM, where formally X = Y and I i= J i. 5 5 For the bigram model in [4] this implies that the word The coupled K-means like equations for either set of <p> A similar criterion based on mutual information has been proposed by Brown et al. <ref> [4] </ref> in their class-based n-gram model. More precisely their model is a special case of the (hard clustering) SCM, where formally X = Y and I i= J i. 5 5 For the bigram model in [4] this implies that the word The coupled K-means like equations for either set of discrete variables are obtained by maximizing the aug mented likelihood in (36) from which we deduce ^ I iff = 1 if ff = arg max hi 0 else ; with (44) h i- j n <p> Although we could in principle insert the expression in (36) directly into the likelihood and derive a local maximization algorithm for I and J (cf. <ref> [4] </ref>), this would result in much more complicated stationary conditions than (44).
Reference: [5] <author> J.M. Buhmann and H. Kuhnel. </author> <title> Vector quantization with complexity costs. </title> <journal> IEEE Transactions on Information Theory, </journal> <volume> 39(4) </volume> <pages> 1133-1145, </pages> <month> July </month> <year> 1993. </year>
Reference-contexts: The second, even more important problem is to avoid overfitting, i.e., maximize the performance on unseen future data. The framework which allows us to improve the presented EM procedures in both aspects is known as deterministic annealing. Deterministic annealing has been applied to many clustering problems, including vectorial clustering <ref> [49, 50, 5] </ref>, pairwise clustering [23], and in the context of COD for distributional clustering [43]. The key idea is to introduce a temperature parameter T and to replace the minimization of a combinatorial objective function by a substitute known as the free energy.
Reference: [6] <author> B. Chaudhuri and N. Sarkar. </author> <title> Texture segmentation using fractal dimension. </title> <journal> IEEE Transactions on Pattern Analysis and Machine Intelligence, </journal> <volume> 17(1) </volume> <pages> 72-77, </pages> <year> 1995. </year>
Reference-contexts: Numerous approaches to texture segmentation have been proposed over the past decades, most of which obey a two-stage scheme: 1. A modeling stage: characteristic features are extracted from the textured input image, which range from spatial frequencies [25, 24], MRF-models [33, 34], co-occurrence matrices [16] to fractal in dices <ref> [6] </ref>. 2. In the clustering stage features are grouped into homogeneous segments, where homogeneity of features has to be formalized by a mathematical no tion of similarity. <p> In the clustering stage features are grouped into homogeneous segments, where homogeneity of features has to be formalized by a mathematical no tion of similarity. Most widely, features are interpreted as vectors in a Eu-clidean space <ref> [25, 33, 34, 65, 40, 6, 31] </ref> and a segmentation is obtained by minimizing the K-means criterion, which sums over the square distances between feature vectors and their assigned, group-specific prototype feature vectors. K-means clustering can be understood as a statistical mixture model with isotropic Gaussian class distributions.
Reference: [7] <author> S. Chen and J. Goodman. </author> <title> An empirical study of smoothing techniques for language modeling. </title> <booktitle> In Proceedings of the 34th Annual Meeting of the ACL, </booktitle> <pages> pages 310-318, </pages> <year> 1996. </year>
Reference-contexts: Another class of methods are similarity-based local smoothing techniques as, e.g., proposed by Essen and Steinbiss [14] and Dagan et al. [9, 10]. An empirical comparison of smoothing techniques can be found in <ref> [7] </ref>. In information retrieval, there have been essentially two proposals to overcome the sparseness problem. The first class of methods relies on the cluster hypothesis [62, 20] which suggests to make use of inter-document similarities in order to improve the retrieval performance.
Reference: [8] <author> D.R. Cutting, D.R. Karger, and J.O. Pedersen. </author> <title> Constant interaction-time scatter/gather browsing of very large document collections. </title> <booktitle> In Sixteenth Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, </booktitle> <address> Pittsburgh, PA, USA, </address> <pages> pages 126-134, </pages> <year> 1993. </year>
Reference-contexts: The first class of methods relies on the cluster hypothesis [62, 20] which suggests to make use of inter-document similarities in order to improve the retrieval performance. Since it is often prohibitive to compute all pairwise similarities between documents these methods typically rely on random comparisons or random fractionation <ref> [8] </ref>. The second approach focuses on the index terms to derive an improved feature representation of documents. The by far most popular technique in this category is Salton's Vector Space Model [51, 58, 52] of which different variants have been proposed with different word weighting schemes [53]. <p> It offers several new possibilities in data analysis and information retrieval tasks like extracting resolution-dependent meaningful keywords for subcollection of documents and gives a satisfying solution to the problem of cluster summarization (cf. <ref> [8] </ref>) since it explicitly finds the most characteristic terms for each (super-)cluster of documents. There are several additional problems which have to be solved to arrive at a complete algorithm for the HACM. The most important concerns the specification of a procedure to obtain the tree-topology T . <p> The most frequently used methods in this context are linkage algorithms (single linkage, complete linkage, Wards method, cf. [26]), or hybrid combinations of ag 12 on the Cranfield collection. different temperatures on the Cranfield collection. glomerative and centroid-based methods <ref> [8] </ref> which have no probabilistic interpretation and have a number of other disadvantages. In contrast, COD mixture models provide a sound statistical basis and overcome the fundamental sparseness problem of proximity-based clustering.
Reference: [9] <author> I. Dagan, L. Lee, and F.C.N. Pereira. </author> <title> Similarity-based estimation of word cooccurence probabilities. </title> <booktitle> In Proceedings of the Association for Computational Linguistics, </booktitle> <year> 1993. </year>
Reference-contexts: Complexity considerations are in particular relevant for the type of data investigated in this paper which is best described by the term co-occurrence data (COD) <ref> [30, 9] </ref>. The general setting is as follows: Suppose two finite sets X = fx 1 ; : : : ; x N g and Y = fy 1 ; : : : ; y M g of abstract objects with arbitrary labeling are given. <p> Prominent techniques are, for example, the back-off method [30] which makes use of simpler lower order models and model interpolation with held-out data [28, 27]. Another class of methods are similarity-based local smoothing techniques as, e.g., proposed by Essen and Steinbiss [14] and Dagan et al. <ref> [9, 10] </ref>. An empirical comparison of smoothing techniques can be found in [7]. In information retrieval, there have been essentially two proposals to overcome the sparseness problem. <p> full potential in the context of information retrieval is beyond the scope of this paper and will be pursued in future work. 6.2 Computational Linguistics In computational linguistics, the statistical analysis of word co-occurrences in lexical structures like adjective/noun or verb/direct object has recently received a considerable degree of attention <ref> [22, 43, 9, 10] </ref>. Potential applications of these methods are in word-sense disambiguation, a problem which occurs in different linguistic tasks ranging from parsing and tagging to machine translation.
Reference: [10] <author> I. Dagan, L. Lee, and F.C.N. Pereira. </author> <title> Similarity-based methods for word sense disambiguation. </title> <booktitle> In Proceedings of the Association for Computational Linguistics, </booktitle> <year> 1997. </year>
Reference-contexts: Or consider an application in computational linguistics, where the two sets correspond to words being part of a binary syntactic structure such as verbs with direct objects or nouns with corresponding adjectives <ref> [22, 43, 10] </ref>. In computer vision, X may correspond to image locations and Y to (discretized or categorical) feature values. The local histograms n jji in an image neighborhood around x i can then be utilized for a subsequent image segmentation [24]. <p> Prominent techniques are, for example, the back-off method [30] which makes use of simpler lower order models and model interpolation with held-out data [28, 27]. Another class of methods are similarity-based local smoothing techniques as, e.g., proposed by Essen and Steinbiss [14] and Dagan et al. <ref> [9, 10] </ref>. An empirical comparison of smoothing techniques can be found in [7]. In information retrieval, there have been essentially two proposals to overcome the sparseness problem. <p> full potential in the context of information retrieval is beyond the scope of this paper and will be pursued in future work. 6.2 Computational Linguistics In computational linguistics, the statistical analysis of word co-occurrences in lexical structures like adjective/noun or verb/direct object has recently received a considerable degree of attention <ref> [22, 43, 9, 10] </ref>. Potential applications of these methods are in word-sense disambiguation, a problem which occurs in different linguistic tasks ranging from parsing and tagging to machine translation.
Reference: [11] <author> P. Dayan, G.E. Hinton, R.M. Neal, </author> <title> and R.S. Zemel. The Helmholtz machine. </title> <journal> Neural Computation, </journal> <volume> 7(5) </volume> <pages> 889-904, </pages> <year> 1995. </year>
Reference-contexts: Thus the HACM incorporates a novel notion of hierarchical modeling, which differs from multiresolution approaches, but also from other hierarchical concepts of unsupervised learning (e.g. <ref> [11] </ref>). It offers several new possibilities in data analysis and information retrieval tasks like extracting resolution-dependent meaningful keywords for subcollection of documents and gives a satisfying solution to the problem of cluster summarization (cf. [8]) since it explicitly finds the most characteristic terms for each (super-)cluster of documents.
Reference: [12] <author> S. Deerwester, S. T. Dumais, T. K. Landauer, G. W. Furnas, and R. A. Harshman. </author> <title> Indexing by latent semantics analysis. </title> <journal> Journal of the American Society for Information Science, </journal> <volume> 41(6) </volume> <pages> 391-407, </pages> <year> 1991. </year>
Reference-contexts: The by far most popular technique in this category is Salton's Vector Space Model [51, 58, 52] of which different variants have been proposed with different word weighting schemes [53]. A more recent variant known as latent semantics indexing <ref> [12] </ref> performs a dimension reduction by singular value decomposition. Related methods of feature selection have been proposed for text categorization, e.g., the term strength criterion [66].
Reference: [13] <author> A.P. Dempster, N.M. Laird, and D.B. Rubin. </author> <title> Maximum likelihood from incomplete data via the EM algorithm. </title> <journal> Journal of the Royal Statistical Society B, </journal> <volume> 39 </volume> <pages> 1-38, </pages> <year> 1977. </year>
Reference-contexts: The canonical way of complexity control is to vary the number of components in the mixture. Yet, we will introduce a different technique to avoid overfitting problems which relies on an annealed generalization of the classical EM algorithm <ref> [13] </ref>. As we will argue, annealed EM has some additional advantages making it an important tool for fitting mix 1 Word n-gram models are examples of such higher order co-occurrence data. 1 ture models. Moreover, mixture models are a natural framework for unifying statistical inference and clustering. <p> To overcome the difficulties in maximizing a log of a sum, a set of unobserved variables is introduced and the corresponding EM algorithm <ref> [13, 36] </ref> is derived. <p> The EM algorithm is known to increase the likelihood in every step and converges to a (local) maximum of L under mild assumptions, cf. <ref> [13, 35, 38, 36] </ref>. Denote by R rff an indicator variable to represent the unknown class C ff from which the observation (x i (r) ; y j (r) ; r) 2 S was generated.
Reference: [14] <author> U. Essen and V. Steinbiss. </author> <title> Cooccurrence smoothing for stochastic language modeling. </title> <booktitle> In Proceedings of the IEEE nternational Conference on Acoustics, S peech, and Signal Processing, </booktitle> <pages> pages 161-164, </pages> <year> 1992. </year>
Reference-contexts: Prominent techniques are, for example, the back-off method [30] which makes use of simpler lower order models and model interpolation with held-out data [28, 27]. Another class of methods are similarity-based local smoothing techniques as, e.g., proposed by Essen and Steinbiss <ref> [14] </ref> and Dagan et al. [9, 10]. An empirical comparison of smoothing techniques can be found in [7]. In information retrieval, there have been essentially two proposals to overcome the sparseness problem.
Reference: [15] <author> Davi Geiger and Federico Girosi. </author> <title> Coupled markov random fields and mean field theory. </title> <booktitle> In Advances in Neural Information Processing Systems 2, </booktitle> <pages> pages 660-667, </pages> <year> 1990. </year>
Reference-contexts: This variational technique is known as mean-field approximation [42, 2] and has been successfully applied for optimization problems [61, 23], in computer vision <ref> [15, 67, 24] </ref>, and for inference in graphical models [55]. In general, solutions of the mean-field approximation have to fulfill the stationary conditions hR rff i = Z 1 hH (R; S; ; R rff = 1)i (66) where expectations are taken with respect to P (Rjp rff ) [24].
Reference: [16] <author> D. Geman, S. Geman, C. Graffigne, and P. Dong. </author> <title> Boundary detection by constrained optimization. </title> <journal> IEEE Transactions on Pattern Analysis and Machine Intelligence, </journal> <volume> 12(7) </volume> <pages> 609-628, </pages> <year> 1990. </year>
Reference-contexts: Numerous approaches to texture segmentation have been proposed over the past decades, most of which obey a two-stage scheme: 1. A modeling stage: characteristic features are extracted from the textured input image, which range from spatial frequencies [25, 24], MRF-models [33, 34], co-occurrence matrices <ref> [16] </ref> to fractal in dices [6]. 2. In the clustering stage features are grouped into homogeneous segments, where homogeneity of features has to be formalized by a mathematical no tion of similarity. <p> K-means clustering can be understood as a statistical mixture model with isotropic Gaussian class distributions. Occasionally, the grouping process has been based on pairwise similarity measurements between image sites, where similarity is measured by a non-parametric statistical test applied to the feature distribution of a surrounding neighborhood <ref> [16, 39, 24] </ref>. Agglomerative techniques [39] and, more rigorously, optimization approaches [24, 57] have been developed and applied for the grouping of similarity data in the texture segmentation context.
Reference: [17] <author> W.R. Gilks, S. Richardson, </author> <title> and D.J. Spiegelhal-ter, editors. Markov chain Monte Carlo in practice. </title> <publisher> Chapman & Hall, </publisher> <year> 1997. </year> <month> 19 </month>
Reference-contexts: To distinguish more clearly between the different models proposed in the sequel, a representation in terms of directed graphical models (belief networks) is utilized. In this formalism, random variables as well as parameters are represented as nodes in a directed acyclic graph (cf. <ref> [41, 32, 17] </ref> for the general semantics of graphical models). Nodes of observed quantities are shaded and a number of i.i.d. observations is represented by a frame with a number in the corner to indicate the number of observations (called a plate).
Reference: [18] <author> I.J. </author> <title> Good. The Estimation of Probabilities. Re--search Monograph 30. </title> <publisher> MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1965. </year>
Reference-contexts: However, the intrinsic problem of COD is that of data sparseness, also known as the zero frequency problem <ref> [19, 18, 30, 64] </ref>. When N and M are very large, a majority of pairs (x i ; y j ) only have a small probability of occurring together in S.
Reference: [19] <author> I.J. </author> <title> Good. The population frequencies of species and the estimation of population parameters. </title> <journal> Biometrika, </journal> <volume> 40(3) </volume> <pages> 237-264, </pages> <year> 1991. </year>
Reference-contexts: However, the intrinsic problem of COD is that of data sparseness, also known as the zero frequency problem <ref> [19, 18, 30, 64] </ref>. When N and M are very large, a majority of pairs (x i ; y j ) only have a small probability of occurring together in S.
Reference: [20] <author> A. Griffiths, H.C. Luckhurst, and P. Willett. </author> <title> Using interdocument similarity information in document retrieval systems. </title> <journal> Journal of the American Society for Information Science, </journal> <volume> 37 </volume> <pages> 3-11, </pages> <year> 1986. </year>
Reference-contexts: An empirical comparison of smoothing techniques can be found in [7]. In information retrieval, there have been essentially two proposals to overcome the sparseness problem. The first class of methods relies on the cluster hypothesis <ref> [62, 20] </ref> which suggests to make use of inter-document similarities in order to improve the retrieval performance. Since it is often prohibitive to compute all pairwise similarities between documents these methods typically rely on random comparisons or random fractionation [8].
Reference: [21] <author> F. Heitz, P. Perez, and P. Bouthemy. </author> <title> Multiscale minimization of global energy functions in some visual recovery problems. CVGIP: </title> <booktitle> Image Understanding, </booktitle> <volume> 59(1) </volume> <pages> 125-134, </pages> <year> 1994. </year>
Reference-contexts: In case that a constraint is violated after performing an overrelaxed M-step, the parameter set is projected back on the admissible parameter space. For an overview on more elab orated acceleration methods for EM we refer to [36]. 5.4 Multiscale Optimization Multiscale optimization <ref> [21, 46] </ref> is an approach for accelerating clustering algorithms whenever a topological structure exists on the object space (s). In image segmentation, for example, it is a natural assumption that adjacent image sites belong with high probability to the same cluster or image segment.
Reference: [22] <author> D. Hindle. </author> <title> Noun classification from predicate-argument structures. </title> <booktitle> In Proceedings of the Association for Computational Linguistics, </booktitle> <pages> pages 268-275, </pages> <year> 1990. </year>
Reference-contexts: Or consider an application in computational linguistics, where the two sets correspond to words being part of a binary syntactic structure such as verbs with direct objects or nouns with corresponding adjectives <ref> [22, 43, 10] </ref>. In computer vision, X may correspond to image locations and Y to (discretized or categorical) feature values. The local histograms n jji in an image neighborhood around x i can then be utilized for a subsequent image segmentation [24]. <p> full potential in the context of information retrieval is beyond the scope of this paper and will be pursued in future work. 6.2 Computational Linguistics In computational linguistics, the statistical analysis of word co-occurrences in lexical structures like adjective/noun or verb/direct object has recently received a considerable degree of attention <ref> [22, 43, 9, 10] </ref>. Potential applications of these methods are in word-sense disambiguation, a problem which occurs in different linguistic tasks ranging from parsing and tagging to machine translation.
Reference: [23] <author> T. Hofmann and J.M. Buhmann. </author> <title> Pairwise data clustering by deterministic annealing. </title> <journal> IEEE Transactions on Pattern Analysis and Machine Intelligence, </journal> <volume> 19(1), </volume> <year> 1997. </year>
Reference-contexts: Moreover, mixture models are a natural framework for unifying statistical inference and clustering. This is particularly important, since one is often interested in discovering structure, typically represented by groups of similar objects as in pairwise data clustering <ref> [23] </ref>. The major advantage of clustering based on COD compared to similarity-based clustering is the fact that it does not require an external similarity measure, but exclusively relies on the objects occurrence statistics. <p> The framework which allows us to improve the presented EM procedures in both aspects is known as deterministic annealing. Deterministic annealing has been applied to many clustering problems, including vectorial clustering [49, 50, 5], pairwise clustering <ref> [23] </ref>, and in the context of COD for distributional clustering [43]. The key idea is to introduce a temperature parameter T and to replace the minimization of a combinatorial objective function by a substitute known as the free energy. Details on this topic are given in Appendix A. <p> This variational technique is known as mean-field approximation [42, 2] and has been successfully applied for optimization problems <ref> [61, 23] </ref>, in computer vision [15, 67, 24], and for inference in graphical models [55].
Reference: [24] <author> T. Hofmann, J. Puzicha, and J.M. Buhmann. </author> <title> Deterministic annealing for unsupervised texture segmentation. </title> <booktitle> In Proceedings of the International Workshop on Energy Minimization Methods in Computer Vision and Pattern Recognition, volume 1223 of Lecture Notes in Computer Science, </booktitle> <pages> pages 213-228, </pages> <month> May </month> <year> 1997. </year>
Reference-contexts: In computer vision, X may correspond to image locations and Y to (discretized or categorical) feature values. The local histograms n jji in an image neighborhood around x i can then be utilized for a subsequent image segmentation <ref> [24] </ref>. Many more examples from data mining, molecular biology, preference analysis, etc. could be enumerated here to stress that analyzing co-occurrences of events is in fact a very general and fundamental problem of unsupervised learning. In this contribution a general statistical framework for COD is presented. <p> Numerous approaches to texture segmentation have been proposed over the past decades, most of which obey a two-stage scheme: 1. A modeling stage: characteristic features are extracted from the textured input image, which range from spatial frequencies <ref> [25, 24] </ref>, MRF-models [33, 34], co-occurrence matrices [16] to fractal in dices [6]. 2. In the clustering stage features are grouped into homogeneous segments, where homogeneity of features has to be formalized by a mathematical no tion of similarity. <p> K-means clustering can be understood as a statistical mixture model with isotropic Gaussian class distributions. Occasionally, the grouping process has been based on pairwise similarity measurements between image sites, where similarity is measured by a non-parametric statistical test applied to the feature distribution of a surrounding neighborhood <ref> [16, 39, 24] </ref>. Agglomerative techniques [39] and, more rigorously, optimization approaches [24, 57] have been developed and applied for the grouping of similarity data in the texture segmentation context. <p> Occasionally, the grouping process has been based on pairwise similarity measurements between image sites, where similarity is measured by a non-parametric statistical test applied to the feature distribution of a surrounding neighborhood [16, 39, 24]. Agglomerative techniques [39] and, more rigorously, optimization approaches <ref> [24, 57] </ref> have been developed and applied for the grouping of similarity data in the texture segmentation context. Pairwise similarity clustering thus provides an indirect way to group (discrete) feature distributions without reducing information in a distribution to their mean. <p> This variational technique is known as mean-field approximation [42, 2] and has been successfully applied for optimization problems [61, 23], in computer vision <ref> [15, 67, 24] </ref>, and for inference in graphical models [55]. In general, solutions of the mean-field approximation have to fulfill the stationary conditions hR rff i = Z 1 hH (R; S; ; R rff = 1)i (66) where expectations are taken with respect to P (Rjp rff ) [24]. <p> In general, solutions of the mean-field approximation have to fulfill the stationary conditions hR rff i = Z 1 hH (R; S; ; R rff = 1)i (66) where expectations are taken with respect to P (Rjp rff ) <ref> [24] </ref>. Notice that expected costs appear in the exponent, however the expectation is taken with respect to all hidden variables except R rff itself which is fixed.
Reference: [25] <author> A. Jain and F. Farrokhnia. </author> <title> Unsupervised texture segmentation using Gabor filters. </title> <journal> Pattern Recognition, </journal> <volume> 24(12) </volume> <pages> 1167-1186, </pages> <year> 1991. </year>
Reference-contexts: Numerous approaches to texture segmentation have been proposed over the past decades, most of which obey a two-stage scheme: 1. A modeling stage: characteristic features are extracted from the textured input image, which range from spatial frequencies <ref> [25, 24] </ref>, MRF-models [33, 34], co-occurrence matrices [16] to fractal in dices [6]. 2. In the clustering stage features are grouped into homogeneous segments, where homogeneity of features has to be formalized by a mathematical no tion of similarity. <p> In the clustering stage features are grouped into homogeneous segments, where homogeneity of features has to be formalized by a mathematical no tion of similarity. Most widely, features are interpreted as vectors in a Eu-clidean space <ref> [25, 33, 34, 65, 40, 6, 31] </ref> and a segmentation is obtained by minimizing the K-means criterion, which sums over the square distances between feature vectors and their assigned, group-specific prototype feature vectors. K-means clustering can be understood as a statistical mixture model with isotropic Gaussian class distributions. <p> The sample set S i for site x i consists of all Gabor responses in a window centered at x i , where the size of the window is chosen proportional to the filter scale <ref> [25] </ref>. Hence each image location x i is effectively characterized by 12 one-dimensional histograms over Gabor coefficients. We have applied the ACM-based texture segmentation algorithm to a collection of textured images. Fig. 13 shows exemplary results for images which were randomly generated from the Brodatz texture collection of micro-textures.
Reference: [26] <author> A.K. Jain and R.C. Dubes. </author> <title> Algorithms for Clustering Data. </title> <publisher> Prentice Hall, </publisher> <address> Englewood Cliffs, NJ 07632, </address> <year> 1988. </year>
Reference-contexts: Both types of clustering approaches, for the set of documents as well as for the keywords, have been proposed in the literature. The most frequently used methods in this context are linkage algorithms (single linkage, complete linkage, Wards method, cf. <ref> [26] </ref>), or hybrid combinations of ag 12 on the Cranfield collection. different temperatures on the Cranfield collection. glomerative and centroid-based methods [8] which have no probabilistic interpretation and have a number of other disadvantages.
Reference: [27] <author> F. Jelinek. </author> <title> The development of an experimental discrete dictation recogniser. </title> <booktitle> Proceedings of the IEEE, </booktitle> <volume> 73(11), </volume> <year> 1985. </year>
Reference-contexts: Typical state-of-the-art techniques in natural language processing apply smoothing techniques to deal with zero frequencies of unobserved events. Prominent techniques are, for example, the back-off method [30] which makes use of simpler lower order models and model interpolation with held-out data <ref> [28, 27] </ref>. Another class of methods are similarity-based local smoothing techniques as, e.g., proposed by Essen and Steinbiss [14] and Dagan et al. [9, 10]. An empirical comparison of smoothing techniques can be found in [7]. In information retrieval, there have been essentially two proposals to overcome the sparseness problem.
Reference: [28] <author> F. Jelinek and R. Mercer. </author> <title> Interpolated estimation of Markov source parameters from sparse data. </title> <booktitle> In Proceedings of the Workshop of Pattern Recognition in Practice, </booktitle> <year> 1980. </year>
Reference-contexts: Typical state-of-the-art techniques in natural language processing apply smoothing techniques to deal with zero frequencies of unobserved events. Prominent techniques are, for example, the back-off method [30] which makes use of simpler lower order models and model interpolation with held-out data <ref> [28, 27] </ref>. Another class of methods are similarity-based local smoothing techniques as, e.g., proposed by Essen and Steinbiss [14] and Dagan et al. [9, 10]. An empirical comparison of smoothing techniques can be found in [7]. In information retrieval, there have been essentially two proposals to overcome the sparseness problem.
Reference: [29] <author> M.I. Jordan and R.A. Jacobs. </author> <title> Hierarchical mixtures of experts and the EM algorithm. </title> <journal> Neural Computation, </journal> <volume> 6(2) </volume> <pages> 181-214, </pages> <year> 1994. </year>
Reference-contexts: it is straightforward to check that the predictive probabilities for s = (x i ; y j ; L + 1) are given by P (sjS; ) = p i - X hI iff it -jff;i : (59) 4.3 Hierarchies and Abstraction With other hierarchical mixture models proposed for supervised <ref> [3, 29] </ref> and unsupervised learning [37] the HACM shares the organization of clusters in a tree structure. It extracts hierarchical relations between clusters, i.e., it breaks the permutation-symmetry of the cluster labeling. Even more important, however, it is capable to perform statistical abstraction.
Reference: [30] <author> S.M. Katz. </author> <title> Estimation of probabilities for sparse data for the language model component of a speech recogniser. </title> <journal> ASSP, </journal> <volume> 35(3) </volume> <pages> 400-401, </pages> <year> 1987. </year>
Reference-contexts: Complexity considerations are in particular relevant for the type of data investigated in this paper which is best described by the term co-occurrence data (COD) <ref> [30, 9] </ref>. The general setting is as follows: Suppose two finite sets X = fx 1 ; : : : ; x N g and Y = fy 1 ; : : : ; y M g of abstract objects with arbitrary labeling are given. <p> However, the intrinsic problem of COD is that of data sparseness, also known as the zero frequency problem <ref> [19, 18, 30, 64] </ref>. When N and M are very large, a majority of pairs (x i ; y j ) only have a small probability of occurring together in S. <p> Typical state-of-the-art techniques in natural language processing apply smoothing techniques to deal with zero frequencies of unobserved events. Prominent techniques are, for example, the back-off method <ref> [30] </ref> which makes use of simpler lower order models and model interpolation with held-out data [28, 27]. Another class of methods are similarity-based local smoothing techniques as, e.g., proposed by Essen and Steinbiss [14] and Dagan et al. [9, 10].
Reference: [31] <author> A. Laine and J. Fan. </author> <title> Frame representations for texture segmentation. </title> <journal> IEEE Transactions on Image Processing, </journal> <volume> 5(5) </volume> <pages> 771-779, </pages> <year> 1996. </year>
Reference-contexts: In the clustering stage features are grouped into homogeneous segments, where homogeneity of features has to be formalized by a mathematical no tion of similarity. Most widely, features are interpreted as vectors in a Eu-clidean space <ref> [25, 33, 34, 65, 40, 6, 31] </ref> and a segmentation is obtained by minimizing the K-means criterion, which sums over the square distances between feature vectors and their assigned, group-specific prototype feature vectors. K-means clustering can be understood as a statistical mixture model with isotropic Gaussian class distributions.
Reference: [32] <author> S.L. Lauritzen, </author> <title> editor. Graphical models. </title> <publisher> Clarendon Press & Oxford University Press, </publisher> <year> 1996. </year>
Reference-contexts: To distinguish more clearly between the different models proposed in the sequel, a representation in terms of directed graphical models (belief networks) is utilized. In this formalism, random variables as well as parameters are represented as nodes in a directed acyclic graph (cf. <ref> [41, 32, 17] </ref> for the general semantics of graphical models). Nodes of observed quantities are shaded and a number of i.i.d. observations is represented by a frame with a number in the corner to indicate the number of observations (called a plate).
Reference: [33] <author> B. Manjunath and R. Chellappa. </author> <title> Unsupervised texture segmentation using Markov random field models. </title> <journal> IEEE Transactions on Pattern Analysis and Machine Intelligence, </journal> <volume> 13 </volume> <pages> 478-482, </pages> <year> 1991. </year>
Reference-contexts: Numerous approaches to texture segmentation have been proposed over the past decades, most of which obey a two-stage scheme: 1. A modeling stage: characteristic features are extracted from the textured input image, which range from spatial frequencies [25, 24], MRF-models <ref> [33, 34] </ref>, co-occurrence matrices [16] to fractal in dices [6]. 2. In the clustering stage features are grouped into homogeneous segments, where homogeneity of features has to be formalized by a mathematical no tion of similarity. <p> In the clustering stage features are grouped into homogeneous segments, where homogeneity of features has to be formalized by a mathematical no tion of similarity. Most widely, features are interpreted as vectors in a Eu-clidean space <ref> [25, 33, 34, 65, 40, 6, 31] </ref> and a segmentation is obtained by minimizing the K-means criterion, which sums over the square distances between feature vectors and their assigned, group-specific prototype feature vectors. K-means clustering can be understood as a statistical mixture model with isotropic Gaussian class distributions.
Reference: [34] <author> J. Mao and A. Jain. </author> <title> Texture classification and segmentation using multiresolution simultaneous au-toregressive models. </title> <journal> Pattern Recognition, </journal> <volume> 25 </volume> <pages> 173-188, </pages> <year> 1992. </year>
Reference-contexts: Numerous approaches to texture segmentation have been proposed over the past decades, most of which obey a two-stage scheme: 1. A modeling stage: characteristic features are extracted from the textured input image, which range from spatial frequencies [25, 24], MRF-models <ref> [33, 34] </ref>, co-occurrence matrices [16] to fractal in dices [6]. 2. In the clustering stage features are grouped into homogeneous segments, where homogeneity of features has to be formalized by a mathematical no tion of similarity. <p> In the clustering stage features are grouped into homogeneous segments, where homogeneity of features has to be formalized by a mathematical no tion of similarity. Most widely, features are interpreted as vectors in a Eu-clidean space <ref> [25, 33, 34, 65, 40, 6, 31] </ref> and a segmentation is obtained by minimizing the K-means criterion, which sums over the square distances between feature vectors and their assigned, group-specific prototype feature vectors. K-means clustering can be understood as a statistical mixture model with isotropic Gaussian class distributions.
Reference: [35] <author> G.J. McLachlan and K. E. Basford. </author> <title> Mixture Models. </title> <publisher> Marcel Dekker, INC, </publisher> <address> New York Basel, </address> <year> 1988. </year>
Reference-contexts: Related methods of feature selection have been proposed for text categorization, e.g., the term strength criterion [66]. In contrast, we propose a model-based statistical approach and present a family of finite mixture models <ref> [59, 35] </ref> as a way to deal with the data sparseness problem. Since mixture or class-based models can also be combined with other models our goal is orthogonal to standard interpolation techniques. <p> The EM algorithm is known to increase the likelihood in every step and converges to a (local) maximum of L under mild assumptions, cf. <ref> [13, 35, 38, 36] </ref>. Denote by R rff an indicator variable to represent the unknown class C ff from which the observation (x i (r) ; y j (r) ; r) 2 S was generated. <p> In fact, this interpretation is consistent with other common mixture models <ref> [35, 59] </ref> and might be preferred in the context of statistical modeling, in particular if N scales with L.
Reference: [36] <author> G.J. McLachlan and T. Krishnan. </author> <title> The EM Algorithm and Extensions. </title> <publisher> Wiley, </publisher> <address> New York, </address> <year> 1997. </year>
Reference-contexts: To overcome the difficulties in maximizing a log of a sum, a set of unobserved variables is introduced and the corresponding EM algorithm <ref> [13, 36] </ref> is derived. <p> The EM algorithm is known to increase the likelihood in every step and converges to a (local) maximum of L under mild assumptions, cf. <ref> [13, 35, 38, 36] </ref>. Denote by R rff an indicator variable to represent the unknown class C ff from which the observation (x i (r) ; y j (r) ; r) 2 S was generated. <p> In case that a constraint is violated after performing an overrelaxed M-step, the parameter set is projected back on the admissible parameter space. For an overview on more elab orated acceleration methods for EM we refer to <ref> [36] </ref>. 5.4 Multiscale Optimization Multiscale optimization [21, 46] is an approach for accelerating clustering algorithms whenever a topological structure exists on the object space (s). In image segmentation, for example, it is a natural assumption that adjacent image sites belong with high probability to the same cluster or image segment.
Reference: [37] <author> D. Miller and K. Rose. </author> <title> Hierarchical, unsupervised learning with growing via phase transitions. </title> <journal> Neural Computation, </journal> <volume> 8(8) </volume> <pages> 425-450, </pages> <year> 1996. </year>
Reference-contexts: that the predictive probabilities for s = (x i ; y j ; L + 1) are given by P (sjS; ) = p i - X hI iff it -jff;i : (59) 4.3 Hierarchies and Abstraction With other hierarchical mixture models proposed for supervised [3, 29] and unsupervised learning <ref> [37] </ref> the HACM shares the organization of clusters in a tree structure. It extracts hierarchical relations between clusters, i.e., it breaks the permutation-symmetry of the cluster labeling. Even more important, however, it is capable to perform statistical abstraction.
Reference: [38] <author> R.M. Neal and G.E. Hinton. </author> <title> A new view of the EM algorithm that justifies incremental and other variants. </title> <address> Biometrica, </address> <year> 1993. </year> <note> submitted. </note>
Reference-contexts: The EM algorithm is known to increase the likelihood in every step and converges to a (local) maximum of L under mild assumptions, cf. <ref> [13, 35, 38, 36] </ref>. Denote by R rff an indicator variable to represent the unknown class C ff from which the observation (x i (r) ; y j (r) ; r) 2 S was generated. <p> In addition, for fixed T &gt; 1 the annealed E-step performs a regularization based on entropy, because the posterior probabilities minimize the generalized free energy at T = 1 which balances expected costs and (relative) entropy <ref> [38] </ref> (cf. Appendix A). This is the reason why annealed EM not only reduces the sensitivity to local minima but also controls the effective model complexity. It thereby has the potential to improve the generalization for otherwise overfitting models. The advantages of deterministic annealing are investigated experimentally in Section 6. <p> Appendix A First, we establish an important relationship between the log-likelihood and a quantity known as free energy in statistical physics <ref> [38] </ref>. Consider the data log-likelihood L = log P (Sj; R) as a function of the discrete hidden states R over R for fixed parameters, and let H (R; S; ) = L define a cost function on the hidden variable space.
Reference: [39] <author> T. Ojala and M. Pietikainen. </author> <title> Unsupervised texture segmentation using feature distributions. </title> <type> Technical Report CAR-TR-837, </type> <institution> Center for Automation Research, University of Maryland, </institution> <year> 1996. </year>
Reference-contexts: K-means clustering can be understood as a statistical mixture model with isotropic Gaussian class distributions. Occasionally, the grouping process has been based on pairwise similarity measurements between image sites, where similarity is measured by a non-parametric statistical test applied to the feature distribution of a surrounding neighborhood <ref> [16, 39, 24] </ref>. Agglomerative techniques [39] and, more rigorously, optimization approaches [24, 57] have been developed and applied for the grouping of similarity data in the texture segmentation context. <p> Occasionally, the grouping process has been based on pairwise similarity measurements between image sites, where similarity is measured by a non-parametric statistical test applied to the feature distribution of a surrounding neighborhood [16, 39, 24]. Agglomerative techniques <ref> [39] </ref> and, more rigorously, optimization approaches [24, 57] have been developed and applied for the grouping of similarity data in the texture segmentation context. Pairwise similarity clustering thus provides an indirect way to group (discrete) feature distributions without reducing information in a distribution to their mean.
Reference: [40] <author> D. Panjwani and G. Healey. </author> <title> Markov random field models for unsupervised segmentation of textured color images. </title> <journal> IEEE Transactions on Pattern Analysis and Machine Intelligence, </journal> <volume> 17(10) </volume> <pages> 939-954, </pages> <year> 1995. </year>
Reference-contexts: In the clustering stage features are grouped into homogeneous segments, where homogeneity of features has to be formalized by a mathematical no tion of similarity. Most widely, features are interpreted as vectors in a Eu-clidean space <ref> [25, 33, 34, 65, 40, 6, 31] </ref> and a segmentation is obtained by minimizing the K-means criterion, which sums over the square distances between feature vectors and their assigned, group-specific prototype feature vectors. K-means clustering can be understood as a statistical mixture model with isotropic Gaussian class distributions.
Reference: [41] <author> J. Pearl. </author> <title> Probabilistic reasoning in intelligent systems: networks of plausible inference. </title> <publisher> Morgan Kaufmann Publishers, </publisher> <address> San Mateo, CA, </address> <year> 1988. </year>
Reference-contexts: To distinguish more clearly between the different models proposed in the sequel, a representation in terms of directed graphical models (belief networks) is utilized. In this formalism, random variables as well as parameters are represented as nodes in a directed acyclic graph (cf. <ref> [41, 32, 17] </ref> for the general semantics of graphical models). Nodes of observed quantities are shaded and a number of i.i.d. observations is represented by a frame with a number in the corner to indicate the number of observations (called a plate).
Reference: [42] <author> R. E. Peierls. </author> <title> On a minimum property of the free energy. </title> <journal> Physical Review, </journal> <volume> 54:918, </volume> <year> 1938. </year>
Reference-contexts: We make use of the more suggestive notation hR rff i = p rff to stress that the variational parameters p rff can actually be thought of as an approximation of the posterior marginals. This variational technique is known as mean-field approximation <ref> [42, 2] </ref> and has been successfully applied for optimization problems [61, 23], in computer vision [15, 67, 24], and for inference in graphical models [55].
Reference: [43] <author> F.C.N. Pereira, N.Z. Tishby, and L. Lee. </author> <title> Distributional clustering of english words. </title> <booktitle> In Proceedings of the Association for Computational Linguistics, </booktitle> <pages> pages 183-190, </pages> <year> 1993. </year>
Reference-contexts: Or consider an application in computational linguistics, where the two sets correspond to words being part of a binary syntactic structure such as verbs with direct objects or nouns with corresponding adjectives <ref> [22, 43, 10] </ref>. In computer vision, X may correspond to image locations and Y to (discretized or categorical) feature values. The local histograms n jji in an image neighborhood around x i can then be utilized for a subsequent image segmentation [24]. <p> Since the models are directly applicable to co-occurrence and histogram data, the necessity for pairwise comparisons is avoided altogether. Probabilistic models for COD have recently been investigated under the titles of class-based n-gram models [4], distributional clustering <ref> [43] </ref>, and aggregate Markov models [54] in natural language processing. All three approaches are recovered as special cases in our COD framework and we will clarify the relation to our approach in the following sections. <p> probability of R rff for a given parameter estimate ^ (t) (E-step) is computed by exploiting Bayes' rule and is in general obtained by hR rff i P (R rff = 1j; S) 2 The joint probability model in (1) was the starting point for the distributional clustering algorithm in <ref> [43] </ref>, however the authors have in fact restricted their investigations to the (asymmetric) clustering model (cf. <p> jji ; q jji ff p ffji q jjff : (11) Hence a specific conditional distribution q jji defined on Y is associated with each object x i , which can be understood as a linear combination of the prototypical conditional distributions q jjff weighted with probabilities p ffji (cf. <ref> [43] </ref>). Notice, that although p ffji defines a probabilistic assignment of objects to classes, these probabilities are not induced by the uncertainty of a hidden class membership of object x i as is typically the case in mixture models. <p> The update scheme to solve the likelihood equations is structurally very similar to the K-means algorithm: calculate assignments for given centroids according to the nearest neighbor rule and recalculate the centroid distributions in alternation. The ACM is in fact similar to the distributional clustering model proposed in <ref> [43] </ref> as the minimization of H = i=1 -=1 fi fl In distributional clustering, the KL-divergence as a distortion measure for distributions has been motivated by the fact that the centroid equation (24) is satisfied at stationary points 3 . <p> In the ACM all contributions first enter a huge product. In particular, for n i ! 1 the posteriors hI iff i approach Boolean values and automatically result in a hard partitioning of X . Compared to the original distributional clustering model as proposed in <ref> [43] </ref> our maximum likelihood approach naturally includes additional parameters for the mixing proportions . Notice that in this model, to which we refer as probabilistic ACM, different observations involving the same object x i are actually not independent, even if conditioned on the parameters (; p; q). <p> The framework which allows us to improve the presented EM procedures in both aspects is known as deterministic annealing. Deterministic annealing has been applied to many clustering problems, including vectorial clustering [49, 50, 5], pairwise clustering [23], and in the context of COD for distributional clustering <ref> [43] </ref>. The key idea is to introduce a temperature parameter T and to replace the minimization of a combinatorial objective function by a substitute known as the free energy. Details on this topic are given in Appendix A. Here, we present annealing methods without reference to statistical physics. <p> To summarize, annealed EM solves three problems at once: 1. It avoids unfavorable local minima by applying a temperature based continuation method, as the modified likelihood becomes convex at high tem perature, 10 Eq. (60) differs from the original formula by Pereira et al. <ref> [43] </ref> in that it scales the temperature with the frequency n i and includes the mixing proportions. As pointed out before this is naturally obtained in the ML framework, while in the distributional clustering cost function (25) the weights n i are not considered. <p> full potential in the context of information retrieval is beyond the scope of this paper and will be pursued in future work. 6.2 Computational Linguistics In computational linguistics, the statistical analysis of word co-occurrences in lexical structures like adjective/noun or verb/direct object has recently received a considerable degree of attention <ref> [22, 43, 9, 10] </ref>. Potential applications of these methods are in word-sense disambiguation, a problem which occurs in different linguistic tasks ranging from parsing and tagging to machine translation.
Reference: [44] <author> B.C. Peters and H.F. Walker. </author> <title> An iterative procedure for obtaining maximum-likelihood estimates of the parameters of a mixture of normal distribution. </title> <journal> SIAM Journal of Applied Mathematics, </journal> <volume> 35 </volume> <pages> 362-378, </pages> <year> 1978. </year>
Reference-contexts: A simple way to accelerate EM algorithms is by overrelaxation in the M-step. This has been discussed early in the context of mixture models <ref> [44, 45] </ref> and was `rediscovered' more recently under the title of EM () in [1]. We found this method useful in accelerating the fitting procedure for all discussed models.
Reference: [45] <author> B.C. Peters and H.F. Walker. </author> <title> The numerical evaluation of the maximum-likelihood estimates of a subset of mixture proportions. </title> <journal> SIAM Journal of Applied Mathematics, </journal> <volume> 35 </volume> <pages> 447-452, </pages> <year> 1978. </year>
Reference-contexts: A simple way to accelerate EM algorithms is by overrelaxation in the M-step. This has been discussed early in the context of mixture models <ref> [44, 45] </ref> and was `rediscovered' more recently under the title of EM () in [1]. We found this method useful in accelerating the fitting procedure for all discussed models.
Reference: [46] <author> J. Puzicha and J. Buhmann. </author> <title> Multiscale annealing for real-time unsupervised texture segmentation. </title> <type> Technical Report IAI-97-4, </type> <institution> Institut fur In-formatik III, </institution> <year> 1997. </year>
Reference-contexts: In case that a constraint is violated after performing an overrelaxed M-step, the parameter set is projected back on the admissible parameter space. For an overview on more elab orated acceleration methods for EM we refer to [36]. 5.4 Multiscale Optimization Multiscale optimization <ref> [21, 46] </ref> is an approach for accelerating clustering algorithms whenever a topological structure exists on the object space (s). In image segmentation, for example, it is a natural assumption that adjacent image sites belong with high probability to the same cluster or image segment.
Reference: [47] <author> J. Rissanen. </author> <title> Universal coding, information, prediction and estimation. </title> <journal> IEEE Transactions on Information Theory, </journal> <volume> 30(4) </volume> <pages> 629-636, </pages> <year> 1984. </year>
Reference-contexts: In order to serve as a useful explanation, the model should reduce the complexity of the raw data and has to offer a certain degree of simplification. In this sense statistical modeling is related to the information theoretic concept of minimum description length <ref> [47, 48] </ref>. A model is a `good' explanation for the given data if encoding the model and describing the data conditioned on that model yields a significant reduction in encoding complexity as compared to a `direct' encoding of the data.
Reference: [48] <author> J. Rissanen. </author> <title> Stochastic complexity and modeling. </title> <journal> The Annals of Statistics, </journal> <volume> 14(3) </volume> <pages> 1080-1100, </pages> <year> 1986. </year>
Reference-contexts: In order to serve as a useful explanation, the model should reduce the complexity of the raw data and has to offer a certain degree of simplification. In this sense statistical modeling is related to the information theoretic concept of minimum description length <ref> [47, 48] </ref>. A model is a `good' explanation for the given data if encoding the model and describing the data conditioned on that model yields a significant reduction in encoding complexity as compared to a `direct' encoding of the data.
Reference: [49] <author> K. Rose, E. Gurewitz, and G. Fox. </author> <title> Statistical mechanics and phase transitions in clustering. </title> <journal> Physical Review Letters, </journal> <volume> 65(8) </volume> <pages> 945-948, </pages> <year> 1990. </year>
Reference-contexts: The second, even more important problem is to avoid overfitting, i.e., maximize the performance on unseen future data. The framework which allows us to improve the presented EM procedures in both aspects is known as deterministic annealing. Deterministic annealing has been applied to many clustering problems, including vectorial clustering <ref> [49, 50, 5] </ref>, pairwise clustering [23], and in the context of COD for distributional clustering [43]. The key idea is to introduce a temperature parameter T and to replace the minimization of a combinatorial objective function by a substitute known as the free energy. <p> It thereby has the potential to improve the generalization for otherwise overfitting models. The advantages of deterministic annealing are investigated experimentally in Section 6. In addition, the annealed EM algorithm offers a way to generate tree topologies. As is known from adaptive vector quantization <ref> [49] </ref>, starting at a high value of T and successively lowering T leads through a sequence of phase transitions. At each phase transition the effective number of distinguishable clusters grows until some maximal number is reached or the annealing is stopped.
Reference: [50] <author> K. Rose, E. Gurewitz, and G. Fox. </author> <title> Vector quantization by deterministic annealing. </title> <journal> IEEE Transactions on Information Theory, </journal> <volume> 38(4) </volume> <pages> 1249-1257, </pages> <year> 1992. </year> <month> 20 </month>
Reference-contexts: The second, even more important problem is to avoid overfitting, i.e., maximize the performance on unseen future data. The framework which allows us to improve the presented EM procedures in both aspects is known as deterministic annealing. Deterministic annealing has been applied to many clustering problems, including vectorial clustering <ref> [49, 50, 5] </ref>, pairwise clustering [23], and in the context of COD for distributional clustering [43]. The key idea is to introduce a temperature parameter T and to replace the minimization of a combinatorial objective function by a substitute known as the free energy.
Reference: [51] <author> G. Salton. </author> <title> Experiments in automatic thesaurus con-struction for information retrieval. </title> <booktitle> In Proceedings IFIP Congress, TA-2, </booktitle> <pages> pages 43-49, </pages> <year> 1971. </year>
Reference-contexts: The second approach focuses on the index terms to derive an improved feature representation of documents. The by far most popular technique in this category is Salton's Vector Space Model <ref> [51, 58, 52] </ref> of which different variants have been proposed with different word weighting schemes [53]. A more recent variant known as latent semantics indexing [12] performs a dimension reduction by singular value decomposition.
Reference: [52] <author> G. Salton. </author> <title> Developments in automatic text retrieval. </title> <journal> Science, </journal> <volume> 253 </volume> <pages> 974-979, </pages> <year> 1991. </year>
Reference-contexts: The second approach focuses on the index terms to derive an improved feature representation of documents. The by far most popular technique in this category is Salton's Vector Space Model <ref> [51, 58, 52] </ref> of which different variants have been proposed with different word weighting schemes [53]. A more recent variant known as latent semantics indexing [12] performs a dimension reduction by singular value decomposition.
Reference: [53] <author> G. Salton and C. Buckley. </author> <title> Term weighting approaches in automatic text retrieval. </title> <booktitle> Information Processing and Management, </booktitle> <volume> 24(5) </volume> <pages> 513-523, </pages> <year> 1988. </year>
Reference-contexts: The second approach focuses on the index terms to derive an improved feature representation of documents. The by far most popular technique in this category is Salton's Vector Space Model [51, 58, 52] of which different variants have been proposed with different word weighting schemes <ref> [53] </ref>. A more recent variant known as latent semantics indexing [12] performs a dimension reduction by singular value decomposition. Related methods of feature selection have been proposed for text categorization, e.g., the term strength criterion [66].
Reference: [54] <author> L. Saul and F.C.N. Pereira. </author> <title> Aggregate and mixed-order Markov models for statistical language processing. </title> <booktitle> In Proceedings of the Second Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics, </booktitle> <year> 1997. </year>
Reference-contexts: Since the models are directly applicable to co-occurrence and histogram data, the necessity for pairwise comparisons is avoided altogether. Probabilistic models for COD have recently been investigated under the titles of class-based n-gram models [4], distributional clustering [43], and aggregate Markov models <ref> [54] </ref> in natural language processing. All three approaches are recovered as special cases in our COD framework and we will clarify the relation to our approach in the following sections. In particular we discuss the distributional clustering model which has been a major stimulus for our research in Section 3. <p> In the special case of X = Y the SMM is equivalent to the word clustering model of Saul and Pereira <ref> [54] </ref> which has been developed parallel to our work.
Reference: [55] <author> L. K. Saul, T. Jaakkola, and M.I. Jordan. </author> <title> Mean field theory for simoid belief networks. </title> <journal> Journal of Artificial Intelligence Research, </journal> <volume> 4 </volume> <pages> 61-76, </pages> <year> 1996. </year>
Reference-contexts: This variational technique is known as mean-field approximation [42, 2] and has been successfully applied for optimization problems [61, 23], in computer vision [15, 67, 24], and for inference in graphical models <ref> [55] </ref>. In general, solutions of the mean-field approximation have to fulfill the stationary conditions hR rff i = Z 1 hH (R; S; ; R rff = 1)i (66) where expectations are taken with respect to P (Rjp rff ) [24].
Reference: [56] <author> P. Schroeter and J. Bigun. </author> <title> Hierarchical image segmentation by multi-dimensional clustering and orientation-adaptive boundary refinement. </title> <journal> Pattern Recognition, </journal> <volume> 28(5) </volume> <pages> 695-709, </pages> <year> 1995. </year>
Reference-contexts: COD mixture models, especially the ACM model, formalize the grouping of feature distribution in a more direct manner. In contrast to pairwise similarity clustering, they offer a sound generative model for texture class description which can be utilized in subsequent processing stages like edge localization <ref> [56] </ref>. Furthermore, there is no need to compute a large matrix of pairwise similarity scores between image sites, which greatly reduces the overall processing time and memory requirements. Compared to the mixture of Gaussian model, ACM provides significantly more flexibility in distribution modeling.
Reference: [57] <author> J. Shi and J. Malik. </author> <title> Normalized cuts and image segmentation. </title> <booktitle> In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR'97), </booktitle> <pages> pages 731-737, </pages> <year> 1997. </year>
Reference-contexts: Occasionally, the grouping process has been based on pairwise similarity measurements between image sites, where similarity is measured by a non-parametric statistical test applied to the feature distribution of a surrounding neighborhood [16, 39, 24]. Agglomerative techniques [39] and, more rigorously, optimization approaches <ref> [24, 57] </ref> have been developed and applied for the grouping of similarity data in the texture segmentation context. Pairwise similarity clustering thus provides an indirect way to group (discrete) feature distributions without reducing information in a distribution to their mean.
Reference: [58] <author> K. Sparck Jones. </author> <title> Automatic Keyword Classification for Information Retrieval. </title> <publisher> Butterworths, </publisher> <address> London, </address> <year> 1971. </year>
Reference-contexts: The second approach focuses on the index terms to derive an improved feature representation of documents. The by far most popular technique in this category is Salton's Vector Space Model <ref> [51, 58, 52] </ref> of which different variants have been proposed with different word weighting schemes [53]. A more recent variant known as latent semantics indexing [12] performs a dimension reduction by singular value decomposition.
Reference: [59] <author> D.M. Titterington, A.F.M. Smith, and U.E. Makov. </author> <title> Statistical Analysis of Finite Mixture Distributions. </title> <publisher> John Wiley & Sons, </publisher> <year> 1985. </year>
Reference-contexts: Related methods of feature selection have been proposed for text categorization, e.g., the term strength criterion [66]. In contrast, we propose a model-based statistical approach and present a family of finite mixture models <ref> [59, 35] </ref> as a way to deal with the data sparseness problem. Since mixture or class-based models can also be combined with other models our goal is orthogonal to standard interpolation techniques. <p> In fact, this interpretation is consistent with other common mixture models <ref> [35, 59] </ref> and might be preferred in the context of statistical modeling, in particular if N scales with L.
Reference: [60] <author> N. Ueda and R. Nakano. </author> <title> Deterministic annealing variants of EM. </title> <booktitle> In Advances in Neural Information Processing Systems 7, </booktitle> <pages> pages 545-52, </pages> <year> 1995. </year>
Reference-contexts: The posterior thus minimizes F T at T = 1. The annealed EM algorithm is the generalization defined by an arbitrary choice of (the temperature) T <ref> [60] </ref>. In the E-step for T &gt; 1 this amounts to discounting the likelihood as compared to the prior by taking it to the 1=T - th power.
Reference: [61] <author> D. van den Bout and T. Miller. </author> <title> Graph partitioning using annealed neural networks. </title> <journal> IEEE Transactions on Neural Networks, </journal> <volume> 1(2) </volume> <pages> 192-203, </pages> <year> 1990. </year>
Reference-contexts: This variational technique is known as mean-field approximation [42, 2] and has been successfully applied for optimization problems <ref> [61, 23] </ref>, in computer vision [15, 67, 24], and for inference in graphical models [55].
Reference: [62] <editor> C.J. van Rijsbergen. </editor> <booktitle> Information retrieval. </booktitle> <address> Butter-worths, London Boston, </address> <year> 1979. </year>
Reference-contexts: An empirical comparison of smoothing techniques can be found in [7]. In information retrieval, there have been essentially two proposals to overcome the sparseness problem. The first class of methods relies on the cluster hypothesis <ref> [62, 20] </ref> which suggests to make use of inter-document similarities in order to improve the retrieval performance. Since it is often prohibitive to compute all pairwise similarities between documents these methods typically rely on random comparisons or random fractionation [8]. <p> Yet, it is often difficult to reliably estimate similarities, because the query may not contain enough information, e.g., not all possibly relevant keywords might occur in a query for documents. Therefore, one often applies the cluster hypothesis <ref> [62] </ref>: if an entry is relevant to a query, similar entries may also be relevant to the query although they may not possess a high similarity to the query itself due to the small number of keywords. <p> We have performed experiments for information retrieval on different collections of abstracts. The index terms for each dataset have been automatically extracted from all documents with the help of a standard word stemmer. Following <ref> [62] </ref>, a list of stop words has been utilized to exclude frequently used words. Words with few overall occurrences have also been eliminated. The documents are identified with the set of objects X , while index terms correspond to Y.
Reference: [63] <author> P. Willett. </author> <title> Recent trends in hierarchical document clustering: a critical review. </title> <booktitle> Information Processing & Management, </booktitle> <volume> 24(5) </volume> <pages> 577-597, </pages> <year> 1988. </year>
Reference-contexts: Clustering thus provides a way of pre-structuring a database for the purpose of improved information retrieval, cf. <ref> [63] </ref> for an overview. Both types of clustering approaches, for the set of documents as well as for the keywords, have been proposed in the literature.
Reference: [64] <author> I.H. Witten and T.C. Bell. </author> <title> The zero-frequency problem: Estimating the probabilities of novel events in adaptive text compression. </title> <journal> IEEE Transactions on Information Theory, </journal> <volume> 37(4) </volume> <pages> 1085-1094, </pages> <year> 1991. </year>
Reference-contexts: However, the intrinsic problem of COD is that of data sparseness, also known as the zero frequency problem <ref> [19, 18, 30, 64] </ref>. When N and M are very large, a majority of pairs (x i ; y j ) only have a small probability of occurring together in S.
Reference: [65] <author> C.S. Won and H. Derin. </author> <title> Unsupervised segmentation of noisy and textured images using Markov random fields. CVGIP: Graphical Models and Image Processing, </title> <booktitle> 54(4) </booktitle> <pages> 308-328, </pages> <month> July </month> <year> 1992. </year>
Reference-contexts: In the clustering stage features are grouped into homogeneous segments, where homogeneity of features has to be formalized by a mathematical no tion of similarity. Most widely, features are interpreted as vectors in a Eu-clidean space <ref> [25, 33, 34, 65, 40, 6, 31] </ref> and a segmentation is obtained by minimizing the K-means criterion, which sums over the square distances between feature vectors and their assigned, group-specific prototype feature vectors. K-means clustering can be understood as a statistical mixture model with isotropic Gaussian class distributions.
Reference: [66] <author> Y. Yang and J. Willbur. </author> <title> Using corpus statistics to remove redundant words in text categorization. </title> <journal> Journal of the American Society for Information Science, </journal> <volume> 47(5) </volume> <pages> 357-369, </pages> <year> 1996. </year>
Reference-contexts: A more recent variant known as latent semantics indexing [12] performs a dimension reduction by singular value decomposition. Related methods of feature selection have been proposed for text categorization, e.g., the term strength criterion <ref> [66] </ref>. In contrast, we propose a model-based statistical approach and present a family of finite mixture models [59, 35] as a way to deal with the data sparseness problem. Since mixture or class-based models can also be combined with other models our goal is orthogonal to standard interpolation techniques.
Reference: [67] <author> J. Zhang. </author> <title> The mean-field theory in EM procedures for blind Markov random fields. </title> <journal> IEEE Transactions on Image Processing, </journal> <volume> 2(1) </volume> <pages> 27-40, </pages> <year> 1993. </year> <month> 21 </month>
Reference-contexts: This variational technique is known as mean-field approximation [42, 2] and has been successfully applied for optimization problems [61, 23], in computer vision <ref> [15, 67, 24] </ref>, and for inference in graphical models [55]. In general, solutions of the mean-field approximation have to fulfill the stationary conditions hR rff i = Z 1 hH (R; S; ; R rff = 1)i (66) where expectations are taken with respect to P (Rjp rff ) [24].
References-found: 67

