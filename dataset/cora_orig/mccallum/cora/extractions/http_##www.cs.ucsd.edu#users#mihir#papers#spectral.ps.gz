URL: http://www.cs.ucsd.edu/users/mihir/papers/spectral.ps.gz
Refering-URL: http://www.cs.ucsd.edu/users/mihir/papers/complexity-papers.html
Root-URL: http://www.cs.ucsd.edu
Email: E-mail: mihir@cs.ucsd.edu  
Title: A Technique for Upper Bounding the Spectral Norm with Applications to Learning  
Author: Mihir Bellare 
Address: San Diego, 9500 Gilman Drive, La Jolla, CA 92093.  
Affiliation: Department of Computer Science Engineering, Mail Code 0114, University of California at  
Date: December 26, 1992  
Note: Appears in Proceedings of the Fifth Annual ACM Workshop on Computational Learning Theory (COLT), 1992.  
Abstract: We present a general technique to upper bound the spectral norm of an arbitrary function. At the heart of our technique is a theorem which shows how to obtain an upper bound on the spectral norm of a decision tree given the spectral norms of the functions in the nodes of this tree. The theorem applies to trees whose nodes may compute any boolean functions. Applications are to the design of efficient learning algorithms and the construction of small depth threshold circuits (or neural nets). In particular, we present polynomial time algorithms for learning O(log n) term DNF formulas and various classes of decision trees, all under the uniform distribution with membership queries. 
Abstract-found: 1
Intro-found: 1
Reference: [Be1] <author> M. Bellare. </author> <title> The Spectral Norm of Finite Functions. </title> <note> MIT Laboratory for Computer Science Technical Report MIT/LCS/TR-495 , February 1991. </note>
Reference-contexts: The above theorem is particularly suited to this type of application, since it automatically provides a way to get small depth circuits for combinations of simple functions. We note that the theorem of [BS], and in consequence ours, are non-constructive. The results of this paper appeared previously in <ref> [Be1] </ref> and [Be2]. 2 Preliminaries "Boolean" for us means 1 valued. A function f : f1; +1g n ! R is boolean if its range is f1; +1g. <p> Our techniques and results can be similarly extended to the case of mutually independent distributions. We discuss these extensions briefly here. For more details the reader is referred to <ref> [Be1] </ref>. A probability distribution q : f1; +1g n ! [0; 1] is mutually independent if the random variables x 1 ; : : : ; x n are independent. <p> Theorem 7.2 Let q be a mutually independent distribution. Then L q (T ) w q (T ) for any universal decision tree T . We omit the proof which is identical to that of Theorem 3.7. It was observed in <ref> [Be1] </ref> that the learning algorithm of [KM] also extends to the case of mutually independent distributions. Thus in combination with Theorem 7.2 we have another tool for learning under mutually independent distributions. Acknowledgements I thank Marek Karpinski and Ron Rivest for helpful discussions. Work done while the author was at MIT.
Reference: [Be2] <author> M. Bellare. </author> <title> A Technique for Upper Bounding the Spectral Norm with Applications to Learning. </title> <booktitle> Proceedings of the Fifth Annual Workshop on Computational Learning Theory, ACM, </booktitle> <year> 1992. </year> <month> 13 </month>
Reference-contexts: We note that the theorem of [BS], and in consequence ours, are non-constructive. The results of this paper appeared previously in [Be1] and <ref> [Be2] </ref>. 2 Preliminaries "Boolean" for us means 1 valued. A function f : f1; +1g n ! R is boolean if its range is f1; +1g.
Reference: [BR] <author> A. Blum and S. Rudich. </author> <title> Fast Learning of k-term DNF formulas with queries. </title> <booktitle> Pro--ceedings of the 24th Annual Symposium on Theory of Computing, ACM, </booktitle> <year> 1992. </year>
Reference-contexts: More generally, we show that a k-term DNF formula is learnable in time polynomial in n; 2 k ; * 1 , and lg ffi 1 . Independently of this work, Blum and Rudich <ref> [BR] </ref> present an algorithm which uses membership and equivalence queries to (exactly) learn a k-clause DNF formula in time polynomial in n and 2 k . Learning Decision Trees. We present a general result about learning decision trees. <p> Minimizing the depth of neural nets. Threshold circuits (or neural nets) are circuits whose gates are linear threshold functions (see Section 6.4 for full definitions). The depth of the circuit is an important measure of efficiency, and much work in the area has concentrated on depth 4 minimization <ref> [HMPST, Br, BS, SB] </ref>. Spectral theory has provided an important tool for this task with a theorem which says that functions of polynomially bounded spectral norm have depth two threshold circuits [BS]. Combining this with our main theorem yields a novel tool.
Reference: [Br] <author> J. Bruck. </author> <title> Harmonic Analysis of Polynomial Threshold Functions. </title> <journal> SIAM J. Discrete Math. </journal> <volume> 3(2), </volume> <month> 168-177 </month> <year> (1990). </year>
Reference-contexts: More generally, we show that a k-term DNF formula is learnable in time polynomial in n; 2 k ; * 1 , and lg ffi 1 . Independently of this work, Blum and Rudich <ref> [BR] </ref> present an algorithm which uses membership and equivalence queries to (exactly) learn a k-clause DNF formula in time polynomial in n and 2 k . Learning Decision Trees. We present a general result about learning decision trees. <p> Minimizing the depth of neural nets. Threshold circuits (or neural nets) are circuits whose gates are linear threshold functions (see Section 6.4 for full definitions). The depth of the circuit is an important measure of efficiency, and much work in the area has concentrated on depth 4 minimization <ref> [HMPST, Br, BS, SB] </ref>. Spectral theory has provided an important tool for this task with a theorem which says that functions of polynomially bounded spectral norm have depth two threshold circuits [BS]. Combining this with our main theorem yields a novel tool.
Reference: [BS] <author> J. Bruck and R. Smolensky. </author> <title> Polynomial Threshold Functions, AC 0 Functions, and Spectral Norms. </title> <booktitle> Proceedings of the 31st Symposium on Foundations of Computer Science, IEEE, </booktitle> <year> 1990. </year>
Reference-contexts: Another example is in the domain of threshold circuits (or neural nets): we know that if L (f ) is polynomially bounded then f can by computed by a threshold circuit of small depth <ref> [BS] </ref>. To use such theorems, we need to first show that L (f ) is small. So it is important to find ways of computing good upper bounds on the spectral norm, and thereby, in particular, identifying the functions which have polynomially bounded spectral norm. <p> Minimizing the depth of neural nets. Threshold circuits (or neural nets) are circuits whose gates are linear threshold functions (see Section 6.4 for full definitions). The depth of the circuit is an important measure of efficiency, and much work in the area has concentrated on depth 4 minimization <ref> [HMPST, Br, BS, SB] </ref>. Spectral theory has provided an important tool for this task with a theorem which says that functions of polynomially bounded spectral norm have depth two threshold circuits [BS]. Combining this with our main theorem yields a novel tool. <p> Spectral theory has provided an important tool for this task with a theorem which says that functions of polynomially bounded spectral norm have depth two threshold circuits <ref> [BS] </ref>. Combining this with our main theorem yields a novel tool. Theorem 1.4 If a boolean function is expressible as a universal decision tree of polynomially bounded weight then it is computable by a depth two threshold circuit. <p> The above theorem is particularly suited to this type of application, since it automatically provides a way to get small depth circuits for combinations of simple functions. We note that the theorem of <ref> [BS] </ref>, and in consequence ours, are non-constructive. The results of this paper appeared previously in [Be1] and [Be2]. 2 Preliminaries "Boolean" for us means 1 valued. A function f : f1; +1g n ! R is boolean if its range is f1; +1g. <p> The class of boolean function computable by depth d, polynomial sized threshold circuits is usually denoted c LT d . Spectral theory has provided an important tool for proving the existence of small depth threshold circuits. Bruck and Smolensky <ref> [BS] </ref> have shown that if a boolean function has polynomially bounded spectral norm then it is computable by a depth two threshold circuit. This theorem (and extensions of it) are exploited in [BS, SB] to prove that many functions (comparison and addition are examples) are in c LT 2 . <p> Bruck and Smolensky [BS] have shown that if a boolean function has polynomially bounded spectral norm then it is computable by a depth two threshold circuit. This theorem (and extensions of it) are exploited in <ref> [BS, SB] </ref> to prove that many functions (comparison and addition are examples) are in c LT 2 . <p> This theorem (and extensions of it) are exploited in [BS, SB] to prove that many functions (comparison and addition are examples) are in c LT 2 . By combining the result of <ref> [BS] </ref> with our main theorem we get a new method of proving the existence of small depth threshold circuits: Theorem 6.4 If a boolean function is expressible as a universal decision tree of polynomially bounded weight then it is computable by a depth two threshold circuit.
Reference: [FJS] <author> M. Furst, J. Jackson and S. Smith. </author> <title> Learning AC 0 Functions Sampled under Mutually Independent Distributions. </title> <note> Manuscript (October 1990). </note>
Reference-contexts: Learning DNF. We address the problem of learning DNF formulas [Va]. Linial, Mansour and Nisan [LMN], Verbeurgt [Ve], and Furst, Jackson and Smith <ref> [FJS] </ref> consider (*; ffi; D) learning of DNF formulas: after seeing a collection of examples of the target f drawn under distribution D, the algorithm must output a hypothesis h which with probability at least 1 ffi satisfies Pr x2 R D [f (x) 6= h (x)] *. <p> Our main theorem is particularly suited to this type of application, since it automatically provides a way to get small depth circuits for combinations of simple functions. 7 Extensions Furst, Jackson and Smith <ref> [FJS] </ref> introduce the notion of mutually independent distributions and show how to extend the spectral techniques of Linial, Mansour and Nisan [LMN] (used for learning DNF under the uniform distribution) to the case of mutually independent distributions. <p> We will denote by L q (f ) the spectral norm of f under the Fourier series given by mutually independent distribution q. We call it the q-spectral norm. The exact definitions of these quantities are not necessary here; for these the reader is referred to <ref> [FJS] </ref>. To state the extension of our main theorem to the q-framework we need to redefine the degree. The rest will follow. Definition 7.1 Let q be a mutually independent distribution.
Reference: [HMPST] <author> A. Hajnal, W. Maass, P. Pudlak, M. Szegedy and G. Turan. </author> <title> Threshold Circuits of Bounded Depth. </title> <booktitle> Proceedings of the 28th Symposium on Foundations of Computer Science, IEEE, </booktitle> <year> 1987. </year>
Reference-contexts: Minimizing the depth of neural nets. Threshold circuits (or neural nets) are circuits whose gates are linear threshold functions (see Section 6.4 for full definitions). The depth of the circuit is an important measure of efficiency, and much work in the area has concentrated on depth 4 minimization <ref> [HMPST, Br, BS, SB] </ref>. Spectral theory has provided an important tool for this task with a theorem which says that functions of polynomially bounded spectral norm have depth two threshold circuits [BS]. Combining this with our main theorem yields a novel tool.
Reference: [KKL] <author> J. Kahn, G. Kalai and N. Linial. </author> <title> The Influence of Variables on Boolean Functions. </title> <booktitle> Proceedings of the 29th Symposium on Foundations of Computer Science, IEEE, </booktitle> <year> 1988. </year>
Reference: [KM] <author> E. Kushilevitz and Y. Mansour. </author> <title> Learning Decision Trees using the Fourier Spectrum. </title> <booktitle> Proceedings of the 23rd Annual Symposium on Theory of Computing, ACM, </booktitle> <year> 1991. </year>
Reference-contexts: One example is learning: we know that if L (f ) is polynomially bounded then f is learnable (under the uniform distribution with membership queries) in polynomial time <ref> [KM] </ref>. Another example is in the domain of threshold circuits (or neural nets): we know that if L (f ) is polynomially bounded then f can by computed by a threshold circuit of small depth [BS]. <p> In particular for parity decision trees (each internal node is labeled by a parity function s and each of leaf is labeled +1 or 1) our theorem yields L (T ) djT j=2e which is (a slight improvement of) what <ref> [KM] </ref> had already proved for this special case (jT j is the number of nodes in T ). <p> Several special cases of this theorem are of interest. See Section 6.1 for details on these applications. All of the above learning results are obtained by using our main theorem to bound the spectral norm of the class of functions in question and then applying the learning algorithm of <ref> [KM] </ref>. Minimizing the depth of neural nets. Threshold circuits (or neural nets) are circuits whose gates are linear threshold functions (see Section 6.4 for full definitions). <p> We note that w (T ) = djT j=2e for any parity decision tree T (cf. Proposition 3.4 and Proposition 4.1). Thus the main theorem implies that L (T ) djT j=2e for any parity decision tree T (cf. <ref> [KM] </ref>). 6 Applications One of the main tasks in computational learning theory is to identify which functions are efficiently learnable. <p> We say that l: N ! N is a bound on the spectral norm of a class of functions C if L (f ) l (n) for all f : f1; +1g n ! f1; +1g in C. The main result we will exploit is the following. Theorem 6.1 <ref> [KM] </ref> A class of boolean functions is learnable in time polynomial in l (n); n; * 1 and lg ffi 1 , where l is a bound on the spectral norm of the class. 6.2 Learning DNF A DNF formula is a disjunction C 1 _ : : : _ C <p> In particular this means that for DNF with a super-logarithmic number of clauses one may not be able to prove polynomial time learnability with these same techniques. 6.3 Learning Decision Trees The original motivation of the learning algorithm of <ref> [KM] </ref> was to learn parity decision trees with polynomially many nodes. We can extend this to learn many larger classes of decision trees. 11 Theorem 6.3 Let C be a class of universal decision trees with polynomially bounded weight. Then C is learnable in polynomial time. <p> Theorem 7.2 Let q be a mutually independent distribution. Then L q (T ) w q (T ) for any universal decision tree T . We omit the proof which is identical to that of Theorem 3.7. It was observed in [Be1] that the learning algorithm of <ref> [KM] </ref> also extends to the case of mutually independent distributions. Thus in combination with Theorem 7.2 we have another tool for learning under mutually independent distributions. Acknowledgements I thank Marek Karpinski and Ron Rivest for helpful discussions. Work done while the author was at MIT.
Reference: [LMN] <author> N. Linial, Y. Mansour and N. Nisan. </author> <title> Constant Depth Circuits, Fourier Transform, and Learnability. </title> <booktitle> Proceedings of the 30th Symposium on Foundations of Computer Science, IEEE, </booktitle> <year> 1989. </year>
Reference-contexts: Learning DNF. We address the problem of learning DNF formulas [Va]. Linial, Mansour and Nisan <ref> [LMN] </ref>, Verbeurgt [Ve], and Furst, Jackson and Smith [FJS] consider (*; ffi; D) learning of DNF formulas: after seeing a collection of examples of the target f drawn under distribution D, the algorithm must output a hypothesis h which with probability at least 1 ffi satisfies Pr x2 R D [f <p> particularly suited to this type of application, since it automatically provides a way to get small depth circuits for combinations of simple functions. 7 Extensions Furst, Jackson and Smith [FJS] introduce the notion of mutually independent distributions and show how to extend the spectral techniques of Linial, Mansour and Nisan <ref> [LMN] </ref> (used for learning DNF under the uniform distribution) to the case of mutually independent distributions. Our techniques and results can be similarly extended to the case of mutually independent distributions. We discuss these extensions briefly here. For more details the reader is referred to [Be1].
Reference: [SB] <author> K. Siu and J. Bruck. </author> <title> On the Power of Threshold Circuits with Small Weights. </title> <type> Manuscript. </type>
Reference-contexts: But the consequence is that good upper bounds on L (f ) are only known either for very simple functions (like parity, AND and OR) or for functions where one can compute L (f ) exactly by exploiting a convenient inductive structure (like the comparison function <ref> [SB] </ref>). And so far there have been no general techniques to compute upper bounds: the approach used is to work directly from the definition. This paper presents a general technique to upper bound the spectral norm. <p> Minimizing the depth of neural nets. Threshold circuits (or neural nets) are circuits whose gates are linear threshold functions (see Section 6.4 for full definitions). The depth of the circuit is an important measure of efficiency, and much work in the area has concentrated on depth 4 minimization <ref> [HMPST, Br, BS, SB] </ref>. Spectral theory has provided an important tool for this task with a theorem which says that functions of polynomially bounded spectral norm have depth two threshold circuits [BS]. Combining this with our main theorem yields a novel tool. <p> It is well known (cf. <ref> [SB] </ref>) that L (C n ) = n 2 + 1. Similarly, each output bit of the addition function can be shown to have degree O (n). <p> Bruck and Smolensky [BS] have shown that if a boolean function has polynomially bounded spectral norm then it is computable by a depth two threshold circuit. This theorem (and extensions of it) are exploited in <ref> [BS, SB] </ref> to prove that many functions (comparison and addition are examples) are in c LT 2 .
Reference: [Va] <author> L. Valiant. </author> <title> A Theory of the Learnable. </title> <journal> Communications of the ACM 27(11), </journal> <month> 1134-1142 </month> <year> (1984). </year>
Reference-contexts: Learning DNF. We address the problem of learning DNF formulas <ref> [Va] </ref>.
Reference: [Ve] <author> K. Verbeurgt. </author> <title> Learning DNF under the uniform distribution in polynomial time. </title> <booktitle> Proceedings of the Second Annual Workshop on Computational Learning Theory, </booktitle> <publisher> Mor-gan Kaufmann Publishers Inc. </publisher> <year> (1989). </year> <month> 14 </month>
Reference-contexts: Learning DNF. We address the problem of learning DNF formulas [Va]. Linial, Mansour and Nisan [LMN], Verbeurgt <ref> [Ve] </ref>, and Furst, Jackson and Smith [FJS] consider (*; ffi; D) learning of DNF formulas: after seeing a collection of examples of the target f drawn under distribution D, the algorithm must output a hypothesis h which with probability at least 1 ffi satisfies Pr x2 R D [f (x) 6=
References-found: 13

