URL: http://www.tc.cornell.edu/Papers/bergmark/keystone.ps
Refering-URL: http://www.tc.cornell.edu/Papers/
Root-URL: http://www.tc.cornell.edu
Title: Software Tools for High Performance Computing: Survey and Recommendations  
Author: Bill Appelbe Donna Bergmark (eds.) 
Date: November 1994 Revised October 1995  
Note: Received  1996 John Wiley Sons, Inc. Scientific Programming, Vol. 5, pp. 239-249 (1996) CCC 1058-9244/96/0302039-11  
Address: Atlanta, GA 30332-0280 Ithaca, NY 14853-3801  
Affiliation: College of Computing Cornell Theory Center Georgia Institute of Technology Cornell University  
Abstract: Applications programming for High Performance Computing is notoriously difficult. Although Parallel Programming is intrinsically complex, the principal reason why High Performance Computing is difficult is the lack of effective software tools. We believe that the lack of tools in turn is largely due to market forces rather than our inability to design and build such tools. Unfortunately, the poor availability and utilization of parallel tools hurts the entire supercomputing industry and the US High Performance Computing initiative which is focused on applications. A disproportionate amount of resources are being spent on faster hardware and architectures, while tools are being neglected. This paper introduces a taxonomy of tools, analyzes the major factors that contribute to this situation, and suggest ways that the imbalance could be redressed and the likely evolution of tools. Many attendees at the May 1993 Workshop on Parallel Computing Systems, at Keystone, Colorado, contributed to this paper, especially Gail A. Alverson, from Tera Corp., and Wayne Smith from Intel. The paper also includes discussions from the October 1994 Workshop on Debugging and Performance Tuning for Parallel Computer Systems, at Chatham, Massachusetts. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Appelbe, B., McDowell, C., and Smith, K. Start/Pat: </author> <title> A parallel-programming toolkit. </title> <booktitle> IEEE Software 6, </booktitle> <month> 4 (July </month> <year> 1989), </year> <pages> 29-38. </pages>
Reference-contexts: There are a few tools that we do not have the current software technology to build, and may not in the foreseeable future. For example, we do not know how to automatically compile any sequential program into efficient code for a distributed memory multiprocessor <ref> [1, 11] </ref>. However, for alternative approaches such as user-supplied directives and interactive compilers, we have the software technology we need to build prototypes.
Reference: [2] <author> Aydt, R. </author> <title> The Pablo Self-Defining Data Format, </title> <year> 1993. </year>
Reference-contexts: Ideally, trace data formats can be standardized across platforms, or tools can use trace specifications. Some effort has been devoted to trace standardization, but it has not gone far [9]. We note, however, that there is a small trend toward converting trace files into SDDF format <ref> [2] </ref> for interchangeable tool use. 2.2.3 Syntax Analysis and Program Instrumentation Tools such as performance monitors and execution analyzers are far more effective if their interface is at the program level.
Reference: [3] <author> Bergmark, D. </author> <title> Update on tools for parallel programming at the CNSF. </title> <type> Tech. rep., </type> <institution> Cornell University, </institution> <year> 1994. </year>
Reference-contexts: The simplest classification of tools is by their functionality. The list below (adapted from <ref> [3] </ref>) gives the major classes of tools, together with some representative examples in parentheses: Compilers Most vendors of parallel systems supply compilers for parallel dialects of C and Fortran. Sometimes they are integrated with paral-lelizers: the input to the compiler can be either a sequential or parallel program.
Reference: [4] <author> IBM. </author> <title> AIX Parallel Environment Parallel Programming Operation and Use, </title> <month> June </month> <year> 1994. </year>
Reference-contexts: The majority of scientists working on grand challenge problems are aware of the need for tools. The report from the Pasadena workshop on System Software and Tools for High Performance Computing documents these needs in detail <ref> [6, chap. 3, 4] </ref>. Unfortunately, the current tools (other than compilers) are very limited in their capabilities and often do not meet the needs of scientists. Vendors supply few tools, and the tools available from research laboratories and universities are generally only prototypes. <p> There are some examples of high quality tools provided by vendors, such ATExpert from Cray Research (a performance analysis tool). Also, vendors occasionally provided interfaces and tools for developers, such as the trace facility provided by IBM's Parallel Environment for the SP1/2 <ref> [4] </ref>.
Reference: [5] <author> Kohn, J., and Williams, W. ATExpert. </author> <note> Journal of Parallel and Distributed Computing 14 (May 1993). </note>
Reference-contexts: Almost all parallel computers now come with parallel debuggers. Vendors of "classic" supercomputers (primarily IBM and Cray) have more mature tools (e.g., Cray's ATExpert for performance analysis <ref> [5] </ref>). However, good tools are less available for the massively parallel and distributed computers. SIMD computers need somewhat fewer tools, and the tools are simpler, because SIMD computers are deterministic. Unfortunately, SIMD computers have been largely relegated to specialized applications.
Reference: [6] <author> Messina, P., and Sterling, T., Eds. </author> <title> System software and tools for high performance computing environments. </title> <publisher> SIAM (Philadelphia), </publisher> <year> 1993. </year> <note> A report on the findings of the Pasadena Workshop April 14-16, </note> <year> 1992. </year>
Reference-contexts: There have been several other workshops on this topic, such as the Pasadena workshop on System Software and Tools for High Performance Computing documents <ref> [6] </ref>, and proposals such as the National Software Exchange. This article is an attempt to summarize the conclusions of these meetings, and suggest courses of action. The authors' experiences ranges from tool building and maintaining, to programming, advising users, and managing parallel systems. <p> The majority of scientists working on grand challenge problems are aware of the need for tools. The report from the Pasadena workshop on System Software and Tools for High Performance Computing documents these needs in detail <ref> [6, chap. 3, 4] </ref>. Unfortunately, the current tools (other than compilers) are very limited in their capabilities and often do not meet the needs of scientists. Vendors supply few tools, and the tools available from research laboratories and universities are generally only prototypes. <p> They want to know exactly what is happening at the machine level, but disdain any tools that "get in the way", such as parallelizing compilers and multi-user operating systems. But, most terageeks do insist on basic UNIX tools on all processors, such as dbx <ref> [6] </ref>. They regard squeezing the last MFLOP out of even the most recalcitrant architecture as another grand challenge. The Gordon Bell award has been established to reward these pioneers. Scientists Scientists first and foremost want to solve their problem and share their results with the community. <p> Given the current US political climate, it is more likely that government funding will decline, rather than expand, in the near future. Industry, on the other hand, is unwilling to invest in tools that are not proven to be useful. Tools can be classified in three levels <ref> [6] </ref>: 17 Research Prototypes These are typically the product of a doctoral dissertation or research laboratory project. They are primarily "proofs on concept", are only usable by a skilled person, have very limited application, and little documentation.
Reference: [7] <author> Pancake, C. </author> <title> Where are we headed? Communications of the ACM 34, </title> <address> 11 (Nov. </address> <year> 1991), </year> <pages> 53-64. </pages>
Reference-contexts: Vendors supply few tools, and the tools available from research laboratories and universities are generally only prototypes. Supercomputers have polarized the scientific and engineering programming community into two groups, each with their own set of unmet tool needs <ref> [7] </ref>: "TeraGeeks" These are scientists, turned programmers, who will stop at nothing to achieve peak performance and solve bigger problems. They want to know exactly what is happening at the machine level, but disdain any tools that "get in the way", such as parallelizing compilers and multi-user operating systems.
Reference: [8] <author> Pancake, C., and Cook, C. </author> <title> What users need in parallel tool support: Survey results and analysis. </title> <booktitle> In Proceedings of Scalable High-Performance Computing Conference (Knoxville, </booktitle> <address> TN) (May 23-25 1994), </address> <publisher> IEEE Computer Society, </publisher> <pages> pp. 40-47. </pages> <address> http:/www.cs.orst.edu/ pancake/surveys/surveys.html. </address> <month> 20 </month>
Reference-contexts: Rerunning the same program with the same data often produces significantly difference performance results. Despite these challenges, many parallel computer users and vendors doubt the value of tools <ref> [8] </ref>. Users try to debug their programs with print statements, and they avoid runtime software monitoring of programs because of its overhead or side-effects. Early high performance computers came with almost no tools, and programmers often resorted to assembler language.
Reference: [9] <author> Pancake, C. M., Utter, P. S., Bergmark, D., and Gannon, D. </author> <title> Supercomputing '90 BOF session on standardizing parallel trace for mats. </title> <type> Tech. Rep. </type> <institution> TC91TR53, Cornell Theory Center, </institution> <month> Mar. </month> <year> 1991. </year>
Reference-contexts: If the overhead of tracing can be kept below 10%, then tracing can be the default. Ideally, trace data formats can be standardized across platforms, or tools can use trace specifications. Some effort has been devoted to trace standardization, but it has not gone far <ref> [9] </ref>. We note, however, that there is a small trend toward converting trace files into SDDF format [2] for interchangeable tool use. 2.2.3 Syntax Analysis and Program Instrumentation Tools such as performance monitors and execution analyzers are far more effective if their interface is at the program level.
Reference: [10] <author> Research, A. P. </author> <title> FORGE Explorer User's Guide. 550 Main Street, Suite I, </title> <address> Placerville, CA 95667, </address> <month> Jan. </month> <year> 1995. </year>
Reference-contexts: For example, tools that generally can be used equally well with parallel and sequential programs (such as configuration control) have been omitted. Also, the list excludes tools used primarily by systems administrators (such as load monitors). Some tools provide functionality from different categories. For example, FORGE <ref> [10] </ref> provides both static analysis and profiling (performance monitoring). Some of these tools have graphical user interfaces (GUIs).
Reference: [11] <author> Zima, H., and Chapman, B. </author> <title> Supercompilers for Parallel and Vector Computers. </title> <publisher> ACM Press, </publisher> <address> New York, New York, </address> <year> 1990. </year> <month> 21 </month>
Reference-contexts: There are a few tools that we do not have the current software technology to build, and may not in the foreseeable future. For example, we do not know how to automatically compile any sequential program into efficient code for a distributed memory multiprocessor <ref> [1, 11] </ref>. However, for alternative approaches such as user-supplied directives and interactive compilers, we have the software technology we need to build prototypes.
References-found: 11

