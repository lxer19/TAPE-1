URL: http://www.cis.hut.fi/~oja/ICNNinvited95.ps
Refering-URL: http://www.cis.hut.fi/~oja/papers.html
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Email: Email: Juha.Karhunen@hut.fi, Erkki.Oja@hut.fi  
Title: Signal Separation by Nonlinear Hebbian Learning  
Author: Erkki Oja and Juha Karhunen 
Address: Rakentajanaukio 2 C, FIN-02150 Espoo, Finland  
Affiliation: Helsinki University of Technology Laboratory of Computer and Information Science  
Abstract-found: 0
Intro-found: 1
Reference: [1] <author> A. Bell and T. Sejnowski, </author> <title> "Blind separation and blind deconvolution: an information-theoretic approach," </title> <booktitle> in Proc. 1995 IEEE Int. Conf. on Acoustics, Speech, and Signal Processing, </booktitle> <address> Detroit, USA, </address> <month> May </month> <year> 1995, </year> <pages> pp. 3415-3418. </pages>
Reference-contexts: However, some of these adaptive algorithms, such as the HJ algorithm and its modifications as well as the PFS/EASI algorithm proposed by Cardoso and Laheld [3, 16], can be interpreted as learning algorithms of a neural network. Quite recently, some authors <ref> [1, 7] </ref> have derived unsupervised "neural" learning rules from information-theoretic measures. The resulting algorithms show good separation performance, but are not truly realizable in neural networks because they are based on relatively complicated numerical operations, requiring for example the inversion of a matrix. <p> The latter case is then more stable. A simple example of a sub-Gaussian density is the uniform density on <ref> [1; 1] </ref>. Let us assume this for the elements of u to illustrate the Theorem 1. Condition 1 of Theorem 1 is then satisfied. It remains to check the stability conditions 4 and 5 of Theorem 1. <p> However, we have applied especially the bigradient algorithm for super-Gaussian sources that have positive kurtosis, too. In [28], up to 10 real speech signals were separated from their mixtures using the bigradient algorithm. The speech signals are typically super-Gaussian <ref> [1] </ref>. In these experiments, the learning functions and parameter values were chosen in much the same manner as before, but k must have the opposite sign, because the sum of the fourth moments is maximized instead of minimizing it.
Reference: [2] <author> G. Burel, </author> <title> "Blind separation of sources: a nonlinear neural algorithm," </title> <booktitle> Neural Networks, </booktitle> <volume> vol. 5, </volume> <pages> pp. 937-947, </pages> <year> 1992. </year>
Reference-contexts: Attempts to extend source separation and ICA into these directions have already been made by some authors <ref> [2, 7, 17, 24] </ref>. Acknowledgement. We are grateful to Mr. Liuyue Wang for the experiments on the one-dimensional signals, and to Mr. Ricardo Vigario for the experiments on the image data.
Reference: [3] <author> J.-F. Cardoso and B. Laheld, </author> <title> "Equivariant adaptive source separation," </title> <note> manuscript submitted to IEEE Trans. on Signal Processing, </note> <month> October </month> <year> 1994. </year>
Reference-contexts: The noise term n k is often omitted from (2), because under the weak assumptions made here it is usually not possible to separate the noise from the source signals. The source separation problem <ref> [10, 3, 16] </ref> is now to find an M fi L separating matrix B so that the M -vector y k = Bx k (3) is an estimate y k = ^ s k of the original independent source signals. <p> Most of the approaches dealing with signal separation and ICA are non-neural, and are based on some batch type or adaptive signal processing algorithm. However, some of these adaptive algorithms, such as the HJ algorithm and its modifications as well as the PFS/EASI algorithm proposed by Cardoso and Laheld <ref> [3, 16] </ref>, can be interpreted as learning algorithms of a neural network. Quite recently, some authors [1, 7] have derived unsupervised "neural" learning rules from information-theoretic measures. <p> Both these algorithms require that the original source signals have a kurtosis with the same sign: sgn (cum [s (i) 4 ]) = +1 or 1 for i = 1; : : : ; M . In <ref> [3] </ref>, Cardoso and Laheld show that this condition can be mildened for their PFS/EASI algorithm somewhat so that the sum of kurtosises must have the same sign for any two sources pairwisely. The same condition seems to hold for the neural learning algorithms (5),(6) in practice. <p> The other parameter fl was 0:9. Also the PFS/EASI algorithm <ref> [3, 16] </ref> performs well with suitable choices. B. Image data. Here we present a larger scale experiment with image data, taken from [22]. The 3 source signals were the digital images shown in Fig. 3, first row (flowers, model, waterfall).
Reference: [4] <author> A. Cichocki and R. Unbehauen, </author> <title> Neural Networks for Optimization and Signal Processing. </title> <address> New York: </address> <publisher> John Wiley, </publisher> <year> 1993. </year>
Reference-contexts: Introduction Principal Component Analysis (PCA) is a widely used technique in signal processing. It is now well-known how it can be realized in different ways using neural networks; for examples, cf. <ref> [4, 9, 21] </ref>. Recently, there has been an increasing interest in extending the unsupervised Hebbian learning rules used in PCA to nonlinear Hebbian learning: such techniques are often called nonlinear PCA methods. <p> Source Separation and Independent Component Analy sis In source separation for linear memoryless channels, and the related technique of Independent Component Analysis (ICA), the following basic data model is usually assumed (see e.g. <ref> [10, 6, 4] </ref>). There are M scalar-valued signals s k (1); :::; s k (M ) indexed by an index k. We assume that the signals have zero mean and they are mutually statistically independent.
Reference: [5] <author> P. Comon, </author> <title> "Separation of stochastic processes," </title> <booktitle> in Proc. of Workshop on Higher-Order Spectral Analysis, </booktitle> <address> Vail, Colorado, </address> <month> June </month> <year> 1989, </year> <pages> pp. 174-179. </pages>
Reference-contexts: Experimental results In this section, we demonstrate the performance of the ICA network of Fig. 1 using both artificial and real-world data. A. The artificial data by Comon. Consider first a test example used earlier by Comon <ref> [5] </ref>. Here, the original 3 source signals s k (1), s k (2), and s k (3) in (2) consist of uniformly distributed noise, a ramp signal, and a pure sinusoid. Figure 2a shows 100 samples of them.
Reference: [6] <author> P. Comon, </author> <title> "Independent component analysis anew concept?," </title> <booktitle> Signal Processing, </booktitle> <volume> vol. 36, </volume> <pages> pp. 287-314, </pages> <year> 1994. </year>
Reference-contexts: Several other authors have proposed neural extensions of PCA by choosing optimization of some information-theoretic criterion as their starting point; see [9, 26] for further information. Independent Component Analysis (ICA) is a useful extension of PCA that was developed in context with source or signal separation applications <ref> [6, 10] </ref>. In a sense, it is an extension of PCA: instead of requiring that the coefficients of a linear expansion of data vectors be uncorrelated, in ICA they must be mutually independent or as independent as possible. <p> Source Separation and Independent Component Analy sis In source separation for linear memoryless channels, and the related technique of Independent Component Analysis (ICA), the following basic data model is usually assumed (see e.g. <ref> [10, 6, 4] </ref>). There are M scalar-valued signals s k (1); :::; s k (M ) indexed by an index k. We assume that the signals have zero mean and they are mutually statistically independent. <p> More details of this example are given in the sixth section of this paper. It is usually not possible to verify the independence condition exactly in practice because the involved probability densities are unknown. Therefore, approximating contrast functions which are maximized for a separating matrix have been introduced <ref> [6] </ref>. Even these often lead to relatively intensive batch type computations.
Reference: [7] <author> G. Deco and W. </author> <title> Brauer, "Nonlinear higher-order statistical decorrelation by volume-conserving neural architectures," </title> <booktitle> Neural Networks, </booktitle> <volume> vol. 8, </volume> <pages> pp. 525 - 535, </pages> <year> 1995. </year>
Reference-contexts: However, some of these adaptive algorithms, such as the HJ algorithm and its modifications as well as the PFS/EASI algorithm proposed by Cardoso and Laheld [3, 16], can be interpreted as learning algorithms of a neural network. Quite recently, some authors <ref> [1, 7] </ref> have derived unsupervised "neural" learning rules from information-theoretic measures. The resulting algorithms show good separation performance, but are not truly realizable in neural networks because they are based on relatively complicated numerical operations, requiring for example the inversion of a matrix. <p> Attempts to extend source separation and ICA into these directions have already been made by some authors <ref> [2, 7, 17, 24] </ref>. Acknowledgement. We are grateful to Mr. Liuyue Wang for the experiments on the one-dimensional signals, and to Mr. Ricardo Vigario for the experiments on the image data.
Reference: [8] <author> J. Friedman, </author> <title> "Exploratory projection pursuit," </title> <journal> J. Amer. Statistical Assoc., </journal> <volume> vol. 82, no. 397, </volume> <pages> pp. 249-266, </pages> <year> 1987. </year>
Reference-contexts: They should be useful e.g. in Exploratory Projection Pursuit <ref> [8] </ref> where one tries to project the data onto directions that reveal as much of the structure as possible. In a sense, ICA basis vectors provide such directions.
Reference: [9] <author> S. Haykin, </author> <title> Neural Networks: A Comprehensive Foundation. </title> <address> New York: </address> <publisher> IEEE Computer Society Press and Macmillan, </publisher> <year> 1994. </year>
Reference-contexts: Introduction Principal Component Analysis (PCA) is a widely used technique in signal processing. It is now well-known how it can be realized in different ways using neural networks; for examples, cf. <ref> [4, 9, 21] </ref>. Recently, there has been an increasing interest in extending the unsupervised Hebbian learning rules used in PCA to nonlinear Hebbian learning: such techniques are often called nonlinear PCA methods. <p> We have earlier derived robust and nonlinear extensions of PCA starting either from maximization of output variances or minimization of mean-square representation error [11, 13]. Several other authors have proposed neural extensions of PCA by choosing optimization of some information-theoretic criterion as their starting point; see <ref> [9, 26] </ref> for further information. Independent Component Analysis (ICA) is a useful extension of PCA that was developed in context with source or signal separation applications [6, 10].
Reference: [10] <author> C. Jutten and J. Herault, </author> <title> "Blind separation of sources, part I: an adaptive algorithm based on neu-romimetic architecture," </title> <booktitle> Signal Processing, </booktitle> <volume> vol. 24, no. 1, </volume> <pages> pp. 1-10, </pages> <month> July </month> <year> 1991. </year>
Reference-contexts: Several other authors have proposed neural extensions of PCA by choosing optimization of some information-theoretic criterion as their starting point; see [9, 26] for further information. Independent Component Analysis (ICA) is a useful extension of PCA that was developed in context with source or signal separation applications <ref> [6, 10] </ref>. In a sense, it is an extension of PCA: instead of requiring that the coefficients of a linear expansion of data vectors be uncorrelated, in ICA they must be mutually independent or as independent as possible. <p> Source Separation and Independent Component Analy sis In source separation for linear memoryless channels, and the related technique of Independent Component Analysis (ICA), the following basic data model is usually assumed (see e.g. <ref> [10, 6, 4] </ref>). There are M scalar-valued signals s k (1); :::; s k (M ) indexed by an index k. We assume that the signals have zero mean and they are mutually statistically independent. <p> The noise term n k is often omitted from (2), because under the weak assumptions made here it is usually not possible to separate the noise from the source signals. The source separation problem <ref> [10, 3, 16] </ref> is now to find an M fi L separating matrix B so that the M -vector y k = Bx k (3) is an estimate y k = ^ s k of the original independent source signals. <p> Various approaches have been proposed for achieving separation. The learning algorithms are constructed in such a way that they should satisfy some kind of independence condition after convergence. An example is the seminal Herault-Jutten (HJ) algorithm <ref> [10] </ref>, which uses a neural-like network structure. This algorithm is simple and elegant, but may fail in separating more than two independent sources. Most of the approaches dealing with signal separation and ICA are non-neural, and are based on some batch type or adaptive signal processing algorithm.
Reference: [11] <author> J. Karhunen and J. Joutsensalo, </author> <title> "Representation and separation of signals using nonlinear PCA type learning," </title> <booktitle> Neural Networks, </booktitle> <volume> vol. 7, no. 1, </volume> <pages> pp. 113-127, </pages> <year> 1994. </year>
Reference-contexts: In PCA, the data are represented in an orthonormal basis determined merely by the second-order statistics (covariances) of the input data. Various nonlinear PCA methods take into account higher-order statistics, too, and they may have efficient implementations in learning neural networks <ref> [20, 11, 12, 13] </ref>. Nonlinear or robust PCA type methods can be developed from various starting points, usually leading to mutually different solutions. We have earlier derived robust and nonlinear extensions of PCA starting either from maximization of output variances or minimization of mean-square representation error [11, 13]. <p> Nonlinear or robust PCA type methods can be developed from various starting points, usually leading to mutually different solutions. We have earlier derived robust and nonlinear extensions of PCA starting either from maximization of output variances or minimization of mean-square representation error <ref> [11, 13] </ref>. Several other authors have proposed neural extensions of PCA by choosing optimization of some information-theoretic criterion as their starting point; see [9, 26] for further information. <p> Neural Independent Component Analysis Separation algorithms In [20], one of the authors proposed several nonlinear extensions of his learning rule for computing the standard PCA subspace. These extensions can be applied to learning a separating matrix for prewhitened inputs. One of the proposed extensions is the Nonlinear PCA rule <ref> [11, 13, 20] </ref>: W k+1 = W k + k [v k W k g (y k )]g (y T We assume here that the input vectors v k are obtained from the mixture vectors x k by whitening them, and we denote y k = W T k v k <p> Finally, we emphasize that preprocessing the input data by whitening is essential for achieving good separation results using nonlinear PCA type learning algorithms. Without whitening, the algorithms are able to somehow separate sinusoidal signals <ref> [11] </ref>, but usually not other signals. The obvious reason is that without whitening the algorithms still largely respond to second-order statistics in spite of using nonlinearities. Conclusions and remarks In this paper, we have introduced a neural network for performing Independent Component Analysis (ICA).
Reference: [12] <author> J. Karhunen, </author> <title> "Optimization criteria and nonlinear PCA neural networks," </title> <booktitle> in Proc. 1994 IEEE Int. Conf. on Neural Networks, </booktitle> <address> Orlando, Florida, </address> <month> June </month> <year> 1994, </year> <pages> pp. 1241-1246. </pages>
Reference-contexts: In PCA, the data are represented in an orthonormal basis determined merely by the second-order statistics (covariances) of the input data. Various nonlinear PCA methods take into account higher-order statistics, too, and they may have efficient implementations in learning neural networks <ref> [20, 11, 12, 13] </ref>. Nonlinear or robust PCA type methods can be developed from various starting points, usually leading to mutually different solutions. We have earlier derived robust and nonlinear extensions of PCA starting either from maximization of output variances or minimization of mean-square representation error [11, 13].
Reference: [13] <author> J. Karhunen and J. Joutsensalo, </author> <title> "Generalizations of principal component analysis, optimization problems, and neural networks," </title> <booktitle> Neural Networks, </booktitle> <volume> vol. 8, </volume> <pages> pp. 549 - 562, </pages> <year> 1995. </year>
Reference-contexts: In PCA, the data are represented in an orthonormal basis determined merely by the second-order statistics (covariances) of the input data. Various nonlinear PCA methods take into account higher-order statistics, too, and they may have efficient implementations in learning neural networks <ref> [20, 11, 12, 13] </ref>. Nonlinear or robust PCA type methods can be developed from various starting points, usually leading to mutually different solutions. We have earlier derived robust and nonlinear extensions of PCA starting either from maximization of output variances or minimization of mean-square representation error [11, 13]. <p> Nonlinear or robust PCA type methods can be developed from various starting points, usually leading to mutually different solutions. We have earlier derived robust and nonlinear extensions of PCA starting either from maximization of output variances or minimization of mean-square representation error <ref> [11, 13] </ref>. Several other authors have proposed neural extensions of PCA by choosing optimization of some information-theoretic criterion as their starting point; see [9, 26] for further information. <p> Neural Independent Component Analysis Separation algorithms In [20], one of the authors proposed several nonlinear extensions of his learning rule for computing the standard PCA subspace. These extensions can be applied to learning a separating matrix for prewhitened inputs. One of the proposed extensions is the Nonlinear PCA rule <ref> [11, 13, 20] </ref>: W k+1 = W k + k [v k W k g (y k )]g (y T We assume here that the input vectors v k are obtained from the mixture vectors x k by whitening them, and we denote y k = W T k v k <p> Thus kurtosis is minimized/maximized when f (y (j)) = y (j) 4 , when we assume that Efy (j) 2 g = 1. However, the relation of the Nonlinear PCA learning rule (5) is only indirectly related to an optimization criterion <ref> [13] </ref>, and so a convergence analysis should be provided. This section provides some results on the asymptotic solutions of the Nonlinear PCA learning rule.
Reference: [14] <author> J. Karhunen, L. Wang, and J. Joutsensalo, </author> <title> "Neural estimation of basis vectors in Independent Component Analysis", </title> <booktitle> in Proc. Int. Conf. on Artificial Neural Networks, </booktitle> <address> Paris, France, </address> <month> Oct. </month> <year> 1995. </year>
Reference-contexts: However, they may be useful because they show the directions in the input space aligned with the independent components; in a way, the ICA basis vectors are generalizations of the Principal Component Analysis eigenvectors but in many cases the ICA basis vectors characterize the data better <ref> [14] </ref>. They should be useful e.g. in Exploratory Projection Pursuit [8] where one tries to project the data onto directions that reveal as much of the structure as possible. In a sense, ICA basis vectors provide such directions. <p> They can be normalized and ordered suitably. A completely neural algorithm for estimating the basis vectors of ICA which does not require any inversion of matrices can be developed as follows <ref> [14] </ref>. Let us denote by Q the L fi M weight matrix whose columns are the desired estimates ^ a (j); j = 1; :::; M of the basis vectors of the ICA expansion (in any order). <p> The mean square error EfkxQykg must be minimal, for obtaining the pseudoinverse solution; 2. The components of vector y must be statistically independent. It is usually possible to satisfy both of these requirements, leading to the required ICA solution <ref> [14] </ref>. Assume that as a result of the whitening and separation stages, the matrix B k has converged to a separating solution B, and the components of y are as independent as possible.
Reference: [15] <author> J. Karhunen, L. Wang, and R. Vigario, </author> <title> "Nonlinear PCA type approaches for source separation and Independent Component Analysis", </title> <booktitle> in Proc. 1995 IEEE Int. Conf. on Neural Networks, </booktitle> <address> Perth, Australia, </address> <month> Dec. </month> <year> 1995. </year>
Reference-contexts: The same condition seems to hold for the neural learning algorithms (5),(6) in practice. A more extensive discussion about neural separation algorithms is given in <ref> [15] </ref>. Estimation of the basis vectors of ICA If the goal is signal separation only, then the ICA basis vectors are not needed explicitly. <p> The definitely poorer results on the third row of Fig. 3 show what standard PCA is able to achieve in this application. A more extensive demonstration in which 6 other source images are added to the set is given in <ref> [15] </ref>. The system is able to separate the 9 images with good results. In the simulations described above, the sources are mostly sub-Gaussian with a negative kurtosis. However, we have applied especially the bigradient algorithm for super-Gaussian sources that have positive kurtosis, too.
Reference: [16] <author> B. Laheld and J.-F. Cardoso, </author> <title> "Adaptive source separation with uniform performance," </title> <booktitle> in Signal Processing VII: Theories and Applications (Proc. </booktitle> <editor> EUSIPCO-94), M. Holt et al. (Eds.). Lausanne: EURASIP, </editor> <booktitle> 1994, </booktitle> <volume> vol. 2, </volume> <pages> pp. 183-186. </pages>
Reference-contexts: The noise term n k is often omitted from (2), because under the weak assumptions made here it is usually not possible to separate the noise from the source signals. The source separation problem <ref> [10, 3, 16] </ref> is now to find an M fi L separating matrix B so that the M -vector y k = Bx k (3) is an estimate y k = ^ s k of the original independent source signals. <p> Most of the approaches dealing with signal separation and ICA are non-neural, and are based on some batch type or adaptive signal processing algorithm. However, some of these adaptive algorithms, such as the HJ algorithm and its modifications as well as the PFS/EASI algorithm proposed by Cardoso and Laheld <ref> [3, 16] </ref>, can be interpreted as learning algorithms of a neural network. Quite recently, some authors [1, 7] have derived unsupervised "neural" learning rules from information-theoretic measures. <p> The other parameter fl was 0:9. Also the PFS/EASI algorithm <ref> [3, 16] </ref> performs well with suitable choices. B. Image data. Here we present a larger scale experiment with image data, taken from [22]. The 3 source signals were the digital images shown in Fig. 3, first row (flowers, model, waterfall).
Reference: [17] <author> K. Matsuoka, M. Ohya, and M. Kawamoto, </author> <title> "A neural net for blind separation of nonstationary signals," </title> <booktitle> Neural Networks, </booktitle> <volume> vol. 8, no. 3, </volume> <pages> pp. 411-419, </pages> <year> 1995. </year>
Reference-contexts: Attempts to extend source separation and ICA into these directions have already been made by some authors <ref> [2, 7, 17, 24] </ref>. Acknowledgement. We are grateful to Mr. Liuyue Wang for the experiments on the one-dimensional signals, and to Mr. Ricardo Vigario for the experiments on the image data.
Reference: [18] <author> E. Moreau and O. Macchi, </author> <title> "New self-adaptive algorithms for source separation based on contrast functions," </title> <booktitle> in Proc. IEEE Signal Proc. Workshop on Higher Order Statistics, </booktitle> <address> Lake Tahoe, USA, </address> <month> June </month> <year> 1993, </year> <pages> pp. 215-219. </pages>
Reference-contexts: Therefore, approximating contrast functions which are maximized for a separating matrix have been introduced [6]. Even these often lead to relatively intensive batch type computations. However, for prewhitened input vectors it can be shown <ref> [18] </ref> that a relatively simple contrast function, the sum of absolute values of the fourth order cumulants (kurtoses) J kurt (y) = M X j cum (y (i) 4 ) j = i=1 is maximized by a separating matrix B under certain conditions given by Moreau and Macchi [18]. <p> be shown <ref> [18] </ref> that a relatively simple contrast function, the sum of absolute values of the fourth order cumulants (kurtoses) J kurt (y) = M X j cum (y (i) 4 ) j = i=1 is maximized by a separating matrix B under certain conditions given by Moreau and Macchi [18]. The central conclusion is that among vectors of the form y = Hs, with vector s having independent components, the criterion (4) is maximized when y is either s itself or a permutation of its elements, possibly with changed signs.
Reference: [19] <author> E. Oja, </author> <title> Subspace methods of pattern recognition. </title> <publisher> Letchworth: Research Studies Press, </publisher> <year> 1983. </year>
Reference-contexts: To show this, the difference equation (12) can be further analyzed by writing down the corresponding averaged differential equation; for a discussion of the technique, see e.g. <ref> [19] </ref>. The limit of convergence of the difference equation is among the asymptotically stable solutions of the averaged differential equation.
Reference: [20] <author> E. Oja, H. Ogawa, and J. Wangviwattana, </author> <title> "Learning in nonlinear constrained Hebbian networks," </title> <booktitle> in Artificial Neural Networks (Proc. ICANN-91), </booktitle> <editor> T. Kohonen et al. (Eds.). </editor> <publisher> Amsterdam: North-Holland, </publisher> <year> 1991, </year> <pages> pp. 385-390. </pages>
Reference-contexts: In PCA, the data are represented in an orthonormal basis determined merely by the second-order statistics (covariances) of the input data. Various nonlinear PCA methods take into account higher-order statistics, too, and they may have efficient implementations in learning neural networks <ref> [20, 11, 12, 13] </ref>. Nonlinear or robust PCA type methods can be developed from various starting points, usually leading to mutually different solutions. We have earlier derived robust and nonlinear extensions of PCA starting either from maximization of output variances or minimization of mean-square representation error [11, 13]. <p> Kurtosis minimization/maximization is one of the criteria to be used in context with our neural nonlinear PCA type learning algorithms outlined in detail in the next Section. Neural Independent Component Analysis Separation algorithms In <ref> [20] </ref>, one of the authors proposed several nonlinear extensions of his learning rule for computing the standard PCA subspace. These extensions can be applied to learning a separating matrix for prewhitened inputs. <p> Neural Independent Component Analysis Separation algorithms In [20], one of the authors proposed several nonlinear extensions of his learning rule for computing the standard PCA subspace. These extensions can be applied to learning a separating matrix for prewhitened inputs. One of the proposed extensions is the Nonlinear PCA rule <ref> [11, 13, 20] </ref>: W k+1 = W k + k [v k W k g (y k )]g (y T We assume here that the input vectors v k are obtained from the mixture vectors x k by whitening them, and we denote y k = W T k v k
Reference: [21] <author> E. Oja, </author> <title> "Principal components, minor components, and linear neural networks," </title> <booktitle> Neural Networks, </booktitle> <volume> vol. 5, </volume> <pages> pp. 927-935, </pages> <year> 1992. </year>
Reference-contexts: Introduction Principal Component Analysis (PCA) is a widely used technique in signal processing. It is now well-known how it can be realized in different ways using neural networks; for examples, cf. <ref> [4, 9, 21] </ref>. Recently, there has been an increasing interest in extending the unsupervised Hebbian learning rules used in PCA to nonlinear Hebbian learning: such techniques are often called nonlinear PCA methods.
Reference: [22] <author> E. Oja, J. Karhunen, L. Wang, and R. Vigario, </author> <title> "Principal and independent components in neural networks recent developments," </title> <booktitle> to appear in Proc. Italian Workshop on Neural Networks WIRN'95, </booktitle> <address> Vietri, Italy, </address> <month> May </month> <year> 1995. </year>
Reference-contexts: The other parameter fl was 0:9. Also the PFS/EASI algorithm [3, 16] performs well with suitable choices. B. Image data. Here we present a larger scale experiment with image data, taken from <ref> [22] </ref>. The 3 source signals were the digital images shown in Fig. 3, first row (flowers, model, waterfall). We have not tested the mutual independence of these sources in any way.
Reference: [23] <author> E. Oja, </author> <title> "The Nonlinear PCA learning rule and signal separation mathematical analysis". </title> <type> Technical Report A26. </type> <month> August </month> <year> 1995, </year> <institution> Helsinki University of Technology, Lab. of Computer and Information Science. </institution>
Reference-contexts: This implies that the separating matrix must be orthogonal. To make the analysis easier, we proceed by making a linear transforma tion to the learning rule (5): we multiply both sides by R T , with R an orthogonal separating matrix <ref> [23] </ref>. We obtain R T W k+1 = R T W k + k [R T v k R T W k g (W T k W k ) (10) k RR T v k )]g (v T where we have used the fact that RR T = I. <p> We are ready to state the main result of this section, which is a simplified version of a more general theorem originally presented and proven by one of the authors in <ref> [23] </ref>: Theorem 1. In the matrix differential equation (13), assume the following: 1. The random vector u has a symmetrical density with Efug = 0; 2. The elements of u, denoted here u 1 ; :::; u n , are statistically mutually independent and all have the same density; 3. <p> The proof is given in <ref> [23] </ref>. Note 1. We only consider a diagonal matrix D = ffI as the asymptotically stable solution. However, any permutation of D can be shown to be an asymptotically stable solution, too, by making another orthogonal rotation of the coordinate axes that will permute some of them. <p> All these functions obviously satisfy the condition 3 of the Theorem. For more details, see <ref> [23] </ref>. 1. Polynomials. <p> It remains to check the stability conditions 4 and 5 of Theorem 1. Now, a closed form solution for ff in eq. (17) is not feasible and numerical methods must be used. It turns out that Condition (5) holds for fi &gt; 0 (for details, see <ref> [23] </ref>). Condition (4) is always satisfied. The conclusion is that for the uniform density the sigmoidal function gives asymptotic stability when fi &gt; 0. Asymptotic stability is a local effect, and Theorem 1 does not say anything about the basin of attraction of the asymptotic solution, i.e., global stability.
Reference: [24] <author> J. Platt and F. Faggin, </author> <title> "Networks for the separation of sources that are superimposed and delayed," </title> <booktitle> in Advances in Neural Processing Systems 4, </booktitle> <editor> J. Moody et al. (Eds.). </editor> <address> San Mateo, California: </address> <publisher> Morgan Kaufmann, </publisher> <year> 1991, </year> <pages> pp. 730-737. </pages>
Reference-contexts: Attempts to extend source separation and ICA into these directions have already been made by some authors <ref> [2, 7, 17, 24] </ref>. Acknowledgement. We are grateful to Mr. Liuyue Wang for the experiments on the one-dimensional signals, and to Mr. Ricardo Vigario for the experiments on the image data.
Reference: [25] <author> M. Plumbley, </author> <title> "A Hebbian/anti-Hebbian network which optimizes information capacity by orthonor-malizing the principal subspace, </title> <booktitle> in Proc. IEE Conf. on Artificial Neural Networks, </booktitle> <address> Brighton, UK, </address> <month> May </month> <year> 1993, </year> <pages> pp. 86-90. </pages>
Reference-contexts: In the ICA network of Fig. 1, the first layer is linear and whitens the input vectors x k . The whitening transformation V is implemented by the weights of the linear layer and can be learned in a neural learning algorithm introduced by Plumbley <ref> [25] </ref>: V k+1 = V k + k (V k x k x T k I)V k : (29) 2.
Reference: [26] <author> J. Taylor and M. Plumbley, </author> <title> "Information theory and neural networks," in Mathematical Approaches to Neural Networks, </title> <editor> J. Taylor (Ed.). </editor> <publisher> Amsterdam: Elsevier Science Publ., </publisher> <year> 1993, </year> <pages> pp. 307-340. </pages>
Reference-contexts: We have earlier derived robust and nonlinear extensions of PCA starting either from maximization of output variances or minimization of mean-square representation error [11, 13]. Several other authors have proposed neural extensions of PCA by choosing optimization of some information-theoretic criterion as their starting point; see <ref> [9, 26] </ref> for further information. Independent Component Analysis (ICA) is a useful extension of PCA that was developed in context with source or signal separation applications [6, 10].
Reference: [27] <author> L. Wang, J. Karhunen, and E. Oja, </author> <title> "A bigradient optimization approach for robust PCA, MCA, and source separation," </title> <booktitle> to appear in Proc. 1995 IEEE Int. Conf. on Neural Networks, </booktitle> <address> Perth, Australia, </address> <month> November </month> <year> 1995. </year>
Reference-contexts: We can justify that the Nonlinear PCA rule (5) converges to a separating matrix W by an analysis presented in the next section. We have recently developed another learning rule, the so-called bigradient algorithm <ref> [27] </ref>, which is applied for learning the orthonormal separating matrix W T as follows: W k+1 = W k + k v k g (y T k W k ): (6) Here k is again a small learning rate, this time positive or negative, and fl k is another positive gain <p> The algorithm (6) is derived and discussed in more detail in <ref> [27] </ref>. Kurtosis can be minimized or maximized by choosing g (y) = y 3 , although other choices are possible, too.
Reference: [28] <author> L. Wang, J. Karhunen, E. Oja, and R. Vigario, </author> <title> "Blind separation of sources using nonlinear PCA type learning algorithms", </title> <booktitle> to appear in Proc. Int. Conf. on Neural Networks and Signal Processing, </booktitle> <institution> Nanjing, P.R. China, </institution> <month> Dec. </month> <year> 1995. </year>
Reference-contexts: The system is able to separate the 9 images with good results. In the simulations described above, the sources are mostly sub-Gaussian with a negative kurtosis. However, we have applied especially the bigradient algorithm for super-Gaussian sources that have positive kurtosis, too. In <ref> [28] </ref>, up to 10 real speech signals were separated from their mixtures using the bigradient algorithm. The speech signals are typically super-Gaussian [1].
References-found: 28

