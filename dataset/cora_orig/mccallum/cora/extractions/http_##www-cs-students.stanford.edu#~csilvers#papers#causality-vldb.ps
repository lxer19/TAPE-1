URL: http://www-cs-students.stanford.edu/~csilvers/papers/causality-vldb.ps
Refering-URL: http://www-cs-students.stanford.edu/~csilvers/
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Email: E-mail: fcsilvers,brin,rajeev,ullmang@cs.stanford.edu  Email: csilvers@cs.stanford.edu  
Phone: Phone: (650) 723-1787 Fax: (650) 725-4671  
Title: Scalable Techniques for Mining Causal Structures  
Author: Craig Silverstein Sergey Brin Rajeev Motwani Jeff Ullman 
Address: Stanford, CA 94305  
Affiliation: Department of Computer Science Stanford University  of IBM and Hitachi Corp.  
Note: Paper number 146  Contact Author: Craig Silverstein Address: (as above)  Supported by an ARCS Fellowship and NSF Award CCR-9357849, with matching funds from IBM, Mitsubishi, Schlumberger Foundation, Shell Foundation, and Xerox Corporation. Supported by an Alfred P. Sloan Research Fellowship, an IBM Faculty Partnership Award, an ARO MURI Grant DAAH04-96-1-0007, and NSF Young Investigator Award CCR-9357849, with matching funds from IBM, Mitsubishi, Schlumberger Foundation, Shell Foundation, and Xerox Corporation. This work was partially supported by the Community Management Staff's Massive Digital Data Systems Program, NSF grant IRI-96-31952, ARO grant DAAH04-95-1-0192, and grants  
Abstract: Mining for association rules in market basket data has proved a fruitful area of research. Measures such as conditional probability (confidence) and correlation have been used to infer rules of the form "the existence of item A implies the existence of item B." However, such rules indicate only a statistical relationship between A and B. They do not specify the nature of the relationship: whether the presence of A causes the presence of B, or the converse, or some other attribute or phenomenon causes both to appear together. In applications, knowing such causal relationships is extremely useful for enhancing understanding and effecting change. While distinguishing causality from correlation is a truly difficult problem, recent work in statistics and Bayesian learning provide some avenues of attack. In these fields, the goal has generally been to learn complete causal models, which are essentially impossible to learn in large-scale data mining applications with a large number of variables. In this paper, we consider the problem of determining casual relationships, instead of mere associations, when mining market basket data. We identify some problems with the direct application of Bayesian learning ideas to mining large databases, concerning both the scalability of algorithms and the appropriateness of the statistical techniques, and introduce some initial ideas for dealing with these problems. We present experimental results from applying our algorithms on several large, real-world data sets. The results indicate that the approach proposed here is both computationally feasible and successful in identifying interesting causal structures. An interesting outcome is that it is perhaps easier to infer the lack of causality than to infer causality, information that is useful in preventing erroneous decision making. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> S. Abiteboul, R. Motwani, and S. Nestorov. </author> <title> Inferring structure in semistructured data. </title> <booktitle> PODS/SIGMOD Workshop on Management of Semistructured Data, </booktitle> <pages> pp. 42-47, </pages> <year> 1997. </year>
Reference: [2] <author> R. Agrawal, T. Imielinski, and A. Swami. </author> <title> Mining association rules between sets of items in large databases. </title> <booktitle> In Proceedings of the ACM SIGMOD International Conference on the Management of Data, </booktitle> <pages> pages 207-216, </pages> <month> May </month> <year> 1993. </year> <month> 20 </month>
Reference-contexts: An early approach, due to Agrawal, Imielinski, and Swami <ref> [2] </ref>, was to find a pair of items that occur together often (that is, have high support), and also have the property that one item often occurs in baskets containing the other item (that is, have high confidence). 1 In effect, this framework chooses conditional probability as the measure of interest.
Reference: [3] <author> R. Agrawal, T. Imielinski, and A. Swami. </author> <title> Database mining: a performance perspective. </title> <journal> IEEE Transactions on Knowledge and Data Engineering, </journal> <volume> 5(1993): </volume> <pages> 914-925. </pages>
Reference: [4] <author> R. Agrawal, H. Mannila, R. Srikant, H. Toivonen, and A.I. Verkamo. </author> <title> Fast discovery of association rules. </title> <booktitle> In Fayyad et al [13], </booktitle> <pages> pages 307-328, </pages> <year> 1996. </year>
Reference: [5] <author> R. Agrawal and R. Srikant. </author> <title> Fast algorithms for mining association rules in large databases. </title> <booktitle> In Proceedings of the 20th International Conference on Very Large Data Bases, </booktitle> <pages> pages 487-499, </pages> <month> September </month> <year> 1994. </year>
Reference: [6] <author> A. </author> <title> Agresti. Categorical Data Analysis. </title> <publisher> John Wiley & Sons, </publisher> <address> New York, </address> <year> 1990. </year>
Reference: [7] <author> A. </author> <title> Agresti. A Survey of exact inference for contingency tables. </title> <booktitle> Statistical Science 7(1992): </booktitle> <pages> 131-177. </pages>
Reference: [8] <author> A. Balke and J. Pearl. </author> <title> Probabilistic evaluation of counterfactual queries. </title> <booktitle> In Proceedings of the Tenth Conference on Uncertainty in Artificial Intelligence, </booktitle> <address> Seattle, WA, </address> <pages> pages 46-54. </pages>
Reference: [9] <author> S. Brin, R. Motwani, and C. Silverstein. </author> <title> Beyond market baskets: Generalizing association rules to correlations. </title> <booktitle> In Proceedings of the 1997 ACM SIGMOD Conference on Management of Data, </booktitle> <pages> pages 265-276, </pages> <address> Tucson, AZ, </address> <month> May </month> <year> 1997. </year>
Reference-contexts: Many variants of this interest measure have been considered in the literature, but they all have a flavor similar to conditional probability. These measures were critiqued by Brin, Motwani, and Silverstein <ref> [9] </ref>, who proposed statistical correlation as being a more appropriate interest measure for capturing the intuition behind association rules. <p> In our approach, we use instead the much simpler 2 statistic; refer to <ref> [9] </ref> for a discussion on using the chi-squared tests in market basket applications. The necessary fact is that if two boolean variables are independent, the 2 value is likely to exceed the threshold value 2 ff with probability at most ff. <p> It is not given for the AB pair because this pair is uncorrelated. and henceforth we shall only consider processor time in our comparisons. 5 Experimental Results We use two data sets for our analysis, similar to the ones used by Brin, Motwani, and Silverstein <ref> [9] </ref>. One holds boolean census data (Section 5.1). The other data set is a collection of text data from UPI and Reuters newswires (Section 5.2). We actually study two newsgroup corpora, one of which is significantly larger than the other. <p> We actually study two newsgroup corpora, one of which is significantly larger than the other. In the experiments below, we used a chi-squared cutoff c = 5% for C-edges and c = 95% for U - edges. We use the definition of support given by Brin, Motwani, and Silverstein <ref> [9] </ref>. All experiments were performed on a Pentium Pro with a 166 MHz processor running Solaris x86 2.5.1, with 96 Meg. of main memory.
Reference: [10] <author> S. Brin, R. Motwani, S. Tsur, and J.D. Ullman. </author> <title> Dynamic itemset counting and implication rules for market basket data. </title> <booktitle> In Proceedings of the 1997 ACM SIGMOD Conference on Management of Data, </booktitle> <pages> pages 255-264, </pages> <address> Tucson, AZ, </address> <month> May </month> <year> 1997. </year>
Reference: [11] <author> G. Cooper. </author> <title> A simple constraint-based algorithm for efficiently mining observational databases for causal relationships. Data Mining and Knowledge Discovery, </title> <type> 2(1997). </type>
Reference-contexts: discovery, which focuses on learning complete causal models for small data sets [8, 12, 14, 15, 16, 17, 21, 22, 23, 25, 26, 27], and an offshoot of the Bayesian learning method called constraint-based causal discovery, which use the data to limit | sometimes severely | the possible causal models <ref> [11, 26, 24] </ref>. While techniques in the first class are still not practical on very large data sets, a limited version of the constraint-based approach is linear in the database size and thus practical on even gigabytes of data. <p> Despite the cubic time bound, the algorithm proves to be practical for databases with thousands of items. In this paper, we explore the applicability of a constraint-based causal discovery to discovering causal relationships in market basket data. Particularly, we build on ideas presented by Cooper <ref> [11] </ref>, using local tests to find a subset of the causal relationships. In the rest of this section, we discuss causality for data mining in the context of research into causal learning. We begin, in Section 2 with a particular constraint-based algorithm, due to Cooper [11], upon which we build the <p> on ideas presented by Cooper <ref> [11] </ref>, using local tests to find a subset of the causal relationships. In the rest of this section, we discuss causality for data mining in the context of research into causal learning. We begin, in Section 2 with a particular constraint-based algorithm, due to Cooper [11], upon which we build the algorithms 2 This example is borrowed from a talk given by Heckerman. 3 presented in this paper. <p> We then enhance the algorithm so that for the first time causality can be inferred in large-scale market-basket problems. * Section 2 introduces "CCU" inferences, a form of causal structure not used by <ref> [11] </ref>. * Section 3 discusses weaknesses of the Cooper algorithm, notably a susceptibility to statistical error, and how power statistics such as correlation can be used to mitigate these problems. * In Section 4 we describe in detail the algorithms we developed for discovering causal relationships, and we also discuss discovery <p> For our class of 4 applications, the so-called constraint-based causal discovery method [24, 26] appears to be more useful. The basic insight here, as articulated by Cooper <ref> [11] </ref>, is that information about statistical independence and dependence relationships among a set of variables can be used to constrain (sometimes significantly) the possible causal relationships among a subset of the variables. <p> These constraint-based algorithms, like the Bayesian algorithms, attempt to form a complete causal model and therefore can take exponential time. (Due to the complexity of their causal tests, they may also be less reliable than simpler algorithms.) Cooper <ref> [11] </ref> has described an algorithm called LCD that is a special case of the PC and FCI algorithms and runs in polynomial time. <p> constraint-based methods as being well suited to market basket analysis, while others indicate that tailoring constraint-based methods | for instance by providing error analysis predicated on boolean data and discovering lack of causality | can yield sizable advantages over using generic constraint-based techniques. 2 The LCD Algorithm The LCD algorithm <ref> [11] </ref> is a polynomial time, constraint-based algorithm. It uses tests of variable dependence, independence, and conditional independence to restrict the possible causal relationships between variables. The crux of this technique is the Markov condition [26]. <p> If one test wrongly indicates dependence or conditional independence, the results will be invalid, with both false positives and false negatives. An additional assumption, as has already been stated, is in the applicability of the Markov condition. We list some other assumptions, as described by Cooper <ref> [11] </ref>, and their validity for market basket data. Database Completeness The value of every variable is known for every database record. This is commonly assumed for market basket applications. Discrete Variables Every variable has a finite number of possible values. The market basket problem has boolean variables. <p> While not ideal, finding only a small number of causal relationships is acceptable for data mining. 3 Determining Dependence and Independence Cooper <ref> [11] </ref> uses tests for dependence and independence as primitives in the LCD algorithm, and also proposes Bayesian statistics for these tests. In our approach, we use instead the much simpler 2 statistic; refer to [9] for a discussion on using the chi-squared tests in market basket applications.
Reference: [12] <author> G. Cooper and E. Herskovits. </author> <title> A Bayesian method for the Induction of Probabilistic Networks from Data. </title> <journal> Machine Learning, </journal> <volume> 9, </volume> <pages> pages 309-347. </pages>
Reference: [13] <author> U.M. Fayyad, G. Piatetsky-Shapiro, P. Smyth, and R. Uthrusamy. </author> <title> Advances in Knowledge Discovery and Data Mining. </title> <publisher> AAAI Press, </publisher> <address> Menlo Park, CA, </address> <year> 1996. </year>
Reference: [14] <author> D. Heckerman. </author> <title> A Bayesian approach to learning causal networks. </title> <booktitle> In Proceedings of the Eleventh Conference on Uncertainty in Artificial Intelligence, </booktitle> <pages> pages 285-295, </pages> <year> 1995. </year>
Reference: [15] <author> D. Heckerman. </author> <title> Bayesian networks for data mining. Data Mining and Knowledge Discovery, </title> <booktitle> 1(1997): </booktitle> <pages> 79-119. </pages>
Reference: [16] <author> D. Heckerman, D. Geiger, and D. Chickering. </author> <title> Learning Bayesian networks: The combination of knowledge and statistical data. </title> <booktitle> In Proceedings of the Tenth Conference on Uncertainty in Artificial Intelligence, </booktitle> <pages> pages 293-301, </pages> <month> July </month> <year> 1994. </year>
Reference: [17] <author> D. Heckerman, C. Meek, and G. Cooper. </author> <title> A Bayesian approach to causal discovery. </title> <type> Technical Report MSR-TR-97-05, </type> <institution> Microsoft Research, </institution> <month> February </month> <year> 1997. </year>
Reference: [18] <author> D. Heckerman and R. Shachter. </author> <title> Decision-theoretic foundations for causal reasoning. </title> <journal> Journal of Artificial Intelligence Research, </journal> <volume> 3(1995), </volume> <pages> pages 405-430. </pages>
Reference: [19] <author> D. Heckerman and R. Shachter. </author> <title> A definition and graphical representation of causality. </title> <booktitle> In Proceedings of the Conference on Uncertainty in Artificial Intelligence, </booktitle> <year> 1995, </year> <pages> pages 262-273. 21 </pages>
Reference: [20] <author> W. Mendenhall, R. Scheaffer, and D. Wackerly. </author> <title> Mathematical Statistics with Applications. </title> <publisher> Duxbury Press, 3rd edition, </publisher> <year> 1986. </year>
Reference-contexts: If the manager could infer the correct causal model, or even infer that "hot-dogs causes barbecue-sauce" is not part of any possible causal model, he could avoid a pricing fiasco. A basic tenet of classical statistics ([6], <ref> [20] </ref>) is that correlation does not imply causation. Thus, it appears impossible to infer causal relationships from mere observational data available for data mining, since we can only infer correlations from such data.
Reference: [21] <author> J. Pearl. </author> <title> From Bayesian networks to causal networks. </title> <booktitle> In Proceedings of the Adaptive Computing and Information Processing Seminar, </booktitle> <pages> pages 25-27, </pages> <month> January </month> <year> 1993. </year>
Reference: [22] <author> J. Pearl. </author> <title> Causal diagrams for empirical research. </title> <journal> Biometrika, </journal> <volume> 82(1995): </volume> <pages> 669-709. </pages>
Reference: [23] <author> J. Pearl. </author> <title> Graphical models for probabilistic and causal reasoning. </title> <booktitle> The Computer Science and Engineering Handbook, </booktitle> <year> 1997, </year> <pages> pages 697-714, </pages>
Reference: [24] <author> J. Pearl and T.S. Verma. </author> <title> A theory of inferred causation. </title> <booktitle> In Proceedings of the Second International Conference on the Principles of Knowledge Representation and Reasoning, </booktitle> <year> 1991, </year> <pages> pages 441-452. </pages>
Reference-contexts: discovery, which focuses on learning complete causal models for small data sets [8, 12, 14, 15, 16, 17, 21, 22, 23, 25, 26, 27], and an offshoot of the Bayesian learning method called constraint-based causal discovery, which use the data to limit | sometimes severely | the possible causal models <ref> [11, 26, 24] </ref>. While techniques in the first class are still not practical on very large data sets, a limited version of the constraint-based approach is linear in the database size and thus practical on even gigabytes of data. <p> In our view, inferring complete causal models (i.e., causal Bayesian networks) is essentially impossible in large-scale data mining applications with thousands of variables. For our class of 4 applications, the so-called constraint-based causal discovery method <ref> [24, 26] </ref> appears to be more useful. The basic insight here, as articulated by Cooper [11], is that information about statistical independence and dependence relationships among a set of variables can be used to constrain (sometimes significantly) the possible causal relationships among a subset of the variables.
Reference: [25] <author> P. Spirtes, C. Glymour, and R. Scheiner. </author> <title> An algorithm for fast recovery of sparse causal graphs. </title> <journal> Social Science Computer Review, </journal> <volume> 9(1991): </volume> <pages> 62-72. </pages>
Reference: [26] <author> P. Spirtes, C. Glymour, and R. Scheines. </author> <title> Causation, Prediction, and Search. </title> <publisher> Springer-Verlag, </publisher> <address> New York, </address> <year> 1993. </year>
Reference-contexts: discovery, which focuses on learning complete causal models for small data sets [8, 12, 14, 15, 16, 17, 21, 22, 23, 25, 26, 27], and an offshoot of the Bayesian learning method called constraint-based causal discovery, which use the data to limit | sometimes severely | the possible causal models <ref> [11, 26, 24] </ref>. While techniques in the first class are still not practical on very large data sets, a limited version of the constraint-based approach is linear in the database size and thus practical on even gigabytes of data. <p> In our view, inferring complete causal models (i.e., causal Bayesian networks) is essentially impossible in large-scale data mining applications with thousands of variables. For our class of 4 applications, the so-called constraint-based causal discovery method <ref> [24, 26] </ref> appears to be more useful. The basic insight here, as articulated by Cooper [11], is that information about statistical independence and dependence relationships among a set of variables can be used to constrain (sometimes significantly) the possible causal relationships among a subset of the variables. <p> It has been shown that, under some reasonable set of assumptions about the data (to be discussed later), a whole array of valid constraints can be derived on the causal relationships between the variables. Constraint-based methods provide an alternative to Bayesian methods. The PC and FCI algorithms <ref> [26] </ref> use observational data to constrain the possible causal relationships between variables. They allow claims to be made such as "X causes Y ," "X is not caused by Y ," "X and Y have a common cause," and so on. <p> It uses tests of variable dependence, independence, and conditional independence to restrict the possible causal relationships between variables. The crux of this technique is the Markov condition <ref> [26] </ref>. Definition 1 (Markov Condition) Let A be a node in a causal Bayesian network, and let B be any node that is not a descendant of A in the causal network. Then the Markov condition holds if A and B are independent, conditioned on the parents of A.
Reference: [27] <author> P. Spirtes, C. Meek, and T. Richardson. </author> <title> Causal inference in the presence of latent variables and selection bias. </title> <booktitle> In Proceedings of the Eleventh Conference on Uncertainty in Artificial Intelligence, </booktitle> <pages> pages 499-506, </pages> <year> 1995. </year>
Reference: [28] <author> R. Srikant and R. Agrawal. </author> <title> Mining generalized association rules. </title> <booktitle> In Proceedings of the 21st International Conference on Very Large Data Bases, </booktitle> <pages> pages 407-419, </pages> <month> September </month> <year> 1995. </year>
Reference: [29] <author> H. Toivonen. </author> <title> Sampling large databases for finding association rules. </title> <booktitle> In Proceedings of the 22nd International Conference on Very Large Data Bases, </booktitle> <pages> pages 134-145, </pages> <month> September </month> <year> 1996. </year>
Reference: [30] <author> S. Wright. </author> <title> Correlation and causation. </title> <journal> Journal of Agricultural Research, </journal> <volume> 20(1921): </volume> <pages> 557-585. </pages>
References-found: 30

