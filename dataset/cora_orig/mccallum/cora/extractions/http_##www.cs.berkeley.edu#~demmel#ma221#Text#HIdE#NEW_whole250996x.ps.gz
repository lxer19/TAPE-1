URL: http://www.cs.berkeley.edu/~demmel/ma221/Text/HIdE/NEW_whole250996x.ps.gz
Refering-URL: http://www.cs.berkeley.edu/~randit/cs267-project.lsi.html
Root-URL: http://www.cs.berkeley.edu
Abstract-found: 0
Intro-found: 1
Reference: [1] <author> R. Agarwal, F. Gustavson, and M. Zubair. </author> <title> Exploiting functional parallelism of POWER2 to design high performance numerical algorithms. </title> <journal> IBM J. of Research and Development, </journal> <volume> 38(5) </volume> <pages> 563-576, </pages> <month> September </month> <year> 1994. </year>
Reference-contexts: Here is an example where the error bound of the last section is much too pessimistic. Example 2.1 Let A = diag (fl; 1) (a diagonal matrix with entries a 11 = fl and a 22 = 1) and b = <ref> [fl; 1] </ref> T , where fl &gt; 1. Then x = A 1 b = [1; 1] T . <p> Example 2.1 Let A = diag (fl; 1) (a diagonal matrix with entries a 11 = fl and a 22 = 1) and b = [fl; 1] T , where fl &gt; 1. Then x = A 1 b = <ref> [1; 1] </ref> T . Any reasonable direct method will solve Ax = b very accurately (using two divisions b i =a ii ) to get ^x, yet the condition number (A) = fl may be arbitrarily large. Therefore our error bound (2.3) may be arbitrarily large. <p> Recall that Theorem 2.1 related the condition number (A) to the distance from A to the nearest singular matrix. For a similar interpretation of CR (A), see [71, 206]. 42 Chapter 2 Example 2.2 Consider our earlier example with A = diag (fl; 1) and b = <ref> [fl; 1] </ref> T . It is easy to confirm that CR (A) = 1, since jA 1 j jAj = I. Indeed, CR (A) = 1 for any diagonal matrix A, capturing our intuition that a diagonal system of equations should be solvable quite accurately. <p> Let us see what happens when we go on to solve Ax = <ref> [1; 2] </ref> T for x using this LU factorization. The correct answer is x [1; 1] T . Instead we get the following. Solving Ly = [1; 2] T yields y 1 = fl (1=1) = 1 and y 2 = fl (2 10 4 1) = 10 4 ; note <p> Let us see what happens when we go on to solve Ax = [1; 2] T for x using this LU factorization. The correct answer is x <ref> [1; 1] </ref> T . Instead we get the following. Solving Ly = [1; 2] T yields y 1 = fl (1=1) = 1 and y 2 = fl (2 10 4 1) = 10 4 ; note that the value 2 has been "lost" by subtracting 10 4 from it. <p> Let us see what happens when we go on to solve Ax = <ref> [1; 2] </ref> T for x using this LU factorization. The correct answer is x [1; 1] T . Instead we get the following. Solving Ly = [1; 2] T yields y 1 = fl (1=1) = 1 and y 2 = fl (2 10 4 1) = 10 4 ; note that the value 2 has been "lost" by subtracting 10 4 from it. <p> Recall that kBk 1 is defined by kBk 1 = max kBxk 1 kxk 1 j i=1 It is easy to show that the maximum over x 6= 0 is attained at x = e j 0 = <ref> [0; :::; 0; 1; 0; :::; 0] </ref> T (the single nonzero entry is component j 0 where max j i jb ij j occurs at j = j 0 ). <p> Let = 3=2 29 , = 2 14 , A = 4 1 1 0 3 4 6:1035 10 5 6:1035 10 5 0 3 and b = A <ref> [1; 1 + "; 1] </ref> T . A can be computed without any roundoff error, but b has a bit of roundoff, which means that it is not exactly in the space spanned by the columns of A, so Ax = b has no solution. <p> There may be special hardware instructions which perform both a multiply and an addition in one cycle. It may also be possible to execute several multiply-add operations simultaneously if they do not interfere. For a detailed discussion of these issues for one high performance workstation, the IBM RS6000/590, see <ref> [1] </ref>, PARALLEL HOMEPAGE, or http://www.austin.ibm.com/tech/essl.html. Figure 2.5 shows the speeds of the three basic BLAS for this machine. The horizontal axis is matrix size, and the vertical axis is speed in megaflops. The peak machine speed is 266 megaflops. The top curve (peaking near 250 megaflops) is square matrix-matrix multiplication. <p> We claim u T A ~ V = 0 too because otherwise = kAk 2 = kU T AV k 2 k <ref> [1; 0; : : : ; 0] </ref>U T AV k 2 = k [ju T A ~ V ]k 2 &gt; , a contradiction (we have used part 7 of Lemma 1.7). So U T AV = 0 0 ~ A . <p> For the sake of argument, suppose that columns 3 and 4 are identical (which may be the case if the weights are rounded to the nearest pound). This means that matrix A is rank deficient, and that x 0 = <ref> [0; 0; 1; 1; 0; :::; 0] </ref> T is a right null vector of A. <p> For example, the 2-by-2 matrix A = diag (1; 0) is exactly singular, and its smallest nonzero singular value is = 1. As described in Proposition 3.3, the minimum norm least squares solution to min x kAx bk 2 with b = <ref> [1; 1] </ref> T is x = [1; 0] T , with condition number 1= = 1. But if we make an arbitrarily tiny perturbation to get ^ A = diag (1; *), then drops to *, x = [1; 1=*] T becomes enormous, and so does its condition number 1=*. <p> For example, the 2-by-2 matrix A = diag (1; 0) is exactly singular, and its smallest nonzero singular value is = 1. As described in Proposition 3.3, the minimum norm least squares solution to min x kAx bk 2 with b = [1; 1] T is x = <ref> [1; 0] </ref> T , with condition number 1= = 1. But if we make an arbitrarily tiny perturbation to get ^ A = diag (1; *), then drops to *, x = [1; 1=*] T becomes enormous, and so does its condition number 1=*. <p> But if we make an arbitrarily tiny perturbation to get ^ A = diag (1; *), then drops to *, x = <ref> [1; 1=*] </ref> T becomes enormous, and so does its condition number 1=*. In general, roundoff will make such tiny perturbations, of magnitude O (")kAk 2 . As we just saw, this can increase the condition number from 1= to 1=". We deal with this discontinuity algorithmically as follows. <p> In section 4.3, we will see some of the difficulties such matrices cause. Symmetric matrices, discussed in Chapter 5, are never defective. Proposition 4.2 A Jordan block has only one right eigenvector, e 1 = <ref> [1; 0; : : : ; 0] </ref> T and one left eigenvector e n = [0; : : : ; 0; 1] T . Therefore a matrix has n eigenvectors matching its n eigenvalues if and only if it is diagonalizable. <p> Symmetric matrices, discussed in Chapter 5, are never defective. Proposition 4.2 A Jordan block has only one right eigenvector, e 1 = [1; 0; : : : ; 0] T and one left eigenvector e n = <ref> [0; : : : ; 0; 1] </ref> T . Therefore a matrix has n eigenvectors matching its n eigenvalues if and only if it is diagonalizable. In this case, S 1 AS = diag ( i ). <p> Each component of S T x is bounded by 1 by the Cauchy-Schwartz inequality so kS T xk 2 fl <ref> [1; : : : ; 1] </ref> T fl p Proof of the Bauer-Fike Theorem: We will apply Gershgorin's Theorem to S 1 (A + ffiA)S = fl + F , where fl = S 1 AS = diag ( 1 ; :::; n ) and F = S 1 ffiAS. <p> Then x = e 1 and y is parallel to ~y = <ref> [1; A 12 (I A 22 ) 1 ] </ref> fl , or y = ~y=k~yk 2 . <p> Now GW = W H implies G (W ) i = (GW ) i = (W H) i = P i+1 h i+1;i (W ) i+1 = G (W ) i j=1 h ji (W ) j . Since (W ) 1 = <ref> [1; 0; : : : ; 0] </ref> T and G is upper Hessenberg, we can use induction on i to show that (W ) i is nonzero in entries 1 to i only; i.e. W is upper triangular. <p> 2 4 . . . 1 7 m by m + 1 Right singular block L T 2 6 4 . . . 1 3 7 5 m + 1 by m Left singular block L m is called a right singular block because it has a right null vector <ref> [ m ; m1 ; ; 1] </ref> for all . L T m has an analogous left null vector. For a proof, see [108]. Just as Schur form generalized to regular matrix pencils in the last section, it can be generalized to arbitrary singular pencils as well. <p> Let A = 1 + g * , where 0 &lt; * &lt; g. Let x = <ref> [1; 0] </ref> T and fi = (x; A) = 1 + g. Then r = Ax fix = [0; *] T and krk 2 = *. <p> We will see below that this bound is much looser than bound (5.7). When * is much smaller than g, there will be one eigenvalue near 1 + g with its eigenvector near x, and another eigenvalue near 1 with its eigenvector near <ref> [0; 1] </ref> T . <p> If one uses the shift i = a nn in QR iteration, and starts Rayleigh Quotient Iteration with x 0 = <ref> [0; :::; 0; 1] </ref> T , then the connection between QR and Inverse Iteration discussed in section 4.4 can be used to show that the sequence of i and i from the two algorithms are identical (see Question 5.13). <p> Consider running shifted QR iteration (Algorithm 4.5 on page 177) with a Rayleigh Quotient shift ( i = a nn ) at every iteration, yielding a sequence 1 ; 2 ; ::: of shifts. Also run Rayleigh Quotient Iteration (Algorithm 5.1 on page 237), starting with x 0 = <ref> [0; :::; 0; 1] </ref> T , yielding a sequence of Rayleigh Quotients 1 ; 2 ; :::. Show that these sequences are identical: i = i for all i. This justifies the claim in section 5.3.2 on page 236, that shifted QR iteration enjoys local cubic convergence. <p> We want to prove kR GS k 1 = kjR GS jek 1 kjR J jek 1 = kR J k 1 ; (6.22) where e = <ref> [1; :::; 1] </ref> T is the vector of all ones. <p> If T k is nonsingular and x k = Q k T 1 k e 1 kbk 2 , where e kfi1 1 = <ref> [1; 0; :::0] </ref> T , then Q T k r k = 0. If A is also positive definite, then T k must be nonsingular, and this choice of x k also minimizes kr k k A 1 over all x k 2 K k . <p> . . . 3 7 5 2 6 4 . . . d k 7 7 6 6 1 . . . . . . 3 7 5 = L k1 k1 1 diag (D k1 ; d k ) l k1 ^e T T where ^e T k1 = <ref> [0; :::; 0; 1] </ref> has dimension k 1. <p> Chapter 6 is called a discrete Fourier transform because of its close mathematical relationship to two other kinds of Fourier analyses: the Fourier Transform F () = R 1 and its inverse f (x) = R 1 the Fourier series c j = R 1 where f is periodic on <ref> [0; 1] </ref> and its inverse f (x) = P 1 the discrete Fourier Transform y j = (x) j = P N1 and its inverse x k = ( 1 y) k = 1 N j=0 e +2ijk=N y j We will make this close relationship more concrete in two ways. <p> In this section we show how the algorithm for solving the discrete model problem is a natural analog of using Fourier series to solve the original differential equation (6.1). We will do this for the one-dimensional model problem. Recall that Poisson's equation on <ref> [0; 1] </ref> is d 2 v dx 2 = f (x) with boundary conditions v (0) = v (1). <p> We let k = 1 and Q 1 = [e 1 ], so T 1 = 1 + g and the approximate eigenvector is simply e 1 . But as shown in Example 5.4, the eigenvectors of T are close to <ref> [1; *=g] </ref> T and [*=g; 1] T . So without a lower bound on g, i.e. the gap between the eigenvalue of T k and all the other eigenvalues, including those of T u , we cannot bound the error in the computed eigenvector. <p> We let k = 1 and Q 1 = [e 1 ], so T 1 = 1 + g and the approximate eigenvector is simply e 1 . But as shown in Example 5.4, the eigenvectors of T are close to [1; *=g] T and <ref> [*=g; 1] </ref> T . So without a lower bound on g, i.e. the gap between the eigenvalue of T k and all the other eigenvalues, including those of T u , we cannot bound the error in the computed eigenvector. <p> for j = 1 to j = k, and write these k equations as the single equation AQ k = Q k T k + [0; :::; 0; fi k q k+1 ] + F k k + F k ; where e T k is the k-dimensional row vector <ref> [0; :::; 0; 1] </ref> and F k = [f 1 ; :::; f k ] is the matrix of roundoff errors. We simplify notation by dropping the subscript k, to get AQ = QT + fiqe T + F .
Reference: [2] <author> L. Ahlfors. </author> <title> Complex Analysis. </title> <publisher> McGraw-Hill, </publisher> <year> 1966. </year>
Reference-contexts: Let us see what happens when we go on to solve Ax = <ref> [1; 2] </ref> T for x using this LU factorization. The correct answer is x [1; 1] T . Instead we get the following. Solving Ly = [1; 2] T yields y 1 = fl (1=1) = 1 and y 2 = fl (2 10 4 1) = 10 4 ; note <p> Let us see what happens when we go on to solve Ax = <ref> [1; 2] </ref> T for x using this LU factorization. The correct answer is x [1; 1] T . Instead we get the following. Solving Ly = [1; 2] T yields y 1 = fl (1=1) = 1 and y 2 = fl (2 10 4 1) = 10 4 ; note that the value 2 has been "lost" by subtracting 10 4 from it. <p> Proof: It suffices to prove the continuity of roots of polynomials, since the coefficients of the characteristic polynomial are continuous (in fact polynomial) functions of the matrix entries. We use the argument principle from complex analysis <ref> [2] </ref>: the number of roots of a polynomial p inside a simple closed curve fl is 1 2i fl p (z) dz. If p is changed just a lit tle, p (z) is changed just a little, so 1 2i fl p (z) dz is changed just a little.
Reference: [3] <author> A. Aho, J. Hopcroft, and J. Ullman. </author> <title> The design and analysis of computer algorithms. </title> <publisher> Addison-Wesley, </publisher> <year> 1974. </year>
Reference-contexts: Both the above matrix-matrix multiplication algorithms perform 2n 3 arithmetic operations. It turns out that there are other implementations of matrix-matrix multiplication that use far fewer operations. Strassen's method <ref> [3] </ref> was the first of these algorithms to be discovered, and is the simplest to explain. <p> The Basic Linear Algebra Subroutines are described in [85, 87, 167]. These and other routines are available electronically in NETLIB. An analysis of blocking strategies for matrix multiplication is given in [149]. Strassen's matrix multiplication algorithm is presented in <ref> [3] </ref>, its performance in practice described in [22], and its numerical stability described in [76, 147]. A survey of parallel and other block algorithms is given in [75]. <p> Domain decomposition are discussed in [48, 114, 203, 230]. Chebyshev and other polynomials are discussed in [238]. The FFT is discussed in any good textbook on computer science algorithms, such as <ref> [3] </ref>, or in [246]. A stabilized version of block cyclic reduction is found in [46, 45]. Iterative Methods for Linear Systems 395 6.12 Questions for Chapter 6 Question 6.1 (Easy) Prove Lemma 6.1 on page 296.
Reference: [4] <author> G. Alefeld and J. Herzberger. </author> <title> Introduction to interval computations. </title> <publisher> Academic Press, </publisher> <year> 1983. </year>
Reference: [5] <author> P. R. Amestoy and I. S. Duff. </author> <title> Vectorization of a multiprocessor multifrontal code. </title> <journal> The International Journal of Supercomputer Applications, </journal> <volume> 3 </volume> <pages> 41-59, </pages> <year> 1989. </year>
Reference-contexts: SuperLU LL, partial, BLAS-2.5 Pub/UCB nonsym. UMFPACK [61, 62] MF, Markowitz, BLAS-3 Pub/NETLIB MA38 (same as UMFPACK) Com/HSL nonsym. MA48 [94] Anal: RL, Markowitz Com/HSL Fact: LL, partial, BLAS-1, SD nonsym. SPARSE [165] RL, Markowitz, Scalar Pub/NETLIB sym pattern MUPS <ref> [5] </ref> MA42 [96] MF, threshold, BLAS-3 Frontal, BLAS-3 Com/HSL sym. MA27 [95]/MA47 [93] MF, LDL T , BLAS-1 Com/HSL s.p.d. Ng & Peyton [189] LL, BLAS-3 Pub/Author Shared Memory Algorithms nonsym. SuperLU LL, partial, BLAS-2.5 Pub/UCB nonsym.
Reference: [6] <author> Patrick R. Amestoy. </author> <title> Factorization of large unsymmetric sparse matrices based on a multifrontal approach in a multiprocessor environment. </title> <type> Technical Report TH/PA/91/2, </type> <institution> CERFACS, Toulouse, France, </institution> <month> February </month> <year> 1991. </year> <type> Ph.D thesis. </type>
Reference-contexts: MA27 [95]/MA47 [93] MF, LDL T , BLAS-1 Com/HSL s.p.d. Ng & Peyton [189] LL, BLAS-3 Pub/Author Shared Memory Algorithms nonsym. SuperLU LL, partial, BLAS-2.5 Pub/UCB nonsym. PARASPAR [268, 269] RL, Markowitz, BLAS-1, SD Res/Author sym- MUPS <ref> [6] </ref> MF, threshold, BLAS-3 Res/Author pattern nonsym. George & Ng [113] RL, partial, BLAS-1 Res/Author s.p.d. Gupta, Rothberg, LL, BLAS-3 Com/SGI Ng & Peyton [131] Pub/Author s.p.d.
Reference: [7] <author> A. Anda and H. Park. </author> <title> Fast plane rotations with dynamic scaling. </title> <journal> SIAM J. Mat. Anal. Appl., </journal> <volume> 15 </volume> <pages> 162-174, </pages> <year> 1994. </year>
Reference-contexts: Note also that we may recover either s and c, or s and c; this is adequate in practice. There is also a way to apply a sequence of Givens rotations while performing fewer floating point operations than described above. These are called Fast Givens rotations <ref> [7, 8, 33] </ref>.
Reference: [8] <author> A. Anda and H. Park. </author> <title> Self scaling fast rotations for stiff least squares problems. </title> <journal> Lin. Alg. Appl., </journal> <volume> 234 </volume> <pages> 137-162, </pages> <year> 1996. </year>
Reference-contexts: Note also that we may recover either s and c, or s and c; this is adequate in practice. There is also a way to apply a sequence of Givens rotations while performing fewer floating point operations than described above. These are called Fast Givens rotations <ref> [7, 8, 33] </ref>.
Reference: [9] <author> A. Anderson, D. Culler, D. Patterson, </author> <title> and the NOW Team. A case for networks of workstations: NOW. </title> <journal> IEEE Micro, </journal> <volume> 15(1), </volume> <month> February </month> <year> 1995. </year>
Reference-contexts: LAPACK++ (at NETLIB/c++/lapack++)) and LAPACK90 (at NETLIB/lapack90)) are C++ and Fortran 90 interfaces to LAPACK, respectively. Linear Equation Solving 71 <ref> [9] </ref>. These libraries are available on NETLIB, including a comprehensive manual [10]. A more comprehensive discussion of algorithms for high performance (especially parallel) machines may be found on the World Wide Web at PARALLEL HOMEPAGE. <p> The Distributed Memory Algorithms are for machines like the IBM SP-2 [254], Intel Paragon [255], Cray T3 series [253], and networks of workstations <ref> [9] </ref>. As you can see, most software has been written for serial machines, some for shared memory machines, and very little (besides research software) for distributed memory. The first column gives the Matrix Type.
Reference: [10] <author> E. Anderson, Z. Bai, C. Bischof, J. Demmel, J. Dongarra, J. Du Croz, A. Greenbaum, S. Hammarling, A. McKenney, S. Os-trouchov, and D. Sorensen. </author> <note> LAPACK Users' Guide (second edition). SIAM, Philadelphia, </note> <year> 1995. </year> <pages> 324 pages. 429 430 Bibliography </pages>
Reference-contexts: LAPACK++ (at NETLIB/c++/lapack++)) and LAPACK90 (at NETLIB/lapack90)) are C++ and Fortran 90 interfaces to LAPACK, respectively. Linear Equation Solving 71 [9]. These libraries are available on NETLIB, including a comprehensive manual <ref> [10] </ref>. A more comprehensive discussion of algorithms for high performance (especially parallel) machines may be found on the World Wide Web at PARALLEL HOMEPAGE. LAPACK was originally motivated by the poor performance of its predecessors LINPACK and EISPACK (also available on Netlib) on some high performance machines. <p> The directory NETLIB/blas includes documentation and (unoptimized) implementations of all the BLAS. For a quick summary of all the BLAS, see NETLIB/blas/blasqr.ps. This summary also appears in <ref> [10, Appendix C] </ref> (or NETLIB/lapack/lug/lapack lug.html). <p> Values of b = 32 or b = 64 are commonly used. To see detailed implementations of Algorithms 2.9 and 2.10, see subroutines sgetf2 and sgetrf, respectively, in LAPACK (NETLIB/lapack). For more information on block algorithms, including detailed performance number on a variety of machines, see also <ref> [10] </ref>, or the course notes at PARALLEL HOMEPAGE. 2.6.4 More About Parallelism and Other Performance Issues In this section we briefly survey other issues involved in implementing Gaussian elimination (and other linear algebra routines) as efficiently as possible. <p> But some care is needed to be as efficient as possible. Two standard pieces of software are available. The LAPACK routine sgetrf described in the last section <ref> [10] </ref> runs on shared memory parallel machines, provided one has available implementations of the BLAS that run in parallel. A related library called ScaLAPACK, for Scalable LAPACK [52], is designed for distributed memory parallel machines, i.e. those that require special operations to move data between different processors. <p> For the symmetric indefinite factorization, see [43]. Sparse matrix algorithms are described in [112, 91], as well as the numerous references in Table 2.2. Implementations of many of the algorithms for dense and band matrices described in this chapter are available in LAPACK and Linear Equation Solving 103 CLAPACK <ref> [10] </ref>, which includes a discussion of block algorithms suitable for high performance computers. The Basic Linear Algebra Subroutines are described in [85, 87, 167]. These and other routines are available electronically in NETLIB. An analysis of blocking strategies for matrix multiplication is given in [149]. <p> Show that there is a unique solution under the assumption that C has full column rank. Show how to compute x using two QR decompositions, some matrix-vector multiplications, and solving some triangular systems of equations. Hint: look at LAPACK routine sgglse and its description in the LAPACK manual <ref> [10] </ref> (NETLIB/lapack/lug/lapack lug.html). Question 3.19 (Hard; Programming) Write a program (in Matlab or any other language) to update a geodetic database using least squares, as described in example 3.3. <p> For a proof, see [68]. For an overview of condition numbers for the eigenproblem, including eigenvectors, invariant subspaces, and the eigenvalues corresponding to an invariant subspace, see chapter 4 of the LAPACK manual <ref> [10] </ref>, as well as [159, 235]. <p> with the 2n 3 cost of matrix multiply would indicate: Instead of taking (10n 3 )=(2n 3 ) = 5 times longer to compute eigenvalues than to multiply matrices, it takes 23 times longer for n = 100 and 19 times longer for n = 1000 on an IBM RS6000/590 <ref> [10, page 62] </ref>. Instead of taking (27n 3 )=(2n 3 ) = 13:5 times longer to compute eigen-values and eigenvectors, it takes 41 times longer for n = 100 and 60 times longer for n = 1000 on the same machine. <p> For more details about perturbation theory of eigenvalues and eigenvectors, see [159, 235, 51], and chapter 4 of <ref> [10] </ref>. For a proof of Theorem 4.7, see [68]. For a discussion of Weierstrass and Kronecker canon ical forms, see [108, 116]. For their application to systems and control theory, see [244, 245, 77]. For applications to computational geometry, graphics, and mechanical CAD, see [179, 180, 163]. <p> You do not have to provide error bounds when the eigenproblem is a more general one. (For a description of error bounds for more general eigenproblems, see <ref> [10, 235] </ref>. Write a second Matlab program, which plots S and C for the case n = 3, and marks the intersection points. <p> Since Jacobi often takes 5-10 sweeps to converge, it is much slower than the competition. 5.3.6 Performance Comparison In this section we analyze the performance of the three fastest algorithms for the symmetric eigenproblem: QR iteration, Bisection with Inverse Iteration, and divide-and-conquer. More details may be found in <ref> [10, chap. 3] </ref>, or NETLIB/lapack/lug/lapack lug.html. We begin by discussing the fastest algorithm, and later compare the others. We used the LAPACK routine ssyevd. <p> We see that for large enough matrices, matrix-multiplication and finding only the eigenvalues of a symmetric matrix are about equally expensive. (In contrast, the nonsymmetric eigenproblem is least 16 times more costly <ref> [10] </ref>.) Finding the eigenvectors as well is a little under 3 times as expensive as matrix multiplication.
Reference: [11] <author> ANSI/IEEE, </author> <title> New York. IEEE Standard for Binary Floating Point Arithmetic, </title> <address> Std 754-1985 edition, </address> <year> 1985. </year>
Reference: [12] <author> ANSI/IEEE, </author> <title> New York. IEEE Standard for Radix Independent Floating Point Arithmetic, </title> <address> Std 854-1987 edition, </address> <year> 1987. </year>
Reference: [13] <author> P. Arbenz and G. Golub. </author> <title> On the spectral decomposition of Hermi-tian matrices modified by row rank perturbations with applications. </title> <journal> SIAM J. Matrix Anal. Appl., </journal> <volume> 9(1) </volume> <pages> 40-58, </pages> <month> January </month> <year> 1988. </year>
Reference-contexts: For an error analysis of the Bisection algorithm, see [72, 73, 154], and for recent attempts to accelerate Bisection see [103, 201, 199, 174, 171, 173, 267]. Current work in improving Inverse Iteration appears in [103, 199, 201]. The divide-and-conquer eigenroutine was introduced in [58] and further developed in <ref> [13, 88, 125, 129, 151, 170, 208, 232] </ref>. The possibility of high accuracy eigenvalues obtained from Ja-cobi is discussed in [65, 74, 81, 90, 181, 226].
Reference: [14] <author> M. Arioli, J. Demmel, and I. S. Duff. </author> <title> Solving sparse linear systems with sparse backward error. </title> <journal> SIAM J. Matrix Anal. Appl., </journal> <volume> 10(2) </volume> <pages> 165-190, </pages> <month> April </month> <year> 1989. </year>
Reference-contexts: In other words, the componentwise relative backward error is as small as possible. For example, this means that if A and b are sparse then ffiA and ffib have the same sparsity structures as A and b, respectively. For a proof, see [147], as well as <ref> [14, 223, 224, 225] </ref> for more details. Single precision iterative refinement and the error bound (2.14) are implemented in LAPACK routines like sgesvx. <p> An average case analysis of pivot growth is described in [240], and an example of bad pivot growth with complete pivoting is given in [120]. Condition estimators are described in [136, 144, 146]. Single precision iterative refinement is analyzed in <ref> [14, 223, 224] </ref>. A comprehensive discussion of error analysis for linear equation solvers, which covers most of these topics, can be found in [147]. For the symmetric indefinite factorization, see [43]. Sparse matrix algorithms are described in [112, 91], as well as the numerous references in Table 2.2.
Reference: [15] <author> O. Axelsson. </author> <title> Iterative Solution Methods. </title> <publisher> Cambridge University Press, </publisher> <year> 1994. </year>
Reference-contexts: In other words, matrix-vector multiplication is a "black-box" called by the template. It is the user's responsibility to supply an implementation of this black-box. Iterative Methods for Linear Systems 295 An analogous templates project for eigenvalue problems is underway. Other recent textbooks on iterative methods are <ref> [15, 134, 212] </ref>. For the most challenging practical problems arising from differential equations more challenging than our model problem, the linear system Ax = b must be "preconditioned", or replaced by the equivalent systems M 1 Ax = M 1 b which is somehow easier to solve. <p> We also refer the reader to the more comprehensive on-line help at NETLIB/templates, which includes a book [24] and implementations in Matlab, Fortran, and C++. For a survey of current research in Krylov subspace methods, see <ref> [15, 105, 134, 212] </ref>. <p> It is called PETSc, for Portable Extensible Toolkit for Scientific computing. PETSc is available at http://www.mcs.anl.gov/petsc/petsc.html, and described briefly in [230]. 6.11 References and Other Topics for Chap ter 6 Up-to-date surveys of modern iterative methods are given in <ref> [15, 105, 134, 212] </ref>, and their parallel implementations are also surveyed in [75]. Classical methods like Jacobi, Gauss-Seidel and SOR are discussed in detail in [247, 135].
Reference: [16] <author> Z. Bai. </author> <title> Error analysis of the Lanczos algorithm for the nonsymmetric eigenvalue problem. </title> <journal> Math. Comp., </journal> <volume> 62 </volume> <pages> 209-226, </pages> <year> 1994. </year>
Reference-contexts: In fact, it is not always possible to find an appropriate similarity because of a phenomenon known as "breakdown" [41, 132, 133, 197]. Attempts to repair breakdown by by a process called "lookahead" have been proposed, implemented and analyzed in <ref> [16, 18, 54, 55, 63, 106, 200, 263, 264] </ref>.
Reference: [17] <author> Z. Bai. </author> <title> Progress in the numerical solution of the nonsymmetric eigenvalue problem. </title> <journal> J. Num. Lin. Alg. Appl., </journal> <volume> 2(3) </volume> <pages> 219-234, </pages> <year> 1995. </year>
Reference-contexts: (Algorithm 4.3) [19], Davidson's algorithm [214] or the Jacobi-Davidson algorithm [228] to the sparse nonsymmetric eigenproblem. 7.8 References and Other Topics for Chap ter 7 In addition to the references in sections 7.6 and 7.7, there are a number of good surveys available on algorithms for sparse eigenvalues problems: see <ref> [17, 50, 123, 161, 195, 211, 260] </ref>. Parallel implementations are also discussed in [75]. In section 6.2 we discussed the existence of on-line help to choose among the variety of iterative methods available for solving Ax = b.
Reference: [18] <author> Z. Bai, D. Day, and Q. Ye. ABLE: </author> <title> An adaptive block Lanczos method for non-Hermitian eigenvalue problems. </title> <institution> Mathematics Dept. </institution> <type> Report 95-04, </type> <institution> University of Kentucky, </institution> <month> May </month> <year> 1995. </year> <note> submitted to Math. Comp. </note>
Reference-contexts: In fact, it is not always possible to find an appropriate similarity because of a phenomenon known as "breakdown" [41, 132, 133, 197]. Attempts to repair breakdown by by a process called "lookahead" have been proposed, implemented and analyzed in <ref> [16, 18, 54, 55, 63, 106, 200, 263, 264] </ref>.
Reference: [19] <author> Z. Bai and G. W. Stewart. SRRIT: </author> <title> a Fortran subroutine to calculate the dominant invariant subspace of a nonsymmetric matrix. </title> <institution> Computer Science Dept. </institution> <type> Report TR 2908, </type> <institution> University of Maryland, </institution> <month> April </month> <year> 1992. </year> <note> Available as pub/reports for reports and pub/srrit for programs via anonymous ftp from thales.cs.umd.edu. </note>
Reference-contexts: Attempts to repair breakdown by by a process called "lookahead" have been proposed, implemented and analyzed in [16, 18, 54, 55, 63, 106, 200, 263, 264]. Finally, it is possible to apply subspace iteration (Algorithm 4.3) <ref> [19] </ref>, Davidson's algorithm [214] or the Jacobi-Davidson algorithm [228] to the sparse nonsymmetric eigenproblem. 7.8 References and Other Topics for Chap ter 7 In addition to the references in sections 7.6 and 7.7, there are a number of good surveys available on algorithms for sparse eigenvalues problems: see [17, 50, 123,
Reference: [20] <author> D. H. Bailey. </author> <title> Multiprecision translation and execution of Fortran programs. </title> <journal> ACM Trans. Math. Soft., </journal> <volume> 19(3) </volume> <pages> 288-319, </pages> <month> Sept. </month> <year> 1993. </year>
Reference: [21] <author> D. H. Bailey. </author> <title> A Fortran-90 based multiprecision system. </title> <journal> ACM Trans. Math. Soft., </journal> <volume> 21(4) </volume> <pages> 379-387, </pages> <month> Dec. </month> <year> 1995. </year>
Reference: [22] <author> D. H. Bailey, K. Lee, and H. D. Simon. </author> <title> Using Strassen's algorithm to accelerate the solution of linear systems. </title> <journal> J. Supercomputing, </journal> <volume> 4 </volume> <pages> 97-371, </pages> <year> 1991. </year>
Reference-contexts: This approach has led to speedups on relatively large matrices on some machines <ref> [22] </ref>. A drawback is the need for significant workspace, and somewhat lower numerical stability, although it is adequate for many purposes [76]. There are a number of other even faster matrix multiplication algorithms; the current record is about O (n 2:376 ), due to Winograd and Coppersmith [261]. <p> The Basic Linear Algebra Subroutines are described in [85, 87, 167]. These and other routines are available electronically in NETLIB. An analysis of blocking strategies for matrix multiplication is given in [149]. Strassen's matrix multiplication algorithm is presented in [3], its performance in practice described in <ref> [22] </ref>, and its numerical stability described in [76, 147]. A survey of parallel and other block algorithms is given in [75].
Reference: [23] <author> J. Barnes and P. Hut. </author> <title> A hierarchical o(n log n) force calculation algorithm. </title> <journal> Nature, </journal> <volume> 324 </volume> <pages> 446-449, </pages> <year> 1986. </year> <note> Bibliography 431 </note>
Reference-contexts: Evaluating this sum for j = 1; :::; n appears to require O (n 2 ) flops. The Fast Multipole Method and others like it <ref> [122, 23] </ref> can be used to approximately (but very accurately) evaluate this sum in O (n log n) time (or even O (n)) time instead. (See the lectures on "Fast Hierarchical Methods for the N-body Problem" at PARALLEL HOMEPAGE for details.) But this idea alone is not enough to reduce the
Reference: [24] <author> R. Barrett, M. Berry, T. Chan, J. Demmel, J. Donato, J. Don-garra, V. Eijkhout, V. Pozo, Romime C., and H. van der Vorst. </author> <title> Templates for the solution of linear systems: Building blocks for iterative methods. </title> <publisher> SIAM, </publisher> <year> 1994. </year> <note> also available electronically at http://www.netlib.org/templates. </note>
Reference-contexts: Chapter 6 is devoted to methods for solving sparse linear systems other than Gaussian elimination and its variants. There are a large number of sparse methods, and choosing the best one often requires substantial knowledge about the matrix <ref> [24] </ref>. In this section we will only sketch the basic issues in sparse Gaussian elimination, and give pointers to the literature and available software. <p> To help users select the best method for solving their linear systems among the many available, on-line help is available at NETLIB/templates. This directory contains a short book <ref> [24] </ref> and software for most of the iterative methods discussed in this chapter. The book is available both in postscript (NETLIB/templates/templates.ps) and html (NETLIB/templates/template.html). The software is available in Matlab, Fortran and C++. <p> In section 6.6.6 we will give a short summary of the other methods available, besides CG, along with advice on which method to use in which situation. We also refer the reader to the more comprehensive on-line help at NETLIB/templates, which includes a book <ref> [24] </ref> and implementations in Matlab, Fortran, and C++. For a survey of current research in Krylov subspace methods, see [15, 105, 134, 212].
Reference: [25] <author> S. Batterson. </author> <title> Convergence of the shifted QR algorithm on 3 by 3 normal matrices. </title> <journal> Num. Math., </journal> <volume> 58 </volume> <pages> 341-352, </pages> <year> 1990. </year>
Reference-contexts: The main direct method used in practice is QR iteration with implicit shifts (see section 4.4.8). It is interesting that after more than 30 years of dependable service, convergence failures of this algorithm have quite recently been observed, analyzed and patched <ref> [25, 64] </ref>. But there is still no global convergence proof, even though the current algorithm is considered quite reliable. So the problem of devising an algorithm that is numerically stable, and globally (and quickly!) convergent remains open. <p> Finally, we discuss strategies for choosing shifts and to provide reliable quadratic convergence. However, there have been recent discoveries of rare situation where convergence does not occur <ref> [25, 64] </ref>, so finding a completely reliable and fast implementation of QR iteration remains an open problem. Implicit Q Theorem Our eventual implementation of QR Iteration will depend on Theorem 4.9 (Implicit Q Theorem.) Suppose Q T AQ = H is unre-duced upper Hessenberg. <p> So the practical algorithm in Nonsymmetric Eigenvalue Problems 191 use for decades had an "exceptional shift" every 10 shifts if convergence had not occurred. Still, tiny sets of matrices where that algorithm did not converge were only recently discovered <ref> [25, 64] </ref>; matrices in a small neighborhood of 2 6 0 1 0 0 0 h 0 1 3 7 where h is a few thousand times machine epsilon, form such a set. So another "exceptional shift" was recently added to the algorithm to patch this case.
Reference: [26] <author> F. L. Bauer. </author> <title> Genauigkeitsfragen bei der Losung linearer Gle-ichungssysteme. </title> <journal> Z. Angew. Math. Mech., </journal> <volume> 46 </volume> <pages> 409-421, </pages> <year> 1966. </year>
Reference-contexts: It is sometimes also called the Bauer condition number <ref> [26] </ref> or Skeel condition number [223, 224, 225]. For a proof that bounds (2.7) and (2.8) are attainable, see Question 2.4. Recall that Theorem 2.1 related the condition number (A) to the distance from A to the nearest singular matrix.
Reference: [27] <author> T. Beelen and P. Van Dooren. </author> <title> An improved algorithm for the computation of Kronecker's canonical form of a singular pencil. </title> <journal> Lin. Alg. Appl., </journal> <volume> 105 </volume> <pages> 9-65, </pages> <year> 1988. </year>
Reference-contexts: L T m has an analogous left null vector. For a proof, see [108]. Just as Schur form generalized to regular matrix pencils in the last section, it can be generalized to arbitrary singular pencils as well. For the canonical form, perturbation theory and software, see <ref> [27, 78, 244] </ref>. Singular pencils are used to model systems arising in systems and control. We give two examples. Application of Kronecker Form to Differential Equations Suppose we want to solve B _x = Ax + f (t), where A B is a singular pencil. <p> Cost: The most general and reliable version of the algorithm can cost as much as O (n 4 ), depending on the details of the Kronecker Structure; this is much more than for regular A B. There is also a slightly less reliable O (n 3 ) algorithm known <ref> [27] </ref>. Application to ODEs: Provides solution of B _x (t) = Ax (t) + f (t), where the solution may be overdetermined or underde termined.
Reference: [28] <author> C. Bischof. </author> <title> Incremental condition estimation. </title> <journal> SIAM, J. Mat. Anal. Appl., </journal> <volume> 11 </volume> <pages> 312-322, </pages> <year> 1990. </year>
Reference-contexts: Linear Least Squares Problems 145 More sophisticated pivoting schemes than QR with column pivoting, called rank-revealing QR algorithms, have been a subject of much recent study. Rank-revealing QR algorithms which detect rank more reliably and sometimes also faster than QR with column pivoting have been developed <ref> [28, 30, 47, 49, 107, 124, 126, 148, 194, 234] </ref>. We discuss these further in the next section. QRD with column pivoting is available as subroutine sgeqpf in LA-PACK. LAPACK also has several similar factorizations available: RQ (sgerqf), LQ (sgelqf), and QL (sgeqlf). <p> See also chapter 5 of [119] and [166]. Perturbation theory and error bounds for the least squares solution is discussed in detail in [147]. Rank-revealing QRD decompositions are discussed in <ref> [28, 30, 47, 49, 124, 148, 194, 204, 234] </ref>.
Reference: [29] <author> C. Bischof, A. Carle, G. Corliss, A. Griewank, and P. Hovland. ADIFOR: </author> <title> Generating derivative codes from Fortran programs. </title> <journal> Scientific Programming, </journal> <volume> 1(1) </volume> <pages> 11-29, </pages> <year> 1992. </year> <note> software available at http://www.mcs.anl.gov/adifor/. </note>
Reference-contexts: If f is simple enough, this can be done by hand. For complicated f , compiler tools exist that can take 332 Chapter 6 a (nearly) arbitrary subroutine for computing f (x), and automatically produce another subroutine for computing (rf (x)) z <ref> [29] </ref>. This can also be done by using the operator overloading facilities of C++ or Fortran 90, although this is less efficient. A variety of different Krylov subspace methods exist. Some are suitable for nonsymmetric matrices, and others assume symmetry or positive definiteness.
Reference: [30] <author> C. Bischof and G. Quintana-Orti. </author> <title> Computing rank-revealing QR factorizations of dense matrices. </title> <type> Argonne Preprint ANL-MCS-P559-0196, </type> <institution> Argonne National Laboratory, </institution> <year> 1996. </year>
Reference-contexts: Linear Least Squares Problems 145 More sophisticated pivoting schemes than QR with column pivoting, called rank-revealing QR algorithms, have been a subject of much recent study. Rank-revealing QR algorithms which detect rank more reliably and sometimes also faster than QR with column pivoting have been developed <ref> [28, 30, 47, 49, 107, 124, 126, 148, 194, 234] </ref>. We discuss these further in the next section. QRD with column pivoting is available as subroutine sgeqpf in LA-PACK. LAPACK also has several similar factorizations available: RQ (sgerqf), LQ (sgelqf), and QL (sgeqlf). <p> If m n then the initial QR decomposition dominates the the cost of the subsequent operations on the n-by-n matrix R, and all the algorithms cost about the same. The fastest version of rank-revealing QR was that of <ref> [30, 194] </ref>. On Type 1 matrices, this algorithm ranged from 3.2 times slower than QR without pivoting for n = m = 20 to just 1.1 times slower for n = m = 1600. <p> See also chapter 5 of [119] and [166]. Perturbation theory and error bounds for the least squares solution is discussed in detail in [147]. Rank-revealing QRD decompositions are discussed in <ref> [28, 30, 47, 49, 124, 148, 194, 204, 234] </ref>.
Reference: [31] <author> A. Bjorck. </author> <title> Solution of Equations in R n , volume 1 of Handbook of Numerical Analysis, chapter Least Squares Methods. </title> <publisher> Else-vier/North Holland, </publisher> <year> 1987. </year>
Reference-contexts: MGS is Linear Least Squares Problems 119 more stable, and will be used in algorithms later in this book, but may still result in Q being far from orthogonal (kQQ T Ik being far larger than ") when A is ill-conditioned <ref> [31, 32, 33, 147] </ref>. Algorithm 3.2 in section 3.4.1 is a stable alternative algorithm for factoring A = QR. See Question 3.2. We will derive the formula for the x that minimizes kAx bk 2 using the decomposition A = QR in three slightly different ways.
Reference: [32] <author> A. Bjorck. </author> <title> Least Squares Methods. </title> <institution> Mathematics department report, Linkoping University, </institution> <year> 1991. </year>
Reference-contexts: MGS is Linear Least Squares Problems 119 more stable, and will be used in algorithms later in this book, but may still result in Q being far from orthogonal (kQQ T Ik being far larger than ") when A is ill-conditioned <ref> [31, 32, 33, 147] </ref>. Algorithm 3.2 in section 3.4.1 is a stable alternative algorithm for factoring A = QR. See Question 3.2. We will derive the formula for the x that minimizes kAx bk 2 using the decomposition A = QR in three slightly different ways.
Reference: [33] <author> A. Bjorck. </author> <title> Numerical Methods for Least Squares Problem. </title> <publisher> SIAM, </publisher> <year> 1996. </year>
Reference-contexts: A is m-by-n, x is n-by-1, and b and r are m-by-1. A good choice of basis functions f i (y) can lead to better fits and less ill-conditioned systems than using polynomials <ref> [33, 82, 166] </ref>. Example 3.2 In statistical modeling, one often wishes to estimate certain parameters x j based on some observations, where the observations are contaminated by noise. <p> We will see the matrix (A T A) 1 again below when we solve the least squares problem using the normal equations. For more details on the connection to statistics 1 , see for example <ref> [33, 257] </ref>. Example 3.3 The least squares problem was first posed and formulated by Gauss to solve a practical problem for the German government. There are important economic and legal reasons to know exactly where the boundaries lie between plots of land owned by different people. <p> So the surveyors of the day went out and remeasured many angles and distances between landmarks, and it fell to Gauss to figure out how to take these more accurate measurements and update the government database of locations. For this he invented least squares, as we will explain shortly <ref> [33] </ref>. The problem Gauss solved did not go away, and must be periodically revisited. In 1974 the US National Geodetic Survey undertook to update the US geodetic database, which consisted of about 700,000 points. <p> The last method lets us do iterative refinement and improve the solution when the problem is ill-conditioned. All methods but the third can be adapted to deal efficiently with sparse matrices <ref> [33] </ref>. We will discuss each solution in turn. <p> MGS is Linear Least Squares Problems 119 more stable, and will be used in algorithms later in this book, but may still result in Q being far from orthogonal (kQQ T Ik being far larger than ") when A is ill-conditioned <ref> [31, 32, 33, 147] </ref>. Algorithm 3.2 in section 3.4.1 is a stable alternative algorithm for factoring A = QR. See Question 3.2. We will derive the formula for the x that minimizes kAx bk 2 using the decomposition A = QR in three slightly different ways. <p> Arnoldi and Lanczos are used as the basis of algorithms for solving sparse linear systems and finding eigenvalues of sparse matrices. 130 Chapter 3 MGS can also be modified to solve the least squares problem stably, but Q may still be far from orthogonal <ref> [33] </ref>. 3.4.1 Householder Transformations A Householder transformation (or reflection) is a matrix of the form P = I 2uu T where kuk 2 = 1. <p> Note also that we may recover either s and c, or s and c; this is adequate in practice. There is also a way to apply a sequence of Givens rotations while performing fewer floating point operations than described above. These are called Fast Givens rotations <ref> [7, 8, 33] </ref>. <p> Future LAPACK releases will contain improved versions of both rank revealing QR and SVD algorithms for the least squares problem. Linear Least Squares Problems 147 3.7 References and Other Topics for Chap ter 3 The best recent reference on least squares problems is <ref> [33] </ref>, which also discusses variations on the basic problem discussed here (such as constrained, weighted and updating least squares), different ways to regularize rank-deficient problems, and software for sparse least squares problems. See also chapter 5 of [119] and [166].
Reference: [34] <author> J. </author> <title> Blue. A portable FORTRAN program to find the Euclidean norm of a vector. </title> <journal> ACM Trans. Math. Soft., </journal> <volume> 4 </volume> <pages> 15-23, </pages> <year> 1978. </year>
Reference: [35] <author> J. H. Bramble, J. E. Pasciak, and A. H. Schatz. </author> <title> The construction of preconditioners for elliptic problems by substructuring, I. </title> <journal> Math. Comp., </journal> <volume> 47(175):103- 134, </volume> <year> 1986. </year> <note> 432 Bibliography </note>
Reference-contexts: The diagonal blocks of S are complicated, but may be approximated by T 1=2 N , which may be inverted efficiently using the FFT <ref> [35, 36, 37, 38, 39] </ref>.
Reference: [36] <author> J. H. Bramble, J. E. Pasciak, and A. H. Schatz. </author> <title> An iterative method for elliptic problems on regions partitioned into substructures. </title> <journal> Math. Comp., </journal> <volume> 46(173) </volume> <pages> 361-369, </pages> <year> 1986. </year>
Reference-contexts: The diagonal blocks of S are complicated, but may be approximated by T 1=2 N , which may be inverted efficiently using the FFT <ref> [35, 36, 37, 38, 39] </ref>.
Reference: [37] <author> J. H. Bramble, J. E. Pasciak, and A. H. Schatz. </author> <title> The construction of preconditioners for elliptic problems by substructuring, II. </title> <journal> Math. Comp., </journal> <volume> 49 </volume> <pages> 1-16, </pages> <year> 1987. </year>
Reference-contexts: The diagonal blocks of S are complicated, but may be approximated by T 1=2 N , which may be inverted efficiently using the FFT <ref> [35, 36, 37, 38, 39] </ref>.
Reference: [38] <author> J. H. Bramble, J. E. Pasciak, and A. H. Schatz. </author> <title> The construction of preconditioners for elliptic problems by substructuring, III. </title> <journal> Math. Comp., </journal> <volume> 51:415 - 430, </volume> <year> 1988. </year>
Reference-contexts: The diagonal blocks of S are complicated, but may be approximated by T 1=2 N , which may be inverted efficiently using the FFT <ref> [35, 36, 37, 38, 39] </ref>.
Reference: [39] <author> J. H. Bramble, J. E. Pasciak, and A. H. Schatz. </author> <title> The construction of preconditioners for elliptic problems by substructuring, IV. </title> <journal> Math. Comp., </journal> <volume> 53:1 - 24, </volume> <year> 1989. </year>
Reference-contexts: The diagonal blocks of S are complicated, but may be approximated by T 1=2 N , which may be inverted efficiently using the FFT <ref> [35, 36, 37, 38, 39] </ref>.
Reference: [40] <author> K. Brenan, S. Campbell, and L. Petzold. </author> <title> Numerical solution of initial-value problems in differential-algebraic equations. </title> <publisher> North-Holland, </publisher> <address> New York, </address> <year> 1989. </year>
Reference-contexts: Infinite eigenvalues also arise naturally in practice. For example, later in this section we will show how infinite eigenvalues correspond to impulse response in a system described by ordinary differential equations with linear constraints, or differential-algebraic equations (DAEs) <ref> [40] </ref>. See also Question 4.16, for an application of matrix pencils to computational geometry and computer graphics. <p> Furthermore, to have a continuous solution y must satisfy certain consistency conditions at t = 0: i X dt ki g k (0) Numerical methods, based on time-stepping, for solving such differential algebraic equations (DAEs), or ODE's with algebraic constraints, are described in <ref> [40] </ref>. Generalized Schur Form for Regular Pencils Just as we cannot compute the Jordan form stably, we cannot compute its generalization by Weierstrass stably. Instead, we compute the generalized Schur form: Theorem 4.11 (Generalized Schur Form) Let A B be regular.
Reference: [41] <author> C. Brezinski, M. Redivo Zaglia, and H. Sadok. </author> <title> Avoiding breakdown and near-breakdown in Lanczos type algorithms. </title> <journal> Numer. Alg., </journal> <volume> 1 </volume> <pages> 261-284, </pages> <year> 1991. </year>
Reference-contexts: Unfortunately, the similarity transformations can be quite ill-conditioned, which means that the eigen 428 Chapter 7 values of the tridiagonal and of the original matrix may greatly differ. In fact, it is not always possible to find an appropriate similarity because of a phenomenon known as "breakdown" <ref> [41, 132, 133, 197] </ref>. Attempts to repair breakdown by by a process called "lookahead" have been proposed, implemented and analyzed in [16, 18, 54, 55, 63, 106, 200, 263, 264].
Reference: [42] <author> W. Briggs. </author> <title> A Multigrid Tutorial. </title> <publisher> SIAM, </publisher> <year> 1987. </year>
Reference-contexts: We will simplify the proof by looking at one V-cycle, and assuming by induction that the coarse grid problem is solved exactly <ref> [42] </ref>. In reality, the coarse grid problem is not solved quite exactly, but this rough analysis suffices to capture the spirit of the proof: that low frequency error is eliminated on the coarser grid, and high frequency error on the fine grid. <p> Classical methods like Jacobi, Gauss-Seidel and SOR are discussed in detail in [247, 135]. Multigrid methods are discussed in <ref> [42, 183, 184, 258, 266] </ref> and the references therein; [89] is a web site with pointers to an extensive bibliography, software, and so on. Domain decomposition are discussed in [48, 114, 203, 230]. Chebyshev and other polynomials are discussed in [238].
Reference: [43] <author> J. Bunch and L. Kaufman. </author> <title> Some stable methods for calculating inertia and solving symmetric linear systems. </title> <journal> Mathematics of Computation, </journal> <volume> 31(137) </volume> <pages> 163-179, </pages> <month> January </month> <year> 1977. </year>
Reference-contexts: This factorization can be computed stably, saving about half the work and space compared to standard Gaussian elimination. The name of the LAPACK subroutine which does this operation is ssysv. The algorithm is described in <ref> [43] </ref>. 2.7.3 Band Matrices A matrix A is called a band matrix with lower bandwidth b L and upper bandwidth b U if a ij = 0 whenever i &gt; j + b L or i &lt; j b U : A = 6 6 6 6 4 . . . <p> Condition estimators are described in [136, 144, 146]. Single precision iterative refinement is analyzed in [14, 223, 224]. A comprehensive discussion of error analysis for linear equation solvers, which covers most of these topics, can be found in [147]. For the symmetric indefinite factorization, see <ref> [43] </ref>. Sparse matrix algorithms are described in [112, 91], as well as the numerous references in Table 2.2.
Reference: [44] <author> J. Bunch, P. Nielsen, and D. Sorensen. </author> <title> Rank-one modification of the symmetric eigenproblem. </title> <journal> Num. Math., </journal> <volume> 31 </volume> <pages> 31-48, </pages> <year> 1978. </year>
Reference-contexts: i.e. h () = d i c 2 + c 3 : There are several ways to choose the constants c 1 , c 2 and c 3 so that h () approximates f (); we present a slightly simplified version of the one used in the LAPACK routine slaed4 <ref> [170, 44] </ref>.
Reference: [45] <author> B. Buzbee, F. Dorr, J. George, and G. Golub. </author> <title> The direct solution of the discrete Poisson equation on irregular regions. </title> <journal> SIAM J. Num. Anal., </journal> <volume> 8 </volume> <pages> 722-736, </pages> <year> 1971. </year>
Reference-contexts: The final algorithm is due to Buneman, and described in <ref> [46, 45] </ref>. We analyze the cost of the simple algorithm as follows; the stable algorithm is analogous. Multiplying by a tridiagonal matrix or solving a tridiagonal system of size N costs O (N ) flops. <p> Domain decomposition are discussed in [48, 114, 203, 230]. Chebyshev and other polynomials are discussed in [238]. The FFT is discussed in any good textbook on computer science algorithms, such as [3], or in [246]. A stabilized version of block cyclic reduction is found in <ref> [46, 45] </ref>. Iterative Methods for Linear Systems 395 6.12 Questions for Chapter 6 Question 6.1 (Easy) Prove Lemma 6.1 on page 296. Question 6.2 (Easy) Prove the following formulas for triangular factorizations of T N . 1.
Reference: [46] <author> B. Buzbee, G. Golub, and C. Nielsen. </author> <title> On direct methods for solving Poisson's equation. </title> <journal> SIAM J. Num. Anal., </journal> <volume> 7 </volume> <pages> 627-656, </pages> <year> 1970. </year>
Reference-contexts: The final algorithm is due to Buneman, and described in <ref> [46, 45] </ref>. We analyze the cost of the simple algorithm as follows; the stable algorithm is analogous. Multiplying by a tridiagonal matrix or solving a tridiagonal system of size N costs O (N ) flops. <p> Domain decomposition are discussed in [48, 114, 203, 230]. Chebyshev and other polynomials are discussed in [238]. The FFT is discussed in any good textbook on computer science algorithms, such as [3], or in [246]. A stabilized version of block cyclic reduction is found in <ref> [46, 45] </ref>. Iterative Methods for Linear Systems 395 6.12 Questions for Chapter 6 Question 6.1 (Easy) Prove Lemma 6.1 on page 296. Question 6.2 (Easy) Prove the following formulas for triangular factorizations of T N . 1.
Reference: [47] <author> T. Chan. </author> <title> Rank revealing QR factorizations. </title> <journal> Lin. Alg. Appl., </journal> 88/89:67-82, 1987. 
Reference-contexts: Linear Least Squares Problems 145 More sophisticated pivoting schemes than QR with column pivoting, called rank-revealing QR algorithms, have been a subject of much recent study. Rank-revealing QR algorithms which detect rank more reliably and sometimes also faster than QR with column pivoting have been developed <ref> [28, 30, 47, 49, 107, 124, 126, 148, 194, 234] </ref>. We discuss these further in the next section. QRD with column pivoting is available as subroutine sgeqpf in LA-PACK. LAPACK also has several similar factorizations available: RQ (sgerqf), LQ (sgelqf), and QL (sgeqlf). <p> See also chapter 5 of [119] and [166]. Perturbation theory and error bounds for the least squares solution is discussed in detail in [147]. Rank-revealing QRD decompositions are discussed in <ref> [28, 30, 47, 49, 124, 148, 194, 204, 234] </ref>.
Reference: [48] <author> T. Chan and T. Mathew. </author> <title> Domain decomposition algorithms. </title> <editor> In A. Iserles, editor, </editor> <booktitle> Acta Numerica, </booktitle> <volume> volume 3. </volume> <publisher> Cambridge University Press, </publisher> <year> 1994. </year> <note> Bibliography 433 </note>
Reference-contexts: For an implementation of this algorithm, see Question 6.16. The web site [89] contains pointers to an extensive literature, software, and so on. Iterative Methods for Linear Systems 385 6.10 Domain Decomposition Domain decomposition for solving sparse systems of linear equations is a topic of current research. See <ref> [48, 114, 203] </ref> and especially [230] for recent surveys. We will only give simple examples. The need for methods beyond those we have discussed arises from of the irregularity and size of real problems, and also from the need for algorithms for parallel computers. <p> Classical methods like Jacobi, Gauss-Seidel and SOR are discussed in detail in [247, 135]. Multigrid methods are discussed in [42, 183, 184, 258, 266] and the references therein; [89] is a web site with pointers to an extensive bibliography, software, and so on. Domain decomposition are discussed in <ref> [48, 114, 203, 230] </ref>. Chebyshev and other polynomials are discussed in [238]. The FFT is discussed in any good textbook on computer science algorithms, such as [3], or in [246]. A stabilized version of block cyclic reduction is found in [46, 45].
Reference: [49] <author> S. Chandrasekaran and I. Ispen. </author> <title> On rank-revealing QR factorizations. </title> <journal> SIAM Journal on Matrix Analysis and Applications, </journal> <volume> 15, </volume> <year> 1994. </year>
Reference-contexts: Linear Least Squares Problems 145 More sophisticated pivoting schemes than QR with column pivoting, called rank-revealing QR algorithms, have been a subject of much recent study. Rank-revealing QR algorithms which detect rank more reliably and sometimes also faster than QR with column pivoting have been developed <ref> [28, 30, 47, 49, 107, 124, 126, 148, 194, 234] </ref>. We discuss these further in the next section. QRD with column pivoting is available as subroutine sgeqpf in LA-PACK. LAPACK also has several similar factorizations available: RQ (sgerqf), LQ (sgelqf), and QL (sgeqlf). <p> See also chapter 5 of [119] and [166]. Perturbation theory and error bounds for the least squares solution is discussed in detail in [147]. Rank-revealing QRD decompositions are discussed in <ref> [28, 30, 47, 49, 124, 148, 194, 204, 234] </ref>.
Reference: [50] <author> F. Chatelin. </author> <title> Eigenvalues of Matrices. </title> <publisher> Wiley, </publisher> <address> Chichester, England, </address> <year> 1993. </year> <note> English translation of the original 1988 French edition. </note>
Reference-contexts: (Algorithm 4.3) [19], Davidson's algorithm [214] or the Jacobi-Davidson algorithm [228] to the sparse nonsymmetric eigenproblem. 7.8 References and Other Topics for Chap ter 7 In addition to the references in sections 7.6 and 7.7, there are a number of good surveys available on algorithms for sparse eigenvalues problems: see <ref> [17, 50, 123, 161, 195, 211, 260] </ref>. Parallel implementations are also discussed in [75]. In section 6.2 we discussed the existence of on-line help to choose among the variety of iterative methods available for solving Ax = b.
Reference: [51] <author> F. Chatelin and V. Fraysse. </author> <title> Lectures on Finite Precision Computations. </title> <publisher> SIAM, </publisher> <address> Philadelphia, PA, </address> <year> 1996. </year>
Reference-contexts: For more details about perturbation theory of eigenvalues and eigenvectors, see <ref> [159, 235, 51] </ref>, and chapter 4 of [10]. For a proof of Theorem 4.7, see [68]. For a discussion of Weierstrass and Kronecker canon ical forms, see [108, 116]. For their application to systems and control theory, see [244, 245, 77].
Reference: [52] <author> J. Choi, J. Demmel, I. Dhillon, J. Dongarra, S. Ostrouchov, A. Pe-titet, K. Stanley, D. Walker, and R. C. Whaley. </author> <title> ScaLAPACK: A portable linear algebra library for distributed memory computers - Design issues and performance. </title> <institution> Computer Science Dept. </institution> <type> Technical Report CS-95-283, </type> <institution> University of Tennessee, Knoxville, </institution> <month> March </month> <year> 1995. </year> <note> (LAPACK Working Note #95). </note>
Reference-contexts: Two standard pieces of software are available. The LAPACK routine sgetrf described in the last section [10] runs on shared memory parallel machines, provided one has available implementations of the BLAS that run in parallel. A related library called ScaLAPACK, for Scalable LAPACK <ref> [52] </ref>, is designed for distributed memory parallel machines, i.e. those that require special operations to move data between different processors. All software is available on NETLIB in the LAPACK and ScaLAPACK subdirectories. ScaLAPACK is described in more detail in the notes at PARALLEL HOMEPAGE.
Reference: [53] <author> J. Coonen. </author> <title> Underflow and the denormalized numbers. </title> <journal> Computer, </journal> <volume> 14 </volume> <pages> 75-87, </pages> <year> 1981. </year>
Reference: [54] <author> J. Cullum, W. Kerner, and R. Willoughby. </author> <title> A generalized nonsymmetric Lanczos procedure. </title> <journal> Comput. Phys. Commun., </journal> <volume> 53 </volume> <pages> 19-48, </pages> <year> 1989. </year>
Reference-contexts: In fact, it is not always possible to find an appropriate similarity because of a phenomenon known as "breakdown" [41, 132, 133, 197]. Attempts to repair breakdown by by a process called "lookahead" have been proposed, implemented and analyzed in <ref> [16, 18, 54, 55, 63, 106, 200, 263, 264] </ref>.
Reference: [55] <author> J. Cullum and R. Willoughby. </author> <title> A practical procedure for computing eigenvalues of large sparse nonsymmetric matrices. </title> <editor> In J. Cul-lum and R. Willoughby, editors, </editor> <title> Large Scale Eigenvalue Problems. </title> <publisher> North-Holland, </publisher> <address> Amsterdam, </address> <year> 1986. </year> <booktitle> Mathematics Studies Series Vol. 127, Proceedings of the IBM Institute Workshop on Large Scale Eigenvalue Problems, </booktitle> <month> July 8-12, </month> <year> 1985, </year> <institution> Oberlech, Austria. </institution>
Reference-contexts: In fact, it is not always possible to find an appropriate similarity because of a phenomenon known as "breakdown" [41, 132, 133, 197]. Attempts to repair breakdown by by a process called "lookahead" have been proposed, implemented and analyzed in <ref> [16, 18, 54, 55, 63, 106, 200, 263, 264] </ref>.
Reference: [56] <author> J. Cullum and R. A. Willoughby. </author> <title> Lanczos algorithms for large symmetric eigenvalue computations. </title> <publisher> Birkhauser, </publisher> <address> Basel, </address> <year> 1985. </year> <title> Vol.1, Theory, Vol.2. Program. </title>
Reference-contexts: This is not a disaster if one is not concerned about computing multiplicities of eigenvalues, and does not mind the resulting delayed convergence of interior eigenvalues. See <ref> [56] </ref> for a detailed description of a Lanczos implementation that operates in this fashion, and NETLIB/lanczos for the software itself. But if accurate multiplicities are important, then one needs to keep the Lanczos vectors (nearly) orthogonal.
Reference: [57] <author> J. J. M. Cuppen. </author> <title> The singular value decomposition in product form. </title> <journal> SIAM J. Sci. Stat. Comput., </journal> <volume> 4 </volume> <pages> 216-221, </pages> <year> 1983. </year>
Reference-contexts: In contrast, the current LAPACK algorithm, dgeqpf, was 2 times to 2.5 times slower for both matrix types. The fastest version of the SVD was the one in <ref> [57] </ref>, although one based on divide-and-conquer (see section 5.3.3 on page 239) was about equally fast for n = m = 1600 (the one based on divide-and-conquer also used much less memory).
Reference: [58] <author> J.J.M. Cuppen. </author> <title> A divide and conquer method for the symmetric tridiagonal eigenproblem. </title> <journal> Numer. Math., </journal> <volume> 36 </volume> <pages> 177-195, </pages> <year> 1981. </year>
Reference-contexts: Indeed, although this method was first introduced in 1981 <ref> [58] </ref>, the "right" implementation was not discovered until 1992 [125, 129]). This routine is available as LAPACK routines ssyevd for dense matrices, and sstevd for tridiagonal matrices. <p> with random dense matrices with uniformly distributed eigenvalues, over 15% of the eigenvalues of the largest D + uu T deflated, and in experiments with random dense matrices with eigenvalues approaching 0 geometrically, over 85% deflated! It is essential to take advantage of this behavior to make the algorithm fast <ref> [58, 208] </ref>. The payoff in deflation is not in making the solution of the secular equation faster; this only costs O (n 2 ) anyway. The payoff is in making the matrix multiplication in the last step of the algorithm fast. <p> Unfortunately, the formula can be unstable <ref> [58, 88, 232] </ref>, in particular when two eigenvalues ff i and ff i+1 are very close together. Intuitively, the problem is that (Dff i I) 1 u and (Dff i+1 I) 1 n are "very close" formulas, yet are supposed to yield orthogonal eigenvectors. <p> For an error analysis of the Bisection algorithm, see [72, 73, 154], and for recent attempts to accelerate Bisection see [103, 201, 199, 174, 171, 173, 267]. Current work in improving Inverse Iteration appears in [103, 199, 201]. The divide-and-conquer eigenroutine was introduced in <ref> [58] </ref> and further developed in [13, 88, 125, 129, 151, 170, 208, 232]. The possibility of high accuracy eigenvalues obtained from Ja-cobi is discussed in [65, 74, 81, 90, 181, 226].
Reference: [59] <author> E. Davidson. </author> <title> The iteration calculation of a few ofthe lowest eigen-values and corresponding eigenvectors of large real symmetric matrices. </title> <journal> J. Comp. Phys., </journal> <volume> 17 </volume> <pages> 87-94, </pages> <year> 1975. </year>
Reference-contexts: If we apply Lanczos to the shifted and inverted matrix (AI) 1 , then we expect the eigenvalues closest to to converge first. There are other methods to "precondition" a matrix A to converge to certain eigenvalues more quickly. For example, Davidson's method <ref> [59] </ref> is used in quantum chemistry problems, where A is strongly diagonally dominant. It is also possible to combine Davidson's method with Jacobi [227]. 7.7 Iterative Algorithms for the Nonsym metric Eigenproblem When A is nonsymmetric the Lanczos algorithm described above is no longer applicable. There are two alternatives.
Reference: [60] <author> P. Davis. </author> <title> Interpolation and Approximation. </title> <publisher> Dover, </publisher> <address> New York, </address> <year> 1975. </year> <note> 434 Bibliography </note>
Reference-contexts: We can see how erratic the plot of the degree 19 polynomial is on the left (the blue line). This is due to ill-conditioning, as we will later see. Typically, one only does polynomial fitting with relatively low degree polynomials, avoiding ill-conditioning <ref> [60] </ref>. Polynomial fitting is available as the function polyfit in Matlab. Here is an alternative to polynomial fitting.
Reference: [61] <author> T. A. Davis and I. S. Duff. </author> <title> An unsymmetric-pattern multifrontal method for sparse LU factorization. </title> <type> Technical Report RAL 93-036, </type> <institution> Rutherford Appleton Laboratory, Chilton, Didcot, Oxford-shire, </institution> <year> 1994. </year>
Reference-contexts: The inverse of a Cauchy matrix turns out to be a Cauchy matrix, Linear Equation Solving 101 Matrix Status Type Name Algorithm /Source Serial Algorithms nonsym. SuperLU LL, partial, BLAS-2.5 Pub/UCB nonsym. UMFPACK <ref> [61, 62] </ref> MF, Markowitz, BLAS-3 Pub/NETLIB MA38 (same as UMFPACK) Com/HSL nonsym. MA48 [94] Anal: RL, Markowitz Com/HSL Fact: LL, partial, BLAS-1, SD nonsym. SPARSE [165] RL, Markowitz, Scalar Pub/NETLIB sym pattern MUPS [5] MA42 [96] MF, threshold, BLAS-3 Frontal, BLAS-3 Com/HSL sym.
Reference: [62] <author> T. A. Davis and I. S. Duff. </author> <title> A combined unifrontal/multifrontal method for unsymmetric sparse matrices. </title> <type> Technical Report TR-95-020, </type> <institution> Computer and Information Sciences Department, University of Florida, </institution> <year> 1995. </year>
Reference-contexts: The inverse of a Cauchy matrix turns out to be a Cauchy matrix, Linear Equation Solving 101 Matrix Status Type Name Algorithm /Source Serial Algorithms nonsym. SuperLU LL, partial, BLAS-2.5 Pub/UCB nonsym. UMFPACK <ref> [61, 62] </ref> MF, Markowitz, BLAS-3 Pub/NETLIB MA38 (same as UMFPACK) Com/HSL nonsym. MA48 [94] Anal: RL, Markowitz Com/HSL Fact: LL, partial, BLAS-1, SD nonsym. SPARSE [165] RL, Markowitz, Scalar Pub/NETLIB sym pattern MUPS [5] MA42 [96] MF, threshold, BLAS-3 Frontal, BLAS-3 Com/HSL sym.
Reference: [63] <author> D. Day. </author> <title> Semi-duality in the two-sided Lanczos algorithm. </title> <type> PhD thesis, </type> <institution> University of California, Berkeley, </institution> <address> CA, </address> <year> 1993. </year>
Reference-contexts: In fact, it is not always possible to find an appropriate similarity because of a phenomenon known as "breakdown" [41, 132, 133, 197]. Attempts to repair breakdown by by a process called "lookahead" have been proposed, implemented and analyzed in <ref> [16, 18, 54, 55, 63, 106, 200, 263, 264] </ref>.
Reference: [64] <author> D. Day. </author> <title> How the QR algorithm fails to converge and how to fix it. </title> <type> Tech Report 96-0913J, </type> <institution> Sandia National Laboratory, </institution> <month> April </month> <year> 1996. </year>
Reference-contexts: The main direct method used in practice is QR iteration with implicit shifts (see section 4.4.8). It is interesting that after more than 30 years of dependable service, convergence failures of this algorithm have quite recently been observed, analyzed and patched <ref> [25, 64] </ref>. But there is still no global convergence proof, even though the current algorithm is considered quite reliable. So the problem of devising an algorithm that is numerically stable, and globally (and quickly!) convergent remains open. <p> Finally, we discuss strategies for choosing shifts and to provide reliable quadratic convergence. However, there have been recent discoveries of rare situation where convergence does not occur <ref> [25, 64] </ref>, so finding a completely reliable and fast implementation of QR iteration remains an open problem. Implicit Q Theorem Our eventual implementation of QR Iteration will depend on Theorem 4.9 (Implicit Q Theorem.) Suppose Q T AQ = H is unre-duced upper Hessenberg. <p> So the practical algorithm in Nonsymmetric Eigenvalue Problems 191 use for decades had an "exceptional shift" every 10 shifts if convergence had not occurred. Still, tiny sets of matrices where that algorithm did not converge were only recently discovered <ref> [25, 64] </ref>; matrices in a small neighborhood of 2 6 0 1 0 0 0 h 0 1 3 7 where h is a few thousand times machine epsilon, form such a set. So another "exceptional shift" was recently added to the algorithm to patch this case.
Reference: [65] <author> A. Deichmoller. </author> <title> Uber die Berechnung verallgemeinerter singularer Werte mittles Jacobi-ahnlicher Verfahren. </title> <type> PhD thesis, </type> <institution> Fernuniver-sitat - Hagen, Hagen, Germany, </institution> <year> 1991. </year>
Reference-contexts: For extensions of the preceding result to indefinite symmetric eigen problems, see [226, 248]. 3. For extensions to generalized symmetric eigenproblem A B and the generalized SVD, see <ref> [65, 90] </ref>. The Symmetric Eigenproblem and SVD 281 5.5 Differential Equations and Eigenvalue Problems We seek our motivation for this section from conservation laws in physics. We consider once again the mass-spring system introduced in Example 4.1, and reexamined in Example 5.1. <p> The material on relative perturbation theory can be found in [74, 81, 99]; section 5.2.1 was based on the latter of these references. Related work is found in <ref> [65, 90, 226, 248] </ref> A classical text on perturbation theory for general linear operators is [159]. For a survey of parallel algorithms for the symmetric eigenproblem, see [75]. <p> Current work in improving Inverse Iteration appears in [103, 199, 201]. The divide-and-conquer eigenroutine was introduced in [58] and further developed in [13, 88, 125, 129, 151, 170, 208, 232]. The possibility of high accuracy eigenvalues obtained from Ja-cobi is discussed in <ref> [65, 74, 81, 90, 181, 226] </ref>. The Toda flow and related phenomena are discussed in [66, 67, 104, 142, 164, 168, 185, 186, 237]. 5.7 Questions for Chapter 5 Question 5.1 (Easy; Z.
Reference: [66] <author> P. Deift, J. Demmel, L.-C. Li, and C. Tomei. </author> <title> The bidiagonal singular values decomposition and Hamiltonian mechanics. </title> <journal> SIAM J. Num. Anal., </journal> <volume> 28(5) </volume> <pages> 1463-1516, </pages> <month> October </month> <year> 1991. </year> <note> (LAPACK Working Note #11). </note>
Reference-contexts: The transformations are not straightforward, however, because the added structure of the SVD can 262 Chapter 5 tridiagonal matrix, relative to divide-and-conquer. The Symmetric Eigenproblem and SVD 263 dense matrix, relative to divide-and-conquer. 264 Chapter 5 often be exploited to make the algorithms more efficient or more accurate <ref> [118, 79, 66] </ref>. All the algorithms for the eigendecomposition of a symmetric matrix A, except Jacobi, had the following structure: 1. Reduce A to tridiagonal form T with an orthogonal matrix Q 1 : A = Q 1 T Q T 2. <p> In other words, there is an infinite sequence of energy-like quantities conserved by KdV. This is important both for theoretical and numerical reasons. For more details on the Toda flow, see <ref> [142, 168, 66, 67, 237] </ref>, and papers by Kruskal [164], Flaschka [104] and Moser [185] in [186]. 5.6 References and Other Topics for Chap ter 5 An excellent general reference for the symmetric eigenproblem is [195]. <p> Related work is found in [65, 90, 226, 248] A classical text on perturbation theory for general linear operators is [159]. For a survey of parallel algorithms for the symmetric eigenproblem, see [75]. The QR algorithm for finding the SVD of bidiagonal matrices is discussed in <ref> [79, 66, 118] </ref>. and the dqds algorithm is in [102, 198, 207]. For an error analysis of the Bisection algorithm, see [72, 73, 154], and for recent attempts to accelerate Bisection see [103, 201, 199, 174, 171, 173, 267]. Current work in improving Inverse Iteration appears in [103, 199, 201]. <p> The divide-and-conquer eigenroutine was introduced in [58] and further developed in [13, 88, 125, 129, 151, 170, 208, 232]. The possibility of high accuracy eigenvalues obtained from Ja-cobi is discussed in [65, 74, 81, 90, 181, 226]. The Toda flow and related phenomena are discussed in <ref> [66, 67, 104, 142, 164, 168, 185, 186, 237] </ref>. 5.7 Questions for Chapter 5 Question 5.1 (Easy; Z. Bai) Show that A = B + iC is Hermitian if and only if M = B C is symmetric.
Reference: [67] <author> P. Deift, T. Nanda, and C. Tomei. </author> <title> ODEs and the symmetric eigenvalue problem. </title> <journal> SIAM J. Num. Anal., </journal> <volume> 20(1), </volume> <year> 1983. </year>
Reference-contexts: In other words, there is an infinite sequence of energy-like quantities conserved by KdV. This is important both for theoretical and numerical reasons. For more details on the Toda flow, see <ref> [142, 168, 66, 67, 237] </ref>, and papers by Kruskal [164], Flaschka [104] and Moser [185] in [186]. 5.6 References and Other Topics for Chap ter 5 An excellent general reference for the symmetric eigenproblem is [195]. <p> The divide-and-conquer eigenroutine was introduced in [58] and further developed in [13, 88, 125, 129, 151, 170, 208, 232]. The possibility of high accuracy eigenvalues obtained from Ja-cobi is discussed in [65, 74, 81, 90, 181, 226]. The Toda flow and related phenomena are discussed in <ref> [66, 67, 104, 142, 164, 168, 185, 186, 237] </ref>. 5.7 Questions for Chapter 5 Question 5.1 (Easy; Z. Bai) Show that A = B + iC is Hermitian if and only if M = B C is symmetric.
Reference: [68] <author> J. Demmel. </author> <title> The condition number of equivalence transformations that block diagonalize matrix pencils. </title> <journal> SIAM J. Num. Anal., </journal> <volume> 20(3) </volume> <pages> 599-610, </pages> <month> June </month> <year> 1983. </year>
Reference-contexts: For a proof, see <ref> [68] </ref>. For an overview of condition numbers for the eigenproblem, including eigenvectors, invariant subspaces, and the eigenvalues corresponding to an invariant subspace, see chapter 4 of the LAPACK manual [10], as well as [159, 235]. <p> For more details about perturbation theory of eigenvalues and eigenvectors, see [159, 235, 51], and chapter 4 of [10]. For a proof of Theorem 4.7, see <ref> [68] </ref>. For a discussion of Weierstrass and Kronecker canon ical forms, see [108, 116]. For their application to systems and control theory, see [244, 245, 77]. For applications to computational geometry, graphics, and mechanical CAD, see [179, 180, 163]. <p> Then among all block diagonal preconditioners M = 6 M 11 M bb 7 where M ii and A ii have the same dimensions, the choice M ii = A ii minimizes the condition number of M 1=2 AM 1=2 to within a factor of b <ref> [68] </ref>.
Reference: [69] <author> J. Demmel. </author> <title> Underflow and the reliability of numerical software. </title> <journal> SIAM J. Sci. Stat. Comput., </journal> <volume> 5(4) </volume> <pages> 887-919, </pages> <month> Dec </month> <year> 1984. </year>
Reference: [70] <author> J. Demmel. </author> <title> On condition numbers and the distance to the nearest ill-posed problem. </title> <journal> Num. Math., </journal> <volume> 51(3) </volume> <pages> 251-289, </pages> <month> July </month> <year> 1987. </year>
Reference-contexts: This reciprocal relationship is quite common in numerical analysis <ref> [70] </ref>. Here is a slightly different way to do perturbation theory for Ax = b; we will need it to derive practical error bounds later in section 2.4.4. <p> The reciprocal relationship between condition numbers and distance to the nearest ill-posed problem is further explored in <ref> [70] </ref>. An average case analysis of pivot growth is described in [240], and an example of bad pivot growth with complete pivoting is given in [120]. Condition estimators are described in [136, 144, 146]. Single precision iterative refinement is analyzed in [14, 223, 224].
Reference: [71] <author> J. Demmel. </author> <title> The componentwise distance to the nearest singular matrix. </title> <journal> SIAM J. Mat. Anal. Appl., </journal> <volume> 13(1) </volume> <pages> 10-19, </pages> <year> 1992. </year>
Reference-contexts: For a proof that bounds (2.7) and (2.8) are attainable, see Question 2.4. Recall that Theorem 2.1 related the condition number (A) to the distance from A to the nearest singular matrix. For a similar interpretation of CR (A), see <ref> [71, 206] </ref>. 42 Chapter 2 Example 2.2 Consider our earlier example with A = diag (fl; 1) and b = [fl; 1] T . It is easy to confirm that CR (A) = 1, since jA 1 j jAj = I.
Reference: [72] <author> J. Demmel, I. Dhillon, and H. Ren. </author> <title> On the correctness of some bisection-like parallel eigenvalue algorithms in floating point arithmetic. </title> <journal> Electronic Trans. Num. Anal., </journal> <volume> 3 </volume> <pages> 116-140, </pages> <month> December </month> <year> 1995. </year> <note> LAPACK working note 70. Bibliography 435 </note>
Reference-contexts: In fact, since it is tridiagonal it can be shown to be very stable <ref> [72, 73, 154] </ref>: 254 Chapter 5 Lemma 5.3 The d i computed in floating point arithmetic, using equation (5.17), have the same signs (and so compute the same Inertia) as the ^ d i computed exactly from ^ A, where ^ A is very close to A: ( ^ A) ii <p> Indeed, using the exception handling facilities of IEEE arithmetic, one can safely compute even when some d i1 is exactly zero! For in this case d i = 1, d i+1 = a i+1 z, and the computation continues unexceptionally <ref> [72, 80] </ref>. The cost of a single call to Negcount on a tridiagonal matrix is 4n flops. Therefore the overall cost to find k eigenvalues is O (kn). This is implemented in LAPACK routine sstebz. <p> For a survey of parallel algorithms for the symmetric eigenproblem, see [75]. The QR algorithm for finding the SVD of bidiagonal matrices is discussed in [79, 66, 118]. and the dqds algorithm is in [102, 198, 207]. For an error analysis of the Bisection algorithm, see <ref> [72, 73, 154] </ref>, and for recent attempts to accelerate Bisection see [103, 201, 199, 174, 171, 173, 267]. Current work in improving Inverse Iteration appears in [103, 199, 201]. The divide-and-conquer eigenroutine was introduced in [58] and further developed in [13, 88, 125, 129, 151, 170, 208, 232].
Reference: [73] <author> J. Demmel and W. Gragg. </author> <title> On computing accurate singular values and eigenvalues of acyclic matrices. </title> <journal> Lin. Alg. Appl., </journal> <volume> 185 </volume> <pages> 203-218, </pages> <year> 1993. </year>
Reference-contexts: In fact, since it is tridiagonal it can be shown to be very stable <ref> [72, 73, 154] </ref>: 254 Chapter 5 Lemma 5.3 The d i computed in floating point arithmetic, using equation (5.17), have the same signs (and so compute the same Inertia) as the ^ d i computed exactly from ^ A, where ^ A is very close to A: ( ^ A) ii <p> For a survey of parallel algorithms for the symmetric eigenproblem, see [75]. The QR algorithm for finding the SVD of bidiagonal matrices is discussed in [79, 66, 118]. and the dqds algorithm is in [102, 198, 207]. For an error analysis of the Bisection algorithm, see <ref> [72, 73, 154] </ref>, and for recent attempts to accelerate Bisection see [103, 201, 199, 174, 171, 173, 267]. Current work in improving Inverse Iteration appears in [103, 199, 201]. The divide-and-conquer eigenroutine was introduced in [58] and further developed in [13, 88, 125, 129, 151, 170, 208, 232].
Reference: [74] <author> J. Demmel, M. Gu, S. Eisenstat, I. Slapnicar, K. Veselic, and Z. Drmac. </author> <title> Computing the singular value decomposition with high relative accuracy. </title> <booktitle> in progress, </booktitle> <year> 1996. </year>
Reference-contexts: Many other examples appear in <ref> [74] </ref>. 1. If A = LL T is the Cholesky decomposition of a symmetric positive definite matrix, then the SVD of L = U V T provides the eigendecomposition of A = U 2 U T . <p> The material on relative perturbation theory can be found in <ref> [74, 81, 99] </ref>; section 5.2.1 was based on the latter of these references. Related work is found in [65, 90, 226, 248] A classical text on perturbation theory for general linear operators is [159]. For a survey of parallel algorithms for the symmetric eigenproblem, see [75]. <p> Current work in improving Inverse Iteration appears in [103, 199, 201]. The divide-and-conquer eigenroutine was introduced in [58] and further developed in [13, 88, 125, 129, 151, 170, 208, 232]. The possibility of high accuracy eigenvalues obtained from Ja-cobi is discussed in <ref> [65, 74, 81, 90, 181, 226] </ref>. The Toda flow and related phenomena are discussed in [66, 67, 104, 142, 164, 168, 185, 186, 237]. 5.7 Questions for Chapter 5 Question 5.1 (Easy; Z.
Reference: [75] <author> J. Demmel, M. Heath, and H. van der Vorst. </author> <title> Parallel numerical linear algebra. </title> <editor> In A. Iserles, editor, </editor> <booktitle> Acta Numerica, </booktitle> <volume> volume 2. </volume> <publisher> Cambridge University Press, </publisher> <year> 1993. </year>
Reference-contexts: An analysis of blocking strategies for matrix multiplication is given in [149]. Strassen's matrix multiplication algorithm is presented in [3], its performance in practice described in [22], and its numerical stability described in [76, 147]. A survey of parallel and other block algorithms is given in <ref> [75] </ref>. For a recent survey of algorithms for structured dense matrices depending only on O (n) parameters, see [158]. 2.9 Questions for Chapter 2 Question 2.1 (Easy) Using your favorite World Wide Web browser, go to NETLIB (http://www.netlib.org), and answer the following questions. 1. <p> For a discussion of Weierstrass and Kronecker canon ical forms, see [108, 116]. For their application to systems and control theory, see [244, 245, 77]. For applications to computational geometry, graphics, and mechanical CAD, see [179, 180, 163]. For a discussion of parallel algorithms for the nonsymmetric eigenproblem, see <ref> [75] </ref>. 4.8 Questions for Chapter 4 Question 4.1 (Easy) Let A be defined as in equation (4.1). <p> Note that Bisection and Inverse Iteration are "embarrassingly parallel", since each eigenvalue and later eigenvector may be found independently of the others (this presumes Inverse Iteration has been repaired so that reorthogonalization with many other eigenvectors is no longer necessary). This makes these algorithms very attractive for parallel computers <ref> [75] </ref>. 256 Chapter 5 5.3.5 Jacobi's Method Jacobi's method does not start by reducing A to tridiagonal from as do the previous methods, but instead works on the original dense matrix. <p> Related work is found in [65, 90, 226, 248] A classical text on perturbation theory for general linear operators is [159]. For a survey of parallel algorithms for the symmetric eigenproblem, see <ref> [75] </ref>. The QR algorithm for finding the SVD of bidiagonal matrices is discussed in [79, 66, 118]. and the dqds algorithm is in [102, 198, 207]. <p> PETSc is available at http://www.mcs.anl.gov/petsc/petsc.html, and described briefly in [230]. 6.11 References and Other Topics for Chap ter 6 Up-to-date surveys of modern iterative methods are given in [15, 105, 134, 212], and their parallel implementations are also surveyed in <ref> [75] </ref>. Classical methods like Jacobi, Gauss-Seidel and SOR are discussed in detail in [247, 135]. Multigrid methods are discussed in [42, 183, 184, 258, 266] and the references therein; [89] is a web site with pointers to an extensive bibliography, software, and so on. <p> Parallel implementations are also discussed in <ref> [75] </ref>. In section 6.2 we discussed the existence of on-line help to choose among the variety of iterative methods available for solving Ax = b.
Reference: [76] <author> J. Demmel and N. J. Higham. </author> <title> Stability of block algorithms with fast Level 3 BLAS. </title> <journal> ACM Trans. Math. Soft., </journal> <volume> 18(3) </volume> <pages> 274-291, </pages> <year> 1992. </year>
Reference-contexts: This approach has led to speedups on relatively large matrices on some machines [22]. A drawback is the need for significant workspace, and somewhat lower numerical stability, although it is adequate for many purposes <ref> [76] </ref>. There are a number of other even faster matrix multiplication algorithms; the current record is about O (n 2:376 ), due to Winograd and Coppersmith [261]. But these algorithms only perform fewer operations than Strassen for impractically large values of n. <p> These and other routines are available electronically in NETLIB. An analysis of blocking strategies for matrix multiplication is given in [149]. Strassen's matrix multiplication algorithm is presented in [3], its performance in practice described in [22], and its numerical stability described in <ref> [76, 147] </ref>. A survey of parallel and other block algorithms is given in [75].
Reference: [77] <author> J. Demmel and B. K-agstrom. </author> <title> Accurate solutions of ill-posed problems in control theory. </title> <journal> SIAM J. Mat. Anal. Appl., </journal> <volume> 9(1) </volume> <pages> 126-145, </pages> <month> January </month> <year> 1988. </year>
Reference-contexts: To compute this space in practice, in order to determine whether the physical system being modeled can in fact be controlled by input u (t), one applies a QR-like algorithm to the singular pencil [B; A I]. For details, see <ref> [77, 244, 245] </ref>. 4.5.3 Nonlinear Eigenvalue Problems Finally, we consider the nonlinear eigenvalue problem or matrix polynomial d X i A i = d A d + d1 A d1 + A 1 + A 0 : (4.7) Suppose for simplicity that the A i are n-by-n matrices and A d <p> For more details about perturbation theory of eigenvalues and eigenvectors, see [159, 235, 51], and chapter 4 of [10]. For a proof of Theorem 4.7, see [68]. For a discussion of Weierstrass and Kronecker canon ical forms, see [108, 116]. For their application to systems and control theory, see <ref> [244, 245, 77] </ref>. For applications to computational geometry, graphics, and mechanical CAD, see [179, 180, 163]. For a discussion of parallel algorithms for the nonsymmetric eigenproblem, see [75]. 4.8 Questions for Chapter 4 Question 4.1 (Easy) Let A be defined as in equation (4.1).
Reference: [78] <author> J. Demmel and B. K-agstrom. </author> <title> The generalized Schur decomposition of an arbitrary pencil A B: Robust software with error bounds and applications. Parts I and II. </title> <journal> ACM Trans. Math. Soft., </journal> <volume> 19(2), </volume> <month> June </month> <year> 1993. </year>
Reference-contexts: L T m has an analogous left null vector. For a proof, see [108]. Just as Schur form generalized to regular matrix pencils in the last section, it can be generalized to arbitrary singular pencils as well. For the canonical form, perturbation theory and software, see <ref> [27, 78, 244] </ref>. Singular pencils are used to model systems arising in systems and control. We give two examples. Application of Kronecker Form to Differential Equations Suppose we want to solve B _x = Ax + f (t), where A B is a singular pencil. <p> See <ref> [78, 244] </ref> for details of the form and algorithms. Cost: The most general and reliable version of the algorithm can cost as much as O (n 4 ), depending on the details of the Kronecker Structure; this is much more than for regular A B.
Reference: [79] <author> J. Demmel and W. Kahan. </author> <title> Accurate singular values of bidiagonal matrices. </title> <journal> SIAM J. Sci. Stat. Comput., </journal> <volume> 11(5) </volume> <pages> 873-912, </pages> <month> September </month> <year> 1990. </year>
Reference-contexts: The transformations are not straightforward, however, because the added structure of the SVD can 262 Chapter 5 tridiagonal matrix, relative to divide-and-conquer. The Symmetric Eigenproblem and SVD 263 dense matrix, relative to divide-and-conquer. 264 Chapter 5 often be exploited to make the algorithms more efficient or more accurate <ref> [118, 79, 66] </ref>. All the algorithms for the eigendecomposition of a symmetric matrix A, except Jacobi, had the following structure: 1. Reduce A to tridiagonal form T with an orthogonal matrix Q 1 : A = Q 1 T Q T 2. <p> In contrast, symmetric tridiagonal QR may compute tiny eigenvalues with no relative accuracy at all. A different variation of QR <ref> [79] </ref> is used to compute the singular vectors as well: by using QR with a zero shift to compute the smallest singular vectors, this variation computes the singular values nearly as accurately, as well as getting singular vectors as accurately as described in section 5.2.1. <p> The algorithm in the LAPACK routine sbdsqr was originally based on <ref> [79] </ref>, and later updated to use the algorithm in [102] in the case when singular values only are desired. This latter algorithm, called dqds for historical reasons, 6 is elegant, fast, and accurate, so we will present it. <p> Related work is found in [65, 90, 226, 248] A classical text on perturbation theory for general linear operators is [159]. For a survey of parallel algorithms for the symmetric eigenproblem, see [75]. The QR algorithm for finding the SVD of bidiagonal matrices is discussed in <ref> [79, 66, 118] </ref>. and the dqds algorithm is in [102, 198, 207]. For an error analysis of the Bisection algorithm, see [72, 73, 154], and for recent attempts to accelerate Bisection see [103, 201, 199, 174, 171, 173, 267]. Current work in improving Inverse Iteration appears in [103, 199, 201].
Reference: [80] <author> J. Demmel and X. Li. </author> <title> Faster numerical algorithms via exception handling. </title> <journal> IEEE Trans. Comp., </journal> <volume> 43(8) </volume> <pages> 983-992, </pages> <year> 1994. </year> <note> LAPACK Working Note 59. </note>
Reference-contexts: Indeed, using the exception handling facilities of IEEE arithmetic, one can safely compute even when some d i1 is exactly zero! For in this case d i = 1, d i+1 = a i+1 z, and the computation continues unexceptionally <ref> [72, 80] </ref>. The cost of a single call to Negcount on a tridiagonal matrix is 4n flops. Therefore the overall cost to find k eigenvalues is O (kn). This is implemented in LAPACK routine sstebz.
Reference: [81] <author> J. Demmel and K. Veselic. </author> <title> Jacobi's method is more accurate than QR. </title> <journal> SIAM J. Mat. Anal. Appl., </journal> <volume> 13(4) </volume> <pages> 1204-1246, </pages> <year> 1992. </year> <note> (also LA-PACK Working Note #15). </note>
Reference-contexts: But the method remains interesting, because it is sometimes much more accurate than the above methods. This is because Jacobi is sometimes capable of attaining the relative accuracy described in section 5.2.1, and so can sometimes compute tiny eigenvalues much more accurately than the previous methods <ref> [81] </ref>. We discuss the high-accuracy property of Jacobi in section 5.4.3, where we show how to compute the SVD. Subsequent sections describe these algorithms in more detail. Section 5.3.6 presents comparative performance results. 5.3.1 Tridiagonal QR Iteration Recall that the QR algorithm for the nonsymmetric eigenproblem had two phases: 1. <p> So if X is well-conditioned, all the eigenvalues of A will be computed to high relative accuracy (see Question 5.23 and <ref> [81, 90, 181] </ref>). <p> The material on relative perturbation theory can be found in <ref> [74, 81, 99] </ref>; section 5.2.1 was based on the latter of these references. Related work is found in [65, 90, 226, 248] A classical text on perturbation theory for general linear operators is [159]. For a survey of parallel algorithms for the symmetric eigenproblem, see [75]. <p> Current work in improving Inverse Iteration appears in [103, 199, 201]. The divide-and-conquer eigenroutine was introduced in [58] and further developed in [13, 88, 125, 129, 151, 170, 208, 232]. The possibility of high accuracy eigenvalues obtained from Ja-cobi is discussed in <ref> [65, 74, 81, 90, 181, 226] </ref>. The Toda flow and related phenomena are discussed in [66, 67, 104, 142, 164, 168, 185, 186, 237]. 5.7 Questions for Chapter 5 Question 5.1 (Easy; Z.
Reference: [82] <author> P. Dierckx. </author> <title> Curve and surface fitting with splines. </title> <publisher> Oxford University Press, </publisher> <year> 1993. </year>
Reference-contexts: A is m-by-n, x is n-by-1, and b and r are m-by-1. A good choice of basis functions f i (y) can lead to better fits and less ill-conditioned systems than using polynomials <ref> [33, 82, 166] </ref>. Example 3.2 In statistical modeling, one often wishes to estimate certain parameters x j based on some observations, where the observations are contaminated by noise.
Reference: [83] <author> J. Dongarra. </author> <title> Performance of various computers using standard linear equations software. </title> <institution> Computer science dept. </institution> <type> technical report, </type> <institution> University of Tennessee, Knoxville, TN, </institution> <month> April </month> <year> 1996. </year> <note> up-to-date version available in netlib/benchmark. 436 Bibliography </note>
Reference-contexts: All software is available on NETLIB in the LAPACK and ScaLAPACK subdirectories. ScaLAPACK is described in more detail in the notes at PARALLEL HOMEPAGE. Extensive performance data for linear equation solvers is available as the LINPACK Benchmark <ref> [83] </ref>, with an up-to-date version available at NETLIB/benchmark/performance.ps, or in the Performance Database Server 8 .
Reference: [84] <author> J. Dongarra, J. Du Croz, I. Duff, and S. Hammarling. </author> <title> Algorithm 679: A set of Level 3 Basic Linear Algebra Subprograms. </title> <journal> ACM Trans. Math. Soft., </journal> <volume> 16(1) </volume> <pages> 18-28, </pages> <month> March </month> <year> 1990. </year>
Reference-contexts: Operations like matrix-matrix multiplication perform O (n 3 ) flops on pairs of matrices, and offer the best q values; these are called Level 3 BLAS or BLAS3 <ref> [85, 84] </ref>, and include solving triangular systems of equations with many right hand sides. The directory NETLIB/blas includes documentation and (unoptimized) implementations of all the BLAS. For a quick summary of all the BLAS, see NETLIB/blas/blasqr.ps. This summary also appears in [10, Appendix C] (or NETLIB/lapack/lug/lapack lug.html).
Reference: [85] <author> J. Dongarra, J. Du Croz, I. Duff, and S. Hammarling. </author> <title> A set of Level 3 Basic Linear Algebra Subprograms. </title> <journal> ACM Trans. Math. Soft., </journal> <volume> 16(1) </volume> <pages> 1-17, </pages> <month> March </month> <year> 1990. </year>
Reference-contexts: Since operations like matrix-matrix multiplication are so common, computer manufacturers have standardized them as the Basic Linear Algebra Subroutines or BLAS <ref> [167, 87, 85] </ref>, and optimized them for their machines. In other words, a library of subroutines for matrix-matrix multiplication, matrix-vector multiplication, and other similar operations is available with a standard Fortran or C interface on high performance machines (and many others), but underneath they have been optimized for each machine. <p> Operations like matrix-matrix multiplication perform O (n 3 ) flops on pairs of matrices, and offer the best q values; these are called Level 3 BLAS or BLAS3 <ref> [85, 84] </ref>, and include solving triangular systems of equations with many right hand sides. The directory NETLIB/blas includes documentation and (unoptimized) implementations of all the BLAS. For a quick summary of all the BLAS, see NETLIB/blas/blasqr.ps. This summary also appears in [10, Appendix C] (or NETLIB/lapack/lug/lapack lug.html). <p> Implementations of many of the algorithms for dense and band matrices described in this chapter are available in LAPACK and Linear Equation Solving 103 CLAPACK [10], which includes a discussion of block algorithms suitable for high performance computers. The Basic Linear Algebra Subroutines are described in <ref> [85, 87, 167] </ref>. These and other routines are available electronically in NETLIB. An analysis of blocking strategies for matrix multiplication is given in [149]. Strassen's matrix multiplication algorithm is presented in [3], its performance in practice described in [22], and its numerical stability described in [76, 147].
Reference: [86] <author> J. Dongarra, J. Du Croz, S. Hammarling, and Richard J. Hanson. </author> <title> Algorithm 656: An extended set of FORTRAN Basic Linear Algebra Subroutines. </title> <journal> ACM Trans. Math. Soft., </journal> <volume> 14(1) </volume> <pages> 18-32, </pages> <month> March </month> <year> 1988. </year>
Reference-contexts: Operations like matrix-vector multiplication perform O (n 2 ) flops on matrices and vectors, and offer slightly better q values; these are called Level 2 BLAS or BLAS2 <ref> [87, 86] </ref>, and include solving triangular systems of equations and rank-1 updates of matrices (A + xy T , x and y column vectors).
Reference: [87] <author> J. Dongarra, J. Du Croz, S. Hammarling, and Richard J. Hanson. </author> <title> An Extended Set of FORTRAN Basic Linear Algebra Subroutines. </title> <journal> ACM Trans. Math. Soft., </journal> <volume> 14(1) </volume> <pages> 1-17, </pages> <month> March </month> <year> 1988. </year>
Reference-contexts: Since operations like matrix-matrix multiplication are so common, computer manufacturers have standardized them as the Basic Linear Algebra Subroutines or BLAS <ref> [167, 87, 85] </ref>, and optimized them for their machines. In other words, a library of subroutines for matrix-matrix multiplication, matrix-vector multiplication, and other similar operations is available with a standard Fortran or C interface on high performance machines (and many others), but underneath they have been optimized for each machine. <p> Operations like matrix-vector multiplication perform O (n 2 ) flops on matrices and vectors, and offer slightly better q values; these are called Level 2 BLAS or BLAS2 <ref> [87, 86] </ref>, and include solving triangular systems of equations and rank-1 updates of matrices (A + xy T , x and y column vectors). <p> Implementations of many of the algorithms for dense and band matrices described in this chapter are available in LAPACK and Linear Equation Solving 103 CLAPACK [10], which includes a discussion of block algorithms suitable for high performance computers. The Basic Linear Algebra Subroutines are described in <ref> [85, 87, 167] </ref>. These and other routines are available electronically in NETLIB. An analysis of blocking strategies for matrix multiplication is given in [149]. Strassen's matrix multiplication algorithm is presented in [3], its performance in practice described in [22], and its numerical stability described in [76, 147].
Reference: [88] <author> J. Dongarra and D. Sorensen. </author> <title> A fully parallel algorithm for the symmetric eigenproblem. </title> <journal> SIAM J. Sci. Stat. Comput., </journal> <volume> 8(2) </volume> <pages> 139-154, </pages> <month> March </month> <year> 1987. </year>
Reference-contexts: Unfortunately, the formula can be unstable <ref> [58, 88, 232] </ref>, in particular when two eigenvalues ff i and ff i+1 are very close together. Intuitively, the problem is that (Dff i I) 1 u and (Dff i+1 I) 1 n are "very close" formulas, yet are supposed to yield orthogonal eigenvectors. <p> Either way, d i ff i and d i ff i+1 may contain large relative errors, so the computed eigenvectors (D ff i ) 1 u and (D ff i+1 ) 1 u are quite inaccurate, and far from orthogonal. Early attempts to address this problem <ref> [88, 232] </ref> used double precision arithmetic (when the input data was single precision) to solve the secular equation to high accuracy, so that d i ff i and d i ff i+1 could be computed to high accuracy. <p> For an error analysis of the Bisection algorithm, see [72, 73, 154], and for recent attempts to accelerate Bisection see [103, 201, 199, 174, 171, 173, 267]. Current work in improving Inverse Iteration appears in [103, 199, 201]. The divide-and-conquer eigenroutine was introduced in [58] and further developed in <ref> [13, 88, 125, 129, 151, 170, 208, 232] </ref>. The possibility of high accuracy eigenvalues obtained from Ja-cobi is discussed in [65, 74, 81, 90, 181, 226].
Reference: [89] <author> C. Douglas. MGNET: </author> <title> Multi-Grid net. </title> <address> http://NA.CS.Yale.EDU/mgnet/www/mgnet.html. </address>
Reference-contexts: Therefore multigrid converges at a fixed rate independent of the number of unknowns. For a proof, see Question 6.15. For a more general analysis, see [266]. For an implementation of this algorithm, see Question 6.16. The web site <ref> [89] </ref> contains pointers to an extensive literature, software, and so on. Iterative Methods for Linear Systems 385 6.10 Domain Decomposition Domain decomposition for solving sparse systems of linear equations is a topic of current research. See [48, 114, 203] and especially [230] for recent surveys. <p> Classical methods like Jacobi, Gauss-Seidel and SOR are discussed in detail in [247, 135]. Multigrid methods are discussed in [42, 183, 184, 258, 266] and the references therein; <ref> [89] </ref> is a web site with pointers to an extensive bibliography, software, and so on. Domain decomposition are discussed in [48, 114, 203, 230]. Chebyshev and other polynomials are discussed in [238]. The FFT is discussed in any good textbook on computer science algorithms, such as [3], or in [246].
Reference: [90] <author> Z. Drmac. </author> <title> Computing the Singular and the Generalized Singular Values. </title> <type> PhD thesis, </type> <institution> Fernuniversitat - Hagen, Hagen, Germany, </institution> <year> 1994. </year>
Reference-contexts: So if X is well-conditioned, all the eigenvalues of A will be computed to high relative accuracy (see Question 5.23 and <ref> [81, 90, 181] </ref>). <p> For extensions of the preceding result to indefinite symmetric eigen problems, see [226, 248]. 3. For extensions to generalized symmetric eigenproblem A B and the generalized SVD, see <ref> [65, 90] </ref>. The Symmetric Eigenproblem and SVD 281 5.5 Differential Equations and Eigenvalue Problems We seek our motivation for this section from conservation laws in physics. We consider once again the mass-spring system introduced in Example 4.1, and reexamined in Example 5.1. <p> The material on relative perturbation theory can be found in [74, 81, 99]; section 5.2.1 was based on the latter of these references. Related work is found in <ref> [65, 90, 226, 248] </ref> A classical text on perturbation theory for general linear operators is [159]. For a survey of parallel algorithms for the symmetric eigenproblem, see [75]. <p> Current work in improving Inverse Iteration appears in [103, 199, 201]. The divide-and-conquer eigenroutine was introduced in [58] and further developed in [13, 88, 125, 129, 151, 170, 208, 232]. The possibility of high accuracy eigenvalues obtained from Ja-cobi is discussed in <ref> [65, 74, 81, 90, 181, 226] </ref>. The Toda flow and related phenomena are discussed in [66, 67, 104, 142, 164, 168, 185, 186, 237]. 5.7 Questions for Chapter 5 Question 5.1 (Easy; Z.
Reference: [91] <author> I. S. Duff, A. M. Erisman, and J.K.Reid. </author> <title> Direct methods for sparse matrices. </title> <publisher> Oxford University Press, </publisher> <address> London, </address> <year> 1986. </year>
Reference-contexts: First, we need to design a data structure that holds only the nonzero entries of A; there are several in common use <ref> [91] </ref>. Next, we need a data structure to accommodate new entries of L and U that fill in during elimination. This means that the data structure must either grow dynamically during the algorithm, or we must cheaply precompute it without actually performing the elimination. <p> In other words, we can not afford to do O (n 3 ) integer and logical operations to discover the few floating point operations we want to do. A more complete discussion of these algorithms is beyond the scope of this book <ref> [112, 91] </ref>, but we will point to available software. Example 2.9 We illustrate sparse Cholesky on a more realistic example that arises from modeling the displacement of a mechanical structure subject to external forces. <p> The number of nonzeros in L, and the number of flops required to compute L, can be changed significantly by reordering the rows and columns of A. The middle pair of plots in Figure 2.11 show the results of one such popular reordering, called Reverse Cuthill-McKee <ref> [112, 91] </ref>, which is designed to make A a narrow band matrix. As can be seen, it is quite successful at this, reducing the fill-in of L 21% (from 11533 to 9073), and reducing the flop count almost 39% (from 296923 to 181525). <p> As can be seen, it is quite successful at this, reducing the fill-in of L 21% (from 11533 to 9073), and reducing the flop count almost 39% (from 296923 to 181525). Another popular ordering algorithm is called the minimum degree ordering <ref> [112, 91] </ref>, which is designed to create as little fill-in at each step of Cholesky as possible. <p> Single precision iterative refinement is analyzed in [14, 223, 224]. A comprehensive discussion of error analysis for linear equation solvers, which covers most of these topics, can be found in [147]. For the symmetric indefinite factorization, see [43]. Sparse matrix algorithms are described in <ref> [112, 91] </ref>, as well as the numerous references in Table 2.2. Implementations of many of the algorithms for dense and band matrices described in this chapter are available in LAPACK and Linear Equation Solving 103 CLAPACK [10], which includes a discussion of block algorithms suitable for high performance computers.
Reference: [92] <author> Iain S. Duff. </author> <title> Sparse numerical linear algebra: direct methods and preconditioning. </title> <type> Technical Report RAL-TR-96-047, </type> <institution> Rutherford Appleton Laboratory, </institution> <year> 1996. </year>
Reference-contexts: Table 2.2 [175] gives a list of available software, categorized in several ways. We restrict ourselves to supported codes (either public or commercial), or else research codes when no other software is available for that type of problem or machine. We refer to <ref> [175, 92] </ref> for more complete lists and explanations of the algorithms below. Table 2.2 is organized as follows. The top group of routines, labeled Serial Algorithms, are designed for single processor workstations and PCs.
Reference: [93] <author> I.S Duff and J. K. Reid. MA47, </author> <title> a Fortran code for direct solution of indefinite sparse symmetric linear systems. </title> <type> Technical Report RAL-95-001, </type> <institution> Rutherford Appleton Laboratory, </institution> <year> 1995. </year>
Reference-contexts: UMFPACK [61, 62] MF, Markowitz, BLAS-3 Pub/NETLIB MA38 (same as UMFPACK) Com/HSL nonsym. MA48 [94] Anal: RL, Markowitz Com/HSL Fact: LL, partial, BLAS-1, SD nonsym. SPARSE [165] RL, Markowitz, Scalar Pub/NETLIB sym pattern MUPS [5] MA42 [96] MF, threshold, BLAS-3 Frontal, BLAS-3 Com/HSL sym. MA27 [95]/MA47 <ref> [93] </ref> MF, LDL T , BLAS-1 Com/HSL s.p.d. Ng & Peyton [189] LL, BLAS-3 Pub/Author Shared Memory Algorithms nonsym. SuperLU LL, partial, BLAS-2.5 Pub/UCB nonsym. PARASPAR [268, 269] RL, Markowitz, BLAS-1, SD Res/Author sym- MUPS [6] MF, threshold, BLAS-3 Res/Author pattern nonsym.
Reference: [94] <author> I.S Duff and J. K. Reid. </author> <title> The design of MA48, a code for the direct solution of sparse unsymmetric linear systems of equations. </title> <journal> ACM Transactions on Mathematical Software, </journal> <volume> 22 </volume> <pages> 187-226, </pages> <year> 1996. </year>
Reference-contexts: The inverse of a Cauchy matrix turns out to be a Cauchy matrix, Linear Equation Solving 101 Matrix Status Type Name Algorithm /Source Serial Algorithms nonsym. SuperLU LL, partial, BLAS-2.5 Pub/UCB nonsym. UMFPACK [61, 62] MF, Markowitz, BLAS-3 Pub/NETLIB MA38 (same as UMFPACK) Com/HSL nonsym. MA48 <ref> [94] </ref> Anal: RL, Markowitz Com/HSL Fact: LL, partial, BLAS-1, SD nonsym. SPARSE [165] RL, Markowitz, Scalar Pub/NETLIB sym pattern MUPS [5] MA42 [96] MF, threshold, BLAS-3 Frontal, BLAS-3 Com/HSL sym. MA27 [95]/MA47 [93] MF, LDL T , BLAS-1 Com/HSL s.p.d.
Reference: [95] <author> I.S Duff and J.K Reid. </author> <title> The multifrontal solution of indefinite sparse symmetric linear equations. </title> <journal> ACM Transactions on Mathematical Software, </journal> <volume> 9(3) </volume> <pages> 302-325, </pages> <month> September </month> <year> 1983. </year> <note> Bibliography 437 </note>
Reference: [96] <author> I.S Duff and J. A. Scott. </author> <title> The design of a new frontal code for solving sparse unsymmetric systems. </title> <journal> ACM Transactions on Mathematical Software, </journal> <volume> 22(1) </volume> <pages> 30-45, </pages> <year> 1996. </year>
Reference-contexts: SuperLU LL, partial, BLAS-2.5 Pub/UCB nonsym. UMFPACK [61, 62] MF, Markowitz, BLAS-3 Pub/NETLIB MA38 (same as UMFPACK) Com/HSL nonsym. MA48 [94] Anal: RL, Markowitz Com/HSL Fact: LL, partial, BLAS-1, SD nonsym. SPARSE [165] RL, Markowitz, Scalar Pub/NETLIB sym pattern MUPS [5] MA42 <ref> [96] </ref> MF, threshold, BLAS-3 Frontal, BLAS-3 Com/HSL sym. MA27 [95]/MA47 [93] MF, LDL T , BLAS-1 Com/HSL s.p.d. Ng & Peyton [189] LL, BLAS-3 Pub/Author Shared Memory Algorithms nonsym. SuperLU LL, partial, BLAS-2.5 Pub/UCB nonsym.
Reference: [97] <author> A. Edelman. </author> <title> The complete pivoting conjecture for Gaussian elimination is false. </title> <journal> The Mathematica Journal, </journal> <volume> 2 </volume> <pages> 58-61, </pages> <year> 1992. </year>
Reference-contexts: The average behavior of g CP is n 1=2 . It was an old open conjecture that g CP n, but this was recently disproved <ref> [97, 120] </ref>.
Reference: [98] <author> A. Edelman and H. Murakami. </author> <title> Polynomial roots from companion matrices. </title> <journal> Math. Comp., </journal> <volume> 64(210) </volume> <pages> 763-776, </pages> <month> April </month> <year> 1995. </year>
Reference-contexts: of the matrix C, which is called the companion matrix of the polynomial (4.7). (The Matlab routine roots for finding roots of a polynomial applies the Hessenberg QR iteration of section 4.4.8 to the companion matrix C, since this is currently one of the most reliable, if expensive, methods known <ref> [98, 115, 239] </ref>. Cheaper alternatives are under development.) The same idea works when the A i are matrices. C becomes an n d-by-n d block companion matrix, where the 1's and 0's below the top row become n-by-n identity and zero matrices, respectively.
Reference: [99] <author> S. Eisenstat and I. Ipsen. </author> <title> Relative perturbation techniques for singular value problems. </title> <journal> SIAM J. Numer. Anal., </journal> <volume> 32(6), </volume> <year> 1995. </year>
Reference-contexts: * 2 , and so by the triangle inequality (see Question 5.11) 1 sin 2 2 1 sin 2 2 1 sin 2 1 + sin 2 * 1 + * 2 The Symmetric Eigenproblem and SVD 231 as desired. 2 An analogous theorem can be proven for singular vectors <ref> [99] </ref>. Example 5.6 We again consider the mass-spring system of Example 5.1, and use it to show that bounds on eigenvalues provided by Weyl's Theorem (Theorem 5.1) can be much worse (looser) than the "relative" version of Weyl's Theorem (Theorem 5.6). <p> The material on relative perturbation theory can be found in <ref> [74, 81, 99] </ref>; section 5.2.1 was based on the latter of these references. Related work is found in [65, 90, 226, 248] A classical text on perturbation theory for general linear operators is [159]. For a survey of parallel algorithms for the symmetric eigenproblem, see [75].
Reference: [100] <author> V. Faber and T. Manteuffel. </author> <title> Necessary and sufficient conditions for the existence of a conjugate gradient method. </title> <journal> SIAM J. Num. Anal., </journal> <volume> 21 </volume> <pages> 315-339, </pages> <year> 1984. </year>
Reference-contexts: Essentially, algorithms satisfying these two properties exist only for matrices of the form e i (T + I), where T = T T (or T H = (HT ) T for some symmetric positive definite H), is real, and is complex <ref> [100, 249] </ref>. For these symmetric and special nonsymmetric A, it turns out we can find a short recurrence, as in the Lanczos algorithm, for computing an orthogonal basis [q 1 ; :::; q k ] of K k (A; b).
Reference: [101] <author> David M. Fenwick, Denis J. Foley, William B. Gist, Stephen R. Van-Doren, and Daniel Wissel. </author> <title> The AlphaServer 8000 series: High-end server platform development. </title> <journal> Digital Technical Journal, </journal> <volume> 7(1) </volume> <pages> 43-65, </pages> <year> 1995. </year>
Reference-contexts: LAPACK (and its versions in other languages) are suitable for PCs, workstations, vector computers, and shared memory parallel computers. These include the SUN SPARCcenter 2000 [236], SGI Power Challenge [221], DEC AlphaServer 8400 <ref> [101] </ref>, and Cray C90/J90 [251, 252]. ScaLAPACK is suitable for distributed memory parallel computers, such as the IBM SP-2 [254], Intel Paragon [255], Cray T3 series [253], and networks of workstations 6 A C translation of LAPACK, called CLAPACK (at NETLIB/clapack), is also available. <p> The top group of routines, labeled Serial Algorithms, are designed for single processor workstations and PCs. The Shared Memory Algorithms are for symmetric multiprocessors, such as the SUN SPARCcenter 2000 [236], SGI Power Challenge [221], DEC 98 Chapter 2 Linear Equation Solving 99 , AlphaServer 8400 <ref> [101] </ref>, and Cray C90/J90 [251, 252]. The Distributed Memory Algorithms are for machines like the IBM SP-2 [254], Intel Paragon [255], Cray T3 series [253], and networks of workstations [9].
Reference: [102] <author> K. Fernando and B. Parlett. </author> <title> Accurate singular values and differential qd algorithms. </title> <journal> Numerische Mathematik, </journal> <volume> 67 </volume> <pages> 191-229, </pages> <year> 1994. </year>
Reference-contexts: It is asymptotically cubically convergent for almost all matrices. A proof of this theorem can be found in [195]. In LAPACK this routine is available as ssyev. The inner loop of the algorithm can be organized more efficiently when eigenvalues only are desired (ssterf; see also <ref> [102, 198] </ref>) than when eigenvectors are also computed (ssteqr). 236 Chapter 5 Example 5.7 Here is an illustration of the convergence of tridiagonal QR iteration, starting with the following tridiagonal matrix (diagonals only are shown, in columns) T 0 = tridiag 2 6 6 6 6 :24929 :96880 :48539 :91563 7 <p> Because of the instability caused by computing T BB T or T B T B , good SVD algorithms work directly on B, or possibly T ps . In summary, we describe the practical algorithms used for computing the SVD. 1. QR iteration and its variations. Properly implemented <ref> [102] </ref>, this is the fastest algorithm for finding all the singular values of a bidiagonal matrix. Furthermore, it finds all the singular values to high relative accuracy, as discussed in section 5.2.1, This means that all the digits of all the singular values are correct, even the tiniest ones. <p> The algorithm in the LAPACK routine sbdsqr was originally based on [79], and later updated to use the algorithm in <ref> [102] </ref> in the case when singular values only are desired. This latter algorithm, called dqds for historical reasons, 6 is elegant, fast, and accurate, so we will present it. <p> This modification pays off handsomely in guaranteed high relative accuracy, as described in the next section. There are two important issues we have not discussed: choosing a shift ffi = t 2 i+1 t 2 i , and detecting convergence. These are discussed in detail in <ref> [102] </ref>. 5.4.2 Computing the Bidiagonal SVD to High Rel ative Accuracy This section, which depends on section 5.2.1, may be skipped on a first reading. <p> Therefore, ^ B is computed to high relative accuracy, and so by Theorem 5.13, the singular values of B and ^ B agree to high relative accuracy. The general case, where ffi &gt; 0, is trickier <ref> [102] </ref>: Theorem 5.14 One step of Algorithm 5.11 in floating point arithmetic, applied to B and yielding ^ B, is equivalent to the following sequence of operations: 1. Make a small relative change (by at most 1:5") in each entry of B, getting ~ B. 2. <p> For a survey of parallel algorithms for the symmetric eigenproblem, see [75]. The QR algorithm for finding the SVD of bidiagonal matrices is discussed in [79, 66, 118]. and the dqds algorithm is in <ref> [102, 198, 207] </ref>. For an error analysis of the Bisection algorithm, see [72, 73, 154], and for recent attempts to accelerate Bisection see [103, 201, 199, 174, 171, 173, 267]. Current work in improving Inverse Iteration appears in [103, 199, 201].
Reference: [103] <author> V. Fernando, B. Parlett, and I. Dhillon. </author> <title> A way to find the most redundant equation in a tridiagonal system. </title> <institution> Berkeley Mathematics Dept. </institution> <type> Preprint, </type> <year> 1995. </year>
Reference-contexts: Bisection and Inverse Iteration are available as options in the LAPACK routine ssyevx . There is current research on Inverse Iteration, addressing the problem of close eigenvalues, which may make it the fastest method to find all the eigenvectors eigenvectors (besides, theoretically, divide-and-conquer with the FMM) <ref> [103, 201, 199, 174, 171, 173, 267] </ref>. But software implementing this improved version of Inverse Iteration is not yet available. 5. Jacobi's method. This method is historically the oldest method for the eigenproblem, dating to 1846. <p> Worse, there is no guarantee that the computed eigenvectors are accurate or orthogonal. (The trouble is that after reorthogonalizing a set of nearly dependent ^q k , cancellation may mean some computed eigenvectors consist of little more than roundoff errors.) There has been recent progress on this problem, however <ref> [103, 199, 201] </ref>, and it now appears possible that Inverse Iteration may be "repaired" to provide accurate, orthogonal eigenvectors without spending more than O (n) flops per eigenvector to reorthogonalize. <p> The QR algorithm for finding the SVD of bidiagonal matrices is discussed in [79, 66, 118]. and the dqds algorithm is in [102, 198, 207]. For an error analysis of the Bisection algorithm, see [72, 73, 154], and for recent attempts to accelerate Bisection see <ref> [103, 201, 199, 174, 171, 173, 267] </ref>. Current work in improving Inverse Iteration appears in [103, 199, 201]. The divide-and-conquer eigenroutine was introduced in [58] and further developed in [13, 88, 125, 129, 151, 170, 208, 232]. <p> For an error analysis of the Bisection algorithm, see [72, 73, 154], and for recent attempts to accelerate Bisection see [103, 201, 199, 174, 171, 173, 267]. Current work in improving Inverse Iteration appears in <ref> [103, 199, 201] </ref>. The divide-and-conquer eigenroutine was introduced in [58] and further developed in [13, 88, 125, 129, 151, 170, 208, 232]. The possibility of high accuracy eigenvalues obtained from Ja-cobi is discussed in [65, 74, 81, 90, 181, 226].
Reference: [104] <author> H. Flaschka. </author> <title> Dynamical Systems, Theory and Applications, volume 38 of Lecture Notes in Physics, chapter Discrete and periodic solutions of some aspects of the inverse method. </title> <publisher> Springer-Verlag, </publisher> <year> 1975. </year>
Reference-contexts: In other words, there is an infinite sequence of energy-like quantities conserved by KdV. This is important both for theoretical and numerical reasons. For more details on the Toda flow, see [142, 168, 66, 67, 237], and papers by Kruskal [164], Flaschka <ref> [104] </ref> and Moser [185] in [186]. 5.6 References and Other Topics for Chap ter 5 An excellent general reference for the symmetric eigenproblem is [195]. The material on relative perturbation theory can be found in [74, 81, 99]; section 5.2.1 was based on the latter of these references. <p> The divide-and-conquer eigenroutine was introduced in [58] and further developed in [13, 88, 125, 129, 151, 170, 208, 232]. The possibility of high accuracy eigenvalues obtained from Ja-cobi is discussed in [65, 74, 81, 90, 181, 226]. The Toda flow and related phenomena are discussed in <ref> [66, 67, 104, 142, 164, 168, 185, 186, 237] </ref>. 5.7 Questions for Chapter 5 Question 5.1 (Easy; Z. Bai) Show that A = B + iC is Hermitian if and only if M = B C is symmetric.
Reference: [105] <author> R. Freund, G. Golub, and N. Nachtigal. </author> <title> Iterative solution of linear systems. </title> <editor> In A. Iserles, editor, </editor> <booktitle> Acta Numerica 1992, </booktitle> <pages> pages 57-100. </pages> <publisher> Cambridge University Press, </publisher> <year> 1992. </year>
Reference-contexts: We also refer the reader to the more comprehensive on-line help at NETLIB/templates, which includes a book [24] and implementations in Matlab, Fortran, and C++. For a survey of current research in Krylov subspace methods, see <ref> [15, 105, 134, 212] </ref>. <p> It is called PETSc, for Portable Extensible Toolkit for Scientific computing. PETSc is available at http://www.mcs.anl.gov/petsc/petsc.html, and described briefly in [230]. 6.11 References and Other Topics for Chap ter 6 Up-to-date surveys of modern iterative methods are given in <ref> [15, 105, 134, 212] </ref>, and their parallel implementations are also surveyed in [75]. Classical methods like Jacobi, Gauss-Seidel and SOR are discussed in detail in [247, 135].
Reference: [106] <author> R. Freund, M. Gutknecht, and N. Nachtigal. </author> <title> An implementation of the look-ahead Lanczos algorithm for non-Hermitian matrices. </title> <journal> SIAM J. Sci. Comput., </journal> <volume> 14 </volume> <pages> 137-158, </pages> <year> 1993. </year>
Reference-contexts: In fact, it is not always possible to find an appropriate similarity because of a phenomenon known as "breakdown" [41, 132, 133, 197]. Attempts to repair breakdown by by a process called "lookahead" have been proposed, implemented and analyzed in <ref> [16, 18, 54, 55, 63, 106, 200, 263, 264] </ref>.
Reference: [107] <author> X. Sun G. Quintana-Orti and C. Bischof. </author> <title> A blas-3 version of the QR factorization with column pivoting. </title> <type> Argonne Preprint MCS-P551-1295, </type> <institution> Argonne National Laboratory, </institution> <year> 1995. </year> <note> 438 Bibliography </note>
Reference-contexts: Linear Least Squares Problems 145 More sophisticated pivoting schemes than QR with column pivoting, called rank-revealing QR algorithms, have been a subject of much recent study. Rank-revealing QR algorithms which detect rank more reliably and sometimes also faster than QR with column pivoting have been developed <ref> [28, 30, 47, 49, 107, 124, 126, 148, 194, 234] </ref>. We discuss these further in the next section. QRD with column pivoting is available as subroutine sgeqpf in LA-PACK. LAPACK also has several similar factorizations available: RQ (sgerqf), LQ (sgelqf), and QL (sgeqlf).
Reference: [108] <author> F. Gantmacher. </author> <title> The Theory of Matrices, vol. II (transl.). </title> <publisher> Chelsea, </publisher> <address> New York, </address> <year> 1959. </year>
Reference-contexts: For a proof of this theorem, see a book on linear algebra such as <ref> [108] </ref> or [137]. Each J m () is called a Jordan block with eigenvalue of algebraic multiplicity m. If some n i = 1, and i is an eigenvalue of only that one Jordan block, then i is called a simple eigenvalue. <p> 6 i 1 . . . i 7 7 and N m i is a "Jordan block for = 1 with multiplicity m i " N m i = 6 6 1 . . . 1 7 7 = I m i J m i (0) For a proof, see <ref> [108] </ref>. Application of Jordan and Weierstrass Forms to Differential Equations Consider the linear differential equation _x (t) = Ax (t) + f (t), x (0) = x 0 . <p> L T m has an analogous left null vector. For a proof, see <ref> [108] </ref>. Just as Schur form generalized to regular matrix pencils in the last section, it can be generalized to arbitrary singular pencils as well. For the canonical form, perturbation theory and software, see [27, 78, 244]. Singular pencils are used to model systems arising in systems and control. <p> For more details about perturbation theory of eigenvalues and eigenvectors, see [159, 235, 51], and chapter 4 of [10]. For a proof of Theorem 4.7, see [68]. For a discussion of Weierstrass and Kronecker canon ical forms, see <ref> [108, 116] </ref>. For their application to systems and control theory, see [244, 245, 77]. For applications to computational geometry, graphics, and mechanical CAD, see [179, 180, 163].
Reference: [109] <author> M. Garey and D. Johnson. </author> <title> Computers and Intractability. </title> <editor> W. H. </editor> <publisher> Freeman, </publisher> <address> San Francisco, </address> <year> 1979. </year>
Reference-contexts: Even if we do not have to worry about pivoting for numerical stability (such as in Cholesky), choosing the optimal permutations of rows and columns to minimize storage or work is an extremely hard problem. In fact, it is NP-complete <ref> [109] </ref>, which means all known algorithms for finding the optimal permutation run in time which grows exponentially with n, and so are vastly more expensive than even dense Gaussian elimination for large n. Thus we must settle for using heuristics, of which there are several successful candidates.
Reference: [110] <author> A. George. </author> <title> Nested dissection of a regular finite element mesh. </title> <journal> SIAM J. Num. Anal., </journal> <volume> 10 </volume> <pages> 345-363, </pages> <year> 1973. </year>
Reference-contexts: Furthermore, we are assuming the rows and columns of T NfiN have been "optimally ordered" to minimize work and storage (using nested dissection <ref> [110, 111] </ref>).
Reference: [111] <author> A. George, M. Heath, J. Liu, and E. Ng. </author> <title> Solution of sparse positive definite systems on a shared memory multiprocessor. </title> <journal> Internat. J. Parallel Programming, </journal> <volume> 15 </volume> <pages> 309-325, </pages> <year> 1986. </year>
Reference-contexts: Furthermore, we are assuming the rows and columns of T NfiN have been "optimally ordered" to minimize work and storage (using nested dissection <ref> [110, 111] </ref>).
Reference: [112] <author> A. George and J. Liu. </author> <title> Computer Solution of Large Sparse Positive Definite Systems. </title> <publisher> Prentice-Hall Inc., </publisher> <address> Englewood Cliffs, New Jersey, </address> <year> 1981. </year>
Reference-contexts: In other words, we can not afford to do O (n 3 ) integer and logical operations to discover the few floating point operations we want to do. A more complete discussion of these algorithms is beyond the scope of this book <ref> [112, 91] </ref>, but we will point to available software. Example 2.9 We illustrate sparse Cholesky on a more realistic example that arises from modeling the displacement of a mechanical structure subject to external forces. <p> The number of nonzeros in L, and the number of flops required to compute L, can be changed significantly by reordering the rows and columns of A. The middle pair of plots in Figure 2.11 show the results of one such popular reordering, called Reverse Cuthill-McKee <ref> [112, 91] </ref>, which is designed to make A a narrow band matrix. As can be seen, it is quite successful at this, reducing the fill-in of L 21% (from 11533 to 9073), and reducing the flop count almost 39% (from 296923 to 181525). <p> As can be seen, it is quite successful at this, reducing the fill-in of L 21% (from 11533 to 9073), and reducing the flop count almost 39% (from 296923 to 181525). Another popular ordering algorithm is called the minimum degree ordering <ref> [112, 91] </ref>, which is designed to create as little fill-in at each step of Cholesky as possible. <p> Single precision iterative refinement is analyzed in [14, 223, 224]. A comprehensive discussion of error analysis for linear equation solvers, which covers most of these topics, can be found in [147]. For the symmetric indefinite factorization, see [43]. Sparse matrix algorithms are described in <ref> [112, 91] </ref>, as well as the numerous references in Table 2.2. Implementations of many of the algorithms for dense and band matrices described in this chapter are available in LAPACK and Linear Equation Solving 103 CLAPACK [10], which includes a discussion of block algorithms suitable for high performance computers.
Reference: [113] <author> Alan George and Esmond Ng. </author> <title> Parallel sparse Gaussian elimination with partial pivoting. </title> <journal> Annals of Operation Research, </journal> <volume> 22 </volume> <pages> 219-240, </pages> <year> 1990. </year>
Reference-contexts: MA27 [95]/MA47 [93] MF, LDL T , BLAS-1 Com/HSL s.p.d. Ng & Peyton [189] LL, BLAS-3 Pub/Author Shared Memory Algorithms nonsym. SuperLU LL, partial, BLAS-2.5 Pub/UCB nonsym. PARASPAR [268, 269] RL, Markowitz, BLAS-1, SD Res/Author sym- MUPS [6] MF, threshold, BLAS-3 Res/Author pattern nonsym. George & Ng <ref> [113] </ref> RL, partial, BLAS-1 Res/Author s.p.d. Gupta, Rothberg, LL, BLAS-3 Com/SGI Ng & Peyton [131] Pub/Author s.p.d. SPLASH [153] RL, 2-D block, BLAS-3 Pub/Stanford Distributed Memory Algorithms sym. van der Stappen [243] RL, Markowitz, Scalar Res/Author sym- Lucas et al. [178] MF, no pivoting, BLAS-1 Res/Author pattern s.p.d.
Reference: [114] <editor> R. Glowinski, G. Golub, G. Meurant, and J. Periaux, editors. </editor> <title> Domain Decomposition Methods for Partial Differential Equations, </title> <address> Philadelphia, </address> <year> 1988. </year> <title> SIAM. Proceedings of the First International Symposium on Domain Decomposition Methods for Partial Differential Equations, </title> <address> Paris, France, </address> <month> January </month> <year> 1987. </year>
Reference-contexts: For an implementation of this algorithm, see Question 6.16. The web site [89] contains pointers to an extensive literature, software, and so on. Iterative Methods for Linear Systems 385 6.10 Domain Decomposition Domain decomposition for solving sparse systems of linear equations is a topic of current research. See <ref> [48, 114, 203] </ref> and especially [230] for recent surveys. We will only give simple examples. The need for methods beyond those we have discussed arises from of the irregularity and size of real problems, and also from the need for algorithms for parallel computers. <p> What makes domain decomposition so attractive is that S turns out to be much better conditioned that the original matrix A (a condition number which grows like O (N ) instead of O (N 2 )), and so convergence is fast <ref> [114, 203] </ref>. More generally, one has k &gt; 2 subdomains, separated by boundaries (see Figure 6.21, where the heavy lines separate subdomains). <p> Classical methods like Jacobi, Gauss-Seidel and SOR are discussed in detail in [247, 135]. Multigrid methods are discussed in [42, 183, 184, 258, 266] and the references therein; [89] is a web site with pointers to an extensive bibliography, software, and so on. Domain decomposition are discussed in <ref> [48, 114, 203, 230] </ref>. Chebyshev and other polynomials are discussed in [238]. The FFT is discussed in any good textbook on computer science algorithms, such as [3], or in [246]. A stabilized version of block cyclic reduction is found in [46, 45].
Reference: [115] <author> S. Goedecker. </author> <title> Remark on algorithms to find roots of polynomials. </title> <journal> SIAM J. Sci. Stat. Comp., </journal> <volume> 15(5) </volume> <pages> 1059-1063, </pages> <month> September </month> <year> 1994. </year>
Reference-contexts: of the matrix C, which is called the companion matrix of the polynomial (4.7). (The Matlab routine roots for finding roots of a polynomial applies the Hessenberg QR iteration of section 4.4.8 to the companion matrix C, since this is currently one of the most reliable, if expensive, methods known <ref> [98, 115, 239] </ref>. Cheaper alternatives are under development.) The same idea works when the A i are matrices. C becomes an n d-by-n d block companion matrix, where the 1's and 0's below the top row become n-by-n identity and zero matrices, respectively.
Reference: [116] <author> I. Gohberg, P. Lancaster, and L. Rodman. </author> <title> Matrix Polynomials. </title> <publisher> Academic Press, </publisher> <address> New York, </address> <year> 1982. </year>
Reference-contexts: For more details about perturbation theory of eigenvalues and eigenvectors, see [159, 235, 51], and chapter 4 of [10]. For a proof of Theorem 4.7, see [68]. For a discussion of Weierstrass and Kronecker canon ical forms, see <ref> [108, 116] </ref>. For their application to systems and control theory, see [244, 245, 77]. For applications to computational geometry, graphics, and mechanical CAD, see [179, 180, 163].
Reference: [117] <author> D. Goldberg. </author> <title> What every computer scientist should know about floating point arithmetic. </title> <journal> ACM Computing Surveys, </journal> <volume> 23(1), </volume> <year> 1991. </year>
Reference: [118] <author> G. Golub and W. Kahan. </author> <title> Calculating the singular values and pseudo-inverse of a matrix. </title> <journal> SIAM J. Num. Anal. (Series B), </journal> <volume> 2(2) </volume> <pages> 205-224, </pages> <year> 1965. </year>
Reference-contexts: The transformations are not straightforward, however, because the added structure of the SVD can 262 Chapter 5 tridiagonal matrix, relative to divide-and-conquer. The Symmetric Eigenproblem and SVD 263 dense matrix, relative to divide-and-conquer. 264 Chapter 5 often be exploited to make the algorithms more efficient or more accurate <ref> [118, 79, 66] </ref>. All the algorithms for the eigendecomposition of a symmetric matrix A, except Jacobi, had the following structure: 1. Reduce A to tridiagonal form T with an orthogonal matrix Q 1 : A = Q 1 T Q T 2. <p> Related work is found in [65, 90, 226, 248] A classical text on perturbation theory for general linear operators is [159]. For a survey of parallel algorithms for the symmetric eigenproblem, see [75]. The QR algorithm for finding the SVD of bidiagonal matrices is discussed in <ref> [79, 66, 118] </ref>. and the dqds algorithm is in [102, 198, 207]. For an error analysis of the Bisection algorithm, see [72, 73, 154], and for recent attempts to accelerate Bisection see [103, 201, 199, 174, 171, 173, 267]. Current work in improving Inverse Iteration appears in [103, 199, 201].
Reference: [119] <author> G. Golub and C. Van Loan. </author> <title> Matrix Computations. </title> <publisher> Johns Hopkins University Press, </publisher> <address> Baltimore, MD, </address> <note> 3rd edition, </note> <year> 1996. </year>
Reference-contexts: The factor 3g PP n 3 in the bound causes it to almost always greatly overestimate the true kffiAk, 4 This definition is slightly different from the usual one in the literature, but essen tially equivalent <ref> [119, p. 115] </ref>. 56 Chapter 2 even if g PP = 1. For example, if " = 10 7 and n = 150, a very modest sized matrix, then 3n 3 " &gt; 1, meaning that all precision is potentially lost. <p> Using Newton interpolation we can solve V T a = y in 5 2 n 2 instead of 2 3 n 3 flops. There is a similar trick to solve V a = y in 5 2 n 2 flops too. See <ref> [119, p. 178.] </ref>. <p> They arise in problems of signal processing. There are algorithms for solving such systems that take only O (n 2 ) operations. All these methods generalize to many other similar matrices depend ing on only O (n) parameters. See <ref> [119, p. 183.] </ref>, or [158] for a recent survey. 2.8 References and Other Topics for Chap ter 2 Further details about linear equation solving in general may be found in chapters 3 and 4 of [119]. <p> See [119, p. 183.], or [158] for a recent survey. 2.8 References and Other Topics for Chap ter 2 Further details about linear equation solving in general may be found in chapters 3 and 4 of <ref> [119] </ref>. The reciprocal relationship between condition numbers and distance to the nearest ill-posed problem is further explored in [70]. An average case analysis of pivot growth is described in [240], and an example of bad pivot growth with complete pivoting is given in [120]. <p> See also chapter 5 of <ref> [119] </ref> and [166]. Perturbation theory and error bounds for the least squares solution is discussed in detail in [147]. Rank-revealing QRD decompositions are discussed in [28, 30, 47, 49, 124, 148, 194, 204, 234].
Reference: [120] <author> N. Gould. </author> <title> On growth in Gaussian elimination with complete pivoting. </title> <journal> SIAM J. Mat. Anal. Appl., </journal> <volume> 12(2), </volume> <year> 1991. </year> <note> see also editor's note in n. 3 of the same volume of journal. Bibliography 439 </note>
Reference-contexts: The average behavior of g CP is n 1=2 . It was an old open conjecture that g CP n, but this was recently disproved <ref> [97, 120] </ref>. <p> The reciprocal relationship between condition numbers and distance to the nearest ill-posed problem is further explored in [70]. An average case analysis of pivot growth is described in [240], and an example of bad pivot growth with complete pivoting is given in <ref> [120] </ref>. Condition estimators are described in [136, 144, 146]. Single precision iterative refinement is analyzed in [14, 223, 224]. A comprehensive discussion of error analysis for linear equation solvers, which covers most of these topics, can be found in [147]. For the symmetric indefinite factorization, see [43].
Reference: [121] <author> A. Greenbaum and Z. Strakos. </author> <title> Predicting the behavior of finite precision Lanczos and conjugate gradient computations. </title> <journal> SIAM J. Mat. Anal. Appl., </journal> <volume> 13(1) </volume> <pages> 121-137, </pages> <year> 1992. </year>
Reference-contexts: Eventually it was recognized as a perfectly good iterative method, often providing quite accurate answers after k t n steps. Recently, a subtle backward error analysis was devised to explain the observed behavior of CG in floating point, and explain how it can differ from exact arithmetic <ref> [121] </ref>. This behavior can also include long "plateaus" in the convergence, with kr k k 2 decreasing little for many iterations, interspersed by periods of rapid convergence.
Reference: [122] <author> L. Greengard and V. Rokhlin. </author> <title> A fast algorithm for particle simulations. </title> <journal> J. Comp. Phys., </journal> <volume> 73 </volume> <pages> 325-348, </pages> <year> 1987. </year>
Reference-contexts: In theory, divide-and-conquer could be implemented to run in O (n log p n) flops, where p is a small integer [129]. This superfast implementation uses the Fast Multipole Method (FMM) <ref> [122] </ref>, originally invented for the completely different problem of computing the mutual forces on n electrically charged particles. <p> After discussing deflation in the next section, we discuss details of solving the secular equation, and computing the eigenvectors stably. Finally, we discuss how to accelerate the method by exploiting Fast Multipole Method (FMM) techniques used in electrostatic particle simulation <ref> [122] </ref>. These sections may be skipped on a first reading. Deflation So far in our presentation we have assumed that the d i are distinct, and the u i nonzero. <p> This inexpensive computation does not 250 Chapter 5 This algorithm is described in more detail in [127, 129], and implemented in LAPACK routine slaed3. Accelerating Divide-and-Conquer using the Fast Multipole Method The Fast Multipole Method (FMM) <ref> [122] </ref> was originally invented for the completely different problem of computing the mutual forces on n electrically charged particles, or the mutual gravitational forces on n masses. We only sketch how these problems are related to finding eigenvalues and eigenvectors, and leave details to [129]. <p> Evaluating this sum for j = 1; :::; n appears to require O (n 2 ) flops. The Fast Multipole Method and others like it <ref> [122, 23] </ref> can be used to approximately (but very accurately) evaluate this sum in O (n log n) time (or even O (n)) time instead. (See the lectures on "Fast Hierarchical Methods for the N-body Problem" at PARALLEL HOMEPAGE for details.) But this idea alone is not enough to reduce the
Reference: [123] <author> R. Grimes, J. Lewis, and H. Simon. </author> <title> A shifted block Lanczos algorithm for solving sparse symmetric generalized eigenproblems. </title> <journal> SIAM J. Mat. Anal. Appl., </journal> <volume> 15(1) </volume> <pages> 228-272, </pages> <year> 1994. </year>
Reference-contexts: Indeed, there is a simple and inexpensive recurrence for deciding when to reorthogonal-ize [222, 190]. Another enhancement is to use the error bounds to efficiently distinguish between converged and "misconverged" eigenvalues [196]. A current state-of-the-art implementation of Lanczos is described in <ref> [123] </ref>. A different software implementation is available in ARPACK (NETLIB/scalapack/readme.arpack, [169, 231]). If we apply Lanczos to the shifted and inverted matrix (AI) 1 , then we expect the eigenvalues closest to to converge first. <p> (Algorithm 4.3) [19], Davidson's algorithm [214] or the Jacobi-Davidson algorithm [228] to the sparse nonsymmetric eigenproblem. 7.8 References and Other Topics for Chap ter 7 In addition to the references in sections 7.6 and 7.7, there are a number of good surveys available on algorithms for sparse eigenvalues problems: see <ref> [17, 50, 123, 161, 195, 211, 260] </ref>. Parallel implementations are also discussed in [75]. In section 6.2 we discussed the existence of on-line help to choose among the variety of iterative methods available for solving Ax = b.
Reference: [124] <author> M. Gu. </author> <title> Studies in numerical linear algebra. </title> <type> Ph.D. thesis, </type> <year> 1993. </year>
Reference-contexts: Linear Least Squares Problems 145 More sophisticated pivoting schemes than QR with column pivoting, called rank-revealing QR algorithms, have been a subject of much recent study. Rank-revealing QR algorithms which detect rank more reliably and sometimes also faster than QR with column pivoting have been developed <ref> [28, 30, 47, 49, 107, 124, 126, 148, 194, 234] </ref>. We discuss these further in the next section. QRD with column pivoting is available as subroutine sgeqpf in LA-PACK. LAPACK also has several similar factorizations available: RQ (sgerqf), LQ (sgelqf), and QL (sgeqlf). <p> See also chapter 5 of [119] and [166]. Perturbation theory and error bounds for the least squares solution is discussed in detail in [147]. Rank-revealing QRD decompositions are discussed in <ref> [28, 30, 47, 49, 124, 148, 194, 204, 234] </ref>.
Reference: [125] <author> M. Gu and S. Eisenstat. </author> <title> A stable algorithm for the rank-1 modification of the symmetric eigenproblem. </title> <institution> Computer Science Dept. Report YALEU/DCS/RR-916, Yale University, </institution> <month> September </month> <year> 1992. </year>
Reference-contexts: Indeed, although this method was first introduced in 1981 [58], the "right" implementation was not discovered until 1992 <ref> [125, 129] </ref>). This routine is available as LAPACK routines ssyevd for dense matrices, and sstevd for tridiagonal matrices. This routine uses divide-and-conquer for matrices of dimension larger than 25, and automatically switches to QR iteration for smaller matrices (or if eigenvalues only are desired). <p> For an error analysis of the Bisection algorithm, see [72, 73, 154], and for recent attempts to accelerate Bisection see [103, 201, 199, 174, 171, 173, 267]. Current work in improving Inverse Iteration appears in [103, 199, 201]. The divide-and-conquer eigenroutine was introduced in [58] and further developed in <ref> [13, 88, 125, 129, 151, 170, 208, 232] </ref>. The possibility of high accuracy eigenvalues obtained from Ja-cobi is discussed in [65, 74, 81, 90, 181, 226].
Reference: [126] <author> M. Gu and S. Eisenstat. </author> <title> An efficient algorithm for computing a rank-revealing QR decomposition. </title> <institution> Computer Science Dept. Report YALEU/DCS/RR-967, Yale University, </institution> <month> June </month> <year> 1993. </year>
Reference-contexts: Linear Least Squares Problems 145 More sophisticated pivoting schemes than QR with column pivoting, called rank-revealing QR algorithms, have been a subject of much recent study. Rank-revealing QR algorithms which detect rank more reliably and sometimes also faster than QR with column pivoting have been developed <ref> [28, 30, 47, 49, 107, 124, 126, 148, 194, 234] </ref>. We discuss these further in the next section. QRD with column pivoting is available as subroutine sgeqpf in LA-PACK. LAPACK also has several similar factorizations available: RQ (sgerqf), LQ (sgelqf), and QL (sgeqlf).
Reference: [127] <author> M. Gu and S. C. Eisenstat. </author> <title> A stable and efficient algorithm for the rank-1 modification of the symmetric eigenproblem. </title> <journal> SIAM J. Mat. Anal. Appl., </journal> <volume> 15(4) </volume> <pages> 1266-1276, </pages> <month> October </month> <year> 1994. </year> <type> Yale Tech report YALEU/DCS/RR-916, </type> <month> Sept </month> <year> 1992. </year>
Reference-contexts: Unfortunately, the Cray 2, YMP, and C90 do not round accurately enough to use these efficient algorithms. Finally, an alternative formula was found that makes simulating high precision arithmetic unnecessary. It is based on the following theorem of Lowner <ref> [127, 177] </ref>: Theorem 5.10 (Lowner) Let D = diag (d 1 ; :::; d n ) be diagonal with d n &lt; &lt; d 1 . <p> This inexpensive computation does not 250 Chapter 5 This algorithm is described in more detail in <ref> [127, 129] </ref>, and implemented in LAPACK routine slaed3. Accelerating Divide-and-Conquer using the Fast Multipole Method The Fast Multipole Method (FMM) [122] was originally invented for the completely different problem of computing the mutual forces on n electrically charged particles, or the mutual gravitational forces on n masses.
Reference: [128] <author> M. Gu and S. C. Eisenstat. </author> <title> A divide-and-conquer algorithm for the bidiagonal SVD. </title> <journal> SIAM J. Mat. Anal. Appl., </journal> <volume> 16(1) </volume> <pages> 79-92, </pages> <month> January </month> <year> 1995. </year>
Reference-contexts: We omit divide-and-conquer because of its overall similarity to the algorithm discussed in section 5.3.3, and refer the reader to <ref> [128] </ref> for details. 5.4.1 QR Iteration and its Variations for the Bidi agonal SVD There is a long history of variations on QR iteration for the SVD, designed to be as efficient and accurate as possible; see [198] for a good survey.
Reference: [129] <author> M. Gu and S. C. Eisenstat. </author> <title> A divide-and-conquer algorithm for the symmetric tridiagonal eigenproblem. </title> <journal> SIAM J. Mat. Anal. Appl., </journal> <volume> 16(1) </volume> <pages> 172-191, </pages> <month> January </month> <year> 1995. </year>
Reference-contexts: In theory, divide-and-conquer could be implemented to run in O (n log p n) flops, where p is a small integer <ref> [129] </ref>. This superfast implementation uses the Fast Multipole Method (FMM) [122], originally invented for the completely different problem of computing the mutual forces on n electrically charged particles. <p> Indeed, although this method was first introduced in 1981 [58], the "right" implementation was not discovered until 1992 <ref> [125, 129] </ref>). This routine is available as LAPACK routines ssyevd for dense matrices, and sstevd for tridiagonal matrices. This routine uses divide-and-conquer for matrices of dimension larger than 25, and automatically switches to QR iteration for smaller matrices (or if eigenvalues only are desired). <p> This inexpensive computation does not 250 Chapter 5 This algorithm is described in more detail in <ref> [127, 129] </ref>, and implemented in LAPACK routine slaed3. Accelerating Divide-and-Conquer using the Fast Multipole Method The Fast Multipole Method (FMM) [122] was originally invented for the completely different problem of computing the mutual forces on n electrically charged particles, or the mutual gravitational forces on n masses. <p> We only sketch how these problems are related to finding eigenvalues and eigenvectors, and leave details to <ref> [129] </ref>. Let d 1 through d n be the 3-dimensional position vectors of n particles with charges z i u i . Let ff 1 through ff n be the position vectors of n other particles with unit positive charges. <p> For an error analysis of the Bisection algorithm, see [72, 73, 154], and for recent attempts to accelerate Bisection see [103, 201, 199, 174, 171, 173, 267]. Current work in improving Inverse Iteration appears in [103, 199, 201]. The divide-and-conquer eigenroutine was introduced in [58] and further developed in <ref> [13, 88, 125, 129, 151, 170, 208, 232] </ref>. The possibility of high accuracy eigenvalues obtained from Ja-cobi is discussed in [65, 74, 81, 90, 181, 226].
Reference: [130] <author> A. Gupta and V. Kumar. </author> <title> Optimally scalable parallel sparse Cholesky factorization. </title> <booktitle> In Proceedings of the Seventh SIAM Conference on Parallel Proceesing for Scientific Computing, </booktitle> <pages> pages 442-447. </pages> <publisher> SIAM, </publisher> <year> 1995. </year>
Reference-contexts: SPLASH [153] RL, 2-D block, BLAS-3 Pub/Stanford Distributed Memory Algorithms sym. van der Stappen [243] RL, Markowitz, Scalar Res/Author sym- Lucas et al. [178] MF, no pivoting, BLAS-1 Res/Author pattern s.p.d. Rothberg et al. [205] RL, 2-D block, BLAS-3 Res/Author s.p.d. Gupta <ref> [130] </ref> MF, 2-D block, BLAS-3 Res/Author s.p.d. CAPSS [141] MF, full parallel, BLAS-1 Pub/NETLIB (require coordinates) Table 2.2: Software to solve sparse linear systems using direct methods.
Reference: [131] <author> A. Gupta, E. Rothberg, E. Ng, and B. W. Peyton. </author> <title> Parallel sparse Cholesky factorization algorithms for shared-memory multiprocessor systems. </title> <editor> In R. Vichnevetsky, D. Knight, and G. Richter, editors, </editor> <booktitle> Advances in Computer Methods for Partial Differential Equations-VII. IMACS, </booktitle> <year> 1992. </year> <note> 440 Bibliography </note>
Reference-contexts: SuperLU LL, partial, BLAS-2.5 Pub/UCB nonsym. PARASPAR [268, 269] RL, Markowitz, BLAS-1, SD Res/Author sym- MUPS [6] MF, threshold, BLAS-3 Res/Author pattern nonsym. George & Ng [113] RL, partial, BLAS-1 Res/Author s.p.d. Gupta, Rothberg, LL, BLAS-3 Com/SGI Ng & Peyton <ref> [131] </ref> Pub/Author s.p.d. SPLASH [153] RL, 2-D block, BLAS-3 Pub/Stanford Distributed Memory Algorithms sym. van der Stappen [243] RL, Markowitz, Scalar Res/Author sym- Lucas et al. [178] MF, no pivoting, BLAS-1 Res/Author pattern s.p.d. Rothberg et al. [205] RL, 2-D block, BLAS-3 Res/Author s.p.d.
Reference: [132] <author> M. Gutknecht. </author> <title> A completed theory of the unsymmetric Lanczos process and related algorithms, Part I. </title> <journal> SIAM J. Mat. Anal. Appl., </journal> <volume> 13 </volume> <pages> 594-639, </pages> <year> 1992. </year>
Reference-contexts: Unfortunately, the similarity transformations can be quite ill-conditioned, which means that the eigen 428 Chapter 7 values of the tridiagonal and of the original matrix may greatly differ. In fact, it is not always possible to find an appropriate similarity because of a phenomenon known as "breakdown" <ref> [41, 132, 133, 197] </ref>. Attempts to repair breakdown by by a process called "lookahead" have been proposed, implemented and analyzed in [16, 18, 54, 55, 63, 106, 200, 263, 264].
Reference: [133] <author> M. Gutknecht. </author> <title> A completed theory of the unsymmetric Lanczos process and related algorithms, Part II. </title> <journal> SIAM J. Mat. Anal. Appl., </journal> <volume> 15 </volume> <pages> 15-58, </pages> <year> 1994. </year>
Reference-contexts: Unfortunately, the similarity transformations can be quite ill-conditioned, which means that the eigen 428 Chapter 7 values of the tridiagonal and of the original matrix may greatly differ. In fact, it is not always possible to find an appropriate similarity because of a phenomenon known as "breakdown" <ref> [41, 132, 133, 197] </ref>. Attempts to repair breakdown by by a process called "lookahead" have been proposed, implemented and analyzed in [16, 18, 54, 55, 63, 106, 200, 263, 264].
Reference: [134] <author> W. Hackbusch. </author> <title> Iterative solution of large sparse linear systems of equations. </title> <publisher> Springer, </publisher> <address> Berlin, </address> <year> 1994. </year>
Reference-contexts: In other words, matrix-vector multiplication is a "black-box" called by the template. It is the user's responsibility to supply an implementation of this black-box. Iterative Methods for Linear Systems 295 An analogous templates project for eigenvalue problems is underway. Other recent textbooks on iterative methods are <ref> [15, 134, 212] </ref>. For the most challenging practical problems arising from differential equations more challenging than our model problem, the linear system Ax = b must be "preconditioned", or replaced by the equivalent systems M 1 Ax = M 1 b which is somehow easier to solve. <p> We also refer the reader to the more comprehensive on-line help at NETLIB/templates, which includes a book [24] and implementations in Matlab, Fortran, and C++. For a survey of current research in Krylov subspace methods, see <ref> [15, 105, 134, 212] </ref>. <p> It is called PETSc, for Portable Extensible Toolkit for Scientific computing. PETSc is available at http://www.mcs.anl.gov/petsc/petsc.html, and described briefly in [230]. 6.11 References and Other Topics for Chap ter 6 Up-to-date surveys of modern iterative methods are given in <ref> [15, 105, 134, 212] </ref>, and their parallel implementations are also surveyed in [75]. Classical methods like Jacobi, Gauss-Seidel and SOR are discussed in detail in [247, 135].
Reference: [135] <author> L. A. Hageman and D. M. Young. </author> <title> Applied Iterative Methods. </title> <publisher> Academic Press, </publisher> <address> New York, </address> <year> 1981. </year>
Reference-contexts: Example 6.12 Let us apply SSOR (!) with Chebyshev acceleration to the model problem. We need to both choose ! and estimate the spectral radius = (E ! ). The optimal ! that minimizes is not known but Young <ref> [265, 135] </ref> has shown that the choice ! = 2 1+[2 (1 (R J ))] is a good one, yielding (E ! ) 1 2N . With Chebyshev acceleration the error is multiplied by m 1 T m (1+ p N ) at step m. <p> Classical methods like Jacobi, Gauss-Seidel and SOR are discussed in detail in <ref> [247, 135] </ref>. Multigrid methods are discussed in [42, 183, 184, 258, 266] and the references therein; [89] is a web site with pointers to an extensive bibliography, software, and so on. Domain decomposition are discussed in [48, 114, 203, 230]. Chebyshev and other polynomials are discussed in [238].
Reference: [136] <author> W. W. Hager. </author> <title> Condition estimators. </title> <journal> SIAM J. Sci. Stat. Comput., </journal> <volume> 5 </volume> <pages> 311-316, </pages> <year> 1984. </year>
Reference-contexts: It has been been conjectured that no such estimator exists, but this has not been proven. Linear Equation Solving 59 The algorithm was developed in <ref> [136, 144, 146] </ref>, with the latest version in [145]. <p> The reciprocal relationship between condition numbers and distance to the nearest ill-posed problem is further explored in [70]. An average case analysis of pivot growth is described in [240], and an example of bad pivot growth with complete pivoting is given in [120]. Condition estimators are described in <ref> [136, 144, 146] </ref>. Single precision iterative refinement is analyzed in [14, 223, 224]. A comprehensive discussion of error analysis for linear equation solvers, which covers most of these topics, can be found in [147]. For the symmetric indefinite factorization, see [43].
Reference: [137] <author> P. Halmos. </author> <title> Finite dimensional vector spaces. </title> <publisher> Van Nostrand, </publisher> <address> New York, </address> <year> 1958. </year>
Reference-contexts: Proof: We give two proofs of this theorem. First, this theorem is a restatement of the Gram-Schmidt orthogonalization process <ref> [137] </ref> . . <p> For a proof of this theorem, see a book on linear algebra such as [108] or <ref> [137] </ref>. Each J m () is called a Jordan block with eigenvalue of algebraic multiplicity m. If some n i = 1, and i is an eigenvalue of only that one Jordan block, then i is called a simple eigenvalue. <p> I 0 0 . . . . . . . . . 0 0 I 0 7 7 7 6 6 6 A d I I 7 7 7 : 4.7 References and Other Topics for Chap ter 4 For a general discussion of properties of eigenvalues and eigenvectors, see <ref> [137] </ref>. For more details about perturbation theory of eigenvalues and eigenvectors, see [159, 235, 51], and chapter 4 of [10]. For a proof of Theorem 4.7, see [68]. For a discussion of Weierstrass and Kronecker canon ical forms, see [108, 116].
Reference: [138] <author> E. R. Hansen. </author> <title> Global Optimization Using Interval Analysis. </title> <publisher> Marcel Dekker, Inc., </publisher> <address> New York, </address> <year> 1992. </year>
Reference: [139] <author> P. C. Hansen. </author> <title> The truncated SVD as a method for regularization. </title> <journal> BIT, </journal> <volume> 27(4) </volume> <pages> 534-553, </pages> <year> 1987. </year>
Reference-contexts: What happens when A is rank deficient, or "close" to rank deficient? Such problems arise in practice in many ways, such as extracting signals from noisy data, solution of some integral equations, digital image restoration, computing inverse Laplace transforms, and so on <ref> [139, 140] </ref>. These problems are very ill-conditioned, so we will need to impose extra conditions on their solutions to make them well-conditioned. <p> It is much more reasonable to choose fi = 0, corresponding to the minimum norm solution x. For further justification of using the minimum norm solution for rank-deficient problems, see <ref> [139, 140] </ref>. When A is square and nonsingular, the unique solution of Ax = b is of course b = A 1 x.
Reference: [140] <author> P. C. Hansen. </author> <title> Truncated singular value decomposition solutions to discrete ill-posed problems ill-determined numerical rank. </title> <journal> SIAM J. Sci. Stat. Comp., </journal> <volume> 11(3) </volume> <pages> 503-518, </pages> <year> 1990. </year>
Reference-contexts: What happens when A is rank deficient, or "close" to rank deficient? Such problems arise in practice in many ways, such as extracting signals from noisy data, solution of some integral equations, digital image restoration, computing inverse Laplace transforms, and so on <ref> [139, 140] </ref>. These problems are very ill-conditioned, so we will need to impose extra conditions on their solutions to make them well-conditioned. <p> It is much more reasonable to choose fi = 0, corresponding to the minimum norm solution x. For further justification of using the minimum norm solution for rank-deficient problems, see <ref> [139, 140] </ref>. When A is square and nonsingular, the unique solution of Ax = b is of course b = A 1 x.
Reference: [141] <author> M. T. Heath and P. Raghavan. </author> <title> Performance of a fully parallel sparse solver. </title> <booktitle> In Proc. Scalable High-Performance Computing Conf., </booktitle> <pages> pages 334-341, </pages> <address> Los Alamitos,CA, 1994. </address> <publisher> IEEE. </publisher>
Reference-contexts: Rothberg et al. [205] RL, 2-D block, BLAS-3 Res/Author s.p.d. Gupta [130] MF, 2-D block, BLAS-3 Res/Author s.p.d. CAPSS <ref> [141] </ref> MF, full parallel, BLAS-1 Pub/NETLIB (require coordinates) Table 2.2: Software to solve sparse linear systems using direct methods. Abbreviations used in the table: nonsym. nonsymmetric. sym-pattern symmetric nonzero structure, nonsymmetric values. sym. symmetric and may be indefinite. s.p.d symmetric and positive definite.
Reference: [142] <author> M. Henon. </author> <title> Integrals of the Toda lattice. </title> <journal> Phys. Rev. B, </journal> <volume> 9 </volume> <pages> 1421-1423, </pages> <year> 1974. </year>
Reference-contexts: In other words, there is an infinite sequence of energy-like quantities conserved by KdV. This is important both for theoretical and numerical reasons. For more details on the Toda flow, see <ref> [142, 168, 66, 67, 237] </ref>, and papers by Kruskal [164], Flaschka [104] and Moser [185] in [186]. 5.6 References and Other Topics for Chap ter 5 An excellent general reference for the symmetric eigenproblem is [195]. <p> The divide-and-conquer eigenroutine was introduced in [58] and further developed in [13, 88, 125, 129, 151, 170, 208, 232]. The possibility of high accuracy eigenvalues obtained from Ja-cobi is discussed in [65, 74, 81, 90, 181, 226]. The Toda flow and related phenomena are discussed in <ref> [66, 67, 104, 142, 164, 168, 185, 186, 237] </ref>. 5.7 Questions for Chapter 5 Question 5.1 (Easy; Z. Bai) Show that A = B + iC is Hermitian if and only if M = B C is symmetric.
Reference: [143] <author> M. R. Hestenes and E. </author> <title> Stiefel. Methods of conjugate gradients for solving linear systems. </title> <institution> J. Res. Natl. Bur. Stand., </institution> <month> 49 </month> <pages> 409-436, </pages> <year> 1954. </year>
Reference-contexts: We say the "best" x k minimizes kr k k A 1 . This norm is the same as kx k xk A . The algorithm is called the Conjugate Gradient algorithm, or CG <ref> [143] </ref>. When A is symmetric positive definite, the last two definitions of "best" also turn out to be equivalent: Theorem 6.8 Let A be symmetric, T k = Q T k AQ k , and r k = b Ax k , where x k 2 K k .
Reference: [144] <author> N. J. Higham. </author> <title> A survey of condition number estimation for triangular matrices. </title> <journal> SIAM Review, </journal> <volume> 29 </volume> <pages> 575-596, </pages> <year> 1987. </year>
Reference-contexts: This is all one needs for an error bound which tells you about how many decimal digits of accuracy you have (a factor of 10 error is one decimal digit). 5 There are a variety of such estimators available (see <ref> [144] </ref> for a survey). We choose to present one which is widely applicable to problems besides solving Ax = b, at the cost of being slightly slower than algorithms specialized for Ax = b (but it is still reasonably fast). <p> It has been been conjectured that no such estimator exists, but this has not been proven. Linear Equation Solving 59 The algorithm was developed in <ref> [136, 144, 146] </ref>, with the latest version in [145]. <p> The reciprocal relationship between condition numbers and distance to the nearest ill-posed problem is further explored in [70]. An average case analysis of pivot growth is described in [240], and an example of bad pivot growth with complete pivoting is given in [120]. Condition estimators are described in <ref> [136, 144, 146] </ref>. Single precision iterative refinement is analyzed in [14, 223, 224]. A comprehensive discussion of error analysis for linear equation solvers, which covers most of these topics, can be found in [147]. For the symmetric indefinite factorization, see [43].
Reference: [145] <author> N. J. Higham. </author> <title> FORTRAN codes for estimating the one-norm of a real or complex matrix, with applications to condition estimation. </title> <journal> ACM Trans. Math. Soft., </journal> <volume> 14(4) </volume> <pages> 381-396, </pages> <year> 1988. </year> <note> Bibliography 441 </note>
Reference-contexts: It has been been conjectured that no such estimator exists, but this has not been proven. Linear Equation Solving 59 The algorithm was developed in [136, 144, 146], with the latest version in <ref> [145] </ref>. <p> Choose ~x = e j sign (z j ), where j is chosen so that jz j j = kzk 1 . Then f (~x) f (x) + 5f (~x x) = f (x) + z T (~x x) where the last inequality is true by construction. 2 Higham <ref> [145, 146] </ref> tested a slightly improved version of this algorithm by trying many random matrices of sizes 10; 25; 50 and condition numbers = 10; 10 3 ; 10 6 ; 10 9 ; in the worst case the computed underestimated the true by a factor .44.
Reference: [146] <author> N. J. Higham. </author> <title> Experience with a matrix norm estimator. </title> <journal> SIAM J. Sci. Stat. Comput., </journal> <volume> 11 </volume> <pages> 804-809, </pages> <year> 1990. </year>
Reference-contexts: It has been been conjectured that no such estimator exists, but this has not been proven. Linear Equation Solving 59 The algorithm was developed in <ref> [136, 144, 146] </ref>, with the latest version in [145]. <p> Choose ~x = e j sign (z j ), where j is chosen so that jz j j = kzk 1 . Then f (~x) f (x) + 5f (~x x) = f (x) + z T (~x x) where the last inequality is true by construction. 2 Higham <ref> [145, 146] </ref> tested a slightly improved version of this algorithm by trying many random matrices of sizes 10; 25; 50 and condition numbers = 10; 10 3 ; 10 6 ; 10 9 ; in the worst case the computed underestimated the true by a factor .44. <p> The reciprocal relationship between condition numbers and distance to the nearest ill-posed problem is further explored in [70]. An average case analysis of pivot growth is described in [240], and an example of bad pivot growth with complete pivoting is given in [120]. Condition estimators are described in <ref> [136, 144, 146] </ref>. Single precision iterative refinement is analyzed in [14, 223, 224]. A comprehensive discussion of error analysis for linear equation solvers, which covers most of these topics, can be found in [147]. For the symmetric indefinite factorization, see [43].
Reference: [147] <author> N. J. Higham. </author> <title> Accuracy and stability of numerical algorithms. </title> <publisher> SIAM, </publisher> <year> 1996. </year>
Reference-contexts: In other words, the componentwise relative backward error is as small as possible. For example, this means that if A and b are sparse then ffiA and ffib have the same sparsity structures as A and b, respectively. For a proof, see <ref> [147] </ref>, as well as [14, 223, 224, 225] for more details. Single precision iterative refinement and the error bound (2.14) are implemented in LAPACK routines like sgesvx. <p> Condition estimators are described in [136, 144, 146]. Single precision iterative refinement is analyzed in [14, 223, 224]. A comprehensive discussion of error analysis for linear equation solvers, which covers most of these topics, can be found in <ref> [147] </ref>. For the symmetric indefinite factorization, see [43]. Sparse matrix algorithms are described in [112, 91], as well as the numerous references in Table 2.2. <p> These and other routines are available electronically in NETLIB. An analysis of blocking strategies for matrix multiplication is given in [149]. Strassen's matrix multiplication algorithm is presented in [3], its performance in practice described in [22], and its numerical stability described in <ref> [76, 147] </ref>. A survey of parallel and other block algorithms is given in [75]. <p> MGS is Linear Least Squares Problems 119 more stable, and will be used in algorithms later in this book, but may still result in Q being far from orthogonal (kQQ T Ik being far larger than ") when A is ill-conditioned <ref> [31, 32, 33, 147] </ref>. Algorithm 3.2 in section 3.4.1 is a stable alternative algorithm for factoring A = QR. See Question 3.2. We will derive the formula for the x that minimizes kAx bk 2 using the decomposition A = QR in three slightly different ways. <p> An alternative form for the bound in Theorem 3.4, that eliminates the O (* 2 ) term, is as follows <ref> [256, 147] </ref> (here ~r is the perturbed residual ~r = (b + ffib) (A + ffiA)~x): k~x xk 2 1 * 2 (A) 2 + ( 2 (A) + 1) kAk 2 kxk 2 k~r rk 2 (1 + 2* 2 (A)) : We will see that, properly implemented, both the <p> See also chapter 5 of [119] and [166]. Perturbation theory and error bounds for the least squares solution is discussed in detail in <ref> [147] </ref>. Rank-revealing QRD decompositions are discussed in [28, 30, 47, 49, 124, 148, 194, 204, 234].
Reference: [148] <author> P. Hong and C. T. Pan. </author> <title> The rank revealing QR and SVD. </title> <journal> Math. Comp., </journal> <volume> 58 </volume> <pages> 575-232, </pages> <year> 1992. </year>
Reference-contexts: Linear Least Squares Problems 145 More sophisticated pivoting schemes than QR with column pivoting, called rank-revealing QR algorithms, have been a subject of much recent study. Rank-revealing QR algorithms which detect rank more reliably and sometimes also faster than QR with column pivoting have been developed <ref> [28, 30, 47, 49, 107, 124, 126, 148, 194, 234] </ref>. We discuss these further in the next section. QRD with column pivoting is available as subroutine sgeqpf in LA-PACK. LAPACK also has several similar factorizations available: RQ (sgerqf), LQ (sgelqf), and QL (sgeqlf). <p> See also chapter 5 of [119] and [166]. Perturbation theory and error bounds for the least squares solution is discussed in detail in [147]. Rank-revealing QRD decompositions are discussed in <ref> [28, 30, 47, 49, 124, 148, 194, 204, 234] </ref>.
Reference: [149] <author> X. Hong and H. T. Kung. </author> <title> I/O complexity: the red blue pebble game. </title> <booktitle> In Proceedings of the 13th Symposium on the Theory of Computing, </booktitle> <pages> pages 326-334. </pages> <publisher> ACM, </publisher> <year> 1981. </year>
Reference-contexts: These are both attractive properties. In fact, it can be shown that Algorithm 2.7 is asymptotically optimal <ref> [149] </ref>. In other words, no reorganization of matrix-matrix multiplication 78 Chapter 2 (that performs the same 2n 3 arithmetic operations) can have a q larger than O ( p On the other hand, this brief analysis ignores a number of practical issues: 1. <p> The Basic Linear Algebra Subroutines are described in [85, 87, 167]. These and other routines are available electronically in NETLIB. An analysis of blocking strategies for matrix multiplication is given in <ref> [149] </ref>. Strassen's matrix multiplication algorithm is presented in [3], its performance in practice described in [22], and its numerical stability described in [76, 147]. A survey of parallel and other block algorithms is given in [75].
Reference: [150] <author> A. K. Jain. </author> <title> Fundamentals of Digital Image Processing. </title> <publisher> Prentice-Hall, </publisher> <address> Englewood Cliffs, NJ, </address> <year> 1989. </year>
Reference-contexts: were produced by the following commands (the clown and other images are available in Matlab among the visualization demonstration files; check your local installation for location): load clown.mat; [U,S,V]=svd (X); colormap ('gray'); image (U (:,1:k)*S (1:k,1:k)*V (:,1:k)') There are also many other, cheaper, image compression techniques available than the SVD <ref> [187, 150] </ref>. 126 Chapter 3 , Top right: Rank k = 3 approximation. Bottom left: Rank k = 10 approximation. Bottom right: Rank k = 20 approximation.
Reference: [151] <author> E. Jessup and D. Sorensen. </author> <title> A divide and conquer algorithm for computing the singular value decomposition of a matrix. </title> <booktitle> In Proceedings of the Third SIAM Conference on Parallel Processing for Scientific Computing, </booktitle> <pages> pages 61-66, </pages> <address> Philadelphia, PA, </address> <year> 1989. </year> <note> SIAM. </note>
Reference-contexts: For an error analysis of the Bisection algorithm, see [72, 73, 154], and for recent attempts to accelerate Bisection see [103, 201, 199, 174, 171, 173, 267]. Current work in improving Inverse Iteration appears in [103, 199, 201]. The divide-and-conquer eigenroutine was introduced in [58] and further developed in <ref> [13, 88, 125, 129, 151, 170, 208, 232] </ref>. The possibility of high accuracy eigenvalues obtained from Ja-cobi is discussed in [65, 74, 81, 90, 181, 226].
Reference: [152] <author> Z. Jia. </author> <title> Some numerical methods for large unsymmetric eigenprob-lems. </title> <type> PhD thesis, </type> <institution> Universitat Bielefeld, Bielefeld, Germany, </institution> <year> 1994. </year>
Reference-contexts: Since A is nonsymmetric, its eigenvalues may be complex and/or badly conditioned, so many of the attractive error bounds and monotonic convergence properties enjoyed by Lanczos and described in section 7.3 no longer hold. Nonetheless, effective algorithms and implementations exist. Good references include <ref> [152, 169, 210, 214, 215, 231] </ref>, including the book [211]. The latest software is described in [169, 231], and may be found in NETLIB/scalapack/readme.arpack. The Matlab command speig (for "sparse eigenvalues") uses this software. A second alternative is to use the nonsymmetric Lanczos algorithm.
Reference: [153] <author> W-D. Webber J.P. Singh and A. Gupta. </author> <title> Splash: Stanford parallel applications for shared-memory. </title> <journal> Computer Architecture News, </journal> <volume> 20(1) </volume> <pages> 5-44, </pages> <year> 1992. </year>
Reference-contexts: SuperLU LL, partial, BLAS-2.5 Pub/UCB nonsym. PARASPAR [268, 269] RL, Markowitz, BLAS-1, SD Res/Author sym- MUPS [6] MF, threshold, BLAS-3 Res/Author pattern nonsym. George & Ng [113] RL, partial, BLAS-1 Res/Author s.p.d. Gupta, Rothberg, LL, BLAS-3 Com/SGI Ng & Peyton [131] Pub/Author s.p.d. SPLASH <ref> [153] </ref> RL, 2-D block, BLAS-3 Pub/Stanford Distributed Memory Algorithms sym. van der Stappen [243] RL, Markowitz, Scalar Res/Author sym- Lucas et al. [178] MF, no pivoting, BLAS-1 Res/Author pattern s.p.d. Rothberg et al. [205] RL, 2-D block, BLAS-3 Res/Author s.p.d. Gupta [130] MF, 2-D block, BLAS-3 Res/Author s.p.d.
Reference: [154] <author> W. Kahan. </author> <title> Accurate eigenvalues of a symmetric tridiagonal matrix. </title> <institution> Computer Science Dept. </institution> <type> Technical Report CS41, </type> <institution> Stanford University, Stanford, </institution> <address> CA, </address> <month> July </month> <year> 1966 </year> <month> (revised June </month> <year> 1968). </year>
Reference-contexts: In fact, since it is tridiagonal it can be shown to be very stable <ref> [72, 73, 154] </ref>: 254 Chapter 5 Lemma 5.3 The d i computed in floating point arithmetic, using equation (5.17), have the same signs (and so compute the same Inertia) as the ^ d i computed exactly from ^ A, where ^ A is very close to A: ( ^ A) ii <p> For a survey of parallel algorithms for the symmetric eigenproblem, see [75]. The QR algorithm for finding the SVD of bidiagonal matrices is discussed in [79, 66, 118]. and the dqds algorithm is in [102, 198, 207]. For an error analysis of the Bisection algorithm, see <ref> [72, 73, 154] </ref>, and for recent attempts to accelerate Bisection see [103, 201, 199, 174, 171, 173, 267]. Current work in improving Inverse Iteration appears in [103, 199, 201]. The divide-and-conquer eigenroutine was introduced in [58] and further developed in [13, 88, 125, 129, 151, 170, 208, 232].
Reference: [155] <author> W. Kahan. </author> <title> A survey of error analysis. </title> <booktitle> In Information Processing 71, </booktitle> <pages> pages 1214-1239, </pages> <address> Amsterdam, 1972. </address> <publisher> North Holland. </publisher>
Reference: [156] <author> W. Kahan. </author> <title> The baleful effect of computer benchmarks upon applied mathematics, </title> <journal> physics and chemistry. </journal> <note> http://HTTP.CS.Berkeley.EDU/~wkahan/ieee754status/baleful.ps, 1995. </note>
Reference: [157] <author> W. Kahan. </author> <title> Lecture notes on the status of ieee standard 754 for binary floating point arithmetic. </title> <note> http://HTTP.CS.Berkeley.EDU/~wkahan/ieee754status/ieee754.ps, 1995. 442 Bibliography </note>
Reference: [158] <author> T. Kailath and A. H. Sayed. </author> <title> Displacement structure: </title> <journal> Theory and applications. SIAM Review, </journal> <year> 1995. </year>
Reference-contexts: They arise in problems of signal processing. There are algorithms for solving such systems that take only O (n 2 ) operations. All these methods generalize to many other similar matrices depend ing on only O (n) parameters. See [119, p. 183.], or <ref> [158] </ref> for a recent survey. 2.8 References and Other Topics for Chap ter 2 Further details about linear equation solving in general may be found in chapters 3 and 4 of [119]. The reciprocal relationship between condition numbers and distance to the nearest ill-posed problem is further explored in [70]. <p> A survey of parallel and other block algorithms is given in [75]. For a recent survey of algorithms for structured dense matrices depending only on O (n) parameters, see <ref> [158] </ref>. 2.9 Questions for Chapter 2 Question 2.1 (Easy) Using your favorite World Wide Web browser, go to NETLIB (http://www.netlib.org), and answer the following questions. 1. You need a Fortran subroutine to compute the eigenvalues and eigenvectors of real symmetric matrices in double precision.
Reference: [159] <author> T. Kato. </author> <title> Perturbation Theory for Linear Operators. </title> <publisher> Springer Ver-lag, </publisher> <address> Berlin, 2 edition, </address> <year> 1980. </year>
Reference-contexts: For a proof, see [68]. For an overview of condition numbers for the eigenproblem, including eigenvectors, invariant subspaces, and the eigenvalues corresponding to an invariant subspace, see chapter 4 of the LAPACK manual [10], as well as <ref> [159, 235] </ref>. <p> For more details about perturbation theory of eigenvalues and eigenvectors, see <ref> [159, 235, 51] </ref>, and chapter 4 of [10]. For a proof of Theorem 4.7, see [68]. For a discussion of Weierstrass and Kronecker canon ical forms, see [108, 116]. For their application to systems and control theory, see [244, 245, 77]. <p> Since the eigenvalues differ so much in magnitude, their relative gaps are all quite large, and so their eigenvectors can rotate only by about 3% in angle too. For a different approach to relative error analysis, more suitable for matrices arising from differential ("unbounded") operators, see <ref> [159] </ref>. 5.3 Algorithms for the Symmetric Eigen problem We discuss a variety of algorithms for the symmetric eigenproblem. As mentioned in the introduction, we will discuss only direct methods, leaving iterative methods for Chapter 7. <p> The material on relative perturbation theory can be found in [74, 81, 99]; section 5.2.1 was based on the latter of these references. Related work is found in [65, 90, 226, 248] A classical text on perturbation theory for general linear operators is <ref> [159] </ref>. For a survey of parallel algorithms for the symmetric eigenproblem, see [75]. The QR algorithm for finding the SVD of bidiagonal matrices is discussed in [79, 66, 118]. and the dqds algorithm is in [102, 198, 207].
Reference: [160] <author> R. B. Kearfott. </author> <title> Rigorous Global Search: Continuous Problems. </title> <publisher> Kluwer, </publisher> <address> Dordrecht, Netherlands, </address> <year> 1996. </year> <note> see also http://interval.usl.edu/euromath.html. </note>
Reference: [161] <author> W. Kerner. </author> <title> Large-scale complex eigenvalue problems. </title> <journal> J. of Comp. Phy., </journal> <volume> 85 </volume> <pages> 1-85, </pages> <year> 1989. </year>
Reference-contexts: (Algorithm 4.3) [19], Davidson's algorithm [214] or the Jacobi-Davidson algorithm [228] to the sparse nonsymmetric eigenproblem. 7.8 References and Other Topics for Chap ter 7 In addition to the references in sections 7.6 and 7.7, there are a number of good surveys available on algorithms for sparse eigenvalues problems: see <ref> [17, 50, 123, 161, 195, 211, 260] </ref>. Parallel implementations are also discussed in [75]. In section 6.2 we discussed the existence of on-line help to choose among the variety of iterative methods available for solving Ax = b.
Reference: [162] <author> G. Kolata. </author> <title> Geodesy: dealing with an enormous computer task. </title> <journal> Science, </journal> <volume> 200 </volume> <pages> 421-422, </pages> <year> 1978. </year>
Reference-contexts: The corresponding least squares problem was the largest ever solved at the time: about 2.5 million equations in 400 thousand unknowns. It was also very sparse, which made it tractable on the computers available in 1978, when the computation was done <ref> [162] </ref>. Now we briefly discuss the formulation of this problem. It is actually nonlinear, and solved by approximating it by a sequence of linear ones, each of which is a linear least squares problem.
Reference: [163] <author> S. Krishnan, A. Narkhede, and D. Manocha. </author> <title> BOOLE: A system to compute Boolean combinations of sculptured solids. </title> <institution> Computer Science Dept. </institution> <type> Technical Report TR95-008, </type> <institution> University of North Carolina, Chapel Hill, </institution> <year> 1995. </year> <note> http://www.cs.unc.edu/~geom/geom.html. </note>
Reference-contexts: For a proof of Theorem 4.7, see [68]. For a discussion of Weierstrass and Kronecker canon ical forms, see [108, 116]. For their application to systems and control theory, see [244, 245, 77]. For applications to computational geometry, graphics, and mechanical CAD, see <ref> [179, 180, 163] </ref>. For a discussion of parallel algorithms for the nonsymmetric eigenproblem, see [75]. 4.8 Questions for Chapter 4 Question 4.1 (Easy) Let A be defined as in equation (4.1). <p> What do you get asymptotically? hold off e=diag (a); for i=1:m, [q,r]=qr (a);dd=diag (sign (diag (r)));r=dd*r;q=q*dd;a=r*q; ... e=[e,diag (a)]; end plot (e','w'),grid Question 4.16 (Hard; Programming) This problem describes an application of the nonlinear eigenproblem to computer graphics, computational geometry and mechanical CAD; see also <ref> [179, 180, 163] </ref>. Let F = [f ij (x 1 ; x 2 ; x 3 )] be a matrix whose entries are polynomials in the three variables x i . Then det (F ) = 0 will (generally) define a two-dimensional surface S in 3-space.
Reference: [164] <author> M. Kruskal. </author> <title> Dynamical Systems, Theory and Applications, volume 38 of Lecture Notes in Physics, chapter Nonlinear Wave Equations. </title> <publisher> Springer-Verlag, </publisher> <year> 1975. </year>
Reference-contexts: In other words, there is an infinite sequence of energy-like quantities conserved by KdV. This is important both for theoretical and numerical reasons. For more details on the Toda flow, see [142, 168, 66, 67, 237], and papers by Kruskal <ref> [164] </ref>, Flaschka [104] and Moser [185] in [186]. 5.6 References and Other Topics for Chap ter 5 An excellent general reference for the symmetric eigenproblem is [195]. The material on relative perturbation theory can be found in [74, 81, 99]; section 5.2.1 was based on the latter of these references. <p> The divide-and-conquer eigenroutine was introduced in [58] and further developed in [13, 88, 125, 129, 151, 170, 208, 232]. The possibility of high accuracy eigenvalues obtained from Ja-cobi is discussed in [65, 74, 81, 90, 181, 226]. The Toda flow and related phenomena are discussed in <ref> [66, 67, 104, 142, 164, 168, 185, 186, 237] </ref>. 5.7 Questions for Chapter 5 Question 5.1 (Easy; Z. Bai) Show that A = B + iC is Hermitian if and only if M = B C is symmetric.
Reference: [165] <author> K. Kundert. </author> <title> Sparse matrix techniques. In Albert Ruehli, editor, Circuit Analysis, Simulation and Design. </title> <publisher> North-Holland, </publisher> <year> 1986. </year>
Reference-contexts: SuperLU LL, partial, BLAS-2.5 Pub/UCB nonsym. UMFPACK [61, 62] MF, Markowitz, BLAS-3 Pub/NETLIB MA38 (same as UMFPACK) Com/HSL nonsym. MA48 [94] Anal: RL, Markowitz Com/HSL Fact: LL, partial, BLAS-1, SD nonsym. SPARSE <ref> [165] </ref> RL, Markowitz, Scalar Pub/NETLIB sym pattern MUPS [5] MA42 [96] MF, threshold, BLAS-3 Frontal, BLAS-3 Com/HSL sym. MA27 [95]/MA47 [93] MF, LDL T , BLAS-1 Com/HSL s.p.d. Ng & Peyton [189] LL, BLAS-3 Pub/Author Shared Memory Algorithms nonsym. SuperLU LL, partial, BLAS-2.5 Pub/UCB nonsym.
Reference: [166] <author> C. Lawson and R Hanson. </author> <title> Solving least squares problems. </title> <publisher> Prentice-Hall, </publisher> <year> 1974. </year>
Reference-contexts: A is m-by-n, x is n-by-1, and b and r are m-by-1. A good choice of basis functions f i (y) can lead to better fits and less ill-conditioned systems than using polynomials <ref> [33, 82, 166] </ref>. Example 3.2 In statistical modeling, one often wishes to estimate certain parameters x j based on some observations, where the observations are contaminated by noise. <p> See also chapter 5 of [119] and <ref> [166] </ref>. Perturbation theory and error bounds for the least squares solution is discussed in detail in [147]. Rank-revealing QRD decompositions are discussed in [28, 30, 47, 49, 124, 148, 194, 204, 234].
Reference: [167] <author> C. Lawson, R. Hanson, D. Kincaid, and F. Krogh. </author> <title> Basic Linear Algebra Subprograms for Fortran usage. </title> <journal> ACM Trans. Math. Soft., </journal> <volume> 5 </volume> <pages> 308-323, </pages> <year> 1979. </year>
Reference-contexts: Since operations like matrix-matrix multiplication are so common, computer manufacturers have standardized them as the Basic Linear Algebra Subroutines or BLAS <ref> [167, 87, 85] </ref>, and optimized them for their machines. In other words, a library of subroutines for matrix-matrix multiplication, matrix-vector multiplication, and other similar operations is available with a standard Fortran or C interface on high performance machines (and many others), but underneath they have been optimized for each machine. <p> This means that algorithms with the larger q values are better building blocks for other algorithms. Table 2.1 reflects a hierarchy of operations: Operations like saxpy perform O (n 1 ) flops on vectors, and offer the worst q values; these are called Level 1 BLAS or BLAS1 <ref> [167] </ref>, and include inner products, multiplying a scalar times a vector, and other simple operations. <p> Implementations of many of the algorithms for dense and band matrices described in this chapter are available in LAPACK and Linear Equation Solving 103 CLAPACK [10], which includes a discussion of block algorithms suitable for high performance computers. The Basic Linear Algebra Subroutines are described in <ref> [85, 87, 167] </ref>. These and other routines are available electronically in NETLIB. An analysis of blocking strategies for matrix multiplication is given in [149]. Strassen's matrix multiplication algorithm is presented in [3], its performance in practice described in [22], and its numerical stability described in [76, 147].
Reference: [168] <author> P. Lax. </author> <title> Integrals of nonlinear equations of evolution and solitary waves. </title> <journal> Comm. Pure Appl. Math., </journal> <volume> 21 </volume> <pages> 467-490, </pages> <year> 1968. </year>
Reference-contexts: In other words, there is an infinite sequence of energy-like quantities conserved by KdV. This is important both for theoretical and numerical reasons. For more details on the Toda flow, see <ref> [142, 168, 66, 67, 237] </ref>, and papers by Kruskal [164], Flaschka [104] and Moser [185] in [186]. 5.6 References and Other Topics for Chap ter 5 An excellent general reference for the symmetric eigenproblem is [195]. <p> The divide-and-conquer eigenroutine was introduced in [58] and further developed in [13, 88, 125, 129, 151, 170, 208, 232]. The possibility of high accuracy eigenvalues obtained from Ja-cobi is discussed in [65, 74, 81, 90, 181, 226]. The Toda flow and related phenomena are discussed in <ref> [66, 67, 104, 142, 164, 168, 185, 186, 237] </ref>. 5.7 Questions for Chapter 5 Question 5.1 (Easy; Z. Bai) Show that A = B + iC is Hermitian if and only if M = B C is symmetric.
Reference: [169] <author> R. Lehoucq. </author> <title> Analysis and Implementation of an Implicitly Restarted Arnoldi Iteration. </title> <type> PhD thesis, </type> <institution> Rice University, Houston, TX, </institution> <year> 1995. </year>
Reference-contexts: Full reorthogonalization corresponds to applying the Gram-Schmidt orthogonalization process "z = z P j1 i=1 (z T q i )q i " twice, in order to almost surely make z orthogonal to q 1 through q j1 (see Algorithm 3.1 on page 118, as well as [195, sec. 6-9] and <ref> [169, chap. 7] </ref> for discussions of when "twice is enough"). In exact arithmetic, we showed in section 6.6.1 that z is orthogonal to q 1 through q j1 without reorthogonalization. Unfortunately, we will see that roundoff destroys this orthogonality property, upon which all of our analysis has depended so far. <p> Another enhancement is to use the error bounds to efficiently distinguish between converged and "misconverged" eigenvalues [196]. A current state-of-the-art implementation of Lanczos is described in [123]. A different software implementation is available in ARPACK (NETLIB/scalapack/readme.arpack, <ref> [169, 231] </ref>). If we apply Lanczos to the shifted and inverted matrix (AI) 1 , then we expect the eigenvalues closest to to converge first. There are other methods to "precondition" a matrix A to converge to certain eigenvalues more quickly. <p> Since A is nonsymmetric, its eigenvalues may be complex and/or badly conditioned, so many of the attractive error bounds and monotonic convergence properties enjoyed by Lanczos and described in section 7.3 no longer hold. Nonetheless, effective algorithms and implementations exist. Good references include <ref> [152, 169, 210, 214, 215, 231] </ref>, including the book [211]. The latest software is described in [169, 231], and may be found in NETLIB/scalapack/readme.arpack. The Matlab command speig (for "sparse eigenvalues") uses this software. A second alternative is to use the nonsymmetric Lanczos algorithm. <p> Nonetheless, effective algorithms and implementations exist. Good references include [152, 169, 210, 214, 215, 231], including the book [211]. The latest software is described in <ref> [169, 231] </ref>, and may be found in NETLIB/scalapack/readme.arpack. The Matlab command speig (for "sparse eigenvalues") uses this software. A second alternative is to use the nonsymmetric Lanczos algorithm. This algorithm attempts to reduce A to nonsymmetric tridiagonal form by a nonorthogonal similarity.
Reference: [170] <author> R.-C. Li. </author> <title> Solving secular equations stably and efficiently. </title> <institution> Computer Science Dept. </institution> <type> Technical Report CS-94-260, </type> <institution> University of Tennessee, Knoxville, </institution> <month> November </month> <year> 1994. </year> <note> (LAPACK Working Note #89). Bibliography 443 </note>
Reference-contexts: i.e. h () = d i c 2 + c 3 : There are several ways to choose the constants c 1 , c 2 and c 3 so that h () approximates f (); we present a slightly simplified version of the one used in the LAPACK routine slaed4 <ref> [170, 44] </ref>. <p> Continuing, 2 is accurate to 11 digits, and 3 is accurate to all 16 digits. The algorithm used in LAPACK routine slaed4 is a slight variation on the one described here (the one here is called the Middle Way in <ref> [170] </ref>). The LAPACK routine averages 2-3 iterations per eigenvalue to converge to full machine precision, and never took more than 7 steps in extensive numerical tests. <p> For an error analysis of the Bisection algorithm, see [72, 73, 154], and for recent attempts to accelerate Bisection see [103, 201, 199, 174, 171, 173, 267]. Current work in improving Inverse Iteration appears in [103, 199, 201]. The divide-and-conquer eigenroutine was introduced in [58] and further developed in <ref> [13, 88, 125, 129, 151, 170, 208, 232] </ref>. The possibility of high accuracy eigenvalues obtained from Ja-cobi is discussed in [65, 74, 81, 90, 181, 226].
Reference: [171] <author> T.-Y. Li and Z. Zeng. </author> <title> Homotopy-determinant algorithm for solving nonsymmetric eigenvalue problems. </title> <journal> Math. Comp., </journal> <volume> 59(200) </volume> <pages> 483-502, </pages> <month> October </month> <year> 1992. </year>
Reference-contexts: Bisection and Inverse Iteration are available as options in the LAPACK routine ssyevx . There is current research on Inverse Iteration, addressing the problem of close eigenvalues, which may make it the fastest method to find all the eigenvectors eigenvectors (besides, theoretically, divide-and-conquer with the FMM) <ref> [103, 201, 199, 174, 171, 173, 267] </ref>. But software implementing this improved version of Inverse Iteration is not yet available. 5. Jacobi's method. This method is historically the oldest method for the eigenproblem, dating to 1846. <p> There are many ways to accelerate convergence, using algorithms like Newton's method and its relatives, to find zeros of the characteristic polynomial (which may be computed by multiplying all the d i 's together) <ref> [171, 172, 173, 174, 176, 267] </ref>. The Symmetric Eigenproblem and SVD 255 To compute eigenvectors once we have computed (selected) eigenval-ues, we can use Inverse Iteration (Algorithm 4.2); this is available in LA-PACK routine sstein. Since we can use accurate eigenvalues as shifts, convergence usually takes one or two iterations. <p> The QR algorithm for finding the SVD of bidiagonal matrices is discussed in [79, 66, 118]. and the dqds algorithm is in [102, 198, 207]. For an error analysis of the Bisection algorithm, see [72, 73, 154], and for recent attempts to accelerate Bisection see <ref> [103, 201, 199, 174, 171, 173, 267] </ref>. Current work in improving Inverse Iteration appears in [103, 199, 201]. The divide-and-conquer eigenroutine was introduced in [58] and further developed in [13, 88, 125, 129, 151, 170, 208, 232].
Reference: [172] <author> T.-Y. Li and Z. Zeng. </author> <title> Laguerre's iteration in solving the symmetric tridiagonal eigenproblem a revisit. </title> <institution> Michigan State University preprint, </institution> <year> 1992. </year>
Reference-contexts: There are many ways to accelerate convergence, using algorithms like Newton's method and its relatives, to find zeros of the characteristic polynomial (which may be computed by multiplying all the d i 's together) <ref> [171, 172, 173, 174, 176, 267] </ref>. The Symmetric Eigenproblem and SVD 255 To compute eigenvectors once we have computed (selected) eigenval-ues, we can use Inverse Iteration (Algorithm 4.2); this is available in LA-PACK routine sstein. Since we can use accurate eigenvalues as shifts, convergence usually takes one or two iterations.
Reference: [173] <author> T.-Y. Li, Z. Zeng, and L. Cong. </author> <title> Solving eigenvalue problems of nonsymmetric matrices with real homotopies. </title> <journal> SIAM J. Num. Anal., </journal> <volume> 29(1) </volume> <pages> 229-248, </pages> <year> 1992. </year>
Reference-contexts: Bisection and Inverse Iteration are available as options in the LAPACK routine ssyevx . There is current research on Inverse Iteration, addressing the problem of close eigenvalues, which may make it the fastest method to find all the eigenvectors eigenvectors (besides, theoretically, divide-and-conquer with the FMM) <ref> [103, 201, 199, 174, 171, 173, 267] </ref>. But software implementing this improved version of Inverse Iteration is not yet available. 5. Jacobi's method. This method is historically the oldest method for the eigenproblem, dating to 1846. <p> There are many ways to accelerate convergence, using algorithms like Newton's method and its relatives, to find zeros of the characteristic polynomial (which may be computed by multiplying all the d i 's together) <ref> [171, 172, 173, 174, 176, 267] </ref>. The Symmetric Eigenproblem and SVD 255 To compute eigenvectors once we have computed (selected) eigenval-ues, we can use Inverse Iteration (Algorithm 4.2); this is available in LA-PACK routine sstein. Since we can use accurate eigenvalues as shifts, convergence usually takes one or two iterations. <p> The QR algorithm for finding the SVD of bidiagonal matrices is discussed in [79, 66, 118]. and the dqds algorithm is in [102, 198, 207]. For an error analysis of the Bisection algorithm, see [72, 73, 154], and for recent attempts to accelerate Bisection see <ref> [103, 201, 199, 174, 171, 173, 267] </ref>. Current work in improving Inverse Iteration appears in [103, 199, 201]. The divide-and-conquer eigenroutine was introduced in [58] and further developed in [13, 88, 125, 129, 151, 170, 208, 232].
Reference: [174] <author> T.-Y. Li, H. Zhang, and X.-H. Sun. </author> <title> Parallel homotopy algorithm for symmetric tridiagonal eigenvalue problem. </title> <journal> SIAM J. Sci. Stat. Comput., </journal> <volume> 12(3) </volume> <pages> 469-487, </pages> <year> 1991. </year>
Reference-contexts: Bisection and Inverse Iteration are available as options in the LAPACK routine ssyevx . There is current research on Inverse Iteration, addressing the problem of close eigenvalues, which may make it the fastest method to find all the eigenvectors eigenvectors (besides, theoretically, divide-and-conquer with the FMM) <ref> [103, 201, 199, 174, 171, 173, 267] </ref>. But software implementing this improved version of Inverse Iteration is not yet available. 5. Jacobi's method. This method is historically the oldest method for the eigenproblem, dating to 1846. <p> There are many ways to accelerate convergence, using algorithms like Newton's method and its relatives, to find zeros of the characteristic polynomial (which may be computed by multiplying all the d i 's together) <ref> [171, 172, 173, 174, 176, 267] </ref>. The Symmetric Eigenproblem and SVD 255 To compute eigenvectors once we have computed (selected) eigenval-ues, we can use Inverse Iteration (Algorithm 4.2); this is available in LA-PACK routine sstein. Since we can use accurate eigenvalues as shifts, convergence usually takes one or two iterations. <p> The QR algorithm for finding the SVD of bidiagonal matrices is discussed in [79, 66, 118]. and the dqds algorithm is in [102, 198, 207]. For an error analysis of the Bisection algorithm, see [72, 73, 154], and for recent attempts to accelerate Bisection see <ref> [103, 201, 199, 174, 171, 173, 267] </ref>. Current work in improving Inverse Iteration appears in [103, 199, 201]. The divide-and-conquer eigenroutine was introduced in [58] and further developed in [13, 88, 125, 129, 151, 170, 208, 232].
Reference: [175] <author> X. Li. </author> <title> Sparse Gaussian Elimination on High Performance Computers. </title> <type> PhD thesis, </type> <institution> Computer Science Division, Department of Electrical Engineering and Computer Science, University of California, Berkeley, </institution> <month> September </month> <year> 1996. </year>
Reference-contexts: Sparse Matrix Software Besides Matlab, there is a variety of public domain and commercial sparse matrix software available in Fortran or C. Since this is still an active research area (especially with regard to high performance machines), it is impossible to recommend a single best algorithm. Table 2.2 <ref> [175] </ref> gives a list of available software, categorized in several ways. We restrict ourselves to supported codes (either public or commercial), or else research codes when no other software is available for that type of problem or machine. <p> Table 2.2 [175] gives a list of available software, categorized in several ways. We restrict ourselves to supported codes (either public or commercial), or else research codes when no other software is available for that type of problem or machine. We refer to <ref> [175, 92] </ref> for more complete lists and explanations of the algorithms below. Table 2.2 is organized as follows. The top group of routines, labeled Serial Algorithms, are designed for single processor workstations and PCs.
Reference: [176] <author> S.-S. Lo, B. Phillipe, and A. Sameh. </author> <title> A multiprocessor algorithm for the symmetric eigenproblem. </title> <journal> SIAM J. Sci. Stat. Comput., </journal> <volume> 8(2) </volume> <pages> 155-165, </pages> <month> March </month> <year> 1987. </year>
Reference-contexts: There are many ways to accelerate convergence, using algorithms like Newton's method and its relatives, to find zeros of the characteristic polynomial (which may be computed by multiplying all the d i 's together) <ref> [171, 172, 173, 174, 176, 267] </ref>. The Symmetric Eigenproblem and SVD 255 To compute eigenvectors once we have computed (selected) eigenval-ues, we can use Inverse Iteration (Algorithm 4.2); this is available in LA-PACK routine sstein. Since we can use accurate eigenvalues as shifts, convergence usually takes one or two iterations.
Reference: [177] <author> K. Lowner. </author> <title> Uber monotone matrixfunctionen. </title> <journal> Math. Z., </journal> <volume> 38 </volume> <pages> 177-216, </pages> <year> 1934. </year>
Reference-contexts: Unfortunately, the Cray 2, YMP, and C90 do not round accurately enough to use these efficient algorithms. Finally, an alternative formula was found that makes simulating high precision arithmetic unnecessary. It is based on the following theorem of Lowner <ref> [127, 177] </ref>: Theorem 5.10 (Lowner) Let D = diag (d 1 ; :::; d n ) be diagonal with d n &lt; &lt; d 1 .
Reference: [178] <author> R. Lucas, W. Blank, and J. Tieman. </author> <title> A parallel solution method for large sparse systems of equations. </title> <journal> IEEE Trans. Computer Aided Design, </journal> <volume> CAD-6:981-991, </volume> <year> 1987. </year>
Reference-contexts: George & Ng [113] RL, partial, BLAS-1 Res/Author s.p.d. Gupta, Rothberg, LL, BLAS-3 Com/SGI Ng & Peyton [131] Pub/Author s.p.d. SPLASH [153] RL, 2-D block, BLAS-3 Pub/Stanford Distributed Memory Algorithms sym. van der Stappen [243] RL, Markowitz, Scalar Res/Author sym- Lucas et al. <ref> [178] </ref> MF, no pivoting, BLAS-1 Res/Author pattern s.p.d. Rothberg et al. [205] RL, 2-D block, BLAS-3 Res/Author s.p.d. Gupta [130] MF, 2-D block, BLAS-3 Res/Author s.p.d. CAPSS [141] MF, full parallel, BLAS-1 Pub/NETLIB (require coordinates) Table 2.2: Software to solve sparse linear systems using direct methods.
Reference: [179] <author> D. Manocha and J. Demmel. </author> <title> Algorithms for intersecting parametric and algebraic curves i: simple intersections. </title> <journal> ACM Transactions on Graphics, </journal> <volume> 13(1) </volume> <pages> 73-100, </pages> <year> 1994. </year>
Reference-contexts: For a proof of Theorem 4.7, see [68]. For a discussion of Weierstrass and Kronecker canon ical forms, see [108, 116]. For their application to systems and control theory, see [244, 245, 77]. For applications to computational geometry, graphics, and mechanical CAD, see <ref> [179, 180, 163] </ref>. For a discussion of parallel algorithms for the nonsymmetric eigenproblem, see [75]. 4.8 Questions for Chapter 4 Question 4.1 (Easy) Let A be defined as in equation (4.1). <p> What do you get asymptotically? hold off e=diag (a); for i=1:m, [q,r]=qr (a);dd=diag (sign (diag (r)));r=dd*r;q=q*dd;a=r*q; ... e=[e,diag (a)]; end plot (e','w'),grid Question 4.16 (Hard; Programming) This problem describes an application of the nonlinear eigenproblem to computer graphics, computational geometry and mechanical CAD; see also <ref> [179, 180, 163] </ref>. Let F = [f ij (x 1 ; x 2 ; x 3 )] be a matrix whose entries are polynomials in the three variables x i . Then det (F ) = 0 will (generally) define a two-dimensional surface S in 3-space.
Reference: [180] <author> D. Manocha and J. Demmel. </author> <title> Algorithms for intersecting parametric and algebraic curves ii: Higher order intersections. Computer Vision, Graphics and Image Processing: Graphical Models and Image Processing, </title> <booktitle> 57(2) </booktitle> <pages> 80-100, </pages> <year> 1995. </year>
Reference-contexts: For a proof of Theorem 4.7, see [68]. For a discussion of Weierstrass and Kronecker canon ical forms, see [108, 116]. For their application to systems and control theory, see [244, 245, 77]. For applications to computational geometry, graphics, and mechanical CAD, see <ref> [179, 180, 163] </ref>. For a discussion of parallel algorithms for the nonsymmetric eigenproblem, see [75]. 4.8 Questions for Chapter 4 Question 4.1 (Easy) Let A be defined as in equation (4.1). <p> What do you get asymptotically? hold off e=diag (a); for i=1:m, [q,r]=qr (a);dd=diag (sign (diag (r)));r=dd*r;q=q*dd;a=r*q; ... e=[e,diag (a)]; end plot (e','w'),grid Question 4.16 (Hard; Programming) This problem describes an application of the nonlinear eigenproblem to computer graphics, computational geometry and mechanical CAD; see also <ref> [179, 180, 163] </ref>. Let F = [f ij (x 1 ; x 2 ; x 3 )] be a matrix whose entries are polynomials in the three variables x i . Then det (F ) = 0 will (generally) define a two-dimensional surface S in 3-space.
Reference: [181] <author> R. Mathias. </author> <title> Accurate eigensystem computations by jacobi methods. </title> <journal> SIAM J. Mat. Anal. Appl., </journal> <volume> 16(3) </volume> <pages> 977-1003, </pages> <year> 1996. </year> <note> 444 Bibliography </note>
Reference-contexts: So if X is well-conditioned, all the eigenvalues of A will be computed to high relative accuracy (see Question 5.23 and <ref> [81, 90, 181] </ref>). <p> Current work in improving Inverse Iteration appears in [103, 199, 201]. The divide-and-conquer eigenroutine was introduced in [58] and further developed in [13, 88, 125, 129, 151, 170, 208, 232]. The possibility of high accuracy eigenvalues obtained from Ja-cobi is discussed in <ref> [65, 74, 81, 90, 181, 226] </ref>. The Toda flow and related phenomena are discussed in [66, 67, 104, 142, 164, 168, 185, 186, 237]. 5.7 Questions for Chapter 5 Question 5.1 (Easy; Z.
Reference: [182] <author> The MathWorks, Inc., Natick, MA. </author> <title> MATLAB Reference Guide, </title> <year> 1992. </year>
Reference: [183] <editor> S. McMormick, editor. </editor> <title> Multigrid Methods, </title> <booktitle> volume 3 of SIAM Frontiers in Applied Mathematics. </booktitle> <publisher> SIAM, </publisher> <year> 1987. </year>
Reference-contexts: Classical methods like Jacobi, Gauss-Seidel and SOR are discussed in detail in [247, 135]. Multigrid methods are discussed in <ref> [42, 183, 184, 258, 266] </ref> and the references therein; [89] is a web site with pointers to an extensive bibliography, software, and so on. Domain decomposition are discussed in [48, 114, 203, 230]. Chebyshev and other polynomials are discussed in [238].
Reference: [184] <author> S. </author> <title> McMormick. Multilevel adaptive methods for partial differential equations, </title> <booktitle> volume 6 of SIAM Frontiers in Applied Mathematics. </booktitle> <publisher> SIAM, </publisher> <year> 1989. </year>
Reference-contexts: Classical methods like Jacobi, Gauss-Seidel and SOR are discussed in detail in [247, 135]. Multigrid methods are discussed in <ref> [42, 183, 184, 258, 266] </ref> and the references therein; [89] is a web site with pointers to an extensive bibliography, software, and so on. Domain decomposition are discussed in [48, 114, 203, 230]. Chebyshev and other polynomials are discussed in [238].
Reference: [185] <author> J. Moser. </author> <title> Dynamical Systems, Theory and Applications, volume 38 of Lecture Notes in Physics, chapter Finitely many mass points on the line under the influence of an exponential potential an integrable system. </title> <publisher> Springer-Verlag, </publisher> <year> 1975. </year>
Reference-contexts: In other words, there is an infinite sequence of energy-like quantities conserved by KdV. This is important both for theoretical and numerical reasons. For more details on the Toda flow, see [142, 168, 66, 67, 237], and papers by Kruskal [164], Flaschka [104] and Moser <ref> [185] </ref> in [186]. 5.6 References and Other Topics for Chap ter 5 An excellent general reference for the symmetric eigenproblem is [195]. The material on relative perturbation theory can be found in [74, 81, 99]; section 5.2.1 was based on the latter of these references. <p> The divide-and-conquer eigenroutine was introduced in [58] and further developed in [13, 88, 125, 129, 151, 170, 208, 232]. The possibility of high accuracy eigenvalues obtained from Ja-cobi is discussed in [65, 74, 81, 90, 181, 226]. The Toda flow and related phenomena are discussed in <ref> [66, 67, 104, 142, 164, 168, 185, 186, 237] </ref>. 5.7 Questions for Chapter 5 Question 5.1 (Easy; Z. Bai) Show that A = B + iC is Hermitian if and only if M = B C is symmetric.
Reference: [186] <editor> J. Moser, editor. </editor> <booktitle> Dynamical Systems, Theory and Applications, volume 38 of Lecture Notes in Physics. </booktitle> <publisher> Springer-Verlag, </publisher> <year> 1975. </year>
Reference-contexts: In other words, there is an infinite sequence of energy-like quantities conserved by KdV. This is important both for theoretical and numerical reasons. For more details on the Toda flow, see [142, 168, 66, 67, 237], and papers by Kruskal [164], Flaschka [104] and Moser [185] in <ref> [186] </ref>. 5.6 References and Other Topics for Chap ter 5 An excellent general reference for the symmetric eigenproblem is [195]. The material on relative perturbation theory can be found in [74, 81, 99]; section 5.2.1 was based on the latter of these references. <p> The divide-and-conquer eigenroutine was introduced in [58] and further developed in [13, 88, 125, 129, 151, 170, 208, 232]. The possibility of high accuracy eigenvalues obtained from Ja-cobi is discussed in [65, 74, 81, 90, 181, 226]. The Toda flow and related phenomena are discussed in <ref> [66, 67, 104, 142, 164, 168, 185, 186, 237] </ref>. 5.7 Questions for Chapter 5 Question 5.1 (Easy; Z. Bai) Show that A = B + iC is Hermitian if and only if M = B C is symmetric.
Reference: [187] <author> A. Netravali and B. </author> <title> Haskell. Digital Pictures. </title> <publisher> Plenum Press, </publisher> <address> New York, </address> <year> 1988. </year>
Reference-contexts: were produced by the following commands (the clown and other images are available in Matlab among the visualization demonstration files; check your local installation for location): load clown.mat; [U,S,V]=svd (X); colormap ('gray'); image (U (:,1:k)*S (1:k,1:k)*V (:,1:k)') There are also many other, cheaper, image compression techniques available than the SVD <ref> [187, 150] </ref>. 126 Chapter 3 , Top right: Rank k = 3 approximation. Bottom left: Rank k = 10 approximation. Bottom right: Rank k = 20 approximation.
Reference: [188] <author> A. Neumaier. </author> <title> Interval Methods for Systems of Equations. </title> <publisher> Cam-bridge University Press, </publisher> <address> Cambridge, England, </address> <year> 1990. </year>
Reference: [189] <author> E. G. Ng and B. W. Peyton. </author> <title> Block sparse Cholesky algorithms on advanced uniprocessor computers. </title> <journal> SIAM J. Sci. Stat. Comp., </journal> <volume> 14 </volume> <pages> 1034-1056, </pages> <year> 1993. </year>
Reference-contexts: MA48 [94] Anal: RL, Markowitz Com/HSL Fact: LL, partial, BLAS-1, SD nonsym. SPARSE [165] RL, Markowitz, Scalar Pub/NETLIB sym pattern MUPS [5] MA42 [96] MF, threshold, BLAS-3 Frontal, BLAS-3 Com/HSL sym. MA27 [95]/MA47 [93] MF, LDL T , BLAS-1 Com/HSL s.p.d. Ng & Peyton <ref> [189] </ref> LL, BLAS-3 Pub/Author Shared Memory Algorithms nonsym. SuperLU LL, partial, BLAS-2.5 Pub/UCB nonsym. PARASPAR [268, 269] RL, Markowitz, BLAS-1, SD Res/Author sym- MUPS [6] MF, threshold, BLAS-3 Res/Author pattern nonsym. George & Ng [113] RL, partial, BLAS-1 Res/Author s.p.d.
Reference: [190] <author> B. Nour-Omid, B. Parlett, and A. Liu. </author> <title> How to maintain semi-orthogonality among lanczos vectors. </title> <type> CPAM Technical Report 420, </type> <institution> University of California, Berkeley, </institution> <address> CA, </address> <year> 1988. </year>
Reference-contexts: Iterative Methods for Eigenvalue Problems 427 ticular Ritz vector y, it takes many steps before the Lanczos vector again requires orthogonalization against y. So much of the orthogo-nalization work in Algorithm 7.3 can be eliminated. Indeed, there is a simple and inexpensive recurrence for deciding when to reorthogonal-ize <ref> [222, 190] </ref>. Another enhancement is to use the error bounds to efficiently distinguish between converged and "misconverged" eigenvalues [196]. A current state-of-the-art implementation of Lanczos is described in [123]. A different software implementation is available in ARPACK (NETLIB/scalapack/readme.arpack, [169, 231]).
Reference: [191] <author> W. Oettli and W. Prager. </author> <title> Compatibility of approximate solution of linear equations with given error bounds for coefficients and right hand sides. </title> <journal> Num. Math., </journal> <volume> 6 </volume> <pages> 405-409, </pages> <year> 1964. </year>
Reference-contexts: In section 2.4.4 we will see that this bound can sometimes be much smaller than the similar bound (2.5), in particular when A is badly scaled. There is also an analogue to Theorem 2.2 <ref> [191] </ref>: Theorem 2.3 The smallest * &gt; 0 such that there exist jffiAj *jAj and jffibj *jbj satisfying (A + ffiA)^x = b + ffib is called the componentwise relative backward error.
Reference: [192] <author> C. C. Paige and M. A. Saunders. </author> <title> Solution of sparse indefinite systems of linear equations. </title> <journal> SIAM J. Numer. Anal., </journal> <volume> 12 </volume> <pages> 617-629, </pages> <year> 1975. </year>
Reference-contexts: Unfortunately, we do not have enough information in our Krylov subspace to compute this x k . 2. The "best" x k minimizes kr k k 2 . This is implementable, and the corresponding algorithms are called MINRES (for minimum residual) when A is symmetric <ref> [192] </ref>, and GMRES (for generalized minimum residual) when A is nonsymmetric [213]. 3. The "best" x k makes r k ? K k , i.e. Q T k r k = 0. <p> Q T k r k = 0. This is sometimes called the orthogonal residual property, or a Galerkin condition, by analogy to a similar condition in the theory of finite elements. When A is symmetric the corresponding algorithm is called SYMMLQ <ref> [192] </ref>. When A is nonsymmetric, a variation of GMRES works [209]. 4. When A is symmetric and positive definite, it defines a norm krk A 1 = (r T A 1 r) 1=2 (see Lemma 1.3 on page 23). <p> We can minimize the 2-norm of the residual instead of the A 1 -norm when A is symmetric positive definite. This is called the Minimum Residual algorithm, or MINRES <ref> [192] </ref>. Since MINRES is more expensive than CG, and often less accurate because of numerical instabilities, it is not 354 Chapter 6 used for positive definite systems. But MINRES can be used when the matrix is symmetric indefinite, whereas CG cannot. <p> But MINRES can be used when the matrix is symmetric indefinite, whereas CG cannot. In this case, we can also use the SYMMLQ algorithm of Paige and Saunders <ref> [192] </ref>, which produces a residual r k ? K k (A; b) at each step.
Reference: [193] <author> V. Pan. </author> <title> How can we speed up matrix multiplication. </title> <journal> SIAM Review, </journal> <volume> 26 </volume> <pages> 393-416, </pages> <year> 1984. </year>
Reference-contexts: There are a number of other even faster matrix multiplication algorithms; the current record is about O (n 2:376 ), due to Winograd and Coppersmith [261]. But these algorithms only perform fewer operations than Strassen for impractically large values of n. For a survey see <ref> [193] </ref>. 2.6.3 Reorganizing Gaussian Elimination to use Level 3 BLAS First we will reorganize Gaussian Elimination to use the Level 2 BLAS, and then the Level 3 BLAS. For simplicity, we assume no pivoting is necessary.
Reference: [194] <author> V. Pan and P. Tang. </author> <title> Bounds on singular values revealed by QR factorization. </title> <type> Technical Report MCS-P332-1092, </type> <institution> Mathematics and Computer Science Division, Argonne National Laboratory, </institution> <year> 1992. </year> <note> Bibliography 445 </note>
Reference-contexts: Linear Least Squares Problems 145 More sophisticated pivoting schemes than QR with column pivoting, called rank-revealing QR algorithms, have been a subject of much recent study. Rank-revealing QR algorithms which detect rank more reliably and sometimes also faster than QR with column pivoting have been developed <ref> [28, 30, 47, 49, 107, 124, 126, 148, 194, 234] </ref>. We discuss these further in the next section. QRD with column pivoting is available as subroutine sgeqpf in LA-PACK. LAPACK also has several similar factorizations available: RQ (sgerqf), LQ (sgelqf), and QL (sgeqlf). <p> If m n then the initial QR decomposition dominates the the cost of the subsequent operations on the n-by-n matrix R, and all the algorithms cost about the same. The fastest version of rank-revealing QR was that of <ref> [30, 194] </ref>. On Type 1 matrices, this algorithm ranged from 3.2 times slower than QR without pivoting for n = m = 20 to just 1.1 times slower for n = m = 1600. <p> See also chapter 5 of [119] and [166]. Perturbation theory and error bounds for the least squares solution is discussed in detail in [147]. Rank-revealing QRD decompositions are discussed in <ref> [28, 30, 47, 49, 124, 148, 194, 204, 234] </ref>.
Reference: [195] <author> B. Parlett. </author> <title> The Symmetric Eigenvalue Problem. </title> <publisher> Prentice Hall, </publisher> <address> Englewood Cliffs, NJ, </address> <year> 1980. </year>
Reference-contexts: Then Theorems 5.4 and 5.5 indicate that we cannot hope to get the individual eigenvectors accurately. However, it is possible to compute the k-dimensional invariant subspace spanned by these vectors quite accurately. See <ref> [195] </ref> for details. The Symmetric Eigenproblem and SVD 229 5.2.1 Relative Perturbation Theory This section describes tighter bounds on eigenvalues and eigenvectors than in the last section. These bounds are needed to justify the high accuracy algorithms for computing singular values and eigenvalues described in sections 5.4.2 and 5.4.3. <p> It turns out to be cubically convergent for almost all matrices, as shown in the next section. Unfortunately, examples exist where it does not converge <ref> [195, p. 76] </ref>, so to get global convergence a slightly more complicated shift strategy is needed: We let the shift i be the eigenvalue of b n1 a n that is closest to a n . This is called Wilkinson's shift. Theorem 5.8 (Wilkinson). <p> This is called Wilkinson's shift. Theorem 5.8 (Wilkinson). QR iteration with Wilkinson's shift is globally, and at least linearly, convergent. It is asymptotically cubically convergent for almost all matrices. A proof of this theorem can be found in <ref> [195] </ref>. In LAPACK this routine is available as ssyev. <p> For more details on the Toda flow, see [142, 168, 66, 67, 237], and papers by Kruskal [164], Flaschka [104] and Moser [185] in [186]. 5.6 References and Other Topics for Chap ter 5 An excellent general reference for the symmetric eigenproblem is <ref> [195] </ref>. The material on relative perturbation theory can be found in [74, 81, 99]; section 5.2.1 was based on the latter of these references. Related work is found in [65, 90, 226, 248] A classical text on perturbation theory for general linear operators is [159]. <p> In other words, it depends on quantities unknown during the computation, so it is not of practical use. But it shows that if q 1 is nearly orthogonal to the desired eigenvector, or if the desired eigenvalue is nearly multiple, then we can expect slow convergence. See <ref> [195, sec. 12-4] </ref> for details. 7.4 The Lanczos Algorithm in Floating Point Arithmetic The example in the last section described the behavior of the "ideal" Lanczos algorithm, essentially without roundoff. <p> T k end for Full reorthogonalization corresponds to applying the Gram-Schmidt orthogonalization process "z = z P j1 i=1 (z T q i )q i " twice, in order to almost surely make z orthogonal to q 1 through q j1 (see Algorithm 3.1 on page 118, as well as <ref> [195, sec. 6-9] </ref> and [169, chap. 7] for discussions of when "twice is enough"). In exact arithmetic, we showed in section 6.6.1 that z is orthogonal to q 1 through q j1 without reorthogonalization. <p> In order not to be overwhelmed by taking all possible roundoff errors into account, we will draw on others' experience to identify those few rounding errors that are important, and simply ignore the rest <ref> [195, sec 13-4] </ref>. <p> (Algorithm 4.3) [19], Davidson's algorithm [214] or the Jacobi-Davidson algorithm [228] to the sparse nonsymmetric eigenproblem. 7.8 References and Other Topics for Chap ter 7 In addition to the references in sections 7.6 and 7.7, there are a number of good surveys available on algorithms for sparse eigenvalues problems: see <ref> [17, 50, 123, 161, 195, 211, 260] </ref>. Parallel implementations are also discussed in [75]. In section 6.2 we discussed the existence of on-line help to choose among the variety of iterative methods available for solving Ax = b.
Reference: [196] <author> B. Parlett. </author> <title> Misconvergence in the lanczos algorithm. </title> <editor> In M. G. Cox and S. Hammarling, editors, </editor> <booktitle> Reliable Numerical Computation, chapter 1. </booktitle> <publisher> Clarendon Press, Oxford, </publisher> <year> 1990. </year>
Reference-contexts: So much of the orthogo-nalization work in Algorithm 7.3 can be eliminated. Indeed, there is a simple and inexpensive recurrence for deciding when to reorthogonal-ize [222, 190]. Another enhancement is to use the error bounds to efficiently distinguish between converged and "misconverged" eigenvalues <ref> [196] </ref>. A current state-of-the-art implementation of Lanczos is described in [123]. A different software implementation is available in ARPACK (NETLIB/scalapack/readme.arpack, [169, 231]). If we apply Lanczos to the shifted and inverted matrix (AI) 1 , then we expect the eigenvalues closest to to converge first.
Reference: [197] <author> B. Parlett. </author> <title> Reduction to tridiagonal form and minimal realizations. </title> <journal> SIAM J. Mat. Anal. Appl., </journal> <volume> 13(2) </volume> <pages> 567-593, </pages> <year> 1992. </year>
Reference-contexts: Unfortunately, the similarity transformations can be quite ill-conditioned, which means that the eigen 428 Chapter 7 values of the tridiagonal and of the original matrix may greatly differ. In fact, it is not always possible to find an appropriate similarity because of a phenomenon known as "breakdown" <ref> [41, 132, 133, 197] </ref>. Attempts to repair breakdown by by a process called "lookahead" have been proposed, implemented and analyzed in [16, 18, 54, 55, 63, 106, 200, 263, 264].
Reference: [198] <author> B. Parlett. </author> <booktitle> Acta Numerica, chapter The new qd algorithms, </booktitle> <pages> pages 459-491. </pages> <publisher> Cambridge University Press, </publisher> <year> 1995. </year>
Reference-contexts: It is asymptotically cubically convergent for almost all matrices. A proof of this theorem can be found in [195]. In LAPACK this routine is available as ssyev. The inner loop of the algorithm can be organized more efficiently when eigenvalues only are desired (ssterf; see also <ref> [102, 198] </ref>) than when eigenvectors are also computed (ssteqr). 236 Chapter 5 Example 5.7 Here is an illustration of the convergence of tridiagonal QR iteration, starting with the following tridiagonal matrix (diagonals only are shown, in columns) T 0 = tridiag 2 6 6 6 6 :24929 :96880 :48539 :91563 7 <p> similarity to the algorithm discussed in section 5.3.3, and refer the reader to [128] for details. 5.4.1 QR Iteration and its Variations for the Bidi agonal SVD There is a long history of variations on QR iteration for the SVD, designed to be as efficient and accurate as possible; see <ref> [198] </ref> for a good survey. The algorithm in the LAPACK routine sbdsqr was originally based on [79], and later updated to use the algorithm in [102] in the case when singular values only are desired. <p> For a survey of parallel algorithms for the symmetric eigenproblem, see [75]. The QR algorithm for finding the SVD of bidiagonal matrices is discussed in [79, 66, 118]. and the dqds algorithm is in <ref> [102, 198, 207] </ref>. For an error analysis of the Bisection algorithm, see [72, 73, 154], and for recent attempts to accelerate Bisection see [103, 201, 199, 174, 171, 173, 267]. Current work in improving Inverse Iteration appears in [103, 199, 201].
Reference: [199] <author> B. Parlett. </author> <title> The construction of orthogonal eigenvectors for tight clusters by use of submatrices. </title> <institution> Center for Pure and Applied Mathematics PAM-664, University of California, Berkeley, </institution> <address> CA, </address> <month> January </month> <year> 1996. </year> <note> submitted to SIMAX. </note>
Reference-contexts: 1=2 n ), and multiply M 1 Kx (0) = fl 2 x (0) by M 1=2 on both sides to get M 1=2 Kx (0) = M 1=2 K (M 1=2 M 1=2 )x (0) = fl 2 M 1=2 x (0) 1 There is yet more recent work <ref> [199, 201] </ref> on an algorithm based on Inverse Iteration (Algorithm 4.2), which may provide a still faster and more accurate algorithm. But as of September 1996 the theory and software were still under development. <p> Bisection and Inverse Iteration are available as options in the LAPACK routine ssyevx . There is current research on Inverse Iteration, addressing the problem of close eigenvalues, which may make it the fastest method to find all the eigenvectors eigenvectors (besides, theoretically, divide-and-conquer with the FMM) <ref> [103, 201, 199, 174, 171, 173, 267] </ref>. But software implementing this improved version of Inverse Iteration is not yet available. 5. Jacobi's method. This method is historically the oldest method for the eigenproblem, dating to 1846. <p> Worse, there is no guarantee that the computed eigenvectors are accurate or orthogonal. (The trouble is that after reorthogonalizing a set of nearly dependent ^q k , cancellation may mean some computed eigenvectors consist of little more than roundoff errors.) There has been recent progress on this problem, however <ref> [103, 199, 201] </ref>, and it now appears possible that Inverse Iteration may be "repaired" to provide accurate, orthogonal eigenvectors without spending more than O (n) flops per eigenvector to reorthogonalize. <p> The QR algorithm for finding the SVD of bidiagonal matrices is discussed in [79, 66, 118]. and the dqds algorithm is in [102, 198, 207]. For an error analysis of the Bisection algorithm, see [72, 73, 154], and for recent attempts to accelerate Bisection see <ref> [103, 201, 199, 174, 171, 173, 267] </ref>. Current work in improving Inverse Iteration appears in [103, 199, 201]. The divide-and-conquer eigenroutine was introduced in [58] and further developed in [13, 88, 125, 129, 151, 170, 208, 232]. <p> For an error analysis of the Bisection algorithm, see [72, 73, 154], and for recent attempts to accelerate Bisection see [103, 201, 199, 174, 171, 173, 267]. Current work in improving Inverse Iteration appears in <ref> [103, 199, 201] </ref>. The divide-and-conquer eigenroutine was introduced in [58] and further developed in [13, 88, 125, 129, 151, 170, 208, 232]. The possibility of high accuracy eigenvalues obtained from Ja-cobi is discussed in [65, 74, 81, 90, 181, 226].
Reference: [200] <author> B. N. Parlett, D. R. Taylor, and Z. A. Liu. </author> <title> A look-ahead Lanczos algorithm for unsymmetric matrices. </title> <journal> Math. Comp., </journal> <volume> 44 </volume> <pages> 105-124, </pages> <year> 1985. </year>
Reference-contexts: In fact, it is not always possible to find an appropriate similarity because of a phenomenon known as "breakdown" [41, 132, 133, 197]. Attempts to repair breakdown by by a process called "lookahead" have been proposed, implemented and analyzed in <ref> [16, 18, 54, 55, 63, 106, 200, 263, 264] </ref>.
Reference: [201] <author> B.N. Parlett and I.S. Dhillon. </author> <title> On Fernando's method to find the most redundant equation in a tridiagonal system. Linear Algebra and its Applications, </title> <note> 1996. to appear. </note>
Reference-contexts: 1=2 n ), and multiply M 1 Kx (0) = fl 2 x (0) by M 1=2 on both sides to get M 1=2 Kx (0) = M 1=2 K (M 1=2 M 1=2 )x (0) = fl 2 M 1=2 x (0) 1 There is yet more recent work <ref> [199, 201] </ref> on an algorithm based on Inverse Iteration (Algorithm 4.2), which may provide a still faster and more accurate algorithm. But as of September 1996 the theory and software were still under development. <p> Bisection and Inverse Iteration are available as options in the LAPACK routine ssyevx . There is current research on Inverse Iteration, addressing the problem of close eigenvalues, which may make it the fastest method to find all the eigenvectors eigenvectors (besides, theoretically, divide-and-conquer with the FMM) <ref> [103, 201, 199, 174, 171, 173, 267] </ref>. But software implementing this improved version of Inverse Iteration is not yet available. 5. Jacobi's method. This method is historically the oldest method for the eigenproblem, dating to 1846. <p> Worse, there is no guarantee that the computed eigenvectors are accurate or orthogonal. (The trouble is that after reorthogonalizing a set of nearly dependent ^q k , cancellation may mean some computed eigenvectors consist of little more than roundoff errors.) There has been recent progress on this problem, however <ref> [103, 199, 201] </ref>, and it now appears possible that Inverse Iteration may be "repaired" to provide accurate, orthogonal eigenvectors without spending more than O (n) flops per eigenvector to reorthogonalize. <p> The QR algorithm for finding the SVD of bidiagonal matrices is discussed in [79, 66, 118]. and the dqds algorithm is in [102, 198, 207]. For an error analysis of the Bisection algorithm, see [72, 73, 154], and for recent attempts to accelerate Bisection see <ref> [103, 201, 199, 174, 171, 173, 267] </ref>. Current work in improving Inverse Iteration appears in [103, 199, 201]. The divide-and-conquer eigenroutine was introduced in [58] and further developed in [13, 88, 125, 129, 151, 170, 208, 232]. <p> For an error analysis of the Bisection algorithm, see [72, 73, 154], and for recent attempts to accelerate Bisection see [103, 201, 199, 174, 171, 173, 267]. Current work in improving Inverse Iteration appears in <ref> [103, 199, 201] </ref>. The divide-and-conquer eigenroutine was introduced in [58] and further developed in [13, 88, 125, 129, 151, 170, 208, 232]. The possibility of high accuracy eigenvalues obtained from Ja-cobi is discussed in [65, 74, 81, 90, 181, 226].
Reference: [202] <author> D. Priest. </author> <title> Algorithms for arbitrary precision floating point arithmetic. </title> <editor> In P. Kornerup and D. Matula, editors, </editor> <booktitle> Proceedings of the 10th Symposium on Computer Arithmetic, </booktitle> <pages> pages 132-145, </pages> <address> Greno-ble, France, June 26-28 1991. </address> <publisher> IEEE Computer Society Press. </publisher>
Reference-contexts: But when the input data is already in double precision, this means quadruple precision would be needed, and this is not available in many machines and languages, or at least not cheaply. As described in section 1.5, it is possible to simulate quadruple precision using double precision <ref> [232, 202] </ref>. This can be done portably and relatively efficiently, as long as the underlying floating point arithmetic rounds sufficiently accurately. In particular, these simulations require that fl (a b) = (a b)(1 + ffi) with jffij = O ("), barring over/underflow (see Section 1.5 and Question 1.18). <p> This means it is numerically stable. The reader should note that our need for sufficiently accurate floating point arithmetic is precisely what prevented the simulation of quadruple precision proposed in <ref> [232, 202] </ref> from working on some Crays. So we have not yet succeeded in providing an algorithm that works reliably on these machines.
Reference: [203] <author> A. Quarteroni, </author> <title> editor. Domain Decomposition Methods, </title> <address> Providence, R.I., </address> <year> 1993. </year> <title> AMS. </title> <booktitle> Proceedings of the Sixth International Symposium on Domain Decomposition Methods , Como, </booktitle> <address> Italy, </address> <year> 1992. </year> <note> To appear. </note>
Reference-contexts: For an implementation of this algorithm, see Question 6.16. The web site [89] contains pointers to an extensive literature, software, and so on. Iterative Methods for Linear Systems 385 6.10 Domain Decomposition Domain decomposition for solving sparse systems of linear equations is a topic of current research. See <ref> [48, 114, 203] </ref> and especially [230] for recent surveys. We will only give simple examples. The need for methods beyond those we have discussed arises from of the irregularity and size of real problems, and also from the need for algorithms for parallel computers. <p> What makes domain decomposition so attractive is that S turns out to be much better conditioned that the original matrix A (a condition number which grows like O (N ) instead of O (N 2 )), and so convergence is fast <ref> [114, 203] </ref>. More generally, one has k &gt; 2 subdomains, separated by boundaries (see Figure 6.21, where the heavy lines separate subdomains). <p> Classical methods like Jacobi, Gauss-Seidel and SOR are discussed in detail in [247, 135]. Multigrid methods are discussed in [42, 183, 184, 258, 266] and the references therein; [89] is a web site with pointers to an extensive bibliography, software, and so on. Domain decomposition are discussed in <ref> [48, 114, 203, 230] </ref>. Chebyshev and other polynomials are discussed in [238]. The FFT is discussed in any good textbook on computer science algorithms, such as [3], or in [246]. A stabilized version of block cyclic reduction is found in [46, 45].
Reference: [204] <author> H. Ren. </author> <title> On error analysis and implementation of some eigenvalue and singular value algorithms. </title> <type> PhD thesis, </type> <institution> University of California at Berkeley, </institution> <year> 1996. </year>
Reference-contexts: When A is not well-conditioned but far from rank deficient, we should use QR. Since the design of fast algorithms for rank-deficient least squares problems is a current research area, it is difficult to recommend a single algorithm to use. We summarize a recent study <ref> [204] </ref> that compared the performance of several algorithms, comparing them to the fastest stable algorithm for the non-rank-deficient case: QR without pivoting, implemented using Householder transformations as described in section 3.4.1, with memory hierarchy optimizations described in Question 3.17. <p> See also chapter 5 of [119] and [166]. Perturbation theory and error bounds for the least squares solution is discussed in detail in [147]. Rank-revealing QRD decompositions are discussed in <ref> [28, 30, 47, 49, 124, 148, 194, 204, 234] </ref>. <p> Perturbation theory and error bounds for the least squares solution is discussed in detail in [147]. Rank-revealing QRD decompositions are discussed in [28, 30, 47, 49, 124, 148, 194, 204, 234]. In particular, these papers examine the tradeoff between cost and accuracy in rank determination, and in <ref> [204] </ref> there is a comprehensive performance comparison of the available methods for rank-deficient least squares problems. 3.8 Questions for Chapter 3 Question 3.1 (Easy) Show that the two variations of Algorithm 3.1 on page 118, Classical Gram-Schmidt (CGS) and Modified Gram-Schmidt (MGS) are mathematically equivalent, by showing that the two formulas
Reference: [205] <author> Edward Rothberg and Robert Schreiber. </author> <title> Improved load distribution in parallel sparse cholesky factorization. </title> <booktitle> In Supercomputing, </booktitle> <pages> pages 783-792, </pages> <month> November </month> <year> 1994. </year>
Reference-contexts: Gupta, Rothberg, LL, BLAS-3 Com/SGI Ng & Peyton [131] Pub/Author s.p.d. SPLASH [153] RL, 2-D block, BLAS-3 Pub/Stanford Distributed Memory Algorithms sym. van der Stappen [243] RL, Markowitz, Scalar Res/Author sym- Lucas et al. [178] MF, no pivoting, BLAS-1 Res/Author pattern s.p.d. Rothberg et al. <ref> [205] </ref> RL, 2-D block, BLAS-3 Res/Author s.p.d. Gupta [130] MF, 2-D block, BLAS-3 Res/Author s.p.d. CAPSS [141] MF, full parallel, BLAS-1 Pub/NETLIB (require coordinates) Table 2.2: Software to solve sparse linear systems using direct methods.
Reference: [206] <author> S. Rump. </author> <title> Bounds for the componentwise distance to the nearest singular matrix. </title> <note> to appear in SIAM J. Mat. Anal. Appl., 1996. 446 Bibliography </note>
Reference-contexts: For a proof that bounds (2.7) and (2.8) are attainable, see Question 2.4. Recall that Theorem 2.1 related the condition number (A) to the distance from A to the nearest singular matrix. For a similar interpretation of CR (A), see <ref> [71, 206] </ref>. 42 Chapter 2 Example 2.2 Consider our earlier example with A = diag (fl; 1) and b = [fl; 1] T . It is easy to confirm that CR (A) = 1, since jA 1 j jAj = I.
Reference: [207] <author> H. </author> <title> Rutishauser. </title> <booktitle> Lectures on Numerical Mathematics. </booktitle> <address> Biskhauser, </address> <year> 1990. </year>
Reference-contexts: Let T 0 be any symmetric positive definite matrix. The following algorithm produces a sequence of similar symmetric positive definite matrices T i . i = 0 repeat 6 dqds is short for "differential quotient-difference algorithm with shifts" <ref> [207] </ref>. 268 Chapter 5 Choose a shift t 2 i smaller than the smallest eigenvalue of T i . <p> To emphasize that we are computing squares of entries, we change variables to q j a 2 j and e j b 2 yielding the penultimate algorithm qds (the name is again for historical reasons that do not concern us <ref> [207] </ref>). 270 Chapter 5 Algorithm 5.10 One step of the qds algorithm. for j = 1 to n 1 ^e j = e j (q j+1 =^q j ) ^q n = q n ^e n1 ffi The final algorithm, dqds, will do about the same amount of work as qds, <p> For a survey of parallel algorithms for the symmetric eigenproblem, see [75]. The QR algorithm for finding the SVD of bidiagonal matrices is discussed in [79, 66, 118]. and the dqds algorithm is in <ref> [102, 198, 207] </ref>. For an error analysis of the Bisection algorithm, see [72, 73, 154], and for recent attempts to accelerate Bisection see [103, 201, 199, 174, 171, 173, 267]. Current work in improving Inverse Iteration appears in [103, 199, 201].
Reference: [208] <author> J. Rutter. </author> <title> A serial implementation of Cuppen's divide and conquer algorithm for the symmetric eigenvalue problem. </title> <institution> Mathematics Dept. </institution> <note> Master's Thesis available by anonymous ftp to tr-ftp.cs.berkeley.edu, directory pub/tech-reports/csd/csd-94-799, file all.ps, </note> <institution> University of California, </institution> <year> 1994. </year>
Reference-contexts: with random dense matrices with uniformly distributed eigenvalues, over 15% of the eigenvalues of the largest D + uu T deflated, and in experiments with random dense matrices with eigenvalues approaching 0 geometrically, over 85% deflated! It is essential to take advantage of this behavior to make the algorithm fast <ref> [58, 208] </ref>. The payoff in deflation is not in making the solution of the secular equation faster; this only costs O (n 2 ) anyway. The payoff is in making the matrix multiplication in the last step of the algorithm fast. <p> For an error analysis of the Bisection algorithm, see [72, 73, 154], and for recent attempts to accelerate Bisection see [103, 201, 199, 174, 171, 173, 267]. Current work in improving Inverse Iteration appears in [103, 199, 201]. The divide-and-conquer eigenroutine was introduced in [58] and further developed in <ref> [13, 88, 125, 129, 151, 170, 208, 232] </ref>. The possibility of high accuracy eigenvalues obtained from Ja-cobi is discussed in [65, 74, 81, 90, 181, 226].
Reference: [209] <author> Y. Saad. </author> <title> Krylov subspace methods for solving large unsymmetric linear system. </title> <journal> Math. Comp., </journal> <volume> 37 </volume> <pages> 105-126, </pages> <year> 1981. </year>
Reference-contexts: This is sometimes called the orthogonal residual property, or a Galerkin condition, by analogy to a similar condition in the theory of finite elements. When A is symmetric the corresponding algorithm is called SYMMLQ [192]. When A is nonsymmetric, a variation of GMRES works <ref> [209] </ref>. 4. When A is symmetric and positive definite, it defines a norm krk A 1 = (r T A 1 r) 1=2 (see Lemma 1.3 on page 23). We say the "best" x k minimizes kr k k A 1 .
Reference: [210] <author> Y. Saad. </author> <title> Numerical solution of large nonsymmetric eigenvalue problems. </title> <journal> Comput. Phys. Comm., </journal> <volume> 53 </volume> <pages> 71-90, </pages> <year> 1989. </year>
Reference-contexts: Since A is nonsymmetric, its eigenvalues may be complex and/or badly conditioned, so many of the attractive error bounds and monotonic convergence properties enjoyed by Lanczos and described in section 7.3 no longer hold. Nonetheless, effective algorithms and implementations exist. Good references include <ref> [152, 169, 210, 214, 215, 231] </ref>, including the book [211]. The latest software is described in [169, 231], and may be found in NETLIB/scalapack/readme.arpack. The Matlab command speig (for "sparse eigenvalues") uses this software. A second alternative is to use the nonsymmetric Lanczos algorithm.
Reference: [211] <author> Y. Saad. </author> <title> Numerical methods for large eigenvalue problems. </title> <publisher> Manch-ester University Press, </publisher> <year> 1992. </year>
Reference-contexts: Nonetheless, effective algorithms and implementations exist. Good references include [152, 169, 210, 214, 215, 231], including the book <ref> [211] </ref>. The latest software is described in [169, 231], and may be found in NETLIB/scalapack/readme.arpack. The Matlab command speig (for "sparse eigenvalues") uses this software. A second alternative is to use the nonsymmetric Lanczos algorithm. This algorithm attempts to reduce A to nonsymmetric tridiagonal form by a nonorthogonal similarity. <p> (Algorithm 4.3) [19], Davidson's algorithm [214] or the Jacobi-Davidson algorithm [228] to the sparse nonsymmetric eigenproblem. 7.8 References and Other Topics for Chap ter 7 In addition to the references in sections 7.6 and 7.7, there are a number of good surveys available on algorithms for sparse eigenvalues problems: see <ref> [17, 50, 123, 161, 195, 211, 260] </ref>. Parallel implementations are also discussed in [75]. In section 6.2 we discussed the existence of on-line help to choose among the variety of iterative methods available for solving Ax = b.
Reference: [212] <author> Y. Saad. </author> <title> Iterative Methods for Sparse Linear Systems. </title> <publisher> PWS Publishing Co., </publisher> <address> Boston, </address> <year> 1996. </year>
Reference-contexts: In other words, matrix-vector multiplication is a "black-box" called by the template. It is the user's responsibility to supply an implementation of this black-box. Iterative Methods for Linear Systems 295 An analogous templates project for eigenvalue problems is underway. Other recent textbooks on iterative methods are <ref> [15, 134, 212] </ref>. For the most challenging practical problems arising from differential equations more challenging than our model problem, the linear system Ax = b must be "preconditioned", or replaced by the equivalent systems M 1 Ax = M 1 b which is somehow easier to solve. <p> We also refer the reader to the more comprehensive on-line help at NETLIB/templates, which includes a book [24] and implementations in Matlab, Fortran, and C++. For a survey of current research in Krylov subspace methods, see <ref> [15, 105, 134, 212] </ref>. <p> It is called PETSc, for Portable Extensible Toolkit for Scientific computing. PETSc is available at http://www.mcs.anl.gov/petsc/petsc.html, and described briefly in [230]. 6.11 References and Other Topics for Chap ter 6 Up-to-date surveys of modern iterative methods are given in <ref> [15, 105, 134, 212] </ref>, and their parallel implementations are also surveyed in [75]. Classical methods like Jacobi, Gauss-Seidel and SOR are discussed in detail in [247, 135].
Reference: [213] <author> Y. Saad and M. H. Schultz. </author> <title> GMRES: a generalized minimal residual algorithm for solving nonsymmetric linear systems. </title> <journal> SIAM J. Sci. Statist. Comput., </journal> <volume> 7 </volume> <pages> 856-869, </pages> <year> 1986. </year>
Reference-contexts: The "best" x k minimizes kr k k 2 . This is implementable, and the corresponding algorithms are called MINRES (for minimum residual) when A is symmetric [192], and GMRES (for generalized minimum residual) when A is nonsymmetric <ref> [213] </ref>. 3. The "best" x k makes r k ? K k , i.e. Q T k r k = 0. This is sometimes called the orthogonal residual property, or a Galerkin condition, by analogy to a similar condition in the theory of finite elements.
Reference: [214] <author> M. Sadkane. </author> <title> Block-Arnoldi and Davidson methods for unsymmetric large eigenvalue problems. </title> <journal> Numer. Math., </journal> <volume> 64 </volume> <pages> 195-211, </pages> <year> 1993. </year>
Reference-contexts: Since A is nonsymmetric, its eigenvalues may be complex and/or badly conditioned, so many of the attractive error bounds and monotonic convergence properties enjoyed by Lanczos and described in section 7.3 no longer hold. Nonetheless, effective algorithms and implementations exist. Good references include <ref> [152, 169, 210, 214, 215, 231] </ref>, including the book [211]. The latest software is described in [169, 231], and may be found in NETLIB/scalapack/readme.arpack. The Matlab command speig (for "sparse eigenvalues") uses this software. A second alternative is to use the nonsymmetric Lanczos algorithm. <p> Attempts to repair breakdown by by a process called "lookahead" have been proposed, implemented and analyzed in [16, 18, 54, 55, 63, 106, 200, 263, 264]. Finally, it is possible to apply subspace iteration (Algorithm 4.3) [19], Davidson's algorithm <ref> [214] </ref> or the Jacobi-Davidson algorithm [228] to the sparse nonsymmetric eigenproblem. 7.8 References and Other Topics for Chap ter 7 In addition to the references in sections 7.6 and 7.7, there are a number of good surveys available on algorithms for sparse eigenvalues problems: see [17, 50, 123, 161, 195, 211,
Reference: [215] <author> M. Sadkane. </author> <title> A block Arnoldi-Chebyshev method for computing the leading eigenpairs of large sparse unsymmetric matrices. </title> <journal> Numer. Math., </journal> <volume> 64 </volume> <pages> 181-193, </pages> <year> 1993. </year>
Reference-contexts: Since A is nonsymmetric, its eigenvalues may be complex and/or badly conditioned, so many of the attractive error bounds and monotonic convergence properties enjoyed by Lanczos and described in section 7.3 no longer hold. Nonetheless, effective algorithms and implementations exist. Good references include <ref> [152, 169, 210, 214, 215, 231] </ref>, including the book [211]. The latest software is described in [169, 231], and may be found in NETLIB/scalapack/readme.arpack. The Matlab command speig (for "sparse eigenvalues") uses this software. A second alternative is to use the nonsymmetric Lanczos algorithm.
Reference: [216] <author> Jonathan Richard Shewchuk. </author> <title> Adaptive Precision Floating-Point Arithmetic and Fast Robust Geometric Predicates. </title> <type> Technical Report CMU-CS-96-140, </type> <institution> School of Computer Science, Carnegie Mel-lon University, Pittsburgh, Pennsylvania, </institution> <month> May </month> <year> 1996. </year> <note> submitted to Discrete & Computational Geometry. </note>
Reference: [217] <author> M. Shub and S. Smale. </author> <title> Complexity of Bezout's Theorem I: Geometric aspects. </title> <journal> J. of the Amer. Math. Soc., </journal> <volume> 6 </volume> <pages> 459-501, </pages> <year> 1993. </year>
Reference: [218] <author> M. Shub and S. Smale. </author> <title> Complexity of Bezout's Theorem II: Volumes and probabilities. </title> <editor> In F. Eyssette and A. Galligo, editors, </editor> <booktitle> Progress in Mathematics, v. 109 Computational Algebraic Geometry. </booktitle> <publisher> Birkhauser, </publisher> <year> 1993. </year> <note> Bibliography 447 </note>
Reference: [219] <author> M. Shub and S. Smale. </author> <title> Complexity of Bezout's Theorem III: Condition number and packing. </title> <journal> J. of Complexity, </journal> <volume> 9 </volume> <pages> 4-14, </pages> <year> 1993. </year>
Reference: [220] <author> M. Shub and S. Smale. </author> <title> Complexity of Bezout's Theorem IV: Probability of success; extensions. </title> <institution> Mathematics department preprint, University of California, </institution> <year> 1993. </year>
Reference: [221] <editor> SGI Power Challenge. </editor> <title> Silicon Graphics, 1995. </title> <type> Technical Report. </type>
Reference-contexts: LAPACK (and its versions in other languages) are suitable for PCs, workstations, vector computers, and shared memory parallel computers. These include the SUN SPARCcenter 2000 [236], SGI Power Challenge <ref> [221] </ref>, DEC AlphaServer 8400 [101], and Cray C90/J90 [251, 252]. ScaLAPACK is suitable for distributed memory parallel computers, such as the IBM SP-2 [254], Intel Paragon [255], Cray T3 series [253], and networks of workstations 6 A C translation of LAPACK, called CLAPACK (at NETLIB/clapack), is also available. <p> Table 2.2 is organized as follows. The top group of routines, labeled Serial Algorithms, are designed for single processor workstations and PCs. The Shared Memory Algorithms are for symmetric multiprocessors, such as the SUN SPARCcenter 2000 [236], SGI Power Challenge <ref> [221] </ref>, DEC 98 Chapter 2 Linear Equation Solving 99 , AlphaServer 8400 [101], and Cray C90/J90 [251, 252]. The Distributed Memory Algorithms are for machines like the IBM SP-2 [254], Intel Paragon [255], Cray T3 series [253], and networks of workstations [9].
Reference: [222] <author> H. Simon. </author> <title> The Lanczos algorithm with partial reorthogonalization. </title> <journal> Math. Comp., </journal> <volume> 42(165) </volume> <pages> 115-142, </pages> <month> January </month> <year> 1984. </year>
Reference-contexts: Iterative Methods for Eigenvalue Problems 427 ticular Ritz vector y, it takes many steps before the Lanczos vector again requires orthogonalization against y. So much of the orthogo-nalization work in Algorithm 7.3 can be eliminated. Indeed, there is a simple and inexpensive recurrence for deciding when to reorthogonal-ize <ref> [222, 190] </ref>. Another enhancement is to use the error bounds to efficiently distinguish between converged and "misconverged" eigenvalues [196]. A current state-of-the-art implementation of Lanczos is described in [123]. A different software implementation is available in ARPACK (NETLIB/scalapack/readme.arpack, [169, 231]).
Reference: [223] <author> R. D. Skeel. </author> <title> Scaling for numerical stability in Gaussian elimination. </title> <journal> J. of the ACM, </journal> <volume> 26 </volume> <pages> 494-526, </pages> <year> 1979. </year>
Reference-contexts: It is sometimes also called the Bauer condition number [26] or Skeel condition number <ref> [223, 224, 225] </ref>. For a proof that bounds (2.7) and (2.8) are attainable, see Question 2.4. Recall that Theorem 2.1 related the condition number (A) to the distance from A to the nearest singular matrix. <p> In other words, the componentwise relative backward error is as small as possible. For example, this means that if A and b are sparse then ffiA and ffib have the same sparsity structures as A and b, respectively. For a proof, see [147], as well as <ref> [14, 223, 224, 225] </ref> for more details. Single precision iterative refinement and the error bound (2.14) are implemented in LAPACK routines like sgesvx. <p> An average case analysis of pivot growth is described in [240], and an example of bad pivot growth with complete pivoting is given in [120]. Condition estimators are described in [136, 144, 146]. Single precision iterative refinement is analyzed in <ref> [14, 223, 224] </ref>. A comprehensive discussion of error analysis for linear equation solvers, which covers most of these topics, can be found in [147]. For the symmetric indefinite factorization, see [43]. Sparse matrix algorithms are described in [112, 91], as well as the numerous references in Table 2.2.
Reference: [224] <author> R. D. Skeel. </author> <title> Iterative refinement implies numerical stability for Gaussian elimination. </title> <journal> Math. Comput., </journal> <volume> 35 </volume> <pages> 817-832, </pages> <year> 1980. </year>
Reference-contexts: It is sometimes also called the Bauer condition number [26] or Skeel condition number <ref> [223, 224, 225] </ref>. For a proof that bounds (2.7) and (2.8) are attainable, see Question 2.4. Recall that Theorem 2.1 related the condition number (A) to the distance from A to the nearest singular matrix. <p> In other words, the componentwise relative backward error is as small as possible. For example, this means that if A and b are sparse then ffiA and ffib have the same sparsity structures as A and b, respectively. For a proof, see [147], as well as <ref> [14, 223, 224, 225] </ref> for more details. Single precision iterative refinement and the error bound (2.14) are implemented in LAPACK routines like sgesvx. <p> An average case analysis of pivot growth is described in [240], and an example of bad pivot growth with complete pivoting is given in [120]. Condition estimators are described in [136, 144, 146]. Single precision iterative refinement is analyzed in <ref> [14, 223, 224] </ref>. A comprehensive discussion of error analysis for linear equation solvers, which covers most of these topics, can be found in [147]. For the symmetric indefinite factorization, see [43]. Sparse matrix algorithms are described in [112, 91], as well as the numerous references in Table 2.2.
Reference: [225] <author> R. D. Skeel. </author> <title> Effect of equilibration on residual size for partial pivoting. </title> <journal> SIAM J. Numer. Anal., </journal> <volume> 18 </volume> <pages> 449-454, </pages> <year> 1981. </year>
Reference-contexts: It is sometimes also called the Bauer condition number [26] or Skeel condition number <ref> [223, 224, 225] </ref>. For a proof that bounds (2.7) and (2.8) are attainable, see Question 2.4. Recall that Theorem 2.1 related the condition number (A) to the distance from A to the nearest singular matrix. <p> In other words, the componentwise relative backward error is as small as possible. For example, this means that if A and b are sparse then ffiA and ffib have the same sparsity structures as A and b, respectively. For a proof, see [147], as well as <ref> [14, 223, 224, 225] </ref> for more details. Single precision iterative refinement and the error bound (2.14) are implemented in LAPACK routines like sgesvx.
Reference: [226] <author> I. Slapnicar. </author> <title> Accurate symmetric eigenreduction by a Jacobi method. </title> <type> PhD thesis, </type> <institution> Fernuniversitat - Hagen, Hagen, Germany, </institution> <year> 1992. </year>
Reference-contexts: In contrast, one-sided Jacobi has no trouble computing the correct square-roots of eigenvalues of A, namely 1 + p p :99 10 20 , to nearly full machine precision. 2. For extensions of the preceding result to indefinite symmetric eigen problems, see <ref> [226, 248] </ref>. 3. For extensions to generalized symmetric eigenproblem A B and the generalized SVD, see [65, 90]. The Symmetric Eigenproblem and SVD 281 5.5 Differential Equations and Eigenvalue Problems We seek our motivation for this section from conservation laws in physics. <p> The material on relative perturbation theory can be found in [74, 81, 99]; section 5.2.1 was based on the latter of these references. Related work is found in <ref> [65, 90, 226, 248] </ref> A classical text on perturbation theory for general linear operators is [159]. For a survey of parallel algorithms for the symmetric eigenproblem, see [75]. <p> Current work in improving Inverse Iteration appears in [103, 199, 201]. The divide-and-conquer eigenroutine was introduced in [58] and further developed in [13, 88, 125, 129, 151, 170, 208, 232]. The possibility of high accuracy eigenvalues obtained from Ja-cobi is discussed in <ref> [65, 74, 81, 90, 181, 226] </ref>. The Toda flow and related phenomena are discussed in [66, 67, 104, 142, 164, 168, 185, 186, 237]. 5.7 Questions for Chapter 5 Question 5.1 (Easy; Z.
Reference: [227] <author> G. Sleijpen and H. van der Vorst. </author> <title> A Jacobi-Davidson iteration method for linear eigenavalue problems. </title> <institution> Dept. of Mathematics 856, University of Utrecht, </institution> <year> 1994. </year>
Reference-contexts: There are other methods to "precondition" a matrix A to converge to certain eigenvalues more quickly. For example, Davidson's method [59] is used in quantum chemistry problems, where A is strongly diagonally dominant. It is also possible to combine Davidson's method with Jacobi <ref> [227] </ref>. 7.7 Iterative Algorithms for the Nonsym metric Eigenproblem When A is nonsymmetric the Lanczos algorithm described above is no longer applicable. There are two alternatives. The first alternative is to use the Arnoldi algorithm (Algorithm 6.9 on page 334).
Reference: [228] <author> S. Sleijpen, A. Booten, D. Fokkema, and H. van der Vorst. </author> <title> Jacobi-Davidson type methods for generalized eigenproblems and polynomial eigenproblems, Part I. </title> <institution> Dept. of Mathematics 923, University of Utrecht, </institution> <year> 1995. </year>
Reference-contexts: Attempts to repair breakdown by by a process called "lookahead" have been proposed, implemented and analyzed in [16, 18, 54, 55, 63, 106, 200, 263, 264]. Finally, it is possible to apply subspace iteration (Algorithm 4.3) [19], Davidson's algorithm [214] or the Jacobi-Davidson algorithm <ref> [228] </ref> to the sparse nonsymmetric eigenproblem. 7.8 References and Other Topics for Chap ter 7 In addition to the references in sections 7.6 and 7.7, there are a number of good surveys available on algorithms for sparse eigenvalues problems: see [17, 50, 123, 161, 195, 211, 260].
Reference: [229] <author> B. Smith. </author> <title> Domain decomposition algorithms for partial differential equations of linear elasticity. </title> <type> Technical Report 517, </type> <institution> Department of Computer Science, Courant Institute, </institution> <year> 1990 </year> <month> Sept. </month> <type> Ph.D thesis. </type>
Reference-contexts: To summarize the current state of the art, by choosing the preconditioner for S appropriately, one can make the number of steps of conjugate gradient independent of the number of boundary grid points N <ref> [229] </ref>. 6.10.2 Overlapping methods The methods in the last section were called nonoverlapping because the domains corresponding to the nodes in A i;i were disjoint, leading to the block diagonal structure in equation (6.62). In this section we permit overlapping domains, as shown in the figure below.
Reference: [230] <author> B. Smith, P. Bjorstad, and W. Gropp. </author> <title> Domain decomposition: Parallel multilevel methods for elliptic partial differential equations. </title> <publisher> Cambridge University Press, </publisher> <year> 1996. </year> <note> corresponding PETSc software available at http://www.mcs.anl.gov/petsc/petsc.html. </note>
Reference-contexts: This is discussed at length in sections 6.6.5 and 6.10. Implementations, including parallel ones, of many of these techniques are available on-line in the package PETSc or Portable Extensible Toolkit for Scientific computing, at http://www.mcs.anl.gov/petsc/petsc.html <ref> [230] </ref>. 6.3 Poisson's Equation 6.3.1 Poisson's Equation in One Dimension We begin with a one-dimensional version of Poisson's equation, dx 2 = f (x) ; 0 &lt; x &lt; 1 (6.1) where f (x) is a given function, and v (x) is the unknown function we want to compute. v (x) <p> Solving these subproblem corresponds to a block diagonal M (if the subregions are disjoint) or a product of block diagonal M (if the subregions overlap). This is discussed in more detail in section 6.10. A number of these preconditioners have been implemented in the software packages PETSc <ref> [230] </ref> and PARPRE (NETLIB/scalapack/parpre.tar.gz). 6.6.6 Other Krylov Subspace Algorithms for Solv ing Ax = b So far we have concentrated on the symmetric positive definite linear systems, and minimized the A 1 -norm of the residual. <p> The web site [89] contains pointers to an extensive literature, software, and so on. Iterative Methods for Linear Systems 385 6.10 Domain Decomposition Domain decomposition for solving sparse systems of linear equations is a topic of current research. See [48, 114, 203] and especially <ref> [230] </ref> for recent surveys. We will only give simple examples. The need for methods beyond those we have discussed arises from of the irregularity and size of real problems, and also from the need for algorithms for parallel computers. <p> There is software available on-line that implements many of the building blocks described here, and also runs on parallel machines. It is called PETSc, for Portable Extensible Toolkit for Scientific computing. PETSc is available at http://www.mcs.anl.gov/petsc/petsc.html, and described briefly in <ref> [230] </ref>. 6.11 References and Other Topics for Chap ter 6 Up-to-date surveys of modern iterative methods are given in [15, 105, 134, 212], and their parallel implementations are also surveyed in [75]. Classical methods like Jacobi, Gauss-Seidel and SOR are discussed in detail in [247, 135]. <p> Classical methods like Jacobi, Gauss-Seidel and SOR are discussed in detail in [247, 135]. Multigrid methods are discussed in [42, 183, 184, 258, 266] and the references therein; [89] is a web site with pointers to an extensive bibliography, software, and so on. Domain decomposition are discussed in <ref> [48, 114, 203, 230] </ref>. Chebyshev and other polynomials are discussed in [238]. The FFT is discussed in any good textbook on computer science algorithms, such as [3], or in [246]. A stabilized version of block cyclic reduction is found in [46, 45].
Reference: [231] <author> D. Sorensen. </author> <title> Implicit application of polynomial filters in a k-step Arnoldi method. </title> <journal> SIAM J. Mat. Anal. Appl., </journal> <volume> 13(1) </volume> <pages> 357-385, </pages> <year> 1992. </year> <note> 448 Bibliography </note>
Reference-contexts: Another enhancement is to use the error bounds to efficiently distinguish between converged and "misconverged" eigenvalues [196]. A current state-of-the-art implementation of Lanczos is described in [123]. A different software implementation is available in ARPACK (NETLIB/scalapack/readme.arpack, <ref> [169, 231] </ref>). If we apply Lanczos to the shifted and inverted matrix (AI) 1 , then we expect the eigenvalues closest to to converge first. There are other methods to "precondition" a matrix A to converge to certain eigenvalues more quickly. <p> Since A is nonsymmetric, its eigenvalues may be complex and/or badly conditioned, so many of the attractive error bounds and monotonic convergence properties enjoyed by Lanczos and described in section 7.3 no longer hold. Nonetheless, effective algorithms and implementations exist. Good references include <ref> [152, 169, 210, 214, 215, 231] </ref>, including the book [211]. The latest software is described in [169, 231], and may be found in NETLIB/scalapack/readme.arpack. The Matlab command speig (for "sparse eigenvalues") uses this software. A second alternative is to use the nonsymmetric Lanczos algorithm. <p> Nonetheless, effective algorithms and implementations exist. Good references include [152, 169, 210, 214, 215, 231], including the book [211]. The latest software is described in <ref> [169, 231] </ref>, and may be found in NETLIB/scalapack/readme.arpack. The Matlab command speig (for "sparse eigenvalues") uses this software. A second alternative is to use the nonsymmetric Lanczos algorithm. This algorithm attempts to reduce A to nonsymmetric tridiagonal form by a nonorthogonal similarity.
Reference: [232] <author> D. Sorensen and P. Tang. </author> <title> On the orthogonality of eigenvectors computed by divide-and-conquer techniques. </title> <journal> SIAM J. Num. Anal., </journal> <volume> 28(6) </volume> <pages> 1752-1775, </pages> <year> 1991. </year>
Reference-contexts: Unfortunately, the formula can be unstable <ref> [58, 88, 232] </ref>, in particular when two eigenvalues ff i and ff i+1 are very close together. Intuitively, the problem is that (Dff i I) 1 u and (Dff i+1 I) 1 n are "very close" formulas, yet are supposed to yield orthogonal eigenvectors. <p> Either way, d i ff i and d i ff i+1 may contain large relative errors, so the computed eigenvectors (D ff i ) 1 u and (D ff i+1 ) 1 u are quite inaccurate, and far from orthogonal. Early attempts to address this problem <ref> [88, 232] </ref> used double precision arithmetic (when the input data was single precision) to solve the secular equation to high accuracy, so that d i ff i and d i ff i+1 could be computed to high accuracy. <p> But when the input data is already in double precision, this means quadruple precision would be needed, and this is not available in many machines and languages, or at least not cheaply. As described in section 1.5, it is possible to simulate quadruple precision using double precision <ref> [232, 202] </ref>. This can be done portably and relatively efficiently, as long as the underlying floating point arithmetic rounds sufficiently accurately. In particular, these simulations require that fl (a b) = (a b)(1 + ffi) with jffij = O ("), barring over/underflow (see Section 1.5 and Question 1.18). <p> This means it is numerically stable. The reader should note that our need for sufficiently accurate floating point arithmetic is precisely what prevented the simulation of quadruple precision proposed in <ref> [232, 202] </ref> from working on some Crays. So we have not yet succeeded in providing an algorithm that works reliably on these machines. <p> For an error analysis of the Bisection algorithm, see [72, 73, 154], and for recent attempts to accelerate Bisection see [103, 201, 199, 174, 171, 173, 267]. Current work in improving Inverse Iteration appears in [103, 199, 201]. The divide-and-conquer eigenroutine was introduced in [58] and further developed in <ref> [13, 88, 125, 129, 151, 170, 208, 232] </ref>. The possibility of high accuracy eigenvalues obtained from Ja-cobi is discussed in [65, 74, 81, 90, 181, 226].
Reference: [233] <author> G. W. Stewart. </author> <title> Introduction to Matrix Computations. </title> <publisher> Academic Press, </publisher> <address> New York, </address> <year> 1973. </year>
Reference: [234] <author> G. W. Stewart. </author> <title> Rank degeneracy. </title> <journal> SIAM J. Sci. Stat. Comput., </journal> <volume> 5 </volume> <pages> 403-413, </pages> <year> 1984. </year>
Reference-contexts: Linear Least Squares Problems 145 More sophisticated pivoting schemes than QR with column pivoting, called rank-revealing QR algorithms, have been a subject of much recent study. Rank-revealing QR algorithms which detect rank more reliably and sometimes also faster than QR with column pivoting have been developed <ref> [28, 30, 47, 49, 107, 124, 126, 148, 194, 234] </ref>. We discuss these further in the next section. QRD with column pivoting is available as subroutine sgeqpf in LA-PACK. LAPACK also has several similar factorizations available: RQ (sgerqf), LQ (sgelqf), and QL (sgeqlf). <p> See also chapter 5 of [119] and [166]. Perturbation theory and error bounds for the least squares solution is discussed in detail in [147]. Rank-revealing QRD decompositions are discussed in <ref> [28, 30, 47, 49, 124, 148, 194, 204, 234] </ref>.
Reference: [235] <author> G. W. Stewart and J.-G. Sun. </author> <title> Matrix Perturbation Theory. </title> <publisher> Academic Press, </publisher> <address> New York, </address> <year> 1990. </year>
Reference-contexts: For a proof, see [68]. For an overview of condition numbers for the eigenproblem, including eigenvectors, invariant subspaces, and the eigenvalues corresponding to an invariant subspace, see chapter 4 of the LAPACK manual [10], as well as <ref> [159, 235] </ref>. <p> For more details about perturbation theory of eigenvalues and eigenvectors, see <ref> [159, 235, 51] </ref>, and chapter 4 of [10]. For a proof of Theorem 4.7, see [68]. For a discussion of Weierstrass and Kronecker canon ical forms, see [108, 116]. For their application to systems and control theory, see [244, 245, 77]. <p> You do not have to provide error bounds when the eigenproblem is a more general one. (For a description of error bounds for more general eigenproblems, see <ref> [10, 235] </ref>. Write a second Matlab program, which plots S and C for the case n = 3, and marks the intersection points.
Reference: [236] <institution> SPARCcenter 2000 architecture and implementation. Sun Microsys-tems, Inc., </institution> <month> November </month> <year> 1993. </year> <type> Technical White Paper. </type>
Reference-contexts: These codes are available in public domain software libraries like LAPACK (in Fortran, at NETLIB/lapack) 6 , and ScaLAPACK (at NETLIB/scalapack). LAPACK (and its versions in other languages) are suitable for PCs, workstations, vector computers, and shared memory parallel computers. These include the SUN SPARCcenter 2000 <ref> [236] </ref>, SGI Power Challenge [221], DEC AlphaServer 8400 [101], and Cray C90/J90 [251, 252]. <p> Table 2.2 is organized as follows. The top group of routines, labeled Serial Algorithms, are designed for single processor workstations and PCs. The Shared Memory Algorithms are for symmetric multiprocessors, such as the SUN SPARCcenter 2000 <ref> [236] </ref>, SGI Power Challenge [221], DEC 98 Chapter 2 Linear Equation Solving 99 , AlphaServer 8400 [101], and Cray C90/J90 [251, 252]. The Distributed Memory Algorithms are for machines like the IBM SP-2 [254], Intel Paragon [255], Cray T3 series [253], and networks of workstations [9].
Reference: [237] <author> W. Symes. </author> <title> The QR algorithm for the finite nonperiodic Toda lattice. </title> <journal> Phys. D, </journal> <volume> 4 </volume> <pages> 275-280, </pages> <year> 1982. </year>
Reference-contexts: In other words, there is an infinite sequence of energy-like quantities conserved by KdV. This is important both for theoretical and numerical reasons. For more details on the Toda flow, see <ref> [142, 168, 66, 67, 237] </ref>, and papers by Kruskal [164], Flaschka [104] and Moser [185] in [186]. 5.6 References and Other Topics for Chap ter 5 An excellent general reference for the symmetric eigenproblem is [195]. <p> The divide-and-conquer eigenroutine was introduced in [58] and further developed in [13, 88, 125, 129, 151, 170, 208, 232]. The possibility of high accuracy eigenvalues obtained from Ja-cobi is discussed in [65, 74, 81, 90, 181, 226]. The Toda flow and related phenomena are discussed in <ref> [66, 67, 104, 142, 164, 168, 185, 186, 237] </ref>. 5.7 Questions for Chapter 5 Question 5.1 (Easy; Z. Bai) Show that A = B + iC is Hermitian if and only if M = B C is symmetric.
Reference: [238] <author> G. </author> <title> Szego. Orthogonal Polynomials. </title> <publisher> American Mathematical Society, </publisher> <year> 1967. </year>
Reference-contexts: Chebyshev polynomials have many interesting properties <ref> [238] </ref>. <p> Multigrid methods are discussed in [42, 183, 184, 258, 266] and the references therein; [89] is a web site with pointers to an extensive bibliography, software, and so on. Domain decomposition are discussed in [48, 114, 203, 230]. Chebyshev and other polynomials are discussed in <ref> [238] </ref>. The FFT is discussed in any good textbook on computer science algorithms, such as [3], or in [246]. A stabilized version of block cyclic reduction is found in [46, 45].
Reference: [239] <author> K.-C. Toh and L. N. Trefethen. </author> <title> Pseudozeros of polynomials and pseudospectra of companion matrices. </title> <journal> Numer. Math., </journal> <volume> 68 </volume> <pages> 403-425, </pages> <year> 1994. </year>
Reference-contexts: of the matrix C, which is called the companion matrix of the polynomial (4.7). (The Matlab routine roots for finding roots of a polynomial applies the Hessenberg QR iteration of section 4.4.8 to the companion matrix C, since this is currently one of the most reliable, if expensive, methods known <ref> [98, 115, 239] </ref>. Cheaper alternatives are under development.) The same idea works when the A i are matrices. C becomes an n d-by-n d block companion matrix, where the 1's and 0's below the top row become n-by-n identity and zero matrices, respectively.
Reference: [240] <author> L. Trefethen and R. Schreiber. </author> <title> Average case analysis of Gaussian elimination. </title> <journal> SIAM J. Mat. Anal. Appl., </journal> <volume> 11(3) </volume> <pages> 335-360, </pages> <year> 1990. </year>
Reference-contexts: In practice, g PP is almost always n or less. The average behavior seems to be n 2=3 or perhaps even just n 1=2 <ref> [240] </ref>. (See Figure 2.1 below.) This makes GEPP the algorithm of choice for many problems. Unfortunately, there are rare examples where g PP can be as large as 2 n1 : Proposition 2.1 Gaussian elimination with partial pivoting (GEPP) guarantees that g PP 2 n1 . This bound is attainable. <p> The reciprocal relationship between condition numbers and distance to the nearest ill-posed problem is further explored in [70]. An average case analysis of pivot growth is described in <ref> [240] </ref>, and an example of bad pivot growth with complete pivoting is given in [120]. Condition estimators are described in [136, 144, 146]. Single precision iterative refinement is analyzed in [14, 223, 224].
Reference: [241] <author> L. N. Trefethen and D. Bau. </author> <note> Numerical linear algebra. to be published by SIAM, 1996. http://www.cs.cornell.edu/Info/People/lnt/text.html. </note>
Reference: [242] <author> A. Van Der Sluis. </author> <title> Condition numbers and equilibration of matrices. </title> <journal> Num. Math., </journal> <volume> 14 </volume> <pages> 14-23, </pages> <year> 1969. </year>
Reference-contexts: It is possible to show that choosing D this way reduces the condition number of DA to within a factor of p n of its smallest possible value for any diagonal D <ref> [242] </ref>. In practice we may also choose two diagonal matrices D row and D col , and solve (D row AD col )x = D row b, x = D col x. The techniques of iterative refinement and equilibration are implemented in the LAPACK subroutines like sgerfs and sgeequ, respectively. <p> One can show that among all possible diagonal preconditioners, this choice reduces the condition number of M 1 A to within a factor of n of its minimum value <ref> [242] </ref>. This is also called Jacobi preconditioning. * As a generalization of the first preconditioner, let A = 6 A 11 A 1b . . . A b1 A bb 7 be a block matrix, where the diagonal blocks A ii are square.
Reference: [243] <author> A. Frank van der Stappen, Rob H. Bisseling, and Johannes G. G. van der Vorst. </author> <title> Parallel sparse LU decomposition on a mesh network of transputers. </title> <journal> SIAM J. Mat. Anal. Appl., </journal> <volume> 14(3) </volume> <pages> 853-879, </pages> <month> July </month> <year> 1993. </year>
Reference-contexts: George & Ng [113] RL, partial, BLAS-1 Res/Author s.p.d. Gupta, Rothberg, LL, BLAS-3 Com/SGI Ng & Peyton [131] Pub/Author s.p.d. SPLASH [153] RL, 2-D block, BLAS-3 Pub/Stanford Distributed Memory Algorithms sym. van der Stappen <ref> [243] </ref> RL, Markowitz, Scalar Res/Author sym- Lucas et al. [178] MF, no pivoting, BLAS-1 Res/Author pattern s.p.d. Rothberg et al. [205] RL, 2-D block, BLAS-3 Res/Author s.p.d. Gupta [130] MF, 2-D block, BLAS-3 Res/Author s.p.d.
Reference: [244] <author> P. Van Dooren. </author> <title> The computation of Kronecker's canonical form of a singular pencil. </title> <journal> Lin. Alg. Appl., </journal> <volume> 27 </volume> <pages> 103-141, </pages> <year> 1979. </year>
Reference-contexts: L T m has an analogous left null vector. For a proof, see [108]. Just as Schur form generalized to regular matrix pencils in the last section, it can be generalized to arbitrary singular pencils as well. For the canonical form, perturbation theory and software, see <ref> [27, 78, 244] </ref>. Singular pencils are used to model systems arising in systems and control. We give two examples. Application of Kronecker Form to Differential Equations Suppose we want to solve B _x = Ax + f (t), where A B is a singular pencil. <p> To compute this space in practice, in order to determine whether the physical system being modeled can in fact be controlled by input u (t), one applies a QR-like algorithm to the singular pencil [B; A I]. For details, see <ref> [77, 244, 245] </ref>. 4.5.3 Nonlinear Eigenvalue Problems Finally, we consider the nonlinear eigenvalue problem or matrix polynomial d X i A i = d A d + d1 A d1 + A 1 + A 0 : (4.7) Suppose for simplicity that the A i are n-by-n matrices and A d <p> See <ref> [78, 244] </ref> for details of the form and algorithms. Cost: The most general and reliable version of the algorithm can cost as much as O (n 4 ), depending on the details of the Kronecker Structure; this is much more than for regular A B. <p> For more details about perturbation theory of eigenvalues and eigenvectors, see [159, 235, 51], and chapter 4 of [10]. For a proof of Theorem 4.7, see [68]. For a discussion of Weierstrass and Kronecker canon ical forms, see [108, 116]. For their application to systems and control theory, see <ref> [244, 245, 77] </ref>. For applications to computational geometry, graphics, and mechanical CAD, see [179, 180, 163]. For a discussion of parallel algorithms for the nonsymmetric eigenproblem, see [75]. 4.8 Questions for Chapter 4 Question 4.1 (Easy) Let A be defined as in equation (4.1).
Reference: [245] <author> P. Van Dooren. </author> <title> The generalized eigenstructure problem in linear system theory. </title> <journal> IEEE Trans. Aut. Cntl., </journal> <note> AC-26:111-128, 1981. Bibliography 449 </note>
Reference-contexts: To compute this space in practice, in order to determine whether the physical system being modeled can in fact be controlled by input u (t), one applies a QR-like algorithm to the singular pencil [B; A I]. For details, see <ref> [77, 244, 245] </ref>. 4.5.3 Nonlinear Eigenvalue Problems Finally, we consider the nonlinear eigenvalue problem or matrix polynomial d X i A i = d A d + d1 A d1 + A 1 + A 0 : (4.7) Suppose for simplicity that the A i are n-by-n matrices and A d <p> For more details about perturbation theory of eigenvalues and eigenvectors, see [159, 235, 51], and chapter 4 of [10]. For a proof of Theorem 4.7, see [68]. For a discussion of Weierstrass and Kronecker canon ical forms, see [108, 116]. For their application to systems and control theory, see <ref> [244, 245, 77] </ref>. For applications to computational geometry, graphics, and mechanical CAD, see [179, 180, 163]. For a discussion of parallel algorithms for the nonsymmetric eigenproblem, see [75]. 4.8 Questions for Chapter 4 Question 4.1 (Easy) Let A be defined as in equation (4.1).
Reference: [246] <author> C. V. Van Loan. </author> <title> Computational Frameworks for the Fast Fourier Transform. </title> <publisher> SIAM, </publisher> <address> Philadelphia, </address> <year> 1992. </year>
Reference-contexts: Domain decomposition are discussed in [48, 114, 203, 230]. Chebyshev and other polynomials are discussed in [238]. The FFT is discussed in any good textbook on computer science algorithms, such as [3], or in <ref> [246] </ref>. A stabilized version of block cyclic reduction is found in [46, 45]. Iterative Methods for Linear Systems 395 6.12 Questions for Chapter 6 Question 6.1 (Easy) Prove Lemma 6.1 on page 296. Question 6.2 (Easy) Prove the following formulas for triangular factorizations of T N . 1.
Reference: [247] <author> R. S. Varga. </author> <title> Matrix Iterative Analysis. </title> <publisher> Prentice-Hall, </publisher> <address> Englewood Cliffs N.J., </address> <year> 1962. </year>
Reference-contexts: Theorem 6.3 If A is irreducible and weakly row diagonally dominant, then both Jacobi and Gauss-Seidel converge, and (R GS ) &lt; (R J ) &lt; 1. For a proof of this theorem, see <ref> [247] </ref>. Example 6.6 The model problem is weakly diagonally dominant and irreducible, but not strongly diagonally dominant (the diagonal is 4, and the offdiagonal sums are either 2, 3 or 4). So Jacobi and Gauss-Seidel converge on the model problem. <p> Despite the above results showing that under certain conditions Gauss-Seidel is faster than Jacobi, no such general result holds. This is because there are nonsymmetric matrices for which Jacobi converges and Gauss-Seidel diverges, as well as matrices for which Gauss-Seidel converges and Jacobi diverges <ref> [247] </ref>. Now we consider the convergence of SOR (!) [247]. Recall its definition: R SOR (!) = (I !L) 1 ((1 !)I + !U ) : Theorem 6.4 (R SOR (!) ) j! 1j. Therefore 0 &lt; ! &lt; 2 is required for convergence. <p> This is because there are nonsymmetric matrices for which Jacobi converges and Gauss-Seidel diverges, as well as matrices for which Gauss-Seidel converges and Jacobi diverges <ref> [247] </ref>. Now we consider the convergence of SOR (!) [247]. Recall its definition: R SOR (!) = (I !L) 1 ((1 !)I + !U ) : Theorem 6.4 (R SOR (!) ) j! 1j. Therefore 0 &lt; ! &lt; 2 is required for convergence. <p> Example 6.9 Any block tridiagonal matrix 2 4 B 1 . . . . . . A n1 3 5 is consistently ordered when the D i are diagonal. Consistent ordering implies there are simple formulas relating the eigenvalues of R J , R GS and R SOR (!) <ref> [247] </ref>. <p> This means Gauss-Seidel is twice as fast as Jacobi. Proof: The choice ! = 1 is equivalent to Gauss-Seidel so 2 = 2 or = 2 2 To get the most benefit from overrelaxation, we would like to find ! opt minimizing (R SOR (!) ) <ref> [247] </ref>. Theorem 6.7 Suppose A is consistently ordered, R J has real eigenval ues, and = (R J ) &lt; 1. <p> Classical methods like Jacobi, Gauss-Seidel and SOR are discussed in detail in <ref> [247, 135] </ref>. Multigrid methods are discussed in [42, 183, 184, 258, 266] and the references therein; [89] is a web site with pointers to an extensive bibliography, software, and so on. Domain decomposition are discussed in [48, 114, 203, 230]. Chebyshev and other polynomials are discussed in [238].
Reference: [248] <author> K. Veselic and I. Slapnicar. </author> <title> Floating point perturbations of Hermi-tian matrices. </title> <journal> Lin. Alg. Appl., </journal> <volume> 195 </volume> <pages> 81-116, </pages> <year> 1993. </year>
Reference-contexts: In contrast, one-sided Jacobi has no trouble computing the correct square-roots of eigenvalues of A, namely 1 + p p :99 10 20 , to nearly full machine precision. 2. For extensions of the preceding result to indefinite symmetric eigen problems, see <ref> [226, 248] </ref>. 3. For extensions to generalized symmetric eigenproblem A B and the generalized SVD, see [65, 90]. The Symmetric Eigenproblem and SVD 281 5.5 Differential Equations and Eigenvalue Problems We seek our motivation for this section from conservation laws in physics. <p> The material on relative perturbation theory can be found in [74, 81, 99]; section 5.2.1 was based on the latter of these references. Related work is found in <ref> [65, 90, 226, 248] </ref> A classical text on perturbation theory for general linear operators is [159]. For a survey of parallel algorithms for the symmetric eigenproblem, see [75].
Reference: [249] <author> V. Voevodin. </author> <title> The problem of non-self-adjoint generalization of the conjugate gradient method is closed. USSR Compute. </title> <journal> Maths. and Math. Phys., </journal> <volume> 23 </volume> <pages> 143-144, </pages> <year> 1983. </year>
Reference-contexts: Essentially, algorithms satisfying these two properties exist only for matrices of the form e i (T + I), where T = T T (or T H = (HT ) T for some symmetric positive definite H), is real, and is complex <ref> [100, 249] </ref>. For these symmetric and special nonsymmetric A, it turns out we can find a short recurrence, as in the Lanczos algorithm, for computing an orthogonal basis [q 1 ; :::; q k ] of K k (A; b).
Reference: [250] <author> D. Watkins. </author> <title> Fundamentals of Matrix Computations. </title> <publisher> Wiley, </publisher> <year> 1991. </year>
Reference: [251] <institution> The Cray C90 series. http://www.cray.com/PUBLIC/product-info/C90/. Cray Research, Inc. </institution>
Reference-contexts: LAPACK (and its versions in other languages) are suitable for PCs, workstations, vector computers, and shared memory parallel computers. These include the SUN SPARCcenter 2000 [236], SGI Power Challenge [221], DEC AlphaServer 8400 [101], and Cray C90/J90 <ref> [251, 252] </ref>. ScaLAPACK is suitable for distributed memory parallel computers, such as the IBM SP-2 [254], Intel Paragon [255], Cray T3 series [253], and networks of workstations 6 A C translation of LAPACK, called CLAPACK (at NETLIB/clapack), is also available. <p> The Shared Memory Algorithms are for symmetric multiprocessors, such as the SUN SPARCcenter 2000 [236], SGI Power Challenge [221], DEC 98 Chapter 2 Linear Equation Solving 99 , AlphaServer 8400 [101], and Cray C90/J90 <ref> [251, 252] </ref>. The Distributed Memory Algorithms are for machines like the IBM SP-2 [254], Intel Paragon [255], Cray T3 series [253], and networks of workstations [9].
Reference: [252] <institution> The Cray J90 series. http://www.cray.com/PUBLIC/product-info/J90/. Cray Research, Inc. </institution>
Reference-contexts: LAPACK (and its versions in other languages) are suitable for PCs, workstations, vector computers, and shared memory parallel computers. These include the SUN SPARCcenter 2000 [236], SGI Power Challenge [221], DEC AlphaServer 8400 [101], and Cray C90/J90 <ref> [251, 252] </ref>. ScaLAPACK is suitable for distributed memory parallel computers, such as the IBM SP-2 [254], Intel Paragon [255], Cray T3 series [253], and networks of workstations 6 A C translation of LAPACK, called CLAPACK (at NETLIB/clapack), is also available. <p> The Shared Memory Algorithms are for symmetric multiprocessors, such as the SUN SPARCcenter 2000 [236], SGI Power Challenge [221], DEC 98 Chapter 2 Linear Equation Solving 99 , AlphaServer 8400 [101], and Cray C90/J90 <ref> [251, 252] </ref>. The Distributed Memory Algorithms are for machines like the IBM SP-2 [254], Intel Paragon [255], Cray T3 series [253], and networks of workstations [9].
Reference: [253] <institution> The Cray T3E series. http://www.cray.com/PUBLIC/product-info/T3E/. Cray Research, Inc. </institution>
Reference-contexts: These include the SUN SPARCcenter 2000 [236], SGI Power Challenge [221], DEC AlphaServer 8400 [101], and Cray C90/J90 [251, 252]. ScaLAPACK is suitable for distributed memory parallel computers, such as the IBM SP-2 [254], Intel Paragon [255], Cray T3 series <ref> [253] </ref>, and networks of workstations 6 A C translation of LAPACK, called CLAPACK (at NETLIB/clapack), is also available. LAPACK++ (at NETLIB/c++/lapack++)) and LAPACK90 (at NETLIB/lapack90)) are C++ and Fortran 90 interfaces to LAPACK, respectively. Linear Equation Solving 71 [9]. These libraries are available on NETLIB, including a comprehensive manual [10]. <p> The Distributed Memory Algorithms are for machines like the IBM SP-2 [254], Intel Paragon [255], Cray T3 series <ref> [253] </ref>, and networks of workstations [9]. As you can see, most software has been written for serial machines, some for shared memory machines, and very little (besides research software) for distributed memory. The first column gives the Matrix Type.
Reference: [254] <institution> The IBM SP-2. http://www.rs6000.ibm.com/software/sp products/sp2.html. IBM. </institution>
Reference-contexts: These include the SUN SPARCcenter 2000 [236], SGI Power Challenge [221], DEC AlphaServer 8400 [101], and Cray C90/J90 [251, 252]. ScaLAPACK is suitable for distributed memory parallel computers, such as the IBM SP-2 <ref> [254] </ref>, Intel Paragon [255], Cray T3 series [253], and networks of workstations 6 A C translation of LAPACK, called CLAPACK (at NETLIB/clapack), is also available. LAPACK++ (at NETLIB/c++/lapack++)) and LAPACK90 (at NETLIB/lapack90)) are C++ and Fortran 90 interfaces to LAPACK, respectively. Linear Equation Solving 71 [9]. <p> The Shared Memory Algorithms are for symmetric multiprocessors, such as the SUN SPARCcenter 2000 [236], SGI Power Challenge [221], DEC 98 Chapter 2 Linear Equation Solving 99 , AlphaServer 8400 [101], and Cray C90/J90 [251, 252]. The Distributed Memory Algorithms are for machines like the IBM SP-2 <ref> [254] </ref>, Intel Paragon [255], Cray T3 series [253], and networks of workstations [9]. As you can see, most software has been written for serial machines, some for shared memory machines, and very little (besides research software) for distributed memory. The first column gives the Matrix Type.
Reference: [255] <institution> The intel paragon. </institution> <address> http://www.ssd.intel.com/homepage.html. In-tel. </address>
Reference-contexts: These include the SUN SPARCcenter 2000 [236], SGI Power Challenge [221], DEC AlphaServer 8400 [101], and Cray C90/J90 [251, 252]. ScaLAPACK is suitable for distributed memory parallel computers, such as the IBM SP-2 [254], Intel Paragon <ref> [255] </ref>, Cray T3 series [253], and networks of workstations 6 A C translation of LAPACK, called CLAPACK (at NETLIB/clapack), is also available. LAPACK++ (at NETLIB/c++/lapack++)) and LAPACK90 (at NETLIB/lapack90)) are C++ and Fortran 90 interfaces to LAPACK, respectively. Linear Equation Solving 71 [9]. <p> The Distributed Memory Algorithms are for machines like the IBM SP-2 [254], Intel Paragon <ref> [255] </ref>, Cray T3 series [253], and networks of workstations [9]. As you can see, most software has been written for serial machines, some for shared memory machines, and very little (besides research software) for distributed memory. The first column gives the Matrix Type.
Reference: [256] <author> P. A. Wedin. </author> <title> Perturbation theory for pseudoinverses. </title> <journal> BIT, </journal> <volume> 13 </volume> <pages> 217-232, </pages> <year> 1973. </year>
Reference-contexts: An alternative form for the bound in Theorem 3.4, that eliminates the O (* 2 ) term, is as follows <ref> [256, 147] </ref> (here ~r is the perturbed residual ~r = (b + ffib) (A + ffiA)~x): k~x xk 2 1 * 2 (A) 2 + ( 2 (A) + 1) kAk 2 kxk 2 k~r rk 2 (1 + 2* 2 (A)) : We will see that, properly implemented, both the
Reference: [257] <author> S. Weisberg. </author> <title> Applied Linear Regression. </title> <publisher> Wiley, </publisher> <address> 2nd edition, </address> <year> 1985. </year>
Reference-contexts: We will see the matrix (A T A) 1 again below when we solve the least squares problem using the normal equations. For more details on the connection to statistics 1 , see for example <ref> [33, 257] </ref>. Example 3.3 The least squares problem was first posed and formulated by Gauss to solve a practical problem for the German government. There are important economic and legal reasons to know exactly where the boundaries lie between plots of land owned by different people.
Reference: [258] <author> P. </author> <title> Wesseling. An Introduction to Multigrid Methods. </title> <publisher> John Wiley and Sons, </publisher> <address> Chichester, </address> <year> 1992. </year>
Reference-contexts: Classical methods like Jacobi, Gauss-Seidel and SOR are discussed in detail in [247, 135]. Multigrid methods are discussed in <ref> [42, 183, 184, 258, 266] </ref> and the references therein; [89] is a web site with pointers to an extensive bibliography, software, and so on. Domain decomposition are discussed in [48, 114, 203, 230]. Chebyshev and other polynomials are discussed in [238].
Reference: [259] <author> J. H. Wilkinson. </author> <title> Rounding Errors in Algebraic Processes. </title> <publisher> Prentice Hall, </publisher> <address> Englewood Cliffs, </address> <year> 1963. </year>
Reference: [260] <author> J. H. Wilkinson. </author> <title> The Algebraic Eigenvalue Problem. </title> <publisher> Oxford University Press, Oxford, </publisher> <year> 1965. </year>
Reference-contexts: solution of Ax = b, that are much smaller than what we get from using kffiAk 1 3g PP n 3 "kAk 1 It can be shown that Gaussian elimination with complete pivoting (GECP) is even more stable than GEPP, with its pivot growth g CP satisfying the worst-case bound <ref> [260, p. 213] </ref> g CP = max ij ja ij j p 1 4 : This upper bound is also much too large in practice. The average behavior of g CP is n 1=2 . <p> Cyclic Jacobi is also asymptotically quadratically convergent like Classical Jacobi <ref> [260, p. 270] </ref>. The cost of one Jacobi "sweep" (where each j; k pair is selected once) is approximately half the cost of reduction to tridiagonal form and the computation of eigenvalues and eigenvectors using QR iteration, and more than the cost using divide-and-conquer. <p> (Algorithm 4.3) [19], Davidson's algorithm [214] or the Jacobi-Davidson algorithm [228] to the sparse nonsymmetric eigenproblem. 7.8 References and Other Topics for Chap ter 7 In addition to the references in sections 7.6 and 7.7, there are a number of good surveys available on algorithms for sparse eigenvalues problems: see <ref> [17, 50, 123, 161, 195, 211, 260] </ref>. Parallel implementations are also discussed in [75]. In section 6.2 we discussed the existence of on-line help to choose among the variety of iterative methods available for solving Ax = b.
Reference: [261] <author> S. Winograd and D. Coppersmith. </author> <title> Matrix multiplication via arithmetic progressions. </title> <booktitle> In Proceedings of the Nineteenth Annual ACM Symposium on the Theory of Computing, </booktitle> <pages> pages 1-6. </pages> <note> ACM, 1987. 450 Bibliography </note>
Reference-contexts: A drawback is the need for significant workspace, and somewhat lower numerical stability, although it is adequate for many purposes [76]. There are a number of other even faster matrix multiplication algorithms; the current record is about O (n 2:376 ), due to Winograd and Coppersmith <ref> [261] </ref>. But these algorithms only perform fewer operations than Strassen for impractically large values of n. For a survey see [193]. 2.6.3 Reorganizing Gaussian Elimination to use Level 3 BLAS First we will reorganize Gaussian Elimination to use the Level 2 BLAS, and then the Level 3 BLAS.
Reference: [262] <author> M. Wolfe. </author> <title> High performance compilers for parallel computing. </title> <publisher> Addison-Wesley, </publisher> <year> 1996. </year>
Reference-contexts: While there is much current research on this topic (see the bibliography in the recent compiler textbook <ref> [262] </ref>), there is still no reliably fast alternative to optimized libraries such as LAPACK and ScaLAPACK. 8 http://performance.netlib.org/performance/html/PDStop.html Linear Equation Solving 85 2.7 Special Linear Systems As mentioned in section 1.2, it is important to exploit any special structure of the matrix to increase speed of solution and decrease storage.
Reference: [263] <author> Q. Ye. </author> <title> A convergence analysis for nonsymmetric Lanczos algorithms. </title> <journal> Math. Comp., </journal> <volume> 56 </volume> <pages> 677-691, </pages> <year> 1991. </year>
Reference-contexts: In fact, it is not always possible to find an appropriate similarity because of a phenomenon known as "breakdown" [41, 132, 133, 197]. Attempts to repair breakdown by by a process called "lookahead" have been proposed, implemented and analyzed in <ref> [16, 18, 54, 55, 63, 106, 200, 263, 264] </ref>.
Reference: [264] <author> Q. Ye. </author> <title> A breakdown-free variation of the nonsymmetric Lanczos algorithm. </title> <journal> Math. Comp., </journal> <volume> 62 </volume> <pages> 179-207, </pages> <year> 1994. </year>
Reference-contexts: In fact, it is not always possible to find an appropriate similarity because of a phenomenon known as "breakdown" [41, 132, 133, 197]. Attempts to repair breakdown by by a process called "lookahead" have been proposed, implemented and analyzed in <ref> [16, 18, 54, 55, 63, 106, 200, 263, 264] </ref>.
Reference: [265] <author> D. Young. </author> <title> Iterative Solution of Large Linear Systems. </title> <publisher> Academic Press, </publisher> <address> New York, </address> <year> 1971. </year>
Reference-contexts: Example 6.12 Let us apply SSOR (!) with Chebyshev acceleration to the model problem. We need to both choose ! and estimate the spectral radius = (E ! ). The optimal ! that minimizes is not known but Young <ref> [265, 135] </ref> has shown that the choice ! = 2 1+[2 (1 (R J ))] is a good one, yielding (E ! ) 1 2N . With Chebyshev acceleration the error is multiplied by m 1 T m (1+ p N ) at step m.
Reference: [266] <author> H. Yserentant. </author> <title> Old and new convergence proofs for multigrid methods. </title> <editor> In A. Iserles, editor, </editor> <booktitle> Acta Numerica 1993, </booktitle> <pages> pages 285-326. </pages> <publisher> Cambridge University Press, </publisher> <year> 1993. </year>
Reference-contexts: This lets us compute the eigenvalues of T explicitly: Theorem 6.11 The matrix M has eigenvalues 1=9 and 0, independent of i. Therefore multigrid converges at a fixed rate independent of the number of unknowns. For a proof, see Question 6.15. For a more general analysis, see <ref> [266] </ref>. For an implementation of this algorithm, see Question 6.16. The web site [89] contains pointers to an extensive literature, software, and so on. Iterative Methods for Linear Systems 385 6.10 Domain Decomposition Domain decomposition for solving sparse systems of linear equations is a topic of current research. <p> Classical methods like Jacobi, Gauss-Seidel and SOR are discussed in detail in [247, 135]. Multigrid methods are discussed in <ref> [42, 183, 184, 258, 266] </ref> and the references therein; [89] is a web site with pointers to an extensive bibliography, software, and so on. Domain decomposition are discussed in [48, 114, 203, 230]. Chebyshev and other polynomials are discussed in [238].
Reference: [267] <author> Z. Zeng. </author> <title> Homotopy-determinant algorithm for solving matrix eigenvalue problems and its parallelizations. </title> <type> PhD thesis, </type> <institution> Michigan State University, </institution> <year> 1991. </year>
Reference-contexts: Bisection and Inverse Iteration are available as options in the LAPACK routine ssyevx . There is current research on Inverse Iteration, addressing the problem of close eigenvalues, which may make it the fastest method to find all the eigenvectors eigenvectors (besides, theoretically, divide-and-conquer with the FMM) <ref> [103, 201, 199, 174, 171, 173, 267] </ref>. But software implementing this improved version of Inverse Iteration is not yet available. 5. Jacobi's method. This method is historically the oldest method for the eigenproblem, dating to 1846. <p> There are many ways to accelerate convergence, using algorithms like Newton's method and its relatives, to find zeros of the characteristic polynomial (which may be computed by multiplying all the d i 's together) <ref> [171, 172, 173, 174, 176, 267] </ref>. The Symmetric Eigenproblem and SVD 255 To compute eigenvectors once we have computed (selected) eigenval-ues, we can use Inverse Iteration (Algorithm 4.2); this is available in LA-PACK routine sstein. Since we can use accurate eigenvalues as shifts, convergence usually takes one or two iterations. <p> The QR algorithm for finding the SVD of bidiagonal matrices is discussed in [79, 66, 118]. and the dqds algorithm is in [102, 198, 207]. For an error analysis of the Bisection algorithm, see [72, 73, 154], and for recent attempts to accelerate Bisection see <ref> [103, 201, 199, 174, 171, 173, 267] </ref>. Current work in improving Inverse Iteration appears in [103, 199, 201]. The divide-and-conquer eigenroutine was introduced in [58] and further developed in [13, 88, 125, 129, 151, 170, 208, 232].
Reference: [268] <author> Z. Zlatev, J. Wasniewski, P. C. Hansen, and Tz. Ostromsky. PARASPAR: </author> <title> a package for the solution of large linear algebraic equations on parallel computers with shared memory. </title> <type> Technical Report 95-10, </type> <institution> Technical University of Denmark, </institution> <month> September </month> <year> 1995. </year>
Reference-contexts: SPARSE [165] RL, Markowitz, Scalar Pub/NETLIB sym pattern MUPS [5] MA42 [96] MF, threshold, BLAS-3 Frontal, BLAS-3 Com/HSL sym. MA27 [95]/MA47 [93] MF, LDL T , BLAS-1 Com/HSL s.p.d. Ng & Peyton [189] LL, BLAS-3 Pub/Author Shared Memory Algorithms nonsym. SuperLU LL, partial, BLAS-2.5 Pub/UCB nonsym. PARASPAR <ref> [268, 269] </ref> RL, Markowitz, BLAS-1, SD Res/Author sym- MUPS [6] MF, threshold, BLAS-3 Res/Author pattern nonsym. George & Ng [113] RL, partial, BLAS-1 Res/Author s.p.d. Gupta, Rothberg, LL, BLAS-3 Com/SGI Ng & Peyton [131] Pub/Author s.p.d.

References-found: 268

