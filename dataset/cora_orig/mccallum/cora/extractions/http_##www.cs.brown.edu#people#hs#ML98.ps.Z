URL: http://www.cs.brown.edu/people/hs/ML98.ps.Z
Refering-URL: http://www.cs.brown.edu/people/hs/
Root-URL: 
Email: fhs,lpkg @cs.brown.edu  
Title: Heading in the Right Direction  
Author: Hagit Shatkay Leslie P. Kaelbling 
Address: Providence, RI 02912  
Affiliation: Department of Computer Science Brown University  
Abstract: Stochastic topological models, and hidden Markov models in particular, are a useful tool for robotic navigation and planning. In previous work we have shown how weak odometric data can be used to improve learning topological models, overcoming the common problems of the standard Baum-Welch algorithm. Odomet-ric data typically contain directional information, which imposes two difficulties: First, the cyclic-ity of the data requires the use of special circular distributions. Second, small errors in the heading of the robot result in large displacements in the odometric readings it maintains. The cumulative rotational error leads to unreliable odomet-ric readings. In the paper we present solutions to these problems by using a circular distribution and relative coordinate systems. We validate their effectiveness through experimental results from a model-learning application.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> A. R. Cassandra, L. P. Kaelbling, J. A. Kurien. </author> <title> Acting under uncertainty: Discrete Bayesian models for mobile-robot navigation. </title> <booktitle> In Proc. of IEEE/RSJ Int. Conf. on Intelligent Robots and Systems. </booktitle> <year> 1996. </year>
Reference: [2] <author> P. Cheeseman, et al. </author> <title> Autoclass: A Bayesian classification system. </title> <editor> In J. W. Shavlik, T. G. Dietterich, eds., </editor> <booktitle> Readings in Machine Learning. </booktitle> <publisher> Morgan-Kaufmann, </publisher> <year> 1990. </year>
Reference-contexts: It is important to be aware of the need for circular distributions as well as of their existence. Moreover, it would be useful to have widely used applications such as Autoclass <ref> [2] </ref> support such distributions. A problematic aspect of directional data which manifests itself when learning maps and models for robot navigation is that of cumulative rotational errors.
Reference: [3] <author> A. P. Dempster, N. M. Laird, D. B. Rubin. </author> <title> Maximum likelihood from incomplete data via the EM algorithm. </title> <journal> Journal of the Royal Statistical Society, </journal> <volume> 39(1), 138, </volume> <year> 1977. </year>
Reference: [4] <author> F. C. Dyer. </author> <title> Bees acquire route-based memories but not cognitive maps in a familiar landscape. Animal Behaviour, </title> <type> 41, 239246, </type> <year> 1991. </year>
Reference: [5] <author> Z. Ghahramani, M. I. Jordan. </author> <title> Factorial hidden Markov models. </title> <booktitle> In 15 th Int. Conf. on Machine Learning. </booktitle> <year> 1997. </year>
Reference: [6] <author> E. G. Gumbel, J. A. Greenwood, D. Durand. </author> <title> The circular normal distribution: Theory and tables. </title> <journal> American Statistical Society Journal, </journal> <volume> 48, 131152, </volume> <month> March </month> <year> 1953. </year>
Reference-contexts: introduction to the concepts and techniques used for handling directional data. In particular we concentrate on the von Mises distribution a circular version of the normal distribution. Further discussion can be found in the statistical literature <ref> [6, 10, 13] </ref>. <p> Applying this derivation to the normal distribution results in a circular version of the normal distribution, but estimating its parameters from sample data can be hard <ref> [6, 13] </ref>. An easier-to-estimate circular version of the normal distribution was derived, by von Mises [6, 13]. <p> Applying this derivation to the normal distribution results in a circular version of the normal distribution, but estimating its parameters from sample data can be hard <ref> [6, 13] </ref>. An easier-to-estimate circular version of the normal distribution was derived, by von Mises [6, 13].
Reference: [7] <author> D. Heckerman, D. Geiger. </author> <title> Learning Bayesian networks: A unification for discrete and Gaussian domains. </title> <booktitle> In Proc. of the 11 th Int. Conf. on Uncertainty in AI. </booktitle> <year> 1995. </year>
Reference: [8] <author> B. H. Juang. </author> <title> Maximum likelihood estimation for mixture multivariate stochastic observations of Markov chains. </title> <journal> AT&T Technical Journal, </journal> <volume> 64(6), </volume> <month> July-August </month> <year> 1985. </year>
Reference: [9] <author> B. H. Juang, L. R. Rabiner. </author> <title> A probabilistic distance measure for hidden Markov models. </title> <journal> AT&T Technical Journal, </journal> <volume> 64(2), 391408, </volume> <month> February </month> <year> 1985. </year>
Reference-contexts: We report our simulation results in terms of a sampled version of the KL divergence, as described by Juang and Rabiner <ref> [9] </ref>. It is based on generating sequences of sufficient length according to the distribution induced by the true model, and comparing their likelihoods according to the learned model with the true model likelihoods.
Reference: [10] <author> S. Kotz, N. L. Johnson, </author> <title> eds. </title> <journal> Encyclopedia of Statistical Sciences, </journal> <volume> vol. 2, </volume> <pages> pp. 381386. </pages> <publisher> John Wiley and Sons, </publisher> <year> 1982. </year>
Reference-contexts: introduction to the concepts and techniques used for handling directional data. In particular we concentrate on the von Mises distribution a circular version of the normal distribution. Further discussion can be found in the statistical literature <ref> [6, 10, 13] </ref>.
Reference: [11] <author> S. Kullback, R. A. Leibler. </author> <title> On information and sufficiency. </title> <journal> Annals of Mathematical Statistics, </journal> <volume> 22(1), 7986, </volume> <year> 1951. </year>
Reference-contexts: Traditionally, in simulation experiments, learned models are quantitatively compared to the actual model that generated the data. Each of the models induces a probability distribution on strings of observations; the asymmetric Kullback-Leibler divergence <ref> [11] </ref> between the two distributions is a measure of how far the learned model is from the true model. We report our simulation results in terms of a sampled version of the KL divergence, as described by Juang and Rabiner [9].
Reference: [12] <author> F. Lu, E. E. Millios. </author> <title> Globally consistent range scan alignment for environment mapping. </title> <booktitle> Autonomous Robots, </booktitle> <volume> 4, 333349, </volume> <year> 1997. </year>
Reference: [13] <author> K. V. Mardia. </author> <title> Statistics of Directional Data. </title> <publisher> Academic Press, </publisher> <year> 1972. </year>
Reference-contexts: introduction to the concepts and techniques used for handling directional data. In particular we concentrate on the von Mises distribution a circular version of the normal distribution. Further discussion can be found in the statistical literature <ref> [6, 10, 13] </ref>. <p> Applying this derivation to the normal distribution results in a circular version of the normal distribution, but estimating its parameters from sample data can be hard <ref> [6, 13] </ref>. An easier-to-estimate circular version of the normal distribution was derived, by von Mises [6, 13]. <p> Applying this derivation to the normal distribution results in a circular version of the normal distribution, but estimating its parameters from sample data can be hard <ref> [6, 13] </ref>. An easier-to-estimate circular version of the normal distribution was derived, by von Mises [6, 13]. <p> Figure 3 shows an unwrapped plot of the von Mises distribution for various values of k where = 0. We now describe how to estimate the parameters and k given a set of heading samples (angles 1 ; : : : n ) from a von Mises distribution <ref> [13] </ref>. We are looking for maximum likelihood estimates for and k.
Reference: [14] <author> I. Nourbakhsh, R. Powers, S. Birchfield. Dervish: </author> <title> An office-navigating robot. </title> <journal> AI Magazine, </journal> <volume> 16(1), 5360, </volume> <year> 1995. </year>
Reference: [15] <author> L. R. Rabiner. </author> <title> A tutorial on hidden Markov models and selected applications in speech recognition. </title> <journal> Proc. of the IEEE, </journal> <volume> 77(2), 257285, </volume> <month> February </month> <year> 1989. </year>
Reference: [16] <author> H. Shatkay, L. P. Kaelbling. </author> <title> Learning hidden Markov models with geometric information. </title> <type> Tech. Rep. </type> <institution> CS-97-04, Dept. of Computer Science, Brown University, </institution> <year> 1997. </year>
Reference-contexts: Their correctness can be proved along the lines of the proof pro vided in our previous document <ref> [16] </ref>. 4 STATE-RELATIVE COORDINATE SYSTEMS In our previous work we assumed that there is a single global coordinate system within which the robot operates. <p> Their correctness can be proved along the lines of the correctness proofs for all other formulae <ref> [16] </ref>. 5 EXPERIMENTS AND RESULTS The goal of this work is to use odometry to improve the learning of topological models, while using fewer iterations and less data. We tested our algorithm in a simple robot-navigation world.
Reference: [17] <author> H. Shatkay, L. P. Kaelbling. </author> <title> Learning topological maps with weak local odometric information. </title> <booktitle> In Proc. of the 15 th Int. Joint Conf. on AI. </booktitle> <year> 1997. </year>
Reference: [18] <author> R. G. Simmons, S. Koenig. </author> <title> Probabilistic navigation in partially observable environments. </title> <booktitle> In Proc. of the Int. Joint Conf. on AI. </booktitle> <year> 1995. </year>
Reference: [19] <author> R. Smith, M. Self, P. Cheeseman. </author> <title> A stochastic map for uncertain spatial relationships. </title> <editor> In S. S. Iyengar, A. Elfes, eds., </editor> <booktitle> Autonomous Mobile Robots. </booktitle> <publisher> IEEE Press, </publisher> <year> 1991. </year>
Reference: [20] <author> S. Thrun, W. Burgard, D. Fox. </author> <title> A probabilistic approach to concurrent map acquisition and localization for mobile robots. </title> <journal> Machine Learning, </journal> <volume> 31, 2953, </volume> <year> 1998. </year>
References-found: 20

