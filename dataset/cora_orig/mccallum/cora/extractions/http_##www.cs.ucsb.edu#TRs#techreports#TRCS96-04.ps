URL: http://www.cs.ucsb.edu/TRs/techreports/TRCS96-04.ps
Refering-URL: http://www.cs.ucsb.edu/TRs/
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Title: An Eigenspace Update Algorithm for Image Analysis  
Author: S. Chandrasekaran B.S. Manjunath Y.F. Wang J. Winkeler and H. Zhang 
Date: April 18, 1996 9:44 am  
Address: Santa Barbara, CA 93106  
Affiliation: Department of Computer Science Department of Electrical and Computer Engineering University of California,  
Abstract: During the past few years several interesting applications of eigenspace representation of the images have been proposed. These include face recognition, video coding, and pose estimation. However, the vision research community has largely overlooked parallel developments in signal processing and numerical linear algebra concerning efficient eigenspace updating algorithms. These new developments are significant for two reasons: Adopting them will make some of the current vision algorithms more robust and efficient. More important is the fact that incremental updating of eigenspace representations will open up new and interesting research applications in vision such as active recognition and learning. The main objective of this paper is to put these in perspective and discuss a new updating scheme for low numerical rank matrices that can be shown to be numerically stable and fast. A comparison with a non-adaptive SVD scheme shows that our algorithm achieves similar accuracy levels for image reconstruction and recognition at a significantly lower computational cost. We also illustrate applications to adaptive view selection for 3D object representation from projections. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> E. Anderson et al., </author> <note> LAPACK Users Guide, </note> <institution> Society for Industrial and Applied Mathematics, </institution> <year> 1992. </year>
Reference-contexts: Step 3 involves the computation of the SVD of a broken-arrowhead matrix. Using standard dense SVD algorithms (see LAPACK manual <ref> [1] </ref>) this can be done in ops. So the total cost will be flops. <p> The standard algorithms for computing the SVD (see the LAPACK manual <ref> [1] </ref>) and the fast algorithm of [4] are backward stable. Therefore step 3 in our algorithm is numerically stable. The potential source of instability is step 2, where we need to ensure that is numerically perpendicular to .
Reference: [2] <author> P.E. Gill, G.H. Golub, W. Murray, and M.A. </author> <title> Saunders (1974). Methods for Modifying Matrix Factorizations, </title> <journal> Math. Comp. </journal> <volume> 28, </volume> <pages> 505-35 </pages>
Reference-contexts: The idea of SVD updating has been prevalent in signal processing for about two decades. One of the first papers on the numerical issues of updating matrix factorizations appeared in 1974 <ref> [2] </ref>. However, till recently there was no fast and stable updating algorithm for the SVD [4].
Reference: [3] <author> G. H. Golub and C. F. van Loan, </author> <title> Matrix Computations, </title> <publisher> The Johns Hopkins Press, </publisher> <year> 1989. </year>
Reference-contexts: Note that can be reconstructed to accuracy by . That is . The algo rithmic requirement in many applications is to compute efficiently. This can be directly computed from by using standard SVD algorithms (e.g., Golub-Reinsch <ref> [3] </ref>). <p> Some researchers ([8],[14]) have suggested computing SVD by computing the eigende composition of . While this has the same complexity, its numerical properties are not as good <ref> [3] </ref>. Nevertheless, in applications involving a large number of images, computing the SVD of can be too slow. In many situations (as in face recognition, database browsing, video coding, and active recognition) is available and this can be used to speedup the computations. <p> This is the approach taken in [8] but they compute the SVD of by computing the eigendecomposition of . This costs . While more efficient than computing the SVD of , this still suffers from potential numerical instability <ref> [3] </ref>. Murase [10] advocates the use of iterative methods for computing the SVD/eigendecomposition. But as is well known in numerical linear algebra [3] it is dif ficult to get robust implementations of such iterative methods. <p> This costs . While more efficient than computing the SVD of , this still suffers from potential numerical instability <ref> [3] </ref>. Murase [10] advocates the use of iterative methods for computing the SVD/eigendecomposition. But as is well known in numerical linear algebra [3] it is dif ficult to get robust implementations of such iterative methods. In this paper, we propose instead the use of a direct update algorithm to compute the SVD of . This algorithm has good numerical properties, and is as ef ficient as the approach of [8].
Reference: [4] <author> M. Gu and S. C. Eisenstat, </author> <title> A Stable and Fast Algorithm for Updating the Singular Value Decomposition, </title> <note> Research Report YALEU/DCS/RR-966, 1994, </note> <institution> Yale University, </institution> <address> New Haven, CT. </address>
Reference-contexts: The idea of SVD updating has been prevalent in signal processing for about two decades. One of the first papers on the numerical issues of updating matrix factorizations appeared in 1974 [2]. However, till recently there was no fast and stable updating algorithm for the SVD <ref> [4] </ref>. In the context of image analysis in eigenspace, this paper makes the following contributions: We provide a comparison of some of the popular techniques existing in the vision literature for SVD/KLT computations, and point out the problems associated with those techniques. <p> Note that if and are square matrices then we can easily diagonalize the first columns of and obtain a broken arrowhead matrix, whose SVD can be computed quickly using the techniques suggested by Gu and Eisenstat <ref> [4] </ref>. But and are not square matrices. Notice that can be extended by adding the part of the new image which is perpendicular to . It turns out that this is all we need as far as is concerned. <p> Step 3 involves the computation of the SVD of a broken-arrowhead matrix. Using standard dense SVD algorithms (see LAPACK manual [1]) this can be done in ops. So the total cost will be flops. If we use the fast stable algorithm of Gu and Eisenstat <ref> [4] </ref> for computing the SVD of broken-arrowhead matrices the cost can be further reduced to . (But the overheads in the implementation may make this worthwhile only for bigger than 100.) S k i T 0 a ^ A i 1+ T k i 1+( ) k i 1+( ) U <p> Again, this can be speeded up to ops using the fast and stable algorithm, outlined in <ref> [4] </ref>. Thus the total cost is ops or ops. For step 5 the cost is ops if done in a straightforward manner, and ops using the technique outlined in [4]. The total cost is either ops or ops respectively. <p> Again, this can be speeded up to ops using the fast and stable algorithm, outlined in <ref> [4] </ref>. Thus the total cost is ops or ops. For step 5 the cost is ops if done in a straightforward manner, and ops using the technique outlined in [4]. The total cost is either ops or ops respectively. So we see that the dominant cost of the algorithm is the matrix multiplications. If they are done in a straightforward manner the total cost of the algorithm is flops. <p> If they are done in a straightforward manner the total cost of the algorithm is flops. This should be compared with the cost of computing the SVD of once, ops. If we use the fast and stable algorithm of <ref> [4] </ref>, the total number of ops can be reduced to , though this will be useful only if is large enough. <p> The standard algorithms for computing the SVD (see the LAPACK manual [1]) and the fast algorithm of <ref> [4] </ref> are backward stable. Therefore step 3 in our algorithm is numerically stable. The potential source of instability is step 2, where we need to ensure that is numerically perpendicular to . If is very small, then as computed may no longer be numerically perpendicular to .
Reference: [5] <author> H. Hotelling, </author> <title> Analysis of a complex of statistical variables into principal components, </title> <journal> J. Educ. </journal> <volume> Psychology Vol. 24, </volume> <pages> pp. 417-441 and pp. 448-520, </pages> <year> 1933. </year>
Reference: [6] <author> A. Jain, </author> <title> Fundamentals of Digital Image Processing, </title> <publisher> Prentice Hall 1989. </publisher>
Reference: [7] <author> B. S. Manjunath, S. Chandrasekaran, and Y. F. Wang, </author> <title> Learning in eigenspace: theory and application, </title> <institution> CIPR-TR-94-17, ECE Dept., UCSB, </institution> <month> November </month> <year> 1994. </year>
Reference: [8] <author> H. Murakami and B. V. K. V. Kumar, </author> <title> Efficient calculation of primary images from a set of images, </title> <journal> IEEE T-PAMI, </journal> <volume> Vol. 4, No. 5, </volume> <month> September </month> <year> 1982, </year> <pages> pp. 511-515. </pages>
Reference-contexts: In many situations (as in face recognition, database browsing, video coding, and active recognition) is available and this can be used to speedup the computations. W e can approxi mately compute by computing the SVD of . This is the approach taken in <ref> [8] </ref> but they compute the SVD of by computing the eigendecomposition of . This costs . While more efficient than computing the SVD of , this still suffers from potential numerical instability [3]. Murase [10] advocates the use of iterative methods for computing the SVD/eigendecomposition. <p> In this paper, we propose instead the use of a direct update algorithm to compute the SVD of . This algorithm has good numerical properties, and is as ef ficient as the approach of <ref> [8] </ref>. Moreover , for data sets with large , a fast version of the algorithm with time complexity is available. Table 1 summarizes the algorithmic dif ferences among some previous work in vision and ours.
Reference: [9] <author> H. Murase and S. K. Nayar, </author> <title> Visual learning of object modules from appearance, </title> <booktitle> Proc. Image Understanding Workshop 1993, </booktitle> <address> (San Diego, CA), </address> <pages> pp. 547-555, </pages> <year> 1993. </year>
Reference-contexts: This is born out by the fact these techniques have been rediscovered several times in the vision literature. There are several other papers concerning various pattern recognition applications of eigenspace representations (for example, <ref> [9] </ref>, Ojas book on subspace methods [11]), but are not very relevant to our discussion here. <p> the accuracy in image reconstruction to that of the complete SVD and demonstrates that similar performance can be obtained at a fraction of computational cost. 3 Applications Previous research has already demonstrated that the eigenspace approach is a powerful tool in recognition and pose estimation of objects from image projections <ref> [9] </ref>, [14], [13]. Our objective is to illustrate the efficacy and efficiency of the incremental eigenspace updating algorithm over the traditional batch algorithm. While the batch algorithm is in some sense the best-case, it is computationally expensive.
Reference: [10] <author> H. Murase and M. Lindenbaum, </author> <title> Partial eigenvalue decomposition of large images using the spatial temporal adaptive method, </title> <journal> IEEE T-IP, </journal> <volume> Vol. 4 (5), </volume> <month> May </month> <year> 1995, </year> <pages> pp. 620-629. </pages>
Reference-contexts: This is the approach taken in [8] but they compute the SVD of by computing the eigendecomposition of . This costs . While more efficient than computing the SVD of , this still suffers from potential numerical instability [3]. Murase <ref> [10] </ref> advocates the use of iterative methods for computing the SVD/eigendecomposition. But as is well known in numerical linear algebra [3] it is dif ficult to get robust implementations of such iterative methods.
Reference: [11] <author> E. Oja, </author> <title> Subspace methods of pattern recognition, </title> <publisher> John Wiley, </publisher> <year> 1983. </year> <month> 15 </month>
Reference-contexts: This is born out by the fact these techniques have been rediscovered several times in the vision literature. There are several other papers concerning various pattern recognition applications of eigenspace representations (for example, [9], Ojas book on subspace methods <ref> [11] </ref>), but are not very relevant to our discussion here.
Reference: [12] <author> A. Pentland, B. Moghaddam, and T. Starner, </author> <title> View-based and modular eigenspaces for face recognition, </title> <booktitle> Proc. IEEE Conf. </booktitle> <address> CVPR 94, (Seattle, Washington), </address> <pages> pp. 84-91, </pages> <month> June </month> <year> 1994. </year>
Reference: [13] <author> L. Sirovich and M. Kirby, </author> <title> Low dimensional procedure for the characterization of human faces, </title> <journal> Journal of Optical Society of America, </journal> <volume> Vol. 4, No. 3, </volume> <pages> pp. 519-524, </pages> <year> 1987. </year>
Reference-contexts: Let us consider the following scenario: A camera is mounted on a robot which explores a 3D object by viewing it from different angles, and builds an internal representation in terms of image projections. This is a slightly dif ferent formulation from the face recognition problem introduced in <ref> [13] </ref> and later made popular by [14]. In all these cases, we need to be able to recognize an object from its projections only. <p> in image reconstruction to that of the complete SVD and demonstrates that similar performance can be obtained at a fraction of computational cost. 3 Applications Previous research has already demonstrated that the eigenspace approach is a powerful tool in recognition and pose estimation of objects from image projections [9], [14], <ref> [13] </ref>. Our objective is to illustrate the efficacy and efficiency of the incremental eigenspace updating algorithm over the traditional batch algorithm. While the batch algorithm is in some sense the best-case, it is computationally expensive.
Reference: [14] <author> M. Turk and A. Pentland, </author> <title> Eigenfaces for Recognition, </title> <journal> Journal of Cognitive Neuroscience, </journal> <volume> Vol. 3, No. 1, </volume> <pages> pp. </pages> <address> 71- 86, </address> <month> March </month> <year> 1991. </year>
Reference-contexts: This is a slightly dif ferent formulation from the face recognition problem introduced in [13] and later made popular by <ref> [14] </ref>. In all these cases, we need to be able to recognize an object from its projections only. We assume that image data are directly used in building a representation, but the formulation is valid for any set of image features extracted from the image data. <p> accuracy in image reconstruction to that of the complete SVD and demonstrates that similar performance can be obtained at a fraction of computational cost. 3 Applications Previous research has already demonstrated that the eigenspace approach is a powerful tool in recognition and pose estimation of objects from image projections [9], <ref> [14] </ref>, [13]. Our objective is to illustrate the efficacy and efficiency of the incremental eigenspace updating algorithm over the traditional batch algorithm. While the batch algorithm is in some sense the best-case, it is computationally expensive.

References-found: 14

