URL: ftp://ftp.cag.lcs.mit.edu/pub/papers/two-case-delivery.ps.Z
Refering-URL: http://www.cag.lcs.mit.edu/alewife/papers/two-case-delivery.html
Root-URL: 
Email: fkenmac,kubitron,mfrank,walt,wklee,agarwal,kaashoekg@cag.lcs.mit.edu  
Title: Exploiting Two-Case Delivery for Fast Protected Messaging  
Author: Kenneth Mackenzie John Kubiatowicz Matthew Frank, Walter Lee, Victor Lee Anant Agarwal and M. Frans Kaashoek 
Address: Cambridge, MA 02139  
Affiliation: Laboratory for Computer Science Massachusetts Institute of Technology  
Date: February, 1998  
Note: To appear in the Proceedings of the Fourth International Symposium on High-Performance Computer Architecture,  
Abstract: We propose and evaluate two complementary techniques to protect and virtualize a tightly-coupled network interface in a multicom-puter. The techniques allow efficient, direct application access to network hardware in a multiprogrammed environment while gaining most of the benefits of a memory-based network interface. First, two-case delivery allows an application to receive a message directly from the network hardware in ordinary circumstances, but provides buffering transparently when required for protection. Second, virtual buffering stores messages in virtual memory on demand, providing the convenience of effectively unlimited buffer capacity while keeping actual physical memory consumption low. The evaluation is based on workloads of real and synthetic applications running on a simulator and partly on emulated hardware. The results show that the direct path is also the common path, justifying the use of software buffering. Further results show that physical buffering requirements remain low in our applications despite the use of unacknowledged messages and despite adverse scheduling conditions. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Anant Agarwal, Ricardo Bianchini, David Chaiken, Kirk Johnson, David Kranz, John Kubiatowicz, Beng-Hong Lim, Kenneth Macken-zie, and Donald Yeung. </author> <title> The MIT Alewife Machine: Architecture and Performance. </title> <booktitle> In Proceedings of the 22nd Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 213, </pages> <month> June </month> <year> 1995. </year>
Reference-contexts: User-level code thus enjoys the same level of control over the interface as an in-kernel device driver. We have implemented our techniques in the FUGU multiprocessor system [21, 22]. The FUGU hardware is based on extensions to the (single-user) Alewife machine <ref> [1] </ref>. We have constructed both a fast simulator and emulated hardware platforms. The emulator extends the Alewife chipset with a modified Cache and Memory Management Controller (CMMU) gate array and an auxiliary FPGA that implement protection and virtualization features for the direct delivery case. <p> Section 6 concludes. 2 Related Work Recent architectures demonstrate emerging agreement that it is important to provide support for efficient, fine-grain message passing, even in conjunction with hardware support for shared memory <ref> [1, 2, 13, 18, 27, 30] </ref>. The trend in message interfaces has been to reduce end-to-end overhead by providing user access to the interface hardware. We build on previous work in messaging models and mechanisms. Model. <p> A polling watchdog mode could be implemented in the FUGU system. Direct Network Interfaces. Several machines have provided direct network interfaces. These include the CM-5, the J-machine, iWarp, the *T interface, Alewife, and Wisconsin's CNI <ref> [20, 8, 5, 26, 1, 24] </ref>. These interfaces feature low latency by allowing the processor direct access to the network queue. Direct NIs can be inefficient unless placed close to the processor. Anticipating continued system integration, we place our NI on the processor-cache bus.
Reference: [2] <author> Boon S. Ang, Derek Chiou, Larry Rudolph, and Arvind. </author> <title> Message Passing Support on StartT-Voyager. CSG Memo 387, MIT, Computation Structures Group, </title> <address> Cambridge, MA, </address> <month> July </month> <year> 1996. </year>
Reference-contexts: Section 6 concludes. 2 Related Work Recent architectures demonstrate emerging agreement that it is important to provide support for efficient, fine-grain message passing, even in conjunction with hardware support for shared memory <ref> [1, 2, 13, 18, 27, 30] </ref>. The trend in message interfaces has been to reduce end-to-end overhead by providing user access to the interface hardware. We build on previous work in messaging models and mechanisms. Model. <p> The hardware requirements are kept minimal for on-chip implementation and amount to a small, single message queue and a simple DMA engine. The *T-Voyager system <ref> [2] </ref> represents an intermediate hybrid. In *T-Voyager, the network interface demultiplexes incoming messages into several moderate-sized hardware queues. The multi-2 ple queues allow multiple applications to be active simultaneously. Like UDM, *T-Voyager overflows its queues to memory if necessary.
Reference: [3] <author> R. Arpaci, A. Dusseau, A. Vahdat, L. Liu, T. Anderson, and D. Patter son. </author> <title> The Interaction of Parallel and Sequential Workloads on a Network of Workstations. </title> <booktitle> In Proceedings of the Joint International Conference on Measurement and Modeling of Computer Systems, </booktitle> <pages> pages 267278, </pages> <month> May </month> <year> 1995. </year>
Reference-contexts: Increased integration of computer systems and the mainstreaming of parallel processing challenges both of these assumptions: on-chip network interfaces can have low overhead and parallel programs frequently require coordinated scheduling for predictable, low latencies <ref> [3] </ref>. FUGU provides low latency for applications where latency matters while including low-cost and reasonably efficient buffering as a fallback mode. Bulk data transfers are handled by a separate direct memory access (DMA) mechanism in FUGU [21]. Hybrids.
Reference: [4] <author> Matthias A. Blumrich, Kai Li, Richard Alpert, Cezary Dubnicki, Ed ward W. Felten, and Jonathan Sandberg. </author> <title> Virtual Memory Mapped Network Interface for the SHRIMP Multicomputer. </title> <booktitle> In Proceedings 21st Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 142153, </pages> <month> April </month> <year> 1994. </year>
Reference-contexts: The *T NI [26] would have included GID checks and a timeout on message handling for protection as in FUGU. The M-machine [12] receives messages with a trusted handler that has the ability to quickly forward the message body to a user thread. Memory-Based Interfaces. Memory-based interfaces in multi-computers <ref> [4, 7, 29, 30, 32] </ref> and workstations [9, 11, 33, 34] provide easy protection for multiprogramming if the NI also demultiplexes messages into per-process buffers.
Reference: [5] <author> S. Borkar, R. Cohn, G. Cox, T. Gross, H.T. Kung, M. Lam, M. Levine, B. Moore, W. Moore, C. Peterson, J. Susman, J. Sutton, J. Urbanski, and J. Webb. </author> <title> Supporting Systolic and Memory Communication in iWarp. </title> <booktitle> In Proceedings of the 17th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 7081, </pages> <month> June </month> <year> 1990. </year> <month> 11 </month>
Reference-contexts: A polling watchdog mode could be implemented in the FUGU system. Direct Network Interfaces. Several machines have provided direct network interfaces. These include the CM-5, the J-machine, iWarp, the *T interface, Alewife, and Wisconsin's CNI <ref> [20, 8, 5, 26, 1, 24] </ref>. These interfaces feature low latency by allowing the processor direct access to the network queue. Direct NIs can be inefficient unless placed close to the processor. Anticipating continued system integration, we place our NI on the processor-cache bus.
Reference: [6] <author> Eric Brewer, Fred Chong, Lok Liu, Shamik Sharma, and John Kubia towicz. </author> <title> Remote Queues: Exposing Message Queues for Optimization and Atomicity. </title> <booktitle> In Proceedings of the Symposium on Parallel Algorithms and Architectures, </booktitle> <year> 1995. </year>
Reference-contexts: The trend in message interfaces has been to reduce end-to-end overhead by providing user access to the interface hardware. We build on previous work in messaging models and mechanisms. Model. The UDM model is similar to Active Messages [35] and related to Remote Queues (RQ) <ref> [6] </ref> as an efficient building-block for messaging within a protection domain. UDM differs from Active Messages in that it includes explicit control over message delivery for efficiency.
Reference: [7] <author> Greg Buzzard, David Jacobson, Milon Mackey, Scott Marovich, and John Wilkes. </author> <title> An Implementation of the Hamlyn Sender-Managed Interface Architecture. </title> <booktitle> In Proceedings of the Second Symposium on Operating System Design and Implementation, </booktitle> <pages> pages 245259, </pages> <year> 1996. </year>
Reference-contexts: The *T NI [26] would have included GID checks and a timeout on message handling for protection as in FUGU. The M-machine [12] receives messages with a trusted handler that has the ability to quickly forward the message body to a user thread. Memory-Based Interfaces. Memory-based interfaces in multi-computers <ref> [4, 7, 29, 30, 32] </ref> and workstations [9, 11, 33, 34] provide easy protection for multiprogramming if the NI also demultiplexes messages into per-process buffers.
Reference: [8] <author> William J. Dally et al. </author> <title> The J-Machine: A Fine-Grain Concurrent Com puter. </title> <booktitle> In Proceedings of the IFIP (International Federation for Information Processing), 11th World Congress, </booktitle> <pages> pages 11471153, </pages> <address> New York, 1989. </address> <publisher> Elsevier Science Publishing. </publisher>
Reference-contexts: A polling watchdog mode could be implemented in the FUGU system. Direct Network Interfaces. Several machines have provided direct network interfaces. These include the CM-5, the J-machine, iWarp, the *T interface, Alewife, and Wisconsin's CNI <ref> [20, 8, 5, 26, 1, 24] </ref>. These interfaces feature low latency by allowing the processor direct access to the network queue. Direct NIs can be inefficient unless placed close to the processor. Anticipating continued system integration, we place our NI on the processor-cache bus. <p> The second network is used infrequently for this purpose so its performance is not critical. The network might be shared with some other use, such as supporting shared memory. An extra virtual channel <ref> [8] </ref> in the main network, a LAN or a service network would serve the purpose.
Reference: [9] <author> Chris Dalton, Greg Watson, David Banks, Costas Calamvokis, Aled Edwards, and John Lumley. </author> <title> Afterburner. </title> <journal> IEEE Network, </journal> <pages> pages 36 43, </pages> <month> July </month> <year> 1993. </year>
Reference-contexts: The M-machine [12] receives messages with a trusted handler that has the ability to quickly forward the message body to a user thread. Memory-Based Interfaces. Memory-based interfaces in multi-computers [4, 7, 29, 30, 32] and workstations <ref> [9, 11, 33, 34] </ref> provide easy protection for multiprogramming if the NI also demultiplexes messages into per-process buffers. Automatic hardware buffering also deals well with sinking bursts of messages and provides the lowest overhead (by avoiding the processors) when messages are not handled immediately.
Reference: [10] <author> Peter Druschel and Larry L. Peterson. Fbufs: </author> <title> A High-Bandwidth Cross-Domain Transfer Facility. </title> <booktitle> In Proceedings of the Fourteenth ACM Symposium on Operating System Principles, </booktitle> <pages> pages 189202, </pages> <month> December </month> <year> 1993. </year>
Reference-contexts: The Active Message implementation in SUNMOS [28] on the Intel Paragon uses kernel code to unload the message interface and to queue messages to be handled by a user thread. The SUNMOS approach corresponds to using the software-buffered path in UDM continuously. Fbufs <ref> [10] </ref> are an operating-system construct used to efficiently feed streams of data across protection domains. The UDM virtual buffering system employs similar techniques in a specialized implementation to manage its buffer memory. 3 UDM Model This section describes the UDM model in abstract terms. The model has two goals.
Reference: [11] <author> Peter Druschel, Larry L. Peterson, and Bruce S. Davie. </author> <title> Experiences with a High-Speed Network Adaptor: A Software Perspective. </title> <booktitle> In Proceedings of the Conference on Communication Architectures, Protocols and Applications, </booktitle> <pages> pages 213, </pages> <year> 1994. </year>
Reference-contexts: The M-machine [12] receives messages with a trusted handler that has the ability to quickly forward the message body to a user thread. Memory-Based Interfaces. Memory-based interfaces in multi-computers [4, 7, 29, 30, 32] and workstations <ref> [9, 11, 33, 34] </ref> provide easy protection for multiprogramming if the NI also demultiplexes messages into per-process buffers. Automatic hardware buffering also deals well with sinking bursts of messages and provides the lowest overhead (by avoiding the processors) when messages are not handled immediately.
Reference: [12] <author> Marco Fillo, Stephen W. Keckler, W.J. Dally, Nicholas P. Carter, An drew Chang, Yevgeny Gurevich, and Whay S. Lee. </author> <title> The M-Machine Multicomputer. </title> <booktitle> In Proceedings of the 28th Annual International Symposium on Microarchitecture, </booktitle> <pages> pages 146156. </pages> <publisher> IEEE Computer Society, </publisher> <month> November </month> <year> 1995. </year>
Reference-contexts: The CM-5 provides restricted multiprogramming by strict gang scheduling and by context-switching the network with the processors. The *T NI [26] would have included GID checks and a timeout on message handling for protection as in FUGU. The M-machine <ref> [12] </ref> receives messages with a trusted handler that has the ability to quickly forward the message body to a user thread. Memory-Based Interfaces.
Reference: [13] <author> John Heinlein, Kourosh Gharachorloo, Scott Dresser, and Anoop Gupta. </author> <title> Integration of Message Passing and Shared Memory in the Stanford FLASH Multiprocessor. </title> <booktitle> In Sixth International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pages 3850, </pages> <month> October </month> <year> 1994. </year>
Reference-contexts: Section 6 concludes. 2 Related Work Recent architectures demonstrate emerging agreement that it is important to provide support for efficient, fine-grain message passing, even in conjunction with hardware support for shared memory <ref> [1, 2, 13, 18, 27, 30] </ref>. The trend in message interfaces has been to reduce end-to-end overhead by providing user access to the interface hardware. We build on previous work in messaging models and mechanisms. Model. <p> Using virtual memory is particularly natural when the processor initiates all the buffering because existing support for virtual memory (e.g. the processor's TLB) is reused. It requires a relatively complex DMA engine or coprocessor to manipulate virtual memory independently <ref> [13, 25, 29] </ref>. Buffering adds a performance cost when used. The buffered path introduces two components of overhead over the fast path. First, there is an extra copy operation: an operating system handler must copy the message from the network interface to memory.
Reference: [14] <author> Dana S. Henry and Christopher F. Joerg. </author> <title> A Tightly-Coupled Processor-Network Interface. </title> <booktitle> In Fifth International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <month> October </month> <year> 1992. </year>
Reference-contexts: Although a definitive conclusion awaits further research, past research indicates that direct interfaces tend to be more efficient than memory-based interfaces. Direct interfaces that can be accessed at cache speeds offer even better performance <ref> [14] </ref>.
Reference: [15] <author> Kirk L. Johnson, M. Frans Kaashoek, and Deborah A. Wallach. </author> <title> CRL: High-Performance All-Software Distributed Shared Memory. </title> <booktitle> In Proceedings of the Fifteenth ACM Symposium on Operating Systems Principles, </booktitle> <month> December </month> <year> 1995. </year>
Reference-contexts: Three applications, LU, Water and Barnes come from the SPLASH [31] suite and are slightly modified to make use of the CRL all-software shared-memory system <ref> [15] </ref>. CRL presents a message-passing load that is representative of coherence protocols such as Stache [24] and can be considered operating-system-like: many low-latency request-reply packets mixed with fewer larger data packets. A fourth application, enum, is a fine-grain, data-parallel application that exchanges numerous unacknowledged short messages and synchronizes only infrequently.
Reference: [16] <author> M. Frans Kaashoek, Dawson R. Engler, Gregory R. Ganger, Hector M. Briceno, Russell Hunt, David Mazieres, Thomas Pinckney, Robert Grimm, John Jannotti, and Kenneth Mackenzie. </author> <title> Application Performance and Flexibility on Exokernel Systems. </title> <booktitle> In Proceedings of the Sixteenth ACM Symposium on Operating Systems Principles, </booktitle> <pages> pages 5265, </pages> <month> December </month> <year> 1997. </year>
Reference-contexts: The emulator extends the Alewife chipset with a modified Cache and Memory Management Controller (CMMU) gate array and an auxiliary FPGA that implement protection and virtualization features for the direct delivery case. The FUGU operating system, called Glaze, is a custom, multiuser operating system based on the Exokernel <ref> [16] </ref> and supports virtual buffering. The new hardware, with the modified CMMU and the FPGA, has been implemented and currently runs a subset of our applications on two nodes. Most of our results are from the simulator. We performed experiments using a multiprogrammed workload of real and synthetic benchmarks. <p> The GID stamp and check and the divert-mode bit are also currently implemented in software. The FUGU operating system, Glaze, is a custom multiuser operating system based on the Aegis Exokernel <ref> [16] </ref>. The operating system supports multiprogramming, virtual memory, messages and 3 Compared to measurements on the hardware, the simulator reports cycle counts within +/-20%. user-level threads.
Reference: [17] <author> John Kubiatowicz and Anant Agarwal. </author> <title> Anatomy of a Message in the Alewife Multiprocessor. </title> <booktitle> In Proceedings of the International Supercomputing Conference, </booktitle> <month> July </month> <year> 1993. </year>
Reference-contexts: The injection operation is atomic in that messages are committed to the network in their entirety; no partial packets are ever seen by the communication substrate <ref> [17] </ref>. Message injection can thus be viewed in the following fashion: inject (header, handler, word0, word1, . . . ) If resource contention prevents the network from accepting a given message, the corresponding inject operation blocks until successful. <p> Further discussion of buffering is deferred to Section 4.2. Send and Receive. The inject operation of the abstract model is decomposed into a two-phase process of describe and launch, as in <ref> [17] </ref>.
Reference: [18] <author> John D. Kubiatowicz. </author> <title> Integrated Message-Passing and Shared Memory Communication in the Alewife Multiprocessor. </title> <type> PhD thesis, </type> <institution> Massachusetts Institute of Technology, Department of Electrical Engineering and Computer Science, </institution> <month> February </month> <year> 1998. </year>
Reference-contexts: Section 6 concludes. 2 Related Work Recent architectures demonstrate emerging agreement that it is important to provide support for efficient, fine-grain message passing, even in conjunction with hardware support for shared memory <ref> [1, 2, 13, 18, 27, 30] </ref>. The trend in message interfaces has been to reduce end-to-end overhead by providing user access to the interface hardware. We build on previous work in messaging models and mechanisms. Model.
Reference: [19] <author> Walter Lee, Matthew Frank, Victor Lee, Kenneth Mackenzie, and Larry Rudolph. </author> <title> Implications of I/O for Gang Scheduled Workloads. In Workshop on Parallel Job Scheduling, </title> <booktitle> IPPS '97. </booktitle> <publisher> Springer Verlag, </publisher> <year> 1997. </year>
Reference-contexts: Glaze implements the UDM model including virtual buffering used in response to GID mismatches and page faults 4 , although message timeouts are currently fatal. The system scheduler, implemented as a user-level server, supports loose gang scheduling with synchronized clocks <ref> [19] </ref>. Overflow control is implemented partly cooperatively in a user library. Glaze, the scheduler and the benchmarks and synthetic applications described in the next section are all functional on the fast simulator. The simulated system includes eight processors. The scheduler timeslice is set at 500,000 cycles.
Reference: [20] <author> Charles E. Leiserson, Aahil S. Abuhamdeh, and David C. Douglas et al. </author> <title> The Network Architecture of the Connection Machine CM-5. </title> <booktitle> In Proceedings of the Fourth Annual ACM Symposium on Parallel Algorithms and Architectures, </booktitle> <year> 1992. </year>
Reference-contexts: A polling watchdog mode could be implemented in the FUGU system. Direct Network Interfaces. Several machines have provided direct network interfaces. These include the CM-5, the J-machine, iWarp, the *T interface, Alewife, and Wisconsin's CNI <ref> [20, 8, 5, 26, 1, 24] </ref>. These interfaces feature low latency by allowing the processor direct access to the network queue. Direct NIs can be inefficient unless placed close to the processor. Anticipating continued system integration, we place our NI on the processor-cache bus.
Reference: [21] <author> Kenneth Mackenzie, John Kubiatowicz, Anant Agarwal, and M. Frans Kaashoek. FUGU: </author> <title> Implementing Protection and Virtual Memory in a Multiuser, Multimodel Multiprocessor. </title> <note> Technical Memo MIT/LCS/TM-503, </note> <month> October </month> <year> 1994. </year>
Reference-contexts: User-level code thus enjoys the same level of control over the interface as an in-kernel device driver. We have implemented our techniques in the FUGU multiprocessor system <ref> [21, 22] </ref>. The FUGU hardware is based on extensions to the (single-user) Alewife machine [1]. We have constructed both a fast simulator and emulated hardware platforms. <p> FUGU provides low latency for applications where latency matters while including low-cost and reasonably efficient buffering as a fallback mode. Bulk data transfers are handled by a separate direct memory access (DMA) mechanism in FUGU <ref> [21] </ref>. Hybrids. Our UDM implementation employs both a direct interface for speed and provides buffering for convenience. Other projects share the same goals. Figure 1 gives a schematic view of the different approaches to message delivery. Parts (a) and (b) show direct and memory-based delivery, respectively. <p> The space-available register, used to implement injectc, reflects the number of send buffer words that may be written without blocking. The buffer in our implementation is limited to 16 words; larger messages utilize an associated user-level DMA mechanism <ref> [21] </ref> which is beyond the scope of this paper. Once the message has been completely described, it is guaranteed that the network will accept it. At that point, the message is injected into the network with an atomic launch instruction whose operand reflects the length of the message.
Reference: [22] <author> Kenneth M. Mackenzie. </author> <title> The FUGU Scalable Workstation: Architec ture and Performance. </title> <type> PhD thesis, </type> <institution> Massachusetts Institute of Technology, Department of Electrical Engineering and Computer Science, </institution> <month> February </month> <year> 1998. </year>
Reference-contexts: User-level code thus enjoys the same level of control over the interface as an in-kernel device driver. We have implemented our techniques in the FUGU multiprocessor system <ref> [21, 22] </ref>. The FUGU hardware is based on extensions to the (single-user) Alewife machine [1]. We have constructed both a fast simulator and emulated hardware platforms.
Reference: [23] <author> Olivier Maquelin, Guang R. Gao, Herbert H. J. Hum, Kevin Theobald, and Xin-Min Tian. </author> <title> Polling Watchdog: Combining Polling and Interrupts for Efficient Message Handling. </title> <booktitle> In Proceedings of the 23rd Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 179 188, </pages> <month> May </month> <year> 1996. </year>
Reference-contexts: We present the details of that hardware atomicity mechanism here in the context of FUGU. Like RQ, UDM depends on buffering to avoid deadlock rather than on explicit request and reply networks. The Polling Watchdog <ref> [23] </ref> integrates polling and interrupts for performance improvement. The resulting programming model is interrupt-based in that application code may receive an interrupt at any point; the application cannot rely on atomicity implicit in a polling model.
Reference: [24] <author> Shubhendu S. Mukherjee, Babak Falsafi, Mark D. Hill, and David A. Wood. </author> <title> Coherent Network Interfaces for Fine-Grain Communication. </title> <booktitle> In Proceedings of the 23rd International Symposium on Computer Architecture, </booktitle> <pages> pages 247258, </pages> <month> May </month> <year> 1996. </year>
Reference-contexts: Although a definitive conclusion awaits further research, past research indicates that direct interfaces tend to be more efficient than memory-based interfaces. Direct interfaces that can be accessed at cache speeds offer even better performance [14]. For example, the CNI paper <ref> [24] </ref> fl Current affiliation: Georgia Institute of Technology, Atlanta, GA 30332 y Current affiliation: University of California at Berkeley, Berkeley CA 94720 z Current affiliation: Intel Corporation, Santa Clara, CA 95052 showed that a direct, cache-level interface exhibited 50% higher bandwidth than their best interface placed on the memory bus. <p> A polling watchdog mode could be implemented in the FUGU system. Direct Network Interfaces. Several machines have provided direct network interfaces. These include the CM-5, the J-machine, iWarp, the *T interface, Alewife, and Wisconsin's CNI <ref> [20, 8, 5, 26, 1, 24] </ref>. These interfaces feature low latency by allowing the processor direct access to the network queue. Direct NIs can be inefficient unless placed close to the processor. Anticipating continued system integration, we place our NI on the processor-cache bus. <p> Other projects share the same goals. Figure 1 gives a schematic view of the different approaches to message delivery. Parts (a) and (b) show direct and memory-based delivery, respectively. Parts (c) and (d) depict hybrid schemes with different approaches. The memory based CNI interface (CNI 16 Q m ) <ref> [24, 25] </ref> provides both a fast path and a (potentially virtual) buffered path by using the network interface to buffer messages. This approach is hardware-intensive, for instance requiring a duplicate translation cache in the network interface. <p> Three applications, LU, Water and Barnes come from the SPLASH [31] suite and are slightly modified to make use of the CRL all-software shared-memory system [15]. CRL presents a message-passing load that is representative of coherence protocols such as Stache <ref> [24] </ref> and can be considered operating-system-like: many low-latency request-reply packets mixed with fewer larger data packets. A fourth application, enum, is a fine-grain, data-parallel application that exchanges numerous unacknowledged short messages and synchronizes only infrequently.
Reference: [25] <author> Shubhendu S. Mukherjee and Mark D. Hill. </author> <title> A Survey of User-Level Network Interfaces for System Area Networks. </title> <type> Technical Report 1340, </type> <institution> Computer Sciences Dept., University of Wisconsin, </institution> <month> February </month> <year> 1997. </year>
Reference-contexts: Other projects share the same goals. Figure 1 gives a schematic view of the different approaches to message delivery. Parts (a) and (b) show direct and memory-based delivery, respectively. Parts (c) and (d) depict hybrid schemes with different approaches. The memory based CNI interface (CNI 16 Q m ) <ref> [24, 25] </ref> provides both a fast path and a (potentially virtual) buffered path by using the network interface to buffer messages. This approach is hardware-intensive, for instance requiring a duplicate translation cache in the network interface. <p> Using virtual memory is particularly natural when the processor initiates all the buffering because existing support for virtual memory (e.g. the processor's TLB) is reused. It requires a relatively complex DMA engine or coprocessor to manipulate virtual memory independently <ref> [13, 25, 29] </ref>. Buffering adds a performance cost when used. The buffered path introduces two components of overhead over the fast path. First, there is an extra copy operation: an operating system handler must copy the message from the network interface to memory.
Reference: [26] <author> Gregory M. Papadopoulos, G. Andy Boughton, Robert Greiner, and Michael J. Beckerle. </author> <title> *T: Integrated Building Blocks for Parallel Computing. </title> <booktitle> In Supercomputing '93, </booktitle> <pages> pages 624635, </pages> <month> November </month> <year> 1993. </year>
Reference-contexts: A polling watchdog mode could be implemented in the FUGU system. Direct Network Interfaces. Several machines have provided direct network interfaces. These include the CM-5, the J-machine, iWarp, the *T interface, Alewife, and Wisconsin's CNI <ref> [20, 8, 5, 26, 1, 24] </ref>. These interfaces feature low latency by allowing the processor direct access to the network queue. Direct NIs can be inefficient unless placed close to the processor. Anticipating continued system integration, we place our NI on the processor-cache bus. <p> Direct interface designs have mostly ignored issues of multiprogramming and demand paging. The CM-5 provides restricted multiprogramming by strict gang scheduling and by context-switching the network with the processors. The *T NI <ref> [26] </ref> would have included GID checks and a timeout on message handling for protection as in FUGU. The M-machine [12] receives messages with a trusted handler that has the ability to quickly forward the message body to a user thread. Memory-Based Interfaces.
Reference: [27] <author> Steve K. Reinhardt, James R. Larus, and David A. Wood. Tempest and Typhoon: </author> <title> User-Level Shared Memory. </title> <booktitle> In Proceedings of the 21st Annual International Symposium on Computer Architecture, </booktitle> <month> April </month> <year> 1994. </year>
Reference-contexts: Section 6 concludes. 2 Related Work Recent architectures demonstrate emerging agreement that it is important to provide support for efficient, fine-grain message passing, even in conjunction with hardware support for shared memory <ref> [1, 2, 13, 18, 27, 30] </ref>. The trend in message interfaces has been to reduce end-to-end overhead by providing user access to the interface hardware. We build on previous work in messaging models and mechanisms. Model.
Reference: [28] <author> Rolf Riesen, Arthur B. Maccabe, and Stephen R. Wheat. </author> <title> Split-C and Active Messages under SUNMOS on the Intel Paragon. </title> <type> Unpublished, </type> <month> April </month> <year> 1994. </year>
Reference-contexts: The multi-2 ple queues allow multiple applications to be active simultaneously. Like UDM, *T-Voyager overflows its queues to memory if necessary. Techniques used in our virtual buffering system are related to several other systems. The Active Message implementation in SUNMOS <ref> [28] </ref> on the Intel Paragon uses kernel code to unload the message interface and to queue messages to be handled by a user thread. The SUNMOS approach corresponds to using the software-buffered path in UDM continuously.
Reference: [29] <author> Klaus E. Schauser and Chris J. Scheiman. </author> <title> Experience with Active Messages on the Meiko CS-2. </title> <booktitle> In Proceedings of the 9th International Symposium on Parallel Processing, </booktitle> <year> 1995. </year>
Reference-contexts: The *T NI [26] would have included GID checks and a timeout on message handling for protection as in FUGU. The M-machine [12] receives messages with a trusted handler that has the ability to quickly forward the message body to a user thread. Memory-Based Interfaces. Memory-based interfaces in multi-computers <ref> [4, 7, 29, 30, 32] </ref> and workstations [9, 11, 33, 34] provide easy protection for multiprogramming if the NI also demultiplexes messages into per-process buffers. <p> Using virtual memory is particularly natural when the processor initiates all the buffering because existing support for virtual memory (e.g. the processor's TLB) is reused. It requires a relatively complex DMA engine or coprocessor to manipulate virtual memory independently <ref> [13, 25, 29] </ref>. Buffering adds a performance cost when used. The buffered path introduces two components of overhead over the fast path. First, there is an extra copy operation: an operating system handler must copy the message from the network interface to memory.
Reference: [30] <author> Steven L. Scott. </author> <title> Synchronization and Communication in the T3E Multiprocessor. </title> <booktitle> In Seventh International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pages 2636, </pages> <month> October </month> <year> 1996. </year>
Reference-contexts: Section 6 concludes. 2 Related Work Recent architectures demonstrate emerging agreement that it is important to provide support for efficient, fine-grain message passing, even in conjunction with hardware support for shared memory <ref> [1, 2, 13, 18, 27, 30] </ref>. The trend in message interfaces has been to reduce end-to-end overhead by providing user access to the interface hardware. We build on previous work in messaging models and mechanisms. Model. <p> The *T NI [26] would have included GID checks and a timeout on message handling for protection as in FUGU. The M-machine [12] receives messages with a trusted handler that has the ability to quickly forward the message body to a user thread. Memory-Based Interfaces. Memory-based interfaces in multi-computers <ref> [4, 7, 29, 30, 32] </ref> and workstations [9, 11, 33, 34] provide easy protection for multiprogramming if the NI also demultiplexes messages into per-process buffers.
Reference: [31] <author> J.P. Singh, W.-D. Weber, and A. Gupta. </author> <title> SPLASH: Stanford Parallel Applications for Shared-Memory. </title> <type> Technical Report CSL-TR-92-526, </type> <institution> Stanford University, </institution> <month> June </month> <year> 1992. </year>
Reference-contexts: The scheduler timeslice is set at 500,000 cycles. All numbers represent the average of three trials. 5.1 Application Performance We evaluate the performance of our implementation by examining the effects of buffering in several applications tabulated in Table 6. Three applications, LU, Water and Barnes come from the SPLASH <ref> [31] </ref> suite and are slightly modified to make use of the CRL all-software shared-memory system [15]. CRL presents a message-passing load that is representative of coherence protocols such as Stache [24] and can be considered operating-system-like: many low-latency request-reply packets mixed with fewer larger data packets.
Reference: [32] <author> Marc Snir and Peter Hochschild. </author> <title> The Communication Software and Parallel Environment of the IBM SP-2. </title> <type> Technical Report IBM-RC-19812, </type> <institution> IBM, IBM Research Center, </institution> <address> Yorktown Heights, NY, </address> <month> January </month> <year> 1995. </year>
Reference-contexts: The *T NI [26] would have included GID checks and a timeout on message handling for protection as in FUGU. The M-machine [12] receives messages with a trusted handler that has the ability to quickly forward the message body to a user thread. Memory-Based Interfaces. Memory-based interfaces in multi-computers <ref> [4, 7, 29, 30, 32] </ref> and workstations [9, 11, 33, 34] provide easy protection for multiprogramming if the NI also demultiplexes messages into per-process buffers.
Reference: [33] <author> Chandramohan A. Thekkath, Henry M. Levy, and Edward D. La zowska. </author> <title> Efficient Support for Multicomputing on ATM Networks. </title> <institution> UW-CSE 93-04-03, University of Washington, </institution> <address> Seattle, WA, </address> <month> April </month> <year> 1993. </year>
Reference-contexts: The M-machine [12] receives messages with a trusted handler that has the ability to quickly forward the message body to a user thread. Memory-Based Interfaces. Memory-based interfaces in multi-computers [4, 7, 29, 30, 32] and workstations <ref> [9, 11, 33, 34] </ref> provide easy protection for multiprogramming if the NI also demultiplexes messages into per-process buffers. Automatic hardware buffering also deals well with sinking bursts of messages and provides the lowest overhead (by avoiding the processors) when messages are not handled immediately.
Reference: [34] <author> Thorsten von Eicken, Anindya Basu, Vineet Buch, and Werner Vo gels. U-Net: </author> <title> A User-Level Network Interface for Parallel and Distributed Computing. </title> <booktitle> In Proceedings of the Fifteenth ACM Symposium on Operating Systems Principles, </booktitle> <month> December </month> <year> 1995. </year>
Reference-contexts: The M-machine [12] receives messages with a trusted handler that has the ability to quickly forward the message body to a user thread. Memory-Based Interfaces. Memory-based interfaces in multi-computers [4, 7, 29, 30, 32] and workstations <ref> [9, 11, 33, 34] </ref> provide easy protection for multiprogramming if the NI also demultiplexes messages into per-process buffers. Automatic hardware buffering also deals well with sinking bursts of messages and provides the lowest overhead (by avoiding the processors) when messages are not handled immediately.
Reference: [35] <author> Thorsten von Eicken, David Culler, Seth Goldstein, and Klaus Schauser. </author> <title> Active Messages: A Mechanism for Integrated Communication and Computation. </title> <booktitle> In Proceedings of the 19th Annual International Symposium on Computer Architecture, </booktitle> <month> May </month> <year> 1992. </year> <month> 12 </month>
Reference-contexts: Virtual buffering is applicable to any message system that employs buffering. The context in which we evaluate our techniques is that of an interface that supports a User Direct Messaging (UDM) model. Simi lar to Active Messages <ref> [35] </ref>, UDM defines a lightweight messaging discipline in which every incoming message invokes a user handler. The UDM discipline differs from others by codifying explicit control over handler atomicity as part of the user model and thus provides for a smooth integration of interrupt- and polling-driven operation. <p> The trend in message interfaces has been to reduce end-to-end overhead by providing user access to the interface hardware. We build on previous work in messaging models and mechanisms. Model. The UDM model is similar to Active Messages <ref> [35] </ref> and related to Remote Queues (RQ) [6] as an efficient building-block for messaging within a protection domain. UDM differs from Active Messages in that it includes explicit control over message delivery for efficiency.
References-found: 35

