URL: http://csg-www.lcs.mit.edu:8001/Users/vivek/ps/memo78.ps
Refering-URL: http://csg-www.lcs.mit.edu:8001/Users/vivek/sark_pub.html
Root-URL: 
Title: ffi Advanced Compilers, Architectures and Parallel Systems Location Consistency: Stepping Beyond the Barriers of Memory
Author: fifl Guang R. Gao Vivek Sarkar 
Web: FTP: ftp-acaps.cs.mcgill.ca WWW: http://www-acaps.cs.mcgill.ca/  
Address: 555 Bailey Avenue, San Jose, California 95141  St. Montreal Canada H3A 2A7  
Date: December 31, 1993 Revised December 31, 1994  
Affiliation: McGill University School of Computer Science ACAPS Laboratory  yIBM Santa Teresa Laboratory,  ACAPS School of Computer Science 3480 University  
Pubnum: ACAPS Technical Memo 78  
Abstract: A condensed version of this paper appears in the proceedings of the 1995 International Conference on Parallel Processing. gao@cs.mcgill.ca, vivek sarkar@vnet.ibm.com
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Sarita V. Adve and Mark D. Hill. </author> <title> Weak ordering|a new definition. </title> <booktitle> In Proceedings of the 17th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 2-14, </pages> <address> Seattle, Washington, </address> <month> May 28-31, </month> <year> 1990. </year> <journal> IEEE Computer Society and ACM SIGARCH. Computer Architecture News, </journal> <volume> 18(2), </volume> <month> June </month> <year> 1990. </year>
Reference-contexts: As observed in [2], a sequentially consistent multiprocessor behaves like a multiprogrammed uniprocessor. However, it has been found that sequential consistency limits performance by preventing the use of common uniprocessor hardware optimizations such as store buffers and out-of-order memory operations <ref> [13, 1] </ref>. In 1 addition, we observe that sequential consistency prevents the use of common uniprocessor compiler optimizations such as instruction scheduling and register allocation (this observation is discussed in more detail in Section 2). <p> Therefore, we will refer to these weaker models as SC-derived models. Recently proposed SC-derived memory consistency models include weak ordering (WC) [10] , release consistency (RC) [13], data-race-free-0 <ref> [1] </ref>, and data-race-free-1 [2]. These models allow performance optimizations to be correctly applied, while guaranteeing that sequential consistency is retained for a specified class of programs. In this sense, it has been observed in [2] that the SC-derived models are very similar. <p> As discussed earlier, the SC model limits performance by preventing the use of common uniprocessor hardware optimizations such as store buffers and out-of-order memory operations <ref> [13, 1] </ref>. To relax the ordering constraints on memory operations, while still maintaining sequential consistency, a number of weaker memory consistency model were proposed. The weak consistency (WC) model has been proposed by Dubois, Sceurich and Briggs [10].
Reference: [2] <author> Sarita V. Adve and Mark D. Hill. </author> <title> A unified formalization of four shared-memory models. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <pages> pages 613-624, </pages> <month> June </month> <year> 1993. </year>
Reference-contexts: In the dependence-based model, memory operations are partially ordered, with the constraint that there be no concurrency among conflicting accesses directed to the same memory location. As discussed below, the memory consistency models typically assumed by hardware are based on the notions of memory coherence and serializability <ref> [19, 13, 2] </ref>, which assume that (possibly concurrent) memory operations on the same location are totally ordered at run-time and that the same total ordering is observed by all processors. <p> It has been argued that the SC model retains the simplicity of the uniprocessor model because it is a direct extension of the uniprocessor memory abstraction to multiprocessor systems. As observed in <ref> [2] </ref>, a sequentially consistent multiprocessor behaves like a multiprogrammed uniprocessor. However, it has been found that sequential consistency limits performance by preventing the use of common uniprocessor hardware optimizations such as store buffers and out-of-order memory operations [13, 1]. <p> Therefore, we will refer to these weaker models as SC-derived models. Recently proposed SC-derived memory consistency models include weak ordering (WC) [10] , release consistency (RC) [13], data-race-free-0 [1], and data-race-free-1 <ref> [2] </ref>. These models allow performance optimizations to be correctly applied, while guaranteeing that sequential consistency is retained for a specified class of programs. In this sense, it has been observed in [2] that the SC-derived models are very similar. <p> proposed SC-derived memory consistency models include weak ordering (WC) [10] , release consistency (RC) [13], data-race-free-0 [1], and data-race-free-1 <ref> [2] </ref>. These models allow performance optimizations to be correctly applied, while guaranteeing that sequential consistency is retained for a specified class of programs. In this sense, it has been observed in [2] that the SC-derived models are very similar. A more detailed discussion of these SC-derived models is provided in Section 7. <p> Let us illustrate the constraints imposed by sequential consistency by the simple example in Figure 1, which was also presented in <ref> [2] </ref>. Under the sequential consistency model, the memory operations from processor 1 and 2 appear to be "serialized" in some order which is observed by both processors. Therefore, in this example, it is easy to show that the result r1 = r2 = 0 is prohibited by sequential consistency. <p> The data races are also considered to be program errors in previously defined weak consistency models <ref> [2] </ref>, unless all memory operations on array a are classified as synchronizing operations (which naturally increases the overhead of the memory operations). <p> In particular, we explicitly distinguish between data and control synchronization operations, while many other memory consistency models have focused only on the former <ref> [2] </ref>. 8 4 Abstraction of Memory System 4.1 Memory Abstraction as Partially Ordered Multisets The state of a memory location, L, is represented by a partially ordered multiset (pomset), state (L) = (S; ), where S is a multiset and S fi S is a partial order on S [22]. <p> L, the abstract interpreter computes the value set V (r) from the pomset for location L, and (arbitrarily) returns a value from the set V (r) as the result of the read operation 2 Analogous to the notion of a sequential consistent execution used in defining the data-race-free-1 model in <ref> [2] </ref>, we introduce the notion of a location consistent execution as any execution of a program by the abstract interpreter discussed above. Note that, the execution model of the abstract interpreter makes no assumption on the timing of events in the program execution. <p> A brief survey of these models can be found in <ref> [2] </ref>. 8 Conclusions and Future Work Memory consistency models play an important role on the correct and efficient implementation of shared memory multiprocessor systems.
Reference: [3] <author> Alfred V. Aho, Ravi Sethi, and Jeffrey D. Ullman. </author> <booktitle> Compilers | Principles, Techniques, and Tools. </booktitle> <publisher> Addison-Wesley Publishing Company, </publisher> <address> Reading, Massachusetts, </address> <year> 1986. </year>
Reference-contexts: Given the concurrency defined in the program, we see no reason why this scenario should be prohibited by a memory consistency model. Let us look more closely at the impact of the memory coherence assumption on register allocation, for the example in Figure 3. Register allocation <ref> [3] </ref> is a well-known compiler optimization in which load and store operations are eliminated by saving values in registers.
Reference: [4] <author> Arvind, M. L. Dertouzos, R. S. Nikhil, and G. M. Papadopoulos. </author> <title> Project dataflow|the Monsoon architecture and the Id programming language. CSG Memo 285, Computation Structures Group, </title> <institution> MIT Laboratory for Computer Science, Cambridge, Massachusetts, </institution> <month> March </month> <year> 1988. </year>
Reference-contexts: As in other work <ref> [4, 27] </ref>, we assume a single-writer multiple reader model for this form of producer-consumer synchronization. * Acquire-release data synchronization | if processors P 1 ; : : : ; P k need exclusive access to a set of shared locations fL 1 ; : : : ; L m g, the <p> There are proposed solutions in the literature for satisfying this requirement in the presence of sync read and sync write operations <ref> [27, 4] </ref> and of acquire and release operations [20]. These solutions would also apply to satisfying the requirements imposed by these synchronization operations 15 under the LC model.
Reference: [5] <author> P. Brinch Hansen. </author> <title> The programming language Concurrent Pascal. </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> 1(2) </volume> <pages> 199-206, </pages> <month> June </month> <year> 1975. </year>
Reference-contexts: Using these memory and synchronization operations as primitives, the LC model can be used to support parallel control structures selected from a variety of different parallel program models such as fork-join [26], cobegin-coend <ref> [5] </ref>, SPMD [6], program dependence graphs [11], and parallel program graphs [23].
Reference: [6] <author> F. Darema, D. A. George, V. A. Norton, and G. F. Pfister. </author> <title> A single-program-multiple-data computational model for EPEX/FORTRAN. </title> <journal> Parallel Computing, </journal> <volume> 7 </volume> <pages> 11-24, </pages> <month> April </month> <year> 1988. </year>
Reference-contexts: Using these memory and synchronization operations as primitives, the LC model can be used to support parallel control structures selected from a variety of different parallel program models such as fork-join [26], cobegin-coend [5], SPMD <ref> [6] </ref>, program dependence graphs [11], and parallel program graphs [23].
Reference: [7] <author> R. De Leone and O. L. Mangasarian. </author> <title> Asynchronous parallel successive overrelaxation for the symmetric linear complementarity problem. </title> <journal> Mathematical Programming, </journal> <volume> 42(2) </volume> <pages> 347-361, </pages> <month> November </month> <year> 1988. </year>
Reference-contexts: The semantics of data races is well-defined by the partial order semantics of program execution under the LC model. These properties of the LC model are consistent with the assumptions made by designers of asynchronous algorithms <ref> [7] </ref>. From the previous examples, one can imagine several situations in which the SC model prevents the use of instruction reordering, an optimization that is commonly used in compilers and in uniprocessor hardware. For example, in Figure 1 the result r1 = r2 = 0 is prohibited by sequential consistency.
Reference: [8] <author> E. W. Dijkstra. </author> <title> Co-operating sequential processes. </title> <editor> In F. Genuys, editor, </editor> <booktitle> Programming Languages, </booktitle> <pages> pages 43-112. </pages> <address> New York: </address> <publisher> Academic Press, </publisher> <year> 1968. </year> <month> 19 </month>
Reference-contexts: This operation can be efficiently implemented by a binary semaphore <ref> [8] </ref>. * Undirected control synchronization | if processors P 1 ; : : : ; P k all need to synchronize among each other, the synchronization is accomplished by each processor performing a sync (fP 1 ; : : : ; P k g) operation.
Reference: [9] <author> Anne Dinning and Edith Schonberg. </author> <title> An empirical comparison of monitoring algorithms for access anomaly detection. </title> <booktitle> In Proceedings of the Second ACM SIGPLAN Symposium on Principles & Practice of Parallel Programming (PPOPP), </booktitle> <pages> pages 1-10, </pages> <address> Seattle, Washington, March 14-16, </address> <year> 1990. </year> <journal> SIGPLAN Notices, </journal> <volume> 25(3), </volume> <month> March </month> <year> 1990. </year>
Reference-contexts: in iteration (j=1,i=1) and the read operation on a (i,j-1) in iteration (j=2,i=1). 6 These data races are considered to be program errors in compiler-oriented parallel program models such as the program dependence graph [11] and the parallel program graph [23], as well as in systems for detecting access anomalies <ref> [9] </ref>. The data races are also considered to be program errors in previously defined weak consistency models [2], unless all memory operations on array a are classified as synchronizing operations (which naturally increases the overhead of the memory operations).
Reference: [10] <author> Michel Dubois, Christoph Scheurich, and Faye Briggs. </author> <title> Memory access buffering in multiprocessors. </title> <booktitle> In Proceedings of the 13th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 434-442, </pages> <address> Tokyo, Japan, </address> <month> June 2-5, </month> <year> 1986. </year> <journal> IEEE Computer Society and ACM SIGARCH. Computer Architecture News, </journal> <volume> 14(2), </volume> <month> June </month> <year> 1986. </year>
Reference-contexts: Therefore, we will refer to these weaker models as SC-derived models. Recently proposed SC-derived memory consistency models include weak ordering (WC) <ref> [10] </ref> , release consistency (RC) [13], data-race-free-0 [1], and data-race-free-1 [2]. These models allow performance optimizations to be correctly applied, while guaranteeing that sequential consistency is retained for a specified class of programs. In this sense, it has been observed in [2] that the SC-derived models are very similar. <p> To relax the ordering constraints on memory operations, while still maintaining sequential consistency, a number of weaker memory consistency model were proposed. The weak consistency (WC) model has been proposed by Dubois, Sceurich and Briggs <ref> [10] </ref>. Under the WC model, ordinary shared memory access operations and synchronization memory access operations are distinguished. The synchronization access operations are used to control the interaction among concurrent processes to maintain the consistency of the shared memory.
Reference: [11] <author> Jeanne Ferrante, Karl J. Ottenstein, and Joe D. Warren. </author> <title> The program dependence graph and its use in optimization. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 9(3) </volume> <pages> 319-349, </pages> <month> July </month> <year> 1987. </year>
Reference-contexts: There is a noticeable dichotomy between recent software and hardware viewpoints of memory consistency in shared-memory multiprocessors. The memory consistency model typically assumed by software is a dependence-based model, as exemplified by program dependence graphs <ref> [11] </ref> and parallel program graphs [23] and various high-level programming languages that can be translated to these program graph representations. In the dependence-based model, memory operations are partially ordered, with the constraint that there be no concurrency among conflicting accesses directed to the same memory location. <p> e.g. there is a data race on a (1,1) between the write operation on a (i,j) in iteration (j=1,i=1) and the read operation on a (i,j-1) in iteration (j=2,i=1). 6 These data races are considered to be program errors in compiler-oriented parallel program models such as the program dependence graph <ref> [11] </ref> and the parallel program graph [23], as well as in systems for detecting access anomalies [9]. <p> Using these memory and synchronization operations as primitives, the LC model can be used to support parallel control structures selected from a variety of different parallel program models such as fork-join [26], cobegin-coend [5], SPMD [6], program dependence graphs <ref> [11] </ref>, and parallel program graphs [23].
Reference: [12] <author> Kourosh Gharachorloo, Anoop Gupta, and John Hennessy. </author> <title> Revision to `memory consistency and event ordering in scalable shared-memory multiprocessors'. </title> <type> Technical Report No. </type> <institution> CSL-TR-93-568, Computer Systems Laboratory, Stanford University, Stanford, California, </institution> <month> April </month> <year> 1993. </year>
Reference-contexts: Recently, the authors of [13] observed that the memory coherence assumption conflicts with hardware optimizations found in modern processor architectures with agressive instruction-level parallelism e.g. superscalar architectures <ref> [12] </ref>. For example, one hardware optimization that conflicts with the memory coherence assumption is the use of write buffers [15], in which a processor may retrieve its most recently written value to a given location from its write buffer without involving the memory system. [12] discusses the implementation difficulties that arise <p> agressive instruction-level parallelism e.g. superscalar architectures <ref> [12] </ref>. For example, one hardware optimization that conflicts with the memory coherence assumption is the use of write buffers [15], in which a processor may retrieve its most recently written value to a given location from its write buffer without involving the memory system. [12] discusses the implementation difficulties that arise when trying to accomodate the memory coherence assumption in conjunction with such hardware optimizations; it also observes that the behavior of existing multiprocessor systems, such as the DASH multiprocessor [20], often does not completely satisfy the requirements of sequential consistency.
Reference: [13] <author> Kourosh Gharachorloo, Daniel Lenoski, James Laudon, Phillip Gibbons, Anoop Gupta, and John Hennessy. </author> <title> Memory consistency and event ordering in scalable shared-memory multiprocessors. </title> <booktitle> In Proceedings of the 17th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 15-26, </pages> <address> Seattle, Washington, </address> <month> May 28-31, </month> <year> 1990. </year> <journal> IEEE Computer Society and ACM SIGARCH. Computer Architecture News, </journal> <volume> 18(2), </volume> <month> June </month> <year> 1990. </year>
Reference-contexts: In the dependence-based model, memory operations are partially ordered, with the constraint that there be no concurrency among conflicting accesses directed to the same memory location. As discussed below, the memory consistency models typically assumed by hardware are based on the notions of memory coherence and serializability <ref> [19, 13, 2] </ref>, which assume that (possibly concurrent) memory operations on the same location are totally ordered at run-time and that the same total ordering is observed by all processors. <p> As observed in [2], a sequentially consistent multiprocessor behaves like a multiprogrammed uniprocessor. However, it has been found that sequential consistency limits performance by preventing the use of common uniprocessor hardware optimizations such as store buffers and out-of-order memory operations <ref> [13, 1] </ref>. In 1 addition, we observe that sequential consistency prevents the use of common uniprocessor compiler optimizations such as instruction scheduling and register allocation (this observation is discussed in more detail in Section 2). <p> Therefore, we will refer to these weaker models as SC-derived models. Recently proposed SC-derived memory consistency models include weak ordering (WC) [10] , release consistency (RC) <ref> [13] </ref>, data-race-free-0 [1], and data-race-free-1 [2]. These models allow performance optimizations to be correctly applied, while guaranteeing that sequential consistency is retained for a specified class of programs. In this sense, it has been observed in [2] that the SC-derived models are very similar. <p> A more detailed discussion of these SC-derived models is provided in Section 7. A central assumption in the definitions of all SC-derived memory consistency models is the memory coherence assumption, which can be stated as follows <ref> [13] </ref>: "all writes to the same location are serialized in some order and are performed in that order with respect to any processor". <p> Third, the definition of memory consistency depends on the notion of a memory operation 2 being "globally performed", for both sequential consistency and recently proposed weaker consistency models <ref> [13] </ref>. The notion of "globally performed" implies some idealized assumption about global time in order to explain the serialization of events. As explained in section 7, the notion of "globally performed" is hard to interpret and maintain in a multiprocessor system. <p> In Section 1, we observed that the SC and SC-derived models make a strong assumption on memory coherence, which amounts to assuming that all writes to the same memory location are serialized in some order and are performed in that order with respect to all processors <ref> [13] </ref>. Note that all read and write operations are targeted to the same memory location L. The initial value of L is 0, and the two concurrent writes store the values 1 and 2 into L. <p> For SC-derived models, the same restriction holds if the accesses to X and Y are classified as synchronizing operations; but if they are not classified as synchronizing operations, the example program will not belong to the restricted program classes (e.g. properly labeled programs <ref> [13] </ref>) that are of primary interest to the SC-derived models. Note that, in the LC model, it is perfectly legal to reorder the instructions within a process since they operate on distinct locations. 3 Program Model In this section, we outline the program model assumed for this work. <p> set of shared locations fL 1 ; : : : ; L m g, the synchronization is accomplished by each processor performing an acquire (fL 1 ; : : : ; L m g) operation followed by a release (fL 1 ; : : : ; L m g) operation <ref> [13] </ref>. <p> the value set V (r) from the pomset for location L, and (arbitrarily) returns a value from the set V (r) as the result of the read operation 1 The execution model of the abstract interpreter makes the following two assumptions: * Assumption 1: As in other memory consistency models <ref> [13] </ref>, we retain the assumption that each processor in the abstract interpreter sequentially executes its node program, subject to all uniprocessor control and data dependences being satisfied. * Assumption 2: It updates the states of memory locations according to the rules specified in section 4.2. <p> As mentioned earlier, the LC model does not rely on the memory coherence assumption present in <ref> [13] </ref> and other SC-derived models. That is, we do not make the assumption that all writes to the same location are serialized in some order and are performed in that order with respect to all processors. Recently, the authors of [13] observed that the memory coherence assumption conflicts with hardware optimizations <p> does not rely on the memory coherence assumption present in <ref> [13] </ref> and other SC-derived models. That is, we do not make the assumption that all writes to the same location are serialized in some order and are performed in that order with respect to all processors. Recently, the authors of [13] observed that the memory coherence assumption conflicts with hardware optimizations found in modern processor architectures with agressive instruction-level parallelism e.g. superscalar architectures [12]. <p> As discussed earlier, the SC model limits performance by preventing the use of common uniprocessor hardware optimizations such as store buffers and out-of-order memory operations <ref> [13, 1] </ref>. To relax the ordering constraints on memory operations, while still maintaining sequential consistency, a number of weaker memory consistency model were proposed. The weak consistency (WC) model has been proposed by Dubois, Sceurich and Briggs [10]. <p> For ordinary shared memory accesses, mutual exclusion should be ensured by using constructs such as critical sections implemented by synchronization memory operations. To further relax the ordering constraints, Gharchorloo et al have proposed the release consistency (RC) model <ref> [13] </ref>. The RC model exploits additional information about shared access operations. In particular, synchronization access operations are partitioned into acquire and release operations. <p> The assumption of memory coherence: the SC-derived models assume that the memory is kept coherent, that is, "all writes to the same location are serialized in some order and are performed in that order with respect to any processor" <ref> [13] </ref>. 2. The abstraction of memory access operations: the SC-derived models use the concepts of a memory access being performed or globally performed. These concepts are defined in [13]. The notions of "perfomed" and "globally performed" are based on Dubois' memory abstraction as summarized in the following definitions from [13]: Definition <p> "all writes to the same location are serialized in some order and are performed in that order with respect to any processor" <ref> [13] </ref>. 2. The abstraction of memory access operations: the SC-derived models use the concepts of a memory access being performed or globally performed. These concepts are defined in [13]. The notions of "perfomed" and "globally performed" are based on Dubois' memory abstraction as summarized in the following definitions from [13]: Definition 1: A LOAD by a processor P i is considered performed with respect 17 to a processor P k at a point in time when the issuing of <p> processor" <ref> [13] </ref>. 2. The abstraction of memory access operations: the SC-derived models use the concepts of a memory access being performed or globally performed. These concepts are defined in [13]. The notions of "perfomed" and "globally performed" are based on Dubois' memory abstraction as summarized in the following definitions from [13]: Definition 1: A LOAD by a processor P i is considered performed with respect 17 to a processor P k at a point in time when the issuing of a STORE to the same address by P k cannot affect the value returned by the LOAD.
Reference: [14] <author> J. R. Goodman. </author> <title> Cache consistency and sequential consistency. </title> <type> Technical Report 1006, </type> <institution> Department of Computer Science, University of Wisconsin, Madison, </institution> <month> February </month> <year> 1991. </year>
Reference-contexts: There are many variations of the three memory consistency models discussed so far (SC, WC and RC). For example, Goodman has introduced the processor consistency (PC) model to relax the odering constraints imposed by SC <ref> [14] </ref>. Under the PC model, the order in which write accesses from two processors (to different locations) occur may not be the same as observed by themselves or by a third processor. Read accesses may bypass write accesses in a processor an architectural optimization supported by many today's processor architectures.
Reference: [15] <author> John L. Hennessy and David A. Patterson. </author> <title> Computer Architecture: A Quantitative Approach. </title> <publisher> Morgan Kaufmann Publishers, Inc., </publisher> <year> 1990. </year>
Reference-contexts: Recently, the authors of [13] observed that the memory coherence assumption conflicts with hardware optimizations found in modern processor architectures with agressive instruction-level parallelism e.g. superscalar architectures [12]. For example, one hardware optimization that conflicts with the memory coherence assumption is the use of write buffers <ref> [15] </ref>, in which a processor may retrieve its most recently written value to a given location from its write buffer without involving the memory system. [12] discusses the implementation difficulties that arise when trying to accomodate the memory coherence assumption in conjunction with such hardware optimizations; it also observes that the
Reference: [16] <author> P. W. Hutto and M. Ahamad. </author> <title> Slow memory: Weakening consistency to enhance concurrency in distributed shared memories. </title> <booktitle> In Proceedings of the 10th International Conference on Distributed Computing Systems, </booktitle> <pages> pages 302-311, </pages> <address> Paris, France, May 28-June 1, 1990. </address> <publisher> IEEE Computer Society. </publisher>
Reference-contexts: Due to space limitations, we will not enumerate many other interesting memory consistency models, such as PRAM [21], the total memory ordering [29], the partial memory ordering [29], the slow memory model <ref> [16] </ref>, the casual memory model [16], and the concurrent consistency model [25]. A brief survey of these models can be found in [2]. 8 Conclusions and Future Work Memory consistency models play an important role on the correct and efficient implementation of shared memory multiprocessor systems. <p> Due to space limitations, we will not enumerate many other interesting memory consistency models, such as PRAM [21], the total memory ordering [29], the partial memory ordering [29], the slow memory model <ref> [16] </ref>, the casual memory model [16], and the concurrent consistency model [25]. A brief survey of these models can be found in [2]. 8 Conclusions and Future Work Memory consistency models play an important role on the correct and efficient implementation of shared memory multiprocessor systems.
Reference: [17] <author> Alan H. Karp and Vivek Sarkar. </author> <title> Data merging for shared-memory multiprocessors. </title> <booktitle> In Proceedings of the Twenty-Sixth Annual Hawaii International Conference on System Sciences, </booktitle> <volume> volume II, </volume> <pages> pages 244-256, </pages> <address> Koaloa, Hawaii, </address> <month> January 5-8, </month> <year> 1993. </year> <journal> ACM and IEEE Computer Society. </journal> <note> Also published as Technical Report 03.454, </note> <institution> IBM Santa Teresa Laboratory, </institution> <month> October </month> <year> 1992. </year>
Reference-contexts: We believe that this flexibility offered by the LC model will simplify the implementation of cache coherence support and provide the potential for additional performance improvements. For example, the Data Merging protocol presented in <ref> [17] </ref> can be used to provide efficient cache coherence support under the LC model, but not under the SC and SC-derived models. 7 Relation with Other Memory Consistency Models In this section, we discuss related work in the area of memory consistency models.
Reference: [18] <author> Pete Keleher, Alan L. Cox, and Willy Zwaenepoel. </author> <title> Lazy release consistency for software distributed shared memory. </title> <booktitle> In Proceedings of the 19th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 13-21, </pages> <address> Gold Coast, Australia, </address> <month> May 19-21, </month> <year> 1992. </year> <journal> ACM SIGARCH and IEEE Computer Society. Computer Architecture News, </journal> <volume> 20(2), </volume> <month> May </month> <year> 1992. </year> <month> 20 </month>
Reference-contexts: Read accesses may bypass write accesses in a processor an architectural optimization supported by many today's processor architectures. Another example is the lazy release consistency (LRC) model has been proposed by Keleher et. al <ref> [18] </ref>. The objective of LRC is to reduce both the number of messages and the amount of data transferred when implementing the RC model. A major feature of LRC is that it does not make modifications globally visible at the time of a release operation is performed.
Reference: [19] <author> Leslie Lamport. </author> <title> How to make a multiprocessor computer that correctly executes multiprocess programs. </title> <journal> IEEE Transactions on Computers, </journal> <volume> 28(9) </volume> <pages> 690-691, </pages> <month> September </month> <year> 1979. </year>
Reference-contexts: In the dependence-based model, memory operations are partially ordered, with the constraint that there be no concurrency among conflicting accesses directed to the same memory location. As discussed below, the memory consistency models typically assumed by hardware are based on the notions of memory coherence and serializability <ref> [19, 13, 2] </ref>, which assume that (possibly concurrent) memory operations on the same location are totally ordered at run-time and that the same total ordering is observed by all processors. <p> The hardware memory consistency model that has been most commonly used as a basis for past work is sequential consistency (SC) <ref> [19] </ref>. It has been argued that the SC model retains the simplicity of the uniprocessor model because it is a direct extension of the uniprocessor memory abstraction to multiprocessor systems. As observed in [2], a sequentially consistent multiprocessor behaves like a multiprogrammed uniprocessor. <p> We have already discussed some comparisons between the LC model and the SC and SC-derived models in earlier sections of this paper. The purpose of this section is is to provide more detailed background information on the related work. The notion of sequential consistency was originally introduced by Lamport <ref> [19] </ref> as follows: Sequential Consistency (SC): A system is sequentially conistent if the result of any execution is the same as if the operations of all the processors were executed in some sequential order, and the operations of each individual processor appear in this sequence in the order specified by its
Reference: [20] <author> Daniel Lenoski, James Laudon, Truman Joe, David Nakahira, Luis Stevens, Anoop Gupta, and John Hennessy. </author> <title> The DASH prototype: Implementation and performance. </title> <booktitle> In Proceedings of the 19th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 92-103, </pages> <address> Gold Coast, Australia, </address> <month> May 19-21, </month> <year> 1992. </year> <journal> ACM SIGARCH and IEEE Computer Society. Computer Architecture News, </journal> <volume> 20(2), </volume> <month> May </month> <year> 1992. </year>
Reference-contexts: value to a given location from its write buffer without involving the memory system. [12] discusses the implementation difficulties that arise when trying to accomodate the memory coherence assumption in conjunction with such hardware optimizations; it also observes that the behavior of existing multiprocessor systems, such as the DASH multiprocessor <ref> [20] </ref>, often does not completely satisfy the requirements of sequential consistency. Since the LC model does not depend on the memory coherence assumption, such complications in implementing hardware optimizations will not exist, hence the potential for higher performance. <p> There are proposed solutions in the literature for satisfying this requirement in the presence of sync read and sync write operations [27, 4] and of acquire and release operations <ref> [20] </ref>. These solutions would also apply to satisfying the requirements imposed by these synchronization operations 15 under the LC model.
Reference: [21] <author> R. J. Lipton and J. S. Sandberg. </author> <title> PRAM: a scalable shared memory. </title> <type> Technical Report CS-TR 180-88, </type> <institution> Princeton University, </institution> <month> September </month> <year> 1988. </year>
Reference-contexts: Instead, the LC model should be viewed as an extension of existing models by further relaxing the ordering constraints imposed by the memory coherence assumption. Due to space limitations, we will not enumerate many other interesting memory consistency models, such as PRAM <ref> [21] </ref>, the total memory ordering [29], the partial memory ordering [29], the slow memory model [16], the casual memory model [16], and the concurrent consistency model [25].
Reference: [22] <author> V. R. Pratt. </author> <title> The duality of time and information. </title> <booktitle> In Proceedings of the Third International Conference on Concurrency Theory, </booktitle> <pages> pages 237-253. </pages> <publisher> Springer-Verlag, </publisher> <month> August </month> <year> 1992. </year>
Reference-contexts: Instead of assuming that all writes to the same location are serialized according to some total order, we model the state of a memory location as a partially ordered multiset (pomset) <ref> [22] </ref> of write operations and synchronization operations. Unlike the position taken by the SC-derived models, we do not believe that the sequential consistency model is simple or natural to the programmer's way of thinking when writing concurrent programs, for the following reasons. <p> As explained in section 7, the notion of "globally performed" is hard to interpret and maintain in a multiprocessor system. In contrast to the SC and SC-derived models, we model the state of a memory location as a partially ordered multiset (pomset) <ref> [22] </ref>. Each element of the pomset corresponds to either a write operation to the location or a synchronization operation. The partial order in the pomset naturally follows from the ordering constraints defined by a concurrent program and its execution. <p> former [2]. 8 4 Abstraction of Memory System 4.1 Memory Abstraction as Partially Ordered Multisets The state of a memory location, L, is represented by a partially ordered multiset (pomset), state (L) = (S; ), where S is a multiset and S fi S is a partial order on S <ref> [22] </ref>. Each element e of the multiset S denotes a write operation or a synchronization operation involving location L. We also define processorset (e) to be the set of processors involved in operation e.
Reference: [23] <author> V. Sarkar. </author> <title> A concurrent execution semantics for parallel program graphs and program dependence graphs. </title> <editor> In Uptal Banerjee, David Gelernter, Alex Nicolau, and David Padua, editors, </editor> <booktitle> Proceedings of the 5th International Workshop on Languages and Compilers for Parallel Computing, number 757 in Lecture Notes in Computer Science, </booktitle> <pages> pages 16-30, </pages> <address> New Haven, Connecticut, </address> <month> August 3-5, </month> <year> 1992. </year> <note> Springer-Verlag. Published in 1993. </note>
Reference-contexts: There is a noticeable dichotomy between recent software and hardware viewpoints of memory consistency in shared-memory multiprocessors. The memory consistency model typically assumed by software is a dependence-based model, as exemplified by program dependence graphs [11] and parallel program graphs <ref> [23] </ref> and various high-level programming languages that can be translated to these program graph representations. In the dependence-based model, memory operations are partially ordered, with the constraint that there be no concurrency among conflicting accesses directed to the same memory location. <p> on a (1,1) between the write operation on a (i,j) in iteration (j=1,i=1) and the read operation on a (i,j-1) in iteration (j=2,i=1). 6 These data races are considered to be program errors in compiler-oriented parallel program models such as the program dependence graph [11] and the parallel program graph <ref> [23] </ref>, as well as in systems for detecting access anomalies [9]. The data races are also considered to be program errors in previously defined weak consistency models [2], unless all memory operations on array a are classified as synchronizing operations (which naturally increases the overhead of the memory operations). <p> Using these memory and synchronization operations as primitives, the LC model can be used to support parallel control structures selected from a variety of different parallel program models such as fork-join [26], cobegin-coend [5], SPMD [6], program dependence graphs [11], and parallel program graphs <ref> [23] </ref>.
Reference: [24] <author> Vivek Sarkar. </author> <title> Synchronization using counting semaphores. </title> <booktitle> In Conference Proceedings, 1988 International Conference on Supercomputing, </booktitle> <pages> pages 627-637, </pages> <address> St. Malo, France, </address> <month> July 4-8, </month> <year> 1988. </year> <note> ACM. </note>
Reference-contexts: This operation can be efficiently implemented by a counting semaphore <ref> [24] </ref>.
Reference: [25] <author> C. E. Scheurich. </author> <title> Access Ordering and Coherence in Shared Memory Multiprocessors. </title> <type> PhD thesis, </type> <institution> University of Southern California, </institution> <year> 1989. </year>
Reference-contexts: Due to space limitations, we will not enumerate many other interesting memory consistency models, such as PRAM [21], the total memory ordering [29], the partial memory ordering [29], the slow memory model [16], the casual memory model [16], and the concurrent consistency model <ref> [25] </ref>. A brief survey of these models can be found in [2]. 8 Conclusions and Future Work Memory consistency models play an important role on the correct and efficient implementation of shared memory multiprocessor systems.
Reference: [26] <author> A. C. Shaw. </author> <title> The Logical Design of Operating Systems. </title> <publisher> Prentice-Hall, Inc., </publisher> <year> 1974. </year>
Reference-contexts: far from simple because it forces the programmer to deal with complex interactions among concurrent regions of a program e.g. consider the subtleties and pitfalls identified in various attempted solutions to the critical section problem, and the complex reasoning about inter-processor interactions required to prove the correctness of Dekker's solution <ref> [26] </ref>. Apart from making programmer errors more likely, we believe that the inherent complexity of understanding parallel program execution under the SC model naturally makes it more difficult to implement software and hardware optimizations for the SC model. <p> Using these memory and synchronization operations as primitives, the LC model can be used to support parallel control structures selected from a variety of different parallel program models such as fork-join <ref> [26] </ref>, cobegin-coend [5], SPMD [6], program dependence graphs [11], and parallel program graphs [23].
Reference: [27] <author> Burton Smith. </author> <title> The architecture of HEP. </title> <editor> In Janusz S. Kowalik, editor, </editor> <title> Parallel MIMD Computation: HEP Supercomputer and its Applications, </title> <booktitle> Scientific Computation Series, </booktitle> <pages> pages 41-55. </pages> <publisher> MIT Press, </publisher> <address> Cambridge, Massachusetts, </address> <year> 1985. </year>
Reference-contexts: As in other work <ref> [4, 27] </ref>, we assume a single-writer multiple reader model for this form of producer-consumer synchronization. * Acquire-release data synchronization | if processors P 1 ; : : : ; P k need exclusive access to a set of shared locations fL 1 ; : : : ; L m g, the <p> There are proposed solutions in the literature for satisfying this requirement in the presence of sync read and sync write operations <ref> [27, 4] </ref> and of acquire and release operations [20]. These solutions would also apply to satisfying the requirements imposed by these synchronization operations 15 under the LC model.
Reference: [28] <author> Harold S. Stone. </author> <title> High-Performance Computer Architecture. </title> <publisher> Addison-Wesley Publishing Company, 3rd edition, </publisher> <year> 1993. </year>
Reference-contexts: Considerations related 3 to the program model and compiler optimization were relatively neglected. For example, the interpretation of "globally performed", essential for understanding previous memory consistency models, is tied closely to understanding the underlying hardware memory architecture <ref> [28] </ref>. We believe that the recent progress made in compiler optimization techniques for uniprocessor architectures deserve equal attention when defining a memory consistency model. As explained in Section 2, SC and SC-derived models often put unnecessary constraints to the ordering of memory operations. <p> These notions implicitly rely on the assumption of some form of global time which, we argue, is hard to interpret and maintain in a multiprocessor system. As pointed out by Stone <ref> [28] </ref>, the rules for defining these terms are quite "vague", and it is hard to pin down just what is the "point of time" used in the definition, in particular when dealing with multiprocessors with coherent cache systems and buffered memory-processor interconnection networks..
Reference: [29] <author> Sun Microsystems Inc. </author> <title> The SPARC Architecture Manual, </title> <booktitle> 1991. </booktitle> <pages> 21 </pages>
Reference-contexts: Instead, the LC model should be viewed as an extension of existing models by further relaxing the ordering constraints imposed by the memory coherence assumption. Due to space limitations, we will not enumerate many other interesting memory consistency models, such as PRAM [21], the total memory ordering <ref> [29] </ref>, the partial memory ordering [29], the slow memory model [16], the casual memory model [16], and the concurrent consistency model [25]. <p> Due to space limitations, we will not enumerate many other interesting memory consistency models, such as PRAM [21], the total memory ordering <ref> [29] </ref>, the partial memory ordering [29], the slow memory model [16], the casual memory model [16], and the concurrent consistency model [25].
References-found: 29

