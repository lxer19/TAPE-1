URL: ftp://playfair.stanford.edu/pub/donoho/isaws.ps.Z
Refering-URL: http://www.mathsoft.com/wavelets.html
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Title: Ideal Spatial Adaptation by Wavelet Shrinkage doing well at a point; Orthogonal Wavelet Bases of
Author: David L. Donoho Iain M. Johnstone 
Keyword: Minimax estimation subject  
Note: to  
Date: June 1992 Revised April 1993  
Address: Stanford, CA, 94305-4065, U.S.A.  
Affiliation: Department of Statistics, Stanford University,  
Abstract: With ideal spatial adaptation, an oracle furnishes information about how best to adapt a spatially variable estimator, whether piecewise constant, piecewise polynomial, variable knot spline, or variable bandwidth kernel, to the unknown function. Estimation with the aid of an oracle offers dramatic advantages over traditional linear estimation by nonadaptive kernels; however, it is a priori unclear whether such performance can be obtained by a procedure relying on the data alone. We describe a new principle for spatially-adaptive estimation: selective wavelet reconstruction. We show that variable-knot spline fits and piecewise-polynomial fits, when equipped with an oracle to select the knots, are not dramatically more powerful than selective wavelet reconstruction with an oracle. We develop a practical spatially adaptive method, RiskShrink, which works by shrinkage of empirical wavelet coefficients. RiskShrink mimics the performance of an oracle for selective wavelet reconstruction as well as it is possible to do so. A new inequality in multivariate normal decision theory which we call the oracle inequality shows that attained performance differs from ideal performance by at most a factor ~ 2 log n, where n is the sample size. Moreover no estimator can give a better guarantee than this. Within the class of spatially adaptive procedures, RiskShrink is essentially optimal. Relying only on the data, it comes within a factor log 2 n of the performance of piecewise polynomial and variable-knot spline methods equipped with an oracle. In contrast, it is unknown how or if piecewise polynomial methods could be made to function this well when denied access to an oracle and forced to rely on data alone. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> BICKEL, P. J. </author> <year> (1983). </year> <title> Minimax estimation of a normal mean subject to doing well at a point. In Recent Advances in Statistics (M. </title> <editor> H. Rizvi, J. S. Rustagi, and D. Siegmund, eds.), </editor> <publisher> Academic Press, </publisher> <address> New York, </address> <pages> 511-528. </pages>
Reference-contexts: A clearer picture of what we intend emerges from five examples. <ref> [1] </ref>. Piecewise Constant Reconstruction T P C (y; ffi). <p> A clearer picture of what we intend emerges from five examples. [1]. Piecewise Constant Reconstruction T P C (y; ffi). Here ffi is a finite list of, say, L real numbers defining a partition (I 1 ; : : : ; I L ) of <ref> [0; 1] </ref> via I 1 = [0; ffi 1 ); I 2 = [ffi 1 ; ffi 1 + ffi 2 ); : : : ; I L = [ffi 1 + +ffi L1 ; ffi 1 + +ffi L ] ,so that P L 1 ffi i = 1. <p> The reconstruction formula is T P C (y; ffi)(t) = `=1 piecewise constant reconstruction using the mean of the data within each piece to estimate the pieces. [2]. Piecewise Polynomials T P P (D) (y; ffi). Here the interpretation of ffi is the same as in <ref> [1] </ref>, only the reconstruction uses polynomials of degree D. <p> Variable Bandwidth Kernel Methods T V K;2 (y; ffi). Now ffi is a function on <ref> [0; 1] </ref>; ffi (t) represents the "bandwidth of the kernel at t"; the smoothing kernel K is a C 2 function of compact support which is also a probability density, and if ^ f = T V K;2 (y; ffi) then ^ f (t) = n i=1 ffi (t) ffi (t): <p> These reconstruction techniques, when equipped with appropriate selectors of the spatial smoothing parameter ffi, duplicate essential features of certain well-known methods. <ref> [1] </ref> The piecewise constant reconstruction formula T P C , equipped with choice of partition ffi by recursive partitioning and cross-validatory choice of "pruning constant" as described by Breiman, Friedman, Olshen and Stone (1983) results in the method CART applied to 1-dimensional data. [2] The spline reconstruction formula T spl;D , <p> Other near-minimax properties are described in detail in our report [asymp.tex]. 18 4.6 Boundary correction As described in the Introduction, Cohen, Daubechies, Jawerth and Vial (1993), have introduced separate `boundary filters' to correct the non-orthogonality on <ref> [0; 1] </ref> of the restriction to [0; 1] of basis functions that intersect [0; 1] c . To preserve the important property [W1] of orthogonality to polynomials of degree M , a further `preconditioning' transformation P of the data y is necessary. <p> Other near-minimax properties are described in detail in our report [asymp.tex]. 18 4.6 Boundary correction As described in the Introduction, Cohen, Daubechies, Jawerth and Vial (1993), have introduced separate `boundary filters' to correct the non-orthogonality on <ref> [0; 1] </ref> of the restriction to [0; 1] of basis functions that intersect [0; 1] c . To preserve the important property [W1] of orthogonality to polynomials of degree M , a further `preconditioning' transformation P of the data y is necessary. <p> Other near-minimax properties are described in detail in our report [asymp.tex]. 18 4.6 Boundary correction As described in the Introduction, Cohen, Daubechies, Jawerth and Vial (1993), have introduced separate `boundary filters' to correct the non-orthogonality on <ref> [0; 1] </ref> of the restriction to [0; 1] of basis functions that intersect [0; 1] c . To preserve the important property [W1] of orthogonality to polynomials of degree M , a further `preconditioning' transformation P of the data y is necessary. <p> We consider two ranges. For 2 <ref> [1; 1] </ref>, the numerator ST ( 0 n ; ) is monotone increas ing in , and the denominator is constant. For 2 [0; 1], we apply (39) to ST ( 0 n ; ). <p> We consider two ranges. For 2 [1; 1], the numerator ST ( 0 n ; ) is monotone increas ing in , and the denominator is constant. For 2 <ref> [0; 1] </ref>, we apply (39) to ST ( 0 n ; ). An argument similar to that following (43) shows that p (n 1=2 ) 0 for n 3 so that 0 n n 1=2 . <p> Combining this with (39), L ( 0 ( 0 n 1 + 2 n ( 0 so that L attains its maximum over 2 <ref> [0; 1] </ref> at 0, establishing the required equivalence. 5.5 Theorem 3 The main idea is to make a random variable, with prior distribution chosen so that a randomly selected subset of about log n coordinates are each of size roughly (2 log n) 1=2 , and to derive information from the <p> For ST , we require that c &lt; 5 and for HT , that c &lt; 1. For 2 <ref> [(2 log n) 1=2 ; 1] </ref>, the numerator of L is bounded above by 1 + 2 (from (37)) and the denominator is bounded below by 2 log n=(2 log n + 1). <p> For 2 [(2 log n) 1=2 ; 1], the numerator of L is bounded above by 1 + 2 (from (37)) and the denominator is bounded below by 2 log n=(2 log n + 1). For 2 <ref> [1; (2 log n) 1=2 ] </ref>, bound the numerator by (38) to obtain L (; ) 2 (1 + 2 ) 2 (2 log n)f1 + o (1)g: For 2 [0; 1], use (39): ( ; 0) (; ) (; 0) n (; 0) + 2c 2 : (46) If n <p> For 2 [1; (2 log n) 1=2 ], bound the numerator by (38) to obtain L (; ) 2 (1 + 2 ) 2 (2 log n)f1 + o (1)g: For 2 <ref> [0; 1] </ref>, use (39): ( ; 0) (; ) (; 0) n (; 0) + 2c 2 : (46) If n (c) = (2 log n c log log n) 1=2 , then n ( n (c)) = (0)(log n) c=2 . <p> The expansion (23) shows that this range includes fl n and hence ^ fl . 5.7 Theorem 7 When 2 = (2 log n) 1=2 , the bounds over <ref> [1; (2 log n) 1=2 ] </ref> and [(2 log n) 1=2 ; 1] in the previous section become simply [1 + 2 log n] 2 =2 log n 2 log n + 2:4 for n 4. <p> The expansion (23) shows that this range includes fl n and hence ^ fl . 5.7 Theorem 7 When 2 = (2 log n) 1=2 , the bounds over [1; (2 log n) 1=2 ] and <ref> [(2 log n) 1=2 ; 1] </ref> in the previous section become simply [1 + 2 log n] 2 =2 log n 2 log n + 2:4 for n 4. For 2 [0; 1], the bounds follow by direct evaluation from (46), (40) and (41). <p> For 2 <ref> [0; 1] </ref>, the bounds follow by direct evaluation from (46), (40) and (41). We note that these bounds can be improved slightly by considering the cases separately.
Reference: [2] <author> BREIMAN, L., FRIEDMAN, J.H., OLSHEN, R.A.,& STONE, C.J. </author> <year> (1983). </year> <title> CART: Classification and Regression Trees. </title> <publisher> Wadsworth: </publisher> <address> CBelmont, CA. </address>
Reference-contexts: Note that L is a variable. The reconstruction formula is T P C (y; ffi)(t) = `=1 piecewise constant reconstruction using the mean of the data within each piece to estimate the pieces. <ref> [2] </ref>. Piecewise Polynomials T P P (D) (y; ffi). Here the interpretation of ffi is the same as in [1], only the reconstruction uses polynomials of degree D. <p> duplicate essential features of certain well-known methods. [1] The piecewise constant reconstruction formula T P C , equipped with choice of partition ffi by recursive partitioning and cross-validatory choice of "pruning constant" as described by Breiman, Friedman, Olshen and Stone (1983) results in the method CART applied to 1-dimensional data. <ref> [2] </ref> The spline reconstruction formula T spl;D , equipped with a backwards deletion scheme models the methods of Friedman and Silverman (1989) and Friedman (1991) applied to 1-dimensional data. [3] The kernel method T K;2 equipped with the variable bandwidth selector described in Brockmann, Gasser and Herrmann (1992) results in the
Reference: [3] <author> BROCKMANN, M., GASSER, T., & HERRMANN, E. </author> <year> (1992). </year> <title> Locally Adaptive Bandwidth Choice for Kernel Regression Estimators. </title> <note> To appear. J. Amer. Statist. Assoc.. </note>
Reference-contexts: T P P (D) (y; ffi)(t) = `=1 2 where ^p ` (t) = P D k=0 a k t k is determined by applying the least squares principle to the data arising for interval I ` X t i 2I ` <ref> [3] </ref>. Variable-Knot Splines T spl;D (y; ffi). Here ffi defines a partition as above, and on each interval of the partition the reconstruction formula is a polynomial of degree D, but now the reconstruction must be continuous and have continuous derivatives through order D 1. <p> of "pruning constant" as described by Breiman, Friedman, Olshen and Stone (1983) results in the method CART applied to 1-dimensional data. [2] The spline reconstruction formula T spl;D , equipped with a backwards deletion scheme models the methods of Friedman and Silverman (1989) and Friedman (1991) applied to 1-dimensional data. <ref> [3] </ref> The kernel method T K;2 equipped with the variable bandwidth selector described in Brockmann, Gasser and Herrmann (1992) results in the "Heidelberg" variable bandwidth smoothing method. Compare also Terrell and Scott (1992). 3 These schemes are computationally feasible and intuitively appealing.
Reference: [4] <author> BROWN, L.D. & LOW, M.G. </author> <year> (1993). </year> <title> Superefficiency and lack of adaptability in non-parametric functional estimation. </title> <note> To appear, Annals of Statistics. </note>
Reference-contexts: those piecewise polynomials s (t) satisfying dt s (t ` ) = d k ! for k = 0; : : : ; D 1, ` = 2; : : :; L; subject to this constraint, one solves n X (s (t i ) y i ) 2 = min! <ref> [4] </ref>. Variable Bandwidth Kernel Methods T V K;2 (y; ffi).
Reference: [5] <author> COHEN, A., DAUBECHIES, I., JAWERTH, B. & VIAL, P. </author> <year> (1993). </year> <title> Multiresolution analysis, wavelets, and fast algorithms on an interval. </title> <journal> Comptes Rendus Acad. Sci. Paris (A). </journal> <volume> 316., </volume> <pages> 417-421. </pages>
Reference-contexts: compact support which is also a probability density, and if ^ f = T V K;2 (y; ffi) then ^ f (t) = n i=1 ffi (t) ffi (t): (3) More refined versions of this formula would adjust K for boundary effects near t = 0 and t = 1. <ref> [5] </ref>. Variable-Bandwidth High-Order Kernels T V K;D (y; ffi), D &gt; 2.
Reference: [6] <author> CHUI, C.K. </author> <year> (1992)., </year> <title> An Introduction to Wavelets. </title> <publisher> Academic Press, </publisher> <address> Boston, MA. </address>
Reference: [7] <author> DAUBECHIES, I. </author> <year> (1988). </year> <title> Orthonormal bases of compactly supported wavelets. </title> <note> Communications in Pure and Applied Mathematics 41, </note> <month> Nov. </month> <year> 1988, </year> <pages> pp. 909-996. </pages>
Reference: [8] <author> DAUBECHIES, I. </author> <year> (1992). </year> <title> Ten Lectures on Wavelets SIAM: </title> <address> Philadelphia. </address>
Reference: [9] <author> DAUBECHIES, I. </author> <year> (1993). </year> <title> Orthonormal Bases of Compactly Supported Wavelets II: Variations on a theme. </title> <journal> SIAM J. Math. Anal., </journal> <volume> 24, </volume> <pages> 499-519. </pages>
Reference: [10] <author> EFROIMOVICH, S. YU. & PINSKER, M.S. </author> <year> (1984). </year> <title> A learning algorithm for non-parametric filtering. </title> <journal> Automat. </journal> <note> i Telemeh. 11 58-65 (in Russian). 25 </note>
Reference: [11] <author> FRAZIER M., JAWERTH B., & WEISS G. </author> <year> (1991). </year> <title> Littlewood-Paley Theory and the study of function spaces. </title> <booktitle> NSF-CBMS Regional Conf. Ser in Mathematics, </booktitle> <volume> 79. </volume> <publisher> American Math. Soc.: </publisher> <address> Providence, RI. </address>
Reference: [12] <author> FRIEDMAN, J.H. & SILVERMAN, B.W. </author> <year> (1989). </year> <title> Flexible Parsimonious Smoothing and Additive Modeling. (with discussion). </title> <type> Technometrics 31, </type> <pages> 3-39. </pages>
Reference: [13] <author> FRIEDMAN, J.H. </author> <year> (1991). </year> <title> Multiple Additive Regression Splines (with discussion). </title> <journal> Annals of Statistics, </journal> <volume> 19, </volume> <pages> 1-67. </pages>
Reference: [14] <author> GEORGE, E. I. & Foster, D. P.(1990). </author> <title> The risk inflation of variable selection in regression. </title> <type> Technical Report, </type> <institution> University of Chicago. </institution>
Reference: [15] <author> LEPSKII, O.V. </author> <year> (1990). </year> <title> On one problem of adaptive estimation on white Gaussian noise. </title> <journal> Teor. </journal> <note> Veoryatnost. i Primenen. 35 459-470 (in Russian). Theory of Probability and Appl. 35, 454-466 (in English). </note>
Reference: [16] <author> MALGOUYRES, G. </author> <year> (1991). </year> <title> Ondelettes sur l'Intervalle: </title> <institution> algorithmes rapides. Prepublications Mathematiques Orsay. </institution>
Reference: [17] <author> MEYER, Y. </author> <year> (1990). </year> <editor> Ondelettes et Operateurs: I. Ondelettes Hermann et Cie, </editor> <address> Paris. </address>
Reference: [18] <author> MEYER, </author> <title> Yves (1991). Ondelettes sur l'intervalle. </title> <booktitle> Revista Matematica Ibero-Americana 7 (2), </booktitle> <pages> 115-133. </pages>
Reference: [19] <author> MILLER, A.J. </author> <year> (1984). </year> <title> Selection of subsets of regression variables (with discussion). </title> <editor> J. R. Statist. </editor> <publisher> Soc. A., 147,389-425. </publisher>
Reference: [20] <author> MILLER, A.J. </author> <year> (1990). </year> <title> Subset Selection in Regression. </title> <publisher> Chapman and Hall. </publisher> <address> London, New York. </address>
Reference: [21] <author> M ULLER, Hans-Georg & STADTMULLER, Ulrich. </author> <year> (1987). </year> <title> Variable bandwidth kernel estimators of regression curves. </title> <journal> Ann. Statist., </journal> <volume> 15(1), </volume> <pages> 182-201. </pages>

References-found: 21

