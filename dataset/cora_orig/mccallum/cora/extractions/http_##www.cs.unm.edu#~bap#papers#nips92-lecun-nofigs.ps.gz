URL: http://www.cs.unm.edu/~bap/papers/nips92-lecun-nofigs.ps.gz
Refering-URL: http://www.cs.unm.edu/~bap/publications.html
Root-URL: http://www.cs.unm.edu
Phone: 2  
Title: Automatic Learning Rate Maximization by On-Line Estimation of the Hessian's Eigenvectors  
Author: Yann LeCun, Patrice Y. Simard, and Barak Pearlmutter 
Address: 101 Crawfords Corner Rd, Holmdel, NJ 07733  19600 NW vonNeumann Dr, Beaverton, OR 97006  
Affiliation: 1 AT&T Bell Laboratories  CS&E Dept. Oregon Grad. Inst.,  
Abstract: We propose a very simple, and well principled way of computing the optimal step size in gradient descent algorithms. The on-line version is very efficient computationally, and is applicable to large backpropagation networks trained on large data sets. The main ingredient is a technique for estimating the principal eigenvalue(s) and eigenvector(s) of the objective function's second derivative matrix (Hessian), which does not require to even calculate the Hessian. Several other applications of this technique are proposed for speeding up learning, or for eliminating useless parameters. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Becker, S. and Le Cun, Y.1988. </author> <title> Improving the Convergence of Back-Propagation Learning with Second-Order Methods. </title> <type> Technical Report CRG-TR-88-5, </type> <institution> University of Toronto Connectionist Research Group. </institution>
Reference-contexts: As many authors have shown, Stochastic Gradient Descent (SGD) is much faster on large problems than the "batch" version. In fact, on large problems, a carefully tuned SGD algorithm outperforms most accelerated or second-order batch techniques, including Conjugate Gradient. Although there have been attempts to "stochasticize" second-order algorithms <ref> (Becker and Le Cun1988) </ref> (Moller1992), most of the resulting procedures also rely on a global scaling parameter similar to j.
Reference: <author> Hassibi, B. and Stork, D.1993. </author> <title> Optimal Brain Surgeon. </title> <editor> In Giles, L., Hanson, S., and Cowan, J., editors, </editor> <booktitle> Advances in Neural Information Processing Systems, volume 5, </booktitle> <address> (Denver, 1992). </address> <publisher> Morgan Kaufman. </publisher>
Reference-contexts: There are obvious applications of this to weight elimination methods: a better version of OBD (Le Cun et al.1990b) or a more efficient version of OBS <ref> (Hassibi and Stork1993) </ref>. We have proposed efficient methods for (a) computing the product of the Hessian by any vector, and (b) estimating the few eigenvectors of largest or smallest eigenvalues.
Reference: <author> Jacobs, R. A.1987. </author> <title> Increased Rates of Convergence Through Learning Rate Adaptation. </title> <institution> Department of Computer and Information Sciences COINS-TR-87-117, University of Massachusetts, </institution> <address> Amherst, Ma. </address>
Reference: <author> Le Cun, Y.1987. </author> <title> Modeles connexionnistes de l'apprentissage (connectionist learning models). </title> <type> PhD thesis, </type> <institution> Universite P. et M. Curie (Paris 6). </institution>
Reference: <author> Le Cun, Y., Boser, B., Denker, J. S., Henderson, D., Howard, R. E., Hubbard, W., and Jackel, L. D.1990a. </author> <title> Handwritten digit recognition with a back-propagation network. </title> <editor> In Touretzky, D., editor, </editor> <booktitle> Advances in Neural Information Processing Systems 2 (NIPS*89), </booktitle> <address> Denver, </address> <publisher> CO. Morgan Kaufman. </publisher>
Reference-contexts: Inputs to the networks were 28x28 pixel images containing a centered and size-normalized image of the character. Network 1 was a 4-hidden layer, locally-connected network with shared weights similar to <ref> (Le Cun et al.1990a) </ref> but with fewer feature maps. Each layer is only connected to the layer above. the input is 32x32 (there is a border around the 28x28 image), layer 1 is 2x28x28, with 5x5 convolutional (shared) connections. Layer 2 is 2x14x14 with 2x2 subsampled, averaging connections.
Reference: <author> Le Cun, Y., Denker, J. S., Solla, S., Howard, R. E., and Jackel, L. D.1990b. </author> <title> Optimal Brain Damage. </title> <editor> In Touretzky, D., editor, </editor> <booktitle> Advances in Neural Information Processing Systems 2 (NIPS*89), </booktitle> <address> Denver, </address> <publisher> CO. Morgan Kaufman. </publisher>
Reference-contexts: This can be used to determine the direction (s) of displacement in parameter space that will cause the least increase of the objective function. There are obvious applications of this to weight elimination methods: a better version of OBD <ref> (Le Cun et al.1990b) </ref> or a more efficient version of OBS (Hassibi and Stork1993). We have proposed efficient methods for (a) computing the product of the Hessian by any vector, and (b) estimating the few eigenvectors of largest or smallest eigenvalues.
Reference: <author> Le Cun, Y., Kanter, I., and Solla, S.1991. </author> <title> Eigenvalues of covariance matrices: application to neural-network learning. </title> <journal> Physical Review Letters, </journal> <volume> 66(18) </volume> <pages> 2396-2399. Moller, </pages> <year> M.1992. </year> <title> supervised learning on large redundant training sets. In Neural Networks for Signal Processing 2. </title> <publisher> IEEE press. Pearlmutter, </publisher> <year> B.1993. </year> <type> Phd thesis, </type> <institution> Carnegie-Mellon University, </institution> <address> Pittsburgh PA. </address>
Reference-contexts: This can be performed by projecting each k onto the space orthogonal to the space subtended by the l ; l &lt; k. This is an N K process, which is relatively cheap if the network uses shared weights. A generalization of the acceleration method introduced in <ref> (Le Cun, Kanter and Solla1991) </ref> can be implemented with this technique.
Reference: <author> Widrow, B. and Stearns, S. D.1985. </author> <title> Adaptive Signal Processing. </title> <publisher> Prentice-Hall. </publisher>
Reference-contexts: In this section we informally explain why the best learning rate is the inverse of this eigenvalue. More detailed analysis of gradient descent procedures can be found in Optimization, Statistical Estimation, or Adaptive Filtering textbooks (see for example <ref> (Widrow and Stearns1985) </ref>). For didactical purposes, consider an objective function of the form E (w) = h 2 (w z) 2 + C where w is a scalar parameter (see fig 1 (a)).
References-found: 8

