URL: ftp://www.cs.rutgers.edu/pub/technical-reports/ml-tr-38.ps.Z
Refering-URL: http://www.cs.rutgers.edu/pub/technical-reports/
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Email: gweiss@paul.rutgers.edu  
Title: The Problem with Noise and Small Disjuncts  
Author: Gary M. Weiss 
Affiliation: AT&T Bell Labs/Rutgers University  
Abstract: Systems that learn from examples often create a disjunctive concept definition. The disjuncts in the concept definition which cover only a few training examples are referred to as small disjuncts. The problem with small disjuncts is that they are more error prone than large disjuncts, but may be necessary to achieve a high level of predictive accuracy [Holte, Acker, and Porter, 1989]. This paper extends previous work done on the problem of small disjuncts by taking noise into account. It investigates the assertion that it is hard to learn from noisy data because it is difficult to distinguish between noise and true exceptions. In the process of evaluating this assertion, insights are gained into the mechanisms by which noise affects learning. Two domains are investigated. The experimental results in this paper suggest that for both Shapiro's chess endgame domain [Shapiro, 1987] and for the Wisconsin breast cancer domain [Wolberg, 1990], the assertion is true, at least for low levels (5-10%) of class noise. 
Abstract-found: 1
Intro-found: 1
Reference: [Danyluk and Provost, 1993] <author> Danyluk, A.P. and Provost, F.J., </author> <title> Small Disjuncts in Action: Learning to Diagnose Errors in the Local Loop of the Telephone Network, </title> <booktitle> in Proceedings of the Tenth International Conference on Machine Learning. </booktitle>
Reference-contexts: Section 4 will empirically test whether the observations made by Holte, et al., hold true for the domains used in this paper. 2.2 The Problem with Noise and Small Disjuncts The initial motivation for this paper came from <ref> [Danyluk and Provost, 1993] </ref>. That paper described the NYNEX MAX system, which is used for diagnosing problems in the local loop of the telephone system.
Reference: [Holte et al., 1989] <author> Holte, R.C., Acker, L.E., and Porter, B.W., </author> <title> Concept Learning and the Problem of Small Disjuncts, </title> <booktitle> in Proceedings of the Eleventh International Joint Conference on Artificial Intelligence, </booktitle> <address> p813-818. San Mateo, California, </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Some of these subconcepts may describe only a few cases, whereas others may describe many cases. The subconcepts which individually cover only a small fraction of the overall cases are called small disjuncts. As defined in <ref> [Holte et al., 1989] </ref>, the coverage of a disjunct is defined as the number of training examples it correctly classifies. That paper showed that, although small disjuncts individually cover only a small fraction of the examples, collectively they can cover a significant percentage (e.g., 20%) of the total examples. <p> However, since this definition of coverage is defined over the test set, it has the disadvantage of not being able to be employed in the learning strategy (which isn't a __________________ 1. Both <ref> [Holte et al., 1989] </ref> and this paper use the KPa7KR chess end game domain, with 200 training and 3000 test cases. <p> Finally, the error rate is high for small disjuncts, and, for the most part, drops as the disjunct size increases. These results agree with those from <ref> [Holte et al., 1989] </ref>. Figures 1 and 2 show that small disjuncts cover a significant percentage of the cases.
Reference: [Langley and Kibler, 1991] <author> Langley, P. and Kibler, D., </author> <title> The Experimental Study of Machine Learning. </title>
Reference-contexts: The default value for the -m option is 2, which stops a node from being split if the resulting nodes will have 2 or fewer outcomes. 3. This definition is consistent with the one used in <ref> [Langley and Kibler, 1991] </ref>, but is very different from the one in [Quinlan, 1986], in which the same value can be selected again.
Reference: [Quinlan, 1986] <author> Quinlan, J.R., </author> <title> The Effect of Noise on Concept Learning, </title> <booktitle> in Machine Learning, an Artificial Intelligence Approach (Volume II), </booktitle> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: The default value for the -m option is 2, which stops a node from being split if the resulting nodes will have 2 or fewer outcomes. 3. This definition is consistent with the one used in [Langley and Kibler, 1991], but is very different from the one in <ref> [Quinlan, 1986] </ref>, in which the same value can be selected again. For classes with two values, this makes a factor of 2 difference. -4- In all cases, the resulting overall noise level may vary slightly from the specified level, since the noise is applied to each case independently. <p> Frequently, however, when the effects of noise are studied, noise is applied to the training set but not to the test set <ref> [Quinlan, 1986] </ref>. In this case, what is being studied is the ability to learn the "correct" concept (i.e., the noise free definition) when noise is present. -9- Noise can be thought of as having two distinct, albeit interacting, effects: 1.
Reference: [Quinlan, 1993] <author> Quinlan, J.R., C4.5: </author> <title> Programs for Machine Learning, </title> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Description of Experiments This section will describe the inductive learning program used, the problem domains, and the experimental methodology. 3.1 C4.5 C4.5, a descendant of ID3, is a program for inducing decision trees from a set of preclassified training examples <ref> [Quinlan, 1993] </ref>. C4.5 is used for all of the experiments in this paper, and was modified by the author to collect statistics relating to disjunct size and to optionally disable the default pruning strategy (pruning would obscure the small -3- disjuncts in the underlying concept definition).
Reference: [Schaffer, 1992] <author> Schaffer, C., </author> <title> Sparse Data and the Effect of Overfitting Avoidance in Decision Tree Induction, </title> <booktitle> in Proceedings of the Tenth National Conference on Artificial Intelligence (AAAI-92). </booktitle>
Reference: [Shapiro, 1987] <author> Shapiro, </author> <title> A.D., Structured Induction in Expert Systems, </title> <publisher> Addison-Wesley. </publisher>
Reference-contexts: a few experiments, pruning is disabled and larger values are given to the -m option, to simulate a more aggressive overfitting avoidance strategy. 3.2 The Problem Domains 3.2.1 KPa7KR Chess Endgame The first problem domain is the chess endgame King+Rook versus King+Pawn on a7 (abbreviated KPa7KR) described by Shapiro in <ref> [Shapiro, 1987] </ref>. There are 3196 total examples, each of which represents a board position.
Reference: [Weiss, 1995] <author> Weiss, G.M., </author> <title> Learning with Small Disjuncts, </title> <type> (Technical Report ML-TR-39). </type> <institution> New Brunswick, NJ: Rutgers University, Dept. of Computer Science. </institution>
Reference-contexts: In addition, the effect of using alternative overfitting avoidance strategies should be investigated, since some of the results in this paper depend on C4.5's default pruning strategy ([Holte, et al., 1989] suggest using strategies which test both significance and error-rate). Some further research has already been undertaken. <ref> [Weiss, 1995] </ref> examines the effect of systematic and random attribute noise as well as training set size on learning with small disjuncts, using two artificial domains. 7. Conclusion This paper investigated the assertion that true exceptions make learning from noisy data difficult. <p> However, these results need to be kept in perspective the assertion was evaluated by examining the behavior of the small disjuncts, not by comparing the results of learning on a domain with true exceptions and then with the true exceptions removed (see <ref> [Weiss, 1995] </ref> for this). This paper also showed some trends and effects which are likely to exist independent of a particular domain.
Reference: [Wolberg, 1990] <author> Wolberg, </author> <title> W.H. and Mangasarian, O.L., Multisurface Method of Pattern Separation for Medical Diagnosis Applied to Breast Cytology, </title> <booktitle> in Proceedings of the National Academy of Sciences, U.S.A. </booktitle> <volume> (Vol 87), </volume> <pages> pages 9193-9196. </pages>
Reference-contexts: Each example has 36 attributes, and belongs to the class "won" or "nowin" (the class distribution is approximately equal). 3.2.2 Wisconsin Breast Cancer Database The Wisconsin breast cancer database was obtained from the University of Wisconsin hospitals, Madison, from Dr. William Wolberg <ref> [Wolberg, 1990] </ref>. There are 699 total examples. Each example has nine attributes (sample number is omitted), and belongs to the class "benign" or "malignant" (about 2/3 of the examples are benign).
References-found: 9

