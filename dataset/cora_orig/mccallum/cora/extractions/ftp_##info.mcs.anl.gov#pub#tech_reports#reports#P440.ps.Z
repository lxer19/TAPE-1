URL: ftp://info.mcs.anl.gov/pub/tech_reports/reports/P440.ps.Z
Refering-URL: http://www.mcs.anl.gov/publications/abstracts/abstracts94.htm
Root-URL: http://www.mcs.anl.gov
Title: Task Parallelism and High-Performance Languages  
Author: Ian Foster 
Address: Argonne, IL 60439  
Affiliation: Mathematics and Computer Science Division Argonne National Laboratory  
Abstract: High-Performance Fortran as currently defined cannot be used to solve all programming problems. However, its focus on regular problems and data-parallel algorithms stems not from a belief that it is only these problems that matter, but rather from the fact that it was in this area that it was easiest to build consensus as to what was required in a language for high-performance computing. In future work, this initial consensus will have to be extended. One direction to be considered in the next round of HPF Forum meetings is task parallelism. In this paper, we examine and illustrate the considerations that motivate the use of task parallelism. We also describe one particular approach to task parallelism in Fortran, namely the Fortran M extensions. Finally, we contrast Fortran M with other proposed approaches and discuss the implications of this work for task parallelism and high-performance languages. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> ANSI Technical Committee X3H5, </author> <title> Parallel Processing Model for High Level Programming Models, </title> <year> 1992. </year>
Reference-contexts: For ex-ample, in the shared-memory extensions proposed by ANSI committee X3H5 <ref> [1] </ref>, programs can create explicit threads, which execute concurrently and interact by reading and writing shared data structures. It is the programmer's responsibility to prevent unwanted race conditions by using explicit locks or other mutual exclusion constructs to control access to shared data.
Reference: [2] <author> K. M. Chandy and C. Kesselman, </author> <title> CC++: A declarative concurrent object-oriented programming notation, Research Directions in Concurrent Object-Oriented Programming, </title> <editor> Gul Agha, Peter Wegner, and Akinori Yonezawa (eds.), </editor> <publisher> MIT Press, </publisher> <year> 1993. </year>
Reference-contexts: One candidate set of mechanisms uses single assignment variables for synchronization and remote procedure calls for communication <ref> [2] </ref>. Runtime and compiler design issues must also be addressed. An integrated task/data-parallel language may create concurrent task- and data-parallel computations on the same or different processors. The programmer, compiler, and runtime system need to cooperate to ensure efficient scheduling of these different computations.
Reference: [3] <author> B. Chapman, P. Mehrotra, J. van Rosendale, and H. Zima, </author> <title> A software architecture for multidisciplinary applications: Integrating task and data parallelism, </title> <type> Technical Report 94-18, </type> <institution> ICASE, MS 132C, NASA Langley Research Center, Hampton, Va., </institution> <year> 1994. </year>
Reference-contexts: It is the programmer's responsibility to prevent unwanted race conditions by using explicit locks or other mutual exclusion constructs to control access to shared data. Chapman et al. <ref> [3] </ref> have proposed mechanisms based on "spawn" and "shared data abstraction" constructs. The shared data abstraction, a form of monitor, is used to control interactions between tasks created using spawn. Again, the programmer must use these constructs in a structured fashion to ensure deterministic execution.
Reference: [4] <author> I. Foster, B. Avalani, A. Choudhary, and M. Xu, </author> <title> A compilation system that integrates High Performance Fortran and Fortran M, </title> <booktitle> Proc. 1994 Scalable High Performance Computing Conf., IEEE, </booktitle> <year> 1994. </year>
Reference-contexts: FM has also been used as a task-parallel coordination language for HPF <ref> [4] </ref>. FM is a small set of extensions to Fortran 77 (F77) for specifying concurrent execution, communication, synchronization, and resource management. A major design goal was to define extensions consistent with F77 concepts. <p> The same basic concepts can be used to integrate HPF into the multiparadigm framework. This integration has been demonstrated in a prototype compilation system developed with Bhaven Avalani and Alok Choudhary of Syracuse University <ref> [4] </ref>. HPF procedures are compiled to message-passing code using an HPF compiler; the message-passing code is then linked with both FM coordination code and interface routines that handle data transfer between the two languages.
Reference: [5] <author> I. Foster and K. M. Chandy. </author> <title> Fortran M: A language for modular parallel programming, </title> <journal> J. Parallel and Distributed Computing, </journal> <year> 1994. </year>
Reference-contexts: engineering advantages to formulating a multigrid code as a task-parallel collection of interacting data-parallel programs; task parallelism can also be used to improve performance by executing multiple grids concurrently to improve locality, and by scheduling these computations efficiently. 4 The Fortran M Approach The Fortran M (FM) extensions to Fortran <ref> [5] </ref> represent a particularly simple approach to task parallelism in Fortran. FM has also been used as a task-parallel coordination language for HPF [4]. FM is a small set of extensions to Fortran 77 (F77) for specifying concurrent execution, communication, synchronization, and resource management.
Reference: [6] <author> D. Gannon et al., </author> <title> Implementing a parallel C++ runtime system for scalable parallel systems, </title> <booktitle> Proc. Supercomputing '93, IEEE, </booktitle> <year> 1993. </year>
Reference-contexts: HPF has also stimulated similar developments in other languages, notably pC++ <ref> [6] </ref>. These languages are commonly referred to as high-performance languages, because they are designed to allow efficient compilation for high-performance parallel computers. HPF as currently defined cannot be used to solve all programming problems. Indeed, its focus on regular problems and data-parallel algorithms makes it dangerously limited.
Reference: [7] <author> P. Hatcher and M. Quinn, </author> <title> Data-Parallel Programming on MIMD Computers, </title> <publisher> MIT Press, </publisher> <year> 1991. </year>
Reference-contexts: Computations that apply the same operation to all elements of regular data structures can be expressed elegantly using data-parallel constructs and can be compiled efficiently for parallel computers. For these problems, there is plentiful evidence that data parallelism is an effective solution <ref> [7] </ref>. Unfortunately, heterogeneity | whether in data structures, computation, or data dependencies | appears to be equally prevalent, particularly as high-performance computers are used to solve more complex problems and as scientists and engineers use more sophisticated algorithms.
Reference: [8] <author> C. Koelbel, D. Loveman, R. Schreiber, G. Steele, and M. Zosel, </author> <title> The High Performance Fortran Handbook, </title> <publisher> MIT Press, </publisher> <year> 1994. </year> <month> 15 </month>
Reference-contexts: Finally, we contrast Fortran M with other proposed approaches and discuss the implications of this work for task parallelism and high-performance languages. 2 High Performance Fortran We first provide a brief review of HPF <ref> [8] </ref>. As currently defined, this is primarily a data-parallel language, meaning that it allows programmers to exploit the concurrency that derives from the application of the same operations to all or most elements of large data structures.
Reference: [9] <author> C. Pancake and D. Bergmark, </author> <title> Do parallel languages respond to the needs of scientific programmers?, </title> <booktitle> Computer 23(12), </booktitle> <pages> 13-23, </pages> <year> 1990. </year>
Reference-contexts: In the vast majority of cases, this is the desired behavior: few parallel algorithms in science and engineering are nondeterministic, and indeed unwanted nondeterminism ("race conditions") in parallel programs has historically been a major source of problems <ref> [9] </ref>. Motivated by these observations, we chose when designing FM to make determinism the default behavior.

References-found: 9

