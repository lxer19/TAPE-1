URL: http://kmi.open.ac.uk/techreports/papers/kmi-tr-28.ps.gz
Refering-URL: http://kmi.open.ac.uk/techreports/kmi-tr-list.html
Root-URL: 
Title: Robust Learning with Missing Data  
Author: Marco Ramoni Paola Sebastiani 
Date: 22 July 1996  
Affiliation: Knowledge Media Institute  
Pubnum: KMI-TR-28  
Abstract-found: 0
Intro-found: 1
Reference: [Berger, 1984] <author> J.O. Berger. </author> <title> The robust bayesian viewpoint. </title> <editor> In J.B. Kadane, editor, </editor> <title> Robustness of Bayesian Analyses. </title> <publisher> Elsevier, </publisher> <year> 1984. </year>
Reference-contexts: The most conservative interpretation of Convex Bayesianism is the so called Sensitivity Analysis or Robust Bayesian approach <ref> [Berger, 1984] </ref>, within which the set P (x) is regarded as a set of precise probability functions and Robust Learning with Missing Data 10 inference is carried out over the possible combinations of these probability functions.
Reference: [Breeze and Fertig, 1990] <author> J. Breeze and K.W. Fertig. </author> <title> Decision making with interval influence diagrams. </title> <booktitle> In Proceedings of the Conference on Uncertainty in Artificial Intelligence, </booktitle> <pages> pages 122-129, </pages> <year> 1990. </year>
Reference-contexts: A natural evolution of this approach leaded to the combination of the computational advantages provided by conditional independence assumptions with the expressive power of probability intervals [van der Gaag, 1991]. Therefore, some efforts have been addressed to extend bbns from real-valued probabilities to interval probabilities <ref> [Breeze and Fertig, 1990, Ramoni, 1995] </ref>, thus combining the advantages of conditional independence assumptions with the explicit representation of ignorance.
Reference: [Buntine, 1994] <author> W. L. Buntine. </author> <title> Operations for learning with graphical models. </title> <journal> Journal of Artificial Intelligence Research, </journal> <volume> 2 </volume> <pages> 159-225, </pages> <year> 1994. </year>
Reference-contexts: However, bbns have a strong statistical root and, during the past few years, this root prompted for the development of methods able to learn bbns directly from databases of cases rather than from the insight of human domain experts <ref> [Cooper and Herskovitz, 1992, Buntine, 1994, Heckerman and Chickering, 1995] </ref>.
Reference: [Buntine, 1996] <author> W. Buntine. </author> <title> A guide to the literature on learning probabilistic networs from data. </title> <address> ieeetdke, </address> <year> 1996. </year> <note> To appear. </note>
Reference-contexts: In practical applications, the algorithm iterates a number of times and then, when stability seems to be reached, a final sample from the joint posterior distribution of the parameters is taken <ref> [Buntine, 1996] </ref>. When some of the entries in the database are missing, the Gibbs Sampling treats the missing data as unknown parameters, so that, for each missing entry, a values is sampled from the conditional distribution of the corresponding variables, given all the parameters and the available data. <p> Robust Learning with Missing Data 20 5.1 Materials In order to experimentally compare our method to the Gibbs Sampling, we choose the program BUGS [Thomas et al., 1992] which is commonly regarded as a reliable implementation of such a technique <ref> [Buntine, 1996] </ref>. In the following experiments, we used the implementation of BUGS version 0.5 running on a Sun Sparc 5 under SunOS 5.5 and the era implementation of our method running on a Machintosh PowerBook 5300 under Machintosh Common Lisp version 3.9.
Reference: [Cooper and Herskovitz, 1992] <author> G.F. Cooper and E. Herskovitz. </author> <title> A bayesian method for the induction of probabilistic networks from data. </title> <journal> Machine Learning, </journal> <volume> 9 </volume> <pages> 309-347, </pages> <year> 1992. </year>
Reference-contexts: However, bbns have a strong statistical root and, during the past few years, this root prompted for the development of methods able to learn bbns directly from databases of cases rather than from the insight of human domain experts <ref> [Cooper and Herskovitz, 1992, Buntine, 1994, Heckerman and Chickering, 1995] </ref>. <p> Under certain constraints, this second task can be accomplished exactly and efficiently when the database is complete <ref> [Cooper and Herskovitz, 1992, Heckerman and Chickering, 1995] </ref>. We can call this assumption the "Database Completeness" assumption. Unfortunately, databases are rarely complete: unreported, lost, and corrupted data are a distinguished feature of real-world databases.
Reference: [Cowel et al., 1996] <author> R G Cowel, A.P. Dawid, and P. Sebastiani. </author> <title> A comparison of sequential learning methods for incomplete data. </title> <booktitle> Bayesian Statistics 5, </booktitle> <pages> pages 581-588, </pages> <year> 1996. </year>
Reference-contexts: During the past few years, several methods have been proposed to learn conditional probabilities in bbns from incomplete databases, either deterministic, such as sequential updating [Spiegelhalter and Lauritzen, 1990], and <ref> [Cowel et al., 1996] </ref>, or the EM-algorithm [Dempster et al., 1977], or stochastic such as the Gibbs Sampling [Neal, 1993]. All these methods share the common assumption that unreported data are missing at random. <p> Exact analysis would require to compute the joint posterior distribution of the parameters given each possible completion of the database, and then to mix these over all possible completions. This is apparently infeasible. A deterministic method, proposed by [Spiegelhalter and Lauritzen, 1990] and improved by <ref> [Cowel et al., 1996] </ref>, provides a way to approximate the exact posterior distribution by processing data sequentially. However, [Spiegelhalter and Cowel, 1992] show that this method is not robust enough to cope with systematically missing data: in this case the estimates rely heavily on the prior distribution.
Reference: [Dempster et al., 1977] <author> A. Dempster, D. Laird, and Rubin D. </author> <title> Maximum likelihood from incomplete data via the em algorithm. </title> <journal> J. Roy. Statist. Soc. B, </journal> <volume> 39 </volume> <pages> 1-38, </pages> <year> 1977. </year>
Reference-contexts: During the past few years, several methods have been proposed to learn conditional probabilities in bbns from incomplete databases, either deterministic, such as sequential updating [Spiegelhalter and Lauritzen, 1990], and [Cowel et al., 1996], or the EM-algorithm <ref> [Dempster et al., 1977] </ref>, or stochastic such as the Gibbs Sampling [Neal, 1993]. All these methods share the common assumption that unreported data are missing at random.
Reference: [Dempster, 1967] <author> A. P. Dempster. </author> <title> Upper and lower probabilities induced by multivalued mapping. </title> <journal> Annals of Mathematical Statistics, </journal> <volume> 38 </volume> <pages> 325-339, </pages> <year> 1967. </year>
Reference-contexts: The original interest for the notion of belief functions proposed by Dempster <ref> [Dempster, 1967] </ref> and further developed by Shafer [Shafer, 1976] was mainly due to its ability to represent credal states as intervals rather than point-valued probability measures [Grosof, 1986].
Reference: [Frankin et al., 1989] <author> R. C. G. Frankin, D J Spiegelhalter, F Macrtney, and K Bull. </author> <title> Combining clinical judgments and clinical data in expert systems. </title> <journal> Internation Journal of Clinical Monitoring and Computing, </journal> <volume> 6 </volume> <pages> 157-166, </pages> <year> 1989. </year>
Reference-contexts: The clinical problem underlying the bbn depicted in Figure 4 is described in <ref> [Frankin et al., 1989] </ref>. The task of the bbn is to diagnose the occurrence of congenital heart disease in pediatric patients using clinical data reported over the phone by referring pediatricians.
Reference: [Good, 1962] <author> I.J. </author> <title> Good. Subjective probability as a measure of a nonmeasurable set. </title> <editor> In E. Nagel, P. Suppes, and A. Tarsky, editors, </editor> <booktitle> Logic, Robust Learning with Missing Data 32 Methodology and Philosophy of Science, </booktitle> <pages> pages 319-329. </pages> <publisher> Stanford University Press, Stanford, </publisher> <address> CA, </address> <year> 1962. </year>
Reference-contexts: Good <ref> [Good, 1962] </ref> proposes to model this process as a "black box" which translates these probability functions into a set of constraints of an ideal, point-valued probability function. A review of these methods may be found in [Walley, 1991].
Reference: [Grosof, 1986] <author> B. N. Grosof. </author> <title> An inequality paradigm for probabilistic knowledge. </title> <editor> In L. N. Kanal and J. F. Lemmer, editors, </editor> <booktitle> Uncertainty in Artificial Intelligence, </booktitle> <pages> pages 259-275. </pages> <publisher> North-Holland, </publisher> <address> Amsterdam, </address> <year> 1986. </year>
Reference-contexts: The original interest for the notion of belief functions proposed by Dempster [Dempster, 1967] and further developed by Shafer [Shafer, 1976] was mainly due to its ability to represent credal states as intervals rather than point-valued probability measures <ref> [Grosof, 1986] </ref>. Several efforts have been addressed to interpret probability intervals within a coherent Bayesian framework, thus preserving the probabilistic soundness and the normative character of traditional probability and decision theory [Levi, 1980, Kyburg, 1983, Stiling and Morrel, 1991].
Reference: [Heckerman and Chickering, 1995] <author> D. Heckerman, D. amd Geiger and D.M. Chickering. </author> <title> Learning bayesian networks: The combinations of knowledge and statistical data. </title> <booktitle> Machine Learning, </booktitle> <year> 1995. </year>
Reference-contexts: However, bbns have a strong statistical root and, during the past few years, this root prompted for the development of methods able to learn bbns directly from databases of cases rather than from the insight of human domain experts <ref> [Cooper and Herskovitz, 1992, Buntine, 1994, Heckerman and Chickering, 1995] </ref>. <p> Under certain constraints, this second task can be accomplished exactly and efficiently when the database is complete <ref> [Cooper and Herskovitz, 1992, Heckerman and Chickering, 1995] </ref>. We can call this assumption the "Database Completeness" assumption. Unfortunately, databases are rarely complete: unreported, lost, and corrupted data are a distinguished feature of real-world databases.
Reference: [Kyburg, 1983] <author> H.E. Kyburg. </author> <title> Rational belief. </title> <journal> Behavioral and Brain Sciences, </journal> <volume> 6 </volume> <pages> 231-273, </pages> <year> 1983. </year>
Reference-contexts: Several efforts have been addressed to interpret probability intervals within a coherent Bayesian framework, thus preserving the probabilistic soundness and the normative character of traditional probability and decision theory <ref> [Levi, 1980, Kyburg, 1983, Stiling and Morrel, 1991] </ref>. This approach regards probability intervals as a convex sets of standard probability distributions and is therefore referred to as Convex Bayesianism.
Reference: [Levi, 1980] <author> I. Levi. </author> <title> The Enterprise of Knowledge. An Essay on Knowledge, Credal Probability and Chance. </title> <publisher> MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1980. </year>
Reference-contexts: Several efforts have been addressed to interpret probability intervals within a coherent Bayesian framework, thus preserving the probabilistic soundness and the normative character of traditional probability and decision theory <ref> [Levi, 1980, Kyburg, 1983, Stiling and Morrel, 1991] </ref>. This approach regards probability intervals as a convex sets of standard probability distributions and is therefore referred to as Convex Bayesianism.
Reference: [Neal, 1993] <author> R. Neal. </author> <title> Probabilistic inference using markov chain monte carlo methods. </title> <type> Technical report, </type> <institution> Department of Computer Science, University of Toronto, </institution> <year> 1993. </year>
Reference-contexts: During the past few years, several methods have been proposed to learn conditional probabilities in bbns from incomplete databases, either deterministic, such as sequential updating [Spiegelhalter and Lauritzen, 1990], and [Cowel et al., 1996], or the EM-algorithm [Dempster et al., 1977], or stochastic such as the Gibbs Sampling <ref> [Neal, 1993] </ref>. All these methods share the common assumption that unreported data are missing at random. Unfortunately this assumption is as unrealistic as the "Database Completeness" assumption because in the real world there is often a reason so that data are missing.
Reference: [Pearl, 1988] <author> J. Pearl. </author> <title> Probabilistic Reasoning in Intelligent Systems: Networks of plausible inference. </title> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, CA, </address> <year> 1988. </year>
Reference-contexts: In order to exploit this property, researchers developed a new formalism to capture, in graphical terms, the assumptions of independence in a domain, thus reducing the number of probabilities to be assessed. This formalism is known as Bayesian Belief Networks (bbns) <ref> [Pearl, 1988] </ref>. bbns provide a compact representation for encoding probabilistic information and they may be easily extended into a powerful decision-theoretic formalism called Influence Diagrams. More technically, a bbn is a direct acyclic graph in which nodes represent stochastic variables and links represent conditional dependencies among variables.
Reference: [Quinlan, 1984] <author> J. R. Quinlan. </author> <title> Learning efficient classification procedures and their application to chess and games. In R.S. </title> <editor> Michalsky, J.G Car-bonell, and T.M. Mitchell, editors, </editor> <booktitle> Machine Learning: An Artificial Intelligence Approach, </booktitle> <pages> pages 463-481. </pages> <publisher> Springer Verlag, </publisher> <address> Berlin, </address> <year> 1984. </year>
Reference-contexts: This task can be accomplished using common techniques in the machine learning community to estimate the information structure of classification trees <ref> [Quinlan, 1984] </ref>. 4.4 Implementation This method has been implemented in Common Lisp on a Machintosh Per-forma 6300 under Machintosh Common Lisp. A porting to CLISP running on a Sun Sparc Station is under development.
Reference: [Ramoni et al., 1995] <author> M. Ramoni, A. Riva, M. Stefanelli, and V. Patel. </author> <title> An ignorant belief network to forecast glucose concentration from clinical databases. </title> <journal> Artificial Intelligence in Medicine Journal, </journal> <volume> 7 </volume> <pages> 541-559, </pages> <year> 1995. </year>
Reference-contexts: In order to develop an efficient algorithm, we used discrimination trees to store the parameter counters, following a slightly modified version of the approach proposed by <ref> [Ramoni et al., 1995] </ref>. Along this approach, each state of each variable of the network is assigned to a discrimination tree. Each level of the discrimination tree is defined by the possible states of a parent variable.
Reference: [Ramoni, 1995] <author> M. Ramoni. </author> <title> Ignorant influence diagrams. </title> <booktitle> In Proceedings of the International Joint Conference on Artificial Intelligence, </booktitle> <pages> pages 1808-1814, </pages> <address> S. Mateo, CA, 1995. </address> <publisher> Morgan Kaufman. </publisher>
Reference-contexts: A natural evolution of this approach leaded to the combination of the computational advantages provided by conditional independence assumptions with the expressive power of probability intervals [van der Gaag, 1991]. Therefore, some efforts have been addressed to extend bbns from real-valued probabilities to interval probabilities <ref> [Breeze and Fertig, 1990, Ramoni, 1995] </ref>, thus combining the advantages of conditional independence assumptions with the explicit representation of ignorance. <p> A porting to CLISP running on a Sun Sparc Station is under development. The system has been implemented as a module of a general environment for probabilistic inference called Epistemic Refinment Architecture (era) <ref> [Ramoni, 1995] </ref>. era has been originally developed on a Sun Sparc 10 using the Lucid Common Lisp development environment. We were very careful to use only standard Common Lisp resources in order to develop code conforming to the new established ANSI standard.
Reference: [Shafer, 1976] <author> G. Shafer. </author> <title> A Mathematical Theory of Evidence. </title> <publisher> Princeton University Press, Princeton, </publisher> <year> 1976. </year>
Reference-contexts: The original interest for the notion of belief functions proposed by Dempster [Dempster, 1967] and further developed by Shafer <ref> [Shafer, 1976] </ref> was mainly due to its ability to represent credal states as intervals rather than point-valued probability measures [Grosof, 1986].
Reference: [Snow, 1991] <author> P. Snow. </author> <title> Improved posterior probability estimates from prior and linear constraint system. </title> <journal> IEEE Transactions on Systems, Man, and Cybernetics, </journal> <volume> 21 </volume> <pages> 464-469, </pages> <year> 1991. </year> <title> Robust Learning with Missing Data 33 </title>
Reference-contexts: The suitability of Convex Bayesianism to represent incomplete probabilistic information motivated the development of computational methods to reason on the basis of probability intervals <ref> [White, 1986, Snow, 1991] </ref>. A natural evolution of this approach leaded to the combination of the computational advantages provided by conditional independence assumptions with the expressive power of probability intervals [van der Gaag, 1991].
Reference: [Spiegelhalter and Cowel, 1992] <author> D J Spiegelhalter and R G Cowel. </author> <title> Learning in probabilistic expert systems. </title> <booktitle> Bayesian Statistics 4, </booktitle> <pages> pages 447-466, </pages> <year> 1992. </year>
Reference-contexts: This is apparently infeasible. A deterministic method, proposed by [Spiegelhalter and Lauritzen, 1990] and improved by [Cowel et al., 1996], provides a way to approximate the exact posterior distribution by processing data sequentially. However, <ref> [Spiegelhalter and Cowel, 1992] </ref> show that this method is not robust enough to cope with systematically missing data: in this case the estimates rely heavily on the prior distribution. When deterministic methods fail, current practice has to resort to an estimate of the posterior means using stochastic methods.
Reference: [Spiegelhalter and Lauritzen, 1990] <author> D.J. Spiegelhalter and S.L. Lauritzen. </author> <title> Sequential updating of conditional probabilities on directed graphical structures. </title> <journal> Networks, </journal> <volume> 20 </volume> <pages> 157-224, </pages> <year> 1990. </year>
Reference-contexts: In order to move on applications, methods to learn bbns have to face the challenge of learning from databases with unreported data. During the past few years, several methods have been proposed to learn conditional probabilities in bbns from incomplete databases, either deterministic, such as sequential updating <ref> [Spiegelhalter and Lauritzen, 1990] </ref>, and [Cowel et al., 1996], or the EM-algorithm [Dempster et al., 1977], or stochastic such as the Gibbs Sampling [Neal, 1993]. All these methods share the common assumption that unreported data are missing at random. <p> Assumption (i) allows to factorise the joint prior density of as () = i=1 thus allowing "local computations", while (ii) facilitates computations of the posterior density by taking advantages of conjugate analysis. Full details are given for instance by <ref> [Spiegelhalter and Lauritzen, 1990] </ref>. <p> Exact analysis would require to compute the joint posterior distribution of the parameters given each possible completion of the database, and then to mix these over all possible completions. This is apparently infeasible. A deterministic method, proposed by <ref> [Spiegelhalter and Lauritzen, 1990] </ref> and improved by [Cowel et al., 1996], provides a way to approximate the exact posterior distribution by processing data sequentially.
Reference: [Spiegelhalter et al., 1993] <author> D.J. Spiegelhalter, A.P. Dawid, S.L. Lauritzen, and R.G. Cowell. </author> <title> Bayesian analysis in expert systems. </title> <journal> Statistical Science, </journal> <volume> 8 </volume> <pages> 219-247, </pages> <year> 1993. </year>
Reference-contexts: Different materials used for each set of experiments will be illustrated during the description of each experiment. 5.2 Experiment 1: The CHILD Network In the first set of experiments, we used a well-known medical problem. This problem has been already used in the bbns literature <ref> [Spiegelhalter et al., 1993] </ref> and concerns the early diagnosis of congenital hearth disease in newborn babies.
Reference: [Stiling and Morrel, 1991] <author> W. Stiling and D. Morrel. </author> <title> Covex bayesian decision theory. </title> <journal> IEEE Transactions on Systems, Man, and Cybernetics, </journal> <volume> 21 </volume> <pages> 173-183, </pages> <year> 1991. </year>
Reference-contexts: Several efforts have been addressed to interpret probability intervals within a coherent Bayesian framework, thus preserving the probabilistic soundness and the normative character of traditional probability and decision theory <ref> [Levi, 1980, Kyburg, 1983, Stiling and Morrel, 1991] </ref>. This approach regards probability intervals as a convex sets of standard probability distributions and is therefore referred to as Convex Bayesianism.
Reference: [Thomas et al., 1992] <author> A Thomas, D J Spiegelhalter, and W R Gilks. </author> <title> Bugs: A program to perform bayesian inference using gibbs sampling. </title> <booktitle> Bayesian Statistics 4, </booktitle> <pages> pages 837-42, </pages> <year> 1992. </year>
Reference-contexts: The algorithm is then iterated to reach stability, and then a sample from the joint posterior distribution is taken which can be used to provide empirical estimates of the posterior means <ref> [Thomas et al., 1992] </ref>. This approach relies on the assumption that the unreported data are missing at random. When the "Missing at random" assumption is violated, as when data are systematically missing, such method suffers of a dramatic decrease in accuracy. <p> Robust Learning with Missing Data 20 5.1 Materials In order to experimentally compare our method to the Gibbs Sampling, we choose the program BUGS <ref> [Thomas et al., 1992] </ref> which is commonly regarded as a reliable implementation of such a technique [Buntine, 1996].
Reference: [van der Gaag, 1991] <author> L. van der Gaag. </author> <title> Computing probability intervals under independency constraints. </title> <booktitle> In Uncertainty in Artificial Intelligence, </booktitle> <pages> pages 491-497, </pages> <year> 1991. </year>
Reference-contexts: A natural evolution of this approach leaded to the combination of the computational advantages provided by conditional independence assumptions with the expressive power of probability intervals <ref> [van der Gaag, 1991] </ref>. Therefore, some efforts have been addressed to extend bbns from real-valued probabilities to interval probabilities [Breeze and Fertig, 1990, Ramoni, 1995], thus combining the advantages of conditional independence assumptions with the explicit representation of ignorance.
Reference: [Walley, 1991] <author> P. Walley. </author> <title> Statistical Reasoning with Imprecise Probabilities. </title> <publisher> Chapman and Hall, </publisher> <address> London, </address> <year> 1991. </year>
Reference-contexts: Good [Good, 1962] proposes to model this process as a "black box" which translates these probability functions into a set of constraints of an ideal, point-valued probability function. A review of these methods may be found in <ref> [Walley, 1991] </ref>. This theoretical framework is especially appealing when we are faced with the problem of assessing probabilities from an incomplete database.
Reference: [White, 1986] <author> C.C. White. </author> <title> A posteriori representations based on linear inequality descriptions of a priori conditional probabilities. </title> <journal> IEEE Transactions on Systems, Man, and Cybernetics, </journal> <volume> 16 </volume> <pages> 570-573, </pages> <year> 1986. </year>
Reference-contexts: The suitability of Convex Bayesianism to represent incomplete probabilistic information motivated the development of computational methods to reason on the basis of probability intervals <ref> [White, 1986, Snow, 1991] </ref>. A natural evolution of this approach leaded to the combination of the computational advantages provided by conditional independence assumptions with the expressive power of probability intervals [van der Gaag, 1991].
References-found: 29

