URL: http://www.cs.wisc.edu/~dewitt/papers/paralleldb/vldb85.ps
Refering-URL: http://www.cs.wisc.edu/~dewitt/paralleldb.html
Root-URL: 
Title: Multiprocessor Hash-Based Join Algorithms  
Author: David J. DeWitt Robert Gerber 
Note: This research was partially supported by the Department of Energy under contract #DE-AC02-81ER10920 and the National Science Foundation under grant MCS82-01870.  
Address: Wisconsin  
Affiliation: Computer Sciences Department University of  
Abstract-found: 0
Intro-found: 1
Reference: [ASTR76] <author> Astrahan, M., et. al., </author> <title> System R: Relational Approach to Database Management, </title> <journal> ACM Transactions on Data Systems, </journal> <volume> Volume 1, No. 2, </volume> <month> (June </month> <year> 1976), </year> <pages> pp. 119-120. </pages>
Reference-contexts: In retrospect, it is interesting to observe that a simple, but very good algorithm has been virtually ignored 2 simply because System R <ref> [ASTR76] </ref> did not support hashing as an access method. The motivation for the research described in this paper was twofold.
Reference: [BABB79] <author> Babb, E., </author> <title> Implementing a Relational Database by Means of Specialized Hardware, </title> <journal> ACM Transactions on Database Systems, </journal> <volume> Volume 4, No. 1, </volume> <year> 1979. </year>
Reference-contexts: The phantom tuples will, however, be eliminated by the final joining process. The number of phantom tuples can be reduced by increasing the size of the bit vector or by splitting the vector into a number of smaller vectors <ref> [BABB79] </ref>. A separate hash function would be associated with each of the smaller bit vectors. The costs associated with bit vector filtering are modest. For the given test, a single bit vector of length 4K bytes was used. <p> In addition to providing CPU and I/O utilization figures for use in the simulation of the multiprocessor algorithms, these centralized experiments provided two interesting results. First, the measured performance of the algorithms was very similar to that predicted analytically in [DEWI84a]. Second, bit vector filtering <ref> [BABB79] </ref> was shown to provide a dramatic reduction in the execution time of all algorithms including the sort merge join algorithm. In fact, for the one query tested, with bit-vector filtering all algorithms had virtually the same execution time. <p> A second promising area is the use of bit filtering in multiprocessor hash join algorithms. There are a number of ways bit filtering <ref> [BABB79, KITS83] </ref> can be exploited by the multiprocessor hashing algorithms. For example, each joining node can build a bit vector simultaneously with the construction of a hash table. When completed, the bit vectors would be distributed to the partitioning processors.
Reference: [BITT83] <author> Bitton D., D.J. DeWitt, and C. Turbyfill, </author> <title> Benchmarking Database Systems A Systematic Approach, </title> <booktitle> Proceedings of the 1983 Very Large Database Conference, </booktitle> <month> October, </month> <year> 1983. </year>
Reference-contexts: Similarly, all the algorithms accessed relations on disk a page at a time, blocking until disk operations completed. 3.3. Presentation of Performance Results The join algorithms were compared using queries and data from the Wisconsin Benchmark Database <ref> [BITT83] </ref>. As in Figure 1, the execution time of each join algorithm is shown as a function of the amount of available memory relative to the size of the smaller relation.
Reference: [BLAS77] <author> Blasgen, M. W., and K. P. Eswaran, </author> <title> Storage and Access in Relational Data Bases, </title> <journal> IBM Systems Journal, </journal> <volume> No. 4, </volume> <year> 1977. </year>
Reference-contexts: 1. Introduction After the publication of the classic join algorithm paper in 1977 by Blasgen and Eswaran <ref> [BLAS77] </ref>, the topic was virtually abandoned as a research area. Everybody "knew" that a nested-loops algorithm provided acceptable performance on small relations or large relations when a suitable index existed and that sort-merge was the algorithm of choice for ad-hoc 1 queries. <p> Thus, the tuples that are immediately processed do not have to be written to and retrieved from the disk between the partitioning and join phases. These savings become significant as the amount of memory increases. 2.5. Sort-Merge Join Algorithm The standard sort-merge <ref> [BLAS77] </ref> algorithm begins by producing sorted runs of tuples that are, on the average, twice as long as the number of tuples that can fit into a priority queue in memory [KNUT73]. This requires one pass over each relation.
Reference: [BRAT84] <author> Bratbergsengen, Kjell, </author> <title> Hashing Methods and Relational Algebra Operations, </title> <booktitle> Proceedings of the 1984 Very Large Database Conference, </booktitle> <month> August, </month> <year> 1984. </year> <month> 21 </month>
Reference-contexts: Everybody "knew" that a nested-loops algorithm provided acceptable performance on small relations or large relations when a suitable index existed and that sort-merge was the algorithm of choice for ad-hoc 1 queries. Last year two papers <ref> [DEWI84a, BRAT84] </ref> took another look at join algorithms for centralized relational database systems. In particular, both papers compared the performance of the more traditional join algorithms with a variety of algorithms based on hashing. <p> In retrospect, it is interesting to observe that a simple, but very good algorithm has been virtually ignored 2 simply because System R [ASTR76] did not support hashing as an access method. The motivation for the research described in this paper was twofold. First, since [DEWI84a] and <ref> [BRAT84] </ref> were both analytical evaluations, we wanted to implement and measure the algorithms proposed in these papers in a common framework in order to verify the performance of the hash-based join algorithms. Second, we wanted to see if the results for a single processor could be extended to multiple processors. <p> The relations could be partitioned again with another hash function. This solution is almost always too expensive. A better alternative is to apply the partitioning process recursively to the oversized buckets <ref> [DEWI84a, BRAT84] </ref>. The net effect of this solution is to split an oversized bucket into two or more smaller buckets. If relation R is partitioned before relation S, then this method only requires rescanning the particular bucket that overflowed. <p> Summary of Algorithms Evaluated Centralized versions of the Grace, Simple and Hybrid hash-partitioned join algorithms were implemented in the manner described in Section 2. A modified version of the nested loops algorithm, termed Hashed Loops, was also implemented <ref> [BRAT84] </ref>. The Hashed Loops algorithms is so named because it uses hashing as means of effecting the internal join of tuples in main memory. It is similar to the algorithm used by the university version of INGRES [STON76].
Reference: [BROW85] <author> Browne, J. C., Dale, A. G., Leung, C. and R. Jenevein, </author> <title> A Parallel Multi-Stage I/O Architecture with Self-Managing Disk Cache for Database Management Applications, </title> <booktitle> Proceedings of the 4th International Workshop on Database Machines, </booktitle> <month> March, </month> <year> 1985. </year>
Reference-contexts: Horizontal Partitioning of Relations All relations are assumed to be horizontally partitioned [RIES78] across all disk drives in the system. From the view point of raw bandwidth, this approach has the same aggregate bandwidth as the disk striping strategies <ref> [GARC84, KIM85, BROW85] </ref> given an equal number of disk drives. The difference is that in our approach once 11 the data has been read, it can be processed directly rather than being transmitted first through some interconnection network to a processor.
Reference: [CHOU83] <author> Chou, H-T, DeWitt, D. J., Katz, R., and T. Klug, </author> <title> Design and Implementation of the Wisconsin Storage System (WiSS) to appear, </title> <journal> Software Practice and Experience, </journal> <note> also Computer Sciences Department, </note> <institution> University of Wisconsin, </institution> <type> Technical Report #524, </type> <month> November </month> <year> 1983. </year>
Reference-contexts: In addition to the three hash-partitioned join algorithms, two other popular join algorithms were studied. These algorithms, a sort-merge algorithm and a hash-based nested loops algorithm, provide a context for comparing the performance of the hash-partitioned join algorithms. All the algorithms were implemented using the Wisconsin Storage System (WiSS) <ref> [CHOU83] </ref>. 3.1. An Overview of WiSS The WiSS project was begun approximately 3 years ago when we recognized the need for a flexible data storage system that could serve as the basis for constructing experimental database management systems. <p> However, insertion and deletion at arbitrary locations is supported. Associated with each stretch item (and each record) is a unique identifier (RID). By including the RID of a stretch item in a record, one can construct records of arbitrary length. As demonstrated in <ref> [CHOU83] </ref>, WiSS's performance is comparable to that of commercially available database systems. 3.2. Summary of Algorithms Evaluated Centralized versions of the Grace, Simple and Hybrid hash-partitioned join algorithms were implemented in the manner described in Section 2. <p> Conclusions and Future Research In this paper, the hash-join algorithms presented in [DEWI84a] were extended to a multiprocessor architecture. As a first step, the algorithms described in [DEWI84a] were implemented using WiSS <ref> [CHOU83] </ref> running on a VAX 11/750 running 4.2 Berkeley UNIX. In addition to providing CPU and I/O utilization figures for use in the simulation of the multiprocessor algorithms, these centralized experiments provided two interesting results. <p> Finally, we intend to use these algorithms as part of the Gamma Project. Gamma is a new database machine project that was begun recently. Gamma will provide a test vehicle for validating our multiprocessor hash-join results. Gamma will be built using the Crystal multicomputer [DEWI84b] and WiSS <ref> [CHOU83] </ref> as a basis. The Crystal Multicomputer project was funded as part of the National Science Foundation's Coordinate Experimental Research Program. Crystal is a network of bare VAX 11/750 processors (currently twenty, eventually forty) serving as nodes, connected by a 10 Mbit/second token ring from Proteon Associates [PROT83].
Reference: [DEWI84a] <author> DeWitt, D., Katz, R., Olken, F., Shapiro, D., Stonebraker, M. and D. Wood, </author> <title> Implementation Techniques for Main Memory Database Systems, </title> <booktitle> Proceedings of the 1984 SIGMOD Conference, </booktitle> <address> Boston, MA, </address> <month> June, </month> <year> 1984. </year>
Reference-contexts: Everybody "knew" that a nested-loops algorithm provided acceptable performance on small relations or large relations when a suitable index existed and that sort-merge was the algorithm of choice for ad-hoc 1 queries. Last year two papers <ref> [DEWI84a, BRAT84] </ref> took another look at join algorithms for centralized relational database systems. In particular, both papers compared the performance of the more traditional join algorithms with a variety of algorithms based on hashing. <p> In retrospect, it is interesting to observe that a simple, but very good algorithm has been virtually ignored 2 simply because System R [ASTR76] did not support hashing as an access method. The motivation for the research described in this paper was twofold. First, since <ref> [DEWI84a] </ref> and [BRAT84] were both analytical evaluations, we wanted to implement and measure the algorithms proposed in these papers in a common framework in order to verify the performance of the hash-based join algorithms. <p> Second, we wanted to see if the results for a single processor could be extended to multiple processors. The hash-based join algorithms described in <ref> [DEWI84a] </ref>, and in particular the Hybrid algorithm, made very effective use of main memory to minimize disk traffic. <p> Consequently, the algorithm never received the recognition it deserves. 2 multiprocessor hash-join algorithms in a multiprocessor environment that enabled us to identify CPU, communications, and I/O bandwidth design parameters. In Section 2, we review the join algorithms and analytical results presented in <ref> [DEWI84a] </ref>. As a first step toward developing a multiprocessor version of the hash based join algorithms, we implemented the join algorithms described in [DEWI84a] on top of the Wisconsin Storage System (WiSS). The results presented in Section 3 verify the analytical results presented in [DEWI84a]. <p> In Section 2, we review the join algorithms and analytical results presented in <ref> [DEWI84a] </ref>. As a first step toward developing a multiprocessor version of the hash based join algorithms, we implemented the join algorithms described in [DEWI84a] on top of the Wisconsin Storage System (WiSS). The results presented in Section 3 verify the analytical results presented in [DEWI84a]. <p> algorithms and analytical results presented in <ref> [DEWI84a] </ref>. As a first step toward developing a multiprocessor version of the hash based join algorithms, we implemented the join algorithms described in [DEWI84a] on top of the Wisconsin Storage System (WiSS). The results presented in Section 3 verify the analytical results presented in [DEWI84a]. Based on these results, we feel that all relational database systems should provide a hash-based join algorithm in order to effectively exploit main memory as it becomes increasingly inexpensive. <p> In Section 5, our conclusions and our plans for a new database machine based on these multiprocessor join algorithms are described. 2. An Overview of Hash-Partitioned Join Operations In <ref> [DEWI84a] </ref>, the performance of three hashed-based join algorithms (termed Simple, Grace [KITS83], and Hybrid) were compared with that of the more traditional sort merge algorithm. In the following discussion of the hash-partitioned join algorithms, the two source relations will be named R and S. <p> The relations could be partitioned again with another hash function. This solution is almost always too expensive. A better alternative is to apply the partitioning process recursively to the oversized buckets <ref> [DEWI84a, BRAT84] </ref>. The net effect of this solution is to split an oversized bucket into two or more smaller buckets. If relation R is partitioned before relation S, then this method only requires rescanning the particular bucket that overflowed. <p> This strategy is termed bucket tuning [KITS83]. Bucket tuning is a useful method for avoiding bucket overflow. 2.4. Hybrid Hash Join The Hybrid hash join was first described in <ref> [DEWI84a] </ref>. All partitioning is finished in the first stage of the algorithm in a fashion similar to the Grace algorithm. <p> In retrospect, the results are not too surprising, as sorting creates a total ordering of the records in both files, while hashing simply groups related records together in the same bucket. 3. Evaluation of Centralized Hash Partitioned Join Algorithms To verify the analysis presented in <ref> [DEWI84a] </ref> and to gather information on CPU and I/O utilizations during the partitioning and joining phases of the three hashing algorithms, we implemented the Simple, Grace, and Hybrid algorithms on a VAX 11/750 running 4.2 Berkeley UNIX. <p> Conclusions and Future Research In this paper, the hash-join algorithms presented in <ref> [DEWI84a] </ref> were extended to a multiprocessor architecture. As a first step, the algorithms described in [DEWI84a] were implemented using WiSS [CHOU83] running on a VAX 11/750 running 4.2 Berkeley UNIX. <p> Conclusions and Future Research In this paper, the hash-join algorithms presented in <ref> [DEWI84a] </ref> were extended to a multiprocessor architecture. As a first step, the algorithms described in [DEWI84a] were implemented using WiSS [CHOU83] running on a VAX 11/750 running 4.2 Berkeley UNIX. In addition to providing CPU and I/O utilization figures for use in the simulation of the multiprocessor algorithms, these centralized experiments provided two interesting results. <p> In addition to providing CPU and I/O utilization figures for use in the simulation of the multiprocessor algorithms, these centralized experiments provided two interesting results. First, the measured performance of the algorithms was very similar to that predicted analytically in <ref> [DEWI84a] </ref>. Second, bit vector filtering [BABB79] was shown to provide a dramatic reduction in the execution time of all algorithms including the sort merge join algorithm. In fact, for the one query tested, with bit-vector filtering all algorithms had virtually the same execution time.
Reference: [DEWI84b] <author> DeWitt, D. J., Finkel, R., and M. Solomon, </author> <title> The CRYSTAL Multicomputer: Design and Implementation Experience, </title> <note> submitted for publication IEEE Transactions on Software Engineering, also, Computer Sciences Department Technical Report #553, </note> <institution> University of Wisconsin, </institution> <month> September, </month> <year> 1984. </year>
Reference-contexts: While originally conceived as a replacement for the UNIX file system (WiSS can run on top of a raw disk under UNIX), WiSS has also been ported to run on the Crystal multicomputer <ref> [DEWI84b] </ref>. The services provided by WiSS include structured sequential files, byte-stream files as in UNIX, B + indices, stretch data items, a sort utility, and a scan mechanism. A sequential file is a sequence of records. <p> Finally, we intend to use these algorithms as part of the Gamma Project. Gamma is a new database machine project that was begun recently. Gamma will provide a test vehicle for validating our multiprocessor hash-join results. Gamma will be built using the Crystal multicomputer <ref> [DEWI84b] </ref> and WiSS [CHOU83] as a basis. The Crystal Multicomputer project was funded as part of the National Science Foundation's Coordinate Experimental Research Program.
Reference: [GARC84] <author> Garcia-Molina, H. and Kenneth Salem, </author> <title> Disk Striping, </title> <booktitle> to appear Proceedings of the 1985 SIGMOD Conference, </booktitle> <institution> also Dept. of Electrical Engineering and Computer Science Technical Report #332, </institution> <month> December, </month> <year> 1982. </year>
Reference-contexts: Horizontal Partitioning of Relations All relations are assumed to be horizontally partitioned [RIES78] across all disk drives in the system. From the view point of raw bandwidth, this approach has the same aggregate bandwidth as the disk striping strategies <ref> [GARC84, KIM85, BROW85] </ref> given an equal number of disk drives. The difference is that in our approach once 11 the data has been read, it can be processed directly rather than being transmitted first through some interconnection network to a processor. <p> c c c c c c c #P number of Processors #D number of Disks #PP number of Partitioning Processors #JP number of Joining Processors Table 1 Resource Costs for Hash-Join Configurations (1 MIP Processors) To minimize overhead, all disk transfers are done a track (28 Kbytes) at a time <ref> [GARC84] </ref>. In addition, a double buffer is associated with each open file so that while a process is processing track i, track i+1 can be read. The maximum packet size supported by the network is assumed to be 2K bytes.
Reference: [GOOD81] <author> Goodman, J. R., </author> <title> An Investigation of Multiprocessor Structures and Algorithms for Database Management, </title> <institution> University of California at Berkeley, </institution> <type> Technical Report UCB/ERL, </type> <institution> M81/33, </institution> <month> May, </month> <year> 1981. </year>
Reference-contexts: It seemed that since multiprocessor joins require that data be moved between processors, that the multiprocessor hash-based join algorithms might minimize the amount of data moved in the process of executing a join algorithm. Hash-based multiprocessor join algorithms for multiprocessors are not new. They were first suggested in <ref> [GOOD81] </ref>, next adopted by the Grace database machine project [KITS83], and evaluated in [VALD84]. While each of these papers made important contributions to understanding multiprocessor hash-based join algorithms, a number of questions remain. First, in [GOOD81], it is hard to factor out the influence of the X-tree architecture and the parallel <p> Hash-based multiprocessor join algorithms for multiprocessors are not new. They were first suggested in <ref> [GOOD81] </ref>, next adopted by the Grace database machine project [KITS83], and evaluated in [VALD84]. While each of these papers made important contributions to understanding multiprocessor hash-based join algorithms, a number of questions remain. First, in [GOOD81], it is hard to factor out the influence of the X-tree architecture and the parallel readout disks on the results obtained. [KITS83], on the other hand, concentrates on the speed of the sort-engine and not the overall performance of the Grace hash-join algorithm. <p> In the following discussion of the hash-partitioned join algorithms, the two source relations will be named R and S. R is assumed to be smaller (in pages) than S. All hash-join algorithms begin by partitioning R and S into disjoint subsets called buckets <ref> [GOOD81, KITS83] </ref>. These partitions have the important characteristic that all tuples with the same join attribute value will share the same bucket. The term bucket should not be confused with the overflow buckets of a hash table. The partitioned buckets are merely disjoint subsets of the original relations. <p> As the algorithm progresses, the R_output (S_output) file becomes progressively smaller as the buckets of interest are consumed. The algorithm finishes when either R_output or S_output are empty following a processing stage. 2.3. Grace Hash-Join The Grace hash join algorithm <ref> [GOOD81, KITS83] </ref> is characterized by a complete separation of the partitioning and joining phases. The partitioning of relations R and S is completed prior to the start of the join phase.
Reference: [GROS53] <author> Grosch, H. R. J., </author> <title> High Speed Arithmetic: The Digital Computer as a Research Tool, </title> <journal> Journal of the Optical Society of America, </journal> <volume> Volume 4, No. 4, </volume> <month> April, </month> <year> 1953. </year>
Reference-contexts: The approach we adopted was to assume that a 1 MIP processor cost the same as a disk drive and controller. The relative cost of the 2 and 3 MIP processors was computed using Grosch's law <ref> [GROS53] </ref> which relates the cost of a processor to the performance (speed) of the processor: Performance = Technology_Constant * Processor_Cost g The technology constant and cost exponent were assigned, respectively, values of 1 and 1.5. 5 The cost of a particular configuration is calculated by computing the aggregate cost of all
Reference: [HE83] <author> He, X. et. al. </author> <title> The Implementation of a Multibackend Database Systems (MDBS), in Advanced Database Machine Architecture, edited by David Hsiao, </title> <publisher> Prentice-Hall, </publisher> <year> 1983. </year>
Reference-contexts: A controlling processor acts, in effect, as the root page of the index. We intend to investigate whether traditional tree balancing algorithms provide acceptable performance in such an environment. This approach is similar to, but much simpler than, the clustering approach employed by MDBS <ref> [HE83] </ref>. In MDBS [HE83], each backend processor must examine every query as the clustering mechanism is implemented by the backends, not the controlling processor. The real advantage of the second approach comes when processing queries. <p> A controlling processor acts, in effect, as the root page of the index. We intend to investigate whether traditional tree balancing algorithms provide acceptable performance in such an environment. This approach is similar to, but much simpler than, the clustering approach employed by MDBS <ref> [HE83] </ref>. In MDBS [HE83], each backend processor must examine every query as the clustering mechanism is implemented by the backends, not the controlling processor. The real advantage of the second approach comes when processing queries.
Reference: [KIM85] <author> Kim, M. </author> <title> Y, Parallel Operation of Magnetic Disk Storage Devices, </title> <booktitle> Proceedings of the 4th International Workshop on Database Machines, </booktitle> <month> March, </month> <year> 1985. </year>
Reference-contexts: Horizontal Partitioning of Relations All relations are assumed to be horizontally partitioned [RIES78] across all disk drives in the system. From the view point of raw bandwidth, this approach has the same aggregate bandwidth as the disk striping strategies <ref> [GARC84, KIM85, BROW85] </ref> given an equal number of disk drives. The difference is that in our approach once 11 the data has been read, it can be processed directly rather than being transmitted first through some interconnection network to a processor.
Reference: [KITS83a] <author> Kitsuregawa, M., Tanaka, H., and T. Moto-oka, </author> <title> Application of Hash to Data Base Machine and Its Architecture, </title> <journal> New Generation Computing, </journal> <volume> Volume 1, No. 1, </volume> <year> 1983. </year>
Reference: [KITS83b] <author> Kitsuregawa, M., Tanaka, H., and T. Moto-oka, </author> <title> Relational Algebra Machine Grace, </title> <booktitle> RIMS Symposia on Software Science and Engineering, 1982, Lecture Notes in Computer Science, </booktitle> <publisher> Springer Verlag, </publisher> <year> 1983. </year>
Reference: [KITS83c] <author> Kitsuregawa, M., Tanaka, H., and T. Moto-oka, </author> <title> Architecture and Performance of Relational Algebra Machine Grace, </title> <institution> University of Tokyo, </institution> <type> Technical Report, </type> <year> 1983. </year>
Reference: [KNUT73] <author> Knuth, </author> <title> The Art of Computer Programming: Sorting and Searching, </title> <booktitle> Volume III, </booktitle> <year> 1973. </year>
Reference-contexts: These savings become significant as the amount of memory increases. 2.5. Sort-Merge Join Algorithm The standard sort-merge [BLAS77] algorithm begins by producing sorted runs of tuples that are, on the average, twice as long as the number of tuples that can fit into a priority queue in memory <ref> [KNUT73] </ref>. This requires one pass over each relation. During the second phase, the runs are merged using an n-way merge, where n is as large as possible. If n is less than the number of runs produced by the first phase, more than two phases will be needed.
Reference: [PROT83] <author> Proteon Associates, </author> <title> Operation and Maintenance Manual for the ProNet Model p1000 Unibus, </title> <address> Waltham, Mass, </address> <year> 1983. </year>
Reference-contexts: The effective DMA bandwidth at which these buffers can be filled or flushed to the main memory of a processor is assumed to be either 4 Mbits/second or 20 Mbits/second. The 4 Mbits/second number is derived from measurements made on a VAX 11/750 with a Proteon ProNet interface <ref> [PROT83] </ref> attached to the Unibus. The 20 Mbits/second is an estimate of the DMA rate if the device were attached to the internal bus of a VAX 11/750. The token ring is assumed to have a bandwidth of either 10 Mbits/second or 80 Mbits/second. <p> The Crystal Multicomputer project was funded as part of the National Science Foundation's Coordinate Experimental Research Program. Crystal is a network of bare VAX 11/750 processors (currently twenty, eventually forty) serving as nodes, connected by a 10 Mbit/second token ring from Proteon Associates <ref> [PROT83] </ref>. This ring is currently being upgraded to an 80 Mbit/second ring. Nine node machines have attached disks. File and database services are provided to Crystal "users" using WiSS.
Reference: [RIES78] <author> Ries, D. and R. Epstein, </author> <title> Evaluation of Distribution Criteria for Distributed Database Systems, </title> <type> UCB/ERL Technical Report M78/22, </type> <institution> UC Berkeley, </institution> <month> May, </month> <year> 1978. </year>
Reference-contexts: Finally, it appears that control of these algorithms can also be decentralized in a straightforward manner. 4.1. Horizontal Partitioning of Relations All relations are assumed to be horizontally partitioned <ref> [RIES78] </ref> across all disk drives in the system. From the view point of raw bandwidth, this approach has the same aggregate bandwidth as the disk striping strategies [GARC84, KIM85, BROW85] given an equal number of disk drives.
Reference: [SIEW82] <author> Siewiorek, D. P., Bell, C. G., and A. Newell, </author> <title> Computer Structures: Principles and Examples, </title> <publisher> McGraw Hill, </publisher> <year> 1982. </year>
Reference-contexts: Processes have associated priorities that are used to resolve contention for system resources. hhhhhhhhhhhhhhhhhhhhhhhhhhhhh 5 The price/performance relationship of the IBM System/370 series correlates well with a value of 1.6 for the cost exponent <ref> [SIEW82] </ref>). 6 Note that in the case of the Grace algorithm, different performance processors might be used for the partitioning and joining nodes. 15 iiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiii Resource Hybrid Grace 1:1 Grace 2:1 Grace 1:2 Cost #P #D #PP #D #JP #PP #D #JP #PP #D #JP iiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiii 2 1 1 4 2
Reference: [STON76] <author> Stonebraker, Michael, Eugene Wong, and Peter Kreps, </author> <title> The Design and Implementation of INGRES, </title> <journal> ACM Transactions on Database Systems, </journal> <volume> Volume 1, No. 3, </volume> <month> September, </month> <year> 1976. </year>
Reference-contexts: The goal of our research was to examine the hhhhhhhhhhhhhhhhhhhhhhhhhhhhh 1 By "ad-hoc" we mean a join for which no suitable index exists. 2 While INGRES <ref> [STON76] </ref> uses hashing for ad-hoc queries, the limited address space of the PDP 11 on which INGRES was first implemented made it impossible to exploit the use of large amounts of memory effectively. <p> The Hashed Loops algorithms is so named because it uses hashing as means of effecting the internal join of tuples in main memory. It is similar to the algorithm used by the university version of INGRES <ref> [STON76] </ref>. For each phase of the Hashed Loops algorithm, a hash table is constructed from those pages of R that have been staged into memory. Tuples from S are used as probes into the hash table.
Reference: [VALD84] <author> Valduriez, P., and G. Gardarin, </author> <title> Join and Semi-Join Algorithms for a Multiprocessor Database Machine, </title> <journal> ACM Transactions on Database Systems, </journal> <volume> Volume 9, No. 1, </volume> <month> March, </month> <year> 1984. </year>
Reference-contexts: Hash-based multiprocessor join algorithms for multiprocessors are not new. They were first suggested in [GOOD81], next adopted by the Grace database machine project [KITS83], and evaluated in <ref> [VALD84] </ref>. While each of these papers made important contributions to understanding multiprocessor hash-based join algorithms, a number of questions remain. <p> Finally, the algorithm presented in <ref> [VALD84] </ref> exploits hashing only during the partitioning process and resorts to a pure nested loops algorithm aided by bit vector filtering during the join phase.
References-found: 23

