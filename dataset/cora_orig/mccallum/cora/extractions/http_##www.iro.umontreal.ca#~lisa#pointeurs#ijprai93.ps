URL: http://www.iro.umontreal.ca/~lisa/pointeurs/ijprai93.ps
Refering-URL: http://www.iro.umontreal.ca/labs/neuro/bib/journals/journals.html
Root-URL: http://www.iro.umontreal.ca
Title: A Connectionist Approach to Speech Recognition  
Author: Yoshua Bengio 
Keyword: neural networks, speech recognition, sequence recognition, recurrent networks, hidden Markov models, hybrid systems.  
Note: Paper to appear in the special issue of IJPRAI on Neural Nets.  
Address: HO-4G312, Crawfords Corner Rd, Holmdel, NJ 07733.  
Affiliation: AT&T Bell Laboratories,  
Abstract: The task discussed in this paper is that of learning to map input sequences to output sequences. In particular, problems of phoneme recognition in continuous speech are considered, but most of the discussed techniques could be applied to other tasks, such as the recognition of sequences of handwritten characters. The systems considered in this paper are based on connectionist models, or artificial neural networks, sometimes combined with statistical techniques for recognition of sequences of patterns, stressing the integration of prior knowledge and learning. Different architectures for sequence and speech recognition are reviewed, including recurrent networks as well as hybrid systems involving hidden Markov models. 
Abstract-found: 1
Intro-found: 1
Reference: 1. <author> Y.S. Abu-Mostafa, </author> <title> "Learning from Hints in Neural Networks," </title> <editor> J. </editor> <booktitle> of Complexity 6, </booktitle> <year> 1990, </year> <pages> pp. 192-198. </pages>
Reference: 2. <author> B.S. Atal, J.J. Chang, M.V. Mathews, J.W. Tukeys, </author> <title> "Inversion of articulatory-to-acoustic transformation in the vocal tract by a computer sorting technique," </title> <journal> J. Acoust. Soc. Am. </journal> <volume> 63 (5), </volume> <year> 1970, </year> <pages> pp. 1535-1555. </pages>
Reference: 3. <author> Y. Bengio, R. Cardin, R. De Mori and E. Merlo, </author> <title> "Programmable execution of multi-layered networks for automatic speech recognition," </title> <journal> Communications of the Association for Computing Machinery, </journal> <volume> vol. 22 32, no. 2, </volume> <year> 1989, </year> <pages> pp. 195-199. </pages>
Reference-contexts: In that case, it would probably be advantageous to use specialized preprocessing for specialized networks. We adopt such an approach in the experiments described in <ref> [3] </ref> on E-set classification as well as those described in [8] on plosive recognition in continuous speech. For example, in [3] we use different temporal and frequency resolution for different classification tasks (see also modular architectures, section 2.3). 2.2 Output Coding Most NNs designed for phoneme recognition have a simple output <p> In that case, it would probably be advantageous to use specialized preprocessing for specialized networks. We adopt such an approach in the experiments described in <ref> [3] </ref> on E-set classification as well as those described in [8] on plosive recognition in continuous speech. For example, in [3] we use different temporal and frequency resolution for different classification tasks (see also modular architectures, section 2.3). 2.2 Output Coding Most NNs designed for phoneme recognition have a simple output coding scheme consisting of one output unit per phoneme, with a high desired output for the output unit corresponding to
Reference: 4. <author> Y. Bengio, R. Cardin, R. De Mori and Y. Normandin, </author> <title> "A hybrid coder for hidden Markov models using a recurrent neural network," </title> <booktitle> Proceedings of the International Conference on Acoustics, Speech, and Signal Processing, </booktitle> <address> Albuquerque, NM, </address> <year> 1990, </year> <pages> pp. 537-540. </pages>
Reference-contexts: On the other hand, [14,22,38, 45] interpret the outputs of the NN as posterior probabilities 12 and use the Viterbi algorithm [42]. The systems proposed in [4,6,7,32] combine a NN with a HMM. In particular, in <ref> [4] </ref>, the outputs of the NN are quantized, hence no global optimization of the hybrid can be performed.
Reference: 5. <author> Y. Bengio Y., R. Cardin R., R. De Mori, </author> <title> "Speaker-independent speech recognition with neural networks and speech knowledge," </title> <booktitle> Advances in Neural Information Processing Systems 2, </booktitle> <editor> ed. D.S. Touretzky, </editor> <publisher> Morgan Kaufmann Publishers, </publisher> <year> 1990, </year> <month> pp.218-225. </month>
Reference: 6. <author> Y. Bengio, </author> <title> "Artificial Neural Networks and their Application to Sequence Recognition," </title> <type> Ph.D. Thesis, </type> <institution> McGill University, (Computer Science), </institution> <address> 1991, Montreal, Qc., Canada. </address>
Reference-contexts: In addition to the choice of input features, one has to decide on how to code these features with the activations of the input neurons of the network. This choice may influence generalization. For example, in <ref> [6] </ref> we found that smoothing the input "image" improved generalization. This is consistent with results from [25] on how smoothing the input reduces the "effective capacity" of the system. <p> It is easy to imagine cases in which each module in a modular system is trained separately, which is sub-optimal for the given complete architecture, even though each module may have reached a local optimum of its local cost function (see <ref> [6] </ref> for such a demonstration). In many instances, we would be able to perform a separate training using prior knowledge of what each module should do. <p> In <ref> [6] </ref>, we compare recurrent and static networks with experiments on the speaker-independent recognition of vowels in the TIMIT database. The task is the discrimination of 11 vowel classes in every context. <p> At each time frame, the input to the neural network is a 32-filter Bark-scale spectrogram (see [54]). The results are summarized in Table 1. Figure 2 shows the architecture that yielded the best results (see also <ref> [6] </ref> for the other architectures). An important aspect of this architecture is that it combines recurrence with delays. Another interesting point is that it is not a fully recurrent network.
Reference: 7. <author> Y. Bengio, R. De Mori, G. Flammia, and R. Kompe, </author> <title> "Global Optimization of a Neural Network Hidden Markov Model Hybrid," </title> <journal> IEEE Transactions on Neural Networks, </journal> <volume> vol. 3, no. 2, </volume> <year> 1992, </year> <pages> pp. 252-259. </pages>
Reference-contexts: Our experiments tended to indicate that their parameters settle in a sub-optimal solution which takes into account short-term dependencies but not long-term dependencies. For example, in experiments described in <ref> [7] </ref>, we found that simple duration constraints on phonemes had not at all been captured by the recurrent network. Hence its performance was much worse than that of a NN / dynamic programming hybrid in which these constraints were incorporated. <p> Its parameters are transition probabilities and initial state probabilities. The output of the HMM module may be computed with the forward pass of the Baum-Welsh algorithm [42]. In <ref> [7] </ref>, we present a system built as the concatenation NN + GM + HMM. Experiments were performed on the recognition of plosives with the TIMIT database. First, the NN (which is recurrent, here) is trained to recognize phonetic features. <p> However, if the system modules are initialized separately, global optimization can bring the system to a local maximum of the ML criterion which corresponds to improved performance. In fact, in our experiments <ref> [7] </ref> using the ML criterion, the error rate improved from 19% to 14% after global optimization, and was much better than that of the NN alone (47%) or the GM+HMM alone (24%) (i.e. with the input observations directly modeled by the GM+HMM system). <p> in equation 4 is the same for all k, we can select recognized interpretation = argmax k p2P k t2p f (t; O o ) (6) The HMM in this system is very similar to the one in the NN + GM + HMM with the MMI criterion discussed earlier <ref> [7] </ref>. <p> In order to compute these quantities, we introduce some intermediate variables ff i;o associated to state i at time o . ff i;o = max k consistent with i;o X Y o =1;2;:::L p 21 For example, we used it in <ref> [7] </ref>. 20 We also associate with each (state,time) pair the best interpretation k that is consistent with it (argmax instead of max in the equation above).
Reference: 8. <author> Y. Bengio, R. De Mori, G. Flammia, and R. Kompe, </author> <title> "Phonetically motivated acoustic parameters for continuous speech recognition using artificial neural networks," </title> <journal> Speech Communication, </journal> <pages> 11(2-3), </pages> <year> 1992, </year> <pages> pp. 261-271. </pages>
Reference-contexts: Other examples of the introduction of input features and preprocessing that influenced performance improvements in speech recognition tasks are detailed in [8,6,21,43]. For example, in consonant recognition experiments <ref> [8] </ref>, we found the second-order derivative of spectral energy with respect to time and frequency to be useful for characterizing formant 4 transitions. <p> In that case, it would probably be advantageous to use specialized preprocessing for specialized networks. We adopt such an approach in the experiments described in [3] on E-set classification as well as those described in <ref> [8] </ref> on plosive recognition in continuous speech. <p> One should also notice that the use of articulatory features in the vowel targets does not seem to be as successful in the case of vowels as we found it to be with consonants <ref> [8] </ref>, when compared to the simpler "one-output-per-class" scheme. This may be explained as follows. For vowels, the mapping from articulatory position to speech acoustics (or formant frequencies) is known to be many-to-one [2,9]. This makes the inversion problem particularly difficult.
Reference: 9. <author> Y. Bengio, J. Houde, M. Jordan, </author> <title> "Representations based on articulatory dynamics for speech recognition", </title> <booktitle> presented at the Neural Network for Computing Conference, </booktitle> <year> 1992. </year>
Reference: 10. <author> Y. Bengio and L. Bottou, </author> <title> "A new approach to estimating density functions with artificial neural networks," </title> <institution> Computational Cognitive Science, Massachusetts Institute of Technology, </institution> <type> T.R. </type> <institution> 92.2, </institution> <year> 1992. </year>
Reference-contexts: The following classical result can be shown <ref> [10] </ref>: f X (x 0 ) = f Y (y (x 0 )) fi fi Determinant @(y) (x 0 ) fi fi (2) Hence the output of the NN + GM system should be scaled by the determinant of the Jacobian of the NN. In [10], we generalize this result to <p> classical result can be shown <ref> [10] </ref>: f X (x 0 ) = f Y (y (x 0 )) fi fi Determinant @(y) (x 0 ) fi fi (2) Hence the output of the NN + GM system should be scaled by the determinant of the Jacobian of the NN. In [10], we generalize this result to the case m &lt; n, in which the network produces a compressed representation. In [10], we consider how to use these results to perform density estimation with a hybrid system consisting of a neural network followed by a pdf such as a Gaussian or a <p> In <ref> [10] </ref>, we generalize this result to the case m &lt; n, in which the network produces a compressed representation. In [10], we consider how to use these results to perform density estimation with a hybrid system consisting of a neural network followed by a pdf such as a Gaussian or a Gaussian mixture.
Reference: 11. <author> Y. Bengio, P. Frasconi and P. Simard, </author> <title> "The problem of learning long-term dependencies in recurrent networks", </title> <booktitle> invited paper to appear in the Proceedings of the International Conference on Neural Networks, </booktitle> <address> March 1993, San Francisco, CA. </address>
Reference-contexts: The criterion function (a function of the free parameters) appears to have many local minima, plateaus and abrupt changes. Experiments with a recurrent network on a minimal task involving one bit of state information showed <ref> [11] </ref> that learning becomes more and more difficult for longer term dependencies. We demonstrate [11] several theoretical results on parametric dynamical systems (such as recurrent networks) where bits of state information are stored with hyperbolic attractors. <p> The criterion function (a function of the free parameters) appears to have many local minima, plateaus and abrupt changes. Experiments with a recurrent network on a minimal task involving one bit of state information showed <ref> [11] </ref> that learning becomes more and more difficult for longer term dependencies. We demonstrate [11] several theoretical results on parametric dynamical systems (such as recurrent networks) where bits of state information are stored with hyperbolic attractors. For such systems, it is shown that perturbation resistance is incompatible with efficient learning by gradient descent. The demonstration [11] relies on the fact that if the 2-norm of <p> We demonstrate <ref> [11] </ref> several theoretical results on parametric dynamical systems (such as recurrent networks) where bits of state information are stored with hyperbolic attractors. For such systems, it is shown that perturbation resistance is incompatible with efficient learning by gradient descent. The demonstration [11] relies on the fact that if the 2-norm of the Jacobian of the system dynamics around attractors is greater than 1, then the system may be very sensitive to noise.
Reference: 12. <author> L. Bottou, </author> <title> "Une approche theorique de l'apprentissage connexioniste; applications a la reconnaissance de la parole". </title> <type> PhD thesis, </type> <institution> Universite de Paris XI, </institution> <year> 1991. </year>
Reference-contexts: Note that unlike the system proposed in [14,38], the outputs of the NN are not interpreted as probabilities and hence are not required to sum to 1. Instead, 16 This model is similar to one proposed in <ref> [12] </ref> and was developed in discussions with L. Bottou and J.
Reference: 13. <author> L. Bottou and P. Gallinari, </author> <title> "A framework for the cooperation of learning algorithms", </title> <booktitle> Advances in Neural Information Processing Systems 3, </booktitle> <editor> eds. Lippmann, Moody and Touretzky, </editor> <publisher> publ. Morgan Kaufmann, </publisher> <year> 1991, </year> <pages> pp. 781-788. </pages>
Reference: 14. <author> H. Bourlard and C.J. Wellekens, </author> <title> "Speech pattern discrimination and multilayer perceptrons," </title> <booktitle> Computer, Speech and Language, </booktitle> <volume> vol. 3, </volume> <year> 1989, </year> <month> pp.1-19. </month>
Reference: 15. <author> J.S. Bridle, </author> <title> "Training stochastic model recognition algorithms as networks can lead to maximum mutual information estimation of parameters," </title> <booktitle> Advances in Neural Information Processing Systems 2, </booktitle> <editor> (ed. D.S. </editor> <publisher> Touretzky) Morgan Kaufmann Publ., </publisher> <year> 1990, </year> <pages> pp. 211-217. 23 </pages>
Reference-contexts: Hence the system is trained to approximately maximize the likelihood of the observations (given the predictive network model). It is an approximation because the Viterbi algorithm is used instead of the Baum-Welsh algorithm [42]. A different kind of hybrid consists in constructing a HMM with connectionist elements. For example <ref> [15] </ref> shows how the Baum-Welsh forward pass [34] in a HMM can be realized by combining sums, products, and recurrences. It is also shown how the backward pass is equivalent to computing error gradients with the back-propagation algorithm when applied to this architecture.
Reference: 16. <author> C. Burges, O. Matan, Y. Le Cun, J. Denker, L. Jackel, C. Stenard, C. Nohl and J. Ben, </author> <title> "Shortest Path Segmentation: A Method for Training a Neural Network to Recognize character Strings," </title> <booktitle> Proceedings of the International Joint Conference on Neural Networks, IJCNN 1992, Baltimore, vol. III, </booktitle> <address> pp.165-172. </address>
Reference: 17. <author> P. Cosi, Y. Bengio and R. De Mori, </author> <title> "Phonetically-based multi-layered networks for acoustic property extraction and automatic speech recognition," Speech Communication 9(1), </title> <booktitle> 1990, </booktitle> <pages> pp. 15-30. </pages>
Reference-contexts: Considering, for example, automatic speech recognition, our work (see [6,5]), as well as the results of others [43], indicate that the choice of signal processing significantly influences the performance of a recognition system. Thus, in experiments described in <ref> [17] </ref>, we found an auditory model to perform better as a preprocessor for a feedforward neural network than a Mel-scaled filter bank based on the discrete Fourier transform. Other examples of the introduction of input features and preprocessing that influenced performance improvements in speech recognition tasks are detailed in [8,6,21,43]. <p> Examples of such features are horizontal and vertical place of articulation, voicing and nasality. Such a representation is in general more compact 5 than the "one-output-per-phoneme" representation. Furthermore, it describes a more general space of phonetic characteristics, allowing a network trained with some phonemes to generalize to new phonemes <ref> [17] </ref> that were not part of the training set 5 . Another way to use prior knowledge for the design of output representation is to use `hint' output units. For example, in [17], in addition to the units representing each vowel, supervision was given to extra output units representing phonetic features. <p> more general space of phonetic characteristics, allowing a network trained with some phonemes to generalize to new phonemes <ref> [17] </ref> that were not part of the training set 5 . Another way to use prior knowledge for the design of output representation is to use `hint' output units. For example, in [17], in addition to the units representing each vowel, supervision was given to extra output units representing phonetic features. These additional constraints on the network guided it towards computing these phonetic features, and yielded an improvement in generalization. <p> As in <ref> [17] </ref>, the input sequence is scaled to have a constant length L. The first hidden layer is fully connected to the L fi D inputs (where D is the number of input features per time frame 7 ). Usually, there is one output unit per word. <p> An unstructured network similar to the static isolated word recognizer may be applied to each segment. Each input segment is scaled to constant length. The simpler approach assumes a single segmentation hypothesis (a sequence of adjacent segments), as in <ref> [17] </ref>. It is much faster than a frame-based method (as those described above) but is unfortunately very sensitive to errors of the segmentation preprocessor. A more sophisticated approach considers multiple segmentation hypotheses, with each segment constructed from a pair of boundaries satisfying some constraints, as in [31,16, 45].
Reference: 18. <author> B. de Vries & J.C. Principe, </author> <title> "The gamma model Anew neural net model for temporal processing", </title> <booktitle> Neural Networks, </booktitle> <volume> 5, </volume> <pages> pp. 565-576. </pages>
Reference-contexts: This architecture still has some of the problems of the static fixed size window network: the delays are fixed a priori and a postprocessor is required to perform continuous speech recognition. A related architecture is based on the gamma memory <ref> [18] </ref>, in which the hidden units perform a computation equivalent to a temporal convolution of their input with a kernel that is a gamma density function. This convolution is computed incrementally.
Reference: 19. <author> J.S. Denker, C. Burges, </author> <title> "Image segmentation and recognition: an overview", </title> <type> Technical Memorandum 11389-921212-30TM, </type> <institution> AT&T Bell Laboratories, Adaptive Systems Research Department, </institution> <month> Decem-ber </month> <year> 1992. </year>
Reference-contexts: Instead, 16 This model is similar to one proposed in [12] and was developed in discussions with L. Bottou and J. Denker (see also <ref> [19] </ref>). 17 or similarly, we could write f (t; O o ) = t:pr g (t:dest; O o ) where t:pr is the transition probability and g (t:dest; O o ) is the output of the NN associated to the destination state t:dest. 18 the normalization is performed at the "model" <p> The recognized interpretation is then the one associated to the final state at the final time 23 . Another possibility for approximately obtaining the best interpretation is studied in <ref> [19] </ref> in the context of image segmentation and recognition. They use the same criterion as proposed in this section (equation 4), corresponding to a recognition criterion equivalent to that defined in equation 6. They approximate equation 6 as follows. <p> Each of these forward passes is rather inexpensive in comparison to computing 6 exactly. Finally, the interpretation k yielding the best score is selected. Comparative experimental results <ref> [19] </ref> on zip-code recognition confirm the advantages of the hybrid and training criterion discussed in this section. 5 Conclusion 22 a single entering state whose input transitions come from outside the group, and a single exiting state. 23 or if there are many final states, the interpretation associated to the best
Reference: 20. <author> X. Driancourt, L. Bottou and P. Gallinari, </author> <title> "Learning vector quantization, multi layer perceptron and dynamic programming: comparison and cooperation," </title> <booktitle> Proceedings of the International Joint Conference on Neural Networks, IJCNN 1991, </booktitle> <volume> vol. II, </volume> <month> pp.815-819. </month>
Reference: 21. <author> G. Flammia, </author> <title> Speaker Independent Consonant Recognition in Continuous Speech with Distinctive Phonetic Features, M.Sc. </title> <type> Thesis, </type> <institution> McGill University, School of Computer Science, </institution> <year> 1991. </year>
Reference-contexts: Experiments on output coding schemes are described in [6,8]. It was found that a "one-output-per-phoneme" coding for plosives and nasals was worse than using horizontal place, vertical place and voicing of the current phoneme. Furthermore, using context-dependent output units improved performance even more. As described in <ref> [21] </ref>, for the recognition of the 10 classes of plosives and nasals /p,t,k,b,d,g,dx,m,n,ng/ in the TIMIT database, generalization error was 36.4% with one output node per phoneme, but only 28.7% by coding with 7 nodes representing the following phonetic features: labial, alveolar, velar, flap, voiced, stop, nasal.
Reference: 22. <author> M. Franzini, K.F. Lee and A. Waibel, </author> <title> "Connectionist Viterbi training: a new hybrid method for continuous speech recognition," </title> <booktitle> Proceedings of the International Conference on Acoustics, Speech and Signal Processing, </booktitle> <address> Albuquerque, NM, </address> <year> 1990, </year> <pages> pp. 425-428. </pages>
Reference: 23. <author> P. Frasconi, M. Gori, M. Maggini, and G. </author> <title> Soda, "Unified Integration of Explicit Rules and Learning by Example in Recurrent Networks," </title> <journal> IEEE Trans. on Knowledge and Data Engineering, </journal> <note> in press. </note>
Reference-contexts: Global search methods such as simulated annealing can be applied to such problems, but they are generally very slow. One way to help in the training of recurrent networks is to set their connectivity and initial weights using prior knowledge. For example, this is accomplished in <ref> [23] </ref> using prior rules and sequentiality constraints. Another way to introduce prior knowledge about temporal structure is with hidden Markov models (HMM), which have been used for speech recognition for many years, and that can be combined with NNs.
Reference: 24. <author> M. Gori, Y. Bengio and R. De Mori, </author> <title> "BPS: a learning algorithm for capturing the dynamic nature of speech," </title> <booktitle> Proc. IEEE Int. Joint Conf. on Neural Networks, </booktitle> <address> Washington DC, </address> <year> 1989, </year> <pages> pp. </pages> <month> II.417-II.424. </month>
Reference: 25. <author> I. Guyon, V. Vapnik, B. Boser, L. Bottou, and S.A. Solla, </author> <title> "Structural risk minimization for character recognition," </title> <booktitle> Advances in Neural Information Processing Systems 4, </booktitle> <editor> (eds. Moody, </editor> <publisher> Hanson & Lippmann) Morgan Kaufmann Publ., </publisher> <year> 1992, </year> <pages> pp. 471-479. </pages>
Reference-contexts: This choice may influence generalization. For example, in [6] we found that smoothing the input "image" improved generalization. This is consistent with results from <ref> [25] </ref> on how smoothing the input reduces the "effective capacity" of the system. In our experiments, the inputs represented formants with a two-dimensional grid of energy and frequency, where an individual cell represented the presence of a formant at a given energy and frequency.
Reference: 26. <author> P. Haffner, M. Franzini and A. Waibel, </author> <title> "Integrating time alignment and neural networks for high performance continuous speech recognition," </title> <booktitle> Proceedings of the International Conference on Acoustics, Speech, and Signal Processing (ICASSP-91), </booktitle> <address> Toronto, </address> <year> 1991, </year> <pages> pp. 105-108. </pages>
Reference: 27. <author> R.A. Jacobs, M.I. Jordan, S.J. Nowlan, G.E. Hinton, </author> <title> "Adaptive mixtures of local experts", </title> <journal> Neural Computation, </journal> <volume> 3 (1), </volume> <year> 1991, </year> <pages> pp. 79-87. </pages>
Reference-contexts: See section 2.1 above on examples of this approach. In some cases, we don't know at all how to modularize the network. In <ref> [27] </ref>, an algorithm is proposed that achieves self-organization of the modules by automatically performing a corresponding partition in the input space. 3 Connectionist Architectures for Speech Recognition Various basic structures for speech recognition are described in this section and illustrated in Figure 1.
Reference: 28. <author> Y. Le Cun, </author> <title> "Generalization and network design strategies,". In Connectionism in Perspective, </title> <publisher> 24 Elsevier Science Publ., </publisher> <year> 1989, </year> <pages> pp. 143-155. </pages>
Reference-contexts: Architectural constraints can be introduced in several ways. The network architecture itself can be constrained by reducing connectivity, using share weights, or more generally using a weight space transformation <ref> [28] </ref>. The choice of input preprocessing and coding, as well as output representation and coding are also important. Finally, the system can be decomposed into modules by decomposing the problem into subproblems (e.g., see [51,6,41]), sometimes creating intermediate variables.
Reference: 29. <author> Y. Le Cun, B. Boser, J.S. Denker, D. Henderson, R.E. Howard, W. Hubbard and L.D. Jackel, </author> <title> "Backpropagation applied to handwritten zip code recognition," </title> <journal> Neural Computation, </journal> <volume> vol. 1, no. 4, </volume> <year> 1989, </year> <month> pp.541-551. </month>
Reference: 30. <author> K.F. Lee, </author> <title> Automatic Speech Recognition: the development of the SPHINX system., </title> <publisher> Kluwer Academic Publ., </publisher> <year> 1989. </year>
Reference: 31. <author> H.C. Leung, J.R. Glass, M.S. Phillips and V.W. Zue, </author> <title> "Phonetic classification and recognition using the multi-layer perceptron," </title> <booktitle> Advances in Neural Information Processing Systems 3, </booktitle> <editor> eds. Lippmann, Moody and Touretzky, </editor> <publisher> publ. Morgan Kaufmann, </publisher> <year> 1991, </year> <pages> pp. 248-254. </pages>
Reference-contexts: A more sophisticated approach considers multiple segmentation hypotheses, with each segment constructed from a pair of boundaries satisfying some constraints, as in [31,16, 45]. The network produces as output a classification score for each hypothetical segment. A post-processor interprets those scores 11 . In <ref> [31] </ref>, segment probabilities are estimated and incorporated to phonetic evidence computed by the network.
Reference: 32. <author> E. Levin, </author> <title> "Word recognition using hidden control neural architecture," </title> <booktitle> Proceedings of the International Conference on Acoustics, Speech and Signal Processing, </booktitle> <address> Albuquerque, NM, </address> <year> 1990, </year> <pages> pp. 433-436. </pages>
Reference-contexts: In [32,48], a network per class or per state is trained to predict the next input frame given only a few previous frames. In [48] the difference between predicted and actual input is used to compute a local distance whereas in <ref> [32] </ref> it is used to compute a state conditional observation likelihood (that can be used in the HMM). In the latter system, the Viterbi algorithm is used to obtain a segmentation and train the network.
Reference: 33. <author> E. Levin, R. Pieraccini and E. Bocchieri, </author> <title> "Time-warping network: a hybrid framework for speech recognition," </title> <booktitle> Advances in Neural Information Processing Systems 4, </booktitle> <editor> eds. Moody, Hanson, and Lipp-mann, </editor> <publisher> publ. Morgan Kaufmann, </publisher> <year> 1992, </year> <pages> pp. 151-158. </pages>
Reference-contexts: A similar model is proposed in [40], which includes "log" and "divide" elements to perform normalization (required to prevent numerical underflow). In the more recent work of <ref> [33] </ref>, DP is incorporated in the operation of each unit, which allows the authors to propose interesting generalizations of HMMs which contain multiple layers of such units.
Reference: 34. <author> S.E. Levinson, </author> <title> L.R. Rabiner and M.M. Sondhi, "An introduction to the application of the theory of probabilistic functions of a Markov process to automatic speech recognition," </title> <journal> Bell System Technical Journal, </journal> <volume> vol. 64, no. 4, </volume> <year> 1983, </year> <pages> pp. 1035-1074. </pages>
Reference-contexts: It is an approximation because the Viterbi algorithm is used instead of the Baum-Welsh algorithm [42]. A different kind of hybrid consists in constructing a HMM with connectionist elements. For example [15] shows how the Baum-Welsh forward pass <ref> [34] </ref> in a HMM can be realized by combining sums, products, and recurrences. It is also shown how the backward pass is equivalent to computing error gradients with the back-propagation algorithm when applied to this architecture. <p> First, the NN (which is recurrent, here) is trained to recognize phonetic features. Second, the parameters of the NN are fixed and the parameters of the GM + HMM (Gaussian mixture + hidden Markov model) subsystem are estimated for the ML criterion with the associated efficient EM algorithms <ref> [34] </ref>. Finally, a global tuning can be performed with the MMI criterion or the ML criterion and stochastic gradient descent on the whole system. The MMI criterion is a discriminant criterion and it can be defined as the maximization of the logarithm of the ratio of two likelihoods.
Reference: 35. <author> K.A. Al-Mashouq and I.S. Reed, </author> <title> "Including Hints in Training Neural Nets," </title> <journal> Neural Computation, </journal> <volume> Vol. 3, No. 4, </volume> <year> 1991, </year> <note> p. 418. </note>
Reference: 36. <author> O. Matan, C.J.C. Burges, Y. Le Cun and J.S. Denker, </author> <title> "Multi-digit recognition using a space displacement neural network," </title> <booktitle> Advances in Neural Information Processing Systems 4, </booktitle> <editor> eds. Moody, Hanson, and Lippmann, </editor> <publisher> publ. Morgan Kaufmann, </publisher> <year> 1992, </year> <pages> pp. 488-495. </pages>
Reference-contexts: This is a popular model used by several groups [50,51,12,26,6] and extended to recognition of images 10 Hence the higher level layers capture some of the temporal structure in the previous layers. 9 in <ref> [36] </ref>. The last layer may be used to perform an integration of the outputs of the previous layer over time.
Reference: 37. <author> C. McMillan, M.C. Mozer and P. Smolensky, </author> <title> "Learning explicit rules in a neural network," </title> <booktitle> Proceedings of the International Joint Conference on Neural Networks, IJCNN 1991, </booktitle> <volume> vol. II, </volume> <month> pp.83-88. </month>
Reference: 38. <author> N. Morgan and H. Bourlard, </author> <title> "Continuous speech recognition using multilayer perceptrons with hidden Markov models," </title> <booktitle> Proceedings of the International Conference on Acoustics, Speech and Signal Processing, </booktitle> <address> Albuquerque, NM, </address> <year> 1990, </year> <pages> pp. 413-416. </pages>
Reference: 39. <author> M. C. Mozer, </author> <title> "Neural net architectures for temporal sequence processing", </title> <note> to appear in: </note> <editor> A. Weigend & N. Gershenfeld (Eds), </editor> <title> Predicting the future and understanding the past, </title> <address> Redwood City, CA: </address> <publisher> Addison-Wesley Publ., </publisher> <year> 1993. </year>
Reference-contexts: A related architecture is based on the gamma memory [18], in which the hidden units perform a computation equivalent to a temporal convolution of their input with a kernel that is a gamma density function. This convolution is computed incrementally. Different forms of short-term memory are reviewed in <ref> [39] </ref>. 3.4 Segment-Based Classification In this approach, one assumes a preprocessor that proposes segments or segment boundaries. An unstructured network similar to the static isolated word recognizer may be applied to each segment. Each input segment is scaled to constant length. <p> In contrast, in network with time delays such as TDNNs, the designer of the network must decide a priori (by the choice of delay connections) which part of the past input sequence should be used to predict the next output. According to the terminology introduced in <ref> [39] </ref>, the memory is static in the case of TDNNs but it is adaptive in the case of recurrent networks.
Reference: 40. <author> L.T. Niles and H.F. Silverman, </author> <title> "Combining hidden Markov models and neural network classifiers," </title> <booktitle> Proceedings of the International Conference on Acoustics, Speech and Signal Processing, </booktitle> <address> Albuquerque, 25 NM, </address> <year> 1990, </year> <pages> pp. 417-420. </pages>
Reference-contexts: It is also shown how the backward pass is equivalent to computing error gradients with the back-propagation algorithm when applied to this architecture. A similar model is proposed in <ref> [40] </ref>, which includes "log" and "divide" elements to perform normalization (required to prevent numerical underflow). In the more recent work of [33], DP is incorporated in the operation of each unit, which allows the authors to propose interesting generalizations of HMMs which contain multiple layers of such units.
Reference: 41. <author> L.Y. Pratt and C.A. Kamm, </author> <title> "Improving a phoneme classification neural network through problem decomposition," </title> <booktitle> Proceedings of the International Joint Conference on Neural Networks, IJCNN 1991, </booktitle> <volume> vol. II, </volume> <pages> pp. 821-826. </pages>
Reference: 42. <author> L.R. Rabiner and B.H. Juang, </author> <title> "An introduction to hidden Markov models," </title> <journal> IEEE ASSP Magazine, </journal> <month> Jan. </month> <year> 1986, </year> <pages> pp. 257-285. </pages>
Reference-contexts: Since a NN criterion function usually has several local minima and a limited time is given to train the NN, initial values of the parameters can influence their final value. A module of the system may be constructed in a non-connectionist way, for example a hidden Markov model <ref> [42] </ref>, and its parameters (if any) may be initialized in a different way from the network weights. <p> In [6,26,20,48,16] the outputs of the 14 NN are not interpreted as probabilities, but rather are used as scores and generally combined with DP. On the other hand, [14,22,38, 45] interpret the outputs of the NN as posterior probabilities 12 and use the Viterbi algorithm <ref> [42] </ref>. The systems proposed in [4,6,7,32] combine a NN with a HMM. In particular, in [4], the outputs of the NN are quantized, hence no global optimization of the hybrid can be performed. <p> Hence the system is trained to approximately maximize the likelihood of the observations (given the predictive network model). It is an approximation because the Viterbi algorithm is used instead of the Baum-Welsh algorithm <ref> [42] </ref>. A different kind of hybrid consists in constructing a HMM with connectionist elements. For example [15] shows how the Baum-Welsh forward pass [34] in a HMM can be realized by combining sums, products, and recurrences. <p> Its parameters are transition probabilities and initial state probabilities. The output of the HMM module may be computed with the forward pass of the Baum-Welsh algorithm <ref> [42] </ref>. In [7], we present a system built as the concatenation NN + GM + HMM. Experiments were performed on the recognition of plosives with the TIMIT database. First, the NN (which is recurrent, here) is trained to recognize phonetic features. <p> In practice, for the MMI systems, speech recognizers are built by taking approximations of equation 6. The most widespread such approximation is the Viterbi <ref> [42] </ref> algorithm 21 . It computes s = argmax p2M t2p f (t; O o ) with an efficient recursive formula similar to that in equation 8. Since each path p is uniquely associated to an interpretation k, this interpretation is the one the algorithm produces as the recognized interpretation.
Reference: 43. <author> M.L. </author> <title> Rossen Speech Syllable Recognition with a Neural Net, </title> <type> Ph.D. </type> <institution> Th., Brown University, </institution> <year> 1989. </year>
Reference-contexts: Considering, for example, automatic speech recognition, our work (see [6,5]), as well as the results of others <ref> [43] </ref>, indicate that the choice of signal processing significantly influences the performance of a recognition system. Thus, in experiments described in [17], we found an auditory model to perform better as a preprocessor for a feedforward neural network than a Mel-scaled filter bank based on the discrete Fourier transform.
Reference: 44. <author> D.E. Rumelhart, G.E. Hinton and R.J. Williams, </author> <title> "Learning internal representation by error propagation," Parallel Distributed Processing volume 1. </title> <editor> Rumelhart D.E. and McClelland J.L. (eds.), </editor> <publisher> Brad-ford Books, MIT Press, </publisher> <year> 1986, </year> <pages> pp. 318-362. </pages>
Reference-contexts: We consider in particular recurrent networks and hybrids combining neural networks and systems based on dynamic programming (DP), such as hidden Markov models (HMM). The connectionist models considered here are mostly multi-layer networks and recurrent networks, usually trained by stochastic gradient descent with a back-propagation algorithm <ref> [44] </ref>. In Section 2, several aspects of the integration of prior knowledge with learning are discussed with examples. We classify a priori constraints into three categories 1 : constraints on the architecture, constraints that guide training, and the use of prior knowledge to initialize free parameters. <p> For example, [47] shows how to train a back-propagation network such that the classification it performs is constrained to be locally invariant 4 to input image transformations such as translation, rotation, scaling and thinning. 2 i.e., for which an error criterion is satisfied. 3 such as weight decay <ref> [44] </ref>. 4 along the hyperplane tangent to the invariance manifold. 3 Finally, the initialization of the parameters of the learning system can also use prior knowledge and constraints. <p> According to the terminology introduced in [39], the memory is static in the case of TDNNs but it is adaptive in the case of recurrent networks. We will now discuss speech recognition experiments on recurrent NNs trained using stochastic gradient descent and back-propagation through time <ref> [44] </ref> or the BPS algorithm [24,6], in both cases computing the full gradient (derivative of the criterion function with respect to the parameters).
Reference: 45. <author> M. Schenkel, H. Weissman, I. Guyon, C. Nohl, D. Henderson, </author> <title> "Recognition-based segmentation of on-line hand-printed words", </title> <note> to appear in Advances in Neural Information Processing Systems 5, </note> <editor> (ed. S. Hanson), </editor> <publisher> Morgan Kaufmann, </publisher> <year> 1993. </year>
Reference-contexts: It is much faster than a frame-based method (as those described above) but is unfortunately very sensitive to errors of the segmentation preprocessor. A more sophisticated approach considers multiple segmentation hypotheses, with each segment constructed from a pair of boundaries satisfying some constraints, as in <ref> [31,16, 45] </ref>. The network produces as output a classification score for each hypothetical segment. A post-processor interprets those scores 11 . In [31], segment probabilities are estimated and incorporated to phonetic evidence computed by the network. <p> Alternatively, [53] uses the NN to rescore the N-best hypotheses produced with a HMM. In [6,26,20,48,16] the outputs of the 14 NN are not interpreted as probabilities, but rather are used as scores and generally combined with DP. On the other hand, <ref> [14,22,38, 45] </ref> interpret the outputs of the NN as posterior probabilities 12 and use the Viterbi algorithm [42]. The systems proposed in [4,6,7,32] combine a NN with a HMM. In particular, in [4], the outputs of the NN are quantized, hence no global optimization of the hybrid can be performed.
Reference: 46. <author> T. Sejnowski and C.R. Rosenberg, "NETtalk: </author> <title> a parallel network that learns to read aloud," </title> <type> Technical report JHU/EECS-86/01, </type> <institution> John Hopkins University, Electr. and Comp. Sc. Dept., </institution> <year> 1986. </year>
Reference-contexts: seen to require many more free parameters because it doesn't take into account inherent regularities in the data. 3.2 Static Fixed Window For this architecture, illustrated in Figure 1.2, the network "sees" only a few frames at a time and produces an output at every frame (see for example NETtalk <ref> [46] </ref>). It can be used in continuous speech recognition but requires a higher level post-processor that interprets its output sequence to generate a phoneme sequence or a word sequence. It is insensitive to time alignment and less sensitive to time warping than the static isolated word recognizer.
Reference: 47. <author> P. Simard, B. Victorri, Y. Le Cun and J. Denker, </author> <title> "Tangent Prop a formalism for specifying selected invariances in an adaptive network," </title> <booktitle> Advances in Neural Information Processing Systems 4, </booktitle> <editor> eds. Moody, Hanson, and Lippmann, </editor> <publisher> publ. Morgan Kaufmann, </publisher> <year> 1992, </year> <pages> pp. 895-903. </pages>
Reference-contexts: Of the constraints that guide training, the first one is in the choice of cost function and possible penalty terms 3 . A particularly interesting type of constraint that guides training is one which imposes some invariance on the function computed by the network. For example, <ref> [47] </ref> shows how to train a back-propagation network such that the classification it performs is constrained to be locally invariant 4 to input image transformations such as translation, rotation, scaling and thinning. 2 i.e., for which an error criterion is satisfied. 3 such as weight decay [44]. 4 along the hyperplane
Reference: 48. <author> J. Tebelskis, A. Waibel, B. Petek and O. Schmidbauer, </author> <title> "Continuous speech recognition using linked predictive networks," </title> <booktitle> Advances in Neural Information Processing Systems 3, </booktitle> <editor> eds. Lippmann, Moody and Touretzky, </editor> <publisher> publ. Morgan Kaufmann, </publisher> <year> 1991, </year> <pages> pp. 199-205. </pages>
Reference-contexts: In [32,48], a network per class or per state is trained to predict the next input frame given only a few previous frames. In <ref> [48] </ref> the difference between predicted and actual input is used to compute a local distance whereas in [32] it is used to compute a state conditional observation likelihood (that can be used in the HMM).
Reference: 49. <author> G. Towell and J.W. Shavlik, </author> <title> "Interpretation of artificial neural networks: mapping knowledge-based neural networks into rules," </title> <booktitle> Advances in Neural Information Processing Systems 4, </booktitle> <editor> eds. Moody, Hanson, and Lippmann, </editor> <publisher> publ. Morgan Kaufmann, </publisher> <year> 1992, </year> <pages> pp. 977-984. </pages>
Reference: 50. <author> A. Waibel, T. Hanazawa, G. Hinton, K. Shikano, K. Lang, </author> <title> "Phoneme Recognition Using Time-Delay Neural Networks," </title> <journal> IEEE Trans. on ASSP, </journal> <volume> vol. 37, no. 3, </volume> <month> March </month> <year> 1989. </year>
Reference-contexts: section 2.3). 2.2 Output Coding Most NNs designed for phoneme recognition have a simple output coding scheme consisting of one output unit per phoneme, with a high desired output for the output unit corresponding to the target phoneme and a low desired output for the other units (see for example <ref> [50] </ref>). In these cases, one can interpret the output activations of the network as representing degrees of evidence. Using phonetic knowledge, we have explored coding schemes based on phonetic features related to speech production. Examples of such features are horizontal and vertical place of articulation, voicing and nasality. <p> This task can be performed for example by a hidden Markov model (see Section 4). 3.3 Time-Delay Neural Network (TDNN) This is a generalization of the static fixed window network, in which there are multiple time delays between successive layers 10 . On can also view a TDNN <ref> [50] </ref> as an ordinary isolated word (or phoneme) recognizer constrained by shared weights and local connectivity.
Reference: 51. <author> A. Waibel, H. Sawai and K. Shikano, </author> <title> "Modularity and scaling in large phonemic neural networks," </title> <journal> IEEE Transactions on Acoustics, Speech and Signal Processing, </journal> <volume> vol. ASSP-37, </volume> <year> 1989, </year> <pages> pp. 1888-1898. </pages>
Reference: 52. <author> Y.H. Yu and R.F. Simmons, </author> <title> "Extra output biased learning," </title> <booktitle> Proceedings of the International Joint Conference on Neural Networks, IJCNN 1990. </booktitle>
Reference-contexts: For example, in [17], in addition to the units representing each vowel, supervision was given to extra output units representing phonetic features. These additional constraints on the network guided it towards computing these phonetic features, and yielded an improvement in generalization. In <ref> [52] </ref>, adding extra output units in order to improve generalization is also suggested. In [8,21], context-dependent output units are used to impose additional constraints, asking more of the network than necessary but in this way helping its performance. Experiments on output coding schemes are described in [6,8].
Reference: 53. <author> G. Zavaliagkos, S. Austin, J. Makhoul and R. Schwartz, </author> <title> "A hybrid continuous speech recognition system using segmental neural nets with hidden Markov models ", in this issue of the International 26 Journal of Pattern Recognition and Artificial Intelligence, </title> <year> 1993. </year>
Reference-contexts: Several others propose a NN architecture that can emulate a HMM [15,40] and/or integrate DP in a connectionist unit [26,33]. Alternatively, <ref> [53] </ref> uses the NN to rescore the N-best hypotheses produced with a HMM. In [6,26,20,48,16] the outputs of the 14 NN are not interpreted as probabilities, but rather are used as scores and generally combined with DP.
Reference: 54. <author> Zwicker E. and Terhardt E. </author> <year> (1980). </year> <title> Analytical expressions for critical band rate and critical bandwidths as a function of frequency. </title> <journal> Journal of the Acoustical Society of America, </journal> <volume> vol. 68, no. 5, </volume> <month> pp.1523-1525. </month> <title> (experiment #5 in Table 1). 29 to robustly store bits of state information. fi is the basin of attraction of an attractor X . is the region where the 2-norm of the Jacobian of the system dynamics map is less than 1. A ball of uncertainty grows exponentially (a) outside , but is bounded (b) inside . 30 </title>
Reference-contexts: The 192 SI and SX sentences from the core test set were used for evaluating generalization. At each time frame, the input to the neural network is a 32-filter Bark-scale spectrogram (see <ref> [54] </ref>). The results are summarized in Table 1. Figure 2 shows the architecture that yielded the best results (see also [6] for the other architectures). An important aspect of this architecture is that it combines recurrence with delays. Another interesting point is that it is not a fully recurrent network.
References-found: 54

