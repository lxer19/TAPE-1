URL: http://www.cs.utoronto.ca/~cogrobo/indexmodal.ps.Z
Refering-URL: http://www.cs.utoronto.ca/~cogrobo/
Root-URL: 
Email: flesperan,hectorg@ai.toronto.edu  
Title: Theories of Interaction and  Knowledge and Robot Action A Logical Account  
Author: Agency, Agre, P.E. and Rosenschein, S.J. (Eds.). Indexical Yves Lesperance and Hector J. Levesque 
Date: January 10, 1994  
Address: Toronto, ON, M5S 1A4 Canada  
Affiliation: Department of Computer Science University of Toronto  
Note: To appear in Artificial Intelligence, special issue on Computational  Fellow of the Canadian Institute for Advanced Research  
Abstract-found: 0
Intro-found: 1
Reference: [1] <author> Philip E. Agre. </author> <title> Review of [54]. </title> <journal> Artificial Intelligence, </journal> <volume> 43(3) </volume> <pages> 369-384, </pages> <year> 1990. </year>
Reference-contexts: Agre and Chapman [2, 3] have been among the most radical in their reevaluation of the classical paradigm. Their views have been influenced by anthropological theories of action <ref> [1, 54] </ref>. They emphasize the complexity of the real situations in which action occurs, the uncertainty of the information the agent may have about them, and the need for reactivity.
Reference: [2] <author> Philip E. Agre and David Chapman. Pengi: </author> <title> An implementation of a theory of activity. </title> <booktitle> In Proceedings of the Sixth National Conference on Artificial Intelligence, </booktitle> <pages> pages 268-272, </pages> <address> Seattle, WA, </address> <month> July </month> <year> 1987. </year> <booktitle> American Association for Artificial Intelligence, </booktitle> <publisher> Morgan Kaufmann Publishing. </publisher>
Reference-contexts: Indexicality has of course been a central theme of the "situated action" paradigm [52, 5, 54]. Several researchers have argued that indexical knowledge plays a major role in the operation of reactive agents <ref> [2, 53] </ref>. Existing theories of the relationship between knowledge and action cannot handle these cases properly, as they do not accommodate the distinction between indexical knowledge and objective knowledge. <p> The alternative architectures proposed achieve reactivity by emphasizing environment monitoring and the selection of actions appropriate to conditions; the focus is on developing a sophisticated executor. Agre and Chapman <ref> [2, 3] </ref> have been among the most radical in their reevaluation of the classical paradigm. Their views have been influenced by anthropological theories of action [1, 54]. <p> This leads them to argue that the production of most activity does not involve the construction and manipulation of explicit representations of the world; the associated computational costs are just too prohibitive. They say that <ref> [2, p. 268] </ref>: Rather than relying on reasoning to intervene between perception and action, we believe activity mostly derives from very simple sorts of machinery interacting with the immediate situation. <p> They have built various application systems to provide support for their analysis. One of these systems, called Pengi, plays a videogame where one controls a penguin that navigates in a maze, pushes away ice blocks, and confronts malicious "killer bees" <ref> [2] </ref>. <p> Work on reactive agent architectures supplies other reasons for wanting a formalism that can represent indexical knowledge. As pointed out by Agre and Chapman <ref> [2] </ref>, the world can change in unexpected ways and reasoning about change can be very costly; in some cases it is better to rely on perception to get fresh information at every time step rather than try to update a representation of the world; in such cases, the problem of updating <p> Rosenschein and Kaelbling [49, 24] use a logical formalism as a design notation and as a specification language in their robot design framework. Since indexical knowledge appears to be centrally involved in the production of reactive behavior <ref> [2, 53] </ref>, it seems that elements of our logic could prove useful in these roles. It might even be useful as a representation language for a planner or sophisticated executor.
Reference: [3] <author> Philip E. Agre and David Chapman. </author> <title> What are plans for? Robotics and Autonomous Systems, </title> <booktitle> 6 </booktitle> <pages> 17-34, </pages> <year> 1990. </year>
Reference-contexts: The alternative architectures proposed achieve reactivity by emphasizing environment monitoring and the selection of actions appropriate to conditions; the focus is on developing a sophisticated executor. Agre and Chapman <ref> [2, 3] </ref> have been among the most radical in their reevaluation of the classical paradigm. Their views have been influenced by anthropological theories of action [1, 54]. <p> It should be possible to extend our account to model this notion of "ability relative to background conditions". More challenging would be developing a propositional attitude model of the kind of opportunistic acting/planning that Agre and Chapman describe in <ref> [3] </ref>, but not obviously beyond current techniques. The theory holds great potential for applications in the modeling of communication (both natural-language and that involving artificial agents exchanging messages in designed languages), especially in conjunction with the extensions mentioned earlier.
Reference: [4] <author> Alfred V. Aho, John E. Hopcroft, and Jeffrey D. Ullman. </author> <title> The Design and Analysis of Computer Algorithms. </title> <publisher> Addison-Wesley Publishing, </publisher> <address> Reading, MA, </address> <year> 1974. </year>
Reference-contexts: An example involving the heapsort algorithm <ref> [4] </ref> was partly formalized.
Reference: [5] <author> Jon Barwise. </author> <title> Information and circumstance. </title> <journal> Notre Dame Journal of Formal Logic, </journal> <volume> 27(3) </volume> <pages> 324-338, </pages> <year> 1986. </year>
Reference-contexts: For example, by using his sonar, a robot comes to know how far from him an object is (at the current time); he does not learn anything about the object's absolute position. Indexicality has of course been a central theme of the "situated action" paradigm <ref> [52, 5, 54] </ref>. Several researchers have argued that indexical knowledge plays a major role in the operation of reactive agents [2, 53]. Existing theories of the relationship between knowledge and action cannot handle these cases properly, as they do not accommodate the distinction between indexical knowledge and objective knowledge. <p> How do the different levels in such a plan relate to each other? Route planning should be a good test domain. 36 Indexicality is but one aspect of situatedness. One should look at how other aspects can be modeled. For instance, Barwise <ref> [5] </ref> talks about how "situated inference" is often relative to background conditions; as an example, he describes how one might infer that an object will fall from the fact that it has been released in mid-air, an inference that is relative to the presence of gravity.
Reference: [6] <author> Jon Barwise and John Perry. </author> <title> Situations and Attitudes. </title> <publisher> MIT Press/Bradford Books, </publisher> <address> Cambridge, MA, </address> <year> 1983. </year>
Reference-contexts: Some, like Cohen and Levesque's [9], include sophisticated accounts of how the knowledge and intentions of agents are involved in speech acts, but ignore indexicality, in both utterances and mental attitudes. Others, such as Barwise and Perry's <ref> [6] </ref>, include elaborate treatments of indexicality, but do not provide any kind of computational account of how an agent can draw inferences from his prior knowledge and the indexical representations that result from interpreting an utterance. 37 A Additional Proofs A.1 Proof of proposition 4.7 First, let us formally state one
Reference: [7] <author> Hector-Neri. Casta~neda. </author> <title> On the logic of attributions of self-knowledge to others. </title> <journal> Journal of Philosophy, </journal> <volume> 65(15) </volume> <pages> 439-456, </pages> <year> 1968. </year>
Reference-contexts: In English, there are true indexicals (I, you, now, here, etc.), which refer to aspects of the utterance context no matter where they appear, and there are quasi-indexicals/quasi-indicators <ref> [7] </ref> (I myself, you yourself, he himself, etc.), which are used to report that an agent has an indexical mental state. The behavior of our primitive indexicals self and now displays characteristics of both categories.
Reference: [8] <author> Brian F. Chellas. </author> <title> Modal Logic: An Introduction. </title> <publisher> Cambridge University Press, </publisher> <address> Cam-bridge, UK, </address> <year> 1980. </year>
Reference-contexts: Our theory is formulated in a many-sorted first-order modal logic with equality. We assume familiarity with conventional first-order logic as in [39], and at least some acquaintance with the standard apparatus of modal logics, as described in [21] or <ref> [8] </ref>. 3.1 Syntax We want to be able to express attributions of indexical knowledge in our logic, for example, that Rob knows that he himself was holding a cup five minutes ago. In such cases, what is known is a "proposition" that is relative.
Reference: [9] <author> Philip R. Cohen and Hector J. Levesque. </author> <title> Rational interaction as the basis for communication. </title> <editor> In Philip R. Cohen, J. Morgan, and Martha E. Pollack, editors, </editor> <booktitle> Intentions in Communication, </booktitle> <pages> pages 221-255. </pages> <publisher> MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1990. </year>
Reference-contexts: It might even be useful as a representation language for a planner or sophisticated executor. This kind of application might also yield back some insights into indexicality and suggest refinements to our theory. 21 No existing model of natural language use does this. Some, like Cohen and Levesque's <ref> [9] </ref>, include sophisticated accounts of how the knowledge and intentions of agents are involved in speech acts, but ignore indexicality, in both utterances and mental attitudes.
Reference: [10] <author> Ernest Davis. </author> <title> Inferring ignorance from the locality of visual perception. </title> <booktitle> In Proceedings of the Seventh National Conference on Artificial Intelligence, </booktitle> <pages> pages 786-791, </pages> <address> St. Paul, MN, </address> <month> August </month> <year> 1988. </year> <journal> American Association for Artificial Intelligence. </journal>
Reference-contexts: N, j= Can ((sense; scan k (9x (Object (x) ^ pos (x) = here ^ :Holding (x)))), Know (9x (Object (x) ^ pos (x) = here ^ :Holding (x))) _ Know (:9n (k n 0 ^ 9x (Object (x) ^ rpos (x) = hn; 0i ^ :Holding (x))))) 14 Davis <ref> [10, 12, 11] </ref> has done interesting work on the topic of reasoning about knowledge and perception; however, he does not address the fact that the knowledge obtained from perception is indexical knowledge. 28 Appendix A.4 contains a proof of this proposition.
Reference: [11] <author> Ernest Davis. </author> <title> Reasoning about hand-eye coordination. </title> <booktitle> In Working Notes, IJCAI Workshop on Knowledge, Perception, and Planning, </booktitle> <pages> pages 1-6, </pages> <address> Detroit, MI, </address> <month> August </month> <year> 1989. </year>
Reference-contexts: N, j= Can ((sense; scan k (9x (Object (x) ^ pos (x) = here ^ :Holding (x)))), Know (9x (Object (x) ^ pos (x) = here ^ :Holding (x))) _ Know (:9n (k n 0 ^ 9x (Object (x) ^ rpos (x) = hn; 0i ^ :Holding (x))))) 14 Davis <ref> [10, 12, 11] </ref> has done interesting work on the topic of reasoning about knowledge and perception; however, he does not address the fact that the knowledge obtained from perception is indexical knowledge. 28 Appendix A.4 contains a proof of this proposition.
Reference: [12] <author> Ernest Davis. </author> <title> Solutions to a paradox of perception with limited acuity. </title> <booktitle> In Proceedings of the First International Conference on Principles of Knowledge Representation and Reasoning, </booktitle> <pages> pages 79-82, </pages> <address> Toronto, ON, May 1989. </address> <publisher> Morgan Kaufmann Publishers. </publisher>
Reference-contexts: N, j= Can ((sense; scan k (9x (Object (x) ^ pos (x) = here ^ :Holding (x)))), Know (9x (Object (x) ^ pos (x) = here ^ :Holding (x))) _ Know (:9n (k n 0 ^ 9x (Object (x) ^ rpos (x) = hn; 0i ^ :Holding (x))))) 14 Davis <ref> [10, 12, 11] </ref> has done interesting work on the topic of reasoning about knowledge and perception; however, he does not address the fact that the knowledge obtained from perception is indexical knowledge. 28 Appendix A.4 contains a proof of this proposition.
Reference: [13] <author> Robert Goldblatt. </author> <title> Logics of Time and Computation. CSLI Lecture Notes No. 7. Center for the Study of Language and Information, </title> <institution> Stanford University, Stanford, </institution> <address> CA, </address> <year> 1987. </year>
Reference-contexts: The most influential work in this area is Moore's theory of knowledge and action [40, 41]. His framework can be described as a combination of first-order dynamic logic (a modal logic of action) <ref> [13] </ref> with an S4 modal logic of knowledge [19, 18]. 1 On the matter of knowledge prerequisites of action, notice that one may know that an action would achieve a goal without knowing how to execute that action; for example, one may know that cooking beef bourguignon would impress the party's <p> The ones given here make the operators behave exactly as their dynamic logic <ref> [13] </ref> analogues.
Reference: [14] <author> Adam J. Grove. </author> <title> Topics in Multi-Agent Epistemic Logic. </title> <type> PhD thesis, </type> <institution> Department of Computer Science, Stanford University, Stanford, </institution> <address> CA, </address> <month> October </month> <year> 1992. </year> <month> 45 </month>
Reference-contexts: In [47], Perry argues convincingly that indexicality is an essential feature of propositional attitudes. Let us say a bit more about some recent work by Grove <ref> [14] </ref> and Grove and Halpern [15], where they propose a logic of knowledge that does handle indexicality. Their account of knowledge is quite similar to ours; in fact, their semantics is essentially the same as ours. <p> One can also imagine mass produced computers or robots that do not have their name etched in memory at the factory or even entered at boot-time. One might also look at processes running in a computer as agents. Grove <ref> [14] </ref> and Grove and Halpern [15] describe cases in the area of distributed systems and communication protocols where one does not want to assume that agents know who they are.
Reference: [15] <author> Adam J. Grove and Joseph Y. Halpern. </author> <title> Naming and identity in a multi-agent epis--temic logic. </title> <editor> In James Allen, Richard Fikes, and Erik Sandewall, editors, </editor> <booktitle> Principles of Knowledge Representation and Reasoning: Proceedings of the Second International Conference, </booktitle> <pages> pages 301-312, </pages> <address> Cambridge, MA, 1991. </address> <publisher> Morgan Kaufmann Publishing. </publisher>
Reference-contexts: In [47], Perry argues convincingly that indexicality is an essential feature of propositional attitudes. Let us say a bit more about some recent work by Grove [14] and Grove and Halpern <ref> [15] </ref>, where they propose a logic of knowledge that does handle indexicality. Their account of knowledge is quite similar to ours; in fact, their semantics is essentially the same as ours. <p> One can also imagine mass produced computers or robots that do not have their name etched in memory at the factory or even entered at boot-time. One might also look at processes running in a computer as agents. Grove [14] and Grove and Halpern <ref> [15] </ref> describe cases in the area of distributed systems and communication protocols where one does not want to assume that agents know who they are.
Reference: [16] <author> Andrew R. Haas. </author> <title> A syntactic theory of belief and action. </title> <journal> Artificial Intelligence, </journal> <volume> 28 </volume> <pages> 245-292, </pages> <year> 1986. </year>
Reference-contexts: Neither Konolige nor Morgenstern recognize the role of indexical knowledge in action; their formalisms have the same limitations as Moore's in this respect. One researcher who did recognize it is Haas. In <ref> [16] </ref>, he sketches how indexical knowledge might be handled in a specification of ability; but he does not formalize his proposals. 2.2 Theories of Indexical Knowledge There has been a lot of work on indexical knowledge in philosophy, but we can only mention a few references here (see [29]).
Reference: [17] <author> Andrew R. Haas. </author> <title> Indexical expressions and planning. </title> <type> Unpublished manuscript, </type> <institution> Department of Computer Science, State University of New York, Albany, </institution> <address> NY, </address> <year> 1991. </year>
Reference-contexts: For instance, if an agent's knowledge base describes some facts as holding `now', then at the next time step, it should describe these facts as holding `one time step before now'. 19 Haas <ref> [17] </ref> points out that the cost of adjusting a knowledge base that contains indexical time references for the passage of time would be high, if implemented in the obvious way. He proposes that a robot use its internal clock to eliminate all occurrences of `now' in its representations.
Reference: [18] <author> Joseph Y. Halpern and Yoram Moses. </author> <title> A guide to the modal logics of knowledge and belief: Preliminary draft. </title> <booktitle> In Proceedings of the Ninth International Joint Conference on Artificial Intelligence, </booktitle> <pages> pages 480-490, </pages> <address> Los Angeles,CA, August 1985. </address> <publisher> Morgan Kauf-mann Publishing. </publisher>
Reference-contexts: We provide a formal semantics for the knowledge operator that handles indexicality; it is a simple and natural extension of the standard possible-world semantic scheme for the logic of knowledge <ref> [20, 18] </ref>. The complete system permits an adequate characterization of indexical knowledge prerequisites and effects of actions in a wide range of circumstances. The specification of the theory as a logical system has several advantages. <p> The most influential work in this area is Moore's theory of knowledge and action [40, 41]. His framework can be described as a combination of first-order dynamic logic (a modal logic of action) [13] with an S4 modal logic of knowledge <ref> [19, 18] </ref>. 1 On the matter of knowledge prerequisites of action, notice that one may know that an action would achieve a goal without knowing how to execute that action; for example, one may know that cooking beef bourguignon would impress the party's guests without knowing how to cook beef bourguignon. <p> In reasoning within a theory, we only deal with semantic structures where the assumptions come out true. Note that due to this, assumptions not only hold at time now, but at all times, and it is common knowledge <ref> [18] </ref> that this is the case (i.e., everyone knows it, everyone knows that everyone knows it, and so on). 11 Even though we are specifically talking about the agent and time of the context in the above, the attribution in fact applies to all agents and times, since it is assumed
Reference: [19] <author> Jaakko Hintikka. </author> <title> Knowledge and Belief. </title> <publisher> Cornell University Press, </publisher> <address> Ithaca, NY, </address> <year> 1962. </year>
Reference-contexts: The most influential work in this area is Moore's theory of knowledge and action [40, 41]. His framework can be described as a combination of first-order dynamic logic (a modal logic of action) [13] with an S4 modal logic of knowledge <ref> [19, 18] </ref>. 1 On the matter of knowledge prerequisites of action, notice that one may know that an action would achieve a goal without knowing how to execute that action; for example, one may know that cooking beef bourguignon would impress the party's guests without knowing how to cook beef bourguignon. <p> If he is later told that the new manager is Paul, he will then come to have a de re belief of Paul that he must be happy. Following Hintikka <ref> [19] </ref>, knowing who/what is is usually taken to amount to knowing of some x that it is (de re). The question of what precisely is required for an agent to have de re knowledge has been the subject of much philosophical debate (see [28] for a discussion).
Reference: [20] <author> Jaakko Hintikka. </author> <title> Semantics for the propositional attitudes. </title> <editor> In J. W. Davis, D. J. Hock-ney, and K. W. Wilson, editors, </editor> <booktitle> Philosophical Logic, </booktitle> <pages> pages 21-45. </pages> <address> D. </address> <publisher> Reidel Publishing, Dordrecht, Holland, </publisher> <year> 1969. </year>
Reference-contexts: We provide a formal semantics for the knowledge operator that handles indexicality; it is a simple and natural extension of the standard possible-world semantic scheme for the logic of knowledge <ref> [20, 18] </ref>. The complete system permits an adequate characterization of indexical knowledge prerequisites and effects of actions in a wide range of circumstances. The specification of the theory as a logical system has several advantages. <p> Finally, our semantics for knowledge is a simple generalization of the standard possible-world scheme <ref> [27, 20] </ref>.
Reference: [21] <author> G. E. Hughes and M. J. Cresswell. </author> <title> An Introduction to Modal Logic. </title> <publisher> Methuen, </publisher> <address> London, UK, </address> <year> 1968. </year>
Reference-contexts: Our theory is formulated in a many-sorted first-order modal logic with equality. We assume familiarity with conventional first-order logic as in [39], and at least some acquaintance with the standard apparatus of modal logics, as described in <ref> [21] </ref> or [8]. 3.1 Syntax We want to be able to express attributions of indexical knowledge in our logic, for example, that Rob knows that he himself was holding a cup five minutes ago. In such cases, what is known is a "proposition" that is relative.
Reference: [22] <author> David J. Israel. </author> <title> The role of propositional objects of belief in action. </title> <type> Technical Report CSLI-87-72, CSLI, </type> <institution> Stanford University, Stanford, </institution> <address> CA, </address> <month> January </month> <year> 1987. </year>
Reference-contexts: An agent can be quite ignorant of what characterizes his absolute position in that space, things like area code, country code, etc. This domain was formalized and it was proven that an agent 16 Many of these characteristics of the map navigation problem were pointed out by Israel <ref> [22] </ref>. 30 is able to establish a connection to some phone if he knows what its number is and either knows that it is in his own area, or knows that it is not and knows what the phone's area code is (international and same-area long distance calls were ignored).
Reference: [23] <author> Leslie P. Kaelbling. </author> <title> An architecture for intelligent systems. </title> <booktitle> In Reasoning about Actions and Plans: Proceedings of the 1986 Workshop, </booktitle> <pages> pages 395-410, </pages> <address> Timberline, OR, July 1987. </address> <publisher> Morgan Kaufmann Publishing. </publisher>
Reference-contexts: Design tools based on the logic are also developed; these tools facilitate the specification of complex agents and allow high-level specifications to be "compiled" into circuit-level ones. The architecture they propose for reactive agents <ref> [23] </ref> also avoids formal manipulation of explicit representations. It involves a perceptual component and an action selection component, both of which may have state (registers). They also argue that the lack of explicit representations does not mean that one loses the ability to ascribe semantic content to machine states.
Reference: [24] <author> Leslie P. Kaelbling and Stanley J. Rosenschein. </author> <title> Action and planning in embedded agents. </title> <booktitle> Robotics and Autonomous Systems, </booktitle> <volume> 6 </volume> <pages> 35-48, </pages> <year> 1990. </year>
Reference-contexts: While it uses logic, it does not take any representational stance (i.e., assume that knowledge is represented by sentence-like entities inside agents); this is similar to the position taken by Rosenschein and Kaelbling in their situated automata work <ref> [49, 24] </ref>. But there is an emerging consensus that satisfactory architectural designs for agents will need to be based on an adequate theory of agent-environment interaction. Our work attempts to provide this kind of foundation. <p> Finally, the theory does not account for the distinctive logical behavior of indexical terms, for example, the fact that the past is determined while the future is not. Another conception of how reactive behavior can be produced has been proposed by Rosenschein and Kaelbling <ref> [49, 24] </ref>. Their approach is of particular interest to us because a logic is used to specify reactive agents and the environments in which they operate. <p> Last but not least, the theory would appear to hold much promise for applications in the design of reactive agents. Rosenschein and Kaelbling <ref> [49, 24] </ref> use a logical formalism as a design notation and as a specification language in their robot design framework. Since indexical knowledge appears to be centrally involved in the production of reactive behavior [2, 53], it seems that elements of our logic could prove useful in these roles.
Reference: [25] <author> Kurt Konolige. </author> <title> A first order formalization of knowledge and action for a multi-agent planning system. </title> <editor> In J.E. Hays and D. Michie, editors, </editor> <booktitle> Machine Intelligence, </booktitle> <volume> volume 10. </volume> <publisher> Ellis Horwood, </publisher> <address> Chichester, UK, </address> <year> 1982. </year>
Reference-contexts: Let us briefly discuss other theories of knowledge and action. One of the unattractive features of the logic of knowledge included in Moore's framework is that knowledge is assumed to be closed under logical consequence. Konolige <ref> [25] </ref> has developed a theory of knowledge and action, based on an account of knowledge as a predicate on sentences, that avoids this defect. 3 His account of ability is essentially a recasting of Moore's into his framework; only the same restricted class of actions is handled.
Reference: [26] <author> Kurt Konolige. </author> <title> A Deduction Model of Belief. </title> <publisher> Pitman Publishing, </publisher> <year> 1986. </year>
Reference-contexts: In AI, the common answer has been that having de re knowledge of some entity requires knowing a standard name for that entity <ref> [26, 35] </ref>, a view 1 Strictly speaking, the framework is not the modal logic just described, but the encoding in first-order logic of the semantics of this modal logic; thus the part of the logic dealing with action is closely related to the situation calculus [38]. 5 shared by Moore as
Reference: [27] <author> Saul A. Kripke. </author> <title> Semantical considerations on modal logic. </title> <journal> Acta Philosophica Fennica, </journal> <volume> 16 </volume> <pages> 83-94, </pages> <year> 1963. </year>
Reference-contexts: Finally, our semantics for knowledge is a simple generalization of the standard possible-world scheme <ref> [27, 20] </ref>.
Reference: [28] <author> Amichai Kronfeld. </author> <title> Reference and Computation : An Essay in Applied Philosophy of Language. </title> <publisher> Cambridge University Press, </publisher> <address> New York, NY, </address> <year> 1990. </year> <month> 46 </month>
Reference-contexts: Following Hintikka [19], knowing who/what is is usually taken to amount to knowing of some x that it is (de re). The question of what precisely is required for an agent to have de re knowledge has been the subject of much philosophical debate (see <ref> [28] </ref> for a discussion).
Reference: [29] <author> Yves Lesperance. </author> <title> A Formal Theory of Indexical Knowledge and Action. </title> <type> PhD thesis, </type> <institution> Department of Computer Science, University of Toronto, Toronto, </institution> <note> ON, January 1991. Also published as technical report CSRI-248. </note>
Reference-contexts: discussing the contributions and limitations of this work, and suggesting various directions for further research; we discuss how this work might be applied to the design of autonomous robots and other types of agents. 2 Related Work This survey is selective and often sketchy; for a more complete discussion, see <ref> [29] </ref>. 2.1 Theories of Knowledge and Action As argued in the introduction, a theory that explains how agents manage to achieve their goals by doing actions, and in particular why they perform knowledge acquisition actions, must account for the effects of action upon knowledge and for the knowledge prerequisites of action; <p> In [16], he sketches how indexical knowledge might be handled in a specification of ability; but he does not formalize his proposals. 2.2 Theories of Indexical Knowledge There has been a lot of work on indexical knowledge in philosophy, but we can only mention a few references here (see <ref> [29] </ref>). Our account of indexical knowledge is inspired from Lewis's view that having a belief involves ascribing a property to oneself and the current time [37]. In [47], Perry argues convincingly that indexicality is an essential feature of propositional attitudes. <p> A much more detailed examination of the theory, including a discussion of the general issues involved can be found in <ref> [29, 33] </ref>. Here, we simply present the theory in sufficient detail to underwrite the applications of the next two sections. Our theory is formulated in a many-sorted first-order modal logic with equality. <p> The language we use is called LIKA and as any first-order logical language, it divides syntactically into terms and formulas. 6 The terms here, however, are of four different 6 LIKA stands for "Language of Indexical Knowledge and Action". In <ref> [29] </ref>, we used the name L index instead of LIKA. 11 sorts: terms for ordinary individuals (as usual), temporal terms, agent terms, and action terms. <p> means that self knows whether ' holds; DoneWhen (ffi; ') means that self has just done ffi and that ' was true when he started; and SomePast (') means that ' held at some point in the past. 8 These definitions are a bit different from the ones given in <ref> [29, 31] </ref>. The ones given here make the operators behave exactly as their dynamic logic [13] analogues. <p> Also, as discussed in section 5, the fact that our account of ability is based on a more expressive temporal logic has important 9 This way of defining Can is preferable to the one in <ref> [29, 31] </ref> as it separates the knowledge prerequisites involving the goal from the rest; this makes it easier to prove results involving complex actions see [33] for further discussion. 18 advantages when dealing with actions whose prerequisites or effects involve knowledge of absolute times and knowing what time it is. 3.7 <p> We show that the logic indeed satisfies these properties in <ref> [29, 33] </ref>, where we also discuss their significance. <p> Proposition 4.2 6j=9p (here = p ^ Know (9x (Object (x) ^ pos (x) = p) ^ :9y Holding (y))) Can (pickup; 9x Holding (x)) 22 The proof is similar to that of the previous proposition; it appears in <ref> [29] </ref>. <p> Proposition 4.4 j=Know (9x (Object (x) ^ pos (x) = here) ^ :9y Holding (y)) Can (pickup; 9x (Holding (x) ^ DoneWhen (pickup; Object (x) ^ pos (x) = here))) 23 The proof is similar to that of proposition 4.3; it appears in <ref> [29] </ref>. This result can be strengthened further to require uniqueness. But it should be clear that identifying the objects involved in the initial and goal situations, without requiring that it be known what objects they are, is not a trivial matter. <p> Let rpos ( i ) stand for the position of i relative to self: Definition 4.8 rpos ( i ) def = ((pos ( i ) here) fi rot (selfori)) In <ref> [29] </ref>, we prove the following result: Proposition 4.12 For all n 2 N, j=Know (9x (Object (x) ^ rpos (x) = hn; 0i) ^ :9y Holding (y)) Can ((forward n ; pickup); 9x Holding (x)) This says that if the robot knows that there is an object at position hn; 0i <p> Know (here = p) _ Know (:9n (k n 0 ^ 9x (Object (x) ^ rpos (x) = hn; 0i ^ :Holding (x) ^ OfShape (x; s)))))) The proof is omitted as it is similar to that of proposition 4.13; a compete proof of a closely related result appears in <ref> [29] </ref>. Similarly, an agent might find out what its absolute orientation is by searching for a pair of landmarks whose orientation relative to one another is indicated on the map, or perhaps by searching for a single landmark whose faces have distinctive shapes that are represented on the map. <p> But this is not the case; the notion is really rather abstract. It is useful as long as the domain involves agents that operate in some kind of space, from some kind of point of view into that space. In <ref> [29] </ref>, two non-robotics domains that involve very abstract notions of space are examined. The first involves an agent making a phone call. The phone system is naturally viewed as a kind of space structured into various kinds of regions and sub-regions. <p> In <ref> [29] </ref>, a formalization of the first two examples is sketched. The third example does not require time to be explicitly represented; it can be formalized much like the scanning example of section 4.5. <p> There is one significant limitation: one cannot express the occurrence of actions involving indefinite iteration (unbounded while-loops). More generally, our account of the logic is limited by the fact that we have not identified a full axiomatization (even though the set of properties identified in <ref> [33, 29] </ref> is an important step in that direction); in fact, we do not know whether the set of valid formulas is recursively enumerable. Our formalization of ability improves over previous accounts in several ways. <p> Lemma A.6 For all n 2 N, j= Know (:UOBTW (n; 0)) Can (forward; :UOBTW (n 1; 1)) This lemma can be proven from assumptions 4.2 and 4.4, as well as the frame assumptions for forward mentioned in section 4.3; a detailed proof of a closely related result appears in <ref> [29] </ref>.
Reference: [30] <author> Yves Lesperance. </author> <title> An approach to modeling indexicality in action and communication. </title> <editor> In John Horty and Yoav Shoham, editors, </editor> <title> Reasoning about Mental States: Formal Theories & Applications, </title> <booktitle> Papers from the 1993 AAAI Spring Symposium, </booktitle> <pages> pages 79-85, </pages> <address> Stanford, CA, </address> <month> March </month> <year> 1993. </year> <note> Technical Report SS-93-05, AAAI Press. Also appears in Proceedings of the IJCAI Workshop on Using Knowledge in its Context, Chambery, </note> <institution> France, </institution> <month> August </month> <year> 1993. </year>
Reference-contexts: For example, I can help you get to my place by telling you where it is relative to your current position. To model this, one needs a formal account of communication that relates the context sensitivity of language to that of mental states and action. 21 In <ref> [30] </ref>, a preliminary version of such an account is developed, using our theory of indexical knowledge and action as a foundation.
Reference: [31] <author> Yves Lesperance and Hector J. Levesque. </author> <title> Indexical knowledge in robot plans. </title> <booktitle> In Proceedings of the Eight National Conference on Artificial Intelligence, </booktitle> <pages> pages 868-874, </pages> <address> Boston, </address> <month> August </month> <year> 1990. </year> <booktitle> American Association for Artificial Intelligence, </booktitle> <publisher> AAAI Press/MIT Press. </publisher>
Reference-contexts: means that self knows whether ' holds; DoneWhen (ffi; ') means that self has just done ffi and that ' was true when he started; and SomePast (') means that ' held at some point in the past. 8 These definitions are a bit different from the ones given in <ref> [29, 31] </ref>. The ones given here make the operators behave exactly as their dynamic logic [13] analogues. <p> Also, as discussed in section 5, the fact that our account of ability is based on a more expressive temporal logic has important 9 This way of defining Can is preferable to the one in <ref> [29, 31] </ref> as it separates the knowledge prerequisites involving the goal from the rest; this makes it easier to prove results involving complex actions see [33] for further discussion. 18 advantages when dealing with actions whose prerequisites or effects involve knowledge of absolute times and knowing what time it is. 3.7
Reference: [32] <author> Yves Lesperance and Hector J. Levesque. </author> <title> An argument for indexical representations in temporal reasoning. </title> <booktitle> Submitted to the Canadian Artificial Intelligence Conference (AI'94), </booktitle> <month> November </month> <year> 1993. </year>
Reference-contexts: In [29], a formalization of the first two examples is sketched. The third example does not require time to be explicitly represented; it can be formalized much like the scanning example of section 4.5. In <ref> [32] </ref>, we also formalize an example of a common temporal reasoning/planning problem that can only be handled in a formalism that includes both indexical and non-indexical concepts and supports reasoning using both. <p> require knowing a standard name), then anything that one knows in an indexical way is also known in an objective way. 20 But is it reasonable to assume that an agent always knows who he is and what time it is? Let's consider the temporal part of this question (see <ref> [32] </ref> for a more detailed discussion). First, humans do not have internal clocks that they can use in the way a robot can, and they do not always know what time it is.
Reference: [33] <author> Yves Lesperance and Hector J. Levesque. </author> <title> On the logic of indexical knowledge and action. </title> <note> In preparation, </note> <year> 1994. </year>
Reference-contexts: A much more detailed examination of the theory, including a discussion of the general issues involved can be found in <ref> [29, 33] </ref>. Here, we simply present the theory in sufficient detail to underwrite the applications of the next two sections. Our theory is formulated in a many-sorted first-order modal logic with equality. <p> See <ref> [33] </ref> for discussion of these constraints. 3.5 Abbreviations To simplify writing formulas in LIKA, it is convenient to use certain abbreviations or notational conventions. Firstly, we assume the usual definitions for _; ^; ; 9; 6=; &gt;; , and . <p> The ones given here make the operators behave exactly as their dynamic logic [13] analogues. The differences are discussed in <ref> [33] </ref>. 17 3.6 Ability We base our formalization of ability on that of Moore [40], which in spite of its relative simplicity, does get at the essential connection between the ability of agents to achieve goals and the knowledge they have about relevant actions. <p> account of ability is based on a more expressive temporal logic has important 9 This way of defining Can is preferable to the one in [29, 31] as it separates the knowledge prerequisites involving the goal from the rest; this makes it easier to prove results involving complex actions see <ref> [33] </ref> for further discussion. 18 advantages when dealing with actions whose prerequisites or effects involve knowledge of absolute times and knowing what time it is. 3.7 Properties of the Logic In this subsection, we list some properties of the logic of LIKA that are used in the proofs of the robotics <p> We show that the logic indeed satisfies these properties in <ref> [29, 33] </ref>, where we also discuss their significance. <p> (' c ) ^ Can (ffi 1 ; ' g )) _ (Know (:' c ) ^ Can (ffi 2 ; ' g )) 19 4 Formalizing a Simple Robotics Domain In the previous section, we briefly reviewed a theory of indexical knowledge, action, and ability (described in detail in <ref> [33] </ref>). We claim that this theory forms an adequate framework for the formalization of actions involving indexical knowledge prerequisites or effects. Let us now substantiate this claim. <p> There is one significant limitation: one cannot express the occurrence of actions involving indefinite iteration (unbounded while-loops). More generally, our account of the logic is limited by the fact that we have not identified a full axiomatization (even though the set of properties identified in <ref> [33, 29] </ref> is an important step in that direction); in fact, we do not know whether the set of valid formulas is recursively enumerable. Our formalization of ability improves over previous accounts in several ways.
Reference: [34] <author> Yves Lesperance, Hector J. Levesque, and Fangzhen Lin. </author> <title> A formalization of ability and knowing how that avoids the frame problem. </title> <booktitle> Submitted to the Fourth International Conference on Principles of Knowledge Representation and Reasoning (KR'94), </booktitle> <month> November </month> <year> 1993. </year>
Reference-contexts: We are currently working on a formalization that deals with these distinctions, handles indefinite iteration and non-determinism, and is compatible with the approach to the frame problem mentioned earlier <ref> [34] </ref>. As well, the formalization of section 3.6 requires that it be known that performing the action absolutely guarantees that the goal will be achieved. Yet, in most real situations, agents cannot hope to attain such a degree of certainty.
Reference: [35] <author> Hector J. Levesque. </author> <title> Foundations of a functional approach to knowledge representation. </title> <journal> Artificial Intelligence, </journal> <volume> 23 </volume> <pages> 155-212, </pages> <year> 1984. </year>
Reference-contexts: In AI, the common answer has been that having de re knowledge of some entity requires knowing a standard name for that entity <ref> [26, 35] </ref>, a view 1 Strictly speaking, the framework is not the modal logic just described, but the encoding in first-order logic of the semantics of this modal logic; thus the part of the logic dealing with action is closely related to the situation calculus [38]. 5 shared by Moore as
Reference: [36] <author> Hector J. Levesque. </author> <title> A logic of implicit and explicit belief. </title> <booktitle> In Proceedings of the National Conference on Artificial Intelligence, </booktitle> <pages> pages 198-202, </pages> <address> Austin, TX, </address> <year> 1984. </year> <journal> American Association for Artificial Intelligence. </journal>
Reference-contexts: This should become clearer after our accounts of knowledge and ability have been introduced. 3 Note however that such "syntactic" accounts have been claimed to have the opposite defect, that is, to individuate knowledge states too finely <ref> [36] </ref>; for instance, it is far from clear that a belief that ' and ' 0 is any different from a belief that ' 0 and '. 7 on the claim that they cannot express the weak knowledge prerequisites involved in multi--agent planning.
Reference: [37] <author> David Lewis. </author> <title> Attitudes de dicto and de se. </title> <journal> The Philosophical Review, </journal> <volume> 88(4) </volume> <pages> 513-543, </pages> <year> 1979. </year>
Reference-contexts: Our account of indexical knowledge is inspired from Lewis's view that having a belief involves ascribing a property to oneself and the current time <ref> [37] </ref>. In [47], Perry argues convincingly that indexicality is an essential feature of propositional attitudes. Let us say a bit more about some recent work by Grove [14] and Grove and Halpern [15], where they propose a logic of knowledge that does handle indexicality.
Reference: [38] <author> John McCarthy and Patrick Hayes. </author> <title> Some philosophical problems from the standpoint of artificial intelligence. </title> <editor> In B. Meltzer and D. Michie, editors, </editor> <booktitle> Machine Intelligence, </booktitle> <volume> volume 4, </volume> <pages> pages 463-502. </pages> <publisher> Edinburgh University Press, Edinburgh, </publisher> <address> UK, </address> <year> 1979. </year>
Reference-contexts: standard name for that entity [26, 35], a view 1 Strictly speaking, the framework is not the modal logic just described, but the encoding in first-order logic of the semantics of this modal logic; thus the part of the logic dealing with action is closely related to the situation calculus <ref> [38] </ref>. 5 shared by Moore as well as the present work. Since what standard names refer to must be common knowledge; this means that they must be objective, and thus, that de re knowledge must in some sense always be objective knowledge. <p> To model the world as viewed by reactive agents, they use a version of the situation calculus <ref> [38] </ref> with a vocabulary that includes indexical terms. The logical constant Now is used to refer the current situation.
Reference: [39] <author> Elliott Mendelson. </author> <title> Introduction to Mathematical Logic (Second Edition). </title> <address> D. </address> <publisher> Van Nos-trand Co., </publisher> <address> New York, </address> <year> 1979. </year>
Reference-contexts: Here, we simply present the theory in sufficient detail to underwrite the applications of the next two sections. Our theory is formulated in a many-sorted first-order modal logic with equality. We assume familiarity with conventional first-order logic as in <ref> [39] </ref>, and at least some acquaintance with the standard apparatus of modal logics, as described in [21] or [8]. 3.1 Syntax We want to be able to express attributions of indexical knowledge in our logic, for example, that Rob knows that he himself was holding a cup five minutes ago. <p> We show that the logic indeed satisfies these properties in [29, 33], where we also discuss their significance. The basis of our logic, that is, the part concerned with first-order logic with equality, is standard (the axiomatization in <ref> [39] </ref> can be used) with one exception: the "axiom" of specialization is restricted to prevent non-rigid terms from being substituted into modal contexts.
Reference: [40] <author> Robert C. Moore. </author> <title> Reasoning about knowledge and action. </title> <type> Technical Report 191, </type> <institution> AI Center, SRI International, </institution> <address> Menlo Park, CA, </address> <month> October </month> <year> 1980. </year> <month> 47 </month>
Reference-contexts: Many theories of action and most existing planners have completely ignored the need to deal with what agents know and need to know by unrealistically assuming that agents always have perfect knowledge of the domain under consideration. But in the past decade, Moore <ref> [41, 40] </ref>, Morgenstern [42, 43], and other researchers have proposed theories of knowledge and action that do address this need. <p> Finally, the logic can be used to reason formally about knowledge, action, and ability | the kind of reasoning a designer might do in ensuring that the agent being designed is able to achieve a goal. Our methodology differs little from that used by Moore <ref> [41, 40] </ref>, Morgenstern [42, 43], and others who have previously worked on theories of knowledge and action. Note that our theory is at the "knowledge level" [45], and as such, does not say much about agent architecture. <p> The most influential work in this area is Moore's theory of knowledge and action <ref> [40, 41] </ref>. <p> The ones given here make the operators behave exactly as their dynamic logic [13] analogues. The differences are discussed in [33]. 17 3.6 Ability We base our formalization of ability on that of Moore <ref> [40] </ref>, which in spite of its relative simplicity, does get at the essential connection between the ability of agents to achieve goals and the knowledge they have about relevant actions. It is simpler than his because we do not attempt to handle indefinite iteration (while-loop actions). <p> And perception yields just the kind of indexical knowledge that is needed. Previous formal accounts of the ability of agents to achieve goals by doing actions, such 33 as that of Moore <ref> [40, 41] </ref> and Morgenstern [42, 43], have ignored this, and thus end up imposing knowledge requirements that are neither necessary nor sufficient for ability; they fail to properly specify the knowledge prerequisites and effects of actions.
Reference: [41] <author> Robert C. Moore. </author> <title> A formal theory of knowledge and action. </title> <editor> In J. R. Hobbs and Robert C. Moore, editors, </editor> <booktitle> Formal Theories of the Common Sense World, </booktitle> <pages> pages 319-358. </pages> <publisher> Ablex Publishing, </publisher> <address> Norwood, NJ, </address> <year> 1985. </year>
Reference-contexts: Many theories of action and most existing planners have completely ignored the need to deal with what agents know and need to know by unrealistically assuming that agents always have perfect knowledge of the domain under consideration. But in the past decade, Moore <ref> [41, 40] </ref>, Morgenstern [42, 43], and other researchers have proposed theories of knowledge and action that do address this need. <p> Finally, the logic can be used to reason formally about knowledge, action, and ability | the kind of reasoning a designer might do in ensuring that the agent being designed is able to achieve a goal. Our methodology differs little from that used by Moore <ref> [41, 40] </ref>, Morgenstern [42, 43], and others who have previously worked on theories of knowledge and action. Note that our theory is at the "knowledge level" [45], and as such, does not say much about agent architecture. <p> The most influential work in this area is Moore's theory of knowledge and action <ref> [40, 41] </ref>. <p> So the agent is not able to achieve the goal by doing pickup and the structure falsifies the formula. In a discussion of the robot action of "putting a block on another block", Moore <ref> [41] </ref> recognizes that knowing what blocks are involved may not be enough and suggests that the action be defined in terms of lower-level actions involving arm motions to the objects' positions, grasping, and ungrasping. <p> And perception yields just the kind of indexical knowledge that is needed. Previous formal accounts of the ability of agents to achieve goals by doing actions, such 33 as that of Moore <ref> [40, 41] </ref> and Morgenstern [42, 43], have ignored this, and thus end up imposing knowledge requirements that are neither necessary nor sufficient for ability; they fail to properly specify the knowledge prerequisites and effects of actions.
Reference: [42] <author> Leora Morgenstern. </author> <title> Knowledge preconditions for actions and plans. </title> <booktitle> In Proceedings of the Tenth International Joint Conference on Artificial Intelligence, </booktitle> <pages> pages 867-874, </pages> <address> Milan, Italy, August 1987. </address> <publisher> Morgan Kaufmann Publishing. </publisher>
Reference-contexts: Many theories of action and most existing planners have completely ignored the need to deal with what agents know and need to know by unrealistically assuming that agents always have perfect knowledge of the domain under consideration. But in the past decade, Moore [41, 40], Morgenstern <ref> [42, 43] </ref>, and other researchers have proposed theories of knowledge and action that do address this need. <p> Finally, the logic can be used to reason formally about knowledge, action, and ability | the kind of reasoning a designer might do in ensuring that the agent being designed is able to achieve a goal. Our methodology differs little from that used by Moore [41, 40], Morgenstern <ref> [42, 43] </ref>, and others who have previously worked on theories of knowledge and action. Note that our theory is at the "knowledge level" [45], and as such, does not say much about agent architecture. <p> A theory that significantly extends this coverage has been developed by Morgenstern <ref> [42, 43] </ref>. It handles both concurrent actions and plans involving multiple agents. Simpler cases are treated as in Moore's. <p> And perception yields just the kind of indexical knowledge that is needed. Previous formal accounts of the ability of agents to achieve goals by doing actions, such 33 as that of Moore [40, 41] and Morgenstern <ref> [42, 43] </ref>, have ignored this, and thus end up imposing knowledge requirements that are neither necessary nor sufficient for ability; they fail to properly specify the knowledge prerequisites and effects of actions. <p> As mentioned earlier, our formalization of ability has many limitations. It should be extended to handle more complex types of actions, such as those involving indefinite iteration, nondeterminism, and concurrency, as well as plans involving multiple agents. Morgenstern <ref> [42, 43] </ref> as well as Nunes and Levesque [46] handle some of these cases, but their accounts do not deal with indexicality.
Reference: [43] <author> Leora Morgenstern. </author> <title> Foundations of a Logic of Knowledge, Action, and Communication. </title> <type> PhD thesis, </type> <institution> Department of Computer Science, </institution> <address> New York University, New York, NY, </address> <year> 1988. </year>
Reference-contexts: Many theories of action and most existing planners have completely ignored the need to deal with what agents know and need to know by unrealistically assuming that agents always have perfect knowledge of the domain under consideration. But in the past decade, Moore [41, 40], Morgenstern <ref> [42, 43] </ref>, and other researchers have proposed theories of knowledge and action that do address this need. <p> Finally, the logic can be used to reason formally about knowledge, action, and ability | the kind of reasoning a designer might do in ensuring that the agent being designed is able to achieve a goal. Our methodology differs little from that used by Moore [41, 40], Morgenstern <ref> [42, 43] </ref>, and others who have previously worked on theories of knowledge and action. Note that our theory is at the "knowledge level" [45], and as such, does not say much about agent architecture. <p> A theory that significantly extends this coverage has been developed by Morgenstern <ref> [42, 43] </ref>. It handles both concurrent actions and plans involving multiple agents. Simpler cases are treated as in Moore's. <p> And perception yields just the kind of indexical knowledge that is needed. Previous formal accounts of the ability of agents to achieve goals by doing actions, such 33 as that of Moore [40, 41] and Morgenstern <ref> [42, 43] </ref>, have ignored this, and thus end up imposing knowledge requirements that are neither necessary nor sufficient for ability; they fail to properly specify the knowledge prerequisites and effects of actions. <p> As mentioned earlier, our formalization of ability has many limitations. It should be extended to handle more complex types of actions, such as those involving indefinite iteration, nondeterminism, and concurrency, as well as plans involving multiple agents. Morgenstern <ref> [42, 43] </ref> as well as Nunes and Levesque [46] handle some of these cases, but their accounts do not deal with indexicality.
Reference: [44] <author> Leora Morgenstern. </author> <title> Knowledge and the frame problem. </title> <editor> In K. Ford and P. Hayes, editors, </editor> <title> Reasoning Agents in a Dynamic World: The Frame Problem. </title> <publisher> JAI Press, </publisher> <address> Green-wich, </address> <year> 1991. </year>
Reference-contexts: However, multiple agents settings give rise to new difficulties for reasoning about change. One issue is whether agents know about all event occurrences that could affect their situation and if not, what assumptions they make about such events. Morgenstern <ref> [44] </ref> has proposed an approach to deal with this. Another issue connected to indexicality is the question of how agents refer to each other.
Reference: [45] <author> A. Newell. </author> <title> The knowledge level. </title> <journal> Artificial Intelligence, </journal> <volume> 18(1) </volume> <pages> 87-127, </pages> <year> 1982. </year>
Reference-contexts: Our methodology differs little from that used by Moore [41, 40], Morgenstern [42, 43], and others who have previously worked on theories of knowledge and action. Note that our theory is at the "knowledge level" <ref> [45] </ref>, and as such, does not say much about agent architecture.
Reference: [46] <author> Jose H. T. Nunes and Hector J. Levesque. </author> <title> Ability and commitment. </title> <type> Unpublished manuscript, </type> <institution> Department of Computer Science, University of Toronto, Toronto, ON, </institution> <year> 1990. </year>
Reference-contexts: For example, if Paul does not know how to fix a leaky faucet, but knows that his friend John is somehow able to do it, then he is able to fix the faucet by asking John to do it for him. However, recent work by Nunes and Levesque <ref> [46] </ref> undermines this argument; they shows that the notion of "somehow being able to achieve a goal" can in fact be modeled in a possible-world framework. Neither Konolige nor Morgenstern recognize the role of indexical knowledge in action; their formalisms have the same limitations as Moore's in this respect. <p> A more general and robust approach has been proposed by Nunes and Levesque <ref> [46] </ref>. 4.6 Map Navigation A general account of ability must be based on a formalism that handles both indexical knowledge and objective knowledge, as well as knowledge that relates the agent's perspective to the objective frame of reference, what we might call orienting knowledge. <p> As mentioned earlier, our formalization of ability has many limitations. It should be extended to handle more complex types of actions, such as those involving indefinite iteration, nondeterminism, and concurrency, as well as plans involving multiple agents. Morgenstern [42, 43] as well as Nunes and Levesque <ref> [46] </ref> handle some of these cases, but their accounts do not deal with indexicality. Situations involving interacting agents are especially interesting from the indexicality point of view, since the difference in perspective between agents must be accounted for if they are to have indexical knowledge of the same facts.
Reference: [47] <author> John Perry. </author> <title> The problem of the essential indexical. </title> <journal> No^us, </journal> <volume> 13 </volume> <pages> 3-21, </pages> <year> 1979. </year>
Reference-contexts: Our account of indexical knowledge is inspired from Lewis's view that having a belief involves ascribing a property to oneself and the current time [37]. In <ref> [47] </ref>, Perry argues convincingly that indexicality is an essential feature of propositional attitudes. Let us say a bit more about some recent work by Grove [14] and Grove and Halpern [15], where they propose a logic of knowledge that does handle indexicality.
Reference: [48] <author> Raymond Reiter. </author> <title> The frame problem in the situation calculus: A simple solution (sometimes) and a completeness result for goal regression. </title> <editor> In Vladimir Lifschitz, editor, </editor> <booktitle> Artificial Intelligence and Mathematical Theory of Computation: Papers in Honor of John McCarthy, </booktitle> <pages> pages 359-380. </pages> <publisher> Academic Press, </publisher> <address> San Diego, CA, </address> <year> 1991. </year>
Reference-contexts: Recently, Reiter <ref> [48] </ref> has proposed a solution to the frame problem within the situation calculus and Scherl and Levesque [50] have extended 35 this solution to deal with knowledge and knowledge-producing actions.
Reference: [49] <author> Stanley J. Rosenschein and Leslie P. Kaelbling. </author> <title> The synthesis of digital machines with provable epistemic properties. </title> <editor> In Joseph Y. Halpern, editor, </editor> <booktitle> Theoretical Aspects of Reasoning about Knowledge: Proceedings of the 1986 Conference, </booktitle> <pages> pages 83-98, </pages> <address> Monterey, CA, 1986. </address> <publisher> Morgan Kaufmann Publishing. </publisher>
Reference-contexts: While it uses logic, it does not take any representational stance (i.e., assume that knowledge is represented by sentence-like entities inside agents); this is similar to the position taken by Rosenschein and Kaelbling in their situated automata work <ref> [49, 24] </ref>. But there is an emerging consensus that satisfactory architectural designs for agents will need to be based on an adequate theory of agent-environment interaction. Our work attempts to provide this kind of foundation. <p> Finally, the theory does not account for the distinctive logical behavior of indexical terms, for example, the fact that the past is determined while the future is not. Another conception of how reactive behavior can be produced has been proposed by Rosenschein and Kaelbling <ref> [49, 24] </ref>. Their approach is of particular interest to us because a logic is used to specify reactive agents and the environments in which they operate. <p> This may come as a surprise because their formalization of examples such as the one involving a robot that keeps track of whether a moving object is within shouting distance in <ref> [49] </ref> uses many indexical-sounding propositions. But their logic does not really model all of that indexicality. When indexical-sounding domain-dependent symbols are used, it's easy to lose track of what the formalism really handles. <p> And as Rosenschein and Kaelbling <ref> [49] </ref> have shown, it is legitimate to ascribe knowledge to agents even when they have no explicit representation of this knowledge. In such cases, one needs a formalism that distinguishes between indexical and objective knowledge just to accurately model the agent's thinking. <p> Last but not least, the theory would appear to hold much promise for applications in the design of reactive agents. Rosenschein and Kaelbling <ref> [49, 24] </ref> use a logical formalism as a design notation and as a specification language in their robot design framework. Since indexical knowledge appears to be centrally involved in the production of reactive behavior [2, 53], it seems that elements of our logic could prove useful in these roles.
Reference: [50] <author> Richard B. Scherl and Hector J. Levesque. </author> <title> The frame problem and knowledge-producing actions. </title> <booktitle> In Proceedings of the Eleventh National Conference on Artificial Intelligence, </booktitle> <pages> pages 689-695, </pages> <address> Washington, DC, July 1993. </address> <publisher> AAAI Press/The MIT Press. </publisher>
Reference-contexts: Recently, Reiter [48] has proposed a solution to the frame problem within the situation calculus and Scherl and Levesque <ref> [50] </ref> have extended 35 this solution to deal with knowledge and knowledge-producing actions. The approach allows a form of regression to be used to reduce reasoning about what facts hold in a situation to reasoning about what facts held in an initial situation.
Reference: [51] <author> Richard B. Scherl, Hector J. Levesque, and Yves Lesperance. </author> <title> Indexicals in the situation calculus. </title> <note> In preparation, </note> <year> 1994. </year>
Reference-contexts: We are currently reformulating our framework into an extended version of the situation calculus so as to incorporate this approach to the frame problem <ref> [51] </ref>. As mentioned earlier, our formalization of ability has many limitations. It should be extended to handle more complex types of actions, such as those involving indefinite iteration, nondeterminism, and concurrency, as well as plans involving multiple agents. <p> In many cases, agents know of each other only under some indexical description (e.g., "the person in front of me now"); it would be inappropriate to assume that agents always know who all the agents involved are. The formalization proposed in <ref> [51] </ref> deals with this.
Reference: [52] <author> Brian Cantwell Smith. </author> <title> The owl and the electric encyclopedia. </title> <journal> Artificial Intelligence, </journal> <volume> 47 </volume> <pages> 251-288, </pages> <year> 1991. </year>
Reference-contexts: For example, by using his sonar, a robot comes to know how far from him an object is (at the current time); he does not learn anything about the object's absolute position. Indexicality has of course been a central theme of the "situated action" paradigm <ref> [52, 5, 54] </ref>. Several researchers have argued that indexical knowledge plays a major role in the operation of reactive agents [2, 53]. Existing theories of the relationship between knowledge and action cannot handle these cases properly, as they do not accommodate the distinction between indexical knowledge and objective knowledge.
Reference: [53] <author> Devika Subramanian and John Woodfill. </author> <title> Making the situation calculus indexical. </title> <booktitle> In Proceedings of the First International Conference on Principles of Knowledge Representation and Reasoning, </booktitle> <pages> pages 467-474, </pages> <address> Toronto, ON, May 1989. </address> <publisher> Morgan Kaufmann Publishing. </publisher> <pages> 48 </pages>
Reference-contexts: Indexicality has of course been a central theme of the "situated action" paradigm [52, 5, 54]. Several researchers have argued that indexical knowledge plays a major role in the operation of reactive agents <ref> [2, 53] </ref>. Existing theories of the relationship between knowledge and action cannot handle these cases properly, as they do not accommodate the distinction between indexical knowledge and objective knowledge. <p> Clearly, Agre and Chapman find the notion of indexical information useful in designing their robots and explaining how they behave. However, they do not propose any formal version of this modeling scheme. Subramanian and Woodfill <ref> [53] </ref> have recently proposed an interesting account of reactive agent architectures and the role indexical representations play in them; their work includes a computational complexity analysis that attempts to trace the source of the efficiency 5 This is Agre and Chapman's terminology. <p> turkey in the oven, setting the timer to one hour, then listening to the radio until the time is announced while keeping track of the roasting time with the timer, and finally calculating the time the turkey started roasting, and leaving a message to that effect. 19 Subramanian and Woodfill <ref> [53] </ref> prove that such a transformation is truth-preserving within their indexical situation calculus framework. 20 E.g., if at 10 a.m. <p> Rosenschein and Kaelbling [49, 24] use a logical formalism as a design notation and as a specification language in their robot design framework. Since indexical knowledge appears to be centrally involved in the production of reactive behavior <ref> [2, 53] </ref>, it seems that elements of our logic could prove useful in these roles. It might even be useful as a representation language for a planner or sophisticated executor.
Reference: [54] <author> Lucy A. Suchman. </author> <title> Plans and Situated Action: The Problem of Human-Machine Com--munication. </title> <publisher> Cambridge University Press, </publisher> <address> Cambridge, UK, </address> <year> 1987. </year>
Reference-contexts: For example, by using his sonar, a robot comes to know how far from him an object is (at the current time); he does not learn anything about the object's absolute position. Indexicality has of course been a central theme of the "situated action" paradigm <ref> [52, 5, 54] </ref>. Several researchers have argued that indexical knowledge plays a major role in the operation of reactive agents [2, 53]. Existing theories of the relationship between knowledge and action cannot handle these cases properly, as they do not accommodate the distinction between indexical knowledge and objective knowledge. <p> Agre and Chapman [2, 3] have been among the most radical in their reevaluation of the classical paradigm. Their views have been influenced by anthropological theories of action <ref> [1, 54] </ref>. They emphasize the complexity of the real situations in which action occurs, the uncertainty of the information the agent may have about them, and the need for reactivity.
Reference: [55] <author> Bonnie Webber, Norman Badler, F. Breckenridge Baldwin, Welton Becket, Barbara Di Eugenio, Christopher Geib, Moon Jung, Libby Levison, Michael Moore, and Michael White. </author> <title> Doing what you're told: Following task instructions in changing, but hospitable environments. </title> <type> Technical Report MS-CIS-92-74, </type> <institution> LINC LAB 236, Computer and Information Science Department, University of Pennsylvania, </institution> <address> Philadelphia, PA, </address> <month> September </month> <year> 1992. </year> <note> Submitted to Artificial Intelligence, special issue on Computational Theories of Interaction and Agency. 49 </note>
Reference-contexts: Some recent work <ref> [55] </ref> has focussed on providing agents that interact with the world with the ability to understand natural language instructions; this would be an ideal setting for exploring concrete applications of such an account.
References-found: 55

