URL: http://playfair.stanford.edu/~trevor/Papers/tangent.ps
Refering-URL: http://playfair.stanford.edu/~trevor/Papers/
Root-URL: 
Title: Metrics and Models for Handwritten Character Recognition  
Author: Trevor Hastie Patrice Y. Simard 
Keyword: Nearest Neighbor, Classification, Invariance  
Date: December 8, 1997  
Abstract: While many different techniques have been pushed very hard to solve this task, the most successful and intuitively appropriate is due to Simard (Simard, LeCun & Denker 1993). Their approach combined nearest-neighbor classification with a subject-specific invariant metric that allows for small rotations, translations, and other natural transformations. We report on Simard's classifier, and compare it to other approaches. One important negative aspect of near-neighbor classification is that all the work gets done at lookup time, and with around 10,000 training images in high dimensions this can be exorbitant. In this paper we develop rich models for representing large subsets of the prototypes. One example is a low-dimensional hyperplane defined by a point and a set of basis or tangent vectors. The components of these models are learned from the training set, chosen to minimize the average tangent distance from a subset of the training images | as such they are similar in flavor to the Singular Value Decomposition (SVD), which finds closest hyperplanes in Euclidean distance. These models are either used singly per class, or as basic building blocks in conjunction with the K-means clustering algorithm.
Abstract-found: 1
Intro-found: 1
Reference: <author> Bishop, C. </author> <year> (1995), </year> <title> Neural Networks for Pattern Recognition, </title> <publisher> Clarendon Press, Oxford. </publisher>
Reference: <author> Boser, B., Guyon, I. & Vapnik, I. </author> <year> (1992), </year> <title> A training algorithm for optimal margin classifiers, </title> <booktitle> in `Proceedings of COLT II', </booktitle> <address> Philadelphia, Pa. </address>
Reference: <author> Friedman, J. & Stuetzle, W. </author> <year> (1981), </year> <title> `Projection pursuit regression', </title> <journal> Journal of the American Statistical Association 76, </journal> <pages> 817-823. </pages>
Reference: <author> Gold, S., Mjolsness, E. & Rangarajan, A. </author> <year> (1994), </year> <title> Clustering with a domain specific distance measure, </title> <booktitle> in `Advances in Neural Information Processing Systems', </booktitle> <publisher> Morgan Kaufman, </publisher> <address> San Mateo, CA. </address>
Reference: <author> Hastie, T., Buja, A. & Tibshirani, R. </author> <year> (1995), </year> <title> `Penalized discriminant analysis', </title> <journal> Annals of Statistics 23(1), </journal> <pages> 73-102. </pages>
Reference: <author> Hastie, T. & Tibshirani, R. </author> <year> (1993), </year> <title> Handwritten digit recognition via deformable prototypes. </title> <type> unpublished manuscript. </type>
Reference: <author> Hastie, T. & Tibshirani, R. </author> <year> (1996), </year> <title> `Discriminant analysis by gaussian mixtures', </title> <journal> J. Royal Statist. Soc. </journal> <volume> (Series B) 58, </volume> <pages> 155-176. </pages>
Reference: <author> Hastie, T., Tibshirani, R. & Buja, A. </author> <year> (1994), </year> <title> `Flexible discriminant analysis by optimal scoring', </title> <journal> Journal of the American Statistical Association 89, </journal> <pages> 1255-1270. </pages> <note> 16 Kohonen, </note> <author> T. </author> <year> (1989), </year> <title> Self-Organization and Associative Memory (3rd edition), </title> <publisher> Springer--Verlag, </publisher> <address> Berlin. </address>
Reference-contexts: The performance was worse than Euclidean distance. We also tried 2 fi 2 variants in which * We replaced S i by a regularized version S i + to enforce stability and spatial smooth ness of the metric <ref> (Hastie, Tibshirani & Buja 1994) </ref>. * We corrected each distance for the size of the covariance in a way consistent with Gaussian likelihood-ratio tests, by adding the term log jS i j to the distance. 15 Neither of these gave any improvements either.
Reference: <author> Le Cun, Y., Boser, B., Denker, J. S., Henderson, D., Howard, R., Hubbard, W. & Jackel, L. </author> <year> (1990), </year> <title> Handwritten digit recognition with a back-propogation network, </title> <editor> in D. Touret-zky, ed., </editor> <booktitle> `Advances in Neural Information Processing Systems', </booktitle> <volume> Vol. 2, </volume> <publisher> Morgan Kauf-man, </publisher> <address> Denver, </address> <publisher> CO. </publisher>
Reference-contexts: The algorithms for fitting the networks require tuning as well, and are 3 sensitive to learning rates, regularization parameters, and initialization. Many different neural network configurations have been tailored for this particular application along the lines outlined above <ref> (Le Cun, Boser, Denker, Henderson, Howard, Hubbard & Jackel 1990) </ref>. The error rates are typically around 5%. A word of caution is needed when tackling a popular problem of this kind.
Reference: <author> Oja, E. </author> <year> (1989), </year> <title> `Neural networks, principal components, and subspaces', </title> <journal> International Journal Of Neural Systems 1(1), </journal> <pages> 61-68. </pages>
Reference: <author> Ripley, B. D. </author> <year> (1996), </year> <title> Pattern recognition and neural networks. </title> <publisher> Cambridge University Press. </publisher>
Reference-contexts: This model can be seen as an extension of the polychotomous logistic regression model, and is similar in structure and flavor to the projection pursuit regression models of Friedman & Stuetzle (1981). There is a large literature on such models <ref> (Ripley 1996, Bishop 1995) </ref>, with many possibilities for fitting, regularizing and sizing the networks. While the addition of possibly many nonlinear basis functions will reduce the bias of linear methods in this problem, the number of effective parameters grows rapidly along with the variance.
Reference: <author> Sackinger, E. </author> <year> (1992), </year> <title> Recurrent networks for elastic matching in pattern recognition, </title> <type> Technical report, </type> <institution> AT&T Bell Laboratories. </institution>
Reference-contexts: Figure 4 illustrates the approximation. The approximation is valid locally, and thus permits local transformations. Non-local transformations are not interesting anyway (we don't want to flip 6s into 9s, or shrink all digits down to nothing <ref> (Sackinger 1992) </ref>). Simard et al. (1993) report that they found it unnecessary to restrict the transformations to be local, since the degradation of the linear approximation far from the origin produced images that were extremely distorted. The computations involved in the approximation are relatively straightforward.
Reference: <author> Simard, P. Y., LeCun, Y. & Denker, J. </author> <year> (1993), </year> <title> Efficient pattern recognition using a new transformation distance, </title> <booktitle> in `Advances in Neural Information Processing Systems', </booktitle> <publisher> Mor-gan Kaufman, </publisher> <address> San Mateo, CA, </address> <pages> pp. 50-58. </pages>
Reference: <author> Vapnik, V. </author> <year> (1996), </year> <title> The Nature of Statistical Learning, </title> <publisher> Springer-Verlag. </publisher>
Reference-contexts: Their models could also accommodate the regularization used in penalized discriminant analysis. The error rates were only a slight improvement over penalized discriminant analysis. Boser, Guyon & Vapnik (1992) fit optimal margin hyperplanes, also known as support vector machines between each class and the rest <ref> (Vapnik 1996) </ref>. These techniques are related to the preceding, in that they fit linear decision boundaries (typically in a space augmented by basis expansions of the original pixels.) The idea is to find hyperplanes that either separate or approximately separate the data well.
References-found: 14

