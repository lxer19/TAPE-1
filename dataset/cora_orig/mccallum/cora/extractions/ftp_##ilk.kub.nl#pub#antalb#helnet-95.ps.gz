URL: ftp://ilk.kub.nl/pub/antalb/helnet-95.ps.gz
Refering-URL: http://ilk.kub.nl/~antalb/pubs-time.html
Root-URL: 
Email: email: fantal,weijtersg@cs.rulimburg.nl  
Title: Stretching the Limits of Learning Without Modules  
Author: Antal van den Bosch and Ton Weijters 
Address: PO Box 616, NL-6200 MD Maastricht, The Netherlands  
Affiliation: MATRIKS Department of Computer Science, University of Maastricht  
Abstract: Decomposing a hard problem into easier sub-problems (`modularisation') is a powerful problem-solving technique. Modularisation is often based on expert knowledge and can lead to efficient high-performance models. Contrasting with this expert-based approach is the approach of machine-learning algorithms such as back-propagation and symbolic inductive-learning algorithms that do not make us of a predetermined modular architecture. We present examples of machine-learned models without modules of problems that are traditionally solved by expert-based modularisation. The machine-learned models perform equally good as or better than the expert-based models. This surprising fact gives rise to the question whether the performance of machine-learned models could be further increased when modularisation is somehow incorporated in the learning algorithms. We describe work in progress on the development of machine learning algorithms that automatically construct modular architectures during learning.
Abstract-found: 1
Intro-found: 1
Reference: <author> Daelemans, W. </author> <year> (1988). </year> <title> GRAFON: A Grapheme-to-phoneme Conversion System for Dutch. </title> <booktitle> in Proceedings Twelfth International Conference on Computational Linguistics (COLING-88), Budapest, </booktitle> <pages> 133-138. </pages>
Reference: <author> Daelemans, W. </author> <year> (1989). </year> <title> Automatic hyphenation: Linguistics versus engineering. </title> <editor> In F.J. Heyvaert and F. Steurs (Eds.), </editor> <title> Worlds behind words. Leuven: </title> <publisher> Leuven University Press, </publisher> <pages> 347-364. </pages>
Reference-contexts: Tested on a Dutch text, they obtained a generalisation performance of 96.9% correctly placed hyphens. Using a simple table-lookup technique (Weijters, 1991), even better performance was obtained (98.0% correctly placed hyphens). A modular, rule-based, linguistic model, CHYP <ref> (Daelemans, 1989) </ref>, was also tested on the same material. CHYP placed only 95.3% of all hyphens correctly (Daelemans and Van den Bosch, 1992). These results indicate that the complex problems of grapheme-phoneme conversion and hyphenation can be learned with high accuracy without decomposing the problems in sub-problems.
Reference: <author> Daelemans, W., and Van den Bosch, A. </author> <year> (1992). </year> <title> A neural network for hyphenation. </title> <editor> In I. Aleksander and J. Taylor (Eds.), </editor> <booktitle> Artificial Neural Networks 2, </booktitle> <volume> volume 2: </volume> <pages> 1647-1650. </pages> <address> Amsterdam: </address> <publisher> North-Holland. </publisher>
Reference-contexts: Using a simple table-lookup technique (Weijters, 1991), even better performance was obtained (98.0% correctly placed hyphens). A modular, rule-based, linguistic model, CHYP (Daelemans, 1989), was also tested on the same material. CHYP placed only 95.3% of all hyphens correctly <ref> (Daelemans and Van den Bosch, 1992) </ref>. These results indicate that the complex problems of grapheme-phoneme conversion and hyphenation can be learned with high accuracy without decomposing the problems in sub-problems. Apparently, BP and IG-tree are able to find efficient solution paths through complex problem search spaces.
Reference: <author> Daelemans, W., Van den Bosch, A., and Weijters, T. </author> <year> (1995). </year> <note> IG-tree: A variant of IBL. submitted. Available from request to antal@cs.rulimburg.nl Fahlman, </note> <author> S. E. and Lebiere, C. </author> <year> (1990). </year> <title> The Cascade-correlation Learning Architecture. </title> <type> Technical Report CMU-CS-90-100, </type> <institution> School of Computer Science, Carnegie-Mellon University, </institution> <address> Pittsburgh, PA. </address>
Reference-contexts: Nevertheless, we have trained several machine-learning algorithms on full problems. Here, we report on experiments performed on Dutch grapheme-phoneme conversion (the problem of converting text to phonemic speech), and Dutch hyphenation. The algorithms applied to these problems are (i) back-propagation (BP, Rumelhart, Hinton, and Williams, 1986), and (ii) IG-tree <ref> (Daelemans, Van den Bosch, and Weijters, 1995) </ref>, a symbolic inductive decision-tree learning algorithm. it is important to note here that whereas IG-tree can easily be characterised as non-modular, this is not so clear for BP.
Reference: <author> Heemskerk, J. </author> <year> (1993). </year> <title> A probabilistic context-free grammar for disambiguation in morphological parsing. </title> <note> ITK Research Report No. 44. Tilburg: ITK. </note>
Reference-contexts: MORPA-CUM-MORPHON is a modular system of which the modular architecture is comparable to that proposed by Daelemans (1988) as displayed in Figure 1. When the morphological parsing module of MORPA-CUM-MORPHON was replaced by a data-oriented morphological parsing module <ref> (Heemskerk, 1993) </ref>, performance increased to 88.7%, remaining slightly worse than that of IG-tree. Daelemans and Van den Bosch (1992) applied BP to Dutch hyphenation using a data set of approximately 20,000 hyphenated words. Tested on a Dutch text, they obtained a generalisation performance of 96.9% correctly placed hyphens.
Reference: <author> Kohonen, T. </author> <year> (1984). </year> <title> Self-organisation and Associative Memory. </title> <publisher> Berlin: Springer Verlag. </publisher>
Reference: <author> Norris, D. </author> <year> (1990). </year> <title> How to build a connectionist idiot (savant). </title> <journal> Cognition, </journal> <volume> 35: </volume> <pages> 277-291. </pages>
Reference: <author> Nunn, A. & van Heuven, V. J. </author> <year> (1993). </year> <title> Morphon, lexicon-based text-to-phoneme conversion and phonological rules. </title> <editor> In V. J. van Heuven & L. C. Pols (Eds.), </editor> <title> Analysis and Synthesis of Speech: Strategic Research Towards High-Quality Text-to-Speech Generation. </title> <publisher> Berlin: Mouton de Gruyter. </publisher>
Reference-contexts: IG-tree was then trained on 70,000 Dutch word-pronunciation pairs, and tested on a word list containing neologisms, complex compounds, and low-frequency words. IG-tree converted 89.5% of these words correctly. A Dutch state-of-the-art rule-based text-to-speech conversion system, MORPA-CUM-MORPHON <ref> (Nunn and Van Heuven, 1993) </ref>, was also tested on the same data, and converted only 85.3% of the words correctly (Van den Bosch and Daelemans, 1993). MORPA-CUM-MORPHON is a modular system of which the modular architecture is comparable to that proposed by Daelemans (1988) as displayed in Figure 1.
Reference: <author> Refenes, A. & Vithlani, S. </author> <year> (1991). </year> <title> Constructive learning by specialisation. </title> <editor> In T. Kohonen, K. M akisara, </editor> <address> O. </address>
Reference-contexts: Another related `constructive' learning algorithm is constructive learning by specialisation <ref> (Refenes and Vithlani, 1991) </ref>. Figure 1 displays the simplest form of SBP, in which each hidden group contains only one unit. newly added group. Arrows depict the direction of activation feed-forward.
Reference: <editor> Simula, & J. Kangas (Eds.), </editor> <booktitle> Proceedings of ICANN-91, </booktitle> <address> Espoo, Finland, 923-929. Amsterdam: </address> <publisher> North Holland. </publisher>
Reference: <author> Rumelhart, D.E., Hinton, G.E., and Williams, R.J. </author> <year> (1986). </year> <title> Learning internal representations by error propagation. </title> <editor> In D. E. Rumelhart and J. L. McClelland (Eds.), </editor> <booktitle> Parallel Distributed Processing, volume 1: Foundations, </booktitle> <pages> 318-362. </pages> <address> Cambridge, MA: </address> <publisher> The MIT Press. </publisher>
Reference-contexts: Nevertheless, we have trained several machine-learning algorithms on full problems. Here, we report on experiments performed on Dutch grapheme-phoneme conversion (the problem of converting text to phonemic speech), and Dutch hyphenation. The algorithms applied to these problems are (i) back-propagation <ref> (BP, Rumelhart, Hinton, and Williams, 1986) </ref>, and (ii) IG-tree (Daelemans, Van den Bosch, and Weijters, 1995), a symbolic inductive decision-tree learning algorithm. it is important to note here that whereas IG-tree can easily be characterised as non-modular, this is not so clear for BP. <p> We first describe some work in progress on `step-wise' constructive back-propagation networks; then, we describe how we envisage the development of an `automatic modularisation' algorithm based on these constructive BP algorithms. 3.1 Step-wise back-propagation: Towards automatic modularisation One of the limitations of standard BP <ref> (Rumelhart et al., 1986) </ref> is that the size of the hidden layer has to be determined beforehand. The success of the network strongly depends on a well-chosen hidden layer size.
Reference: <author> Sejnowski, T.J., and Rosenberg, C.S. </author> <year> (1987). </year> <title> Parallel networks that learn to pronounce English text. </title> <journal> Complex Systems, </journal> <volume> 1: </volume> <pages> 145-168. </pages>
Reference-contexts: Our data was obtained from CELEX, a collection of lexical data bases of Dutch, English, and German, maintained at Nijmegen University, The Netherlands. A within-comparison of BP and IG-tree showed that when trained on the NETtalk data <ref> (Sejnowski and Rosenberg, 1987) </ref>, IG-tree performed best on unseen data (95.1% correctly classified test windows); BP classified 91.3% of the test windows correctly. IG-tree was then trained on 70,000 Dutch word-pronunciation pairs, and tested on a word list containing neologisms, complex compounds, and low-frequency words.
Reference: <author> Van den Bosch, A. & Daelemans, W. </author> <year> (1993). </year> <title> Data-oriented methods for grapheme-to-phoneme conversion. </title> <booktitle> In Proceedings of the 6th Conference of the EACL: </booktitle> <pages> 45-53. </pages>
Reference-contexts: IG-tree was then trained on 70,000 Dutch word-pronunciation pairs, and tested on a word list containing neologisms, complex compounds, and low-frequency words. IG-tree converted 89.5% of these words correctly. A Dutch state-of-the-art rule-based text-to-speech conversion system, MORPA-CUM-MORPHON <ref> (Nunn and Van Heuven, 1993) </ref>, was also tested on the same data, and converted only 85.3% of the words correctly (Van den Bosch and Daelemans, 1993). MORPA-CUM-MORPHON is a modular system of which the modular architecture is comparable to that proposed by Daelemans (1988) as displayed in Figure 1. <p> IG-tree converted 89.5% of these words correctly. A Dutch state-of-the-art rule-based text-to-speech conversion system, MORPA-CUM-MORPHON (Nunn and Van Heuven, 1993), was also tested on the same data, and converted only 85.3% of the words correctly <ref> (Van den Bosch and Daelemans, 1993) </ref>. MORPA-CUM-MORPHON is a modular system of which the modular architecture is comparable to that proposed by Daelemans (1988) as displayed in Figure 1.
Reference: <author> Van den Bosch, A., Weijters, A., and Van den Herik, J. </author> <year> (1995). </year> <title> Scaling effects with greedy and lazy machine-learning algorithms. </title> <booktitle> To appear in Proceedings of the Dutch AI Conference, </booktitle> <address> NAIC-95. </address>
Reference-contexts: Nevertheless, we have trained several machine-learning algorithms on full problems. Here, we report on experiments performed on Dutch grapheme-phoneme conversion (the problem of converting text to phonemic speech), and Dutch hyphenation. The algorithms applied to these problems are (i) back-propagation (BP, Rumelhart, Hinton, and Williams, 1986), and (ii) IG-tree <ref> (Daelemans, Van den Bosch, and Weijters, 1995) </ref>, a symbolic inductive decision-tree learning algorithm. it is important to note here that whereas IG-tree can easily be characterised as non-modular, this is not so clear for BP. <p> Having a `greedy' learning style, BP is aimed at storing as much input-output associations as possible in an often limited structure. This often leads to good generalisation, but limited storage capacities. The ability to store exceptions, however, is a large profit when learning complex problems <ref> (Van den Bosch, Weijters, and Van den Herik, 1995) </ref>. When an architecture is used in which units are added when necessary, i.e., when exceptions have to be stored, there would be no need for determining hidden layer size beforehand. <p> This is not only efficient; it provides a means to step-wise decompose a problem, albeit in a bottom-up manner. It should be noted that this is in principle the case; whether BP-learning is able to make use of the possibilities remains to be investigated. The experiments performed thus far <ref> (Van den Bosch et al., 1995) </ref> with a cascaded version of SBP show a roughly equal performance to that of BP; we are currently running a series of experiments in which we systematically test all variable features of SBP.
Reference: <author> Weijters, A. </author> <year> (1991). </year> <title> A Simple Look-up Procedure Superior to NETtalk? In Proceedings of the International Conference on Artificial Neural Networks - ICANN-91, </title> <address> Espoo, Finland. </address>
Reference-contexts: Daelemans and Van den Bosch (1992) applied BP to Dutch hyphenation using a data set of approximately 20,000 hyphenated words. Tested on a Dutch text, they obtained a generalisation performance of 96.9% correctly placed hyphens. Using a simple table-lookup technique <ref> (Weijters, 1991) </ref>, even better performance was obtained (98.0% correctly placed hyphens). A modular, rule-based, linguistic model, CHYP (Daelemans, 1989), was also tested on the same material. CHYP placed only 95.3% of all hyphens correctly (Daelemans and Van den Bosch, 1992).
References-found: 15

