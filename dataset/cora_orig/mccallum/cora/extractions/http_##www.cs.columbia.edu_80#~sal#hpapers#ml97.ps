URL: http://www.cs.columbia.edu:80/~sal/hpapers/ml97.ps
Refering-URL: http://www.cs.columbia.edu:80/~sal/recent-papers.html
Root-URL: 
Email: pkc@cs.fit.edu  
Phone: Phone  
Title: Title: Metrics for Analyzing the Integration of Multiple Learned Classifiers Author(s) with address(es):  
Author: Philip K. Chan Salvatore J. Stolfo 
Keyword: Analytical metrics, integration of multiple learned models, inductive learning  
Note: Email address of contact author:  number of contact author: 407-768-8000 x7280 Multiple submission statement (if applicable): N/A  
Address: Melbourne, FL 32901  New York, NY 10027  
Affiliation: Computer Science Florida Institute of Technology  Department of Computer Science Columbia University  
Abstract: 200 word maximum): Integrating multiple learned models has been used to increase the accuracy and efficiency of inductive learning systems. To gain a deeper understanding of the behavior of various integration methods, we discuss some basic quantitative metrics for analyzing the methods' behavior. Metrics that measure diversity, coverage, correlated error, and specialty are discussed and utilized in a set of empirical studies. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Abramson, N. </author> <year> (1963). </year> <title> Information Theory and Coding. </title> <address> New York, NY: </address> <publisher> McGraw-Hill. </publisher>
Reference-contexts: In this study some of the base classifiers are quite inaccurate and, as we discussed before, obtaining higher accuracy might not be easy. 5 3.2 Diversity In information theory <ref> (Abramson, 1963) </ref> entropy measures the average amount of information required to represent each event given the probabilities of the different possible events. For digital communication channels, the amount of information is measured in bits. Entropy can also measure how random the different events can occur.
Reference: <author> Ali, K. & Pazzani, M. </author> <year> (1996). </year> <title> Error reduction through learning multiple descriptions. </title> <journal> Machine Learning, </journal> <volume> 24, </volume> <pages> 173-202. </pages>
Reference-contexts: A value close to one indicates the errors made by the base classifiers are not likely to be independent. accuracy improvement when correlated error increases. Our findings here are consistent with those from <ref> (Ali & Pazzani, 1996) </ref>. Hansen and Salamon (1990) proved that for a neural-network ensemble, if the networks in the ensemble produce independent errors and have accuracy of at least 50%, the expected ensemble error rate goes to zero as the number of networks approaches infinity.
Reference: <author> Breiman, L. </author> <year> (1996a). </year> <title> Bagging predictors. </title> <journal> Machine Learning, </journal> <volume> 24, </volume> <pages> 123-140. </pages>
Reference: <author> Breiman, L. </author> <year> (1996b). </year> <title> Bias, variance, and arcing classifiers. </title> <type> (technical report, </type> <institution> Berkeley, CA: Statistics Dept., U. of California. </institution>
Reference: <author> Breiman, L., Friedman, J. H., Olshen, R. A., & Stone, C. J. </author> <year> (1984). </year> <title> Classification and Regression Trees. </title> <address> Belmont, CA: </address> <publisher> Wadsworth. </publisher>
Reference-contexts: The results are based on the different permutations of four learning algorithms (ID3 (Quinlan, 1986), CART <ref> (Breiman et al., 1984) </ref>, naive BAYES (Duda & Hart, 1973), and CN2 (Clark & Niblett, 1989)) and applied to learning tasks (RNA Splice Junctions (Towell et al., 1990), Protein Coding Regions (Craven & Shavlik, 1993), Protein Secondary Structures (Qian & Sejnowski, 1988), and Artificial (Chan & Stolfo, 1996)).
Reference: <author> Brodley, C. & Lane, T. </author> <year> (1996). </year> <title> Creating and exploiting coverage and diversity. Work. </title> <booktitle> Notes AAAI-96 Workshop Integrating Multiple Learned Models (pp. </booktitle> <pages> 8-14). </pages>
Reference-contexts: Although not explicitly stated, Krogh and Vedelsby's (1995) decomposition of squared classification 8 error follows the same spirit of bias-variance decomposition and their ambigu-ity metric measures variance. Our diversity metric attempts to quantitatively approximate the variance characteristics as well. 3.3 Coverage Coverage <ref> (Brodley & Lane, 1996) </ref> measures the fraction of instances for which at least one of the base classifiers produces the correct predictions. That is, an instance is not covered if and only if all the base classifiers generate an incorrect prediction for that instance.
Reference: <author> Chan, P. & Stolfo, S. </author> <year> (1995a). </year> <title> A comparative evaluation of voting and meta-learning on partitioned data. </title> <booktitle> Proc. Twelfth Intl. Conf. Machine Learning (pp. </booktitle> <pages> 90-98). </pages>
Reference-contexts: Four integrating schemes (class-combiner, class-attribute-combiner, arbiter, and weighted voting 2 <ref> (Chan & Stolfo, 1995a) </ref>) were used to integrate base classifiers trained from eight disjoint data subsets that are partitioned from the original whole data sets. Integration is performed in a one-level (Chan & Stolfo, 1995a) (not hierarchical as in (Chan & Stolfo, 1995b)) manner. <p> Four integrating schemes (class-combiner, class-attribute-combiner, arbiter, and weighted voting 2 <ref> (Chan & Stolfo, 1995a) </ref>) were used to integrate base classifiers trained from eight disjoint data subsets that are partitioned from the original whole data sets. Integration is performed in a one-level (Chan & Stolfo, 1995a) (not hierarchical as in (Chan & Stolfo, 1995b)) manner. We did not vary the number of data subsets (hence the subset size in each domain) in this evaluation because the variation generates vastly different base classifiers.
Reference: <author> Chan, P. & Stolfo, S. </author> <year> (1995b). </year> <title> Learning arbiter and combiner trees from partitioned data for scaling machine learning. </title> <booktitle> Proc. Intl. Conf. Knowledge Discovery and Data Mining (pp. </booktitle> <pages> 39-44). </pages>
Reference-contexts: 1 Introduction Integrating multiple learned models has been used to increase the accuracy (e.g., (Wolpert, 1992; Breiman, 1996a; Freund & Schapire, 1996)) and efficiency (e.g., <ref> (Chan & Stolfo, 1995b) </ref>) of inductive learning systems. Various integration methods have been proposed and different analysis metrics have been used to gain a deeper understanding of the behavior of integration methods so that more effective methods can be devised. <p> Four integrating schemes (class-combiner, class-attribute-combiner, arbiter, and weighted voting 2 (Chan & Stolfo, 1995a)) were used to integrate base classifiers trained from eight disjoint data subsets that are partitioned from the original whole data sets. Integration is performed in a one-level (Chan & Stolfo, 1995a) (not hierarchical as in <ref> (Chan & Stolfo, 1995b) </ref>) manner. We did not vary the number of data subsets (hence the subset size in each domain) in this evaluation because the variation generates vastly different base classifiers. Ten-fold cross validation runs were performed for each of the 64 permutations.
Reference: <author> Chan, P. & Stolfo, S. </author> <year> (1996). </year> <title> Sharing learned models among remote database partitions by local meta-learning. </title> <booktitle> Proc. Second Intl. Conf. Knowledge Discovery and Data Mining (pp. </booktitle> <pages> 2-7). </pages>
Reference-contexts: learning algorithms (ID3 (Quinlan, 1986), CART (Breiman et al., 1984), naive BAYES (Duda & Hart, 1973), and CN2 (Clark & Niblett, 1989)) and applied to learning tasks (RNA Splice Junctions (Towell et al., 1990), Protein Coding Regions (Craven & Shavlik, 1993), Protein Secondary Structures (Qian & Sejnowski, 1988), and Artificial <ref> (Chan & Stolfo, 1996) </ref>). Four integrating schemes (class-combiner, class-attribute-combiner, arbiter, and weighted voting 2 (Chan & Stolfo, 1995a)) were used to integrate base classifiers trained from eight disjoint data subsets that are partitioned from the original whole data sets.
Reference: <author> Clark, P. & Niblett, T. </author> <year> (1989). </year> <title> The CN2 induction algorithm. </title> <journal> Machine Learning, </journal> <volume> 3, </volume> <pages> 261-285. </pages> <note> 15 Craven, </note> <author> M. & Shavlik, J. </author> <year> (1993). </year> <title> Learning to represent codons: A challenge problem for constructive induction. </title> <booktitle> Proc. </booktitle> <pages> IJCAI-93 (pp. 1319-1324). </pages>
Reference-contexts: The results are based on the different permutations of four learning algorithms (ID3 (Quinlan, 1986), CART (Breiman et al., 1984), naive BAYES (Duda & Hart, 1973), and CN2 <ref> (Clark & Niblett, 1989) </ref>) and applied to learning tasks (RNA Splice Junctions (Towell et al., 1990), Protein Coding Regions (Craven & Shavlik, 1993), Protein Secondary Structures (Qian & Sejnowski, 1988), and Artificial (Chan & Stolfo, 1996)).
Reference: <author> Duda, R. & Hart, P. </author> <year> (1973). </year> <title> Pattern classification and scene analysis. </title> <address> New York, NY: </address> <publisher> Wiley. </publisher>
Reference-contexts: The results are based on the different permutations of four learning algorithms (ID3 (Quinlan, 1986), CART (Breiman et al., 1984), naive BAYES <ref> (Duda & Hart, 1973) </ref>, and CN2 (Clark & Niblett, 1989)) and applied to learning tasks (RNA Splice Junctions (Towell et al., 1990), Protein Coding Regions (Craven & Shavlik, 1993), Protein Secondary Structures (Qian & Sejnowski, 1988), and Artificial (Chan & Stolfo, 1996)).
Reference: <author> Freund, Y. & Schapire, R. </author> <year> (1996). </year> <title> Experiments with a new boosting algorithm. </title> <booktitle> Proc. Thirteenth Conf. Machine Learning (pp. </booktitle> <pages> 148-156). </pages>
Reference: <author> Grammes, C. </author> <year> (1993). </year> <note> Gnufit v1.2. (Available at ftp://ftp.dartmouth.edu/pub/gnuplot/gnufit12.tar.gz). </note>
Reference-contexts: To facilitate the observation of possible general trends, a line is fitted to the 16 data points using the Marquardt-Levenberg algorithm (Ralston & Rabinowitz, 1978), a nonlinear least squares curve fitting mechanism, available in the GNUFIT <ref> (Grammes, 1993) </ref> package. Before we discuss the different metrics used in this study. We first inspect 3 how the average accuracy of base classifiers (fi) affects the overall accuracy (ff). Their relationship is plotted in Figure 1.
Reference: <author> Hansen, L. & Salamon, P. </author> <year> (1990). </year> <title> Neural network ensembles. </title> <journal> IEEE Trans. Pattern Analysis and Mach. Itell., </journal> <volume> 12, </volume> <pages> 993-1001. </pages>
Reference: <author> Kohavi, R. & Wolpert, D. </author> <year> (1996). </year> <title> Bias plus variacne decomposition for zero-one loss functions. </title> <booktitle> Proc. Thirteenth Intl. Conf. Machine Learning (pp. </booktitle> <pages> 275-283). </pages>
Reference: <author> Kong, E. B. & Dietterich, T. </author> <year> (1995). </year> <title> Error-correcting output coding corrects bias and variance. </title> <booktitle> Proc. Twelfth Intl. Conf. Machine Learning (pp. </booktitle> <pages> 313-321). </pages>
Reference: <author> Krogh, A. & Vedelsby, J. </author> <year> (1995). </year> <title> Neural network ensembles, cross validation, and active learning. </title> <editor> In G. Tesauro, D. Touretzky, & T. Leen (Eds.), </editor> <booktitle> Advances in Neural Info. Proc. Sys. </booktitle> <pages> 7 (pp. 231-238). </pages> <publisher> MIT Press. </publisher>
Reference: <author> Qian, N. & Sejnowski, T. </author> <year> (1988). </year> <title> Predicting the secondary structure of globular proteins using neural network models. </title> <journal> J. Mol. Biol., </journal> <volume> 202, </volume> <pages> 865-884. </pages>
Reference-contexts: on the different permutations of four learning algorithms (ID3 (Quinlan, 1986), CART (Breiman et al., 1984), naive BAYES (Duda & Hart, 1973), and CN2 (Clark & Niblett, 1989)) and applied to learning tasks (RNA Splice Junctions (Towell et al., 1990), Protein Coding Regions (Craven & Shavlik, 1993), Protein Secondary Structures <ref> (Qian & Sejnowski, 1988) </ref>, and Artificial (Chan & Stolfo, 1996)). Four integrating schemes (class-combiner, class-attribute-combiner, arbiter, and weighted voting 2 (Chan & Stolfo, 1995a)) were used to integrate base classifiers trained from eight disjoint data subsets that are partitioned from the original whole data sets.
Reference: <author> Quinlan, J. R. </author> <year> (1986). </year> <title> Induction of decision trees. </title> <journal> Machine Learning, </journal> <volume> 1, </volume> <pages> 81-106. </pages>
Reference-contexts: The results are based on the different permutations of four learning algorithms (ID3 <ref> (Quinlan, 1986) </ref>, CART (Breiman et al., 1984), naive BAYES (Duda & Hart, 1973), and CN2 (Clark & Niblett, 1989)) and applied to learning tasks (RNA Splice Junctions (Towell et al., 1990), Protein Coding Regions (Craven & Shavlik, 1993), Protein Secondary Structures (Qian & Sejnowski, 1988), and Artificial (Chan & Stolfo, 1996)).
Reference: <author> Ralston, A. & Rabinowitz, P. </author> <year> (1978). </year> <title> A first course in numerical analysis. </title> <address> New York, NY: </address> <publisher> McGraw-Hill. </publisher>
Reference-contexts: Many of the figures below have four plots, one for each integrating scheme. Within each plot, results from the 16 permutations of learning algorithms and tasks are plotted. To facilitate the observation of possible general trends, a line is fitted to the 16 data points using the Marquardt-Levenberg algorithm <ref> (Ralston & Rabinowitz, 1978) </ref>, a nonlinear least squares curve fitting mechanism, available in the GNUFIT (Grammes, 1993) package. Before we discuss the different metrics used in this study. We first inspect 3 how the average accuracy of base classifiers (fi) affects the overall accuracy (ff).
Reference: <author> Towell, G., Shavlik, J., & Noordewier, M. </author> <year> (1990). </year> <title> Refinement of approximate domain theories by knowledge-based neural networks. </title> <booktitle> Proc. </booktitle> <pages> AAAI-90 (pp. 861-866). </pages>
Reference-contexts: The results are based on the different permutations of four learning algorithms (ID3 (Quinlan, 1986), CART (Breiman et al., 1984), naive BAYES (Duda & Hart, 1973), and CN2 (Clark & Niblett, 1989)) and applied to learning tasks (RNA Splice Junctions <ref> (Towell et al., 1990) </ref>, Protein Coding Regions (Craven & Shavlik, 1993), Protein Secondary Structures (Qian & Sejnowski, 1988), and Artificial (Chan & Stolfo, 1996)).
Reference: <author> Wolpert, D. </author> <year> (1992). </year> <title> Stacked generalization. </title> <booktitle> Neural Networks, </booktitle> <volume> 5, </volume> <pages> 241-259. 16 </pages>
References-found: 22

