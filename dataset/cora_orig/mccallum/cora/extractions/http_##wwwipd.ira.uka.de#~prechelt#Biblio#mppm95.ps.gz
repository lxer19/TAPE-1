URL: http://wwwipd.ira.uka.de/~prechelt/Biblio/mppm95.ps.gz
Refering-URL: 
Root-URL: 
Title: A Parallel Programming Model for Irregular Dynamic Neural Networks a programming model that allows to
Author: Lutz Prechelt 
Keyword: Key words: Data and process locality, load balancing, compiler, portability.  
Note: The present paper describes  
Address: D-76128 Karlsruhe, Germany  
Affiliation: Fakultat fur Informatik Universitat Karlsruhe  
Email: (prechelt@ira.uka.de)  
Phone: +49/721/608-4068, Fax: +49/721/694092  
Abstract: A compiler for CuPit has been built for the MasPar MP-1/MP-2 using compilation techniques that can also be applied to most other parallel machines. The paper shortly presents the main ideas of the techniques used and results obtained by the various optimizations. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> David Cann. </author> <title> Retire Fortran? A debate rekindled. </title> <journal> Communications of the ACM, </journal> <volume> 35(8) </volume> <pages> 81-89, </pages> <month> August </month> <year> 1992. </year>
Reference-contexts: In many respects, the ease with which information about data access patterns can be extracted from the program in this approach is comparable to that found in functional or single assignment languages such as SISAL <ref> [1] </ref>, because in both cases no arbitrary interactions between global data objects are possible.
Reference: [2] <author> Yann Le Cun, John S. Denker, and Sara A. Sol-la. </author> <title> Optimal brain damage. </title> <booktitle> In [9], </booktitle> <pages> pages 598-605, </pages> <year> 1990. </year>
Reference-contexts: This language allows for the convenient modular specification of constructive neural learning algorithms, no matter whether these add resources during learning or remove resources or both. Algorithms such as the additive CasCor method [3] or the Optimal Brain Damage pruning method <ref> [2] </ref> can be expressed easily. CuPit programs are explicitly parallel for the compiler, yet natural for the programmer, because the domain metaphor of the neural network is used to express the parallelism.
Reference: [3] <author> Scott E. Fahlman and Christian Lebiere. </author> <booktitle> The Cascade-Correlation learning architecture. In [9], </booktitle> <pages> pages 524-532, </pages> <year> 1990. </year>
Reference-contexts: This language allows for the convenient modular specification of constructive neural learning algorithms, no matter whether these add resources during learning or remove resources or both. Algorithms such as the additive CasCor method <ref> [3] </ref> or the Optimal Brain Damage pruning method [2] can be expressed easily. CuPit programs are explicitly parallel for the compiler, yet natural for the programmer, because the domain metaphor of the neural network is used to express the parallelism.
Reference: [4] <author> High Performance Fortran (HPF): </author> <title> Language specification. </title> <type> Technical report, </type> <institution> Center for Research on Parallel Computation, Rice University, </institution> <year> 1992. </year>
Reference-contexts: This approach makes a lot of information readily available to the compiler that would be very difficult or impossible to extract from an equivalent program text in a normal parallel programming language such as HPF <ref> [4] </ref>. The information can then be used to generate efficient code that exhibits almost optimal data/process-locality and balanced load, even for irregular networks.
Reference: [5] <author> Lutz Prechelt. </author> <title> CuPit | a parallel language for neural algorithms: Language reference and tutorial. </title> <type> Technical Report 4/94, </type> <institution> Fakultat fur In-formatik, Universitat Karlsruhe, Germany, </institution> <note> Jan-uary 1994. Anonymous FTP: /pub/papers/tech-reports/1994/1994-4.ps.Z on ftp.ira.uka.de. </note>
Reference-contexts: Yet the optimization capabilities arising from this information are still better in our case, since the types of operations are restricted and thus known in advance | allowing to design optimizations for their implementation into the compiler. 4 CuPit The programming language CuPit <ref> [5] </ref> is a realization of the programming model described above. The most important features of its design will be described below, mostly using example program fragments instead of formal definitions. CuPit is a procedural, object-centered language, i.e., there are object types and associated operations but no inheritance.
Reference: [6] <author> Lutz Prechelt. </author> <title> PROBEN1 | A set of benchmarks and benchmarking rules for neural network training algorithms. </title> <type> Technical Report 21/94, </type> <institution> Fakultat fur Informatik, Universitat Karl-sruhe, Germany, </institution> <month> September </month> <year> 1994. </year> <note> Anonymous FTP: /pub/papers/techreports/1994/1994-21.ps.Z on ftp.ira.uka.de. </note>
Reference-contexts: O 5 : The number of network replicates used is adapted dynamically during run time by a direct search method using halving/doubling steps. 7 Results The compiler and optimizations was evaluated on a variety of neural network learning problems, which are described in <ref> [6] </ref> Table 1 presents the relative changes in run time for each of these problems on a 16384 processor MasPar MP-1, when each one of the optimizations in turn was switched off.
Reference: [7] <author> Lutz Prechelt. </author> <title> Adaptive parameter pruning in neural networks. </title> <type> Technical Report 95-009, </type> <institution> International Computer Science Institute, Berkeley, </institution> <address> CA, </address> <month> March </month> <year> 1995. </year>
Reference-contexts: All experiments used the lprune pruning algorithm <ref> [7] </ref>, because the size and topology of the neural network has a much larger impact on the results than the choice of the actual learning program.
Reference: [8] <author> Lutz Prechelt. </author> <title> The CuPit compiler for the Mas-Par | a literate programming document. </title> <type> Technical Report 1/95, </type> <institution> Fakultat fur Informatik, Univer-sitat Karlsruhe, Germany, </institution> <month> January </month> <year> 1995. </year> <note> Anonymous FTP: /pub/papers/techreports/1995/1995-1.ps.Z on ftp.ira.uka.de. </note>
Reference-contexts: The compiler source code is available as a literate programming document <ref> [8] </ref>. Put very shortly, the following techniques and optimizations are employed in the compiler: O 1 : Data locality is maintained by locating node objects and the associated connection objects on the same processor.
Reference: [9] <editor> David S. Touretzky, editor. </editor> <booktitle> Advances in Neural Information Processing Systems 2, </booktitle> <address> San Mateo, CA, 1990. </address> <publisher> Morgan Kaufman Publishers Inc. </publisher>
References-found: 9

