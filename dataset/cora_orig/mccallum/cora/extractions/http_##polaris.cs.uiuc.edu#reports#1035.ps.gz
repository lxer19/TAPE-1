URL: http://polaris.cs.uiuc.edu/reports/1035.ps.gz
Refering-URL: http://polaris.cs.uiuc.edu/tech_reports.html
Root-URL: http://www.cs.uiuc.edu
Title: Parallelization and Performance of Conjugate Gradient Algorithms on the Cedar hierarchical-memory Multiprocessor  
Author: Ulrike Meier and Rudolf Eigenmann 
Abstract: The conjugate gradient method is a powerful algorithm for solving well-structured sparse linear systems that arise from partial differential equations. The broad application range makes it an interesting object for investigating novel architectures and programming systems. In this paper we analyze the computational structure of three different conjugate gradient schemes for solving elliptic partial differential equations. We describe its parallel implementation on the Cedar hierarchical memory multiprocessor from both angles, explicit manual parallelization and automatic compilation. We report performance measurements taken on Cedar, which allow us a number of conclusions on the Cedar architecture, the programming methodology for hierarchical computer structures, and the contrast of manual vs automatic parallelization. 
Abstract-found: 1
Intro-found: 1
Reference: [CGM85] <author> P. Concus, G. Golub, and G. Meurant. </author> <title> Block preconditioning for the conjugate gradient method. </title> <journal> SIAM J. Sci. Stat. Comput., </journal> <volume> 6 </volume> <pages> 220-252, </pages> <year> 1985. </year>
Reference-contexts: Among the preconditioners considered were many variants of Incomplete Cholesky factorization preconditioners which improve the condition number of the preconditioned system very efficiently <ref> [CGM85] </ref>, are however highly recursive and not suited for parallel computation. Vectorization efforts improved the performance, worsened however the convergence of the method [Meu84].
Reference: [EHJP90] <author> R. Eigenmann, J. Hoeflinger, G. Jaxon, and D. Padua. </author> <title> Cedar Fortran and its Compiler. </title> <booktitle> In CONPAR 90, </booktitle> <year> 1990. </year>
Reference-contexts: Cedar Fortran is basically Fortran77 with a few additional constructs for exploiting Cedar architectural features [GPHL90] . Users program Cedar by either writing directly in Cedar Fortran, or by starting from a sequential Fortran77 code and applying the auto-parallelizing Cedar Restructurer, outputing Cedar Fortran <ref> [EHJP90] </ref>. Important Cedar Fortran constructs, refered to in this paper are: CTSKSTART forks a new task. The fork operation is costly, taking up to 200ms. It is used for initiating long-term parallel activities. Synchronization primitives: A variety of constructs is available to manually perform synchronization. <p> n 50 sec. 1 cluster 4 clusters CGGM CGM RSCG RSCG BDBICG BDBICG 8 50 100 150 200 250 300 350 n 10 20 Mflops CGM CGGM 4 cl 2 cl 5 Automatic compilation for Cedar The work reported in this section is part of the Cedar Fortran Compiler project <ref> [EHJP90] </ref>. The auto-parallelizer component is built upon the Kuck & Associates, Inc. proprietary Fortran restructurer Kap [Kuc88][HMD + 86]. In our first project phase we were mainly concerned with applying traditional compiler techniques and measuring their effectiveness on Cedar. This corresponds roughly to the scheme described in section 5.2.1. <p> Also, in practice, this code looks less readable. For example, it contains additional computation to deal with the case that `n' is not a multiple of 256. A more detailed view of the compiler is given in <ref> [EHJP90] </ref>. 5.2.2 Applying data partitioning and distribution techniques Using the above scheme, Cedar is not yet fully exploited. There are only few variables placed in cluster memory, namely variables used but inside one sdoall iteration, in which case they are declared sdoall-local.
Reference: [Emr85] <author> Perry A. Emrath. </author> <title> Xylem: An Operating System for the Cedar Multiprocessor. </title> <journal> IEEE Software, </journal> <volume> 2(4) </volume> <pages> 30-37, </pages> <month> July </month> <year> 1985. </year>
Reference-contexts: The cluster shared global memory is 32 megabytes. The final Cedar configuration is expected to have 8 processors per cluster and a global memory of 64 megabytes. 2.2 Cedar system software The software that coordinates the clusters is the Xylem Operating System <ref> [Emr85] </ref>, an extension of Unix. Its functionality is made available to the user through Cedar Fortran, the main application programming language. Cedar Fortran is basically Fortran77 with a few additional constructs for exploiting Cedar architectural features [GPHL90] .
Reference: [GJG88] <author> Kyle Gallivan, William Jalby, and Dennis Gannon. </author> <title> On the problem of optimizing data transfers for complex memory systems. </title> <booktitle> Proc. of 1988 Int'l. Conf. on Supercomputing, </booktitle> <address> St. Malo, France, </address> <pages> pages 238-253, </pages> <month> July </month> <year> 1988. </year>
Reference-contexts: There are only few known approaches, most of which include user assistance for the partitioning process [], or tackle the problem at a theoretical end, not yet proven useful in practice <ref> [GJG88] </ref>. Our approach here shall be to find heuristics that deal with significant program patterns.
Reference: [GPHL90] <author> Mark D. Guzzi, David A. Padua, Jay P. Hoeflinger, and Duncan H. Lawrie. </author> <title> Cedar Fortran and other Vector and Parallel Fortran dialects. </title> <journal> Journal of Supercomputing, </journal> <pages> pages 37-62, </pages> <month> March </month> <year> 1990. </year>
Reference-contexts: Its functionality is made available to the user through Cedar Fortran, the main application programming language. Cedar Fortran is basically Fortran77 with a few additional constructs for exploiting Cedar architectural features <ref> [GPHL90] </ref> . Users program Cedar by either writing directly in Cedar Fortran, or by starting from a sequential Fortran77 code and applying the auto-parallelizing Cedar Restructurer, outputing Cedar Fortran [EHJP90]. Important Cedar Fortran constructs, refered to in this paper are: CTSKSTART forks a new task.
Reference: [HMD + 86] <author> Christopher Huson, Thomas Macke, James R.B. Davies, Michael J. Wolfe, and Bruce Leasure. </author> <title> The KAP/205: An Advanced Source-to-Source Vectorizer for the Cyber 205 Supercomputer. </title> <editor> In Kai Hwang, Steven M. Jacobs, and Earl E. Swartzlander, editors, </editor> <booktitle> Proceedings of the 1986 International Conference on Parallel Processing, </booktitle> <pages> pages 827-832, </pages> <address> 1730 Massachusetts Avenue, N.W., Washington D.C, 20036-1903, 1986. </address> <publisher> IEEE Computer Society Press. </publisher>
Reference: [Kuc88] <editor> Kuck&Associates, </editor> <publisher> Inc., </publisher> <address> Champaign, IL 61820. </address> <note> KAP User's Guide, </note> <year> 1988. </year>
Reference: [Meu84] <author> G. Meurant. </author> <title> The block preconditioned conjugate gradient algorithm on vector computers. </title> <journal> BIT, </journal> <volume> 24 </volume> <pages> 623-633, </pages> <year> 1984. </year> <month> 16 </month>
Reference-contexts: Among the preconditioners considered were many variants of Incomplete Cholesky factorization preconditioners which improve the condition number of the preconditioned system very efficiently [CGM85], are however highly recursive and not suited for parallel computation. Vectorization efforts improved the performance, worsened however the convergence of the method <ref> [Meu84] </ref>. Polynomial preconditioners [Saa85] were another attempt to combine higher convergence rates and a higher degree of parallelism, turned however out to be not as efficient as a high convergence rate requires a high degree polynomial which increases however the computational complexity. <p> Consider the block diagonal matrix obtained by taking the block diagonal of A (see Fig 1) and approximate each diagonal block by a block Incomplete Cholesky preconditioner. For the sake of vectorizability we chose the vectorized block Incomplete Cholesky preconditioner INVC3 (1) <ref> [Meu84] </ref>. This preconditioner is completely paralleliz-able across clusters, has however the disadvantage that the number of iterations depends on the number of clusters used.
Reference: [MS88] <author> U. Meier and A. Sameh. </author> <title> The behavior of conjugate gradient algorithms on a multivector processor with a hierarchical memory. </title> <journal> J. Comp. App. Math., </journal> <volume> 24 </volume> <pages> 13-32, </pages> <year> 1988. </year>
Reference-contexts: Former experiments on an Alliant FX/8 (which is equivalent to a cluster of Cedar) have shown that the performance of iterative methods on one cluster is limited by its cache size <ref> [MS88] </ref>. But the use of this new architecture showed an improvement in performance. Data locality could be increased significantly by distributing the data across clusters and handling smaller chunks on each cluster. <p> Many efforts have been made to implement it with a variety of preconditioning techniques on different parallel computers, trying to take advantage of the various architectures <ref> [MS88] </ref> [vdV86].
Reference: [Saa85] <author> Y. Saad. </author> <title> Practical use of polynomial preconditionings for the conjugate gradient method. </title> <journal> SIAM J. Sci. Stat. Comput, </journal> <volume> 6 </volume> <pages> 865-881, </pages> <year> 1985. </year>
Reference-contexts: Among the preconditioners considered were many variants of Incomplete Cholesky factorization preconditioners which improve the condition number of the preconditioned system very efficiently [CGM85], are however highly recursive and not suited for parallel computation. Vectorization efforts improved the performance, worsened however the convergence of the method [Meu84]. Polynomial preconditioners <ref> [Saa85] </ref> were another attempt to combine higher convergence rates and a higher degree of parallelism, turned however out to be not as efficient as a high convergence rate requires a high degree polynomial which increases however the computational complexity.
Reference: [vdV86] <author> H. van der Vorst. </author> <title> The performance of fortran implementations for preconditioned conjugate gradients on vector computers. </title> <journal> Parallel Computing, </journal> <volume> 3 </volume> <pages> 49-58, </pages> <year> 1986. </year>
Reference-contexts: Many efforts have been made to implement it with a variety of preconditioning techniques on different parallel computers, trying to take advantage of the various architectures [MS88] <ref> [vdV86] </ref>.
References-found: 11

