URL: ftp://ftp.cs.brown.edu/pub/techreports/97/cs97-05.ps.Z
Refering-URL: http://www.cs.brown.edu/publications/techreports/reports/CS-97-05.html
Root-URL: http://www.cs.brown.edu/
Abstract-found: 0
Intro-found: 1
Reference: [ Bellman, 1957 ] <author> Bellman, Richard 1957. </author> <title> Dynamic Programming. </title> <publisher> Princeton University Press. </publisher>
Reference: [ Bertsekas and Casta~non, 1989 ] <author> Bertsekas, D. P. and Casta~non, D. A. </author> <year> 1989. </year> <title> Adaptive aggregation for infinite horizon dynamic programming. </title> <journal> IEEE Transactions on Automatic Control 34(6) </journal> <pages> 589-598. </pages>
Reference: [ Boutilier and Dearden, 1994 ] <author> Boutilier, Craig and Dearden, Richard 1994. </author> <title> Using abstractions for decision theoretic planning with time constraints. </title> <booktitle> In Proceedings AAAI-94. AAAI. </booktitle> <pages> 1016-1022. </pages>
Reference: [ Boutilier et al., 1995a ] <author> Boutilier, Craig; Dean, Thomas; and Hanks, </author> <title> Steve 1995a. Planning under uncertainty: Structural assumptions and computational leverage. </title> <booktitle> In Proceedings of the Third European Workshop on Planning. </booktitle>
Reference-contexts: 1 Introduction The theory of Markov decision processes (MDPs) provides the semantic foundations for a wide range of problems involving planning under uncertainty <ref> [ Boutilier et al., 1995a, Littman, 1997 ] </ref> . In this paper, we introduce a generalization of Markov decision processes called bounded parameter Markov decision processes (BMDPs) that allows us to model uncertainty in the parameters that comprise an MDP.
Reference: [ Boutilier et al., 1995b ] <author> Boutilier, Craig; Dearden, Richard; and Goldszmidt, </author> <month> Moises </month> <year> 1995b. </year> <title> Exploiting structure in policy construction. </title> <booktitle> In Proceedings IJCAI 14. IJCAII. </booktitle> <pages> 1104-1111. </pages>
Reference: [ Dean and Givan, 1997 ] <author> Dean, Thomas and Givan, Robert 1997. </author> <title> Model minimization in Markov decision processes. </title> <booktitle> In Proceedings AAAI-97. </booktitle> <publisher> AAAI. </publisher>
Reference: [ Dean et al., 1997 ] <author> Dean, Thomas; Givan, Robert; and Leach, </author> <title> Sonia 1997. Model reduction techniques for computing approximately optimal solutions for Markov decision processes. </title> <booktitle> In Thirteenth Conference on Uncertainty in Artificial Intelligence. </booktitle>
Reference-contexts: In a related paper, we have shown how BMDPs can be used as part of a strategy for efficiently approximating the solution of MDPs with very large state spaces and dynamics compactly encoded in a factored (or implicit) representation <ref> [ Dean et al., 1997 ] </ref> . In this paper, we focus exclusively on BMDPs, on the BMDP analog of value functions, called interval value functions, and on policy selection for a BMDP.
Reference: [ Littman et al., 1995 ] <author> Littman, Michael; Dean, Thomas; and Kaelbling, </author> <title> Leslie 1995. On the complexity of solving Markov decision problems. </title> <booktitle> In Eleventh Conference on Uncertainty in Artificial Intelligence. </booktitle> <pages> 394-402. </pages>
Reference-contexts: Theorem 6 guarantees the convergence of value iteration to the optimal value function. Similarly, we can specify an algorithm called policy evaluation which finds V by repeatedly apply V I starting with an initial v 0 2 V. The following theorem from <ref> [ Littman et al., 1995 ] </ref> states the convergence rate of value iteration and policy evaluation. <p> We now show how to determine, for 5 We conjecture that this convergence is complete in time polynomial in the input size, following the analogous theorem for standard MDP policy evaluation <ref> [ Littman et al., 1995 ] </ref> . This result should be proven by publication time. 11 fixed ff and p, the value of ^ F pq (ff) for each state q so as to minimize (maxi-mize) the expression P q2Q (F pq (ff)V (q)).
Reference: [ Littman, 1996 ] <author> Littman, </author> <title> Michael Lederman 1996. Algorithms for Sequential Decision Making. </title> <type> Ph.D. Dissertation, </type> <institution> Department of Computer Science, Brown University. </institution>
Reference: [ Littman, 1997 ] <author> Littman, Michael L. </author> <year> 1997. </year> <title> Probabilistic propositional planning: Representations and complexity. </title> <booktitle> In Proceedings AAAI-97. </booktitle> <publisher> AAAI. </publisher>
Reference-contexts: 1 Introduction The theory of Markov decision processes (MDPs) provides the semantic foundations for a wide range of problems involving planning under uncertainty <ref> [ Boutilier et al., 1995a, Littman, 1997 ] </ref> . In this paper, we introduce a generalization of Markov decision processes called bounded parameter Markov decision processes (BMDPs) that allows us to model uncertainty in the parameters that comprise an MDP.
Reference: [ Lovejoy, 1991 ] <author> Lovejoy, William S. </author> <year> 1991. </year> <title> A survey of algorithmic methods for partially observed Markov decision processes. </title> <journal> Annals of Operations Research 28 </journal> <pages> 47-66. </pages>
Reference: [ Puterman, 1994 ] <author> Puterman, Martin L. </author> <year> 1994. </year> <title> Markov Decision Processes. </title> <publisher> John Wiley & Sons, </publisher> <address> New York. </address> <month> 20 </month>
Reference-contexts: We adopt a less general formulation to simplify the presentation. 2 In this paper, we focus on expected discounted cumulative reward as a performance criterion, but other criteria, e.g., total or average reward <ref> [ Puterman, 1994 ] </ref> , are also ap plicable to bounded parameter MDPs. 2 Markov decision process with three states and a single action. The arcs indicate possible transitions and are labeled by their lower and upper bounds. F and R but yield closed real intervals instead of real values. <p> Methods for bounding value functions are frequently used in approximate algorithms for solving MDPs; Love-joy [ 1991 ] describes their use in solving partially observable MDPs. Puter-man <ref> [ Puterman, 1994 ] </ref> provides an excellent introduction to Markov decision processes and estimation techniques that involve bounding value functions.
References-found: 12

