URL: http://www.cs.gatech.edu/reverse/repository/ggram.ps
Refering-URL: http://www.cs.gatech.edu/reverse/repos.html
Root-URL: 
Email: linda@ee.gatech.edu  
Title: Using Attributed Flow Graph Parsing to Recognize Cliches in Programs  
Author: Linda Mary Wills 
Web: http://www.ee.gatech.edu/users/linda/  
Address: Atlanta, Georgia 30332-0250  
Affiliation: School of Electrical and Computer Engineering Georgia Institute of Technology  
Abstract: This paper presents a graph parsing approach to recognizing common, stereotypical computational structures, called cliches, in computer programs. Recognition is a powerful technique for efficiently reconstructing useful design information from existing software. We use a flow graph formalism, which is closely related to hypergraph formalisms, to represent programs and cliches and we use attributed flow graph parsing to automate recognition. The formalism includes mechanisms for tolerating variations in programs due to structure sharing (a common optimization in which a structural component is used to play more than one functional role). The formalism has also been designed to capture aggregation relationships on graph edges, which is used to encode aggregate data structure cliches and the abstract operations on them. A chart parsing algorithm is used to solve the problem of determining which cliches in a given cliche library are in a given program.
Abstract-found: 1
Intro-found: 1
Reference: 1. <author> D. Brotsky. </author> <title> An algorithm for parsing flow graphs. </title> <type> Technical Report 704, </type> <institution> MIT Artificial Intelligence Lab., </institution> <month> March </month> <year> 1984. </year> <type> Master's thesis. </type>
Reference-contexts: Fig. 1. GRASPR's architecture. GRASPR uses a graph parsing approach to automating program recognition, shown in Figure 1. It uses data and control flow analysis to represent a program as a restricted form of directed acyclic graph, called a flow graph <ref> [1, 26] </ref>, which is annotated with attributes. Nodes in the flow graph represent functions, ports on nodes represent inputs and outputs of the functions, edges connect ports and denote dataflow, and attributes capture additional information, such as recursion, control flow and data aggregation. <p> To do this, we are building upon the flow graph formalisms of Brotsky <ref> [1] </ref> and Lutz [15]. A flow graph is an attributed, directed, acyclic graph, whose nodes have ports entry and exit points for edges. Flow graphs have the following properties and restrictions: 1. Each node has a type which is taken from a vocabulary of node types. 2. <p> Each attribute has a (possibly infinite) set of possible values. Notions of flow graphs and flow diagrams have appeared frequently in the literature for more than 20 years. However, our specific type of flow graph was first defined by Brotsky <ref> [1] </ref>, drawing upon the earlier work on web grammars [4, 16, 17, 19, 23]. Wills [22, 25] extended Brotsky's definition so that flow graphs can include sinks and sources, fan-in and fan-out edges, and attributes. <p> To solve the subgraph parsing problem, GRASPR uses a graph parser which has evolved from Earley's string parsing algorithm [5] and string chart parsing. It incorporates four key improvements: 1. generalization of string parsing to flow graph parsing (Brotsky <ref> [1] </ref>, Lutz [15]); 2. generalization of the control strategy to allow flexibility in the rule-invocation and search strategies used (Kay [11], Thompson [24], Lutz [15], Wills [27]); 3. extension of the grammar formalism to handle variation in graphs due to structure-sharing (Lutz [15], Wills [25, 26]), which is useful in dealing
Reference: 2. <author> H. Bunke and B. Haller. </author> <title> A parser for context free plex grammars. </title> <editor> In M. Nagl, editor, </editor> <booktitle> 15th Int. Workshop on Graph-Theoretic Concepts in Computer Science, </booktitle> <pages> pages 136-150. </pages> <publisher> Springer-Verlag, </publisher> <month> June </month> <year> 1989. </year> <booktitle> Lecture Notes in Computer Science Series, </booktitle> <volume> Vol. </volume> <pages> 411. </pages>
Reference-contexts: This is used, for example, in encoding the Fetch+Update cliche shown in Figure 3. Other parsers have been developed which are related to our first two extensions described above. Bunke and Haller <ref> [2] </ref> and Peng, et al. [18] have both developed a parser for plex grammars which are generalizations of Earley's algorithm similar to Brotsky's. Wittenburg, et al. [28] give a unification-based, bottom-up chart parser which is similar to Lutz's and our chart parser.
Reference: 3. <author> W. Dally, A. Chien, S. Fiske, W. Horwat, J. Keene, M. Larivee, R. Lethin, P. Nuth, S. Wills, P. Carrick, and G. Fyler. </author> <title> The J-Machine: A fine-grain concurrent computer. </title> <booktitle> In Int. Fed. of Info. Processing Societies, </booktitle> <year> 1989. </year>
Reference-contexts: of how the parser deals with structure-sharing and aggregation is given in [26], including how residual Spreads and Makes are handled and how straight-throughs are matched. 6 Efficiency We are studying the graph parsing approach by experimenting with two real-world simulator programs, written in Common Lisp by parallel processing researchers <ref> [3] </ref>. These programs are in the 500 to 1000 line range. The largest program recognized by any other existing recognition system is a 300-line database program recognized by CPU [13]. All other systems work with "toy programs" on the order of tens of lines.
Reference: 4. <author> P. Della-Vigna and C. Ghezzi. </author> <title> Context-free graph grammars. </title> <journal> Information and Control, </journal> <volume> 37(2) </volume> <pages> 207-233, </pages> <year> 1978. </year>
Reference-contexts: Each attribute has a (possibly infinite) set of possible values. Notions of flow graphs and flow diagrams have appeared frequently in the literature for more than 20 years. However, our specific type of flow graph was first defined by Brotsky [1], drawing upon the earlier work on web grammars <ref> [4, 16, 17, 19, 23] </ref>. Wills [22, 25] extended Brotsky's definition so that flow graphs can include sinks and sources, fan-in and fan-out edges, and attributes. Our flow graph formalism is related to that of Lutz, which in turn is equivalent to hypergraph formalisms with hyperedge replacement [8].
Reference: 5. <author> J. Earley. </author> <title> An efficient context-free parsing algorithm. </title> <journal> Comm. of the ACM, </journal> <volume> 13(2) </volume> <pages> 94-102, </pages> <year> 1970. </year>
Reference-contexts: To solve the subgraph parsing problem, GRASPR uses a graph parser which has evolved from Earley's string parsing algorithm <ref> [5] </ref> and string chart parsing.
Reference: 6. <author> R. Farrow, K. Kennedy, and L. Zucconi. </author> <title> Graph grammars and global program data flow analysis. </title> <booktitle> In Proc. 17th Annual IEEE Symposium on Foundations of Computer Science, </booktitle> <pages> pages 42-56, </pages> <address> Houston, Texas, </address> <year> 1976. </year>
Reference-contexts: This creates fewer partial analyses, which is advantageous in terms of efficiency, but is a drawback in terms of generating partial results when the graph contains unrecognizable sections. Efficient parsers for control flow graphs have also been developed and applied to problems in global dataflow analysis <ref> [6] </ref> and program restructuring [14]. These parsers are able to take advantage of the fact that their reduction rules have the finite Church-Rosser property, which gives rise to a linear-time parsing process. These parsers also aim for a single full parse of the entire control flow graph.
Reference: 7. <author> J. </author> <title> Feder. </title> <journal> Plex languages. Information Sciences Journal, </journal> <volume> 3 </volume> <pages> 225-241, </pages> <year> 1971. </year>
Reference-contexts: Our flow graph formalism is related to that of Lutz, which in turn is equivalent to hypergraph formalisms with hyperedge replacement [8]. More specifically, Lutz's "flowgraphs" are a special type of our flow graph. (They derived from research on plex languages <ref> [7] </ref>.) In addition to nodes, ports, and edges, they contain tie-points, which are intermediate points through which ports are connected to each other.
Reference: 8. <author> A. Habel. </author> <title> Hyperedge Replacement: Grammars and Languages. </title> <publisher> Springer-Verlag, </publisher> <address> New York, </address> <year> 1992. </year> <booktitle> Lecture Notes in Computer Science Series, </booktitle> <volume> Vol. </volume> <pages> 643. </pages>
Reference-contexts: Wills [22, 25] extended Brotsky's definition so that flow graphs can include sinks and sources, fan-in and fan-out edges, and attributes. Our flow graph formalism is related to that of Lutz, which in turn is equivalent to hypergraph formalisms with hyperedge replacement <ref> [8] </ref>. More specifically, Lutz's "flowgraphs" are a special type of our flow graph. (They derived from research on plex languages [7].) In addition to nodes, ports, and edges, they contain tie-points, which are intermediate points through which ports are connected to each other.
Reference: 9. <author> J. Hartman. </author> <title> Automatic control understanding for natural programs. </title> <type> Technical Report AI 91-161, </type> <institution> University of Texas at Austin, </institution> <year> 1991. </year> <type> PhD thesis. </type>
Reference-contexts: It bypasses complex reasoning about how behaviors and properties arise from certain combinations of language primitives. Several researchers have shown the feasibility and usefulness of automating recognition, most recently <ref> [9, 10, 12, 13, 20, 25, 26] </ref>. A primary motivation for automating recognition is to facilitate tasks requiring program understanding, such as maintaining, debugging, and reusing software. We have developed an experimental recognition system, called GRASPR ("GRAph-based System for Program Recognition") [26], to automate program recognition.
Reference: 10. <author> W. L. Johnson. </author> <title> Intention-Based Diagnosis of Novice Programming Errors. </title> <publisher> Mor-gan Kaufmann Publishers, Inc., </publisher> <address> Los Altos, CA, </address> <year> 1986. </year>
Reference-contexts: It bypasses complex reasoning about how behaviors and properties arise from certain combinations of language primitives. Several researchers have shown the feasibility and usefulness of automating recognition, most recently <ref> [9, 10, 12, 13, 20, 25, 26] </ref>. A primary motivation for automating recognition is to facilitate tasks requiring program understanding, such as maintaining, debugging, and reusing software. We have developed an experimental recognition system, called GRASPR ("GRAph-based System for Program Recognition") [26], to automate program recognition.
Reference: 11. <author> M. Kay. </author> <title> Algorithm schemata and data structures in syntactic processing. </title> <editor> In B. Grosz, K. Sparck-Jones, and B. Webber, editors, </editor> <booktitle> Readings in Natural Language Processing, </booktitle> <pages> pages 35-70. </pages> <publisher> Morgan Kaufmann Publishers, Inc., </publisher> <address> Los Altos, CA, </address> <year> 1986. </year>
Reference-contexts: It incorporates four key improvements: 1. generalization of string parsing to flow graph parsing (Brotsky [1], Lutz [15]); 2. generalization of the control strategy to allow flexibility in the rule-invocation and search strategies used (Kay <ref> [11] </ref>, Thompson [24], Lutz [15], Wills [27]); 3. extension of the grammar formalism to handle variation in graphs due to structure-sharing (Lutz [15], Wills [25, 26]), which is useful in dealing with variation due to common function-sharing optimizations; and 4. extension of the grammar formalism to capture aggregation relationships (Wills [26])
Reference: 12. <author> V. Kozaczynski and J.Q. Ning. </author> <title> Automated program understanding by concept recognition. </title> <journal> Automated Software Engineering, </journal> <volume> 1(1) </volume> <pages> 61-78, </pages> <month> March </month> <year> 1994. </year>
Reference-contexts: It bypasses complex reasoning about how behaviors and properties arise from certain combinations of language primitives. Several researchers have shown the feasibility and usefulness of automating recognition, most recently <ref> [9, 10, 12, 13, 20, 25, 26] </ref>. A primary motivation for automating recognition is to facilitate tasks requiring program understanding, such as maintaining, debugging, and reusing software. We have developed an experimental recognition system, called GRASPR ("GRAph-based System for Program Recognition") [26], to automate program recognition.
Reference: 13. <author> S. Letovsky. </author> <title> Plan analysis of programs. </title> <type> Research Report 662, </type> <institution> Yale University, </institution> <month> December </month> <year> 1988. </year> <type> PhD Thesis. </type>
Reference-contexts: It bypasses complex reasoning about how behaviors and properties arise from certain combinations of language primitives. Several researchers have shown the feasibility and usefulness of automating recognition, most recently <ref> [9, 10, 12, 13, 20, 25, 26] </ref>. A primary motivation for automating recognition is to facilitate tasks requiring program understanding, such as maintaining, debugging, and reusing software. We have developed an experimental recognition system, called GRASPR ("GRAph-based System for Program Recognition") [26], to automate program recognition. <p> These programs are in the 500 to 1000 line range. The largest program recognized by any other existing recognition system is a 300-line database program recognized by CPU <ref> [13] </ref>. All other systems work with "toy programs" on the order of tens of lines. We empirically and analytically studied the computational cost of GRASPR's parsing algorithm with respect to the simulator programs. <p> GRASPR is able to recognize structured programs and cliches containing conditionals, loops with any number of exits, recursion, aggregate data structures, and simple side effects due to variable assignments. With the exception of CPU <ref> [13] </ref>, existing recognition systems cannot handle aggregate data structure cliches and a majority do not handle recursion.
Reference: 14. <author> U Lichtblau. </author> <title> Decompilation of control structures by means of graph transformation. </title> <editor> In H. Ehrig, editor, </editor> <booktitle> International Joint Conference on Theory and Practice of Software Development, </booktitle> <pages> pages 284-297. </pages> <publisher> Springer-Verlag, </publisher> <year> 1985. </year> <booktitle> Lecture Notes In Computer Science Series, </booktitle> <volume> Vol. </volume> <pages> 185. </pages>
Reference-contexts: Efficient parsers for control flow graphs have also been developed and applied to problems in global dataflow analysis [6] and program restructuring <ref> [14] </ref>. These parsers are able to take advantage of the fact that their reduction rules have the finite Church-Rosser property, which gives rise to a linear-time parsing process. These parsers also aim for a single full parse of the entire control flow graph.
Reference: 15. <author> R. Lutz. </author> <title> Chart parsing of flowgraphs. </title> <booktitle> In Proc. 11th Int. Joint Conf. Artificial Intelligence, </booktitle> <pages> pages 116-121, </pages> <address> Detroit, Michigan, </address> <year> 1989. </year>
Reference-contexts: To do this, we are building upon the flow graph formalisms of Brotsky [1] and Lutz <ref> [15] </ref>. A flow graph is an attributed, directed, acyclic graph, whose nodes have ports entry and exit points for edges. Flow graphs have the following properties and restrictions: 1. Each node has a type which is taken from a vocabulary of node types. 2. <p> To solve the subgraph parsing problem, GRASPR uses a graph parser which has evolved from Earley's string parsing algorithm [5] and string chart parsing. It incorporates four key improvements: 1. generalization of string parsing to flow graph parsing (Brotsky [1], Lutz <ref> [15] </ref>); 2. generalization of the control strategy to allow flexibility in the rule-invocation and search strategies used (Kay [11], Thompson [24], Lutz [15], Wills [27]); 3. extension of the grammar formalism to handle variation in graphs due to structure-sharing (Lutz [15], Wills [25, 26]), which is useful in dealing with variation <p> It incorporates four key improvements: 1. generalization of string parsing to flow graph parsing (Brotsky [1], Lutz <ref> [15] </ref>); 2. generalization of the control strategy to allow flexibility in the rule-invocation and search strategies used (Kay [11], Thompson [24], Lutz [15], Wills [27]); 3. extension of the grammar formalism to handle variation in graphs due to structure-sharing (Lutz [15], Wills [25, 26]), which is useful in dealing with variation due to common function-sharing optimizations; and 4. extension of the grammar formalism to capture aggregation relationships (Wills [26]) between single inputs or <p> of string parsing to flow graph parsing (Brotsky [1], Lutz <ref> [15] </ref>); 2. generalization of the control strategy to allow flexibility in the rule-invocation and search strategies used (Kay [11], Thompson [24], Lutz [15], Wills [27]); 3. extension of the grammar formalism to handle variation in graphs due to structure-sharing (Lutz [15], Wills [25, 26]), which is useful in dealing with variation due to common function-sharing optimizations; and 4. extension of the grammar formalism to capture aggregation relationships (Wills [26]) between single inputs or outputs of a left-hand side node and a tuple of inputs or outputs of a right-hand side sub-flow <p> GRASPR, on the other hand, finds all recognizable subgraphs in order to deal with programs that are not constructed completely of cliches. This partial recognition process is inherently more expensive, but more flexible in dealing with code that contains novel or buggy structures. 4.1 Share-Equivalence Following Lutz <ref> [15] </ref>, we expand the language of a flow graph grammar to include all flow graphs derivable not only from a start type of the flow graph grammar, but also from flow graphs that are "share-equivalent" to a sentential form 1 of the grammar. <p> Chart parsers maintain a database, called a chart, of partial and complete analyses of the input. The elements in the chart are called items. (In string chart parsing, they are called "edges." Lutz <ref> [15] </ref> calls them "patches.") An item might be either complete or partial. Complete items represent the recognition of some terminal or non-terminal in the grammar. Partial items represent a partial recognition of a non-terminal.
Reference: 16. <author> U. G. Montanari. </author> <title> Separable graphs, planar graphs, and web grammars. </title> <journal> Information and Control, </journal> <volume> 16(3) </volume> <pages> 243-267, </pages> <month> March </month> <year> 1970. </year>
Reference-contexts: Each attribute has a (possibly infinite) set of possible values. Notions of flow graphs and flow diagrams have appeared frequently in the literature for more than 20 years. However, our specific type of flow graph was first defined by Brotsky [1], drawing upon the earlier work on web grammars <ref> [4, 16, 17, 19, 23] </ref>. Wills [22, 25] extended Brotsky's definition so that flow graphs can include sinks and sources, fan-in and fan-out edges, and attributes. Our flow graph formalism is related to that of Lutz, which in turn is equivalent to hypergraph formalisms with hyperedge replacement [8].
Reference: 17. <author> T. Pavlidis. </author> <title> Linear and context-free graph grammars. </title> <journal> Journal of the ACM, </journal> <volume> 19(1) </volume> <pages> 11-23, </pages> <month> January </month> <year> 1972. </year>
Reference-contexts: Each attribute has a (possibly infinite) set of possible values. Notions of flow graphs and flow diagrams have appeared frequently in the literature for more than 20 years. However, our specific type of flow graph was first defined by Brotsky [1], drawing upon the earlier work on web grammars <ref> [4, 16, 17, 19, 23] </ref>. Wills [22, 25] extended Brotsky's definition so that flow graphs can include sinks and sources, fan-in and fan-out edges, and attributes. Our flow graph formalism is related to that of Lutz, which in turn is equivalent to hypergraph formalisms with hyperedge replacement [8].
Reference: 18. <author> K. Peng, T. Yamamoto, and Y. Aoki. </author> <title> A new parsing algorithm for plex grammars. </title> <journal> Pattern Recognition, </journal> <volume> 23(3-4):393-402, </volume> <year> 1990. </year>
Reference-contexts: This is used, for example, in encoding the Fetch+Update cliche shown in Figure 3. Other parsers have been developed which are related to our first two extensions described above. Bunke and Haller [2] and Peng, et al. <ref> [18] </ref> have both developed a parser for plex grammars which are generalizations of Earley's algorithm similar to Brotsky's. Wittenburg, et al. [28] give a unification-based, bottom-up chart parser which is similar to Lutz's and our chart parser.
Reference: 19. <author> J. L. Pfaltz and A. Rosenfeld. </author> <title> Web grammars. </title> <booktitle> In Proc. 1st Int. Joint Conf. Artificial Intelligence, </booktitle> <pages> pages 609-619, </pages> <address> Washington, D.C., </address> <month> September </month> <year> 1969. </year>
Reference-contexts: Each attribute has a (possibly infinite) set of possible values. Notions of flow graphs and flow diagrams have appeared frequently in the literature for more than 20 years. However, our specific type of flow graph was first defined by Brotsky [1], drawing upon the earlier work on web grammars <ref> [4, 16, 17, 19, 23] </ref>. Wills [22, 25] extended Brotsky's definition so that flow graphs can include sinks and sources, fan-in and fan-out edges, and attributes. Our flow graph formalism is related to that of Lutz, which in turn is equivalent to hypergraph formalisms with hyperedge replacement [8].
Reference: 20. <author> A. Quilici. </author> <title> Memory-based approach to recognizing programming plans. </title> <journal> Comm. of the ACM, </journal> <volume> 37(5) </volume> <pages> 84-93, </pages> <month> May </month> <year> 1994. </year>
Reference-contexts: It bypasses complex reasoning about how behaviors and properties arise from certain combinations of language primitives. Several researchers have shown the feasibility and usefulness of automating recognition, most recently <ref> [9, 10, 12, 13, 20, 25, 26] </ref>. A primary motivation for automating recognition is to facilitate tasks requiring program understanding, such as maintaining, debugging, and reusing software. We have developed an experimental recognition system, called GRASPR ("GRAph-based System for Program Recognition") [26], to automate program recognition.
Reference: 21. <author> C. Rich and R. C. Waters. </author> <title> The Programmer's Apprentice. </title> <publisher> Addison-Wesley, </publisher> <address> Reading, </address> <publisher> MA and ACM Press, </publisher> <address> Baltimore, MD, </address> <year> 1990. </year>
Reference-contexts: 1 Program Recognition An experienced programmer can often reconstruct much of the hierarchy of a program's design by recognizing commonly used data structures and algorithms in it and reasoning about how they typically implement higher-level abstractions. We call these commonly used computational structures cliches <ref> [21] </ref>. Examples of cliches are algorithmic computations, such as list enumeration, binary search, and event-driven simulation, and common data structures, such as priority queue and hash table.
Reference: 22. <author> C. Rich and L. M. Wills. </author> <title> Recognizing a program's design: A graph-parsing approach. </title> <journal> IEEE Software, </journal> <volume> 7(1) </volume> <pages> 82-89, </pages> <month> January </month> <year> 1990. </year>
Reference-contexts: Notions of flow graphs and flow diagrams have appeared frequently in the literature for more than 20 years. However, our specific type of flow graph was first defined by Brotsky [1], drawing upon the earlier work on web grammars [4, 16, 17, 19, 23]. Wills <ref> [22, 25] </ref> extended Brotsky's definition so that flow graphs can include sinks and sources, fan-in and fan-out edges, and attributes. Our flow graph formalism is related to that of Lutz, which in turn is equivalent to hypergraph formalisms with hyperedge replacement [8].
Reference: 23. <author> A. Rosenfeld and D. Milgram. </author> <title> Web automata and web grammars. </title> <editor> In B. Meltzer and D. Michie, editors, </editor> <booktitle> Machine Intelligence 7, </booktitle> <pages> pages 307-324. </pages> <publisher> John Wiley and Sons, </publisher> <address> New York, </address> <year> 1972. </year>
Reference-contexts: Each attribute has a (possibly infinite) set of possible values. Notions of flow graphs and flow diagrams have appeared frequently in the literature for more than 20 years. However, our specific type of flow graph was first defined by Brotsky [1], drawing upon the earlier work on web grammars <ref> [4, 16, 17, 19, 23] </ref>. Wills [22, 25] extended Brotsky's definition so that flow graphs can include sinks and sources, fan-in and fan-out edges, and attributes. Our flow graph formalism is related to that of Lutz, which in turn is equivalent to hypergraph formalisms with hyperedge replacement [8].
Reference: 24. <author> H. Thompson. </author> <title> Chart parsing and rule schemata in GPSG. </title> <booktitle> In Proc. 19th Annual Meeting of the ACL, </booktitle> <address> Stanford, CA, </address> <year> 1981. </year>
Reference-contexts: It incorporates four key improvements: 1. generalization of string parsing to flow graph parsing (Brotsky [1], Lutz [15]); 2. generalization of the control strategy to allow flexibility in the rule-invocation and search strategies used (Kay [11], Thompson <ref> [24] </ref>, Lutz [15], Wills [27]); 3. extension of the grammar formalism to handle variation in graphs due to structure-sharing (Lutz [15], Wills [25, 26]), which is useful in dealing with variation due to common function-sharing optimizations; and 4. extension of the grammar formalism to capture aggregation relationships (Wills [26]) between single
Reference: 25. <author> L. Wills. </author> <title> Automated program recognition: A feasibility demonstration. </title> <journal> Artificial Intelligence, </journal> <volume> 45(1-2):113-172, </volume> <year> 1990. </year>
Reference-contexts: It bypasses complex reasoning about how behaviors and properties arise from certain combinations of language primitives. Several researchers have shown the feasibility and usefulness of automating recognition, most recently <ref> [9, 10, 12, 13, 20, 25, 26] </ref>. A primary motivation for automating recognition is to facilitate tasks requiring program understanding, such as maintaining, debugging, and reusing software. We have developed an experimental recognition system, called GRASPR ("GRAph-based System for Program Recognition") [26], to automate program recognition. <p> Notions of flow graphs and flow diagrams have appeared frequently in the literature for more than 20 years. However, our specific type of flow graph was first defined by Brotsky [1], drawing upon the earlier work on web grammars [4, 16, 17, 19, 23]. Wills <ref> [22, 25] </ref> extended Brotsky's definition so that flow graphs can include sinks and sources, fan-in and fan-out edges, and attributes. Our flow graph formalism is related to that of Lutz, which in turn is equivalent to hypergraph formalisms with hyperedge replacement [8]. <p> parsing to flow graph parsing (Brotsky [1], Lutz [15]); 2. generalization of the control strategy to allow flexibility in the rule-invocation and search strategies used (Kay [11], Thompson [24], Lutz [15], Wills [27]); 3. extension of the grammar formalism to handle variation in graphs due to structure-sharing (Lutz [15], Wills <ref> [25, 26] </ref>), which is useful in dealing with variation due to common function-sharing optimizations; and 4. extension of the grammar formalism to capture aggregation relationships (Wills [26]) between single inputs or outputs of a left-hand side node and a tuple of inputs or outputs of a right-hand side sub-flow graph.
Reference: 26. <author> L. Wills. </author> <title> Automated program recognition by graph parsing. </title> <type> Technical Report 1358, </type> <institution> MIT Artificial Intelligence Lab., </institution> <month> July </month> <year> 1992. </year> <type> PhD Thesis. </type>
Reference-contexts: It bypasses complex reasoning about how behaviors and properties arise from certain combinations of language primitives. Several researchers have shown the feasibility and usefulness of automating recognition, most recently <ref> [9, 10, 12, 13, 20, 25, 26] </ref>. A primary motivation for automating recognition is to facilitate tasks requiring program understanding, such as maintaining, debugging, and reusing software. We have developed an experimental recognition system, called GRASPR ("GRAph-based System for Program Recognition") [26], to automate program recognition. <p> A primary motivation for automating recognition is to facilitate tasks requiring program understanding, such as maintaining, debugging, and reusing software. We have developed an experimental recognition system, called GRASPR ("GRAph-based System for Program Recognition") <ref> [26] </ref>, to automate program recognition. Given a program and a library of cliches, GRASPR finds all instances of the cliches in a program. It can generate multiple views of a program as well as near-miss recognitions of cliches. <p> Fig. 1. GRASPR's architecture. GRASPR uses a graph parsing approach to automating program recognition, shown in Figure 1. It uses data and control flow analysis to represent a program as a restricted form of directed acyclic graph, called a flow graph <ref> [1, 26] </ref>, which is annotated with attributes. Nodes in the flow graph represent functions, ports on nodes represent inputs and outputs of the functions, edges connect ports and denote dataflow, and attributes capture additional information, such as recursion, control flow and data aggregation. <p> Similar constraints are also imposed on edges, restricting the control environments in which they carry dataflow. (Attribute constraints and transfer rules are stated informally in Fig. 2; <ref> [26] </ref> gives a formal description of the attribute language.) Parsing yields a hierarchical description of a plausible design of the program in the form of derivation trees specifying the cliches found and their relationships to each other. In general, GRASPR generates a forest of design trees for a given program. <p> Further details of how the embedding relation is restricted are given in <ref> [26] </ref>. 4 Partial Program Recognition as Flow Graph Parsing The subgraph parsing problem for flow graphs is: Given a flow graph F and a context-free flow graph grammar G, find all parses of all sub-flow graphs of F that are in the language of G. <p> parsing to flow graph parsing (Brotsky [1], Lutz [15]); 2. generalization of the control strategy to allow flexibility in the rule-invocation and search strategies used (Kay [11], Thompson [24], Lutz [15], Wills [27]); 3. extension of the grammar formalism to handle variation in graphs due to structure-sharing (Lutz [15], Wills <ref> [25, 26] </ref>), which is useful in dealing with variation due to common function-sharing optimizations; and 4. extension of the grammar formalism to capture aggregation relationships (Wills [26]) between single inputs or outputs of a left-hand side node and a tuple of inputs or outputs of a right-hand side sub-flow graph. <p> [11], Thompson [24], Lutz [15], Wills [27]); 3. extension of the grammar formalism to handle variation in graphs due to structure-sharing (Lutz [15], Wills [25, 26]), which is useful in dealing with variation due to common function-sharing optimizations; and 4. extension of the grammar formalism to capture aggregation relationships (Wills <ref> [26] </ref>) between single inputs or outputs of a left-hand side node and a tuple of inputs or outputs of a right-hand side sub-flow graph. <p> If both are sets, the sets must have a non-empty intersection for the comparison to succeed. If one is a set and the other a tuple, the comparison fails. This is a brief description; a fuller account of how the parser deals with structure-sharing and aggregation is given in <ref> [26] </ref>, including how residual Spreads and Makes are handled and how straight-throughs are matched. 6 Efficiency We are studying the graph parsing approach by experimenting with two real-world simulator programs, written in Common Lisp by parallel processing researchers [3]. These programs are in the 500 to 1000 line range. <p> This is inherently exponential. In fact, the subgraph parsing problem for flow graphs is NP-complete <ref> [26] </ref>, so it is unlikely that there is a subgraph parsing algorithm that is not worst-case exponential. However, in the practical application of graph parsing to recognizing complete instances of cliches, constraints are strong enough to prevent exponential behavior in practice. <p> This task is useful in robustly dealing with buggy programs, learning new cliches, and eliciting advice. Fortunately, the complexity of near-miss recognition can be controlled by using grammar indexing and flow graph partitioning advice and controlling the application order of constraints <ref> [26, 27] </ref>. 7 Expressiveness The flow graph representation is able to suppress many common forms of program variation which hinder recognition. <p> With the exception of CPU [13], existing recognition systems cannot handle aggregate data structure cliches and a majority do not handle recursion. Side effects to mutable data structures still present an open problem in program recognition, but see <ref> [26] </ref> for future directions in interleaving dataflow analysis with the recognition of stereotypical aliasing patterns. 8 Future Directions One reason we developed a parsing algorithm with flexible control is that we wanted to complement our purely code-driven recognition with other design recovery techniques based on information from other sources, such as
Reference: 27. <author> L. Wills. </author> <title> Flexible control for program recognition. </title> <booktitle> In Proc. 1st Working Conference on Reverse Engineering, </booktitle> <address> Baltimore, MD, </address> <month> May </month> <year> 1993. </year>
Reference-contexts: It incorporates four key improvements: 1. generalization of string parsing to flow graph parsing (Brotsky [1], Lutz [15]); 2. generalization of the control strategy to allow flexibility in the rule-invocation and search strategies used (Kay [11], Thompson [24], Lutz [15], Wills <ref> [27] </ref>); 3. extension of the grammar formalism to handle variation in graphs due to structure-sharing (Lutz [15], Wills [25, 26]), which is useful in dealing with variation due to common function-sharing optimizations; and 4. extension of the grammar formalism to capture aggregation relationships (Wills [26]) between single inputs or outputs of <p> This explicit control can be used to enforce a particular rule invocation strategy or search strategy. In fact, the parser has several "control knobs" that can be set to achieve a desired control strategy or to implement various focusing heuristics <ref> [27] </ref>. These include parameters like bottom-up or top-down rule invocation, the criterion used to determine whether one item can extend another, the ordering in which right-hand side nodes are matched, and the ordering of attribute condition checking in general. <p> This task is useful in robustly dealing with buggy programs, learning new cliches, and eliciting advice. Fortunately, the complexity of near-miss recognition can be controlled by using grammar indexing and flow graph partitioning advice and controlling the application order of constraints <ref> [26, 27] </ref>. 7 Expressiveness The flow graph representation is able to suppress many common forms of program variation which hinder recognition.
Reference: 28. <author> K. Wittenburg, L. Weitzman, and J. Talley. </author> <title> Unification-based grammars and tabular parsing for graphical languages. </title> <type> Technical Report ACT-OODS-208-91, </type> <institution> MCC, </institution> <month> June </month> <year> 1991. </year> <title> This article was processed using the L a T E X macro package with LLNCS style </title>
Reference-contexts: Other parsers have been developed which are related to our first two extensions described above. Bunke and Haller [2] and Peng, et al. [18] have both developed a parser for plex grammars which are generalizations of Earley's algorithm similar to Brotsky's. Wittenburg, et al. <ref> [28] </ref> give a unification-based, bottom-up chart parser which is similar to Lutz's and our chart parser. Grammar rules place a strict (total) ordering on the nodes in their right-hand sides. This specifies the order in which right-hand side nodes are matched (e.g., match the most salient node types first).
References-found: 28

