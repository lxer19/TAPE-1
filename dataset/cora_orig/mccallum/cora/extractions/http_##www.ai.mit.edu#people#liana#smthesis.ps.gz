URL: http://www.ai.mit.edu/people/liana/smthesis.ps.gz
Refering-URL: http://www.ai.mit.edu/people/liana/liana.html
Root-URL: 
Title: Visually-Guided Obstacle Avoidance in Unstructured Environments  
Author: by Liana M. Lorigo Rodney A. Brooks W. Eric L. Grimson 
Degree: Submitted to the Department of Electrical Engineering and Computer Science in partial fulfillment of the requirements for the degree of Master of Science in Computer Science and Engineering at the  Signature of Author  Certified by  Professor Thesis Supervisor Certified by  Professor Thesis Supervisor Accepted by Frederic R. Morgenthaler Chairman, Departmental Committee on Graduate Students  
Note: c Massachusetts Institute of Technology  
Date: February 1996  1996  January 31, 1996  
Affiliation: MASSACHUSETTS INSTITUTE OF TECHNOLOGY  Department of Electrical Engineering and Computer Science  
Abstract-found: 0
Intro-found: 1
Reference: <author> Agre, P. E. & Chapman, D. </author> <year> (1987), </year> <title> Pengi: An implementation of a theory of activity, </title> <booktitle> in `Proceedings of the Sixth Nat'l Conf. on Artificial Intelligence', </booktitle> <pages> pp. 268-272. </pages>
Reference-contexts: This theory has received attention from both the planning community and the vision community. Agre and Chapman's implementation of a VRP used to play a video game was the first well-known implementation, although they bypassed vision and interfaced the "sensor" directly to a world model <ref> (Agre & Chapman 1987) </ref>. Mahoney has defined and implemented a visual programming language in which elementary operations are combined to form visual routines (Mahoney 1993) (Mahoney forthcoming). His language uses the operations suggested by Ullman as well as additional operations and has been used for document processing.
Reference: <author> Aloimonos, Y. </author> <year> (1993), </year> <title> Active Perception, </title> <publisher> Lawrence Erlbaum Assoc. </publisher>
Reference-contexts: However, the current system does not combine the elementary operations according to the VRP model. 2.3.2 Animate Vision A related strategy in the construction of computer vision systems is animate vision (Krotkov 1989) (Ballard 1991) (Coombs 1992) <ref> (Aloimonos 1993) </ref>. Animate vision is the strategy of controlling the camera view based on previous visual sensing so 15 that subsequent computations may be simplified. Such systems therefore normally include a gaze control mechanism (Pahlavan & Eklundh 1992).
Reference: <author> Ballard, D. H. </author> <year> (1991), </year> <title> `Animate vision', </title> <booktitle> Artificial Intelligence 48, </booktitle> <pages> 57-86. </pages>
Reference-contexts: However, the current system does not combine the elementary operations according to the VRP model. 2.3.2 Animate Vision A related strategy in the construction of computer vision systems is animate vision (Krotkov 1989) <ref> (Ballard 1991) </ref> (Coombs 1992) (Aloimonos 1993). Animate vision is the strategy of controlling the camera view based on previous visual sensing so 15 that subsequent computations may be simplified. Such systems therefore normally include a gaze control mechanism (Pahlavan & Eklundh 1992).
Reference: <author> Beer, R. </author> <year> (1988), </year> <title> A dynamical systems perspective on autonomous agents, </title> <note> in `CES 92-11, </note> <institution> Case Western Reserve University'. </institution>
Reference-contexts: One major idea is that the distinction between the agent and its environment is arbitrary. For example, a motor of a robot can readily be considered part of the environment of the robot's overall controller. Current research focuses on understanding agents, environments, and interactions as complete non-linear dynamical systems <ref> (Beer 1988) </ref>. That is, three criteria must be met: 1) the agents, environments, and interactions have identifiable states, 2) state changes are continuous, and 3) such changes are captured by a set of (non-linear) functions that define a vector field over the state space (Smithers 1995).
Reference: <author> Brooks, R. A. </author> <year> (1986), </year> <title> `A robust layered control system for a mobile robot', </title> <journal> IEEE Journal of Robotics and Automation 2(1), </journal> <pages> 14-23. </pages>
Reference-contexts: There have been several general strategies for the cooperation of disparate modules within a single system. The subsumption architecture advocates the decomposition of a system in terms of its task-achieving behaviors rather than in terms of the functional units of the system <ref> (Brooks 1986) </ref>. Examples of task-achieving behaviors for a mobile robot are obstacle avoidance, exploration, goal-directed navigation, and reasoning, while examples of 11 functional units are perception, modeling, planning, and motor control. Systems using the subsumption architecture are comprised of layers of increasingly complex behaviors and have no central planning unit. <p> The current system is behavior-based in that it uses relatively simple modular behaviors to exhibit increasingly complex behaviors <ref> (Brooks 1986) </ref>. In short, the "stuck" layer subsumes the vision layer when appropriate. Future work could incorporate additional layers such as higher-level navigation capabilities. These strategies are examples of Brooks' subsumption architecture, in which behaviors are layered to achieve increased complexity. <p> Incorporating such additional modules into the current obstacle avoidance framework is straightforward although it is outside the scope of this thesis. Perhaps the most straightforward way to do this is to add additional layers implementing higher-level navigation according to the subsumption architecture <ref> (Brooks 1986) </ref>. This navigation could be based on a variety of sensory modalities depending on the target, including vision, infrared, motor-based odometry, radio-positioning, and sonar. Alternatively, the current obstacle avoidance system could be incorporated into an interactive approach for directing a robot to a target.
Reference: <author> Brooks, R. A. </author> <year> (1994), </year> <title> L, IS Robotics, </title> <publisher> Inc. </publisher>
Reference-contexts: The processor is a Texas Instruments C30 DSP. The CVM holds 1MB of memory and supports high-speed video input. The vision software is written in C and runs on the CVM. The system runs a smaller control program, written in L (a subset of LISP) <ref> (Brooks 1994) </ref> on the 68332. 3.2 Vision Modules The system is comprised of multiple independent vision modules. The purpose of each module is to generate a relative depth map of obstacle locations based on the input 24 image frame.
Reference: <author> Coombs, D. </author> <year> (1992), </year> <title> Real-Time Gaze Holding in Binocular Robot Vision, </title> <type> Technical Report Rocherster TR 415, </type> <institution> University of Rochester, Computer Science. </institution>
Reference-contexts: However, the current system does not combine the elementary operations according to the VRP model. 2.3.2 Animate Vision A related strategy in the construction of computer vision systems is animate vision (Krotkov 1989) (Ballard 1991) <ref> (Coombs 1992) </ref> (Aloimonos 1993). Animate vision is the strategy of controlling the camera view based on previous visual sensing so 15 that subsequent computations may be simplified. Such systems therefore normally include a gaze control mechanism (Pahlavan & Eklundh 1992). <p> Combined with spatial information, a temporal approach has been used in a successful road-following system which attains very high speeds and robustness to some environmental changes (Dickmanns, Mysliwetz & Christians 1990). Related work in visually-guided robotics has used a simplification of optical flow for obstacle avoidance <ref> (Coombs & Roberts 1992) </ref> (Santos-Victor, Sandini, Curotto & Garibaldi 1995). Only the flow in the direction of the normal to the image intensity gradient is computed. This flow is called normal flow or gradient-parallel optical flow, and it is the only flow component that can be computed locally.
Reference: <author> Coombs, D. & Roberts, K. </author> <year> (1992), </year> <title> "Bee-bot": using peripheral optical flow to avoid obstacles, </title> <booktitle> in `Intelligent Robots and Computer Vision', </booktitle> <volume> Vol. 1825, </volume> <booktitle> SPIE, </booktitle> <pages> pp. 714-721. </pages>
Reference-contexts: However, the current system does not combine the elementary operations according to the VRP model. 2.3.2 Animate Vision A related strategy in the construction of computer vision systems is animate vision (Krotkov 1989) (Ballard 1991) <ref> (Coombs 1992) </ref> (Aloimonos 1993). Animate vision is the strategy of controlling the camera view based on previous visual sensing so 15 that subsequent computations may be simplified. Such systems therefore normally include a gaze control mechanism (Pahlavan & Eklundh 1992). <p> Combined with spatial information, a temporal approach has been used in a successful road-following system which attains very high speeds and robustness to some environmental changes (Dickmanns, Mysliwetz & Christians 1990). Related work in visually-guided robotics has used a simplification of optical flow for obstacle avoidance <ref> (Coombs & Roberts 1992) </ref> (Santos-Victor, Sandini, Curotto & Garibaldi 1995). Only the flow in the direction of the normal to the image intensity gradient is computed. This flow is called normal flow or gradient-parallel optical flow, and it is the only flow component that can be computed locally.
Reference: <author> Crisman, J. D. </author> <year> (1991), </year> <title> Color Region Tracking for Vehicle Guidance, in `Active Vision', </title> <publisher> The MIT Press, </publisher> <address> Cambridge, MA, </address> <pages> pp. 1080-1085. </pages>
Reference-contexts: Motor commands are generated from this model. Another road-following project which relies on vision uses the SCARF classification algorithm <ref> (Crisman 1991) </ref>. The SCARF algorithm clusters image pixels according to some image property. This system is effective, but, like the UGV, can require intensive computation. Particular features, however, can be chosen for efficiency.
Reference: <author> Dickmanns, E. D., Mysliwetz, B. & Christians, T. </author> <year> (1990), </year> <title> `An Integrated Spatio-Temporal Approach to Automatic Visual Guidance of Autonomous Vehicles', </title> <journal> IEEE Transactions on Systems, Man, and Cybernetics 20(6), </journal> <pages> 1273-1284. </pages> <note> 69 Franceschini, </note> <author> N., Pichon, J. & Blanes, C. </author> <year> (1991), </year> <title> Real time visuomotor control: from flies to robots, </title> <booktitle> in `Fifth Int'l Conf. on Advanced Robotics', </booktitle> <address> Pisa, Italy. </address>
Reference-contexts: However, for particular platforms and applications, this temporal approach to depth perception is appropriate. Combined with spatial information, a temporal approach has been used in a successful road-following system which attains very high speeds and robustness to some environmental changes <ref> (Dickmanns, Mysliwetz & Christians 1990) </ref>. Related work in visually-guided robotics has used a simplification of optical flow for obstacle avoidance (Coombs & Roberts 1992) (Santos-Victor, Sandini, Curotto & Garibaldi 1995). Only the flow in the direction of the normal to the image intensity gradient is computed.
Reference: <author> Gavin, A. S. </author> <year> (1994), </year> <title> Low Computation Vision-Based Navigation For Mobile Robots, </title> <type> Master's Thesis, </type> <institution> Massachusetts Institute of Technology, </institution> <address> Cambridge, MA. </address>
Reference-contexts: Gavin worked on extending the approaches of lightweight vision to handle the case in which the environment is the surface of Mars <ref> (Gavin 1994) </ref>. He analyzed processing costs and hardware requirements for a set of separate vision routines.
Reference: <author> Gomi, T. </author> <year> (1995a), </year> <title> Autonomous Wheelchair Project, Applied AI Systems, </title> <publisher> Inc., </publisher> <address> Kanata, Ontario, Canada. </address>
Reference-contexts: The presence of well-defined corridors simplified the problem of determining the robot's direction. These principles are currently being incorporated into many commercial applications, including a semi-automatic visually guided wheelchair, an autonomous office messenger robot, and other systems <ref> (Gomi 1995a) </ref> (Gomi 1995b) (Gomi, Ide & Maheral 1995) (Gomi, Ide & Matsuo 1994). Gavin worked on extending the approaches of lightweight vision to handle the case in which the environment is the surface of Mars (Gavin 1994).
Reference: <author> Gomi, T. </author> <year> (1995b), </year> <title> A Highly Efficient Vision System for Fast Robot/Vehicle Navigation, </title> <booktitle> in `Proceedings, Second Asian Conf. on Computer Vision', </booktitle> <address> Singapore. </address>
Reference-contexts: The presence of well-defined corridors simplified the problem of determining the robot's direction. These principles are currently being incorporated into many commercial applications, including a semi-automatic visually guided wheelchair, an autonomous office messenger robot, and other systems (Gomi 1995a) <ref> (Gomi 1995b) </ref> (Gomi, Ide & Maheral 1995) (Gomi, Ide & Matsuo 1994). Gavin worked on extending the approaches of lightweight vision to handle the case in which the environment is the surface of Mars (Gavin 1994). He analyzed processing costs and hardware requirements for a set of separate vision routines.
Reference: <author> Gomi, T., Ide, K. & Maheral, P. </author> <year> (1995), </year> <title> Vision-based Navigation for an Office Messenger Robot, in `Volker Graefe, </title> <publisher> Elsevier Science, </publisher> <editor> editor, </editor> <booktitle> Intelligent Robots and Systems'. </booktitle>
Reference-contexts: The presence of well-defined corridors simplified the problem of determining the robot's direction. These principles are currently being incorporated into many commercial applications, including a semi-automatic visually guided wheelchair, an autonomous office messenger robot, and other systems (Gomi 1995a) (Gomi 1995b) <ref> (Gomi, Ide & Maheral 1995) </ref> (Gomi, Ide & Matsuo 1994). Gavin worked on extending the approaches of lightweight vision to handle the case in which the environment is the surface of Mars (Gavin 1994). He analyzed processing costs and hardware requirements for a set of separate vision routines.
Reference: <author> Gomi, T., Ide, K. & Matsuo, H. </author> <year> (1994), </year> <title> The Development of a Fully Autonmous Ground Vehicle (FAGV), </title> <booktitle> in `Intelligent Vehicles Symposium', </booktitle> <address> Paris, France. </address>
Reference-contexts: The presence of well-defined corridors simplified the problem of determining the robot's direction. These principles are currently being incorporated into many commercial applications, including a semi-automatic visually guided wheelchair, an autonomous office messenger robot, and other systems (Gomi 1995a) (Gomi 1995b) (Gomi, Ide & Maheral 1995) <ref> (Gomi, Ide & Matsuo 1994) </ref>. Gavin worked on extending the approaches of lightweight vision to handle the case in which the environment is the surface of Mars (Gavin 1994). He analyzed processing costs and hardware requirements for a set of separate vision routines.
Reference: <author> Hebert, M., Pomerleau, D., Stentz, A. & Thorpe, C. </author> <year> (1995), </year> <title> Computer Vision for Navigation: The CMU UGV Project, </title> <booktitle> in `Proceedings of the Workshop on Vision for Robots', </booktitle> <publisher> IEEE Computer Society Press, </publisher> <pages> pp. 97-102. </pages>
Reference-contexts: Inconsistencies are considered to be obstacles as they lie outside the ground plane. 2.3.5 Road and Off-Road Navigation A related project in outdoor navigation is the UGV (Unmanned Ground Vehicle) <ref> (Hebert, Pomerleau, Stentz & Thorpe 1995) </ref>. It maintains a "traversability map" 18 according to range data for obstacle avoidance, combines vision and a large neural network for road following, and chooses behaviors by a Gaussian distribution-based arbiter (Rosenblatt & Thorpe 1995). <p> The current work does not address the issue of higher-level navigation but for obstacle avoidance uses a combination of simpler but less reliable on-board algorithms to determine depth. Experiments with the Ratler robot, a prototype lunar rover, also use stereo vision algorithms for depth perception <ref> (Krotkov & Hebert 1995) </ref> (Simmons, Krotkov, Chrisman, Cozman, Goodwin, Hebert, Katragadda, Koenig, Krishnaswamy, Shinoda, Whittaker & Klarer 1995). The stereo component of the system produces terrain elevation maps which are used by the obstacle avoidance component for path planning.
Reference: <author> Horn, B. K. P. </author> <year> (1986), </year> <title> Robot Vision, </title> <publisher> MIT Press, </publisher> <address> Cambridge, MA. </address>
Reference-contexts: Optical flow is the use of temporal variations in image intensity patterns to extract information about relative motion. The optical flow field is a two-dimensional field of vectors (u; v) corresponding to the apparent motion of these patterns over a sequence of frames <ref> (Horn 1986) </ref>.
Reference: <author> Horn, B. K. P. & Weldon, E. J. </author> <year> (1988), </year> <title> `Direct Methods for Recovering Motion', </title> <booktitle> Int'l Journal of Computer Vision 2, </booktitle> <pages> 51-76. </pages>
Reference-contexts: While the first two limitations are inherent to optical flow, the computational difficulty can be reduced if the motion can be constrained <ref> (Horn & Weldon 1988) </ref>. Iterative approaches to acquiring depth from motion focus on general depth re 17 construction. One such approach uses Kalman filters (Matthies, Szeliski & Kanade 1988). <p> Specifically, approaches based on optical flow as discussed in 2.3.4 were considered. Optical Flow The initial approach of this work was to use an optical flow-based strategy for obstacle detection. Flow computation which is not correlation-based requires a large number of iterations for convergence, so a simplification derived in <ref> (Horn & Weldon 1988) </ref> to yield the depth directly without first determining the flow was tested.
Reference: <author> Horridge, G. A. </author> <year> (1986), </year> <title> A theory of insect vision: velocity parallax, </title> <journal> in `Proceedings of the Royal Society of London, B', </journal> <volume> Vol. 229, </volume> <pages> pp. 13-27. </pages>
Reference-contexts: Biological evidence motivates the use of normal flow for navigation. Bees navigate using specialized visual-sensing cells that can only detect brightness gradients along a particular orientation (Srinivasan, Lehrer, Kirchner & Zhang 1991) <ref> (Horridge 1986) </ref> (Franceschini, Pichon & Blanes 1991). Further, because the eyes of bees are aimed sideways, a bee is equidistant between obstacles to its right and to its left exactly when the normal flow is equal between its two eyes.
Reference: <author> Horswill, I. D. </author> <year> (1993a), </year> <title> Polly: A vision based Artificial Agent, </title> <booktitle> in `Proceedings of the Eleventh Nat'l Conf. on AI', </booktitle> <publisher> AAAI, MIT Press, </publisher> <pages> pp. 824-829. </pages>
Reference-contexts: Notice that the close rock on the right side of the image appears lower than the distant rocks. main can be predicted. The term specialization refers to the formal theory that supports these ideas. Specialization, as described by Horswill (Horswill 1993b) <ref> (Horswill 1993a) </ref>, is used in the development of the current system. Given some hypothesized general agent, one can derive an actual specialized agent that exhibits a target behavior (s) in a target environment (s). The derivation consists of a series of transformations. Each transformation is paired with an environmental property. <p> particular target remains at the center of its field of view, then the task of segmenting the target from the image is easier since its position is known. 2.3.3 Most Similar Visually-Guided Robotics The current system is most closely related to Horswill's research with the vision-based robot Polly (Horswill 1993b) <ref> (Horswill 1993a) </ref>. Polly's task was to roam the corridors and rooms of MIT's AI Laboratory giving tours to visitors. The entire system performed a variety of behaviors, from low-level obstacle avoidance and corridor-following to interacting with people. It relied on relatively simple algorithms, an approach termed "lightweight vision". <p> This idea of scanning up image slices is motivated by Horswill's work with the robot Polly <ref> (Horswill 1993a) </ref>. The difference is that, with Polly, "windows" were single pixels, and the "measure" was whether or not an edge of at least a particular image-dependent threshold was present. <p> A possible solution is an adaptive threshold, as used (but for a different purpose) in <ref> (Horswill 1993a) </ref>, but finding correct thresholds may be difficult as lighting inconsistencies are often present even within 52 a single image. Although giving reasonable results, greyscale intensity performed strictly worse than color histograms, so it is not recommended if color information is available.
Reference: <author> Horswill, I. D. </author> <year> (1993b), </year> <title> Specialization of Perceptual Processes, </title> <type> Ph.D. Thesis, </type> <institution> Mas-sachusetts Institute of Technology, </institution> <address> Cambridge, MA. 70 Horswill, I. D. </address> <year> (1995), </year> <title> `Visual routines and visual search: a real-time implementation and an automata-theoretic analysis', </title> <note> submitted to IJCAI. </note>
Reference-contexts: Notice that the close rock on the right side of the image appears lower than the distant rocks. main can be predicted. The term specialization refers to the formal theory that supports these ideas. Specialization, as described by Horswill <ref> (Horswill 1993b) </ref> (Horswill 1993a), is used in the development of the current system. Given some hypothesized general agent, one can derive an actual specialized agent that exhibits a target behavior (s) in a target environment (s). The derivation consists of a series of transformations. <p> that a particular target remains at the center of its field of view, then the task of segmenting the target from the image is easier since its position is known. 2.3.3 Most Similar Visually-Guided Robotics The current system is most closely related to Horswill's research with the vision-based robot Polly <ref> (Horswill 1993b) </ref> (Horswill 1993a). Polly's task was to roam the corridors and rooms of MIT's AI Laboratory giving tours to visitors. The entire system performed a variety of behaviors, from low-level obstacle avoidance and corridor-following to interacting with people. It relied on relatively simple algorithms, an approach termed "lightweight vision".
Reference: <author> IS Robotics, I. </author> <year> (1995), </year> <title> The Pebbles III Robot, User Manual, </title> <note> Version 2.0. </note>
Reference-contexts: The SCARF algorithm clusters image pixels according to some image property. This system is effective, but, like the UGV, can require intensive computation. Particular features, however, can be chosen for efficiency. For example, a more recent system uses one-dimensional "categorical" color instead of three-dimensional RGB color <ref> (Zeng & Crisman 1995) </ref>. A behavior-based off-road navigation system is described by Langer, Rosenblatt & Hebert (1994). This system uses range-data for depth perception and operates on a military vehicle. The task is not higher-level navigation but is to avoid untraversable regions while traveling in rough, unmapped terrain. <p> The stereo component of the system produces terrain elevation maps which are used by the obstacle avoidance component for path planning. These experiments have been very effective at obstacle avoidance but require much compute power and off-board image processing. Work at Jet Propulsion Laboratories <ref> (Matthies, Gat, Harrison, Wilcox, Volpe & Litwin 1995) </ref> on Mars rover navigation focuses on reliable user-assisted navigation for upcoming Mars exploration missions. Their system does not use passive vision; 20 rather it uses active triangulation from a light-striper, in conjunction with several other sensors for reliability. <p> The chapter then analyzes the running time and concludes with remarks on the overall design. 3.1 Platform The system is housed in the Pebbles III Robot, pictured in Figure 3-2, designed and built by IS Robotics, Inc. <ref> (IS Robotics 1995) </ref> 1 . Pebbles' sturdy design, especially the 1 Some hardware modifications have been made to Pebbles for this project, including repositioning the camera. 23 tracked rather than wheeled base, makes it well-suited for travel in rough terrain.
Reference: <author> Krotkov, E. & Hebert, M. </author> <year> (1995), </year> <title> Mapping and Positioning for a Prototype Lunar Rover, </title> <booktitle> in `Proceedings of IEEE Int'l Conf. on Robotics and Automation', </booktitle> <pages> pp. 2913-2919. </pages>
Reference-contexts: The current work does not address the issue of higher-level navigation but for obstacle avoidance uses a combination of simpler but less reliable on-board algorithms to determine depth. Experiments with the Ratler robot, a prototype lunar rover, also use stereo vision algorithms for depth perception <ref> (Krotkov & Hebert 1995) </ref> (Simmons, Krotkov, Chrisman, Cozman, Goodwin, Hebert, Katragadda, Koenig, Krishnaswamy, Shinoda, Whittaker & Klarer 1995). The stereo component of the system produces terrain elevation maps which are used by the obstacle avoidance component for path planning.
Reference: <author> Krotkov, E. P. </author> <year> (1989), </year> <title> Active Computer Vision by Cooperative Focus and Stereo, </title> <publisher> Springer-Verlag New York, Inc. </publisher>
Reference-contexts: However, the current system does not combine the elementary operations according to the VRP model. 2.3.2 Animate Vision A related strategy in the construction of computer vision systems is animate vision <ref> (Krotkov 1989) </ref> (Ballard 1991) (Coombs 1992) (Aloimonos 1993). Animate vision is the strategy of controlling the camera view based on previous visual sensing so 15 that subsequent computations may be simplified. Such systems therefore normally include a gaze control mechanism (Pahlavan & Eklundh 1992).
Reference: <author> Langer, D., Rosenblatt, J. K. & Hebert, M. </author> <year> (1994), </year> <title> `A Behavior-Based System for OffRoad Navigation', </title> <journal> IEEE Transactions on Robotics and Automation 10(6), </journal> <pages> 776-783. </pages>
Reference: <author> Lynn, S. W. </author> <year> (1994), </year> <title> A Motion Vision System for a Martian Micro-Rover, in `Draper Laboratory CSDL-T-1197', </title> <type> Master's Thesis, </type> <institution> Massachusetts Institute of Technology. </institution>
Reference-contexts: This current research in passive vision can potentially be used to reduce the energy consumption on future Mars rovers. At Draper Laboratories, Lynn explored the choice of a one-camera system using correlation-based depth from motion for a Mars Micro-Rover robot <ref> (Lynn 1994) </ref>. Correlation was implemented in hardware for efficiency, and he compared this approach with gradient-based optical flow. He concludes that the pattern matching approach offers equally good results as a gradient-based flow method at a fraction of the computational cost.
Reference: <author> Mahoney, J. V. </author> <year> (1993), </year> <title> Elements of signal geometry, </title> <type> Technical report, </type> <institution> XEROX Palo Alto Research Center. </institution>
Reference-contexts: Mahoney has defined and implemented a visual programming language in which elementary operations are combined to form visual routines <ref> (Mahoney 1993) </ref> (Mahoney forthcoming). His language uses the operations suggested by Ullman as well as additional operations and has been used for document processing. The first implementation of visual routines theory on real-time video is discussed in (Horswill 1995).
Reference: <author> Mahoney, J. V. </author> <title> (forthcoming), Signal-based figure/ground separation, </title> <type> Technical report, </type> <institution> XEROX Palo Alto Research Center. </institution>
Reference: <author> Matthies, L., Gat, E., Harrison, R., Wilcox, B., Volpe, R. & Litwin, T. </author> <year> (1995), </year> <title> Mars Microrover Navigation: Performance Evaluation and Enhancement, </title> <booktitle> in `Proceedings of IEEE Int'l Conf. on Intelligent Robots and Systems', </booktitle> <volume> Vol. 1, </volume> <pages> pp. 433-440. </pages>
Reference-contexts: The stereo component of the system produces terrain elevation maps which are used by the obstacle avoidance component for path planning. These experiments have been very effective at obstacle avoidance but require much compute power and off-board image processing. Work at Jet Propulsion Laboratories <ref> (Matthies, Gat, Harrison, Wilcox, Volpe & Litwin 1995) </ref> on Mars rover navigation focuses on reliable user-assisted navigation for upcoming Mars exploration missions. Their system does not use passive vision; 20 rather it uses active triangulation from a light-striper, in conjunction with several other sensors for reliability.
Reference: <author> Matthies, L., Szeliski, R. & Kanade, T. </author> <year> (1988), </year> <title> Kalman Filter-based Algorithms for Estimating Depth from Image Sequences, </title> <institution> in `Carnegie Mellon University, The Robotics Institute, </institution> <note> Technical Report CMU-RI-TR-88-1'. </note>
Reference-contexts: While the first two limitations are inherent to optical flow, the computational difficulty can be reduced if the motion can be constrained (Horn & Weldon 1988). Iterative approaches to acquiring depth from motion focus on general depth re 17 construction. One such approach uses Kalman filters <ref> (Matthies, Szeliski & Kanade 1988) </ref>. The system described by this work differs in that it maintains little or no information from previous frames whereas the Kalman filter system stores previously computed depth maps and associated uncertainty values.
Reference: <author> Pagnot, R. & Grandjean, P. </author> <year> (1995), </year> <title> Fast Cross-Country Navigation on Fair Terrains, </title> <booktitle> in `Proceedings of IEEE Int'l Conf. on Robotics and Automation', </booktitle> <pages> pp. 2593-2598. </pages> <note> 71 Pahlavan, </note> <author> K. & Eklundh, J. </author> <year> (1992), </year> <title> `A Head-Eye System Analysis and Design', CVGIP: </title> <booktitle> Image Understanding 56, </booktitle> <pages> 41-56. </pages>
Reference: <author> Payton, D., Rosenblatt, J. & Kersey, D. </author> <year> (1990), </year> <title> `Plan Guided Reaction', </title> <journal> IEEE Transactions on Systems, Man, and Cybernetics 1(6), </journal> <pages> 1370-1382. </pages>
Reference-contexts: Following this approach, the current design has no central planner and consists of two layers of behaviors. Since the advent of the subsumption architecture, several related approaches have been investigated. One example advocates minimizing the information loss that occurs between layers of a subsumption architecture-based system <ref> (Payton, Rosenblatt & Kersey 1990) </ref> (Rosenblatt & Payton 1989). Toward this end, "fine-grained behaviors" are described which minimize the amount of processing obscured by each module, making intermediate processing steps available outside of the given module.
Reference: <author> Rosenblatt, J. & Payton, D. </author> <year> (1989), </year> <title> A Fine-Grained Alternative to the Subsumption Architecture for Mobile Robot Control, </title> <booktitle> in `Proceedings of IEEE Int'l Joint Conf. on Neural Networks', </booktitle> <volume> Vol. 2, </volume> <pages> pp. 317-323. </pages>
Reference-contexts: Since the advent of the subsumption architecture, several related approaches have been investigated. One example advocates minimizing the information loss that occurs between layers of a subsumption architecture-based system (Payton, Rosenblatt & Kersey 1990) <ref> (Rosenblatt & Payton 1989) </ref>. Toward this end, "fine-grained behaviors" are described which minimize the amount of processing obscured by each module, making intermediate processing steps available outside of the given module. The current system uses this idea in combining the outputs of the various vision modules. <p> Instead of viewing the modules as indivisible units that process all the way to the motor commands, the system allows the intermediate computation of the depth array to be available to the merging procedure. This strategy is an example of "fine-grained behaviors" <ref> (Rosenblatt & Payton 1989) </ref>, since the depth arrays computed by each module are available to the merging procedure instead of only motor commands which could be generated from these individual depth arrays. 3.4 From Depth Arrays to Motor Commands The final processing step for each image is the determination of the
Reference: <author> Rosenblatt, J. & Thorpe, C. </author> <year> (1995), </year> <title> Combining Multiple Goals in a Behavior-Based Architecture, </title> <booktitle> in `Proceedings of IEEE Int'l Conf. on Intelligent Robots and Systems', </booktitle> <volume> Vol. 1, </volume> <pages> pp. 136-141. </pages>
Reference-contexts: It maintains a "traversability map" 18 according to range data for obstacle avoidance, combines vision and a large neural network for road following, and chooses behaviors by a Gaussian distribution-based arbiter <ref> (Rosenblatt & Thorpe 1995) </ref>. The Autonomous Land Vehicle (ALV) project is a large-scale project that combines advanced hardware architectures, computer vision, autonomous navigation, and other areas of artificial intelligence (Turk, Morgenthaler, Gremban & Marra 1988).
Reference: <author> Santos-Victor, J. & Sandini, G. </author> <year> (1995), </year> <title> Uncalibrated Obstacle Detection using Normal Flow. </title>
Reference: <author> Santos-Victor, J., Sandini, G., Curotto, F. & Garibaldi, S. </author> <year> (1995), </year> <title> `Divergent Stereo in Autonomous Navigation: From Bees to Robots', </title> <journal> Int'l Journal of Computer Vision pp. </journal> <pages> 159-177. </pages>
Reference: <author> Simmons, R., Krotkov, E., Chrisman, L., Cozman, F., Goodwin, R., Hebert, M., Katragadda, L., Koenig, S., Krishnaswamy, G., Shinoda, Y., Whittaker, W. & Klarer, P. </author> <year> (1995), </year> <title> `Experience with Rover Navigation for Lunar-Like Terrains', </title> <booktitle> Proceedings of IEEE Int'l Conf. on Intelligent Robots and Systems 1, </booktitle> <pages> 441-446. </pages>
Reference: <author> Smithers, T. </author> <year> (1995), </year> <title> What the Dynamics of Adaptive Behavior and Cognition Might Look Like in Agent-Environment Interaction Systems, </title> <booktitle> in `Practice and Future of Autonomous Agents', </booktitle> <volume> Vol. 2, </volume> <pages> pp. 0-27. </pages>
Reference-contexts: That is, three criteria must be met: 1) the agents, environments, and interactions have identifiable states, 2) state changes are continuous, and 3) such changes are captured by a set of (non-linear) functions that define a vector field over the state space <ref> (Smithers 1995) </ref>. The work discussed above is independent of the sensor used by a system.
Reference: <author> Srinivasan, M. V., Lehrer, M., Kirchner, W. H. & Zhang, S. W. </author> <year> (1991), </year> <title> `Range perception through apparent image speed in freely flying honeybees', </title> <journal> Visual Neuroscience 6, </journal> <pages> 519-535. </pages> <note> 72 Turk, </note> <author> M. A., Morgenthaler, D. G., Gremban, K. D. & Marra, M. </author> <year> (1988), </year> <month> `VITS </month> - 
Reference-contexts: This flow is called normal flow or gradient-parallel optical flow, and it is the only flow component that can be computed locally. Biological evidence motivates the use of normal flow for navigation. Bees navigate using specialized visual-sensing cells that can only detect brightness gradients along a particular orientation <ref> (Srinivasan, Lehrer, Kirchner & Zhang 1991) </ref> (Horridge 1986) (Franceschini, Pichon & Blanes 1991). Further, because the eyes of bees are aimed sideways, a bee is equidistant between obstacles to its right and to its left exactly when the normal flow is equal between its two eyes.
References-found: 39

