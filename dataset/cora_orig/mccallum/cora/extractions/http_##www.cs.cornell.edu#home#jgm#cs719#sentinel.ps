URL: http://www.cs.cornell.edu/home/jgm/cs719/sentinel.ps
Refering-URL: http://www.cs.cornell.edu/home/jgm/cs719/cs719.html
Root-URL: 
Title: Sentinel Scheduling for VLIW and Superscalar Processors  
Author: Scott A. Mahlke William Y. Chen Wen-mei W. Hwu B. Ramakrishna Rau Michael S. Schlansker 
Address: IL 61801  Palo Alto, CA 94303  
Affiliation: Center for Reliable and High-Performance Computing University of Illinois Urbana-Champaign,  Hewlett Packard Laboratories  
Date: October 1992 1  
Note: To appear in ASPLOS-V Conference Proceedings,  
Abstract: Speculative execution is an important source of parallelism for VLIW and superscalar processors. A serious challenge with compiler-controlled speculative execution is to accurately detect and report all program execution errors at the time of occurrence. In this paper, a set of architectural features and compile-time scheduling support referred to as sentinel scheduling is introduced. Sentinel scheduling provides an effective framework for compiler-controlled speculative execution that accurately detects and reports all exceptions. Sentinel scheduling also supports speculative execution of store instructions by providing a store buffer which allows probationary entries. Experimental results show that sentinel scheduling is highly effective for a wide range of VLIW and superscalar processors. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> B. R. Rau and C. D. Glaeser, </author> <title> "Some scheduling techniques and an easily schedulable horizontal architecture for high performance scientific computing," </title> <booktitle> in Proceedings of the 20th Annual Workshop on Microprogramming and Microarchitecture, </booktitle> <pages> pp. 183-198, </pages> <month> October </month> <year> 1981. </year>
Reference-contexts: 1 Introduction Instruction level parallelism (ILP) within basic blocks is extremely limited. An effective VLIW or superscalar machine must schedule instructions across basic block boundaries to achieve higher performance. When branch conditions may be determined early, scheduling techniques such as software pipelining <ref> [1] </ref> [2] [3] are effective for exposing ILP. Also, predicated instructions can be used in conjunction with software pipeline loop scheduling [4] or straight-line code scheduling [5] to mask out the effects of unnecessary instructions from alternate paths of control. <p> An efficient structure to perform scheduling across basic blocks is a superblock. All scheduling techniques in this paper will be described based on the superblock structure, however they can be easily generalized to other structures. For example, trace scheduling [9], modulo scheduling <ref> [1] </ref>, and enhanced pipelining [10] may effectively utilize the speculative execution models discussed in this paper. Tirumalai et al. showed that modulo scheduling of while loops depend on speculative support to achieve high performance [7]. <p> Since instruction F is not speculatively executed, an explicit sentinel is not inserted for it. In the final schedule, instructions F and G serve as sentinels for A: if (r2==0) goto L1 fl B <ref> [1] </ref>: r1 = mem (r2+0) B: r1 = mem (r2+0) fl C [1]: r3 = mem (r4+0) D: r4 = r1+1 fl E [2]: r5 = r3fi9 y E: r5 = r3fi9 A [3]: if (r2==0) goto L1 y F: mem (r2+4) = r4 z F [3]: mem (r2+0) = r4 <p> Since instruction F is not speculatively executed, an explicit sentinel is not inserted for it. In the final schedule, instructions F and G serve as sentinels for A: if (r2==0) goto L1 fl B <ref> [1] </ref>: r1 = mem (r2+0) B: r1 = mem (r2+0) fl C [1]: r3 = mem (r4+0) D: r4 = r1+1 fl E [2]: r5 = r3fi9 y E: r5 = r3fi9 A [3]: if (r2==0) goto L1 y F: mem (r2+4) = r4 z F [3]: mem (r2+0) = r4 z G [3]: check exception (r5) y unprotected instruction fl speculative instruction <p> Also, this restriction has to be enforced for both register and mem ory operands. The code scheduler can perform renaming transformations to overcome restrictions 3 and 4 for speculative code motion. A typical application is to convert an increment of a register A: jsr A <ref> [1] </ref>: jsr C: if (r5==0) goto L1 B [2]: r5 = mem (r3+0) y D: r1 = mem (r6+0) E 0 [2]: r10 = r2+1 E: r2 = r2+1 C [3]: if (r5==0) goto L1 F: mem (r4+0) = r7 z G [4]: r8 = r1+1 H: r9 = mem (r2+0)
Reference: [2] <author> M. S. Lam, </author> <title> "Software pipelining: An effective scheduling technique for VLIW machines," </title> <booktitle> in Proceedings of the ACM SIGPLAN 1988 Conference on Programming Language Design and Implementation, </booktitle> <pages> pp. 318-328, </pages> <month> June </month> <year> 1988. </year>
Reference-contexts: 1 Introduction Instruction level parallelism (ILP) within basic blocks is extremely limited. An effective VLIW or superscalar machine must schedule instructions across basic block boundaries to achieve higher performance. When branch conditions may be determined early, scheduling techniques such as software pipelining [1] <ref> [2] </ref> [3] are effective for exposing ILP. Also, predicated instructions can be used in conjunction with software pipeline loop scheduling [4] or straight-line code scheduling [5] to mask out the effects of unnecessary instructions from alternate paths of control. <p> In the final schedule, instructions F and G serve as sentinels for A: if (r2==0) goto L1 fl B [1]: r1 = mem (r2+0) B: r1 = mem (r2+0) fl C [1]: r3 = mem (r4+0) D: r4 = r1+1 fl E <ref> [2] </ref>: r5 = r3fi9 y E: r5 = r3fi9 A [3]: if (r2==0) goto L1 y F: mem (r2+4) = r4 z F [3]: mem (r2+0) = r4 z G [3]: check exception (r5) y unprotected instruction fl speculative instruction z sentinel [n] indicates in which cycle the instruction is executed <p> The code scheduler can perform renaming transformations to overcome restrictions 3 and 4 for speculative code motion. A typical application is to convert an increment of a register A: jsr A [1]: jsr C: if (r5==0) goto L1 B <ref> [2] </ref>: r5 = mem (r3+0) y D: r1 = mem (r6+0) E 0 [2]: r10 = r2+1 E: r2 = r2+1 C [3]: if (r5==0) goto L1 F: mem (r4+0) = r7 z G [4]: r8 = r1+1 H: r9 = mem (r2+0) H 0 [5]: r9 = mem (r10+0) y <p> A typical application is to convert an increment of a register A: jsr A [1]: jsr C: if (r5==0) goto L1 B <ref> [2] </ref>: r5 = mem (r3+0) y D: r1 = mem (r6+0) E 0 [2]: r10 = r2+1 E: r2 = r2+1 C [3]: if (r5==0) goto L1 F: mem (r4+0) = r7 z G [4]: r8 = r1+1 H: r9 = mem (r2+0) H 0 [5]: r9 = mem (r10+0) y instruction considered for speculative execution z sentinel for D [n] indicates in which
Reference: [3] <author> A. Aiken and A. Nicolau, </author> <title> "Optimal loop paralleliza-tion," </title> <booktitle> in Proceedings of the ACM SIGPLAN 1988 Conference on Programming Language Design and Implementation, </booktitle> <pages> pp. 308-317, </pages> <month> June </month> <year> 1988. </year>
Reference-contexts: 1 Introduction Instruction level parallelism (ILP) within basic blocks is extremely limited. An effective VLIW or superscalar machine must schedule instructions across basic block boundaries to achieve higher performance. When branch conditions may be determined early, scheduling techniques such as software pipelining [1] [2] <ref> [3] </ref> are effective for exposing ILP. Also, predicated instructions can be used in conjunction with software pipeline loop scheduling [4] or straight-line code scheduling [5] to mask out the effects of unnecessary instructions from alternate paths of control. <p> final schedule, instructions F and G serve as sentinels for A: if (r2==0) goto L1 fl B [1]: r1 = mem (r2+0) B: r1 = mem (r2+0) fl C [1]: r3 = mem (r4+0) D: r4 = r1+1 fl E [2]: r5 = r3fi9 y E: r5 = r3fi9 A <ref> [3] </ref>: if (r2==0) goto L1 y F: mem (r2+4) = r4 z F [3]: mem (r2+0) = r4 z G [3]: check exception (r5) y unprotected instruction fl speculative instruction z sentinel [n] indicates in which cycle the instruction is executed (a) (b) instructions B, C, D, and E. <p> goto L1 fl B [1]: r1 = mem (r2+0) B: r1 = mem (r2+0) fl C [1]: r3 = mem (r4+0) D: r4 = r1+1 fl E [2]: r5 = r3fi9 y E: r5 = r3fi9 A <ref> [3] </ref>: if (r2==0) goto L1 y F: mem (r2+4) = r4 z F [3]: mem (r2+0) = r4 z G [3]: check exception (r5) y unprotected instruction fl speculative instruction z sentinel [n] indicates in which cycle the instruction is executed (a) (b) instructions B, C, D, and E. <p> mem (r2+0) B: r1 = mem (r2+0) fl C [1]: r3 = mem (r4+0) D: r4 = r1+1 fl E [2]: r5 = r3fi9 y E: r5 = r3fi9 A <ref> [3] </ref>: if (r2==0) goto L1 y F: mem (r2+4) = r4 z F [3]: mem (r2+0) = r4 z G [3]: check exception (r5) y unprotected instruction fl speculative instruction z sentinel [n] indicates in which cycle the instruction is executed (a) (b) instructions B, C, D, and E. An execution sequence for the scheduled code segment in which instruction B causes an exception is shown in Figure 2. <p> A typical application is to convert an increment of a register A: jsr A [1]: jsr C: if (r5==0) goto L1 B [2]: r5 = mem (r3+0) y D: r1 = mem (r6+0) E 0 [2]: r10 = r2+1 E: r2 = r2+1 C <ref> [3] </ref>: if (r5==0) goto L1 F: mem (r4+0) = r7 z G [4]: r8 = r1+1 H: r9 = mem (r2+0) H 0 [5]: r9 = mem (r10+0) y instruction considered for speculative execution z sentinel for D [n] indicates in which cycle the instruction is executed, each instruction requires 1
Reference: [4] <author> B. R. Rau, D. W. L. Yen, W. Yen, and R. A. Towle, </author> <title> "The Cydra 5 departmental supercomputer," </title> <booktitle> IEEE Computer, </booktitle> <pages> pp. 12-35, </pages> <month> January </month> <year> 1989. </year>
Reference-contexts: When branch conditions may be determined early, scheduling techniques such as software pipelining [1] [2] [3] are effective for exposing ILP. Also, predicated instructions can be used in conjunction with software pipeline loop scheduling <ref> [4] </ref> or straight-line code scheduling [5] to mask out the effects of unnecessary instructions from alternate paths of control. For applications in which branch conditions cannot be determined early, speculative execution of instructions is an important source of ILP [6] [7] [8]. <p> The pc of I can be obtained from a PC History Queue which keeps a record of the last m pc values to enable reporting exceptions with non-uniform latency function units [11] <ref> [4] </ref>. If one or more of the source register exception tags of I are set, an exception propagation occurs. This is independent of whether I causes an exception or not. <p> an increment of a register A: jsr A [1]: jsr C: if (r5==0) goto L1 B [2]: r5 = mem (r3+0) y D: r1 = mem (r6+0) E 0 [2]: r10 = r2+1 E: r2 = r2+1 C [3]: if (r5==0) goto L1 F: mem (r4+0) = r7 z G <ref> [4] </ref>: r8 = r1+1 H: r9 = mem (r2+0) H 0 [5]: r9 = mem (r10+0) y instruction considered for speculative execution z sentinel for D [n] indicates in which cycle the instruction is executed, each instruction requires 1 cycle to execute (a) (b) (a) Original program segment. (b) Program segment
Reference: [5] <author> P. Y. T. Hsu and E. S. Davidson, </author> <title> "Highly concurrent scalar processing," </title> <booktitle> in Proceedings of the 13th International Symposium on Computer Architecture, </booktitle> <pages> pp. 386-395, </pages> <month> June </month> <year> 1986. </year>
Reference-contexts: When branch conditions may be determined early, scheduling techniques such as software pipelining [1] [2] [3] are effective for exposing ILP. Also, predicated instructions can be used in conjunction with software pipeline loop scheduling [4] or straight-line code scheduling <ref> [5] </ref> to mask out the effects of unnecessary instructions from alternate paths of control. For applications in which branch conditions cannot be determined early, speculative execution of instructions is an important source of ILP [6] [7] [8]. <p> if (r5==0) goto L1 B [2]: r5 = mem (r3+0) y D: r1 = mem (r6+0) E 0 [2]: r10 = r2+1 E: r2 = r2+1 C [3]: if (r5==0) goto L1 F: mem (r4+0) = r7 z G [4]: r8 = r1+1 H: r9 = mem (r2+0) H 0 <ref> [5] </ref>: r9 = mem (r10+0) y instruction considered for speculative execution z sentinel for D [n] indicates in which cycle the instruction is executed, each instruction requires 1 cycle to execute (a) (b) (a) Original program segment. (b) Program segment after scheduling and transformation. into two instructions, one addition that writes
Reference: [6] <author> M. D. Smith, M. S. Lam, and M. A. Horowitz, </author> <title> "Boosting beyond static scheduling in a superscalar processor," </title> <booktitle> in Proceedings of the 17th International Symposium on Computer Architecture, </booktitle> <pages> pp. 344-354, </pages> <month> May </month> <year> 1990. </year>
Reference-contexts: For applications in which branch conditions cannot be determined early, speculative execution of instructions is an important source of ILP <ref> [6] </ref> [7] [8]. Speculative execution refers to executing an instruction before knowing that its execution is required. Such an instruction will be referred to as a speculative instruction. Speculative execution may either be engineered at run-time using dynamic scheduling or at compile-time. <p> The limiting factor of restricted percolation is the inability to move potential trap-causing instructions with long latency, such as load instructions, above branches. 2.3 Instruction Boosting Scheduling Model The scheduler enforces neither restriction when using the instruction boosting scheduling model <ref> [6] </ref>. The restrictions are overcome by providing sufficient hardware storage to buffer results until the branches an instruction moved past are committed. If all branches are found to be correctly predicted, the machine state is updated by the boosted instructions' effects.
Reference: [7] <author> P. Tirumalai, M. Lee, and M. Schlansker, </author> <title> "Paralleliza-tion of loops with exits on pipelined architectures," </title> <booktitle> in Proceedings of Supercomputing '90, </booktitle> <month> November </month> <year> 1990. </year>
Reference-contexts: For applications in which branch conditions cannot be determined early, speculative execution of instructions is an important source of ILP [6] <ref> [7] </ref> [8]. Speculative execution refers to executing an instruction before knowing that its execution is required. Such an instruction will be referred to as a speculative instruction. Speculative execution may either be engineered at run-time using dynamic scheduling or at compile-time. <p> For example, trace scheduling [9], modulo scheduling [1], and enhanced pipelining [10] may effectively utilize the speculative execution models discussed in this paper. Tirumalai et al. showed that modulo scheduling of while loops depend on speculative support to achieve high performance <ref> [7] </ref>. Without speculative support, dependences limit the amount of execution overlap between loop iterations. 2.1 Superblock Scheduling Superblock scheduling is an extension of trace scheduling [9] which reduces some of the bookkeeping complexity [8].
Reference: [8] <author> P. P. Chang, S. A. Mahlke, W. Y. Chen, N. J. Warter, and W. W. Hwu, </author> <title> "IMPACT: An architectural framework for multiple-instruction-issue processors," </title> <booktitle> in Proceedings of the 18th International Symposium on Computer Architecture, </booktitle> <pages> pp. 266-275, </pages> <month> May </month> <year> 1991. </year>
Reference-contexts: For applications in which branch conditions cannot be determined early, speculative execution of instructions is an important source of ILP [6] [7] <ref> [8] </ref>. Speculative execution refers to executing an instruction before knowing that its execution is required. Such an instruction will be referred to as a speculative instruction. Speculative execution may either be engineered at run-time using dynamic scheduling or at compile-time. <p> Without speculative support, dependences limit the amount of execution overlap between loop iterations. 2.1 Superblock Scheduling Superblock scheduling is an extension of trace scheduling [9] which reduces some of the bookkeeping complexity <ref> [8] </ref>. A superblock is a block of instructions in which control may only enter from the top but may leave at one or more exit points. Superblock scheduling consists of two steps, dependence graph construction and list scheduling. <p> appropriate control dependences are eliminated according to the model used, list scheduling using the dependence graph, instruction latencies, and resource constraints is performed to determine which instructions are scheduled together. 2.2 Restricted Percolation Scheduling Model The scheduler enforces both restrictions (1) and (2) when using the restricted percolation scheduling model <ref> [8] </ref>. Thus, only instructions which the compiler can guarantee to never cause execution-altering exceptions are candidates for speculative code motion. For conventional processors, memory load, memory store, integer divide, and all floating point instructions are potential trap-causing instructions. <p> An exception that occurred for a boosted instruction whose result is thrown away is ignored. 2.4 Ignoring Exceptions with the General Per colation Scheduling Model The scheduler removes restriction (2) using the general percolation model <ref> [8] </ref>. Exceptions that may alter program execution are avoided by converting all speculative instructions which potentially cause traps into non-trapping or silent versions of those instructions. Memory stores, though, are not allowed to be speculative instructions. <p> However, the hardware overhead is very large, and the number of branches an instruction can be boosted above is limited to a small number. General percolation, on the other hand, achieves nearly the same performance level of instruction boosting <ref> [8] </ref> with a much lower implementation cost. The problem is that there is no guarantee of detecting exceptions and determining the cause of an exception. In the next section, a new scheduling model referred to as sentinel scheduling is introduced. <p> The IMPACT-I compiler is a prototype optimizing compiler designed to generate efficient code for VLIW and superscalar processors <ref> [8] </ref>. A superblock is the basic scope for the instruction scheduler. The instruction scheduler takes as an input a machine description file that characterizes the instruction set, the mi-croarchitecture (including the number of instructions that can be fetched/issued in a cycle and the instruction latencies), and the code scheduling model.
Reference: [9] <author> J. A. Fisher, </author> <title> "Trace scheduling: A technique for global microcode compaction," </title> <journal> IEEE Transactions on Computers, </journal> <volume> vol. c-30, </volume> <pages> pp. 478-490, </pages> <month> July </month> <year> 1981. </year>
Reference-contexts: An efficient structure to perform scheduling across basic blocks is a superblock. All scheduling techniques in this paper will be described based on the superblock structure, however they can be easily generalized to other structures. For example, trace scheduling <ref> [9] </ref>, modulo scheduling [1], and enhanced pipelining [10] may effectively utilize the speculative execution models discussed in this paper. Tirumalai et al. showed that modulo scheduling of while loops depend on speculative support to achieve high performance [7]. <p> Tirumalai et al. showed that modulo scheduling of while loops depend on speculative support to achieve high performance [7]. Without speculative support, dependences limit the amount of execution overlap between loop iterations. 2.1 Superblock Scheduling Superblock scheduling is an extension of trace scheduling <ref> [9] </ref> which reduces some of the bookkeeping complexity [8]. A superblock is a block of instructions in which control may only enter from the top but may leave at one or more exit points. Superblock scheduling consists of two steps, dependence graph construction and list scheduling.
Reference: [10] <author> K. Ebcioglu, </author> <title> "A compilation technique for software pipelining of loops with conditional jumps," </title> <booktitle> in Proceedings of the 20th Annual Workshop on Microprogramming and Microarchitecture, </booktitle> <pages> pp. 69-79, </pages> <month> December </month> <year> 1987. </year>
Reference-contexts: An efficient structure to perform scheduling across basic blocks is a superblock. All scheduling techniques in this paper will be described based on the superblock structure, however they can be easily generalized to other structures. For example, trace scheduling [9], modulo scheduling [1], and enhanced pipelining <ref> [10] </ref> may effectively utilize the speculative execution models discussed in this paper. Tirumalai et al. showed that modulo scheduling of while loops depend on speculative support to achieve high performance [7].
Reference: [11] <author> R. P. Colwell, R. P. Nix, J. J. O'Donnell, D. B. Pa-pworth, and P. K. Rodman, </author> <title> "A VLIW architecture for a trace scheduling compiler," </title> <booktitle> in Proceedings of the 2nd International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pp. 180-192, </pages> <month> April </month> <year> 1987. </year>
Reference-contexts: The inability to always detect exceptions and determine the excepting instruction limits the application of this scheduling model. Colwell et al. detect some exceptions by writing NaN into the destination register of any non-trapping instruction which produces an exception <ref> [11] </ref>. The use of NaN is then signaled by any trapping instruction. This method, however, has difficulties determining the original excepting instruction, and is not guaranteed to signal an exception if the result of a speculative exception-causing instruction is conditionally used. <p> The pc of I can be obtained from a PC History Queue which keeps a record of the last m pc values to enable reporting exceptions with non-uniform latency function units <ref> [11] </ref> [4]. If one or more of the source register exception tags of I are set, an exception propagation occurs. This is independent of whether I causes an exception or not.
Reference: [12] <author> A. Aho, R. Sethi, and J. Ullman, </author> <booktitle> Compilers: Principles, Techniques, and Tools. </booktitle> <address> Reading, MA: </address> <publisher> Addison-Wesley, </publisher> <year> 1986. </year>
Reference-contexts: Registers which are not defined may have their exception tag set. The use of this register will therefore lead to an immediate or eventual exception signal. However, this exception should not be reported. To prevent an exception from occurring with uninitialized registers, the compiler performs live variable analysis <ref> [12] </ref>, and inserts additional instructions to reset the exception tags of the corresponding registers before they are used. 3.6 Reporting Multiple Exceptions Multiple exceptions in a program are handled efficiently with sentinel scheduling. The exceptions can either occur within different basic blocks or within the same basic block.
Reference: [13] <author> S. A. Mahlke, W. Y. Chen, W. W. Hwu, B. R. Rau, and M. S. Schlansker, </author> <title> "Exception recovery for systems with compiler-controlled speculative execution," </title> <type> tech. rep., </type> <institution> Center for Reliable and High-Performance Computing, University of Illinois, Urbana, IL, </institution> <note> in preparation 1992. </note>
Reference-contexts: The following technique is informally described in order to demonstrate that exception retry is possible with sentinel scheduling. A more complete treatment of this technique is presented in <ref> [13] </ref>. A restartable instruction sequence is a list of consecutive instructions that satisfy the following two constraints. First, none of the instructions in the sequence may cause irreversible side effects that prevent re-execution of any instructions in the sequence.
Reference: [14] <author> W. M. Johnson, </author> <title> Superscalar Microprocessor Design. </title> <address> Englewood Cliffs, NJ: </address> <publisher> Prentice-Hall, Inc., </publisher> <year> 1991. </year>
Reference-contexts: In this discussion, it will be assumed that an N entry store buffer exists between the CPU and the data cache <ref> [14] </ref>. Operation of a Conventional Store Buffer. A store buffer has three primary functions. First, it creates a new entry for each store instruction executed by the CPU. Each store buffer entry consists of the store address, store data, and several status bits.
References-found: 14

