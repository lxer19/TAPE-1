URL: http://www.isi.edu/natural-language/mt/skip.ps
Refering-URL: http://www.isi.edu/natural-language/nlg-publications.html
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Email: kyamada@isi.edu  
Title: A Controlled Skip Parser  
Author: Kenji Yamada 
Affiliation: USC/Information Sciences Institute  
Abstract: Real-world natural language sentences are long and complex, and always contain unexpected grammatical constructions. It even includes noise and ungrammaticality. This paper describes the Controlled Skip Parser, a program that parses such real-world sentences by skipping some of the words in the sentence. The new feature of this parser is that it can control its behavior to find out which words to skip, without using domain-specific knowledge. Statistical information (N-grams), which is a generalized approximation of the grammar learned from past successful experiences, is used for the controlled skip. Experiments on real newspaper articles are shown, and our experience with this parser in a machine translation system is described. 
Abstract-found: 1
Intro-found: 1
Reference: [Ayuso, et al. 1994] <author> D. M. </author> <title> Ayuso and the PLUM Research Group, "Pattern Matching in a Linguistically-Motivated Text Understanding System", </title> <booktitle> In Proceedings of the Human Language Technology Workshop, </booktitle> <pages> pp. 182-186, </pages> <year> 1994. </year>
Reference-contexts: Further, if domain-specific lexical and syntactic knowledge is available, grammar rule can be designed to create parse fragments which can be easily combined and used, as in <ref> [Ayuso, et al. 1994] </ref>. However, these methods, which rely heavily on domain-specific knowledge, are useful only when such knowledge is available. When such knowledge is not available, or in systems which need to handle texts in any domain, domain independent syntactic methods are more useful.
Reference: [Brown, et al. 1990] <author> P. F. Brown, J. Cocke, S. A. Della Peitra, V. J. Della Pietra, F. Jelinek, J. D. Lafferty, R. L. Mercer, and P. S. Roossin, </author> <title> "A Statistical Approach to Machine Translation", </title> <journal> Computational Linguistics, Vol.16, No.2, </journal> <pages> pp. 79-85, </pages> <year> 1990. </year>
Reference-contexts: One extreme way to deal with these difficulties is to abandon syntactic analysis completely, instead relying purely on statistics for machine translation <ref> [Brown, et al. 1990] </ref>, or using template matching for information extraction [Jackson, et al. 1991]. With these methods, no grammar rules are used, and no analysis of syntactic and semantic structure of sentences is performed.
Reference: [Hobbs, et al. 1992] <author> J. R. Hobbs, D. E. Appelt, J. Bear, and M. Tyson, </author> <title> "Robust Processing of Real-World Natural Language Texts", </title> <booktitle> In Proceedings Applied Natural Language Processing, </booktitle> <pages> pp. 186-192, </pages> <year> 1992. </year>
Reference-contexts: These approaches, however, cannot fully utilize the semantic and pragmatic knowledge even when the syntactic structure is easy to parse. When domain-specific semantic and pragmatic knowledge is available, a certain level of loose syntactic analysis has been shown to be useful, such as in [McDonald 1992], <ref> [Hobbs, et al. 1992] </ref>, [Seneff 1992], and [Stallard and Bobrow 1993]. In these systems, a regular parser first attempts to parse a sentence, and when the parser fails, the parse fragments, which represent syntactic structure of parts of the sentence, are combined together using domain-specific semantics and pragmatics.
Reference: [Jackson, et al. 1991] <author> E. Jackson, D. Appelt, J. Bear, R. Moore, and A. Podlozny, </author> <title> "A Template Matcher for Robust NL Interpretation", </title> <booktitle> In Proceedings DARPA Speech and Natural Language Workshop, </booktitle> <pages> pp. 190-194, </pages> <year> 1991. </year>
Reference-contexts: One extreme way to deal with these difficulties is to abandon syntactic analysis completely, instead relying purely on statistics for machine translation [Brown, et al. 1990], or using template matching for information extraction <ref> [Jackson, et al. 1991] </ref>. With these methods, no grammar rules are used, and no analysis of syntactic and semantic structure of sentences is performed. Target translation or extracted information is directly obtained from the surface input, using context sensitive word-by-word translation statistics or target-dependent database templates.
Reference: [Kay 1980] <author> M. Kay, </author> <title> "Algorithm Schemata and Data Structures in Syntactic Processing", </title> <institution> CSL-80-12, Xerox Palo Alto Research Center, </institution> <year> 1980. </year>
Reference: [Knight, et al. 1995] <author> K. Knight, I. Chander, M. Haines, V. Hatzivassiloglou, E. Hovy, M. Iida, S. K. Luk, R. Whitney, and K. Yamada, </author> <title> "Filling Knowledge Gaps in a Broad-Coverage Machine Translation System", </title> <booktitle> In Proceedings of the International Joint Conference on Artificial Intelligence, </booktitle> <pages> pp. 1390-1396, </pages> <year> 1995. </year>
Reference-contexts: Without skipping these two words, the parser is left with six major parse fragments (shown as lower trees in the figure). This example is taken from the parser described in this paper. This parser is used in a broad-coverage Japanese-to-English machine translation system <ref> [Knight, et al. 1995] </ref>. Because this system operates across domains, it is impossible to rely on domain-specific semantic and pragmatic knowledge to complete a parse. Syntax-only domain-independent mechanism for unparsable sentence is the most effective method for a broad-coverage system. There is another reason for using a skip parser. <p> sentences parsed without skip parsed with skip Total parsed sentences Test2 458 189 (41.3%) 255 (49.1%) 414 (90.4%) Test3 482 253 (52.5%) 198 (41.1%) 451 (93.6%) Test4 500 295 (59.0%) 186 (37.2%) 481 (96.2%) Test4n 500 295 (59.0%) 147 (29.4%) 442 (88.4%) Table 1: Coverage of Skip Parsing translation system <ref> [Knight, et al. 1995] </ref>. Table 1 is the summary of the performance as the raw grammar coverage expanded from around 40% to 60%, as seen in the third column of the table, which shows the number of sentences parsable without skipping.
Reference: [Lavie 1994] <author> A. Lavie, </author> <title> "An Integrated Heuristic Scheme for Partial Parse Evaluation", </title> <booktitle> In Proceedings of the 32rd Annual Meeting of the Association for Computational Linguistics, </booktitle> <pages> pp. 316-318, </pages> <year> 1994. </year>
Reference-contexts: An important direction of further research is to find a way to identify an unreasonable skipping after parsing. One possible strategy is to reject a parse which skipped words with rich content information in the dictionary. Ultimately, it would be best handled by semantic and pragmatic level. <ref> [Lavie 1994] </ref> uses semantic coherence for ranking N-best results of his skip parser. 5 Conclusion We presented a controlled skip parser, which selectively skips words to parse an unparsable sentence. The control information comes from heuristics obtained by statistical information.
Reference: [Lavie and Tomita 1993] <author> A. Lavie and M. Tomita, </author> <title> "GLR* An Efficient Noise-skipping Parsing Algorithm For Context Free Grammars", </title> <booktitle> In Proceedings of Third International Workshop on Parsing Technology, </booktitle> <pages> pp. 123-134, </pages> <year> 1993. </year>
Reference-contexts: For example, [Mellish 1989] shows how to enhance chart-based techniques for parsing ill-formed sentences. In his method, when bottom-up chart parsing fails, top-down predictions, derived from grammar rules, make hypotheses to add/delete/substitute words to parse the sentence. <ref> [Lavie and Tomita 1993] </ref> show an extension to the GLR (Tomita) parser, which skips words by allowing shift operations at inactive states. Skipping words in the input sentence to obtain a complete parse is effective in processing spoken language with noise. <p> Due to undesirable or unsuccessful skipping, a practical implementation of a skip parser has to deal with time and space limitations. In other words, the exhaustive search must be avoided by using efficient technique for reducing the search space. <ref> [Lavie and Tomita 1993] </ref> reduce the search space by using beam-search which limits the number of active nodes. Although this is a heuristic and is not guaranteed to find a optimal skipping, it is reported to be useful. <p> As expected, we experienced the good effect of skip parser when the grammar was being developed, and it boosted the grammar coverage from about 60% to 90%. And about half of the extra coverage, the skipping was reasonable. The heuristics presented here are more powerful than beam-search as in <ref> [Lavie and Tomita 1993] </ref> or than top-down prediction as in [Mellish 1989], yet our method is still simple, efficient and domain-independent. Using the N-gram as the primary heuristic is a convenient way to approximate grammar rules, generalized from past successful experiences.
Reference: [Matsumoto, et al. 1993] <author> Y. Matsumoto, S. Kurohashi, T. Utsuro, Y. Myoki, and M. Nagao, </author> <title> "Japanese Morphological Analysis System JUMAN Manual", </title> <publisher> Kyoto University, </publisher> <year> 1993. </year>
Reference: [McDonald 1992] <author> D. D. McDonald, </author> <title> "An Efficient Chart-based Algorithm for Partial-Parsing of Unrestricted Texts", </title> <booktitle> In Proceedings Applied Natural Language Processing, </booktitle> <pages> pp. 193-200, </pages> <year> 1992. </year>
Reference-contexts: These approaches, however, cannot fully utilize the semantic and pragmatic knowledge even when the syntactic structure is easy to parse. When domain-specific semantic and pragmatic knowledge is available, a certain level of loose syntactic analysis has been shown to be useful, such as in <ref> [McDonald 1992] </ref>, [Hobbs, et al. 1992], [Seneff 1992], and [Stallard and Bobrow 1993]. In these systems, a regular parser first attempts to parse a sentence, and when the parser fails, the parse fragments, which represent syntactic structure of parts of the sentence, are combined together using domain-specific semantics and pragmatics.
Reference: [Mellish 1989] <author> C. S. Mellish, </author> <title> "Some Chart-based Techniques for Parsing Ill-formed Input", </title> <booktitle> In Proceedings of the 27th Annual Meeting of the Association for Computational Linguistics, </booktitle> <pages> pp. 102-109, </pages> <year> 1989. </year>
Reference-contexts: However, these methods, which rely heavily on domain-specific knowledge, are useful only when such knowledge is available. When such knowledge is not available, or in systems which need to handle texts in any domain, domain independent syntactic methods are more useful. For example, <ref> [Mellish 1989] </ref> shows how to enhance chart-based techniques for parsing ill-formed sentences. <p> And about half of the extra coverage, the skipping was reasonable. The heuristics presented here are more powerful than beam-search as in [Lavie and Tomita 1993] or than top-down prediction as in <ref> [Mellish 1989] </ref>, yet our method is still simple, efficient and domain-independent. Using the N-gram as the primary heuristic is a convenient way to approximate grammar rules, generalized from past successful experiences. Note that using N-grams is not a method dependent on the parsing mechanism.
Reference: [Pereira, et al. 1995] <author> F. C. Pereira, Y. Singer, and N. Tishby, </author> <title> "Beyond Word N-grams", </title> <booktitle> In Proceedings of the 33rd Annual Meeting of the Association for Computational Linguistics, </booktitle> <pages> pp. 95-106, </pages> <year> 1995. </year>
Reference-contexts: Another possible extension is to use more complicated N-grams, such as collecting data for N=3 or more, or using finer-grained grammatical categories and features for indexing open-class words. Using variable length N-grams and more generalized N-grams as in <ref> [Pereira, et al. 1995] </ref> would be an interesting extension. Skipping words in a sentence means some kind of loss of information unless the skipped words are noise.
Reference: [Seneff 1992] <author> S. Seneff, </author> <title> "A Relaxation Method for Understanding Spontaneous Speech Utterances", </title> <booktitle> In Proceedings of the Human Language Technology Workshop, </booktitle> <pages> pp. 299-304, </pages> <year> 1992. </year>
Reference-contexts: When domain-specific semantic and pragmatic knowledge is available, a certain level of loose syntactic analysis has been shown to be useful, such as in [McDonald 1992], [Hobbs, et al. 1992], <ref> [Seneff 1992] </ref>, and [Stallard and Bobrow 1993]. In these systems, a regular parser first attempts to parse a sentence, and when the parser fails, the parse fragments, which represent syntactic structure of parts of the sentence, are combined together using domain-specific semantics and pragmatics.
Reference: [Stallard and Bobrow 1993] <author> D. Stallard and R. Bobrow, </author> <title> "The Semantic Linker ANew Fragment Combining Method", </title> <booktitle> In Proceedings of the Human Language Technology Workshop, </booktitle> <pages> pp. 37-42, </pages> <year> 1993. </year>
Reference-contexts: When domain-specific semantic and pragmatic knowledge is available, a certain level of loose syntactic analysis has been shown to be useful, such as in [McDonald 1992], [Hobbs, et al. 1992], [Seneff 1992], and <ref> [Stallard and Bobrow 1993] </ref>. In these systems, a regular parser first attempts to parse a sentence, and when the parser fails, the parse fragments, which represent syntactic structure of parts of the sentence, are combined together using domain-specific semantics and pragmatics.
References-found: 14

