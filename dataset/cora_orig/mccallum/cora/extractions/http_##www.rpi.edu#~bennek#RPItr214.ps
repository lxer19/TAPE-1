URL: http://www.rpi.edu/~bennek/RPItr214.ps
Refering-URL: http://www.rpi.edu/~bennek/
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Title: Optimal Decision Trees  
Author: Kristin P. Bennett Jennifer A. Blue 
Keyword: Key Words: decision trees, tabu search, classification, machine learning, global optimization.  
Note: Knowledge Discovery and Data  This material is based on research supported by National Science Foundation Grant 949427.  
Address: Troy, NY 12180  Troy, NY 12180.  
Affiliation: Department of Mathematical Sciences Rensselaer Polytechnic Institute  R.P.I. Math  Mining Group, Department of Mathematical Sciences, Rensselaer Polytechnic Institute,  
Pubnum: Report No.  
Email: Email bennek@rpi.edu, bluej@rpi.edu.  
Date: 214  
Abstract: We propose an Extreme Point Tabu Search (EPTS) algorithm that constructs globally optimal decision trees for classification problems. Typically, decision tree algorithms are greedy. They optimize the misclassification error of each decision sequentially. Our non-greedy approach minimizes the misclassification error of all the decisions in the tree concurrently. Using Global Tree Optimization (GTO), we can optimize existing decision trees. This capability can be used in classification and data mining applications to avoid overfitting, transfer knowledge, incorporate domain knowledge, and maintain existing decision trees. Our method works by fixing the structure of the decision tree and then representing it as a set of disjunctive linear inequalities. An optimization problem is constructed that minimizes the errors within the disjunctive linear inequalities. To reduce the misclassification error, a nonlinear error function is minimized over a polyhedral region. We show that it is sufficient to restrict our search to the extreme points of the polyhedral region. A new EPTS algorithm is used to search the extreme points of the polyhedral region for an optimal solution. Promising computational results are given for both randomly generated and real-world problems. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> R. Aboudi and K. J-ornsten. </author> <title> Tabu search for general zero-one integer programs using the pivot and complement heuristic. </title> <journal> ORSA Journal on Computing, </journal> <volume> 6(1) </volume> <pages> 82-936, </pages> <year> 1994. </year>
Reference-contexts: Else consider next best neighbor as a possible move. 4. Go to step 1 Our extreme point tabu search (EPTS) is an extension of the algorithm described above. Other versions of tabu search applied to extreme points can be found in <ref> [16, 1, 20] </ref>. We begin with a description of the features of our algorithm. The algorithm utilizes both recency-based and frequency-based memory and oscillates between local improvement and diversification phases. The resulting algorithm is summarized in Algorithm 5.2.
Reference: [2] <author> K. P. Bennett. </author> <title> Decision tree construction via linear programming. </title> <editor> In M. Evans, editor, </editor> <booktitle> Proceedings of the 4th Midwest Artificial Intelligence and Cognitive Science Society Conference, </booktitle> <pages> pages 97-101, </pages> <address> Utica, Illinois, </address> <year> 1992. </year>
Reference-contexts: Popular decision tree algorithms are greedy. Greedy univariate algorithms such as CART [9] and C4.5 [30] construct a decision based on one attribute at a time. Greedy multivariate decision tree algorithms such as those in <ref> [2, 10, 24] </ref> construct decisions one at a time using linear combinations of all the attributes. In each case, an optimization problem is solved at each decision node to determine the locally best decision. The decision divides the attribute space into two or more regions. <p> Pruning may not be sufficient to compensate for over-fitting. This problem is readily shown in multivariate decision trees. The pruning process frequently produces a tree consisting of a single decision <ref> [2, 5, 33] </ref>. Univariate algorithms appear less susceptible to this problem. Murthy and Salzburg found that greedy heuristics worked well and lookahead algorithms offered little improvement [25, 26]. We believe that this is because univariate decision trees have only one degree of freedom at each decision. <p> An iteration limit of 10,000 was used for each data set. As a starting point for the EPTS and FW algorithms, an initial tree was generated using the greedy MSMT decision tree algorithm <ref> [2] </ref>. MSMT used a linear program [6] to recursively construct the decisions. The tree was then pruned to three decisions. Both EPTS and FW can be started using a random starting point, but we found the MSMT starting point worked better.
Reference: [3] <author> K. P. Bennett. </author> <title> Global tree optimization: A non-greedy decision tree algorithm. </title> <journal> Computing Science and Statistics, </journal> <volume> 26 </volume> <pages> 156-160, </pages> <year> 1994. </year> <month> 21 </month>
Reference-contexts: The key idea is that a given multivariate decision tree can be represented as a set of disjunctive linear inequalities <ref> [7, 3] </ref>. If we can find a solution to the set of disjunctive linear inequalities, we have found a tree that correctly classifies all the points. Note that the structure of the tree must be fixed, but the actual decisions to be used are not. <p> In the Frank-Wolfe algorithm (FW), nonlinear problems are solved using a series of linear programs created by linearizing the objective function. A Frank-Wolfe method was successfully applied to the product error problem <ref> [7, 3] </ref>. However, the Frank-Wolfe algorithm is a descent method that stops at the first local minimum encountered and is limited to differentiable functions. We would like a more robust approach that can be used for nondifferentiable objectives, such as the count objective.
Reference: [4] <author> K. P. Bennett. </author> <title> Optimal decision trees through multilinear programming. </title> <type> Manuscript, </type> <institution> Rensselaer Polytechnic Institute, </institution> <address> Troy, New York, </address> <year> 1995. </year>
Reference: [5] <author> K. P. Bennett and E. J. Bredensteiner. </author> <title> A parametric optimization method for machine learning. R.P.I. Math Report No. </title> <type> 217, </type> <institution> Rensselaer Polytechnic Institute, </institution> <address> Troy, New York, </address> <year> 1995. </year> <note> To appear in INFORMS Journal on Computing. </note>
Reference-contexts: Pruning may not be sufficient to compensate for over-fitting. This problem is readily shown in multivariate decision trees. The pruning process frequently produces a tree consisting of a single decision <ref> [2, 5, 33] </ref>. Univariate algorithms appear less susceptible to this problem. Murthy and Salzburg found that greedy heuristics worked well and lookahead algorithms offered little improvement [25, 26]. We believe that this is because univariate decision trees have only one degree of freedom at each decision.
Reference: [6] <author> K. P. Bennett and O. L. Mangasarian. </author> <title> Robust linear programming discrimination of two linearly inseparable sets. </title> <journal> Optimization Methods and Software, </journal> <volume> 1 </volume> <pages> 23-34, </pages> <year> 1992. </year>
Reference-contexts: become A i w fl 1 i = 1; : : : ; m A and B j w fl 1 j = 1; : : : ; m B (1) We can easily determine in polynomial time if such a w and fl exist using a single linear program <ref> [6] </ref>. Notice that since there is no scaling on w and fl the constant (in this case 1) may be any positive number. 5 2.1.2 Decision Tree With Three Decisions Now consider the problem of a tree with multiple decisions such as the three-decision tree in Figure 2. <p> An iteration limit of 10,000 was used for each data set. As a starting point for the EPTS and FW algorithms, an initial tree was generated using the greedy MSMT decision tree algorithm [2]. MSMT used a linear program <ref> [6] </ref> to recursively construct the decisions. The tree was then pruned to three decisions. Both EPTS and FW can be started using a random starting point, but we found the MSMT starting point worked better.
Reference: [7] <author> K. P. Bennett and O. L. Mangasarian. </author> <title> Bilinear separation of two sets in n-space. </title> <journal> Computational Optimization and Applications, </journal> <volume> 2 </volume> <pages> 207-227, </pages> <year> 1993. </year>
Reference-contexts: The key idea is that a given multivariate decision tree can be represented as a set of disjunctive linear inequalities <ref> [7, 3] </ref>. If we can find a solution to the set of disjunctive linear inequalities, we have found a tree that correctly classifies all the points. Note that the structure of the tree must be fixed, but the actual decisions to be used are not. <p> In the Frank-Wolfe algorithm (FW), nonlinear problems are solved using a series of linear programs created by linearizing the objective function. A Frank-Wolfe method was successfully applied to the product error problem <ref> [7, 3] </ref>. However, the Frank-Wolfe algorithm is a descent method that stops at the first local minimum encountered and is limited to differentiable functions. We would like a more robust approach that can be used for nondifferentiable objectives, such as the count objective.
Reference: [8] <author> J. Blue and K. Bennett. </author> <title> Hybrid extreme point tabu search. R.P.I. Math Report No. </title> <type> 240, </type> <institution> Rensselaer Polytechnic Institute, </institution> <address> Troy, NY, </address> <year> 1996. </year> <note> To appear in the European Journal of Operations Research. </note>
Reference-contexts: Extreme point tabu search has been previously applied to other extreme point tabu search problems <ref> [34, 8] </ref>. A very basic TS search algorithm is described below. For each point in the search space a "neighborhood" is defined. The algorithm moves to the best neighbor of the current solution that is not "tabu". <p> This suggests that a hybrid approach 2 All these datasets are available in the machine learning repository at the University of California at Irvine at http://www.ics.uci.edu/~mlearn/Machine-Learning.html. 20 combining both approaches may do even better. In fact this approach is investigated in <ref> [8] </ref>. Recall that a decision tree with three decisions was used for GTO on all the datasets.
Reference: [9] <author> L. Breiman, J. Friedman, R. Olshen, and C. Stone. </author> <title> Classification and Regression Trees. </title> <booktitle> Wadsworth International, </booktitle> <address> California, </address> <year> 1984. </year>
Reference-contexts: Since decision trees have a readily interpretable logical structure, they provide insight into the characteristics of the classes. We propose a non-greedy non-parametric approach to constructing multivariate decision trees that is fundamentally different than greedy approaches. Popular decision tree algorithms are greedy. Greedy univariate algorithms such as CART <ref> [9] </ref> and C4.5 [30] construct a decision based on one attribute at a time. Greedy multivariate decision tree algorithms such as those in [2, 10, 24] construct decisions one at a time using linear combinations of all the attributes.
Reference: [10] <author> C. E. Brodley and P. E. Utgoff. </author> <title> Multivariate decision trees. </title> <journal> Machine Learning, </journal> <volume> 19(1) </volume> <pages> 45-77, </pages> <year> 1995. </year>
Reference-contexts: Popular decision tree algorithms are greedy. Greedy univariate algorithms such as CART [9] and C4.5 [30] construct a decision based on one attribute at a time. Greedy multivariate decision tree algorithms such as those in <ref> [2, 10, 24] </ref> construct decisions one at a time using linear combinations of all the attributes. In each case, an optimization problem is solved at each decision node to determine the locally best decision. The decision divides the attribute space into two or more regions.
Reference: [11] <author> W. Buntine. </author> <title> Learning classification trees. </title> <booktitle> In In Artificial Intelligence Frontiers in Statistics: AI and Statistics III, </booktitle> <pages> pages 183-201, </pages> <address> London, 1993. </address> <publisher> Chapman and Hall. </publisher>
Reference-contexts: This flexibility of our method is very promising. We are just beginning to explore these possibilities. Since GTO is non-parametric, it is fundamentally different from the few prior nongreedy decision tree algorithms. Unlike the Bayesian or parametric approaches <ref> [32, 11, 18, 19] </ref>, the strict logical rules of the tree are maintained. GTO is based on the idea of formulating a decision tree as a set of disjunctive linear inequalities.
Reference: [12] <author> G. Das and M. G. Goodrich. </author> <title> On the complexity of optimization problems for 3-dimensional convex polyhedra and decision trees. </title> <booktitle> In Workshop on Algorithms and Data Structures (WADS), </booktitle> <year> 1995. </year>
Reference-contexts: Each of these problems is NP-complete <ref> [17, 12, 21] </ref>. Although the constraints are linear, the objectives are nonconvex. <p> Note that no method consistently achieved 100% accuracy on any training set even though such a solution existed. It is not surprising that neither FW nor EPTS found the optimal solution since the underlying problem is NP-complete <ref> [12, 21] </ref>. The smallest GTO problem solved, 5 dimensions and 200 training points, has on the order of 10 485 extreme points. The largest GTO problem, solved with 9 dimensions and 768 points, has on the 18 problems. The MSMT starting point for FW and EPTS is also given.
Reference: [13] <author> R. Detrano, A. Janosi, W. Steinbrunn, M. Pfisterer, J. Schmid, S. Sandhu, K. Guppy, S. Lee, and V. Froelicher. </author> <title> International application of a new probability algorithm for the diagnosis of coronary artery disease. </title> <journal> American Journal of Cardiology, </journal> <volume> 64 </volume> <pages> 304-310, </pages> <year> 1989. </year>
Reference-contexts: In the second experiment, GTO was evaluated on larger random problems and real-world datasets. The real-world data 2 consisted of: the BUPA Liver Disease dataset (Liver); the PIMA Indians Diabetes dataset (Diabetes), the Wisconsin Breast Cancer Database (Cancer) [35], and the Cleveland Heart Disease Database (Heart) <ref> [13] </ref>. Five fold cross validation was used for the real-life problems. Each dataset was divided into 5 parts. Training sets used 4/5 of the data and testing was done on the remaining 1/5. This was repeated five times leaving out a different test set for each run.
Reference: [14] <author> F. Glover. </author> <title> Tabu search part I. </title> <journal> ORSA Journal of Computing, </journal> <volume> 1(3) </volume> <pages> 190-206, </pages> <year> 1989. </year>
Reference-contexts: nondifferentiable functions, we also developed a heuristic search approach. 14 5.2 Extreme Point Tabu Search We have developed an Extreme Point Tabu Search (EPTS) algorithm to solve the GTO problem. 1 Tabu Search (TS) is a heuristic that has achieved great success on a wide variety of practical optimization problems <ref> [14] </ref>. Extreme point tabu search has been previously applied to other extreme point tabu search problems [34, 8]. A very basic TS search algorithm is described below. For each point in the search space a "neighborhood" is defined.
Reference: [15] <author> F. Glover. </author> <title> Tabu search fundamentals and uses. </title> <type> Technical report, </type> <institution> School of Business, University of Colorado, Boulder, Colorado, </institution> <year> 1995. </year>
Reference-contexts: For frequency-based memory, we keep count of the number of times each variable is used in the basis. We increment the count of the variables in the basis at every iteration, not just for critical events as suggested in <ref> [15] </ref>. We explored the critical event strategy but found it did not enhance the quality of our results. The best solution found so far in terms of the given objective function is used as the aspiration criterion. <p> EPTS oscillates between a local improvement mode and a diversification mode. In the local improvement mode, the objective function is used to evaluate the quality of the move. The diversification strategy is a variation of the approach suggested in <ref> [15] </ref>.
Reference: [16] <author> F. Glover and A. Ltkketangen. </author> <title> Probabilistic tabu search for zero-one mixed integer programming problems. </title> <type> Manuscript, </type> <institution> School of Business, University of Colorado, </institution> <year> 1994. </year>
Reference-contexts: Else consider next best neighbor as a possible move. 4. Go to step 1 Our extreme point tabu search (EPTS) is an extension of the algorithm described above. Other versions of tabu search applied to extreme points can be found in <ref> [16, 1, 20] </ref>. We begin with a description of the features of our algorithm. The algorithm utilizes both recency-based and frequency-based memory and oscillates between local improvement and diversification phases. The resulting algorithm is summarized in Algorithm 5.2.
Reference: [17] <author> L. Hyafile and R. Rivest. </author> <title> Constructing optimal binary decision trees is np-complete. </title> <journal> Information Processing Letters, </journal> <volume> 5:1:15-17, </volume> <year> 1976. </year>
Reference-contexts: Each of these problems is NP-complete <ref> [17, 12, 21] </ref>. Although the constraints are linear, the objectives are nonconvex.
Reference: [18] <author> M. Jordan. </author> <title> A statistical approach to decision tree modeling. </title> <editor> In M. Warmuth, editor, </editor> <booktitle> Proceedings of the Seventh Annual ACM Conference on Computational Learning Theory, </booktitle> <address> New York, 1994. </address> <publisher> ACM Press. </publisher> <pages> 22 </pages>
Reference-contexts: This flexibility of our method is very promising. We are just beginning to explore these possibilities. Since GTO is non-parametric, it is fundamentally different from the few prior nongreedy decision tree algorithms. Unlike the Bayesian or parametric approaches <ref> [32, 11, 18, 19] </ref>, the strict logical rules of the tree are maintained. GTO is based on the idea of formulating a decision tree as a set of disjunctive linear inequalities.
Reference: [19] <author> M. Jordan and R. Jacobs. </author> <title> Hierarchical mixtures of experts and the EM algorithm. </title> <journal> Neural Computation, </journal> <volume> 6(3) </volume> <pages> 181-214, </pages> <year> 1994. </year>
Reference-contexts: This flexibility of our method is very promising. We are just beginning to explore these possibilities. Since GTO is non-parametric, it is fundamentally different from the few prior nongreedy decision tree algorithms. Unlike the Bayesian or parametric approaches <ref> [32, 11, 18, 19] </ref>, the strict logical rules of the tree are maintained. GTO is based on the idea of formulating a decision tree as a set of disjunctive linear inequalities.
Reference: [20] <author> A. Ltkketangen, K. J-ornsten, and S. Storty. </author> <title> Tabu search within a pivot and complement framework. </title> <journal> International Transactions of Operations Research, </journal> <volume> 1(3) </volume> <pages> 305-316, </pages> <year> 1994. </year>
Reference-contexts: Else consider next best neighbor as a possible move. 4. Go to step 1 Our extreme point tabu search (EPTS) is an extension of the algorithm described above. Other versions of tabu search applied to extreme points can be found in <ref> [16, 1, 20] </ref>. We begin with a description of the features of our algorithm. The algorithm utilizes both recency-based and frequency-based memory and oscillates between local improvement and diversification phases. The resulting algorithm is summarized in Algorithm 5.2.
Reference: [21] <author> N. Megiddo. </author> <title> On the complexity of polyhedral separability. </title> <journal> Discrete and Computational Geometry, </journal> <volume> 3 </volume> <pages> 325-337, </pages> <year> 1988. </year>
Reference-contexts: Each of these problems is NP-complete <ref> [17, 12, 21] </ref>. Although the constraints are linear, the objectives are nonconvex. <p> Note that no method consistently achieved 100% accuracy on any training set even though such a solution existed. It is not surprising that neither FW nor EPTS found the optimal solution since the underlying problem is NP-complete <ref> [12, 21] </ref>. The smallest GTO problem solved, 5 dimensions and 200 training points, has on the order of 10 485 extreme points. The largest GTO problem, solved with 9 dimensions and 768 points, has on the 18 problems. The MSMT starting point for FW and EPTS is also given.
Reference: [22] <author> J. Mingers. </author> <title> An empirical comparison of pruning methods for decision tree induction. </title> <journal> Machine Learning, </journal> <volume> 4 </volume> <pages> 227-243, </pages> <year> 1989. </year>
Reference-contexts: However, the resulting tree may be overparameterized and thus may not reflect the underlying characteristics of the data set. When the overfitting occurs, the tree may not classify future points well. To avoid this problem, heuristics are applied to prune decisions from the tree <ref> [29, 22] </ref>. In this paper, optimization techniques are used to minimize the error of the entire decision tree. Our global approach is analogous to the widely used back propagation algorithm for constructing neural networks [31].
Reference: [23] <author> B.A. Murtagh and M.A. Saunders. </author> <title> MINOS 5.4 user's guide. </title> <type> Technical Report SOL 83.20, </type> <institution> Stanford University, </institution> <year> 1993. </year>
Reference-contexts: Thus EPTS could not assess which moves are better. Thus no results are reported for the count error problem. The linear programming package MINOS 5.4 <ref> [23] </ref> was used to implement both FW and EPTS. FW was implemented using the standard linear programming subroutines provided in MINOS. MINOS was customized so that EPTS could select the pivots to be taken. The following parameter setting for the EPTS algorithm were chosen through experimentation.
Reference: [24] <author> S. Murthy, S. Kasif, S. Salzberg, and R. Beigel. </author> <title> OC1: Randomized induction of oblique decision trees. </title> <booktitle> In Proceedings of the Eleventh National Conference on Artificial Intelligence, </booktitle> <pages> pages 322-327, </pages> <address> Boston, MA, 1993. </address> <publisher> MIT Press. </publisher>
Reference-contexts: Popular decision tree algorithms are greedy. Greedy univariate algorithms such as CART [9] and C4.5 [30] construct a decision based on one attribute at a time. Greedy multivariate decision tree algorithms such as those in <ref> [2, 10, 24] </ref> construct decisions one at a time using linear combinations of all the attributes. In each case, an optimization problem is solved at each decision node to determine the locally best decision. The decision divides the attribute space into two or more regions.
Reference: [25] <author> S. Murthy and S. </author> <title> Salzburg. Decision tree induction: How effective is the greedy heuristic? In Proceedings of the First International Conference on Knowledge Discovery and Data Mining, </title> <year> 1995. </year>
Reference-contexts: This problem is readily shown in multivariate decision trees. The pruning process frequently produces a tree consisting of a single decision [2, 5, 33]. Univariate algorithms appear less susceptible to this problem. Murthy and Salzburg found that greedy heuristics worked well and lookahead algorithms offered little improvement <ref> [25, 26] </ref>. We believe that this is because univariate decision trees have only one degree of freedom at each decision. The problem with univariate trees, however, is that many decisions are required to represent 2 C4.5 GTO simple linear relations. Multivariate decisions are much more powerful.
Reference: [26] <author> S. Murthy and S. </author> <title> Salzburg. Lookahead and pathology in decision tree induction. </title> <booktitle> In Proceedings of of the Fourteenth Intl. Joint Conference. on Artificial Intelligence, </booktitle> <year> 1995. </year>
Reference-contexts: This problem is readily shown in multivariate decision trees. The pruning process frequently produces a tree consisting of a single decision [2, 5, 33]. Univariate algorithms appear less susceptible to this problem. Murthy and Salzburg found that greedy heuristics worked well and lookahead algorithms offered little improvement <ref> [25, 26] </ref>. We believe that this is because univariate decision trees have only one degree of freedom at each decision. The problem with univariate trees, however, is that many decisions are required to represent 2 C4.5 GTO simple linear relations. Multivariate decisions are much more powerful.
Reference: [27] <author> K. Murty. </author> <title> Linear Programming. </title> <publisher> Wiley, </publisher> <address> New York, </address> <year> 1983. </year>
Reference-contexts: We can take advantage of the numerically efficient methods for representing extreme points developed for the Simplex Method of linear programming <ref> [27] </ref>. In the Frank-Wolfe algorithm (FW), nonlinear problems are solved using a series of linear programs created by linearizing the objective function. A Frank-Wolfe method was successfully applied to the product error problem [7, 3].
Reference: [28] <author> K.G. Murty. </author> <title> Linear Programming. </title> <publisher> John Wiley & Sons, </publisher> <address> New York, New York, </address> <year> 1983. </year>
Reference-contexts: B j w d fl d + 1 g ) j B j w d + fl d + 1 d = 1; : : : ; D y; z 0 This LP can be put into standard form and solved using any LP algorithm such as the Simplex Method <ref> [28] </ref>. There exists an optimal basic feasible solution ~w; ~fl; ~y; ~z with an objective value of zero. This solution is an extreme point.
Reference: [29] <author> J. R. Quinlan. </author> <title> Simplifying decision trees. </title> <journal> International Journal of Man-Machine Studies, </journal> <volume> 27(1) </volume> <pages> 221-234, </pages> <year> 1987. </year>
Reference-contexts: However, the resulting tree may be overparameterized and thus may not reflect the underlying characteristics of the data set. When the overfitting occurs, the tree may not classify future points well. To avoid this problem, heuristics are applied to prune decisions from the tree <ref> [29, 22] </ref>. In this paper, optimization techniques are used to minimize the error of the entire decision tree. Our global approach is analogous to the widely used back propagation algorithm for constructing neural networks [31].
Reference: [30] <author> J. R. Quinlan. C4.5: </author> <title> Programs for Machine Learning. </title> <publisher> Morgan Kaufmann, </publisher> <year> 1993. </year>
Reference-contexts: We propose a non-greedy non-parametric approach to constructing multivariate decision trees that is fundamentally different than greedy approaches. Popular decision tree algorithms are greedy. Greedy univariate algorithms such as CART [9] and C4.5 <ref> [30] </ref> construct a decision based on one attribute at a time. Greedy multivariate decision tree algorithms such as those in [2, 10, 24] construct decisions one at a time using linear combinations of all the attributes. <p> But greedy methods are more easily led astray. In GTO, we search for the best multivariate tree with a given structure. Fixing the structure prevents the method from overfitting the data. An example of this can be seen in Figure 1. The algorithm C4.5 <ref> [30] </ref> and the GTO algorithm were both applied to this sample data set of 250 points in two dimensions. The C4.5 algorithm used six univariate decisions and still was unable to correctly classify all of the points. <p> Based on the results of the first experiment, we applied the best GTO methods to a wider set of both randomly generated and real-world problems. In both algorithms, Quinlan's C4.5 17 <ref> [30] </ref> was used for a baseline comparison of how a very powerful and popular greedy univariate decision tree algorithm would perform on each problem. To keep as many factors constant as possible, a tree with three decisions such as the one in Figure 2 was used for GTO.
Reference: [31] <author> D.E. Rumelhart, G.E. Hinton, and R.J. Williams. </author> <title> Learning internal representations. </title> <editor> In D.E. Rumelhart and J.L. McClelland, editors, </editor> <booktitle> Parallel Distributed Processing, </booktitle> <pages> pages 318-362, </pages> <address> Cambridge, Massachusetts, 1986. </address> <publisher> MIT Press. </publisher>
Reference-contexts: To avoid this problem, heuristics are applied to prune decisions from the tree [29, 22]. In this paper, optimization techniques are used to minimize the error of the entire decision tree. Our global approach is analogous to the widely used back propagation algorithm for constructing neural networks <ref> [31] </ref>. For a neural network, one specifies an initial structure, the number of units, and their interconnections. An error function which measures the error of the neural network is then constructed. The decisions in a multivariate decision tree are the same as linear threshold units in a neural network.
Reference: [32] <author> P. Smyth, A. Gray, and U. Fayyad. </author> <title> Retrofitting decision tree classifiers using kernel density estimation. </title> <booktitle> In Proceedings of the Twelfth International Conference on Machine Learning, </booktitle> <pages> pages 506-514, </pages> <address> Amherst, MA, </address> <year> 1995. </year>
Reference-contexts: This flexibility of our method is very promising. We are just beginning to explore these possibilities. Since GTO is non-parametric, it is fundamentally different from the few prior nongreedy decision tree algorithms. Unlike the Bayesian or parametric approaches <ref> [32, 11, 18, 19] </ref>, the strict logical rules of the tree are maintained. GTO is based on the idea of formulating a decision tree as a set of disjunctive linear inequalities.
Reference: [33] <author> W.N. </author> <title> Street. Cancer diagnosis and prognosis via linear-programming-based machine learning. </title> <type> Technical Report 94-14, </type> <institution> Computer Sciences Department, University of Wis-consin, Madison, Wisconsin, </institution> <month> August </month> <year> 1994. </year> <type> Ph.D. thesis. </type>
Reference-contexts: Pruning may not be sufficient to compensate for over-fitting. This problem is readily shown in multivariate decision trees. The pruning process frequently produces a tree consisting of a single decision <ref> [2, 5, 33] </ref>. Univariate algorithms appear less susceptible to this problem. Murthy and Salzburg found that greedy heuristics worked well and lookahead algorithms offered little improvement [25, 26]. We believe that this is because univariate decision trees have only one degree of freedom at each decision.
Reference: [34] <author> M. Sun and P. G. McKeown. </author> <title> Tabu search applied to the general fixed charge problem. </title> <journal> Annals of Operations Research, </journal> <volume> 41 </volume> <pages> 405-420, </pages> <year> 1993. </year> <month> 23 </month>
Reference-contexts: Extreme point tabu search has been previously applied to other extreme point tabu search problems <ref> [34, 8] </ref>. A very basic TS search algorithm is described below. For each point in the search space a "neighborhood" is defined. The algorithm moves to the best neighbor of the current solution that is not "tabu".
Reference: [35] <author> W. H. Wolberg and O.L. Mangasarian. </author> <title> Multisurface method of pattern separation for medical diagnosis applied to breast cytology. </title> <booktitle> Proceedings of the National Academy of Sciences,U.S.A., </booktitle> <volume> 87 </volume> <pages> 9193-9196, </pages> <year> 1990. </year> <month> 24 </month>
Reference-contexts: In the second experiment, GTO was evaluated on larger random problems and real-world datasets. The real-world data 2 consisted of: the BUPA Liver Disease dataset (Liver); the PIMA Indians Diabetes dataset (Diabetes), the Wisconsin Breast Cancer Database (Cancer) <ref> [35] </ref>, and the Cleveland Heart Disease Database (Heart) [13]. Five fold cross validation was used for the real-life problems. Each dataset was divided into 5 parts. Training sets used 4/5 of the data and testing was done on the remaining 1/5.
References-found: 35

