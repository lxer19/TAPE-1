URL: http://www.cs.wisc.edu/~fischer/course/html/submissions/135.ps.gz
Refering-URL: http://www.cs.wisc.edu/~fischer/course/html/submissions/
Root-URL: http://www.cs.wisc.edu
Email: prasanna@aloft.att.com  
Phone: Tel: 1-908-582-4376  
Title: Some Techniques for Compilation of Parallel Multimedia Computations  
Author: G. N. Srinivasa Prasanna, 
Address: 600 Mountain Ave., PO Box 636, Murray Hill, NJ 07974-0636,  
Affiliation: AT&T Bell Laboratories,  
Pubnum: 7D-311,  
Abstract: Multimedia applications operate on datastreams, which are periodic sequences of data elements, called datasets. This paper examines how multimedia applications can be compiled to run efficiently on parallel machines. This job requires optimizing both throughput (T ) and latency (L). This paper is the first to perform T L optimization for an arbitrary (parallel) multimedia application (also called a multimedia system). Optimal T L curves are obtained by running multiple datasets in parallel (task multiplicity). The paper presents T L optimization techniques for series systems, general feedforward (acyclic) systems, and cyclic systems. These techniques determine close to optimal multiplicity for each task in the system, and can often simultaneously increase throughput and decrease latency. Arbitrary pipelining in the system dataflow graphs is handled by appropriate retiming. The techniques use task speedup functions (measured/estimated), and are fast and accurate enough to be used in a parallelizing compiler to yield close to optimal (T; L) operating points. Experiments on an NCUBE-2 multiprocessor show substantial performance gains. These ideas are a significant advance in parallel multimedia compilation, and are general enough to be used in other multiobjective optimization contexts. 
Abstract-found: 1
Intro-found: 1
Reference: [2] <author> A. Choudhary et al. </author> <title> Optimal processor assignment for a class of pipelined computations. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 5(4) </volume> <pages> 439-445, </pages> <month> April 94. </month>
Reference-contexts: Predictions and measurement match well under even greatly simplified system assumptions. It is believed that this work is a significant advance in the state-of-art in running multimedia computations on parallel machines. This is a relatively new problem, and hence the literature is scanty. Choudhary et al. <ref> [2] </ref> have presented techniques to explore throughput-latency tradeoffs using speedup functions for series-parallel graphs. However, they do not consider running multiple datasets in parallel, and hence do not produce optimal T L curves.
Reference: [3] <author> J. Subhlok and et al. </author> <title> Communication and memory requirements as the basis for mapping task and data parallel programs. </title> <booktitle> In Proc. of Supercomputing '94, </booktitle> <pages> pages 330-349, </pages> <month> November </month> <year> 1994. </year>
Reference-contexts: Choudhary et al. [2] have presented techniques to explore throughput-latency tradeoffs using speedup functions for series-parallel graphs. However, they do not consider running multiple datasets in parallel, and hence do not produce optimal T L curves. Subhlok et al. <ref> [3, 4] </ref> have considered running multiple datasets in parallel in the Fx compiler (they call it task replication). In [4] they attempt to maximize throughput (not determine the entire T L curves) in parallelizable pipelines. Neither paper incorporates delay elements, and hence DAGs and cyclic systems are not handled.
Reference: [4] <author> J Subhlok and G. Vondran. </author> <title> Optimal mapping of sequences of data parallel tasks. </title> <booktitle> In Proceedings of PPoPP '95, </booktitle> <month> July </month> <year> 1995. </year>
Reference-contexts: Choudhary et al. [2] have presented techniques to explore throughput-latency tradeoffs using speedup functions for series-parallel graphs. However, they do not consider running multiple datasets in parallel, and hence do not produce optimal T L curves. Subhlok et al. <ref> [3, 4] </ref> have considered running multiple datasets in parallel in the Fx compiler (they call it task replication). In [4] they attempt to maximize throughput (not determine the entire T L curves) in parallelizable pipelines. Neither paper incorporates delay elements, and hence DAGs and cyclic systems are not handled. <p> However, they do not consider running multiple datasets in parallel, and hence do not produce optimal T L curves. Subhlok et al. [3, 4] have considered running multiple datasets in parallel in the Fx compiler (they call it task replication). In <ref> [4] </ref> they attempt to maximize throughput (not determine the entire T L curves) in parallelizable pipelines. Neither paper incorporates delay elements, and hence DAGs and cyclic systems are not handled. Partitioning and scheduling techniques [5, 6, 7, 8] have been widely applied to non real-time computation. <p> However the computation is run only once, with speedup being the only criterion. Much is also known about various forms of data and control transformations [9, 10], but again the objective function is one dimensional speedup. These papers (apart from <ref> [4] </ref>) do not present an analytical framework incorporating task speedup behaviour, to guide optimization. <p> The predicted curve and the measured curve are close to each other (within 10-20 %), validating the approximations. "Highest throughput" is obtained at the flat "top" region (high efficiency) of the T L curve. Unlike in the Fx compiler <ref> [4] </ref>, the three task multiplicities at highest throughput depend on task speedups, and are different in general.
Reference: [5] <author> V. Sarkar. </author> <title> Partitioning and Scheduling Programs for Multiprocessors. </title> <type> Technical Report CSL-TR-87-328, Ph.D Thesis, </type> <institution> Computer Systems Lab., Stanford University, </institution> <month> April </month> <year> 1987. </year>
Reference-contexts: In [4] they attempt to maximize throughput (not determine the entire T L curves) in parallelizable pipelines. Neither paper incorporates delay elements, and hence DAGs and cyclic systems are not handled. Partitioning and scheduling techniques <ref> [5, 6, 7, 8] </ref> have been widely applied to non real-time computation. However the computation is run only once, with speedup being the only criterion. Much is also known about various forms of data and control transformations [9, 10], but again the objective function is one dimensional speedup. <p> Second, the speedup function for each cluster is determined, using standard partitioning and scheduling algorithms & possibly measurement (bottom up - <ref> [5, 6] </ref>, top down - [7, 8]). Their runtimes range from (O (E i (N i + E i )) to O ((N i + E i )logN i ), where N i (E i ) is the number of nodes (edges) internal to cluster i.
Reference: [6] <author> T. Yang & A. Gerasoulis. </author> <title> A Fast Static Scheduling Algorithm for DAGs on an Unbounded Number of Processors. </title> <booktitle> In Proc. of Supercomputing 91, </booktitle> <volume> volume 3, </volume> <pages> pages 633-642, </pages> <month> Nov. </month> <year> 1991. </year>
Reference-contexts: In [4] they attempt to maximize throughput (not determine the entire T L curves) in parallelizable pipelines. Neither paper incorporates delay elements, and hence DAGs and cyclic systems are not handled. Partitioning and scheduling techniques <ref> [5, 6, 7, 8] </ref> have been widely applied to non real-time computation. However the computation is run only once, with speedup being the only criterion. Much is also known about various forms of data and control transformations [9, 10], but again the objective function is one dimensional speedup. <p> Second, the speedup function for each cluster is determined, using standard partitioning and scheduling algorithms & possibly measurement (bottom up - <ref> [5, 6] </ref>, top down - [7, 8]). Their runtimes range from (O (E i (N i + E i )) to O ((N i + E i )logN i ), where N i (E i ) is the number of nodes (edges) internal to cluster i.
Reference: [7] <author> K. P. Belkhale and P. Banerjee. </author> <title> Scheduling Algorithms for Parallelizable Tasks. </title> <booktitle> In International Parallel Processing Symposium, </booktitle> <month> June </month> <year> 1993. </year>
Reference-contexts: In [4] they attempt to maximize throughput (not determine the entire T L curves) in parallelizable pipelines. Neither paper incorporates delay elements, and hence DAGs and cyclic systems are not handled. Partitioning and scheduling techniques <ref> [5, 6, 7, 8] </ref> have been widely applied to non real-time computation. However the computation is run only once, with speedup being the only criterion. Much is also known about various forms of data and control transformations [9, 10], but again the objective function is one dimensional speedup. <p> Second, the speedup function for each cluster is determined, using standard partitioning and scheduling algorithms & possibly measurement (bottom up - [5, 6], top down - <ref> [7, 8] </ref>). Their runtimes range from (O (E i (N i + E i )) to O ((N i + E i )logN i ), where N i (E i ) is the number of nodes (edges) internal to cluster i.
Reference: [8] <author> G.N.Srinivasa Prasanna and Bruce R. Musicus. </author> <title> Generalised Multiprocessor Scheduling for Directed Acyclic Graphs. </title> <booktitle> In Supercomputing '94, </booktitle> <pages> pages 216-228, </pages> <month> November </month> <year> 1994. </year>
Reference-contexts: In [4] they attempt to maximize throughput (not determine the entire T L curves) in parallelizable pipelines. Neither paper incorporates delay elements, and hence DAGs and cyclic systems are not handled. Partitioning and scheduling techniques <ref> [5, 6, 7, 8] </ref> have been widely applied to non real-time computation. However the computation is run only once, with speedup being the only criterion. Much is also known about various forms of data and control transformations [9, 10], but again the objective function is one dimensional speedup. <p> Second, the speedup function for each cluster is determined, using standard partitioning and scheduling algorithms & possibly measurement (bottom up - [5, 6], top down - <ref> [7, 8] </ref>). Their runtimes range from (O (E i (N i + E i )) to O ((N i + E i )logN i ), where N i (E i ) is the number of nodes (edges) internal to cluster i.
Reference: [9] <author> M. E. Wolf and M. S. Lam. </author> <title> A Loop Transformation Theory and an Algorithm to Maximize Parallelism. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 2(4) </volume> <pages> 452-471, </pages> <month> October </month> <year> 1991. </year>
Reference-contexts: Partitioning and scheduling techniques [5, 6, 7, 8] have been widely applied to non real-time computation. However the computation is run only once, with speedup being the only criterion. Much is also known about various forms of data and control transformations <ref> [9, 10] </ref>, but again the objective function is one dimensional speedup. These papers (apart from [4]) do not present an analytical framework incorporating task speedup behaviour, to guide optimization.
Reference: [10] <author> M. Ciernaik and W. Li. </author> <title> Unifying Control and Data Transformations for Distributed Shared Memory Machines. </title> <booktitle> In Proceedings of PLDI'95, </booktitle> <pages> pages 205-217, </pages> <month> June </month> <year> 1995. </year>
Reference-contexts: Partitioning and scheduling techniques [5, 6, 7, 8] have been widely applied to non real-time computation. However the computation is run only once, with speedup being the only criterion. Much is also known about various forms of data and control transformations <ref> [9, 10] </ref>, but again the objective function is one dimensional speedup. These papers (apart from [4]) do not present an analytical framework incorporating task speedup behaviour, to guide optimization.
Reference: [11] <author> J Gustafson. </author> <title> Scaled Speedup: A revision of Amdahl's law. </title> <booktitle> In Proceedings of Supercomputing 89, </booktitle> <volume> volume 1, </volume> <month> November </month> <year> 1989. </year>
References-found: 10

