URL: http://www.cs.huji.ac.il/labs/learning/Papers/typical-stoc93.ps.gz
Refering-URL: http://www.cs.huji.ac.il/labs/learning/Papers/MLT_list.html
Root-URL: http://www.cs.huji.ac.il
Title: Efficient Learning of Typical Finite Automata from Random Walks (Extended Abstract)  
Author: Yoav Freund Michael Kearns Dana Ron Ronitt Rubinfeld Robert E. Schapire Linda Sellie 
Address: Santa Cruz, CA 95064  Murray Hill, NJ 07974  Jerusalem 91904, Israel  Ithaca, NY 14853  Murray Hill, NJ 07974  Chicago, IL 60637  
Affiliation: University of California  AT&T Bell Laboratories  Hebrew University  Cornell University  AT&T Bell Laboratories  University of Chicago  
Abstract-found: 0
Intro-found: 1
Reference: [1] <author> Dana Angluin. </author> <title> On the complexity of minimum inference of regular sets. </title> <journal> Information and Control, </journal> <volume> 39 </volume> <pages> 337-350, </pages> <year> 1978. </year>
Reference-contexts: The intractability results for various passive learning models begin with the work of Gold [11] and Angluin <ref> [1] </ref>, who proved that the problem of finding the smallest automaton consistent with a set of accepted and rejected strings is NP-complete. <p> In this model no binary labels are associated with the states Q. Instead, there is a real-valued function ' : Q ! <ref> [0; 1] </ref>. The underlying state graph (defined by the transition function t ) together with ' defines a probabilistic generator of binary strings in the following manner: we now think of the bits on the edges of the machine as the output bits.
Reference: [2] <author> Dana Angluin. </author> <title> Learning regular sets from queries and counterexamples. </title> <journal> Information and Computation, </journal> <volume> 75 </volume> <pages> 87-106, </pages> <month> November </month> <year> 1987. </year>
Reference-contexts: This again implies intractability for the same problem in the mistake-bound models. The situation becomes considerably brighter when we turn to the problem of actively learning finite automata. Angluin <ref> [2] </ref>, elaborating on an algorithm of Gold [10], proved that if a learning algorithm is provided with both passive counterexamples to its current hypothesis automaton (that is, arbitrary strings on which the hypothesis automaton disagrees with the target) and the ability to actively query the target machine on any string of
Reference: [3] <author> Yossi Azar, Andrei Z. Broder, Anna R. Karlin, Nathan Linial, and Steven Phillips. </author> <title> Biased random walks. </title> <booktitle> In Proceedings of the Twenty-Fourth Annual ACM Symposium on the Theory of Computing, </booktitle> <pages> pages 1-9, </pages> <month> May </month> <year> 1992. </year>
Reference-contexts: In addition to investigations of their properties as a computational resource [6, 22, 25, 26], semi-random sources have also been used as a model for studying the complexity of graph coloring that falls between worst-case and average-case (random) models [5], and as a model for biased random walks on graphs <ref> [3] </ref>. <p> However, we are not the first authors to use semi-randomness to investigate models between the worst case and the average (random) case. Blum [5] studied the complexity of coloring semi-random graphs, and Azar et al. have considered semi-random sources to model biased random walks on graphs <ref> [3] </ref>. Now assume that the adversary can choose the label of each state in G M by flipping a coin whose bias is chosen by the adversary from the range [D 1 ; 1 D 1 ].
Reference: [4] <author> Ya. M. Barzdin'. </author> <title> Deciphering of sequential networks in the absence of an upper limit on the number of states. </title> <journal> Soviet Physics Doklady, </journal> <volume> 15(2) </volume> <pages> 94-97, </pages> <month> August </month> <year> 1970. </year>
Reference-contexts: However, average-case models have been examined in the extensive and pioneering work of Trakhten-brot and Barzdin' <ref> [4, 23] </ref>.
Reference: [5] <author> Avrim Blum. </author> <title> Some tools for approximate 3-coloring. </title> <booktitle> In 31st Annual Symposium on Foundations of Computer Science, </booktitle> <pages> pages 554-562, </pages> <month> October </month> <year> 1990. </year>
Reference-contexts: In addition to investigations of their properties as a computational resource [6, 22, 25, 26], semi-random sources have also been used as a model for studying the complexity of graph coloring that falls between worst-case and average-case (random) models <ref> [5] </ref>, and as a model for biased random walks on graphs [3]. <p> However, we are not the first authors to use semi-randomness to investigate models between the worst case and the average (random) case. Blum <ref> [5] </ref> studied the complexity of coloring semi-random graphs, and Azar et al. have considered semi-random sources to model biased random walks on graphs [3].
Reference: [6] <author> B. Chor and O. Goldreich. </author> <title> Unbiased bits from sources of weak randomness and probabilistic communication complexity. </title> <journal> SIAM Journal on Computing, </journal> <volume> 17 </volume> <pages> 230-261, </pages> <year> 1988. </year>
Reference-contexts: In addition to investigations of their properties as a computational resource <ref> [6, 22, 25, 26] </ref>, semi-random sources have also been used as a model for studying the complexity of graph coloring that falls between worst-case and average-case (random) models [5], and as a model for biased random walks on graphs [3]. <p> The next output bit is then generated by flipping a coin of bias . Thus, a semi-random source guarantees only a rather weak form of independence among its output bits. Semi-randomness was introduced by Santha and Vazirani and subsequently investigated by several researchers <ref> [6, 22, 25, 26] </ref> for its abstract properties as a computational resource and its relationship to true randomness. However, we are not the first authors to use semi-randomness to investigate models between the worst case and the average (random) case.
Reference: [7] <author> Thomas Dean, Dana Angluin, Kenneth Basye, Sean Engel-son, Leslie Kaelbling, Evangelos Kokkevis, and Oded Maron. </author> <title> Inferring finite automata with stochastic output functions and an application to map learning. </title> <booktitle> In Proceedings Tenth National Conference on Artificial Intelligence, </booktitle> <pages> pages 208-214, </pages> <month> July </month> <year> 1992. </year>
Reference-contexts: Nevertheless, Rivest and Schapire extend Angluin's algorithm and provide a polynomial time algorithm for inferring any finite automaton from a single continuous walk on the target automaton. Variants of this algorithm have recently been examined by Dean et al. <ref> [7] </ref>. All of the results discussed above, whether in a passive or an active model, have considered the worst-case complexity of learning: to be considered efficient, algorithms must have small running time on any finite automaton.
Reference: [8] <author> M. Feder, N. Merhav, and M. Gutman. </author> <title> Universal prediction of individual sequences. </title> <journal> IEEE Transactions on Information Theory, </journal> <volume> 38 </volume> <pages> 1258-1270, </pages> <year> 1992. </year>
Reference-contexts: from n2 ` (`2 ` + 1)m R to n2 ` 0 2 + 1)m 0 5.2 Learning Distributions on Strings An interesting extension of our results is to a model where automata are used to represent distributions over binary strings such as those discussed by Feder, Merhav and Gutman <ref> [8] </ref>, rather than as acceptors of languages. In this model no binary labels are associated with the states Q. Instead, there is a real-valued function ' : Q ! [0; 1].
Reference: [9] <author> William Feller. </author> <title> An Introduction to Probability and its Applications, volume 1. </title> <publisher> John Wiley and Sons, </publisher> <address> third edition, </address> <year> 1968. </year>
Reference-contexts: The expected number of default mistakes made is then the sum of the expectations of the random variables defined above. Computing the expectation of each of these variables in turn reduces to the so-called Coupon Collector's Problem <ref> [9] </ref>: there are N types of coupons, and at each step we are given a uniformly chosen coupon.
Reference: [10] <author> E. Mark Gold. </author> <title> System identification via state characterization. </title> <journal> Automatica, </journal> <volume> 8 </volume> <pages> 621-636, </pages> <year> 1972. </year>
Reference-contexts: This again implies intractability for the same problem in the mistake-bound models. The situation becomes considerably brighter when we turn to the problem of actively learning finite automata. Angluin [2], elaborating on an algorithm of Gold <ref> [10] </ref>, proved that if a learning algorithm is provided with both passive counterexamples to its current hypothesis automaton (that is, arbitrary strings on which the hypothesis automaton disagrees with the target) and the ability to actively query the target machine on any string of the algorithm's choosing (known as membership queries),
Reference: [11] <author> E. Mark Gold. </author> <title> Complexity of automaton identification from given data. </title> <journal> Information and Control, </journal> <volume> 37 </volume> <pages> 302-320, </pages> <year> 1978. </year>
Reference-contexts: The intractability results for various passive learning models begin with the work of Gold <ref> [11] </ref> and Angluin [1], who proved that the problem of finding the smallest automaton consistent with a set of accepted and rejected strings is NP-complete.
Reference: [12] <author> David Haussler, Nick Littlestone, and Manfred K. Warmuth. </author> <title> Predicting f0; 1g-functions on randomly drawn points. </title> <booktitle> In 29th Annual Symposium on Foundations of Computer Science, </booktitle> <pages> pages 100-109, </pages> <month> October </month> <year> 1988. </year>
Reference-contexts: Such results imply the intractability of learning finite automata (using finite automata as the hypothesis representation) in a variety of passive learning models, including the well-studied probably approximately correct (or PAC) model introduced by Valiant [24] and the mistake-bound models of Littlestone [15] and Haussler, Littlestone and Warmuth <ref> [12] </ref>. These results demonstrated the intractability of passively learning finite automaton when we insist that the hypothesis constructed by the learner also be a finite automaton, but did not address the complexity of passively learning finite automata by more powerful representations.
Reference: [13] <author> Michael Kearns and Leslie G. Valiant. </author> <title> Cryptographic limitations on learning Boolean formulae and finite automata. </title> <booktitle> In Proceedings of the Twenty First Annual ACM Symposium on Theory of Computing, </booktitle> <pages> pages 433-444, </pages> <month> May </month> <year> 1989. </year> <note> To appear, Journal of the Association for Computing Machinery. </note>
Reference-contexts: Although such changes of representation can in some instances provably reduce the complexity of certain learning problems from NP-hard to polynomial time [16], Kearns and Valiant <ref> [13] </ref> demonstrated that this is not the case for finite automata by proving that passive learning in the PAC model by any reasonable representation is as hard as breaking various cryptographic protocols that are based on factoring. This again implies intractability for the same problem in the mistake-bound models. <p> This gives, for instance, an efficient algorithm for learning finite automata in the PAC model augmented with membership queries. Together with the results of Kearns and Valiant <ref> [13] </ref>, this separates (under cryptographic assumptions) the PAC model and the PAC model with membership queries, so experimentation provably helps for learning finite automata. <p> machine. 1 Each input sequence is generated by the 1 The mere fact that we are providingthe learner with the entire output of the machine on a complete walk does not in itself make (worst-case) PAC-learning of finite automata any easier; for instance, the negative results of Kearns and Valiant <ref> [13] </ref> hold even when the learner is provided with this extra information. following process: first, the length ` of the sequence is chosen according to an arbitrary distribution; then an input sequence is chosen uniformly at random from f0; 1g ` .
Reference: [14] <author> Kevin J. Lang. </author> <title> Random DFA's can be approximately learned from sparse uniform examples. </title> <booktitle> In Proceedings of the Fifth Annual ACM Workshop on Computational Learning Theory, </booktitle> <pages> pages 45-52, </pages> <month> July </month> <year> 1992. </year>
Reference-contexts: For an interesting empirical study of the performance of one of these algo-rithms, see Lang's paper <ref> [14] </ref> on experiments he conducted using automata that were chosen partially or completely at random.
Reference: [15] <author> Nick Littlestone. </author> <title> Learning when irrelevant attributes abound. </title> <booktitle> In 28th Annual Symposium on Foundations of Computer Science, </booktitle> <pages> pages 68-77, </pages> <month> October </month> <year> 1987. </year> <month> 8 </month>
Reference-contexts: Such results imply the intractability of learning finite automata (using finite automata as the hypothesis representation) in a variety of passive learning models, including the well-studied probably approximately correct (or PAC) model introduced by Valiant [24] and the mistake-bound models of Littlestone <ref> [15] </ref> and Haussler, Littlestone and Warmuth [12]. These results demonstrated the intractability of passively learning finite automaton when we insist that the hypothesis constructed by the learner also be a finite automaton, but did not address the complexity of passively learning finite automata by more powerful representations.
Reference: [16] <author> Leonard Pitt and Leslie G. Valiant. </author> <title> Computational limitations on learning from examples. </title> <journal> Journal of the Association for Computing Machinery, </journal> <volume> 35(4) </volume> <pages> 965-984, </pages> <month> October </month> <year> 1988. </year>
Reference-contexts: Although such changes of representation can in some instances provably reduce the complexity of certain learning problems from NP-hard to polynomial time <ref> [16] </ref>, Kearns and Valiant [13] demonstrated that this is not the case for finite automata by proving that passive learning in the PAC model by any reasonable representation is as hard as breaking various cryptographic protocols that are based on factoring.
Reference: [17] <author> Leonard Pitt and Manfred K. Warmuth. </author> <title> The minimum consistent DFA problem cannot be approximated within any polynomial. </title> <booktitle> In Proceedings of the Twenty First Annual ACM Symposium on Theory of Computing, </booktitle> <month> May </month> <year> 1989. </year> <note> Available as Technical Report UIUCDCS-R-89-1499, </note> <institution> University of Illinois at Urbana-Champaign, Department of Computer Science. </institution> <note> To appear, Journal of the Association for Computing Machinery. </note>
Reference-contexts: This result left open the possibility of efficiently approximating the smallest machine, which was later dismissed in a very strong sense by the NP - hardness results of Pitt and Warmuth <ref> [17, 18] </ref>.
Reference: [18] <author> Leonard Pitt and Manfred K. Warmuth. </author> <title> Prediction-preserving reducibility. </title> <journal> Journal of Computer and System Sciences, </journal> <volume> 41(3) </volume> <pages> 430-467, </pages> <month> December </month> <year> 1990. </year>
Reference-contexts: This result left open the possibility of efficiently approximating the smallest machine, which was later dismissed in a very strong sense by the NP - hardness results of Pitt and Warmuth <ref> [17, 18] </ref>.
Reference: [19] <author> Ronald L. Rivest and Robert E. Schapire. </author> <title> Diversity-based inference of finite automata. </title> <booktitle> In 28th Annual Symposium on Foundations of Computer Science, </booktitle> <pages> pages 78-87, </pages> <month> October </month> <year> 1987. </year> <note> To appear, Journal of the Association for Computing Machinery. </note>
Reference-contexts: Rivest and Schapire <ref> [19, 20] </ref> considered the natural extension in which we regard the target automaton as representing the learner's environment, and in which experimentation is allowed, but without a reset. The problem becomes more difficult since the learner is not directly provided with the means to orient itself in the target machine.
Reference: [20] <author> Ronald L. Rivest and Robert E. Schapire. </author> <title> Inference of finite automata using homing sequences. </title> <booktitle> In Proceedings of the Twenty First Annual ACM Symposium on Theory of Computing, </booktitle> <pages> pages 411-420, </pages> <month> May </month> <year> 1989. </year> <note> To appear, Information and Computation. </note>
Reference-contexts: Rivest and Schapire <ref> [19, 20] </ref> considered the natural extension in which we regard the target automaton as representing the learner's environment, and in which experimentation is allowed, but without a reset. The problem becomes more difficult since the learner is not directly provided with the means to orient itself in the target machine. <p> For our second algorithm, which eliminates the reset mechanism, the important combinatorial object is the notion of a local homing sequence, which is related to but weaker than the homing sequences used by Rivest and Schapire <ref> [20] </ref>. The algorithm hinges on our theorem stating that with high probability a short local homing sequence exists for every state, and proceeds to identify this sequence by simulating many copies of our first algorithm. <p> In contrast, without a reset, the learner can easily get lost with no obvious means of reorienting itself. In a related setting, Rivest and Schapire <ref> [20] </ref> introduced the idea of using a homing sequence for learning finite automata in the absence of a reset.
Reference: [21] <author> Ronald L. Rivest and Robert Sloan. </author> <title> Learning complicated concepts reliably and usefully. </title> <booktitle> In Proceedings AAAI-88, </booktitle> <pages> pages 635-639, </pages> <month> August </month> <year> 1988. </year>
Reference-contexts: Our first algorithm, for any state graph, and with high probability over the random labeling of the state graph, will make only an expected polynomial number of mistakes. In fact, we show that this algorithm has the stronger property of reliability <ref> [21] </ref>: if allowed to output either a f+; g-prediction or the special symbol ? (called a default mistake) the algorithm will make no prediction mistakes, and only an expected polynomial number of default mistakes. In other words, every f+; g-prediction made by the algorithm will be correct.
Reference: [22] <author> M. Santha and U. V. Vazirani. </author> <title> Generating quasi-random sequences from semi-random sources. </title> <journal> Journal of Computer and System Sciences, </journal> <volume> 33(1) </volume> <pages> 75-87, </pages> <month> August </month> <year> 1986. </year>
Reference-contexts: Limited independence is formalized using the semi-random model of Santha and Vazirani <ref> [22] </ref> in which each label is the outcome of a coin flip of variable bias chosen by an omniscient adversary to be between D and 1 D. <p> In addition to investigations of their properties as a computational resource <ref> [6, 22, 25, 26] </ref>, semi-random sources have also been used as a model for studying the complexity of graph coloring that falls between worst-case and average-case (random) models [5], and as a model for biased random walks on graphs [3]. <p> Then our algorithms still work even in the case that G does not generate independent, unbiased bits but is instead a semi-random source as defined by Santha and Vazirani <ref> [22] </ref>. Briefly, a semi-random source in our context is an omniscient adversary with complete knowledge of the current state of our algorithm and complete memory of all bits previously generated. <p> The next output bit is then generated by flipping a coin of bias . Thus, a semi-random source guarantees only a rather weak form of independence among its output bits. Semi-randomness was introduced by Santha and Vazirani and subsequently investigated by several researchers <ref> [6, 22, 25, 26] </ref> for its abstract properties as a computational resource and its relationship to true randomness. However, we are not the first authors to use semi-randomness to investigate models between the worst case and the average (random) case.
Reference: [23] <author> B. A. Trakhtenbrot and Ya. M. Barzdin'. </author> <title> Finite Automata: Behavior and Synthesis. </title> <publisher> North-Holland, </publisher> <year> 1973. </year>
Reference-contexts: However, average-case models have been examined in the extensive and pioneering work of Trakhten-brot and Barzdin' <ref> [4, 23] </ref>. <p> The expression uniformly almost all automata is borrowed from Trakhtenbrot and Barzdin' <ref> [23] </ref>, and was used by them to refer to a property holding with high probability for any fixed underlying graph. (The term uniformly thus indicates that the graph is chosen in a worst-case manner.) Throughout the paper ffi quantifies confidence only over the random choice of labeling for the target automaton <p> Definition 4 A string x 2 f0; 1g fl is a distinguishing string for q 1 and q 2 if q 1 hxi 6= q 2 hxi. The statement of the key combinatorial theorem needed for our algorithm follows. This theorem is also presented by Trakhtenbrot and Barzdin' <ref> [23] </ref>. However, our proof, which we have included in the appendix for completeness, yields a slightly stronger property of automata. Theorem 2 For uniformly almost all automata, every pair of inequivalent states have a distinguishing string of length at most 2 lg (n =ffi).
Reference: [24] <author> L. G. Valiant. </author> <title> A theory of the learnable. </title> <journal> Communications of the ACM, </journal> <volume> 27(11) </volume> <pages> 1134-1142, </pages> <month> November </month> <year> 1984. </year>
Reference-contexts: Such results imply the intractability of learning finite automata (using finite automata as the hypothesis representation) in a variety of passive learning models, including the well-studied probably approximately correct (or PAC) model introduced by Valiant <ref> [24] </ref> and the mistake-bound models of Littlestone [15] and Haussler, Littlestone and Warmuth [12].
Reference: [25] <author> U. V. Vazirani. </author> <title> Strong communication complexity or generating quasi-random sequences from two communicating semi-random sources. </title> <journal> Combinatorica, </journal> <volume> 7 </volume> <pages> 375-392, </pages> <year> 1987. </year>
Reference-contexts: In addition to investigations of their properties as a computational resource <ref> [6, 22, 25, 26] </ref>, semi-random sources have also been used as a model for studying the complexity of graph coloring that falls between worst-case and average-case (random) models [5], and as a model for biased random walks on graphs [3]. <p> The next output bit is then generated by flipping a coin of bias . Thus, a semi-random source guarantees only a rather weak form of independence among its output bits. Semi-randomness was introduced by Santha and Vazirani and subsequently investigated by several researchers <ref> [6, 22, 25, 26] </ref> for its abstract properties as a computational resource and its relationship to true randomness. However, we are not the first authors to use semi-randomness to investigate models between the worst case and the average (random) case.

References-found: 25

