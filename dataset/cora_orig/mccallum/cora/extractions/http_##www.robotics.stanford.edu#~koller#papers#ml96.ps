URL: http://www.robotics.stanford.edu/~koller/papers/ml96.ps
Refering-URL: http://www.robotics.stanford.edu/~koller/papers/ml96.html
Root-URL: http://www.robotics.stanford.edu
Email: koller@cs.stanford.edu  sahami@cs.stanford.edu  
Title: Toward Optimal Feature Selection  
Author: Daphne Koller 
Affiliation: Computer Science Department Stanford University  Mehran Sahami  Computer Science Department Stanford University  
Address: Gates Building 1A  Stanford, CA 94305-9010  Gates Building 1A  Stanford, CA 94305-9010  
Abstract: In this paper, we examine a method for feature subset selection based on Information Theory. Initially, a framework for defining the theoretically optimal, but computation-ally intractable, method for feature subset selection is presented. We show that our goal should be to eliminate a feature if it gives us little or no additional information beyond that subsumed by the remaining features. In particular, this will be the case for both irrelevant and redundant features. We then give an efficient algorithm for feature selection which computes an approximation to the optimal feature selection criterion. The conditions under which the approximate algorithm is successful are examined. Empirical results are given on a number of data sets, showing that the algorithm effectively han dles datasets with large numbers of features.
Abstract-found: 1
Intro-found: 1
Reference: <author> Almuallim, H., and Dietterich, T. G. </author> <year> 1991. </year> <title> Learning with many irrelevant features. </title> <booktitle> In Proc. AAAI-91, </booktitle> <pages> 547-552. </pages> <publisher> MIT Press. </publisher>
Reference-contexts: Thus the bias of the learning algorithm does not interact with the bias inherent in the feature selection algorithm. Two of the most well-known filter methods for feature selec tion are RELIEF (Kira & Rendell 1992) and FOCUS <ref> (Almuallim & Dietterich 1991) </ref>. In RELIEF, a subset of features in not directly selected, but rather each feature is given a weighting indicating its level of relevance to the class label.
Reference: <author> Blumer, A.; Ehrenfeucht, A.; Haussler, D.; and War-muth, M. K. </author> <year> 1987. </year> <title> Occam's razor. </title> <journal> Information Processing Letters 24 </journal> <pages> 377-380. </pages>
Reference-contexts: This makes it very difficult to obtain good estimates of the many probabilistic parameters. In order to avoid over-fitting the model to the particular distribution seen in the training data, many algorithms employ the Occam's Razor <ref> (Blumer et al. 1987) </ref> bias to build as simple a model as possible that still achieves some acceptable level of performance on the training data.
Reference: <author> Caruana, R., and Freitag, D. </author> <year> 1994. </year> <title> Greedy attribute selection. </title> <booktitle> In Proc. </booktitle> <address> ML-94. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Moreover, the exponential growth of the power set of the features makes this algorithm impractical for domains with more than 25-30 features. Another feature selection methodolgy which has recently received much attention is the wrapper model (John, Kohavi, & Pfleger 1994) <ref> (Caruana & Freitag 1994) </ref> (Langley & Sage 1994). This model searches through the space of feature subsets using the estimated accuracy from an induction algorithm as the measure of goodness for a particular feature subset.
Reference: <author> Cover, T. M., and Thomas, J. A. </author> <year> 1991. </year> <title> Elements of Information Theory. </title> <publisher> Wiley. </publisher>
Reference-contexts: In this work, we address both theoretical and empirical aspects of feature selection. We describe a formal framework for understanding feature selection, based on ideas from Information Theory <ref> (Cover & Thomas 1991) </ref>. We then present an efficient implemented algorithm based on these theoretical intuitions.
Reference: <author> Duda, R., and Hart, P. </author> <year> 1973. </year> <title> Pattern Classification and Scene Analysis. </title> <publisher> Wiley. </publisher>
Reference-contexts: Again, this conjecture was verified experimentally, as our method in fact only consistently selected the 7 relevant features when we conditioned on no variables. To test how our method of feature subset selection affected classification, we employed both a Naive Bayesian classifier <ref> (Duda & Hart 1973) </ref> and C4.5 (Quinlan 1993) as induction algorithms; these were applied both to the original datasets and to the datasets filtered through our feature selection algorithm (using both forward selection and backward elimination). Accuracy results for the UCI data are given in Table 2.
Reference: <author> Fukunaga, K. </author> <year> 1990. </year> <title> Introduction to Statistical Pattern Recognition. </title> <publisher> Academic Press. </publisher>
Reference-contexts: Thus, we can view this as selecting a set of features G which causes us to lose the least amount of information in these distributions. While other measures of separability (notably divergence) have been suggested in the statistics community for feature selection <ref> (Fukunaga 1990) </ref>, these measures are often aimed at selecting features to enhance the separability of the data and may have difficulty in very large dimensional spaces. Hence, they bring with them an inherent bias which may not be appropriate for particular induction algorithms.
Reference: <author> John, G.; Kohavi, R.; and Pfleger, K. </author> <year> 1994. </year> <title> Irrelevant features and the subset selection problem. </title> <booktitle> In Proc. ML-94, </booktitle> <pages> 121-129. </pages> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: We can considerably decrease the running time of the induction algorithm, and we can increase the accuracy of the resulting model. In light of this, a number of researchers have recently addressed the issue of feature subset selection in machine learning. As defined by <ref> (John, Kohavi, & Pfleger 1994) </ref>, this work is often divided along two lines: filter and wrapper models. In the filter model, feature selection is performed as a preprocessing step to induction. Thus the bias of the learning algorithm does not interact with the bias inherent in the feature selection algorithm. <p> This consistency criterion makes FOCUS very sensitive to noise in the training data. Moreover, the exponential growth of the power set of the features makes this algorithm impractical for domains with more than 25-30 features. Another feature selection methodolgy which has recently received much attention is the wrapper model <ref> (John, Kohavi, & Pfleger 1994) </ref> (Caruana & Freitag 1994) (Langley & Sage 1994). This model searches through the space of feature subsets using the estimated accuracy from an induction algorithm as the measure of goodness for a particular feature subset. <p> We first analyze the artificial domains. The Corral dataset has been noted by previous researchers <ref> (John, Kohavi, & Pfleger 1994) </ref> as particularly difficult for filter methods since, of the 6 features in this domain, the target concept is a Boolean function of only four of the features: (A ^ B) _ (C ^ D).
Reference: <author> Kira, K., and Rendell, L. A. </author> <year> 1992. </year> <title> The feature selection problem: Traditional methods and a new algorithm. </title> <booktitle> In Proc. AAAI-92, </booktitle> <pages> 129-134. </pages> <publisher> MIT Press. </publisher>
Reference-contexts: In the filter model, feature selection is performed as a preprocessing step to induction. Thus the bias of the learning algorithm does not interact with the bias inherent in the feature selection algorithm. Two of the most well-known filter methods for feature selec tion are RELIEF <ref> (Kira & Rendell 1992) </ref> and FOCUS (Almuallim & Dietterich 1991). In RELIEF, a subset of features in not directly selected, but rather each feature is given a weighting indicating its level of relevance to the class label.
Reference: <author> Kohavi, R. </author> <year> 1995. </year> <title> Wrappers for Performance Enhancement and Oblivious Decision Graphs. </title> <type> Ph.D. Dissertation, </type> <institution> Stanford, Computer Science Dept. </institution>
Reference: <author> Kullback, S., and Leibler, R. A. </author> <year> 1951. </year> <title> On information and sufficiency. </title> <journal> Annals of Mathematical Statistics 22 </journal> <pages> 76-86. </pages>
Reference-contexts: Our goal is to select G so that these two distributions are as close as possible. As our distance metric, we use the information-theoretic measure of cross-entropy (also known as KL-distance <ref> (Kullback & Leibler 1951) </ref>). Thus, we can view this as selecting a set of features G which causes us to lose the least amount of information in these distributions.
Reference: <author> Langley, P., and Sage, S. </author> <year> 1994. </year> <title> Induction of selective bayesian classifiers. </title> <booktitle> In Proc. UAI-94, </booktitle> <pages> 399-406. </pages> <address> Seattle, WA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Moreover, the exponential growth of the power set of the features makes this algorithm impractical for domains with more than 25-30 features. Another feature selection methodolgy which has recently received much attention is the wrapper model (John, Kohavi, & Pfleger 1994) (Caruana & Freitag 1994) <ref> (Langley & Sage 1994) </ref>. This model searches through the space of feature subsets using the estimated accuracy from an induction algorithm as the measure of goodness for a particular feature subset.
Reference: <author> Murphy, P. M., and Aha, D. W. </author> <year> 1995. </year> <note> UCI repository of machine learning databases. http://www.ics.uci.edu/~mlearn/MLRepository.html. </note>
Reference-contexts: These datasets include: the Corral data which was artificially constructed by John et al (1994) specifically for research in feature selection; the LED24, Vote, and DNA datasets from the UCI repository <ref> (Murphy & Aha 1995) </ref>; and two datasets which are a subset of the Reuters document collection (Reuters 1995). These datasets are detailed in Table 1.
Reference: <author> Pearl, J. </author> <year> 1988. </year> <title> Probabilistic Reasoning in Intelligent Systems. </title> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Therefore, as the number of features grow, our ability to use the training set to approximate this conditional distribution decreases (exponentially). As we now show, we can utilize ideas from probabilistic reasoning <ref> (Pearl 1988) </ref> to circumvent this problem (to some extent). Intuitively, features that cause a small increase in are those that give us the least additional information beyond what we would obtain from the other features in G. We can capture this intuition via the formal notion of conditional independence. <p> Definition 2 Let M be some set of features which does not contain F i . We say that M is a Markov blanket for F i if F i is conditionally independent of (F [C) M fF i g given M . <ref> (Pearl 1988, p. 97) </ref>. It is easy to see that if M is a Markov blanket of F i , then it is also the case that the class C is conditionally independent of the feature F i given M . <p> Then F i also has a Markov blanket within G fF j g. Proof: The proof is based on the basic independence properties of probability distributions, as described in <ref> (Pearl 1988, p. 84) </ref>. We will use the notation I (X; Y j Z) to denote the conditional independence of two variables or sets of variables X and Y given a set of variables Z. <p> In a way, this fact is not surprising. It is well-known that additional information can cause correlations that were not present before to appear <ref> (Pearl 1988) </ref>. To illustrate this phenomenon in our context, consider a text classification problem, where the data instances are documents, the features are the presence or absence of a word, and the classes are document topics. The word mining is not significantly correlated with the topic machine-learning.
Reference: <author> Quinlan, J. R. </author> <year> 1993. </year> <title> C4.5: Programs for Machine Learning. </title> <address> Los Altos, California: </address> <publisher> Morgan Kaufmann. </publisher> <address> Reuters. </address> <year> 1995. </year> <note> Reuters collection available via anonymous ftp. ftp://ciir-ftp.cs.umass.edu/pub/reuters1. </note>
Reference-contexts: Again, this conjecture was verified experimentally, as our method in fact only consistently selected the 7 relevant features when we conditioned on no variables. To test how our method of feature subset selection affected classification, we employed both a Naive Bayesian classifier (Duda & Hart 1973) and C4.5 <ref> (Quinlan 1993) </ref> as induction algorithms; these were applied both to the original datasets and to the datasets filtered through our feature selection algorithm (using both forward selection and backward elimination). Accuracy results for the UCI data are given in Table 2.
Reference: <author> Singh, M., and Provan, G. M. </author> <year> 1996. </year> <title> Efficient learning of selective bayesian network classifiers. </title> <note> Submitted for publication. </note>
Reference-contexts: It is interesting to compare our approach to another, seemingly very similar one, often used in the literature <ref> (Singh & Provan 1996) </ref>. There, rather than starting with the full feature set and eliminating features, we begin with an empty set of features and add features one by one.
References-found: 15

