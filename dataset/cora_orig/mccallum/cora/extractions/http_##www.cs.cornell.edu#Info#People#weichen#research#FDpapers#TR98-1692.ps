URL: http://www.cs.cornell.edu/Info/People/weichen/research/FDpapers/TR98-1692.ps
Refering-URL: http://www.cs.cornell.edu/Info/People/weichen/research/FDpapers.html
Root-URL: http://www.cs.cornell.edu
Email: aguilera,weichen,sam@cs.cornell.edu  
Title: On Quiescent Reliable Communication  
Author: Marcos Kawazoe Aguilera Wei Chen Sam Toueg 
Date: August 24, 1998  
Address: Upson Hall,  Ithaca, NY 14853-7501, USA.  
Affiliation: Department of Computer Science  Cornell University  
Abstract: We study the problem of achieving reliable communication with quiescent algorithms (i.e., algorithms that eventually stop sending messages) in asynchronous systems with process crashes and lossy links. We first show that it is impossible to solve this problem without failure detectors. We then show that, among failure detectors that output lists of suspects, the weakest one that can be used to solve this problem is 3P, a failure detector that cannot be implemented. To overcome this difficulty, we introduce an implementable failure detector called Heartbeat and show that it can be used to achieve quiescent reliable communication. Heartbeat is novel: in contrast to typical failure detectors, it does not output lists of suspects and it is implementable without timeouts. With Heartbeat, many existing algorithms that tolerate only process crashes can be transformed into quiescent algorithms that tolerate both process crashes and message losses. This can be applied to consensus, atomic broadcast, k-set agreement, atomic commitment, etc.
Abstract-found: 1
Intro-found: 1
Reference: [ACT] <author> Marcos Kawazoe Aguilera, Wei Chen, and Sam Toueg. </author> <title> Using the heartbeat failure detector for quiescent reliable communication and consensus in partitionable networks. </title> <note> Submitted to the journal Theoretical Computer Science. 26 </note>
Reference-contexts: By a slight abuse of notation, we sometimes use HB to refer to an arbitrary member of that class. It is easy to generalize the definition of HB so that the failure detector module at each process p outputs the heartbeat of every process in the system <ref> [ACT] </ref>, rather than just the heartbeats of the neighbors of p, but we do not need this generality here. 7 Quiescent Reliable Communication Using HB The communication networks that we consider are not necessarily completely connected, but we assume that every pair of correct processes is connected through a fair path. <p> Extension to Partitionable Networks In this paper, we considered networks that do not partition: we assumed that every pair of correct processes are reachable from each other through fair paths. In a subsequent paper <ref> [ACT] </ref>, we drop this assumption and consider partitionable networks. We first generalize the definition of HB and show how to implement it in such networks.
Reference: [AT96] <author> Marcos Kawazoe Aguilera and Sam Toueg. </author> <title> Randomization and failure detection: a hybrid approach to solve consensus. </title> <booktitle> In Proceedings of the 10th International Workshop on Distributed Algorithms, Lecture Notes on Computer Science, </booktitle> <pages> pages 2939. </pages> <publisher> Springer-Verlag, </publisher> <month> October </month> <year> 1996. </year>
Reference-contexts: First, we explain how HB can be used to transform many existing algorithms that tolerate process crashes into quiescent algorithms that tolerate both process crashes and message losses (fair links). This transformation can be applied to the algorithms for consensus in <ref> [AT96, Ben83, BT85, CT96, CMS89, FM90, Rab83] </ref>, for atomic broadcast in [CT96], for k-set agreement in [Cha93], for atomic commitment in [Gue95], for approximate agreement in [DLP + 86], etc. Next, we show that HB can be used to extend the work in [BCBT96] to obtain the following result. <p> For example, consider the randomized consensus algorithms of [Ben83, CMS89, FM90, Rab83], the failure-detector based ones of <ref> [AT96, CT96] </ref>, the 23 probabilistic one of [BT85], and the algorithms for atomic broadcast in [CT96], k-set agreement in [Cha93], atomic commitment in [Gue95], and approximate agreement in [DLP + 86]. All these algorithms tolerate process crashes.
Reference: [BCBT96] <author> Anindya Basu, Bernadette Charron-Bost, and Sam Toueg. </author> <title> Simulating reliable links with unreliable links in the presence of process crashes. </title> <booktitle> In Proceedings of the 10th International Workshop on Distributed Algorithms, Lecture Notes on Computer Science, </booktitle> <pages> pages 105122. </pages> <publisher> Springer-Verlag, </publisher> <month> October </month> <year> 1996. </year>
Reference-contexts: These implementations 1 As we will see, however, in some types of networks the actual implementation is not as easy. 2 Note that a list of suspects has bounded size. 3 assume that a majority of processes are correct (a result in <ref> [BCBT96] </ref> shows that this assumption is necessary). We conclude the paper by showing how HB can be used to extend previous work in order to solve problems with algorithms that are both quiescent and tolerant of process crashes and messages losses. <p> Next, we show that HB can be used to extend the work in <ref> [BCBT96] </ref> to obtain the following result. Let P be a problem. Suppose P is correct-restricted (i.e., its specification refers only to the behavior of correct processes) or a majority of processes are correct. <p> To overcome this obstacle, we introduce HB: this failure detector can be used to achieve quiescent reliable communication and it is implementable. In contrast to common failure detectors [BDM97, 3 The link failure model in <ref> [BCBT96] </ref> is slightly different from the one used here (cf. Section 10). 4 CT96, Gue95, GLS95, LH94, SM95], HB does not output a list of suspects, and it can be implemented without timeouts. 4. <p> We show that HB can be used to extend existing algorithms for many fundamental problems (e.g., consensus, atomic broadcast, k-set agreement, atomic commitment, approximate agreement) to toler ate message losses. It can also be used to extend the results of <ref> [BCBT96] </ref>. Result (2) above implies that failure detectors with bounded output size are either (a) too weak to achieve quiescent reliable communication, or (b) not implementable. <p> Our work differs from previous results because we seek quiescent algorithms in systems where processes and links can fail (and this requires the use of unreliable failure detectors). The works that are the closest to ours are due to Moses et al. [MR89] and Basu et al. <ref> [BCBT96] </ref>. The main goal of [MR89] is to achieve quiescent reliable communication with algorithms that garbage collect old messages in systems with lossy links (the issue of garbage collection is only briefly considered here). The algorithms in [MR89], however, are not resilient to process crashes. The protocols in [BCBT96] tolerate both <p> et al. <ref> [BCBT96] </ref>. The main goal of [MR89] is to achieve quiescent reliable communication with algorithms that garbage collect old messages in systems with lossy links (the issue of garbage collection is only briefly considered here). The algorithms in [MR89], however, are not resilient to process crashes. The protocols in [BCBT96] tolerate both process crashes and lossy links but they are not quiescent (and they do not use failure detectors). In Section 10, we use HB to extend the results of [BCBT96] and obtain quiescent protocols. The paper is organized as follows. Our model is given in Section 2. <p> The algorithms in [MR89], however, are not resilient to process crashes. The protocols in <ref> [BCBT96] </ref> tolerate both process crashes and lossy links but they are not quiescent (and they do not use failure detectors). In Section 10, we use HB to extend the results of [BCBT96] and obtain quiescent protocols. The paper is organized as follows. Our model is given in Section 2. Section 3 defines the reliable communication primitives that we focus on. In Section 4, we show that, without failure detectors, quiescent reliable communication is impossible. <p> However, stronger types of communication primitives, namely, reliable send and receive, and uniform reliable broadcast, are sometimes needed. We now give quiescent implementations of these primitives for systems with process crashes and message losses. Let t be the number of processes that may crash. <ref> [BCBT96] </ref> shows that if t n=2 (i.e., half of the processes may crash) these primitives cannot be implemented, even if we assume that links may lose only a finite number of messages and we do not require that the implementation be quiescent. <p> Thus, in systems where HB is available, all these algorithms can be made to tolerate both process crashes and message losses (with fair links) by simply plugging in the quiescent communication primitives given in Section 7. 14 The resulting algorithms tolerate message losses and are quiescent. 10.2 Extending Results of <ref> [BCBT96] </ref> Another way to solve problems with quiescent algorithms that tolerate both process crashes and message losses is obtained by extending the results of [BCBT96]. <p> by simply plugging in the quiescent communication primitives given in Section 7. 14 The resulting algorithms tolerate message losses and are quiescent. 10.2 Extending Results of <ref> [BCBT96] </ref> Another way to solve problems with quiescent algorithms that tolerate both process crashes and message losses is obtained by extending the results of [BCBT96]. That work addresses the following question: given a problem that can be solved in a system where the only possible failures are process crashes, is the problem still solvable if links can also fail by losing messages? One of the models of lossy links considered in [BCBT96] is called fair <p> the results of <ref> [BCBT96] </ref>. That work addresses the following question: given a problem that can be solved in a system where the only possible failures are process crashes, is the problem still solvable if links can also fail by losing messages? One of the models of lossy links considered in [BCBT96] is called fair lossy. Roughly speaking, a fair lossy link p ! q satisfies the following property: If p sends an infinite number of messages to q and q is correct, then q receives an infinite number of messages from p. <p> : and p ! q is fair lossy, q may never receive a copy of m 2 (while it receives m 1 infinitely often), whereas if p ! q is fair, q is guaranteed to receive an infinite number of copies of both m 1 and m 2 . 15 <ref> [BCBT96] </ref> establishes the following result: any problem P that can be solved in systems with process crashes can also be solved in systems with process crashes and fair lossy links, provided P is correct-restricted 16 or a majority of processes are correct. For each of these two cases, [BCBT96] shows how <p> . 15 <ref> [BCBT96] </ref> establishes the following result: any problem P that can be solved in systems with process crashes can also be solved in systems with process crashes and fair lossy links, provided P is correct-restricted 16 or a majority of processes are correct. For each of these two cases, [BCBT96] shows how to transform any algorithm that solves P in a system with process crashes, into one that solves P in a system with process crashes and fair lossy links. <p> The algorithms that result from these transformations, however, are not quiescent: each transformation requires processes to repeatedly send messages forever. Given HB, we can modify the transformations in <ref> [BCBT96] </ref> to ensure that if the original algorithm is quiescent then so is the transformed one. <p> The results in <ref> [BCBT96] </ref>, combined with the above modification, show that if a problem P can be solved with a quiescent algorithm in a system with crash failures only, and either P is correct-restricted or a majority of processes are correct, then P is solvable with a quiescent algorithm that uses HB in a <p> a quiescent algorithm that uses HB in a system with crash failures and fair lossy links. 14 This can also be done to algorithms that require reliable send/receive or uniform reliable broadcast by plugging in the implementations given in Section 9, provided a majority of processes are correct. 15 In <ref> [BCBT96] </ref>, message piggybacking is used to overcome message losses.
Reference: [BDM97] <author> Ozalp Babao ffglu, Renzo Davoli, and Alberto Montresor. </author> <title> Partitionable group membership: specification and algorithms. </title> <type> Technical Report UBLCS-97-1, </type> <institution> Dept. of Computer Science, University of Bologna, Bologna, Italy, </institution> <month> January </month> <year> 1997. </year>
Reference-contexts: Introduced in [CT96], the abstraction of unreliable failure detectors has been used to solve several important problems such as consensus, atomic broadcast, group membership, non-blocking atomic commitment, and leader election <ref> [BDM97, Gue95, HMR97, LH94, OGS97, SM95] </ref>. Our goal is to use unreliable failure detectors to achieve quiescence, but before we do so we must address the following important question. <p> To overcome this obstacle, we introduce HB: this failure detector can be used to achieve quiescent reliable communication and it is implementable. In contrast to common failure detectors <ref> [BDM97, 3 The link failure model in [BCBT96] </ref> is slightly different from the one used here (cf. Section 10). 4 CT96, Gue95, GLS95, LH94, SM95], HB does not output a list of suspects, and it can be implemented without timeouts. 4.
Reference: [Ben83] <author> Michael Ben-Or. </author> <title> Another advantage of free choice: Completely asynchronous agreement protocols. </title> <booktitle> In Proceedings of the 2nd ACM Symposium on Principles of Distributed Computing, </booktitle> <pages> pages 2730, </pages> <month> August </month> <year> 1983. </year>
Reference-contexts: First, we explain how HB can be used to transform many existing algorithms that tolerate process crashes into quiescent algorithms that tolerate both process crashes and message losses (fair links). This transformation can be applied to the algorithms for consensus in <ref> [AT96, Ben83, BT85, CT96, CMS89, FM90, Rab83] </ref>, for atomic broadcast in [CT96], for k-set agreement in [Cha93], for atomic commitment in [Gue95], for approximate agreement in [DLP + 86], etc. Next, we show that HB can be used to extend the work in [BCBT96] to obtain the following result. <p> For example, consider the randomized consensus algorithms of <ref> [Ben83, CMS89, FM90, Rab83] </ref>, the failure-detector based ones of [AT96, CT96], the 23 probabilistic one of [BT85], and the algorithms for atomic broadcast in [CT96], k-set agreement in [Cha93], atomic commitment in [Gue95], and approximate agreement in [DLP + 86]. All these algorithms tolerate process crashes.
Reference: [BN92] <author> R. Bazzi and G. Neiger. </author> <title> Simulating crash failures with many faulty processors. </title> <editor> In A. Segal and S. Zaks, editors, </editor> <booktitle> Proceedings of the 6th International Workshop on Distributed Algorithms, volume 647 of Lecture Notes on Computer Science, </booktitle> <pages> pages 166184. </pages> <publisher> Springer-Verlag, </publisher> <year> 1992. </year>
Reference-contexts: To avoid this piggybacking, in this paper we adopted the model of fair links: message losses can now be overcome by separately sending each message repeatedly. 16 Intuitively, a problem P is correct-restricted if its specification does not refer to the behavior of faulty processes <ref> [BN92, Gop92] </ref>. 24 (quiescent and terminating) instance #1 of atomic commitment Reliable Communication (quiescent, non-terminating) Failure Detection (non-quiescent, non-terminating) instance #1 of consensus Applications Instance #2 of consensus 11 Concluding Remarks About Message Buffering We now address the issue of message buffering.
Reference: [BT85] <author> Gabriel Bracha and Sam Toueg. </author> <title> Asynchronous consensus and broadcast protocols. </title> <journal> Journal of the ACM, </journal> <volume> 32(4):824840, </volume> <month> October </month> <year> 1985. </year>
Reference-contexts: First, we explain how HB can be used to transform many existing algorithms that tolerate process crashes into quiescent algorithms that tolerate both process crashes and message losses (fair links). This transformation can be applied to the algorithms for consensus in <ref> [AT96, Ben83, BT85, CT96, CMS89, FM90, Rab83] </ref>, for atomic broadcast in [CT96], for k-set agreement in [Cha93], for atomic commitment in [Gue95], for approximate agreement in [DLP + 86], etc. Next, we show that HB can be used to extend the work in [BCBT96] to obtain the following result. <p> We want to implement quasi reliable send/receive primitives using the communication service provided by the network links. Informally, such an implementation is quiescent if it sends only a finite number of messages when qr-send s;r is invoked a finite number of times. 6 3.2 Reliable Broadcast Reliable broadcast <ref> [BT85] </ref> is defined in terms of two primitives: broadcast (m) and deliver (m). We say that process p broadcasts message m if p invokes broadcast (m). <p> For example, consider the randomized consensus algorithms of [Ben83, CMS89, FM90, Rab83], the failure-detector based ones of [AT96, CT96], the 23 probabilistic one of <ref> [BT85] </ref>, and the algorithms for atomic broadcast in [CT96], k-set agreement in [Cha93], atomic commitment in [Gue95], and approximate agreement in [DLP + 86]. All these algorithms tolerate process crashes.
Reference: [BT93] <author> Ozalp Babao ffglu and Sam Toueg. </author> <title> Non-blocking atomic commitment. </title> <editor> In Sape J. Mullender, editor, </editor> <booktitle> Distributed Systems, chapter 6. </booktitle> <publisher> Addison-Wesley, </publisher> <year> 1993. </year>
Reference-contexts: This requirement allows a faulty process (i.e., one that subsequently crashes) to deliver a message that is never delivered by the correct processes. This behavior is undesirable in some applications, such as atomic commitment in distributed databases <ref> [BT93, Gra78, Had86] </ref>. For such applications, a stronger version of reliable broadcast is more suitable, namely, uniform reliable broadcast which satisfies Uniform Integrity, Validity (Section 3.2) and: * Uniform Agreement [NT90]: If any process delivers a message m, then all correct processes eventually deliver m.
Reference: [Cha93] <author> Soma Chaudhuri. </author> <title> More choices allow more faults: Set consensus problems in totally asynchronous systems. Information and Computation, </title> <address> 105(1):132158, </address> <month> July </month> <year> 1993. </year>
Reference-contexts: This transformation can be applied to the algorithms for consensus in [AT96, Ben83, BT85, CT96, CMS89, FM90, Rab83], for atomic broadcast in [CT96], for k-set agreement in <ref> [Cha93] </ref>, for atomic commitment in [Gue95], for approximate agreement in [DLP + 86], etc. Next, we show that HB can be used to extend the work in [BCBT96] to obtain the following result. Let P be a problem. <p> For example, consider the randomized consensus algorithms of [Ben83, CMS89, FM90, Rab83], the failure-detector based ones of [AT96, CT96], the 23 probabilistic one of [BT85], and the algorithms for atomic broadcast in [CT96], k-set agreement in <ref> [Cha93] </ref>, atomic commitment in [Gue95], and approximate agreement in [DLP + 86]. All these algorithms tolerate process crashes. Moreover, it is easy to verify that the only communication primitives that they actually need are quasi reliable send and receive, and/or reliable broadcast.
Reference: [Cha97] <author> Tushar Deepak Chandra, </author> <month> April </month> <year> 1997. </year> <title> Private Communication. </title>
Reference-contexts: is the obvious one: each process periodically sends an I-am-alive message (a heartbeat) and every process receiving a heartbeat increases the corresponding counter. 1 HB should not be confused with existing failure detectors (some of which, such as those in Ensemble and Phoenix, have modules that are also called heartbeat <ref> [vR97, Cha97] </ref>): HB does not output lists of suspects and its implementation does not use any timeout mechanism.
Reference: [CHT96] <author> Tushar Deepak Chandra, Vassos Hadzilacos, and Sam Toueg. </author> <title> The weakest failure detector for solving consensus. </title> <journal> Journal of the ACM, </journal> <volume> 43(4):685722, </volume> <month> July </month> <year> 1996. </year>
Reference-contexts: We do not assume that the network is completely connected or that the links are bidirectional. The system can experience both process failures and link failures. Processes can fail by crashing, and links can fail by dropping messages. The model, based on the one in <ref> [CHT96] </ref>, is described next. A network is a directed graph G = (; fl) where = f1; : : : ; ng is the set of processes, and fl fi is the set of links. <p> We first give a rough outline of this proof (Section 5.3), and then the proof itself (Sections 5.4 and 5.5). In the Appendix, we give the full proof without the simplifying assumption. 5.1 Failure Detector Transformations Failure detectors can be compared via algorithmic transformations <ref> [CT96, CHT96] </ref>. A transformation algorithm T D!D 0 uses failure detector D to emulate D 0 , as we now explain. At each process p, the algorithm T D!D 0 maintains a variable D 0 p that emulates the output of D 0 at p.
Reference: [CMS89] <author> Benny Chor, Michael Merritt, and David B. Shmoys. </author> <title> Simple constant-time consensus protocols in realistic failure models. </title> <journal> Journal of the ACM, </journal> <volume> 36(3):591614, </volume> <year> 1989. </year>
Reference-contexts: First, we explain how HB can be used to transform many existing algorithms that tolerate process crashes into quiescent algorithms that tolerate both process crashes and message losses (fair links). This transformation can be applied to the algorithms for consensus in <ref> [AT96, Ben83, BT85, CT96, CMS89, FM90, Rab83] </ref>, for atomic broadcast in [CT96], for k-set agreement in [Cha93], for atomic commitment in [Gue95], for approximate agreement in [DLP + 86], etc. Next, we show that HB can be used to extend the work in [BCBT96] to obtain the following result. <p> For example, consider the randomized consensus algorithms of <ref> [Ben83, CMS89, FM90, Rab83] </ref>, the failure-detector based ones of [AT96, CT96], the 23 probabilistic one of [BT85], and the algorithms for atomic broadcast in [CT96], k-set agreement in [Cha93], atomic commitment in [Gue95], and approximate agreement in [DLP + 86]. All these algorithms tolerate process crashes.
Reference: [CT96] <author> Tushar Deepak Chandra and Sam Toueg. </author> <title> Unreliable failure detectors for reliable distributed systems. </title> <journal> Journal of the ACM, </journal> <volume> 43(2):225267, </volume> <month> March </month> <year> 1996. </year>
Reference-contexts: Moreover, the local lists of suspects dynamically change and lists of different processes do not have to agree (or even eventually agree). Introduced in <ref> [CT96] </ref>, the abstraction of unreliable failure detectors has been used to solve several important problems such as consensus, atomic broadcast, group membership, non-blocking atomic commitment, and leader election [BDM97, Gue95, HMR97, LH94, OGS97, SM95]. <p> failure detection service as a tool to achieve this goal. 1.2 Achieving Quiescent Reliable Communication Using Failure Detectors How can we use an unreliable failure detector to achieve quiescent reliable communication in the presence of process and link failures? This can be done with the eventually perfect failure detector 3P <ref> [CT96] </ref>. Intuitively, 3P satisfies the following two properties: (a) if a process crashes then there is a time after which it is permanently suspected, and (b) if a process does not crash then there is a time after which it is never suspected. <p> But is it necessary? In the first part of the paper, we show that among all failure detectors that output lists of suspects, 3P is indeed the weakest one that can be used to solve this problem. Unfortunately, 3P is not implementable (this would violate a known impossibility result <ref> [FLP85, CT96] </ref>). Thus, at a first glance, it seems that achieving quiescent reliable communication requires a failure detector that cannot be implemented. <p> First, we explain how HB can be used to transform many existing algorithms that tolerate process crashes into quiescent algorithms that tolerate both process crashes and message losses (fair links). This transformation can be applied to the algorithms for consensus in <ref> [AT96, Ben83, BT85, CT96, CMS89, FM90, Rab83] </ref>, for atomic broadcast in [CT96], for k-set agreement in [Cha93], for atomic commitment in [Gue95], for approximate agreement in [DLP + 86], etc. Next, we show that HB can be used to extend the work in [BCBT96] to obtain the following result. <p> This transformation can be applied to the algorithms for consensus in [AT96, Ben83, BT85, CT96, CMS89, FM90, Rab83], for atomic broadcast in <ref> [CT96] </ref>, for k-set agreement in [Cha93], for atomic commitment in [Gue95], for approximate agreement in [DLP + 86], etc. Next, we show that HB can be used to extend the work in [BCBT96] to obtain the following result. Let P be a problem. <p> D (F ) denotes the set of possible failure detector histories permitted by D for the failure pattern F . We now define the eventually perfect failure detector 3P <ref> [CT96] </ref>. 4 Each failure detector module of 3P outputs a set of processes that are suspected to have crashed, i.e., R 3P = 2 . <p> More precisely: 9t 2 T ; 8t 0 t; 8p; q 2 correct proc (F ) : p 62 H (q; t 0 ) 4 In <ref> [CT96] </ref>, 3P denotes a class of failure detectors. 6 2.4 Runs of Algorithms An algorithm A is a collection of n deterministic automata, one for each process in the system. Computation proceeds in atomic steps of A. <p> We first give a rough outline of this proof (Section 5.3), and then the proof itself (Sections 5.4 and 5.5). In the Appendix, we give the full proof without the simplifying assumption. 5.1 Failure Detector Transformations Failure detectors can be compared via algorithmic transformations <ref> [CT96, CHT96] </ref>. A transformation algorithm T D!D 0 uses failure detector D to emulate D 0 , as we now explain. At each process p, the algorithm T D!D 0 maintains a variable D 0 p that emulates the output of D 0 at p. <p> For example, consider the randomized consensus algorithms of [Ben83, CMS89, FM90, Rab83], the failure-detector based ones of <ref> [AT96, CT96] </ref>, the 23 probabilistic one of [BT85], and the algorithms for atomic broadcast in [CT96], k-set agreement in [Cha93], atomic commitment in [Gue95], and approximate agreement in [DLP + 86]. All these algorithms tolerate process crashes. <p> For example, consider the randomized consensus algorithms of [Ben83, CMS89, FM90, Rab83], the failure-detector based ones of [AT96, CT96], the 23 probabilistic one of [BT85], and the algorithms for atomic broadcast in <ref> [CT96] </ref>, k-set agreement in [Cha93], atomic commitment in [Gue95], and approximate agreement in [DLP + 86]. All these algorithms tolerate process crashes. Moreover, it is easy to verify that the only communication primitives that they actually need are quasi reliable send and receive, and/or reliable broadcast. <p> We then consider generalized versions of reliable communication and of consensus for partitionable networks, and use HB to solve these problems with quiescent protocols (to solve consensus we also use a generalization of the Eventually Strong failure detector <ref> [CT96] </ref>). Acknowledgments We are grateful to Anindya Basu and Bernadette Charron-Bost for having provided extensive comments that improved the presentation of this paper.
Reference: [DLP + 86] <author> Danny Dolev, Nancy A. Lynch, Shlomit S. Pinter, Eugene W. Stark, and William E. Weihl. </author> <title> Reaching approximate agreement in the presence of faults. </title> <journal> Journal of the ACM, </journal> <volume> 33(3):499516, </volume> <month> July </month> <year> 1986. </year>
Reference-contexts: This transformation can be applied to the algorithms for consensus in [AT96, Ben83, BT85, CT96, CMS89, FM90, Rab83], for atomic broadcast in [CT96], for k-set agreement in [Cha93], for atomic commitment in [Gue95], for approximate agreement in <ref> [DLP + 86] </ref>, etc. Next, we show that HB can be used to extend the work in [BCBT96] to obtain the following result. Let P be a problem. Suppose P is correct-restricted (i.e., its specification refers only to the behavior of correct processes) or a majority of processes are correct. <p> For example, consider the randomized consensus algorithms of [Ben83, CMS89, FM90, Rab83], the failure-detector based ones of [AT96, CT96], the 23 probabilistic one of [BT85], and the algorithms for atomic broadcast in [CT96], k-set agreement in [Cha93], atomic commitment in [Gue95], and approximate agreement in <ref> [DLP + 86] </ref>. All these algorithms tolerate process crashes. Moreover, it is easy to verify that the only communication primitives that they actually need are quasi reliable send and receive, and/or reliable broadcast.
Reference: [FLP85] <author> Michael J. Fischer, Nancy A. Lynch, and Michael S. Paterson. </author> <title> Impossibility of distributed consensus with one faulty process. </title> <journal> Journal of the ACM, </journal> <volume> 32(2):374382, </volume> <month> April </month> <year> 1985. </year>
Reference-contexts: But is it necessary? In the first part of the paper, we show that among all failure detectors that output lists of suspects, 3P is indeed the weakest one that can be used to solve this problem. Unfortunately, 3P is not implementable (this would violate a known impossibility result <ref> [FLP85, CT96] </ref>). Thus, at a first glance, it seems that achieving quiescent reliable communication requires a failure detector that cannot be implemented.
Reference: [FM90] <author> Paul Feldman and Silvio Micali. </author> <title> An optimal algorithm for synchronous Byzantine agreement. </title> <type> Technical Report MIT/LCS/TM-425, </type> <institution> Laboratory for Computer Science, Massachusetts Institute of Technology, </institution> <month> June </month> <year> 1990. </year> <month> 27 </month>
Reference-contexts: First, we explain how HB can be used to transform many existing algorithms that tolerate process crashes into quiescent algorithms that tolerate both process crashes and message losses (fair links). This transformation can be applied to the algorithms for consensus in <ref> [AT96, Ben83, BT85, CT96, CMS89, FM90, Rab83] </ref>, for atomic broadcast in [CT96], for k-set agreement in [Cha93], for atomic commitment in [Gue95], for approximate agreement in [DLP + 86], etc. Next, we show that HB can be used to extend the work in [BCBT96] to obtain the following result. <p> For example, consider the randomized consensus algorithms of <ref> [Ben83, CMS89, FM90, Rab83] </ref>, the failure-detector based ones of [AT96, CT96], the 23 probabilistic one of [BT85], and the algorithms for atomic broadcast in [CT96], k-set agreement in [Cha93], atomic commitment in [Gue95], and approximate agreement in [DLP + 86]. All these algorithms tolerate process crashes.
Reference: [GLS95] <author> Rachid Guerraoui, Michael Larrea, and Andre Schiper. </author> <title> Non blocking atomic commitment with an unreliable failure detector. </title> <booktitle> In Proceedings of the 14th IEEE Symposium on Reliable Distributed Systems, </booktitle> <pages> pages 1315, </pages> <year> 1995. </year>
Reference: [Gop92] <author> Ajei Gopal. </author> <title> Fault-Tolerant Broadcasts and Multicasts: The Problem of Inconsistency and Contamination. </title> <type> PhD thesis, </type> <institution> Cornell University, </institution> <month> January </month> <year> 1992. </year>
Reference-contexts: To avoid this piggybacking, in this paper we adopted the model of fair links: message losses can now be overcome by separately sending each message repeatedly. 16 Intuitively, a problem P is correct-restricted if its specification does not refer to the behavior of faulty processes <ref> [BN92, Gop92] </ref>. 24 (quiescent and terminating) instance #1 of atomic commitment Reliable Communication (quiescent, non-terminating) Failure Detection (non-quiescent, non-terminating) instance #1 of consensus Applications Instance #2 of consensus 11 Concluding Remarks About Message Buffering We now address the issue of message buffering.
Reference: [Gra78] <author> James N. Gray. </author> <title> Notes on database operating systems. </title> <editor> In R. Bayer, R. M. Graham, and G. Seeg-muller, editors, </editor> <booktitle> Operating Systems: An Advanced Course, volume 66 of Lecture Notes on Computer Science. </booktitle> <publisher> Springer-Verlag, </publisher> <year> 1978. </year> <note> Also appears as IBM Research Laboratory Technical report RJ2188. </note>
Reference-contexts: This requirement allows a faulty process (i.e., one that subsequently crashes) to deliver a message that is never delivered by the correct processes. This behavior is undesirable in some applications, such as atomic commitment in distributed databases <ref> [BT93, Gra78, Had86] </ref>. For such applications, a stronger version of reliable broadcast is more suitable, namely, uniform reliable broadcast which satisfies Uniform Integrity, Validity (Section 3.2) and: * Uniform Agreement [NT90]: If any process delivers a message m, then all correct processes eventually deliver m.
Reference: [Gue95] <author> Rachid Guerraoui. </author> <title> Revisiting the relationship between non-blocking atomic commitment and consensus. </title> <booktitle> In Proceedings of the 9th International Workshop on Distributed Algorithms, </booktitle> <pages> pages 87100, </pages> <address> Le Mont-St-Michel, France, 1995. </address> <publisher> Springer Verlag, LNCS 972. </publisher>
Reference-contexts: Introduced in [CT96], the abstraction of unreliable failure detectors has been used to solve several important problems such as consensus, atomic broadcast, group membership, non-blocking atomic commitment, and leader election <ref> [BDM97, Gue95, HMR97, LH94, OGS97, SM95] </ref>. Our goal is to use unreliable failure detectors to achieve quiescence, but before we do so we must address the following important question. <p> This transformation can be applied to the algorithms for consensus in [AT96, Ben83, BT85, CT96, CMS89, FM90, Rab83], for atomic broadcast in [CT96], for k-set agreement in [Cha93], for atomic commitment in <ref> [Gue95] </ref>, for approximate agreement in [DLP + 86], etc. Next, we show that HB can be used to extend the work in [BCBT96] to obtain the following result. Let P be a problem. <p> For example, consider the randomized consensus algorithms of [Ben83, CMS89, FM90, Rab83], the failure-detector based ones of [AT96, CT96], the 23 probabilistic one of [BT85], and the algorithms for atomic broadcast in [CT96], k-set agreement in [Cha93], atomic commitment in <ref> [Gue95] </ref>, and approximate agreement in [DLP + 86]. All these algorithms tolerate process crashes. Moreover, it is easy to verify that the only communication primitives that they actually need are quasi reliable send and receive, and/or reliable broadcast.
Reference: [Had86] <author> Vassos Hadzilacos. </author> <title> On the relationship between the atomic commitment and consensus problems. </title> <booktitle> In Proceedings of the Workshop on Fault-Tolerant Distributed Computing, volume 448 of Lecture Notes on Computer Science, pages 201208. </booktitle> <publisher> Springer-Verlag, </publisher> <month> March </month> <year> 1986. </year>
Reference-contexts: This requirement allows a faulty process (i.e., one that subsequently crashes) to deliver a message that is never delivered by the correct processes. This behavior is undesirable in some applications, such as atomic commitment in distributed databases <ref> [BT93, Gra78, Had86] </ref>. For such applications, a stronger version of reliable broadcast is more suitable, namely, uniform reliable broadcast which satisfies Uniform Integrity, Validity (Section 3.2) and: * Uniform Agreement [NT90]: If any process delivers a message m, then all correct processes eventually deliver m.
Reference: [HMR97] <author> Michel Hurfin, Achour Mostefaoui, and Michel Raynal. </author> <title> Consensus in asynchronous systems where processes can crash and recover. </title> <type> Technical Report 1144, </type> <institution> Institut de Recherche en Infor-matique et Systemes Aleatoires, Universite de Rennes, </institution> <month> November </month> <year> 1997. </year>
Reference-contexts: Introduced in [CT96], the abstraction of unreliable failure detectors has been used to solve several important problems such as consensus, atomic broadcast, group membership, non-blocking atomic commitment, and leader election <ref> [BDM97, Gue95, HMR97, LH94, OGS97, SM95] </ref>. Our goal is to use unreliable failure detectors to achieve quiescence, but before we do so we must address the following important question.
Reference: [HT94] <author> Vassos Hadzilacos and Sam Toueg. </author> <title> A modular approach to fault-tolerant broadcasts and related problems. </title> <type> Technical Report 94-1425, </type> <institution> Department of Computer Science, Cornell University, </institution> <address> Ithaca, New York, </address> <month> May </month> <year> 1994. </year>
Reference-contexts: Roughly speaking, a pair of send/receive primitives is quasi reliable if it satisfies the following property: if processes s and r are correct (i.e., they do not crash), then r receives a message from s exactly as many times as s sent that message to r. Reliable broadcast <ref> [HT94] </ref> ensures that if a correct process broadcasts a message m then all correct processes deliver m; moreover, all correct processes deliver the same set of messages. Our goal is to obtain quiescent implementations of these primitives in networks that do not partition permanently. <p> These fields make every message unique. We say that q delivers message m if q returns from the invocation of deliver (m). Primitives broadcast and deliver satisfy the following properties <ref> [HT94] </ref>: * Validity: If a correct process broadcasts a message m, then it eventually delivers m. * Agreement: If a correct process delivers a message m, then all correct processes eventually deliver m. * Uniform Integrity: For every message m, every process delivers m at most once, and only if m <p> To implement reliable broadcast from qr-send and qr-receive one can use a simple diffusion algorithm (e.g. see <ref> [HT94] </ref>). 4 Impossibility of Quiescent Reliable Communication We now show that quiescent reliable communication cannot be achieved in a network with process crashes and message losses. This holds even if the network is completely connected and only a finite number of messages can be lost. <p> This algorithm is shown in Figure 5 (the code consisting of lines 7 and 8 is executed atomically). 13 The No Loss and Quasi No Loss properties are very similar to the Strong Validity and Validity properties in Section 6 of <ref> [HT94] </ref>. 1 For process s: 2 3 Initialization: 4 seq 0 f seq is the current sequence number g 5 6 To execute r-send s;r (m): 7 seq seq + 1 8 lseq seq 9 broadcast (m; lseq; s; r) 10 wait until qr-received (ACK; lseq) from t + 1 processes
Reference: [KT88] <author> Richard Koo and Sam Toueg. </author> <title> Effects of message loss on the termination of distributed protocols. </title> <journal> Information Processing Letters, </journal> <volume> 27(4):181188, </volume> <month> April </month> <year> 1988. </year>
Reference-contexts: Can we use HB to obtain reliable communication protocols that are terminating? The answer is no, even for systems with no process crashes. This follows from the result in <ref> [KT88] </ref> which shows that in a system 17 In some group membership protocols, the timeout used to remove a process is on the order of minutes: killing a process is expensive and so timeouts are set conservatively. 25 with message losses (fair links) and no process crashes there is no terminating
Reference: [LH94] <author> Wai-Kau Lo and Vassos Hadzilacos. </author> <title> Using failure detectors to solve consensus in asynchronous shared-memory systems. </title> <booktitle> In Proceedings of the 8th International Workshop on Distributed Algorithms, </booktitle> <pages> pages 280295, </pages> <address> Terschelling, The Netherlands, </address> <year> 1994. </year>
Reference-contexts: Introduced in [CT96], the abstraction of unreliable failure detectors has been used to solve several important problems such as consensus, atomic broadcast, group membership, non-blocking atomic commitment, and leader election <ref> [BDM97, Gue95, HMR97, LH94, OGS97, SM95] </ref>. Our goal is to use unreliable failure detectors to achieve quiescence, but before we do so we must address the following important question.
Reference: [Lyn96] <author> Nancy A. Lynch. </author> <title> Distributed Algorithms. </title> <publisher> Morgan Kaufmann Publishers, Inc., </publisher> <year> 1996. </year>
Reference-contexts: To the best of our knowledge, this is the first work that shows that failure detectors with bounded output size have inherent limitations. The problem of achieving reliable communication despite failures has been extensively studied, especially in the context of data link protocols (see Chapter 22 of <ref> [Lyn96] </ref> for a compendium). Our work differs from previous results because we seek quiescent algorithms in systems where processes and links can fail (and this requires the use of unreliable failure detectors).
Reference: [MR89] <author> Yoram Moses and Gil Roth. </author> <title> On reliable message diffusion. </title> <booktitle> In Proceedings of the 8th ACM Symposium on Principles of Distributed Computing, </booktitle> <pages> pages 119128, </pages> <month> August </month> <year> 1989. </year>
Reference-contexts: Our work differs from previous results because we seek quiescent algorithms in systems where processes and links can fail (and this requires the use of unreliable failure detectors). The works that are the closest to ours are due to Moses et al. <ref> [MR89] </ref> and Basu et al. [BCBT96]. The main goal of [MR89] is to achieve quiescent reliable communication with algorithms that garbage collect old messages in systems with lossy links (the issue of garbage collection is only briefly considered here). The algorithms in [MR89], however, are not resilient to process crashes. <p> The works that are the closest to ours are due to Moses et al. <ref> [MR89] </ref> and Basu et al. [BCBT96]. The main goal of [MR89] is to achieve quiescent reliable communication with algorithms that garbage collect old messages in systems with lossy links (the issue of garbage collection is only briefly considered here). The algorithms in [MR89], however, are not resilient to process crashes. <p> to ours are due to Moses et al. <ref> [MR89] </ref> and Basu et al. [BCBT96]. The main goal of [MR89] is to achieve quiescent reliable communication with algorithms that garbage collect old messages in systems with lossy links (the issue of garbage collection is only briefly considered here). The algorithms in [MR89], however, are not resilient to process crashes. The protocols in [BCBT96] tolerate both process crashes and lossy links but they are not quiescent (and they do not use failure detectors). In Section 10, we use HB to extend the results of [BCBT96] and obtain quiescent protocols.
Reference: [NT90] <author> Gil Neiger and Sam Toueg. </author> <title> Automatically increasing the fault-tolerance of distributed algorithms. </title> <journal> Journal of Algorithms, </journal> <volume> 11(3):374419, </volume> <year> 1990. </year>
Reference-contexts: This behavior is undesirable in some applications, such as atomic commitment in distributed databases [BT93, Gra78, Had86]. For such applications, a stronger version of reliable broadcast is more suitable, namely, uniform reliable broadcast which satisfies Uniform Integrity, Validity (Section 3.2) and: * Uniform Agreement <ref> [NT90] </ref>: If any process delivers a message m, then all correct processes eventually deliver m. A quiescent implementation of uniform reliable broadcast can be obtained using quiescent implementations of reliable broadcast, and of quasi reliable send and receive between every pair of processes.
Reference: [OGS97] <author> Rui Oliveira, Rachid Guerraoui, and Andre Schiper. </author> <title> Consensus in the crash-recover model. </title> <type> Technical Report 97-239, </type> <institution> Departement d'Informatique, Ecole Polytechnique Federale, Lau-sanne, Switzerland, </institution> <month> August </month> <year> 1997. </year>
Reference-contexts: Introduced in [CT96], the abstraction of unreliable failure detectors has been used to solve several important problems such as consensus, atomic broadcast, group membership, non-blocking atomic commitment, and leader election <ref> [BDM97, Gue95, HMR97, LH94, OGS97, SM95] </ref>. Our goal is to use unreliable failure detectors to achieve quiescence, but before we do so we must address the following important question.
Reference: [Rab83] <author> Michael Rabin. </author> <title> Randomized Byzantine generals. </title> <booktitle> In Proceedings of the 24th Symposium on Foundations of Computer Science, </booktitle> <pages> pages 403409. </pages> <publisher> IEEE Computer Society Press, </publisher> <month> November </month> <year> 1983. </year>
Reference-contexts: First, we explain how HB can be used to transform many existing algorithms that tolerate process crashes into quiescent algorithms that tolerate both process crashes and message losses (fair links). This transformation can be applied to the algorithms for consensus in <ref> [AT96, Ben83, BT85, CT96, CMS89, FM90, Rab83] </ref>, for atomic broadcast in [CT96], for k-set agreement in [Cha93], for atomic commitment in [Gue95], for approximate agreement in [DLP + 86], etc. Next, we show that HB can be used to extend the work in [BCBT96] to obtain the following result. <p> For example, consider the randomized consensus algorithms of <ref> [Ben83, CMS89, FM90, Rab83] </ref>, the failure-detector based ones of [AT96, CT96], the 23 probabilistic one of [BT85], and the algorithms for atomic broadcast in [CT96], k-set agreement in [Cha93], atomic commitment in [Gue95], and approximate agreement in [DLP + 86]. All these algorithms tolerate process crashes.
Reference: [SM95] <author> Laura S. Sabel and Keith Marzullo. </author> <title> Election vs. consensus in asynchronous systems. </title> <type> Techni--cal Report 95-1488, </type> <institution> Department of Computer Science, Cornell University, </institution> <address> Ithaca, New York, </address> <month> Febrary </month> <year> 1995. </year>
Reference-contexts: Introduced in [CT96], the abstraction of unreliable failure detectors has been used to solve several important problems such as consensus, atomic broadcast, group membership, non-blocking atomic commitment, and leader election <ref> [BDM97, Gue95, HMR97, LH94, OGS97, SM95] </ref>. Our goal is to use unreliable failure detectors to achieve quiescence, but before we do so we must address the following important question.
Reference: [vR97] <author> Robbert van Renesse, </author> <month> April </month> <year> 1997. </year> <title> Private Communication. </title> <type> 29 </type>
Reference-contexts: is the obvious one: each process periodically sends an I-am-alive message (a heartbeat) and every process receiving a heartbeat increases the corresponding counter. 1 HB should not be confused with existing failure detectors (some of which, such as those in Ensemble and Phoenix, have modules that are also called heartbeat <ref> [vR97, Cha97] </ref>): HB does not output lists of suspects and its implementation does not use any timeout mechanism.
References-found: 32

