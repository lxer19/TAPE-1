URL: http://www.cs.utah.edu/~cs522/Resources/painless-conjugate-gradient.ps
Refering-URL: http://www.cs.utah.edu/~cs522/Resources/
Root-URL: 
Title: An Introduction to the Conjugate Gradient Method Without the Agonizing Pain Edition 1 1  
Author:  Jonathan Richard Shewchuk 
Address: Pittsburgh, PA 15213  
Affiliation: School of Computer Science Carnegie Mellon University  
Date: August 4, 1994  
Abstract: The Conjugate Gradient Method is the most prominent iterative method for solving sparse systems of linear equations. Unfortunately, many textbook treatments of the topic are written with neither illustrations nor intuition, and their victims can be found to this day babbling senselessly in the corners of dusty libraries. For this reason, a deep, geometric understanding of the method has been reserved for the elite brilliant few who have painstakingly decoded the mumblings of their forebears. Nevertheless, the Conjugate Gradient Method is a composite of simple, elegant ideas that almost anyone can understand. Of course, a reader as intelligent as yourself will learn them almost effortlessly. The idea of quadratic forms is introduced and used to derive the methods of Steepest Descent, Conjugate Directions, and Conjugate Gradients. Eigenvectors are explained and used to examine the convergence of the Jacobi Method, Steepest Descent, and Conjugate Gradients. Other topics include preconditioning and the nonlinear Conjugate Gradient Method. I have taken pains to make this article easy to read. Sixty-six illustrations are provided. Dense prose is avoided. Concepts are explained in several different ways. Most equations are coupled with an intuitive interpretation. Supported in part by the Natural Sciences and Engineering Research Council of Canada under a 1967 Science and Engineering Scholarship and by the National Science Foundation under Grant ASC-9318163. The views and conclusions contained in this document are those of the author and should not be interpreted as representing the official policies, either express or implied, of NSERC, NSF, or the U.S. Government. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Richard Barrett, Michael Berry, Tony Chan, James Demmel, June Donato, Jack Dongarra, Victor Eijkhout, Roldan Pozo, Charles Romine, and Henk van der Vorst, </author> <title> Templates for the Solution of Linear Systems: Building Blocks for Iterative Methods, </title> <publisher> SIAM, </publisher> <address> Philadelphia, Pennsylvania, </address> <year> 1993. </year>
Reference-contexts: To find the eigenvector associated with = 7, (I A)v = 4 2 # " v 2 = 0 Convergence Analysis of Steepest Descent 13 Any solution to this equation is an eigenvector; say, v = <ref> [1; 2] </ref> T . By the same method, we find that [2; 1] T is an eigenvector corresponding to the eigenvalue 2. <p> To find the eigenvector associated with = 7, (I A)v = 4 2 # " v 2 = 0 Convergence Analysis of Steepest Descent 13 Any solution to this equation is an eigenvector; say, v = [1; 2] T . By the same method, we find that <ref> [2; 1] </ref> T is an eigenvector corresponding to the eigenvalue 2. <p> Using the constants specified by Equation 4, we have x (i+1) = 1 0 1 # " 2 0 x (i) + 1 0 1 # " 8 = 0 2 1 # " 3 3 The eigenvectors of B are <ref> [ p 2; 1] </ref> T with eigenvalue p p 2; 1] T with eigenvalue p These are graphed in Figure 13 (a); note that they do not coincide with the eigenvectors of A, and are not related to the axes of the paraboloid. be understood by watching the eigenvector components of <p> The Chebyshev polynomials have the property that jT i (!)j 1 (in fact, they oscillate between 1 and 1) on the domain ! 2 <ref> [1; 1] </ref>, and furthermore that jT i (!)j is maximum on the domain ! 62 [1; 1] among all such polynomials. <p> The Chebyshev polynomials have the property that jT i (!)j 1 (in fact, they oscillate between 1 and 1) on the domain ! 2 <ref> [1; 1] </ref>, and furthermore that jT i (!)j is maximum on the domain ! 62 [1; 1] among all such polynomials. Loosely speaking, jT i (!)j increases as quickly as possible outside the boxes in the illustration. 36 Jonathan Richard Shewchuk This curve is a scaled version of the Chebyshev polynomial of degree 2. <p> A history and extensive annotated bibliography of CG to the mid-seventies is provided by Golub and O'Leary [9]. Most research since that time has focused on nonsymmetric systems. A survey of iterative methods for the solution of linear systems is offered by Barrett et al. <ref> [1] </ref>. Canned Algorithms 49 B Canned Algorithms The code given in this section represents efficient implementations of the algorithms discussed in this article. B1. <p> Thus, A has 4 orthogonal eigenvectors. C3. Optimality of Chebyshev Polynomials Chebyshev polynomials are optimal for minimization of expressions like Expression 50 because they increase in magnitude more quickly outside the range <ref> [1; 1] </ref> than any other polynomial that is restricted to have magnitude no greater than one inside the range [1; 1]. <p> C3. Optimality of Chebyshev Polynomials Chebyshev polynomials are optimal for minimization of expressions like Expression 50 because they increase in magnitude more quickly outside the range <ref> [1; 1] </ref> than any other polynomial that is restricted to have magnitude no greater than one inside the range [1; 1]. The Chebyshev polynomial of degree i, T i (!) = 2 (! + ! 2 + 1) i + (! ! 2 1) i ; can also be expressed on the region [1; 1] as T i (!) = cos (i cos 1 !); 1 ! 1: From this <p> any other polynomial that is restricted to have magnitude no greater than one inside the range <ref> [1; 1] </ref>. The Chebyshev polynomial of degree i, T i (!) = 2 (! + ! 2 + 1) i + (! ! 2 1) i ; can also be expressed on the region [1; 1] as T i (!) = cos (i cos 1 !); 1 ! 1: From this expression (and from Figure 32), one can deduce that the Chebyshev polynomials have the property jT i (!)j 1; 1 ! 1 and oscillate rapidly between 1 and 1: T i cos k 56 <p> the Chebyshev polynomials have the property jT i (!)j 1; 1 ! 1 and oscillate rapidly between 1 and 1: T i cos k 56 Jonathan Richard Shewchuk Notice that the i zeros of T i must fall between the i + 1 extrema of T i in the range <ref> [1; 1] </ref>. For an example, see the five zeros of T 5 (!) in Figure 32.
Reference: [2] <author> William L. Briggs, </author> <title> A Multigrid Tutorial, </title> <publisher> SIAM, </publisher> <address> Philadelphia, Pennsylvania, </address> <year> 1987. </year>
Reference-contexts: In general, the solution x lies at the intersection point of n hyperplanes, each having dimension n 1. For this problem, the solution is x = <ref> [2; 2] </ref> T . The corresponding quadratic form f (x) appears in Figure 2. A contour plot of f (x) is illustrated in Figure 3. <p> For nonlinear problems, discussed in Section 14, only the latter definition applies. So remember, whenever you read residual, think direction of steepest descent. Suppose we start at x (0) = <ref> [2; 2] </ref> T . Our first step, along the direction of steepest descent, will fall somewhere on the solid line in Figure 6 (a). <p> To find the eigenvector associated with = 7, (I A)v = 4 2 # " v 2 = 0 Convergence Analysis of Steepest Descent 13 Any solution to this equation is an eigenvector; say, v = <ref> [1; 2] </ref> T . By the same method, we find that [2; 1] T is an eigenvector corresponding to the eigenvalue 2. <p> To find the eigenvector associated with = 7, (I A)v = 4 2 # " v 2 = 0 Convergence Analysis of Steepest Descent 13 Any solution to this equation is an eigenvector; say, v = [1; 2] T . By the same method, we find that <ref> [2; 1] </ref> T is an eigenvector corresponding to the eigenvalue 2. <p> Unlike the eigenvectors of A, these eigenvectors are not the axes of the paraboloid. (b) The Jacobi Method starts at <ref> [2; 2] </ref> T and converges at [2; 2] T . (c, d, e) The error vectors e (0) , e (1) , e (2) (solid arrows) and their eigenvector components (dashed arrows). (f) Arrowheads represent the eigenvector components of the first four error vectors. <p> Unlike the eigenvectors of A, these eigenvectors are not the axes of the paraboloid. (b) The Jacobi Method starts at <ref> [2; 2] </ref> T and converges at [2; 2] T . (c, d, e) The error vectors e (0) , e (1) , e (2) (solid arrows) and their eigenvector components (dashed arrows). (f) Arrowheads represent the eigenvector components of the first four error vectors.
Reference: [3] <author> James W. Daniel, </author> <title> Convergence of the Conjugate Gradient Method with Computationally Convenient Modifications, </title> <booktitle> Numerische Mathematik 10 (1967), </booktitle> <pages> 125-131. </pages>
Reference-contexts: CG was generalized to nonlinear problems in 1964 by Fletcher and Reeves [6], based on work by Davidon [4] and Fletcher and Powell [5]. Convergence of nonlinear CG with inexact line searches was analyzed by Daniel <ref> [3] </ref>. The choice of fi for nonlinear CG is discussed by Gilbert and Nocedal [8]. A history and extensive annotated bibliography of CG to the mid-seventies is provided by Golub and O'Leary [9]. Most research since that time has focused on nonsymmetric systems.
Reference: [4] <author> W. C. Davidon, </author> <title> Variable Metric Method for Minimization, </title> <type> Tech. Report ANL-5990, </type> <institution> Argonne National Laboratory, Argonne, Illinois, </institution> <year> 1959. </year>
Reference-contexts: CG was popularized as an iterative method for large, sparse matrices by Reid [13] in 1971. CG was generalized to nonlinear problems in 1964 by Fletcher and Reeves [6], based on work by Davidon <ref> [4] </ref> and Fletcher and Powell [5]. Convergence of nonlinear CG with inexact line searches was analyzed by Daniel [3]. The choice of fi for nonlinear CG is discussed by Gilbert and Nocedal [8].
Reference: [5] <author> R. Fletcher and M. J. D. Powell, </author> <title> A Rapidly Convergent Descent Method for Minimization, </title> <journal> Computer Journal 6 (1963), </journal> <pages> 163-168. </pages>
Reference-contexts: CG was popularized as an iterative method for large, sparse matrices by Reid [13] in 1971. CG was generalized to nonlinear problems in 1964 by Fletcher and Reeves [6], based on work by Davidon [4] and Fletcher and Powell <ref> [5] </ref>. Convergence of nonlinear CG with inexact line searches was analyzed by Daniel [3]. The choice of fi for nonlinear CG is discussed by Gilbert and Nocedal [8]. A history and extensive annotated bibliography of CG to the mid-seventies is provided by Golub and O'Leary [9].
Reference: [6] <author> R. Fletcher and C. M. Reeves, </author> <title> Function Minimization by Conjugate Gradients, </title> <journal> Computer Journal 7 (1964), </journal> <pages> 149-154. </pages>
Reference-contexts: A more thorough analysis of CG convergence is provided by van der Sluis and van der Vorst [16]. CG was popularized as an iterative method for large, sparse matrices by Reid [13] in 1971. CG was generalized to nonlinear problems in 1964 by Fletcher and Reeves <ref> [6] </ref>, based on work by Davidon [4] and Fletcher and Powell [5]. Convergence of nonlinear CG with inexact line searches was analyzed by Daniel [3]. The choice of fi for nonlinear CG is discussed by Gilbert and Nocedal [8].
Reference: [7] <author> L. Fox, H. D. Huskey, and J. H. Wilkinson, </author> <title> Notes on the Solution of Algebraic Linear Simultaneous Equations, </title> <journal> Quarterly Journal of Mechanics and Applied Mathematics 1 (1948), </journal> <pages> 149-173. </pages>
Reference-contexts: Here, I have cheated by using the diagonal of f 00 at the solution point x to precondition every iteration. A Notes Conjugate Direction methods were probably first presented by Schmidt [14] in 1908, and were independently reinvented by Fox, Huskey, and Wilkinson <ref> [7] </ref> in 1948. In the early fifties, the method of Conjugate Gradients was discovered independently by Hestenes [10] and Stiefel [15]; shortly thereafter, they jointly published what is considered the seminal reference on CG [11]. Convergence bounds for CG in terms of Chebyshev polynomials were developed by Kaniel [12].
Reference: [8] <author> Jean Charles Gilbert and Jorge Nocedal, </author> <title> Global Convergence Properties of Conjugate Gradient Methods for Optimization, </title> <journal> SIAM Journal on Optimization 2 (1992), </journal> <volume> no. 1, </volume> <pages> 21-42. </pages>
Reference-contexts: Convergence of nonlinear CG with inexact line searches was analyzed by Daniel [3]. The choice of fi for nonlinear CG is discussed by Gilbert and Nocedal <ref> [8] </ref>. A history and extensive annotated bibliography of CG to the mid-seventies is provided by Golub and O'Leary [9]. Most research since that time has focused on nonsymmetric systems. A survey of iterative methods for the solution of linear systems is offered by Barrett et al. [1].
Reference: [9] <author> Gene H. Golub and Dianne P. O'Leary, </author> <title> Some History of the Conjugate Gradient and Lanczos Algo rithms: 1948-1976, </title> <journal> SIAM Review 31 (1989), </journal> <volume> no. 1, </volume> <pages> 50-102. </pages>
Reference-contexts: Convergence of nonlinear CG with inexact line searches was analyzed by Daniel [3]. The choice of fi for nonlinear CG is discussed by Gilbert and Nocedal [8]. A history and extensive annotated bibliography of CG to the mid-seventies is provided by Golub and O'Leary <ref> [9] </ref>. Most research since that time has focused on nonsymmetric systems. A survey of iterative methods for the solution of linear systems is offered by Barrett et al. [1].
Reference: [10] <author> Magnus R. Hestenes, </author> <title> Iterative Methods for Solving Linear Equations, </title> <journal> Journal of Optimization Theory and Applications 11 (1973), </journal> <volume> no. 4, </volume> <pages> 323-334, </pages> <note> Originally published in 1951 as NAML Report No. 52-9, </note> <institution> National Bureau of Standards, </institution> <address> Washington, D.C. </address>
Reference-contexts: A Notes Conjugate Direction methods were probably first presented by Schmidt [14] in 1908, and were independently reinvented by Fox, Huskey, and Wilkinson [7] in 1948. In the early fifties, the method of Conjugate Gradients was discovered independently by Hestenes <ref> [10] </ref> and Stiefel [15]; shortly thereafter, they jointly published what is considered the seminal reference on CG [11]. Convergence bounds for CG in terms of Chebyshev polynomials were developed by Kaniel [12]. A more thorough analysis of CG convergence is provided by van der Sluis and van der Vorst [16].
Reference: [11] <author> Magnus R. Hestenes and Eduard Stiefel, </author> <title> Methods of Conjugate Gradients for Solving Linear Systems, </title> <booktitle> Journal of Research of the National Bureau of Standards 49 (1952), </booktitle> <pages> 409-436. </pages>
Reference-contexts: In the early fifties, the method of Conjugate Gradients was discovered independently by Hestenes [10] and Stiefel [15]; shortly thereafter, they jointly published what is considered the seminal reference on CG <ref> [11] </ref>. Convergence bounds for CG in terms of Chebyshev polynomials were developed by Kaniel [12]. A more thorough analysis of CG convergence is provided by van der Sluis and van der Vorst [16]. CG was popularized as an iterative method for large, sparse matrices by Reid [13] in 1971.
Reference: [12] <author> Shmuel Kaniel, </author> <title> Estimates for Some Computational Techniques in Linear Algebra, </title> <booktitle> Mathematics of Computation 20 (1966), </booktitle> <pages> 369-378. </pages>
Reference-contexts: In the early fifties, the method of Conjugate Gradients was discovered independently by Hestenes [10] and Stiefel [15]; shortly thereafter, they jointly published what is considered the seminal reference on CG [11]. Convergence bounds for CG in terms of Chebyshev polynomials were developed by Kaniel <ref> [12] </ref>. A more thorough analysis of CG convergence is provided by van der Sluis and van der Vorst [16]. CG was popularized as an iterative method for large, sparse matrices by Reid [13] in 1971.
Reference: [13] <author> John K. Reid, </author> <title> On the Method of Conjugate Gradients for the Solution of Large Sparse Systems of Linear Equations, Large Sparse Sets of Linear Equations (London and New York) (John K. Reid, </title> <editor> ed.), </editor> <publisher> Academic Press, </publisher> <address> London and New York, </address> <year> 1971, </year> <pages> pp. 231-254. </pages>
Reference-contexts: Convergence bounds for CG in terms of Chebyshev polynomials were developed by Kaniel [12]. A more thorough analysis of CG convergence is provided by van der Sluis and van der Vorst [16]. CG was popularized as an iterative method for large, sparse matrices by Reid <ref> [13] </ref> in 1971. CG was generalized to nonlinear problems in 1964 by Fletcher and Reeves [6], based on work by Davidon [4] and Fletcher and Powell [5]. Convergence of nonlinear CG with inexact line searches was analyzed by Daniel [3].
Reference: [14] <author> E. Schmidt, </author> <booktitle> Title unknown, Rendiconti del Circolo Matematico di Palermo 25 (1908), </booktitle> <pages> 53-77. </pages>
Reference-contexts: Here, I have cheated by using the diagonal of f 00 at the solution point x to precondition every iteration. A Notes Conjugate Direction methods were probably first presented by Schmidt <ref> [14] </ref> in 1908, and were independently reinvented by Fox, Huskey, and Wilkinson [7] in 1948. In the early fifties, the method of Conjugate Gradients was discovered independently by Hestenes [10] and Stiefel [15]; shortly thereafter, they jointly published what is considered the seminal reference on CG [11].
Reference: [15] <author> Eduard Stiefel, </author> <title> Uber einige Methoden der Relaxationsrechnung, </title> <journal> Zeitschrift f ur Angewandte Mathe matik und Physik 3 (1952), </journal> <volume> no. 1, </volume> <pages> 1-33. </pages>
Reference-contexts: A Notes Conjugate Direction methods were probably first presented by Schmidt [14] in 1908, and were independently reinvented by Fox, Huskey, and Wilkinson [7] in 1948. In the early fifties, the method of Conjugate Gradients was discovered independently by Hestenes [10] and Stiefel <ref> [15] </ref>; shortly thereafter, they jointly published what is considered the seminal reference on CG [11]. Convergence bounds for CG in terms of Chebyshev polynomials were developed by Kaniel [12]. A more thorough analysis of CG convergence is provided by van der Sluis and van der Vorst [16].
Reference: [16] <author> A. van der Sluis and H. A. van der Vorst, </author> <title> The Rate of Convergence of Conjugate Gradients, </title> <journal> Numerische Mathematik 48 (1986), </journal> <volume> no. 5, </volume> <pages> 543-560. </pages>
Reference-contexts: Convergence bounds for CG in terms of Chebyshev polynomials were developed by Kaniel [12]. A more thorough analysis of CG convergence is provided by van der Sluis and van der Vorst <ref> [16] </ref>. CG was popularized as an iterative method for large, sparse matrices by Reid [13] in 1971. CG was generalized to nonlinear problems in 1964 by Fletcher and Reeves [6], based on work by Davidon [4] and Fletcher and Powell [5].
References-found: 16

