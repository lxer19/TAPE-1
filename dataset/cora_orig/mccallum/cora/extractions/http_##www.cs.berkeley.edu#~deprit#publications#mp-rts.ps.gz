URL: http://www.cs.berkeley.edu/~deprit/publications/mp-rts.ps.gz
Refering-URL: http://www.cs.berkeley.edu/~deprit/publications.html
Root-URL: 
Title: RUNTIME SUPPORT FOR PORTABLE DISTRIBUTED DATA STRUCTURES  
Author: Chih-Po Wen, Soumen Chakrabarti, Etienne Deprit, Arvind Krishnamurthy, Katherine Yelick 
Address: Berkeley, California 94720 USA  
Affiliation: Computer Science Division, Department of EECS University of California,  
Abstract: Multipol is a library of distributed data structures designed for irregular applications, including those with asynchronous communication patterns. In this paper, we describe the Multipol runtime layer, which provides an efficient and portable abstraction underlying the data structures. It contains a thread system to express computations with varying degrees of parallelism and to support multiple threads per processor for hiding communication latency. To simplify programming in a mul-tithreaded environment, Multipol threads are small, finite-length computations that are executed atomically. Rather than enforcing a single scheduling policy on threads, users may write their own schedulers or choose one of the schedulers provided by Multipol. The system is designed for distributed memory architectures and performs communication optimizations such as message aggregation to improve efficiency on machines with high communication startup overhead. The runtime system currently runs on the Thinking Machines CM5, Intel Paragon, and IBM SP1, and is being ported to a network of workstations. Multipol applications include an event-driven timing simulator [1], an eigenvalue solver [2], and a program that solves the phylogeny problem [3]. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Chih-Po Wen and Katherine Yelick. </author> <title> Portable parallel asynchronous simulation on distributed memory architectures. </title> <booktitle> In Internation Conference on Parallel Processing, </booktitle> <year> 1995. </year> <note> To appear. </note>
Reference-contexts: 1 INTRODUCTION Multipol is a library of distributed data structures for irregular applications such as discrete event simulation <ref> [1] </ref>, symbolic computation [4], and search problems [3]. These applications have conditional control constructs and dynamic data structures that produce unpredictable communication patterns and computation costs.
Reference: [2] <institution> Soumen Chakrabarti and Abhiram Ranade and Katherine Yelick Randomized Load Balancing for Tree Structured Computation In IEEE Scalable High Performance Computing Conference, </institution> <year> 1995. </year>
Reference: [3] <author> Jeff Jones. </author> <title> Exploiting parallelism in the perfect phylogeny computation. </title> <type> Master's thesis (TR-95-869), </type> <institution> University of California, Berkeley, Computer Science Division, </institution> <month> December </month> <year> 1994. </year>
Reference-contexts: 1 INTRODUCTION Multipol is a library of distributed data structures for irregular applications such as discrete event simulation [1], symbolic computation [4], and search problems <ref> [3] </ref>. These applications have conditional control constructs and dynamic data structures that produce unpredictable communication patterns and computation costs. <p> The scheduling of different modules or data structures can then be tuned separately without affecting other parts of the program. 4 LOAD BALANCE Many irregular applications can be naturally decomposed into parallel tasks. These applications include the phylogeny problem <ref> [3] </ref>, which uses a parallel branch and bound algorithm to search for the largest character subset that forms a perfect phylogeny tree, a divide-and-conquer eigenvalue algorithm, and the Grobner basis problem, which reduces sets of polynomials in parallel with respect to a growing basis.
Reference: [4] <author> Soumen Chakrabarti and Katherine Yelick. </author> <title> Distributed data structures and algorithms for Grobner basis computation. </title> <booktitle> Lisp and Symbolic Computation, </booktitle> <year> 1994. </year>
Reference-contexts: 1 INTRODUCTION Multipol is a library of distributed data structures for irregular applications such as discrete event simulation [1], symbolic computation <ref> [4] </ref>, and search problems [3]. These applications have conditional control constructs and dynamic data structures that produce unpredictable communication patterns and computation costs. <p> In the Grobner basis application, for example, there are two types of tasks, one of which must be scheduled at a higher priority to keep the memory utilization and total work low <ref> [4] </ref>. In this discussion, we use Parswec, a speculative timing simulator [11] and Tripuzzle, a state space search program that counts the number of unique solutions for the tripuzzle problem [12].
Reference: [5] <author> H. Berryman, J. Saltz, and J. Scroggs. </author> <title> Execution time support for adaptive scientific algorithms on distributed memory multiprocessors. </title> <journal> Concurrency: Practice and Experience, </journal> <pages> pages 159-178, </pages> <month> June </month> <year> 1991. </year> <title> Runtime Support for Portable Distributed Data Structures 11 </title>
Reference-contexts: Compiler analysis and runtime preprocessing, such as that used in PARTI <ref> [5] </ref>, are not effective in asynchronous applications, since the computation patterns change dynamically. An overview of the Multipol library with its underlying runtime layer and example applications is depicted in Figure 1. <p> Nexus is designed to support heterogeneous computing, and is built on top of standard thread packages, which are more heavy weight. TAM and Cilk programs are synchronized in a data-flow fashion, while most Multipol programs communicate and synchronize via shared data structures. PARTI <ref> [5] </ref> performs runtime message aggregation, but it does not handle dynamic communication patterns where runtime preprocessing techniques cannot be applied.
Reference: [6] <author> D. Culler, A. Sah, K. Schauser, T. von Eicken, and J. Wawrzynek. </author> <title> Fine-grain parallelism with minimal hardware support: A compiler-controlled threaded abstract machine. </title> <booktitle> In Proc. of 4th Int. Conf. on Architectural Support for Programming Languages and Operating Systems, </booktitle> <address> Santa-Clara, CA, </address> <month> April </month> <year> 1991. </year>
Reference-contexts: The threads also create opportunities for overlapping communication latency with computation, and for aggregating multiple remote operations in large physical messages to reduce communication overhead. In addition to threading support, the runtime system also provides a set of portable communication primitives for bulk-synchronous communication and asynchronous communication. Like TAM <ref> [6] </ref> and Nexus [7], the runtime system can be used as a compilation target, but it is primarily designed for direct programming by library and application programmers. Our design and implementation targets distributed memory architectures, including the Thinking Machines CM5, Intel Paragon, IBM SP1, and future networks of workstations. <p> Portable Distributed Data Structures 9 first decreases due to better communication efficiency, and then increases due to redundant work. size changes from 1K bytes to 16K bytes due to a proportional increase in redundant work. 6 RELATED WORK Past research has produced a variety of runtime systems such as TAM <ref> [6] </ref>, the Chare kernel [15] Cilk [12], and Nexus [7]. TAM threads are similar in spirit to the Multipol atomic threads in that they are used to hide latency.
Reference: [7] <author> Ian Foster, Carl Kesselman, Robert Olson, and Steve Tuccke. </author> <title> Nexus: An interoperability toolkit for parallel and distributed computer systems. </title> <type> Technical Report ANL/MCS-TM-189, </type> <institution> Argonne National Laboratory, </institution> <year> 1991. </year>
Reference-contexts: In addition to threading support, the runtime system also provides a set of portable communication primitives for bulk-synchronous communication and asynchronous communication. Like TAM [6] and Nexus <ref> [7] </ref>, the runtime system can be used as a compilation target, but it is primarily designed for direct programming by library and application programmers. Our design and implementation targets distributed memory architectures, including the Thinking Machines CM5, Intel Paragon, IBM SP1, and future networks of workstations. <p> better communication efficiency, and then increases due to redundant work. size changes from 1K bytes to 16K bytes due to a proportional increase in redundant work. 6 RELATED WORK Past research has produced a variety of runtime systems such as TAM [6], the Chare kernel [15] Cilk [12], and Nexus <ref> [7] </ref>. TAM threads are similar in spirit to the Multipol atomic threads in that they are used to hide latency.
Reference: [8] <author> Thorsten von Eicken, David E. Culler, Seth Copen Goldstein, and Klaus Erik Schauser. </author> <title> Active messages: a mechanism for integrated communication and computation. </title> <booktitle> In International Symposium on Computer Architecture, </booktitle> <year> 1992. </year>
Reference-contexts: Without knowledge from the compiler or the programmer, the runtime system would have to make conservative assumptions and save the entire processor state and the stack frame. 2.2 Invoking Remote Computation Communication layers such as active messages <ref> [8] </ref> provide mechanisms for implementing computation migration. <p> Direct use of active messages as remote request handlers, however, presents three major problems for implementing general data structures other than simple memory cells: Most active message layers either assume a fixed number of arguments (e.g., 4 words <ref> [8, 9] </ref>), whose size is tailored to the network packet size, or require the programmer to pre-allocate remote memory for holding ar Runtime Support for Portable Distributed Data Structures 5 guments [10]. The lack of flexibility makes it difficult to express dynamic communication patterns.
Reference: [9] <author> Eric A. Brewer and Robert D. Blumofe. Strata: </author> <title> A multi-layer communication library. </title> <note> To appear as a MIT Technical Report, </note> <month> February </month> <year> 1994. </year>
Reference-contexts: Direct use of active messages as remote request handlers, however, presents three major problems for implementing general data structures other than simple memory cells: Most active message layers either assume a fixed number of arguments (e.g., 4 words <ref> [8, 9] </ref>), whose size is tailored to the network packet size, or require the programmer to pre-allocate remote memory for holding ar Runtime Support for Portable Distributed Data Structures 5 guments [10]. The lack of flexibility makes it difficult to express dynamic communication patterns.
Reference: [10] <author> David Culler, Kim Keeton, Lok Tim Liu, Alan Mainwaring, Rich Martin, Steve Rodrigues, and Kristin Wright. </author> <title> The generic active message interface specification. </title> <type> Unpublished, </type> <year> 1994. </year>
Reference-contexts: other than simple memory cells: Most active message layers either assume a fixed number of arguments (e.g., 4 words [8, 9]), whose size is tailored to the network packet size, or require the programmer to pre-allocate remote memory for holding ar Runtime Support for Portable Distributed Data Structures 5 guments <ref> [10] </ref>. The lack of flexibility makes it difficult to express dynamic communication patterns. Because active messages do not implement any flow control, they usually require the programmer to follow a request/reply protocol to avoid network level deadlock.
Reference: [11] <author> Chih-Po Wen and Katherine Yelick. </author> <title> Parallel timing simulation on a distributed memory multiprocessor. </title> <booktitle> In International Conference on CAD, </booktitle> <address> Santa Clara, CA, </address> <month> November </month> <year> 1993. </year> <note> An earlier version appeared as UCB Technical Report CSD-93-723. </note>
Reference-contexts: In the Grobner basis application, for example, there are two types of tasks, one of which must be scheduled at a higher priority to keep the memory utilization and total work low [4]. In this discussion, we use Parswec, a speculative timing simulator <ref> [11] </ref> and Tripuzzle, a state space search program that counts the number of unique solutions for the tripuzzle problem [12]. In Parswec, a digital circuit is decomposed into subcircuits that are distributed among the processors, and the simulation proceeds speculatively using an algorithm similar to Timewarp [13].
Reference: [12] <author> Robert D. Blumofe, Christopher F. Joerg, Bradley C. Kuszmaul, Charles E. Leiserson, Keith H. Randall, and Yuli Zhou. Cilk: </author> <title> An efficient multithreaded runtime system. </title> <booktitle> In Principles and Practice of Parallel Programming, </booktitle> <year> 1995. </year>
Reference-contexts: In this discussion, we use Parswec, a speculative timing simulator [11] and Tripuzzle, a state space search program that counts the number of unique solutions for the tripuzzle problem <ref> [12] </ref>. In Parswec, a digital circuit is decomposed into subcircuits that are distributed among the processors, and the simulation proceeds speculatively using an algorithm similar to Timewarp [13]. A separate thread is created for each subcircuit to simulate its state. <p> Runtime systems such as Cilk <ref> [12] </ref> adopt a built-in load balancer and treat threads not only as a latency-hiding mechanism, but also as units of load balance which can be migrated freely. This approach suffers the same problem as providing fixed scheduling policies different applications require different load balancing policies. <p> decreases due to better communication efficiency, and then increases due to redundant work. size changes from 1K bytes to 16K bytes due to a proportional increase in redundant work. 6 RELATED WORK Past research has produced a variety of runtime systems such as TAM [6], the Chare kernel [15] Cilk <ref> [12] </ref>, and Nexus [7]. TAM threads are similar in spirit to the Multipol atomic threads in that they are used to hide latency.
Reference: [13] <author> D.R. Jefferson. </author> <title> Virtual time. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 7(3), </volume> <month> July </month> <year> 1985. </year>
Reference-contexts: In Parswec, a digital circuit is decomposed into subcircuits that are distributed among the processors, and the simulation proceeds speculatively using an algorithm similar to Timewarp <ref> [13] </ref>. A separate thread is created for each subcircuit to simulate its state. These threads are ready to start any time (subject to storage constraints), since they can speculate on the input values. However, some threads are more likely to lead to redundant work than others.
Reference: [14] <author> Chih-Po Wen. </author> <title> The distributed task queue user's guide. </title> <type> Unpublished, </type> <year> 1994. </year>
Reference-contexts: We separate out load balancing policies from the Multipol runtime system and put the functionality in data structures such as a distributed task queue <ref> [14] </ref>. The programmer can then select different data structures or implement new ones to tailor the load balancing policy to a particular application.
Reference: [15] <author> Wei Shu and L.V. Kale. </author> <title> Chare kernel a runtime support system for parallel computations. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 11 </volume> <pages> 198-211, </pages> <year> 1991. </year>
Reference-contexts: 9 first decreases due to better communication efficiency, and then increases due to redundant work. size changes from 1K bytes to 16K bytes due to a proportional increase in redundant work. 6 RELATED WORK Past research has produced a variety of runtime systems such as TAM [6], the Chare kernel <ref> [15] </ref> Cilk [12], and Nexus [7]. TAM threads are similar in spirit to the Multipol atomic threads in that they are used to hide latency.
References-found: 15

