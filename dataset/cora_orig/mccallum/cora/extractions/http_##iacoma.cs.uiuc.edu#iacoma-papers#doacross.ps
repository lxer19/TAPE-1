URL: http://iacoma.cs.uiuc.edu/iacoma-papers/doacross.ps
Refering-URL: http://iacoma.cs.uiuc.edu/papers.html
Root-URL: http://www.cs.uiuc.edu
Title: An Efficient Algorithm for the Run-time Parallelization of DOACROSS Loops 1  
Author: Ding-Kai Chen Josep Torrellas Pen-Chung Yew 
Address: IL 61801  
Affiliation: Center for Supercomputing Research and Development University of Illinois at Urbana-Champaign,  
Abstract: While automatic parallelization of loops usually relies on compile-time analysis of data dependences, for some loops the data dependences cannot be determined at compile time. An example is loops accessing arrays with subscripted subscripts. To parallelize these loops, it is necessary to perform run-time analysis. In this paper, we present a new algorithm to parallelize these loops at run time. Our scheme handles any type of data dependence in the loop without requiring any special architectural support in the multiprocessor. Furthermore, compared to an older scheme with the same generality, our scheme significantly reduces the amount of processor communication required and increases the overlap among dependent iterations. We evaluate our algorithm with parameterized loops running on the 32-processor Cedar shared-memory multiprocessor. The results show speedups over the serial code of up to 14 with the full overhead of run-time analysis and of up to 27 if part of the analysis is reused across loop invocations. Moreover, the algorithm outperforms the older scheme in nearly all cases, reaching speedups of up to times when the loop has many dependences. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> U. Banerjee. </author> <title> Dependence Analysis for Supercomputing. </title> <publisher> Kluwer Academic Publishers, Norwell, </publisher> <address> MA, </address> <year> 1988. </year>
Reference-contexts: 1 Introduction Loop-level parallelism is an important source of speedup in multiprocessors that exploit medium-grain parallelism. To parallelize a loop, it is necessary to identify the data dependence relations between loop iterations via dependence analysis <ref> [1] </ref>. If the loop does not have any dependences across iterations, then it can run in parallel. Otherwise, we have to run it serially or, if we want to exploit some parallelism, insert proper synchronization to ensure that the memory accesses are performed in the correct sequence.
Reference: [2] <author> D.-K. Chen, J. Torrellas, and P.-C. Yew. </author> <title> Run-time parallelization for DOACROSS loops. </title> <type> CSRD Report No. 1345, </type> <institution> Center for Supercomputing Research and Development, Univ. of Illinois at Urbana-Champaign, </institution> <month> April </month> <year> 1994. </year>
Reference-contexts: To see how the speedup varies with the number of processors, we also ran several experiments with mostly-serial and mostly-parallel loops using various numbers of processors. With up to 32 processors, we obtain almost linear speedups when measuring (1) both inspector and executor; and (2) executor only <ref> [2] </ref>. We expect the speedup to be scalable with even larger numbers of processors. 4.4 Performance Comparison with Zhu-Yew's Scheme Since Zhu-Yew's scheme has the same applicability and requires the same system support as ours, we have compared its performance to that of our algorithm.
Reference: [3] <author> D.-K. Chen and P.-C. Yew. </author> <title> A scheme for effective execution of irregular DOACROSS loops. </title> <booktitle> In Int'l. Conf. on Parallel Processing, </booktitle> <pages> pages 285-292, </pages> <month> August </month> <year> 1992. </year> <note> Also available as CSRD tech report No. 1192. </note>
Reference-contexts: Some DOACROSS loops can be transformed into fully parallel loops. For example, if the difference in iteration number between dependent iterations (called dependence distance) is known at compile time, we can use techniques like loop-alignment, loop skewing, or dependence uniformization <ref> [8, 13, 3, 11] </ref>. Then the loop can be run in parallel. Unfortunately, many times it is not possible to determine the dependence distance at compile time. An example is when array elements are accessed via subscripted subscripts. <p> In these cases, the dependence Dependence Example Method to Parallelize Type f (i) g (i) The Loop Uniform Distance 2i+3 2i+1 Loop Alignment [8], Loop Skewing [13] (if loop level&gt; 1) Non-Uniform Distance 2i+9 3i+3 Dependence Uniformization <ref> [11, 3] </ref> (if loop level&gt; 1) Unknown Distance I1 (i) I2 (i) Run-Time Parallelization Table 1: Sample dependence types and parallelization methods. distances are known or can be estimated at compile-time, and parallelizing compilers can generate proper synchronization instructions to preserve dependences.
Reference: [4] <author> V. Krothapalli and P. Sadayappan. </author> <title> An approach to synchronization of parallel computing. </title> <booktitle> In ACM Int'l. Conf. on Supercomputing, </booktitle> <pages> pages 573-581, </pages> <month> June </month> <year> 1988. </year>
Reference-contexts: For these loops we can still perform the dependence analysis at run-time and maybe execute the loop in parallel. This approach is called run-time parallelization. Much previous research has been done to design effective run-time parallelization algorithms <ref> [4, 6, 7, 10, 14] </ref>The main differences among the schemes proposed are the types of dependence patterns that are handled and the required system or architecture support. The key to success in these schemes is to minimize the time spent on the analysis of dependences and on process synchronization. <p> To do so, each data element requires more than one key field. However, it still requires high communication as Zhu-Yew's scheme. Other, less general schemes have been proposed <ref> [4, 9] </ref>. They restrict the problem in three ways. First, no two index elements of a given index array (I1 and I2 in Figure 1 (b)) have the same values. This makes scheduling simpler. Second, they require a serial pre-processing loop, which can reduce speedup.
Reference: [5] <author> D. Kuck et al. </author> <title> The Cedar system and an initial performance study. </title> <booktitle> In 20th Int'l Symp. on Computer Architecture, </booktitle> <month> May </month> <year> 1993. </year>
Reference-contexts: Furthermore, compared to an older scheme with the same generality [14], it speeds up execution by significantly reducing the amount of communication required and by increasing the overlap among dependent iterations. The effectiveness of this algorithm is evaluated via measurements of parameterized loops in the 32-processor Cedar shared-memory multiprocessor <ref> [5] </ref>. The results show speedups that reach up to 14 with the full overhead of the analysis and up to 27 if part of the analysis work is reused across loop invocations. <p> In addition, it removes redundant operations in the inspector. It may, however, increase the spinlocking during execution since the processor may have to wait for A (I2 (i)).key to become valid. 4 The Experimental System Our experiments are timing runs on Cedar <ref> [5] </ref>, a 32-processor scalable shared-memory multiprocessor designed at the Center for Supercomputing Research and Development. The machine has 4 clusters of 8 processors each. The latency to access the cache is 170 ns, while accessing the cluster memory takes ~1190 ns and accessing the global memory takes ~2210 ns.
Reference: [6] <author> S.-T. Leung and J. Zahorjan. </author> <title> Improving the performance of runtime parallelization. </title> <booktitle> In 4th ACM SIG-PLAN Symp. on Principles and Practice of Parallel Programming, </booktitle> <pages> pages 83-91, </pages> <month> May </month> <year> 1993. </year>
Reference-contexts: For these loops we can still perform the dependence analysis at run-time and maybe execute the loop in parallel. This approach is called run-time parallelization. Much previous research has been done to design effective run-time parallelization algorithms <ref> [4, 6, 7, 10, 14] </ref>The main differences among the schemes proposed are the types of dependence patterns that are handled and the required system or architecture support. The key to success in these schemes is to minimize the time spent on the analysis of dependences and on process synchronization. <p> Finally, if the index arrays are not one-to-one mappings, they require shared-memory allocation. Unfortunately, this can be slow and complicated in large-scale multiprocessors. How to do it is not described. Lately, schemes for loops without output dependences have been proposed by Saltz et al. [10] and Leung and Zahorjan <ref> [6] </ref>. This assumption simplifies the problem. The emphasis of Saltz et al.'s work is on the scheduling issues. The results show that the best scheduling method is to statically partition the iterations among the processors and to rearrange the iteration order within each partition.
Reference: [7] <author> S. Midkiff and D. Padua. </author> <title> Compiler algorithms for synchronization. </title> <journal> IEEE Trans. on Computers, </journal> <volume> C-36(12), </volume> <month> December </month> <year> 1987. </year>
Reference-contexts: For these loops we can still perform the dependence analysis at run-time and maybe execute the loop in parallel. This approach is called run-time parallelization. Much previous research has been done to design effective run-time parallelization algorithms <ref> [4, 6, 7, 10, 14] </ref>The main differences among the schemes proposed are the types of dependence patterns that are handled and the required system or architecture support. The key to success in these schemes is to minimize the time spent on the analysis of dependences and on process synchronization. <p> Unfortunately, it requires many memory accesses. Indeed, for each iteration of the repeat loop, each unexecuted iteration i requires at least 3r memory accesses, where r is the number of references to array A per iteration. A second scheme, proposed by Midkiff and Padua <ref> [7] </ref>, improved Zhu-Yew's scheme by allowing concurrent reads to the same array element by several iterations. To do so, each data element requires more than one key field. However, it still requires high communication as Zhu-Yew's scheme. Other, less general schemes have been proposed [4, 9]. <p> The results can also be applied to another scheme proposed by Mid-kiff and Padua <ref> [7] </ref> because they use the same inspector algorithm and therefore have similar communication patterns. Table 4 shows the ratio between the execution time of Zhu-Yew's algorithm and ours for 32 processors. The table shows that our scheme is nearly always faster than Zhu-Yew's.
Reference: [8] <author> D. A. Padua. </author> <title> Multiprocessors: Discussion of Some Theoretical and Practical Problems. </title> <type> PhD thesis, </type> <institution> Dept. of Computer Science, Univ. of Illinois at Urbana-Champaign, </institution> <month> October </month> <year> 1979. </year>
Reference-contexts: Some DOACROSS loops can be transformed into fully parallel loops. For example, if the difference in iteration number between dependent iterations (called dependence distance) is known at compile time, we can use techniques like loop-alignment, loop skewing, or dependence uniformization <ref> [8, 13, 3, 11] </ref>. Then the loop can be run in parallel. Unfortunately, many times it is not possible to determine the dependence distance at compile time. An example is when array elements are accessed via subscripted subscripts. <p> In the first and the second rows of Table 1, f (i) and g (i) are linear functions of the loop index with constant coefficients. In these cases, the dependence Dependence Example Method to Parallelize Type f (i) g (i) The Loop Uniform Distance 2i+3 2i+1 Loop Alignment <ref> [8] </ref>, Loop Skewing [13] (if loop level&gt; 1) Non-Uniform Distance 2i+9 3i+3 Dependence Uniformization [11, 3] (if loop level&gt; 1) Unknown Distance I1 (i) I2 (i) Run-Time Parallelization Table 1: Sample dependence types and parallelization methods. distances are known or can be estimated at compile-time, and parallelizing compilers can generate proper
Reference: [9] <author> C. Polychronopoulos. </author> <title> Advanced loop optimizations for parallel computers. </title> <editor> In E. Houstis, T. Pap-atheodorou, and C. Polychronopoulos, editors, </editor> <booktitle> Lecture Notes in Computer Science No. 297: Proc. First Int'l. Conf. on Supercomputing, </booktitle> <address> Athens, Greece, </address> <pages> pages 255-277, </pages> <address> New York, June 1987. </address> <publisher> Springer-Verlag. </publisher>
Reference-contexts: To do so, each data element requires more than one key field. However, it still requires high communication as Zhu-Yew's scheme. Other, less general schemes have been proposed <ref> [4, 9] </ref>. They restrict the problem in three ways. First, no two index elements of a given index array (I1 and I2 in Figure 1 (b)) have the same values. This makes scheduling simpler. Second, they require a serial pre-processing loop, which can reduce speedup.
Reference: [10] <author> J. Saltz, R. Mirchandaney, and K. Crowley. </author> <title> Run-time parallelization and scheduling of loops. </title> <journal> IEEE Trans. on Computers, </journal> <volume> 40(5) </volume> <pages> 603-611, </pages> <month> May </month> <year> 1991. </year>
Reference-contexts: For these loops we can still perform the dependence analysis at run-time and maybe execute the loop in parallel. This approach is called run-time parallelization. Much previous research has been done to design effective run-time parallelization algorithms <ref> [4, 6, 7, 10, 14] </ref>The main differences among the schemes proposed are the types of dependence patterns that are handled and the required system or architecture support. The key to success in these schemes is to minimize the time spent on the analysis of dependences and on process synchronization. <p> The values of arrays I1 and I2 are assumed to remain constant during the execution of one invocation of the loop, although they are allowed to change outside the loop. In general, run-time parallelization schemes have two stages, namely inspector and executor <ref> [10, 14] </ref>. Both are performed at runtime. The inspector determines the dependence relations among the data accesses. The executor uses this information to execute the iterations in parallel in an order that preserves the dependences. <p> Finally, if the index arrays are not one-to-one mappings, they require shared-memory allocation. Unfortunately, this can be slow and complicated in large-scale multiprocessors. How to do it is not described. Lately, schemes for loops without output dependences have been proposed by Saltz et al. <ref> [10] </ref> and Leung and Zahorjan [6]. This assumption simplifies the problem. The emphasis of Saltz et al.'s work is on the scheduling issues. The results show that the best scheduling method is to statically partition the iterations among the processors and to rearrange the iteration order within each partition.
Reference: [11] <author> T. H. Tzen and L. Ni. </author> <title> Dependence uniformization: A loop parallelization technique. </title> <journal> IEEE Trans. on Parallel and Distributed Systems, </journal> <volume> 4(5), </volume> <month> May </month> <year> 1993. </year>
Reference-contexts: Some DOACROSS loops can be transformed into fully parallel loops. For example, if the difference in iteration number between dependent iterations (called dependence distance) is known at compile time, we can use techniques like loop-alignment, loop skewing, or dependence uniformization <ref> [8, 13, 3, 11] </ref>. Then the loop can be run in parallel. Unfortunately, many times it is not possible to determine the dependence distance at compile time. An example is when array elements are accessed via subscripted subscripts. <p> In these cases, the dependence Dependence Example Method to Parallelize Type f (i) g (i) The Loop Uniform Distance 2i+3 2i+1 Loop Alignment [8], Loop Skewing [13] (if loop level&gt; 1) Non-Uniform Distance 2i+9 3i+3 Dependence Uniformization <ref> [11, 3] </ref> (if loop level&gt; 1) Unknown Distance I1 (i) I2 (i) Run-Time Parallelization Table 1: Sample dependence types and parallelization methods. distances are known or can be estimated at compile-time, and parallelizing compilers can generate proper synchronization instructions to preserve dependences.
Reference: [12] <author> M. J. Wolfe. </author> <title> Optimizing Compilers for Supercomputers. </title> <type> PhD thesis, </type> <institution> University of Illinois at Urbana-Champaign, </institution> <year> 1982. </year>
Reference-contexts: Section 5 concludes our study. 2 Background and Run-Time Parallelization Issues In this section, we first present the basic concepts and then the previous work. 2.1 Loop Model and Basic Concept Consider the loop shown in Figure 1 (a). In order to determine if there is any loop-carried dependence <ref> [12] </ref> between statements S p and S q and, if so, compute its dependence distance, we have to solve the following integer equation f (i) = g (i 0 ) and evaluate the distance (i 0 i) within the constraints of the loop limits.
Reference: [13] <author> M. J. Wolfe. </author> <title> Loop skewing: The wavefront method revisited. </title> <journal> Int. J. of Parallel Programming, </journal> <volume> 4(15), </volume> <year> 1986. </year>
Reference-contexts: Some DOACROSS loops can be transformed into fully parallel loops. For example, if the difference in iteration number between dependent iterations (called dependence distance) is known at compile time, we can use techniques like loop-alignment, loop skewing, or dependence uniformization <ref> [8, 13, 3, 11] </ref>. Then the loop can be run in parallel. Unfortunately, many times it is not possible to determine the dependence distance at compile time. An example is when array elements are accessed via subscripted subscripts. <p> In these cases, the dependence Dependence Example Method to Parallelize Type f (i) g (i) The Loop Uniform Distance 2i+3 2i+1 Loop Alignment [8], Loop Skewing <ref> [13] </ref> (if loop level&gt; 1) Non-Uniform Distance 2i+9 3i+3 Dependence Uniformization [11, 3] (if loop level&gt; 1) Unknown Distance I1 (i) I2 (i) Run-Time Parallelization Table 1: Sample dependence types and parallelization methods. distances are known or can be estimated at compile-time, and parallelizing compilers can generate proper synchronization instructions to
Reference: [14] <author> C.-Q. Zhu and P.-C. Yew. </author> <title> A scheme to enforce data dependence on large multiprocessor systems. </title> <journal> IEEE Trans. on Software Enginering, </journal> <pages> pages 726-739, </pages> <month> June </month> <year> 1987. </year>
Reference-contexts: For these loops we can still perform the dependence analysis at run-time and maybe execute the loop in parallel. This approach is called run-time parallelization. Much previous research has been done to design effective run-time parallelization algorithms <ref> [4, 6, 7, 10, 14] </ref>The main differences among the schemes proposed are the types of dependence patterns that are handled and the required system or architecture support. The key to success in these schemes is to minimize the time spent on the analysis of dependences and on process synchronization. <p> In this paper, we describe and evaluate a new algorithm for the run-time parallelization of DOACROSS loops. Our scheme handles any type of dependence pattern without requiring any special architectural support. Furthermore, compared to an older scheme with the same generality <ref> [14] </ref>, it speeds up execution by significantly reducing the amount of communication required and by increasing the overlap among dependent iterations. The effectiveness of this algorithm is evaluated via measurements of parameterized loops in the 32-processor Cedar shared-memory multiprocessor [5]. <p> The values of arrays I1 and I2 are assumed to remain constant during the execution of one invocation of the loop, although they are allowed to change outside the loop. In general, run-time parallelization schemes have two stages, namely inspector and executor <ref> [10, 14] </ref>. Both are performed at runtime. The inspector determines the dependence relations among the data accesses. The executor uses this information to execute the iterations in parallel in an order that preserves the dependences. <p> The first one was proposed by Zhu and Yew <ref> [14] </ref>. Their scheme is general enough to handle any dependence pattern. Given a loop like that in Figure 1 (b), the compiler transforms it into the code shown in Figure 2. <p> We used loops with varying pa rameters, such as number of iterations and references. The results show that our algorithm gives speedups that reach 14 if the inspector is not reused and 27 if it is. Furthermore, our algorithm outperforms Zhu-Yew's scheme <ref> [14] </ref> in nearly all cases, reaching a 37-fold speedup when the loop has many dependences. There are a few issues in run-time parallelization that we are currently working on.
References-found: 14

