URL: http://www.cm.deakin.edu.au/~zijian/Papers/ecml98-tr-ting.ps.gz
Refering-URL: http://www.cm.deakin.edu.au/~zijian/publications.html
Root-URL: 
Phone: 2  
Title: Boosting Trees for Cost-Sensitive Classifications  
Author: Kai Ming Ting and Zijian Zheng 
Keyword: inductive learning, boosting, cost-sensitive classification, decision tree learning, machine learning.  
Address: Hamilton, New Zealand.  3217, Australia.  
Affiliation: 1 Department of Computer Science, University of Waikato,  School of Computing and Mathematics, Deakin University, Vic  
Abstract: This paper explores two boosting techniques for cost-sensitive tree classification in the situation where misclassification costs change very often. Ideally, one would like to have only one induction, and use the induced model for different misclassification costs. Thus, it demands robustness of the induced model against cost changes. Combining multiple trees gives robust predictions against this change. We demonstrate that ordinary boosting combined with the minimum expected cost criterion to select the prediction class is a good solution under this situation. We also introduce a variant of the ordinary boosting procedure which utilizes the cost information during training. We show that the proposed technique performs better than the ordinary boosting in terms of misclassification cost. However, this technique requires to induce a set of new trees every time the cost changes. Our empirical investigation also reveals some interesting behavior of boosting decision trees for cost-sensitive classification. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Breiman, L. </author> <year> (1996), </year> <title> Bias, Variance, and Arcing Classifiers, </title> <type> Technical Report 460, </type> <institution> Department of Statistics, University of California, Berkeley, </institution> <address> CA. </address>
Reference: <author> Breiman, L., J.H. Friedman, R.A. Olshen, & C.J. </author> <title> Stone (1984), Classification And Regression Trees, </title> <address> Belmont, CA: </address> <publisher> Wadsworth. </publisher>
Reference-contexts: In multi-class domains, at least one of the entries is 1. The only reason for this unity condition is to allow us to measure the number of high cost errors, which is one of two important measures in cost-sensitive classifications to be defined later. One 10-fold cross-validation <ref> (Breiman, Friedman, Olshen, and Stone, 1984) </ref> is carried out in each domain, except in the Waveform domain where 10 pairs of training set of size 300 and test set of size 5000 are randomly generated.
Reference: <author> Freund, Y. & R.E. </author> <title> Schapire (1996), Experiments with a new boosting algo rithm, </title> <booktitle> in Proceedings of the Thirteenth International Conference on Machine Learning, </booktitle> <pages> pp. 148-156, </pages> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Section 5 describes related work, and we summarize our findings in the final section. 2 Boosting and Cost-Boosting Here, Boosting is implemented by maintaining a weight for each training example (Quinlan, 1996) rather than drawing a succession of independent bootstrap samples from the original examples <ref> (Freund & Schapire, 1996) </ref>. Boosting induces multiple individual classifiers in sequential trials. At the end of each trial, the vector of weights is adjusted to reflect its importance for the next induction trial. This adjustment effectively increases the weights of misclassified examples.
Reference: <author> Knoll, U., G. Nakhaeizadeh & B. </author> <month> Tausend </month> <year> (1994), </year> <title> Cost-sensitive pruning of decision trees, </title> <booktitle> in Proceedings of the Eighth European Conference on Machine Learning, </booktitle> <pages> pp. 383-386. </pages> <address> Berlin, Germany: </address> <publisher> Springer-Verlag. </publisher>
Reference: <author> Kubat, M. </author> <year> (1996), </year> <title> Second Tier for Decision Trees, </title> <booktitle> in Proceedings of the Thir teenth International Conference on Machine Learning, </booktitle> <pages> pp. 293-301. </pages>
Reference: <author> Merz, C.J. </author> & <note> P.M. Murphy (1997), UCI Repository of machine learning databases [http:// www.ics.uci.edu/ mlearn/MLRepository.html]. </note> <institution> Irvine, CA: Univer sity of California, Department of Information and Computer Science. </institution>
Reference-contexts: Note that the first tree in both Boosting and Cost-Boosting is exactly the same as that produced by C4.5c. 3 Experiments In this section, we empirically evaluate Boosting and Cost-Boosting by comparing with C4.5c. Twenty natural domains from the UCI machine learning repository <ref> (Merz & Murphy, 1997) </ref> are used in the experiments. Table 1 gives a description of these domains, including the number of examples, number of classes, number of binary, nominal, and continuous attributes, and default accuracy.
Reference: <author> Michalski, R.S. </author> <year> (1987), </year> <title> How to learn imprecise concepts: a method employing a two-tiered Knowledge Representation for learning, </title> <booktitle> in Proceedings of the Fourth International Workshop on Machine Learning, </booktitle> <pages> pp. 50-58. </pages>
Reference: <author> Michie, D., </author> <title> D.J. Spiegelhalter & C.C. Taylor (1994), Machine Learning, Neural and Statistical Classification, </title> <publisher> Ellis Horwood Limited. </publisher>
Reference-contexts: This can be done for a decision tree learning algorithm, such as C4.5 (Quinlan, 1993), in the following fashion. During the classification stage, an example to be classified is assigned the class with the minimum expected misclassification cost <ref> (Michie, Spiegelhalter, & Taylor, 1994) </ref> at the leaf to which the example is traced down, rather than the class with the maximum weight at the leaf. 3 The expected misclassification cost of a class is the expected cost when predicting this class.
Reference: <author> Norton, S.W. </author> <year> (1989), </year> <title> Generating better decision trees, </title> <booktitle> in Proceedings of the Eleventh International Joint Conference on Artificial Intelligence, </booktitle> <pages> pp. 800 805, </pages> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Furthermore, there are a few decision tree learning algorithms that consider the costs of tests, such as EG2 (Nu~nez, 1991), CS-ID3 (Tan, 1993), and IDX <ref> (Norton, 1989) </ref>. Turney (1995) studies both the misclassification cost and the test cost using a genetic algorithmic search in decision tree induction. All the above systems induce a single cost-sensitive tree, given a cost matrix.
Reference: <author> Nu~nez, M. </author> <year> (1991), </year> <title> The use of background knowledge in decision tree induction, </title> <journal> Machine Learning, </journal> <volume> 6, </volume> <pages> pp. 231-250. </pages>
Reference: <author> Pazzani, M., C. Merz, P. Murphy, K. Ali, T. Hume, & C. </author> <month> Brunk </month> <year> (1994), </year> <title> Re ducing misclassification costs, </title> <booktitle> in Proceedings of the Eleventh International Conference on Machine Learning, </booktitle> <pages> pp. 217-225, </pages> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Quinlan, J.R. </author> <year> (1993), </year> <title> C4.5: Program for machine learning, </title> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: The most straightforward and simple approach to this problem is to alter the prediction selection process during classification, without modifying the classifier learning process. This can be done for a decision tree learning algorithm, such as C4.5 <ref> (Quinlan, 1993) </ref>, in the following fashion.
Reference: <author> Quinlan, J.R. </author> <year> (1996), </year> <title> Bagging, boosting, </title> <booktitle> and C4.5, in Proceedings of the 13th National Conference on Artificial Intelligence, </booktitle> <pages> pp. 725-730, </pages> <publisher> AAAI Press. </publisher>
Reference-contexts: Section 3 reports experiments with C4.5c, Boosting, and Cost-Boosting. Section 4 discusses some related issues. Section 5 describes related work, and we summarize our findings in the final section. 2 Boosting and Cost-Boosting Here, Boosting is implemented by maintaining a weight for each training example <ref> (Quinlan, 1996) </ref> rather than drawing a succession of independent bootstrap samples from the original examples (Freund & Schapire, 1996). Boosting induces multiple individual classifiers in sequential trials. At the end of each trial, the vector of weights is adjusted to reflect its importance for the next induction trial. <p> As any other methods which employ multiple models, one loses comprehensibility of the induced trees in Boosting and Cost-Boosting. 5 Related Work Cost-Boosting is inspired by the instance weight adjustment method in decision tree learning for boosting <ref> (Quinlan, 1996) </ref> and for effective cost-sensitive tree induction (Ting, 1997). The principle of using the minimum expected cost criterion for cost-sensitive classification is described by Michie et al. (1994). There are some research on the induction of a single cost-sensitive tree.
Reference: <author> Schapire, R.E., Y. Freund, P. </author> <title> Bartlett & W.S. Lee (1997), Boosting the margin: A new explanation for the effectiveness of voting methods, </title> <booktitle> in Proceedings of the Fourteenth International Conference on Machine Learning, </booktitle> <pages> pp. 322-330. </pages> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Finally, the individual trees are combined through voting to form a composite classifier. The Boosting procedure is shown as follows. Note that the weight adjustment formula in step (iii) below are from a new version of boosting <ref> (Schapire, Freund, Bartlett, & Lee, 1997) </ref>. Boosting procedure: 4 Given a training set T containing N examples, w k (n) denotes the weight of the nth example at the kth trial, where w 1 (n) = 1=N for every n. <p> Only in 3 multi-class domains, does Boosting have slightly fewer high cost errors than Cost-Boosting. 3.2 The Effect of K on Boosting and Cost-Boosting It has been shown that boosting almost always reduces errors of its base classifier when the number of trials, K, increases <ref> (Schapire et al., 1997) </ref>. In this subsection, we study its effect on Boosting and Cost-Boosting for cost-sensitive classification. Table 3 presents the results of K = 10 and K = 50 with Boosting and Cost-Boosting. <p> Boosting's sharp increase in misclassification cost in some domains, when K increases, is a uncharacteristic behavior in the general framework of boosting <ref> (Schapire et al., 1997) </ref>. This can be explained as follows. While Boosting concentrates to reduce the number of high cost errors, it might inadvertently increase the number of low cost errors. In some cases, the reduction might not off-set the increase. This results an increase in total misclassification cost.
Reference: <author> Tan, M. </author> <year> (1993), </year> <title> Cost-sensitive learning of classification knowledge and its ap plications in robotics, </title> <journal> Machine Learning, </journal> <volume> 13, </volume> <pages> pp. 7-33. </pages>
Reference-contexts: Furthermore, there are a few decision tree learning algorithms that consider the costs of tests, such as EG2 (Nu~nez, 1991), CS-ID3 <ref> (Tan, 1993) </ref>, and IDX (Norton, 1989). Turney (1995) studies both the misclassification cost and the test cost using a genetic algorithmic search in decision tree induction. All the above systems induce a single cost-sensitive tree, given a cost matrix.
Reference: <author> Ting, K.M. </author> <year> (1997), </year> <title> Inducing Cost-Sensitive Trees via Instance-Weighting, </title> <type> Work ing Paper 97/22, </type> <institution> Department of Computer Science, University of Waikato. </institution>
Reference-contexts: In some cases, the reduction might not off-set the increase. This results an increase in total misclassification cost. This phenomenon is also observed while inducing a single cost-sensitive tree <ref> (Ting, 1997) </ref> in datasets with highly skewed class distribution. In most cases, Boosting reduces the number of high cost errors faster than Cost-Boosting as K increases. This suggests that Cost-Boosting's weight adjustment procedure is weaker than that of Boosting in reducing high cost errors. <p> One simple remedy is to employ Boosting's weight adjustment procedure in the first trial, and switch back to employ Cost-Boosting's own procedure from the second trial onward. Instead of equal initial weights, one can make use of the misclassification cost information to set the initial weights <ref> (Ting, 1997) </ref> of Cost-Boosting. Thus, all trees would be cost-sensitive, including the first one. However, this makes little difference in performance, both in terms of the misclassification costs and the number of high cost errors, especially when using high value of K. <p> As any other methods which employ multiple models, one loses comprehensibility of the induced trees in Boosting and Cost-Boosting. 5 Related Work Cost-Boosting is inspired by the instance weight adjustment method in decision tree learning for boosting (Quinlan, 1996) and for effective cost-sensitive tree induction <ref> (Ting, 1997) </ref>. The principle of using the minimum expected cost criterion for cost-sensitive classification is described by Michie et al. (1994). There are some research on the induction of a single cost-sensitive tree.
Reference: <author> Turney, P.D. </author> <year> (1995), </year> <title> Cost-sensitive classification: empirical evaluation of a hy brid genetic decision tree induction algorithm, </title> <journal> Journal of Artificial Intelli gence Research, </journal> <volume> 2, </volume> <pages> pp. 369-409. </pages>
Reference: <author> Webb, </author> <title> G.I. (1996) Cost-sensitive specialization, </title> <booktitle> in Proceedings of the 1996 Pacific Rim International Conference on Artificial Intelligence, </booktitle> <pages> pp. 23-34, </pages> <month> Springer-Verlag. </month> <title> This article was processed using the L A T E X macro package with LLNCS style </title>
References-found: 18

