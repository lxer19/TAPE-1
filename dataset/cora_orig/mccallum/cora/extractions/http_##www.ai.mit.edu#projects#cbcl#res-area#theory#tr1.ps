URL: http://www.ai.mit.edu/projects/cbcl/res-area/theory/tr1.ps
Refering-URL: http://www.ai.mit.edu/projects/cbcl/res-area/theory/lemm.html
Root-URL: 
Title: Prior Information and Generalized Questions  
Author: Jorg C. Lemm 
Note: This publication can be retrieved by anonymous ftp to publications.ai.mit.edu. The pathname for this publication is: ai-publications/1500-1999/AIM-1598.ps  
Date: 1598 December 1996  141  
Affiliation: MASSACHUSETTS INSTITUTE OF TECHNOLOGY ARTIFICIAL INTELLIGENCE LABORATORY and CENTER FOR BIOLOGICAL AND COMPUTATIONAL LEARNING DEPARTMENT OF BRAIN AND COGNITIVE SCIENCES  
Pubnum: A.I. Memo No.  C.B.C.L Paper No.  
Abstract: This report describes research done within the Center for Biological and Computational Learning in the Department of Brain and Cognitive Sciences at the Massachusetts Institute of Technology. This research is sponsored by a grant from National Science Foundation under contract ASC-9217041 and a grant from ONR/ARPA under contract N00014-92-J-1879. The author was supported by a Postdoctoral Fellowship (Le 1014/1-1) from the Deutsche Forschungsgemeinschaft and a NSF/CISE Postdoctoral Fellowship. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Aarts, E. & Korts, J. </author> <title> (1989) Simulated Annealing and Boltzmann Machines. </title> <address> New York: </address> <publisher> Wiley. </publisher>
Reference-contexts: to C (A AND B) = 1 for C A = 1 or C B = 1; 8 : C B for C A = 1 6= C B ; C (NOT A) = 0 for C A = 1: and one finds that the functions on the whole interval <ref> [0; 1] </ref> are nonlinear. This representation is interesting in as far as the AND is linear and the nonlinearity of the OR can be implemented by just skipping functions with final C (f 0 ) = 1 from F 0 . <p> to one (or zero) to have a nonzero baseline if all other A i are zero (or one). 28 5.3.2 Log-probabilities Assume 0 p 1 is a probability (or for AND and OR a bounded density 0 p c) then the log-probability L = ln p is in the intervals <ref> [0; 1] </ref> (or [ln c; 1], respectively) and the rules for probabilities become L (A AND B) = L (A) + L (BjA) Especially interesting is the OR for disjunct events, where it is equivalent to an XOR. <p> zero) to have a nonzero baseline if all other A i are zero (or one). 28 5.3.2 Log-probabilities Assume 0 p 1 is a probability (or for AND and OR a bounded density 0 p c) then the log-probability L = ln p is in the intervals [0; 1] (or <ref> [ln c; 1] </ref>, respectively) and the rules for probabilities become L (A AND B) = L (A) + L (BjA) Especially interesting is the OR for disjunct events, where it is equivalent to an XOR. <p> Anal ogously, we get L I 2 = ~g ( 2 Conversely, using for the temperature fi fl M = 1 + fi fl M has the advantage that the infinite interval <ref> [0; 1] </ref> is mapped into the finite interval [0; 1]. <p> Anal ogously, we get L I 2 = ~g ( 2 Conversely, using for the temperature fi fl M = 1 + fi fl M has the advantage that the infinite interval <ref> [0; 1] </ref> is mapped into the finite interval [0; 1]. <p> One sees that it can make a big difference to test different equal spaced values of either fi or fl for example in cross-validation. The parameter fl has the advantage of being completely in the interval <ref> [0; 1] </ref>. Figs.15 and 16 compare the saddle point approximation with the full Bayesian approach depending on the distance of the data a to the two templates T i = 1. <p> = 1:2 for a = 0:6 (thick) , 0.0, 0.6 (thin); row 6: left: a vs. y min for fi = 0; 0:5 (thick), 1.0, 1.2 (dashed), 1.5, 2.0, 2.5, 3.0; right: function at fi = 1:2, 8 a 8. 84 parameterized by fl = fi 1+fi taking values in <ref> [0; 1] </ref>, cor responding to fi = fl 1fl . <p> 0:15; right: cuts at fl = 0:15 for a = 0:5 (thick), 0.0, 0.5 (thin); row 6: left: a vs. y min for 0 fl 1 by steps of 0:2 (thick: fl = 2=3, fl = 0:85); right: function at fl = 0:85, 1 a 1. 1+fi taking values in <ref> [0; 1] </ref>, so that fi = fl 1fl . <p> Including such a `template-distance' loss favors a low temperature approximation and improves the validity of the mean field solution. 86 diagram shows the two templates T 1 and T 2 and data (drawn from the interval <ref> [1; 30] </ref>). The upper right diagram shows the state of nature f 0 (thickly dashed) and T 1 . <p> examples of templates we choose (see the two dashed curves in the upper left picture in Fig.17) T 1 = sin 3 (x 1) T 2 = sin 2 3 (x 1) with m = 40 and a i adjusted so that both functions have mean zero on the interval <ref> [1; 40] </ref>. We consider the case that the learner expects the actual function to be similar, but not identical, to either T 1 OR T 2 . Thus, the templates represent function prototypes. <p> For comparison also shown are, data (points) and the two templates T O i Fig.17) for the given D , 0 . The bars show the (mean square) generalization error (gray) calculated for 1000 newly generated random points in the intervals <ref> [1; 40] </ref> (left) or [1; 30] (right), respectively, and, here and in the following, always normalized with respect to the largest of the errors for all five cases (L M with y 0 = T 1 , T 2 , T random , and linear regularization with T , T 0 <p> For comparison also shown are, data (points) and the two templates T O i Fig.17) for the given D , 0 . The bars show the (mean square) generalization error (gray) calculated for 1000 newly generated random points in the intervals [1; 40] (left) or <ref> [1; 30] </ref> (right), respectively, and, here and in the following, always normalized with respect to the largest of the errors for all five cases (L M with y 0 = T 1 , T 2 , T random , and linear regularization with T , T 0 ) under the same <p> Then data are drawn from f 0 with a Gaussian distribution with = 0:2 and mean y x (f 0 ) from the interval <ref> [1; 30] </ref>. Thus, the task can be seen as a simple two-template prediction or reconstructing problem. with the inter-vall [31; 40] representing either future values (for time series) or a hidden area (in image reconstruction).
Reference: [2] <author> Abu-Mostafa, Y. </author> <title> (1990) Learning from Hints in Neural Networks. </title> <journal> Journal of Complexity 6, </journal> <pages> 192-198. </pages>
Reference: [3] <author> Abu-Mostafa, Y. </author> <title> (1993a) Hints and the VC Dimension. </title> <booktitle> Neural Computation 5, </booktitle> <pages> 278-288. </pages>
Reference: [4] <author> Abu-Mostafa, Y. </author> <title> (1993b) A method for learning from hints. </title> <booktitle> Advances in Neural Information Processing Systems 5, </booktitle> <editor> S. Hanson et al (eds), </editor> <address> 73-80, San Mateo, CA: </address> <publisher> Morgan Kauffmann. </publisher>
Reference: [5] <author> Ackley, D.H., Hinton, G.E., & Sejnowski, T.J. </author> <title> (1985) A Learning Algorithm for Boltzmann machines. </title> <booktitle> Cognitive science 9, </booktitle> <pages> 147-169. </pages>
Reference: [6] <author> Alder, B.J. & Wainwright, T.E. </author> <title> (1959) Studies in molecular dynamics. I. General method. </title> <journal> Journal of Chemical Physics 31,459-466. </journal>
Reference: [7] <author> Allgower, E.L. & Georg, K. </author> <title> (1990) Numerical Continuation Methods, </title> <publisher> Berlin: Springer-Verlag. </publisher>
Reference: [8] <author> Allen, </author> <title> D.M. (1974) The relationship between variable selection and data augmentation and a method of prediction. </title> <type> Technometrics 16, 125. </type>
Reference: [9] <author> Amari, S. </author> <title> (1985) Differential geometrical methods in statistics. </title> <booktitle> Springer Lecture Notes in Statistics, </booktitle> <volume> 28, </volume> <publisher> Springer-Verlag. </publisher>
Reference: [10] <author> Amari, S. </author> <title> (1995) Information Geometry of the EM and em Algorithms for Neural Networks. </title> <booktitle> Neural Networks 8 (9), </booktitle> <pages> 1379-1408. </pages>
Reference: [11] <author> Amit, D., Gutfreund, H., & Sompolinski, H. </author> <title> (1987) Statistical Mechanics of Neural Networks Near Saturation. </title> <journal> Annals of Physics 173, </journal> <pages> 30-67. </pages>
Reference: [12] <author> Amit, D. </author> <title> (1989) Modeling Brain Function. </title> <publisher> Cam-bridge: Cambridge University Press. </publisher>
Reference: [13] <author> Austern, N. </author> <title> (1970) Direct Nuclear Reactions. </title> <address> New York: </address> <publisher> Wiley. </publisher>
Reference: [14] <author> Balasubramanian, V. </author> <title> (1996) Statistical Inference, Occam's Razor and Statistical Mechanics on the Space of Probability Distributions cond-mat/9601030. </title>
Reference: [15] <author> Balian, R. </author> <note> (1991) From Microphysics to Macro-physics. Vol. </note> <editor> I. </editor> <publisher> Berlin: Springer. </publisher>
Reference: [16] <author> Barber, D. & Williams, C.K.I. </author> <title> (1997) Gaussian processes for Bayesian classification via hybrid Monte Carlo. </title> <note> To appear in Advances in Neural Information Processing Systems 9. </note>
Reference: [17] <author> Barndorff-Nielsen, O.E. </author> <title> (1978) Information and exponential families in statistical theory. </title> <address> New York: </address> <publisher> Wiley. </publisher>
Reference: [18] <author> Bazaraa, M.S., Sherali, H.D., & Shetty, </author> <title> C.M. (1993) Nonlinear Programming. (2nd ed.) </title> <address> New York: Wi-ley. </address>
Reference: [19] <author> Beck, C. & Schlogl, F. </author> <title> (1993) Thermodynamics of chaotic systems. </title> <publisher> Cambridge University Press. </publisher>
Reference: [20] <author> Berger, J.O. </author> <title> (1985) Statistical Decision Theory and Bayesian Analysis. (Second Ed.),New York: </title> <publisher> Springer. </publisher>
Reference: [21] <author> Bertsekas, </author> <title> D.P. (1995) Nonlinear Programming. </title> <address> Belmont, MA: </address> <publisher> Athena Scientific. </publisher>
Reference: [22] <author> Berry, </author> <title> M.V. </title> <booktitle> (1966) Proc. </booktitle> <institution> Phys. Soc. </institution> <address> (London) 89, </address> <month> 479. </month>
Reference: [23] <author> Besag, J. & Green, P.J. </author> <title> (1993) Spatial statistics and Bayesian computation. </title> <journal> Journal of the Royal Statistical Society B 55, </journal> <pages> 25-102. </pages>
Reference: [24] <author> Bialek, W., Callan, </author> <title> C.G., & Strong, S.P. (1996) Field Theories for Learning Probability Distributions. </title> <publisher> cond-mat/9607180. </publisher>
Reference: [25] <author> Binder, K. (ed.) </author> <title> (1986) Monte Carlo methods in statistical physics. (2nd ed.) </title> <publisher> Berlin: Springer-Verlag. </publisher>
Reference: [26] <author> Binder, K. (ed.) </author> <title> (1987) Applications of the Monte Carlo methods in statistical physics. (2nd ed.) </title> <publisher> Berlin: Springer-Verlag. </publisher>
Reference: [27] <author> Binder, K. (ed.) </author> <title> (1995) The Monte Carlo method in condensed matter physics. (2nd ed.) </title> <publisher> Berlin: Springer-Verlag. </publisher> <pages> 94 </pages>
Reference: [28] <author> Binder, K., Heermann, D.W. </author> <title> (1988) Monte Carlo simulation in statistical physics: an introduction. </title> <publisher> Berlin: Springer-Verlag. </publisher>
Reference: [29] <author> Binney, J.-J., Dowrick, N.J., Fisher, A.J., & New-man, M.E.J. </author> <title> (1992) The Theory of Critical Phenomena. </title> <publisher> Oxford Science Publications. Oxford: Claren-don Press. </publisher>
Reference: [30] <author> Bishop, </author> <title> C.M. (1995) Training with noise is equivalent to Tikhonov regularization. </title> <booktitle> Neural Computation 7 (1), </booktitle> <pages> 108-116. </pages>
Reference-contexts: Including such a `template-distance' loss favors a low temperature approximation and improves the validity of the mean field solution. 86 diagram shows the two templates T 1 and T 2 and data (drawn from the interval <ref> [1; 30] </ref>). The upper right diagram shows the state of nature f 0 (thickly dashed) and T 1 . <p> For comparison also shown are, data (points) and the two templates T O i Fig.17) for the given D , 0 . The bars show the (mean square) generalization error (gray) calculated for 1000 newly generated random points in the intervals [1; 40] (left) or <ref> [1; 30] </ref> (right), respectively, and, here and in the following, always normalized with respect to the largest of the errors for all five cases (L M with y 0 = T 1 , T 2 , T random , and linear regularization with T , T 0 ) under the same <p> Then data are drawn from f 0 with a Gaussian distribution with = 0:2 and mean y x (f 0 ) from the interval <ref> [1; 30] </ref>. Thus, the task can be seen as a simple two-template prediction or reconstructing problem. with the inter-vall [31; 40] representing either future values (for time series) or a hidden area (in image reconstruction).
Reference: [31] <author> Bishop, </author> <title> C.M. (1995) Neural Networks for Pattern Recognition. </title> <publisher> Oxford: Oxford University Press. </publisher>
Reference-contexts: Then data are drawn from f 0 with a Gaussian distribution with = 0:2 and mean y x (f 0 ) from the interval [1; 30]. Thus, the task can be seen as a simple two-template prediction or reconstructing problem. with the inter-vall <ref> [31; 40] </ref> representing either future values (for time series) or a hidden area (in image reconstruction). See the thickly dashed curve in the upper right picture in Fig.17 for the f 0 used for the results discussed in the following.
Reference: [32] <author> Blatt, M., Wiseman, S., & Domany, E. </author> <title> (1997) Data Clustering Using a Model Granular Magnet, Neural Computation (in print) </title>
Reference: [33] <author> Bleistein, N. & Handelsman, R.A. </author> <title> (1986) Asymptotic Expansions of Integrals. </title> <address> New York: </address> <publisher> Dover. </publisher>
Reference: [34] <author> Buntine, W.L. & Weigend, </author> <title> A.S. (1991) Bayesian back-propagation. </title> <booktitle> Complex Systems 5, </booktitle> <pages> 603-643. </pages>
Reference: [35] <author> Connor, J.N.L. & Marcus, </author> <title> R.A. </title> <journal> (1971) J. Chem. Phys. </journal> <volume> 55, </volume> <pages> 5636. </pages>
Reference: [36] <author> Davis, L. (ed.) </author> <title> (1987) Genetic Algorithms and Simulated Annealing. </title> <publisher> Morgan Kaufmann. </publisher>
Reference: [37] <editor> Davis, L. (ed.) </editor> <booktitle> (1991) Handbook of Genetic Algorithms. </booktitle> <publisher> Van Nostrand Reinhold. </publisher>
Reference: [38] <author> Dayan, P., Hinton, G.E., Neal, R.M., & Zemel, </author> <title> R.S. </title> <booktitle> (1995) The Helmholtz Machine. Neural Computation 7, </booktitle> <pages> 889-904. </pages>
Reference: [39] <author> De Bruijn, N.G. </author> <title> (1981) Asymptotic Methods in Analysis. </title> <address> New York: </address> <publisher> Dover. </publisher>
Reference: [40] <author> Dempster, A.P., Laird, N.M., & Rubin, </author> <title> D.B. (1976) Maximum likelihood from incomplete data via the EM algorithm. </title> <journal> Proceedings of the Royal Statistical Society, </journal> <pages> 1-38. </pages>
Reference-contexts: examples of templates we choose (see the two dashed curves in the upper left picture in Fig.17) T 1 = sin 3 (x 1) T 2 = sin 2 3 (x 1) with m = 40 and a i adjusted so that both functions have mean zero on the interval <ref> [1; 40] </ref>. We consider the case that the learner expects the actual function to be similar, but not identical, to either T 1 OR T 2 . Thus, the templates represent function prototypes. <p> For comparison also shown are, data (points) and the two templates T O i Fig.17) for the given D , 0 . The bars show the (mean square) generalization error (gray) calculated for 1000 newly generated random points in the intervals <ref> [1; 40] </ref> (left) or [1; 30] (right), respectively, and, here and in the following, always normalized with respect to the largest of the errors for all five cases (L M with y 0 = T 1 , T 2 , T random , and linear regularization with T , T 0 <p> Then data are drawn from f 0 with a Gaussian distribution with = 0:2 and mean y x (f 0 ) from the interval [1; 30]. Thus, the task can be seen as a simple two-template prediction or reconstructing problem. with the inter-vall <ref> [31; 40] </ref> representing either future values (for time series) or a hidden area (in image reconstruction). See the thickly dashed curve in the upper right picture in Fig.17 for the f 0 used for the results discussed in the following.
Reference: [41] <author> Derka, R., Buzek, V., Adam, G., & Knight, </author> <title> P.L. (1996) From quantum Bayesian inference to quantum tomography. </title> <publisher> quant-ph/9701029. </publisher>
Reference: [42] <author> Devaney, </author> <title> R.L. (1986) An Introduction to Chaotic Dynamical Systems. </title> <address> Menlo Park: </address> <publisher> Benjamin/- Cummings. </publisher>
Reference: [43] <author> Dudley, </author> <title> R.M. (1984) A course on empirical processes. </title> <booktitle> Lecture Notes in Mathematics, </booktitle> <volume> 1097 </volume> <pages> 2-142. </pages>
Reference: [44] <author> Durbin, R., & Willshaw, D. </author> <title> (1987) An analog approach to the traveling salesman problem using an elastic net method. </title> <booktitle> Nature (London) 326, </booktitle> <pages> 689-691. </pages>
Reference: [45] <author> Efron, B. </author> & <title> Tibshirani R.J. (1993) An Introduction to the Bootstrap. </title> <address> New York: </address> <publisher> Chapman & Hall. </publisher>
Reference: [46] <author> Everitt, B.S. </author> & <title> Hand, D.J. (1981) Finite Mixture Distributions. </title> <publisher> London: Chapman and Hall. </publisher>
Reference: [47] <author> Feller, W. </author> <title> (1957) An Introduction to Probability Theory and Its Applications. </title> <booktitle> Vol. </booktitle> <address> I (2nd edition) New York: </address> <publisher> Wiley. </publisher>
Reference: [48] <author> Fernandez, R., Frohlich, J., & Sokal, </author> <title> A.D. (1992) Random Walks, Critical Phenomena, </title> <booktitle> and Triviality in Quantum Field Theory. </booktitle> <address> Berlin: </address> <publisher> Springer-Verlag. </publisher>
Reference: [49] <author> Ferraro, M. </author> <title> (1992) Invariant Pattern Representations and Lie Groups Theory. </title> <journal> Advances in Electronics and Electron Physics, </journal> <volume> 84, </volume> <pages> 131. </pages>
Reference: [50] <author> Gardiner, C.W. </author> <title> (1990) Handbook of Stochastic Methods. </title> <address> (2nd edition) Berlin: </address> <publisher> Springer Verlag. </publisher>
Reference: [51] <author> Gelfand, A.E. & Smith, A.F.M. </author> <title> (1990) Sampling-based approaches to calculating marginal densities. </title> <journal> Journal of the American Statistical Association 85, </journal> <pages> 398-409. </pages>
Reference: [52] <author> Gelfand, A.E., Hills, S.E., Racine-Poon, A., & Smith, A.F.M. </author> <title> (1990) Illustration of Bayesian inference in normal data models using Gibbs sampling. </title> <journal> Journal of the American Statistical Association 85, </journal> <pages> 972-985. </pages>
Reference: [53] <author> Gelman, A., Carlin, J.B., Stern, H.S., & Rubin, </author> <title> D.B. (1995) Bayesian Data Analysis. </title> <address> New York: </address> <publisher> Chapman & Hall. </publisher>
Reference: [54] <author> Geman, S & Geman, D. </author> <title> (1984) Stochastic relaxation, Gibbs distributions and the Bayesian restoration of images. </title> <journal> IEEE Trans. on Pattern Analysis and Machine Intelligence 6, </journal> <pages> 721-741. </pages> <editor> Reprinted in Shafer & Pearl (eds.) </editor> <booktitle> (1990) Readings in Uncertainty Reasoning. </booktitle> <address> San Mateo, CA: </address> <publisher> Morgan Kauf-mann. </publisher>
Reference: [55] <author> Geyer, C. </author> <title> (1992) Practical Markov chain Monte Carlo (with discussion). </title> <booktitle> Statistical Science 7, </booktitle> <pages> 473-511. </pages>
Reference: [56] <author> Giraud, B.G. </author> & <title> Nagarajan, </title> <address> M.A. (1991) Ann. Phys. (N.Y.) 212, </address> <month> 260. </month>
Reference: [57] <author> Girosi, F. & Chan, </author> <title> N.T. (1995) Prior Knowledge and the Creation of Virtual Examples for RBF Networks. </title> <booktitle> IEEE Workshop on Neural Networks for Signal Processing, </booktitle> <address> September 1995, Cambridge, MA. </address>
Reference: [58] <author> Girosi, F., Jones, M., & Poggio, T. </author> <title> (1995) Regularization Theory and Neural Networks Architectures. </title> <booktitle> Neural Computation 7 (2), </booktitle> <pages> 219-269. 95 </pages>
Reference: [59] <editor> Glimm, J. & Jaffe, A., </editor> <booktitle> (1987) Quntum Physics. </booktitle>
References-found: 59

