URL: http://hobart.cs.umass.edu/~mmedia/postscript/pamitext98.ps.gz
Refering-URL: http://hobart.cs.umass.edu/~mmedia/mm.html
Root-URL: 
Email: Email:fvwu,manmathag@cs.umass.edu  
Title: TextFinder: An Automatic System To Detect And Recognize Text In Images  
Author: Victor Wu, R. Manmatha, Edward M. Riseman 
Date: November 18, 1997  
Address: Amherst, MA 01003-4610  
Affiliation: Multimedia Indexing And Retrieval Group Computer Science Department University of Massachusetts,  
Abstract: fl This material is based on work supported in part by the National Science Foundation, Library of Congress and Department of Commerce under cooperative agreement number EEC-9209623, in part by the United States Patent and Trademark Office and Defense Advanced Research Projects Agency/ITO under ARPA order number D468, issued by ESC/AXS contract number F19628-95-C-0235, in part by the National Science Foundation under grant number IRI-9619117 and in part by NSF Multimedia CDA-9502639. Any opinions, findings and conclusions or recommendations expressed in this material are the author(s) and do not necessarily reflect those of the sponsors. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> H. S. Baird and K. Thompson. </author> <title> Reading Chess. </title> <journal> IEEE Trans. Pattern Anal. Mach. Intell., </journal> <volume> 12(6) </volume> <pages> 552-559, </pages> <year> 1990. </year>
Reference-contexts: However, most such schemes require clean binary input [4, 18, 19, 20]; some assume specific document layouts such as newspapers [8] and technical journals [11]; others utilize domain-specific knowledge such as mail address blocks [14] or configurations of chess games <ref> [1] </ref>. There is thus a need for systems which extract and recognize text from general backgrounds. In this paper, a new end-to-end system is proposed which automatically extracts and recognizes text in images. The system takes greyscale images as input 1 . <p> Although the texture segmentation scheme in [6] can in principle be applied to greyscale images, it was only used on binary document images, and in addition, the binarization problem was not addressed. Other systems utilize domain-specific knowledge such as mail address blocks [14] or configurations of chess games <ref> [1] </ref>. In summary, although a considerable amount of work has been done on different aspects of document analysis and understanding, few working systems have been reported that can read text from document pages with both structured and non-structured layouts and textured or hatched backgrounds.
Reference: [2] <editor> Mindy Bokser. Omnidocument Technoligies. </editor> <booktitle> Proceedings of The IEEE, </booktitle> <volume> 80(7) </volume> <pages> 1066-1078, </pages> <month> July </month> <year> 1992. </year>
Reference-contexts: To build digital libraries, this large volume of information needs to be digitized into images and the text converted to ASCII for storage, retrieval, and easy manipulation. However, current OCR technology <ref> [2, 10] </ref> is largely restricted to finding text printed against clean backgrounds, and cannot handle text printed against shaded or textured backgrounds, and/or embedded in images.
Reference: [3] <author> K. Etemad, D. Doermann, and R. Chellapa. </author> <title> Multiscale Segmentation of Unstructured Document Pages using Soft Decision Integration. </title> <journal> IEEE Transactions on Pattern Analysis And Machine Intelligence, </journal> <volume> 19(1) </volume> <pages> 92-96, </pages> <month> Jan. </month> <year> 1997. </year>
Reference-contexts: Some work has been done to detect text using color information [22]. Smith and Kanade [16] developed a simple technique to detect text in video images. Although fast, the technique is not robust. Etemad et al <ref> [3] </ref> used a neural net to classify the output of wavelets into text and non-text regions. The neural net requires a rich set of training examples to work effectively. The top-down and bottom-up approaches require the input image to be binary.
Reference: [4] <author> Lloyd Alan Fletcher and Rangachar Kasturi. </author> <title> A Robust Algorithm for Text String Separation from Mixed Text/Graphics Images. </title> <journal> IEEE Transactions on Pattern Analysis And Machine Intelligence, </journal> <volume> 10(6) </volume> <pages> 910-918, </pages> <month> Nov. </month> <year> 1988. </year>
Reference-contexts: More sophisticated text 2 reading systems usually employ document analysis (page segmentation) schemes to identify text regions before applying OCR so that the OCR engine does not spend time trying to interpret non-text items. However, most such schemes require clean binary input <ref> [4, 18, 19, 20] </ref>; some assume specific document layouts such as newspapers [8] and technical journals [11]; others utilize domain-specific knowledge such as mail address blocks [14] or configurations of chess games [1]. There is thus a need for systems which extract and recognize text from general backgrounds. <p> Bottom--up methods work by grouping small components (starting with pixels as connected components) into successively larger components until all blocks are found on the page <ref> [4, 12] </ref>. The third category of document segmentation methods treat text as a type of texture and hence use texture segmentation algorithms to detect text [6, 7]. Some work has been done to detect text using color information [22].
Reference: [5] <author> C. A. Glasbey. </author> <title> An Analysis of Histogram-Based Thresholding Algorithms. CVGIP: Graphical Models and Image Processing, </title> <booktitle> 55(6) </booktitle> <pages> 532-537, </pages> <month> Nov. </month> <year> 1993. </year>
Reference: [6] <author> Anil K. Jain and Sushil Bhattacharjee. </author> <title> Text Segmentation Using Gabor Filters for Automatic Document Processing. </title> <journal> Machine Vision and Applications, </journal> <volume> 5 </volume> <pages> 169-184, </pages> <year> 1992. </year> <month> 33 </month>
Reference-contexts: The third category of document segmentation methods treat text as a type of texture and hence use texture segmentation algorithms to detect text <ref> [6, 7] </ref>. Some work has been done to detect text using color information [22]. Smith and Kanade [16] developed a simple technique to detect text in video images. Although fast, the technique is not robust. <p> The projection profile based schemes work if the page has a Manhattan layout: that is, there is only one skew angle and the page can be segmented by horizontal and vertical cuts. Although the texture segmentation scheme in <ref> [6] </ref> can in principle be applied to greyscale images, it was only used on binary document images, and in addition, the binarization problem was not addressed. Other systems utilize domain-specific knowledge such as mail address blocks [14] or configurations of chess games [1]. <p> Thus, one natural way to detect text is by using texture segmentation. A standard approach to texture segmentation is to first filter the image using a bank of linear filters, such as Gaussian derivatives ([9] or Gabor functions <ref> [6] </ref> followed by some non-linear transformation such as half-wave rectification, full-wave rectification, or a hyperbolic function tanh (fft). Then features are computed to form a feature vector for each pixel from the filtered images. These feature vectors are then classified to segment the textures into different classes.
Reference: [7] <author> Anil K. Jain and Yu Zhong. </author> <title> Page Segmentation Using Texture Analysis. </title> <journal> Pattern Recog--nition, </journal> <volume> 29(5) </volume> <pages> 743-770, </pages> <year> 1996. </year>
Reference-contexts: The third category of document segmentation methods treat text as a type of texture and hence use texture segmentation algorithms to detect text <ref> [6, 7] </ref>. Some work has been done to detect text using color information [22]. Smith and Kanade [16] developed a simple technique to detect text in video images. Although fast, the technique is not robust.
Reference: [8] <author> Mohamed Kamel and Aiguo Zhao. </author> <title> Extraction of Binary Character/Graphics Images from Grayscale Document Images. Computer Vision, </title> <journal> Graphics and Image Processing, </journal> <volume> 55(3) </volume> <pages> 203-217, </pages> <month> May. </month> <year> 1993. </year>
Reference-contexts: However, most such schemes require clean binary input [4, 18, 19, 20]; some assume specific document layouts such as newspapers <ref> [8] </ref> and technical journals [11]; others utilize domain-specific knowledge such as mail address blocks [14] or configurations of chess games [1]. There is thus a need for systems which extract and recognize text from general backgrounds. <p> Unfortunately, global thresholding is usually not possible for complicated images, as noted by many researchers ([13], [17]). Consequently, current OCR systems work poorly in these cases. One solution to the global thresholding problem is to use different thresholds for different local regions (adaptive thresholding) <ref> [8] </ref>. Trier and Taxt [17] report an evaluation of eleven local adaptive thresholding schemes. Many document segmentation methods have been proposed in the literature. Some of these methods are top-down approaches, some are bottom-up schemes, and others are based on texture segmentation schemes in computer vision.
Reference: [9] <author> Jitendra Malik and Pietro Perona. </author> <title> Preattentive texture discrimination with early vision mechanisms. </title> <journal> J. Opt. Soc. Am., </journal> <volume> 7(5) </volume> <pages> 923-932, </pages> <month> May </month> <year> 1990. </year>
Reference: [10] <author> S. Mori, C. Y. Suen, and K. Yamamoto. </author> <title> Historical Review of OCR Research and Development. </title> <booktitle> Proceedings of The IEEE, </booktitle> <volume> 80(7) </volume> <pages> 1029-1058, </pages> <month> July </month> <year> 1992. </year>
Reference-contexts: To build digital libraries, this large volume of information needs to be digitized into images and the text converted to ASCII for storage, retrieval, and easy manipulation. However, current OCR technology <ref> [2, 10] </ref> is largely restricted to finding text printed against clean backgrounds, and cannot handle text printed against shaded or textured backgrounds, and/or embedded in images.
Reference: [11] <author> G. Nagy, S. Seth, and M. Viswanathan. </author> <title> A Prototype Document Image Analysis System for Technical Journals. </title> <booktitle> Computer, </booktitle> <pages> pages 10-22, </pages> <month> July </month> <year> 1992. </year>
Reference-contexts: However, most such schemes require clean binary input [4, 18, 19, 20]; some assume specific document layouts such as newspapers [8] and technical journals <ref> [11] </ref>; others utilize domain-specific knowledge such as mail address blocks [14] or configurations of chess games [1]. There is thus a need for systems which extract and recognize text from general backgrounds. In this paper, a new end-to-end system is proposed which automatically extracts and recognizes text in images. <p> Classic top-down techniques are based on the run length smoothing (RLS) algorithm [18, 20] to smooth the image first, then, horizontal and vertical projection profiles [19] are commonly used to 5 cut the page into smaller blocks such as columns and paragraphs <ref> [11, 15, 19] </ref>. Bottom--up methods work by grouping small components (starting with pixels as connected components) into successively larger components until all blocks are found on the page [4, 12].
Reference: [12] <author> Lawrence O'Gorman. </author> <title> The Document Spectrum for Page Layout Analysis. </title> <journal> IEEE Trans. Pattern Analysis and Machine Intelligence, </journal> <volume> 15(11) </volume> <pages> 1162-1173, </pages> <month> Nov. </month> <year> 1993. </year>
Reference-contexts: Bottom--up methods work by grouping small components (starting with pixels as connected components) into successively larger components until all blocks are found on the page <ref> [4, 12] </ref>. The third category of document segmentation methods treat text as a type of texture and hence use texture segmentation algorithms to detect text [6, 7]. Some work has been done to detect text using color information [22].
Reference: [13] <author> Lawrence O'Gorman. </author> <title> Binarization and Multithresholding of Document Images Using Connectivity. Computer Vision, </title> <journal> Graphics and Image Processing, </journal> <volume> 56(6) </volume> <pages> 494-506, </pages> <month> Nov. </month> <year> 1994. </year>
Reference: [14] <author> Paul W. Palumbo, Sargur N. Srihari, Jung Soh, Ramalingam Sridhar, and Victor Dem-janenko. </author> <title> Postal Address Block Location in Real Time. </title> <booktitle> Computer, </booktitle> <pages> pages 34-42, </pages> <month> July </month> <year> 1992. </year>
Reference-contexts: However, most such schemes require clean binary input [4, 18, 19, 20]; some assume specific document layouts such as newspapers [8] and technical journals [11]; others utilize domain-specific knowledge such as mail address blocks <ref> [14] </ref> or configurations of chess games [1]. There is thus a need for systems which extract and recognize text from general backgrounds. In this paper, a new end-to-end system is proposed which automatically extracts and recognizes text in images. The system takes greyscale images as input 1 . <p> Although the texture segmentation scheme in [6] can in principle be applied to greyscale images, it was only used on binary document images, and in addition, the binarization problem was not addressed. Other systems utilize domain-specific knowledge such as mail address blocks <ref> [14] </ref> or configurations of chess games [1]. In summary, although a considerable amount of work has been done on different aspects of document analysis and understanding, few working systems have been reported that can read text from document pages with both structured and non-structured layouts and textured or hatched backgrounds.
Reference: [15] <author> Theo Pavlidis and Jiangying Zhou. </author> <title> Page Segmentation and Classification. CVGIP: Graphical Models and Image Processing, </title> <booktitle> 54(6) </booktitle> <pages> 484-496, </pages> <month> Nov. </month> <year> 1992. </year> <month> 34 </month>
Reference-contexts: Classic top-down techniques are based on the run length smoothing (RLS) algorithm [18, 20] to smooth the image first, then, horizontal and vertical projection profiles [19] are commonly used to 5 cut the page into smaller blocks such as columns and paragraphs <ref> [11, 15, 19] </ref>. Bottom--up methods work by grouping small components (starting with pixels as connected components) into successively larger components until all blocks are found on the page [4, 12].
Reference: [16] <author> M. A. Smith and T. Kanade. </author> <title> Video Skimming and Characterization Through the Com--bination of Image and Language Understanding Techniques. </title> <booktitle> Proc. of the IEEE CVPR '97, </booktitle> <pages> pages 775-781, </pages> <month> June 17 - 19 </month> <year> 1992. </year> <title> [17] ivind Due Trier and Torfinn Taxt. Evaluation of Binarization Methods for Document Images. </title> <journal> IEEE Transactions on Pattern Analysis And Machine Intelligence, </journal> <volume> 17(3) </volume> <pages> 312-315, </pages> <month> March </month> <year> 1995. </year>
Reference-contexts: The third category of document segmentation methods treat text as a type of texture and hence use texture segmentation algorithms to detect text [6, 7]. Some work has been done to detect text using color information [22]. Smith and Kanade <ref> [16] </ref> developed a simple technique to detect text in video images. Although fast, the technique is not robust. Etemad et al [3] used a neural net to classify the output of wavelets into text and non-text regions. The neural net requires a rich set of training examples to work effectively.
Reference: [18] <author> F. M. Wahl, K. Y. Wong, and R. G. Casey. </author> <title> Block Segmentation and Text Extraction in Mixed Text/Image Documents. </title> <journal> Computer Graphics and Image Processing, </journal> <volume> 20 </volume> <pages> 375-390, </pages> <year> 1982. </year>
Reference-contexts: More sophisticated text 2 reading systems usually employ document analysis (page segmentation) schemes to identify text regions before applying OCR so that the OCR engine does not spend time trying to interpret non-text items. However, most such schemes require clean binary input <ref> [4, 18, 19, 20] </ref>; some assume specific document layouts such as newspapers [8] and technical journals [11]; others utilize domain-specific knowledge such as mail address blocks [14] or configurations of chess games [1]. There is thus a need for systems which extract and recognize text from general backgrounds. <p> Many document segmentation methods have been proposed in the literature. Some of these methods are top-down approaches, some are bottom-up schemes, and others are based on texture segmentation schemes in computer vision. Classic top-down techniques are based on the run length smoothing (RLS) algorithm <ref> [18, 20] </ref> to smooth the image first, then, horizontal and vertical projection profiles [19] are commonly used to 5 cut the page into smaller blocks such as columns and paragraphs [11, 15, 19].
Reference: [19] <author> D. Wang and S. N. Srihari. </author> <title> Classification of Newspaper Image Blocks Using Texture Analysis. Computer Vision, </title> <journal> Graphics and Image Processing, </journal> <volume> 47 </volume> <pages> 327-352, </pages> <year> 1989. </year>
Reference-contexts: More sophisticated text 2 reading systems usually employ document analysis (page segmentation) schemes to identify text regions before applying OCR so that the OCR engine does not spend time trying to interpret non-text items. However, most such schemes require clean binary input <ref> [4, 18, 19, 20] </ref>; some assume specific document layouts such as newspapers [8] and technical journals [11]; others utilize domain-specific knowledge such as mail address blocks [14] or configurations of chess games [1]. There is thus a need for systems which extract and recognize text from general backgrounds. <p> Some of these methods are top-down approaches, some are bottom-up schemes, and others are based on texture segmentation schemes in computer vision. Classic top-down techniques are based on the run length smoothing (RLS) algorithm [18, 20] to smooth the image first, then, horizontal and vertical projection profiles <ref> [19] </ref> are commonly used to 5 cut the page into smaller blocks such as columns and paragraphs [11, 15, 19]. Bottom--up methods work by grouping small components (starting with pixels as connected components) into successively larger components until all blocks are found on the page [4, 12]. <p> Classic top-down techniques are based on the run length smoothing (RLS) algorithm [18, 20] to smooth the image first, then, horizontal and vertical projection profiles [19] are commonly used to 5 cut the page into smaller blocks such as columns and paragraphs <ref> [11, 15, 19] </ref>. Bottom--up methods work by grouping small components (starting with pixels as connected components) into successively larger components until all blocks are found on the page [4, 12].
Reference: [20] <author> K. Y. Wong, R. G. Casey, and F. M. Wahl. </author> <title> Document Analysis System. </title> <journal> IBM Journal Res. Dev., </journal> <volume> 26(6) </volume> <pages> 647-656, </pages> <year> 1982. </year>
Reference-contexts: More sophisticated text 2 reading systems usually employ document analysis (page segmentation) schemes to identify text regions before applying OCR so that the OCR engine does not spend time trying to interpret non-text items. However, most such schemes require clean binary input <ref> [4, 18, 19, 20] </ref>; some assume specific document layouts such as newspapers [8] and technical journals [11]; others utilize domain-specific knowledge such as mail address blocks [14] or configurations of chess games [1]. There is thus a need for systems which extract and recognize text from general backgrounds. <p> Many document segmentation methods have been proposed in the literature. Some of these methods are top-down approaches, some are bottom-up schemes, and others are based on texture segmentation schemes in computer vision. Classic top-down techniques are based on the run length smoothing (RLS) algorithm <ref> [18, 20] </ref> to smooth the image first, then, horizontal and vertical projection profiles [19] are commonly used to 5 cut the page into smaller blocks such as columns and paragraphs [11, 15, 19].
Reference: [21] <author> Victor Wu and R. Manmatha. </author> <title> Document Image Clean-up and Binarization. </title> <booktitle> To appear in the Proceedings of IS&T/SPIE Symposium on Electronic Imaging, </booktitle> <month> January </month> <year> 1998. </year>
Reference-contexts: It will be shown that for each chip, a single threshold suffices to clean up and binarize the corresponding region in the input image so that the text stands out. A simple, effective histogram-based algorithm, as described in <ref> [21] </ref>, is used to find the threshold value automatically for each text region. This algorithm is used for the Text Clean-up module in the system. Non-text items might survive the previous processing and occur in the binarized output. <p> The text chips produced usually contain text strings whose characters all roughly have the same intensity. These chips are, therefore, good candidates for local thresholding. The algorithm for background removal and binarization is described in <ref> [21] </ref>. Here, 22 we use Figure 5 to briefly demonstrate how the algorithm works: First, the text chip (Figure 5 (a)) generated by the system is smoothed to produce the chip shown as in Figure 5 (d). <p> This clean-up and binarization procedure has been successfully used on many images (see for example the Experiments section). More discussion can be found in <ref> [21] </ref>. 8 The Text Refinement Experiments show that the text detection phase is able to locate text strings in regular fonts, and some even from script fonts or trademarks. However, sometimes non-text items are identified as text as well.
Reference: [22] <author> Y. Zhong, K. Karu, and Anil K. Jain. </author> <title> Locating Text in Complex Color Images. </title> <journal> Pattern Recognition, </journal> <volume> 28(10) </volume> <pages> 1523-1536, </pages> <month> October </month> <year> 1995. </year> <month> 35 </month>
Reference-contexts: The third category of document segmentation methods treat text as a type of texture and hence use texture segmentation algorithms to detect text [6, 7]. Some work has been done to detect text using color information <ref> [22] </ref>. Smith and Kanade [16] developed a simple technique to detect text in video images. Although fast, the technique is not robust. Etemad et al [3] used a neural net to classify the output of wavelets into text and non-text regions.
References-found: 21

